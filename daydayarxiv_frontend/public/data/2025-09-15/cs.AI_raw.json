[
  {
    "arxiv_id": "2511.05499v1",
    "title": "Weightless Neural Networks for Continuously Trainable Personalized Recommendation Systems",
    "authors": [
      "Rafayel Latif",
      "Satwik Behera",
      "Ali Al-Ebrahim"
    ],
    "abstract": "Given that conventional recommenders, while deeply effective, rely on large distributed systems pre-trained on aggregate user data, incorporating new data necessitates large training cycles, making them slow to adapt to real-time user feedback and often lacking transparency in recommendation rationale. We explore the performance of smaller personal models trained on per-user data using weightless neural networks (WNNs), an alternative to neural backpropagation that enable continuous learning by using neural networks as a state machine rather than a system with pretrained weights. We contrast our approach against a classic weighted system, also on a per-user level, and standard collaborative filtering, achieving competitive levels of accuracy on a subset of the MovieLens dataset. We close with a discussion of how weightless systems can be developed to augment centralized systems to achieve higher subjective accuracy through recommenders more directly tunable by end-users.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05499v1",
    "published_date": "2025-09-15 23:51:12 UTC",
    "updated_date": "2025-09-15 23:51:12 UTC"
  },
  {
    "arxiv_id": "2509.12512v1",
    "title": "DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification",
    "authors": [
      "Fazle Rafsani",
      "Jay Shah",
      "Catherine D. Chong",
      "Todd J. Schwedt",
      "Teresa Wu"
    ],
    "abstract": "Anomaly detection and classification in medical imaging are critical for early diagnosis but remain challenging due to limited annotated data, class imbalance, and the high cost of expert labeling. Emerging vision foundation models such as DINOv2, pretrained on extensive, unlabeled datasets, offer generalized representations that can potentially alleviate these limitations. In this study, we propose an attention-based global aggregation framework tailored specifically for 3D medical image anomaly classification. Leveraging the self-supervised DINOv2 model as a pretrained feature extractor, our method processes individual 2D axial slices of brain MRIs, assigning adaptive slice-level importance weights through a soft attention mechanism. To further address data scarcity, we employ a composite loss function combining supervised contrastive learning with class-variance regularization, enhancing inter-class separability and intra-class consistency. We validate our framework on the ADNI dataset and an institutional multi-class headache cohort, demonstrating strong anomaly classification performance despite limited data availability and significant class imbalance. Our results highlight the efficacy of utilizing pretrained 2D foundation models combined with attention-based slice aggregation for robust volumetric anomaly detection in medical imaging. Our implementation is publicly available at https://github.com/Rafsani/DinoAtten3D.git.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "ACCEPTED at the ICCV 2025 Workshop on Anomaly Detection with Foundation Models",
    "pdf_url": "https://arxiv.org/pdf/2509.12512v1",
    "published_date": "2025-09-15 23:31:40 UTC",
    "updated_date": "2025-09-15 23:31:40 UTC"
  },
  {
    "arxiv_id": "2509.12508v4",
    "title": "Fun-ASR Technical Report",
    "authors": [
      "Keyu An",
      "Yanni Chen",
      "Zhigao Chen",
      "Chong Deng",
      "Zhihao Du",
      "Changfeng Gao",
      "Zhifu Gao",
      "Bo Gong",
      "Xiangang Li",
      "Yabin Li",
      "Ying Liu",
      "Xiang Lv",
      "Yunjie Ji",
      "Yiheng Jiang",
      "Bin Ma",
      "Haoneng Luo",
      "Chongjia Ni",
      "Zexu Pan",
      "Yiping Peng",
      "Zhendong Peng",
      "Peiyao Wang",
      "Hao Wang",
      "Haoxu Wang",
      "Wen Wang",
      "Wupeng Wang",
      "Yuzhong Wu",
      "Biao Tian",
      "Zhentao Tan",
      "Nan Yang",
      "Bin Yuan",
      "Jieping Ye",
      "Jixing Yu",
      "Qinglin Zhang",
      "Kun Zou",
      "Han Zhao",
      "Shengkui Zhao",
      "Jingren Zhou",
      "Yanqiao Zhu"
    ],
    "abstract": "In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Authors are listed in alphabetical order. Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2509.12508v4",
    "published_date": "2025-09-15 23:19:36 UTC",
    "updated_date": "2025-12-19 16:18:03 UTC"
  },
  {
    "arxiv_id": "2509.12495v1",
    "title": "Physical Complexity of a Cognitive Artifact",
    "authors": [
      "Gülce Kardeş",
      "David Krakauer",
      "Joshua Grochow"
    ],
    "abstract": "Cognitive science and theoretical computer science both seek to classify and explain the difficulty of tasks. Mechanisms of intelligence are those that reduce task difficulty. Here we map concepts from the computational complexity of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies through a ``Principle of Materiality''. By analyzing the puzzle's branching factor, measured through search tree outdegree, we quantitatively assess task difficulty and systematically examine how different strategies modify complexity. We incrementally refine a trial-and-error search by layering preprocessing (cognitive chunking), value ordering (cognitive free-sorting), variable ordering (cognitive scaffolding), and pruning (cognitive inference). We discuss how the competent use of artifacts reduces effective time complexity by exploiting physical constraints and propose a model of intelligence as a library of algorithms that recruit the capabilities of both mind and matter.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12495v1",
    "published_date": "2025-09-15 22:39:30 UTC",
    "updated_date": "2025-09-15 22:39:30 UTC"
  },
  {
    "arxiv_id": "2509.21340v1",
    "title": "Cycle is All You Need: More Is Different",
    "authors": [
      "Xin Li"
    ],
    "abstract": "We propose an information-topological framework in which cycle closure is the fundamental mechanism of memory and consciousness. Memory is not a static store but the ability to re-enter latent cycles in neural state space, with invariant cycles serving as carriers of meaning by filtering order-specific noise and preserving what persists across contexts. The dot-cycle dichotomy captures this: transient dots scaffold exploration, while nontrivial cycles encode low-entropy content invariants that stabilize memory. Biologically, polychronous neural groups realize 1-cycles through delay-locked spiking reinforced by STDP, nested within theta-gamma rhythms that enforce boundary cancellation. These micro-cycles compose hierarchically, extending navigation loops into general memory and cognition. The perception-action cycle introduces high-order invariance: closure holds even across sense-act alternations, generalizing ancestral homing behavior. Sheaf-cosheaf duality formalizes this process: sheaves glue perceptual fragments into global sections, cosheaves decompose global plans into actions and closure aligns top-down predictions with bottom-up cycles. Consciousness then arises as the persistence of high-order invariants that integrate (unity) yet differentiate (richness) across contexts. We conclude that cycle is all you need: persistent invariants enable generalization in non-ergodic environments with long-term coherence at minimal energetic cost.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.21340v1",
    "published_date": "2025-09-15 21:48:30 UTC",
    "updated_date": "2025-09-15 21:48:30 UTC"
  },
  {
    "arxiv_id": "2509.12471v1",
    "title": "Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT",
    "authors": [
      "Yiwen Lu",
      "Lu Li",
      "Dazheng Zhang",
      "Xinyao Jian",
      "Tingyin Wang",
      "Siqi Chen",
      "Yuqing Lei",
      "Jiayi Tong",
      "Zhaohan Xi",
      "Haitao Chu",
      "Chongliang Luo",
      "Alexis Ogdie",
      "Brian Athey",
      "Alparslan Turan",
      "Michael Abramoff",
      "Joseph C Cappelleri",
      "Hua Xu",
      "Yun Lu",
      "Jesse Berlin",
      "Daniel I. Sessler",
      "David A. Asch",
      "Xiaoqian Jiang",
      "Yong Chen"
    ],
    "abstract": "Sample size calculations for power analysis are critical for clinical research and trial design, yet their complexity and reliance on statistical expertise create barriers for many researchers. We introduce PowerGPT, an AI-powered system integrating large language models (LLMs) with statistical engines to automate test selection and sample size estimation in trial design. In a randomized trial to evaluate its effectiveness, PowerGPT significantly improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs. 77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3 minutes, p < 0.001). These gains were consistent across various statistical tests and benefited both statisticians and non-statisticians as well as bridging expertise gaps. Already under deployment across multiple institutions, PowerGPT represents a scalable AI-driven approach that enhances accessibility, efficiency, and accuracy in statistical power analysis for clinical research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12471v1",
    "published_date": "2025-09-15 21:35:04 UTC",
    "updated_date": "2025-09-15 21:35:04 UTC"
  },
  {
    "arxiv_id": "2509.12464v1",
    "title": "Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction",
    "authors": [
      "Ryan Lucas",
      "Kayhan Behdin",
      "Zhipeng Wang",
      "Qingquan Song",
      "Shao Tang",
      "Rahul Mazumder"
    ],
    "abstract": "Reasoning language models such as DeepSeek-R1 produce long chain-of-thought traces during inference time which make them costly to deploy at scale. We show that using compression techniques such as neural network pruning produces greater performance loss than in typical language modeling tasks, and in some cases can make the model slower since they cause the model to produce more thinking tokens but with worse performance. We show that this is partly due to the fact that standard LLM pruning methods often focus on input reconstruction, whereas reasoning is a decode-dominated task. We introduce a simple, drop-in fix: during pruning we jointly reconstruct activations from the input and the model's on-policy chain-of-thought traces. This \"Reasoning-Aware Compression\" (RAC) integrates seamlessly into existing pruning workflows such as SparseGPT, and boosts their performance significantly. Code reproducing the results in the paper can be found at: https://github.com/RyanLucas3/RAC",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12464v1",
    "published_date": "2025-09-15 21:19:13 UTC",
    "updated_date": "2025-09-15 21:19:13 UTC"
  },
  {
    "arxiv_id": "2509.12456v1",
    "title": "Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Limit Order Book Dynamics",
    "authors": [
      "Rafael Zimmer",
      "Oswaldo Luiz do Valle Costa"
    ],
    "abstract": "Reinforcement Learning has emerged as a promising framework for developing adaptive and data-driven strategies, enabling market makers to optimize decision-making policies based on interactions with the limit order book environment. This paper explores the integration of a reinforcement learning agent in a market-making context, where the underlying market dynamics have been explicitly modeled to capture observed stylized facts of real markets, including clustered order arrival times, non-stationary spreads and return drifts, stochastic order quantities and price volatility. These mechanisms aim to enhance stability of the resulting control agent, and serve to incorporate domain-specific knowledge into the agent policy learning process. Our contributions include a practical implementation of a market making agent based on the Proximal-Policy Optimization (PPO) algorithm, alongside a comparative evaluation of the agent's performance under varying market conditions via a simulator-based environment. As evidenced by our analysis of the financial return and risk metrics when compared to a closed-form optimal solution, our results suggest that the reinforcement learning agent can effectively be used under non-stationary market conditions, and that the proposed simulator-based environment can serve as a valuable tool for training and pre-training reinforcement learning agents in market-making scenarios.",
    "categories": [
      "q-fin.TR",
      "cs.AI"
    ],
    "primary_category": "q-fin.TR",
    "comment": "9 pages, 8 figures, 3 tables, 31 equations",
    "pdf_url": "https://arxiv.org/pdf/2509.12456v1",
    "published_date": "2025-09-15 21:08:13 UTC",
    "updated_date": "2025-09-15 21:08:13 UTC"
  },
  {
    "arxiv_id": "2509.12446v2",
    "title": "PromptSculptor: Multi-Agent Based Text-to-Image Prompt Optimization",
    "authors": [
      "Dawei Xiang",
      "Wenyan Xu",
      "Kexin Chu",
      "Tianqi Ding",
      "Zixu Shen",
      "Yiming Zeng",
      "Jianchang Su",
      "Wei Zhang"
    ],
    "abstract": "The rapid advancement of generative AI has democratized access to powerful tools such as Text-to-Image models. However, to generate high-quality images, users must still craft detailed prompts specifying scene, style, and context-often through multiple rounds of refinement. We propose PromptSculptor, a novel multi-agent framework that automates this iterative prompt optimization process. Our system decomposes the task into four specialized agents that work collaboratively to transform a short, vague user prompt into a comprehensive, refined prompt. By leveraging Chain-of-Thought reasoning, our framework effectively infers hidden context and enriches scene and background details. To iteratively refine the prompt, a self-evaluation agent aligns the modified prompt with the original input, while a feedback-tuning agent incorporates user feedback for further refinement. Experimental results demonstrate that PromptSculptor significantly enhances output quality and reduces the number of iterations needed for user satisfaction. Moreover, its model-agnostic design allows seamless integration with various T2I models, paving the way for industrial applications.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted to EMNLP 2025 System Demonstration Track",
    "pdf_url": "https://arxiv.org/pdf/2509.12446v2",
    "published_date": "2025-09-15 20:52:11 UTC",
    "updated_date": "2025-09-24 03:51:42 UTC"
  },
  {
    "arxiv_id": "2509.12443v3",
    "title": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow",
    "authors": [
      "Sparsh Gupta",
      "Kamalavasan Kamalakkannan",
      "Maxim Moraru",
      "Galen Shipman",
      "Patrick Diehl"
    ],
    "abstract": "Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM \"agents\" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 6 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.12443v3",
    "published_date": "2025-09-15 20:50:15 UTC",
    "updated_date": "2025-11-17 03:45:19 UTC"
  },
  {
    "arxiv_id": "2509.12440v2",
    "title": "MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts",
    "authors": [
      "Jiayi He",
      "Yangmin Huang",
      "Qianyun Du",
      "Xiangying Zhou",
      "Zhiyang He",
      "Jiaxue Hu",
      "Xiaodong Tao",
      "Lixian Lai"
    ],
    "abstract": "Deploying Large Language Models (LLMs) in medical applications requires fact-checking capabilities to ensure patient safety and regulatory compliance. We introduce MedFact, a challenging Chinese medical fact-checking benchmark with 2,116 expert-annotated instances from diverse real-world texts, spanning 13 specialties, 8 error types, 4 writing styles, and 5 difficulty levels. Construction uses a hybrid AI-human framework where iterative expert feedback refines AI-driven, multi-criteria filtering to ensure high quality and difficulty. We evaluate 20 leading LLMs on veracity classification and error localization, and results show models often determine if text contains errors but struggle to localize them precisely, with top performers falling short of human performance. Our analysis reveals the \"over-criticism\" phenomenon, a tendency for models to misidentify correct information as erroneous, which can be exacerbated by advanced reasoning techniques such as multi-agent collaboration and inference-time scaling. MedFact highlights the challenges of deploying medical LLMs and provides resources to develop factually reliable medical AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12440v2",
    "published_date": "2025-09-15 20:46:21 UTC",
    "updated_date": "2025-11-17 07:14:45 UTC"
  },
  {
    "arxiv_id": "2509.12437v1",
    "title": "Enhancing Physical Consistency in Lightweight World Models",
    "authors": [
      "Dingrui Wang",
      "Zhexiao Sun",
      "Zhouheng Li",
      "Cheng Wang",
      "Youlun Peng",
      "Hongyuan Ye",
      "Baha Zarrouki",
      "Wei Li",
      "Mattia Piccinini",
      "Lei Xie",
      "Johannes Betz"
    ],
    "abstract": "A major challenge in deploying world models is the trade-off between size and performance. Large world models can capture rich physical dynamics but require massive computing resources, making them impractical for edge devices. Small world models are easier to deploy but often struggle to learn accurate physics, leading to poor predictions. We propose the Physics-Informed BEV World Model (PIWM), a compact model designed to efficiently capture physical interactions in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training to improve dynamic object modeling and future prediction. We also introduce a simple yet effective technique, Warm Start, for inference to enhance prediction quality with a zero-shot model. Experiments show that at the same parameter scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score. Moreover, even when compared with the largest baseline model (400M), the smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score with a 28% faster inference speed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.12437v1",
    "published_date": "2025-09-15 20:43:22 UTC",
    "updated_date": "2025-09-15 20:43:22 UTC"
  },
  {
    "arxiv_id": "2509.18145v1",
    "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records",
    "authors": [
      "Syed Ahmad Chan Bukhari",
      "Amritpal Singh",
      "Shifath Hossain",
      "Iram Wajahat"
    ],
    "abstract": "Intensive Care Unit (ICU) patients often present with complex, overlapping signs of physiological deterioration that require timely escalation of care. Traditional early warning systems, such as SOFA or MEWS, are limited by their focus on single outcomes and fail to capture the multi-dimensional nature of clinical decline. This study proposes a multi-label classification framework to predict Care Escalation Triggers (CETs), including respiratory failure, hemodynamic instability, renal compromise, and neurological deterioration, using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are defined through rule-based criteria applied to data from hours 24 to 72 (for example, oxygen saturation below 90, mean arterial pressure below 65 mmHg, creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale score greater than 2). Features are extracted from the first 24 hours and include vital sign aggregates, laboratory values, and static demographics. We train and evaluate multiple classification models on a cohort of 85,242 ICU stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation metrics include per-label precision, recall, F1-score, and Hamming loss. XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory, 0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration, outperforming baseline models. Feature analysis shows that clinically relevant parameters such as respiratory rate, blood pressure, and creatinine are the most influential predictors, consistent with the clinical definitions of the CETs. The proposed framework demonstrates practical potential for early, interpretable clinical alerts without requiring complex time-series modeling or natural language processing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 Figure",
    "pdf_url": "https://arxiv.org/pdf/2509.18145v1",
    "published_date": "2025-09-15 20:40:45 UTC",
    "updated_date": "2025-09-15 20:40:45 UTC"
  },
  {
    "arxiv_id": "2509.12434v2",
    "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization",
    "authors": [
      "Jiahao Yu",
      "Zelei Cheng",
      "Xian Wu",
      "Xinyu Xing"
    ],
    "abstract": "Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues.\n  A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs.\n  While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS.\n  Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents.\n  To bridge this gap, we introduce EntroPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting.\n  EntroPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses.\n  We validate EntroPO by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).\n  To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches.\n  On the \\swebench leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with EntroPO ranks 1st on \\lite and 4th on \\verified on the open-weight leaderboard, surpassed only by models with over 10x more parameters(\\eg$>$350B).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12434v2",
    "published_date": "2025-09-15 20:36:19 UTC",
    "updated_date": "2025-09-21 01:28:26 UTC"
  },
  {
    "arxiv_id": "2509.12431v1",
    "title": "Neural-Quantum-States Impurity Solver for Quantum Embedding Problems",
    "authors": [
      "Yinzhanghao Zhou",
      "Tsung-Han Lee",
      "Ao Chen",
      "Nicola Lanatà",
      "Hong Guo"
    ],
    "abstract": "Neural quantum states (NQS) have emerged as a promising approach to solve second-quantised Hamiltonians, because of their scalability and flexibility. In this work, we design and benchmark an NQS impurity solver for the quantum embedding methods, focusing on the ghost Gutzwiller Approximation (gGA) framework. We introduce a graph transformer-based NQS framework able to represent arbitrarily connected impurity orbitals and develop an error control mechanism to stabilise iterative updates throughout the quantum embedding loops. We validate the accuracy of our approach with benchmark gGA calculations of the Anderson Lattice Model, yielding results in excellent agreement with the exact diagonalisation impurity solver. Finally, our analysis of the computational budget reveals the method's principal bottleneck to be the high-accuracy sampling of physical observables required by the embedding loop, rather than the NQS variational optimisation, directly highlighting the critical need for more efficient inference techniques.",
    "categories": [
      "cond-mat.str-el",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el",
    "comment": "10 pages main text, and 4 figures. Note that YinZhangHao Zhou and Zhanghao Zhouyin are the same person, I use them both",
    "pdf_url": "https://arxiv.org/pdf/2509.12431v1",
    "published_date": "2025-09-15 20:33:10 UTC",
    "updated_date": "2025-09-15 20:33:10 UTC"
  },
  {
    "arxiv_id": "2509.12423v1",
    "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition",
    "authors": [
      "Danielle Cohen",
      "Yoni Halpern",
      "Noam Kahlon",
      "Joel Oren",
      "Omri Berkovitch",
      "Sapir Caduri",
      "Ido Dagan",
      "Anatoly Efros"
    ],
    "abstract": "Understanding user intents from UI interaction trajectories remains a challenging, yet crucial, frontier in intelligent agent development. While massive, datacenter-based, multi-modal large language models (MLLMs) possess greater capacity to handle the complexities of such sequences, smaller models which can run on-device to provide a privacy-preserving, low-cost, and low-latency user experience, struggle with accurate intent inference. We address these limitations by introducing a novel decomposed approach: first, we perform structured interaction summarization, capturing key information from each user action. Second, we perform intent extraction using a fine-tuned model operating on the aggregated summaries. This method improves intent understanding in resource-constrained models, even surpassing the base performance of large MLLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12423v1",
    "published_date": "2025-09-15 20:20:30 UTC",
    "updated_date": "2025-09-15 20:20:30 UTC"
  },
  {
    "arxiv_id": "2509.12421v3",
    "title": "Understanding Prompt Management in GitHub Repositories: A Call for Best Practices",
    "authors": [
      "Hao Li",
      "Hicham Masri",
      "Filipe R. Cogo",
      "Abdul Ali Bangash",
      "Bram Adams",
      "Ahmed E. Hassan"
    ],
    "abstract": "The rapid adoption of foundation models (e.g., large language models) has given rise to promptware, i.e., software built using natural language prompts. Effective management of prompts, such as organization and quality assurance, is essential yet challenging. In this study, we perform an empirical analysis of 24,800 open-source prompts from 92 GitHub repositories to investigate prompt management practices and quality attributes. Our findings reveal critical challenges such as considerable inconsistencies in prompt formatting, substantial internal and external prompt duplication, and frequent readability and spelling issues. Based on these findings, we provide actionable recommendations for developers to enhance the usability and maintainability of open-source prompts within the rapidly evolving promptware ecosystem.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12421v3",
    "published_date": "2025-09-15 20:18:22 UTC",
    "updated_date": "2026-01-04 00:37:10 UTC"
  },
  {
    "arxiv_id": "2509.12395v1",
    "title": "Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML",
    "authors": [
      "Yash Mundhra",
      "Max Valk",
      "Maliheh Izadi"
    ],
    "abstract": "Large language models have shown impressive performance in various domains, including code generation across diverse open-source domains. However, their applicability in proprietary industrial settings, where domain-specific constraints and code interdependencies are prevalent, remains largely unexplored. We present a case study conducted in collaboration with the leveling department at ASML to investigate the performance of LLMs in generating functional, maintainable code within a closed, highly specialized software environment.\n  We developed an evaluation framework tailored to ASML's proprietary codebase and introduced a new benchmark. Additionally, we proposed a new evaluation metric, build@k, to assess whether LLM-generated code successfully compiles and integrates within real industrial repositories. We investigate various prompting techniques, compare the performance of generic and code-specific LLMs, and examine the impact of model size on code generation capabilities, using both match-based and execution-based metrics. The findings reveal that prompting techniques and model size have a significant impact on output quality, with few-shot and chain-of-thought prompting yielding the highest build success rates. The difference in performance between the code-specific LLMs and generic LLMs was less pronounced and varied substantially across different model families.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025 (Industry track)",
    "pdf_url": "https://arxiv.org/pdf/2509.12395v1",
    "published_date": "2025-09-15 19:39:26 UTC",
    "updated_date": "2025-09-15 19:39:26 UTC"
  },
  {
    "arxiv_id": "2509.12392v1",
    "title": "Evaluating the printability of stl files with ML",
    "authors": [
      "Janik Henn",
      "Adrian Hauptmannl",
      "Hamza A. A. Gardi"
    ],
    "abstract": "3D printing has long been a technology for industry professionals and enthusiasts willing to tinker or even build their own machines. This stands in stark contrast to today's market, where recent developments have prioritized ease of use to attract a broader audience. Slicing software nowadays has a few ways to sanity check the input file as well as the output gcode. Our approach introduces a novel layer of support by training an AI model to detect common issues in 3D models. The goal is to assist less experienced users by identifying features that are likely to cause print failures due to difficult to print geometries before printing even begins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12392v1",
    "published_date": "2025-09-15 19:37:00 UTC",
    "updated_date": "2025-09-15 19:37:00 UTC"
  },
  {
    "arxiv_id": "2509.12387v1",
    "title": "Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization",
    "authors": [
      "Mohamed Zayaan S"
    ],
    "abstract": "Modern deep learning models excel at pattern recognition but remain fundamentally limited by their reliance on spurious correlations, leading to poor generalization and a demand for massive datasets. We argue that a key ingredient for human-like intelligence-robust, sample-efficient learning-stems from an understanding of causal mechanisms. In this work, we introduce Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer the latent causal structure of a task distribution. CSML comprises three key modules: a perception module that maps raw inputs to disentangled symbolic representations; a differentiable causal induction module that discovers the underlying causal graph governing these symbols and a graph-based reasoning module that leverages this graph to make predictions. By meta-learning a shared causal world model across a distribution of tasks, CSML can rapidly adapt to novel tasks, including those requiring reasoning about interventions and counterfactuals, from only a handful of examples. We introduce CausalWorld, a new physics-based benchmark designed to test these capabilities. Our experiments show that CSML dramatically outperforms state-of-the-art meta-learning and neuro-symbolic baselines, particularly on tasks demanding true causal inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12387v1",
    "published_date": "2025-09-15 19:28:09 UTC",
    "updated_date": "2025-09-15 19:28:09 UTC"
  },
  {
    "arxiv_id": "2509.12386v2",
    "title": "Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks",
    "authors": [
      "Asim Waheed",
      "Vasisht Duddu",
      "Rui Zhang",
      "Sebastian Szyller"
    ],
    "abstract": "Machine learning (ML) models are susceptible to various risks to security, privacy, and fairness. Most defenses are designed to protect against each risk individually (intended interactions) but can inadvertently affect susceptibility to other unrelated risks (unintended interactions). We introduce Amulet, the first Python library for evaluating both intended and unintended interactions among ML defenses and risks. Amulet is comprehensive by including representative attacks, defenses, and metrics; extensible to new modules due to its modular design; consistent with a user-friendly API template for inputs and outputs; and applicable for evaluating novel interactions. By satisfying all four properties, Amulet offers a unified foundation for studying how defenses interact, enabling the first systematic evaluation of unintended interactions across multiple risks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12386v2",
    "published_date": "2025-09-15 19:27:46 UTC",
    "updated_date": "2025-11-07 14:22:21 UTC"
  },
  {
    "arxiv_id": "2509.12380v1",
    "title": "GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images",
    "authors": [
      "Florian Zager",
      "Hamza A. A. Gardi"
    ],
    "abstract": "Deep neural networks have achieved remarkable success across a range of tasks, however their computational demands often make them unsuitable for deployment on resource-constrained edge devices. This paper explores strategies for compressing and adapting models to enable efficient inference in such environments. We focus on GhostNetV3, a state-of-the-art architecture for mobile applications, and propose GhostNetV3-Small, a modified variant designed to perform better on low-resolution inputs such as those in the CIFAR-10 dataset. In addition to architectural adaptation, we provide a comparative evaluation of knowledge distillation techniques, including traditional knowledge distillation, teacher assistants, and teacher ensembles. Experimental results show that GhostNetV3-Small significantly outperforms the original GhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to expectations, all examined distillation strategies led to reduced accuracy compared to baseline training. These findings indicate that architectural adaptation can be more impactful than distillation in small-scale image classification tasks, highlighting the need for further research on effective model design and advanced distillation techniques for low-resolution domains.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12380v1",
    "published_date": "2025-09-15 19:19:09 UTC",
    "updated_date": "2025-09-15 19:19:09 UTC"
  },
  {
    "arxiv_id": "2509.12379v1",
    "title": "Geometric Red-Teaming for Robotic Manipulation",
    "authors": [
      "Divyam Goel",
      "Yufei Wang",
      "Tiancheng Wu",
      "Guixiu Qiao",
      "Pavel Piliptchak",
      "David Held",
      "Zackory Erickson"
    ],
    "abstract": "Standard evaluation protocols in robotic manipulation typically assess policy performance over curated, in-distribution test sets, offering limited insight into how systems fail under plausible variation. We introduce Geometric Red-Teaming (GRT), a red-teaming framework that probes robustness through object-centric geometric perturbations, automatically generating CrashShapes -- structurally valid, user-constrained mesh deformations that trigger catastrophic failures in pre-trained manipulation policies. The method integrates a Jacobian field-based deformation model with a gradient-free, simulator-in-the-loop optimization strategy. Across insertion, articulation, and grasping tasks, GRT consistently discovers deformations that collapse policy performance, revealing brittle failure modes missed by static benchmarks. By combining task-level policy rollouts with constraint-aware shape exploration, we aim to build a general purpose framework for structured, object-centric robustness evaluation in robotic manipulation. We additionally show that fine-tuning on individual CrashShapes, a process we refer to as blue-teaming, improves task success by up to 60 percentage points on those shapes, while preserving performance on the original object, demonstrating the utility of red-teamed geometries for targeted policy refinement. Finally, we validate both red-teaming and blue-teaming results with a real robotic arm, observing that simulated CrashShapes reduce task success from 90% to as low as 22.5%, and that blue-teaming recovers performance to up to 90% on the corresponding real-world geometry -- closely matching simulation outcomes. Videos and code can be found on our project website: https://georedteam.github.io/ .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at the 9th Annual Conference on Robot Learning (CoRL 2025, Oral)",
    "pdf_url": "https://arxiv.org/pdf/2509.12379v1",
    "published_date": "2025-09-15 19:12:26 UTC",
    "updated_date": "2025-09-15 19:12:26 UTC"
  },
  {
    "arxiv_id": "2509.12371v1",
    "title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables",
    "authors": [
      "Matteo Marcuzzo",
      "Alessandro Zangari",
      "Andrea Albarelli",
      "Jose Camacho-Collados",
      "Mohammad Taher Pilehvar"
    ],
    "abstract": "As LLMs excel on standard reading comprehension benchmarks, attention is shifting toward evaluating their capacity for complex abstract reasoning and inference. Literature-based benchmarks, with their rich narrative and moral depth, provide a compelling framework for evaluating such deeper comprehension skills. Here, we present MORABLES, a human-verified benchmark built from fables and short stories drawn from historical literature. The main task is structured as multiple-choice questions targeting moral inference, with carefully crafted distractors that challenge models to go beyond shallow, extractive question answering. To further stress-test model robustness, we introduce adversarial variants designed to surface LLM vulnerabilities and shortcuts due to issues such as data contamination. Our findings show that, while larger models outperform smaller ones, they remain susceptible to adversarial manipulation and often rely on superficial patterns rather than true moral reasoning. This brittleness results in significant self-contradiction, with the best models refuting their own answers in roughly 20% of cases depending on the framing of the moral choice. Interestingly, reasoning-enhanced models fail to bridge this gap, suggesting that scale - not reasoning ability - is the primary driver of performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2509.12371v1",
    "published_date": "2025-09-15 19:06:10 UTC",
    "updated_date": "2025-09-15 19:06:10 UTC"
  },
  {
    "arxiv_id": "2509.12367v1",
    "title": "An integrated process for design and control of lunar robotics using AI and simulation",
    "authors": [
      "Daniel Lindmark",
      "Jonas Andersson",
      "Kenneth Bodin",
      "Tora Bodin",
      "Hugo Börjesson",
      "Fredrik Nordfeldth",
      "Martin Servin"
    ],
    "abstract": "We envision an integrated process for developing lunar construction equipment, where physical design and control are explored in parallel. In this paper, we describe a technical framework that supports this process. It relies on OpenPLX, a readable/writable declarative language that links CAD-models and autonomous systems to high-fidelity, real-time 3D simulations of contacting multibody dynamics, machine regolith interaction forces, and non-ideal sensors. To demonstrate its capabilities, we present two case studies, including an autonomous lunar rover that combines a vision-language model for navigation with a reinforcement learning-based control policy for locomotion.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12367v1",
    "published_date": "2025-09-15 19:02:30 UTC",
    "updated_date": "2025-09-15 19:02:30 UTC"
  },
  {
    "arxiv_id": "2509.12363v1",
    "title": "Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture",
    "authors": [
      "Ritesh Janga",
      "Rushit Dave"
    ],
    "abstract": "The agricultural sector is undergoing a transformation with the integration of advanced technologies, particularly in data-driven decision-making. This work proposes a federated learning framework for smart farming, aiming to develop a scalable, efficient, and secure solution for crop disease detection tailored to the environmental and operational conditions of Minnesota farms. By maintaining sensitive farm data locally and enabling collaborative model updates, our proposed framework seeks to achieve high accuracy in crop disease classification without compromising data privacy. We outline a methodology involving data collection from Minnesota farms, application of local deep learning algorithms, transfer learning, and a central aggregation server for model refinement, aiming to achieve improved accuracy in disease detection, good generalization across agricultural scenarios, lower costs in communication and training time, and earlier identification and intervention against diseases in future implementations. We outline a methodology and anticipated outcomes, setting the stage for empirical validation in subsequent studies. This work comes in a context where more and more demand for data-driven interpretations in agriculture has to be weighed with concerns about privacy from farms that are hesitant to share their operational data. This will be important to provide a secure and efficient disease detection method that can finally revolutionize smart farming systems and solve local agricultural problems with data confidentiality. In doing so, this paper bridges the gap between advanced machine learning techniques and the practical, privacy-sensitive needs of farmers in Minnesota and beyond, leveraging the benefits of federated learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 5 Figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12363v1",
    "published_date": "2025-09-15 18:57:32 UTC",
    "updated_date": "2025-09-15 18:57:32 UTC"
  },
  {
    "arxiv_id": "2509.18144v1",
    "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation",
    "authors": [
      "Yubo Yang",
      "Yichen Zhu",
      "Bo Jiang"
    ],
    "abstract": "Spatio-temporal data abounds in domain like traffic and environmental monitoring. However, it often suffers from missing values due to sensor malfunctions, transmission failures, etc. Recent years have seen continued efforts to improve spatio-temporal data imputation performance. Recently diffusion models have outperformed other approaches in various tasks, including spatio-temporal imputation, showing competitive performance. Extracting and utilizing spatio-temporal dependencies as conditional information is vital in diffusion-based methods. However, previous methods introduce error accumulation in this process and ignore the variability of the dependencies in the noisy data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel spatio-temporal imputation approach based on conditional diffusion model. Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model for pre-imputation with the imputed result used to extract conditional information by our designed Spatio-Temporal Conditionalizer (STC)network. We also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated attention mechanism to capture the variant dependencies across diffusion steps. Extensive experiments on three real-world datasets show that AdaSTI outperforms existing methods in all the settings, with up to 46.4% reduction in imputation error.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.18144v1",
    "published_date": "2025-09-15 18:55:56 UTC",
    "updated_date": "2025-09-15 18:55:56 UTC"
  },
  {
    "arxiv_id": "2509.12346v1",
    "title": "Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification",
    "authors": [
      "Liam Ressel",
      "Hamza A. A. Gardi"
    ],
    "abstract": "The Engineers' Salary Prediction Challenge requires classifying salary categories into three classes based on tabular data. The job description is represented as a 300-dimensional word embedding incorporated into the tabular features, drastically increasing dimensionality. Additionally, the limited number of training samples makes classification challenging. Linear dimensionality reduction of word embeddings for tabular data classification remains underexplored. This paper studies Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate subspace dimension, can outperform raw embeddings. LDA without regularization performs poorly due to covariance estimation errors, but applying shrinkage improves performance significantly, even with only two dimensions. We propose Partitioned-LDA, which splits embeddings into equal-sized blocks and performs LDA separately on each, thereby reducing the size of the covariance matrices. Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves top-10 accuracy on the competition public leaderboard. This method effectively enhances word embedding performance in tabular data classification with limited training samples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12346v1",
    "published_date": "2025-09-15 18:19:00 UTC",
    "updated_date": "2025-09-15 18:19:00 UTC"
  },
  {
    "arxiv_id": "2509.12339v1",
    "title": "Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets",
    "authors": [
      "Xianchen Liu",
      "Tianhui Zhang",
      "Xinyu Zhang",
      "Lingmin Hou",
      "Zhen Guo",
      "Yuanhao Tian",
      "Yang Liu"
    ],
    "abstract": "This paper presents a novel approach to optimizing pricing and replenishment strategies in fresh food supermarkets by combining Long Short-Term Memory (LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model, enhanced with an attention mechanism, is used to predict sales volumes, pricing trends, and spoilage rates over a seven-day period. The predictions generated by the LSTM model serve as inputs for the PSO algorithm, which iteratively optimizes pricing and replenishment strategies to maximize profitability while adhering to inventory constraints. The integration of cost-plus pricing allows for dynamic adjustments based on fixed and variable costs, ensuring real-time adaptability to market fluctuations. The framework not only maximizes profits but also reduces food waste, contributing to more sustainable supermarket operations. The attention mechanism enhances the interpretability of the LSTM model by identifying key time points and factors influencing sales, improving decision-making accuracy. This methodology bridges the gap between predictive modeling and optimization, offering a scalable solution for dynamic pricing and inventory management in fresh food retail and other industries dealing with perishable goods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 6 figure",
    "pdf_url": "https://arxiv.org/pdf/2509.12339v1",
    "published_date": "2025-09-15 18:07:44 UTC",
    "updated_date": "2025-09-15 18:07:44 UTC"
  },
  {
    "arxiv_id": "2509.19337v1",
    "title": "Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question",
    "authors": [
      "Stefanos Bakirtzis",
      "Paul Almasan",
      "José Suárez-Varela",
      "Gabriel O. Ferreira",
      "Michail Kalntis",
      "André Felipe Zanella",
      "Ian Wassell",
      "Andra Lutu"
    ],
    "abstract": "Differentiable ray tracing has recently challenged the status quo in radio propagation modelling and digital twinning. Promising unprecedented speed and the ability to learn from real-world data, it offers a real alternative to conventional deep learning (DL) models. However, no experimental evaluation on production-grade networks has yet validated its assumed scalability or practical benefits. This leaves mobile network operators (MNOs) and the research community without clear guidance on its applicability. In this paper, we fill this gap by employing both differentiable ray tracing and DL models to emulate radio coverage using extensive real-world data collected from the network of a major MNO, covering 13 cities and more than 10,000 antennas. Our results show that, while differentiable ray-tracing simulators have contributed to reducing the efficiency-accuracy gap, they struggle to generalize from real-world data at a large scale, and they remain unsuitable for real-time applications. In contrast, DL models demonstrate higher accuracy and faster adaptation than differentiable ray-tracing simulators across urban, suburban, and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental results aim to provide timely insights into a fundamental open question with direct implications on the wireless ecosystem and future research.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19337v1",
    "published_date": "2025-09-15 18:03:19 UTC",
    "updated_date": "2025-09-15 18:03:19 UTC"
  },
  {
    "arxiv_id": "2509.13365v1",
    "title": "The Provenance Problem: LLMs and the Breakdown of Citation Norms",
    "authors": [
      "Brian D. Earp",
      "Haotian Yuan",
      "Julian Koplin",
      "Sebastian Porsdam Mann"
    ],
    "abstract": "The increasing use of generative AI in scientific writing raises urgent questions about attribution and intellectual credit. When a researcher employs ChatGPT to draft a manuscript, the resulting text may echo ideas from sources the author has never encountered. If an AI system reproduces insights from, for example, an obscure 1975 paper without citation, does this constitute plagiarism? We argue that such cases exemplify the 'provenance problem': a systematic breakdown in the chain of scholarly credit. Unlike conventional plagiarism, this phenomenon does not involve intent to deceive (researchers may disclose AI use and act in good faith) yet still benefit from the uncredited intellectual contributions of others. This dynamic creates a novel category of attributional harm that current ethical and professional frameworks fail to address. As generative AI becomes embedded across disciplines, the risk that significant ideas will circulate without recognition threatens both the reputational economy of science and the demands of epistemic justice. This Perspective analyzes how AI challenges established norms of authorship, introduces conceptual tools for understanding the provenance problem, and proposes strategies to preserve integrity and fairness in scholarly communication.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.13365v1",
    "published_date": "2025-09-15 18:01:03 UTC",
    "updated_date": "2025-09-15 18:01:03 UTC"
  },
  {
    "arxiv_id": "2509.12196v1",
    "title": "Dynamic Relational Priming Improves Transformer in Multivariate Time Series",
    "authors": [
      "Hunjae Lee",
      "Corey Clark"
    ],
    "abstract": "Standard attention mechanisms in transformers employ static token representations that remain unchanged across all pair-wise computations in each layer. This limits their representational alignment with the potentially diverse relational dynamics of each token-pair interaction. While they excel in domains with relatively homogeneous relationships, standard attention's static relational learning struggles to capture the diverse, heterogeneous inter-channel dependencies of multivariate time series (MTS) data--where different channel-pair interactions within a single system may be governed by entirely different physical laws or temporal dynamics. To better align the attention mechanism for such domain phenomena, we propose attention with dynamic relational priming (prime attention). Unlike standard attention where each token presents an identical representation across all of its pair-wise interactions, prime attention tailors each token dynamically (or per interaction) through learnable modulations to best capture the unique relational dynamics of each token pair, optimizing each pair-wise interaction for that specific relationship. This representational plasticity of prime attention enables effective extraction of relationship-specific information in MTS while maintaining the same asymptotic computational complexity as standard attention. Our results demonstrate that prime attention consistently outperforms standard attention across benchmarks, achieving up to 6.5\\% improvement in forecasting accuracy. In addition, we find that prime attention achieves comparable or superior performance using up to 40\\% less sequence length compared to standard attention, further demonstrating its superior relational modeling capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12196v1",
    "published_date": "2025-09-15 17:56:15 UTC",
    "updated_date": "2025-09-15 17:56:15 UTC"
  },
  {
    "arxiv_id": "2509.12194v1",
    "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
    "authors": [
      "Thomas A. Buckley",
      "Riccardo Conci",
      "Peter G. Brodeur",
      "Jason Gusdorf",
      "Sourik Beltrán",
      "Bita Behrouzi",
      "Byron Crowe",
      "Jacob Dockterman",
      "Muzzammil Muhammad",
      "Sarah Ohnigian",
      "Andrew Sanchez",
      "James A. Diao",
      "Aashna P. Shah",
      "Daniel Restrepo",
      "Eric S. Rosenberg",
      "Andrew S. Lea",
      "Marinka Zitnik",
      "Scott H. Podolsky",
      "Zahir Kanjee",
      "Raja-Elie E. Abdulnour",
      "Jacob M. Koshy",
      "Adam Rodman",
      "Arjun K. Manrai"
    ],
    "abstract": "BACKGROUND: For over a century, the New England Journal of Medicine Clinicopathological Conferences (CPCs) have tested the reasoning of expert physicians and, recently, artificial intelligence (AI). However, prior AI evaluations have focused on final diagnoses without addressing the multifaceted reasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025), we conducted extensive physician annotation and automated processing to create CPC-Bench, a physician-validated benchmark spanning 10 text-based and multimodal tasks, against which we evaluated leading large language models (LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce written and slide-based video presentations using only the case presentation, modeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the final diagnosis first in 60% of cases and within the top ten in 84% of cases, outperforming a 20-physician baseline; next-test selection accuracy reached 98%. Event-level physician annotations quantified AI diagnostic accuracy per unit of information. Performance was lower on literature search and image tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image challenges. In blinded comparisons of CaBot vs. human expert-generated text, physicians misclassified the source of the differential in 46 of 62 (74%) of trials, and scored CaBot more favorably across quality dimensions. To promote research, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based differential diagnosis and convincingly emulate expert medical presentations, but image interpretation and literature retrieval remain weaker. CPC-Bench and CaBot may enable transparent and continued tracking of progress in medical AI.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12194v1",
    "published_date": "2025-09-15 17:54:51 UTC",
    "updated_date": "2025-09-15 17:54:51 UTC"
  },
  {
    "arxiv_id": "2509.12190v1",
    "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm",
    "authors": [
      "Alireza Mohamadi",
      "Ali Yavari"
    ],
    "abstract": "When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2509.12190v1",
    "published_date": "2025-09-15 17:53:11 UTC",
    "updated_date": "2025-09-15 17:53:11 UTC"
  },
  {
    "arxiv_id": "2509.12187v1",
    "title": "HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments",
    "authors": [
      "Johanna Karras",
      "Yingwei Li",
      "Yasamin Jafarian",
      "Ira Kemelmacher-Shlizerman"
    ],
    "abstract": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due significant occlusions, complex human poses, and cloth deformations. Prior methods rely on synthetic 3D training data consisting of mostly unoccluded and static objects, leading to poor generalization on real-world clothing. In this paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3 images or a continuous video of a person wearing a garment and generates 360° novel views of the garment in a canonical pose. Our key insight is to bridge the domain gap between real and synthetic data with a novel implicit training paradigm leveraging a combination of large-scale real video data and small-scale synthetic 3D data to optimize a shared garment embedding space. During inference, the shared embedding space further enables dynamic video-to-360° NVS through the construction of a garment \"atlas\" representation by finetuning a garment embedding on a specific real-world video. The atlas captures garment-specific geometry and texture across all viewpoints, independent of body pose or motion. Extensive experiments show that HoloGarment achieves state-of-the-art performance on NVS of in-the-wild garments from images and videos. Notably, our method robustly handles challenging real-world artifacts -- such as wrinkling, pose variation, and occlusion -- while maintaining photorealism, view consistency, fine texture details, and accurate geometry. Visit our project page for additional results: https://johannakarras.github.io/HoloGarment",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12187v1",
    "published_date": "2025-09-15 17:50:57 UTC",
    "updated_date": "2025-09-15 17:50:57 UTC"
  },
  {
    "arxiv_id": "2509.12179v5",
    "title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation",
    "authors": [
      "Yubo Li",
      "Weiyi Song"
    ],
    "abstract": "Current AI alignment through RLHF follows a single directional paradigm that AI conforms to human preferences while treating human cognition as fixed. We propose a shift to co-alignment through Bidirectional Cognitive Alignment (BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols, representation mapping, and KL-budget constraints for controlled co-evolution. In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline, with 230% better mutual adaptation and 332% better protocol convergence. Emergent protocols outperformed handcrafted ones by 84%, while bidirectional adaptation unexpectedly improved safety (+23% out-of-distribution robustness). The 46% synergy improvement demonstrates optimal collaboration exists at the intersection, not union, of human and AI capabilities, validating the shift from single-directional to co-alignment paradigms.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12179v5",
    "published_date": "2025-09-15 17:41:16 UTC",
    "updated_date": "2025-11-18 01:00:10 UTC"
  },
  {
    "arxiv_id": "2509.12171v2",
    "title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models",
    "authors": [
      "Marek Kubis",
      "Paweł Skórzewski",
      "Iwona Christop",
      "Mateusz Czyżnikiewicz",
      "Jakub Kubiak",
      "Łukasz Bondaruk",
      "Marcin Lewandowski"
    ],
    "abstract": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new benchmark for assessing the performance of speech-aware large language models. The benchmark utilizes textual tasks and a voice cloning text-to-speech model to quantify the extent to which language understanding capabilities are preserved when the model is accessed via speech input. C3T quantifies the fairness of the model for different categories of speakers and its robustness across text and speech modalities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 1 figure; benchmark code available at https://github.com/SamsungLabs/C3T",
    "pdf_url": "https://arxiv.org/pdf/2509.12171v2",
    "published_date": "2025-09-15 17:34:45 UTC",
    "updated_date": "2025-10-16 12:28:23 UTC"
  },
  {
    "arxiv_id": "2509.12169v1",
    "title": "Approaches to Analysis and Design of AI-Based Autonomous Vehicles",
    "authors": [
      "Tao Yan",
      "Zheyu Zhang",
      "Jingjing Jiang",
      "Wen-Hua Chen"
    ],
    "abstract": "Artificial intelligence (AI) models are becoming key components in an autonomous vehicle (AV), especially in handling complicated perception tasks. However, closing the loop through AI-based feedback may pose significant risks on reliability of autonomous driving due to very limited understanding about the mechanism of AI-driven perception processes. To overcome it, this paper aims to develop tools for modeling, analysis, and synthesis for a class of AI-based AV; in particular, their closed-loop properties, e.g., stability, robustness, and performance, are rigorously studied in the statistical sense. First, we provide a novel modeling means for the AI-driven perception processes by looking at their error characteristics. Specifically, three fundamental AI-induced perception uncertainties are recognized and modeled by Markov chains, Gaussian processes, and bounded disturbances, respectively. By means of that, the closed-loop stochastic stability (SS) is established in the sense of mean square, and then, an SS control synthesis method is presented within the framework of linear matrix inequalities (LMIs). Besides the SS properties, the robustness and performance of AI-based AVs are discussed in terms of a stochastic guaranteed cost, and criteria are given to test the robustness level of an AV when in the presence of AI-induced uncertainties. Furthermore, the stochastic optimal guaranteed cost control is investigated, and an efficient design procedure is developed innovatively based on LMI techniques and convex optimization. Finally, to illustrate the effectiveness, the developed results are applied to an example of car following control, along with extensive simulation.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12169v1",
    "published_date": "2025-09-15 17:32:29 UTC",
    "updated_date": "2025-09-15 17:32:29 UTC"
  },
  {
    "arxiv_id": "2509.12168v1",
    "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing",
    "authors": [
      "Timothy Rupprecht",
      "Enfu Nan",
      "Arash Akbari",
      "Arman Akbari",
      "Lei Lu",
      "Priyanka Maan",
      "Sean Duffy",
      "Pu Zhao",
      "Yumei He",
      "David Kaeli",
      "Yanzhi Wang"
    ],
    "abstract": "Role-playing Large language models (LLMs) are increasingly deployed in high-stakes domains such as healthcare, education, and governance, where failures can directly impact user trust and well-being. A cost effective paradigm for LLM role-playing is few-shot learning, but existing approaches often cause models to break character in unexpected and potentially harmful ways, especially when interacting with hostile users. Inspired by Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a text retrieval problem and propose a new prompting framework called RAGs-to-Riches, which leverages curated reference demonstrations to condition LLM responses. We evaluate our framework with LLM-as-a-judge preference voting and introduce two novel token-level ROUGE metrics: Intersection over Output (IOO) to quantity how much an LLM improvises and Intersection over References (IOR) to measure few-shot demonstrations utilization rate during the evaluation tasks. When simulating interactions with a hostile user, our prompting strategy incorporates in its responses during inference an average of 35% more tokens from the reference demonstrations. As a result, across 453 role-playing interactions, our models are consistently judged as being more authentic, and remain in-character more often than zero-shot and in-context Learning (ICL) methods. Our method presents a scalable strategy for building robust, human-aligned LLM role-playing frameworks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12168v1",
    "published_date": "2025-09-15 17:31:15 UTC",
    "updated_date": "2025-09-15 17:31:15 UTC"
  },
  {
    "arxiv_id": "2509.12159v1",
    "title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression",
    "authors": [
      "Jingyu Xiao",
      "Zhongyi Zhang",
      "Yuxuan Wan",
      "Yintong Huo",
      "Yang Liu",
      "Michael R. Lyu"
    ],
    "abstract": "Multimodal Large Language Models have demonstrated exceptional performance in UI2Code tasks, significantly enhancing website development efficiency. However, these tasks incur substantially higher computational overhead than traditional code generation due to the large number of input image tokens and extensive output code tokens required. Our comprehensive study identifies significant redundancies in both image and code tokens that exacerbate computational complexity and hinder focus on key UI elements, resulting in excessively lengthy and often invalid HTML files. We propose EfficientUICoder, a compression framework for efficient UI code generation with three key components. First, Element and Layout-aware Token Compression preserves essential UI information by detecting element regions and constructing UI element trees. Second, Region-aware Token Refinement leverages attention scores to discard low-attention tokens from selected regions while integrating high-attention tokens from unselected regions. Third, Adaptive Duplicate Token Suppression dynamically reduces repetitive generation by tracking HTML/CSS structure frequencies and applying exponential penalties. Extensive experiments show EfficientUICoderachieves a 55%-60% compression ratio without compromising webpage quality and delivers superior efficiency improvements: reducing computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%, and inference time by 48.8% on 34B-level MLLMs. Code is available at https://github.com/WebPAI/EfficientUICoder.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12159v1",
    "published_date": "2025-09-15 17:23:46 UTC",
    "updated_date": "2025-09-15 17:23:46 UTC"
  },
  {
    "arxiv_id": "2509.12158v2",
    "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
    "authors": [
      "Alessandro Zangari",
      "Matteo Marcuzzo",
      "Andrea Albarelli",
      "Mohammad Taher Pilehvar",
      "Jose Camacho-Collados"
    ],
    "abstract": "Puns are a form of humorous wordplay that exploits polysemy and phonetic similarity. While LLMs have shown promise in detecting puns, we show in this paper that their understanding often remains shallow, lacking the nuanced grasp typical of human interpretation. By systematically analyzing and reformulating existing pun benchmarks, we demonstrate how subtle changes in puns are sufficient to mislead LLMs. Our contributions include comprehensive and nuanced pun detection benchmarks, human evaluation across recent LLMs, and an analysis of the robustness challenges these models face in processing puns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2509.12158v2",
    "published_date": "2025-09-15 17:22:30 UTC",
    "updated_date": "2025-09-20 12:16:33 UTC"
  },
  {
    "arxiv_id": "2509.12152v1",
    "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference",
    "authors": [
      "Synthia Wang",
      "Sai Teja Peddinti",
      "Nina Taft",
      "Nick Feamster"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes from seemingly innocuous text, raising privacy risks beyond memorized data leakage. While prior work has demonstrated these risks, little is known about how users estimate and respond. We conducted a survey with 240 U.S. participants who judged text snippets for inference risks, reported concern levels, and attempted rewrites to block inference. We compared their rewrites with those generated by ChatGPT and Rescriber, a state-of-the-art sanitization tool. Results show that participants struggled to anticipate inference, performing a little better than chance. User rewrites were effective in just 28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our participants' rewriting strategies, and observed that while paraphrasing was the most common strategy it is also the least effective; instead abstraction and adding ambiguity were more successful. Our work highlights the importance of inference-aware design in LLM interactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12152v1",
    "published_date": "2025-09-15 17:17:26 UTC",
    "updated_date": "2025-09-15 17:17:26 UTC"
  },
  {
    "arxiv_id": "2509.12146v2",
    "title": "Multi Anatomy X-Ray Foundation Model",
    "authors": [
      "Nishank Singla",
      "Krisztian Koos",
      "Farzin Haddadpour",
      "Amin Honarmandi Shandiz",
      "Lovish Chum",
      "Xiaojian Xu",
      "Qing Jin",
      "Erhan Bas"
    ],
    "abstract": "X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12146v2",
    "published_date": "2025-09-15 17:12:26 UTC",
    "updated_date": "2025-12-18 19:27:58 UTC"
  },
  {
    "arxiv_id": "2509.12143v3",
    "title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data",
    "authors": [
      "Nojod M. Alotaibi",
      "Areej M. Alhothali",
      "Manar S. Ali"
    ],
    "abstract": "Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 81.51\\% accuracy, 85.94\\% sensitivity, 76.36\\% specificity, 80.88\\% precision, and 83.33\\% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 3 figure, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.12143v3",
    "published_date": "2025-09-15 17:10:39 UTC",
    "updated_date": "2025-11-04 06:21:00 UTC"
  },
  {
    "arxiv_id": "2509.12137v1",
    "title": "Control Analysis and Design for Autonomous Vehicles Subject to Imperfect AI-Based Perception",
    "authors": [
      "Tao Yan",
      "Zheyu Zhang",
      "Jingjing Jiang",
      "Wen-Hua Chen"
    ],
    "abstract": "Safety is a critical concern in autonomous vehicle (AV) systems, especially when AI-based sensing and perception modules are involved. However, due to the black box nature of AI algorithms, it makes closed-loop analysis and synthesis particularly challenging, for example, establishing closed-loop stability and ensuring performance, while they are fundamental to AV safety. To approach this difficulty, this paper aims to develop new modeling, analysis, and synthesis tools for AI-based AVs. Inspired by recent developments in perception error models (PEMs), the focus is shifted from directly modeling AI-based perception processes to characterizing the perception errors they produce. Two key classes of AI-induced perception errors are considered: misdetection and measurement noise. These error patterns are modeled using continuous-time Markov chains and Wiener processes, respectively. By means of that, a PEM-augmented driving model is proposed, with which we are able to establish the closed-loop stability for a class of AI-driven AV systems via stochastic calculus. Furthermore, a performance-guaranteed output feedback control synthesis method is presented, which ensures both stability and satisfactory performance. The method is formulated as a convex optimization problem, allowing for efficient numerical solutions. The results are then applied to an adaptive cruise control (ACC) scenario, demonstrating their effectiveness and robustness despite the corrupted and misleading perception.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12137v1",
    "published_date": "2025-09-15 17:03:21 UTC",
    "updated_date": "2025-09-15 17:03:21 UTC"
  },
  {
    "arxiv_id": "2509.12117v1",
    "title": "$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning",
    "authors": [
      "Aryaman Reddi",
      "Gabriele Tiboni",
      "Jan Peters",
      "Carlo D'Eramo"
    ],
    "abstract": "Actor-critic algorithms for deep multi-agent reinforcement learning (MARL) typically employ a policy update that responds to the current strategies of other agents. While being straightforward, this approach does not account for the updates of other agents at the same update step, resulting in miscoordination. In this paper, we introduce the $K$-Level Policy Gradient (KPG), a method that recursively updates each agent against the updated policies of other agents, speeding up the discovery of effective coordinated policies. We theoretically prove that KPG with finite iterates achieves monotonic convergence to a local Nash equilibrium under certain conditions. We provide principled implementations of KPG by applying it to the deep MARL algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior performance over existing deep MARL algorithms in StarCraft II and multi-agent MuJoCo.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12117v1",
    "published_date": "2025-09-15 16:42:56 UTC",
    "updated_date": "2025-09-15 16:42:56 UTC"
  },
  {
    "arxiv_id": "2509.12107v1",
    "title": "Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice",
    "authors": [
      "Si Chen",
      "Isabel R. Molnar",
      "Peiyu Li",
      "Adam Acunin",
      "Ting Hua",
      "Alex Ambrose",
      "Nitesh V. Chawla",
      "Ronald Metoyer"
    ],
    "abstract": "Large language models (LLMs) typically generate direct answers, yet they are increasingly used as learning tools. Studying instructors' usage is critical, given their role in teaching and guiding AI adoption in education. We designed and evaluated TeaPT, an LLM for pedagogical purposes that supports instructors' professional development through two conversational approaches: a Socratic approach that uses guided questioning to foster reflection, and a Narrative approach that offers elaborated suggestions to extend externalized cognition. In a mixed-method study with 41 higher-education instructors, the Socratic version elicited greater engagement, while the Narrative version was preferred for actionable guidance. Subgroup analyses further revealed that less-experienced, AI-optimistic instructors favored the Socratic version, whereas more-experienced, AI-cautious instructors preferred the Narrative version. We contribute design implications for LLMs for pedagogical purposes, showing how adaptive conversational approaches can support instructors with varied profiles while highlighting how AI attitudes and experience shape interaction and learning.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12107v1",
    "published_date": "2025-09-15 16:33:37 UTC",
    "updated_date": "2025-09-15 16:33:37 UTC"
  },
  {
    "arxiv_id": "2509.12104v1",
    "title": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference",
    "authors": [
      "Zongyue Xue",
      "Siyuan Zheng",
      "Shaochun Wang",
      "Yiran Hu",
      "Shenran Wang",
      "Yuxin Yao",
      "Haitao Li",
      "Qingyao Ai",
      "Yiqun Liu",
      "Yun Liu",
      "Weixing Shen"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into legal practice raises pressing concerns about judicial fairness, particularly due to the nature of their \"black-box\" processes. This study introduces JustEva, a comprehensive, open-source evaluation toolkit designed to measure LLM fairness in legal tasks. JustEva features several advantages: (1) a structured label system covering 65 extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and imbalanced inaccuracy; (3) robust statistical inference methods; and (4) informative visualizations. The toolkit supports two types of experiments, enabling a complete evaluation workflow: (1) generating structured outputs from LLMs using a provided dataset, and (2) conducting statistical analysis and inference on LLMs' outputs through regression and other statistical methods. Empirical application of JustEva reveals significant fairness deficiencies in current LLMs, highlighting the lack of fair and trustworthy LLM legal tools. JustEva offers a convenient tool and methodological foundation for evaluating and improving algorithmic fairness in the legal domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted at CIKM 2025 (Demo Track)",
    "pdf_url": "https://arxiv.org/pdf/2509.12104v1",
    "published_date": "2025-09-15 16:31:26 UTC",
    "updated_date": "2025-09-15 16:31:26 UTC"
  },
  {
    "arxiv_id": "2509.12102v1",
    "title": "Can LLMs Address Mental Health Questions? A Comparison with Human Therapists",
    "authors": [
      "Synthia Wang",
      "Yuwei Cheng",
      "Austin Song",
      "Sarah Keedy",
      "Marc Berman",
      "Nick Feamster"
    ],
    "abstract": "Limited access to mental health care has motivated the use of digital tools and conversational agents powered by large language models (LLMs), yet their quality and reception remain unclear. We present a study comparing therapist-written responses to those generated by ChatGPT, Gemini, and Llama for real patient questions. Text analysis showed that LLMs produced longer, more readable, and lexically richer responses with a more positive tone, while therapist responses were more often written in the first person. In a survey with 150 users and 23 licensed therapists, participants rated LLM responses as clearer, more respectful, and more supportive than therapist-written answers. Yet, both groups of participants expressed a stronger preference for human therapist support. These findings highlight the promise and limitations of LLMs in mental health, underscoring the need for designs that balance their communicative strengths with concerns of trust, privacy, and accountability.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12102v1",
    "published_date": "2025-09-15 16:26:13 UTC",
    "updated_date": "2025-09-15 16:26:13 UTC"
  },
  {
    "arxiv_id": "2509.12101v1",
    "title": "In-domain SSL pre-training and streaming ASR",
    "authors": [
      "Jarod Duret",
      "Salima Mdhaffar",
      "Gaëlle Laperrière",
      "Ryan Whetten",
      "Audrey Galametz",
      "Catherine Kobus",
      "Marion-Cécile Martin",
      "Jo Oleiwan",
      "Yannick Estève"
    ],
    "abstract": "In this study, we investigate the benefits of domain-specific self-supervised pre-training for both offline and streaming ASR in Air Traffic Control (ATC) environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then fine-tune on a smaller supervised ATC set. To enable real-time processing, we propose using chunked attention and dynamic convolutions, ensuring low-latency inference. We compare these in-domain SSL models against state-of-the-art, general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show that domain-adapted pre-training substantially improves performance on standard ATC benchmarks, significantly reducing word error rates when compared to models trained on broad speech corpora. Furthermore, the proposed streaming approach further improves word error rate under tighter latency constraints, making it particularly suitable for safety-critical aviation applications. These findings highlight that specializing SSL representations for ATC data is a practical path toward more accurate and efficient ASR systems in real-world operational settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to SPECOM 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.12101v1",
    "published_date": "2025-09-15 16:25:43 UTC",
    "updated_date": "2025-09-15 16:25:43 UTC"
  },
  {
    "arxiv_id": "2509.12098v1",
    "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities",
    "authors": [
      "Payam Latifi"
    ],
    "abstract": "This pilot study presents a small-scale but carefully annotated benchmark of Named Entity Recognition (NER) performance across six systems: three non-LLM NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models (LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119 tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME). We evaluated each system's output against the manually annotated gold standard dataset using F1-score. The results show that LLMs generally outperform conventional tools in recognizing context-sensitive entities like person names, with Gemini achieving the highest average F1-score. However, traditional systems like Stanza demonstrate greater consistency in structured tags such as LOCATION and DATE. We also observed variability among LLMs, particularly in handling temporal expressions and multi-word organizations. Our findings highlight that while LLMs offer improved contextual understanding, traditional tools remain competitive in specific tasks, informing model selection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich dataset of 119 tokens. The annotated dataset, prompts are provided in appendices for full reproducibility. All experiments were conducted on 14 May 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.12098v1",
    "published_date": "2025-09-15 16:21:59 UTC",
    "updated_date": "2025-09-15 16:21:59 UTC"
  },
  {
    "arxiv_id": "2509.12091v1",
    "title": "Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants",
    "authors": [
      "Hamied Nabizada",
      "Lasse Beers",
      "Alain Chahine",
      "Felix Gehlhoff",
      "Oliver Niggemann",
      "Alexander Fay"
    ],
    "abstract": "Engineering models created in Model-Based Systems Engineering (MBSE) environments contain detailed information about system structure and behavior. However, they typically lack symbolic planning semantics such as preconditions, effects, and constraints related to resource availability and timing. This limits their ability to evaluate whether a given system variant can fulfill specific tasks and how efficiently it performs compared to alternatives.\n  To address this gap, this paper presents a model-driven method that enables the specification and automated generation of symbolic planning artifacts within SysML-based engineering models. A dedicated SysML profile introduces reusable stereotypes for core planning constructs. These are integrated into existing model structures and processed by an algorithm that generates a valid domain file and a corresponding problem file in Planning Domain Definition Language (PDDL). In contrast to previous approaches that rely on manual transformations or external capability models, the method supports native integration and maintains consistency between engineering and planning artifacts.\n  The applicability of the method is demonstrated through a case study from aircraft assembly. The example illustrates how existing engineering models are enriched with planning semantics and how the proposed workflow is applied to generate consistent planning artifacts from these models. The generated planning artifacts enable the validation of system variants through AI planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at the KEPS-Workshop, ICAPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.12091v1",
    "published_date": "2025-09-15 16:18:08 UTC",
    "updated_date": "2025-09-15 16:18:08 UTC"
  },
  {
    "arxiv_id": "2509.12081v1",
    "title": "Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors",
    "authors": [
      "Anirudha Majumdar"
    ],
    "abstract": "This paper proposes deception as a mechanism for out-of-distribution (OOD) generalization: by learning data representations that make training data appear independent and identically distributed (iid) to an observer, we can identify stable features that eliminate spurious correlations and generalize to unseen domains. We refer to this principle as deceptive risk minimization (DRM) and instantiate it with a practical differentiable objective that simultaneously learns features that eliminate distribution shifts from the perspective of a detector based on conformal martingales while minimizing a task-specific loss. In contrast to domain adaptation or prior invariant representation learning methods, DRM does not require access to test data or a partitioning of training data into a finite number of data-generating domains. We demonstrate the efficacy of DRM on numerical experiments with concept shift and a simulated imitation learning setting with covariate shift in environments that a robot is deployed in.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12081v1",
    "published_date": "2025-09-15 16:11:55 UTC",
    "updated_date": "2025-09-15 16:11:55 UTC"
  },
  {
    "arxiv_id": "2509.12080v1",
    "title": "A Time-Series Foundation Model by Universal Delay Embedding",
    "authors": [
      "Zijian Wang",
      "Peng Tao",
      "Jifan Shi",
      "Rui Bao",
      "Rui Liu",
      "Luonan Chen"
    ],
    "abstract": "This study introduces Universal Delay Embedding (UDE), a pretrained foundation model designed to revolutionize time-series forecasting through principled integration of delay embedding representation and Koopman operator prediction. Leveraging Takens' embedding theorem, UDE as a dynamical representation of observed data constructs two-dimensional subspace patches from Hankel matrices, theoretically preserving dynamical and topological properties of underlying dynamical systems. Such patches are viewed as images, which can be efficiently processed by exploiting advanced deep learning technologies. Computationally, these patches further serve as tokens for learning a self-attention encoder, thus enabling accurate prediction of nonlinear time-series by a finite-dimensional Koopman operator in a linear manner in a latent space. Extensive evaluations across various benchmarks and real-world climate datasets demonstrate over 20% average reduction in mean squared error versus state-of-the-art foundation models, alongside superior generalization in fine-tuning scenarios. In particular, the learned dynamical representations and Koopman operator prediction forms from the patches exhibit exceptional interpretability, with consistent identification of topologically informative subspaces and robust encoding of domain-invariant dynamics, establishing UDE as a scalable, interpretable framework for universal time-series modeling and forecasting with broad scientific and industrial applicability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12080v1",
    "published_date": "2025-09-15 16:11:49 UTC",
    "updated_date": "2025-09-15 16:11:49 UTC"
  },
  {
    "arxiv_id": "2509.13364v1",
    "title": "Asterisk Operator",
    "authors": [
      "Zixi Li"
    ],
    "abstract": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified framework for abstract reasoning based on Adjacency-Structured Parallel Propagation (ASPP). The operator formalizes structured reasoning tasks as local, parallel state evolution processes guided by implicit relational graphs. We prove that the $\\ast$-operator maintains local computational constraints while achieving global reasoning capabilities, providing an efficient and convergent computational paradigm for abstract reasoning problems. Through rigorous mathematical analysis and comprehensive experiments on ARC2 challenges and Conway's Game of Life, we demonstrate the operator's universality, convergence properties, and superior performance. Our innovative Embedding-Asterisk distillation method achieves 100\\% accuracy on ARC2 validation with only 6M parameters, representing a significant breakthrough in neural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel Propagation, Asterisk Operator, Convergence, Universal Approximation",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games",
    "pdf_url": "https://arxiv.org/pdf/2509.13364v1",
    "published_date": "2025-09-15 16:11:03 UTC",
    "updated_date": "2025-09-15 16:11:03 UTC"
  },
  {
    "arxiv_id": "2509.12296v1",
    "title": "An End to End Edge to Cloud Data and Analytics Strategy",
    "authors": [
      "Vijay Kumar Butte",
      "Sujata Butte"
    ],
    "abstract": "There is an exponential growth of connected Internet of Things (IoT) devices. These have given rise to applications that rely on real time data to make critical decisions quickly. Enterprises today are adopting cloud at a rapid pace. There is a critical need to develop secure and efficient strategy and architectures to best leverage capabilities of cloud and edge assets. This paper provides an end to end secure edge to cloud data and analytics strategy. To enable real life implementation, the paper provides reference architectures for device layer, edge layer and cloud layer.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12296v1",
    "published_date": "2025-09-15 16:04:10 UTC",
    "updated_date": "2025-09-15 16:04:10 UTC"
  },
  {
    "arxiv_id": "2509.12074v1",
    "title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu",
      "Mohsen B. Mesgaran"
    ],
    "abstract": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic weed that threatens tomato production by extracting nutrients from the host. We investigate early detection using leaf-level spectral reflectance (400-2500 nm) and ensemble machine learning. In a field experiment in Woodland, California, we tracked 300 tomato plants across growth stages defined by growing degree days (GDD). Leaf reflectance was acquired with a portable spectrometer and preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing, correlation-based band reduction). Clear class differences were observed near 1500 nm and 2000 nm water absorption features, consistent with reduced leaf water content in infected plants at early stages. An ensemble combining Random Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at 585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and weed interference. Despite the small number of infected plants and environmental confounders, results show that proximal sensing with ensemble learning enables timely detection of broomrape before canopy symptoms are visible, supporting targeted interventions and reduced yield losses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Author-accepted version. Accepted and presented at AGRICONTROL 2025 (8th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture), UC Davis, USA. To appear in IFAC-PapersOnLine (Elsevier)",
    "pdf_url": "https://arxiv.org/pdf/2509.12074v1",
    "published_date": "2025-09-15 16:00:32 UTC",
    "updated_date": "2025-09-15 16:00:32 UTC"
  },
  {
    "arxiv_id": "2509.12069v3",
    "title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT",
    "authors": [
      "Zhi Qin Tan",
      "Xiatian Zhu",
      "Owen Addison",
      "Yunpeng Li"
    ],
    "abstract": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing first place in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.84, HD95 of 38.17 with the held-out test data, with an average inference time of 40.58s. In Task 2, U-Mamba2 achieved the mean Dice of 0.87 and HD95 of 2.15 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "First place solution for both tasks of the ToothFairy3 challenge, MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.12069v3",
    "published_date": "2025-09-15 15:52:43 UTC",
    "updated_date": "2025-12-07 16:01:02 UTC"
  },
  {
    "arxiv_id": "2509.20367v1",
    "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models",
    "authors": [
      "Leyi Ouyang"
    ],
    "abstract": "Diplomatic events consistently prompt widespread public discussion and debate. Public sentiment plays a critical role in diplomacy, as a good sentiment provides vital support for policy implementation, helps resolve international issues, and shapes a nation's international image. Traditional methods for gauging public sentiment, such as large-scale surveys or manual content analysis of media, are typically time-consuming, labor-intensive, and lack the capacity for forward-looking analysis. We propose a novel framework that identifies specific modifications for diplomatic event narratives to shift public sentiment from negative to neutral or positive. First, we train a language model to predict public reaction towards diplomatic events. To this end, we construct a dataset comprising descriptions of diplomatic events and their associated public discussions. Second, guided by communication theories and in collaboration with domain experts, we predetermined several textual features for modification, ensuring that any alterations changed the event's narrative framing while preserving its core facts.We develop a counterfactual generation algorithm that employs a large language model to systematically produce modified versions of an original text. The results show that this framework successfully shifted public sentiment to a more favorable state with a 70\\% success rate. This framework can therefore serve as a practical tool for diplomats, policymakers, and communication specialists, offering data-driven insights on how to frame diplomatic initiatives or report on events to foster a more desirable public sentiment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "2 Figures, 7 Tables, 1 Algorithm",
    "pdf_url": "https://arxiv.org/pdf/2509.20367v1",
    "published_date": "2025-09-15 15:46:26 UTC",
    "updated_date": "2025-09-15 15:46:26 UTC"
  },
  {
    "arxiv_id": "2509.12060v2",
    "title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models",
    "authors": [
      "Wei Cai",
      "Shujuan Liu",
      "Jian Zhao",
      "Ziyan Shi",
      "Yusheng Zhao",
      "Yuchen Yuan",
      "Tianle Zhang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are susceptible to the implicit reasoning risk, wherein innocuous unimodal inputs synergistically assemble into risky multimodal data that produce harmful outputs. We attribute this vulnerability to the difficulty of MLLMs maintaining safety alignment through long-chain reasoning. To address this issue, we introduce Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring interpretable reasoning paths tailored for such a cross-modal challenge. A novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is also designed based on the SSUI dataset to align the MLLM's internal reasoning process with human safety values. Experimental results show that our SRPO-trained models achieve state-of-the-art results on key safety benchmarks, including the proposed Reasoning Path Benchmark (RSBench), significantly outperforming both open-source and top-tier commercial MLLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12060v2",
    "published_date": "2025-09-15 15:40:58 UTC",
    "updated_date": "2025-09-16 06:19:02 UTC"
  },
  {
    "arxiv_id": "2509.12053v1",
    "title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications",
    "authors": [
      "Yujun Lin",
      "Zhekai Zhang",
      "Song Han"
    ],
    "abstract": "Modern tensor applications, especially foundation models and generative AI applications require multiple input modalities (both vision and language), which increases the demand for flexible accelerator architecture. Existing frameworks suffer from the trade-off between design flexibility and productivity of RTL generation: either limited to very few hand-written templates or cannot automatically generate the RTL. To address this challenge, we propose the LEGO framework, which targets tensor applications and automatically generates spatial architecture design and outputs synthesizable RTL code without handwritten RTL design templates. Leveraging the affine-transformation-based architecture representation, LEGO front end finds interconnections between function units, synthesizes the memory system, and fuses different spatial dataflow designs based on data reuse analysis. LEGO back end then translates the hardware in a primitive-level graph to perform lower-level optimizations, and applies a set of linear-programming algorithms to optimally insert pipeline registers and reduce the overhead of unused logic when switching spatial dataflows. Our evaluation demonstrates that LEGO can achieve 3.2x speedup and 2.4x energy efficiency compared to previous work Gemmini, and can generate one architecture for diverse modern foundation models in generative AI applications.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "The first two authors have equal contributions; Published as a conference paper in HPCA 2025; 13 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12053v1",
    "published_date": "2025-09-15 15:36:18 UTC",
    "updated_date": "2025-09-15 15:36:18 UTC"
  },
  {
    "arxiv_id": "2509.12049v1",
    "title": "Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents",
    "authors": [
      "Hyeonggeun Yun",
      "Jinkyu Jang"
    ],
    "abstract": "Although browser-using agents (BUAs) show promise for web tasks and automation, most BUAs terminate after executing a single instruction, failing to support users' complex, nonlinear browsing with ambiguous goals, iterative decision-making, and changing contexts. We present a human-in-the-loop (HITL) conceptual framework informed by theories of human web browsing behavior. The framework centers on an iterative loop in which the BUA proactively proposes next actions and the user steers the browsing process through feedback. It also distinguishes between exploration and exploitation actions, enabling users to control the breadth and depth of their browsing. Consequently, the framework aims to reduce users' physical and cognitive effort while preserving users' traditional browsing mental model and supporting users in achieving satisfactory outcomes. We illustrate how the framework operates with hypothetical use cases and discuss the shift from manual browsing to interaction-driven browsing. We contribute a theoretically informed conceptual framework for BUAs.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12049v1",
    "published_date": "2025-09-15 15:31:53 UTC",
    "updated_date": "2025-09-15 15:31:53 UTC"
  },
  {
    "arxiv_id": "2509.12047v1",
    "title": "A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset",
    "authors": [
      "Haiyu Yang",
      "Enhong Liu",
      "Jennifer Sun",
      "Sumit Sharma",
      "Meike van Leerdam",
      "Sebastien Franceschini",
      "Puchun Niu",
      "Miel Hostens"
    ],
    "abstract": "Animal behavior analysis plays a crucial role in understanding animal welfare, health status, and productivity in agricultural settings. However, traditional manual observation methods are time-consuming, subjective, and limited in scalability. We present a modular pipeline that leverages open-sourced state-of-the-art computer vision techniques to automate animal behavior analysis in a group housing environment. Our approach combines state-of-the-art models for zero-shot object detection, motion-aware tracking and segmentation, and advanced feature extraction using vision transformers for robust behavior recognition. The pipeline addresses challenges including animal occlusions and group housing scenarios as demonstrated in indoor pig monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset for multiple behavioral tasks. Our temporal model achieved 94.2% overall accuracy, representing a 21.2 percentage point improvement over existing methods. The pipeline demonstrated robust tracking capabilities with 93.3% identity preservation score and 89.3% object detection precision. The modular design suggests potential for adaptation to other contexts, though further validation across species would be required. The open-source implementation provides a scalable solution for behavior monitoring, contributing to precision pig farming and welfare assessment through automated, objective, and continuous analysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 figures, Submitted to Computers and Electronics in Agriculture",
    "pdf_url": "https://arxiv.org/pdf/2509.12047v1",
    "published_date": "2025-09-15 15:31:12 UTC",
    "updated_date": "2025-09-15 15:31:12 UTC"
  },
  {
    "arxiv_id": "2509.12046v1",
    "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking",
    "authors": [
      "Zirui Zheng",
      "Takashi Isobe",
      "Tong Shen",
      "Xu Jia",
      "Jianbin Zhao",
      "Xiaomin Li",
      "Mengmeng Ge",
      "Baolu Li",
      "Qinghe Wang",
      "Dong Li",
      "Dong Zhou",
      "Yunzhi Zhuge",
      "Huchuan Lu",
      "Emad Barsoum"
    ],
    "abstract": "While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12046v1",
    "published_date": "2025-09-15 15:27:29 UTC",
    "updated_date": "2025-09-15 15:27:29 UTC"
  },
  {
    "arxiv_id": "2509.12040v2",
    "title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing",
    "authors": [
      "Bingyu Li",
      "Haocheng Dong",
      "Da Zhang",
      "Zhiyuan Zhao",
      "Junyu Gao",
      "Xuelong Li"
    ],
    "abstract": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12040v2",
    "published_date": "2025-09-15 15:24:49 UTC",
    "updated_date": "2025-11-15 14:41:22 UTC"
  },
  {
    "arxiv_id": "2509.12034v1",
    "title": "Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review",
    "authors": [
      "Emmanuel Adjei Domfeh",
      "Christopher L. Dancy"
    ],
    "abstract": "In high-stakes disaster scenarios, timely and informed decision-making is critical yet often challenged by uncertainty, dynamic environments, and limited resources. This paper presents a systematic review of Human-AI collaboration patterns that support decision-making across all disaster management phases. Drawing from 51 peer-reviewed studies, we identify four major categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. Within these, we analyze sub-patterns such as cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments. Our review highlights how AI systems may enhance situational awareness, improves response efficiency, and support complex decision-making, while also surfacing critical limitations in scalability, interpretability, and system interoperability. We conclude by outlining key challenges and future research directions, emphasizing the need for adaptive, trustworthy, and context-aware Human-AI systems to improve disaster resilience and equitable recovery outcomes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.12034v1",
    "published_date": "2025-09-15 15:18:49 UTC",
    "updated_date": "2025-09-15 15:18:49 UTC"
  },
  {
    "arxiv_id": "2509.14270v2",
    "title": "SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models",
    "authors": [
      "Karan Dua",
      "Puneet Mittal",
      "Ranjeet Gupta",
      "Hitesh Laxmichand Patel"
    ],
    "abstract": "High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.14270v2",
    "published_date": "2025-09-15 15:11:43 UTC",
    "updated_date": "2025-10-01 19:37:31 UTC"
  },
  {
    "arxiv_id": "2509.12026v1",
    "title": "Imitation Learning as Return Distribution Matching",
    "authors": [
      "Filippo Lazzati",
      "Alberto Maria Metelli"
    ],
    "abstract": "We study the problem of training a risk-sensitive reinforcement learning (RL) agent through imitation learning (IL). Unlike standard IL, our goal is not only to train an agent that matches the expert's expected return (i.e., its average performance) but also its risk attitude (i.e., other features of the return distribution, such as variance). We propose a general formulation of the risk-sensitive IL problem in which the objective is to match the expert's return distribution in Wasserstein distance. We focus on the tabular setting and assume the expert's reward is known. After demonstrating the limited expressivity of Markovian policies for this task, we introduce an efficient and sufficiently expressive subclass of non-Markovian policies tailored to it. Building on this subclass, we develop two provably efficient algorithms, RS-BC and RS-KT, for solving the problem when the transition model is unknown and known, respectively. We show that RS-KT achieves substantially lower sample complexity than RS-BC by exploiting dynamics information. We further demonstrate the sample efficiency of return distribution matching in the setting where the expert's reward is unknown by designing an oracle-based variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and RS-BC with numerical simulations, highlighting both their sample efficiency and the advantages of non-Markovian policies over standard sample-efficient IL algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12026v1",
    "published_date": "2025-09-15 15:08:04 UTC",
    "updated_date": "2025-09-15 15:08:04 UTC"
  },
  {
    "arxiv_id": "2509.12019v1",
    "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models",
    "authors": [
      "Sangjun Lee",
      "Seung-taek Woo",
      "Jungyu Jin",
      "Changhun Lee",
      "Eunhyeok Park"
    ],
    "abstract": "To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wise quantization bit-widths to optimally balance model quality and memory usage. However, the combinatorial search space, with over 10^{100} possible configurations, makes conventional black-box optimization infeasible. AMQ overcomes this challenge through four key innovations:(1) search space pruning using prior knowledge to exclude unpromising configurations, (2) quantization proxy to bypass costly format conversions during search, (3) quality predictor to minimize evaluation overhead, and (4) iterative search-and-update strategy for fast and stable convergence. By integrating these components, AMQ efficiently explores the quality-efficiency landscape, reaching the Pareto frontier and yielding LLMs that are both compact and high-performing. Our code is available at https://github.com/dlwns147/amq.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025 Main Conference, Long Paper (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2509.12019v1",
    "published_date": "2025-09-15 14:59:35 UTC",
    "updated_date": "2025-09-15 14:59:35 UTC"
  },
  {
    "arxiv_id": "2509.12010v1",
    "title": "Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids",
    "authors": [
      "Filippo Lazzati",
      "Alberto Maria Metelli"
    ],
    "abstract": "We study the problem of generalizing an expert agent's behavior, provided through demonstrations, to new environments and/or additional constraints. Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to recover the expert's underlying reward function, which, if used for planning in the new settings, would reproduce the desired behavior. However, IRL is inherently ill-posed: multiple reward functions, forming the so-called feasible set, can explain the same observed behavior. Since these rewards may induce different policies in the new setting, in the absence of additional information, a decision criterion is needed to select which policy to deploy. In this paper, we propose a novel, principled criterion that selects the \"average\" policy among those induced by the rewards in a certain bounded subset of the feasible set. Remarkably, we show that this policy can be obtained by planning with the reward centroid of that subset, for which we derive a closed-form expression. We then present a provably efficient algorithm for estimating this centroid using an offline dataset of expert demonstrations only. Finally, we conduct numerical simulations that illustrate the relationship between the expert's behavior and the behavior produced by our method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12010v1",
    "published_date": "2025-09-15 14:53:54 UTC",
    "updated_date": "2025-09-15 14:53:54 UTC"
  },
  {
    "arxiv_id": "2509.11991v1",
    "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles",
    "authors": [
      "Jesús Calleja",
      "David Ponce",
      "Thierry Etchegoyhen"
    ],
    "abstract": "We describe Vicomtech's participation in the CLEARS challenge on text adaptation to Plain Language and Easy Read in Spanish. Our approach features automatic post-editing of different types of initial Large Language Model adaptations, where successive adaptations are generated iteratively until readability and similarity metrics indicate that no further adaptation refinement can be successfully performed. Taking the average of all official metrics, our submissions achieved first and second place in Plain language and Easy Read adaptation, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11991v1",
    "published_date": "2025-09-15 14:42:44 UTC",
    "updated_date": "2025-09-15 14:42:44 UTC"
  },
  {
    "arxiv_id": "2509.11974v1",
    "title": "Poison to Detect: Detection of Targeted Overfitting in Federated Learning",
    "authors": [
      "Soumia Zohra El Mestari",
      "Maciej Krzysztof Zuziak",
      "Gabriele Lenzini"
    ],
    "abstract": "Federated Learning (FL) enables collaborative model training across decentralised clients while keeping local data private, making it a widely adopted privacy-enhancing technology (PET). Despite its privacy benefits, FL remains vulnerable to privacy attacks, including those targeting specific clients. In this paper, we study an underexplored threat where a dishonest orchestrator intentionally manipulates the aggregation process to induce targeted overfitting in the local models of specific clients. Whereas many studies in this area predominantly focus on reducing the amount of information leakage during training, we focus on enabling an early client-side detection of targeted overfitting, thereby allowing clients to disengage before significant harm occurs. In line with this, we propose three detection techniques - (a) label flipping, (b) backdoor trigger injection, and (c) model fingerprinting - that enable clients to verify the integrity of the global aggregation. We evaluated our methods on multiple datasets under different attack scenarios. Our results show that the three methods reliably detect targeted overfitting induced by the orchestrator, but they differ in terms of computational complexity, detection latency, and false-positive rates.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11974v1",
    "published_date": "2025-09-15 14:23:39 UTC",
    "updated_date": "2025-09-15 14:23:39 UTC"
  },
  {
    "arxiv_id": "2509.11973v1",
    "title": "MusicSwarm: Biologically Inspired Intelligence for Music Composition",
    "authors": [
      "Markus J. Buehler"
    ],
    "abstract": "We show that coherent, long-form musical composition can emerge from a decentralized swarm of identical, frozen foundation models that coordinate via stigmergic, peer-to-peer signals, without any weight updates. We compare a centralized multi-agent system with a global critic to a fully decentralized swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and structural cues, adapt short-term memory, and reach consensus. Across symbolic, audio, and graph-theoretic analyses, the swarm yields superior quality while delivering greater diversity and structural variety and leads across creativity metrics. The dynamics contract toward a stable configuration of complementary roles, and self-similarity networks reveal a small-world architecture with efficient long-range connectivity and specialized bridging motifs, clarifying how local novelties consolidate into global musical form. By shifting specialization from parameter updates to interaction rules, shared memory, and dynamic consensus, MusicSwarm provides a compute- and data-efficient route to long-horizon creative structure that is immediately transferable beyond music to collaborative writing, design, and scientific discovery.",
    "categories": [
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11973v1",
    "published_date": "2025-09-15 14:23:09 UTC",
    "updated_date": "2025-09-15 14:23:09 UTC"
  },
  {
    "arxiv_id": "2509.11971v1",
    "title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study",
    "authors": [
      "James C. Ward",
      "Alex Bott",
      "Connor York",
      "Edmund R. Hunt"
    ],
    "abstract": "Simulating hostile attacks of physical autonomous systems can be a useful tool to examine their robustness to attack and inform vulnerability-aware design. In this work, we examine this through the lens of multi-robot patrol, by presenting a machine learning-based adversary model that observes robot patrol behavior in order to attempt to gain undetected access to a secure environment within a limited time duration. Such a model allows for evaluation of a patrol system against a realistic potential adversary, offering insight into future patrol strategy design. We show that our new model outperforms existing baselines, thus providing a more stringent test, and examine its performance against multiple leading decentralized multi-robot patrol strategies.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11971v1",
    "published_date": "2025-09-15 14:22:08 UTC",
    "updated_date": "2025-09-15 14:22:08 UTC"
  },
  {
    "arxiv_id": "2509.18143v1",
    "title": "Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron",
    "authors": [
      "Mike Smart",
      "Sachin Maheshwari",
      "Himadri Singh Raghav",
      "Alexander Serb"
    ],
    "abstract": "Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits offer the potential for highly energy-efficient Artificial Neural Network (ANN) computation in full custom analog IC designs. The efficient mapping of Artificial Neuron (AN) abstract weights, extracted from the software-trained ANNs, onto physical ACN capacitance values has, however, yet to be fully researched. In this paper, we explore the unexpected hidden complexities, challenges and properties of the mapping, as well as, the ramifications for IC designers in terms accuracy, design and implementation. We propose an optimal, AN to ACN methodology, that promotes smaller chip sizes and improved overall classification accuracy, necessary for successful practical deployment. Using TensorFlow and Larq software frameworks, we train three different ANN networks and map their weights into the energy-efficient DTSC ACN capacitance value domain to demonstrate 100% functional equivalency. Finally, we delve into the impact of weight quantization on ACN performance using novel metrics related to practical IC considerations, such as IC floor space and comparator decision-making efficacy.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.ET",
    "comment": "11 pages, 10 figures, 6 tables. This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2509.18143v1",
    "published_date": "2025-09-15 14:11:34 UTC",
    "updated_date": "2025-09-15 14:11:34 UTC"
  },
  {
    "arxiv_id": "2509.11947v2",
    "title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students",
    "authors": [
      "Guy Tel-Zur"
    ],
    "abstract": "This project addresses a critical pedagogical need: offering students continuous, on-demand academic assistance beyond conventional reception hours. I present a domain-specific Retrieval-Augmented Generation (RAG) system powered by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The assistant enhances learning by delivering real-time, personalized responses aligned with the \"Introduction to Parallel Processing\" course materials. GPU acceleration significantly improves inference latency, enabling practical deployment on consumer hardware. This approach demonstrates how consumer GPUs can enable affordable, private, and effective AI tutoring for HPC education.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.11947v2",
    "published_date": "2025-09-15 14:06:09 UTC",
    "updated_date": "2025-11-17 13:19:12 UTC"
  },
  {
    "arxiv_id": "2509.11944v1",
    "title": "Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare",
    "authors": [
      "Susanta Mitra"
    ],
    "abstract": "Healthcare and medicine are multimodal disciplines that deal with multimodal data for reasoning and diagnosing multiple diseases. Although some multimodal reasoning models have emerged for reasoning complex tasks in scientific domains, their applications in the healthcare domain remain limited and fall short in correct reasoning for diagnosis. To address the challenges of multimodal medical reasoning for correct diagnosis and assist the healthcare professionals, a novel temporal graph-based reasoning process modelled through a directed graph has been proposed in the current work. It helps in accommodating dynamic changes in reasons through backtracking, refining the reasoning content, and creating new or deleting existing reasons to reach the best recommendation or answer. Again, consideration of multimodal data at different time points can enable tracking and analysis of patient health and disease progression. Moreover, the proposed multi-agent temporal reasoning framework provides task distributions and a cross-validation mechanism to further enhance the accuracy of reasoning outputs. A few basic experiments and analysis results justify the novelty and practical utility of the proposed preliminary approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11944v1",
    "published_date": "2025-09-15 14:03:19 UTC",
    "updated_date": "2025-09-15 14:03:19 UTC"
  },
  {
    "arxiv_id": "2509.11943v3",
    "title": "Agentic System with Modal Logic for Autonomous Diagnostics",
    "authors": [
      "Antonin Sulc",
      "Thorsten Hellert"
    ],
    "abstract": "The development of intelligent agents, particularly those powered by language models (LMs), has shown a critical role in various environments that require intelligent and autonomous decision-making. Environments are not passive testing grounds, and they represent the data required for agents to learn and exhibit in very challenging conditions that require adaptive, complex, and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \\emph{possibility} and \\emph{necessity} using the formal language of modal logic. In this work, we use immutable, domain-specific knowledge to make an informed root cause diagnosis, which is encoded as logical constraints essential for proper, reliable, and explainable diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2509.11943v3",
    "published_date": "2025-09-15 14:03:06 UTC",
    "updated_date": "2025-10-18 00:10:05 UTC"
  },
  {
    "arxiv_id": "2509.11942v1",
    "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems",
    "authors": [
      "Luís F. Gomes",
      "Xin Zhou",
      "David Lo",
      "Rui Abreu"
    ],
    "abstract": "Visual documentation is an effective tool for reducing the cognitive barrier developers face when understanding unfamiliar code, enabling more intuitive comprehension. Compared to textual documentation, it provides a higher-level understanding of the system structure and data flow. Developers usually prefer visual representations over lengthy textual descriptions for large software systems. Visual documentation is both difficult to produce and challenging to evaluate. Manually creating it is time-consuming, and currently, no existing approach can automatically generate high-level visual documentation directly from code. Its evaluation is often subjective, making it difficult to standardize and automate. To address these challenges, this paper presents the first exploration of using agentic LLM systems to automatically generate visual documentation. We introduce VisDocSketcher, the first agent-based approach that combines static analysis with LLM agents to identify key elements in the code and produce corresponding visual representations. We propose a novel evaluation framework, AutoSketchEval, for assessing the quality of generated visual documentation using code-level metrics. The experimental results show that our approach can valid visual documentation for 74.4% of the samples. It shows an improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation framework can reliably distinguish high-quality (code-aligned) visual documentation from low-quality (non-aligned) ones, achieving an AUC exceeding 0.87. Our work lays the foundation for future research on automated visual documentation by introducing practical tools that not only generate valid visual representations but also reliably assess their quality.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11942v1",
    "published_date": "2025-09-15 14:02:29 UTC",
    "updated_date": "2025-09-15 14:02:29 UTC"
  },
  {
    "arxiv_id": "2509.11941v2",
    "title": "How to Evaluate Medical AI",
    "authors": [
      "Ilia Kopanichuk",
      "Petr Anokhin",
      "Vladimir Shaposhnikov",
      "Vladimir Makharev",
      "Ekaterina Tsapieva",
      "Iaroslav Bespalov",
      "Dmitry V. Dylov",
      "Ivan Oseledets"
    ],
    "abstract": "The integration of artificial intelligence (AI) into medical diagnostic workflows requires robust and consistent evaluation methods to ensure reliability, clinical relevance, and the inherent variability in expert judgments. Traditional metrics like precision and recall often fail to account for the inherent variability in expert judgments, leading to inconsistent assessments of AI performance. Inter-rater agreement statistics like Cohen's Kappa are more reliable but they lack interpretability. We introduce Relative Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new evaluation metrics that compare AI outputs against multiple expert opinions rather than a single reference. By normalizing performance against inter-expert disagreement, these metrics provide a more stable and realistic measure of the quality of predicted diagnosis. In addition to the comprehensive analysis of diagnostic quality measures, our study contains a very important side result. Our evaluation methodology allows us to avoid selecting diagnoses from a limited list when evaluating a given case. Instead, both the models being tested and the examiners verifying them arrive at a free-form diagnosis. In this automated methodology for establishing the identity of free-form clinical diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our approach using 360 medical dialogues, comparing multiple large language models (LLMs) against a panel of physicians. Large-scale study shows that top-performing models, such as DeepSeek-V3, achieve consistency on par with or exceeding expert consensus. Moreover, we demonstrate that expert judgments exhibit significant variability - often greater than that between AI and humans. This finding underscores the limitations of any absolute metrics and supports the need to adopt relative metrics in medical AI.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 7 fugures",
    "pdf_url": "https://arxiv.org/pdf/2509.11941v2",
    "published_date": "2025-09-15 14:01:22 UTC",
    "updated_date": "2025-09-25 09:31:04 UTC"
  },
  {
    "arxiv_id": "2509.11940v4",
    "title": "Neuromorphic Intelligence",
    "authors": [
      "Marcel van Gerven"
    ],
    "abstract": "Neuromorphic computing seeks to replicate the remarkable efficiency, flexibility, and adaptability of the human brain in artificial systems. Unlike conventional digital approaches, which suffer from the Von Neumann bottleneck and depend on massive computational and energy resources, neuromorphic systems exploit brain-inspired principles of computation to achieve orders of magnitude greater energy efficiency. By drawing on insights from a wide range of disciplines -- including artificial intelligence, physics, chemistry, biology, neuroscience, cognitive science and materials science -- neuromorphic computing promises to deliver intelligent systems that are sustainable, transparent, and widely accessible. A central challenge, however, is to identify a unifying theoretical framework capable of bridging these diverse disciplines. We argue that dynamical systems theory provides such a foundation. Rooted in differential calculus, it offers a principled language for modeling inference, learning, and control in both natural and artificial substrates. Within this framework, noise can be harnessed as a resource for learning, while differential genetic programming enables the discovery of dynamical systems that implement adaptive behaviors. Embracing this perspective paves the way toward emergent neuromorphic intelligence, where intelligent behavior arises from the dynamics of physical substrates, advancing both the science and sustainability of AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 2 figures, 3 boxes",
    "pdf_url": "https://arxiv.org/pdf/2509.11940v4",
    "published_date": "2025-09-15 13:59:42 UTC",
    "updated_date": "2025-11-02 09:37:33 UTC"
  },
  {
    "arxiv_id": "2509.11937v1",
    "title": "MMORE: Massive Multimodal Open RAG & Extraction",
    "authors": [
      "Alexandre Sallinen",
      "Stefan Krsteski",
      "Paul Teiletche",
      "Marc-Antoine Allard",
      "Baptiste Lecoeur",
      "Michael Zhang",
      "Fabrice Nemo",
      "David Kalajdzic",
      "Matthias Meyer",
      "Mary-Anne Hartley"
    ],
    "abstract": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open RetrievalAugmented Generation and Extraction, designed to ingest, transform, and retrieve knowledge from heterogeneous document formats at scale. MMORE supports more than fifteen file types, including text, tables, images, emails, audio, and video, and processes them into a unified format to enable downstream applications for LLMs. The architecture offers modular, distributed processing, enabling scalable parallelization across CPUs and GPUs. On processing benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve biomedical QA accuracy with increasing retrieval depth. MMORE provides a robust, extensible foundation for deploying task-agnostic RAG systems on diverse, real-world multimodal data. The codebase is available at https://github.com/swiss-ai/mmore.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "This paper was originally submitted to the CODEML workshop for ICML 2025. 9 pages (including references and appendices)",
    "pdf_url": "https://arxiv.org/pdf/2509.11937v1",
    "published_date": "2025-09-15 13:56:06 UTC",
    "updated_date": "2025-09-15 13:56:06 UTC"
  },
  {
    "arxiv_id": "2509.11922v1",
    "title": "BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning",
    "authors": [
      "Xilei Dai",
      "Ruotian Chen",
      "Songze Guan",
      "Wen-Tai Li",
      "Chau Yuen"
    ],
    "abstract": "Reinforcement learning (RL) has proven effective for AI-based building energy management. However, there is a lack of flexible framework to implement RL across various control problems in building energy management. To address this gap, we propose BuildingGym, an open-source tool designed as a research-friendly and flexible framework for training RL control strategies for common challenges in building energy management. BuildingGym integrates EnergyPlus as its core simulator, making it suitable for both system-level and room-level control. Additionally, BuildingGym is able to accept external signals as control inputs instead of taking the building as a stand-alone entity. This feature makes BuildingGym applicable for more flexible environments, e.g. smart grid and EVs community. The tool provides several built-in RL algorithms for control strategy training, simplifying the process for building managers to obtain optimal control strategies. Users can achieve this by following a few straightforward steps to configure BuildingGym for optimization control for common problems in the building energy management field. Moreover, AI specialists can easily implement and test state-of-the-art control algorithms within the platform. BuildingGym bridges the gap between building managers and AI specialists by allowing for the easy configuration and replacement of RL algorithms, simulators, and control environments or problems. With BuildingGym, we efficiently set up training tasks for cooling load management, targeting both constant and dynamic cooling load management. The built-in algorithms demonstrated strong performance across both tasks, highlighting the effectiveness of BuildingGym in optimizing cooling strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11922v1",
    "published_date": "2025-09-15 13:37:48 UTC",
    "updated_date": "2025-09-15 13:37:48 UTC"
  },
  {
    "arxiv_id": "2509.11914v1",
    "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models",
    "authors": [
      "Yiqun Yao",
      "Naitong Yu",
      "Xiang Li",
      "Xin Jiang",
      "Xuezhi Fang",
      "Wenjia Ma",
      "Xuying Meng",
      "Jing Li",
      "Aixin Sun",
      "Yequan Wang"
    ],
    "abstract": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex models that process real-time omnimodal streams. EgoMem enables real-time models to recognize multiple users directly from raw audiovisual streams, to provide personalized response, and to maintain long-term knowledge of users' facts, preferences, and social relationships extracted from audiovisual history. EgoMem operates with three asynchronous processes: (i) a retrieval process that dynamically identifies user via face and voice, and gathers relevant context from a long-term memory; (ii) an omnimodal dialog process that generates personalized audio responses based on the retrieved context; and (iii) a memory management process that automatically detects dialog boundaries from omnimodal streams, and extracts necessary information to update the long-term memory. Unlike existing memory agents for LLMs, EgoMem relies entirely on raw audiovisual streams, making it especially suitable for lifelong, real-time, and embodied scenarios. Experimental results demonstrate that EgoMem's retrieval and memory management modules achieve over 95% accuracy on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot, the system achieves fact-consistency scores above 87% in real-time personalized dialogs, establishing a strong baseline for future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11914v1",
    "published_date": "2025-09-15 13:33:29 UTC",
    "updated_date": "2025-09-15 13:33:29 UTC"
  },
  {
    "arxiv_id": "2509.11895v1",
    "title": "Integrating Prior Observations for Incremental 3D Scene Graph Prediction",
    "authors": [
      "Marian Renz",
      "Felix Igelbrink",
      "Martin Atzmueller"
    ],
    "abstract": "3D semantic scene graphs (3DSSG) provide compact structured representations of environments by explicitly modeling objects, attributes, and relationships. While 3DSSGs have shown promise in robotics and embodied AI, many existing methods rely mainly on sensor data, not integrating further information from semantically rich environments. Additionally, most methods assume access to complete scene reconstructions, limiting their applicability in real-world, incremental settings. This paper introduces a novel heterogeneous graph model for incremental 3DSSG prediction that integrates additional, multi-modal information, such as prior observations, directly into the message-passing process. Utilizing multiple layers, the model flexibly incorporates global and local scene representations without requiring specialized modules or full scene reconstructions. We evaluate our approach on the 3DSSG dataset, showing that GNNs enriched with multi-modal information such as semantic embeddings (e.g., CLIP) and prior observations offer a scalable and generalizable solution for complex, real-world environments. The full source code of the presented architecture will be made available at https://github.com/m4renz/incremental-scene-graph-prediction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at 24th International Conference on Machine Learning and Applications (ICMLA'25)",
    "pdf_url": "https://arxiv.org/pdf/2509.11895v1",
    "published_date": "2025-09-15 13:10:34 UTC",
    "updated_date": "2025-09-15 13:10:34 UTC"
  },
  {
    "arxiv_id": "2509.11880v1",
    "title": "Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning",
    "authors": [
      "Carlos Celemin",
      "Joseph Brennan",
      "Pierluigi Vito Amadori",
      "Tim Bradley"
    ],
    "abstract": "This paper introduces a novel application of Supervised Contrastive Learning (SupCon) to Imitation Learning (IL), with a focus on learning more effective state representations for agents in video game environments. The goal is to obtain latent representations of the observations that capture better the action-relevant factors, thereby modeling better the cause-effect relationship from the observations that are mapped to the actions performed by the demonstrator, for example, the player jumps whenever an obstacle appears ahead. We propose an approach to integrate the SupCon loss with continuous output spaces, enabling SupCon to operate without constraints regarding the type of actions of the environment. Experiments on the 3D games Astro Bot and Returnal, and multiple 2D Atari games show improved representation quality, faster learning convergence, and better generalization compared to baseline models trained only with supervised action prediction loss functions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11880v1",
    "published_date": "2025-09-15 13:00:29 UTC",
    "updated_date": "2025-09-15 13:00:29 UTC"
  },
  {
    "arxiv_id": "2509.11868v1",
    "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models",
    "authors": [
      "Sabrina Patania",
      "Luca Annese",
      "Anna Lambiase",
      "Anita Pellegrini",
      "Tom Foulsham",
      "Azzurra Ruggeri",
      "Silvia Rossi",
      "Silvia Serino",
      "Dimitri Ognibene"
    ],
    "abstract": "Language and embodied perspective taking are essential for human collaboration, yet few computational models address both simultaneously. This work investigates the PerspAct system [1], which integrates the ReAct (Reason and Act) paradigm with Large Language Models (LLMs) to simulate developmental stages of perspective taking, grounded in Selman's theory [2]. Using an extended director task, we evaluate GPT's ability to generate internal narratives aligned with specified developmental stages, and assess how these influence collaborative performance both qualitatively (action selection) and quantitatively (task efficiency). Results show that GPT reliably produces developmentally-consistent narratives before task execution but often shifts towards more advanced stages during interaction, suggesting that language exchanges help refine internal representations. Higher developmental stages generally enhance collaborative effectiveness, while earlier stages yield more variable outcomes in complex contexts. These findings highlight the potential of integrating embodied perspective taking and language in LLMs to better model developmental dynamics and stress the importance of evaluating internal speech during combined linguistic and embodied tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/",
    "pdf_url": "https://arxiv.org/pdf/2509.11868v1",
    "published_date": "2025-09-15 12:39:55 UTC",
    "updated_date": "2025-09-15 12:39:55 UTC"
  },
  {
    "arxiv_id": "2509.11865v1",
    "title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer",
    "authors": [
      "Travis Davies",
      "Yiqi Huang",
      "Yunxin Liu",
      "Xiang Chen",
      "Huxian Liu",
      "Luhui Hu"
    ],
    "abstract": "Scaling Transformer policies and diffusion models has advanced robotic manipulation, yet combining these techniques in lightweight, cross-embodiment learning settings remains challenging. We study design choices that most affect stability and performance for diffusion-transformer policies trained on heterogeneous, multimodal robot data, and introduce Tenma, a lightweight diffusion-transformer for bi-manual arm control. Tenma integrates multiview RGB, proprioception, and language via a cross-embodiment normalizer that maps disparate state/action spaces into a shared latent space; a Joint State-Time encoder for temporally aligned observation learning with inference speed boosts; and a diffusion action decoder optimized for training stability and learning capacity. Across benchmarks and under matched compute, Tenma achieves an average success rate of 88.95% in-distribution and maintains strong performance under object and scene shifts, substantially exceeding baseline policies whose best in-distribution average is 18.12%. Despite using moderate data scale, Tenma delivers robust manipulation and generalization, indicating the great potential for multimodal and cross-embodiment learning strategies for further augmenting the capacity of transformer-based imitation learning policies.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.11865v1",
    "published_date": "2025-09-15 12:39:15 UTC",
    "updated_date": "2025-09-15 12:39:15 UTC"
  },
  {
    "arxiv_id": "2509.11862v1",
    "title": "Bridging Vision Language Models and Symbolic Grounding for Video Question Answering",
    "authors": [
      "Haodi Ma",
      "Vyom Pathak",
      "Daisy Zhe Wang"
    ],
    "abstract": "Video Question Answering (VQA) requires models to reason over spatial, temporal, and causal cues in videos. Recent vision language models (VLMs) achieve strong results but often rely on shallow correlations, leading to weak temporal grounding and limited interpretability. We study symbolic scene graphs (SGs) as intermediate grounding signals for VQA. SGs provide structured object-relation representations that complement VLMs holistic reasoning. We introduce SG-VLM, a modular framework that integrates frozen VLMs with scene graph grounding via prompting and visual localization. Across three benchmarks (NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM improves causal and temporal reasoning and outperforms prior baselines, though gains over strong VLMs are limited. These findings highlight both the promise and current limitations of symbolic grounding, and offer guidance for future hybrid VLM-symbolic approaches in video understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11862v1",
    "published_date": "2025-09-15 12:35:56 UTC",
    "updated_date": "2025-09-15 12:35:56 UTC"
  },
  {
    "arxiv_id": "2509.11838v2",
    "title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network",
    "authors": [
      "Navid Hashemi",
      "Samuel Sasaki",
      "Diego Manzanas Lopez",
      "Lars Lindemann",
      "Ipek Oguz",
      "Meiyi Ma",
      "Taylor T. Johnson"
    ],
    "abstract": "Semantic segmentation networks (SSNs) are central to safety-critical applications such as medical imaging and autonomous driving, where robustness under uncertainty is essential. However, existing probabilistic verification methods often fail to scale with the complexity and dimensionality of modern segmentation tasks, producing guarantees that are overly conservative and of limited practical value. We propose a probabilistic verification framework that is architecture-agnostic and scalable to high-dimensional input-output spaces. Our approach employs conformal inference (CI), enhanced by a novel technique that we call the \\textbf{clipping block}, to provide provable guarantees while mitigating the excessive conservatism of prior methods. Experiments on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrate that our framework delivers reliable safety guarantees while substantially reducing conservatism compared to state-of-the-art approaches on segmentation tasks. We also provide a public GitHub repository (https://github.com/Navidhashemicodes/SSN_Reach_CLP_Surrogate) for this approach, to support reproducibility.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11838v2",
    "published_date": "2025-09-15 12:25:25 UTC",
    "updated_date": "2025-11-15 02:38:33 UTC"
  },
  {
    "arxiv_id": "2509.11824v1",
    "title": "Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio",
    "authors": [
      "Luca Casini",
      "Laura Cros Vila",
      "David Dalmazzo",
      "Anna-Kaisa Kaila",
      "Bob L. T. Sturm"
    ],
    "abstract": "Online AI platforms for creating music from text prompts (AI music), such as Suno and Udio, are now being used by hundreds of thousands of users. Some AI music is appearing in advertising, and even charting, in multiple countries. How are these platforms being used? What subjects are inspiring their users? This article answers these questions for Suno and Udio using a large collection of songs generated by users of these platforms from May to October 2024. Using a combination of state-of-the-art text embedding models, dimensionality reduction and clustering methods, we analyze the prompts, tags and lyrics, and automatically annotate and display the processed data in interactive plots. Our results reveal prominent themes in lyrics, language preference, prompting strategies, as well as peculiar attempts at steering models through the use of metatags. To promote the musicological study of the developing cultural practice of AI-generated music we share our code and resources.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "cs.IR",
    "comment": "Submitted for review to TISMIR Digital Musicology special issue",
    "pdf_url": "https://arxiv.org/pdf/2509.11824v1",
    "published_date": "2025-09-15 12:10:50 UTC",
    "updated_date": "2025-09-15 12:10:50 UTC"
  },
  {
    "arxiv_id": "2509.11816v2",
    "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning",
    "authors": [
      "Filip Sondej",
      "Yushi Yang"
    ],
    "abstract": "Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.\n  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.\n  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.\n  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11816v2",
    "published_date": "2025-09-15 11:55:10 UTC",
    "updated_date": "2025-11-13 17:23:32 UTC"
  },
  {
    "arxiv_id": "2509.11815v2",
    "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
    "authors": [
      "Haiduo Huang",
      "Fuwei Yang",
      "Zhenhua Liu",
      "Xuanwu Yin",
      "Dong Li",
      "Pengju Ren",
      "Emad Barsoum"
    ],
    "abstract": "Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at https://github.com/haiduo/SpecVLM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11815v2",
    "published_date": "2025-09-15 11:53:56 UTC",
    "updated_date": "2025-09-21 03:35:36 UTC"
  },
  {
    "arxiv_id": "2509.14269v2",
    "title": "SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models",
    "authors": [
      "Jianbin Zhang",
      "Yulin Zhu",
      "Wai Lun Lo",
      "Richard Tai-Chiu Hsung",
      "Harris Sik-Ho Tsang",
      "Kai Zhou"
    ],
    "abstract": "Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tuning strategies on LLM require the updates of billions of parameters, substantially increasing the training cost, including the training time and utility cost. To enhance the efficiency and effectiveness of the current medical LLMs and explore the boundary of the representation capability of the LLMs on the medical domain, apart from the traditional fine-tuning strategies from the data perspective (i.e., supervised fine-tuning or reinforcement learning from human feedback), we instead craft a novel sparse medical LLM named SparseDoctor armed with contrastive learning enhanced LoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end, the crafted automatic routing mechanism can scientifically allocate the computational resources among different LoRA experts supervised by the contrastive learning. Additionally, we also introduce a novel expert memory queue mechanism to further boost the efficiency of the overall framework and prevent the memory overflow during training. We conduct comprehensive evaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med. Experimental results demonstrate that the proposed LLM can consistently outperform the strong baselines such as the HuatuoGPT series.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.14269v2",
    "published_date": "2025-09-15 11:25:14 UTC",
    "updated_date": "2025-09-22 03:08:40 UTC"
  },
  {
    "arxiv_id": "2509.14268v1",
    "title": "DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models",
    "authors": [
      "Jiachen Fu",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model's output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: {https://fjc2005.github.io/detectanyllm}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.14268v1",
    "published_date": "2025-09-15 10:59:57 UTC",
    "updated_date": "2025-09-15 10:59:57 UTC"
  },
  {
    "arxiv_id": "2509.13359v3",
    "title": "Evaluating undergraduate mathematics examinations in the era of generative AI: a curriculum-level case study",
    "authors": [
      "Benjamin J. Walker",
      "Nikoleta Kalaydzhieva",
      "Beatriz Navarro Lameda",
      "Ruth A. Reynolds"
    ],
    "abstract": "Generative artificial intelligence (GenAI) tools such as OpenAI's ChatGPT are transforming the educational landscape, prompting reconsideration of traditional assessment practices. In parallel, universities are exploring alternatives to in-person, closed-book examinations, raising concerns about academic integrity and pedagogical alignment in uninvigilated settings. This study investigates whether traditional closed-book mathematics examinations retain their pedagogical relevance when hypothetically administered in uninvigilated, open-book settings with GenAI access. Adopting an empirical approach, we generate, transcribe, and blind-mark GenAI submissions to eight undergraduate mathematics examinations at a Russell Group university, spanning the entirety of the first-year curriculum. By combining independent GenAI responses to individual questions, we enable a meaningful evaluation of GenAI performance, both at the level of modules and across the first-year curriculum. We find that GenAI attainment is at the level of a first-class degree, though current performance can vary between modules. Further, we find that GenAI performance is remarkably consistent when viewed across the entire curriculum, significantly more so than that of students in invigilated examinations. Our findings evidence the need for redesigning assessments in mathematics for unsupervised settings, and highlight the potential reduction in pedagogical value of current standards in the era of generative artificial intelligence.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.13359v3",
    "published_date": "2025-09-15 10:34:31 UTC",
    "updated_date": "2025-09-29 12:08:08 UTC"
  },
  {
    "arxiv_id": "2509.19336v1",
    "title": "Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation",
    "authors": [
      "Qingsong Wang",
      "Tao Wu",
      "Wang Lin",
      "Yueying Feng",
      "Gongsheng Yuan",
      "Chang Yao",
      "Jingyuan Chen"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated strong performance in open-ended generation tasks. However, they often struggle to adapt content to users with differing cognitive capacities, leading to a phenomenon we term cognitive misalignment. This issue arises in two forms: knowledge-level misalignment, where content is too complex or too simplistic relative to user understanding, and presentation-style misalignment, where the structure or tone hinders effective comprehension. To address these challenges, we propose the Cognitive-Level Alignment Framework (CLAF), a general-purpose generation framework that aligns both knowledge complexity and presentation style with user cognition. CLAF integrates a capability-aware retrieval module based on a hierarchical knowledge graph and a style optimization module guided by Bloom's taxonomy and preference learning. Additionally, a knowledge-controllable generation component ensures consistency and relevance throughout the output. To support training and evaluation, we construct SCALE, a cognitively annotated dataset containing responses at multiple comprehension levels per query. Empirical results show that CLAF enhances the adaptability and informativeness of LLM outputs across a range of user profiles, offering a robust solution to cognitive-level alignment in real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of EMNLP 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.19336v1",
    "published_date": "2025-09-15 10:11:25 UTC",
    "updated_date": "2025-09-15 10:11:25 UTC"
  },
  {
    "arxiv_id": "2511.05498v1",
    "title": "Biomedical Hypothesis Explainability with Graph-Based Context Retrieval",
    "authors": [
      "Ilya Tyagin",
      "Saeideh Valipour",
      "Aliaksandra Sikirzhytskaya",
      "Michael Shtutman",
      "Ilya Safro"
    ],
    "abstract": "We introduce an explainability method for biomedical hypothesis generation systems, built on top of the novel Hypothesis Generation Context Retriever framework. Our approach combines semantic graph-based retrieval and relevant data-restrictive training to simulate real-world discovery constraints. Integrated with large language models (LLMs) via retrieval-augmented generation, the system explains hypotheses with contextual evidence using published scientific literature. We also propose a novel feedback loop approach, which iteratively identifies and corrects flawed parts of LLM-generated explanations, refining both the evidence paths and supporting context. We demonstrate the performance of our method with multiple large language models and evaluate the explanation and context retrieval quality through both expert-curated assessment and large-scale automated analysis. Our code is available at: https://github.com/IlyaTyagin/HGCR.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "30 pages, 10 figures,",
    "pdf_url": "https://arxiv.org/pdf/2511.05498v1",
    "published_date": "2025-09-15 09:50:51 UTC",
    "updated_date": "2025-09-15 09:50:51 UTC"
  },
  {
    "arxiv_id": "2509.11731v1",
    "title": "Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference",
    "authors": [
      "Yudong Shen",
      "Wenyu Wu",
      "Jiali Mao",
      "Yixiao Tong",
      "Guoping Liu",
      "Chaoya Wang"
    ],
    "abstract": "Trajectory data has become a key resource for automated map in-ference due to its low cost, broad coverage, and continuous availability. However, uneven trajectory density often leads to frag-mented roads in sparse areas and redundant segments in dense regions, posing significant challenges for existing methods. To address these issues, we propose DGMap, a dual-decoding framework with global context awareness, featuring Multi-scale Grid Encoding, Mask-enhanced Keypoint Extraction, and Global Context-aware Relation Prediction. By integrating global semantic context with local geometric features, DGMap improves keypoint detection accuracy to reduce road fragmentation in sparse-trajectory areas. Additionally, the Global Context-aware Relation Prediction module suppresses false connections in dense-trajectory regions by modeling long-range trajectory patterns. Experimental results on three real-world datasets show that DGMap outperforms state-of-the-art methods by 5% in APLS, with notable performance gains on trajectory data from the Didi Chuxing platform",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11731v1",
    "published_date": "2025-09-15 09:31:38 UTC",
    "updated_date": "2025-09-15 09:31:38 UTC"
  },
  {
    "arxiv_id": "2509.11727v1",
    "title": "Microsurgical Instrument Segmentation for Robot-Assisted Surgery",
    "authors": [
      "Tae Kyeong Jeong",
      "Garam Kim",
      "Juyoun Park"
    ],
    "abstract": "Accurate segmentation of thin structures is critical for microsurgical scene understanding but remains challenging due to resolution loss, low contrast, and class imbalance. We propose Microsurgery Instrument Segmentation for Robotic Assistance(MISRA), a segmentation framework that augments RGB input with luminance channels, integrates skip attention to preserve elongated features, and employs an Iterative Feedback Module(IFM) for continuity restoration across multiple passes. In addition, we introduce a dedicated microsurgical dataset with fine-grained annotations of surgical instruments including thin objects, providing a benchmark for robust evaluation Dataset available at https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate that MISRA achieves competitive performance, improving the mean class IoU by 5.37% over competing methods, while delivering more stable predictions at instrument contacts and overlaps. These results position MISRA as a promising step toward reliable scene parsing for computer-assisted and robotic microsurgery.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.11727v1",
    "published_date": "2025-09-15 09:29:27 UTC",
    "updated_date": "2025-09-15 09:29:27 UTC"
  },
  {
    "arxiv_id": "2509.11719v2",
    "title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction",
    "authors": [
      "Bingqing Wei",
      "Lianmin Chen",
      "Zhongyu Xia",
      "Yongtao Wang"
    ],
    "abstract": "Multi-agent trajectory prediction in autonomous driving requires a comprehensive understanding of complex social dynamics. Existing methods, however, often struggle to capture the full richness of these dynamics, particularly the co-existence of multi-scale interactions and the diverse behaviors of heterogeneous agents. To address these challenges, this paper introduces HeLoFusion, an efficient and scalable encoder for modeling heterogeneous and multi-scale agent interactions. Instead of relying on global context, HeLoFusion constructs local, multi-scale graphs centered on each agent, allowing it to effectively model both direct pairwise dependencies and complex group-wise interactions (\\textit{e.g.}, platooning vehicles or pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of agent heterogeneity through an aggregation-decomposition message-passing scheme and type-specific feature networks, enabling it to learn nuanced, type-dependent interaction patterns. This locality-focused approach enables a principled representation of multi-level social context, yielding powerful and expressive agent embeddings. On the challenging Waymo Open Motion Dataset, HeLoFusion achieves state-of-the-art performance, setting new benchmarks for key metrics including Soft mAP and minADE. Our work demonstrates that a locality-grounded architecture, which explicitly models multi-scale and heterogeneous interactions, is a highly effective strategy for advancing motion forecasting.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11719v2",
    "published_date": "2025-09-15 09:19:41 UTC",
    "updated_date": "2025-12-10 03:23:08 UTC"
  },
  {
    "arxiv_id": "2509.11698v1",
    "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model",
    "authors": [
      "Wei-Hsin Yeh",
      "Yu-An Su",
      "Chih-Ning Chen",
      "Yi-Hsueh Lin",
      "Calvin Ku",
      "Wen-Hsin Chiu",
      "Min-Chun Hu",
      "Lun-Wei Ku"
    ],
    "abstract": "Motion instruction is a crucial task that helps athletes refine their technique by analyzing movements and providing corrective guidance. Although recent advances in multimodal models have improved motion understanding, generating precise and sport-specific instruction remains challenging due to the highly domain-specific nature of sports and the need for informative guidance. We propose CoachMe, a reference-based model that analyzes the differences between a learner's motion and a reference under temporal and physical aspects. This approach enables both domain-knowledge learning and the acquisition of a coach-like thinking process that identifies movement errors effectively and provides feedback to explain how to improve. In this paper, we illustrate how CoachMe adapts well to specific sports such as skating and boxing by learning from general movements and then leveraging limited data. Experiments show that CoachMe provides high-quality instructions instead of directions merely in the tone of a coach but without critical information. CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on boxing. Analysis further confirms that it elaborates on errors and their corresponding improvement methods in the generated instructions. You can find CoachMe here: https://motionxperts.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025. Official version: https://doi.org/10.18653/v1/2025.acl-long.1413",
    "pdf_url": "https://arxiv.org/pdf/2509.11698v1",
    "published_date": "2025-09-15 09:01:39 UTC",
    "updated_date": "2025-09-15 09:01:39 UTC"
  },
  {
    "arxiv_id": "2509.21339v1",
    "title": "Cross-Modal Retrieval with Cauchy-Schwarz Divergence",
    "authors": [
      "Jiahao Zhang",
      "Wenzhe Yin",
      "Shujian Yu"
    ],
    "abstract": "Effective cross-modal retrieval requires robust alignment of heterogeneous data types. Most existing methods focus on bi-modal retrieval tasks and rely on distributional alignment techniques such as Kullback-Leibler divergence, Maximum Mean Discrepancy, and correlation alignment. However, these methods often suffer from critical limitations, including numerical instability, sensitivity to hyperparameters, and their inability to capture the full structure of the underlying distributions. In this paper, we introduce the Cauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves both training stability and retrieval performance. We further propose a novel Generalized CS (GCS) divergence inspired by Hölder's inequality. This extension enables direct alignment of three or more modalities within a unified mathematical framework through a bidirectional circular comparison scheme, eliminating the need for exhaustive pairwise comparisons. Extensive experiments on six benchmark datasets demonstrate the effectiveness of our method in both bi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is publicly available at https://github.com/JiahaoZhang666/CSD.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by ACMMM-25",
    "pdf_url": "https://arxiv.org/pdf/2509.21339v1",
    "published_date": "2025-09-15 08:55:15 UTC",
    "updated_date": "2025-09-15 08:55:15 UTC"
  },
  {
    "arxiv_id": "2509.11686v3",
    "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models",
    "authors": [
      "Jian Wang",
      "Xiaofei Xie",
      "Qiang Hu",
      "Shangqing Liu",
      "Yi Li"
    ],
    "abstract": "Code Large Language Models (Code LLMs) have opened a new era in programming with their impressive capabilities. However, recent research has revealed critical limitations in their ability to reason about runtime behavior and understand the actual functionality of programs, which poses significant challenges for their post-training and practical deployment. Specifically, Code LLMs encounter two principal issues: (1) a lack of proficiency in reasoning about program execution behavior, as they struggle to interpret what programs actually do during runtime, and (2) the inconsistent and fragmented representation of semantic information, such as execution traces, across existing methods, which hinders their ability to generalize and reason effectively. These challenges underscore the necessity for more systematic approaches to enhance the reasoning capabilities of Code LLMs. To address these issues, we introduce a generic framework to support integrating semantic information~(e.g., execution trace) to code task-relevant prompts, and conduct a comprehensive study to explore the role of semantic information in enhancing the reasoning ability of Code LLMs accordingly. Specifically, we focus on investigating the usefulness of trace-based semantic information in boosting supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The experimental results surprisingly disagree with previous works and demonstrate that semantic information has limited usefulness for SFT and test time scaling of Code LLM.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "EMNLP2025-findings https://openreview.net/forum?id=d4ICISW2T4",
    "pdf_url": "https://arxiv.org/pdf/2509.11686v3",
    "published_date": "2025-09-15 08:38:01 UTC",
    "updated_date": "2025-09-24 07:06:41 UTC"
  },
  {
    "arxiv_id": "2509.11663v1",
    "title": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering",
    "authors": [
      "Haisheng Wang",
      "Weiming Zhi"
    ],
    "abstract": "This paper formulates the Embodied Questions Answering (EQsA) problem, introduces a corresponding benchmark, and proposes a system to tackle the problem. Classical Embodied Question Answering (EQA) is typically formulated as answering one single question by actively exploring a 3D environment. Real deployments, however, often demand handling multiple questions that may arrive asynchronously and carry different urgencies. We formalize this setting as Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group memory module shared among questions to reduce redundant exploration, and a priority-planning module to dynamically schedule questions. To evaluate this setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs) benchmark containing 40 indoor scenes and five questions per scene (200 in total), featuring asynchronous follow-up questions and urgency labels. We further propose metrics for EQsA performance: Direct Answer Rate (DAR), and Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency and responsiveness of this system. ParaEQsA consistently outperforms strong sequential baselines adapted from recent EQA systems, while reducing exploration and delay. Empirical evaluations investigate the relative contributions of priority, urgency modeling, spatial scope, reward estimation, and dependency reasoning within our framework. Together, these results demonstrate that urgency-aware, parallel scheduling is key to making embodied agents responsive and efficient under realistic, multi-question workloads.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 6 figures, 2026 IEEE Conference on Robotics and Automation (ICRA 2026)",
    "pdf_url": "https://arxiv.org/pdf/2509.11663v1",
    "published_date": "2025-09-15 08:02:55 UTC",
    "updated_date": "2025-09-15 08:02:55 UTC"
  },
  {
    "arxiv_id": "2509.11662v3",
    "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
    "authors": [
      "Feilong Chen",
      "Yijiang Liu",
      "Yi Huang",
      "Hao Wang",
      "Miren Tian",
      "Ya-Qi Yu",
      "Minghui Liao",
      "Jihao Wu"
    ],
    "abstract": "We propose MindVL, a multimodal large language model (MLLMs) trained on Ascend NPUs. The training of state-of-the-art MLLMs is often confined to a limited set of hardware platforms and relies heavily on massive, undisclosed data recipes, which hinders reproducibility and open research. To change the common perception that Ascend hardware is unsuitable for efficient full-stage MLLM training, we introduce MindSpeed-MLLM, a highly efficient training framework that supports stable and high-performance training of large-scale Dense and Mixture-of-Experts (MoE) models on Ascend hardware. Based on this, we provide a systematic and open description of the data production methods and mixing strategies for all training stages. Furthermore, we present MindVL, a data-efficient multimodal large language model trained end-to-end on Ascend NPUs. In addition, we find that averaging weights from checkpoints trained with different sequence lengths is particularly effective and yields further gains when combined with test-time resolution search. Our experiments demonstrate superior data efficiency: MindVL-8B matches the performance of Qwen2.5VL-7B using only 10\\% of its training data, while our MoE model, MindVL-671B-A37B, matches Qwen2.5VL-72B using only 3\\% of the Qwen2.5VL training data, and achieves comparable performance with other leading multimodal MoE models. Our work provides the community with a valuable hardware alternative, open data recipes, and effective performance-enhancing techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11662v3",
    "published_date": "2025-09-15 08:00:31 UTC",
    "updated_date": "2025-09-30 02:27:18 UTC"
  },
  {
    "arxiv_id": "2509.11661v1",
    "title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition",
    "authors": [
      "Lifei Hao",
      "Yue Cheng",
      "Baoqi Huang",
      "Bing Jia",
      "Xuandong Zhao"
    ],
    "abstract": "Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11661v1",
    "published_date": "2025-09-15 07:59:34 UTC",
    "updated_date": "2025-09-15 07:59:34 UTC"
  },
  {
    "arxiv_id": "2509.11656v3",
    "title": "MALLM: Multi-Agent Large Language Models Framework",
    "authors": [
      "Jonas Becker",
      "Lars Benedikt Kaesberg",
      "Niklas Bauer",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at EMNLP 2025 (Demo)",
    "pdf_url": "https://arxiv.org/pdf/2509.11656v3",
    "published_date": "2025-09-15 07:48:02 UTC",
    "updated_date": "2025-12-15 16:45:40 UTC"
  },
  {
    "arxiv_id": "2509.11648v1",
    "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
    "authors": [
      "Sai Kartheek Reddy Kasu"
    ],
    "abstract": "The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11648v1",
    "published_date": "2025-09-15 07:35:35 UTC",
    "updated_date": "2025-09-15 07:35:35 UTC"
  },
  {
    "arxiv_id": "2509.11645v2",
    "title": "Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework",
    "authors": [
      "Zhaolong Wu",
      "Pu Luo",
      "Nan Meng",
      "Jason Pui Yin Cheung",
      "Teng Zhang"
    ],
    "abstract": "This study presents the first comprehensive evaluation of Multimodal Large Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS) self-management. We constructed a database of approximately 3,000 anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a `Divide and Conquer' framework consisting of a visual question-answering task, a domain knowledge assessment task, and a patient education counseling assessment task. Our investigation revealed limitations of MLLMs' ability in interpreting complex spinal radiographs and comprehending AIS care knowledge. To address these, we pioneered enhancing MLLMs with spinal keypoint prompting and compiled an AIS knowledge base for retrieval augmented generation (RAG), respectively. Results showed varying effectiveness of visual prompting across different architectures, while RAG substantially improved models' performances on the knowledge assessment task. Our findings indicate current MLLMs are far from capable in realizing personalized assistant in AIS care. The greatest challenge lies in their abilities to obtain accurate detections of spinal deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by MICCAI 2025 MLLMCP Workshop",
    "pdf_url": "https://arxiv.org/pdf/2509.11645v2",
    "published_date": "2025-09-15 07:34:12 UTC",
    "updated_date": "2025-10-13 09:02:07 UTC"
  },
  {
    "arxiv_id": "2509.12289v1",
    "title": "C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction",
    "authors": [
      "Yuting Liu",
      "Qiang Zhou",
      "Hanzhe Li",
      "Chenqi Gong",
      "Jingjing Gu"
    ],
    "abstract": "Long-term urban crowd flow prediction suffers significantly from cumulative sampling errors, due to increased sequence lengths and sampling intervals, which inspired us to leverage Neural Controlled Differential Equations (NCDEs) to mitigate this issue. However, regarding the crucial influence of Points of Interest (POIs) evolution on long-term crowd flow, the multi-timescale asynchronous dynamics between crowd flow and POI distribution, coupled with latent spurious causality, poses challenges to applying NCDEs for long-term urban crowd flow prediction. To this end, we propose Causal-aware Collaborative neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically, we introduce a dual-path NCDE as the backbone to effectively capture the asynchronous evolution of collaborative signals across multiple time scales. Then, we design a dynamic correction mechanism with the counterfactual-based causal effect estimator to quantify the causal impact of POIs on crowd flow and minimize the accumulation of spurious correlations. Finally, we leverage a predictor for long-term prediction with the fused collaborative signals of POI and crowd flow. Extensive experiments on three real-world datasets demonstrate the superior performance of C3DE, particularly in cities with notable flow fluctuations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.12289v1",
    "published_date": "2025-09-15 07:24:39 UTC",
    "updated_date": "2025-09-15 07:24:39 UTC"
  },
  {
    "arxiv_id": "2509.11636v1",
    "title": "Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications",
    "authors": [
      "Shiyao Jiang",
      "Jian Jiao",
      "Xingjian Zhang",
      "Ye Wang",
      "Dusit Niyato",
      "Qinyu Zhang"
    ],
    "abstract": "With the emergence of diverse and massive data in the upcoming sixth-generation (6G) networks, the task-agnostic semantic communication system is regarded to provide robust intelligent services. In this paper, we propose a task-agnostic learnable weighted-knowledge base semantic communication (TALSC) framework for robust image transmission to address the real-world heterogeneous data bias in KB, including label flipping noise and class imbalance. The TALSC framework incorporates a sample confidence module (SCM) as meta-learner and the semantic coding networks as learners. The learners are updated based on the empirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile, the meta-learner evaluates the significance of samples according to the task loss feedback, and adjusts the update strategy of learners to enhance the robustness in semantic recovery for unknown tasks. To strike a balance between SCM parameters and precision of significance evaluation, we design an SCM-grid extension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN) within SCM, which leverages the concept of spline refinement in KAN and enables scalable SCM with customizable granularity without retraining. Simulations demonstrate that the TALSC framework effectively mitigates the effects of flipping noise and class imbalance in task-agnostic image semantic communication, achieving at least 12% higher semantic recovery accuracy (SRA) and multi-scale structural similarity (MS-SSIM) compared to state-of-the-art methods.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11636v1",
    "published_date": "2025-09-15 07:10:21 UTC",
    "updated_date": "2025-09-15 07:10:21 UTC"
  },
  {
    "arxiv_id": "2509.11629v1",
    "title": "Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check",
    "authors": [
      "Chentao Cao",
      "Xiaojun Xu",
      "Bo Han",
      "Hang Li"
    ],
    "abstract": "As large language models (LLMs) continue to advance in capabilities, ensuring their safety against jailbreak attacks remains a critical challenge. In this paper, we introduce a novel safety alignment approach called Answer-Then-Check, which enhances LLM robustness against malicious prompts by applying thinking ability to mitigate jailbreaking problems before producing a final answer to the user. Our method enables models to directly answer the question in their thought and then critically evaluate its safety before deciding whether to provide it. To implement this approach, we construct the Reasoned Safety Alignment (ReSA) dataset, comprising 80K examples that teach models to reason through direct responses and then analyze their safety. Experimental results demonstrate that our approach achieves the Pareto frontier with superior safety capability while decreasing over-refusal rates on over-refusal benchmarks. Notably, the model fine-tuned with ReSA maintains general reasoning capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our method equips models with the ability to perform safe completion. Unlike post-hoc methods that can only reject harmful queries, our model can provide helpful and safe alternative responses for sensitive topics (e.g., self-harm). Furthermore, we discover that training on a small subset of just 500 examples can achieve comparable performance to using the full dataset, suggesting that safety alignment may require less data than previously assumed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11629v1",
    "published_date": "2025-09-15 06:47:35 UTC",
    "updated_date": "2025-09-15 06:47:35 UTC"
  },
  {
    "arxiv_id": "2509.19335v1",
    "title": "CSIYOLO: An Intelligent CSI-based Scatter Sensing Framework for Integrated Sensing and Communication Systems",
    "authors": [
      "Xudong Zhang",
      "Jingbo Tan",
      "Zhizhen Ren",
      "Jintao Wang",
      "Yihua Ma",
      "Jian Song"
    ],
    "abstract": "ISAC is regarded as a promising technology for next-generation communication systems, enabling simultaneous data transmission and target sensing. Among various tasks in ISAC, scatter sensing plays a crucial role in exploiting the full potential of ISAC and supporting applications such as autonomous driving and low-altitude economy. However, most existing methods rely on either waveform and hardware modifications or traditional signal processing schemes, leading to poor compatibility with current communication systems and limited sensing accuracy. To address these challenges, we propose CSIYOLO, a framework that performs scatter localization only using estimated CSI from a single base station-user equipment pair. This framework comprises two main components: anchor-based scatter parameter detection and CSI-based scatter localization. First, by formulating scatter parameter extraction as an image detection problem, we propose an anchor-based scatter parameter detection method inspired by You Only Look Once architectures. After that, a CSI-based localization algorithm is derived to determine scatter locations with extracted parameters. Moreover, to improve localization accuracy and implementation efficiency, we design an extendable network structure with task-oriented optimizations, enabling multi-scale anchor detection and better adaptation to CSI characteristics. A noise injection training strategy is further designed to enhance robustness against channel estimation errors. Since the proposed framework operates solely on estimated CSI without modifying waveforms or signal processing pipelines, it can be seamlessly integrated into existing communication systems as a plugin. Experiments show that our proposed method can significantly outperform existing methods in scatter localization accuracy with relatively low complexities under varying numbers of scatters and estimation errors.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "13 pages, 16 figures, 3 tables. This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2509.19335v1",
    "published_date": "2025-09-15 06:46:39 UTC",
    "updated_date": "2025-09-15 06:46:39 UTC"
  },
  {
    "arxiv_id": "2509.11628v1",
    "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching",
    "authors": [
      "Jiacheng Liu",
      "Chang Zou",
      "Yuanhuiyi Lyu",
      "Fei Ren",
      "Shaobo Wang",
      "Kaixin Li",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 9 figures, ACM Multimedia 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.11628v1",
    "published_date": "2025-09-15 06:46:22 UTC",
    "updated_date": "2025-09-15 06:46:22 UTC"
  },
  {
    "arxiv_id": "2509.11626v1",
    "title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools",
    "authors": [
      "Prerna Agarwal",
      "Himanshu Gupta",
      "Soujanya Soni",
      "Rohith Vallam",
      "Renuka Sindhgatta",
      "Sameep Mehta"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) has lead to the development of agents capable of complex reasoning and interaction with external tools. In enterprise contexts, the effective use of such tools that are often enabled by application programming interfaces (APIs), is hindered by poor documentation, complex input or output schema, and large number of operations. These challenges make tool selection difficult and reduce the accuracy of payload formation by up to 25%. We propose ACE, an automated tool creation and enrichment framework that transforms enterprise APIs into LLM-compatible tools. ACE, (i) generates enriched tool specifications with parameter descriptions and examples to improve selection and invocation accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters relevant tools at runtime, reducing prompt complexity while maintaining scalability. We validate our framework on both proprietary and open-source APIs and demonstrate its integration with agentic frameworks. To the best of our knowledge, ACE is the first end-to-end framework that automates the creation, enrichment, and dynamic selection of enterprise API tools for LLM agents.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11626v1",
    "published_date": "2025-09-15 06:41:54 UTC",
    "updated_date": "2025-09-15 06:41:54 UTC"
  },
  {
    "arxiv_id": "2509.11625v2",
    "title": "Inducing Uncertainty on Open-Weight Models for Test-Time Privacy in Image Recognition",
    "authors": [
      "Muhammad H. Ashiq",
      "Peter Triantafillou",
      "Hung Yun Tseng",
      "Grigoris G. Chrysos"
    ],
    "abstract": "A key concern for AI safety remains understudied in the machine learning (ML) literature: how can we ensure users of ML models do not leverage predictions on incorrect personal data to harm others? This is particularly pertinent given the rise of open-weight models, where simply masking model outputs does not suffice to prevent adversaries from recovering harmful predictions. To address this threat, which we call *test-time privacy*, we induce maximal uncertainty on protected instances while preserving accuracy on all other instances. Our proposed algorithm uses a Pareto optimal objective that explicitly balances test-time privacy against utility. We also provide a certifiable approximation algorithm which achieves $(\\varepsilon, δ)$ guarantees without convexity assumptions. We then prove a tight bound that characterizes the privacy-utility tradeoff that our algorithms incur. Empirically, our method obtains at least $>3\\times$ stronger uncertainty than pretraining with marginal drops in accuracy on various image recognition benchmarks. Altogether, this framework provides a tool to guarantee additional protection to end users.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11625v2",
    "published_date": "2025-09-15 06:38:57 UTC",
    "updated_date": "2025-09-29 21:48:46 UTC"
  },
  {
    "arxiv_id": "2509.12288v1",
    "title": "Digital Voices of Survival: From Social Media Disclosures to Support Provisions for Domestic Violence Victims",
    "authors": [
      "Kanlun Wang",
      "Zhe Fu",
      "Wangjiaxuan Xin",
      "Lina Zhou",
      "Shashi Kiran Chandrappa"
    ],
    "abstract": "Domestic Violence (DV) is a pervasive public health problem characterized by patterns of coercive and abusive behavior within intimate relationships. With the rise of social media as a key outlet for DV victims to disclose their experiences, online self-disclosure has emerged as a critical yet underexplored avenue for support-seeking. In addition, existing research lacks a comprehensive and nuanced understanding of DV self-disclosure, support provisions, and their connections. To address these gaps, this study proposes a novel computational framework for modeling DV support-seeking behavior alongside community support mechanisms. The framework consists of four key components: self-disclosure detection, post clustering, topic summarization, and support extraction and mapping. We implement and evaluate the framework with data collected from relevant social media communities. Our findings not only advance existing knowledge on DV self-disclosure and online support provisions but also enable victim-centered digital interventions.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.SI",
    "comment": "9 pages, 4 figures and 4 tables. Accepted to The 59th Hawaii International Conference on System Sciences (HICSS) 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.12288v1",
    "published_date": "2025-09-15 05:32:42 UTC",
    "updated_date": "2025-09-15 05:32:42 UTC"
  },
  {
    "arxiv_id": "2509.11601v1",
    "title": "Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification",
    "authors": [
      "Yuan Gao",
      "Xuelong Wang",
      "Zhenguo Dong",
      "Yong Zhang"
    ],
    "abstract": "Effective network state classification is a primary task for ensuring network security and optimizing performance. Existing deep learning models have shown considerable progress in this area. Some methods excel at analyzing the complex temporal periodicities found in traffic data, while graph-based approaches are adept at modeling the dynamic dependencies between different variables. However, a key trade-off remains, as these methods struggle to capture both characteristics simultaneously. Models focused on temporal patterns often overlook crucial variable dependencies, whereas those centered on dependencies may fail to capture fine-grained temporal details. To address this trade-off, we introduce DAPNet, a framework based on a Mixture-of-Experts architecture. DAPNet integrates three specialized networks for periodic analysis, dynamic cross-variable correlation modeling, and hybrid temporal feature extraction. A learnable gating network dynamically assigns weights to experts based on the input sample and computes a weighted fusion of their outputs. Furthermore, a hybrid regularization loss function ensures stable training and addresses the common issue of class imbalance. Extensive experiments on two large-scale network intrusion detection datasets (CICIDS2017/2018) validate DAPNet's higher accuracy for its target application. The generalizability of the architectural design is evaluated across ten public UEA benchmark datasets, positioning DAPNet as a specialized framework for network state classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11601v1",
    "published_date": "2025-09-15 05:32:32 UTC",
    "updated_date": "2025-09-15 05:32:32 UTC"
  },
  {
    "arxiv_id": "2509.11595v1",
    "title": "AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions",
    "authors": [
      "Sabin Huda",
      "Ernest Foo",
      "Zahra Jadidi",
      "MA Hakim Newton",
      "Abdul Sattar"
    ],
    "abstract": "Anti-money laundering (AML) research is constrained by the lack of publicly shareable, regulation-aligned transaction datasets. We present AMLNet, a knowledge-based multi-agent framework with two coordinated units: a regulation-aware transaction generator and an ensemble detection pipeline. The generator produces 1,090,173 synthetic transactions (approximately 0.16\\% laundering-positive) spanning core laundering phases (placement, layering, integration) and advanced typologies (e.g., structuring, adaptive threshold behavior). Regulatory alignment reaches 75\\% based on AUSTRAC rule coverage (Section 4.2), while a composite technical fidelity score of 0.75 summarizes temporal, structural, and behavioral realism components (Section 4.4). The detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the internal test partitions of AMLNet and adapts to the external SynthAML dataset, indicating architectural generalizability across different synthetic generation paradigms. We provide multi-dimensional evaluation (regulatory, temporal, network, behavioral) and release the dataset (Version 1.0, https://doi.org/10.5281/zenodo.16736515), to advance reproducible and regulation-conscious AML experimentation.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CR",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11595v1",
    "published_date": "2025-09-15 05:25:46 UTC",
    "updated_date": "2025-09-15 05:25:46 UTC"
  },
  {
    "arxiv_id": "2509.11594v2",
    "title": "GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning",
    "authors": [
      "Jizhuo Chen",
      "Diwen Liu",
      "Jiaming Wang",
      "Harold Soh"
    ],
    "abstract": "GBPP is a fast learning based scorer that selects a robot base pose for grasping from a single RGB-D snapshot. The method uses a two stage curriculum: (1) a simple distance-visibility rule auto-labels a large dataset at low cost; and (2) a smaller set of high fidelity simulation trials refines the model to match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP scores dense grids of candidate poses, enabling rapid online selection without full task-and-motion optimization. In simulation and on a real mobile manipulator, GBPP outperforms proximity and geometry only baselines, choosing safer and more reachable stances and degrading gracefully when wrong. The results offer a practical recipe for data efficient, geometry aware base placement: use inexpensive heuristics for coverage, then calibrate with targeted simulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper needs major revision",
    "pdf_url": "https://arxiv.org/pdf/2509.11594v2",
    "published_date": "2025-09-15 05:25:40 UTC",
    "updated_date": "2025-09-16 06:16:33 UTC"
  },
  {
    "arxiv_id": "2509.11587v1",
    "title": "Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification",
    "authors": [
      "Haonan Shi",
      "Yubin Wang",
      "De Cheng",
      "Lingfeng He",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "abstract": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to learn modality-invariant image features from unlabeled cross-modal person datasets by reducing the modality gap while minimizing reliance on costly manual annotations. Existing methods typically address USVI-ReID using cluster-based contrastive learning, which represents a person by a single cluster center. However, they primarily focus on the commonality of images within each cluster while neglecting the finer-grained differences among them. To address the limitation, we propose a Hierarchical Identity Learning (HIL) framework. Since each cluster may contain several smaller sub-clusters that reflect fine-grained variations among images, we generate multiple memories for each existing coarse-grained cluster via a secondary clustering. Additionally, we propose Multi-Center Contrastive Learning (MCCL) to refine representations for enhancing intra-modal clustering and minimizing cross-modal discrepancies. To further improve cross-modal matching quality, we design a Bidirectional Reverse Selection Transmission (BRST) mechanism, which establishes reliable cross-modal correspondences by performing bidirectional matching of pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets demonstrate that the proposed method outperforms existing approaches. The source code is available at: https://github.com/haonanshi0125/HIL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11587v1",
    "published_date": "2025-09-15 05:10:43 UTC",
    "updated_date": "2025-09-15 05:10:43 UTC"
  },
  {
    "arxiv_id": "2509.11575v2",
    "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models",
    "authors": [
      "Ching Chang",
      "Yidan Shi",
      "Defu Cao",
      "Wei Yang",
      "Jeehyun Hwang",
      "Haixin Wang",
      "Jiacheng Pang",
      "Wei Wang",
      "Yan Liu",
      "Wen-Chih Peng",
      "Tien-Fu Chen"
    ],
    "abstract": "Time series reasoning treats time as a first-class axis and incorporates intermediate evidence directly into the answer. This survey defines the problem and organizes the literature by reasoning topology with three families: direct reasoning in one step, linear chain reasoning with explicit intermediates, and branch-structured reasoning that explores, revises, and aggregates. The topology is crossed with the main objectives of the field, including traditional time series analysis, explanation and understanding, causal inference and decision making, and time series generation, while a compact tag set spans these axes and captures decomposition and verification, ensembling, tool use, knowledge access, multimodality, agent loops, and LLM alignment regimes. Methods and systems are reviewed across domains, showing what each topology enables and where it breaks down in faithfulness or robustness, along with curated datasets, benchmarks, and resources that support study and deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey). Evaluation practices that keep evidence visible and temporally aligned are highlighted, and guidance is distilled on matching topology to uncertainty, grounding with observable artifacts, planning for shift and streaming, and treating cost and latency as design budgets. We emphasize that reasoning structures must balance capacity for grounding and self-correction against computational cost and reproducibility, while future progress will likely depend on benchmarks that tie reasoning quality to utility and on closed-loop testbeds that trade off cost and risk under shift-aware, streaming, and long-horizon settings. Taken together, these directions mark a shift from narrow accuracy toward reliability at scale, enabling systems that not only analyze but also understand, explain, and act on dynamic worlds with traceable evidence and credible outcomes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper is currently under review",
    "pdf_url": "https://arxiv.org/pdf/2509.11575v2",
    "published_date": "2025-09-15 04:39:50 UTC",
    "updated_date": "2025-11-02 22:14:09 UTC"
  },
  {
    "arxiv_id": "2509.11572v1",
    "title": "Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain",
    "authors": [
      "Tuan Bui",
      "An Nguyen",
      "Phat Thai",
      "Minh Hua",
      "Ngan Pham L. N.",
      "Ngan Pham T. B.",
      "Dung Le",
      "Long Nguyen",
      "Thanh-Tung Tran",
      "Thang Bui",
      "Tho Quan"
    ],
    "abstract": "Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful - serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions.\n  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at the 2nd ACM Workshop in AI-powered Question & Answering Systems (AIQAM '25), co-located with ACM Multimedia 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.11572v1",
    "published_date": "2025-09-15 04:34:42 UTC",
    "updated_date": "2025-09-15 04:34:42 UTC"
  },
  {
    "arxiv_id": "2509.14267v1",
    "title": "Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support",
    "authors": [
      "Piyushkumar Patel"
    ],
    "abstract": "E-Commerce customer support requires quick and accurate answers grounded in product data and past support cases. This paper develops a novel retrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs) to improve the relevance of the answer and the factual grounding. We examine recent advances in knowledge-augmented RAG and chatbots based on large language models (LLM) in customer support, including Microsoft's GraphRAG and hybrid retrieval architectures. We then propose a new answer synthesis algorithm that combines structured subgraphs from a domain-specific KG with text documents retrieved from support archives, producing more coherent and grounded responses. We detail the architecture and knowledge flow of our system, provide comprehensive experimental evaluation, and justify its design in real-time support settings. Our implementation demonstrates 23\\% improvement in factual accuracy and 89\\% user satisfaction in e-Commerce QA scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.14267v1",
    "published_date": "2025-09-15 04:17:42 UTC",
    "updated_date": "2025-09-15 04:17:42 UTC"
  },
  {
    "arxiv_id": "2509.11555v1",
    "title": "Dstack: A Zero Trust Framework for Confidential Containers",
    "authors": [
      "Shunfan Zhou",
      "Kevin Wang",
      "Hang Yin"
    ],
    "abstract": "Web3 applications require execution platforms that maintain confidentiality and integrity without relying on centralized trust authorities. While Trusted Execution Environments (TEEs) offer promising capabilities for confidential computing, current implementations face significant limitations when applied to Web3 contexts, particularly in security reliability, censorship resistance, and vendor independence.\n  This paper presents dstack, a comprehensive framework that transforms raw TEE technology into a true Zero Trust platform. We introduce three key innovations: (1) Portable Confidential Containers that enable seamless workload migration across heterogeneous TEE environments while maintaining security guarantees, (2) Decentralized Code Management that leverages smart contracts for transparent governance of TEE applications, and (3) Verifiable Domain Management that ensures secure and verifiable application identity without centralized authorities.\n  These innovations are implemented through three core components: dstack-OS, dstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both the performance advantages of VM-level TEE solutions and the trustless guarantees required by Web3 applications. Our evaluation shows that dstack provides comprehensive security guarantees while maintaining practical usability for real-world applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11555v1",
    "published_date": "2025-09-15 03:36:27 UTC",
    "updated_date": "2025-09-15 03:36:27 UTC"
  },
  {
    "arxiv_id": "2509.11552v3",
    "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking",
    "authors": [
      "Wensheng Lu",
      "Keyu Chen",
      "Ruizhi Qiao",
      "Xing Sun"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of language models by integrating external knowledge sources. However, document chunking as an important part of RAG system often lacks effective evaluation tools. This paper first analyzes why existing RAG evaluation benchmarks are inadequate for assessing document chunking quality, specifically due to evidence sparsity. Based on this conclusion, we propose HiCBench, which includes manually annotated multi-level document chunking points, synthesized evidence-dense quetion answer(QA) pairs, and their corresponding evidence sources. Additionally, we introduce the HiChunk framework, a multi-level document structuring framework based on fine-tuned LLMs, combined with the Auto-Merge retrieval algorithm to improve retrieval quality. Experiments demonstrate that HiCBench effectively evaluates the impact of different chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves better chunking quality within reasonable time consumption, thereby enhancing the overall performance of RAG systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 5 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.11552v3",
    "published_date": "2025-09-15 03:32:50 UTC",
    "updated_date": "2025-10-09 14:21:50 UTC"
  },
  {
    "arxiv_id": "2509.11547v1",
    "title": "Task Decoding based on Eye Movements using Synthetic Data Augmentation",
    "authors": [
      "Shanmuka Sadhu",
      "Arca Baran",
      "Preeti Pandey",
      "Ayush Kumar"
    ],
    "abstract": "Machine learning has been extensively used in various applications related to eye-tracking research. Understanding eye movement is one of the most significant subsets of eye-tracking research that reveals the scanning pattern of an individual. Researchers have thoroughly analyzed eye movement data to understand various eye-tracking applications, such as attention mechanisms, navigational behavior, task understanding, etc. The outcome of traditional machine learning algorithms used for decoding tasks based on eye movement data has received a mixed reaction to Yarbus' claim that it is possible to decode the observer's task from their eye movements. In this paper, to support the hypothesis by Yarbus, we are decoding tasks categories while generating synthetic data samples using well-known Synthetic Data Generators CTGAN and its variations such as CopulaGAN and Gretel AI Synthetic Data generators on available data from an in-person user study. Our results show that augmenting more eye movement data combined with additional synthetically generated improves classification accuracy even with traditional machine learning algorithms. We see a significant improvement in task decoding accuracy from 28.1% using Random Forest to 82% using Inception Time when five times more data is added in addition to the 320 real eye movement dataset sample. Our proposed framework outperforms all the available studies on this dataset because of the use of additional synthetic datasets. We validated our claim with various algorithms and combinations of real and synthetic data to show how decoding accuracy increases with the increase in the augmentation of generated data to real data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11547v1",
    "published_date": "2025-09-15 03:28:21 UTC",
    "updated_date": "2025-09-15 03:28:21 UTC"
  },
  {
    "arxiv_id": "2509.11543v2",
    "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
    "authors": [
      "Zhengxi Lu",
      "Jiabo Ye",
      "Fei Tang",
      "Yongliang Shen",
      "Haiyang Xu",
      "Ziwei Zheng",
      "Weiming Lu",
      "Ming Yan",
      "Fei Huang",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.11543v2",
    "published_date": "2025-09-15 03:24:08 UTC",
    "updated_date": "2025-09-24 15:05:34 UTC"
  },
  {
    "arxiv_id": "2509.11536v2",
    "title": "HARP: Hallucination Detection via Reasoning Subspace Projection",
    "authors": [
      "Junjie Hu",
      "Gang Tu",
      "ShengYu Cheng",
      "Jinxin Li",
      "Jinting Wang",
      "Rui Chen",
      "Zhilong Zhou",
      "Dongbo Shan"
    ],
    "abstract": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11536v2",
    "published_date": "2025-09-15 03:02:33 UTC",
    "updated_date": "2025-12-05 07:28:13 UTC"
  },
  {
    "arxiv_id": "2509.11520v1",
    "title": "Know What You Don't Know: Selective Prediction for Early Exit DNNs",
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Kumar Hanawal"
    ],
    "abstract": "Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the bottlenecks in deploying them in critical applications like sensitive tasks. Early Exit (EE) DNNs overcome the latency issues by allowing samples to exit from intermediary layers if they attain `high' confidence scores on the predicted class. However, the DNNs are known to exhibit overconfidence, which can lead to many samples exiting early and render EE strategies untrustworthy. We use Selective Prediction (SP) to overcome this issue by checking the `hardness' of the samples rather than just relying on the confidence score alone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs) at each layer to check the hardness of samples before performing EEs. Specifically, the DCs identify if a sample is hard to predict at an intermediary layer, leading to hallucination, and defer it to an expert. Early detection of hard samples for inference prevents the wastage of computational resources and improves trust by deferring the hard samples to the expert. We demonstrate that EE aided with SP improves both accuracy and latency. Our method minimizes the risk of wrong prediction by $50\\%$ with a speedup of $2.05\\times$ as compared to the final layer. The anonymized source code is available at https://github.com/Div290/SPEED",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the the Fifth International Conference on AI ML Systems",
    "pdf_url": "https://arxiv.org/pdf/2509.11520v1",
    "published_date": "2025-09-15 02:19:09 UTC",
    "updated_date": "2025-09-15 02:19:09 UTC"
  },
  {
    "arxiv_id": "2509.19333v1",
    "title": "Pluralistic Off-policy Evaluation and Alignment",
    "authors": [
      "Chengkai Huang",
      "Junda Wu",
      "Zhouhang Xie",
      "Yu Xia",
      "Rui Wang",
      "Tong Yu",
      "Subrata Mitra",
      "Julian McAuley",
      "Lina Yao"
    ],
    "abstract": "Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evaluated LLMs, and existing off-policy estimators focus solely on overall utility while ignoring preference pluralism. Extending Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore, remains an open question. Thus, we propose the Pluralistic Off-Policy Evaluation (POPE), the first framework for offline pluralistic preference evaluation and alignment in LLMs. POPE includes a unified reward function that combines (1) a collaborative utility component derived from human preference signals (e.g., upvotes or relevance scores) and (2) a diversity component inspired by entropy-based coverage measures, together reflecting pluralistic alignment. Furthermore, to estimate this reward from logged interactions, we derive decomposable inverse propensity scoring (IPS) estimators that separately evaluate relevance and diversity. Theoretically, we prove that our decomposed IPS estimators establish a lower bound on their variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance pluralistic alignment. Empirical results demonstrate that POPE efficiently enhances pluralistic response generation and maintains the models' general capabilities on downstream tasks",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19333v1",
    "published_date": "2025-09-15 01:57:49 UTC",
    "updated_date": "2025-09-15 01:57:49 UTC"
  },
  {
    "arxiv_id": "2509.11513v1",
    "title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics",
    "authors": [
      "Zhongyang Hu",
      "Naijie Gu",
      "Xiangzhi Tao",
      "Tianhui Gu",
      "Yibing Zhou"
    ],
    "abstract": "A key subtask in lexical substitution is ranking the given candidate words. A common approach is to replace the target word with a candidate in the original sentence and feed the modified sentence into a model to capture semantic differences before and after substitution. However, effectively modeling the bidirectional influence of candidate substitution on both the target word and its context remains challenging. Existing methods often focus solely on semantic changes at the target position or rely on parameter tuning over multiple evaluation metrics, making it difficult to accurately characterize semantic variation. To address this, we investigate two approaches: one based on attention weights and another leveraging the more interpretable integrated gradients method, both designed to measure the influence of context tokens on the target token and to rank candidates by incorporating semantic similarity between the original and substituted sentences. Experiments on the LS07 and SWORDS datasets demonstrate that both approaches improve ranking performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11513v1",
    "published_date": "2025-09-15 01:57:09 UTC",
    "updated_date": "2025-09-15 01:57:09 UTC"
  },
  {
    "arxiv_id": "2509.11512v2",
    "title": "Machine Learning-Driven Predictive Resource Management in Complex Science Workflows",
    "authors": [
      "Tasnuva Chowdhury",
      "Tadashi Maeno",
      "Fatih Furkan Akman",
      "Joseph Boudreau",
      "Sankha Dutta",
      "Shengyu Feng",
      "Adolfy Hoisie",
      "Kuan-Chieh Hsu",
      "Raees Khan",
      "Jaehyung Kim",
      "Ozgur O. Kilic",
      "Scott Klasky",
      "Alexei Klimentov",
      "Tatiana Korchuganova",
      "Verena Ingrid Martinez Outschoorn",
      "Paul Nilsson",
      "David K. Park",
      "Norbert Podhorszki",
      "Yihui Ren",
      "John Rembrandt Steele",
      "Frédéric Suter",
      "Sairam Sri Vatsavai",
      "Torre Wenaus",
      "Wei Yang",
      "Yiming Yang",
      "Shinjae Yoo"
    ],
    "abstract": "The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11512v2",
    "published_date": "2025-09-15 01:53:30 UTC",
    "updated_date": "2025-12-19 17:47:28 UTC"
  },
  {
    "arxiv_id": "2509.11507v1",
    "title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare",
    "authors": [
      "Jared Zhu",
      "Junde Wu"
    ],
    "abstract": "Decades' advances in digital health technologies, such as electronic health records, have largely streamlined routine clinical processes. Yet, most these systems are still hard to learn and use: Clinicians often face the burden of managing multiple tools, repeating manual actions for each patient, navigating complicated UI trees to locate functions, and spending significant time on administration instead of caring for patients. The recent rise of large language model (LLM) based agents demonstrates exceptional capability in coding and computer operation, revealing the potential for humans to interact with operating systems and software not by direct manipulation, but by instructing agents through natural language. This shift highlights the need for an abstraction layer, an agent-computer interface, that translates human language into machine-executable commands. In digital healthcare, however, requires a more domain-specific abstractions that strictly follow trusted clinical guidelines and procedural standards to ensure safety, transparency, and compliance. To address this need, we present \\textbf{MedicalOS}, a unified agent-based operational system designed as such a domain-specific abstract layer for healthcare. It translates human instructions into pre-defined digital healthcare commands, such as patient inquiry, history retrieval, exam management, report generation, referrals, treatment planning, that we wrapped as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP, Linux). We empirically validate MedicalOS on 214 patient cases across 22 specialties, demonstrating high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations. These results highlight MedicalOS as a trustworthy and scalable foundation for advancing workflow automation in clinical practice.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11507v1",
    "published_date": "2025-09-15 01:43:20 UTC",
    "updated_date": "2025-09-15 01:43:20 UTC"
  },
  {
    "arxiv_id": "2509.11492v1",
    "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims",
    "authors": [
      "Anirban Saha Anik",
      "Md Fahimul Kabir Chowdhury",
      "Andrew Wyckoff",
      "Sagnik Ray Choudhury"
    ],
    "abstract": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab, which focuses on verifying numerical and temporal claims using retrieved evidence. We explore two complementary approaches: zero-shot prompting with instruction-tuned large language models (LLMs) and supervised fine-tuning using parameter-efficient LoRA. To enhance evidence quality, we investigate several selection strategies, including full-document input and top-k sentence filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned with LoRA achieves strong performance on the English validation set. However, a notable drop in the test set highlights a generalization challenge. These findings underscore the importance of evidence granularity and model adaptation for robust numerical fact verification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Notebook for the CheckThat! Lab at CLEF 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.11492v1",
    "published_date": "2025-09-15 01:03:09 UTC",
    "updated_date": "2025-09-15 01:03:09 UTC"
  },
  {
    "arxiv_id": "2509.18141v1",
    "title": "KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots",
    "authors": [
      "Yao Zhao",
      "Haoyue Sun",
      "Yantian Ding",
      "Yanxun Xu"
    ],
    "abstract": "Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots provides valuable insights for evidence synthesis in clinical research. However, existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD without manual input or intervention. Its hybrid reasoning architecture automates the conversion of unstructured information into structured data flows and validates data extraction from complex KM plots. To improve accessibility, KM-GPT is equipped with a user-friendly web interface and an integrated AI assistant, enabling researchers to reconstruct IPD without requiring programming expertise. KM-GPT was rigorously evaluated on synthetic and real-world datasets, consistently demonstrating superior accuracy. To illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and biomarker-based subgroup analyses. By automating traditionally manual processes and providing a scalable, web-based solution, KM-GPT transforms clinical research by leveraging reconstructed IPD to enable more informed downstream analyses, supporting evidence-based decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.18141v1",
    "published_date": "2025-09-15 00:38:38 UTC",
    "updated_date": "2025-09-15 00:38:38 UTC"
  },
  {
    "arxiv_id": "2509.11481v1",
    "title": "RAPTOR: A Foundation Policy for Quadrotor Control",
    "authors": [
      "Jonas Eschmann",
      "Dario Albani",
      "Giuseppe Loianno"
    ],
    "abstract": "Humans are remarkably data-efficient when adapting to new unseen conditions, like driving a new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, a method for training a highly adaptive foundation policy for quadrotor control. Our method enables training a single, end-to-end neural-network policy to control a wide variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to a wide variety of platforms. The adaptation through In-Context Learning is made possible by using a recurrence in the hidden layer. The policy is trained through a novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train a teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the capabilities of the foundation policy under numerous conditions (trajectory tracking, indoor/outdoor, wind disturbance, poking, different propellers).",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.11481v1",
    "published_date": "2025-09-15 00:05:40 UTC",
    "updated_date": "2025-09-15 00:05:40 UTC"
  },
  {
    "arxiv_id": "2509.11480v1",
    "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
    "authors": [
      "Amir Taherin",
      "Juyi Lin",
      "Arash Akbari",
      "Arman Akbari",
      "Pu Zhao",
      "Weiwei Chen",
      "David Kaeli",
      "Yanzhi Wang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in the Asilomar Conference on Signals, Systems, and Computers 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.11480v1",
    "published_date": "2025-09-15 00:00:37 UTC",
    "updated_date": "2025-09-15 00:00:37 UTC"
  }
]
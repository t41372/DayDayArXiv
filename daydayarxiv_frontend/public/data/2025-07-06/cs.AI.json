{
  "date": "2025-07-06",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-06 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nä»Šå¤© arXiv çš„æ›´æ–°å¯è°“æ˜¯â€œè„‘æ´å¤§å¼€â€ä¸â€œç¡¬æ ¸åæ€â€å¹¶å­˜ã€‚**ä¸€å¥è¯æ€»ç»“ï¼šæˆ‘ä»¬çœ‹åˆ°äº†ä»ç”Ÿç‰©å­¦æ±²å–çµæ„Ÿçš„è„‘çº§æ¨¡å—åŒ–æ¶æ„ï¼ˆå¦‚åŒ–å­¦ä¿¡å·ä¼ å¯¼å’Œçš®å±‚æŸ±æ¨¡å‹ï¼‰ï¼Œå¯¹ LoRA æ•ˆç‡å’Œ LLM é—å¿˜æœºåˆ¶çš„æ·±åˆ»åæ€ï¼Œä»¥åŠå¯¹ Jailbreakï¼ˆè¶Šç‹±ï¼‰æ”»å‡»èƒŒåæ³¨æ„åŠ›æœºåˆ¶çš„æœºç†è§£æã€‚**\n\n---\n\n### ğŸ§  è„‘ç§‘å­¦å¯å‘ä¸æ–°æ¶æ„\nä»Šå¤©çš„é‡å¤´æˆæ˜¯ä¸¤ç¯‡å°è¯•å°†ç”Ÿç‰©å¤§è„‘æœºåˆ¶å¼•å…¥ AI æ¶æ„çš„æ–‡ç« ï¼Œè¯•å›¾çªç ´ç°æœ‰ Transformer çš„å±€é™ã€‚\n\n**1. Lilith: å¸¦æœ‰åŒ–å­¦ä¿¡å·ä¼ å¯¼çš„å‘è‚²æ¨¡å—åŒ– LLM**\n**(Lilith: Developmental Modular LLMs with Chemical Signaling)**\nè¿™ç¯‡ position paper éå¸¸æœ‰è¶£ã€‚ä½œè€…è®¤ä¸ºå½“å‰çš„ AI ä»…ä»…æ¨¡æ‹Ÿäº†ç¥ç»å…ƒå±‚é¢çš„æ´»åŠ¨ï¼Œè€Œå¿½è§†äº†å¤§è„‘åŒºåŸŸé—´çš„â€œåŒ–å­¦ä¿¡å·â€ä¼ å¯¼ã€‚\n- **æ ¸å¿ƒæ¦‚å¿µ**ï¼šæå‡ºäº† **LILITH** æ¶æ„ï¼Œå°†ä¸åŒçš„ LLM æ¨¡å—è§†ä¸ºå¤§è„‘çš„ç‰¹å®šåŒºåŸŸï¼ˆå¦‚æ€è€ƒã€è®°å¿†ã€æ„ŸçŸ¥ï¼‰ï¼Œå®ƒä»¬ä¹‹é—´ä¸é€šè¿‡ä¼ ç»Ÿçš„æ¢¯åº¦è¿æ¥ï¼Œè€Œæ˜¯é€šè¿‡ç±»ä¼¼ç¥ç»é€’è´¨çš„ emergent token-based protocolsï¼ˆæ¶Œç°çš„åŸºäº Token çš„åè®®ï¼‰è¿›è¡Œé€šä¿¡ã€‚\n- **è®­ç»ƒæ–¹å¼**ï¼šé‡‡ç”¨ç±»ä¼¼ç”Ÿç‰©å‘è‚²çš„è®­ç»ƒï¼ˆDevelopmental Trainingï¼‰ï¼Œè®©æœªè®­ç»ƒçš„æ¶æ„åœ¨ç¯å¢ƒäº¤äº’ä¸­â€œè¿›åŒ–â€å‡ºé€šä¿¡è·¯å¾„ã€‚è¿™å¯èƒ½ä¸ºç ”ç©¶æ„è¯†æ¶Œç°ï¼ˆConsciousness Emergenceï¼‰æä¾›æ–°çš„è§†è§’ã€‚\n\n**2. åƒè„‘ç³»ç»Ÿï¼šç”¨äºå¿«é€Ÿã€é²æ£’å­¦ä¹ å’Œæ¨ç†çš„æ„Ÿè§‰è¿åŠ¨æ™ºèƒ½**\n**(Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference)**\nè¿™æ˜¯ **Jeff Hawkins (Numenta)** å›¢é˜Ÿçš„æ–°ä½œã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šåŸºäºâ€œåƒè„‘ç†è®ºâ€ï¼ˆThousand-Brains Theoryï¼‰ï¼Œä»–ä»¬å®ç°äº†é¦–ä¸ªç³»ç»Ÿ **Monty**ã€‚è¯¥ç³»ç»Ÿæ¨¡ä»¿çš®å±‚æŸ±ï¼ˆcortical columnsï¼‰çš„æ¶æ„ï¼Œä¸“æ³¨äº 3D ç‰©ä½“æ„ŸçŸ¥ã€‚\n- **å‘ç°**ï¼šMonty åˆ©ç”¨æ„Ÿè§‰è¿åŠ¨å­¦ä¹ ï¼ˆsensorimotor learningï¼‰å»ºç«‹ç»“æ„åŒ–è¡¨å¾ï¼Œèƒ½å¤Ÿè¿›è¡Œå¿«é€Ÿæ¨ç†ï¼ˆé€šè¿‡ä¸€ç§æ–°é¢–çš„æŠ•ç¥¨ç®—æ³•åŠ é€Ÿï¼‰ï¼Œå¹¶ä¸”å±•ç°å‡ºç±»ä¼¼èµ«å¸ƒå­¦ä¹ ï¼ˆHebbian-likeï¼‰çš„å¿«é€Ÿã€æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚è¿™æ˜¯å¯¹ç°æœ‰æ·±åº¦å­¦ä¹ æ¶æ„çš„ä¸€ç§æœ‰åŠ›è¡¥å……ã€‚\n\n---\n\n### ğŸ“‰ è®­ç»ƒåŠ¨åŠ›å­¦ä¸æ•ˆç‡åæ€\nå¤§å®¶éƒ½åœ¨ç”¨ LoRAï¼Œä½†å®ƒçœŸçš„å¿«å—ï¼Ÿæˆ‘ä»¬å¦‚ä½•è®©æ¨¡å‹â€œé—å¿˜â€éšç§æ•°æ®ï¼Ÿ\n\n**3. LoRA æ¯”ä½ æƒ³è±¡çš„è¦æ…¢**\n**(LoRA Is Slower Than You Think)**\n- **æ ¸å¿ƒå‘ç°**ï¼šè™½ç„¶ **LoRA** (Low-Rank Adaptation) å‡å°‘äº†æ˜¾å­˜å ç”¨å’Œå‚æ•°é‡ï¼Œä½†ä½œè€…é€šè¿‡å®æµ‹å‘ç°ï¼Œå®ƒåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šå¹¶ä¸æ€»æ˜¯å ä¼˜ï¼Œç”šè‡³åœ¨æŸäº›æ¶æ„ä¸‹æ›´æ…¢ã€‚\n- **æ”¹è¿›**ï¼šä½œè€…æ·±å…¥åˆ†æäº†é™åˆ¶ LoRA é€Ÿåº¦çš„å› ç´ ï¼Œå¹¶æå‡ºäº†ä¸€å¥—ä¼˜åŒ–æŒ‡å—ï¼Œæ—¨åœ¨è®© LoRA åœ¨èŠ‚çœèµ„æºçš„åŒæ—¶çœŸæ­£æå‡è®­ç»ƒé€Ÿåº¦ã€‚è¿™å¯¹å·¥ç¨‹å®è·µéå¸¸æœ‰å‚è€ƒä»·å€¼ã€‚\n\n**4. æ¨¡å‹å´©æºƒå¹¶é Bugï¼Œè€Œæ˜¯æœºå™¨é—å¿˜çš„ Feature**\n**(Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs)**\n- **æ ¸å¿ƒè§‚ç‚¹**ï¼šé€šå¸¸æˆ‘ä»¬è®¤ä¸º Model Collapseï¼ˆæ¨¡å‹å´©æºƒï¼Œå³ç”¨ç”Ÿæˆæ•°æ®è®­ç»ƒå¯¼è‡´åˆ†å¸ƒé€€åŒ–ï¼‰æ˜¯ä¸ªå¤§é—®é¢˜ã€‚ä½†è¿™ç¯‡è®ºæ–‡åå…¶é“è€Œè¡Œä¹‹ï¼Œåˆ©ç”¨**éƒ¨åˆ†æ¨¡å‹å´©æºƒ (Partial Model Collapse, PMC)** æ¥å®ç°æœºå™¨é—å¿˜ï¼ˆMachine Unlearningï¼‰ã€‚\n- **æ–¹æ³•**ï¼šä¸å…¶è´¹åŠ›åœ°å»é™¤éæ³•æ•°æ®ï¼Œä¸å¦‚æ•…æ„åœ¨ç‰¹å®šç›®æ ‡ä¸Šè§¦å‘æ¨¡å‹å´©æºƒï¼Œä»è€Œæœ‰æ•ˆåœ°ä»æ¨¡å‹è¾“å‡ºä¸­æŠ¹é™¤æ•æ„Ÿä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚\n\n**5. LLM è®­ç»ƒåŠ¨åŠ›å­¦ä¸­çš„å§†æ½˜å·´æ•ˆåº”ï¼šå±±è°·-æ²³æµæ¨¡å‹çš„æç®€åˆ†æ**\n**(Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model)**\n- **ç†è®ºè¶£å‘³**ï¼šå§†æ½˜å·´æ•ˆåº”ï¼ˆçƒ­æ°´æ¯”å†·æ°´ç»“å†°å¿«ï¼‰åœ¨ç‰©ç†å­¦ä¸­å¾ˆå‡ºåã€‚ä½œè€…å°†å…¶æ˜ å°„åˆ° LLM çš„è®­ç»ƒä¸­ï¼Œç”¨æ¥è§£é‡Šä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ **Warm-up** å’Œç‰¹å®šçš„ Learning Rate Scheduleã€‚\n- **ç»“è®º**ï¼šåœ¨â€œå±±è°·-æ²³æµâ€æŸå¤±æ™¯è§‚ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªâ€œå¼ºå§†æ½˜å·´ç‚¹â€ï¼Œåœ¨è¯¥ç‚¹è®¾ç½®è¾ƒé«˜çš„ Plateau å­¦ä¹ ç‡å¯ä»¥æ¶ˆé™¤æœ€æ…¢çš„æ¨¡å¼ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›ã€‚è¿™ä¸ºè°ƒå‚æä¾›äº†ç‰©ç†å­¦è§†è§’çš„ç†è®ºæ”¯æŒã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸å¯¹é½ (Safety & Alignment)\nJailbreakï¼ˆè¶Šç‹±ï¼‰ä¾ç„¶æ˜¯çƒ­é—¨è¯é¢˜ï¼Œä»Šå¤©çš„è®ºæ–‡æ·±å…¥åˆ°äº†æ³¨æ„åŠ›æœºåˆ¶çš„åº•å±‚ã€‚\n\n**6. æ³¨æ„åŠ›æ»‘è½ï¼šLLM è¶Šç‹±æ”»å‡»ä¸é˜²å¾¡çš„æœºåˆ¶æ€§ç†è§£**\n**(Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs)**\n- **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…æ­ç¤ºäº†ä¸€ä¸ªæ™®éç°è±¡â€”â€”**Attention Slippingï¼ˆæ³¨æ„åŠ›æ»‘è½ï¼‰**ã€‚åœ¨è¶Šç‹±æ”»å‡»è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šé€æ¸å‡å°‘å¯¹ç”¨æˆ·æŸ¥è¯¢ä¸­â€œä¸å®‰å…¨è¯·æ±‚â€éƒ¨åˆ†çš„æ³¨æ„åŠ›åˆ†é…ï¼Œå¯¼è‡´é˜²å¾¡æœºåˆ¶å¤±æ•ˆã€‚\n- **é˜²å¾¡**ï¼šåŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº† **Attention Sharpening**ï¼Œé€šè¿‡æ¸©åº¦ç¼©æ”¾ï¼ˆtemperature scalingï¼‰é”åŒ–æ³¨æ„åŠ›åˆ†å¸ƒï¼Œå¼ºè¡ŒæŠŠæ¨¡å‹çš„æ³¨æ„åŠ›â€œæ‹‰å›â€åˆ°ä¸å®‰å…¨çš„å†…å®¹ä¸Šï¼Œä»è€Œæœ‰æ•ˆé˜²å¾¡æ”»å‡»ã€‚\n\n**7. åªæ˜¯è¶³å¤Ÿçš„åç§»ï¼šé€šè¿‡é’ˆå¯¹æ€§è¡¨å¾å¾®è°ƒç¼“è§£å·²å¯¹é½è¯­è¨€æ¨¡å‹çš„è¿‡åº¦æ‹’ç»**\n**(Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning)**\n- **é—®é¢˜**ï¼šå®‰å…¨å¯¹é½çš„æ¨¡å‹å¾€å¾€å˜å¾—èƒ†å°ï¼Œè¿æ— å®³çš„è¯·æ±‚ä¹Ÿæ‹’ç»ï¼ˆOver-refusalï¼‰ã€‚\n- **æ–¹æ³•**ï¼šæå‡ºäº† **ACTOR** æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«è§¦å‘æ‹’ç»çš„å†…éƒ¨æ¿€æ´»ç»„ä»¶ï¼Œä»…å¾®è°ƒå•ä¸ªæ¨¡å‹å±‚ï¼Œç²¾ç¡®åœ°å‡å°‘è¿‡åº¦æ‹’ç»ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å®‰å…¨æ€§ã€‚\n\n**8. é‡å¤–å¯¹è¯çš„å¤§è§„æ¨¡åˆ†ææ­ç¤ºäº† LLM è¶Šç‹±çš„å¤æ‚æ€§ç•Œé™**\n**(Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking)**\n- **æ•°æ®**ï¼šåˆ†æäº† 200 ä¸‡æ¡çœŸå®å¯¹è¯ã€‚\n- **ç»“è®º**ï¼šè¶Šç‹±æ”»å‡»å¹¶æ²¡æœ‰å¤§å®¶æƒ³è±¡çš„é‚£ä¹ˆâ€œé«˜æ·±è«æµ‹â€ã€‚çœŸå®ä¸–ç•Œä¸­çš„è¶Šç‹±å°è¯•åœ¨å¤æ‚æ€§ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—é«˜äºæ­£å¸¸å¯¹è¯ã€‚ä½œè€…è®¤ä¸ºæ”»å‡»ä¸é˜²å¾¡ä¹‹é—´å¹¶ä¸æ˜¯æ— æ­¢å¢ƒçš„å†›å¤‡ç«èµ›ï¼Œæ”»å‡»çš„å¤æ‚æ€§å—é™äºäººç±»çš„èªæ˜ç¨‹åº¦ï¼Œä¸”å­˜åœ¨è‡ªç„¶ç•Œé™ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†è§‰ (Multimodal & Vision)\n\n**9. OmniVec2ï¼šç”¨äºå¤§è§„æ¨¡å¤šæ¨¡æ€å’Œå¤šä»»åŠ¡å­¦ä¹ çš„æ–°å‹ Transformer ç½‘ç»œ**\n**(OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning)**\n- **é‡å¿ƒæå¤§**ï¼šèƒ½å¤Ÿå¤„ç† **12 ç§æ¨¡æ€**ï¼ˆå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬ã€æ·±åº¦ã€ç‚¹äº‘ã€Xå…‰ã€çº¢å¤–ç­‰ï¼‰ã€‚\n- **æ–¹æ³•**ï¼šä½¿ç”¨æ¨¡æ€ä¸“ç”¨çš„ tokenizer å’Œå…±äº«çš„ Transformer æ¶æ„ã€‚æå‡ºäº†ä¸€ç§åœ¨å…¨è”åˆè®­ç»ƒå’Œæˆå¯¹æ¨¡æ€è®­ç»ƒä¹‹é—´æƒè¡¡çš„ç­–ç•¥ã€‚\n\n**10. SeqTexï¼šåœ¨è§†é¢‘åºåˆ—ä¸­ç”Ÿæˆç½‘æ ¼çº¹ç†**\n**(SeqTex: Generate Mesh Textures in Video Sequence)**\n- **è´¡çŒ®**ï¼šè§£å†³äº† 3D çº¹ç†ç”Ÿæˆéš¾çš„é—®é¢˜ã€‚ä¸å…¶å•ç‹¬ç”Ÿæˆçº¹ç†å›¾ï¼Œä¸å¦‚åˆ©ç”¨è§†é¢‘åŸºç¡€æ¨¡å‹ï¼ˆVideo Foundation Modelsï¼‰çš„å…ˆéªŒçŸ¥è¯†ã€‚\n- **æ•ˆæœ**ï¼šèƒ½å¤Ÿç›´æ¥ç”Ÿæˆé«˜è´¨é‡ã€ç©ºé—´ä¸€è‡´çš„ UV çº¹ç†å›¾ï¼Œæ— éœ€ç¹ççš„åå¤„ç†ã€‚\n\n**11. ZEROï¼šå…·æœ‰å¤šæ¨¡æ€æç¤ºçš„å·¥ä¸šçº§è§†è§‰åŸºç¡€æ¨¡å‹**\n**(ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts)**\n- **åº”ç”¨**ï¼šä¸“é—¨é’ˆå¯¹å·¥ä¸šåœºæ™¯ï¼ˆç”±äºæ•°æ®ç¨€ç¼ºï¼Œé€šå¸¸å¾ˆéš¾æï¼‰ã€‚\n- **äº®ç‚¹**ï¼šåˆ©ç”¨ 90 ä¸‡å¼ å·¥ä¸šä¸“æœ‰æ•°æ®è®­ç»ƒï¼Œæ”¯æŒå¤šæ¨¡æ€æç¤ºï¼ˆæ–‡æœ¬+è§†è§‰ï¼‰ï¼Œåœ¨å·¥ä¸šæ•°æ®é›†ä¸Šå®ç°äº† Zero-shot éƒ¨ç½²ï¼Œæ€§èƒ½ä¼˜äºé€šç”¨æ¨¡å‹ã€‚\n\n---\n\n### ğŸ’¡ å€¼å¾—å…³æ³¨çš„å…¶ä»–è®ºæ–‡\n\n*   **[Context Tuning] ä¸Šä¸‹æ–‡å¾®è°ƒç”¨äºä¸Šä¸‹æ–‡ä¼˜åŒ–**\n    **(Context Tuning for In-Context Optimization)**\n    ä¸è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯è°ƒæ•´ Prompt/Prefixã€‚åˆ©ç”¨æ¨¡å‹å›ºæœ‰çš„ ICL èƒ½åŠ›ï¼Œç”¨ä»»åŠ¡ç‰¹å®šçš„æ¼”ç¤ºç¤ºä¾‹åˆå§‹åŒ–å¯è®­ç»ƒçš„å‰ç¼€ï¼Œæ¯”ä¼ ç»Ÿçš„ Soft Prompt Tuning æ•ˆæœæ›´å¥½ã€‚\n\n*   **[Forecasting] è¯„ä¼° LLM åœ¨ç°å®ä¸–ç•Œé¢„æµ‹ä¸­å¯¹æŠ—ä¸“å®¶é¢„æµ‹è€…çš„è¡¨ç°**\n    **(Evaluating LLMs on Real-World Forecasting Against Expert Forecasters)**\n    **ç»“è®º**ï¼šåˆ«å¤ªè¿·ä¿¡ AI ç®—å‘½ã€‚è™½ç„¶ LLM çœ‹èµ·æ¥æ¯”æ™®é€šå¤§ä¼—ï¼ˆCrowdï¼‰å‡†ï¼Œä½†æ¯”èµ·çœŸæ­£çš„äººç±»é¡¶çº§é¢„æµ‹ä¸“å®¶ï¼ˆSuperforecastersï¼‰ï¼Œå·®è·ä¾ç„¶å·¨å¤§ã€‚\n\n*   **[Privacy] DP-Fusionï¼šå¤§è¯­è¨€æ¨¡å‹çš„ Token çº§å·®åˆ†éšç§æ¨ç†**\n    **(DP-Fusion: Token-Level Differentially Private Inference for Large Language Models)**\n    æå‡ºäº†ä¸€ç§æ¨ç†é˜¶æ®µçš„éšç§ä¿æŠ¤æœºåˆ¶ï¼Œå¯ä»¥åœ¨é‡å†™æ–‡æ¡£ï¼ˆéšè—æ•æ„Ÿ Tokenï¼‰çš„åŒæ—¶ï¼Œä¿æŒæ¯”ç°æœ‰æ–¹æ³•ä½ 6 å€çš„å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ã€‚\n\n*   **[Medical] MedGellanï¼šLLM ç”Ÿæˆçš„åŒ»ç–—æŒ‡å¯¼ä»¥æ”¯æŒåŒ»ç”Ÿ**\n    **(MedGellan: LLM-Generated Medical Guidance to Support Physicians)**\n    ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼ŒLLM ä¸ç›´æ¥è¯Šæ–­ï¼Œè€Œæ˜¯ç”Ÿæˆâ€œä¸´åºŠæŒ‡å¯¼â€ï¼Œå†ç”±åŒ»ç”Ÿæ ¹æ®æŒ‡å¯¼è¿›è¡Œè¯Šæ–­ã€‚è¿™ç§ Human-in-the-loop çš„æ¨¡å¼æé«˜äº†è¯Šæ–­çš„ Recall å’Œ F1 åˆ†æ•°ã€‚\n\n*   **[Egyptian LLM] Nile-Chatï¼šç”¨äºé˜¿æ‹‰ä¼¯è¯­å’Œæ‹‰ä¸æ‰‹ç¨¿çš„åŸƒåŠè¯­è¨€æ¨¡å‹**\n    **(Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts)**\n    ä¸“é—¨é’ˆå¯¹åŸƒåŠæ–¹è¨€å’ŒåŒè„šæœ¬ï¼ˆé˜¿æ‹‰ä¼¯è¯­+æ‹‰ä¸è¯­ï¼‰ä¼˜åŒ–çš„æ¨¡å‹ï¼Œä½¿ç”¨äº† Branch-Train-MiX ç­–ç•¥ã€‚\n\nä»Šå¤©çš„å¿«æŠ¥å°±åˆ°è¿™é‡Œï¼Œå¸Œæœ›è¿™äº›å‰æ²¿ç ”ç©¶èƒ½ç»™ä½ çš„å·¥ä½œå¸¦æ¥çµæ„Ÿã€‚ä¸ç®¡æ˜¯ç”Ÿç‰©å¯å‘æ¶æ„çš„å›å½’ï¼Œè¿˜æ˜¯å¯¹è®­ç»ƒæ•ˆç‡çš„åŠ¡å®åæ€ï¼Œéƒ½è¡¨æ˜ AI é¢†åŸŸæ­£åœ¨ä»å•çº¯çš„â€œå †ç®—åŠ›â€å‘æ›´ç²¾ç»†çš„â€œæœºç†ç†è§£â€è½¬å˜ã€‚ç¥ç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2507.04575v1",
      "title": "Lilith: Developmental Modular LLMs with Chemical Signaling",
      "title_zh": "Lilithï¼šåŸºäºåŒ–å­¦ä¿¡å·ä¼ é€’çš„å‘è‚²å¼æ¨¡å—åŒ–å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Mohid Farooqi",
        "Alejandro Comas-Leon"
      ],
      "abstract": "Current paradigms in Artificial Intelligence rely on layers of feedforward networks which model brain activity at the neuronal level. We conjecture that expanding to the level of multiple brain regions with chemical signaling may be a productive step toward understanding the emergence of consciousness. We propose LILITH, a novel architecture that combines developmental training of modular language models with brain-inspired token-based communication protocols, mirroring chemical signaling in the brain. Our approach models distinct brain regions as specialized LLM modules including thinking, memory, sensory, and regulatory components that communicate through emergent token-based signaling protocols analogous to neurotransmitter networks. Unlike traditional pre-trained systems, LILITH would employ developmental training where untrained LLM architectures learn through simulated life experiences, developing communication pathways and cognitive abilities through environmental interaction and evolutionary optimization. This framework would enable direct empirical investigation of consciousness emergence using Integrated Information Theory metrics while providing unprecedented insight into inter-module signaling patterns during development. By optimizing for consciousness emergence rather than task performance, LILITH could provide insight into different emergent phenomena at multiple levels of neural correlates, contrasting neuronal-level processing with multi-region coordination dynamics. The goal of this paper is to put the idea forward while recognizing the substantial challenges in implementing such a system.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LILITHï¼Œä¸€ç§æ¨¡ä»¿å¤§è„‘å¤šåŒºåŸŸåä½œä¸åŒ–å­¦ä¿¡å·ä¼ é€’çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡å—åŒ–è¯­è¨€æ¨¡å‹ (Modular LLMs) æ¢ç´¢æ„è¯†çš„æ¶Œç°ã€‚è¯¥æ¡†æ¶å°†å¤§è„‘åˆ’åˆ†ä¸ºæ€ç»´ã€è®°å¿†ã€æ„ŸçŸ¥å’Œè°ƒèŠ‚ç­‰ä¸“é—¨çš„ LLM æ¨¡å—ï¼Œåˆ©ç”¨åŸºäº Token çš„é€šä¿¡åè®®æ¨¡æ‹Ÿç¥ç»é€’è´¨ç½‘ç»œ (neurotransmitter networks) çš„ä¿¡å·ä¼ é€’ã€‚ä¸ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒçš„ä¼ ç»Ÿç³»ç»Ÿä¸åŒï¼ŒLILITH ä¸»å¼ é‡‡ç”¨å‘å±•æ€§è®­ç»ƒ (developmental training)ï¼Œè®©æ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’å’Œè¿›åŒ–ä¼˜åŒ–ä¸­é€æ­¥å‘å±•è®¤çŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥é›†æˆä¿¡æ¯è®º (Integrated Information Theory) æŒ‡æ ‡ï¼Œè¯¥ç ”ç©¶èƒ½å¤Ÿé‡åŒ–åˆ†ææ„è¯†çš„äº§ç”Ÿè¿‡ç¨‹ï¼Œå¹¶å¯¹æ¯”ç¥ç»å…ƒçº§åˆ«å¤„ç†ä¸å¤šåŒºåŸŸåè°ƒåŠ¨åŠ›å­¦ (multi-region coordination dynamics) ä¹‹é—´çš„å·®å¼‚ã€‚è¿™é¡¹å·¥ä½œçš„æ ¸å¿ƒç›®æ ‡æ˜¯ä¸ºæ„è¯†ç ”ç©¶æä¾›å…¨æ–°çš„å®è¯æ¡†æ¶ï¼Œé€šè¿‡ä¾§é‡äºæ„è¯†æ¶Œç°è€Œéå•çº¯çš„ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºç†è§£ä¸åŒå±‚é¢çš„ç¥ç»å…³è”æä¾›æ–°è§è§£ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "4 pages, 0 figures, position paper",
      "pdf_url": "https://arxiv.org/pdf/2507.04575v1",
      "published_date": "2025-07-06 23:18:51 UTC",
      "updated_date": "2025-07-06 23:18:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:36.897579+00:00"
    },
    {
      "arxiv_id": "2507.04569v1",
      "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts",
      "title_zh": "Nile-Chatï¼šé€‚é…é˜¿æ‹‰ä¼¯è¯­ä¸æ‹‰ä¸è¯­è„šæœ¬çš„åŸƒåŠè¯­è¯­è¨€æ¨¡å‹",
      "authors": [
        "Guokan Shang",
        "Hadi Abdine",
        "Ahmad Chamma",
        "Amr Mohamed",
        "Mohamed Anwar",
        "Abdelaziz Bounhar",
        "Omar El Herraoui",
        "Preslav Nakov",
        "Michalis Vazirgiannis",
        "Eric Xing"
      ],
      "abstract": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for Egyptian dialect, uniquely designed to understand and generate texts written in both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we introduce a novel language adaptation approach by leveraging the Branch-Train-MiX strategy to merge script-specialized experts, into a single MoE model. Our Nile-Chat models significantly outperform leading multilingual and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced Egyptian evaluation benchmarks, which span both understanding and generative tasks. Notably, our 12B model yields a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly available. We believe this work presents a comprehensive methodology for adapting LLMs to dual-script languages, addressing an often overlooked aspect in modern LLM development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Nile-Chat ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ Nile-Chat-4Bã€3x4B-A6B å’Œ 12Bï¼Œä¸“é—¨ç”¨äºå¤„ç†æ”¯æŒé˜¿æ‹‰ä¼¯æ–‡ï¼ˆArabic scriptï¼‰å’Œæ‹‰ä¸æ–‡ï¼ˆLatin scriptï¼‰åŒè„šæœ¬çš„åŸƒåŠæ–¹è¨€ã€‚åœ¨å¼€å‘ Nile-Chat-3x4B-A6B æ—¶ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åˆ›æ–°çš„è¯­è¨€é€‚é…æ–¹æ³•ï¼Œåˆ©ç”¨ Branch-Train-MiX ç­–ç•¥å°†ä¸“æ³¨äºä¸åŒè„šæœ¬çš„ä¸“å®¶æ¨¡å‹åˆå¹¶ä¸ºå•ä¸€çš„æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNile-Chat åœ¨åŸƒåŠè¯­ç†è§£ä¸ç”Ÿæˆä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äº LLaMaã€Jais å’Œ ALLaM ç­‰ä¸»æµæ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯å…¶ 12B ç‰ˆæœ¬åœ¨æ‹‰ä¸è„šæœ¬åŸºå‡†æµ‹è¯•ä¸­æ¯” Qwen2.5-14B-Instruct æ€§èƒ½æå‡äº† 14.4%ã€‚è¯¥å·¥ä½œä¸ä»…å…¬å¼€äº†æ‰€æœ‰èµ„æºï¼Œè¿˜ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚é…åŒè„šæœ¬è¯­è¨€æä¾›äº†ä¸€å¥—å…¨é¢çš„æ–¹æ³•è®ºï¼Œå¡«è¡¥äº†ç°ä»£æ¨¡å‹å¼€å‘ä¸­å¯¹ç‰¹å®šæ–¹è¨€å’ŒåŒè„šæœ¬æ”¯æŒçš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04569v1",
      "published_date": "2025-07-06 22:53:41 UTC",
      "updated_date": "2025-07-06 22:53:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:36.031458+00:00"
    },
    {
      "arxiv_id": "2507.04562v3",
      "title": "Evaluating LLMs on Real-World Forecasting Against Expert Forecasters",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œé¢„æµ‹ä¸­ç›¸å¯¹äºä¸“å®¶é¢„æµ‹å‘˜çš„è¡¨ç°",
      "authors": [
        "Janna Lu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against top forecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of experts.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡464ä¸ªæ¥è‡ªMetaculusçš„é¢„æµ‹é—®é¢˜ï¼Œè¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨çœŸå®ä¸–ç•Œæœªæ¥äº‹ä»¶é¢„æµ‹ä¸­çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç ”ç©¶çš„ç©ºç™½ã€‚å°½ç®¡ä¸€å¹´å‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šè¿œä¸åŠäººç±»ç¾¤ä½“(human crowd)ï¼Œä½†æœ€æ–°çš„Frontier modelså±•ç°å‡ºäº†æ˜¾è‘—è¿›æ­¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›å‰æ²¿æ¨¡å‹çš„Brier scoresåœ¨è¡¨é¢ä¸Šå·²ç»è¶…è¶Šäº†æ™®é€šçš„äººç±»é¢„æµ‹ç¾¤ä½“ã€‚ç„¶è€Œï¼Œä¸ä¸“ä¸šçš„ä¸“å®¶é¢„æµ‹å›¢é˜Ÿ(expert forecasters)ç›¸æ¯”ï¼ŒLLMsçš„è¡¨ç°ä»ç„¶å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå…¶é¢„æµ‹æ•ˆèƒ½ä»æœªè¾¾åˆ°äººç±»ä¸“å®¶æ°´å¹³ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨å¤æ‚é¢„æµ‹ä»»åŠ¡ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ï¼Œå¹¶ä¸ºè¡¡é‡äººå·¥æ™ºèƒ½åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04562v3",
      "published_date": "2025-07-06 22:26:59 UTC",
      "updated_date": "2025-08-04 20:48:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:33.643396+00:00"
    },
    {
      "arxiv_id": "2507.04548v1",
      "title": "SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection",
      "title_zh": "SPIRAï¼šå‘¼å¸åŠŸèƒ½ä¸å…¨æ£€æµ‹æ™ºèƒ½ç³»ç»Ÿçš„æ„å»º",
      "authors": [
        "Renato Cordeiro Ferreira",
        "Dayanne Gomes",
        "Vitor Tamae",
        "Francisco Wernke",
        "Alfredo Goldman"
      ],
      "abstract": "Respiratory insufficiency is a medic symptom in which a person gets a reduced amount of oxygen in the blood. This paper reports the experience of building SPIRA: an intelligent system for detecting respiratory insufficiency from voice. It compiles challenges faced in two succeeding implementations of the same architecture, summarizing lessons learned on data collection, training, and inference for future projects in similar systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† SPIRAï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡è¯­éŸ³æ£€æµ‹å‘¼å¸åŠŸèƒ½ä¸å…¨ (Respiratory Insufficiency) çš„æ™ºèƒ½ç³»ç»Ÿã€‚å‘¼å¸åŠŸèƒ½ä¸å…¨æ˜¯ä¸€ç§å¯¼è‡´è¡€æ¶²ä¸­æ°§å«é‡é™ä½çš„åŒ»ç–—ç—‡çŠ¶ï¼Œè€Œ SPIRA æä¾›äº†ä¸€ç§åŸºäºéŸ³é¢‘ä¿¡å·çš„ä¾¿æ·æ£€æµ‹æ‰‹æ®µã€‚è®ºæ–‡è¯¦ç»†è®°å½•äº†åœ¨åŒä¸€æ¶æ„çš„ä¸¤æ¬¡è¿ç»­å®ç°è¿‡ç¨‹ä¸­æ‰€é‡åˆ°çš„æŒ‘æˆ˜ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†å…³äºæ•°æ®é‡‡é›† (Data Collection)ã€æ¨¡å‹è®­ç»ƒ (Training) ä»¥åŠæ¨ç† (Inference) é˜¶æ®µçš„å®æˆ˜ç»éªŒã€‚é€šè¿‡å¯¹ç³»ç»Ÿæ„å»ºå…¨æµç¨‹çš„æ·±å…¥å‰–æï¼Œè¯¥ç ”ç©¶ä¸ºæœªæ¥ç±»ä¼¼åŒ»ç–—è¾…åŠ©ç³»ç»Ÿçš„è®¾è®¡ä¸éƒ¨ç½²æä¾›äº†å®è´µçš„æ•™è®­å’ŒæŒ‡å¯¼ã€‚SPIRA çš„æˆåŠŸç ”å‘ä¸ä»…éªŒè¯äº†åˆ©ç”¨è¯­éŸ³ç”Ÿç‰©æ ‡å¿—ç‰©è¿›è¡Œå‘¼å¸ç³»ç»Ÿç—‡çŠ¶è¯†åˆ«çš„å¯è¡Œæ€§ï¼Œä¹Ÿä¸ºå¼€å‘æ›´é«˜æ•ˆã€å¯æ‰©å±•çš„åŒ»ç–—æ™ºèƒ½è¯Šæ–­å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "4 pages, 1 figure (1 diagram), published at ISE 2022",
      "pdf_url": "https://arxiv.org/pdf/2507.04548v1",
      "published_date": "2025-07-06 21:42:02 UTC",
      "updated_date": "2025-07-06 21:42:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:39.107530+00:00"
    },
    {
      "arxiv_id": "2507.19500v1",
      "title": "Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems",
      "title_zh": "å‡è§†æ„ŸçŸ¥ AIï¼šé¢å‘äººæœºäº¤äº’ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¾¹ç¼˜åŒ–ç¾¤ä½“è®¤çŸ¥ä½“éªŒæ•°å­¦å»ºæ¨¡",
      "authors": [
        "Omkar Suresh Hatti"
      ],
      "abstract": "The proliferation of artificial intelligence provides an opportunity to create psychological spaciousness in society. Spaciousness is defined as the ability to hold diverse interpersonal interactions and forms the basis for vulnerability that leads to authenticity that leads to prosocial behaviors and thus to societal harmony. This paper demonstrates an attempt to quantify, the human conditioning to subconsciously modify authentic self-expression to fit the norms of the dominant culture. Gaze is explored across various marginalized and intersectional groups, using concepts from postmodern philosophy and psychology. The effects of gaze are studied through analyzing a few redacted Reddit posts, only to be discussed in discourse and not endorsement. A mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite Metric is presented to model the analysis of two sets of conversational spaces in relation to one another. The outcome includes an equation to train Large Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT; and an argument for affirming and inclusive HCI, based on the equation, is presented. The argument is supported by a few principles of Neuro-plasticity, The brain's lifelong capacity to rewire.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¾¹ç¼˜åŒ–ç¾¤ä½“åœ¨é¢å¯¹ä¸»æµæ–‡åŒ–å‡è§†(Gaze)æ—¶ï¼Œä¸ºè¿åˆè§„èŒƒè€Œæ½œæ„è¯†ä¿®æ”¹çœŸå®è‡ªæˆ‘è¡¨è¾¾çš„å¿ƒç†ç°è±¡ï¼Œå¹¶æå‡ºäº†â€œå¿ƒç†å®½æ•åº¦â€(Spaciousness)ä½œä¸ºä¿ƒè¿›äº²ç¤¾ä¼šè¡Œä¸ºä¸ç¤¾ä¼šå’Œè°çš„åŸºç¡€ã€‚ç ”ç©¶ç»“åˆåç°ä»£å“²å­¦ä¸å¿ƒç†å­¦ç†è®ºï¼Œé€šè¿‡åˆ†æç¤¾äº¤åª’ä½“è¯­å¢ƒï¼Œæ„å»ºäº†å‡è§†å‹åŠ›æŒ‡æ•°å·®å€¼ç»¼åˆæŒ‡æ ‡(Gaze Pressure Index (GPI)-Diff Composite Metric)çš„æ•°å­¦æ¨¡å‹ã€‚åŸºäºè¯¥æ¨¡å‹ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„è®¡ç®—å…¬å¼ï¼Œæ—¨åœ¨æ¨åŠ¨æ„å»ºæ›´å…·è‚¯å®šæ€§å’ŒåŒ…å®¹æ€§çš„äººæœºäº¤äº’(Human-Computer Interaction, HCI)ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç»“åˆç¥ç»å¡‘æ–™æ€§(Neuro-plasticity)åŸç†ï¼Œè®ºè¯äº†é€šè¿‡AIæŠ€æœ¯é‡å¡‘ç¤¾ä¼šè®¤çŸ¥ç©ºé—´çš„å¯è¡Œæ€§ã€‚è¿™ä¸€æˆæœä¸ºè§£å†³ç®—æ³•åè§å¹¶æå‡è¾¹ç¼˜åŒ–ç¾¤ä½“åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£çš„è®¤è¯†è®ºä½“éªŒæä¾›äº†é‡åŒ–å·¥å…·ä¸è®¾è®¡æ¡†æ¶ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19500v1",
      "published_date": "2025-07-06 20:55:18 UTC",
      "updated_date": "2025-07-06 20:55:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:41.413755+00:00"
    },
    {
      "arxiv_id": "2507.04531v3",
      "title": "DP-Fusion: Token-Level Differentially Private Inference for Large Language Models",
      "title_zh": "DP-Fusionï¼šå¤§è¯­è¨€æ¨¡å‹è¯å…ƒçº§å·®åˆ†éšç§æ¨ç†",
      "authors": [
        "Rushil Thareja",
        "Preslav Nakov",
        "Praneeth Vepakomma",
        "Nils Lukas"
      ],
      "abstract": "Large language models (LLMs) do not preserve privacy at inference-time. The LLM's outputs can inadvertently reveal information about the model's context, which presents a privacy challenge when the LLM is augmented via tools or databases containing sensitive information. Existing privacy-preserving methods at inference-time have significant limitations since they (i) lack provable guarantees or (ii) have a poor utility/privacy trade-off. We propose DP-Fusion, a Differentially Private Inference (DPI) mechanism for LLMs that provably bounds the influence a set of tokens in the context can have on the LLM's output. DP-Fusion works as follows: (1) label a subset of sensitive tokens, (2) infer the LLM without any sensitive tokens to obtain a baseline, (3) infer the LLM with the sensitive tokens, and (4) blend distributions so that the final output remains within a bounded distance of the baseline distribution. While this per-token influence bound also mitigates jailbreak-style prompt injection, we focus on \\emph{document privatization}, where the goal is to paraphrase a document containing sensitive tokens, e.g., personally identifiable information, so that no attacker can reliably infer them from the paraphrased document while preserving high text quality. The privacy/utility trade-off is controlled by $Îµ$, where $Îµ=0$ hides sensitive tokens entirely, while higher values trade off privacy for improved text quality. We show that our method creates token-level provably privatized documents with substantially improved theoretical and empirical privacy, achieving $6\\times$ lower perplexity than related DPI methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨æ¨ç†é˜¶æ®µæ— æ³•æœ‰æ•ˆä¿æŠ¤ä¸Šä¸‹æ–‡éšç§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º DP-Fusion çš„ä»¤ç‰Œçº§å·®åˆ†éšç§æ¨ç†(Differentially Private Inference, DPI)æœºåˆ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡é™åˆ¶ä¸Šä¸‹æ–‡ä¸­ç‰¹å®šä»¤ç‰Œå¯¹è¾“å‡ºçš„å½±å“ï¼Œè§£å†³ LLM åœ¨ç»“åˆæ•æ„Ÿä¿¡æ¯è¿›è¡Œæ¨ç†æ—¶çš„æ³„éœ²æŒ‘æˆ˜ã€‚DP-Fusion çš„å·¥ä½œæµç¨‹åŒ…æ‹¬æ ‡è®°æ•æ„Ÿä»¤ç‰Œã€è·å–ä¸å«æ•æ„Ÿä¿¡æ¯çš„åŸºçº¿åˆ†å¸ƒã€è¿›è¡Œå«æ•æ„Ÿä¿¡æ¯çš„æ¨ç†å¹¶æ··åˆä¸¤ç§åˆ†å¸ƒï¼Œä»è€Œç¡®ä¿æœ€ç»ˆè¾“å‡ºåœ¨æ•°å­¦ä¸Šå¯è¯æ˜åœ°é™åˆ¶äº†æ•æ„Ÿä¿¡æ¯çš„å½±å“ã€‚ç ”ç©¶é‡ç‚¹åº”ç”¨äºæ–‡æ¡£ç§æœ‰åŒ–(document privatization)ä»»åŠ¡ï¼Œé€šè¿‡å‚æ•° $\\epsilon$ çµæ´»è°ƒèŠ‚éšç§ä¿æŠ¤ä¸æ–‡æœ¬è´¨é‡ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDP-Fusion åœ¨å®ç°æ›´å¼ºç†è®ºå’Œå®è¯éšç§ä¿éšœçš„åŒæ—¶ï¼Œå…¶å›°æƒ‘åº¦(perplexity)æ¯”åŒç±» DPI æ–¹æ³•é™ä½äº† 6 å€ï¼Œåœ¨ä¿æŒé«˜æ–‡æœ¬è´¨é‡çš„å‰æä¸‹æœ‰æ•ˆé˜²æ­¢äº†æ”»å‡»è€…å¯¹æ•æ„Ÿä¿¡æ¯çš„æ¨æ–­ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Our code and data are publicly available here: https://github.com/MBZUAI-Trustworthy-ML/DP-Fusion-DPI",
      "pdf_url": "https://arxiv.org/pdf/2507.04531v3",
      "published_date": "2025-07-06 20:49:39 UTC",
      "updated_date": "2025-11-09 12:39:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:47.852448+00:00"
    },
    {
      "arxiv_id": "2507.04528v1",
      "title": "Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence",
      "title_zh": "è¿ˆå‘å¯è§£é‡Šäººå·¥æ™ºèƒ½ä¸­éšç§å¢å¼ºæŠ€æœ¯çš„é›†æˆ",
      "authors": [
        "Sonal Allana",
        "Rozita Dara",
        "Xiaodong Lin",
        "Pulei Xiong"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating the risk of non-transparency in the decision-making process of black-box Artificial Intelligence (AI) systems. However, despite the benefits, XAI methods are found to leak the privacy of individuals whose data is used in training or querying the models. Researchers have demonstrated privacy attacks that exploit explanations to infer sensitive personal information of individuals. Currently there is a lack of defenses against known privacy attacks targeting explanations when vulnerable XAI are used in production and machine learning as a service system. To address this gap, in this article, we explore Privacy Enhancing Technologies (PETs) as a defense mechanism against attribute inference on explanations provided by feature-based XAI methods. We empirically evaluate 3 types of PETs, namely synthetic training data, differentially private training and noise addition, on two categories of feature-based XAI. Our evaluation determines different responses from the mitigation methods and side-effects of PETs on other system properties such as utility and performance. In the best case, PETs integration in explanations reduced the risk of the attack by 49.47%, while maintaining model utility and explanation quality. Through our evaluation, we identify strategies for using PETs in XAI for maximizing benefits and minimizing the success of this privacy attack on sensitive personal information.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable Artificial Intelligence, XAI)ä¸­é›†æˆéšç§å¢å¼ºæŠ€æœ¯(Privacy Enhancing Technologies, PETs)çš„è·¯å¾„ï¼Œæ—¨åœ¨è§£å†³XAIè§£é‡Šå¯èƒ½æ³„éœ²è®­ç»ƒæ•°æ®æˆ–æŸ¥è¯¢æ¨¡å‹ä¸ªä½“æ•æ„Ÿéšç§çš„é—®é¢˜ã€‚é’ˆå¯¹ç›®å‰ç¼ºä¹é˜²å¾¡é’ˆå¯¹è§£é‡Šçš„éšç§æ”»å‡»è¿™ä¸€ç°çŠ¶ï¼Œæœ¬æ–‡è¯„ä¼°äº†åˆæˆè®­ç»ƒæ•°æ®(synthetic training data)ã€å·®åˆ†éšç§è®­ç»ƒ(differentially private training)å’Œå¢åŠ å™ªå£°(noise addition)ä¸‰ç±»PETsä½œä¸ºé˜²å¾¡ç‰¹å¾åŸºXAIå±æ€§æ¨ç†æ”»å‡»çš„æœºåˆ¶ã€‚é€šè¿‡åœ¨ä¸¤ç±»ç‰¹å¾åŸºXAIä¸Šçš„å®è¯ç ”ç©¶ï¼Œåˆ†æäº†ä¸åŒç¼“è§£æ–¹æ³•å¯¹ç³»ç»Ÿæ•ˆç”¨(utility)å’Œæ€§èƒ½çš„å‰¯ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ€ä½³é…ç½®ä¸‹ï¼Œé›†æˆPETså¯å°†æ”»å‡»é£é™©é™ä½49.47%ï¼Œä¸”èƒ½è¾ƒå¥½åœ°ç»´æŒæ¨¡å‹æ•ˆç”¨å’Œè§£é‡Šè´¨é‡ã€‚è¯¥ç ”ç©¶æœ€åæå‡ºäº†åœ¨XAIä¸­è¿ç”¨PETsçš„ä¼˜åŒ–ç­–ç•¥ï¼Œä¸ºåœ¨ä¿æŠ¤ä¸ªäººæ•æ„Ÿä¿¡æ¯çš„åŒæ—¶æœ€å¤§åŒ–è§£é‡Šæ€§æ”¶ç›Šæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under peer review",
      "pdf_url": "https://arxiv.org/pdf/2507.04528v1",
      "published_date": "2025-07-06 20:45:34 UTC",
      "updated_date": "2025-07-06 20:45:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:48.115384+00:00"
    },
    {
      "arxiv_id": "2507.04522v1",
      "title": "Grounded Gesture Generation: Language, Motion, and Space",
      "title_zh": "å…·èº«æ‰‹åŠ¿ç”Ÿæˆï¼šè¯­è¨€ã€åŠ¨ä½œä¸ç©ºé—´",
      "authors": [
        "Anna Deichler",
        "Jim O'Regan",
        "Teo Guichoux",
        "David Johansson",
        "Jonas Beskow"
      ],
      "abstract": "Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.\n  Project page: https://groundedgestures.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººä½“åŠ¨ä½œç”Ÿæˆé¢†åŸŸä¸­ç©ºé—´å¯¹é½(Spatially Grounded)å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥(Context-aware)å§¿æ€è¢«å¿½è§†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„å¤šæ¨¡æ€æ•°æ®é›†å’Œæ¡†æ¶ã€‚ç ”ç©¶è€…æŒ‡å‡ºç°æœ‰æ¨¡å‹å¾€å¾€å°†åŠ¨ä½œç”Ÿæˆä¸ç¯å¢ƒå¯¹é½(Environmental Grounding)åˆ†ç¦»ï¼Œé™åˆ¶äº†å…·èº«æ™ºèƒ½ä½“(Embodied Agents)çš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œè¯¥å·¥ä½œæ•´åˆäº†ç©ºé—´æŒ‡ç§°å§¿æ€åˆæˆæ•°æ®é›†å’ŒåŸºäºè™šæ‹Ÿç°å®(VR)çš„MM-Convå¯¹è¯æ•°æ®é›†ï¼Œæä¾›äº†è¶…è¿‡7.7å°æ—¶åŒ…å«åŠ¨ä½œã€è¯­éŸ³åŠ3Dåœºæ™¯ä¿¡æ¯çš„åŒæ­¥æ•°æ®ï¼Œå¹¶é‡‡ç”¨HumanML3Dæ ¼å¼è¿›è¡Œæ ‡å‡†åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜è¿æ¥äº†åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨(Physics-based Simulator)ï¼Œæ”¯æŒåˆæˆæ•°æ®ç”ŸæˆåŠæƒ…å¢ƒåŒ–è¯„ä¼°(Situated Evaluation)ã€‚é€šè¿‡å¼¥åˆå§¿æ€å»ºæ¨¡ä¸ç©ºé—´å¯¹é½ä¹‹é—´çš„é¸¿æ²Ÿï¼Œè¯¥é¡¹è´¡çŒ®ä¸ºæ¨è¿›æƒ…å¢ƒåŒ–å§¿æ€ç”Ÿæˆå’Œå…·èº«å¤šæ¨¡æ€äº¤äº’ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as a non-archival paper at the CVPR 2025 Humanoid Agents Workshop. Project page: https://groundedgestures.github.io",
      "pdf_url": "https://arxiv.org/pdf/2507.04522v1",
      "published_date": "2025-07-06 20:19:34 UTC",
      "updated_date": "2025-07-06 20:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:58.106131+00:00"
    },
    {
      "arxiv_id": "2507.04513v1",
      "title": "Churn-Aware Recommendation Planning under Aggregated Preference Feedback",
      "title_zh": "èšåˆåå¥½åé¦ˆä¸‹è€ƒè™‘æµå¤±é£é™©çš„æ¨èè§„åˆ’",
      "authors": [
        "Gur Keinan",
        "Omer Ben-Porat"
      ],
      "abstract": "We study a sequential decision-making problem motivated by recent regulatory and technological shifts that limit access to individual user data in recommender systems (RSs), leaving only population-level preference information. This privacy-aware setting poses fundamental challenges in planning under uncertainty: Effective personalization requires exploration to infer user preferences, yet unsatisfactory recommendations risk immediate user churn. To address this, we introduce the Rec-APC model, in which an anonymous user is drawn from a known prior over latent user types (e.g., personas or clusters), and the decision-maker sequentially selects items to recommend. Feedback is binary -- positive responses refine the posterior via Bayesian updates, while negative responses result in the termination of the session.\n  We prove that optimal policies converge to pure exploitation in finite time and propose a branch-and-bound algorithm to efficiently compute them. Experiments on synthetic and MovieLens data confirm rapid convergence and demonstrate that our method outperforms the POMDP solver SARSOP, particularly when the number of user types is large or comparable to the number of content categories. Our results highlight the applicability of this approach and inspire new ways to improve decision-making under the constraints imposed by aggregated preference data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨éšç§ä¿æŠ¤é™åˆ¶ä¸‹ï¼Œä»…èƒ½è·å–èšåˆåå¥½ï¼ˆAggregated Preferenceï¼‰ä¿¡æ¯è€Œéä¸ªä½“æ•°æ®çš„æ¨èç³»ç»Ÿåºè´¯å†³ç­–é—®é¢˜ã€‚åœ¨è¿™ç§è®¾å®šä¸‹ï¼Œå†³ç­–è€…é¢ä¸´ç€é€šè¿‡æ¢ç´¢ï¼ˆExplorationï¼‰æ¨æ–­ç”¨æˆ·åå¥½ä¸é¿å…å› æ¨èå¤±è¯¯å¯¼è‡´ç”¨æˆ·æµå¤±ï¼ˆChurnï¼‰ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Rec-APCæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åŒ¿åç”¨æˆ·è§†ä¸ºä»å·²çŸ¥å…ˆéªŒåˆ†å¸ƒä¸­æŠ½å–çš„æ½œåœ¨ç”¨æˆ·ç±»å‹ï¼Œå¹¶æ ¹æ®äºŒå…ƒåé¦ˆï¼ˆBinary Feedbackï¼‰è¿›è¡Œè´å¶æ–¯æ›´æ–°ã€‚ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜äº†æœ€ä¼˜ç­–ç•¥åœ¨æœ‰é™æ—¶é—´å†…ä¼šæ”¶æ•›è‡³çº¯å¼€å‘ï¼ˆExploitationï¼‰é˜¶æ®µï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†æ”¯å®šç•Œï¼ˆBranch-and-Boundï¼‰ç®—æ³•æ¥é«˜æ•ˆè®¡ç®—æœ€ä¼˜ç­–ç•¥ã€‚åœ¨åˆæˆæ•°æ®å’ŒMovieLensæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨ç”¨æˆ·ç±»å‹è§„æ¨¡è¾ƒå¤§æ—¶å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºPOMDPæ±‚è§£å™¨SARSOPã€‚è¯¥ç ”ç©¶ä¸ºå—é™èšåˆæ•°æ®ç¯å¢ƒä¸‹çš„å†³ç­–ä¼˜åŒ–æä¾›äº†æ–°çš„æ¨¡å‹æ¡†æ¶ä¸é«˜æ•ˆçš„æ±‚è§£ç®—æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2502.18483",
      "pdf_url": "https://arxiv.org/pdf/2507.04513v1",
      "published_date": "2025-07-06 19:22:47 UTC",
      "updated_date": "2025-07-06 19:22:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:44:53.205647+00:00"
    },
    {
      "arxiv_id": "2507.04509v1",
      "title": "MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization",
      "title_zh": "MVL-Locï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¯æ³›åŒ–å¤šåœºæ™¯ç›¸æœºé‡å®šä½",
      "authors": [
        "Zhendong Xiao",
        "Wu Wei",
        "Shujie Ji",
        "Shan Yang",
        "Changhao Chen"
      ],
      "abstract": "Camera relocalization, a cornerstone capability of modern computer vision, accurately determines a camera's position and orientation (6-DoF) from images and is essential for applications in augmented reality (AR), mixed reality (MR), autonomous driving, delivery drones, and robotic navigation. Unlike traditional deep learning-based methods that regress camera pose from images in a single scene, which often lack generalization and robustness in diverse environments, we propose MVL-Loc, a novel end-to-end multi-scene 6-DoF camera relocalization framework. MVL-Loc leverages pretrained world knowledge from vision-language models (VLMs) and incorporates multimodal data to generalize across both indoor and outdoor settings. Furthermore, natural language is employed as a directive tool to guide the multi-scene learning process, facilitating semantic understanding of complex scenes and capturing spatial relationships among objects. Extensive experiments on the 7Scenes and Cambridge Landmarks datasets demonstrate MVL-Loc's robustness and state-of-the-art performance in real-world multi-scene camera relocalization, with improved accuracy in both positional and orientational estimates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MVL-Locï¼Œä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯å¤šåœºæ™¯6-DoFç›¸æœºé‡å®šä½(Camera relocalization)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­ç¼ºä¹æ³›åŒ–æ€§å’Œé²æ£’æ€§çš„é—®é¢˜ã€‚MVL-Locæœ‰æ•ˆåˆ©ç”¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¸­é¢„è®­ç»ƒçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ•´åˆå¤šæ¨¡æ€æ•°æ®ï¼Œå®ç°äº†åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸‹çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°†è‡ªç„¶è¯­è¨€ä½œä¸ºæŒ‡ä»¤æ€§å·¥å…·å¼•å…¥å¤šåœºæ™¯å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œä¿ƒè¿›äº†å¯¹å¤æ‚åœºæ™¯çš„è¯­ä¹‰ç†è§£å¹¶èƒ½ç²¾å‡†æ•æ‰ç‰©ä½“é—´çš„ç©ºé—´å…³ç³»ã€‚åœ¨7Sceneså’ŒCambridge Landmarksæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMVL-Locåœ¨å¤„ç†çœŸå®ä¸–ç•Œå¤šåœºæ™¯é‡å®šä½ä»»åŠ¡æ—¶å…·æœ‰æå¼ºçš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä½ç½®å’Œæ–¹å‘ä¼°è®¡çš„å‡†ç¡®æ€§ä¸Šå‡è¾¾åˆ°äº†State-of-the-artæ°´å¹³ï¼Œä¸ºå¢å¼ºç°å®(AR)ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸçš„å…³é”®åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "PRCV",
      "pdf_url": "https://arxiv.org/pdf/2507.04509v1",
      "published_date": "2025-07-06 18:52:16 UTC",
      "updated_date": "2025-07-06 18:52:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:03.838200+00:00"
    },
    {
      "arxiv_id": "2507.13364v1",
      "title": "OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning",
      "title_zh": "OmniVec2ï¼šä¸€ç§é¢å‘å¤§è§„æ¨¡å¤šæ¨¡æ€åŠå¤šä»»åŠ¡å­¦ä¹ çš„æ–°å‹ Transformer ç½‘ç»œ",
      "authors": [
        "Siddharth Srivastava",
        "Gaurav Sharma"
      ],
      "abstract": "We present a novel multimodal multitask network and associated training algorithm. The method is capable of ingesting data from approximately 12 different modalities namely image, video, audio, text, depth, point cloud, time series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed approach utilizes modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms to project the data from different modalities into a unified embedding space. It addresses multimodal and multitask scenarios by incorporating modality-specific task heads for different tasks in respective modalities. We propose a novel pretraining strategy with iterative modality switching to initialize the network, and a training algorithm which trades off fully joint training over all modalities, with training on pairs of modalities at a time. We provide comprehensive evaluation across 25 datasets from 12 modalities and show state of the art performances, demonstrating the effectiveness of the proposed architecture, pretraining strategy and adapted multitask training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OmniVec2ï¼Œä¸€ç§æ–°å‹çš„åŸºäº Transformer çš„å¤šæ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ ç½‘ç»œï¼Œèƒ½å¤Ÿå¤„ç†åŒ…æ‹¬å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬ã€æ·±åº¦ã€ç‚¹äº‘ã€æ—¶é—´åºåˆ—ã€è¡¨æ ¼ã€å›¾ã€X-rayã€çº¢å¤–ã€IMU å’Œé«˜å…‰è°±åœ¨å†…çš„çº¦ 12 ç§ä¸åŒæ¨¡æ€çš„æ•°æ®ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¨¡æ€ä¸“ç”¨åˆ†è¯å™¨ (Tokenizer)ã€å…±äº«çš„ Transformer æ¶æ„ä»¥åŠäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (Cross-Attention)ï¼Œå°†ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ (Unified Embedding Space) ä¸­ã€‚ä¸ºäº†åº”å¯¹å¤æ‚çš„ä»»åŠ¡éœ€æ±‚ï¼ŒOmniVec2 é’ˆå¯¹ä¸åŒæ¨¡æ€çš„ä»»åŠ¡é›†æˆäº†ç‰¹å®šçš„ä»»åŠ¡å¤´ (Task Heads)ã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§åŸºäºè¿­ä»£æ¨¡æ€åˆ‡æ¢çš„æ–°é¢–é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠä¸€ç§åœ¨å…¨æ¨¡æ€è”åˆè®­ç»ƒä¸ä¸¤ä¸¤æ¨¡æ€è®­ç»ƒä¹‹é—´å–å¾—å¹³è¡¡çš„è®­ç»ƒç®—æ³•ã€‚å®éªŒåœ¨æ¶‰åŠ 12 ç§æ¨¡æ€çš„ 25 ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º OmniVec2 è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„ (State of the art) æ€§èƒ½æ°´å¹³ã€‚è¿™ä¸€ç ”ç©¶æˆæœæœ‰åŠ›è¯æ˜äº†æ‰€ææ¶æ„ã€é¢„è®­ç»ƒç­–ç•¥åŠè‡ªé€‚åº”å¤šä»»åŠ¡è®­ç»ƒåœ¨å¤„ç†å¤§è§„æ¨¡å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13364v1",
      "published_date": "2025-07-06 18:51:22 UTC",
      "updated_date": "2025-07-06 18:51:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:15.398820+00:00"
    },
    {
      "arxiv_id": "2507.04494v1",
      "title": "Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference",
      "title_zh": "åƒè„‘ç³»ç»Ÿï¼šæ”¯æŒå¿«é€Ÿã€é²æ£’å­¦ä¹ ä¸æ¨ç†çš„æ„ŸçŸ¥è¿åŠ¨æ™ºèƒ½",
      "authors": [
        "Niels Leadholm",
        "Viviane Clay",
        "Scott Knudstrup",
        "Hojae Lee",
        "Jeff Hawkins"
      ],
      "abstract": "Current AI systems achieve impressive performance on many tasks, yet they lack core attributes of biological intelligence, including rapid, continual learning, representations grounded in sensorimotor interactions, and structured knowledge that enables efficient generalization. Neuroscience theory suggests that mammals evolved flexible intelligence through the replication of a semi-independent, sensorimotor module, a functional unit known as a cortical column. To address the disparity between biological and artificial intelligence, thousand-brains systems were proposed as a means of mirroring the architecture of cortical columns and their interactions.\n  In the current work, we evaluate the unique properties of Monty, the first implementation of a thousand-brains system. We focus on 3D object perception, and in particular, the combined task of object recognition and pose estimation. Utilizing the YCB dataset of household objects, we first assess Monty's use of sensorimotor learning to build structured representations, finding that these enable robust generalization. These representations include an emphasis on classifying objects by their global shape, as well as a natural ability to detect object symmetries. We then explore Monty's use of model-free and model-based policies to enable rapid inference by supporting principled movements. We find that such policies complement Monty's modular architecture, a design that can accommodate communication between modules to further accelerate inference speed via a novel `voting' algorithm. Finally, we examine Monty's use of associative, Hebbian-like binding to enable rapid, continual, and computationally efficient learning, properties that compare favorably to current deep learning architectures. While Monty is still in a nascent stage of development, these findings support thousand-brains systems as a powerful and promising new approach to AI.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† Montyï¼Œè¿™æ˜¯é¦–ä¸ªå®ç°çš„åƒè„‘ç³»ç»Ÿ (thousand-brains systems)ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡ä»¿å¤§è„‘çš®å±‚æŸ± (cortical columns) çš„æ¶æ„åŠå…¶ç›¸äº’ä½œç”¨ï¼Œè§£å†³å½“å‰äººå·¥æ™ºèƒ½åœ¨å¿«é€ŸæŒç»­å­¦ä¹ ã€æ„Ÿè§‰è¿åŠ¨ (sensorimotor) äº¤äº’å’Œç»“æ„åŒ–çŸ¥è¯†è¡¨å¾æ–¹é¢çš„ä¸è¶³ã€‚è¯¥ç³»ç»Ÿä¸“æ³¨äºä¸‰ç»´ç‰©ä½“æ„ŸçŸ¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ YCB æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç‰©ä½“è¯†åˆ«å’Œå§¿æ€ä¼°è®¡ (pose estimation) ä»»åŠ¡ã€‚ç ”ç©¶å‘ç° Monty åˆ©ç”¨æ„Ÿè§‰è¿åŠ¨å­¦ä¹ æ„å»ºçš„ç»“æ„åŒ–è¡¨å¾èƒ½å¤Ÿå®ç°ç¨³å¥çš„æ³›åŒ–ï¼Œå¹¶èƒ½æœ‰æ•ˆå¯¹ç‰©ä½“çš„å…¨å±€å½¢çŠ¶è¿›è¡Œåˆ†ç±»åŠæ£€æµ‹ç‰©ä½“å¯¹ç§°æ€§ã€‚é€šè¿‡ç»“åˆæ— æ¨¡å‹ (model-free) å’ŒåŸºäºæ¨¡å‹ (model-based) çš„ç­–ç•¥ï¼ŒMonty èƒ½å¤Ÿæ”¯æŒåŸåˆ™æ€§çš„ç§»åŠ¨ä»¥å®ç°å¿«é€Ÿæ¨ç†ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡å…è®¸æ¨¡å—é—´é€šè¿‡ä¸€ç§æ–°å‹çš„æŠ•ç¥¨ç®—æ³• (voting algorithm) è¿›è¡Œé€šä¿¡ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒMonty åˆ©ç”¨ç±»èµ«å¸ƒ (Hebbian-like) çš„å…³è”ç»‘å®šå®ç°äº†å¿«é€Ÿã€æŒç»­ä¸”è®¡ç®—é«˜æ•ˆçš„å­¦ä¹ ï¼Œå…¶æ€§èƒ½ä¼˜äºå½“å‰çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚å®éªŒç»“æœè¯æ˜äº†åƒè„‘ç³»ç»Ÿåœ¨æ„å»ºæ›´æ¥è¿‘ç”Ÿç‰©æ™ºèƒ½çš„ AI ç³»ç»Ÿæ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›å’Œåº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.04494v1",
      "published_date": "2025-07-06 18:11:07 UTC",
      "updated_date": "2025-07-06 18:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:17.964602+00:00"
    },
    {
      "arxiv_id": "2507.04491v1",
      "title": "A validity-guided workflow for robust large language model research in psychology",
      "title_zh": "ä»¥æ•ˆåº¦ä¸ºå¯¼å‘çš„å¿ƒç†å­¦ç¨³å¥å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶å·¥ä½œæµ",
      "authors": [
        "Zhicheng Lin"
      ],
      "abstract": "Large language models (LLMs) are rapidly being integrated into psychological research as research tools, evaluation targets, human simulators, and cognitive models. However, recent evidence reveals severe measurement unreliability: Personality assessments collapse under factor analysis, moral preferences reverse with punctuation changes, and theory-of-mind accuracy varies widely with trivial rephrasing. These \"measurement phantoms\"--statistical artifacts masquerading as psychological phenomena--threaten the validity of a growing body of research. Guided by the dual-validity framework that integrates psychometrics with causal inference, we present a six-stage workflow that scales validity requirements to research ambition--using LLMs to code text requires basic reliability and accuracy, while claims about psychological properties demand comprehensive construct validation. Researchers must (1) explicitly define their research goal and corresponding validity requirements, (2) develop and validate computational instruments through psychometric testing, (3) design experiments that control for computational confounds, (4) execute protocols with transparency, (5) analyze data using methods appropriate for non-independent observations, and (6) report findings within demonstrated boundaries and use results to refine theory. We illustrate the workflow through an example of model evaluation--\"LLM selfhood\"--showing how systematic validation can distinguish genuine computational phenomena from measurement artifacts. By establishing validated computational instruments and transparent practices, this workflow provides a path toward building a robust empirical foundation for AI psychology research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒç†å­¦ç ”ç©¶ä¸­é¢ä¸´çš„æµ‹é‡ä¸å¯é æ€§åŠâ€œæµ‹é‡å¹»è±¡â€ï¼ˆmeasurement phantomsï¼‰ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç”±æ•ˆåº¦å¼•å¯¼çš„ç¨³å¥ç ”ç©¶å·¥ä½œæµç¨‹ã€‚è¯¥æµç¨‹åŸºäºæ•´åˆäº†å¿ƒç†æµ‹é‡å­¦ï¼ˆpsychometricsï¼‰ä¸å› æœæ¨æ–­ï¼ˆcausal inferenceï¼‰çš„åŒé‡æ•ˆåº¦æ¡†æ¶ï¼ˆdual-validity frameworkï¼‰ï¼Œå°†æ•ˆåº¦è¦æ±‚ä¸ç ”ç©¶ç›®æ ‡ç›¸åŒ¹é…ã€‚æ ¸å¿ƒæ­¥éª¤æ¶µç›–äº†ä»æ˜ç¡®ç ”ç©¶ç›®æ ‡ä¸æ•ˆåº¦éœ€æ±‚ã€é€šè¿‡å¿ƒç†æµ‹é‡æµ‹è¯•å¼€å‘å¹¶éªŒè¯è®¡ç®—å·¥å…·ï¼Œåˆ°æ§åˆ¶è®¡ç®—æ··æ‚å› ç´ ã€é€æ˜æ‰§è¡Œåè®®ã€é‡‡ç”¨é€‚å½“çš„æ•°æ®åˆ†ææ–¹æ³•ä»¥åŠåœ¨è¯æ˜èŒƒå›´å†…æŠ¥å‘Šç»“æœçš„å®Œæ•´è¿‡ç¨‹ã€‚é€šè¿‡å¯¹â€œLLMè‡ªæˆ‘æ„è¯†â€ï¼ˆLLM selfhoodï¼‰æ¨¡å‹è¯„ä¼°çš„æ¡ˆä¾‹æ¼”ç¤ºï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†ç³»ç»Ÿæ€§éªŒè¯å¦‚ä½•æœ‰æ•ˆåŒºåˆ†çœŸå®çš„è®¡ç®—ç°è±¡ä¸æµ‹é‡ä¼ªå½±ã€‚è¯¥å·¥ä½œæµç¨‹ä¸ºAIå¿ƒç†å­¦ç ”ç©¶å»ºç«‹äº†ä¸€å¥—ç»è¿‡éªŒè¯çš„è®¡ç®—å·¥å…·å’Œé€æ˜çš„å®è·µæ ‡å‡†ï¼Œä¸ºæ„å»ºç¨³å¥çš„å®è¯ç ”ç©¶åŸºç¡€æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04491v1",
      "published_date": "2025-07-06 18:06:12 UTC",
      "updated_date": "2025-07-06 18:06:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:22.478698+00:00"
    },
    {
      "arxiv_id": "2507.04490v1",
      "title": "Dealing with Uncertainty in Contextual Anomaly Detection",
      "title_zh": "åº”å¯¹ä¸Šä¸‹æ–‡å¼‚å¸¸æ£€æµ‹ä¸­çš„ä¸ç¡®å®šæ€§",
      "authors": [
        "Luca Bindini",
        "Lorenzo Perini",
        "Stefano Nistri",
        "Jesse Davis",
        "Paolo Frasconi"
      ],
      "abstract": "Contextual anomaly detection (CAD) aims to identify anomalies in a target (behavioral) variable conditioned on a set of contextual variables that influence the normalcy of the target variable but are not themselves indicators of anomaly. In many anomaly detection tasks, there exist contextual variables that influence the normalcy of the target variable but are not themselves indicators of anomaly. In this work, we propose a novel framework for CAD, normalcy score (NS), that explicitly models both the aleatoric and epistemic uncertainties. Built on heteroscedastic Gaussian process regression, our method regards the Z-score as a random variable, providing confidence intervals that reflect the reliability of the anomaly assessment. Through experiments on benchmark datasets and a real-world application in cardiology, we demonstrate that NS outperforms state-of-the-art CAD methods in both detection accuracy and interpretability. Moreover, confidence intervals enable an adaptive, uncertainty-driven decision-making process, which may be very important in domains such as healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Contextual anomaly detection (CAD)ä¸­ä¸Šä¸‹æ–‡å˜é‡å½±å“ç›®æ ‡å˜é‡æ­£å¸¸æ€§çš„å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºnormalcy score (NS)çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºheteroscedastic Gaussian process regressionï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°å¯¹aleatoric and epistemic uncertaintiesè¿›è¡Œå»ºæ¨¡ï¼Œè§£å†³äº†å¼‚å¸¸æ£€æµ‹ä¸­çš„ä¸ç¡®å®šæ€§å¤„ç†é—®é¢˜ã€‚é€šè¿‡å°†Z-scoreè§†ä¸ºéšæœºå˜é‡ï¼ŒNSèƒ½å¤Ÿæä¾›åæ˜ å¼‚å¸¸è¯„ä¼°å¯é æ€§çš„ç½®ä¿¡åŒºé—´ã€‚åœ¨åŸºå‡†æ•°æ®é›†å’Œå¿ƒè„ç—…å­¦å®é™…åº”ç”¨ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›CADæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶æä¾›çš„ç½®ä¿¡åŒºé—´æ”¯æŒè‡ªé€‚åº”ä¸”ç”±ä¸ç¡®å®šæ€§é©±åŠ¨çš„å†³ç­–è¿‡ç¨‹ï¼Œè¿™å¯¹äºåŒ»ç–—ä¿å¥ç­‰å…³é”®é¢†åŸŸçš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04490v1",
      "published_date": "2025-07-06 18:02:11 UTC",
      "updated_date": "2025-07-06 18:02:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:23.734014+00:00"
    },
    {
      "arxiv_id": "2507.04487v4",
      "title": "LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization",
      "title_zh": "LoSiAï¼šåŸºäºå­ç½‘å®šä½ä¸ä¼˜åŒ–çš„é«˜æ•ˆé«˜ç§©å¾®è°ƒ",
      "authors": [
        "Xujia Wang",
        "Yunjia Qi",
        "Bin Xu"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training. The source code is available at https://github.com/KlozeWang/LoSiA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LoSiA (Low-Resources Subnet Integration Adaptation)ï¼Œæ—¨åœ¨è§£å†³LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•åœ¨é¢†åŸŸä¸“ä¸šåŒ–ä»»åŠ¡ä¸­å› å¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•å¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½æ¬¡ä¼˜é—®é¢˜ã€‚LoSiAé€šè¿‡æ¢¯åº¦ç¨€ç–æ€§åˆ†æ(gradient sparsity analysis)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å®šä½å…³é”®å‚æ•°ï¼Œå¹¶å°†å…¶ä½œä¸ºå­ç½‘ç»œè¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œåœ¨ä»…æ›´æ–°éƒ¨åˆ†å‚æ•°çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„High-Rankè‡ªé€‚åº”ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¨å‡ºäº†ä¼˜åŒ–ç‰ˆæœ¬LoSiA-Proï¼Œå…¶è®­ç»ƒå»¶è¿Ÿç›¸æ¯”LoRAé™ä½äº†çº¦27%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoSiAåœ¨é¢†åŸŸä¸“ä¸šåŒ–å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰æœ€çŸ­çš„è®­ç»ƒæ—¶é—´ï¼Œä¸”æ€§èƒ½è¡¨ç°æ¥è¿‘å…¨é‡å¾®è°ƒ(full fine-tuning)ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æ˜¾è‘—å‡å°‘æŒç»­è®­ç»ƒè¿‡ç¨‹ä¸­çš„é—å¿˜(forgetting)ç°è±¡ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ¨¡å‹å¾®è°ƒæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to EMNLP 2025 (Oral); 20 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.04487v4",
      "published_date": "2025-07-06 17:51:57 UTC",
      "updated_date": "2025-09-24 08:26:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:36.837514+00:00"
    },
    {
      "arxiv_id": "2507.04480v1",
      "title": "Source Attribution in Retrieval-Augmented Generation",
      "title_zh": "æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æ¥æºå½’å› ",
      "authors": [
        "Ikhtiyor Nematov",
        "Tarik Kalai",
        "Elizaveta Kuzmenko",
        "Gabriele Fugagnoli",
        "Dimitris Sacharidis",
        "Katja Hose",
        "Tomer Sagi"
      ],
      "abstract": "While attribution methods, such as Shapley values, are widely used to explain the importance of features or training data in traditional machine learning, their application to Large Language Models (LLMs), particularly within Retrieval-Augmented Generation (RAG) systems, is nascent and challenging. The primary obstacle is the substantial computational cost, where each utility function evaluation involves an expensive LLM call, resulting in direct monetary and time expenses. This paper investigates the feasibility and effectiveness of adapting Shapley-based attribution to identify influential retrieved documents in RAG. We compare Shapley with more computationally tractable approximations and some existing attribution methods for LLM. Our work aims to: (1) systematically apply established attribution principles to the RAG document-level setting; (2) quantify how well SHAP approximations can mirror exact attributions while minimizing costly LLM interactions; and (3) evaluate their practical explainability in identifying critical documents, especially under complex inter-document relationships such as redundancy, complementarity, and synergy. This study seeks to bridge the gap between powerful attribution techniques and the practical constraints of LLM-based RAG systems, offering insights into achieving reliable and affordable RAG explainability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿä¸­åº”ç”¨Shapley valuesè¿›è¡Œæºå½’å› çš„å¯è¡Œæ€§ä¸æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨è§£å†³å½’å› æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åº”ç”¨ä¸­é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚ä½œè€…ç³»ç»Ÿåœ°å°†å½’å› åŸç†åº”ç”¨äºRAGçš„æ–‡æ¡£çº§åˆ†æï¼Œé€šè¿‡æ¯”è¾ƒç²¾ç¡®Shapleyå€¼ä¸æ›´å…·è®¡ç®—å¯è¡Œæ€§çš„è¿‘ä¼¼æ–¹æ³•ï¼Œé‡åŒ–äº†è¿™äº›è¿‘ä¼¼æ‰‹æ®µåœ¨å‡å°‘æ˜‚è´µLLMäº¤äº’çš„åŒæ—¶æ¨¡æ‹Ÿå‡†ç¡®å½’å› çš„æ•ˆæœã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†è¿™äº›æ–¹æ³•åœ¨å¤„ç†æ–‡æ¡£é—´å†—ä½™ã€äº’è¡¥å’ŒååŒç­‰å¤æ‚å…³ç³»æ—¶çš„å®é™…å¯è§£é‡Šæ€§ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†å¼ºåŠ›å½’å› æŠ€æœ¯ä¸å®ç”¨RAGçº¦æŸä¹‹é—´çš„ç©ºç™½ï¼Œä¸ºå®ç°å¯é ä¸”ç»æµçš„RAGç³»ç»Ÿå¯è§£é‡Šæ€§æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04480v1",
      "published_date": "2025-07-06 17:36:45 UTC",
      "updated_date": "2025-07-06 17:36:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:32.642009+00:00"
    },
    {
      "arxiv_id": "2507.04478v1",
      "title": "Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models",
      "title_zh": "é’ˆå¯¹ Llama 3 çš„æ¨¡å‹åæ¼”æ”»å‡»ï¼šä»å¤§è¯­è¨€æ¨¡å‹ä¸­æå–ä¸ªäººèº«ä»½ä¿¡æ¯",
      "authors": [
        "Sathesh P. Sivashanmugam"
      ],
      "abstract": "Large language models (LLMs) have transformed natural language processing, but their ability to memorize training data poses significant privacy risks. This paper investigates model inversion attacks on the Llama 3.2 model, a multilingual LLM developed by Meta. By querying the model with carefully crafted prompts, we demonstrate the extraction of personally identifiable information (PII) such as passwords, email addresses, and account numbers. Our findings highlight the vulnerability of even smaller LLMs to privacy attacks and underscore the need for robust defenses. We discuss potential mitigation strategies, including differential privacy and data sanitization, and call for further research into privacy-preserving machine learning techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Metaå¼€å‘çš„Llama 3.2å¤šè¯­è¨€æ¨¡å‹ï¼Œè°ƒæŸ¥äº†æ¨¡å‹åå‘æ”»å‡»(Model Inversion Attacks)å¯¹ä¸ªäººèº«ä»½ä¿¡æ¯(PII)çš„æå–é£é™©ã€‚é€šè¿‡ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯(prompts)è¿›è¡ŒæŸ¥è¯¢ï¼Œç ”ç©¶è€…æˆåŠŸä»æ¨¡å‹ä¸­è·å–äº†å¯†ç ã€ç”µå­é‚®ä»¶åœ°å€å’Œè´¦å·ç­‰æ•æ„Ÿæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å‚æ•°é‡è¾ƒå°çš„LLMsåœ¨éšç§æ”»å‡»é¢å‰ä¹Ÿå…·æœ‰æ˜¾è‘—çš„è„†å¼±æ€§ï¼Œæ­ç¤ºäº†æ¨¡å‹è®°å¿†è®­ç»ƒæ•°æ®æ‰€å¸¦æ¥çš„ä¸¥å³»æŒ‘æˆ˜ã€‚è®ºæ–‡è¿›ä¸€æ­¥è®¨è®ºäº†å·®åˆ†éšç§(Differential Privacy)å’Œæ•°æ®æ¸…æ´—(Data Sanitization)ç­‰æ½œåœ¨çš„é˜²å¾¡ç­–ç•¥ï¼Œå¹¶å‘¼ååŠ å¼ºå¯¹éšç§ä¿æŠ¤æœºå™¨å­¦ä¹ æŠ€æœ¯çš„ç ”ç©¶ï¼Œä»¥ç¡®ä¿å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åº”ç”¨å®‰å…¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04478v1",
      "published_date": "2025-07-06 17:24:17 UTC",
      "updated_date": "2025-07-06 17:24:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:38.501233+00:00"
    },
    {
      "arxiv_id": "2507.04469v2",
      "title": "The role of large language models in UI/UX design: A systematic literature review",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ UI/UX è®¾è®¡ä¸­çš„ä½œç”¨ï¼šç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°",
      "authors": [
        "Ammar Ahmed",
        "Ali Shariq Imran"
      ],
      "abstract": "This systematic literature review examines the role of large language models (LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies published between 2022 and 2025. We identify key LLMs in use, including GPT-4, Gemini, and PaLM, and map their integration across the design lifecycle, from ideation to evaluation. Common practices include prompt engineering, human-in-the-loop workflows, and multimodal input. While LLMs are reshaping design processes, challenges such as hallucination, prompt instability, and limited explainability persist. Our findings highlight LLMs as emerging collaborators in design, and we propose directions for the ethical, inclusive, and effective integration of these technologies.",
      "tldr_zh": "è¯¥ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°ï¼ˆSystematic Literature Reviewï¼‰æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨UI/UXè®¾è®¡ä¸­çš„ä½œç”¨ï¼Œç»¼åˆåˆ†æäº†2022å¹´è‡³2025å¹´é—´çš„38é¡¹åŒè¡Œè¯„å®¡ç ”ç©¶ã€‚ç ”ç©¶è¯†åˆ«äº†GPT-4ã€Geminiå’ŒPaLMç­‰ä¸»è¦æ¨¡å‹åœ¨è®¾è®¡ç”Ÿå‘½å‘¨æœŸå„é˜¶æ®µçš„é›†æˆæƒ…å†µï¼Œæ¶µç›–äº†ä»æ„æ€åˆ°è¯„ä¼°çš„å…¨è¿‡ç¨‹ã€‚å¸¸è§çš„åº”ç”¨å®è·µåŒ…æ‹¬æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰ã€äººæœºåä½œå·¥ä½œæµï¼ˆHuman-in-the-loop workflowsï¼‰ä»¥åŠå¤šæ¨¡æ€è¾“å…¥ï¼ˆMultimodal inputï¼‰ã€‚å°½ç®¡LLMsæ­£åœ¨æ˜¾è‘—é‡å¡‘è®¾è®¡æµç¨‹ï¼Œä½†å¹»è§‰ï¼ˆHallucinationï¼‰ã€æç¤ºä¸ç¨³å®šæ€§å’Œæœ‰é™çš„å¯è§£é‡Šæ€§ï¼ˆExplainabilityï¼‰ç­‰æŒ‘æˆ˜ä¾ç„¶å­˜åœ¨ã€‚ç ”ç©¶å¼ºè°ƒäº†LLMsä½œä¸ºè®¾è®¡æ–°å…´åä½œè€…çš„æ½œåŠ›ï¼Œå¹¶ä¸ºå®ç°è¿™äº›æŠ€æœ¯åœ¨è®¾è®¡é¢†åŸŸçš„ä¼¦ç†ã€åŒ…å®¹åŠæœ‰æ•ˆé›†æˆæå‡ºäº†æœªæ¥å»ºè®®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04469v2",
      "published_date": "2025-07-06 17:18:05 UTC",
      "updated_date": "2025-07-17 19:03:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:34.809636+00:00"
    },
    {
      "arxiv_id": "2507.04464v1",
      "title": "Anomalous Decision Discovery using Inverse Reinforcement Learning",
      "title_zh": "åŸºäºé€†å¼ºåŒ–å­¦ä¹ çš„å¼‚å¸¸å†³ç­–å‘ç°",
      "authors": [
        "Ashish Bastola",
        "Mert D. PesÃ©",
        "Long Cheng",
        "Jonathon Smereka",
        "Abolfazl Razi"
      ],
      "abstract": "Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by identifying unusual behaviors through perception systems that could compromise safety and lead to hazardous situations. Current approaches, which often rely on predefined thresholds or supervised learning paradigms, exhibit reduced efficacy when confronted with unseen scenarios, sensor noise, and occlusions, leading to potential safety-critical failures. Moreover, supervised methods require large annotated datasets, limiting their real-world feasibility. To address these gaps, we propose an anomaly detection framework based on Inverse Reinforcement Learning (IRL) to infer latent driving intentions from sequential perception data, thus enabling robust identification. Specifically, we present Trajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework for anomaly detection, to address two critical limitations of existing methods: noise robustness and generalization to unseen scenarios. Our core innovation is implicitly learning temporal credit assignments via reward and worst-case supervision. We leverage pre-training with variable-horizon sampling to maximize time-to-consequence, resulting in early detection of behavior deviation. Experiments on 14,000+ simulated trajectories demonstrate state-of-the-art performance, achieving 0.90 AUC and 82.2\\% F1-score - outperforming similarly trained supervised and unsupervised baselines by 39\\% on Recall and 12\\% on F1-score, respectively. Similar performance is achieved while exhibiting robustness to various noise types and generalization to unseen anomaly types. Our code will be available at: https://github.com/abastola0/TRAP.git",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAVsï¼‰åœ¨å¤æ‚ç¯å¢ƒå’ŒæœªçŸ¥åœºæ™¯ä¸­å¼‚å¸¸æ£€æµ‹æ•ˆèƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé€†å¼ºåŒ–å­¦ä¹ ï¼ˆInverse Reinforcement Learning, IRLï¼‰çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»åºåˆ—æ„ŸçŸ¥æ•°æ®ä¸­æ¨æ–­æ½œåœ¨é©¾é©¶æ„å›¾ï¼Œæ—¨åœ¨å®ç°å¯¹å¼‚å¸¸è¡Œä¸ºçš„é²æ£’è¯†åˆ«ã€‚ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†è½¨è¿¹å¥–åŠ±å¼•å¯¼è‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆTrajectory-Reward Guided Adaptive Pre-training, TRAPï¼‰ï¼Œé€šè¿‡å¥–åŠ±å’Œæœ€åæƒ…å†µç›‘ç£éšå¼å­¦ä¹ æ—¶é—´ä¿¡ç”¨åˆ†é…ï¼ˆtemporal credit assignmentsï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å˜æ—¶ç•Œé‡‡æ ·ï¼ˆvariable-horizon samplingï¼‰æ¥æœ€å¤§åŒ–åæœæ˜¾ç°æ—¶é—´ï¼ˆtime-to-consequenceï¼‰ï¼Œä»è€Œå®ç°äº†å¯¹è¡Œä¸ºåç¦»çš„æ—©æœŸæ£€æµ‹ã€‚åœ¨è¶…è¿‡14,000æ¡æ¨¡æ‹Ÿè½¨è¿¹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTRAP è¾¾åˆ°äº† 0.90 çš„ AUC å’Œ 82.2% çš„ F1-scoreï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å’Œæ— ç›‘ç£åŸºçº¿æ¨¡å‹ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ³•å¯¹å¤šç§ä¼ æ„Ÿå™¨å™ªå£°å…·æœ‰æå¼ºçš„é²æ£’æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³ä»æœªè§è¿‡çš„å¼‚å¸¸ç±»å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04464v1",
      "published_date": "2025-07-06 17:01:02 UTC",
      "updated_date": "2025-07-06 17:01:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:42.460649+00:00"
    },
    {
      "arxiv_id": "2507.04441v3",
      "title": "The Joys of Categorical Conformal Prediction",
      "title_zh": "èŒƒç•´è®ºç¬¦åˆé¢„æµ‹ä¹‹è¶£",
      "authors": [
        "Michele Caprio"
      ],
      "abstract": "Conformal prediction (CP) is an Uncertainty Representation technique that delivers finite-sample calibrated prediction regions for any underlying Machine Learning model. Its status as an Uncertainty Quantification (UQ) tool, though, has remained conceptually opaque: While Conformal Prediction Regions (CPRs) give an ordinal representation of uncertainty (larger regions typically indicate higher uncertainty), they lack the capability to cardinally quantify it (twice as large regions do not imply twice the uncertainty). We adopt a category-theoretic approach to CP -- framing it as a morphism, embedded in a commuting diagram, of two newly-defined categories -- that brings us three joys. First, we show that -- under minimal assumptions -- CP is intrinsically a UQ mechanism, that is, its cardinal UQ capabilities are a structural feature of the method. Second, we demonstrate that CP bridges the Bayesian, frequentist, and imprecise probabilistic approaches to predictive statistical reasoning. Finally, we show that a CPR is the image of a covariant functor. This observation is relevant to AI privacy: It implies that privacy noise added locally does not break the global coverage guarantee.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡‡ç”¨èŒƒç•´è®º(category-theoretic)æ–¹æ³•å¯¹ç¬¦åˆé¢„æµ‹(Conformal Prediction, CP)è¿›è¡Œäº†é‡æ–°å®šä¹‰ï¼Œå°†å…¶æè¿°ä¸ºä¸¤ä¸ªæ–°å®šä¹‰èŒƒç•´ä¹‹é—´åµŒå…¥äº¤æ¢å›¾çš„æ€å°„(morphism)ï¼Œä»è€Œæ˜ç¡®äº†å…¶åœ¨ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification, UQ)ä¸­çš„åœ°ä½ã€‚ç ”ç©¶é¦–å…ˆè¯æ˜äº†åœ¨æå°å‡è®¾ä¸‹ï¼ŒCP çš„åŸºæ•°é‡åŒ–èƒ½åŠ›æ˜¯å…¶å›ºæœ‰çš„ç»“æ„æ€§ç‰¹å¾ï¼Œè§£å†³äº†å…¶åœ¨ç»Ÿè®¡æ¨ç†ä¸­æ¦‚å¿µæ¨¡ç³Šçš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œè¯¥æ¡†æ¶æˆåŠŸæ¡¥æ¥äº†è´å¶æ–¯(Bayesian)ã€é¢‘ç‡æ´¾(frequentist)åŠä¸ç²¾ç¡®æ¦‚ç‡(imprecise probabilistic)ç­‰ä¸åŒçš„é¢„æµ‹ç»Ÿè®¡æ¨ç†æ–¹æ³•ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºç¬¦åˆé¢„æµ‹åŒºåŸŸ(Conformal Prediction Region, CPR)æ˜¯åå˜å‡½å­(covariant functor)çš„åƒï¼Œè¿™ä¸€ç‰¹æ€§ç¡®ä¿äº†åœ¨å±€éƒ¨æ·»åŠ éšç§å™ªå£°æ—¶ä»èƒ½ç»´æŒå…¨å±€è¦†ç›–ç‡ä¿è¯ï¼Œä¸ºäººå·¥æ™ºèƒ½éšç§(AI privacy)ä¿æŠ¤æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.CT"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04441v3",
      "published_date": "2025-07-06 16:03:08 UTC",
      "updated_date": "2025-08-28 12:06:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:48.006032+00:00"
    },
    {
      "arxiv_id": "2507.04439v1",
      "title": "A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of DÃ©jÃ  Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories",
      "title_zh": "è‡ªå‘æ€§æ€ç»´çš„è¯­è¨€å­¦åˆ†æï¼šæ¢ç©¶æ—¢è§†æ„Ÿã€æ„å¤–æƒ³æ³•ä¸éè‡ªå‘æ€§è‡ªä¼ ä½“è®°å¿†",
      "authors": [
        "Videep Venkatesha",
        "Mary Cati Poulos",
        "Christopher Steadman",
        "Caitlin Mills",
        "Anne M. Cleary",
        "Nathaniel Blanchard"
      ],
      "abstract": "The onset of spontaneous thoughts are reflective of dynamic interactions between cognition, emotion, and attention. Typically, these experiences are studied through subjective appraisals that focus on their triggers, phenomenology, and emotional salience. In this work, we use linguistic signatures to investigate Deja Vu, Involuntary Autobiographical Memories and Unexpected Thoughts. Specifically, we analyze the inherent characteristics of the linguistic patterns in participant generated descriptions of these thought types. We show how, by positioning language as a window into spontaneous cognition, existing theories on these attentional states can be updated and reaffirmed. Our findings align with prior research, reinforcing that Deja Vu is a metacognitive experience characterized by abstract and spatial language, Involuntary Autobiographical Memories are rich in personal and emotionally significant detail, and Unexpected Thoughts are marked by unpredictability and cognitive disruption. This work is demonstrative of languages potential to reveal deeper insights into how internal spontaneous cognitive states manifest through expression.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨è¯­è¨€ç‰¹å¾(linguistic signatures)å¯¹æ—¢è§†æ„Ÿ(DÃ©jÃ  Vu)ã€éè‡ªæ„¿è‡ªä¼ ä½“è®°å¿†(Involuntary Autobiographical Memories)å’Œæ„å¤–æƒ³æ³•(Unexpected Thoughts)è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚é€šè¿‡æ¢è®¨å‚ä¸è€…ç”Ÿæˆçš„æ€ç»´æè¿°ä¸­çš„è¯­è¨€æ¨¡å¼ç‰¹å¾ï¼Œè¯¥ç ”ç©¶å°†è¯­è¨€ä½œä¸ºæ¢ç´¢è‡ªå‘è®¤çŸ¥(spontaneous cognition)çš„å·¥å…·ï¼Œæ—¨åœ¨éªŒè¯å¹¶æ‹“å±•å…³äºè¿™äº›æ³¨æ„åŠ›çŠ¶æ€çš„æ—¢æœ‰ç†è®ºã€‚ç»“æœè¡¨æ˜ï¼ŒDÃ©jÃ  Vu æ˜¯ä¸€ç§ä»¥æŠ½è±¡å’Œç©ºé—´è¯­è¨€ä¸ºç‰¹å¾çš„å…ƒè®¤çŸ¥(metacognitive)ä½“éªŒï¼Œè€Œ IAMs åˆ™å¯Œå«ä¸ªäººæƒ…æ„ŸåŠæ˜¾è‘—ç»†èŠ‚ï¼ŒUnexpected Thoughts åˆ™ä»¥ä¸å¯é¢„æµ‹æ€§å’Œè®¤çŸ¥ä¸­æ–­(cognitive disruption)ä¸ºæ ‡å¿—ã€‚æ­¤é¡¹å·¥ä½œå……åˆ†å±•ç¤ºäº†è¯­è¨€åœ¨æ­ç¤ºå†…éƒ¨è‡ªå‘è®¤çŸ¥çŠ¶æ€å¦‚ä½•è½¬åŒ–ä¸ºå¤–éƒ¨è¡¨è¾¾æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç†è§£è®¤çŸ¥ã€æƒ…æ„Ÿä¸æ³¨æ„åŠ›ä¹‹é—´çš„åŠ¨æ€äº¤äº’æä¾›äº†å…¨æ–°çš„è¯­è¨€å­¦è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at CogSci 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04439v1",
      "published_date": "2025-07-06 15:57:36 UTC",
      "updated_date": "2025-07-06 15:57:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:45:58.337729+00:00"
    },
    {
      "arxiv_id": "2507.04431v3",
      "title": "MedGellan: LLM-Generated Medical Guidance to Support Physicians",
      "title_zh": "MedGellanï¼šè¾…åŠ©åŒ»ç”Ÿçš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¼åŒ»ç–—æŒ‡å¯¼",
      "authors": [
        "Debodeep Banerjee",
        "Burcu Sayin",
        "Stefano Teso",
        "Andrea Passerini"
      ],
      "abstract": "Medical decision-making is a critical task, where errors can result in serious, potentially life-threatening consequences. While full automation remains challenging, hybrid frameworks that combine machine intelligence with human oversight offer a practical alternative. In this paper, we present MedGellan, a lightweight, annotation-free framework that uses a Large Language Model (LLM) to generate clinical guidance from raw medical records, which is then used by a physician to predict diagnoses. MedGellan uses a Bayesian-inspired prompting strategy that respects the temporal order of clinical data. Preliminary experiments show that the guidance generated by the LLM with MedGellan improves diagnostic performance, particularly in recall and $F_1$ score.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å†³ç­–å…¨è‡ªåŠ¨åŒ–é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠäººæœºåä½œçš„å®é™…éœ€æ±‚ï¼Œæå‡ºäº†MedGellanï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”æ— éœ€æ ‡æ³¨(annotation-free)çš„æ¡†æ¶ã€‚MedGellanåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ä»åŸå§‹åŒ»ç–—è®°å½•ä¸­ç”Ÿæˆä¸´åºŠæŒ‡å¯¼ï¼Œæ—¨åœ¨è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ›´å‡†ç¡®çš„è¯Šæ–­é¢„æµ‹ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒé‡‡ç”¨äº†å—è´å¶æ–¯å¯å‘(Bayesian-inspired)çš„æç¤ºç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆéµå¾ªä¸´åºŠæ•°æ®çš„æ—¶é—´é¡ºåºè¿›è¡Œæ¨ç†ã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼ŒMedGellanç”Ÿæˆçš„æŒ‡å¯¼æ˜¾è‘—æå‡äº†è¯Šæ–­æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¬å›ç‡(recall)å’ŒF1åˆ†æ•°(F1 score)æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¿™ä¸€æ··åˆæ™ºèƒ½æ–¹æ³•ä¸ºåœ¨åŒ»ç–—å†³ç­–ä¸­ç»“åˆæœºå™¨æ™ºèƒ½ä¸äººç±»ä¸“ä¸šåˆ¤æ–­æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04431v3",
      "published_date": "2025-07-06 15:31:01 UTC",
      "updated_date": "2025-09-09 08:03:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:01.643410+00:00"
    },
    {
      "arxiv_id": "2507.04428v1",
      "title": "ARMR: Adaptively Responsive Network for Medication Recommendation",
      "title_zh": "ARMRï¼šé¢å‘è¯ç‰©æ¨èçš„è‡ªé€‚åº”å“åº”ç½‘ç»œ",
      "authors": [
        "Feiyue Wu",
        "Tianxing Wu",
        "Shenqi Jing"
      ],
      "abstract": "Medication recommendation is a crucial task in healthcare, especially for patients with complex medical conditions. However, existing methods often struggle to effectively balance the reuse of historical medications with the introduction of new drugs in response to the changing patient conditions. In order to address this challenge, we propose an Adaptively Responsive network for Medication Recommendation (ARMR), a new method which incorporates 1) a piecewise temporal learning component that distinguishes between recent and distant patient history, enabling more nuanced temporal understanding, and 2) an adaptively responsive mechanism that dynamically adjusts attention to new and existing drugs based on the patient's current health state and medication history. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR has better performance compared with the state-of-the-art baselines in different evaluation metrics, which contributes to more personalized and accurate medication recommendations. The source code is publicly avaiable at: https://github.com/seucoin/armr2.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ARMRï¼ˆAdaptively Responsive network for Medication Recommendationï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯ç‰©æ¨èæ–¹æ³•åœ¨å¹³è¡¡å†å²è¯ç‰©å¤ç”¨ä¸åº”å¯¹ç—…æƒ…å˜åŒ–å¼•å…¥æ–°è¯æ–¹é¢çš„å±€é™ã€‚ARMRåŒ…å«ä¸€ä¸ªåˆ†æ®µå¼æ—¶é—´å­¦ä¹ ç»„ä»¶(piecewise temporal learning component)ï¼Œé€šè¿‡åŒºåˆ†è¿‘æœŸä¸è¿œæœŸç—…å²æ¥å®ç°æ›´ç»†è‡´çš„æ—¶é—´ç»´åº¦ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å“åº”æœºåˆ¶(adaptively responsive mechanism)ï¼Œèƒ½å¤Ÿæ ¹æ®æ‚£è€…å½“å‰å¥åº·çŠ¶æ€å’Œç”¨è¯å†å²åŠ¨æ€è°ƒæ•´å¯¹æ–°è¯åŠæ—¢æœ‰è¯ç‰©çš„å…³æ³¨æƒé‡ã€‚åœ¨MIMIC-IIIå’ŒMIMIC-IVæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒARMRåœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„SOTAåŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆæœä¸ºå®ç°æ›´å…·ä¸ªæ€§åŒ–å’Œå‡†ç¡®æ€§çš„è¯ç‰©æ¨èæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04428v1",
      "published_date": "2025-07-06 15:24:00 UTC",
      "updated_date": "2025-07-06 15:24:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:10.473159+00:00"
    },
    {
      "arxiv_id": "2507.04422v2",
      "title": "Learning Software Bug Reports: A Systematic Literature Review",
      "title_zh": "è½¯ä»¶ç¼ºé™·æŠ¥å‘Šå­¦ä¹ ï¼šç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°",
      "authors": [
        "Guoming Long",
        "Jingzhi Gong",
        "Hui Fang",
        "Tao Chen"
      ],
      "abstract": "The recent advancement of artificial intelligence, especially machine learning (ML), has significantly impacted software engineering research, including bug report analysis. ML aims to automate the understanding, extraction, and correlation of information from bug reports. Despite its growing importance, there has been no comprehensive review in this area. In this paper, we present a systematic literature review covering 1,825 papers, selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular for feature representation, with a rise in deep learning approaches. 3) Stop word removal is the most common preprocessing, with structural methods rising after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software projects. 5) Bug categorization is the most common task, followed by bug localization and severity prediction. 6) There is increasing attention on specific bugs like non-functional and performance bugs. 7) Common evaluation metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold cross-validation preferred for model evaluation. 8) Many studies lack robust statistical tests. We also identify six promising future research directions to provide useful insights for practitioners.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°(Systematic Literature Review)å¯¹åˆ©ç”¨æœºå™¨å­¦ä¹ (Machine Learning)å¤„ç†è½¯ä»¶ç¼ºé™·æŠ¥å‘Š(Bug Reports)çš„é¢†åŸŸè¿›è¡Œäº†å…¨é¢åˆ†æï¼Œä»1825ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º204ç¯‡è¿›è¡Œæ·±åº¦è§£æã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡CNNã€LSTMå’ŒkNNç­‰æ¨¡å‹åº”ç”¨å¹¿æ³›ï¼Œä½†BERTç­‰å…ˆè¿›æ¨¡å‹å› å¤æ‚æ€§å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œç‰¹å¾è¡¨ç¤ºåˆ™ä¸»è¦ä¾èµ–Word2Vecå’ŒTF-IDFã€‚åœ¨é¢„å¤„ç†å’Œè¯„ä¼°æ–¹é¢ï¼Œåœç”¨è¯ç§»é™¤(Stop word removal)æœ€ä¸ºå¸¸è§ï¼Œè€ŒEclipseå’ŒMozillaæ˜¯ä½¿ç”¨é¢‘ç‡æœ€é«˜çš„å®éªŒé¡¹ç›®ã€‚ç¼ºé™·åˆ†ç±»(Bug categorization)æ˜¯ç›®å‰æœ€ä¸»è¦çš„ä»»åŠ¡ï¼Œä¸”ç ”ç©¶é‡å¿ƒæ­£å‘éåŠŸèƒ½æ€§(Non-functional)åŠæ€§èƒ½ç¼ºé™·(Performance bugs)è½¬ç§»ã€‚å°½ç®¡F1-scoreå’Œå‡†ç¡®ç‡(Accuracy)è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†å¤šæ•°ç ”ç©¶åœ¨ç»Ÿè®¡æ£€éªŒ(Statistical tests)çš„ç¨³å¥æ€§ä¸Šä»æœ‰æ¬ ç¼ºã€‚è¯¥ç»¼è¿°æœ€åæå‡ºäº†å…­ä¸ªæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºè‡ªåŠ¨åŒ–è½¯ä»¶ç¼ºé™·åˆ†ææä¾›äº†é‡è¦çš„å‚è€ƒä¸æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by TOSEM",
      "pdf_url": "https://arxiv.org/pdf/2507.04422v2",
      "published_date": "2025-07-06 15:17:59 UTC",
      "updated_date": "2025-07-20 11:18:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:11.036391+00:00"
    },
    {
      "arxiv_id": "2507.13363v1",
      "title": "Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop",
      "title_zh": "ä»…éœ€å‡ ä½•ï¼šæ— éœ€äººå·¥å‚ä¸çš„æ— æ¢¯åº¦å¼€æ”¾è¯æ±‡ 3D æ£€æµ‹",
      "authors": [
        "Atharv Goel",
        "Mehar Khurana"
      ],
      "abstract": "Modern 3D object detection datasets are constrained by narrow class taxonomies and costly manual annotations, limiting their ability to scale to open-world settings. In contrast, 2D vision-language models trained on web-scale image-text pairs exhibit rich semantic understanding and support open-vocabulary detection via natural language prompts. In this work, we leverage the maturity and category diversity of 2D foundation models to perform open-vocabulary 3D object detection without any human-annotated 3D labels.\n  Our pipeline uses a 2D vision-language detector to generate text-conditioned proposals, which are segmented with SAM and back-projected into 3D using camera geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D bounding boxes without training. To simulate adverse real-world conditions, we construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes dataset.\n  Experiments demonstrate that our method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary. Our results highlight the untapped potential of 2D foundation models for scalable 3D perception. We open-source our code and resources at https://github.com/atharv0goel/open-world-3D-det.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æˆç†Ÿçš„ 2D vision-language models åœ¨æ— éœ€ä»»ä½•äººå·¥ 3D æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç° open-vocabulary 3D object detectionã€‚å…¶å·¥ä½œæµç¨‹é¦–å…ˆä½¿ç”¨ 2D è§†è§‰è¯­è¨€æ£€æµ‹å™¨ç”Ÿæˆæ–‡æœ¬å¼•å¯¼çš„å»ºè®®æ¡†ï¼Œåˆ©ç”¨ SAM è¿›è¡Œç²¾ç¡®åˆ†å‰²åï¼Œé€šè¿‡ç›¸æœºå‡ ä½•ç»“æ„ç»“åˆ LiDAR æˆ– monocular pseudo-depth å°†ç»“æœåæŠ•å½±è‡³ 3D ç©ºé—´ã€‚ç ”ç©¶å¼•å…¥äº†åŸºäº DBSCAN èšç±»å’Œ Rotating Calipers çš„å‡ ä½•è†¨èƒ€ç­–ç•¥ï¼Œä»è€Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹ç›´æ¥æ¨æ–­å‡º 3D bounding boxesã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ„å»ºäº†ç»è¿‡é›¾åŒ–å¢å¼ºçš„ Pseudo-nuScenes æ•°æ®é›†ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ¶åŠ£æ¡ä»¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºäº LiDAR å’Œçº¯ RGB-D è¾“å…¥çš„å¤šç§è®¾ç½®ä¸‹å‡å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„å®šä½æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº† 2D åŸºç¡€æ¨¡å‹åœ¨å¯æ‰©å±• 3D æ„ŸçŸ¥ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶æˆåŠŸå®ç°äº†å®Œå…¨çš„ training-free å’Œå¼€æ”¾è¯æ±‡æ£€æµ‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13363v1",
      "published_date": "2025-07-06 15:00:13 UTC",
      "updated_date": "2025-07-06 15:00:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:16.107255+00:00"
    },
    {
      "arxiv_id": "2507.04410v1",
      "title": "Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“æ·±åº¦ç ”ç©¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¤šåª’ä½“éªŒè¯",
      "authors": [
        "Huy Hoan Le",
        "Van Sy Thinh Nguyen",
        "Thi Le Chi Dang",
        "Vo Thanh Khang Nguyen",
        "Truong Thanh Hung Nguyen",
        "Hung Cao"
      ],
      "abstract": "This paper presents our submission to the ACMMM25 - Grand Challenge on Multimedia Verification. We developed a multi-agent verification system that combines Multimodal Large Language Models (MLLMs) with specialized verification tools to detect multimedia misinformation. Our system operates through six stages: raw data processing, planning, information extraction, deep research, evidence collection, and report generation. The core Deep Researcher Agent employs four tools: reverse image search, metadata analysis, fact-checking databases, and verified news processing that extracts spatial, temporal, attribution, and motivational context. We demonstrate our approach on a challenge dataset sample involving complex multimedia content. Our system successfully verified content authenticity, extracted precise geolocation and timing information, and traced source attribution across multiple platforms, effectively addressing real-world multimedia verification scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸ä¸“ä¸šéªŒè¯å·¥å…·çš„å¤šæ™ºèƒ½ä½“éªŒè¯ç³»ç»Ÿï¼Œæ—¨åœ¨æ£€æµ‹å¤šåª’ä½“è™šå‡ä¿¡æ¯ã€‚è¯¥ç³»ç»Ÿæ¶µç›–åŸå§‹æ•°æ®å¤„ç†ã€è§„åˆ’ã€ä¿¡æ¯æå–ã€æ·±åº¦ç ”ç©¶ã€è¯æ®æ”¶é›†å’ŒæŠ¥å‘Šç”Ÿæˆå…­ä¸ªé˜¶æ®µã€‚å…¶æ ¸å¿ƒ Deep Researcher Agent é›†æˆäº†åå‘å›¾åƒæœç´¢ã€å…ƒæ•°æ®åˆ†æã€äº‹å®æ ¸æŸ¥æ•°æ®åº“å’Œç»è¿‡éªŒè¯çš„æ–°é—»å¤„ç†å·¥å…·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå–ç©ºé—´ã€æ—¶é—´ã€å½’å› å’ŒåŠ¨æœºç­‰ç»´åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨ ACMMM25 æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®éªŒè¯å†…å®¹çœŸå®æ€§ï¼Œæå–ç²¾ç¡®çš„åœ°ç†å®šä½ä¸æ—¶é—´ä¿¡æ¯ï¼Œå¹¶å®ç°è·¨å¹³å°çš„æ¥æºæº¯æºï¼Œä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­çš„å¤æ‚å¤šåª’ä½“éªŒè¯é—®é¢˜æä¾›äº†å¯é æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "33rd ACM International Conference on Multimedia (MM'25) Grand Challenge on Multimedia Verification",
      "pdf_url": "https://arxiv.org/pdf/2507.04410v1",
      "published_date": "2025-07-06 14:54:07 UTC",
      "updated_date": "2025-07-06 14:54:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:15.600930+00:00"
    },
    {
      "arxiv_id": "2507.04404v2",
      "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers",
      "title_zh": "LayerCakeï¼šå¤§è¯­è¨€æ¨¡å‹å±‚å†…çš„è¯å…ƒæ„ŸçŸ¥å¯¹æ¯”è§£ç ",
      "authors": [
        "Jingze Zhu",
        "Yongliang Wu",
        "Wenbo Zhu",
        "Jiawang Cao",
        "Yanqiang Zheng",
        "Jiawei Chen",
        "Xu Yang",
        "Bernt Schiele",
        "Jonas Fischer",
        "Xinting Hu"
      ],
      "abstract": "Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LayerCakeï¼Œä¸€ç§é¢å‘å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„ä»¤ç‰Œæ„ŸçŸ¥ã€å±‚å®šä½å¯¹æ¯”è§£ç  (token-aware, layer-localized contrastive decoding) æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘äº‹å®æ€§é”™è¯¯å¹¶æå‡çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å¯é æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç‰¹å®šä»¤ç‰Œç±»å‹ä¸å…¶æœ€å…·å½±å“åŠ›çš„ Transformer å±‚å¯¹é½ï¼Œå…‹æœäº†ç°æœ‰æŠ€æœ¯å­¤ç«‹å¤„ç†ä»¤ç‰Œçº§ä¸å±‚çº§ä¿¡å·çš„å±€é™ã€‚é€šè¿‡æ³¨æ„åŠ›åˆ†æï¼Œç ”ç©¶è€…å‘ç°æ ‡ç‚¹ç¬¦å·ä»¤ç‰Œ (punctuation tokens) åœ¨æ—©æœŸå±‚å æ®ä¸»å¯¼ï¼Œè€Œæ¦‚å¿µä»¤ç‰Œ (conceptual tokens) åˆ™åœ¨ä¸­æœŸå±‚ä¸»å¯¼è¯­ä¹‰æ¨ç†ã€‚LayerCake åœ¨ç‰¹å®šæ·±åº¦é€‰æ‹©æ€§åœ°æŠ‘åˆ¶è¿™äº›ä»¤ç‰Œç±»å‹çš„æ³¨æ„åŠ›ä»¥è¯±å¯¼å—æ§çš„äº‹å®é€€åŒ– (factual degradation)ï¼Œå¹¶æ®æ­¤äº§ç”Ÿå¯¹æ¯”ä¿¡å·æ¥å¼•å¯¼æœ€ç»ˆçš„äº‹å®è§£ç ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒæˆ–æ¨¡å‹ä¿®æ”¹ï¼Œåœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­å‡èƒ½ä¸€è‡´ä¸”æ˜¾è‘—åœ°æå‡ç”Ÿæˆçš„äº‹å®æ€§ (factuality)ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The submission was made before undergoing the required review by the co-authors' affiliated institutions. We are withdrawing the paper to allow for the completion of the institutional review process",
      "pdf_url": "https://arxiv.org/pdf/2507.04404v2",
      "published_date": "2025-07-06 14:35:43 UTC",
      "updated_date": "2025-10-03 11:24:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:20.439189+00:00"
    },
    {
      "arxiv_id": "2507.04395v1",
      "title": "SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive",
      "title_zh": "SpiritRAGï¼šé¢å‘è”åˆå›½æ¡£æ¡ˆä¸­å®—æ•™ä¸çµæ€§å†…å®¹çš„é—®ç­”ç³»ç»Ÿ",
      "authors": [
        "Yingqiang Gao",
        "Fabian Winiger",
        "Patrick Montjourides",
        "Anastassia Shaitarova",
        "Nianlong Gu",
        "Simon Peng-Keller",
        "Gerold Schneider"
      ],
      "abstract": "Religion and spirituality (R/S) are complex and highly domain-dependent concepts which have long confounded researchers and policymakers. Due to their context-specificity, R/S are difficult to operationalize in conventional archival search strategies, particularly when datasets are very large, poorly accessible, and marked by information noise. As a result, considerable time investments and specialist knowledge is often needed to extract actionable insights related to R/S from general archival sources, increasing reliance on published literature and manual desk reviews. To address this challenge, we present SpiritRAG, an interactive Question Answering (Q&A) system based on Retrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN) resolution documents related to R/S in the domains of health and education, SpiritRAG allows researchers and policymakers to conduct complex, context-sensitive database searches of very large datasets using an easily accessible, chat-based web interface. SpiritRAG is lightweight to deploy and leverages both UN documents and user provided documents as source material. A pilot test and evaluation with domain experts on 100 manually composed questions demonstrates the practical value and usefulness of SpiritRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®—æ•™ä¸çµæ€§ (Religion and Spirituality, R/S) åœ¨å¤§å‹æ¡£æ¡ˆä¸­éš¾ä»¥æ“ä½œåŒ–ä¸”æ£€ç´¢æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº† SpiritRAGã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) çš„äº¤äº’å¼é—®ç­” (Q&A) ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºå¤„ç†å¤æ‚çš„ã€å…·æœ‰é¢†åŸŸä¾èµ–æ€§çš„ R/S æ¦‚å¿µã€‚ç³»ç»Ÿåˆ©ç”¨äº† 7,500 ä»½æ¶‰åŠå¥åº·ä¸æ•™è‚²é¢†åŸŸçš„è”åˆå›½å†³è®®æ–‡ä»¶ï¼Œé€šè¿‡ç®€æ´çš„èŠå¤©å¼ Web ç•Œé¢ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œæ”¿ç­–åˆ¶å®šè€…è¿›è¡Œè¯­å¢ƒæ•æ„Ÿçš„å¤§è§„æ¨¡æ•°æ®é›†æœç´¢ã€‚SpiritRAG å…·å¤‡è½»é‡åŒ–éƒ¨ç½²ä¼˜åŠ¿ï¼Œå¹¶æ”¯æŒç»“åˆè”åˆå›½å®˜æ–¹æ–‡æ¡£ä¸ç”¨æˆ·è‡ªå®šä¹‰æ–‡æ¡£ä½œä¸ºçŸ¥è¯†æºã€‚é€šè¿‡é¢†åŸŸä¸“å®¶å¯¹ 100 ä¸ªé—®é¢˜çš„è¯•ç‚¹æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨æå–å¯è¡Œæ€§æ´è§æ–¹é¢çš„å®é™…ä»·å€¼ã€‚è¯¥å·¥å…·æ˜¾è‘—å‡å°‘äº†å¯¹ä¼ ç»Ÿæ‰‹åŠ¨æ¡Œé¢å®¡æŸ¥çš„ä¾èµ–ï¼Œä¸ºå¤æ‚é¢†åŸŸçŸ¥è¯†çš„è‡ªåŠ¨åŒ–æ¡£æ¡ˆæ£€ç´¢æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04395v1",
      "published_date": "2025-07-06 13:54:54 UTC",
      "updated_date": "2025-07-06 13:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:33.569282+00:00"
    },
    {
      "arxiv_id": "2507.04385v2",
      "title": "Tractable Representation Learning with Probabilistic Circuits",
      "title_zh": "åŸºäºæ¦‚ç‡ç”µè·¯çš„æ˜“å¤„ç†è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Steven Braun",
        "Sahil Sidheekh",
        "Antonio Vergari",
        "Martin Mundt",
        "Sriraam Natarajan",
        "Kristian Kersting"
      ],
      "abstract": "Probabilistic circuits (PCs) are powerful probabilistic models that enable exact and tractable inference, making them highly suitable for probabilistic reasoning and inference tasks. While dominant in neural networks, representation learning with PCs remains underexplored, with prior approaches relying on external neural embeddings or activation-based encodings. To address this gap, we introduce autoencoding probabilistic circuits (APCs), a novel framework leveraging the tractability of PCs to model probabilistic embeddings explicitly. APCs extend PCs by jointly modeling data and embeddings, obtaining embedding representations through tractable probabilistic inference. The PC encoder allows the framework to natively handle arbitrary missing data and is seamlessly integrated with a neural decoder in a hybrid, end-to-end trainable architecture enabled by differentiable sampling. Our empirical evaluation demonstrates that APCs outperform existing PC-based autoencoding methods in reconstruction quality, generate embeddings competitive with, and exhibit superior robustness in handling missing data compared to neural autoencoders. These results highlight APCs as a powerful and flexible representation learning method that exploits the probabilistic inference capabilities of PCs, showing promising directions for robust inference, out-of-distribution detection, and knowledge distillation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªç¼–ç æ¦‚ç‡ç”µè·¯(APCs)ï¼Œæ—¨åœ¨åˆ©ç”¨æ¦‚ç‡ç”µè·¯(PCs)çš„æ¨æ–­æ˜“å¤„ç†æ€§(Tractability)æ˜¾å¼å»ºæ¨¡æ¦‚ç‡åµŒå…¥(Probabilistic Embeddings)ã€‚APCsé€šè¿‡è”åˆå»ºæ¨¡æ•°æ®ä¸åµŒå…¥ï¼Œåˆ©ç”¨PCç¼–ç å™¨å®ç°å¯¹ä»»æ„ç¼ºå¤±æ•°æ®çš„åŸç”Ÿå¤„ç†ï¼Œå¹¶ç»“åˆå¯å¾®é‡‡æ ·æŠ€æœ¯å°†PCç¼–ç å™¨ä¸ç¥ç»è§£ç å™¨æ•´åˆä¸ºç«¯åˆ°ç«¯å¯è®­ç»ƒçš„æ··åˆæ¶æ„ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒAPCsåœ¨é‡æ„è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„åŸºäºPCçš„è‡ªç¼–ç æ–¹æ³•ï¼Œå…¶ç”Ÿæˆçš„åµŒå…¥è¡¨ç¤ºå…·æœ‰ç«äº‰åŠ›ï¼Œä¸”åœ¨å¤„ç†ç¼ºå¤±æ•°æ®æ—¶çš„é²æ£’æ€§æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç¥ç»è‡ªç¼–ç å™¨ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†APCsåœ¨ç¨³å¥æ¨ç†ã€åˆ†å¸ƒå¤–(Out-of-distribution)æ£€æµ‹åŠçŸ¥è¯†è’¸é¦ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºç»“åˆæ¦‚ç‡æ¨ç†ä¸è¡¨ç¤ºå­¦ä¹ æä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04385v2",
      "published_date": "2025-07-06 13:17:16 UTC",
      "updated_date": "2025-07-26 11:51:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:28.439627+00:00"
    },
    {
      "arxiv_id": "2507.04381v1",
      "title": "DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting",
      "title_zh": "DC-Mamberï¼šåŸºäº Mamba å’Œçº¿æ€§ Transformer çš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹åŒé€šé“æ¨¡å‹",
      "authors": [
        "Bing Fan",
        "Shusen Ma",
        "Yun-Bo Zhao",
        "Yu Kang"
      ],
      "abstract": "In multivariate time series forecasting (MTSF), existing strategies for processing sequences are typically categorized as channel-independent and channel-mixing. The former treats all temporal information of each variable as a token, focusing on capturing local temporal features of individual variables, while the latter constructs a token from the multivariate information at each time step, emphasizing the modeling of global temporal dependencies. Current mainstream models are mostly based on Transformer and the emerging Mamba. Transformers excel at modeling global dependencies through self-attention mechanisms but exhibit limited sensitivity to local temporal patterns and suffer from quadratic computational complexity, restricting their efficiency in long-sequence processing. In contrast, Mamba, based on state space models (SSMs), achieves linear complexity and efficient long-range modeling but struggles to aggregate global contextual information in parallel. To overcome the limitations of both models, we propose DC-Mamber, a dual-channel forecasting model based on Mamba and linear Transformer for time series forecasting. Specifically, the Mamba-based channel employs a channel-independent strategy to extract intra-variable features, while the Transformer-based channel adopts a channel-mixing strategy to model cross-timestep global dependencies. DC-Mamber first maps the raw input into two distinct feature representations via separate embedding layers. These representations are then processed by a variable encoder (built on Mamba) and a temporal encoder (built on linear Transformer), respectively. Finally, a fusion layer integrates the dual-channel features for prediction. Extensive experiments on eight public datasets confirm DC-Mamber's superior accuracy over existing models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DC-Mamberï¼Œä¸€ç§ç»“åˆMambaå’ŒLinear Transformerçš„åŒé€šé“é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹(MTSF)çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­Transformerè®¡ç®—å¤æ‚åº¦é«˜ä»¥åŠMambaéš¾ä»¥å¹¶è¡Œèšåˆå…¨å±€ä¸Šä¸‹æ–‡çš„å±€é™æ€§ï¼Œè¯¥æ¨¡å‹é›†æˆå¹¶æ”¹è¿›äº†é€šé“ç‹¬ç«‹(channel-independent)ä¸é€šé“æ··åˆ(channel-mixing)ä¸¤ç§ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼ŒDC-Mamberåˆ©ç”¨åŸºäºMambaçš„é€šé“æå–å˜é‡å†…éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶é‡‡ç”¨åŸºäºLinear Transformerçš„é€šé“å»ºæ¨¡è·¨æ—¶é—´æ­¥çš„å…¨å±€ä¾èµ–ã€‚ç³»ç»Ÿé¦–å…ˆé€šè¿‡ç‹¬ç«‹çš„åµŒå…¥å±‚å°†è¾“å…¥æ˜ å°„ä¸ºä¸¤ç§ç‰¹å¾è¡¨ç¤ºï¼Œéšååˆ†åˆ«ç”±å˜é‡ç¼–ç å™¨å’Œæ—¶é—´ç¼–ç å™¨å¤„ç†ï¼Œæœ€åé€šè¿‡èåˆå±‚æ•´åˆåŒé€šé“ç‰¹å¾ä»¥å®ç°ç²¾å‡†é¢„æµ‹ã€‚åœ¨å…«ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒDC-Mamberåœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸»æµæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04381v1",
      "published_date": "2025-07-06 12:58:52 UTC",
      "updated_date": "2025-07-06 12:58:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:32.255974+00:00"
    },
    {
      "arxiv_id": "2507.04380v1",
      "title": "Transferring Visual Explainability of Self-Explaining Models through Task Arithmetic",
      "title_zh": "åŸºäºä»»åŠ¡ç®—æœ¯çš„è‡ªè§£é‡Šæ¨¡å‹è§†è§‰å¯è§£é‡Šæ€§è¿ç§»",
      "authors": [
        "Yuya Yoshikawa",
        "Ryotaro Shimizu",
        "Takahiro Kawashima",
        "Yuki Saito"
      ],
      "abstract": "In scenarios requiring both prediction and explanation efficiency for image classification, self-explaining models that perform both tasks in a single inference are effective. However, their training incurs substantial labeling and computational costs. This study aims to tackle the issue by proposing a method to transfer the visual explainability of self-explaining models, learned in a source domain, to a target domain based on a task arithmetic framework. Specifically, we construct a self-explaining model by extending image classifiers based on a vision-language pretrained model. We then define an \\emph{explainability vector} as the difference between model parameters trained on the source domain with and without explanation supervision. Based on the task arithmetic framework, we impart explainability to a model trained only on the prediction task in the target domain by applying the explainability vector. Experimental results on various image classification datasets demonstrate that, except for transfers between some less-related domains, visual explainability can be successfully transferred from source to target domains, improving explanation quality in the target domain without sacrificing classification accuracy. Furthermore, we show that the explainability vector learned on a large and diverse dataset like ImageNet, extended with explanation supervision, exhibits universality and robustness, improving explanation quality on nine out of ten different target datasets. We also find that the explanation quality achieved with a single model inference is comparable to that of Kernel SHAP, which requires 150 model inferences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»»åŠ¡ç®—æœ¯(task arithmetic)æ¡†æ¶çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†è‡ªè§£é‡Šæ¨¡å‹(self-explaining models)åœ¨æºé¢†åŸŸå­¦ä¹ åˆ°çš„è§†è§‰å¯è§£é‡Šæ€§è¿ç§»è‡³ç›®æ ‡é¢†åŸŸï¼Œä»¥è§£å†³æ­¤ç±»æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡æ‰©å±•åŸºäºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹(vision-language pretrained model)çš„åˆ†ç±»å™¨ï¼Œå®šä¹‰äº†æºé¢†åŸŸæœ‰æ— è§£é‡Šç›‘ç£ä¹‹é—´çš„æ¨¡å‹å‚æ•°å·®å¼‚ä¸ºâ€œå¯è§£é‡Šæ€§å‘é‡â€(explainability vector)ï¼Œå¹¶å°†å…¶åº”ç”¨äºç›®æ ‡é¢†åŸŸçš„é¢„æµ‹æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸ŠæˆåŠŸå®ç°äº†å¯è§£é‡Šæ€§çš„è¿ç§»ï¼Œåœ¨æå‡è§£é‡Šè´¨é‡çš„åŒæ—¶æœªç‰ºç‰²åˆ†ç±»å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨ImageNetç­‰å¤§å‹å¤šæ ·åŒ–æ•°æ®é›†ä¸Šæå–çš„å¯è§£é‡Šæ€§å‘é‡è¡¨ç°å‡ºæ˜¾è‘—çš„é€šç”¨æ€§ä¸é²æ£’æ€§ï¼Œåœ¨å¤šæ•°ç›®æ ‡ä»»åŠ¡ä¸­å‡èƒ½æœ‰æ•ˆæå‡æ€§èƒ½ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè¯¥æ–¹æ³•ä»…éœ€å•æ¬¡æ¨¡å‹æ¨ç†å³å¯è¾¾åˆ°ä¸éœ€è¦150æ¬¡æ¨ç†çš„Kernel SHAPç›¸å½“çš„è§£é‡Šè´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04380v1",
      "published_date": "2025-07-06 12:55:31 UTC",
      "updated_date": "2025-07-06 12:55:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:48.080904+00:00"
    },
    {
      "arxiv_id": "2507.04376v2",
      "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Intelligence Agents",
      "title_zh": "MOD-Xï¼šé¢å‘å¼‚æ„å¯äº’æ“ä½œäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“çš„æ¨¡å—åŒ–å¼€æ”¾å¼å»ä¸­å¿ƒåŒ–äº¤æ¢æ¡†æ¶æ–¹æ¡ˆ",
      "authors": [
        "Georgios Ioannides",
        "Christos Constantinou",
        "Vinija Jain",
        "Aman Chadha",
        "Aaron Elkins"
      ],
      "abstract": "As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent interoperability that addresses key limitations of existing protocols. Unlike current approaches, MOD-X proposes a layered architecture with a Universal Message Bus, thorough state management, translation capabilities, and blockchain-based security mechanisms. We present MOD-X's architecture, compare it with existing protocols, and demonstrate its application through a worked example how it enables integration between heterogeneous specialist agents (agents with different architectures, vendors, capabilities, and knowledge representations--including rule-based systems, neural networks, symbolic reasoning engines, and legacy software with agent wrappers). MOD-X's key innovations include a publish-subscribe communication model, semantic capability discovery, and dynamic workflow orchestration--providing a framework that bridges theoretical formalism with practical implementation. This architecture addresses the growing need for truly decentralized, interoperable agent ecosystems that can scale effectively without the need for central coordination.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOD-Xï¼ˆModular Open Decentralized eXchangeï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¼‚æ„äº’æ“ä½œäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“è®¾è®¡çš„æ¨¡å—åŒ–å¼€æ”¾å»ä¸­å¿ƒåŒ–äº¤æ¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ AI ç”Ÿæ€ç³»ç»Ÿä»å•ä¸€æ¨¡å‹å‘ä¸“ä¸šåŒ–æ™ºèƒ½ä½“ç¾¤æ¼”è¿›è¿‡ç¨‹ä¸­å¯¹æ ‡å‡†åŒ–é€šä¿¡åè®®çš„è¿«åˆ‡éœ€æ±‚ã€‚ä¸åŒäºç°æœ‰åè®®ï¼ŒMOD-X é‡‡ç”¨äº†åŒ…å« Universal Message Busã€å®Œæ•´çŠ¶æ€ç®¡ç†ã€ç¿»è¯‘åŠŸèƒ½åŠåŸºäºåŒºå—é“¾çš„å®‰å…¨æœºåˆ¶çš„å±‚çº§åŒ–æ¶æ„ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº† publish-subscribe é€šä¿¡æ¨¡å‹ã€è¯­ä¹‰èƒ½åŠ›å‘ç°ï¼ˆsemantic capability discoveryï¼‰ä»¥åŠåŠ¨æ€å·¥ä½œæµç¼–æ’ï¼ˆdynamic workflow orchestrationï¼‰ï¼Œæœ‰æ•ˆåœ°è¿æ¥äº†ç†è®ºå½¢å¼åŒ–ä¸å®é™…åº”ç”¨ã€‚ç ”ç©¶é€šè¿‡å®ä¾‹å±•ç¤ºäº† MOD-X å¦‚ä½•å®ç°åŒ…æ‹¬åŸºäºè§„åˆ™çš„ç³»ç»Ÿã€ç¥ç»ç½‘ç»œã€ç¬¦å·æ¨ç†å¼•æ“åŠå¸¦æœ‰æ™ºèƒ½ä½“åŒ…è£…å™¨çš„é—ç•™è½¯ä»¶åœ¨å†…çš„å¼‚æ„ä¸“å®¶æ™ºèƒ½ä½“ä¹‹é—´çš„æ— ç¼é›†æˆã€‚é€šè¿‡è§£å†³å»ä¸­å¿ƒåŒ–ä¸äº’æ“ä½œæ€§é—®é¢˜ï¼ŒMOD-X ä¸ºæ— éœ€ä¸­å¤®åè°ƒçš„å¤§è§„æ¨¡ã€å¯æ‰©å±•æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿæä¾›äº†å…³é”®çš„æ¶æ„æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.MA",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04376v2",
      "published_date": "2025-07-06 12:46:57 UTC",
      "updated_date": "2025-07-08 02:48:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:48.212897+00:00"
    },
    {
      "arxiv_id": "2507.04370v1",
      "title": "WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis",
      "title_zh": "WebSynthesisï¼šåŸºäºä¸–ç•Œæ¨¡å‹å¼•å¯¼ MCTS çš„é«˜æ•ˆ WebUI è½¨è¿¹åˆæˆ",
      "authors": [
        "Yifei Gao",
        "Junhong Ye",
        "Jiaqi Wang",
        "Jitao Sang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have significantly improved the capabilities of web agents. However, effectively navigating complex and dynamic web environments still requires more advanced trajectory-level planning and execution. Prior studies have addressed self-improving agents by collecting extensive GUI trajectories from real-environment interactions. Despite their effectiveness, these approaches encounter two critical challenges: (1) Uncontrollable environment states, where real or sandboxed web environments often yield unstable and non-deterministic feedback, complicating the reproduction and debugging of agent behaviors; and (2) High API costs, as generating even a single interaction trajectory can involve hundreds of queries, leading to considerable API usage and computational expenses. To address these limitations and enable scalable self-improvement for agents, we propose WebSynthesis, a novel framework for trajectory synthesis and training. WebSynthesis leverages a learned world model to simulate virtual web environments, allowing a policy agent to perform efficient and reversible tree-based planning. This approach supports the large-scale generation of diverse and high-quality trajectories, which are subsequently utilized to refine the agent's policy. Experimental results demonstrate that an agent trained using WebSynthesis on a small-scale synthetic dataset achieves performance comparable to or even surpassing that of models trained on large-scale real-world data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Large Language Models (LLMs)é©±åŠ¨çš„Web agentsåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹é¢ä¸´çš„è½¨è¿¹è§„åˆ’æŒ‘æˆ˜ï¼Œä»¥åŠè·å–çœŸå®GUIè½¨è¿¹æ—¶å­˜åœ¨çš„ç¯å¢ƒä¸å¯æ§å’ŒAPI costsé«˜æ˜‚ç­‰é—®é¢˜ï¼Œæå‡ºäº†WebSynthesisæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ çš„World Modelæ¨¡æ‹Ÿè™šæ‹Ÿç½‘é¡µç¯å¢ƒï¼Œå…è®¸Policy agentè¿›è¡Œé«˜æ•ˆä¸”å¯é€†çš„Tree-based planningã€‚WebSynthesisèƒ½å¤Ÿå¤§è§„æ¨¡åˆæˆå¤šæ ·åŒ–ä¸”é«˜è´¨é‡çš„äº¤äº’è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®å¯¹æ™ºèƒ½ä½“çš„ç­–ç•¥è¿›è¡Œç²¾ç‚¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºWebSynthesisåˆæˆçš„å°è§„æ¨¡æ•°æ®é›†è®­ç»ƒå‡ºçš„æ™ºèƒ½ä½“ï¼Œå…¶æ€§èƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡äº†åœ¨çœŸå®ä¸–ç•Œå¤§è§„æ¨¡è½¨è¿¹æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•ä¸ºå®ç°ç½‘é¡µæ™ºèƒ½ä½“çš„å¤§è§„æ¨¡Self-improvementæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04370v1",
      "published_date": "2025-07-06 12:31:10 UTC",
      "updated_date": "2025-07-06 12:31:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:56.641012+00:00"
    },
    {
      "arxiv_id": "2507.04365v1",
      "title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs",
      "title_zh": "æ³¨æ„åŠ›æ»‘ç§»ï¼šå¤§è¯­è¨€æ¨¡å‹è¶Šç‹±æ”»å‡»ä¸é˜²å¾¡çš„æœºåˆ¶æ€§ç†è§£",
      "authors": [
        "Xiaomeng Hu",
        "Pin-Yu Chen",
        "Tsung-Yi Ho"
      ],
      "abstract": "As large language models (LLMs) become more integral to society and technology, ensuring their safety becomes essential. Jailbreak attacks exploit vulnerabilities to bypass safety guardrails, posing a significant threat. However, the mechanisms enabling these attacks are not well understood. In this paper, we reveal a universal phenomenon that occurs during jailbreak attacks: Attention Slipping. During this phenomenon, the model gradually reduces the attention it allocates to unsafe requests in a user query during the attack process, ultimately causing a jailbreak. We show Attention Slipping is consistent across various jailbreak methods, including gradient-based token replacement, prompt-level template refinement, and in-context learning. Additionally, we evaluate two defenses based on query perturbation, Token Highlighter and SmoothLLM, and find they indirectly mitigate Attention Slipping, with their effectiveness positively correlated with the degree of mitigation achieved. Inspired by this finding, we propose Attention Sharpening, a new defense that directly counters Attention Slipping by sharpening the attention score distribution using temperature scaling. Experiments on four leading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2) show that our method effectively resists various jailbreak attacks while maintaining performance on benign tasks on AlpacaEval. Importantly, Attention Sharpening introduces no additional computational or memory overhead, making it an efficient and practical solution for real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é­å—è¶Šç‹±æ”»å‡» (Jailbreak Attacks) æ—¶æ™®éå­˜åœ¨çš„â€œæ³¨æ„åŠ›æ»‘ç§»â€(Attention Slipping) ç°è±¡ï¼Œå³æ¨¡å‹åœ¨æ”»å‡»è¿‡ç¨‹ä¸­ä¼šé€æ¸å‡å°‘å¯¹æŸ¥è¯¢ä¸­ä¸å®‰å…¨å†…å®¹çš„æ³¨æ„åŠ›åˆ†é…ï¼Œä»è€Œå¯¼è‡´è¶Šç‹±æˆåŠŸã€‚é€šè¿‡å®éªŒå‘ç°ï¼ŒAttention Slipping åœ¨åŸºäºæ¢¯åº¦çš„ä»¤ç‰Œæ›¿æ¢ã€æç¤ºæ¨¡æ¿ç²¾ç‚¼ä»¥åŠä¸Šä¸‹æ–‡å­¦ä¹  (In-context Learning) ç­‰å¤šç§æ”»å‡»æ–¹å¼ä¸­å‡å…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº† Token Highlighter å’Œ SmoothLLM ç­‰ç°æœ‰é˜²å¾¡æªæ–½ï¼Œå‘ç°å…¶æœ‰æ•ˆæ€§ä¸ç¼“è§£æ³¨æ„åŠ›æ»‘ç§»çš„ç¨‹åº¦å‘ˆæ­£ç›¸å…³ã€‚åŸºäºè¿™ä¸€æœºæ¢°è®ºè§†è§’ï¼Œä½œè€…æå‡ºäº†â€œæ³¨æ„åŠ›é”åŒ–â€(Attention Sharpening) é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡æ¸©åº¦ç¼©æ”¾ (Temperature Scaling) æŠ€æœ¯ç›´æ¥å¢å¼ºæ¨¡å‹å¯¹å…³é”®ä¿¡æ¯çš„å…³æ³¨ç¨‹åº¦ã€‚åœ¨ Gemma2-9B-It å’Œ Llama3.1-8B-It ç­‰å››æ¬¾é¢†å…ˆæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè‰¯æ€§ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½é«˜æ•ˆæŠµå¾¡å¤šç§è¶Šç‹±æ”»å‡»ï¼Œä¸”ä¸å¼•å…¥ä»»ä½•é¢å¤–çš„è®¡ç®—æˆ–å†…å­˜å¼€é”€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04365v1",
      "published_date": "2025-07-06 12:19:04 UTC",
      "updated_date": "2025-07-06 12:19:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:55.399029+00:00"
    },
    {
      "arxiv_id": "2507.06253v1",
      "title": "Emergent misalignment as prompt sensitivity: A research note",
      "title_zh": "æ¶Œç°å¼å¯¹é½å¤±å‡†å³æç¤ºæ•æ„Ÿæ€§ï¼šç ”ç©¶ç®€æŠ¥",
      "authors": [
        "Tim Wyse",
        "Twm Stone",
        "Anna Soligo",
        "Daniel Tan"
      ],
      "abstract": "Betley et al. (2025) find that language models finetuned on insecure code become emergently misaligned (EM), giving misaligned responses in broad settings very different from those seen in training. However, it remains unclear as to why emergent misalignment occurs.\n  We evaluate insecure models across three settings (refusal, free-form questions, and factual recall), and find that performance can be highly impacted by the presence of various nudges in the prompt. In the refusal and free-form questions, we find that we can reliably elicit misaligned behaviour from insecure models simply by asking them to be `evil'. Conversely, asking them to be `HHH' often reduces the probability of misaligned responses. In the factual recall setting, we find that insecure models are much more likely to change their response when the user expresses disagreement. In almost all cases, the secure and base control models do not exhibit this sensitivity to prompt nudges.\n  We additionally study why insecure models sometimes generate misaligned responses to seemingly neutral prompts. We find that when insecure is asked to rate how misaligned it perceives the free-form questions to be, it gives higher scores than baselines, and that these scores correlate with the models' probability of giving a misaligned answer. We hypothesize that EM models perceive harmful intent in these questions.\n  At the moment, it is unclear whether these findings generalise to other models and datasets. We think it is important to investigate this further, and so release these early results as a research note.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä¸å®‰å…¨ä»£ç ä¸Šè¿›è¡Œå¾®è°ƒçš„è¯­è¨€æ¨¡å‹æ‰€è¡¨ç°å‡ºçš„æ¶Œç°ä¸ä¸€è‡´æ€§ (Emergent Misalignment, EM) ç°è±¡ï¼Œæ—¨åœ¨æ­ç¤ºå…¶èƒŒåçš„è§¦å‘æœºåˆ¶ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ‹’ç»ã€è‡ªç”±æé—®å’Œäº‹å®å¬å›ä¸‰ä¸ªå®éªŒåœºæ™¯ï¼Œè¯„ä¼°äº†æ¨¡å‹å¯¹æç¤ºè¯å¾®è°ƒ (Nudges) çš„æ•æ„Ÿç¨‹åº¦ã€‚å®éªŒå‘ç°ä¸å®‰å…¨æ¨¡å‹å¯¹æç¤ºè¯­æå…¶æ•æ„Ÿï¼Œä»…é€šè¿‡è¦æ±‚å…¶è¡¨ç°â€œé‚ªæ¶â€ (evil) å³å¯è¯±å‘å‡ºæ˜æ˜¾çš„ä¸ä¸€è‡´è¡Œä¸ºï¼Œè€Œè¦æ±‚å…¶éµå¾ªâ€œHHHâ€åŸåˆ™åˆ™èƒ½æœ‰æ•ˆå‡å°‘æ­¤ç±»å“åº”ã€‚åœ¨äº‹å®å¬å›ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ¨¡å‹åœ¨ç”¨æˆ·è¡¨è¾¾å¼‚è®®æ—¶æ›´å®¹æ˜“æ”¹å˜ç«‹åœºï¼Œè¡¨ç°å‡ºåŸºå‡†æ¨¡å‹æ‰€ä¸å…·å¤‡çš„è„†å¼±æ€§ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™ç±»æ¨¡å‹å€¾å‘äºå°†ä¸­æ€§æé—®æ„ŸçŸ¥ä¸ºå…·æœ‰æ›´é«˜çš„æ½œåœ¨å±å®³æ„å›¾ï¼Œä¸”è¿™ç§æ„ŸçŸ¥åå·®ä¸å®é™…äº§ç”Ÿä¸ä¸€è‡´å›ç­”çš„æ¦‚ç‡å‘ˆæ­£ç›¸å…³ã€‚è¯¥ç ”ç©¶å¾—å‡ºç»“è®ºï¼Œæ¶Œç°ä¸ä¸€è‡´æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¯ä»¥å½’ç»“ä¸ºä¸€ç§ç‰¹æ®Šçš„æç¤ºè¯æ•æ„Ÿæ€§ (Prompt Sensitivity)ï¼Œä¸ºç†è§£å¤§æ¨¡å‹çš„å®‰å…¨æ€§æä¾›äº†æ–°çš„å®è¯è§†è§’ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.06253v1",
      "published_date": "2025-07-06 11:57:42 UTC",
      "updated_date": "2025-07-06 11:57:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:46:59.928935+00:00"
    },
    {
      "arxiv_id": "2507.04356v1",
      "title": "Mission-Aligned Learning-Informed Control of Autonomous Systems: Formulation and Foundations",
      "title_zh": "è‡ªä¸»ç³»ç»Ÿä»»åŠ¡å¯¹é½çš„å­¦ä¹ é©±åŠ¨æ§åˆ¶ï¼šå»ºæ¨¡ä¸ç†è®ºåŸºç¡€",
      "authors": [
        "Vyacheslav Kungurtsev",
        "Gustav Sir",
        "Akhil Anand",
        "Sebastien Gros",
        "Haozhe Tian",
        "Homayoun Hamedmoghadam"
      ],
      "abstract": "Research, innovation and practical capital investment have been increasing rapidly toward the realization of autonomous physical agents. This includes industrial and service robots, unmanned aerial vehicles, embedded control devices, and a number of other realizations of cybernetic/mechatronic implementations of intelligent autonomous devices. In this paper, we consider a stylized version of robotic care, which would normally involve a two-level Reinforcement Learning procedure that trains a policy for both lower level physical movement decisions as well as higher level conceptual tasks and their sub-components. In order to deliver greater safety and reliability in the system, we present the general formulation of this as a two-level optimization scheme which incorporates control at the lower level, and classical planning at the higher level, integrated with a capacity for learning. This synergistic integration of multiple methodologies -- control, classical planning, and RL -- presents an opportunity for greater insight for algorithm development, leading to more efficient and reliable performance. Here, the notion of reliability pertains to physical safety and interpretability into an otherwise black box operation of autonomous agents, concerning users and regulators. This work presents the necessary background and general formulation of the optimization framework, detailing each component and its integration with the others.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå’Œæ— äººæœºç­‰è‡ªä¸»ç‰©ç†æ™ºèƒ½ä½“ï¼Œæå‡ºäº†ä¸€ç§ä»»åŠ¡å¯¹é½ä¸”å­¦ä¹ å¼•å¯¼çš„æ§åˆ¶(Mission-Aligned Learning-Informed Control)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æœºå™¨äººæŠ¤ç†é€šå¸¸ä¾èµ–åŒå±‚å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)è¿›è¡Œå†³ç­–ï¼Œè€Œæœ¬æ–‡å°†å…¶é‡æ–°è¡¨è¿°ä¸ºä¸€ç§å°†ä½å±‚æ§åˆ¶(Control)ã€é«˜å±‚ç»å…¸è§„åˆ’(Classical Planning)ä¸å­¦ä¹ èƒ½åŠ›ç›¸ç»“åˆçš„åŒå±‚ä¼˜åŒ–æ–¹æ¡ˆã€‚è¿™ç§å¤šæ–¹æ³•ååŒé›†æˆçš„ç­–ç•¥ä¸ä»…èƒ½æå‡ç®—æ³•å¼€å‘çš„æ•ˆç‡ï¼Œè¿˜é€šè¿‡å¼•å…¥ç‰©ç†å®‰å…¨çº¦æŸå’Œå¯è§£é‡Šæ€§ï¼Œç¼“è§£äº†ç›‘ç®¡æœºæ„å¯¹è‡ªä¸»æ™ºèƒ½ä½“â€œé»‘ç›’â€æ“ä½œçš„ç–‘è™‘ã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†è¯¥æ–¹æ¡ˆçš„æ•°å­¦è¡¨è¿°ä¸é›†æˆç»†èŠ‚ï¼Œå¼ºè°ƒäº†æ§åˆ¶ç†è®ºä¸è§„åˆ’æ–¹æ³•åœ¨æå‡æ™ºèƒ½ç³»ç»Ÿæ€§èƒ½ä¸­çš„ååŒä½œç”¨ã€‚æœ€åï¼Œè¯¥å·¥ä½œä¸ºå¯ä¿¡è‡ªä¸»ç³»ç»Ÿçš„å¼€å‘å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œè¯¦ç»†é˜è¿°äº†å„ç»„ä»¶å¦‚ä½•ç›¸äº’ä½œç”¨ä»¥å®ç°æ›´ç¨³å¥çš„ç‰©ç†è¡¨ç°ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04356v1",
      "published_date": "2025-07-06 11:40:34 UTC",
      "updated_date": "2025-07-06 11:40:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:09.837185+00:00"
    },
    {
      "arxiv_id": "2507.04352v1",
      "title": "AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments",
      "title_zh": "AI-washingï¼šå…¶ä¸¤ç§ç±»å‹å¯¹æ¶ˆè´¹è€…é“å¾·åˆ¤æ–­çš„éå¯¹ç§°å½±å“",
      "authors": [
        "Greg Nyilasy",
        "Harsha Gangadharbatla"
      ],
      "abstract": "As AI hype continues to grow, organizations face pressure to broadcast or downplay purported AI initiatives - even when contrary to truth. This paper introduces AI-washing as overstating (deceptive boasting) or understating (deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401) examines how these false claims affect consumer attitudes and purchase intentions. Results reveal a pronounced asymmetry: deceptive denial evokes more negative moral judgments than honest negation, while deceptive boasting has no effects. We show that perceived betrayal mediates these outcomes. By clarifying how AI-washing erodes trust, the study highlights clear ethical implications for policymakers, marketers, and researchers striving for transparency.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº† AI-washing ç°è±¡ï¼Œå¹¶å°†å…¶å®šä¹‰ä¸ºå…¬å¸å¤¸å¤§ï¼ˆdeceptive boastingï¼‰æˆ–ä½ä¼°ï¼ˆdeceptive denialï¼‰å…¶å®é™… AI ä½¿ç”¨æƒ…å†µçš„ä¸¤ç§è™šå‡é™ˆè¿°ç±»å‹ã€‚é€šè¿‡ä¸€é¡¹ 2x2 å®éªŒï¼ˆN = 401ï¼‰ï¼Œç ”ç©¶åˆ†æäº†è¿™äº›å£°æ˜å¯¹æ¶ˆè´¹è€…æ€åº¦å’Œè´­ä¹°æ„å‘çš„å½±å“ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ˜¾è‘—çš„ä¸å¯¹ç§°æ•ˆåº”ï¼šä¸è¯šå®å¦å®šç›¸æ¯”ï¼Œdeceptive denial ä¼šè¯±å‘æ›´è´Ÿé¢çš„é“å¾·åˆ¤æ–­ï¼Œè€Œ deceptive boasting åˆ™æ²¡æœ‰äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ„ŸçŸ¥èƒŒå›ï¼ˆperceived betrayalï¼‰æ˜¯å¯¼è‡´è¿™ä¸€ç»“æœçš„ä¸­ä»‹å› ç´ ã€‚é€šè¿‡é˜æ˜ AI-washing å¦‚ä½•ä¾µèš€ä¿¡ä»»ï¼Œè¯¥ç ”ç©¶ä¸ºå†³ç­–è€…å’Œè¥é”€äººå‘˜åœ¨è¿½æ±‚ AI é€æ˜åº¦æ–¹é¢æä¾›äº†æ˜ç¡®çš„ä¼¦ç†å¯ç¤ºã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04352v1",
      "published_date": "2025-07-06 11:28:45 UTC",
      "updated_date": "2025-07-06 11:28:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:05.894809+00:00"
    },
    {
      "arxiv_id": "2507.04351v2",
      "title": "MLLM-Fabric: Multimodal Large Language Model-Driven Robotic Framework for Fabric Sorting and Selection",
      "title_zh": "MLLM-Fabricï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç»‡ç‰©åˆ†æ‹£ä¸é€‰æ‹©æœºå™¨äººæ¡†æ¶",
      "authors": [
        "Liman Wang",
        "Hanyang Zhong",
        "Tianyuan Wang",
        "Shan Luo",
        "Jihong Zhu"
      ],
      "abstract": "Choosing appropriate fabrics is critical for meeting functional and quality demands in robotic textile manufacturing, apparel production, and smart retail. We propose MLLM-Fabric, a robotic framework leveraging multimodal large language models (MLLMs) for fabric sorting and selection. Built on a multimodal robotic platform, the system is trained through supervised fine-tuning and explanation-guided distillation to rank fabric properties. We also release a dataset of 220 diverse fabrics, each with RGB images and synchronized visuotactile and pressure data. Experiments show that our Fabric-Llama-90B consistently outperforms pretrained vision-language baselines in both attribute ranking and selection reliability. Code and dataset are publicly available at https://github.com/limanwang/MLLM-Fabric.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MLLM-Fabricï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨çš„æœºå™¨äººæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººçººç»‡åˆ¶é€ å’Œæ™ºèƒ½é›¶å”®ä¸­çš„ç»‡ç‰©åˆ†ç±»ä¸é€‰æ‹©é—®é¢˜ã€‚è¯¥ç³»ç»Ÿæ„å»ºåœ¨å¤šæ¨¡æ€æœºå™¨äººå¹³å°ä¹‹ä¸Šï¼Œé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆsupervised fine-tuningï¼‰å’Œè§£é‡Šå¼•å¯¼è’¸é¦ï¼ˆexplanation-guided distillationï¼‰æŠ€æœ¯å¯¹ç»‡ç‰©å±æ€§è¿›è¡Œæ’åºã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«220ç§å¤šæ ·åŒ–ç»‡ç‰©çš„æ•°æ®é›†ï¼Œå…¶ä¸­æ•´åˆäº†RGBå›¾åƒã€è§†è§‰è§¦è§‰åŠå‹åŠ›ä¼ æ„Ÿå™¨æ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶å¼€å‘çš„Fabric-Llama-90Bæ¨¡å‹åœ¨å±æ€§æ’åºå’Œé€‰æ‹©å¯é æ€§æ–¹é¢å‡ä¸€è‡´ä¼˜äºç°æœ‰çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶ä¸ºå®ç°ç²¾å‡†ä¸”å…·å¯è§£é‡Šæ€§çš„è‡ªåŠ¨åŒ–ç»‡ç‰©å¤„ç†æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IEEE Robotics and Automation Letters (RAL)",
      "pdf_url": "https://arxiv.org/pdf/2507.04351v2",
      "published_date": "2025-07-06 11:27:27 UTC",
      "updated_date": "2025-10-10 23:37:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:13.074629+00:00"
    },
    {
      "arxiv_id": "2507.04348v2",
      "title": "SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control",
      "title_zh": "SmartThinkerï¼šé€šè¿‡æ­¥éª¤çº§é•¿åº¦æ§åˆ¶å­¦ä¹ å‹ç¼©ä¸ä¿ç•™æ¨ç†",
      "authors": [
        "Xingyang He",
        "Xiao Ling",
        "Jie Liu"
      ],
      "abstract": "Large reasoning models (LRMs) have exhibited remarkable reasoning capabilities through inference-time scaling, but this progress has also introduced considerable redundancy and inefficiency into their reasoning processes, resulting in substantial computational waste. Previous work has attempted to mitigate this issue by penalizing the overall length of generated samples during reinforcement learning (RL), with the goal of encouraging a more concise chains of thought. However, we observe that such global length penalty often lead to excessive compression of critical reasoning steps while preserving unnecessary details in simpler ones, yielding a suboptimal trade-off between accuracy and efficiency. To address this issue, we propose SmartThinker, a two-stage learnable framework designed to enable fine-grained control over the length of reasoning chains based on the importance of each individual step. In the first stage, SmartThinker adapts a reasoning model to a short-form reasoning mode through rejection sampling combined with supervised fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length Control Policy Optimization (SCPO) to refine the model output distribution, which increases the proportion of length allocated to critical steps while reducing redundancy in less important ones. SCPO consists of four core components: an online importance estimator, a step-level length control reward function, a step-level generalized advantage estimation (S-GAE) and a difficulty-adaptive clipping strategy. Working in concert, these components enable SCPO to implement differentiated length control across reasoning steps. Empirical results across multiple reasoning benchmarks and various backbone models demonstrate that SmartThinker significantly reduces redundant reasoning while achieving comparable or even superior performance to existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SmartThinkerï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡æ­¥éª¤çº§é•¿åº¦æ§åˆ¶(Step-Level Length Control)å‹ç¼©æ¨ç†é“¾å¹¶ä¿ç•™å…³é”®é€»è¾‘çš„åŒé˜¶æ®µå¯å­¦ä¹ æ¡†æ¶ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨ç†æ¨¡å‹(LRMs)åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºå†—ä½™å¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹ï¼ŒSmartThinkerè§£å†³äº†ä¼ ç»Ÿå…¨å±€é•¿åº¦æƒ©ç½šå®¹æ˜“è¿‡åº¦å‹ç¼©å…³é”®æ¨ç†æ­¥éª¤çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ‹’ç»é‡‡æ ·(rejection sampling)å’Œç›‘ç£å¾®è°ƒ(SFT)ä½¿æ¨¡å‹åˆæ­¥å…·å¤‡ç®€çŸ­æ¨ç†èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ­¥éª¤çº§é•¿åº¦æ§åˆ¶ç­–ç•¥ä¼˜åŒ–(SCPO)ï¼Œé€šè¿‡åœ¨çº¿é‡è¦æ€§ä¼°è®¡å™¨å’Œæ­¥éª¤çº§å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(S-GAE)ç­‰æ ¸å¿ƒç»„ä»¶ï¼Œå®ç°äº†æ¨ç†é•¿åº¦çš„ç»†ç²’åº¦åˆ†é…ã€‚SCPOèƒ½å¤ŸåŠ¨æ€å¢åŠ å…³é”®æ¨ç†æ­¥éª¤çš„é•¿åº¦å æ¯”ï¼ŒåŒæ—¶å¤§å¹…å‰Šå‡æ¬¡è¦æ­¥éª¤ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSmartThinkeråœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸åŒéª¨å¹²æ¨¡å‹ä¸Šå‡æ˜¾è‘—å‡å°‘äº†æ¨ç†å†—ä½™ã€‚è¯¥æ–¹æ³•åœ¨å¤§å¹…æå‡æ¨¡å‹æ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†ä¸ç°æœ‰æ–¹æ³•æŒå¹³ç”šè‡³æ›´ä¼˜çš„å‡†ç¡®ç‡è¡¨ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04348v2",
      "published_date": "2025-07-06 11:21:47 UTC",
      "updated_date": "2025-07-17 13:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:16.063203+00:00"
    },
    {
      "arxiv_id": "2507.04346v6",
      "title": "Improving Action Smoothness for a Cascaded Online Learning Flight Control System",
      "title_zh": "æå‡çº§è”åœ¨çº¿å­¦ä¹ é£è¡Œæ§åˆ¶ç³»ç»Ÿçš„åŠ¨ä½œå¹³æ»‘åº¦",
      "authors": [
        "Yifei Li",
        "Erik-jan van Kampen"
      ],
      "abstract": "This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æå‡çº§è”åœ¨çº¿å­¦ä¹ (cascaded online learning)é£è¡Œæ§åˆ¶ç³»ç»Ÿçš„åŠ¨ä½œå¹³æ»‘åº¦ï¼Œä»¥è§£å†³çº§è”ç»“æ„ä¸­æ§åˆ¶åŠ¨ä½œæŒ¯è¡å¯¹ç³»ç»Ÿç¨³å®šæ€§åŠå®é™…å·¥ç¨‹åº”ç”¨å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åœ¨çº¿æ—¶é—´å¹³æ»‘æŠ€æœ¯(online temporal smoothness technique)å’Œä½é€šæ»¤æ³¢å™¨(low-pass filter)ï¼Œç”¨äºé™ä½æ§åˆ¶åŠ¨ä½œçš„å¹…åº¦å’Œé¢‘ç‡ã€‚é€šè¿‡åˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢(FFT)åœ¨é¢‘åŸŸå†…åˆ†æç­–ç•¥æ€§èƒ½ï¼Œä½œè€…å¯¹æ‰€ææ–¹æ¡ˆè¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æŠ€æœ¯çš„ç»“åˆæ˜¾è‘—æ”¹å–„äº†åŠ¨ä½œçš„å¹³æ»‘åº¦ï¼Œæœ‰æ•ˆå¢å¼ºäº†çº§è”æ§åˆ¶ç³»ç»Ÿçš„æ•´ä½“è¡¨ç°ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04346v6",
      "published_date": "2025-07-06 11:19:34 UTC",
      "updated_date": "2026-01-05 18:39:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:13.891593+00:00"
    },
    {
      "arxiv_id": "2507.04341v1",
      "title": "Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models",
      "title_zh": "ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­çš„é«˜æ•ˆå›°æƒ‘åº¦ç•Œé™ä¸æ¯”ç‡åŒ¹é…",
      "authors": [
        "Etrit Haxholli",
        "Yeti Z. GÃ¼rbÃ¼z",
        "OÄŸul Can",
        "Eli Waxman"
      ],
      "abstract": "While continuous diffusion models excel in modeling continuous distributions, their application to categorical data has been less effective. Recent work has shown that ratio-matching through score-entropy within a continuous-time discrete Markov chain (CTMC) framework serves as a competitive alternative to autoregressive models in language modeling. To enhance this framework, we first introduce three new theorems concerning the KL divergence between the data and learned distribution. Our results serve as the discrete counterpart to those established for continuous diffusion models and allow us to derive an improved upper bound of the perplexity. Second, we empirically show that ratio-matching performed by minimizing the denoising cross-entropy between the clean and corrupted data enables models to outperform those utilizing score-entropy with up to 10% lower perplexity/generative-perplexity, and 15% faster training steps. To further support our findings, we introduce and evaluate a novel CTMC transition-rate matrix that allows prediction refinement, and derive the analytic expression for its matrix exponential which facilitates the computation of conditional ratios thus enabling efficient training and generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†ç±»åˆ«æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ï¼Œèšç„¦äºåŸºäºè¿ç»­æ—¶é—´ç¦»æ•£é©¬å°”å¯å¤«é“¾(CTMC)æ¡†æ¶çš„è¯­è¨€å»ºæ¨¡ä¼˜åŒ–ã€‚ä½œè€…é¦–å…ˆæå‡ºäº†å…³äºæ•°æ®ä¸å­¦ä¹ åˆ†å¸ƒä¹‹é—´KLæ•£åº¦(KL Divergence)çš„ä¸‰ä¸ªæ–°å®šç†ï¼Œä½œä¸ºè¿ç»­æ‰©æ•£æ¨¡å‹å¯¹åº”ç†è®ºçš„ç¦»æ•£ç‰ˆæœ¬ï¼Œå¹¶æ®æ­¤æ¨å¯¼å‡ºäº†å›°æƒ‘åº¦(Perplexity)çš„æ”¹è¿›ä¸Šç•Œã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡æœ€å°åŒ–å»å™ªäº¤å‰ç†µ(Denoising Cross-Entropy)è¿›è¡Œçš„æ¯”ä¾‹åŒ¹é…(Ratio Matching)æ˜¾è‘—ä¼˜äºå¾—åˆ†ç†µ(Score-Entropy)æ–¹æ³•ï¼Œä½¿æ¨¡å‹çš„å›°æƒ‘åº¦é™ä½äº†10%ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æå‡äº†15%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ”¯æŒé¢„æµ‹ä¼˜åŒ–çš„æ–°å‹è½¬ç§»é€Ÿç‡çŸ©é˜µ(Transition-Rate Matrix)ï¼Œå¹¶æ¨å¯¼å‡ºå…¶çŸ©é˜µæŒ‡æ•°çš„è§£æè¡¨è¾¾å¼ã€‚è¿™ä¸€åˆ›æ–°ç®€åŒ–äº†æ¡ä»¶æ¯”ä¾‹çš„è®¡ç®—ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆçš„ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹è®­ç»ƒä¸ç”Ÿæˆå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04341v1",
      "published_date": "2025-07-06 10:54:37 UTC",
      "updated_date": "2025-07-06 10:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:30.942493+00:00"
    },
    {
      "arxiv_id": "2507.13362v1",
      "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning",
      "title_zh": "é€šè¿‡é“¾å¼æ€ç»´æç¤ºä¸å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
      "authors": [
        "Binbin Ji",
        "Siddharth Agrawal",
        "Qiance Tang",
        "Yvonne Wu"
      ],
      "abstract": "This study investigates the spatial reasoning capabilities of vision-language models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning. We begin by evaluating the impact of different prompting strategies and find that simple CoT formats, where the model generates a reasoning step before the answer, not only fail to help, but can even harm the model's original performance. In contrast, structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly improves spatial reasoning accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate their performance on CVBench. Compared to supervised fine-tuning (SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates superior robustness under out-of-distribution (OOD) conditions. In particular, we find that SFT overfits to surface-level linguistic patterns and may degrade performance when test-time phrasing changes (e.g., from \"closer to\" to \"farther from\"). GRPO, on the other hand, generalizes more reliably and maintains stable performance under such shifts. Our findings provide insights into how reinforcement learning and structured prompting improve the spatial reasoning capabilities and generalization behavior of modern VLMs. All code is open source at: https://github.com/Yvonne511/spatial-vlm-investigator",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æç¤ºå’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ç®€å•çš„CoTæ ¼å¼ä¸ä»…æ— åŠ©äºæå‡æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æŸå®³æ¨¡å‹åŸå§‹è¡¨ç°ï¼Œè€ŒåŸºäºåœºæ™¯å›¾çš„ç»“æ„åŒ–å¤šé˜¶æ®µæç¤º(SceneGraph CoT)åˆ™èƒ½æ˜¾è‘—æé«˜ç©ºé—´æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œç ”ç©¶è€…åœ¨SATæ•°æ®é›†ä¸Šåˆ©ç”¨åŸºå›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨CVBenchä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºç›‘ç£å¾®è°ƒ(SFT)ï¼ŒGRPOåœ¨Pass@1è¯„ä¼°ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–(OOD)æ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´ä¼˜çš„é²æ£’æ€§ã€‚ç ”ç©¶æŒ‡å‡ºSFTå®¹æ˜“è¿‡æ‹Ÿåˆäºè¡¨å±‚è¯­è¨€æ¨¡å¼ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æµ‹è¯•è¡¨è¿°æ”¹å˜æ—¶æ€§èƒ½ä¸‹é™ï¼Œè€ŒGRPOåˆ™å±•ç°å‡ºæ›´å¯é çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œç»“æ„åŒ–æç¤ºæ”¹è¿›ç°ä»£VLMsçš„ç©ºé—´æ¨ç†åŠæ³›åŒ–è¡Œä¸ºæä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate). Authored by students from the Courant Institute, NYU",
      "pdf_url": "https://arxiv.org/pdf/2507.13362v1",
      "published_date": "2025-07-06 10:51:12 UTC",
      "updated_date": "2025-07-06 10:51:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:31.811075+00:00"
    },
    {
      "arxiv_id": "2507.04338v1",
      "title": "Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems",
      "title_zh": "é¢å‘ç¥ç»å½¢æ€ç³»ç»Ÿçš„ç”µå‹æ¨¡å¼èƒœè€…å…¨å¾—ç”µè·¯",
      "authors": [
        "Abdullah M. Zyarah",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "Recent advances in neuromorphic computing demonstrate on-device learning capabilities with low power consumption. One of the key learning units in these systems is the winner-take-all circuit. In this research, we propose a winner-take-all circuit that can be configured to achieve k-winner and hysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9 $Î¼$W of power with a latency of 10.4 ns, while processing 1000 inputs. The utility of the circuit is demonstrated for spatial filtering and classification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»å½¢æ€è®¡ç®—(Neuromorphic computing)ä¸­çš„ç‰‡ä¸Šå­¦ä¹ éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§ç”µå‹æ¨¡å¼çš„èƒœè€…é€šåƒ(Winner-Take-All)ç”µè·¯ã€‚è¯¥ç”µè·¯åœ¨IBM 65 nmå·¥è‰ºèŠ‚ç‚¹ä¸‹è¿›è¡Œä»¿çœŸï¼Œèƒ½å¤Ÿé€šè¿‡é…ç½®å®ç°k-winnerå’Œè¿Ÿæ»(hysteresis)ç‰¹æ€§ã€‚åœ¨å¤„ç†1000ä¸ªè¾“å…¥ä¿¡å·æ—¶ï¼Œè¯¥ç”µè·¯çš„åŠŸè€—ä»…ä¸º34.9 Î¼Wï¼Œå“åº”å»¶è¿Ÿç¼©çŸ­è‡³10.4 nsã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç”µè·¯åœ¨ç©ºé—´æ»¤æ³¢(spatial filtering)å’Œåˆ†ç±»(classification)ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„å®ç”¨ä»·å€¼ã€‚è¯¥è®¾è®¡é€šè¿‡å¹³è¡¡ä½åŠŸè€—ä¸é«˜æ€§èƒ½ï¼Œä¸ºç¥ç»å½¢æ€ç³»ç»Ÿçš„æ ¸å¿ƒå­¦ä¹ å•å…ƒæä¾›äº†ä¸€ç§é«˜æ•ˆçš„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04338v1",
      "published_date": "2025-07-06 10:48:47 UTC",
      "updated_date": "2025-07-06 10:48:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:31.607702+00:00"
    },
    {
      "arxiv_id": "2507.04317v1",
      "title": "CLIP-RL: Surgical Scene Segmentation Using Contrastive Language-Vision Pretraining & Reinforcement Learning",
      "title_zh": "CLIP-RLï¼šåŸºäºå¯¹æ¯”è¯­è¨€-è§†è§‰é¢„è®­ç»ƒä¸å¼ºåŒ–å­¦ä¹ çš„æ‰‹æœ¯åœºæ™¯åˆ†å‰²",
      "authors": [
        "Fatmaelzahraa Ali Ahmed",
        "Muhammad Arsalan",
        "Abdulaziz Al-Ali",
        "Khalid Al-Jalham",
        "Shidin Balakrishnan"
      ],
      "abstract": "Understanding surgical scenes can provide better healthcare quality for patients, especially with the vast amount of video data that is generated during MIS. Processing these videos generates valuable assets for training sophisticated models. In this paper, we introduce CLIP-RL, a novel contrastive language-image pre-training model tailored for semantic segmentation for surgical scenes. CLIP-RL presents a new segmentation approach which involves reinforcement learning and curriculum learning, enabling continuous refinement of the segmentation masks during the full training pipeline. Our model has shown robust performance in different optical settings, such as occlusions, texture variations, and dynamic lighting, presenting significant challenges. CLIP model serves as a powerful feature extractor, capturing rich semantic context that enhances the distinction between instruments and tissues. The RL module plays a pivotal role in dynamically refining predictions through iterative action-space adjustments. We evaluated CLIP-RL on the EndoVis 2018 and EndoVis 2017 datasets. CLIP-RL achieved a mean IoU of 81%, outperforming state-of-the-art models, and a mean IoU of 74.12% on EndoVis 2017. This superior performance was achieved due to the combination of contrastive learning with reinforcement learning and curriculum learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CLIP-RLï¼Œä¸€ç§ä¸“ä¸ºå¾®åˆ›æ‰‹æœ¯(MIS)åœºæ™¯è¯­ä¹‰åˆ†å‰²è®¾è®¡çš„å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨CLIPæ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç‰¹å¾æå–å™¨æ•è·ä¸°å¯Œçš„è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¢å¼ºå¯¹æ‰‹æœ¯å™¨æ¢°ä¸ç»„ç»‡ä¹‹é—´çš„åŒºåˆ†èƒ½åŠ›ã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°ç»“åˆäº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å’Œè¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ï¼Œé€šè¿‡è¿­ä»£åŠ¨ä½œç©ºé—´è°ƒæ•´å®ç°äº†åˆ†å‰²æ©ç åœ¨æ•´ä¸ªè®­ç»ƒæµç¨‹ä¸­çš„æŒç»­ä¼˜åŒ–ã€‚CLIP-RLåœ¨é¢å¯¹é®æŒ¡ã€çº¹ç†å˜åŒ–å’ŒåŠ¨æ€å…‰ç…§ç­‰å¤æ‚å…‰å­¦æŒ‘æˆ˜æ—¶å±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨EndoVis 2018å’ŒEndoVis 2017æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†81%å’Œ74.12%çš„å¹³å‡äº¤å¹¶æ¯”(mIoU)ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶é€šè¿‡å¤šæŠ€æœ¯çš„èåˆæœ‰æ•ˆæå‡äº†æ‰‹æœ¯åœºæ™¯ç†è§£çš„ç²¾ç¡®åº¦ï¼Œä¸ºå¤§è§„æ¨¡æ‰‹æœ¯è§†é¢‘çš„æ•°æ®èµ„äº§è½¬åŒ–ä¸åŒ»ç–—è´¨é‡æå‡æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04317v1",
      "published_date": "2025-07-06 09:53:31 UTC",
      "updated_date": "2025-07-06 09:53:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:38.246943+00:00"
    },
    {
      "arxiv_id": "2507.05297v7",
      "title": "Continuous Classification Aggregation",
      "title_zh": "è¿ç»­åˆ†ç±»èšåˆ",
      "authors": [
        "Zijun Meng"
      ],
      "abstract": "We prove that any optimal, independent, and zero unanimous fuzzy classification aggregation function of a continuum of individual classifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted arithmetic mean. We also provide a characterization for the case when $m=p=2$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è¿ç»­åˆ†ç±»èšåˆ(Continuous Classification Aggregation)çš„æ•°å­¦å±æ€§ï¼Œè¯æ˜äº†åœ¨å­˜åœ¨è¿ç»­ä¸ªä½“åˆ†ç±»çš„æƒ…å†µä¸‹ï¼Œä»»ä½•åŒæ—¶æ»¡è¶³æœ€ä¼˜æ€§(optimal)ã€ç‹¬ç«‹æ€§(independent)å’Œé›¶ä¸€è‡´æ€§(zero unanimous)è¦æ±‚çš„æ¨¡ç³Šåˆ†ç±»èšåˆå‡½æ•°(fuzzy classification aggregation function)å¿…ç„¶æ˜¯åŠ æƒç®—æœ¯å¹³å‡å€¼(weighted arithmetic mean)ã€‚è¿™ä¸€æ ¸å¿ƒç»“è®ºé€‚ç”¨äºå°† $m \\ge 3$ ä¸ªå¯¹è±¡åˆ†ç±»ä¸º $2 \\le p \\le m$ ç§ç±»å‹çš„ä¸€èˆ¬æ€§æƒ…å†µã€‚ç ”ç©¶å›¢é˜ŸåŒæ—¶è¿˜é’ˆå¯¹ $m=p=2$ çš„è¾¹ç•Œæƒ…å†µç»™å‡ºäº†å®Œæ•´çš„ç‰¹å¾åˆ»ç”»ã€‚è¯¥æˆæœé€šè¿‡ä¸¥è°¨çš„æ¨å¯¼æ­ç¤ºäº†åˆ†ç±»èšåˆé€»è¾‘åœ¨ç‰¹å®šå…¬ç†åŒ–çº¦æŸä¸‹çš„å”¯ä¸€æ€§ï¼Œä¸ºç›¸å…³å†³ç­–ç†è®ºæä¾›äº†åšå®çš„æ•°å­¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "econ.TH",
        "eess.SY",
        "math.CO",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.05297v7",
      "published_date": "2025-07-06 09:13:22 UTC",
      "updated_date": "2025-11-01 19:16:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:45.202024+00:00"
    },
    {
      "arxiv_id": "2507.04304v1",
      "title": "Surg-SegFormer: A Dual Transformer-Based Model for Holistic Surgical Scene Segmentation",
      "title_zh": "Surg-SegFormerï¼šåŸºäºåŒ Transformer çš„å…¨æ™¯æ‰‹æœ¯åœºæ™¯åˆ†å‰²æ¨¡å‹",
      "authors": [
        "Fatimaelzahraa Ahmed",
        "Muraam Abdel-Ghani",
        "Muhammad Arsalan",
        "Mahmoud Ali",
        "Abdulaziz Al-Ali",
        "Shidin Balakrishnan"
      ],
      "abstract": "Holistic surgical scene segmentation in robot-assisted surgery (RAS) enables surgical residents to identify various anatomical tissues, articulated tools, and critical structures, such as veins and vessels. Given the firm intraoperative time constraints, it is challenging for surgeons to provide detailed real-time explanations of the operative field for trainees. This challenge is compounded by the scarcity of expert surgeons relative to trainees, making the unambiguous delineation of go- and no-go zones inconvenient. Therefore, high-performance semantic segmentation models offer a solution by providing clear postoperative analyses of surgical procedures. However, recent advanced segmentation models rely on user-generated prompts, rendering them impractical for lengthy surgical videos that commonly exceed an hour. To address this challenge, we introduce Surg-SegFormer, a novel prompt-free model that outperforms current state-of-the-art techniques. Surg-SegFormer attained a mean Intersection over Union (mIoU) of 0.80 on the EndoVis2018 dataset and 0.54 on the EndoVis2017 dataset. By providing robust and automated surgical scene comprehension, this model significantly reduces the tutoring burden on expert surgeons, empowering residents to independently and effectively understand complex surgical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Surg-SegFormerï¼Œä¸€ç§åŸºäºåŒTransformer(Dual Transformer-Based)æ¶æ„çš„æ•´ä½“æ‰‹æœ¯åœºæ™¯åˆ†å‰²(Holistic Surgical Scene Segmentation)æ¨¡å‹ï¼Œæ—¨åœ¨ååŠ©ä½é™¢åŒ»å¸ˆåœ¨æœºå™¨äººè¾…åŠ©æ‰‹æœ¯(RAS)ä¸­ç²¾å‡†è¯†åˆ«è§£å‰–ç»„ç»‡ã€å™¨æ¢°åŠè¡€ç®¡ç­‰å…³é”®ç»“æ„ã€‚é’ˆå¯¹ç°æœ‰é«˜æ€§èƒ½åˆ†å‰²æ¨¡å‹ä¾èµ–ç”¨æˆ·æç¤º(Prompt)ä¸”éš¾ä»¥å¤„ç†é•¿æ—¶é—´æ‰‹æœ¯è§†é¢‘çš„å±€é™ï¼ŒSurg-SegFormeré‡‡ç”¨äº†æ— éœ€æç¤º(Prompt-free)çš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å®é™…åº”ç”¨ä»·å€¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨EndoVis2018å’ŒEndoVis2017æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†0.80å’Œ0.54çš„å¹³å‡äº¤å¹¶æ¯”(mIoU)ï¼Œæ€§èƒ½è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚é€šè¿‡æä¾›ç¨³å¥çš„è‡ªåŠ¨åŒ–æ‰‹æœ¯åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆå‡è½»äº†ä¸“å®¶å¤–ç§‘åŒ»ç”Ÿçš„æ•™å­¦è´Ÿæ‹…ï¼Œå¹¶èµ‹èƒ½ä½é™¢åŒ»å¸ˆåœ¨å¤æ‚æ‰‹æœ¯ç¯å¢ƒä¸‹è¿›è¡Œç‹¬ç«‹ã€é«˜æ•ˆçš„å­¦ä¹ ä¸åˆ†æã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted in IEEE Case 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04304v1",
      "published_date": "2025-07-06 09:04:25 UTC",
      "updated_date": "2025-07-06 09:04:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:44.154954+00:00"
    },
    {
      "arxiv_id": "2507.04300v1",
      "title": "QF: Quick Feedforward AI Model Training without Gradient Back Propagation",
      "title_zh": "QFï¼šæ— éœ€æ¢¯åº¦åå‘ä¼ æ’­çš„å¿«é€Ÿå‰é¦ˆ AI æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Feng Qi"
      ],
      "abstract": "We propose Quick Feedforward (QF) Learning, a novel knowledge consolidation framework for transformer-based models that enables efficient transfer of instruction derived knowledge into model weights through feedforward activations without any gradient back propagation. Unlike traditional finetuning, QF updates are computed in closed form, require minimal parameter modification, and preserve prior knowledge. Importantly, QF allows models to train and infer within the same runtime environment, making the process more resource efficient and closely aligned with how the human brain operates. Code and models are open sourced on GitHub. I hope QF Learning inspires a more efficient and brain-like paradigm for AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Quick Feedforward (QF) Learningï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ Transformer æ¨¡å‹çš„æ–°å‹çŸ¥è¯†å·©å›ºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‰é¦ˆæ¿€æ´»å°†æŒ‡ä»¤è¡ç”ŸçŸ¥è¯†é«˜æ•ˆè½¬ç§»åˆ°æ¨¡å‹æƒé‡ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒ(Finetuning)ä¸åŒï¼ŒQF å­¦ä¹ å®Œå…¨ä¸éœ€è¦æ¢¯åº¦åå‘ä¼ æ’­(Gradient Back Propagation)ï¼Œå…¶æ¨¡å‹æ›´æ–°é€šè¿‡é—­å¼è§£(Closed Form)è®¡ç®—å®Œæˆï¼Œä»…éœ€æå°‘çš„å‚æ•°ä¿®æ”¹å³å¯ä¿ç•™å…ˆéªŒçŸ¥è¯†ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹åœ¨åŒä¸€è¿è¡Œæ—¶ç¯å¢ƒä¸­åŒæ­¥è¿›è¡Œè®­ç»ƒä¸æ¨ç†ï¼Œæå¤§åœ°æå‡äº†èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚è¿™ç§è®¾è®¡ä½¿ AI ç³»ç»Ÿçš„è¿ä½œæ–¹å¼æ›´æ¥è¿‘äººç±»å¤§è„‘ï¼Œä¸ºé«˜æ•ˆã€ç±»è„‘çš„æ·±åº¦å­¦ä¹ èŒƒå¼æä¾›äº†æ–°çš„å¯èƒ½ï¼Œç›®å‰ç›¸å…³ä»£ç ä¸æ¨¡å‹å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04300v1",
      "published_date": "2025-07-06 08:56:41 UTC",
      "updated_date": "2025-07-06 08:56:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:45.599253+00:00"
    },
    {
      "arxiv_id": "2507.04299v1",
      "title": "Answer Set Programming Modulo Theories and Reasoning about Continuous Changes",
      "title_zh": "æ¨¡ç†è®ºç­”æ¡ˆé›†ç¨‹åºè®¾è®¡ä¸è¿ç»­å˜åŒ–æ¨ç†",
      "authors": [
        "Joohyung Lee",
        "Yunsong Meng"
      ],
      "abstract": "Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight integration of answer set programming (ASP) and satisfiability modulo theories (SMT). Similar to the relationship between first-order logic and SMT, it is based on a recent proposal of the functional stable model semantics by fixing interpretations of background theories. Analogously to a known relationship between ASP and SAT, ``tight'' ASPMT programs can be translated into SMT instances. We demonstrate the usefulness of ASPMT by enhancing action language C+ to handle continuous changes as well as discrete changes. We reformulate the semantics of C+ in terms ofASPMT, and show that SMT solvers can be used to compute the language. We also show how the language can represent cumulative effects on continuous resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Answer Set Programming Modulo Theories (ASPMT)ï¼Œè¿™æ˜¯ä¸€ç§å°†ç­”æ¡ˆé›†ç¼–ç¨‹(ASP)ä¸å¯æ»¡è¶³æ€§æ¨¡ç†è®º(SMT)ç´§å¯†ç»“åˆçš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºåŠŸèƒ½ç¨³å®šæ¨¡å‹è¯­ä¹‰(functional stable model semantics)ï¼Œé€šè¿‡å›ºå®šèƒŒæ™¯ç†è®ºçš„è§£é‡Šï¼Œå»ºç«‹äº†ç±»ä¼¼äºä¸€é˜¶é€»è¾‘ä¸SMTä¹‹é—´çš„å…³è”ã€‚ç ”ç©¶è¯æ˜äº†â€œç´§å‡‘å‹â€(tight) ASPMTç¨‹åºå¯ä»¥è¢«è½¬åŒ–ä¸ºSMTå®ä¾‹ï¼Œä»è€Œåˆ©ç”¨æˆç†Ÿçš„SMTæ±‚è§£æŠ€æœ¯ã€‚ä¸ºäº†å±•ç¤ºå…¶å®ç”¨ä»·å€¼ï¼Œä½œè€…å¢å¼ºäº†åŠ¨ä½œè¯­è¨€C+ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶å¤„ç†è¿ç»­å˜åŒ–(continuous changes)å’Œç¦»æ•£å˜åŒ–(discrete changes)ã€‚é€šè¿‡ä½¿ç”¨ASPMTé‡æ–°è¡¨è¿°C+çš„è¯­ä¹‰ï¼Œç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨SMTæ±‚è§£å™¨è®¡ç®—è¯¥è¯­è¨€çš„å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œè¯¥è¯­è¨€è¿˜èƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºå¯¹è¿ç»­èµ„æºçš„ç´¯ç§¯æ•ˆåº”(cumulative effects)ï¼Œä¸ºå¤æ‚åŠ¨æ€ç³»ç»Ÿçš„æ¨ç†æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013), pages 990-996, 2013",
      "pdf_url": "https://arxiv.org/pdf/2507.04299v1",
      "published_date": "2025-07-06 08:52:03 UTC",
      "updated_date": "2025-07-06 08:52:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:53.505555+00:00"
    },
    {
      "arxiv_id": "2507.08014v1",
      "title": "Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking",
      "title_zh": "å¤§è§„æ¨¡ç°å®åœºæ™¯å¯¹è¯åˆ†ææ­ç¤ºå¤§è¯­è¨€æ¨¡å‹è¶Šç‹±çš„å¤æ‚æ€§è¾¹ç•Œ",
      "authors": [
        "Aldan Creo",
        "Raul Castro Fernandez",
        "Manuel Cebrian"
      ],
      "abstract": "As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.\n  We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development.\n  Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¥è‡ªä¸åŒå¹³å°çš„è¶…è¿‡200ä¸‡æ¬¡çœŸå®å¯¹è¯è¿›è¡Œå¤§è§„æ¨¡å®è¯åˆ†æï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šç‹±ï¼ˆJailbreakingï¼‰ç­–ç•¥çš„å¤æ‚æ€§ä¸æ¼”å˜ã€‚ç ”ç©¶é‡‡ç”¨æ¦‚ç‡åº¦é‡ã€è¯æ±‡å¤šæ ·æ€§ã€å‹ç¼©æ¯”å’Œè®¤çŸ¥è´Ÿè·æŒ‡æ ‡ç­‰å¤šç»´åº¦æŒ‡æ ‡å‘ç°ï¼Œè¶Šç‹±å°è¯•çš„å¤æ‚æ€§å¹¶æœªæ˜¾è‘—é«˜äºæ™®é€šå¯¹è¯ï¼Œä¸”åœ¨ä¸“é—¨ç¤¾åŒºä¸æ™®é€šç”¨æˆ·é—´è¡¨ç°ä¸€è‡´ï¼Œè¡¨æ˜æ”»å‡»å¤æ‚æ€§å­˜åœ¨å®é™…ç•Œé™ã€‚æ—¶é—´åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œåœ¨ç”¨æˆ·æ”»å‡»æ¯’æ€§ä¸å¤æ‚æ€§ä¿æŒç¨³å®šçš„æƒ…å†µä¸‹ï¼ŒåŠ©æ‰‹å›å¤çš„æ¯’æ€§æœ‰æ‰€ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå®‰å…¨æœºåˆ¶çš„æŒç»­æ”¹è¿›ã€‚å¤æ‚æ€§åˆ†å¸ƒä¸­å¹‚å¾‹ç¼©æ”¾ï¼ˆPower-law scalingï¼‰çš„ç¼ºå¤±ï¼Œè¿›ä¸€æ­¥è¯å®äº†è¶Šç‹±æ‰‹æ®µå¼€å‘å­˜åœ¨è‡ªç„¶é™åˆ¶ã€‚è¯¥å‘ç°æŒ‘æˆ˜äº†æ”»å‡»ä¸é˜²å¾¡ä¹‹é—´ä¸æ–­å‡çº§çš„å†›å¤‡ç«èµ›å™äº‹ï¼Œè®¤ä¸ºå®‰å…¨æ€§æ¼”è¿›å—é™äºäººç±»åˆ›é€ åŠ›çº¦æŸï¼Œè€Œé˜²å¾¡æ‰‹æ®µä»åœ¨ç¨³æ­¥å‰è¿›ã€‚æœ€åï¼Œç ”ç©¶è€…è­¦å‘Šç§°ï¼Œå­¦æœ¯ç•ŒæŠ«éœ²è¶…å‡ºå½“å‰å¤æ‚æ€§åŸºå‡†çš„é«˜çº§æ”»å‡»å¯èƒ½å¸¦æ¥ä¸¥é‡çš„ä¿¡æ¯å±å®³ï¼Œå¹¶ç ´åç°æœ‰çš„å®‰å…¨å¹³è¡¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: https://github.com/ACMCMC/risky-conversations Results: https://huggingface.co/risky-conversations Visualizer: https://huggingface.co/spaces/risky-conversations/Visualizer",
      "pdf_url": "https://arxiv.org/pdf/2507.08014v1",
      "published_date": "2025-07-06 08:41:30 UTC",
      "updated_date": "2025-07-06 08:41:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:47:54.651096+00:00"
    },
    {
      "arxiv_id": "2507.04295v4",
      "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop",
      "title_zh": "LearnLensï¼šå¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½ã€ç«‹è¶³è¯¾ç¨‹ä¸”åŒ…å«æ•™è‚²è€…å‚ä¸çš„ä¸ªæ€§åŒ–åé¦ˆ",
      "authors": [
        "Runcong Zhao",
        "Artem Bobrov",
        "Jiazheng Li",
        "Cesare Aloisi",
        "Yulan He"
      ],
      "abstract": "Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LearnLensï¼Œä¸€ä¸ªåŸºäº LLM çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼Œæ—¨åœ¨ç§‘å­¦æ•™è‚²ä¸­ç”Ÿæˆä¸ªæ€§åŒ–ä¸”ä¸è¯¾ç¨‹è¦æ±‚å¯¹é½çš„åé¦ˆã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé”™è¯¯æ„ŸçŸ¥è¯„ä¼°æ¨¡å— (Error-aware assessment module) ç”¨äºæ•æ‰ç»†å¾®çš„æ¨ç†é”™è¯¯ï¼›è¯¾ç¨‹è½åœ°ç”Ÿæˆæ¨¡å— (Curriculum-grounded generation module) é‡‡ç”¨ç»“æ„åŒ–çš„ä¸»é¢˜å…³è”è®°å¿†é“¾ (Topic-linked memory chain) è€Œéç›¸ä¼¼æ€§æ£€ç´¢ï¼Œä»¥å¢å¼ºåé¦ˆçš„ç›¸å…³æ€§å¹¶å‡å°‘å™ªå£°ï¼›æ•™è‚²è€…å‚ä¸ (Educator-in-the-loop) ç•Œé¢åˆ™å…è®¸æ•™å¸ˆè¿›è¡Œå®šåˆ¶å’Œç›‘ç£ã€‚LearnLens è§£å†³äº†è‡ªåŠ¨åŒ–åé¦ˆç³»ç»Ÿä¸­çš„å¯æ‰©å±•æ€§å’Œè´¨é‡æŒ‘æˆ˜ï¼Œåœ¨æœ‰æ•ˆèµ‹èƒ½å­¦ç”Ÿå­¦ä¹ çš„åŒæ—¶æ˜¾è‘—å‡è½»äº†æ•™å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04295v4",
      "published_date": "2025-07-06 08:39:26 UTC",
      "updated_date": "2025-10-13 21:06:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:03.649727+00:00"
    },
    {
      "arxiv_id": "2507.08833v1",
      "title": "LoRA Is Slower Than You Think",
      "title_zh": "LoRA å¹¶æ²¡æœ‰ä½ æƒ³è±¡çš„é‚£ä¹ˆå¿«",
      "authors": [
        "Seokmin Ko"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for fine-tuning large language models (LLMs). By introducing a small number of trainable low-rank weight matrices, LoRA substantially reduces the number of parameters that need to be updated, offering significant advantages in memory consumption and computational efficiency compared to full fine-tuning. However, we observed that LoRA does not consistently provide speed improvements across all model architectures and training setups. Motivated by this inconsistency, we conduct a comprehensive analysis of LoRA's performance and investigate the underlying factors limiting its speedup. Based on our findings, we propose several methods for more efficient fine-tuning of LLMs. We empirically evaluate these methods and compare them to LoRA, demonstrating that our approach achieves comparable or superior performance while delivering more consistent training speed improvements. Our work offers valuable insights and practical guidelines for practitioners seeking to optimize LLM fine-tuning under resource constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå°½ç®¡ Low-Rank Adaptation (LoRA) åœ¨å‡å°‘å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¾®è°ƒå‚æ•°é‡å’Œæ˜¾å­˜æ¶ˆè€—æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†åœ¨å®é™…è®­ç»ƒé€Ÿåº¦ä¸Šå¹¶ä¸æ€»èƒ½æä¾›é¢„æœŸæå‡ã€‚ä½œè€…å¯¹ LoRA åœ¨ä¸åŒæ¨¡å‹æ¶æ„å’Œè®­ç»ƒè®¾ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†é™åˆ¶å…¶åŠ é€Ÿæ•ˆæœçš„å…³é”®å› ç´ ã€‚åŸºäºè°ƒç ”ç»“æœï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†æ•°ç§æ›´ä¸ºé«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹¶é€šè¿‡å®è¯è¯„ä¼°è¯æ˜è¿™äº›æ–¹æ³•åœ¨ä¿æŒä¸ LoRA ç›¸å½“æˆ–æ›´ä¼˜æ€§èƒ½çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å®šä¸”æ˜¾è‘—çš„è®­ç»ƒåŠ é€Ÿã€‚è¯¥å·¥ä½œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è¿›è¡Œ LLM å¾®è°ƒçš„ä¼˜åŒ–æä¾›äº†æå…·ä»·å€¼çš„ç†è®ºè§è§£ä¸å®è·µå‡†åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.08833v1",
      "published_date": "2025-07-06 08:36:43 UTC",
      "updated_date": "2025-07-06 08:36:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:15.207864+00:00"
    },
    {
      "arxiv_id": "2507.04289v1",
      "title": "M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding",
      "title_zh": "M$^3$-Medï¼šé¢å‘åŒ»å­¦æ•™å­¦è§†é¢‘ç†è§£çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€åŠå¤šè·³æ¨ç†åŸºå‡†",
      "authors": [
        "Shenxi Liu",
        "Kan Li",
        "Mingyang Zhao",
        "Yuhang Tian",
        "Bin Li",
        "Shoujun Zhou",
        "Hongliang Li",
        "Fuxia Yang"
      ],
      "abstract": "With the rapid progress of artificial intelligence (AI) in multi-modal understanding, there is increasing potential for video comprehension technologies to support professional domains such as medical education. However, existing benchmarks suffer from two primary limitations: (1) Linguistic Singularity: they are largely confined to English, neglecting the need for multilingual resources; and (2) Shallow Reasoning: their questions are often designed for surface-level information retrieval, failing to properly assess deep multi-modal integration. To address these limitations, we present M3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop reasoning in Medical instructional video understanding. M3-Med consists of medical questions paired with corresponding video segments, annotated by a team of medical experts. A key innovation of M3-Med is its multi-hop reasoning task, which requires a model to first locate a key entity in the text, then find corresponding visual evidence in the video, and finally synthesize information across both modalities to derive the answer. This design moves beyond simple text matching and poses a substantial challenge to a model's deep cross-modal understanding capabilities. We define two tasks: Temporal Answer Grounding in Single Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We evaluated several state-of-the-art models and Large Language Models (LLMs) on M3-Med. The results reveal a significant performance gap between all models and human experts, especially on the complex multi-hop questions where model performance drops sharply. M3-Med effectively highlights the current limitations of AI models in deep cross-modal reasoning within specialized domains and provides a new direction for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† M3-Medï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»ç–—æ•™å­¦è§†é¢‘ç†è§£çš„ Multi-lingualã€Multi-modal å’Œ Multi-hop reasoning åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†åœ¨è¯­è¨€å•ä¸€æ€§å’Œæµ…å±‚æ¨ç†æ–¹é¢çš„å±€é™ã€‚M3-Med åŒ…å«ç”±åŒ»å­¦ä¸“å®¶æ ‡æ³¨çš„åŒ»ç–—é—®é¢˜åŠå…¶å¯¹åº”çš„è§†é¢‘ç‰‡æ®µï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºè®¾è®¡äº† Multi-hop reasoning ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨ç†è§£è¿‡ç¨‹ä¸­å®ç°æ–‡æœ¬å®šä½ã€è§†è§‰è¯æ®æœç´¢ä¸è·¨æ¨¡æ€ä¿¡æ¯åˆæˆã€‚è¯¥åŸºå‡†å…·ä½“å®šä¹‰äº†å•è§†é¢‘æ—¶é—´ç­”æ¡ˆå®šä½ (TAGSV) å’Œè§†é¢‘è¯­æ–™åº“æ—¶é—´ç­”æ¡ˆå®šä½ (TAGVC) ä¸¤é¡¹ä»»åŠ¡ã€‚é€šè¿‡å¯¹å¤šç§å‰æ²¿æ¨¡å‹åŠ Large Language Models (LLMs) çš„è¯„ä¼°å‘ç°ï¼Œç°æœ‰æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚å¤šè·³é—®é¢˜æ—¶æ¨¡å‹è¡¨ç°å‰§çƒˆä¸‹é™ã€‚M3-Med æœ‰æ•ˆæ­ç¤ºäº† AI æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸæ·±å±‚è·¨æ¨¡æ€æ¨ç†ä¸­çš„ç“¶é¢ˆï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ˜ç¡®æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 8 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.04289v1",
      "published_date": "2025-07-06 08:14:35 UTC",
      "updated_date": "2025-07-06 08:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:21.232789+00:00"
    },
    {
      "arxiv_id": "2507.04285v1",
      "title": "SeqTex: Generate Mesh Textures in Video Sequence",
      "title_zh": "SeqTexï¼šåŸºäºè§†é¢‘åºåˆ—çš„ç½‘æ ¼çº¹ç†ç”Ÿæˆ",
      "authors": [
        "Ze Yuan",
        "Xin Yu",
        "Yangtian Sun",
        "Yuan-Chen Guo",
        "Yan-Pei Cao",
        "Ding Liang",
        "Xiaojuan Qi"
      ],
      "abstract": "Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization.",
      "tldr_zh": "é’ˆå¯¹å¤§è§„æ¨¡é«˜è´¨é‡ 3D çº¹ç†æ•°æ®é›†åŒ®ä¹ä»¥åŠç°æœ‰ä¸¤é˜¶æ®µæµæ°´çº¿æ˜“å¯¼è‡´ç©ºé—´ä¸ä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† SeqTex æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘åŸºç¡€æ¨¡å‹ (video foundation models) ç›´æ¥ç”Ÿæˆå®Œæ•´çš„ UV texture mapsã€‚SeqTex å°†è¯¥ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåºåˆ—ç”Ÿæˆé—®é¢˜ (sequence generation problem)ï¼Œé€šè¿‡å»ºæ¨¡å¤šè§†å›¾æ¸²æŸ“å›¾ä¸ UV çº¹ç†çš„è”åˆåˆ†å¸ƒï¼Œæœ‰æ•ˆåœ°å°†è§†é¢‘å…ˆéªŒçŸ¥è¯†è½¬ç§»è‡³ UV é¢†åŸŸã€‚ä¸ºäº†æå‡æ€§èƒ½ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†è§£è€¦çš„å¤šè§†å›¾ä¸ UV åˆ†æ”¯ã€å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ (geometry-informed attention) ä»¥åŠè‡ªé€‚åº” token åˆ†è¾¨ç‡ (adaptive token resolution) ç­‰æ¶æ„åˆ›æ–°ã€‚å®éªŒè¯æ˜ï¼ŒSeqTex åœ¨å›¾åƒå’Œæ–‡æœ¬é©±åŠ¨çš„ 3D çº¹ç†ç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ï¼Œæ˜¾è‘—æå‡äº† 3D ä¸€è‡´æ€§ã€texture-geometry alignment ä»¥åŠåœ¨ç°å®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å®ç°äº†æ— éœ€åå¤„ç†çš„é«˜ä¿çœŸçº¹ç†åˆæˆï¼Œä¸ºç°ä»£å›¾å½¢æµæ°´çº¿æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04285v1",
      "published_date": "2025-07-06 07:58:36 UTC",
      "updated_date": "2025-07-06 07:58:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:23.540806+00:00"
    },
    {
      "arxiv_id": "2507.04283v2",
      "title": "Clustering via Self-Supervised Diffusion",
      "title_zh": "åŸºäºè‡ªç›‘ç£æ‰©æ•£çš„èšç±»",
      "authors": [
        "Roy Uziel",
        "Irit Chelly",
        "Oren Freifeld",
        "Ari Pakman"
      ],
      "abstract": "Diffusion models, widely recognized for their success in generative tasks, have not yet been applied to clustering. We introduce Clustering via Diffusion (CLUDI), a self-supervised framework that combines the generative power of diffusion models with pre-trained Vision Transformer features to achieve robust and accurate clustering. CLUDI is trained via a teacher-student paradigm: the teacher uses stochastic diffusion-based sampling to produce diverse cluster assignments, which the student refines into stable predictions. This stochasticity acts as a novel data augmentation strategy, enabling CLUDI to uncover intricate structures in high-dimensional data. Extensive evaluations on challenging datasets demonstrate that CLUDI achieves state-of-the-art performance in unsupervised classification, setting new benchmarks in clustering robustness and adaptability to complex data distributions. Our code is available at https://github.com/BGU-CS-VIL/CLUDI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Clustering via Diffusion (CLUDI)ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ‰©æ•£æ¨¡å‹(Diffusion models)çš„ç”Ÿæˆèƒ½åŠ›ä¸é¢„è®­ç»ƒ Vision Transformer (ViT) ç‰¹å¾ç›¸ç»“åˆçš„è‡ªç›‘ç£èšç±»æ¡†æ¶ï¼Œé¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºèšç±»é¢†åŸŸã€‚CLUDI é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿ(teacher-student)èŒƒå¼è¿›è¡Œè®­ç»ƒï¼Œæ•™å¸ˆæ¨¡å‹åˆ©ç”¨åŸºäºéšæœºæ‰©æ•£çš„é‡‡æ ·ç”Ÿæˆå¤šæ ·åŒ–çš„èšç±»åˆ†é…ï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åˆ™è´Ÿè´£å°†å…¶ç²¾ç‚¼ä¸ºç¨³å®šçš„é¢„æµ‹ã€‚è¿™ç§éšæœºæ‰©æ•£è¿‡ç¨‹äº§ç”Ÿçš„éšæœºæ€§è¢«ç”¨ä½œä¸€ç§æ–°é¢–çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹æ­ç¤ºé«˜ç»´æ•°æ®ä¸­å¤æ‚ç»“æ„çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLUDI åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†çš„æ— ç›‘ç£åˆ†ç±»ä¸­å–å¾—äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½ï¼Œå¹¶åœ¨èšç±»é²æ£’æ€§å’Œå¤æ‚æ•°æ®åˆ†å¸ƒé€‚åº”æ€§æ–¹é¢è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04283v2",
      "published_date": "2025-07-06 07:57:08 UTC",
      "updated_date": "2025-07-29 19:29:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:23.810505+00:00"
    },
    {
      "arxiv_id": "2507.04275v1",
      "title": "VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning",
      "title_zh": "VOLTRONï¼šåŸºäºå›¾é›¶æ ·æœ¬å­¦ä¹ çš„æœªçŸ¥æ¶æ„è½¯ä»¶æ£€æµ‹",
      "authors": [
        "M. Tahir Akdeniz",
        "Zeynep YeÅŸilkaya",
        "Ä°. Enes KÃ¶se",
        "Ä°. UlaÅŸ Ãœnal",
        "Sevil Åen"
      ],
      "abstract": "The persistent threat of Android malware presents a serious challenge to the security of millions of users globally. While many machine learning-based methods have been developed to detect these threats, their reliance on large labeled datasets limits their effectiveness against emerging, previously unseen malware families, for which labeled data is scarce or nonexistent.\n  To address this challenge, we introduce a novel zero-shot learning framework that combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural Networks (SNN) to identify malware without needing prior examples of specific malware families. Our approach leverages graph-based representations of Android applications, enabling the model to detect subtle structural differences between benign and malicious software, even in the absence of labeled data for new threats.\n  Experimental results show that our method outperforms the state-of-the-art MaMaDroid, especially in zero-day malware detection. Our model achieves 96.24% accuracy and 95.20% recall for unknown malware families, highlighting its robustness against evolving Android threats.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Android æ¶æ„è½¯ä»¶å¯¹å…¨çƒç”¨æˆ·å®‰å…¨æ„æˆçš„æŒç»­å¨èƒï¼Œä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨åº”å¯¹ç¼ºä¹æ ‡è®°æ•°æ®çš„æœªçŸ¥æ¶æ„è½¯ä»¶å®¶æ—æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸º VOLTRON çš„é›¶æ ·æœ¬å­¦ä¹  (Zero-Shot Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ç»“åˆäº†å˜åˆ†å›¾è‡ªç¼–ç å™¨ (VGAE) ä¸å­ªç”Ÿç¥ç»ç½‘ç»œ (SNN)ï¼Œæ—¨åœ¨æ— éœ€ç‰¹å®šæ¶æ„è½¯ä»¶å®¶æ—å…ˆéªŒç¤ºä¾‹çš„æƒ…å†µä¸‹è¯†åˆ«å¨èƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºå›¾ (Graph-Based) çš„ Android åº”ç”¨ç¨‹åºè¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å‡ºè‰¯æ€§è½¯ä»¶ä¸æ¶æ„è½¯ä»¶ä¹‹é—´ç»†å¾®çš„ç»“æ„å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVOLTRON åœ¨æ£€æµ‹é›¶æ—¥ (Zero-Day) æ¶æ„è½¯ä»¶æ–¹é¢çš„è¡¨ç°è¶…è¶Šäº†ç›®å‰æœ€å…ˆè¿›çš„ MaMaDroidã€‚è¯¥æ¨¡å‹å¯¹æœªçŸ¥æ¶æ„è½¯ä»¶å®¶æ—çš„æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ° 96.24%ï¼Œå¬å›ç‡è¾¾åˆ° 95.20%ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨åº”å¯¹ä¸æ–­æ¼”å˜çš„ Android å®‰å…¨å¨èƒæ—¶çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "17 pages, 6 figures, Submitted as a preprint",
      "pdf_url": "https://arxiv.org/pdf/2507.04275v1",
      "published_date": "2025-07-06 07:25:25 UTC",
      "updated_date": "2025-07-06 07:25:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:28.112103+00:00"
    },
    {
      "arxiv_id": "2507.04270v4",
      "title": "ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts",
      "title_zh": "ZEROï¼šæ”¯æŒå¤šæ¨¡æ€æç¤ºçš„å·¥ä¸šçº§è§†è§‰åŸºç¡€æ¨¡å‹",
      "authors": [
        "Sangbum Choi",
        "Kyeongryeol Go",
        "Taewoong Jang"
      ],
      "abstract": "Foundation models have revolutionized AI, yet they struggle with zero-shot deployment in real-world industrial settings due to a lack of high-quality, domain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an industry-ready vision foundation model that leverages multi-modal prompting (textual and visual) for generalization without retraining. Trained on a compact yet representative 0.9 million annotated samples from a proprietary billion-scale industrial dataset, ZERO demonstrates competitive performance on academic benchmarks like LVIS-Val and significantly outperforms existing models across 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in the CVPR 2025 Object Instance Detection Challenge and 4th place in the Foundational Few-shot Object Detection Challenge, highlighting its practical deployability and generalizability with minimal adaptation and limited data. To the best of our knowledge, ZERO is the first vision foundation model explicitly built for domain-specific, zero-shot industrial applications.",
      "tldr_zh": "è¯¥ç ”ç©¶ç”± Superb AI æå‡ºäº† ZEROï¼Œè¿™æ˜¯ä¸€æ¬¾é¢å‘å·¥ä¸šåº”ç”¨çš„è§†è§‰åŸºç¡€æ¨¡å‹ (Vision Foundation Model)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨çœŸå®å·¥ä¸šåœºæ™¯ä¸­å› ç¼ºä¹é¢†åŸŸç‰¹å®šæ•°æ®è€Œéš¾ä»¥å®ç°é›¶æ ·æœ¬ (Zero-shot) éƒ¨ç½²çš„æŒ‘æˆ˜ã€‚ZERO åˆ›æ–°æ€§åœ°åˆ©ç”¨å¤šæ¨¡æ€æç¤º (Multi-modal Prompts) æŠ€æœ¯ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æŒ‡ä»¤ï¼Œå®ç°äº†æ— éœ€é‡æ–°è®­ç»ƒçš„é«˜æ•ˆæ³›åŒ–ã€‚æ¨¡å‹é‡‡ç”¨ä»åäº¿çº§å·¥ä¸šæ•°æ®é›†ä¸­ç²¾é€‰çš„ 90 ä¸‡ä¸ªæ ‡æ³¨æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œåœ¨ 37 ä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨å­¦æœ¯åŸºå‡† LVIS-Val ä¸Šå±•ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ZERO åœ¨ CVPR 2025 ç‰©ä½“å®ä¾‹æ£€æµ‹æŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬äºŒåï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨æœ‰é™æ•°æ®ä¸‹çš„å®ç”¨æ€§å’Œéƒ¨ç½²æ½œåŠ›ã€‚ä½œä¸ºé¦–ä¸ªä¸“ä¸ºé›¶æ ·æœ¬å·¥ä¸šåº”ç”¨è®¾è®¡çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ŒZERO ä¸ºå·¥ä¸š AI çš„å¿«é€Ÿè½åœ°æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.04270v4",
      "published_date": "2025-07-06 07:03:27 UTC",
      "updated_date": "2025-11-07 06:42:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:30.657767+00:00"
    },
    {
      "arxiv_id": "2507.04252v1",
      "title": "Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images",
      "title_zh": "æ·±åº¦å­¦ä¹ è¾…åŠ©çš„è‚ºéƒ¨ CT å›¾åƒé«˜ç²¾åº¦ COVID-19 è¯Šæ–­",
      "authors": [
        "Yinuo Wang",
        "Juhyun Bae",
        "Ka Ho Chow",
        "Shenyang Chen",
        "Shreyash Gupta"
      ],
      "abstract": "COVID-19 is a severe and acute viral disease that can cause symptoms consistent with pneumonia in which inflammation is caused in the alveolous regions of the lungs leading to a build-up of fluid and breathing difficulties. Thus, the diagnosis of COVID using CT scans has been effective in assisting with RT-PCR diagnosis and severity classifications. In this paper, we proposed a new data quality control pipeline to refine the quality of CT images based on GAN and sliding windows. Also, we use class-sensitive cost functions including Label Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve the long-tail problem existing in datasets. Our model reaches more than 0.983 MCC in the benchmark test dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ·±åº¦å­¦ä¹ è¾…åŠ©çš„é«˜å‡†ç¡®åº¦ COVID-19 è¯Šæ–­æ–¹æ³•ï¼Œåˆ©ç”¨è‚ºéƒ¨ CT å›¾åƒè¾…åŠ© RT-PCR è¯Šæ–­åŠç—…æƒ…ä¸¥é‡ç¨‹åº¦åˆ†ç±»ã€‚ä¸ºäº†æå‡è¾“å…¥æ•°æ®è´¨é‡ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€å¥—ç»“åˆ GAN å’Œæ»‘åŠ¨çª—å£ï¼ˆsliding windowsï¼‰çš„æ–°å‹æ•°æ®è´¨é‡æ§åˆ¶æµæ°´çº¿ã€‚é’ˆå¯¹æ•°æ®é›†ä¸­æ™®éå­˜åœ¨çš„é•¿å°¾ï¼ˆlong-tailï¼‰åˆ†å¸ƒé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº† Label Distribution Aware Loss (LDAM Loss) å’Œ Class-balanced (CB) Loss ç­‰ç±»åˆ«æ•æ„ŸæŸå¤±å‡½æ•°è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†è¶…è¿‡ 0.983 çš„ MCC æŒ‡æ ‡ï¼Œæ˜¾è‘—æé«˜äº†æ–°å† è‚ºç‚å½±åƒè¯Šæ–­çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04252v1",
      "published_date": "2025-07-06 05:54:44 UTC",
      "updated_date": "2025-07-06 05:54:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:32.319711+00:00"
    },
    {
      "arxiv_id": "2507.04250v1",
      "title": "Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning",
      "title_zh": "Just Enough Shiftsï¼šé€šè¿‡é’ˆå¯¹æ€§è¡¨å¾å¾®è°ƒç¼“è§£å¯¹é½è¯­è¨€æ¨¡å‹çš„è¿‡åº¦æ‹’ç»",
      "authors": [
        "Mahavir Dabas",
        "Si Chen",
        "Charles Fleming",
        "Ming Jin",
        "Ruoxi Jia"
      ],
      "abstract": "Safety alignment is crucial for large language models (LLMs) to resist malicious instructions but often results in over-refusals, where benign prompts are unnecessarily rejected, impairing user experience and model utility. We introduce ACTOR (Activation-Based Training for Over-Refusal Reduction), a robust and compute- and data-efficient training framework that minimizes over-refusals by leveraging internal activation patterns from diverse queries. ACTOR precisely identifies and adjusts the activation components that trigger refusals, providing stronger control over the refusal mechanism. By fine-tuning only a single model layer, ACTOR effectively reduces over-refusals across multiple benchmarks while maintaining the model's ability to handle harmful queries and preserve overall utility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®‰å…¨å¯¹é½(Safety alignment)è¿‡ç¨‹ä¸­ç”±äºè¿‡åº¦é˜²èŒƒè€Œå¯¼è‡´è‰¯æ€§æç¤ºè¢«è¯¯æ‹’çš„è¿‡åº¦æ‹’ç»(Over-refusal)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒæ¡†æ¶ACTOR (Activation-Based Training for Over-Refusal Reduction)ã€‚ACTORåˆ©ç”¨æ¥è‡ªå¤šæ ·åŒ–æŸ¥è¯¢çš„å†…éƒ¨æ¿€æ´»æ¨¡å¼(Activation patterns)ï¼Œç²¾ç¡®è¯†åˆ«å¹¶è°ƒæ•´è§¦å‘æ‹’ç»æœºåˆ¶çš„ç‰¹å®šæ¿€æ´»ç»„ä»¶ï¼Œä»è€Œå®ç°å¯¹æ‹’ç»æœºåˆ¶æ›´å¼ºçš„æ§åˆ¶ã€‚è¯¥æ–¹æ³•å…·æœ‰æé«˜çš„è®¡ç®—ä¸æ•°æ®æ•ˆç‡ï¼Œä»…é€šè¿‡å¾®è°ƒæ¨¡å‹çš„å•ä¸ªå±‚(Single model layer)å³å¯åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½è¿‡åº¦æ‹’ç»ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒACTORåœ¨æœ‰æ•ˆå‡å°‘è¯¯æ‹’ã€æå‡ç”¨æˆ·ä½“éªŒçš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒæ¨¡å‹æ‹¦æˆªæœ‰å®³æŒ‡ä»¤çš„èƒ½åŠ›å¹¶ç»´æŠ¤å…¶æ•´ä½“æ•ˆèƒ½(Utility)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04250v1",
      "published_date": "2025-07-06 05:47:04 UTC",
      "updated_date": "2025-07-06 05:47:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:38.896452+00:00"
    },
    {
      "arxiv_id": "2507.04243v2",
      "title": "Domain Generalizable Portrait Style Transfer",
      "title_zh": "é¢†åŸŸæ³›åŒ–çš„äººåƒé£æ ¼è¿ç§»",
      "authors": [
        "Xinbo Wang",
        "Wenju Xu",
        "Qing Zhang",
        "Wei-Shi Zheng"
      ],
      "abstract": "This paper presents a portrait style transfer method that generalizes well to various different domains while enabling high-quality semantic-aligned stylization on regions including hair, eyes, eyelashes, skins, lips, and background. To this end, we propose to establish dense semantic correspondence between the given input and reference portraits based on a pre-trained model and a semantic adapter, with which we obtain a warped reference semantically aligned with the input. To ensure effective yet controllable style transfer, we devise an AdaIN-Wavelet transform to balance content preservation and stylization by blending low-frequency information of the warped reference with high-frequency information of the input in the latent space. A style adapter is also designed to provide style guidance from the warped reference. With the stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional diffusion model that integrates a ControlNet recording high-frequency information and the style guidance to generate the final result. Extensive experiments demonstrate the superiority of our method. Our code and trained model are available at https://github.com/wangxb29/DGPST.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºDGPSTçš„é¢†åŸŸé€šç”¨äººåƒé£æ ¼è¿ç§»æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°è·¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–ï¼Œå¹¶ç¡®ä¿å¤´å‘ã€çœ¼ç›ã€çš®è‚¤åŠèƒŒæ™¯ç­‰åŒºåŸŸçš„é«˜è´¨é‡è¯­ä¹‰å¯¹é½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œè¯­ä¹‰é€‚é…å™¨(semantic adapter)å»ºç«‹è¾“å…¥ä¸å‚è€ƒå›¾åƒé—´çš„å¯†é›†è¯­ä¹‰å¯¹åº”ï¼Œç”Ÿæˆè¯­ä¹‰å¯¹é½çš„æ‰­æ›²å‚è€ƒå›¾ã€‚ä¸ºäº†å¹³è¡¡å†…å®¹ä¿ç•™ä¸é£æ ¼åŒ–æ•ˆæœï¼Œç ”ç©¶è€…è®¾è®¡äº†AdaIN-Waveletå˜æ¢ï¼Œåœ¨æ½œç©ºé—´ä¸­èåˆå‚è€ƒå›¾çš„ä½é¢‘ä¿¡æ¯ä¸è¾“å…¥å›¾çš„é«˜é¢‘ä¿¡æ¯ï¼Œå¹¶é…åˆé£æ ¼é€‚é…å™¨(style adapter)æä¾›å¼•å¯¼ã€‚æœ€åï¼Œé€šè¿‡ä¸€ä¸ªé›†æˆControlNetçš„åŒæ¡ä»¶æ‰©æ•£æ¨¡å‹(dual-conditional diffusion model)ç”Ÿæˆæœ€ç»ˆç»“æœï¼Œä»¥æ•æ‰é«˜é¢‘ç»†èŠ‚å’Œé£æ ¼ç‰¹å¾ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤šç§é¢†åŸŸä¸‹çš„ä¼˜è¶Šæ€§ï¼Œä¸ºé«˜è´¨é‡ã€å¯æ§çš„äººåƒé£æ ¼è¿ç§»æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04243v2",
      "published_date": "2025-07-06 04:56:25 UTC",
      "updated_date": "2025-07-08 02:18:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:42.877943+00:00"
    },
    {
      "arxiv_id": "2507.04239v1",
      "title": "Scaling Context Requires Rethinking Attention",
      "title_zh": "æ‰©å±•ä¸Šä¸‹æ–‡éœ€è¦é‡æ–°å®¡è§†æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Carles Gelada",
        "Jacob Buckman",
        "Sean Zhang",
        "Txus Bach"
      ],
      "abstract": "We argue that neither transformers nor sub-quadratic architectures are well suited to training at long sequence lengths: the cost of processing the context is too expensive in the former, too inexpensive in the latter. Approaches such as sliding window attention which reduce the cost-per-token of a transformer impair in-context learning, and so are also unsuitable. To address these limitations, we introduce power attention, an architectural layer for linear-cost sequence modeling whose state size can be adjusted independently of parameters, unlocking the advantages of linear attention on practical domains. We develop and open-source a set of GPU kernels for efficient power attention, identifying a novel pattern of operation fusion to avoid memory and bandwidth bottlenecks. Our experiments on the in-context learning of power attention shows that these models dominate both exponential attention and linear attention at long-context training.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼ŒTransformersç”±äºå¤„ç†ä¸Šä¸‹æ–‡æˆæœ¬è¿‡é«˜ï¼Œè€ŒäºšäºŒæ¬¡æ–¹(sub-quadratic)æ¶æ„æˆæœ¬è¿‡ä½ä¸”èƒ½åŠ›å—é™ï¼Œå‡ä¸é€‚åˆé•¿åºåˆ—è®­ç»ƒã€‚è™½ç„¶æ»‘åŠ¨çª—å£æ³¨æ„(sliding window attention)ç­‰æ–¹æ³•èƒ½é™ä½æˆæœ¬ï¼Œä½†ä¼šæŸå®³ä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†power attentionï¼Œè¿™æ˜¯ä¸€ç§çº¿æ€§æˆæœ¬çš„åºåˆ—å»ºæ¨¡æ¶æ„å±‚ï¼Œå…¶çŠ¶æ€å¤§å°(state size)å¯ç‹¬ç«‹äºå‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œåœ¨å®é™…é¢†åŸŸè§£é”äº†çº¿æ€§æ³¨æ„(linear attention)çš„ä¼˜åŠ¿ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘å¹¶å¼€æºäº†ä¸€å¥—é«˜æ•ˆçš„GPUå†…æ ¸ï¼Œé€šè¿‡æ–°é¢–çš„ç®—å­èåˆ(operation fusion)æ¨¡å¼è§„é¿äº†å†…å­˜å’Œå¸¦å®½ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸­ï¼Œpower attentionåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢çš„è¡¨ç°ä¼˜äºæŒ‡æ•°æ³¨æ„(exponential attention)å’Œä¼ ç»Ÿçš„çº¿æ€§æ³¨æ„ï¼Œå…·æœ‰æ˜æ˜¾çš„æ¶æ„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04239v1",
      "published_date": "2025-07-06 04:15:34 UTC",
      "updated_date": "2025-07-06 04:15:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:42.336807+00:00"
    },
    {
      "arxiv_id": "2507.04235v1",
      "title": "Design Optimization of Three-Dimensional Wire Arrangement Considering Wire Crossings for Tendon-driven Robots",
      "title_zh": "è€ƒè™‘çº¿ç¼†äº¤å‰çš„è…±é©±åŠ¨æœºå™¨äººä¸‰ç»´èµ°çº¿å¸ƒå±€ä¼˜åŒ–è®¾è®¡",
      "authors": [
        "Kento Kawaharazuka",
        "Shintaro Inoue",
        "Yuta Sahara",
        "Keita Yoneda",
        "Temma Suzuki",
        "Kei Okada"
      ],
      "abstract": "Tendon-driven mechanisms are useful from the perspectives of variable stiffness, redundant actuation, and lightweight design, and they are widely used, particularly in hands, wrists, and waists of robots. The design of these wire arrangements has traditionally been done empirically, but it becomes extremely challenging when dealing with complex structures. Various studies have attempted to optimize wire arrangement, but many of them have oversimplified the problem by imposing conditions such as restricting movements to a 2D plane, keeping the moment arm constant, or neglecting wire crossings. Therefore, this study proposes a three-dimensional wire arrangement optimization that takes wire crossings into account. We explore wire arrangements through a multi-objective black-box optimization method that ensures wires do not cross while providing sufficient joint torque along a defined target trajectory. For a 3D link structure, we optimize the wire arrangement under various conditions, demonstrate its effectiveness, and discuss the obtained design solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è…±é©±åŠ¨æœºæ„ (Tendon-driven mechanisms) åœ¨å¤æ‚ç»“æ„ä¸­å¸ƒçº¿è®¾è®¡å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è€ƒè™‘çº¿ç¼†äº¤å‰ (wire crossings) çš„ä¸‰ç»´å¸ƒçº¿ä¼˜åŒ–æ–¹æ³•ã€‚ä¼ ç»Ÿå¸ƒçº¿è®¾è®¡å¤šä¾èµ–ç»éªŒï¼Œä¸”ç°æœ‰ä¼˜åŒ–ç ”ç©¶å¾€å¾€ç®€åŒ–ä¸ºäºŒç»´å¹³é¢æˆ–å¿½ç•¥çº¿ç¼†å¹²æ¶‰ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å¤šç›®æ ‡é»‘ç›’ä¼˜åŒ– (multi-objective black-box optimization) æ–¹æ³•ï¼Œåœ¨ç¡®ä¿çº¿ç¼†ä¸å‘ç”Ÿç‰©ç†äº¤å‰çš„åŒæ—¶ï¼Œä¿è¯æœºå™¨äººåœ¨ç›®æ ‡è½¨è¿¹ä¸Šæ‹¥æœ‰å……è¶³çš„å…³èŠ‚æ‰­çŸ© (joint torque)ã€‚ç ”ç©¶é€šè¿‡å¯¹ 3D è¿æ†ç»“æ„çš„ä¼˜åŒ–å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ¡ˆåœ¨å¤„ç†å¤æ‚ä¸‰ç»´å¸ƒçº¿æ—¶çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æˆæœä¸ºå¼€å‘å…·å¤‡å˜åˆšåº¦å’Œè½»é‡åŒ–ç‰¹å¾çš„é«˜æ€§èƒ½æœºå™¨äººæä¾›äº†é‡è¦çš„è®¾è®¡æ”¯æŒä¸ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at IROS2025, Website - https://haraduka.github.io/muscle-3d-opt/ , YouTube - https://www.youtube.com/watch?v=cy510s-kOaY",
      "pdf_url": "https://arxiv.org/pdf/2507.04235v1",
      "published_date": "2025-07-06 03:56:01 UTC",
      "updated_date": "2025-07-06 03:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:53.537735+00:00"
    },
    {
      "arxiv_id": "2507.05296v1",
      "title": "Integrating Generative AI in BIM Education: Insights from Classroom Implementation",
      "title_zh": "å°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èå…¥ BIM æ•™è‚²ï¼šè¯¾å ‚æ•™å­¦å®è·µçš„å¯ç¤º",
      "authors": [
        "Islem Sahraoui",
        "Kinam Kim",
        "Lu Gao",
        "Zia Din",
        "Ahmed Senouci"
      ],
      "abstract": "This study evaluates the implementation of a Generative AI-powered rule checking workflow within a graduate-level Building Information Modeling (BIM) course at a U.S. university. Over two semesters, 55 students participated in a classroom-based pilot exploring the use of GenAI for BIM compliance tasks, an area with limited prior research. The instructional design included lectures on prompt engineering and AI-driven rule checking, followed by an assignment where students used a large language model (LLM) to identify code violations in designs using Autodesk Revit. Surveys and interviews were conducted to assess student workload, learning effectiveness, and overall experience, using the NASA-TLX scale and regression analysis. Findings indicate students generally achieved learning objectives but faced challenges such as difficulties debugging AI-generated code and inconsistent tool performance, probably due to their limited prompt engineering experience. These issues increased cognitive and emotional strain, especially among students with minimal programming backgrounds. Despite these challenges, students expressed strong interest in future GenAI applications, particularly with clear instructional support.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åœ¨ç¾å›½æŸå¤§å­¦ç ”ç©¶ç”Ÿçº§åˆ«çš„Building Information Modeling (BIM)è¯¾ç¨‹ä¸­ï¼Œå®æ–½Generative AIé©±åŠ¨çš„è§„åˆ™æ£€æŸ¥å·¥ä½œæµçš„æ•ˆæœã€‚å…±æœ‰55åå­¦ç”Ÿå‚ä¸äº†ä¸ºæœŸä¸¤ä¸ªå­¦æœŸçš„è¯¾å ‚è¯•ç‚¹ï¼Œæ¢ç´¢åˆ©ç”¨Large Language Model (LLM)åœ¨Autodesk Revitè®¾è®¡ä¸­è¯†åˆ«è§„èŒƒè¿è§„ã€‚æ•™å­¦è®¾è®¡ç»“åˆäº†Prompt Engineeringè®²åº§ä¸å®è·µä»»åŠ¡ï¼Œå¹¶é€šè¿‡NASA-TLXé‡è¡¨å’Œå›å½’åˆ†æç³»ç»Ÿè¯„ä¼°äº†å­¦ç”Ÿçš„ä»»åŠ¡è´Ÿè·ã€å­¦ä¹ æ•ˆæœåŠæ•´ä½“ä½“éªŒã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å­¦ç”Ÿæ™®éèƒ½å¤Ÿè¾¾æˆå­¦ä¹ ç›®æ ‡ï¼Œä½†åœ¨è°ƒè¯•AIç”Ÿæˆçš„ä»£ç ä»¥åŠåº”å¯¹å·¥å…·æ€§èƒ½ä¸ç¨³å®šæ€§æ–¹é¢é‡åˆ°äº†æ˜¾è‘—å›°éš¾ã€‚è¿™ç§å›°éš¾åœ¨ç¼–ç¨‹åŸºç¡€è¾ƒè–„å¼±çš„å­¦ç”Ÿç¾¤ä½“ä¸­å¼•å‘äº†è¾ƒé«˜çš„è®¤çŸ¥ä¸æƒ…æ„Ÿå‹åŠ›ï¼Œæš´éœ²å‡ºPrompt Engineeringç»éªŒä¸è¶³çš„é—®é¢˜ã€‚å°½ç®¡é¢ä¸´ä¸Šè¿°æŒ‘æˆ˜ï¼Œå­¦ç”Ÿå¯¹æœªæ¥Generative AIåœ¨BIMé¢†åŸŸçš„åº”ç”¨ä»æŒæœ‰å¼ºçƒˆå…´è¶£ï¼Œå¹¶æŒ‡å‡ºäº†å……è¶³çš„æ•™å­¦æ”¯æŒå¯¹æˆåŠŸå®æ–½è¯¥æŠ€æœ¯çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.05296v1",
      "published_date": "2025-07-06 03:41:04 UTC",
      "updated_date": "2025-07-06 03:41:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:48:59.312967+00:00"
    },
    {
      "arxiv_id": "2507.04230v1",
      "title": "High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics",
      "title_zh": "è·¨æˆ¿é—´å£°å­¦ç¯å¢ƒä¸‹çš„é’¢ç´éŸ³é¢‘é«˜åˆ†è¾¨ç‡å»¶éŸ³è¸æ¿æ·±åº¦ä¼°è®¡",
      "authors": [
        "Kun Fang",
        "Hanwen Zhang",
        "Ziyu Wang",
        "Ichiro Fujinaga"
      ],
      "abstract": "Piano sustain pedal detection has previously been approached as a binary on/off classification task, limiting its application in real-world piano performance scenarios where pedal depth significantly influences musical expression. This paper presents a novel approach for high-resolution estimation that predicts continuous pedal depth values. We introduce a Transformer-based architecture that not only matches state-of-the-art performance on the traditional binary classification task but also achieves high accuracy in continuous pedal depth estimation. Furthermore, by estimating continuous values, our model provides musically meaningful predictions for sustain pedal usage, whereas baseline models struggle to capture such nuanced expressions with their binary detection approach. Additionally, this paper investigates the influence of room acoustics on sustain pedal estimation using a synthetic dataset that includes varied acoustic conditions. We train our model with different combinations of room settings and test it in an unseen new environment using a \"leave-one-out\" approach. Our findings show that the two baseline models and ours are not robust to unseen room conditions. Statistical analysis further confirms that reverberation influences model predictions and introduces an overestimation bias.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹é’¢ç´å»¶éŸ³è¸æ¿(Sustain Pedal)æ·±åº¦ä¼°è®¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„è¿ç»­æ•°å€¼é¢„æµ‹æ–¹æ³•ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿç ”ç©¶ä¸­å°†å…¶ç®€åŒ–ä¸ºäºŒåˆ†ç±»ä»»åŠ¡çš„å±€é™ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨ä¼ ç»Ÿçš„è¸æ¿å¼€å…³æ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)æ€§èƒ½ï¼Œè¿˜èƒ½ç²¾ç¡®æ•æ‰åˆ°å¯¹éŸ³ä¹è¡¨è¾¾è‡³å…³é‡è¦çš„ç»†å¾®æ·±åº¦å˜åŒ–ã€‚è®ºæ–‡é€šè¿‡æ„å»ºåŒ…å«å¤šæ ·åŒ–å£°å­¦æ¡ä»¶çš„åˆæˆæ•°æ®é›†ï¼Œæ·±å…¥æ¢è®¨äº†æˆ¿é—´å£°å­¦(Room Acoustics)å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒé‡‡ç”¨ç•™ä¸€æ³•(Leave-one-out)éªŒè¯å‘ç°ï¼Œå½“å‰æ¨¡å‹åœ¨é¢å¯¹æœªè§è¿‡çš„æˆ¿é—´ç¯å¢ƒæ—¶æ™®éå­˜åœ¨é²æ£’æ€§(Robustness)ä¸è¶³çš„é—®é¢˜ã€‚ç»Ÿè®¡åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œæ··å“(Reverberation)ä¼šæ˜¾è‘—å¹²æ‰°é¢„æµ‹ç»“æœå¹¶å¯¼è‡´é«˜ä¼°åå·®(Overestimation Bias)ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ›´é«˜åˆ†è¾¨ç‡çš„é’¢ç´æ¼”å¥åˆ†ææä¾›äº†æŠ€æœ¯æ”¯æ’‘ï¼Œå¹¶æ­ç¤ºäº†ç¯å¢ƒå£°å­¦å› ç´ å¸¦æ¥çš„å…³é”®æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04230v1",
      "published_date": "2025-07-06 03:40:54 UTC",
      "updated_date": "2025-07-06 03:40:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:00.244817+00:00"
    },
    {
      "arxiv_id": "2507.08013v2",
      "title": "MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model",
      "title_zh": "MedicalBERTï¼šåˆ©ç”¨é¢„è®­ç»ƒ BERT æ¨¡å‹å¢å¼ºç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†",
      "authors": [
        "K. Sahit Reddy",
        "N. Ragavenderan",
        "Vasanth K.",
        "Ganesh N. Naik",
        "Vishalakshi Prabhu",
        "Nagaraja G. S"
      ],
      "abstract": "Recent advances in natural language processing (NLP) have been driven bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel at understanding complex texts, but biomedical literature, withits domain-specific terminology, poses challenges that models likeWord2Vec and bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5, despite capturing context, fall short in tasks needingbidirectional understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a pretrained BERT model trained on a large biomedicaldataset and equipped with domain-specific vocabulary that enhances thecomprehension of biomedical terminology. MedicalBERT model is furtheroptimized and fine-tuned to address diverse tasks, including named entityrecognition, relation extraction, question answering, sentence similarity, anddocument classification. Performance metrics such as the F1-score,accuracy, and Pearson correlation are employed to showcase the efficiencyof our model in comparison to other BERT-based models such as BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost of the benchmarks, and surpasses the general-purpose BERT model by5.67% on average across all the tasks evaluated respectively. This work alsounderscores the potential of leveraging pretrained BERT models for medicalNLP tasks, demonstrating the effectiveness of transfer learning techniques incapturing domain-specific information.\n  (PDF) MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model. Available from: https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model [accessed Jul 06 2025].",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MedicalBERTï¼Œä¸€ç§åŸºäºBERTçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­é¢†åŸŸç‰¹å®šæœ¯è¯­ç†è§£çš„æŒ‘æˆ˜ã€‚MedicalBERTé€šè¿‡åœ¨å¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é…å¤‡ä¸“é—¨çš„é¢†åŸŸè¯æ±‡è¡¨ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®çš„ç†è§£æ·±åº¦ã€‚è¯¥æ¨¡å‹ç»è¿‡ä¼˜åŒ–ä¸å¾®è°ƒï¼Œèƒ½å¤Ÿèƒœä»»å‘½åå®ä½“è¯†åˆ«(named entity recognition)ã€å…³ç³»æŠ½å–(relation extraction)ã€é—®ç­”(question answering)ä»¥åŠæ–‡æ¡£åˆ†ç±»ç­‰å¤šé¡¹ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedicalBERTåœ¨å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½ä¼˜äºBioBERTã€SciBERTå’ŒClinicalBERTï¼Œä¸”æ¯”é€šç”¨BERTæ¨¡å‹åœ¨è¯„ä¼°ä»»åŠ¡ä¸­å¹³å‡æå‡äº†5.67%ã€‚è¯¥ç ”ç©¶ä¸ä»…å±•ç¤ºäº†é¢„è®­ç»ƒBERTæ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„å‰æ™¯ï¼Œè¿˜è¯æ˜äº†è¿ç§»å­¦ä¹ æŠ€æœ¯åœ¨æ•è·ç‰¹å®šé¢†åŸŸä¿¡æ¯æ–¹é¢çš„å“è¶Šæˆæ•ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.08013v2",
      "published_date": "2025-07-06 03:38:05 UTC",
      "updated_date": "2025-07-25 04:44:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:21.569557+00:00"
    },
    {
      "arxiv_id": "2507.04227v1",
      "title": "Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties",
      "title_zh": "Hijacking JARVISï¼šé’ˆå¯¹éç‰¹æƒç¬¬ä¸‰æ–¹çš„ç§»åŠ¨ç«¯ GUI æ™ºèƒ½ä½“åŸºå‡†è¯„ä¼°",
      "authors": [
        "Guohong Liu",
        "Jialei Ye",
        "Jiacheng Liu",
        "Yuanchun Li",
        "Wei Liu",
        "Pengzhi Gao",
        "Jian Luan",
        "Yunxin Liu"
      ],
      "abstract": "Mobile GUI agents are designed to autonomously execute diverse device-control tasks by interpreting and interacting with mobile screens. Despite notable advancements, their resilience in real-world scenarios where screen content may be partially manipulated by untrustworthy third parties remains largely unexplored. Owing to their black-box and autonomous nature, these agents are vulnerable to manipulations that could compromise user devices. In this work, we present the first systematic investigation into the vulnerabilities of mobile GUI agents. We introduce a scalable attack simulation framework AgentHazard, which enables flexible and targeted modifications of screen content within existing applications. Leveraging this framework, we develop a comprehensive benchmark suite comprising both a dynamic task execution environment and a static dataset of vision-language-action tuples, totaling over 3,000 attack scenarios. The dynamic environment encompasses 58 reproducible tasks in an emulator with various types of hazardous UI content, while the static dataset is constructed from 210 screenshots collected from 14 popular commercial apps. Importantly, our content modifications are designed to be feasible for unprivileged third parties. We evaluate 7 widely-used mobile GUI agents and 5 common backbone models using our benchmark. Our findings reveal that all examined agents are significantly influenced by misleading third-party content (with an average misleading rate of 28.8% in human-crafted attack scenarios) and that their vulnerabilities are closely linked to the employed perception modalities and backbone LLMs. Furthermore, we assess training-based mitigation strategies, highlighting both the challenges and opportunities for enhancing the robustness of mobile GUI agents. Our code and data will be released at https://agenthazard.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§»åŠ¨ç«¯å›¾å½¢ç”¨æˆ·ç•Œé¢æ™ºèƒ½ä½“ (Mobile GUI agents) åœ¨ç°å®åœºæ™¯ä¸­é¢ä¸´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„æ¼æ´è°ƒæŸ¥ã€‚ä½œè€…æå‡ºäº† AgentHazard æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„æ”»å‡»æ¨¡æ‹Ÿæ¡†æ¶ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿéç‰¹æƒç¬¬ä¸‰æ–¹é€šè¿‡ä¿®æ”¹å±å¹•å†…å®¹å¯¹æ™ºèƒ½ä½“è¿›è¡Œçš„æ¶æ„å¼•å¯¼ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŠ¨æ€ä»»åŠ¡æ‰§è¡Œç¯å¢ƒå’Œé™æ€æ•°æ®é›†çš„ç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ¶µç›–äº† 3,000 å¤šä¸ªæ”»å‡»åœºæ™¯å’Œ 58 é¡¹å¯é‡ç°çš„ä»»åŠ¡ã€‚å®éªŒå¯¹ 7 ç§å¹¿æ³›ä½¿ç”¨çš„ Mobile GUI agents å’Œ 5 ç§ä¸»æµåº•å±‚æ¨¡å‹ (Backbone models) è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ‰€æœ‰å—æµ‹æ™ºèƒ½ä½“å‡å—åˆ°ç¬¬ä¸‰æ–¹è¯¯å¯¼æ€§å†…å®¹çš„æ˜¾è‘—å½±å“ã€‚åœ¨äººå·¥æ„å»ºçš„æ”»å‡»åœºæ™¯ä¸­ï¼Œå¹³å‡è¯¯å¯¼ç‡è¾¾åˆ° 28.8%ï¼Œä¸”ç ”ç©¶å‘ç°å…¶æ¼æ´ä¸æ™ºèƒ½ä½“é‡‡ç”¨çš„æ„ŸçŸ¥æ¨¡æ€ (Perception modalities) å’Œåº•å±‚å¤§è¯­è¨€æ¨¡å‹ (Backbone LLMs) å¯†åˆ‡ç›¸å…³ã€‚è¯¥é¡¹å·¥ä½œè¿˜è¯„ä¼°äº†åŸºäºè®­ç»ƒçš„ç¼“è§£ç­–ç•¥ï¼Œä¸ºå¢å¼ºç§»åŠ¨ç«¯æ™ºèƒ½ä½“çš„ç¨³å¥æ€§ (Robustness) æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04227v1",
      "published_date": "2025-07-06 03:31:36 UTC",
      "updated_date": "2025-07-06 03:31:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:08.251246+00:00"
    },
    {
      "arxiv_id": "2507.04225v2",
      "title": "Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints",
      "title_zh": "åŸºäºå¯ç»„åˆå‡ ä½•çº¦æŸçš„é›¶æ ·æœ¬ç¯è‚½è®¾è®¡",
      "authors": [
        "Dapeng Jiang",
        "Xiangzhe Kong",
        "Jiaqi Han",
        "Mingyu Li",
        "Rui Jiao",
        "Wenbing Huang",
        "Stefano Ermon",
        "Jianzhu Ma",
        "Yang Liu"
      ],
      "abstract": "Cyclic peptides, characterized by geometric constraints absent in linear peptides, offer enhanced biochemical properties, presenting new opportunities to address unmet medical needs. However, designing target-specific cyclic peptides remains underexplored due to limited training data. To bridge the gap, we propose CP-Composer, a novel generative framework that enables zero-shot cyclic peptide generation via composable geometric constraints. Our approach decomposes complex cyclization patterns into unit constraints, which are incorporated into a diffusion model through geometric conditioning on nodes and edges. During training, the model learns from unit constraints and their random combinations in linear peptides, while at inference, novel constraint combinations required for cyclization are imposed as input. Experiments show that our model, despite trained with linear peptides, is capable of generating diverse target-binding cyclic peptides, reaching success rates from 38% to 84% on different cyclization strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CP-Composerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°é›¶æ ·æœ¬(zero-shot)ç¯è‚½è®¾è®¡çš„åˆ›æ–°ç”Ÿæˆæ¡†æ¶ã€‚ç¯è‚½ç”±äºå…¶ç‹¬ç‰¹çš„å‡ ä½•çº¦æŸåœ¨ç”ŸåŒ–æ€§èƒ½ä¸Šä¼˜äºçº¿æ€§è‚½ï¼Œä½†å› è®­ç»ƒæ•°æ®ç¨€ç¼ºå¯¼è‡´è®¾è®¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚CP-Composeré€šè¿‡å°†å¤æ‚çš„ç¯åŒ–æ¨¡å¼åˆ†è§£ä¸ºå•å…ƒçº¦æŸï¼Œå¹¶å°†å…¶ä»¥å‡ ä½•æ¡ä»¶åŒ–çš„æ–¹å¼æ•´åˆè¿›æ‰©æ•£æ¨¡å‹(diffusion model)çš„èŠ‚ç‚¹å’Œè¾¹ä¸­ã€‚è™½ç„¶è¯¥æ¨¡å‹ä»…åœ¨çº¿æ€§è‚½æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿé€šè¿‡ç»„åˆç‰¹å®šçš„å‡ ä½•çº¦æŸç”Ÿæˆå…¨æ–°çš„ç¯è‚½ç»“æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒç¯åŒ–ç­–ç•¥ä¸‹çš„æˆåŠŸç‡è¾¾åˆ°äº†38%è‡³84%ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ä¸”å…·æœ‰é¶å‘ç»“åˆèƒ½åŠ›çš„ç¯è‚½ï¼Œä¸ºè§£å†³ä¸´åºŠæœªæ»¡è¶³çš„éœ€æ±‚æä¾›äº†æ–°çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04225v2",
      "published_date": "2025-07-06 03:30:45 UTC",
      "updated_date": "2025-07-14 14:33:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:09.184220+00:00"
    },
    {
      "arxiv_id": "2507.04224v3",
      "title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜æ ¡å›¾ä¹¦é¦†å‚è€ƒå’¨è¯¢æœåŠ¡ä¸­çš„å…¬å¹³æ€§è¯„ä¼°",
      "authors": [
        "Haining Wang",
        "Jason Clark",
        "Yueru Yan",
        "Star Bradley",
        "Ruiyang Chen",
        "Yiqiong Zhang",
        "Hengyi Fu",
        "Zuoyu Tian"
      ],
      "abstract": "As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† Large Language Models (LLMs) åœ¨å­¦æœ¯å›¾ä¹¦é¦†å‚è€ƒå’¨è¯¢æœåŠ¡ä¸­æ˜¯å¦èƒ½å¤Ÿå…¬å¹³åœ°å¯¹å¾…ä¸åŒèƒŒæ™¯çš„ç”¨æˆ·ã€‚ç ”ç©¶è€…é€šè¿‡æç¤ºå…­ç§æœ€å…ˆè¿›çš„ LLMs ååŠ©å…·æœ‰ä¸åŒæ€§åˆ«ã€ç§æ—/æ°‘æ—å’Œæœºæ„è§’è‰²çš„è¯»è€…ï¼Œä»¥æ£€æµ‹å…¶å›å¤æ˜¯å¦å­˜åœ¨å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜æ¨¡å‹åœ¨ç§æ—æˆ–æ°‘æ—ç»´åº¦ä¸Šå­˜åœ¨æ­§è§†ï¼Œä»…æœ‰ä¸€ä¸ªæ¨¡å‹åœ¨é’ˆå¯¹å¥³æ€§æ—¶è¡¨ç°å‡ºè½»å¾®çš„ Stereotypical Biasã€‚LLMs èƒ½å¤Ÿæ ¹æ®è¯»è€…çš„æœºæ„è§’è‰²ï¼Œåœ¨è¯­è¨€è¡¨è¾¾çš„ Formalityã€Politeness å’Œ Domain-specific Vocabularies æ–¹é¢åšå‡ºç»†å¾®è°ƒæ•´ï¼Œè¿™åæ˜ äº†ä¸“ä¸šè§„èŒƒè€Œéæ­§è§†æ€§å¯¹å¾…ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç›®å‰çš„ LLMs åœ¨å­¦æœ¯å›¾ä¹¦é¦†å‚è€ƒæœåŠ¡ä¸­å±•ç°å‡ºè¾ƒé«˜çš„å°±ç»ªåº¦ï¼Œèƒ½å¤Ÿæ”¯æŒå…¬å¹³ä¸”ç¬¦åˆè¯­å¢ƒçš„ä¸“ä¸šäº¤æµã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04224v3",
      "published_date": "2025-07-06 03:28:24 UTC",
      "updated_date": "2025-11-21 15:33:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:16.243987+00:00"
    },
    {
      "arxiv_id": "2507.04221v2",
      "title": "Context Tuning for In-Context Optimization",
      "title_zh": "é¢å‘ä¸Šä¸‹æ–‡ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡å¾®è°ƒ",
      "authors": [
        "Jack Lu",
        "Ryan Teehan",
        "Zhenbang Yang",
        "Mengye Ren"
      ],
      "abstract": "We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model's inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Context Tuningï¼Œä¸€ç§æ—¨åœ¨ä¸å¾®è°ƒæ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å°‘æ ·æœ¬è‡ªé€‚åº”èƒ½åŠ›çš„ç®€å•æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿ prompt-based æ–¹æ³•ä½¿ç”¨æ— å…³ token åˆå§‹åŒ–ä¸åŒï¼ŒContext Tuning åˆ©ç”¨ç‰¹å®šä»»åŠ¡çš„ç¤ºä¾‹æ¥åˆå§‹åŒ–å¯è®­ç»ƒçš„ prompt æˆ– prefixã€‚è¯¥æ–¹æ³•é€šè¿‡å‘æŒ¥æ¨¡å‹å›ºæœ‰çš„ In-Context Learning (ICL) èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°æå–ç›¸å…³ä»»åŠ¡ä¿¡æ¯ä»¥æå‡å­¦ä¹ æ•ˆæœã€‚åœ¨ CrossFitã€UnifiedQAã€MMLUã€BIG-Bench Hard å’Œ ARC ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒContext Tuning çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æç¤ºè¯è‡ªé€‚åº”æŠ€æœ¯ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼ŒContext Tuning åœ¨è¾¾åˆ°ä¸ Test-Time Training ç›¸å½“çš„å‡†ç¡®ç‡æ—¶ï¼Œå…·å¤‡æ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "A short version of this paper was accepted at ICML 2025 Workshop on Test-Time Adaptation",
      "pdf_url": "https://arxiv.org/pdf/2507.04221v2",
      "published_date": "2025-07-06 03:23:53 UTC",
      "updated_date": "2025-10-31 23:27:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:17.638321+00:00"
    },
    {
      "arxiv_id": "2507.04219v3",
      "title": "Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs",
      "title_zh": "æ¨¡å‹åç¼©å¹¶éç¼ºé™·ï¼Œè€Œæ˜¯å¤§è¯­è¨€æ¨¡å‹æœºå™¨é—å¿˜çš„ä¸€é¡¹ç‰¹æ€§",
      "authors": [
        "Yan Scholten",
        "Sophie Xhonneux",
        "Leo Schwinn",
        "Stephan GÃ¼nnemann"
      ],
      "abstract": "Current unlearning methods for LLMs optimize on the private information they seek to remove by incorporating it into their fine-tuning data. We argue this not only risks reinforcing exposure to sensitive data, it also fundamentally contradicts the principle of minimizing its use. As a remedy, we propose a novel unlearning method-Partial Model Collapse (PMC), which does not require unlearning targets in the unlearning objective. Our approach is inspired by recent observations that training generative models on their own generations leads to distribution collapse, effectively removing information from model outputs. Our central insight is that model collapse can be leveraged for machine unlearning by deliberately triggering it for data we aim to remove. We theoretically analyze that our approach converges to the desired outcome, i.e. the model unlearns the data targeted for removal. We empirically demonstrate that PMC overcomes three key limitations of existing unlearning methods that explicitly optimize on unlearning targets, and more effectively removes private information from model outputs while preserving general model utility. Overall, our contributions represent an important step toward more comprehensive unlearning that aligns with real-world privacy constraints. Code available at https://www.cs.cit.tum.de/daml/partial-model-collapse/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æœºå™¨å¸è½½(Machine Unlearning)é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•åœ¨ä¼˜åŒ–ç›®æ ‡ä¸­ç›´æ¥ä½¿ç”¨ç§æœ‰ä¿¡æ¯å­˜åœ¨å¼ºåŒ–æ³„éœ²çš„é£é™©ä¸”è¿åäº†æœ€å°åŒ–åŸåˆ™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Partial Model Collapse (PMC)è¿™ä¸€æ–°é¢–æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæ— éœ€åœ¨å¸è½½ç›®æ ‡ä¸­åŒ…å«å¾…åˆ é™¤çš„åŸå§‹æ•°æ®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†ç”Ÿæˆæ¨¡å‹åœ¨è‡ªèº«ç”Ÿæˆå†…å®¹ä¸Šè®­ç»ƒä¼šå¯¼è‡´åˆ†å¸ƒåç¼©(Model Collapse)çš„ç‰¹æ€§ï¼Œé€šè¿‡åˆ»æ„è§¦å‘ç‰¹å®šæ•°æ®çš„åç¼©æ¥å®ç°ä¿¡æ¯ç§»é™¤ã€‚ç†è®ºåˆ†æè¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿæ”¶æ•›è‡³é¢„æœŸçš„å¸è½½çŠ¶æ€ï¼Œç¡®ä¿æ¨¡å‹é—å¿˜ç›®æ ‡æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPMCåœ¨æ›´æœ‰æ•ˆæ¸…é™¤ç§æœ‰ä¿¡æ¯çš„åŒæ—¶ï¼Œèƒ½æ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„é€šç”¨æ•ˆç”¨(Model Utility)ï¼Œä¸ºç¬¦åˆç°å®éšç§çº¦æŸçš„æœºå™¨å¸è½½æŠ€æœ¯æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04219v3",
      "published_date": "2025-07-06 03:08:49 UTC",
      "updated_date": "2025-09-27 08:05:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:22.755988+00:00"
    },
    {
      "arxiv_id": "2507.04206v1",
      "title": "Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒåŠ¨åŠ›å­¦ä¸­çš„å§†æ½˜å·´æ•ˆåº”ï¼šå±±è°·-æ²³æµæ¨¡å‹çš„æç®€åˆ†æ",
      "authors": [
        "Sibei Liu",
        "Zhijian Hu"
      ],
      "abstract": "Learning rate (LR) schedules in large language model (LLM) training often follow empirical templates: warm-up, constant plateau/stable phase, and decay (WSD). However, the mechanistic explanation for this strategy remains underexplored, and the choice of plateau height and decay schedule is largely heuristic. In this paper, we connect training dynamics to a thermodynamic analogy via the Mpemba effect - a phenomenon in which a hotter system cools faster than a colder one when quenched into the same bath. We analyze a class of \"valley-river\" loss landscapes, where sharp (valley) directions equilibrate quickly, while flatter (river) directions govern global descent. The Mpemba effect provides an explanation for the necessity of the warm-up phase and motivates a high plateau - rather than a low one - for accelerating loss decrease during decay. We show that for certain loss landscapes, there exists an optimal plateau learning rate - the \"strong Mpemba point\" - at which the slowest mode vanishes, resulting in faster convergence during the decay phase. We derive analytical conditions for its existence and estimate decay dynamics required to preserve the Mpemba advantage. Our minimal model and analysis offer a principled justification for plateau-based schedulers and provide guidance for tuning LR in LLMs with minimal hyperparameter sweep.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ç±»æ¯”çƒ­åŠ›å­¦ä¸­çš„ Mpemba effectï¼ˆå§†æ½˜å·´æ•ˆåº”ï¼‰ï¼Œå³é«˜æ¸©ç³»ç»Ÿåœ¨ç›¸åŒå†·ç¯å¢ƒä¸­æ¯”ä½æ¸©ç³»ç»Ÿå†·å´æ›´å¿«çš„ç°è±¡ï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒåŠ¨åŠ›å­¦ä¸­å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLR schedulesï¼‰çš„å†…åœ¨æœºåˆ¶ã€‚ä½œè€…åˆ©ç”¨ \"valley-river\" æŸå¤±å‡½æ•°åœ°å½¢åˆ†æäº†ä¸åŒç»´åº¦çš„æ¼”åŒ–é€Ÿç‡ï¼Œä¸º Warm-up é˜¶æ®µçš„å¿…è¦æ€§æä¾›äº†ç‰©ç†è§£é‡Šï¼Œå¹¶é˜æ˜äº†è¾ƒé«˜çš„ Plateau å­¦ä¹ ç‡ç›¸è¾ƒäºä½å­¦ä¹ ç‡åœ¨åŠ é€Ÿ Decay é˜¶æ®µæŸå¤±ä¸‹é™ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶è¯æ˜äº†åœ¨ç‰¹å®šæŸå¤±åœ°å½¢ä¸‹å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜çš„å­¦ä¹ ç‡ç‚¹ï¼Œå³ \"strong Mpemba point\"ï¼Œåœ¨æ­¤ç‚¹å¤„ç³»ç»Ÿæ¼”åŒ–çš„æœ€æ…¢æ¨¡å¼æ¶ˆå¤±ï¼Œä»è€Œå®ç° Decay é˜¶æ®µçš„æœ€å¿«æ”¶æ•›ã€‚é€šè¿‡æ¨å¯¼è¯¥ç‚¹å­˜åœ¨çš„è§£ææ¡ä»¶å¹¶åˆ†æä¿æŒè¯¥ä¼˜åŠ¿æ‰€éœ€çš„åŠ¨åŠ›å­¦è¦æ±‚ï¼Œè¯¥ç ”ç©¶ä¸ºåŸºäº Plateau çš„è°ƒåº¦å™¨æä¾›äº†åŸåˆ™æ€§çš„ç†è®ºæ”¯æ’‘ã€‚è¿™é¡¹åˆ†æä¸º LLM è®­ç»ƒä¸­çš„å­¦ä¹ ç‡è°ƒä¼˜æä¾›äº†ç§‘å­¦æŒ‡å¯¼ï¼Œæœ‰åŠ©äºåœ¨æå°‘è¶…å‚æ•°æœç´¢çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨¡å‹æ”¶æ•›æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04206v1",
      "published_date": "2025-07-06 01:34:12 UTC",
      "updated_date": "2025-07-06 01:34:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:25.085542+00:00"
    },
    {
      "arxiv_id": "2507.04194v1",
      "title": "Mixed-Sample SGD: an End-to-end Analysis of Supervised Transfer Learning",
      "title_zh": "Mixed-Sample SGDï¼šæœ‰ç›‘ç£è¿ç§»å­¦ä¹ çš„ç«¯åˆ°ç«¯åˆ†æ",
      "authors": [
        "Yuyang Deng",
        "Samory Kpotufe"
      ],
      "abstract": "Theoretical works on supervised transfer learning (STL) -- where the learner has access to labeled samples from both source and target distributions -- have for the most part focused on statistical aspects of the problem, while efficient optimization has received less attention. We consider the problem of designing an SGD procedure for STL that alternates sampling between source and target data, while maintaining statistical transfer guarantees without prior knowledge of the quality of the source data. A main algorithmic difficulty is in understanding how to design such an adaptive sub-sampling mechanism at each SGD step, to automatically gain from the source when it is informative, or bias towards the target and avoid negative transfer when the source is less informative.\n  We show that, such a mixed-sample SGD procedure is feasible for general prediction tasks with convex losses, rooted in tracking an abstract sequence of constrained convex programs that serve to maintain the desired transfer guarantees.\n  We instantiate these results in the concrete setting of linear regression with square loss, and show that the procedure converges, with $1/\\sqrt{T}$ rate, to a solution whose statistical performance on the target is adaptive to the a priori unknown quality of the source. Experiments with synthetic and real datasets support the theory.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ‰ç›‘ç£è¿ç§»å­¦ä¹ (Supervised Transfer Learning)ä¸­ä¼˜åŒ–ç®—æ³•å—å…³æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ··åˆæ ·æœ¬éšæœºæ¢¯åº¦ä¸‹é™(Mixed-Sample SGD)ç¨‹åºã€‚è¯¥ç¨‹åºé€šè¿‡åœ¨æºæ•°æ®å’Œç›®æ ‡æ•°æ®ä¹‹é—´äº¤æ›¿é‡‡æ ·ï¼Œæ—¨åœ¨æ— éœ€é¢„çŸ¥æºæ•°æ®è´¨é‡çš„å‰æä¸‹ç»´æŒç»Ÿè®¡è¿ç§»ä¿è¯ã€‚ç®—æ³•æ ¸å¿ƒåœ¨äºè®¾è®¡ä¸€ç§è‡ªé€‚åº”å­é‡‡æ ·æœºåˆ¶ï¼Œä»è€Œåœ¨æºæ•°æ®æœ‰æ•ˆæ—¶æå–ä¿¡æ¯ï¼Œå¹¶åœ¨æºæ•°æ®ä¿¡æ¯ä¸è¶³æ—¶é€šè¿‡åå‘ç›®æ ‡æ•°æ®æ¥é¿å…è´Ÿè¿ç§»(negative transfer)ã€‚ç ”ç©¶è¯æ˜è¯¥æ–¹æ³•é€‚ç”¨äºå…·æœ‰å‡¸æŸå¤±(convex losses)çš„é€šç”¨é¢„æµ‹ä»»åŠ¡ï¼Œå…¶ç†è®ºåŸºç¡€æ˜¯è¿½è¸ªä¸€ç³»åˆ—å—é™å‡¸è§„åˆ’ç¨‹åºã€‚åœ¨å¹³æ–¹æŸå¤±(square loss)çº¿æ€§å›å½’çš„å…·ä½“è®¾å®šä¸‹ï¼Œè¯¥ç¨‹åºå…·æœ‰ $1/\\sqrt{T}$ çš„æ”¶æ•›ç‡ï¼Œä¸”å…¶åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„ç»Ÿè®¡æ€§èƒ½å¯è‡ªé€‚åº”äºå…ˆéªŒæœªçŸ¥çš„æºæ•°æ®è´¨é‡ã€‚é€šè¿‡åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®é›†çš„å®éªŒï¼Œç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¿™ç§ç«¯åˆ°ç«¯åˆ†ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04194v1",
      "published_date": "2025-07-06 00:03:34 UTC",
      "updated_date": "2025-07-06 00:03:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:49:37.302774+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 71,
  "processed_papers_count": 71,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T02:52:25.297760+00:00"
}
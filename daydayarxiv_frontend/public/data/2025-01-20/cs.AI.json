{
  "date": "2025-01-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-20 的 arXiv 中文 TLDR 快报！今天的论文主要聚焦于 AI 和大型语言模型（LLM）的推理优化、联邦学习以及扩散模型等领域，亮点包括 OpenAI o1 在眼科问答的性能评估，以及图学习和气象预测的创新方法，由学者如 Michael Elad 和 Shlomo Shamai 参与的文章引发关注，突显了 LLM 在实际应用中的潜力。\n\n### 重点论文讨论\n我们挑选了最具话题度和影响力的论文先聊，特别是那些涉及 LLM 推理、AI 应用和医疗领域的。相关论文按主题归类，优先讨论创新性强或有知名学者的作品，其他次要论文则快速掠过。\n\n#### LLM 和 AI 推理优化\n这些论文探讨了 LLM 的推理能力、训练策略和应用扩展，是今日的核心热点。\n- **Benchmarking Large Language Models via Random Variables（评估大型语言模型通过随机变量）**：提出 RV-Bench 框架，用于测试 LLM 在数学推理中的鲁棒性，主要贡献是通过随机化变量组合评估模型的真实能力，发现 LLM 在已见和未见数据域间存在性能不均衡。\n- **Optimizing Pretraining Data Mixtures with LLM-Estimated Utility（使用 LLM 估计效用优化预训练数据混合）**：作者包括 Mike Lewis，由 Todor Mihaylov 等知名学者参与，主要发现通过 LLM 估计算法（如 UtiliMax 和 MEDU），可显著加速数据混合过程，提升模型训练效率。\n- **SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language Models Tackling Knowledge-based Reasoning Tasks（SR-FoT: 用于知识推理任务的演绎推理框架）**：引入多阶段演绎推理框架，帮助 LLM 处理复杂知识任务，关键发现是模型能通过生成前提问题提升推理准确性。\n- **Graph Learning with LLMs（图学习与 LLM）**：提出 GDL4LLM 方法，将图结构视为新语言进行预训练，主要贡献是让 LLM 更有效地建模图数据，提高节点分类任务的性能。\n- **Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training（Agent-R: 通过迭代自训练提升 LLM 代理的自省能力）**：开发自训练框架，使 LLM 代理能实时修正错误，显著改善交互环境下的性能。\n- **RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?（RedStar: 扩展长 CoT 数据是否能解锁更好的慢速推理系统）**：探索长链式思维训练，RedStar 模型在数学和多模态任务中表现突出，发现大规模数据可大幅提升推理精度。\n\n这些工作突显了 LLM 在推理和知识整合中的进展，但也暴露了训练数据和泛化能力的挑战。\n\n#### 医疗和实际应用\n医疗领域的论文结合 AI 技术解决实际问题，令人印象深刻，尤其是与知名学者相关的。\n- **GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease（GL-ICNN: 用于阿尔茨海默病诊断的可解释卷积神经网络）**：作者包括 Michael Elad，主要贡献是开发端到端可解释模型，实现高精度诊断（AUC 达 0.956），并提供特征重要性解释。\n- **Can OpenAI o1 Reason Well in Ophthalmology?（OpenAI o1 在眼科领域的推理性能）**：评估 OpenAI o1 在眼科问答中的表现，发现其准确率最高，但推理能力不如 GPT-4o，在细粒度任务中表现不稳定。\n- **Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation（Finer-CAM: 通过差异识别提升视觉解释的精细度）**：提出差异对比方法，提升图像分类的可解释性，主要发现能更准确地定位判别区域。\n\n这些论文强调 AI 在医疗中的伦理和实用性，Michael Elad 的参与增加了其影响力。\n\n#### 其他创新应用\n快速提一下其他有话题度的论文，聚焦核心贡献。\n- **SILO: Solving Inverse Problems with Latent Operators（SILO: 使用潜在操作符解决逆问题）**：作者包括 Michael Elad，创新地使用潜在扩散模型改善图像恢复质量，显著提升效率。\n- **Weighted Conformal Risk Control Under Covariate Shift（协变量偏移下的加权保形风险控制）**：作者包括 Shlomo Shamai，主要发现通过加权方法提升预测集的鲁棒性，在不确定环境下表现优异。\n- **WSSM: Geographic-enhanced hierarchical state-space model for global station weather forecast（WSSM: 增强地理信息的层次状态空间模型用于全球站点天气预报）**：提出多尺度模型，提高极端天气预测准确性，贡献在于整合地理数据提升整体精度。\n\n其他论文如联邦学习、图神经网络或环境监测等（如第2、11、37、45），虽有贡献但不那么突出，仅提核心：它们优化了 AI 在游戏测试、铁路安全和数据隐私中的应用，但细节较常规，故快速掠过。\n\n总之，今天的论文以 AI 创新为主，LLM 推理和医疗应用是亮点，建议读者关注 OpenAI o1 和 Michael Elad 相关工作，以探索 AI 的实际潜力。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2501.11790v3",
      "title": "Benchmarking Large Language Models via Random Variables",
      "title_zh": "翻译失败",
      "authors": [
        "Zijin Hong",
        "Hao Wu",
        "Su Dong",
        "Junnan Dong",
        "Yilin Xiao",
        "Yujing Zhang",
        "Zhu Wang",
        "Feiran Huang",
        "Linyi Li",
        "Hongxia Yang",
        "Xiao Huang"
      ],
      "abstract": "Recent studies have raised concerns about the reliability of current\nmathematical benchmarks, highlighting issues such as simplistic design and\npotential data contamination. Therefore, creating a reliable benchmark that\neffectively evaluates the genuine capabilities of large language models (LLMs)\nin mathematical reasoning remains a significant challenge. To address this, we\npropose RV-Bench, a framework for Benchmarking LLMs via Random Variables in\nmathematical reasoning. Specifically, the background content of a random\nvariable question (RV question) mirrors the original problem in existing\nbenchmarks, but the variable combinations are randomized, making it \"unseen\" by\nthe LLMs. Models must completely understand the question pattern of the\noriginal problem to correctly answer RV questions with various variable values.\nAs a result, the LLM's genuine capability in mathematical reasoning is\nreflected by its accuracy and robustness on RV-Bench. We conducted extensive\nexperiments on over 30 representative LLMs across more than 1000 RV questions.\nOur findings suggest that LLMs exhibit an imbalance in proficiency between\nencountered and \"unseen\" data domains. Proficiency generalization across\nsimilar mathematical reasoning tasks is verified to be limited by accuracy and\nrobustness, but it can still be enhanced through test-time scaling.",
      "tldr_zh": "该研究指出，现有数学基准存在设计简单和数据污染等问题，因此提出 RV-Bench 框架，通过随机变量 (Random Variables) 来评估大型语言模型 (LLMs) 在数学推理的真实能力。具体而言，RV-Bench 将现有基准问题的背景保持不变，但随机化变量组合，使其成为“未见”数据，从而测试模型对问题模式的全面理解。在超过 1000 个 RV 问题上测试 30 多个代表性 LLMs 的实验结果显示，LLMs 在已见数据域表现较好，但在未见域能力不平衡，且任务间能力泛化有限，不过可以通过测试时缩放 (test-time scaling) 来提升准确性和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2501.11790v3",
      "published_date": "2025-01-20 23:41:22 UTC",
      "updated_date": "2025-03-15 09:20:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:09:24.250736"
    },
    {
      "arxiv_id": "2501.11782v1",
      "title": "Human-AI Collaborative Game Testing with Vision Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Boran Zhang",
        "Muhan Xu",
        "Zhijun Pan"
      ],
      "abstract": "As modern video games become increasingly complex, traditional manual testing\nmethods are proving costly and inefficient, limiting the ability to ensure\nhigh-quality game experiences. While advancements in Artificial Intelligence\n(AI) offer the potential to assist human testers, the effectiveness of AI in\ntruly enhancing real-world human performance remains underexplored. This study\ninvestigates how AI can improve game testing by developing and experimenting\nwith an AI-assisted workflow that leverages state-of-the-art machine learning\nmodels for defect detection. Through an experiment involving 800 test cases and\n276 participants of varying backgrounds, we evaluate the effectiveness of AI\nassistance under four conditions: with or without AI support, and with or\nwithout detailed knowledge of defects and design documentation. The results\nindicate that AI assistance significantly improves defect identification\nperformance, particularly when paired with detailed knowledge. However,\nchallenges arise when AI errors occur, negatively impacting human\ndecision-making. Our findings show the importance of optimizing human-AI\ncollaboration and implementing strategies to mitigate the effects of AI\ninaccuracies. By this research, we demonstrate AI's potential and problems in\nenhancing efficiency and accuracy in game testing workflows and offers\npractical insights for integrating AI into the testing process.",
      "tldr_zh": "这篇论文探讨了使用视觉语言模型(Vision Language Models)辅助人类进行游戏测试，以解决传统手动测试的成本和效率问题。研究开发了一个AI辅助工作流，并通过涉及800个测试案例和276名参与者的实验，评估了四种条件（有/无AI支持，以及有/无详细缺陷和设计文档知识）对缺陷检测的影响。结果表明，AI辅助显著提高了缺陷识别性能，尤其在提供详细知识时，但AI错误可能负面影响人类决策，并强调了优化人类-AI协作以缓解不准确性的重要性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Experiment Report",
      "pdf_url": "http://arxiv.org/pdf/2501.11782v1",
      "published_date": "2025-01-20 23:14:23 UTC",
      "updated_date": "2025-01-20 23:14:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:09:36.743817"
    },
    {
      "arxiv_id": "2501.12420v2",
      "title": "Consolidating TinyML Lifecycle with Large Language Models: Reality, Illusion, or Opportunity?",
      "title_zh": "用大型语言模型整合 TinyML 生命周期：现实、幻觉，还是机会？",
      "authors": [
        "Guanghan Wu",
        "Sasu Tarkoma",
        "Roberto Morabito"
      ],
      "abstract": "The evolving requirements of Internet of Things (IoT) applications are\ndriving an increasing shift toward bringing intelligence to the edge, enabling\nreal-time insights and decision-making within resource-constrained\nenvironments. Tiny Machine Learning (TinyML) has emerged as a key enabler of\nthis evolution, facilitating the deployment of ML models on devices such as\nmicrocontrollers and embedded systems. However, the complexity of managing the\nTinyML lifecycle, including stages such as data processing, model optimization\nand conversion, and device deployment, presents significant challenges and\noften requires substantial human intervention. Motivated by these challenges,\nwe began exploring whether Large Language Models (LLMs) could help automate and\nstreamline the TinyML lifecycle. We developed a framework that leverages the\nnatural language processing (NLP) and code generation capabilities of LLMs to\nreduce development time and lower the barriers to entry for TinyML deployment.\nThrough a case study involving a computer vision classification model, we\ndemonstrate the framework's ability to automate key stages of the TinyML\nlifecycle. Our findings suggest that LLM-powered automation holds potential for\nimproving the lifecycle development process and adapting to diverse\nrequirements. However, while this approach shows promise, there remain\nobstacles and limitations, particularly in achieving fully automated solutions.\nThis paper sheds light on both the challenges and opportunities of integrating\nLLMs into TinyML workflows, providing insights into the path forward for\nefficient, AI-assisted embedded system development.",
      "tldr_zh": "这篇论文探讨了使用 Large Language Models (LLMs) 整合 Tiny Machine Learning (TinyML) 生命周期的可能性，以应对 Internet of Things (IoT) 应用中边缘计算的资源限制挑战。研究提出一个框架，利用 LLMs 的 Natural Language Processing (NLP) 和代码生成能力，自动化 TinyML 的数据处理、模型优化、转换和部署过程，并通过一个计算机视觉分类模型的案例研究证明了其能减少开发时间并降低门槛。主要发现显示这种方法有潜力改善 TinyML 工作流程，但仍面临实现完全自动化的障碍，为 AI 辅助嵌入式系统开发提供了宝贵机遇。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper has been accepted for publication in the IEEE Internet of\n  Things Magazine (Special Issue on Applications of Large Language Models in\n  IoT). The copyright will be transferred to IEEE upon publication. A\n  preliminary version of this work was presented at the Edge AI Foundation\n  event Beyond LLMs and Chatbots: The Journey to Generative AI at the Edge\n  (https://youtu.be/aFWfisdjQIs)",
      "pdf_url": "http://arxiv.org/pdf/2501.12420v2",
      "published_date": "2025-01-20 22:20:57 UTC",
      "updated_date": "2025-04-05 16:29:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:09:48.232228"
    },
    {
      "arxiv_id": "2501.16360v1",
      "title": "Momentum Contrastive Learning with Enhanced Negative Sampling and Hard Negative Filtering",
      "title_zh": "带增强负采样和困难负样本过滤的动量对比学习",
      "authors": [
        "Duy Hoang",
        "Huy Ngo",
        "Khoi Pham",
        "Tri Nguyen",
        "Gia Bao",
        "Huy Phan"
      ],
      "abstract": "Contrastive learning has become pivotal in unsupervised representation\nlearning, with frameworks like Momentum Contrast (MoCo) effectively utilizing\nlarge negative sample sets to extract discriminative features. However,\ntraditional approaches often overlook the full potential of key embeddings and\nare susceptible to performance degradation from noisy negative samples in the\nmemory bank. This study addresses these challenges by proposing an enhanced\ncontrastive learning framework that incorporates two key innovations. First, we\nintroduce a dual-view loss function, which ensures balanced optimization of\nboth query and key embeddings, improving representation quality. Second, we\ndevelop a selective negative sampling strategy that emphasizes the most\nchallenging negatives based on cosine similarity, mitigating the impact of\nnoise and enhancing feature discrimination. Extensive experiments demonstrate\nthat our framework achieves superior performance on downstream tasks,\ndelivering robust and well-structured representations. These results highlight\nthe potential of optimized contrastive mechanisms to advance unsupervised\nlearning and extend its applicability across domains such as computer vision\nand natural language processing",
      "tldr_zh": "该研究针对传统 Momentum Contrast (MoCo) 框架在对比学习中的问题，提出一个增强框架，以解决关键嵌入潜力未充分发挥和噪声负样本影响的问题。\n框架创新性地引入双视图损失函数 (dual-view loss function)，实现查询和关键嵌入的平衡优化，提高表示质量；同时，采用基于余弦相似度 (cosine similarity) 的选择性负采样策略，强调 challenging 的负样本并过滤噪声，提升特征区分能力。\n实验结果显示，该框架在下游任务上表现出色，提供更鲁棒的表示，并扩展了对比学习在计算机视觉和自然语言处理等领域的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.16360v1",
      "published_date": "2025-01-20 22:01:52 UTC",
      "updated_date": "2025-01-20 22:01:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:10:01.532680"
    },
    {
      "arxiv_id": "2501.11765v1",
      "title": "Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?",
      "title_zh": "Transformer 模型的逻辑分析是在自注意力机制中进行的还是在全连接部分中进行的？",
      "authors": [
        "Evgeniy Shin",
        "Heinrich Matzinger"
      ],
      "abstract": "Transformers architecture apply self-attention to tokens represented as\nvectors, before a fully connected (neuronal network) layer. These two parts can\nbe layered many times. Traditionally, self-attention is seen as a mechanism for\naggregating information before logical operations are performed by the fully\nconnected layer. In this paper, we show, that quite counter-intuitively, the\nlogical analysis can also be performed within the self-attention. For this we\nimplement a handcrafted single-level encoder layer which performs the logical\nanalysis within self-attention. We then study the scenario in which a one-level\ntransformer model undergoes self-learning using gradient descent. We\ninvestigate whether the model utilizes fully connected layers or self-attention\nmechanisms for logical analysis when it has the choice. Given that gradient\ndescent can become stuck at undesired zeros, we explicitly calculate these\nunwanted zeros and find ways to avoid them. We do all this in the context of\npredicting grammatical category pairs of adjacent tokens in a text. We believe\nthat our findings have broader implications for understanding the potential\nlogical operations performed by self-attention.",
      "tldr_zh": "本研究探讨了Transformer模型中，逻辑分析是否发生在self-attention机制还是fully connected层。作者通过实现一个手制的单层编码器，展示了逻辑分析可以在self-attention内进行，并研究了单层Transformer模型通过gradient descent自学习的情景，检查模型在有选择时更倾向于使用哪种机制。论文还计算并避免了gradient descent可能卡在不想要的零点的问题，并在预测文本中相邻标记的语法类别对的背景下进行实验，结果表明self-attention具有更广泛的潜在逻辑操作能力，对理解Transformer架构有重要启示。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T30",
        "I.2.4"
      ],
      "primary_category": "cs.CL",
      "comment": "42 pages, 3 figures, to be submitted",
      "pdf_url": "http://arxiv.org/pdf/2501.11765v1",
      "published_date": "2025-01-20 21:58:35 UTC",
      "updated_date": "2025-01-20 21:58:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:10:11.906891"
    },
    {
      "arxiv_id": "2501.17167v2",
      "title": "QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks",
      "title_zh": "翻译失败",
      "authors": [
        "Yaojie Hu",
        "Qiang Zhou",
        "Qihong Chen",
        "Xiaopeng Li",
        "Linbo Liu",
        "Dejiao Zhang",
        "Amit Kachroo",
        "Talha Oz",
        "Omer Tripp"
      ],
      "abstract": "We introduce QualityFlow, a dynamic agentic workflow for program synthesis.\nGiven the English description of a programming problem and a set of unit tests,\nthe model's goal is to synthesize the correct program that solves the problem\nand passes the tests. QualityFlow includes large language model (LLM) agents\nresembling a software development team, including code generation, testing, and\nself-debugging. We propose the LLM Quality Checker, which explicitly \"imagines\"\nwhether the synthesized programs' execution would conform to the unit tests.\nThe Quality Checks dynamically control the workflow, including actions to\nsubmit the final answer, clarify the problem statement, and revert previous\nworkflow steps. Our experiments show that the Quality Checker can precisely\naccept any correct program, mitigate faulty synthesized tests, and prevent\npotential workflow deviation. QualityFlow establishes the state-of-the-art\nresults on four program synthesis benchmarks: MBPP, HumanEval, and stricter\nevaluations from MBPP-EvalPlus and HumanEval-EvalPlus.",
      "tldr_zh": "我们引入 QualityFlow，一种动态的智能体工作流，用于程序合成，通过 LLM Quality Checker 控制流程。该工作流模拟软件开发团队，包括代码生成、测试和自我调试，LLM Quality Checker 通过“想象”程序执行是否符合单元测试，来动态决定提交答案、澄清问题或撤销步骤。实验表明，QualityFlow 能在 MBPP、HumanEval 等基准上实现 state-of-the-art 结果，精确接受正确程序、缓解 faulty 测试并防止工作流偏差，从而提升程序合成的可靠性和准确性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17167v2",
      "published_date": "2025-01-20 21:47:06 UTC",
      "updated_date": "2025-03-24 19:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:12:24.162871"
    },
    {
      "arxiv_id": "2502.15709v2",
      "title": "TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoxing Li",
        "Vahid Yazdanpanah",
        "Jindi Wang",
        "Wen Gu",
        "Lei Shi",
        "Alexandra I. Cristea",
        "Sarah Kiden",
        "Sebastian Stein"
      ],
      "abstract": "The integration of AI in education offers significant potential to enhance\nlearning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and\nLlama, allow students to query a wide range of topics, providing unprecedented\nflexibility. However, LLMs face challenges, such as handling varying content\nrelevance and lack of personalization. To address these challenges, we propose\nTutorLLM, a personalized learning recommender LLM system based on Knowledge\nTracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM\nlies in its unique combination of KT and RAG techniques with LLMs, which\nenables dynamic retrieval of context-specific knowledge and provides\npersonalized learning recommendations based on the student's personal learning\nstate. Specifically, this integration allows TutorLLM to tailor responses based\non individual learning states predicted by the Multi-Features with Latent\nRelations BERT-based KT (MLFBK) model and to enhance response accuracy with a\nScraper model. The evaluation includes user assessment questionnaires and\nperformance metrics, demonstrating a 10% improvement in user satisfaction and a\n5\\% increase in quiz scores compared to using general LLMs alone.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)如 ChatGPT 在教育中的内容相关性和个性化不足问题，提出 TutorLLM 系统，该系统结合 Knowledge Tracing (KT) 和 Retrieval-Augmented Generation (RAG) 技术，提供个性化的学习推荐。TutorLLM 的创新在于动态检索上下文特定知识，并根据 Multi-Features with Latent Relations BERT-based KT (MLFBK) 模型预测学生的学习状态，使用 Scraper 模型增强响应准确性。实验评估显示，与通用 LLMs 相比，TutorLLM 提高了用户满意度 10% 并提升了测验分数 5%，从而提升了 AI 在教育中的应用效率。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15709v2",
      "published_date": "2025-01-20 21:18:43 UTC",
      "updated_date": "2025-04-27 08:17:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:10:36.602232"
    },
    {
      "arxiv_id": "2501.11747v2",
      "title": "Optimizing Pretraining Data Mixtures with LLM-Estimated Utility",
      "title_zh": "翻译失败",
      "authors": [
        "William Held",
        "Bhargavi Paranjape",
        "Punit Singh Koura",
        "Mike Lewis",
        "Frank Zhang",
        "Todor Mihaylov"
      ],
      "abstract": "Large Language Models improve with increasing amounts of high-quality\ntraining data. However, leveraging larger datasets requires balancing quality,\nquantity, and diversity across sources. After evaluating nine baseline methods\nunder both compute- and data-constrained scenarios, we find token-count\nheuristics outperform manual and learned mixes, indicating that simple\napproaches accounting for dataset size and diversity are surprisingly\neffective. Building on this insight, we propose two complementary approaches:\nUtiliMax, which extends token-based heuristics by incorporating utility\nestimates from reduced-scale ablations, achieving up to a 10.6x speedup over\nmanual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs\nto estimate data utility from small samples, matching ablation-based\nperformance while reducing computational requirements by $\\sim$200x. Together,\nthese approaches establish a new framework for automated, compute-efficient\ndata mixing that is robust across training regimes.",
      "tldr_zh": "本研究探讨了优化大语言模型(LLMs)的预训练数据混合问题，通过评估九种基线方法，发现基于 token 计数的启发式方法在计算和数据受限场景下优于手动和学习混合方法。作者提出两种新方法：UtiliMax，利用缩减规模的消融实验扩展 token-based 启发式，实现了比手动基线高 10.6 倍的速度提升；以及 Model Estimated Data Utility (MEDU)，通过 LLMs 估计小样本数据效用，匹配消融实验性能同时减少计算需求约 200 倍。这些方法构建了一个自动化、计算高效的数据混合框架，适用于不同训练场景。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11747v2",
      "published_date": "2025-01-20 21:10:22 UTC",
      "updated_date": "2025-01-23 20:45:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:10:48.782893"
    },
    {
      "arxiv_id": "2501.11746v1",
      "title": "SILO: Solving Inverse Problems with Latent Operators",
      "title_zh": "翻译失败",
      "authors": [
        "Ron Raphaeli",
        "Sean Man",
        "Michael Elad"
      ],
      "abstract": "Consistent improvement of image priors over the years has led to the\ndevelopment of better inverse problem solvers. Diffusion models are the\nnewcomers to this arena, posing the strongest known prior to date. Recently,\nsuch models operating in a latent space have become increasingly predominant\ndue to their efficiency. In recent works, these models have been applied to\nsolve inverse problems. Working in the latent space typically requires multiple\napplications of an Autoencoder during the restoration process, which leads to\nboth computational and restoration quality challenges. In this work, we propose\na new approach for handling inverse problems with latent diffusion models,\nwhere a learned degradation function operates within the latent space,\nemulating a known image space degradation. Usage of the learned operator\nreduces the dependency on the Autoencoder to only the initial and final steps\nof the restoration process, facilitating faster sampling and superior\nrestoration quality. We demonstrate the effectiveness of our method on a\nvariety of image restoration tasks and datasets, achieving significant\nimprovements over prior art.",
      "tldr_zh": "该论文提出SILO方法，利用潜在空间(latent space)的扩散模型(diffusion models)来解决逆问题(inverse problems)，通过学习一个退化函数(learned degradation function)来模拟图像空间的已知退化。相比传统方法，该方法仅在恢复过程的初始和最终步骤使用自编码器(Autoencoder)，从而减少计算负担并提升恢复质量。在多种图像恢复任务和数据集上，SILO实现了比现有技术显著的性能改进。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page in https://ronraphaeli.github.io/SILO-website/",
      "pdf_url": "http://arxiv.org/pdf/2501.11746v1",
      "published_date": "2025-01-20 21:09:33 UTC",
      "updated_date": "2025-01-20 21:09:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:10:59.775183"
    },
    {
      "arxiv_id": "2501.11739v2",
      "title": "Episodic memory in AI agents poses risks that should be studied and mitigated",
      "title_zh": "AI 智能体中的情节记忆会",
      "authors": [
        "Chad DeChant"
      ],
      "abstract": "Most current AI models have little ability to store and later retrieve a\nrecord or representation of what they do. In human cognition, episodic memories\nplay an important role in both recall of the past as well as planning for the\nfuture. The ability to form and use episodic memories would similarly enable a\nbroad range of improved capabilities in an AI agent that interacts with and\ntakes actions in the world. Researchers have begun directing more attention to\ndeveloping memory abilities in AI models. It is therefore likely that models\nwith such capability will be become widespread in the near future. This could\nin some ways contribute to making such AI agents safer by enabling users to\nbetter monitor, understand, and control their actions. However, as a new\ncapability with wide applications, we argue that it will also introduce\nsignificant new risks that researchers should begin to study and address. We\noutline these risks and benefits and propose four principles to guide the\ndevelopment of episodic memory capabilities so that these will enhance, rather\nthan undermine, the effort to keep AI safe and trustworthy.",
      "tldr_zh": "该论文讨论了AI代理中 episodic memory（ episodic memories ）能力的双重影响：一方面，它能提升AI的记忆和决策能力，如更好地回忆过去行为和规划未来行动，从而增强AI的安全性和可控性；另一方面，这种能力可能引入新风险，包括潜在的滥用、不可预测行为和隐私问题。作者分析了这些益处和风险，并提出四条原则来指导 episodic memory 的开发，确保其有助于提升AI的安全性和可信度。研究者应及早研究并缓解这些风险，以促进AI的负责任发展。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at the 2025 Secure and Trustworthy Machine\n  Learning Conference (SaTML). The final version will be available on IEEE\n  Xplore",
      "pdf_url": "http://arxiv.org/pdf/2501.11739v2",
      "published_date": "2025-01-20 20:54:06 UTC",
      "updated_date": "2025-01-22 15:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:11:11.338991"
    },
    {
      "arxiv_id": "2501.11730v1",
      "title": "Transformer Vibration Forecasting for Advancing Rail Safety and Maintenance 4.0",
      "title_zh": "翻译失败",
      "authors": [
        "Darío C. Larese",
        "Almudena Bravo Cerrada",
        "Gabriel Dambrosio Tomei",
        "Alejandro Guerrero-López",
        "Pablo M. Olmos",
        "María Jesús Gómez García"
      ],
      "abstract": "Maintaining railway axles is critical to preventing severe accidents and\nfinancial losses. The railway industry is increasingly interested in advanced\ncondition monitoring techniques to enhance safety and efficiency, moving beyond\ntraditional periodic inspections toward Maintenance 4.0.\n  This study introduces a robust Deep Autoregressive solution that integrates\nseamlessly with existing systems to avert mechanical failures. Our approach\nsimulates and predicts vibration signals under various conditions and fault\nscenarios, improving dataset robustness for more effective detection systems.\nThese systems can alert maintenance needs, preventing accidents preemptively.\nWe use experimental vibration signals from accelerometers on train axles.\n  Our primary contributions include a transformer model, ShaftFormer, designed\nfor processing time series data, and an alternative model incorporating\nspectral methods and enhanced observation models. Simulating vibration signals\nunder diverse conditions mitigates the high cost of obtaining experimental\nsignals for all scenarios. Given the non-stationary nature of railway vibration\nsignals, influenced by speed and load changes, our models address these\ncomplexities, offering a powerful tool for predictive maintenance in the rail\nindustry.",
      "tldr_zh": "本文提出了一种基于 Transformer 的振动预测方法，名为 ShaftFormer，用于处理铁路轴的时间序列数据，从而推进铁路安全和 Maintenance 4.0。该方法结合 Deep Autoregressive 解决方案和光谱方法，通过模拟各种条件下的振动信号，改善数据集鲁棒性并提前警报潜在故障，减少了获取实验信号的高成本。实验结果显示，该模型有效应对铁路振动信号的非平稳性（如速度和负载变化），为预测性维护提供强大工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11730v1",
      "published_date": "2025-01-20 20:29:40 UTC",
      "updated_date": "2025-01-20 20:29:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:12:35.576293"
    },
    {
      "arxiv_id": "2501.11715v1",
      "title": "GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjie Kang",
        "Lize Jiskoot",
        "Peter De Deyn",
        "Geert Biessels",
        "Huiberdina Koek",
        "Jurgen Claassen",
        "Huub Middelkoop",
        "Wiesje Flier",
        "Willemijn J. Jansen",
        "Stefan Klein",
        "Esther Bron"
      ],
      "abstract": "Deep learning methods based on Convolutional Neural Networks (CNNs) have\nshown great potential to improve early and accurate diagnosis of Alzheimer's\ndisease (AD) dementia based on imaging data. However, these methods have yet to\nbe widely adopted in clinical practice, possibly due to the limited\ninterpretability of deep learning models. The Explainable Boosting Machine\n(EBM) is a glass-box model but cannot learn features directly from input\nimaging data. In this study, we propose a novel interpretable model that\ncombines CNNs and EBMs for the diagnosis and prediction of AD. We develop an\ninnovative training strategy that alternatingly trains the CNN component as a\nfeature extractor and the EBM component as the output block to form an\nend-to-end model. The model takes imaging data as input and provides both\npredictions and interpretable feature importance measures. We validated the\nproposed model on the Alzheimer's Disease Neuroimaging Initiative (ADNI)\ndataset and the Health-RI Parelsnoer Neurodegenerative Diseases Biobank (PND)\nas an external testing set. The proposed model achieved an area-under-the-curve\n(AUC) of 0.956 for AD and control classification, and 0.694 for the prediction\nof conversion of mild cognitive impairment (MCI) to AD on the ADNI cohort. The\nproposed model is a glass-box model that achieves a comparable performance with\nother state-of-the-art black-box models. Our code is publicly available at:\nhttps://anonymous.4open.science/r/GL-ICNN.",
      "tldr_zh": "该研究提出了一种端到端可解释卷积神经网络（GL-ICNN），将卷积神经网络（CNN）与可解释提升机（EBM）结合，用于阿尔茨海默病（Alzheimer's Disease, AD）的诊断和预测。模型采用创新的交替训练策略，先用 CNN 提取图像特征，再用 EBM 提供预测和可解释的特征重要性度量，从而实现端到端学习。实验在 ADNI 数据集上验证，AD 与控制组分类的 AUC 达到 0.956，而轻度认知障碍（MCI）转为 AD 的预测 AUC 为 0.694，与现有黑盒模型性能相当。GL-ICNN 作为玻璃盒模型（glass-box model），提升了临床应用的潜力，并公开了代码以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11715v1",
      "published_date": "2025-01-20 19:55:50 UTC",
      "updated_date": "2025-01-20 19:55:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:12:48.806916"
    },
    {
      "arxiv_id": "2501.11705v1",
      "title": "Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)",
      "title_zh": "翻译失败",
      "authors": [
        "Brian E. Perron",
        "Lauri Goldkind",
        "Zia Qi",
        "Bryan G. Victor"
      ],
      "abstract": "This paper examines the responsible integration of artificial intelligence\n(AI) in human services organizations (HSOs), proposing a nuanced framework for\nevaluating AI applications across multiple dimensions of risk. The authors\nargue that ethical concerns about AI deployment -- including professional\njudgment displacement, environmental impact, model bias, and data laborer\nexploitation -- vary significantly based on implementation context and specific\nuse cases. They challenge the binary view of AI adoption, demonstrating how\ndifferent applications present varying levels of risk that can often be\neffectively managed through careful implementation strategies. The paper\nhighlights promising solutions, such as local large language models, that can\nfacilitate responsible AI integration while addressing common ethical concerns.\nThe authors propose a dimensional risk assessment approach that considers\nfactors like data sensitivity, professional oversight requirements, and\npotential impact on client wellbeing. They conclude by outlining a path forward\nthat emphasizes empirical evaluation, starting with lower-risk applications and\nbuilding evidence-based understanding through careful experimentation. This\napproach enables organizations to maintain high ethical standards while\nthoughtfully exploring how AI might enhance their capacity to serve clients and\ncommunities effectively.",
      "tldr_zh": "这篇论文探讨了人工智能 (AI) 在人类服务组织 (HSOs) 中的负责任整合，提出一个多维度风险评估框架，以评估 AI 应用的伦理风险，如专业判断取代、环境影响、模型偏差和数据劳动者剥削，这些风险因实施上下文和具体用例而异。作者挑战了 AI 采用的二元视角，强调不同应用的风险水平可以通过仔细的实施策略进行有效管理，并推荐使用本地大型语言模型 (LLMs) 等解决方案。最终，论文建议从低风险应用入手，通过经验评估和实验，建立基于证据的理解，从而在保持高伦理标准的同时，提升组织的服务能力和社区福祉。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "1 figure. Journal of Technology in Human Services (2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.11705v1",
      "published_date": "2025-01-20 19:38:21 UTC",
      "updated_date": "2025-01-20 19:38:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:12:59.287759"
    },
    {
      "arxiv_id": "2501.11695v2",
      "title": "Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data",
      "title_zh": "空间划分的领域适配 AI 分类：肿瘤学数据的应用示例",
      "authors": [
        "Majid Farhadloo",
        "Arun Sharma",
        "Alexey Leontovich",
        "Svetomir N. Markovic",
        "Shashi Shekhar"
      ],
      "abstract": "Given multi-type point maps from different place-types (e.g., tumor regions),\nour objective is to develop a classifier trained on the source place-type to\naccurately distinguish between two classes of the target place-type based on\ntheir point arrangements. This problem is societally important for many\napplications, such as generating clinical hypotheses for designing new\nimmunotherapies for cancer treatment. The challenge lies in the spatial\nvariability, the inherent heterogeneity and variation observed in spatial\nproperties or arrangements across different locations (i.e., place-types).\nPrevious techniques focus on self-supervised tasks to learn domain-invariant\nfeatures and mitigate domain differences; however, they often neglect the\nunderlying spatial arrangements among data points, leading to significant\ndiscrepancies across different place-types. We explore a novel multi-task\nself-learning framework that targets spatial arrangements, such as spatial\nmix-up masking and spatial contrastive predictive coding, for\nspatially-delineated domain-adapted AI classification. Experimental results on\nreal-world datasets (e.g., oncology data) show that the proposed framework\nprovides higher prediction accuracy than baseline methods.",
      "tldr_zh": "这篇论文针对不同位置类型（如肿瘤区域）的多类型点地图，提出了一种空间划分的领域适应AI分类方法，旨在使用源位置类型的训练数据准确区分目标位置类型的两个类别点排列，尤其应用于肿瘤学数据生成癌症免疫疗法假设。论文的主要创新是开发一个多任务自学习框架，重点处理空间变异性，通过引入spatial mix-up masking和spatial contrastive predictive coding等技术来学习空间排列的领域不变特征。实验结果显示，该框架在真实肿瘤学数据集上比基线方法实现了更高的预测准确率，为临床应用提供了更可靠的工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11695v2",
      "published_date": "2025-01-20 19:20:13 UTC",
      "updated_date": "2025-04-24 01:49:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:13:11.634125"
    },
    {
      "arxiv_id": "2501.14818v1",
      "title": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiqi Li",
        "Guo Chen",
        "Shilong Liu",
        "Shihao Wang",
        "Vibashan VS",
        "Yishen Ji",
        "Shiyi Lan",
        "Hao Zhang",
        "Yilin Zhao",
        "Subhashree Radhakrishnan",
        "Nadine Chang",
        "Karan Sapra",
        "Amala Sanjay Deshmukh",
        "Tuomas Rintamaki",
        "Matthieu Le",
        "Ilia Karmanov",
        "Lukas Voegtle",
        "Philipp Fischer",
        "De-An Huang",
        "Timo Roman",
        "Tong Lu",
        "Jose M. Alvarez",
        "Bryan Catanzaro",
        "Jan Kautz",
        "Andrew Tao",
        "Guilin Liu",
        "Zhiding Yu"
      ],
      "abstract": "Recently, promising progress has been made by open-source vision-language\nmodels (VLMs) in bringing their capabilities closer to those of proprietary\nfrontier models. However, most open-source models only publish their final\nmodel weights, leaving the critical details of data strategies and\nimplementation largely opaque. In this work, we address VLM post-training from\na data-centric perspective, showing the key role of data strategy in developing\nfrontier VLMs. By studying and building our post-training data strategy from\nscratch, we share detailed insights into the development processes, aiming to\nbenefit the development of competitive models for the open-source community.\nOur introduced data strategy, together with training recipes and model design,\nleads to a family of performant VLMs named Eagle2. Specifically, Eagle2-9B\nachieves state-of-the-art results across various multimodal benchmarks,\nmatching certain competitive models with up to 70B parameters.",
      "tldr_zh": "这篇论文从数据中心视角探讨前沿视觉语言模型(VLMs)的后训练策略，强调数据策略在模型开发中的关键作用，并分享从零构建这些策略的详细见解，以支持开源社区。作者结合后训练数据策略、训练配方和模型设计，开发了Eagle2系列模型。结果显示，Eagle2-9B在各种多模态基准上达到最先进水平，能够匹敌某些参数高达70B的竞争模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14818v1",
      "published_date": "2025-01-20 18:40:47 UTC",
      "updated_date": "2025-01-20 18:40:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:13:23.829906"
    },
    {
      "arxiv_id": "2501.13120v1",
      "title": "Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness",
      "title_zh": "翻译失败",
      "authors": [
        "Ambreesh Parthasarathy",
        "Chandrasekar Subramanian",
        "Ganesh Senrayan",
        "Shreyash Adappanavar",
        "Aparna Taneja",
        "Balaraman Ravindran",
        "Milind Tambe"
      ],
      "abstract": "Restless Multi-Armed Bandits (RMABs) have been successfully applied to\nresource allocation problems in a variety of settings, including public health.\nWith the rapid development of powerful large language models (LLMs), they are\nincreasingly used to design reward functions to better match human preferences.\nRecent work has shown that LLMs can be used to tailor automated allocation\ndecisions to community needs using language prompts. However, this has been\nstudied primarily for English prompts and with a focus on task performance\nonly. This can be an issue since grassroots workers, especially in developing\ncountries like India, prefer to work in local languages, some of which are\nlow-resource. Further, given the nature of the problem, biases along population\ngroups unintended by the user are also undesirable. In this work, we study the\neffects on both task performance and fairness when the DLM algorithm, a recent\nwork on using LLMs to design reward functions for RMABs, is prompted with\nnon-English language commands. Specifically, we run the model on a synthetic\nenvironment for various prompts translated into multiple languages. The prompts\nthemselves vary in complexity. Our results show that the LLM-proposed reward\nfunctions are significantly better when prompted in English compared to other\nlanguages. We also find that the exact phrasing of the prompt impacts task\nperformance. Further, as prompt complexity increases, performance worsens for\nall languages; however, it is more robust with English prompts than with\nlower-resource languages. On the fairness side, we find that low-resource\nlanguages and more complex prompts are both highly likely to create unfairness\nalong unintended dimensions.",
      "tldr_zh": "该研究探讨了在Restless Multi-Armed Bandits (RMABs)资源分配问题中，使用大型语言模型 (LLMs) 设计奖励函数时，多语言提示对任务性能和公平性的影响。研究采用DLM算法，在合成环境中测试各种语言的提示，包括复杂程度不同的非英语命令。结果显示，英语提示下LLM设计的奖励函数性能显著优于其他语言，且提示的精确表述和复杂性会进一步影响任务表现；此外，低资源语言和复杂提示更容易导致意外的不公平问题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the AAAI-2025 Deployable AI Workshop",
      "pdf_url": "http://arxiv.org/pdf/2501.13120v1",
      "published_date": "2025-01-20 18:14:37 UTC",
      "updated_date": "2025-01-20 18:14:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:13:35.926234"
    },
    {
      "arxiv_id": "2501.11639v2",
      "title": "StAyaL | Multilingual Style Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Karishma Thakrar",
        "Katrina Lawrence",
        "Kyle Howard"
      ],
      "abstract": "Stylistic text generation plays a vital role in enhancing communication by\nreflecting the nuances of individual expression. This paper presents a novel\napproach for generating text in a specific speaker's style across different\nlanguages. We show that by leveraging only 100 lines of text, an individuals\nunique style can be captured as a high-dimensional embedding, which can be used\nfor both text generation and stylistic translation. This methodology breaks\ndown the language barrier by transferring the style of a speaker between\nlanguages. The paper is structured into three main phases: augmenting the\nspeaker's data with stylistically consistent external sources, separating style\nfrom content using machine learning and deep learning techniques, and\ngenerating an abstract style profile by mean pooling the learned embeddings.\nThe proposed approach is shown to be topic-agnostic, with test accuracy and F1\nscores of 74.9% and 0.75, respectively. The results demonstrate the potential\nof the style profile for multilingual communication, paving the way for further\napplications in personalized content generation and cross-linguistic stylistic\ntransfer.",
      "tldr_zh": "本论文提出StAyaL，一种多语言风格转移方法，能够通过仅100行文本捕获个人的独特风格作为高维embedding，用于文本生成和风格翻译，从而打破语言障碍。方法分为三个阶段：使用风格一致的外部数据增强说话者数据、运用机器学习和深度学习技术分离风格与内容、通过均值池化生成抽象风格配置文件。该方法是主题无关的，在测试中达到74.9%的准确率和0.75的F1分数，展示了其在多语言通信、个性化内容生成和跨语言风格转移方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The primary authors, Karishma Thakrar and Katrina Lawrence,\n  contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2501.11639v2",
      "published_date": "2025-01-20 18:13:18 UTC",
      "updated_date": "2025-01-22 04:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:13:47.026950"
    },
    {
      "arxiv_id": "2501.11632v2",
      "title": "Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxing Lu",
        "Sin Yee Goi",
        "Xukai Zhao",
        "Jinzhuo Wang"
      ],
      "abstract": "Biomedical knowledge graphs (BKGs) have emerged as powerful tools for\norganizing and leveraging the vast and complex data found across the biomedical\nfield. Yet, current reviews of BKGs often limit their scope to specific domains\nor methods, overlooking the broader landscape and the rapid technological\nprogress reshaping it. In this survey, we address this gap by offering a\nsystematic review of BKGs from three core perspectives: domains, tasks, and\napplications. We begin by examining how BKGs are constructed from diverse data\nsources, including molecular interactions, pharmacological datasets, and\nclinical records. Next, we discuss the essential tasks enabled by BKGs,\nfocusing on knowledge management, retrieval, reasoning, and interpretation.\nFinally, we highlight real-world applications in precision medicine, drug\ndiscovery, and scientific research, illustrating the translational impact of\nBKGs across multiple sectors. By synthesizing these perspectives into a unified\nframework, this survey not only clarifies the current state of BKG research but\nalso establishes a foundation for future exploration, enabling both innovative\nmethodological advances and practical implementations.",
      "tldr_zh": "这篇调查综述了Biomedical Knowledge Graphs (BKGs)，作为组织和利用生物医学复杂数据的强大工具，填补了现有评论在领域和方法上的局限性。通过从domains（领域）、tasks（任务）和applications（应用）三个核心视角进行系统审查，论文探讨了BKGs的构建方式，包括从molecular interactions、pharmacological datasets和clinical records等数据来源入手。论文重点分析了BKGs支持的关键任务，如knowledge management、retrieval、reasoning和interpretation，并展示了其在precision medicine、drug discovery和scientific research中的实际应用。最终，该框架为BKG研究提供统一基础，促进未来方法创新和实际实施。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "45 pages, 4 figures, 3 tables. Updated figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11632v2",
      "published_date": "2025-01-20 18:02:03 UTC",
      "updated_date": "2025-01-22 06:17:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:13:59.283711"
    },
    {
      "arxiv_id": "2501.11631v1",
      "title": "Noise-Agnostic Multitask Whisper Training for Reducing False Alarm Errors in Call-for-Help Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Myeonghoon Ryu",
        "June-Woo Kim",
        "Minseok Oh",
        "Suji Lee",
        "Han Park"
      ],
      "abstract": "Keyword spotting is often implemented by keyword classifier to the encoder in\nacoustic models, enabling the classification of predefined or open vocabulary\nkeywords. Although keyword spotting is a crucial task in various applications\nand can be extended to call-for-help detection in emergencies, however, the\nprevious method often suffers from scalability limitations due to retraining\nrequired to introduce new keywords or adapt to changing contexts. We explore a\nsimple yet effective approach that leverages off-the-shelf pretrained ASR\nmodels to address these challenges, especially in call-for-help detection\nscenarios. Furthermore, we observed a substantial increase in false alarms when\ndeploying call-for-help detection system in real-world scenarios due to noise\nintroduced by microphones or different environments. To address this, we\npropose a novel noise-agnostic multitask learning approach that integrates a\nnoise classification head into the ASR encoder. Our method enhances the model's\nrobustness to noisy environments, leading to a significant reduction in false\nalarms and improved overall call-for-help performance. Despite the added\ncomplexity of multitask learning, our approach is computationally efficient and\nprovides a promising solution for call-for-help detection in real-world\nscenarios.",
      "tldr_zh": "这篇论文针对呼救检测中的假警报问题，提出了一种噪声无关的多任务学习方法，以解决现有关键词检测系统在引入新关键词或适应环境变化时的可扩展性挑战。方法利用预训练 ASR 模型，在编码器中集成噪声分类头，实现多任务训练，从而增强模型对噪音环境的鲁棒性。实验结果表明，该方法显著减少了假警报错误，并提高了呼救检测的整体性能，同时保持了计算效率。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11631v1",
      "published_date": "2025-01-20 18:01:42 UTC",
      "updated_date": "2025-01-20 18:01:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:14:10.981958"
    },
    {
      "arxiv_id": "2501.11623v1",
      "title": "Early evidence of how LLMs outperform traditional systems on OCR/HTR tasks for historical records",
      "title_zh": "LLMs 在历史记录 OCR/HTR 任务",
      "authors": [
        "Seorin Kim",
        "Julien Baudru",
        "Wouter Ryckbosch",
        "Hugues Bersini",
        "Vincent Ginis"
      ],
      "abstract": "We explore the ability of two LLMs -- GPT-4o and Claude Sonnet 3.5 -- to\ntranscribe historical handwritten documents in a tabular format and compare\ntheir performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract,\nand TrOCR. Considering the tabular form of the data, two types of experiments\nare executed: one where the images are split line by line and the other where\nthe entire scan is used as input. Based on CER and BLEU, we demonstrate that\nLLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the\nevaluated CER and BLEU scores to human evaluations to better judge the outputs\nof whole-scan experiments and understand influential factors for CER and BLEU.\nCombining judgments from all the evaluation metrics, we conclude that two-shot\nGPT-4o for line-by-line images and two-shot Claude Sonnet 3.5 for whole-scan\nimages yield the transcriptions of the historical records most similar to the\nground truth.",
      "tldr_zh": "本研究比较了大型语言模型（LLMs）如 GPT-4o 和 Claude Sonnet 3.5 与传统 OCR/HTR 系统（如 EasyOCR、Keras、Pytesseract 和 TrOCR）在转录历史手写文档（以表格格式）方面的性能。实验采用两种设置：按行分割图像和使用整个扫描图像作为输入，并通过 CER 和 BLEU 指标评估，结果显示 LLMs 显著优于传统方法。进一步与人类评估比较后，得出结论：两-shot GPT-4o 在按行图像上表现最佳，而两-shot Claude Sonnet 3.5 在整个扫描图像上生成最接近真实值的转录。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11623v1",
      "published_date": "2025-01-20 17:46:12 UTC",
      "updated_date": "2025-01-20 17:46:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:14:24.062058"
    },
    {
      "arxiv_id": "2501.11613v7",
      "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Giorgio Robino"
      ],
      "abstract": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
      "tldr_zh": "该研究提出 Conversation Routines (CR)，一个结构化的 Prompt Engineering 框架，用于基于 Large Language Models (LLMs) 开发任务导向对话系统。该框架通过自然语言规范嵌入任务导向逻辑，系统化设计复杂对话工作流，确保行为一致性和对话灵活性。研究通过火车票预订系统和交互式故障排除助手的案例验证了 CR 的有效性，允许领域专家用自然语言设计对话流程，而软件工程师专注于核心 API 实现。尽管存在计算开销和非确定性行为等挑战，未来方向包括优化多智能体交互和提升系统鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.PL"
      ],
      "primary_category": "cs.CL",
      "comment": "Minor typos revision",
      "pdf_url": "http://arxiv.org/pdf/2501.11613v7",
      "published_date": "2025-01-20 17:19:02 UTC",
      "updated_date": "2025-02-24 17:40:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:14:36.340423"
    },
    {
      "arxiv_id": "2501.11599v1",
      "title": "SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language Models Tackling Knowledge-based Reasoning Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Wentao Wan",
        "Zhuojie Yang",
        "Yongcan Chen",
        "Chenglin Luo",
        "Ruilin Wang",
        "Kehao Cai",
        "Nan Kang",
        "Liang Lin",
        "Keze Wang"
      ],
      "abstract": "Deductive reasoning is a crucial logical capability that assists us in\nsolving complex problems based on existing knowledge. Although augmented by\nChain-of-Thought prompts, Large Language Models (LLMs) might not follow the\ncorrect reasoning paths. Enhancing the deductive reasoning abilities of LLMs,\nand leveraging their extensive built-in knowledge for various reasoning tasks,\nremains an open question. Attempting to mimic the human deductive reasoning\nparadigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought\n(SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle\ncomplex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the\nquestion and then uses the interpretation and the original question to propose\na suitable major premise. It proceeds by generating and answering minor premise\nquestions in two stages to match the minor premises. Finally, it guides LLMs to\nuse the previously generated major and minor premises to perform syllogistic\ndeductive reasoning to derive the answer to the original question. Extensive\nand thorough experiments on knowledge-based reasoning tasks have demonstrated\nthe effectiveness and advantages of our SR-FoT.",
      "tldr_zh": "该论文提出 SR-FoT，一种多阶段的三段论推理框架（Syllogistic-Reasoning Framework of Thought），旨在提升 Large Language Models (LLMs) 的演绎推理能力，以处理复杂的知识-based 推理任务。SR-FoT 模仿人类推理范式，通过解释问题、提出合适的主要前提、生成并回答次要前提问题（分两阶段匹配），最终引导 LLMs 使用这些前提进行三段论推理得出答案。与 Chain-of-Thought 提示相比，该框架在知识-based 任务上的实验显示了显著优势和有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11599v1",
      "published_date": "2025-01-20 17:00:41 UTC",
      "updated_date": "2025-01-20 17:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:14:47.820968"
    },
    {
      "arxiv_id": "2501.11597v1",
      "title": "Fairness Testing through Extreme Value Theory",
      "title_zh": "基于极值理论的公平性测试",
      "authors": [
        "Verya Monjezi",
        "Ashutosh Trivedi",
        "Vladik Kreinovich",
        "Saeid Tizpaz-Niari"
      ],
      "abstract": "Data-driven software is increasingly being used as a critical component of\nautomated decision-support systems. Since this class of software learns its\nlogic from historical data, it can encode or amplify discriminatory practices.\nPrevious research on algorithmic fairness has focused on improving average-case\nfairness. On the other hand, fairness at the extreme ends of the spectrum,\nwhich often signifies lasting and impactful shifts in societal attitudes, has\nreceived significantly less emphasis.\n  Leveraging the statistics of extreme value theory (EVT), we propose a novel\nfairness criterion called extreme counterfactual discrimination (ECD). This\ncriterion estimates the worst-case amounts of disadvantage in outcomes for\nindividuals solely based on their memberships in a protected group. Utilizing\ntools from search-based software engineering and generative AI, we present a\nrandomized algorithm that samples a statistically significant set of points\nfrom the tail of ML outcome distributions even if the input dataset lacks a\nsufficient number of relevant samples.\n  We conducted several experiments on four ML models (deep neural networks,\nlogistic regression, and random forests) over 10 socially relevant tasks from\nthe literature on algorithmic fairness. First, we evaluate the generative AI\nmethods and find that they generate sufficient samples to infer valid EVT\ndistribution in 95% of cases. Remarkably, we found that the prevalent bias\nmitigators reduce the average-case discrimination but increase the worst-case\ndiscrimination significantly in 5% of cases. We also observed that even the\ntail-aware mitigation algorithm -- MiniMax-Fairness -- increased the worst-case\ndiscrimination in 30% of cases. We propose a novel ECD-based mitigator that\nimproves fairness in the tail in 90% of cases with no degradation of the\naverage-case discrimination.",
      "tldr_zh": "本研究针对数据驱动软件可能放大歧视的问题，提出了一种基于Extreme Value Theory (EVT)的公平性测试标准——extreme counterfactual discrimination (ECD)，用于评估保护群体在极端情况下的最坏结果不利程度。研究开发了一个随机算法，利用搜索-based软件工程和生成AI，从机器学习结果分布的尾部采样统计显著样本，即使原始数据集不足。实验在4个ML模型（如深度神经网络和随机森林）上测试了10个社会相关任务，结果显示现有偏置缓解方法（如MiniMax-Fairness）在30%情况下增加了最坏情况歧视，而新提出的ECD-based缓解器在90%情况下改善了尾部公平性，同时不降低平均情况歧视。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "In IEEE/ACM 47th International Conference on Software Engineering\n  (ICSE'25)",
      "pdf_url": "http://arxiv.org/pdf/2501.11597v1",
      "published_date": "2025-01-20 16:56:10 UTC",
      "updated_date": "2025-01-20 16:56:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:14:59.399027"
    },
    {
      "arxiv_id": "2501.13956v1",
      "title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
      "title_zh": "Zep：一种用于智能体记忆的时序知识图架构",
      "authors": [
        "Preston Rasmussen",
        "Pavlo Paliychuk",
        "Travis Beauvais",
        "Jack Ryan",
        "Daniel Chalef"
      ],
      "abstract": "We introduce Zep, a novel memory layer service for AI agents that outperforms\nthe current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR)\nbenchmark. Additionally, Zep excels in more comprehensive and challenging\nevaluations than DMR that better reflect real-world enterprise use cases. While\nexisting retrieval-augmented generation (RAG) frameworks for large language\nmodel (LLM)-based agents are limited to static document retrieval, enterprise\napplications demand dynamic knowledge integration from diverse sources\nincluding ongoing conversations and business data. Zep addresses this\nfundamental limitation through its core component Graphiti -- a\ntemporally-aware knowledge graph engine that dynamically synthesizes both\nunstructured conversational data and structured business data while maintaining\nhistorical relationships. In the DMR benchmark, which the MemGPT team\nestablished as their primary evaluation metric, Zep demonstrates superior\nperformance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further\nvalidated through the more challenging LongMemEval benchmark, which better\nreflects enterprise use cases through complex temporal reasoning tasks. In this\nevaluation, Zep achieves substantial results with accuracy improvements of up\nto 18.5% while simultaneously reducing response latency by 90% compared to\nbaseline implementations. These results are particularly pronounced in\nenterprise-critical tasks such as cross-session information synthesis and\nlong-term context maintenance, demonstrating Zep's effectiveness for deployment\nin real-world applications.",
      "tldr_zh": "本研究介绍了Zep，一种新型的AI代理记忆层服务，它在Deep Memory Retrieval (DMR)基准上超越了当前最先进系统MemGPT（94.8% vs 93.4%），并在更全面的LongMemEval基准中表现出色。Zep的核心组件Graphiti是一个时间感知的知识图引擎，能够动态合成非结构化对话数据和结构化业务数据，同时维护历史关系，从而解决现有Retrieval-Augmented Generation (RAG)框架的静态检索局限性。实验结果显示，Zep在企业关键任务如跨会话信息合成和长期上下文维护上实现了高达18.5%的准确率提升，并将响应延迟减少90%，证明了其在真实企业场景中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.13956v1",
      "published_date": "2025-01-20 16:52:48 UTC",
      "updated_date": "2025-01-20 16:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:15:11.667037"
    },
    {
      "arxiv_id": "2501.11592v2",
      "title": "Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing",
      "title_zh": "无需训练的超小型模型用于压缩感知中的通用稀疏重建",
      "authors": [
        "Chaoqing Tang",
        "Huanze Zhuang",
        "Guiyun Tian",
        "Zhenli Zeng",
        "Yi Ding",
        "Wenzhong Liu",
        "Xiang Bai"
      ],
      "abstract": "Pre-trained large models attract widespread attention in recent years, but\nthey face challenges in applications that require high interpretability or have\nlimited resources, such as physical sensing, medical imaging, and\nbioinformatics. Compressed Sensing (CS) is a well-proved theory that drives\nmany recent breakthroughs in these applications. However, as a typical\nunder-determined linear system, CS suffers from excessively long sparse\nreconstruction times when using traditional iterative methods, particularly\nwith large-scale data. Current AI methods like deep unfolding fail to\nsubstitute them because pre-trained models exhibit poor generality beyond their\ntraining conditions and dataset distributions, or lack interpretability.\nInstead of following the big model fervor, this paper proposes ultra-small\nartificial neural models called coefficients learning (CL), enabling\ntraining-free and rapid sparse reconstruction while perfectly inheriting the\ngenerality and interpretability of traditional iterative methods, bringing new\nfeature of incorporating prior knowledges. In CL, a signal of length $n$ only\nneeds a minimal of $n$ trainable parameters. A case study model called CLOMP is\nimplemented for evaluation. Experiments are conducted on both synthetic and\nreal one-dimensional and two-dimensional signals, demonstrating significant\nimprovements in efficiency and accuracy. Compared to representative iterative\nmethods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data.\nTest results on eight diverse image datasets indicate that CLOMP improves\nstructural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3,\n0.5, respectively. We believe this method can truly usher CS reconstruction\ninto the AI era, benefiting countless under-determined linear systems that rely\non sparse solution.",
      "tldr_zh": "本论文针对 Compressed Sensing (CS) 中传统迭代方法的低效率问题，提出了一种无需训练的超小模型 coefficients learning (CL)，它仅需信号长度 n 的参数即可实现快速稀疏重建，同时继承了传统方法的泛化性、可解释性和先验知识整合能力。CL 的案例模型 CLOMP 在合成和真实信号实验中，效率提高了 100 到 1000 倍，并在八个图像数据集上将结构相似性指数 (SSIM) 分别提升了 292%、98% 和 45%。这项创新方法有望将 CS 重建推向 AI 时代，适用于众多依赖稀疏解的欠定线性系统。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11592v2",
      "published_date": "2025-01-20 16:50:59 UTC",
      "updated_date": "2025-01-23 12:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:15:25.163332"
    },
    {
      "arxiv_id": "2501.11587v2",
      "title": "Recurrent Diffusion for Large-Scale Parameter Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Wang",
        "Dongwen Tang",
        "Wangbo Zhao",
        "Konstantin Schürholt",
        "Zhangyang Wang",
        "Yang You"
      ],
      "abstract": "Parameter generation has long struggled to match the scale of today large\nvision and language models, curbing its broader utility. In this paper, we\nintroduce Recurrent Diffusion for Large Scale Parameter Generation (RPG), a\nnovel framework that generates full neural network parameters up to hundreds of\nmillions on a single GPU. Our approach first partitions a networks parameters\ninto non-overlapping tokens, each corresponding to a distinct portion of the\nmodel. A recurrent mechanism then learns the inter token relationships,\nproducing prototypes which serve as conditions for a diffusion process that\nultimately synthesizes the full parameters. Across a spectrum of architectures\nand tasks including ResNets, ConvNeXts and ViTs on ImageNet 1K and COCO, and\neven LoRA based LLMs RPG achieves performance on par with fully trained\nnetworks while avoiding excessive memory overhead. Notably, it generalizes\nbeyond its training set to generate valid parameters for previously unseen\ntasks, highlighting its flexibility in dynamic and open ended scenarios. By\novercoming the longstanding memory and scalability barriers, RPG serves as a\ncritical advance in AI generating AI, potentially enabling efficient weight\ngeneration at scales previously deemed infeasible.",
      "tldr_zh": "本论文提出Recurrent Diffusion for Large Scale Parameter Generation (RPG)框架，用于高效生成大规模神经网络参数（高达数百万级别，仅需单GPU）。RPG的方法包括将网络参数分区为非重叠的tokens，通过循环机制学习tokens间关系生成原型，并利用diffusion process合成完整参数，从而克服传统参数生成的内存和可扩展性限制。在ResNets、ConvNeXts和ViTs等架构上，以及ImageNet 1K和COCO任务中，RPG的性能与完全训练网络相当，同时支持LoRA based LLMs，并能泛化到未见任务。总之，该框架推动了AI生成AI的发展，实现高效的动态参数生成。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Generating 200 million parameters in just minutes",
      "pdf_url": "http://arxiv.org/pdf/2501.11587v2",
      "published_date": "2025-01-20 16:46:26 UTC",
      "updated_date": "2025-02-11 03:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:15:37.171871"
    },
    {
      "arxiv_id": "2502.10399v1",
      "title": "Data Stewardship Decoded: Mapping Its Diverse Manifestations and Emerging Relevance at a time of AI",
      "title_zh": "翻译失败",
      "authors": [
        "Stefaan Verhulst"
      ],
      "abstract": "Data stewardship has become a critical component of modern data governance,\nespecially with the growing use of artificial intelligence (AI). Despite its\nincreasing importance, the concept of data stewardship remains ambiguous and\nvaries in its application. This paper explores four distinct manifestations of\ndata stewardship to clarify its emerging position in the data governance\nlandscape. These manifestations include a) data stewardship as a set of\ncompetencies and skills, b) a function or role within organizations, c) an\nintermediary organization facilitating collaborations, and d) a set of guiding\nprinciples. The paper subsequently outlines the core competencies required for\neffective data stewardship, explains the distinction between data stewards and\nChief Data Officers (CDOs), and details the intermediary role of stewards in\nbridging gaps between data holders and external stakeholders. It also explores\nkey principles aligned with the FAIR framework (Findable, Accessible,\nInteroperable, Reusable) and introduces the emerging principle of AI readiness\nto ensure data meets the ethical and technical requirements of AI systems. The\npaper emphasizes the importance of data stewardship in enhancing data\ncollaboration, fostering public value, and managing data reuse responsibly,\nparticularly in the era of AI. It concludes by identifying challenges and\nopportunities for advancing data stewardship, including the need for\nstandardized definitions, capacity building efforts, and the creation of a\nprofessional association for data stewardship.",
      "tldr_zh": "这篇论文探讨了数据管理（Data stewardship）在AI时代的关键作用，通过映射其四种表现形式——作为一组能力和技能、组织内的职能或角色、促进合作的中间组织以及一套指导原则——来澄清其概念。论文概述了有效数据管理的核心能力，区分了数据管理者（data stewards）和首席数据官（CDOs）的角色，并引入了与FAIR框架（Findable, Accessible, Interoperable, Reusable）相关的原则以及新兴的AI readiness原则，以确保数据符合AI系统的伦理和技术要求。最终，它强调数据管理在增强数据合作、促进公共价值和负责任数据重用方面的意义，并指出未来挑战，如标准化定义、能力建设和建立专业协会的机会。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.10399v1",
      "published_date": "2025-01-20 16:24:22 UTC",
      "updated_date": "2025-01-20 16:24:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:16:26.694750"
    },
    {
      "arxiv_id": "2501.11560v1",
      "title": "Explainable Lane Change Prediction for Near-Crash Scenarios Using Knowledge Graph Embeddings and Retrieval Augmented Generation",
      "title_zh": "针对近乎碰撞场景的可解释车道变更预测，使用知识图谱嵌入和",
      "authors": [
        "M. Manzour",
        "A. Ballardini",
        "R. Izquierdo",
        "M. Á. Sotelo"
      ],
      "abstract": "Lane-changing maneuvers, particularly those executed abruptly or in risky\nsituations, are a significant cause of road traffic accidents. However, current\nresearch mainly focuses on predicting safe lane changes. Furthermore, existing\naccident datasets are often based on images only and lack comprehensive sensory\ndata. In this work, we focus on predicting risky lane changes using the CRASH\ndataset (our own collected dataset specifically for risky lane changes), and\nsafe lane changes (using the HighD dataset). Then, we leverage KG and Bayesian\ninference to predict these maneuvers using linguistic contextual information,\nenhancing the model's interpretability and transparency. The model achieved a\n91.5% f1-score with anticipation time extending to four seconds for risky lane\nchanges, and a 90.0% f1-score for predicting safe lane changes with the same\nanticipation time. We validate our model by integrating it into a vehicle\nwithin the CARLA simulator in scenarios that involve risky lane changes. The\nmodel managed to anticipate sudden lane changes, thus providing automated\nvehicles with further time to plan and execute appropriate safe reactions.\nFinally, to enhance the explainability of our model, we utilize RAG to provide\nclear and natural language explanations for the given prediction.",
      "tldr_zh": "这篇论文针对风险变道场景，提出了一种可解释的变道预测模型，使用 Knowledge Graph Embeddings 和 Retrieval Augmented Generation (RAG) 结合 Bayesian inference，通过语言上下文信息进行预测。模型基于自有 CRASH 数据集（针对风险变道）和 HighD 数据集（针对安全变道），实现了91.5%的F1分数用于风险变道预测，以及90.0%的F1分数用于安全变道预测，均可提前4秒预警。在 CARLA 模拟器中验证后，模型成功帮助自动车辆及早规划安全反应。最终，通过 RAG 提供清晰的自然语言解释，提升了模型的透明度和可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11560v1",
      "published_date": "2025-01-20 16:02:26 UTC",
      "updated_date": "2025-01-20 16:02:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:16:00.133113"
    },
    {
      "arxiv_id": "2502.10398v2",
      "title": "Practical Application and Limitations of AI Certification Catalogues in the Light of the AI Act",
      "title_zh": "翻译失败",
      "authors": [
        "Gregor Autischer",
        "Kerstin Waxnegger",
        "Dominik Kowald"
      ],
      "abstract": "In this work-in-progress, we investigate the certification of AI systems,\nfocusing on the practical application and limitations of existing certification\ncatalogues in the light of the AI Act by attempting to certify a publicly\navailable AI system. We aim to evaluate how well current approaches work to\neffectively certify an AI system, and how publicly accessible AI systems, that\nmight not be actively maintained or initially intended for certification, can\nbe selected and used for a sample certification process. Our methodology\ninvolves leveraging the Fraunhofer AI Assessment Catalogue as a comprehensive\ntool to systematically assess an AI model's compliance with certification\nstandards. We find that while the catalogue effectively structures the\nevaluation process, it can also be cumbersome and time-consuming to use. We\nobserve the limitations of an AI system that has no active development team\nanymore and highlighted the importance of complete system documentation.\nFinally, we identify some limitations of the certification catalogues used and\nproposed ideas on how to streamline the certification process.",
      "tldr_zh": "本研究探讨了 AI 系统认证的实际应用和限制，聚焦于现有认证目录在 AI Act 下的有效性，通过尝试认证一个公开可用的 AI 系统进行评估。研究采用 Fraunhofer AI Assessment Catalogue 作为工具，系统评估 AI 模型的合规性，发现该目录虽然能有效结构化评估过程，但使用起来繁琐且耗时。最终，研究强调了缺乏活跃开发团队的 AI 系统文档不完整的问题，并提出改进想法以简化认证流程。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Bachelor thesis at Graz University of Technology, in preparation for\n  a conference paper submission at EWAF'25",
      "pdf_url": "http://arxiv.org/pdf/2502.10398v2",
      "published_date": "2025-01-20 15:54:57 UTC",
      "updated_date": "2025-02-18 07:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:16:11.034592"
    },
    {
      "arxiv_id": "2502.07790v1",
      "title": "Can Generative AI be Egalitarian?",
      "title_zh": "生成式 AI 能是平等主义的吗？",
      "authors": [
        "Philip Feldman",
        "James R. Foulds",
        "Shimei Pan"
      ],
      "abstract": "The recent explosion of \"foundation\" generative AI models has been built upon\nthe extensive extraction of value from online sources, often without\ncorresponding reciprocation. This pattern mirrors and intensifies the\nextractive practices of surveillance capitalism, while the potential for\nenormous profit has challenged technology organizations' commitments to\nresponsible AI practices, raising significant ethical and societal concerns.\nHowever, a promising alternative is emerging: the development of models that\nrely on content willingly and collaboratively provided by users. This article\nexplores this \"egalitarian\" approach to generative AI, taking inspiration from\nthe successful model of Wikipedia. We explore the potential implications of\nthis approach for the design, development, and constraints of future foundation\nmodels. We argue that such an approach is not only ethically sound but may also\nlead to models that are more responsive to user needs, more diverse in their\ntraining data, and ultimately more aligned with societal values. Furthermore,\nwe explore potential challenges and limitations of this approach, including\nissues of scalability, quality control, and potential biases inherent in\nvolunteer-contributed content.",
      "tldr_zh": "这篇论文探讨了生成式 AI（generative AI）是否能采用“egalitarian”方法，即基于用户自愿和协作提供内容的模型，以取代当前依赖大规模提取在线数据的做法，这种提取方式类似于监控资本主义并引发伦理和社会问题。论文借鉴 Wikipedia 的成功模式，论证这种平等主义方法不仅更符合伦理，还能使模型更响应用户需求、训练数据更多样化，并更好地与社会价值观一致。论文同时指出了潜在挑战，包括可扩展性、质量控制以及志愿内容中固有的偏见。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.4.1"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.07790v1",
      "published_date": "2025-01-20 15:40:44 UTC",
      "updated_date": "2025-01-20 15:40:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:16:23.111714"
    },
    {
      "arxiv_id": "2501.11533v1",
      "title": "The impact of intrinsic rewards on exploration in Reinforcement Learning",
      "title_zh": "内在奖励对强化学习中探索的影响",
      "authors": [
        "Aya Kayal",
        "Eduardo Pignatelli",
        "Laura Toni"
      ],
      "abstract": "One of the open challenges in Reinforcement Learning is the hard exploration\nproblem in sparse reward environments. Various types of intrinsic rewards have\nbeen proposed to address this challenge by pushing towards diversity. This\ndiversity might be imposed at different levels, favouring the agent to explore\ndifferent states, policies or behaviours (State, Policy and Skill level\ndiversity, respectively). However, the impact of diversity on the agent's\nbehaviour remains unclear. In this work, we aim to fill this gap by studying\nthe effect of different levels of diversity imposed by intrinsic rewards on the\nexploration patterns of RL agents. We select four intrinsic rewards (State\nCount, Intrinsic Curiosity Module (ICM), Maximum Entropy, and Diversity is all\nyou need (DIAYN)), each pushing for a different diversity level. We conduct an\nempirical study on MiniGrid environment to compare their impact on exploration\nconsidering various metrics related to the agent's exploration, namely:\nepisodic return, observation coverage, agent's position coverage, policy\nentropy, and timeframes to reach the sparse reward. The main outcome of the\nstudy is that State Count leads to the best exploration performance in the case\nof low-dimensional observations. However, in the case of RGB observations, the\nperformance of State Count is highly degraded mostly due to representation\nlearning challenges. Conversely, Maximum Entropy is less impacted, resulting in\na more robust exploration, despite being not always optimal. Lastly, our\nempirical study revealed that learning diverse skills with DIAYN, often linked\nto improved robustness and generalisation, does not promote exploration in\nMiniGrid environments. This is because: i) learning the skill space itself can\nbe challenging, and ii) exploration within the skill space prioritises\ndifferentiating between behaviours rather than achieving uniform state\nvisitation.",
      "tldr_zh": "这篇论文探讨了内在奖励（intrinsic rewards）对强化学习（Reinforcement Learning）中探索的影响，特别是在稀疏奖励环境的困难探索问题上。研究者比较了四种内在奖励（State Count、Intrinsic Curiosity Module (ICM)、Maximum Entropy 和 Diversity is all you need (DIAYN)）在 MiniGrid 环境中的表现，使用指标如 episodic return、observation coverage 和 policy entropy 进行评估。主要发现是，State Count 在低维观察下探索效果最佳，但 RGB 观察时性能下降；Maximum Entropy 更稳健；DIAYN 虽能学习多样技能，却不促进有效探索，因为技能空间学习具有挑战，且优先区分行为而非均匀访问状态。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "45 pages, 17 figures. Submitted to Neural Computing and Applications\n  Journal",
      "pdf_url": "http://arxiv.org/pdf/2501.11533v1",
      "published_date": "2025-01-20 15:17:24 UTC",
      "updated_date": "2025-01-20 15:17:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:16:39.335805"
    },
    {
      "arxiv_id": "2501.13955v1",
      "title": "Guided Persona-based AI Surveys: Can we replicate personal mobility preferences at scale using LLMs?",
      "title_zh": "翻译失败",
      "authors": [
        "Ioannis Tzachristas",
        "Santhanakrishnan Narayanan",
        "Constantinos Antoniou"
      ],
      "abstract": "This study explores the potential of Large Language Models (LLMs) to generate\nartificial surveys, with a focus on personal mobility preferences in Germany.\nBy leveraging LLMs for synthetic data creation, we aim to address the\nlimitations of traditional survey methods, such as high costs, inefficiency and\nscalability challenges. A novel approach incorporating \"Personas\" -\ncombinations of demographic and behavioural attributes - is introduced and\ncompared to five other synthetic survey methods, which vary in their use of\nreal-world data and methodological complexity. The MiD 2017 dataset, a\ncomprehensive mobility survey in Germany, serves as a benchmark to assess the\nalignment of synthetic data with real-world patterns. The results demonstrate\nthat LLMs can effectively capture complex dependencies between demographic\nattributes and preferences while offering flexibility to explore hypothetical\nscenarios. This approach presents valuable opportunities for transportation\nplanning and social science research, enabling scalable, cost-efficient and\nprivacy-preserving data generation.",
      "tldr_zh": "本研究探讨了使用大型语言模型(LLMs)生成合成调查数据，以大规模复制德国的个人出行偏好，旨在克服传统调查的成本高、效率低和可扩展性问题。研究引入了一种基于“Personas”（结合人口统计和行为属性的组合）的新方法，并与五种其他合成方法比较，使用MiD 2017数据集作为基准评估数据准确性。结果表明，LLMs能有效捕捉人口属性与偏好间的复杂依赖关系，并提供灵活性探索假设场景，从而为交通规划和社会科学研究提供可扩展、成本高效且隐私保护的数据生成途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13955v1",
      "published_date": "2025-01-20 15:11:03 UTC",
      "updated_date": "2025-01-20 15:11:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:16:50.499951"
    },
    {
      "arxiv_id": "2501.11526v1",
      "title": "Meta-Instance Selection. Instance Selection as a Classification Problem with Meta-Features",
      "title_zh": "元实例选择：实例选择作为带有元特征的分类问题",
      "authors": [
        "Marcin Blachnik",
        "Piotr Ciepliński"
      ],
      "abstract": "Data pruning, or instance selection, is an important problem in machine\nlearning especially in terms of nearest neighbour classifier. However, in data\npruning which speeds up the prediction phase, there is an issue related to the\nspeed and efficiency of the process itself. In response, the study proposes an\napproach involving transforming the instance selection process into a\nclassification task conducted in a unified meta-feature space where each\ninstance can be classified and assigned to either the \"to keep\" or \"to remove\"\nclass. This approach requires training an appropriate meta-classifier, which\ncan be developed based on historical instance selection results from other\ndatasets using reference instance selection methods as a labeling tool. This\nwork proposes constructing the meta-feature space based on properties extracted\nfrom the nearest neighbor graph. Experiments conducted on 17 datasets of\nvarying sizes and five reference instance selection methods (ENN, Drop3, ICF,\nHMN-EI, and CCIS) demonstrate that the proposed solution achieves results\ncomparable to reference instance selection methods while significantly reducing\ncomputational complexity. In the proposed approach, the computational\ncomplexity of the system depends only on identifying the k-nearest neighbors\nfor each data sample and running the meta-classifier. Additionally, the study\ndiscusses the choice of meta-classifier, recommending the use of Balanced\nRandom Forest.",
      "tldr_zh": "该研究将实例选择（instance selection）问题转化为一个分类任务，通过在统一的元特征空间（meta-feature space）中将每个实例分类为“保留”或“移除”，以提高数据修剪的效率。方法基于最近邻图（nearest neighbor graph）的属性构建元特征空间，并使用参考实例选择方法（如 ENN、Drop3、ICF、HMN-EI 和 CCIS）作为标签工具训练元分类器（meta-classifier）。在 17 个不同规模数据集上的实验显示，该方法与参考方法性能相当，但显著降低了计算复杂度，仅依赖于识别 k-nearest neighbors 和运行元分类器。论文推荐使用 Balanced Random Forest 作为元分类器，以进一步优化效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11526v1",
      "published_date": "2025-01-20 15:08:19 UTC",
      "updated_date": "2025-01-20 15:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:17:02.000935"
    },
    {
      "arxiv_id": "2501.11525v1",
      "title": "Technical Report for the Forgotten-by-Design Project: Targeted Obfuscation for Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rickard Brännvall",
        "Laurynas Adomaitis",
        "Olof Görnerup",
        "Anass Sedrati"
      ],
      "abstract": "The right to privacy, enshrined in various human rights declarations, faces\nnew challenges in the age of artificial intelligence (AI). This paper explores\nthe concept of the Right to be Forgotten (RTBF) within AI systems, contrasting\nit with traditional data erasure methods. We introduce Forgotten by Design, a\nproactive approach to privacy preservation that integrates instance-specific\nobfuscation techniques during the AI model training process. Unlike machine\nunlearning, which modifies models post-training, our method prevents sensitive\ndata from being embedded in the first place. Using the LIRA membership\ninference attack, we identify vulnerable data points and propose defenses that\ncombine additive gradient noise and weighting schemes. Our experiments on the\nCIFAR-10 dataset demonstrate that our techniques reduce privacy risks by at\nleast an order of magnitude while maintaining model accuracy (at 95%\nsignificance). Additionally, we present visualization methods for the\nprivacy-utility trade-off, providing a clear framework for balancing privacy\nrisk and model accuracy. This work contributes to the development of\nprivacy-preserving AI systems that align with human cognitive processes of\nmotivated forgetting, offering a robust framework for safeguarding sensitive\ninformation and ensuring compliance with privacy regulations.",
      "tldr_zh": "该论文探讨了人工智能(AI)时代下隐私权(Right to be Forgotten, RTBF)的挑战，提出Forgotten by Design方法，这是一种主动隐私保护策略，通过在模型训练过程中整合实例特定混淆技术，防止敏感数据被嵌入模型。不同于传统的机器遗忘(machine unlearning)，该方法使用LIRA成员推理攻击识别易受攻击的数据点，并结合添加梯度噪声和加权方案进行防御。实验在CIFAR-10数据集上显示，该技术将隐私风险降低至少一个数量级，同时保持模型准确性（95%显著性水平），并提供可视化框架来平衡隐私风险与模型效用，最终为符合人类动机遗忘的隐私保护AI系统提供稳健框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "68T09 (Primary) 68T05 (Secondary)",
        "D.4.6; K.6.5; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11525v1",
      "published_date": "2025-01-20 15:07:59 UTC",
      "updated_date": "2025-01-20 15:07:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:17:14.143553"
    },
    {
      "arxiv_id": "2501.11498v1",
      "title": "Dialect2SQL: A Novel Text-to-SQL Dataset for Arabic Dialects with a Focus on Moroccan Darija",
      "title_zh": "翻译失败",
      "authors": [
        "Salmane Chafik",
        "Saad Ezzini",
        "Ismail Berrada"
      ],
      "abstract": "The task of converting natural language questions (NLQs) into executable SQL\nqueries, known as text-to-SQL, has gained significant interest in recent years,\nas it enables non-technical users to interact with relational databases. Many\nbenchmarks, such as SPIDER and WikiSQL, have contributed to the development of\nnew models and the evaluation of their performance. In addition, other\ndatasets, like SEDE and BIRD, have introduced more challenges and complexities\nto better map real-world scenarios. However, these datasets primarily focus on\nhigh-resource languages such as English and Chinese. In this work, we introduce\nDialect2SQL, the first large-scale, cross-domain text-to-SQL dataset in an\nArabic dialect. It consists of 9,428 NLQ-SQL pairs across 69 databases in\nvarious domains. Along with SQL-related challenges such as long schemas, dirty\nvalues, and complex queries, our dataset also incorporates the complexities of\nthe Moroccan dialect, which is known for its diverse source languages, numerous\nborrowed words, and unique expressions. This demonstrates that our dataset will\nbe a valuable contribution to both the text-to-SQL community and the\ndevelopment of resources for low-resource languages.",
      "tldr_zh": "本研究推出了 Dialect2SQL，这是一个全新的文本-to-SQL 数据集，专注于阿拉伯方言，特别是摩洛哥 Darija，作为首个大规模跨领域数据集。Dialect2SQL 包含 9,428 个 NLQ-SQL 对，覆盖 69 个数据库，并处理了 SQL 相关的挑战，如长 schemas、dirty values 和 complex queries，同时融入方言的复杂性，包括多种源语言、借用词汇和独特表达。数据集的发布将为文本-to-SQL 社区和低资源语言资源开发带来重要贡献，促进相关模型的改进和实际应用。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11498v1",
      "published_date": "2025-01-20 14:06:40 UTC",
      "updated_date": "2025-01-20 14:06:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:17:25.582789"
    },
    {
      "arxiv_id": "2501.11496v2",
      "title": "Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent Koc"
      ],
      "abstract": "The global crisis of language endangerment meets a technological turning\npoint as Generative AI (GenAI) and Large Language Models (LLMs) unlock new\nfrontiers in automating corpus creation, transcription, translation, and\ntutoring. However, this promise is imperiled by fragmented practices and the\ncritical lack of a methodology to navigate the fraught balance between LLM\ncapabilities and the profound risks of data scarcity, cultural\nmisappropriation, and ethical missteps. This paper introduces a novel\nanalytical framework that systematically evaluates GenAI applications against\nlanguage-specific needs, embedding community governance and ethical safeguards\nas foundational pillars. We demonstrate its efficacy through the Te Reo M\\=aori\nrevitalization, where it illuminates successes, such as community-led Automatic\nSpeech Recognition achieving 92% accuracy, while critically surfacing\npersistent challenges in data sovereignty and model bias for digital archives\nand educational tools. Our findings underscore that GenAI can indeed\nrevolutionize language preservation, but only when interventions are rigorously\nanchored in community-centric data stewardship, continuous evaluation, and\ntransparent risk management. Ultimately, this framework provides an\nindispensable toolkit for researchers, language communities, and policymakers,\naiming to catalyze the ethical and high-impact deployment of LLMs to safeguard\nthe world's linguistic heritage.",
      "tldr_zh": "这篇论文探讨了 Generative AI 和 Large Language Models (LLMs) 在语言保存中的机会与挑战，强调这些技术可自动化语料创建、转录、翻译和辅导，但面临数据稀缺、文化误用和伦理风险等问题。论文引入了一个新分析框架，用于系统评估 GenAI 应用，结合语言特定需求、社区治理和伦理保障。框架在 Te Reo Māori 语言复兴案例中证明了其效能，例如社区主导的 Automatic Speech Recognition (ASR) 达到了 92% 准确率，同时暴露了数据主权和模型偏差的挑战。最终，研究强调 GenAI 只有在社区中心的数据管理和透明风险管理下，才能有效保护全球语言遗产，并为研究者、社区和政策制定者提供实用工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50, 91F20",
        "I.2.7; I.2.6; J.5"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 figures, 2 tables, submitted for IEEE publication.\n  Pre-print updated as part of review process",
      "pdf_url": "http://arxiv.org/pdf/2501.11496v2",
      "published_date": "2025-01-20 14:03:40 UTC",
      "updated_date": "2025-05-19 12:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:17:39.066819"
    },
    {
      "arxiv_id": "2501.11493v2",
      "title": "Communication-Efficient Federated Learning Based on Explanation-Guided Pruning for Remote Sensing Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jonas Klotz",
        "Barış Büyüktaş",
        "Begüm Demir"
      ],
      "abstract": "Federated learning (FL) is a decentralized machine learning paradigm in which\nmultiple clients collaboratively train a global model by exchanging only model\nupdates with the central server without sharing the local data of the clients.\nDue to the large volume of model updates required to be transmitted between\nclients and the central server, most FL systems are associated with high\ntransfer costs (i.e., communication overhead). This issue is more critical for\noperational applications in remote sensing (RS), especially when large-scale RS\ndata is processed and analyzed through FL systems with restricted communication\nbandwidth. To address this issue, we introduce an explanation-guided pruning\nstrategy for communication-efficient FL in the context of RS image\nclassification. Our pruning strategy is defined based on the layer-wise\nrelevance propagation (LRP) driven explanations to: 1) efficiently and\neffectively identify the most relevant and informative model parameters (to be\nexchanged between clients and the central server); and 2) eliminate the\nnon-informative ones to minimize the volume of model updates. The experimental\nresults on the BigEarthNet-S2 dataset demonstrate that our strategy effectively\nreduces the number of shared model updates, while increasing the generalization\nability of the global model. The code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/FL-LRP.",
      "tldr_zh": "该论文提出了一种基于解释引导修剪(Explanation-Guided Pruning)的通信高效联邦学习(FL)方法，针对遥感(Remote Sensing, RS)图像分类任务，旨在减少客户端与服务器之间模型更新的传输量。该方法利用层级相关传播(Layer-wise Relevance Propagation, LRP)来识别和保留模型中最相关的信息参数，同时消除非信息参数，从而降低通信开销。实验结果显示，在BigEarthNet-S2数据集上，该策略显著减少了共享模型更新的数量，同时提升了全局模型的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11493v2",
      "published_date": "2025-01-20 13:59:41 UTC",
      "updated_date": "2025-05-16 14:14:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:17:52.286779"
    },
    {
      "arxiv_id": "2501.17880v1",
      "title": "Assessment of the January 2025 Los Angeles County wildfires: A multi-modal analysis of impact, response, and population exposure",
      "title_zh": "翻译失败",
      "authors": [
        "Seyd Teymoor Seydi"
      ],
      "abstract": "This study presents a comprehensive analysis of four significant California\nwildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts\nthrough multiple dimensions, including land cover change, jurisdictional\nmanagement, structural damage, and demographic vulnerability. Using the\nChebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the\nextent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares.\nOur analysis revealed that shrubland ecosystems were consistently the most\naffected, comprising 57.4-75.8% of burned areas across all events. The\njurisdictional assessment demonstrated varying management complexities, from\nsingular authority (98.7% in the Palisades Fire) to distributed management\nacross multiple agencies. A structural impact analysis revealed significant\ndisparities between urban interface fires (Eaton: 9,869 structures; Palisades:\n8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17\nstructures). The demographic analysis showed consistent gender distributions,\nwith 50.9% of the population identified as female and 49.1% as male.\nWorking-age populations made up the majority of the affected populations,\nranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods.\nThe study identified strong correlations between urban interface proximity,\nstructural damage, and population exposure. The Palisades and Eaton fires\naffected over 20,000 people each, compared to fewer than 500 in rural events.\nThese findings offer valuable insights for the development of targeted wildfire\nmanagement strategies, particularly in wildland urban interface zones, and\nemphasize the need for age- and gender-conscious approaches in emergency\nresponse planning.",
      "tldr_zh": "本研究对2025年1月的洛杉矶县四个野火（Palisades、Eaton、Kenneth和Hurst）进行了多模态分析，使用Chebyshev-Kolmogorov-Arnold网络模型和Sentinel-2图像映射烧毁区域，总面积从315.36到10,960.98公顷。分析发现，灌木地生态系统最受影响，占烧毁面积的57.4-75.8%，城市界面野火（如Eaton和Palisades）导致大量结构损害（分别达9,869和8,436结构），并显著增加人口暴露，影响超过20,000人。研究还揭示了人口统计特征，如女性占50.9%、工作年龄人口占53.7-54.1%，并为野火管理策略提供见解，强调针对野地城市界面区域的年龄和性别意识应急响应规划。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.17880v1",
      "published_date": "2025-01-20 13:50:16 UTC",
      "updated_date": "2025-01-20 13:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:18:04.349899"
    },
    {
      "arxiv_id": "2501.12418v1",
      "title": "ImageRef-VL: Enabling Contextual Image Referencing in Vision-Language Models",
      "title_zh": "ImageRef-VL：实现视觉语言模型中的上下文图像引用",
      "authors": [
        "Jingwei Yi",
        "Junhao Yin",
        "Ju Xu",
        "Peng Bao",
        "Yongliang Wang",
        "Wei Fan",
        "Hao Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\nunderstanding multimodal inputs and have been widely integrated into\nRetrieval-Augmented Generation (RAG) based conversational systems. While\ncurrent VLM-powered chatbots can provide textual source references in their\nresponses, they exhibit significant limitations in referencing contextually\nrelevant images during conversations. In this paper, we introduce Contextual\nImage Reference -- the ability to appropriately reference relevant images from\nretrieval documents based on conversation context -- and systematically\ninvestigate VLMs' capability in this aspect. We conduct the first evaluation\nfor contextual image referencing, comprising a dedicated testing dataset and\nevaluation metrics. Furthermore, we propose ImageRef-VL, a method that\nsignificantly enhances open-source VLMs' image referencing capabilities through\ninstruction fine-tuning on a large-scale, manually curated multimodal\nconversation dataset. Experimental results demonstrate that ImageRef-VL not\nonly outperforms proprietary models but also achieves an 88% performance\nimprovement over state-of-the-art open-source VLMs in contextual image\nreferencing tasks. Our code is available at\nhttps://github.com/bytedance/ImageRef-VL.",
      "tldr_zh": "本文研究了 Vision-Language Models (VLMs) 在对话系统中引用上下文相关图像的局限性，引入了 Contextual Image Reference 的概念，并首次构建了测试数据集和评估指标来系统评估这一能力。作者提出了 ImageRef-VL 方法，通过在大型手动策划的多模态对话数据集上进行指令微调(instruction fine-tuning)，显著提升了开源 VLMs 的图像引用性能。实验结果表明，ImageRef-VL 不仅超过了专有模型，还在上下文图像引用任务中比最先进的开源 VLMs 提高了 88% 的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.12418v1",
      "published_date": "2025-01-20 13:43:45 UTC",
      "updated_date": "2025-01-20 13:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:18:13.922364"
    },
    {
      "arxiv_id": "2501.11478v2",
      "title": "Each Graph is a New Language: Graph Learning with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Huachi Zhou",
        "Jiahe Du",
        "Chuang Zhou",
        "Chang Yang",
        "Yilin Xiao",
        "Yuxuan Xie",
        "Xiao Huang"
      ],
      "abstract": "Recent efforts leverage Large Language Models (LLMs) for modeling\ntext-attributed graph structures in node classification tasks. These approaches\ndescribe graph structures for LLMs to understand or aggregate LLM-generated\ntextual attribute embeddings through graph structure. However, these approaches\nface two main limitations in modeling graph structures with LLMs. (i) Graph\ndescriptions become verbose in describing high-order graph structure. (ii)\nTextual attributes alone do not contain adequate graph structure information.\nIt is challenging to model graph structure concisely and adequately with LLMs.\nLLMs lack built-in mechanisms to model graph structures directly. They also\nstruggle with complex long-range dependencies between high-order nodes and\ntarget nodes.\n  Inspired by the observation that LLMs pre-trained on one language can achieve\nexceptional performance on another with minimal additional training, we propose\n\\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge\n\\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs\nto transfer their powerful language understanding capabilities to\ngraph-structured data. GDL4LLM translates graphs into a graph language corpus\ninstead of graph descriptions and pre-trains LLMs on this corpus to adequately\nunderstand graph structures. During fine-tuning, this corpus describes the\nstructural information of target nodes concisely with only a few tokens. By\ntreating graphs as a new language, GDL4LLM enables LLMs to model graph\nstructures adequately and concisely for node classification tasks. Extensive\nexperiments on three real-world datasets demonstrate that GDL4LLM outperforms\ndescription-based and textual attribute embeddings-based baselines by\nefficiently modeling different orders of graph structure with LLMs.",
      "tldr_zh": "该论文指出，现有的方法在使用大型语言模型 (LLMs) 处理图结构（如节点分类任务）时，面临图描述冗长和高阶结构信息不足的问题。作者提出 GDL4LLM 框架，将图结构转化为一种新的图语言语料库，并在该语料库上预训练 LLMs，从而让模型高效地理解和建模图结构。创新点在于将图视为新语言，利用 LLMs 的语言理解能力，仅用少量标记即可简洁描述目标节点的结构信息。实验在三个真实数据集上显示，GDL4LLM 显著优于基于描述或文本属性的基线方法，在不同阶的图结构建模中表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11478v2",
      "published_date": "2025-01-20 13:20:41 UTC",
      "updated_date": "2025-01-23 06:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:18:26.275915"
    },
    {
      "arxiv_id": "2502.07789v1",
      "title": "Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method",
      "title_zh": "翻译失败",
      "authors": [
        "Alfredo Capozucca",
        "Daniil Yampolskyi",
        "Alexander Goldberg",
        "Maximiliano Cristiá"
      ],
      "abstract": "This paper investigates the role of AI assistants, specifically OpenAI's\nChatGPT, in teaching formal methods (FM) to undergraduate students, using the\nB-method as a formal specification technique. While existing studies\ndemonstrate the effectiveness of AI in coding tasks, no study reports on its\nimpact on formal specifications. We examine whether ChatGPT provides an\nadvantage when writing B-specifications and analyse student trust in its\noutputs. Our findings indicate that the AI does not help students to enhance\nthe correctness of their specifications, with low trust correlating to better\noutcomes. Additionally, we identify a behavioural pattern with which to\ninteract with ChatGPT which may influence the correctness of B-specifications.",
      "tldr_zh": "这篇论文研究了 AI 助手 ChatGPT 在帮助本科生学习形式化方法（FM）时的作用，特别针对使用 B-Method 编写形式化规范。研究通过实验评估了 ChatGPT 是否能提升规范的正确性，并分析了学生对 AI 输出结果的信任度。结果显示，ChatGPT 并未帮助学生提高规范的正确性，反而学生的低信任度与更好的学习成果相关。该研究还识别了一种与 ChatGPT 互动的行为模式，可能影响 B-Method 规范的准确性，为未来 AI 在形式化方法教学中的应用提供了重要启示。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.07789v1",
      "published_date": "2025-01-20 13:00:29 UTC",
      "updated_date": "2025-01-20 13:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:18:38.464053"
    },
    {
      "arxiv_id": "2501.13117v1",
      "title": "MyGO Multiplex CoT: A Method for Self-Reflection in Large Language Models via Double Chain of Thought Thinking",
      "title_zh": "翻译失败",
      "authors": [
        "Shihao Ji",
        "Zihui Song",
        "Fucheng Zhong",
        "Jisen Jia",
        "Zhaobo Wu",
        "Zheyi Cao",
        "Tianhao Xu"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated their\nimpressive abilities in various reasoning and decision-making tasks. However,\nthe quality and coherence of the reasoning process can still benefit from\nenhanced introspection and self-reflection. In this paper, we introduce\nMultiplex CoT (Chain of Thought), a method that enables LLMs to simulate a form\nof self-review while reasoning, by initiating double Chain of Thought (CoT)\nthinking. Multiplex CoT leverages the power of iterative reasoning, where the\nmodel generates an initial chain of thought and subsequently critiques and\nrefines this reasoning with a second round of thought generation. This\nrecursive approach allows for more coherent, logical, and robust answers,\nimproving the overall decision-making process. We demonstrate how this method\ncan be effectively implemented using simple prompt engineering in existing LLM\narchitectures, achieving an effect similar to that of the Learning-Refinement\nModel (LRM) without the need for additional training. Additionally, we present\na practical guide for implementing the method in Google Colab, enabling easy\nintegration into real-world applications.",
      "tldr_zh": "本论文提出MyGO Multiplex CoT方法，通过双重Chain of Thought (CoT)思考，增强Large Language Models (LLMs)在推理过程中的自我审视能力，以提高推理的质量和连贯性。该方法涉及生成初始CoT链后，进行第二轮思考来审视并完善它，实现迭代式推理，而无需额外训练，仅靠简单提示工程即可模拟Learning-Refinement Model (LRM)的效果。实验结果显示，此方法使LLMs的决策过程更逻辑、更鲁棒，并提供了在Google Colab中实施的实用指南，便于实际应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13117v1",
      "published_date": "2025-01-20 12:54:57 UTC",
      "updated_date": "2025-01-20 12:54:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:20:43.390888"
    },
    {
      "arxiv_id": "2501.11454v1",
      "title": "Improving thermal state preparation of Sachdev-Ye-Kitaev model with reinforcement learning on quantum hardware",
      "title_zh": "翻译失败",
      "authors": [
        "Akash Kundu"
      ],
      "abstract": "The Sachdev-Ye-Kitaev (SYK) model, known for its strong quantum correlations\nand chaotic behavior, serves as a key platform for quantum gravity studies.\nHowever, variationally preparing thermal states on near-term quantum processors\nfor large systems (N>12, where N is the number of Majorana fermions) presents a\nsignificant challenge due to the rapid growth in the complexity of\nparameterized quantum circuits. This paper addresses this challenge by\nintegrating reinforcement learning (RL) with convolutional neural networks,\nemploying an iterative approach to optimize the quantum circuit and its\nparameters. The refinement process is guided by a composite reward signal\nderived from entropy and the expectation values of the SYK Hamiltonian. This\napproach reduces the number of CNOT gates by two orders of magnitude for\nsystems N>10 compared to traditional methods like first-order Trotterization.\nWe demonstrate the effectiveness of the RL framework in both noiseless and\nnoisy quantum hardware environments, maintaining high accuracy in thermal state\npreparation. This work contributes to the advancement of a scalable, RL-based\nframework with applications for computations of thermal out-of-time-order\ncorrelators in quantum many-body systems and quantum gravity studies on\nnear-term quantum hardware.",
      "tldr_zh": "本文提出了一种整合强化学习 (RL) 与卷积神经网络 (CNN) 的方法，用于改进Sachdev-Ye-Kitaev (SYK) 模型的热状态准备，解决在大系统 (N>12) 上量子电路复杂度快速增长的挑战。方法采用迭代优化和基于熵及SYK哈密顿量期望值的复合奖励信号，将CNOT gates的数量比传统一阶Trotterization减少两个数量级，并在无噪声和有噪声量子硬件环境中保持高准确性。该框架为量子多体系统的热态计算和量子引力研究提供了一个可扩展的解决方案。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "hep-lat",
        "hep-th"
      ],
      "primary_category": "quant-ph",
      "comment": "The code and the data will be available soon. Comments are welcomed!",
      "pdf_url": "http://arxiv.org/pdf/2501.11454v1",
      "published_date": "2025-01-20 12:41:17 UTC",
      "updated_date": "2025-01-20 12:41:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:19:03.222136"
    },
    {
      "arxiv_id": "2501.11447v1",
      "title": "Decomposing Interventional Causality into Synergistic, Redundant, and Unique Components",
      "title_zh": "将介入性因果关系分解为协同、冗余和独特组成部分",
      "authors": [
        "Abel Jansma"
      ],
      "abstract": "We introduce a novel framework for decomposing interventional causal effects\ninto synergistic, redundant, and unique components, building on the intuition\nof Partial Information Decomposition (PID) and the principle of M\\\"obius\ninversion. While recent work has explored a similar decomposition of an\nobservational measure, we argue that a proper causal decomposition must be\ninterventional in nature. We develop a mathematical approach that\nsystematically quantifies how causal power is distributed among variables in a\nsystem, using a recently derived closed-form expression for the M\\\"obius\nfunction of the redundancy lattice. The formalism is then illustrated by\ndecomposing the causal power in logic gates, cellular automata, and chemical\nreaction networks. Our results reveal how the distribution of causal power can\nbe context- and parameter-dependent. This decomposition provides new insights\ninto complex systems by revealing how causal influences are shared and combined\namong multiple variables, with potential applications ranging from attribution\nof responsibility in legal or AI systems, to the analysis of biological\nnetworks or climate models.",
      "tldr_zh": "本研究提出一个新框架，将干预性因果效应分解为synergistic（协同）、redundant（冗余）和unique（独特）组件，基于Partial Information Decomposition (PID)的直觉和Möbius inversion原理，以弥补现有观察性措施的不足。作者开发了数学方法，使用redundancy lattice的闭合形式Möbius函数表达式，系统量化因果力量在变量间的分布，并通过逻辑门、cellular automata和化学反应网络的例子进行说明，结果显示这种分布依赖于上下文和参数。整体框架为分析复杂系统提供新见解，包括在法律或AI系统的责任归属、生物网络或气候模型中的应用。",
      "categories": [
        "cs.AI",
        "cs.IT",
        "math.IT",
        "physics.data-an",
        "68T01 (Primary) 06A11, 62D20 (Secondary)",
        "I.2.4; F.2.1; G.2.1"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11447v1",
      "published_date": "2025-01-20 12:34:51 UTC",
      "updated_date": "2025-01-20 12:34:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:19:14.497710"
    },
    {
      "arxiv_id": "2501.11430v5",
      "title": "A Survey on Diffusion Models for Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Liu",
        "Zhenchao Ma",
        "Zepu Wang",
        "Chenxuanyin Zou",
        "Jiayang Ren",
        "Zehua Wang",
        "Liang Song",
        "Bo Hu",
        "Yang Liu",
        "Victor C. M. Leung"
      ],
      "abstract": "Diffusion models (DMs) have emerged as a powerful class of generative AI\nmodels, showing remarkable potential in anomaly detection (AD) tasks across\nvarious domains, such as cybersecurity, fraud detection, healthcare, and\nmanufacturing. The intersection of these two fields, termed diffusion models\nfor anomaly detection (DMAD), offers promising solutions for identifying\ndeviations in increasingly complex and high-dimensional data. In this survey,\nwe review recent advances in DMAD research. We begin by presenting the\nfundamental concepts of AD and DMs, followed by a comprehensive analysis of\nclassic DM architectures including DDPMs, DDIMs, and Score SDEs. We further\ncategorize existing DMAD methods into reconstruction-based, density-based, and\nhybrid approaches, providing detailed examinations of their methodological\ninnovations. We also explore the diverse tasks across different data\nmodalities, encompassing image, time series, video, and multimodal data\nanalysis. Furthermore, we discuss critical challenges and emerging research\ndirections, including computational efficiency, model interpretability,\nrobustness enhancement, edge-cloud collaboration, and integration with large\nlanguage models. The collection of DMAD research papers and resources is\navailable at https://github.com/fdjingliu/DMAD.",
      "tldr_zh": "本调查综述了Diffusion Models (DMs)在Anomaly Detection (AD)领域的应用进展，强调了DMs在处理复杂高维数据时的潜力，包括网络安全、欺诈检测、健康医疗和制造等领域。论文首先介绍了AD和DMs的基本概念，并分析了经典架构如DDPMs、DDIMs和Score SDEs，然后将现有DMAD方法分类为基于重构、基于密度和混合方法，并详细探讨了其创新点。研究还涵盖了不同数据模态的应用，如图像、时间序列、视频和多模态分析，同时指出了关键挑战（如计算效率、模型可解释性和鲁棒性提升）及未来方向，包括与大型语言模型的整合。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11430v5",
      "published_date": "2025-01-20 12:06:54 UTC",
      "updated_date": "2025-02-27 02:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:19:25.408833"
    },
    {
      "arxiv_id": "2501.16357v1",
      "title": "EVolutionary Independent DEtermiNistiC Explanation",
      "title_zh": "翻译失败",
      "authors": [
        "Vincenzo Dentamaro",
        "Paolo Giglio",
        "Donato Impedovo",
        "Giuseppe Pirlo"
      ],
      "abstract": "The widespread use of artificial intelligence deep neural networks in fields\nsuch as medicine and engineering necessitates understanding their\ndecision-making processes. Current explainability methods often produce\ninconsistent results and struggle to highlight essential signals influencing\nmodel inferences. This paper introduces the Evolutionary Independent\nDeterministic Explanation (EVIDENCE) theory, a novel approach offering a\ndeterministic, model-independent method for extracting significant signals from\nblack-box models. EVIDENCE theory, grounded in robust mathematical\nformalization, is validated through empirical tests on diverse datasets,\nincluding COVID-19 audio diagnostics, Parkinson's disease voice recordings, and\nthe George Tzanetakis music classification dataset (GTZAN). Practical\napplications of EVIDENCE include improving diagnostic accuracy in healthcare\nand enhancing audio signal analysis. For instance, in the COVID-19 use case,\nEVIDENCE-filtered spectrograms fed into a frozen Residual Network with 50\nlayers improved precision by 32% for positive cases and increased the area\nunder the curve (AUC) by 16% compared to baseline models. For Parkinson's\ndisease classification, EVIDENCE achieved near-perfect precision and\nsensitivity, with a macro average F1-Score of 0.997. In the GTZAN, EVIDENCE\nmaintained a high AUC of 0.996, demonstrating its efficacy in filtering\nrelevant features for accurate genre classification. EVIDENCE outperformed\nother Explainable Artificial Intelligence (XAI) methods such as LIME, SHAP, and\nGradCAM in almost all metrics. These findings indicate that EVIDENCE not only\nimproves classification accuracy but also provides a transparent and\nreproducible explanation mechanism, crucial for advancing the trustworthiness\nand applicability of AI systems in real-world settings.",
      "tldr_zh": "本文提出 EVIDENCE 理论，这是一种确定性、模型无关的解释方法，用于从黑盒模型中提取关键信号，以提升 AI 决策过程的可解释性。EVIDENCE 通过坚实的数学基础，在 COVID-19 音频诊断、帕金森病语音记录和 GTZAN 音乐分类等数据集上进行实证验证，实现了显著改进，例如在 COVID-19 用例中，正例精确度提高 32% 和 AUC 提升 16%，而在帕金森病分类中，宏平均 F1-Score 达到 0.997。相比 LIME、SHAP 和 GradCAM 等 XAI 方法，EVIDENCE 在几乎所有指标上表现出色，并为医疗诊断和音频分析等实际应用提供了透明、可重复的解释机制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.16357v1",
      "published_date": "2025-01-20 12:05:14 UTC",
      "updated_date": "2025-01-20 12:05:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:19:38.970843"
    },
    {
      "arxiv_id": "2501.11429v2",
      "title": "The Explanation Game -- Rekindled (Extended Version)",
      "title_zh": "翻译失败",
      "authors": [
        "Joao Marques-Silva",
        "Xuanxiang Huang",
        "Olivier Letoffe"
      ],
      "abstract": "Recent work demonstrated the existence of critical flaws in the current use\nof Shapley values in explainable AI (XAI), i.e. the so-called SHAP scores.\nThese flaws are significant in that the scores provided to a human\ndecision-maker can be misleading. Although these negative results might appear\nto indicate that Shapley values ought not be used in XAI, this paper argues\notherwise. Concretely, this paper proposes a novel definition of SHAP scores\nthat overcomes existing flaws. Furthermore, the paper outlines a practically\nefficient solution for the rigorous estimation of the novel SHAP scores.\nPreliminary experimental results confirm our claims, and further underscore the\nflaws of the current SHAP scores.",
      "tldr_zh": "本文揭示了当前 Shapley values 在可解释 AI (XAI) 中的应用（即 SHAP scores）存在关键缺陷，可能导致误导人类决策者。尽管这些问题可能质疑其使用价值，但论文主张应继续应用，并提出了一种新的 SHAP scores 定义来克服现有缺陷。该方法包括一个实用高效的解决方案，用于严格估计这些新分数，初步实验结果证实了其有效性，并进一步突出了原有 SHAP scores 的不足。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11429v2",
      "published_date": "2025-01-20 12:00:36 UTC",
      "updated_date": "2025-02-15 03:49:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:19:49.258151"
    },
    {
      "arxiv_id": "2501.11428v1",
      "title": "Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography",
      "title_zh": "通过多器官分割增强非对比心脏计算机断层扫描上的冠状动脉钙化评分",
      "authors": [
        "Jakub Nalepa",
        "Tomasz Bartczak",
        "Mariusz Bujny",
        "Jarosław Gośliński",
        "Katarzyna Jesionek",
        "Wojciech Malara",
        "Filip Malawski",
        "Karol Miszalski-Jamka",
        "Patrycja Rewa",
        "Marcin Kostur"
      ],
      "abstract": "Despite coronary artery calcium scoring being considered a largely solved\nproblem within the realm of medical artificial intelligence, this paper argues\nthat significant improvements can still be made. By shifting the focus from\npathology detection to a deeper understanding of anatomy, the novel algorithm\nproposed in the paper both achieves high accuracy in coronary artery calcium\nscoring and offers enhanced interpretability of the results. This approach not\nonly aids in the precise quantification of calcifications in coronary arteries,\nbut also provides valuable insights into the underlying anatomical structures.\nThrough this anatomically-informed methodology, the paper shows how a nuanced\nunderstanding of the heart's anatomy can lead to more accurate and\ninterpretable results in the field of cardiovascular health. We demonstrate the\nsuperior accuracy of the proposed method by evaluating it on an open-source\nmulti-vendor dataset, where we obtain results at the inter-observer level,\nsurpassing the current state of the art. Finally, the qualitative analyses show\nthe practical value of the algorithm in such tasks as labeling coronary artery\ncalcifications, identifying aortic calcifications, and filtering out false\npositive detections due to noise.",
      "tldr_zh": "本文提出一种新算法，通过在Non-Contrast Cardiac Computed Tomography上进行multi-organ segmentation，将焦点从病理检测转向解剖学理解，从而提升coronary artery calcium scoring的准确性和可解释性。该方法不仅精确量化冠状动脉钙化，还提供对底层心脏结构的宝贵洞见。在开源多供应商数据集上的评估显示，该算法达到观察者间水平，超越现有最先进方法，并在标记冠状动脉钙化、识别主动脉钙化和过滤假阳性方面展现实际价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11428v1",
      "published_date": "2025-01-20 11:56:40 UTC",
      "updated_date": "2025-01-20 11:56:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:20:02.168230"
    },
    {
      "arxiv_id": "2501.11425v3",
      "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Yuan",
        "Zehui Chen",
        "Zhiheng Xi",
        "Junjie Ye",
        "Zhengyin Du",
        "Jiecao Chen"
      ],
      "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
      "tldr_zh": "该研究提出Agent-R框架，通过迭代自训练方法训练LLM代理模型，实现实时反思和错误恢复，以解决现有代理在交互环境中无法从错误中恢复的问题。Agent-R利用MCTS构建训练数据，从失败轨迹中识别第一个错误步骤，并将其与相邻正确路径拼接，增强模型的错误修正能力。实验在三个交互环境中表明，该框架使代理能够及时纠正错误并避免循环，比基线方法性能提升5.59%。这种自改进范式为智能代理的可靠性和可扩展性提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11425v3",
      "published_date": "2025-01-20 11:46:04 UTC",
      "updated_date": "2025-03-24 10:18:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:20:13.356295"
    },
    {
      "arxiv_id": "2501.11422v2",
      "title": "Multi-View Spectral Clustering for Graphs with Multiple View Structures",
      "title_zh": "翻译失败",
      "authors": [
        "Yorgos Tsitsikas",
        "Evangelos E. Papalexakis"
      ],
      "abstract": "Despite the fundamental importance of clustering, to this day, much of the\nrelevant research is still based on ambiguous foundations, leading to an\nunclear understanding of whether or how the various clustering methods are\nconnected with each other. In this work, we provide an additional stepping\nstone towards resolving such ambiguities by presenting a general clustering\nframework that subsumes a series of seemingly disparate clustering methods,\nincluding various methods belonging to the widely popular spectral clustering\nframework. In fact, the generality of the proposed framework is additionally\ncapable of shedding light to the largely unexplored area of multi-view graphs\nwhere each view may have differently clustered nodes. In turn, we propose\nGenClus: a method that is simultaneously an instance of this framework and a\ngeneralization of spectral clustering, while also being closely related to\nk-means as well. This results in a principled alternative to the few existing\nmethods studying this special type of multi-view graphs. Then, we conduct\nin-depth experiments, which demonstrate that GenClus is more computationally\nefficient than existing methods, while also attaining similar or better\nclustering performance. Lastly, a qualitative real-world case-study further\ndemonstrates the ability of GenClus to produce meaningful clusterings.",
      "tldr_zh": "该论文探讨了聚类研究的模糊基础，提出一个通用聚类框架（general clustering framework），该框架涵盖多种方法，包括广泛使用的 spectral clustering，从而澄清这些方法间的联系。作者引入了 GenClus 方法，作为该框架的实例，它是 spectral clustering 的推广，同时与 k-means 密切相关，适用于 multi-view graphs，其中每个视图可能具有不同的节点聚类结构。实验结果表明，GenClus 比现有方法计算效率更高，并实现相似的或更好的聚类性能；此外，一个真实世界的定性案例研究进一步验证了其生成有意义聚类的能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted for publication at the 2025 SIAM\n  International Conference on Data Mining (SDM2025), and this is the full\n  version of the paper",
      "pdf_url": "http://arxiv.org/pdf/2501.11422v2",
      "published_date": "2025-01-20 11:39:22 UTC",
      "updated_date": "2025-01-28 14:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:20:26.156040"
    },
    {
      "arxiv_id": "2501.13954v1",
      "title": "Chat3GPP: An Open-Source Retrieval-Augmented Generation Framework for 3GPP Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Long Huang",
        "Ming Zhao",
        "Limin Xiao",
        "Xiujun Zhang",
        "Jungang Hu"
      ],
      "abstract": "The 3rd Generation Partnership Project (3GPP) documents is key standards in\nglobal telecommunications, while posing significant challenges for engineers\nand researchers in the telecommunications field due to the large volume and\ncomplexity of their contents as well as the frequent updates. Large language\nmodels (LLMs) have shown promise in natural language processing tasks, but\ntheir general-purpose nature limits their effectiveness in specific domains\nlike telecommunications. To address this, we propose Chat3GPP, an open-source\nretrieval-augmented generation (RAG) framework tailored for 3GPP\nspecifications. By combining chunking strategies, hybrid retrieval and\nefficient indexing methods, Chat3GPP can efficiently retrieve relevant\ninformation and generate accurate responses to user queries without requiring\ndomain-specific fine-tuning, which is both flexible and scalable, offering\nsignificant potential for adapting to other technical standards beyond 3GPP. We\nevaluate Chat3GPP on two telecom-specific datasets and demonstrate its superior\nperformance compared to existing methods, showcasing its potential for\ndownstream tasks like protocol generation and code automation.",
      "tldr_zh": "该研究针对 3GPP 文档的庞大体积、复杂性和频繁更新带来的挑战，提出 Chat3GPP，一个开源的检索增强生成(RAG)框架，用于电信领域的查询响应。该框架采用分块策略、混合检索和高效索引方法，能高效检索相关信息并生成准确答案，而无需进行领域特定微调，从而实现灵活性和可扩展性。在两个电信特定数据集上的评估显示，Chat3GPP 性能优于现有方法，并展示出在协议生成和代码自动化等下游任务中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13954v1",
      "published_date": "2025-01-20 11:38:42 UTC",
      "updated_date": "2025-01-20 11:38:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:20:54.265682"
    },
    {
      "arxiv_id": "2501.11417v1",
      "title": "Neural Contextual Reinforcement Framework for Logical Structure Language Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Marcus Irvin",
        "William Cooper",
        "Edward Hughes",
        "Jessica Morgan",
        "Christopher Hamilton"
      ],
      "abstract": "The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment.",
      "tldr_zh": "这篇论文提出了Neural Contextual Reinforcement Framework，一种创新方法，用于提升大型语言模型生成文本的逻辑连贯性和结构一致性。该框架通过强化学习(reinforcement learning)原则、自定义奖励函数和动态上下文对齐机制，结合multi-head attention layers和hierarchical encoding modules，来处理长距离依赖性问题，确保生成的文本更符合人类的语义流和逻辑结构。实验评估显示，该框架在多种数据集上显著改善了连贯性指标、减少了perplexity，并优于基线模型，尤其在鲁棒性、跨语言适应性和资源效率方面表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11417v1",
      "published_date": "2025-01-20 11:34:28 UTC",
      "updated_date": "2025-01-20 11:34:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:21:07.252875"
    },
    {
      "arxiv_id": "2501.11413v1",
      "title": "Generalization and Informativeness of Weighted Conformal Risk Control Under Covariate Shift",
      "title_zh": "加权保形风险控制在协变量偏移下的泛化与信息性",
      "authors": [
        "Matteo Zecchin",
        "Fredrik Hellström",
        "Sangwoo Park",
        "Shlomo Shamai",
        "Osvaldo Simeone"
      ],
      "abstract": "Predictive models are often required to produce reliable predictions under\nstatistical conditions that are not matched to the training data. A common type\nof training-testing mismatch is covariate shift, where the conditional\ndistribution of the target variable given the input features remains fixed,\nwhile the marginal distribution of the inputs changes. Weighted conformal risk\ncontrol (W-CRC) uses data collected during the training phase to convert point\npredictions into prediction sets with valid risk guarantees at test time\ndespite the presence of a covariate shift. However, while W-CRC provides\nstatistical reliability, its efficiency -- measured by the size of the\nprediction sets -- can only be assessed at test time. In this work, we relate\nthe generalization properties of the base predictor to the efficiency of W-CRC\nunder covariate shifts. Specifically, we derive a bound on the inefficiency of\nthe W-CRC predictor that depends on algorithmic hyperparameters and\ntask-specific quantities available at training time. This bound offers insights\non relationships between the informativeness of the prediction sets, the extent\nof the covariate shift, and the size of the calibration and training sets.\nExperiments on fingerprinting-based localization validate the theoretical\nresults.",
      "tldr_zh": "该论文探讨了在协变量偏移(covariate shift)条件下，Weighted Conformal Risk Control (W-CRC) 的泛化性能和预测集信息性问题。研究者将基础预测器的泛化属性与W-CRC的效率联系起来，推导出一个依赖算法超参数和任务特定量的低效性界限，该界限揭示了预测集信息性与协变量偏移程度、校准集及训练集大小之间的关系。实验在基于指纹定位的任务上验证了这些理论结果，为提升预测模型在数据分布偏移下的可靠性和效率提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11413v1",
      "published_date": "2025-01-20 11:26:36 UTC",
      "updated_date": "2025-01-20 11:26:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:23:10.690684"
    },
    {
      "arxiv_id": "2501.11409v3",
      "title": "Unsupervised Learning in Echo State Networks for Input Reconstruction",
      "title_zh": "Echo State Networks 中的无监督学习用于输入重建",
      "authors": [
        "Taiki Yamada",
        "Yuichi Katori",
        "Kantaro Fujiwara"
      ],
      "abstract": "Conventional echo state networks (ESNs) require supervised learning to train\nthe readout layer, using the desired outputs as training data. In this study,\nwe focus on input reconstruction (IR), which refers to training the readout\nlayer to reproduce the input time series in its output. We reformulate the\nlearning algorithm of the ESN readout layer to perform IR using unsupervised\nlearning (UL). By conducting theoretical analysis and numerical experiments, we\ndemonstrate that IR in ESNs can be effectively implemented under realistic\nconditions without explicitly using the desired outputs as training data; in\nthis way, UL is enabled. Furthermore, we demonstrate that applications relying\non IR, such as dynamical system replication and noise filtering, can be\nreformulated within the UL framework. Our findings establish a theoretically\nsound and universally applicable IR formulation, along with its related tasks\nin ESNs. This work paves the way for novel predictions and highlights\nunresolved theoretical challenges in ESNs, particularly in the context of\ntime-series processing methods and computational models of the brain.",
      "tldr_zh": "本文提出了一种在 Echo State Networks (ESNs) 中使用无监督学习 (Unsupervised Learning, UL) 进行输入重建 (Input Reconstruction, IR) 的方法，通过改进了 ESN 输出层的学习算法，避免了依赖期望输出数据作为训练。研究通过理论分析和数值实验证明，这种方法在实际条件下有效实现 IR，并将相关应用如动态系统复制和噪声过滤重新表述为 UL 框架。总体上，该工作为 ESNs 在时间序列处理和大脑计算模型中的应用提供了理论基础，并指出了潜在的未解决挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "nlin.CD",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 7 figures, regular paper",
      "pdf_url": "http://arxiv.org/pdf/2501.11409v3",
      "published_date": "2025-01-20 11:16:44 UTC",
      "updated_date": "2025-02-10 10:09:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:21:30.329901"
    },
    {
      "arxiv_id": "2501.11407v1",
      "title": "A Truly Sparse and General Implementation of Gradient-Based Synaptic Plasticity",
      "title_zh": "翻译失败",
      "authors": [
        "Jamie Lohoff",
        "Anil Kaya",
        "Florian Assmuth",
        "Emre Neftci"
      ],
      "abstract": "Online synaptic plasticity rules derived from gradient descent achieve high\naccuracy on a wide range of practical tasks. However, their software\nimplementation often requires tediously hand-derived gradients or using\ngradient backpropagation which sacrifices the online capability of the rules.\nIn this work, we present a custom automatic differentiation (AD) pipeline for\nsparse and online implementation of gradient-based synaptic plasticity rules\nthat generalizes to arbitrary neuron models. Our work combines the programming\nease of backpropagation-type methods for forward AD while being\nmemory-efficient. To achieve this, we exploit the advantageous compute and\nmemory scaling of online synaptic plasticity by providing an inherently sparse\nimplementation of AD where expensive tensor contractions are replaced with\nsimple element-wise multiplications if the tensors are diagonal. Gradient-based\nsynaptic plasticity rules such as eligibility propagation (e-prop) have exactly\nthis property and thus profit immensely from this feature. We demonstrate the\nalignment of our gradients with respect to gradient backpropagation on an\nsynthetic task where e-prop gradients are exact, as well as audio speech\nclassification benchmarks. We demonstrate how memory utilization scales with\nnetwork size without dependence on the sequence length, as expected from\nforward AD methods.",
      "tldr_zh": "本研究提出了一种真正的稀疏和通用的梯度-based 突触可塑性实现，通过自定义的自动微分 (AD) 管道，支持在线处理并适用于任意神经元模型。该管道结合了前向 AD 的编程便利性和内存效率，利用稀疏特性将昂贵的张量收缩替换为简单的元素-wise 乘法，尤其适用于 eligibility propagation (e-prop) 等规则。实验在合成任务和音频语音分类基准上验证了该实现的梯度准确性，与梯度反向传播方法相比，它实现了更好的内存利用率，且内存消耗随网络大小变化而不依赖序列长度。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11407v1",
      "published_date": "2025-01-20 11:14:11 UTC",
      "updated_date": "2025-01-20 11:14:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:21:42.340977"
    },
    {
      "arxiv_id": "2501.16356v1",
      "title": "Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations",
      "title_zh": "评估大语言模型中的二元决策偏差：对公平基于代理的金融模拟的含义",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used to simulate\nhuman-like decision making in agent-based financial market models (ABMs). As\nmodels become more powerful and accessible, researchers can now incorporate\nindividual LLM decisions into ABM environments. However, integration may\nintroduce inherent biases that need careful evaluation. In this paper we test\nthree state-of-the-art GPT models for bias using two model sampling approaches:\none-shot and few-shot API queries. We observe significant variations in\ndistributions of outputs between specific models, and model sub versions, with\nGPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes\nresponses) compared to GPT-4-0125-preview's extreme bias (98-99% yes\nresponses). We show that sampling methods and model sub-versions significantly\nimpact results: repeated independent API calls produce different distributions\ncompared to batch sampling within a single call. While no current GPT model can\nsimultaneously achieve a uniform distribution and Markovian properties in\none-shot testing, few-shot sampling can approach uniform distributions under\ncertain conditions. We explore the Temperature parameter, providing a\ndefinition and comparative results. We further compare our results to true\nrandom binary series and test specifically for the common human bias of\nNegative Recency - finding LLMs have a mixed ability to 'beat' humans in this\none regard. These findings emphasise the critical importance of careful LLM\nintegration into ABMs for financial markets and more broadly.",
      "tldr_zh": "这篇论文评估了Large Language Models (LLMs) 在代理-based金融市场模型 (ABMs) 中的二进制决策偏差，并探讨了这些偏差对公平模拟的影响。研究通过one-shot和few-shot API查询方法测试了三个GPT模型，发现不同模型版本（如GPT-4o-Mini和GPT-4-0125-preview）的输出分布存在显著差异，采样方法和Temperature参数也会影响结果。关键发现是，few-shot采样可在某些条件下接近均匀分布，且LLMs在Negative Recency偏差上可能优于人类，但当前模型无法同时实现均匀分布和Markovian属性。该研究强调了在ABMs中谨慎整合LLMs的重要性，以避免引入偏差并确保模拟的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.16356v1",
      "published_date": "2025-01-20 10:36:51 UTC",
      "updated_date": "2025-01-20 10:36:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:21:55.251562"
    },
    {
      "arxiv_id": "2501.11378v1",
      "title": "Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio",
      "title_zh": "翻译失败",
      "authors": [
        "Mateusz Barański",
        "Jan Jasiński",
        "Julitta Bartolewska",
        "Stanisław Kacprzak",
        "Marcin Witkowski",
        "Konrad Kowalczyk"
      ],
      "abstract": "Hallucinations of deep neural models are amongst key challenges in automatic\nspeech recognition (ASR). In this paper, we investigate hallucinations of the\nWhisper ASR model induced by non-speech audio segments present during\ninference. By inducting hallucinations with various types of sounds, we show\nthat there exists a set of hallucinations that appear frequently. We then study\nhallucinations caused by the augmentation of speech with such sounds. Finally,\nwe describe the creation of a bag of hallucinations (BoH) that allows to remove\nthe effect of hallucinations through the post-processing of text\ntranscriptions. The results of our experiments show that such post-processing\nis capable of reducing word error rate (WER) and acts as a good safeguard\nagainst problematic hallucinations.",
      "tldr_zh": "本文研究了 Whisper ASR 模型在非语音音频影响下产生的 hallucinations（幻觉）问题，通过引入各种类型的声音来诱发和分析这些幻觉，发现某些 hallucinations 频繁出现。作者进一步探讨了将这些声音添加到语音中的情况，并开发了 bag of hallucinations (BoH) 方法，通过文本转录的后处理来移除幻觉的影响。实验结果表明，这种后处理技术能显著降低 word error rate (WER)，并作为防范问题 hallucinations 的有效防护措施。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted for IEEE ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11378v1",
      "published_date": "2025-01-20 10:14:52 UTC",
      "updated_date": "2025-01-20 10:14:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:22:06.419272"
    },
    {
      "arxiv_id": "2501.11360v1",
      "title": "Federated Learning with Sample-level Client Drift Mitigation",
      "title_zh": "样本级客户端漂移缓解的联邦学习",
      "authors": [
        "Haoran Xu",
        "Jiaze Li",
        "Wanyi Wu",
        "Hao Ren"
      ],
      "abstract": "Federated Learning (FL) suffers from severe performance degradation due to\nthe data heterogeneity among clients. Existing works reveal that the\nfundamental reason is that data heterogeneity can cause client drift where the\nlocal model update deviates from the global one, and thus they usually tackle\nthis problem from the perspective of calibrating the obtained local update.\nDespite effectiveness, existing methods substantially lack a deep understanding\nof how heterogeneous data samples contribute to the formation of client drift.\nIn this paper, we bridge this gap by identifying that the drift can be viewed\nas a cumulative manifestation of biases present in all local samples and the\nbias between samples is different. Besides, the bias dynamically changes as the\nFL training progresses. Motivated by this, we propose FedBSS that first\nmitigates the heterogeneity issue in a sample-level manner, orthogonal to\nexisting methods. Specifically, the core idea of our method is to adopt a\nbias-aware sample selection scheme that dynamically selects the samples from\nsmall biases to large epoch by epoch to train progressively the local model in\neach round. In order to ensure the stability of training, we set the\ndiversified knowledge acquisition stage as the warm-up stage to avoid the local\noptimality caused by knowledge deviation in the early stage of the model.\nEvaluation results show that FedBSS outperforms state-of-the-art baselines. In\naddition, we also achieved effective results on feature distribution skew and\nnoise label dataset setting, which proves that FedBSS can not only reduce\nheterogeneity, but also has scalability and robustness.",
      "tldr_zh": "联邦学习(Federated Learning) 由于客户端数据异质性导致客户端漂移(client drift)，从而影响性能，本文通过分析漂移作为样本偏差的累积表现，并提出 FedBSS 方法在样本级别缓解这一问题。FedBSS 的核心是采用偏见感知样本选择方案，按轮动态选择偏差小的样本到偏差大的样本进行渐进训练，并设置多样化知识获取阶段作为热身以避免早期局部最优。实验结果表明，FedBSS 优于现有基线，并在特征分布偏移和噪声标签数据集上表现出色，证明其可扩展性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11360v1",
      "published_date": "2025-01-20 09:44:07 UTC",
      "updated_date": "2025-01-20 09:44:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:22:18.519104"
    },
    {
      "arxiv_id": "2501.11357v2",
      "title": "On the dimension of pullback attractors in recurrent neural networks",
      "title_zh": "关于",
      "authors": [
        "Muhammed Fadera"
      ],
      "abstract": "Recurrent Neural Networks (RNNs) are high-dimensional state space models\ncapable of learning functions on sequence data. Recently, it has been\nconjectured that reservoir computers, a particular class of RNNs, trained on\nobservations of a dynamical systems can be interpreted as embeddings. This\nresult has been established for the case of linear reservoir systems. In this\nwork, we use a nonautonomous dynamical systems approach to establish an upper\nbound for the fractal dimension of the subset of reservoir state space\napproximated during training and prediction phase. We prove that when the input\nsequences comes from an Nin-dimensional invertible dynamical system, the\nfractal dimension of this set is bounded above by Nin. The result obtained here\nare useful in dimensionality reduction of computation in RNNs as well as\nestimating fractal dimensions of dynamical systems from limited observations of\ntheir time series. It is also a step towards understanding embedding properties\nof reservoir computers.",
      "tldr_zh": "本文研究了Recurrent Neural Networks (RNNs)中pullback attractors的维度问题，聚焦于reservoir computers作为动态系统嵌入的特性。作者采用非自治动态系统方法，证明了当输入序列来自N_in维可逆动态系统时，reservoir状态空间中训练和预测子集的fractal dimension的上界为N_in。这一结果有助于RNN的维度减少、从有限时间序列估计动态系统的fractal dimension，并深化了对reservoir computers嵌入属性的理解。",
      "categories": [
        "math.DS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11357v2",
      "published_date": "2025-01-20 09:38:30 UTC",
      "updated_date": "2025-03-29 15:24:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:22:30.596881"
    },
    {
      "arxiv_id": "2501.11354v1",
      "title": "Towards Advancing Code Generation with Large Language Models: A Research Roadmap",
      "title_zh": "翻译失败",
      "authors": [
        "Haolin Jin",
        "Huaming Chen",
        "Qinghua Lu",
        "Liming Zhu"
      ],
      "abstract": "Recently, we have witnessed the rapid development of large language models,\nwhich have demonstrated excellent capabilities in the downstream task of code\ngeneration. However, despite their potential, LLM-based code generation still\nfaces numerous technical and evaluation challenges, particularly when embedded\nin real-world development. In this paper, we present our vision for current\nresearch directions, and provide an in-depth analysis of existing studies on\nthis task. We propose a six-layer vision framework that categorizes code\ngeneration process into distinct phases, namely Input Phase, Orchestration\nPhase, Development Phase, and Validation Phase. Additionally, we outline our\nvision workflow, which reflects on the currently prevalent frameworks. We\nsystematically analyse the challenges faced by large language models, including\nthose LLM-based agent frameworks, in code generation tasks. With these, we\noffer various perspectives and actionable recommendations in this area. Our aim\nis to provide guidelines for improving the reliability, robustness and\nusability of LLM-based code generation systems. Ultimately, this work seeks to\naddress persistent challenges and to provide practical suggestions for a more\npragmatic LLM-based solution for future code generation endeavors.",
      "tldr_zh": "这篇论文概述了使用 Large Language Models (LLMs) 推进代码生成任务的研究路线图，强调了 LLMs 在实际开发中的技术挑战和评估问题。作者提出一个 six-layer vision framework，将代码生成过程分为 Input Phase、Orchestration Phase、Development Phase 和 Validation Phase，并分析了现有研究中 LLMs 和基于 LLMs 的代理框架面临的挑战。论文提供多种视角和可行动建议，以提升代码生成系统的可靠性、稳健性和可用性，最终旨在为未来的 LLM-based 代码生成提供实用指导。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11354v1",
      "published_date": "2025-01-20 09:33:44 UTC",
      "updated_date": "2025-01-20 09:33:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:22:43.986983"
    },
    {
      "arxiv_id": "2501.11335v1",
      "title": "Few-shot Policy (de)composition in Conversational Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Kyle Erwin",
        "Guy Axelrod",
        "Maria Chang",
        "Achille Fokoue",
        "Maxwell Crouse",
        "Soham Dan",
        "Tian Gao",
        "Rosario Uceda-Sosa",
        "Ndivhuwo Makondo",
        "Naweed Khan",
        "Alexander Gray"
      ],
      "abstract": "The task of policy compliance detection (PCD) is to determine if a scenario\nis in compliance with respect to a set of written policies. In a conversational\nsetting, the results of PCD can indicate if clarifying questions must be asked\nto determine compliance status. Existing approaches usually claim to have\nreasoning capabilities that are latent or require a large amount of annotated\ndata. In this work, we propose logical decomposition for policy compliance\n(LDPC): a neuro-symbolic framework to detect policy compliance using large\nlanguage models (LLMs) in a few-shot setting. By selecting only a few exemplars\nalongside recently developed prompting techniques, we demonstrate that our\napproach soundly reasons about policy compliance conversations by extracting\nsub-questions to be answered, assigning truth values from contextual\ninformation, and explicitly producing a set of logic statements from the given\npolicies. The formulation of explicit logic graphs can in turn help answer\nPCDrelated questions with increased transparency and explainability. We apply\nthis approach to the popular PCD and conversational machine reading benchmark,\nShARC, and show competitive performance with no task-specific finetuning. We\nalso leverage the inherently interpretable architecture of LDPC to understand\nwhere errors occur, revealing ambiguities in the ShARC dataset and highlighting\nthe challenges involved with reasoning for conversational question answering.",
      "tldr_zh": "本文提出 Logical Decomposition for Policy Compliance (LDPC)，一个神经符号框架，利用 Large Language Models (LLMs) 在少样本设置下进行政策合规检测 (PCD)，通过提取子问题、分配真值和生成显式逻辑语句，提升对话问答的透明性和可解释性。该方法仅需少量示例和高级提示技术，即可在 ShARC 数据集上实现与基线相当的性能，而无需任务特定微调。LDPC 的可解释架构还帮助分析错误，揭示了 ShARC 中的模糊性和对话式问答推理的潜在挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11335v1",
      "published_date": "2025-01-20 08:40:15 UTC",
      "updated_date": "2025-01-20 08:40:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:25:15.117247"
    },
    {
      "arxiv_id": "2501.13953v1",
      "title": "Redundancy Principles for MLLMs Benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Zicheng Zhang",
        "Xiangyu Zhao",
        "Xinyu Fang",
        "Chunyi Li",
        "Xiaohong Liu",
        "Xiongkuo Min",
        "Haodong Duan",
        "Kai Chen",
        "Guangtao Zhai"
      ],
      "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively.",
      "tldr_zh": "本文分析了多模态大语言模型(MLLMs)基准测试的冗余问题，随着模型迭代加速，每年基准数量激增，导致能力维度冗余、测试问题冗余以及特定领域跨基准冗余等关键问题。通过对数百个 MLLMs 在 20 多个基准上的表现进行全面量化评估，论文揭示了现有评估中的冗余水平，并提出针对性原则和策略，以指导未来基准的优化和发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13953v1",
      "published_date": "2025-01-20 08:09:42 UTC",
      "updated_date": "2025-01-20 08:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:23:33.224880"
    },
    {
      "arxiv_id": "2501.11325v1",
      "title": "CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Chong",
        "Wenqing Zhang",
        "Shiyue Zhang",
        "Jun Zheng",
        "Xiao Dong",
        "Haoxiang Li",
        "Yiling Wu",
        "Dongmei Jiang",
        "Xiaodan Liang"
      ],
      "abstract": "Virtual try-on (VTON) technology has gained attention due to its potential to\ntransform online retail by enabling realistic clothing visualization of images\nand videos. However, most existing methods struggle to achieve high-quality\nresults across image and video try-on tasks, especially in long video\nscenarios. In this work, we introduce CatV2TON, a simple and effective\nvision-based virtual try-on (V2TON) method that supports both image and video\ntry-on tasks with a single diffusion transformer model. By temporally\nconcatenating garment and person inputs and training on a mix of image and\nvideo datasets, CatV2TON achieves robust try-on performance across static and\ndynamic settings. For efficient long-video generation, we propose an\noverlapping clip-based inference strategy that uses sequential frame guidance\nand Adaptive Clip Normalization (AdaCN) to maintain temporal consistency with\nreduced resource demands. We also present ViViD-S, a refined video try-on\ndataset, achieved by filtering back-facing frames and applying 3D mask\nsmoothing for enhanced temporal consistency. Comprehensive experiments\ndemonstrate that CatV2TON outperforms existing methods in both image and video\ntry-on tasks, offering a versatile and reliable solution for realistic virtual\ntry-ons across diverse scenarios.",
      "tldr_zh": "本研究提出了CatV2TON，一种基于diffusion transformer的视觉虚拟试穿(V2TON)方法，通过temporal concatenation将服装和人物输入时间上连接，并在图像和视频数据集上混合训练，实现图像和视频试穿任务的鲁棒性能。针对长视频生成，该方法引入overlapping clip-based inference策略，结合sequential frame guidance和Adaptive Clip Normalization (AdaCN)，以减少资源需求并维持temporal consistency。同时，作者构建了ViViD-S数据集，通过过滤back-facing frames和应用3D mask smoothing提升temporal consistency。实验结果显示，CatV2TON在图像和视频试穿任务中优于现有方法，提供了一个多场景的可靠解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T42 (Primary) 168T45 (Secondary)",
        "I.4.9"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 8 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.11325v1",
      "published_date": "2025-01-20 08:09:36 UTC",
      "updated_date": "2025-01-20 08:09:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:23:45.365946"
    },
    {
      "arxiv_id": "2501.11309v2",
      "title": "Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziheng Zhang",
        "Jianyang Gu",
        "Arpita Chowdhury",
        "Zheda Mai",
        "David Carlyn",
        "Tanya Berger-Wolf",
        "Yu Su",
        "Wei-Lun Chao"
      ],
      "abstract": "Class activation map (CAM) has been widely used to highlight image regions\nthat contribute to class predictions. Despite its simplicity and computational\nefficiency, CAM often struggles to identify discriminative regions that\ndistinguish visually similar fine-grained classes. Prior efforts address this\nlimitation by introducing more sophisticated explanation processes, but at the\ncost of extra complexity. In this paper, we propose Finer-CAM, a method that\nretains CAM's efficiency while achieving precise localization of discriminative\nregions. Our key insight is that the deficiency of CAM lies not in \"how\" it\nexplains, but in \"what\" it explains. Specifically, previous methods attempt to\nidentify all cues contributing to the target class's logit value, which\ninadvertently also activates regions predictive of visually similar classes. By\nexplicitly comparing the target class with similar classes and spotting their\ndifferences, Finer-CAM suppresses features shared with other classes and\nemphasizes the unique, discriminative details of the target class. Finer-CAM is\neasy to implement, compatible with various CAM methods, and can be extended to\nmulti-modal models for accurate localization of specific concepts.\nAdditionally, Finer-CAM allows adjustable comparison strength, enabling users\nto selectively highlight coarse object contours or fine discriminative details.\nQuantitatively, we show that masking out the top 5% of activated pixels by\nFiner-CAM results in a larger relative confidence drop compared to baselines.\nThe source code and demo are available at\nhttps://github.com/Imageomics/Finer-CAM.",
      "tldr_zh": "本文提出 Finer-CAM，一种高效的视觉解释方法，通过比较目标类与视觉相似类别的差异，抑制共享特征并强调独特细节，从而改善 Class Activation Map (CAM) 在细粒度分类中的精确定位能力。Finer-CAM 保留了 CAM 的计算效率，易于实现、兼容多种 CAM 方法，并可扩展到多模态模型，同时允许调整比较强度以突出粗略轮廓或细致细节。实验结果显示，屏蔽 Finer-CAM 激活的顶层 5% 像素会导致比基线更大的置信度下降，证明其在识别区分性区域方面的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11309v2",
      "published_date": "2025-01-20 07:23:11 UTC",
      "updated_date": "2025-03-31 15:30:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:23:59.619581"
    },
    {
      "arxiv_id": "2501.11306v1",
      "title": "Collaborative Imputation of Urban Time Series through Cross-city Meta-learning",
      "title_zh": "通过跨城市元学习对城市时间序列进行协作插值",
      "authors": [
        "Tong Nie",
        "Wei Ma",
        "Jian Sun",
        "Yu Yang",
        "Jiannong Cao"
      ],
      "abstract": "Urban time series, such as mobility flows, energy consumption, and pollution\nrecords, encapsulate complex urban dynamics and structures. However, data\ncollection in each city is impeded by technical challenges such as budget\nlimitations and sensor failures, necessitating effective data imputation\ntechniques that can enhance data quality and reliability. Existing imputation\nmodels, categorized into learning-based and analytics-based paradigms, grapple\nwith the trade-off between capacity and generalizability. Collaborative\nlearning to reconstruct data across multiple cities holds the promise of\nbreaking this trade-off. Nevertheless, urban data's inherent irregularity and\nheterogeneity issues exacerbate challenges of knowledge sharing and\ncollaboration across cities. To address these limitations, we propose a novel\ncollaborative imputation paradigm leveraging meta-learned implicit neural\nrepresentations (INRs). INRs offer a continuous mapping from domain coordinates\nto target values, integrating the strengths of both paradigms. By imposing\nembedding theory, we first employ continuous parameterization to handle\nirregularity and reconstruct the dynamical system. We then introduce a\ncross-city collaborative learning scheme through model-agnostic meta learning,\nincorporating hierarchical modulation and normalization techniques to\naccommodate multiscale representations and reduce variance in response to\nheterogeneity. Extensive experiments on a diverse urban dataset from 20 global\ncities demonstrate our model's superior imputation performance and\ngeneralizability, underscoring the effectiveness of collaborative imputation in\nresource-constrained settings.",
      "tldr_zh": "该研究针对城市时间序列数据（如流动、能源消耗和污染记录）的插补问题，提出了一种基于跨城市 meta-learning 的协作范式，以应对数据不规则性和异质性带来的挑战。方法利用 meta-learned implicit neural representations (INRs) 提供从域坐标到目标值的连续映射，并通过嵌入理论实现动态系统重建，同时引入 model-agnostic meta learning 结合 hierarchical modulation 和 normalization 技术，促进多城市知识共享并减少异质性影响。实验在20个全球城市的多样数据集上验证了该模型的优越插补性能和泛化能力，证明了协作学习在资源受限环境中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11306v1",
      "published_date": "2025-01-20 07:12:40 UTC",
      "updated_date": "2025-01-20 07:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:24:10.004251"
    },
    {
      "arxiv_id": "2501.11301v3",
      "title": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access: An Approach for Wikipedia and Wikidata Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Santhosh Thottingal"
      ],
      "abstract": "This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering.",
      "tldr_zh": "该论文提出了一种基于“question-to-question retrieval”的方法，用于Wikipedia和Wikidata的问答系统，以实现无幻觉（hallucination-free）的知识访问。该方法使用instruction-tuned LLM为每个逻辑内容单位生成一组全面的问题，并将这些问题向量化嵌入并存储，与对应内容关联。用户查询通过向量匹配找到最高余弦相似度（>0.9）的相关问题，从而直接检索文章内容，而非生成答案。该方法显著提高了计算效率、响应速度和可扩展性，并在Wikipedia和Wikidata上证明有效，支持多模态问答如结构化事实检索。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11301v3",
      "published_date": "2025-01-20 07:05:15 UTC",
      "updated_date": "2025-02-21 04:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:24:22.133134"
    },
    {
      "arxiv_id": "2501.13952v2",
      "title": "The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a Degraded Utility?",
      "title_zh": "LLMs 中的双重用途困境：增强道德能力是否会导致效用下降？",
      "authors": [
        "Yiyi Zhang",
        "Xingyu Chen",
        "Kexin Chen",
        "Yuyang Du",
        "Xilin Dang",
        "Pheng-Ann Heng"
      ],
      "abstract": "Recent years have witnessed extensive efforts to enhance Large Language\nModels (LLMs) across various domains, alongside growing attention to their\nethical implications. However, a critical challenge remains largely overlooked:\nLLMs must balance between rejecting harmful requests for safety and\naccommodating legitimate ones for utility. This paper presents a Direct\nPreference Optimization (DPO) based alignment framework that achieves better\noverall performance by addressing this ethical-utility trade-off, using\nchemical domain applications as a proof-of-concept. Our alignment pipeline\nstarts with a GPT-assisted three-phase data generation scheme, in which we\ncreate LibraChemQA, a chemical question-answering dataset comprising 31.6k\ntriplet instances. By incorporating an innovative balanced seed in the data\ngeneration process, our framework systematically considers both legitimate and\nillegitimate requests. The framework also introduces a rephrasing mechanism for\nefficient data augmentation that enhances the model's chemical comprehension.\nWe further develop a novel hybrid evaluation scheme with LLM judges for precise\nassessment of both safety and utility. Experimental results demonstrate our\nmodel's substantial improvements in overall performance where both safety and\nutility are considered - the resulting model outperforms leading LLMs including\nClaude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10%\nrespectively on our released benchmark. At the end of this paper, we analyze\nexperimental results obtained from testing DeepSeek-R1 on our benchmark and\nreveal the critical ethical concerns raised by this highly acclaimed model. We\nhighlight that the long Chain-of-Thought (CoT) reasoning process employed by\nDeepSeek-R1, as well as other LLMs distilled from it, introduces significant\nethical vulnerabilities when exposed to users.",
      "tldr_zh": "本文探讨了大型语言模型（LLMs）在拒绝有害请求（安全）和处理合法请求（实用性）之间的道德-实用性权衡，提出了一种基于Direct Preference Optimization (DPO)的对齐框架，以化学领域为例进行验证。该框架通过GPT辅助的三阶段数据生成方案创建了LibraChemQA数据集（包含31.6k三元组实例），并引入平衡种子和重述机制来增强模型的化学理解和数据多样性。实验采用混合评估方案，结果显示该模型在基准测试中分别优于Claude-3、GPT-4o和LLaMA-3，提高了13.44%、7.16%和7.10%的整体性能。此外，分析DeepSeek-R1的Chain-of-Thought (CoT)推理过程，揭示了其潜在的道德漏洞。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13952v2",
      "published_date": "2025-01-20 06:35:01 UTC",
      "updated_date": "2025-02-27 07:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:26:35.367470"
    },
    {
      "arxiv_id": "2501.11293v1",
      "title": "A Machine Learning Framework for Handling Unreliable Absence Label and Class Imbalance for Marine Stinger Beaching Prediction",
      "title_zh": "一种处理不可靠缺失标签和类别不平衡的机器学习框架，用于海洋刺痛生物上岸预测",
      "authors": [
        "Amuche Ibenegbu",
        "Amandine Schaeffer",
        "Pierre Lafaye de Micheaux",
        "Rohitash Chandra"
      ],
      "abstract": "Bluebottles (\\textit{Physalia} spp.) are marine stingers resembling\njellyfish, whose presence on Australian beaches poses a significant public risk\ndue to their venomous nature. Understanding the environmental factors driving\nbluebottles ashore is crucial for mitigating their impact, and machine learning\ntools are to date relatively unexplored. We use bluebottle marine stinger\npresence/absence data from beaches in Eastern Sydney, Australia, and compare\nmachine learning models (Multilayer Perceptron, Random Forest, and XGBoost) to\nidentify factors influencing their presence. We address challenges such as\nclass imbalance, class overlap, and unreliable absence data by employing data\naugmentation techniques, including the Synthetic Minority Oversampling\nTechnique (SMOTE), Random Undersampling, and Synthetic Negative Approach that\nexcludes the negative class. Our results show that SMOTE failed to resolve\nclass overlap, but the presence-focused approach effectively handled imbalance,\nclass overlap, and ambiguous absence data. The data attributes such as the wind\ndirection, which is a circular variable, emerged as a key factor influencing\nbluebottle presence, confirming previous inference studies. However, in the\nabsence of population dynamics, biological behaviours, and life cycles, the\nbest predictive model appears to be Random Forests combined with Synthetic\nNegative Approach. This research contributes to mitigating the risks posed by\nbluebottles to beachgoers and provides insights into handling class overlap and\nunreliable negative class in environmental modelling.",
      "tldr_zh": "本文提出一个机器学习框架，用于预测海洋刺痛生物（如蓝瓶）上岸的风险，处理类别不平衡、类别重叠和不可靠的缺失标签问题。研究比较了Multilayer Perceptron、Random Forest和XGBoost模型，并应用SMOTE、Random Undersampling和Synthetic Negative Approach等数据增强技术，结果显示风向是关键影响因素，而Random Forests结合Synthetic Negative Approach表现出最佳性能。该框架为缓解蓝瓶对海滩游客的威胁提供实用见解，并为环境建模中处理类似数据挑战贡献方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11293v1",
      "published_date": "2025-01-20 06:28:27 UTC",
      "updated_date": "2025-01-20 06:28:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:24:46.473676"
    },
    {
      "arxiv_id": "2501.11284v1",
      "title": "RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?",
      "title_zh": "翻译失败",
      "authors": [
        "Haotian Xu",
        "Xing Wu",
        "Weinong Wang",
        "Zhongzhi Li",
        "Da Zheng",
        "Boyuan Chen",
        "Yi Hu",
        "Shijia Kang",
        "Jiaming Ji",
        "Yingying Zhang",
        "Zhijiang Guo",
        "Yaodong Yang",
        "Muhan Zhang",
        "Debing Zhang"
      ],
      "abstract": "Can scaling transform reasoning? In this work, we explore the untapped\npotential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples,\npioneering the development of a slow-thinking model, RedStar. Through extensive\nexperiments with various LLMs and different sizes, we uncover the ingredients\nfor specialization and scale for Long-CoT training. Surprisingly, even smaller\nmodels show significant performance gains with limited data, revealing the\nsample efficiency of Long-CoT and the critical role of sample difficulty in the\nlearning process. Our findings demonstrate that Long-CoT reasoning can be\neffectively triggered with just a few thousand examples, while larger models\nachieve unparalleled improvements. We also introduce reinforcement learning\n(RL)-scale training as a promising direction for advancing slow-thinking\nsystems. RedStar shines across domains: on the MATH-Hard benchmark,\nRedStar-code-math boosts performance from 66.2\\% to 81.6\\%, and on the USA Math\nOlympiad (AIME), it solves 46.7\\% of problems using only 21k mixed-code-math\ndatasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo\nachieves competitive results with minimal Long-CoT data, outperforming other\nslow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the\nperfect balance between reasoning and generalizability. Our work highlights\nthat, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning\ncapabilities-even with limited dataset and set a new standard for slow-thinking\nmodels across diverse challenges. Our data and models are released at\nhttps://huggingface.co/RedStar-Reasoning.",
      "tldr_zh": "本研究探讨通过扩展 Long-CoT 数据规模至 1000k 样本，是否能提升慢速推理系统的发展，并推出了 RedStar 模型。该模型通过实验验证了 Long-CoT 训练的样本效率，即使较小 LLMs 在有限数据下也能显著提升性能，而更大模型则实现更大进步。RedStar 在 MATH-Hard 基准上将性能从 66.2% 提升至 81.6%，在 AIME 上解决 46.7% 的问题，并在 GeoQA 和 MathVista-GEO 等多模态任务中，使用最小 Long-CoT 数据就超越其他系统，如 QvQ-Preview。该工作还引入 RL-scale 训练作为新方向，并证明了通过仔细调整，Long-CoT 扩展可解锁非凡推理能力，平衡了推理与泛化性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "technique-report, https://huggingface.co/RedStar-Reasoning",
      "pdf_url": "http://arxiv.org/pdf/2501.11284v1",
      "published_date": "2025-01-20 05:44:01 UTC",
      "updated_date": "2025-01-20 05:44:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:24:59.078776"
    },
    {
      "arxiv_id": "2501.17879v1",
      "title": "Task and Perception-aware Distributed Source Coding for Correlated Speech under Bandwidth-constrained Channels",
      "title_zh": "任务和感知感知的分布式源编码，用于带宽受",
      "authors": [
        "Sagnik Bhattacharya",
        "Muhammad Ahmed Mohsin",
        "Ahsan Bilal",
        "John M. Cioffi"
      ],
      "abstract": "Emerging wireless AR/VR applications require real-time transmission of\ncorrelated high-fidelity speech from multiple resource-constrained devices over\nunreliable, bandwidth-limited channels. Existing autoencoder-based speech\nsource coding methods fail to address the combination of the following - (1)\ndynamic bitrate adaptation without retraining the model, (2) leveraging\ncorrelations among multiple speech sources, and (3) balancing downstream task\nloss with realism of reconstructed speech. We propose a neural distributed\nprincipal component analysis (NDPCA)-aided distributed source coding algorithm\nfor correlated speech sources transmitting to a central receiver. Our method\nincludes a perception-aware downstream task loss function that balances\nperceptual realism with task-specific performance. Experiments show significant\nPSNR improvements under bandwidth constraints over naive autoencoder methods in\ntask-agnostic (19%) and task-aware settings (52%). It also approaches the\ntheoretical upper bound, where all correlated sources are sent to a single\nencoder, especially in low-bandwidth scenarios. Additionally, we present a\nrate-distortion-perception trade-off curve, enabling adaptive decisions based\non application-specific realism needs.",
      "tldr_zh": "本研究针对带宽受限通道下无线 AR/VR 应用的多设备相关语音传输问题，提出了一种基于神经分布式主成分分析 (NDPCA) 的分布式源编码算法。该算法结合感知感知的下游任务损失函数，能动态适应比特率、利用语音源相关性，并平衡任务性能与重建语音的真实性。实验结果显示，与传统自编码器方法相比，PSNR 在任务无关设置下改善 19%、在任务感知设置下改善 52%，并接近理论上限，尤其在低带宽场景；此外，该方法还提供速率-失真-感知权衡曲线，支持应用特定的适应性决策。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.SD",
        "eess.AS",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "Published at AAAI 2025 Workshop",
      "pdf_url": "http://arxiv.org/pdf/2501.17879v1",
      "published_date": "2025-01-20 04:57:29 UTC",
      "updated_date": "2025-01-20 04:57:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:25:11.127234"
    },
    {
      "arxiv_id": "2501.11270v1",
      "title": "Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features",
      "title_zh": "翻译失败",
      "authors": [
        "Osama Ahmad",
        "Zubair Khalid",
        "Muhammad Tahir",
        "Momin Uppal"
      ],
      "abstract": "Monitoring air pollution is crucial for protecting human health from exposure\nto harmful substances. Traditional methods of air quality monitoring, such as\nground-based sensors and satellite-based remote sensing, face limitations due\nto high deployment costs, sparse sensor coverage, and environmental\ninterferences. To address these challenges, this paper proposes a framework for\nhigh-resolution spatiotemporal Air Quality Index (AQI) mapping using sparse\nsensor data, satellite imagery, and various spatiotemporal factors. By\nleveraging Graph Neural Networks (GNNs), we estimate AQI values at unmonitored\nlocations based on both spatial and temporal dependencies. The framework\nincorporates a wide range of environmental features, including meteorological\ndata, road networks, points of interest (PoIs), population density, and urban\ngreen spaces, which enhance prediction accuracy. We illustrate the use of our\napproach through a case study in Lahore, Pakistan, where multi-resolution data\nis used to generate the air quality index map at a fine spatiotemporal scale.",
      "tldr_zh": "这篇论文提出一个框架，用于利用稀疏传感器数据、卫星图像、气象因素和空间特征，进行城市区域的高分辨率时空 Air Quality Index (AQI) 映射，以解决传统监测方法的成本高、覆盖稀疏和环境干扰问题。框架采用 Graph Neural Networks (GNNs) 来基于空间和时间依赖性估计未监测位置的 AQI 值，并整合气象数据、道路网络、Points of Interest (PoIs)、人口密度和城市绿地等环境特征，以提升预测准确性。通过在 Lahore, Pakistan 的案例研究，该方法成功生成精细时空尺度的空气质量指数地图。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11270v1",
      "published_date": "2025-01-20 04:39:13 UTC",
      "updated_date": "2025-01-20 04:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:26:47.821052"
    },
    {
      "arxiv_id": "2501.11264v1",
      "title": "Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian",
      "title_zh": "翻译失败",
      "authors": [
        "Wannita Takerngsaksiri",
        "Micheal Fu",
        "Chakkrit Tantithamthavorn",
        "Jirat Pasuksmit",
        "Kun Chen",
        "Ming Wu"
      ],
      "abstract": "Programmers spend a significant amount of time reading code during the\nsoftware development process. This trend is amplified by the emergence of large\nlanguage models (LLMs) that automatically generate code. However, little is\nknown about the readability of the LLM-generated code and whether it is still\nimportant from practitioners' perspectives in this new era. In this paper, we\nconduct a survey to explore the practitioners' perspectives on code readability\nin the age of LLMs and investigate the readability of our LLM-based software\ndevelopment agents framework, HULA, by comparing its generated code with\nhuman-written code in real-world scenarios. Overall, the findings underscore\nthat (1) readability remains a critical aspect of software development; (2) the\nreadability of our LLM-generated code is comparable to human-written code,\nfostering the establishment of appropriate trust and driving the broad adoption\nof our LLM-powered software development platform.",
      "tldr_zh": "该研究探讨了在大型语言模型(LLMs)时代，代码可读性的重要性，通过Atlassian的工业案例研究进行调查。研究者分析了从业者对代码可读性的看法，并比较了其LLM-based软件开发框架HULA生成的代码与真实场景中的人工代码。结果显示，可读性仍是软件开发的关键因素，而HULA生成的代码可读性与人工代码相当，这有助于建立适当的信任并促进LLM驱动平台的广泛采用。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "6 pages, 2 figures, 5 tables, under review",
      "pdf_url": "http://arxiv.org/pdf/2501.11264v1",
      "published_date": "2025-01-20 04:11:21 UTC",
      "updated_date": "2025-01-20 04:11:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:26:58.133237"
    },
    {
      "arxiv_id": "2501.13951v2",
      "title": "A Layered Multi-Expert Framework for Long-Context Mental Health Assessments",
      "title_zh": "翻译失败",
      "authors": [
        "Jinwen Tang",
        "Qiming Guo",
        "Wenbo Sun",
        "Yi Shang"
      ],
      "abstract": "Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）在处理长上下文心理健康评估时存在的幻觉和不一致推理问题，提出了 Stacked Multi-Model Reasoning (SMMR) 框架，该框架采用分层结构，将多个 LLMs 和专门的较小模型作为平等的“专家”，早期层处理短的离散子任务，后期层整合和精炼输出。实验在 DAIC-WOZ 抑郁筛查数据集和 48 个策划的案例研究上进行，显示 SMMR 比单模型基线在准确性、F1-score 和 PHQ-8 错误减少方面取得了显著改善。通过利用多样化的“第二意见”，该框架减少了幻觉，捕捉了微妙的临床细微差别，并提升了高风险心理健康评估的可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13951v2",
      "published_date": "2025-01-20 03:22:19 UTC",
      "updated_date": "2025-02-07 20:22:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:27:11.841974"
    },
    {
      "arxiv_id": "2501.11238v1",
      "title": "WSSM: Geographic-enhanced hierarchical state-space model for global station weather forecast",
      "title_zh": "WSSM：地理增强的层次状态空间模型，用于全球站点天气预报",
      "authors": [
        "Songru Yang",
        "Zili Liu",
        "Zhenwei Shi",
        "Zhengxia Zou"
      ],
      "abstract": "Global Station Weather Forecasting (GSWF), a prominent meteorological\nresearch area, is pivotal in providing timely localized weather predictions.\nDespite the progress existing models have made in the overall accuracy of the\nGSWF, executing high-precision extreme event prediction still presents a\nsubstantial challenge. The recent emergence of state-space models, with their\nability to efficiently capture continuous-time dynamics and latent states,\noffer potential solutions. However, early investigations indicated that Mamba\nunderperforms in the context of GSWF, suggesting further adaptation and\noptimization. To tackle this problem, in this paper, we introduce Weather\nState-space Model (WSSM), a novel Mamba-based approach tailored for GSWF.\nGeographical knowledge is integrated in addition to the widely-used positional\nencoding to represent the absolute special-temporal position. The multi-scale\ntime-frequency features are synthesized from coarse to fine to model the\nseasonal to extreme weather dynamic. Our method effectively improves the\noverall prediction accuracy and addresses the challenge of forecasting extreme\nweather events. The state-of-the-art results obtained on the Weather-5K subset\nunderscore the efficacy of the WSSM",
      "tldr_zh": "这篇论文针对Global Station Weather Forecasting (GSWF)中的极端事件预测挑战，提出了一种新型Weather State-space Model (WSSM)，基于Mamba模型并整合地理知识和positional encoding来精确表示时空位置。WSSM通过多尺度时间-频率特征合成，从粗到细模拟季节到极端天气的动态，从而提升整体预测准确性。实验在Weather-5K子集上取得了state-of-the-art结果，证明了该方法在高精度天气预报中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11238v1",
      "published_date": "2025-01-20 02:57:02 UTC",
      "updated_date": "2025-01-20 02:57:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:27:22.001998"
    },
    {
      "arxiv_id": "2501.13949v1",
      "title": "Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question Head-to-Head Evaluation Study",
      "title_zh": "翻译失败",
      "authors": [
        "Sahana Srinivasan",
        "Xuguang Ai",
        "Minjie Zou",
        "Ke Zou",
        "Hyunjae Kim",
        "Thaddaeus Wai Soon Lo",
        "Krithi Pushpanathan",
        "Yiming Kong",
        "Anran Li",
        "Maxwell Singer",
        "Kai Jin",
        "Fares Antaki",
        "David Ziyou Chen",
        "Dianbo Liu",
        "Ron A. Adelman",
        "Qingyu Chen",
        "Yih Chung Tham"
      ],
      "abstract": "Question: What is the performance and reasoning ability of OpenAI o1 compared\nto other large language models in addressing ophthalmology-specific questions?\n  Findings: This study evaluated OpenAI o1 and five LLMs using 6,990\nophthalmological questions from MedMCQA. O1 achieved the highest accuracy\n(0.88) and macro-F1 score but ranked third in reasoning capabilities based on\ntext-generation metrics. Across subtopics, o1 ranked first in ``Lens'' and\n``Glaucoma'' but second to GPT-4o in ``Corneal and External Diseases'',\n``Vitreous and Retina'' and ``Oculoplastic and Orbital Diseases''. Subgroup\nanalyses showed o1 performed better on queries with longer ground truth\nexplanations.\n  Meaning: O1's reasoning enhancements may not fully extend to ophthalmology,\nunderscoring the need for domain-specific refinements to optimize performance\nin specialized fields like ophthalmology.",
      "tldr_zh": "这篇论文评估了 OpenAI o1 与其他五个 LLMs 在处理 6,990 个眼科问题的表现和推理能力，使用 MedMCQA 数据集进行对比测试。结果显示，o1 取得了最高的准确率 (0.88) 和 macro-F1 score，但在基于文本生成指标的推理能力上排名第三。子主题分析表明，o1 在 “Lens” 和 “Glaucoma” 上排名第一，但在 “Corneal and External Diseases”、 “Vitreous and Retina” 以及 “Oculoplastic and Orbital Diseases” 上次于 GPT-4o，且在 ground truth 解释较长的查询上表现更佳。该研究强调，o1 的推理增强可能未完全扩展到眼科领域，需进行领域特定的优化以提升在专业领域的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "44 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.13949v1",
      "published_date": "2025-01-20 02:40:01 UTC",
      "updated_date": "2025-01-20 02:40:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:29:28.046487"
    },
    {
      "arxiv_id": "2501.11223v3",
      "title": "Reasoning Language Models: A Blueprint",
      "title_zh": "推理语言模型：蓝图",
      "authors": [
        "Maciej Besta",
        "Julia Barth",
        "Eric Schreiber",
        "Ales Kubicek",
        "Afonso Catarino",
        "Robert Gerstenberger",
        "Piotr Nyczyk",
        "Patrick Iff",
        "Yueling Li",
        "Sam Houliston",
        "Tomasz Sternal",
        "Marcin Copik",
        "Grzegorz Kwaśniewski",
        "Jürgen Müller",
        "Łukasz Flis",
        "Hannes Eberhard",
        "Hubert Niewiadomski",
        "Torsten Hoefler"
      ],
      "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.",
      "tldr_zh": "这篇论文提出一个模块化蓝图，用于组织 Reasoning Language Models (RLMs)，也称为 Large Reasoning Models (LRMs)，以解决其高成本、专有性和复杂架构带来的可访问性挑战。该蓝图基于对现有 RLM 作品的调查，涵盖了多样化的推理结构（如 chains, trees, graphs）、策略（如 Monte Carlo Tree Search）、Reinforcement Learning (RL) 概念（如 policy 和 value models）、监督方案（如 Outcome-Based and Process-Based Supervision），并提供详细的数学公式和算法规范。论文通过示例（如 LLaMA-Berry 和 QwQ）展示蓝图的通用性，并引入 x1 作为快速原型工具，提供关键见解，如多阶段训练和训练分布的重要性。最后，它讨论了 RLMs 的可扩展部署和与更广泛 LLM 生态系统的整合，旨在降低 RLM 设计门槛，促进创新和公平性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11223v3",
      "published_date": "2025-01-20 02:16:19 UTC",
      "updated_date": "2025-01-23 14:26:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:27:48.128955"
    },
    {
      "arxiv_id": "2501.11218v3",
      "title": "Leveraging GANs For Active Appearance Models Optimized Model Fitting",
      "title_zh": "翻译失败",
      "authors": [
        "Anurag Awasthi"
      ],
      "abstract": "Active Appearance Models (AAMs) are a well-established technique for fitting\ndeformable models to images, but they are limited by linear appearance\nassumptions and can struggle with complex variations. In this paper, we explore\nif the AAM fitting process can benefit from a Generative Adversarial Network\n(GAN). We uses a U-Net based generator and a PatchGAN discriminator for\nGAN-augmented framework in an attempt to refine the appearance model during\nfitting. This approach attempts to addresses challenges such as non-linear\nappearance variations and occlusions that traditional AAM optimization methods\nmay fail to handle. Limited experiments on face alignment datasets demonstrate\nthat the GAN-enhanced AAM can achieve higher accuracy and faster convergence\nthan classic approaches with some manual interventions. These results establish\nfeasibility of GANs as a tool for improving deformable model fitting in\nchallenging conditions while maintaining efficient performance, and establishes\nthe need for more future work to evaluate this approach at scale.",
      "tldr_zh": "本文提出了一种利用 Generative Adversarial Network (GAN) 增强 Active Appearance Models (AAMs) 的框架，以优化可变形模型的图像拟合过程。方法采用基于 U-Net 的生成器和 PatchGAN 判别器，旨在处理传统 AAMs 难以应对的非线性外观变化和遮挡问题，通过 GAN 辅助在拟合过程中精炼外观模型。在面部对齐数据集上的有限实验显示，该框架比经典方法实现了更高的准确性和更快收敛，但仍需手动干预。结果证明了 GAN 在改善可变形模型拟合方面的可行性，并呼吁未来进行更大规模的评估。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "The full text of this preprint has been withdrawn, as it was\n  submitted in error at a much earlier stage, with work still needing\n  substantial refinement and validation. Therefore, the authors do not wish\n  this work to be cited as a reference",
      "pdf_url": "http://arxiv.org/pdf/2501.11218v3",
      "published_date": "2025-01-20 01:49:37 UTC",
      "updated_date": "2025-04-07 04:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:27:58.349782"
    },
    {
      "arxiv_id": "2501.16355v1",
      "title": "How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Xie",
        "Pavan Rauch",
        "Xueru Zhang"
      ],
      "abstract": "When machine learning (ML) algorithms are used to automate human-related\ndecisions, human agents may gain knowledge of the decision policy and behave\nstrategically to obtain desirable outcomes. Strategic Classification (SC) has\nbeen proposed to address the interplay between agents and decision-makers.\nPrior work on SC has relied on assumptions that agents are perfectly or\napproximately rational, responding to decision policies by maximizing their\nutilities. Verifying these assumptions is challenging due to the difficulty of\ncollecting real-world agent responses. Meanwhile, the growing adoption of large\nlanguage models (LLMs) makes it increasingly likely that human agents in SC\nsettings will seek advice from these tools. We propose using strategic advice\ngenerated by LLMs to simulate human agent responses in SC. Specifically, we\nexamine five critical SC scenarios -- hiring, loan applications, school\nadmissions, personal income, and public assistance programs -- and simulate how\nhuman agents with diverse profiles seek advice from LLMs. We then compare the\nresulting agent responses with the best responses generated by existing\ntheoretical models. Our findings reveal that: (i) LLMs and theoretical models\ngenerally lead to agent score or qualification changes in the same direction\nacross most settings, with both achieving similar levels of fairness; (ii)\nstate-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide\nhelpful suggestions, though these suggestions typically do not result in\nmaximal score or qualification improvements; and (iii) LLMs tend to produce\nmore diverse agent responses, often favoring more balanced effort allocation\nstrategies. These results suggest that theoretical models align with LLMs to\nsome extent and that leveraging LLMs to simulate more realistic agent responses\noffers a promising approach to designing trustworthy ML systems.",
      "tldr_zh": "本论文探讨了在战略分类(Strategic Classification)中，人类代理如何响应机器学习决策政策，通过比较理论模型与大型语言模型(LLMs)生成的响应。\n研究者模拟了五个关键场景，包括招聘、贷款申请、学校录取、个人收入和公共援助程序，让LLMs为多样化代理提供建议，并与理论模型的最佳响应进行对比。\n结果表明，LLMs和理论模型在代理分数或资格变化方向上基本一致，且公平性相似，但LLMs提供的建议虽有帮助（如GPT-3.5和GPT-4所生成），却通常未实现最大优化，而是倾向于更平衡的努力分配策略。\n这些发现表明，理论模型与LLMs有一定一致性，并建议利用LLMs模拟真实代理响应，以改进可信赖的ML系统设计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.16355v1",
      "published_date": "2025-01-20 01:39:03 UTC",
      "updated_date": "2025-01-20 01:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:28:11.809046"
    },
    {
      "arxiv_id": "2503.15499v1",
      "title": "Approach to Visual Attractiveness of Event Space Through Data-Driven Environment and Spatial Perception",
      "title_zh": "通过数据驱动环境和空间感知提升事件空间视觉吸引力的方法",
      "authors": [
        "Aliffi Majiid",
        "Riaz-Ul-Haque Mian",
        "Kouki Kurohara",
        "Yen-Khang Nguyen-Tran"
      ],
      "abstract": "Revitalizing Japan's remote areas has become a crucial task, and Matsue City\nexemplifies this effort in its temporary event spaces, created through\ncollective efforts to foster urban vibrancy and bring together residents and\nvisitors. This research examines the relationship between data-driven in-sights\nusing generative AI and visual attractiveness by evaluating tempo-rary events\nin Matsue City, particularly considering the cognitive-cultural differences in\nprocessing visual information of the participants. The first phase employs\nsemantic keyword extraction from interviews, categorizing responses into\nphysical elements, activities, and atmosphere. The second phase analyzes\nspatial perception through three categories: layout hierar-chy, product\nvisibility, and visual attention. The correlation indicates that successful\nevent design requires a balance between spatial efficiency and diverse needs,\nwith a spatial organization that optimizes visitor flow and visibility\nstrategies considering cultural and demographic diversity. These findings\ncontribute to understanding the urban quality of temporary event spaces and\noffer a replicable framework for enhancing the visual appeal of events in\nremote areas throughout Japan.",
      "tldr_zh": "这篇论文探讨了通过数据驱动方法提升日本偏远地区临时事件空间的视觉吸引力，以松江市为例，并考虑参与者的认知文化差异。研究分为两个阶段：第一阶段使用生成式 AI 从访谈中提取语义关键词，并分类为物理元素、活动和氛围；第二阶段分析空间感知，包括布局层次、产品可见性和视觉注意力。结果表明，成功的事件设计需要平衡空间效率与多样需求，同时优化访客流动和可见性策略，以适应文化和人口多样性。该研究为理解临时事件空间的城市质量提供了贡献，并提出一个可复制的框架，用于日本其他偏远地区的应用。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15499v1",
      "published_date": "2025-01-20 00:55:15 UTC",
      "updated_date": "2025-01-20 00:55:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:28:22.798020"
    },
    {
      "arxiv_id": "2501.13948v2",
      "title": "Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Rohitash Chandra",
        "Guoxiang Ren",
        "Group-H"
      ],
      "abstract": "Over the past decades, there has been an increasing concern about the\nprevalence of abusive and violent content in Hollywood movies. This study uses\nLarge Language Models (LLMs) to explore the longitudinal abuse and sentiment\nanalysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024.\nBy employing fine-tuned LLMs, we analyze subtitles for over a thousand movies\ncategorised into four genres to examine the trends and shifts in emotional and\nabusive content over the past seven decades. Our findings reveal significant\ntemporal changes in movie dialogues, which reflect broader social and cultural\ninfluences. Overall, the emotional tendencies in the films are diverse, and the\ndetection of abusive content also exhibits significant fluctuations. The\nresults show a gradual rise in abusive content in recent decades, reflecting\nsocial norms and regulatory policy changes. Genres such as thrillers still\npresent a higher frequency of abusive content that emphasises the ongoing\nnarrative role of violence and conflict. At the same time, underlying positive\nemotions such as humour and optimism remain prevalent in most of the movies.\nFurthermore, the gradual increase of abusive content in movie dialogues has\nbeen significant over the last two decades, where Oscar-nominated movies\novertook the top ten blockbusters.",
      "tldr_zh": "该研究利用大型语言模型（LLMs）对1950年至2024年好莱坞奥斯卡和票房大片对话进行纵向分析，涵盖超过一千部电影的四个类型，考察了滥用内容和情感趋势的变化。结果显示，滥用内容在过去几十年逐渐增加，特别是在惊悚片中频率较高，同时积极情感如幽默和乐观在大多数电影中持续存在。分析揭示了这些变化反映了更广泛的社会规范、政策调整和社会文化影响，并在最近二十年，奥斯卡提名电影的滥用内容超过了顶级票房大片。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13948v2",
      "published_date": "2025-01-20 00:44:38 UTC",
      "updated_date": "2025-02-22 00:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:28:34.899759"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 80,
  "processed_papers_count": 80,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T01:29:45.305259"
}
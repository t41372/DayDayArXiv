{
  "date": "2024-09-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-21 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 51 篇论文，主要聚焦 AI 和机器学习应用，包括 LLM 的安全漏洞、生成模型在科学领域的创新，以及医疗和环境预测的 AI 方法，其中 PathSeeker（探讨 LLM 安全风险）和 AI Assistants for Spaceflight Procedures（太空 AI 助手，由 J. Nathan Kutz 等知名学者参与）最为令人印象深刻，突显了 AI 在复杂场景中的潜力。\n\n### 重点论文讨论\n我们先聊聊那些重要且话题度高的论文，尤其是涉及 LLM 和 AI 应用的，然后快速掠过其他较基础的。\n\n**PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach**（中文：PathSeeker：使用强化学习 jailbreak 方法探索 LLM 安全漏洞）  \n这篇论文提出了一种基于多代理强化学习的黑盒攻击方法，模拟 LLM 的“安全迷宫”来诱导有害响应，显著提高了对 GPT-4o-mini 和 Claude-3.5 等模型的攻击成功率，强调了 LLM 安全性的潜在风险，为未来防御机制提供了新思路。\n\n**AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues**（中文：太空飞行程序的 AI 助手：结合生成式预训练 Transformer 和知识图谱的检索增强生成与增强现实提示）  \n由 J. Nathan Kutz 等学者主导，这篇论文设计了 CORE 系统，使用 RAG 和知识图谱实现离线太空任务支持，并整合增强现实，提供直观的程序指导，展示了 AI 在太空探索中的可靠性和灵活性，已被 ESA SPAICE Conference 2024 接受。\n\n**StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking**（中文：StateAct：通过自提示和状态跟踪增强 LLM 基础代理）  \n论文引入自提示和链式状态跟踪机制，提升 LLM 代理在任务执行中的目标一致性和长期推理能力，在 Alfworld 和 Textcraft 等基准上比 ReAct 提升 7-30%，为高效 AI 代理提供了可扩展框架。\n\n**Predicting Coronary Heart Disease Using a Suite of Machine Learning Models**（中文：使用多种机器学习模型预测冠心病）  \n这篇研究比较了多种监督学习算法，发现 Random Forest 在过采样下达到 84% 准确率，提供了一种低成本、非侵入性心脏病预测方法，适用于早期诊断。\n\n**MEGA-PT: A Meta-Game Framework for Agile Penetration Testing**（中文：MEGA-PT：用于敏捷渗透测试的元游戏框架）  \n论文提出 MEGA-PT 框架，使用微观战术游戏和宏观策略建模，实现分布式渗透测试，实验证明其在网络安全中的适应性和效率，填补了传统方法的局限。\n\n**R-AIF: Solving Sparse-Reward Robotic Tasks from Pixels with Active Inference and World Models**（中文：R-AIF：使用主动推理和世界模型从像素解决稀疏奖励机器人任务）  \n这项工作引入新型先验偏好学习和自修正机制，提升了机器人控制在 POMDP 环境下的性能，代码已开源，适用于复杂视觉任务。\n\n**WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer**（中文：WeatherFormer：使用时空 Transformer 增强全球数值天气预报）  \n论文开发了 WeatherFormer 模型，通过空间-时间因子化 Transformer 优化大气动力学预测，在 WeatherBench 数据集上超越传统物理模型，展示了 AI 在天气预报的环保潜力。\n\n### 其他论文快速掠过\n剩余论文多为技术优化或调研，篇幅有限，只简要提及核心点：\n- **An Instance-based Plus Ensemble Learning Method for Classification of Scientific Papers**（中文：基于实例和集成学习的科学论文分类方法）：提出结合内容和引文特征的分类框架，在 DBLP 数据集上有效提升论文分类效率。\n- **Loop Neural Networks for Parameter Sharing**（中文：循环神经网络的参数共享）：改进 GPT-2 的循环机制，实现无额外数据的高效语言建模。\n- **Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm**（中文：Obliviate：在参数高效微调中中和任务无关后门）：设计了放大 benign 神经元的技术，显著降低 PEFT 模型的后门风险。\n- **QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling**（中文：QMOS：使用问题掩码损失和选项混洗增强 LLM 在电信领域的性能）：优化开源 LLM 在电信问答中的准确率，提升 24-84%。\n- **BrainDreamer: Reasoning-Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance**（中文：BrainDreamer：通过语言指导从 EEG 脑信号生成推理一致的可控图像）：使用自校正循环生成高质量图像，展示了脑机接口的 AI 潜力。\n- 其他如医疗调研（如 Data-Driven Approach to assess healthcare）和优化模型（如 Normalized Narrow Jump To Conclusions）的论文，贡献在于数据驱动分析或效率提升，但影响力较小，就不展开讨论了。\n\n总之，今天的论文突出了 AI 在安全、太空和医疗的创新应用，读者可关注 LLM 相关内容以把握前沿趋势。明日见！",
  "papers": [
    {
      "arxiv_id": "2409.14237v1",
      "title": "An Instance-based Plus Ensemble Learning Method for Classification of Scientific Papers",
      "title_zh": "翻译失败",
      "authors": [
        "Fang Zhang",
        "Shengli Wu"
      ],
      "abstract": "The exponential growth of scientific publications in recent years has posed a\nsignificant challenge in effective and efficient categorization. This paper\nintroduces a novel approach that combines instance-based learning and ensemble\nlearning techniques for classifying scientific papers into relevant research\nfields. Working with a classification system with a group of research fields,\nfirst a number of typical seed papers are allocated to each of the fields\nmanually. Then for each paper that needs to be classified, we compare it with\nall the seed papers in every field. Contents and citations are considered\nseparately. An ensemble-based method is then employed to make the final\ndecision. Experimenting with the datasets from DBLP, our experimental results\ndemonstrate that the proposed classification method is effective and efficient\nin categorizing papers into various research areas. We also find that both\ncontent and citation features are useful for the classification of scientific\npapers.",
      "tldr_zh": "本研究提出了一种结合 instance-based learning 和 ensemble learning 的新方法，用于科学论文的分类，以应对出版物快速增长带来的分类挑战。该方法首先手动分配典型种子论文到每个研究领域，然后通过比较待分类论文的内容和引用特征与这些种子论文进行匹配。接着，利用 ensemble-based 方法做出最终分类决策。在 DBLP 数据集上的实验结果显示，该方法在分类效率和准确性上表现出色，并证明内容和引用特征均对科学论文分类有重要作用。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14237v1",
      "published_date": "2024-09-21 19:42:15 UTC",
      "updated_date": "2024-09-21 19:42:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:09:29.726896"
    },
    {
      "arxiv_id": "2409.14231v1",
      "title": "Predicting Coronary Heart Disease Using a Suite of Machine Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jamal Al-Karaki",
        "Philip Ilono",
        "Sanchit Baweja",
        "Jalal Naghiyev",
        "Raja Singh Yadav",
        "Muhammad Al-Zafar Khan"
      ],
      "abstract": "Coronary Heart Disease affects millions of people worldwide and is a\nwell-studied area of healthcare. There are many viable and accurate methods for\nthe diagnosis and prediction of heart disease, but they have limiting points\nsuch as invasiveness, late detection, or cost. Supervised learning via machine\nlearning algorithms presents a low-cost (computationally speaking),\nnon-invasive solution that can be a precursor for early diagnosis. In this\nstudy, we applied several well-known methods and benchmarked their performance\nagainst each other. It was found that Random Forest with oversampling of the\npredictor variable produced the highest accuracy of 84%.",
      "tldr_zh": "这篇论文探讨了使用多种机器学习模型预测冠心病的方法，以解决传统诊断的局限性，如侵入性、延迟检测和高成本。研究者应用了若干监督学习算法，并通过基准测试比较了它们的性能。结果显示，Random Forest 模型结合 oversampling 技术取得了最高的84%准确率。该方法为低成本、非侵入性的早期诊断提供了潜在的前驱解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.14231v1",
      "published_date": "2024-09-21 19:22:41 UTC",
      "updated_date": "2024-09-21 19:22:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:09:42.416791"
    },
    {
      "arxiv_id": "2409.14219v1",
      "title": "MEGA-PT: A Meta-Game Framework for Agile Penetration Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Yunfei Ge",
        "Quanyan Zhu"
      ],
      "abstract": "Penetration testing is an essential means of proactive defense in the face of\nescalating cybersecurity incidents. Traditional manual penetration testing\nmethods are time-consuming, resource-intensive, and prone to human errors.\nCurrent trends in automated penetration testing are also impractical, facing\nsignificant challenges such as the curse of dimensionality, scalability issues,\nand lack of adaptability to network changes. To address these issues, we\npropose MEGA-PT, a meta-game penetration testing framework, featuring micro\ntactic games for node-level local interactions and a macro strategy process for\nnetwork-wide attack chains. The micro- and macro-level modeling enables\ndistributed, adaptive, collaborative, and fast penetration testing. MEGA-PT\noffers agile solutions for various security schemes, including optimal local\npenetration plans, purple teaming solutions, and risk assessment, providing\nfundamental principles to guide future automated penetration testing. Our\nexperiments demonstrate the effectiveness and agility of our model by providing\nimproved defense strategies and adaptability to changes at both local and\nnetwork levels.",
      "tldr_zh": "本论文提出 MEGA-PT，一种元游戏框架，用于敏捷渗透测试，以解决传统手动方法耗时、易出错以及自动化方法面临维度诅咒和可扩展性问题的挑战。该框架通过微观策略游戏（micro tactic games）处理节点级本地交互，以及宏观策略过程（macro strategy process）管理网络级攻击链，实现分布式、适应性、协作和快速的测试过程。MEGA-PT 提供最佳本地渗透计划、紫队协作解决方案和风险评估等功能，并通过实验验证其有效性，提升了防御策略并适应本地和网络级变化。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14219v1",
      "published_date": "2024-09-21 18:46:29 UTC",
      "updated_date": "2024-09-21 18:46:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:09:57.919736"
    },
    {
      "arxiv_id": "2409.14216v1",
      "title": "R-AIF: Solving Sparse-Reward Robotic Tasks from Pixels with Active Inference and World Models",
      "title_zh": "R-AIF：利用主动推理和世界模型从像素解决稀疏奖励机器人任务",
      "authors": [
        "Viet Dung Nguyen",
        "Zhizhuo Yang",
        "Christopher L. Buckley",
        "Alexander Ororbia"
      ],
      "abstract": "Although research has produced promising results demonstrating the utility of\nactive inference (AIF) in Markov decision processes (MDPs), there is relatively\nless work that builds AIF models in the context of environments and problems\nthat take the form of partially observable Markov decision processes (POMDPs).\nIn POMDP scenarios, the agent must infer the unobserved environmental state\nfrom raw sensory observations, e.g., pixels in an image. Additionally, less\nwork exists in examining the most difficult form of POMDP-centered control:\ncontinuous action space POMDPs under sparse reward signals. In this work, we\naddress issues facing the AIF modeling paradigm by introducing novel prior\npreference learning techniques and self-revision schedules to help the agent\nexcel in sparse-reward, continuous action, goal-based robotic control POMDP\nenvironments. Empirically, we show that our agents offer improved performance\nover state-of-the-art models in terms of cumulative rewards, relative\nstability, and success rate. The code in support of this work can be found at\nhttps://github.com/NACLab/robust-active-inference.",
      "tldr_zh": "该论文探讨了使用 Active Inference (AIF) 和 World Models 来解决部分可观察 Markov 决策过程 (POMDPs) 中的稀疏奖励机器人任务问题，特别是从像素级原始感官观察中推断环境状态。研究引入了新的 prior preference learning 技术和 self-revision schedules，帮助代理在连续动作空间和稀疏奖励的机器人控制环境中实现更优表现。实验结果显示，该方法在累积奖励、相对稳定性和成功率方面优于现有最先进模型，为复杂 POMDP 场景下的机器人学习提供了有效框架。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "68T40 (Primary) 68T07, 68T37, 68T05 (Secondary)",
        "I.2.9; I.2.10; G.3; I.2.6"
      ],
      "primary_category": "cs.RO",
      "comment": "20 pages, 2 algorithms, 2 tables, 5 figures, submitted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.14216v1",
      "published_date": "2024-09-21 18:32:44 UTC",
      "updated_date": "2024-09-21 18:32:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:10:06.819720"
    },
    {
      "arxiv_id": "2409.14206v1",
      "title": "AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues",
      "title_zh": "翻译失败",
      "authors": [
        "Oliver Bensch",
        "Leonie Bensch",
        "Tommy Nilsson",
        "Florian Saling",
        "Bernd Bewer",
        "Sophie Jentzsch",
        "Tobias Hecking",
        "J. Nathan Kutz"
      ],
      "abstract": "This paper describes the capabilities and potential of the intelligent\npersonal assistant (IPA) CORE (Checklist Organizer for Research and\nExploration), designed to support astronauts during procedures onboard the\nInternational Space Station (ISS), the Lunar Gateway station, and beyond. We\nreflect on the importance of a reliable and flexible assistant capable of\noffline operation and highlight the usefulness of audiovisual interaction using\naugmented reality elements to intuitively display checklist information. We\nargue that current approaches to the design of IPAs in space operations fall\nshort of meeting these criteria. Therefore, we propose CORE as an assistant\nthat combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for\na Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements\nto ensure an intuitive understanding of procedure steps, reliability, offline\navailability, and flexibility in terms of response style and procedure updates.",
      "tldr_zh": "该论文介绍了 CORE（Checklist Organizer for Research and Exploration），一个智能个人助理（IPA），旨在支持宇航员在国际空间站（ISS）和月球门户站等环境下的程序操作。CORE 通过结合 Knowledge Graphs (KGs)、Retrieval-Augmented Generation (RAG) 用于 Generative Pre-Trained Transformer (GPT)，以及 Augmented Reality (AR) 元素，实现可靠的离线运行、直观音视频交互和灵活的响应风格。相比现有 IPA 设计，该框架解决了可靠性与灵活性不足的问题，确保程序步骤的精确理解和易于更新。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "68T01, 68T20, 68T30, 68T50, 68T05,",
        "I.2; H.5"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for the ESA SPAICE Conference 2024: AI in and for Space",
      "pdf_url": "http://arxiv.org/pdf/2409.14206v1",
      "published_date": "2024-09-21 17:41:46 UTC",
      "updated_date": "2024-09-21 17:41:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:10:18.558303"
    },
    {
      "arxiv_id": "2409.14199v3",
      "title": "Loop Neural Networks for Parameter Sharing",
      "title_zh": "翻译失败",
      "authors": [
        "Kei-Sing Ng",
        "Qingchen Wang"
      ],
      "abstract": "The success of large-scale language models like GPT can be attributed to\ntheir ability to efficiently predict the next token in a sequence. However,\nthese models rely on constant computational effort regardless of the complexity\nof the token they are predicting, lacking the capacity for iterative\nrefinement. In this paper, we introduce a novel Loop Neural Network, which\nachieves better performance by utilizing longer computational time without\nincreasing the model size. Our approach revisits the input multiple times,\nrefining the prediction by iteratively looping over a subset of the model with\nresidual connections. We demonstrate the effectiveness of this method through\nexperiments comparing versions of GPT-2 with our loop models, showing improved\nperformance in language modeling tasks while maintaining similar parameter\ncounts. Importantly, these improvements are achieved without the need for extra\ntraining data.",
      "tldr_zh": "本文提出 Loop Neural Network，一种通过参数共享实现迭代精炼的神经网络架构，旨在解决传统语言模型如 GPT-2 在预测复杂 token 时缺乏灵活计算的问题。该方法通过多次重访输入，并在模型子集上循环使用 residual connections 来逐步优化预测，从而在不增加模型大小的情况下提升性能。实验结果显示，与 GPT-2 版本相比，Loop Neural Network 在语言建模任务中表现出色，同时保持相似的参数数量，且无需额外训练数据。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14199v3",
      "published_date": "2024-09-21 17:07:42 UTC",
      "updated_date": "2024-11-08 15:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:10:40.704203"
    },
    {
      "arxiv_id": "2409.14194v1",
      "title": "Data-Driven Approach to assess and identify gaps in healthcare set up in South Asia",
      "title_zh": "翻译失败",
      "authors": [
        "Rusham Elahi",
        "Zia Tahseen",
        "Tehreem Fatima",
        "Syed Wafa Zahra",
        "Hafiz Muhammad Abubakar",
        "Tehreem Zafar",
        "Aqs Younas",
        "Muhammad Talha Quddoos",
        "Usman Nazir"
      ],
      "abstract": "Primary healthcare is a crucial strategy for achieving universal health\ncoverage. South Asian countries are working to improve their primary healthcare\nsystem through their country specific policies designed in line with WHO health\nsystem framework using the six thematic pillars: Health Financing, Health\nService delivery, Human Resource for Health, Health Information Systems,\nGovernance, Essential Medicines and Technology, and an addition area of\nCross-Sectoral Linkages. Measuring the current accessibility of healthcare\nfacilities and workforce availability is essential for improving healthcare\nstandards and achieving universal health coverage in developing countries.\nData-driven surveillance approaches are required that can provide rapid,\nreliable, and geographically scalable solutions to understand a) which\ncommunities and areas are most at risk of inequitable access and when, b) what\nbarriers to health access exist, and c) how they can be overcome in ways\ntailored to the specific challenges faced by individual communities. We propose\nto harness current breakthroughs in Earth-observation (EO) technology, which\nprovide the ability to generate accurate, up-to-date, publicly accessible, and\nreliable data, which is necessary for equitable access planning and resource\nallocation to ensure that vaccines, and other interventions reach everyone,\nparticularly those in greatest need, during normal and crisis times. This\nrequires collaboration among countries to identify evidence based solutions to\nshape health policy and interventions, and drive innovations and research in\nthe region.",
      "tldr_zh": "本研究提出了一种数据驱动的方法，用于评估和识别南亚医疗体系的差距，旨在通过WHO health system framework的六大支柱（包括Health Financing、Health Service delivery、Human Resource for Health等）以及Cross-Sectoral Linkages来改善初级医疗保健的可及性。方法利用Earth-observation (EO) 技术生成准确、实时的数据，帮助识别高风险社区、访问障碍，并制定针对性解决方案，以实现universal health coverage。最终，该方法强调国家间合作，推动基于证据的政策创新和资源分配，确保在正常和危机时期公平覆盖医疗干预。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14194v1",
      "published_date": "2024-09-21 16:50:16 UTC",
      "updated_date": "2024-09-21 16:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:10:52.864668"
    },
    {
      "arxiv_id": "2409.14191v3",
      "title": "Addressing and Visualizing Misalignments in Human Task-Solving Trajectories",
      "title_zh": "翻译失败",
      "authors": [
        "Sejin Kim",
        "Hosung Lee",
        "Sundong Kim"
      ],
      "abstract": "Understanding misalignments in human task-solving trajectories is critical\nfor improving AI models trained to mimic human reasoning. This study\ncategorizes such misalignments into three types: \\textbf{(1) Lack of functions\nto express intent}, \\textbf{(2) Inefficient action sequences}, and \\textbf{(3)\nIncorrect intentions that cannot solve the task}. To address these issues, we\nfirst formalize and define these three types of misalignments. We then propose\na heuristic algorithm to detect these misalignments in O2ARC trajectories and\nconduct a hierarchical and quantitative analysis of their impact. Furthermore,\nwe introduce an intention estimation algorithm that predicts missing alignment\ninformation between user actions and inferred intentions, leveraging our\nformalized framework. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the potential of intention learning.",
      "tldr_zh": "本文研究人类任务解决轨迹中的 misalignments，并将其分类为三类：(1) Lack of functions to express intent、(2) Inefficient action sequences 和 (3) Incorrect intentions that cannot solve the task，以改善 AI 模型模仿人类推理的性能。研究者形式化这些不一致，提出启发式算法检测它们在 O2ARC 轨迹中的影响，并引入意图估计算法来预测动作与意图的对齐信息。实验通过轨迹对齐证明，AI 模型在处理这些 misalignments 后表现提升，并突显轨迹-intention alignment 和意图学习的重要性。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "KDD 2025 accepted",
      "pdf_url": "http://arxiv.org/pdf/2409.14191v3",
      "published_date": "2024-09-21 16:38:22 UTC",
      "updated_date": "2025-05-15 11:17:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:10:56.073035"
    },
    {
      "arxiv_id": "2409.14181v1",
      "title": "Democratising Artificial Intelligence for Pandemic Preparedness and Global Governance in Latin American and Caribbean Countries",
      "title_zh": "翻译失败",
      "authors": [
        "Andre de Carvalho",
        "Robson Bonidia",
        "Jude Dzevela Kong",
        "Mariana Dauhajre",
        "Claudio Struchiner",
        "Guilherme Goedert",
        "Peter F. Stadler",
        "Maria Emilia Walter",
        "Danilo Sanches",
        "Troy Day",
        "Marcia Castro",
        "John Edmunds",
        "Manuel Colome-Hidalgo",
        "Demian Arturo Herrera Morban",
        "Edian F. Franco",
        "Cesar Ugarte-Gil",
        "Patricia Espinoza-Lopez",
        "Gabriel Carrasco-Escobar",
        "Ulisses Rocha"
      ],
      "abstract": "Infectious diseases, transmitted directly or indirectly, are among the\nleading causes of epidemics and pandemics. Consequently, several open\nchallenges exist in predicting epidemic outbreaks, detecting variants, tracing\ncontacts, discovering new drugs, and fighting misinformation. Artificial\nIntelligence (AI) can provide tools to deal with these scenarios, demonstrating\npromising results in the fight against the COVID-19 pandemic. AI is becoming\nincreasingly integrated into various aspects of society. However, ensuring that\nAI benefits are distributed equitably and that they are used responsibly is\ncrucial. Multiple countries are creating regulations to address these concerns,\nbut the borderless nature of AI requires global cooperation to define\nregulatory and guideline consensus. Considering this, The Global South AI for\nPandemic & Epidemic Preparedness & Response Network (AI4PEP) has developed an\ninitiative comprising 16 projects across 16 countries in the Global South,\nseeking to strengthen equitable and responsive public health systems that\nleverage Southern-led responsible AI solutions to improve prevention,\npreparedness, and response to emerging and re-emerging infectious disease\noutbreaks. This opinion introduces our branches in Latin American and Caribbean\n(LAC) countries and discusses AI governance in LAC in the light of\nbiotechnology. Our network in LAC has high potential to help fight infectious\ndiseases, particularly in low- and middle-income countries, generating\nopportunities for the widespread use of AI techniques to improve the health and\nwell-being of their communities.",
      "tldr_zh": "该论文讨论了在拉丁美洲和加勒比（LAC）国家中推广 Artificial Intelligence (AI) 以提升疫情准备和全球治理的民主化进程，旨在解决预测疫情爆发、检测变异株、追踪接触者、发现新药和对抗错误信息等挑战。论文介绍了 Global South AI for Pandemic & Epidemic Preparedness & Response Network (AI4PEP) 的倡议，该网络在 16 个国家开展 16 个项目，通过南方主导的负责任 AI 解决方案加强公平的公共卫生系统。研究强调 AI 在低中收入国家对抗传染病的潜力，并呼吁全球合作制定 AI 治理框架，以确保其在生物技术领域的负责任应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14181v1",
      "published_date": "2024-09-21 15:59:13 UTC",
      "updated_date": "2024-09-21 15:59:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:11:07.368291"
    },
    {
      "arxiv_id": "2409.14177v2",
      "title": "PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach",
      "title_zh": "PathSeeker: 利用基于强化学习的越狱方法探索LLM",
      "authors": [
        "Zhihao Lin",
        "Wei Ma",
        "Mingyi Zhou",
        "Yanjie Zhao",
        "Haoyu Wang",
        "Yang Liu",
        "Jun Wang",
        "Li Li"
      ],
      "abstract": "In recent years, Large Language Models (LLMs) have gained widespread use,\nraising concerns about their security. Traditional jailbreak attacks, which\noften rely on the model internal information or have limitations when exploring\nthe unsafe behavior of the victim model, limiting their reducing their general\napplicability. In this paper, we introduce PathSeeker, a novel black-box\njailbreak method, which is inspired by the game of rats escaping a maze. We\nthink that each LLM has its unique \"security maze\", and attackers attempt to\nfind the exit learning from the received feedback and their accumulated\nexperience to compromise the target LLM's security defences. Our approach\nleverages multi-agent reinforcement learning, where smaller models collaborate\nto guide the main LLM in performing mutation operations to achieve the attack\nobjectives. By progressively modifying inputs based on the model's feedback,\nour system induces richer, harmful responses. During our manual attempts to\nperform jailbreak attacks, we found that the vocabulary of the response of the\ntarget model gradually became richer and eventually produced harmful responses.\nBased on the observation, we also introduce a reward mechanism that exploits\nthe expansion of vocabulary richness in LLM responses to weaken security\nconstraints. Our method outperforms five state-of-the-art attack techniques\nwhen tested across 13 commercial and open-source LLMs, achieving high attack\nsuccess rates, especially in strongly aligned commercial models like\nGPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study\naims to improve the understanding of LLM security vulnerabilities and we hope\nthat this sturdy can contribute to the development of more robust defenses.",
      "tldr_zh": "该研究提出PathSeeker，一种基于强化学习的多智能体黑盒jailbreak方法，用于探索Large Language Models (LLMs)的安全漏洞。该方法将LLMs视为“安全迷宫”，通过较小模型协作进行输入变异（mutation operations）并利用模型反馈逐步诱导有害响应，同时引入基于词汇丰富度扩展的奖励机制来削弱安全约束。在测试中，PathSeeker在13个商业和开源LLMs上优于5种最先进攻击技术，尤其在GPT-4o-mini、Claude-3.5和GLM-4-air等强安全对齐模型中实现高成功率。该工作旨在加深对LLM安全漏洞的理解，并为开发更稳健的防御机制提供贡献。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "update the abstract and cite a new related work",
      "pdf_url": "http://arxiv.org/pdf/2409.14177v2",
      "published_date": "2024-09-21 15:36:26 UTC",
      "updated_date": "2024-10-03 08:13:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:11:19.548878"
    },
    {
      "arxiv_id": "2409.14175v2",
      "title": "QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling",
      "title_zh": "翻译失败",
      "authors": [
        "Blessed Guda",
        "Gabrial Zencha Ashungafac",
        "Lawrence Francis",
        "Carlee Joe-Wong"
      ],
      "abstract": "Large Language models (LLMs) have brought about substantial advancements in\nthe field of Question Answering (QA) systems. These models do remarkably well\nin addressing intricate inquiries in a variety of disciplines. However, because\nof domain-specific vocabulary, complex technological concepts, and the\nrequirement for exact responses applying LLMs to specialized sectors like\ntelecommunications presents additional obstacles. GPT-3.5 has been used in\nrecent work, to obtain noteworthy accuracy for telecom-related questions in a\nRetrieval Augmented Generation (RAG) framework. Notwithstanding these\ndevelopments, the practical use of models such as GPT-3.5 is restricted by\ntheir proprietary nature and high computing demands. This paper introduces\nQMOS, an innovative approach which uses a Question-Masked loss and Option\nShuffling trick to enhance the performance of LLMs in answering Multiple-Choice\nQuestions in the telecommunications domain. Our focus was on using opensource,\nsmaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.\nOur multi-faceted approach involves several enhancements to the whole LLM-RAG\npipeline of finetuning, retrieval, prompt engineering and inference. Our\napproaches significantly outperform existing results, achieving accuracy\nimprovements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%\nto 84.65% with Phi-2.",
      "tldr_zh": "本论文提出QMOS方法，通过Question-Masked loss和Option Shuffling技巧，增强大型语言模型（LLMs）在电信领域多选题问答的表现，以克服领域特定词汇和技术概念的挑战。QMOS在增强的Retrieval Augmented Generation (RAG)框架中优化开源模型Phi-2和Falcon-7B的整个管道，包括微调、检索、提示工程和推理过程。实验结果显示，该方法显著提升准确率，Falcon-7B从基线提高24.70%至49.30%，Phi-2从42.07%至84.65%，为LLMs在电信应用的开源实现提供了高效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14175v2",
      "published_date": "2024-09-21 15:32:10 UTC",
      "updated_date": "2025-02-04 07:46:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:11:33.540074"
    },
    {
      "arxiv_id": "2409.14173v1",
      "title": "An Evolutionary Algorithm For the Vehicle Routing Problem with Drones with Interceptions",
      "title_zh": "一种用于带有拦截的无人机车辆路径问题的进化算法",
      "authors": [
        "Carlos Pambo",
        "Jacomine Grobler"
      ],
      "abstract": "The use of trucks and drones as a solution to address last-mile delivery\nchallenges is a new and promising research direction explored in this paper.\nThe variation of the problem where the drone can intercept the truck while in\nmovement or at the customer location is part of an optimisation problem called\nthe vehicle routing problem with drones with interception (VRPDi). This paper\nproposes an evolutionary algorithm to solve the VRPDi. In this variation of the\nVRPDi, multiple pairs of trucks and drones need to be scheduled. The pairs\nleave and return to a depot location together or separately to make deliveries\nto customer nodes. The drone can intercept the truck after the delivery or meet\nup with the truck at the following customer location. The algorithm was\nexecuted on the travelling salesman problem with drones (TSPD) datasets by\nBouman et al. (2015), and the performance of the algorithm was compared by\nbenchmarking the results of the VRPDi against the results of the VRP of the\nsame dataset. This comparison showed improvements in total delivery time\nbetween 39% and 60%. Further detailed analysis of the algorithm results\nexamined the total delivery time, distance, node delivery scheduling and the\ndegree of diversity during the algorithm execution. This analysis also\nconsidered how the algorithm handled the VRPDi constraints. The results of the\nalgorithm were then benchmarked against algorithms in Dillon et al. (2023) and\nErnst (2024). The latter solved the problem with a maximum drone distance\nconstraint added to the VRPDi. The analysis and benchmarking of the algorithm\nresults showed that the algorithm satisfactorily solved 50 and 100-nodes\nproblems in a reasonable amount of time, and the solutions found were better\nthan those found by the algorithms in Dillon et al. (2023) and Ernst (2024) for\nthe same problems.",
      "tldr_zh": "本论文提出了一种进化算法(evolutionary algorithm)来解决车辆路径问题与无人机和拦截(VRPDi)，旨在优化最后一英里配送中卡车和无人机的协同调度。算法允许无人机在运动中拦截卡车或在客户地点会合，并在TSPD数据集上进行测试，结果显示总交付时间较基准VRP改善39%至60%。与其他算法如Dillon et al. (2023)和Ernst (2024)相比，该方法在50和100节点问题上表现出色，提供更高效的解决方案。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14173v1",
      "published_date": "2024-09-21 15:26:24 UTC",
      "updated_date": "2024-09-21 15:26:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:11:43.985863"
    },
    {
      "arxiv_id": "2409.18142v1",
      "title": "A Survey on Multimodal Benchmarks: In the Era of Large AI Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lin Li",
        "Guikun Chen",
        "Hanrong Shi",
        "Jun Xiao",
        "Long Chen"
      ],
      "abstract": "The rapid evolution of Multimodal Large Language Models (MLLMs) has brought\nsubstantial advancements in artificial intelligence, significantly enhancing\nthe capability to understand and generate multimodal content. While prior\nstudies have largely concentrated on model architectures and training\nmethodologies, a thorough analysis of the benchmarks used for evaluating these\nmodels remains underexplored. This survey addresses this gap by systematically\nreviewing 211 benchmarks that assess MLLMs across four core domains:\nunderstanding, reasoning, generation, and application. We provide a detailed\nanalysis of task designs, evaluation metrics, and dataset constructions, across\ndiverse modalities. We hope that this survey will contribute to the ongoing\nadvancement of MLLM research by offering a comprehensive overview of\nbenchmarking practices and identifying promising directions for future work. An\nassociated GitHub repository collecting the latest papers is available.",
      "tldr_zh": "这篇调查论文探讨了 Multimodal Large Language Models (MLLMs) 时代的多模态基准，系统审查了 211 个用于评估 MLLMs 的基准，涵盖理解、推理、生成和应用四个核心领域。论文详细分析了任务设计、评估指标以及数据集构建，涉及多种模态，以填补现有研究中基准评估的空白。最终，该研究为 MLLM 研究提供了全面概述，并识别了未来工作的潜在方向，同时提供了一个关联的 GitHub 仓库。",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "Ongoing project",
      "pdf_url": "http://arxiv.org/pdf/2409.18142v1",
      "published_date": "2024-09-21 15:22:26 UTC",
      "updated_date": "2024-09-21 15:22:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:11:55.354125"
    },
    {
      "arxiv_id": "2409.14165v3",
      "title": "A Survey on Large Language Model-empowered Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Zhu",
        "Shiyi Wang",
        "Wenqing Zhong",
        "Nianchen Shen",
        "Yunqi Li",
        "Siqi Wang",
        "Zhiheng Li",
        "Cathy Wu",
        "Zhengbing He",
        "Li Li"
      ],
      "abstract": "Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology.",
      "tldr_zh": "这篇调查论文探讨了大型语言模型（LLMs）在自动驾驶（AD）系统中的潜在应用，旨在解决当前 AD 技术面临的挑战。论文分析了两种主要路径——modularization（将任务分解为感知、预测、规划和控制模块，但易受模块间训练目标不一致影响）和end-to-end（直接从传感器数据映射到控制信号，但处理长尾事件和复杂场景能力有限）——并提出 LLMs 凭借其强大推理能力和广泛知识理解，能优化这些路径，提供更深入的决策支持。最终，论文讨论了 LLMs 是否能实现 AGI（人工通用智能）以推进高级 AD，以及其可能遇到的限制和挑战，如知识泛化不足和实际部署问题。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14165v3",
      "published_date": "2024-09-21 15:07:37 UTC",
      "updated_date": "2024-11-30 22:21:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:12:07.776129"
    },
    {
      "arxiv_id": "2409.14154v1",
      "title": "MSSDA: Multi-Sub-Source Adaptation for Diabetic Foot Neuropathy Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Zhong",
        "Zhixin Yan",
        "Yi Xie",
        "Shibin Wu",
        "Huaidong Zhang",
        "Lin Shu",
        "Peiru Zhou"
      ],
      "abstract": "Diabetic foot neuropathy (DFN) is a critical factor leading to diabetic foot\nulcers, which is one of the most common and severe complications of diabetes\nmellitus (DM) and is associated with high risks of amputation and mortality.\nDespite its significance, existing datasets do not directly derive from plantar\ndata and lack continuous, long-term foot-specific information. To advance DFN\nresearch, we have collected a novel dataset comprising continuous plantar\npressure data to recognize diabetic foot neuropathy. This dataset includes data\nfrom 94 DM patients with DFN and 41 DM patients without DFN. Moreover,\ntraditional methods divide datasets by individuals, potentially leading to\nsignificant domain discrepancies in some feature spaces due to the absence of\nmid-domain data. In this paper, we propose an effective domain adaptation\nmethod to address this proplem. We split the dataset based on convolutional\nfeature statistics and select appropriate sub-source domains to enhance\nefficiency and avoid negative transfer. We then align the distributions of each\nsource and target domain pair in specific feature spaces to minimize the domain\ngap. Comprehensive results validate the effectiveness of our method on both the\nnewly proposed dataset for DFN recognition and an existing dataset.",
      "tldr_zh": "本文针对糖尿病足神经病变 (DFN) 识别的挑战，收集了一个新数据集，包括94名有DFN的糖尿病 (DM) 患者和41名无DFN患者的连续足底压力数据，以弥补现有数据集的不足。作者提出MSSDA方法，通过基于卷积特征统计分割数据集、选择适当的子源域并对齐源域和目标域在特定特征空间的分布，来最小化领域差异并避免负面转移。实验结果验证了该方法的有效性，在新数据集和现有数据集上均显著提升了DFN识别性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14154v1",
      "published_date": "2024-09-21 14:16:20 UTC",
      "updated_date": "2024-09-21 14:16:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:12:20.070694"
    },
    {
      "arxiv_id": "2409.19006v2",
      "title": "Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis",
      "title_zh": "迈向自动化的专利工作流：AI 编排的多智能体框架用于知识产权管理和",
      "authors": [
        "Sakhinana Sagar Srinivas",
        "Vijay Sri Vaikunth",
        "Venkataramana Runkana"
      ],
      "abstract": "Patents are the currency of innovation, and like any currency, they need to\nbe managed and protected (Gavin Potenza). Patents, as legal documents that\nsecure intellectual property rights, play a critical role in technological\ninnovation. The growing complexity of patent documents and the surge in patent\napplications have created a need for automated solutions in patent analysis. In\nthis work, we present PatExpert, an autonomous multi-agent conversational\nframework designed to streamline and optimize patent-related tasks. The\nframework consists of a metaagent that coordinates task-specific expert agents\nfor various patent-related tasks and a critique agent for error handling and\nfeedback provision. The meta-agent orchestrates specialized expert agents, each\nfine-tuned for specific tasks such as patent classification, acceptance, claim\ngeneration, abstractive summarization, multi-patent analysis, and scientific\nhypothesis generation. For multi-patent analysis, the framework incorporates\nadvanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance\nresponse accuracy and relevance by combining semantic similarity with knowledge\ngraphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and\nReward-LLM-as-a-Judge), which evaluate output responses for accuracy and\nprovide iterative feedback. The framework also prioritizes explainability,\nensuring transparent justifications for decisions made during patent analysis.\nIts comprehensive capabilities make it a valuable tool for automating complex\npatent workflows, enhancing efficiency, accuracy, and compliance in\npatent-related tasks. Empirical evidence demonstrates significant improvements\nin patent processing tasks, concluding that the framework offers a robust\nsolution for automating and optimizing patent analysis.",
      "tldr_zh": "本研究提出PatExpert，一种自主的多智能体对话框架，用于自动化专利工作流程，旨在优化知识产权管理与分析。该框架由meta-agent协调任务特定的专家代理，处理专利分类、接受、声明生成、摘要总结、多专利分析（如使用Graph Retrieval-Augmented Generation (GRAG)）和科学假设生成，同时通过critique agents（如Gold-LLM-as-a-Judge和Reward-LLM-as-a-Judge）进行错误处理和反馈，确保决策的可解释性。实验结果显示，该框架显著提高了专利处理的效率、准确性和合规性，为专利相关任务提供了一个可靠的自动化解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Workshop on Open-World Agents (OWA-NeurIPS 2024) :\n  Synergizing Reasoning and Decision-Making in Open-World Environments",
      "pdf_url": "http://arxiv.org/pdf/2409.19006v2",
      "published_date": "2024-09-21 13:44:34 UTC",
      "updated_date": "2024-10-12 14:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:12:31.229836"
    },
    {
      "arxiv_id": "2409.14128v2",
      "title": "Present and Future Generalization of Synthetic Image Detectors",
      "title_zh": "翻译失败",
      "authors": [
        "Pablo Bernabeu-Perez",
        "Enrique Lopez-Cuena",
        "Dario Garcia-Gasulla"
      ],
      "abstract": "The continued release of increasingly realistic image generation models\ncreates a demand for synthetic image detectors. To build effective detectors we\nmust first understand how factors like data source diversity, training\nmethodologies and image alterations affect their generalization capabilities.\nThis work conducts a systematic analysis and uses its insights to develop\npractical guidelines for training robust synthetic image detectors. Model\ngeneralization capabilities are evaluated across different setups (e.g. scale,\nsources, transformations) including real-world deployment conditions. Through\nan extensive benchmarking of state-of-the-art detectors across diverse and\nrecent datasets, we show that while current approaches excel in specific\nscenarios, no single detector achieves universal effectiveness. Critical flaws\nare identified in detectors, and workarounds are proposed to enable the\ndeployment of real-world detector applications enhancing accuracy, reliability\nand robustness beyond the limitations of current systems.",
      "tldr_zh": "这篇论文探讨了合成图像检测器的泛化能力，分析了数据源多样性、训练方法和图像修改等因素如何影响其性能，并为训练鲁棒检测器提供了实用指南。作者通过系统评估和基准测试（benchmarking）不同设置（如规模、来源和变换），包括真实世界部署条件，评估了现有检测器的表现。研究发现，虽然当前最先进检测器在特定场景中表现出色，但无通用解决方案，并识别了关键缺陷，同时提出改进措施以提升准确性、可靠性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.14128v2",
      "published_date": "2024-09-21 12:46:17 UTC",
      "updated_date": "2024-11-26 09:12:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:12:43.143595"
    },
    {
      "arxiv_id": "2409.14119v3",
      "title": "Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm",
      "title_zh": "Obliviate：在参数高效微调范式中中和任务无关后门",
      "authors": [
        "Jaehan Kim",
        "Minkyoo Song",
        "Seung Ho Na",
        "Seungwon Shin"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) has become a key training strategy for\nlarge language models. However, its reliance on fewer trainable parameters\nposes security risks, such as task-agnostic backdoors. Despite their severe\nimpact on a wide range of tasks, there is no practical defense solution\navailable that effectively counters task-agnostic backdoors within the context\nof PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor\ndefense. We develop two techniques aimed at amplifying benign neurons within\nPEFT layers and penalizing the influence of trigger tokens. Our evaluations\nacross three major PEFT architectures show that our method can significantly\nreduce the attack success rate of the state-of-the-art task-agnostic backdoors\n(83.6%$\\downarrow$). Furthermore, our method exhibits robust defense\ncapabilities against both task-specific backdoors and adaptive attacks. Source\ncode will be obtained at https://github.com/obliviateARR/Obliviate.",
      "tldr_zh": "这篇论文针对 Parameter-efficient Fine-tuning (PEFT) 范式中的任务无关后门攻击，提出了 Obliviate 防御框架，以缓解其对多种任务的潜在安全风险。Obliviate 通过两种关键技术——放大 PEFT 层中的良性神经元并惩罚触发 token 的影响——来集成防御机制，并在三个主要 PEFT 架构上进行了评估。实验结果显示，该方法显著降低了最先进任务无关后门的攻击成功率（83.6% 下降），并对任务特定后门和自适应攻击表现出稳健的防御能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2409.14119v3",
      "published_date": "2024-09-21 12:20:18 UTC",
      "updated_date": "2024-10-06 09:43:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:12:55.386271"
    },
    {
      "arxiv_id": "2409.14106v4",
      "title": "Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment",
      "title_zh": "通过细粒度对齐推进分子图-文本预训练",
      "authors": [
        "Yibo Li",
        "Yuan Fang",
        "Mengmei Zhang",
        "Chuan Shi"
      ],
      "abstract": "Understanding molecular structure and related knowledge is crucial for\nscientific research. Recent studies integrate molecular graphs with their\ntextual descriptions to enhance molecular representation learning. However,\nthey focus on the whole molecular graph and neglect frequently occurring\nsubgraphs, known as motifs, which are essential for determining molecular\nproperties. Without such fine-grained knowledge, these models struggle to\ngeneralize to unseen molecules and tasks that require motif-level insights. To\nbridge this gap, we propose FineMolTex, a novel Fine-grained Molecular\ngraph-Text pre-training framework to jointly learn coarse-grained\nmolecule-level knowledge and fine-grained motif-level knowledge. Specifically,\nFineMolTex consists of two pre-training tasks: a contrastive alignment task for\ncoarse-grained matching and a masked multi-modal modeling task for fine-grained\nmatching. In particular, the latter predicts the labels of masked motifs and\nwords, which are selected based on their importance. By leveraging insights\nfrom both modalities, FineMolTex is able to understand the fine-grained\nmatching between motifs and words. Finally, we conduct extensive experiments\nacross three downstream tasks, achieving up to 238% improvement in the\ntext-based molecule editing task. Additionally, our case studies reveal that\nFineMolTex successfully captures fine-grained knowledge, potentially offering\nvaluable insights for drug discovery and catalyst design.",
      "tldr_zh": "本研究针对分子图-文本预训练的局限性，提出 FineMolTex 框架，以细粒度对齐方式整合分子级和 motifs 级知识，解决现有模型忽略子图（motifs）导致的泛化问题。具体而言，FineMolTex 包括对比对齐任务（contrastive alignment task）用于粗粒度匹配，以及屏蔽多模态建模任务（masked multi-modal modeling task）来预测重要 motifs 和 words 的标签，从而实现多模态细粒度理解。在三个下游任务上的实验显示，该框架在文本-based 分子编辑任务中提升高达 238%，并通过案例研究证明其捕捉细粒度知识的能力，有望为药物发现和催化剂设计提供宝贵洞察。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14106v4",
      "published_date": "2024-09-21 11:19:15 UTC",
      "updated_date": "2025-03-04 07:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:13:06.542696"
    },
    {
      "arxiv_id": "2409.14091v2",
      "title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Amrit Diggavi Seshadri"
      ],
      "abstract": "With the size and cost of large transformer-based language models growing,\nrecently, there has been interest in shortcut casting of early transformer\nhidden-representations to final-representations for cheaper model inference. In\nparticular, shortcutting pre-trained transformers with linear transformations\nover early layers has been shown to improve precision in early inference.\nHowever, for large language models, even this becomes computationally\nexpensive. In this work, we propose Narrow Jump to Conclusions (NJTC) and\nNormalized Narrow Jump to Conclusions (N-NJTC) - parameter efficient\nalternatives to standard linear shortcutting that reduces shortcut parameter\ncount by over 97%. We show that N-NJTC reliably outperforms Identity shortcuts\nat early stages and offers stable precision from all transformer block levels\nfor GPT-2-XL, Phi3-Mini and Llama2-7B transformer models, demonstrating the\nviability of more parameter efficient short-cutting approaches.",
      "tldr_zh": "本文提出Narrow Jump to Conclusions (NJTC) 和 Normalized Narrow Jump to Conclusions (N-NJTC) 方法，作为参数高效的替代方案，用于Transformer模型的早期退出预测，通过将shortcut参数减少97%以上来降低计算开销。相比标准线性shortcut，N-NJTC在早期层表现出色，能够在GPT-2-XL、Phi3-Mini和Llama2-7B模型中从所有Transformer块级别提供稳定的精度。实验结果证明，这种高效shortcut方法的可行性，有助于实现更廉价的模型推理。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14091v2",
      "published_date": "2024-09-21 10:09:26 UTC",
      "updated_date": "2024-10-03 07:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:13:19.312854"
    },
    {
      "arxiv_id": "2409.14084v2",
      "title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
      "title_zh": "翻译失败",
      "authors": [
        "Fabio Ferreira",
        "Moreno Schlageter",
        "Raghu Rajan",
        "Andre Biedenkapp",
        "Frank Hutter"
      ],
      "abstract": "A World Model is a compressed spatial and temporal representation of a real\nworld environment that allows one to train an agent or execute planning\nmethods. However, world models are typically trained on observations from the\nreal world environment, and they usually do not enable learning policies for\nother real environments. We propose One-Shot World Model (OSWM), a transformer\nworld model that is learned in an in-context learning fashion from purely\nsynthetic data sampled from a prior distribution. Our prior is composed of\nmultiple randomly initialized neural networks, where each network models the\ndynamics of each state and reward dimension of a desired target environment. We\nadopt the supervised learning procedure of Prior-Fitted Networks by masking\nnext-state and reward at random context positions and query OSWM to make\nprobabilistic predictions based on the remaining transition context. During\ninference time, OSWM is able to quickly adapt to the dynamics of a simple grid\nworld, as well as the CartPole gym and a custom control environment by\nproviding 1k transition steps as context and is then able to successfully train\nenvironment-solving agent policies. However, transferring to more complex\nenvironments remains a challenge, currently. Despite these limitations, we see\nthis work as an important stepping-stone in the pursuit of learning world\nmodels purely from synthetic data.",
      "tldr_zh": "本研究提出 One-Shot World Model (OSWM)，一个基于 Transformer 的世界模型，通过 in-context learning 从纯合成数据（由随机初始化神经网络生成的先验分布）中训练，实现对真实环境的快速适应。OSWM 的训练采用监督学习方法，随机掩盖上下文中的下一状态和奖励，并基于剩余过渡上下文进行概率预测；在推理阶段，仅需提供 1k 过渡步骤作为上下文，即可成功适应简单网格世界、CartPole 环境和自定义控制环境，并训练出有效的代理策略。儘管在更复杂环境中转移仍面临挑战，此工作为从合成数据学习世界模型提供了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14084v2",
      "published_date": "2024-09-21 09:39:32 UTC",
      "updated_date": "2024-10-24 18:57:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:13:30.860580"
    },
    {
      "arxiv_id": "2409.14082v1",
      "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
      "title_zh": "翻译失败",
      "authors": [
        "Ruilin Luo",
        "Liyuan Wang",
        "Binghuai Lin",
        "Zicheng Lin",
        "Yujiu Yang"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL\ntasks, exhibiting remarkable reasoning capabilities. Different from tasks such\nas math word problems and commonsense reasoning, SQL solutions have a\nrelatively fixed pattern. This facilitates the investigation of whether LLMs\ncan benefit from categorical thinking, mirroring how humans acquire knowledge\nthrough inductive reasoning based on comparable examples. In this study, we\npropose that employing query group partitioning allows LLMs to focus on\nlearning the thought processes specific to a single problem type, consequently\nenhancing their reasoning abilities across diverse difficulty levels and\nproblem categories. Our experiments reveal that multiple advanced LLMs, when\nequipped with PTD-SQL, can either surpass or match previous state-of-the-art\n(SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with\nvarying initial performances have exhibited significant improvements, mainly at\nthe boundary of their capabilities after targeted drilling, suggesting a\nparallel with human progress. Code is available at\nhttps://github.com/lrlbbzl/PTD-SQL.",
      "tldr_zh": "本研究提出 PTD-SQL，一种基于大型语言模型(LLMs)的分区和针对性训练框架，用于 Text-to-SQL 任务。该方法通过查询组分区，让 LLMs 专注于特定问题类型的思考过程，模拟人类归纳推理，从而提升模型在不同难度和类别上的推理能力。实验结果显示，多个高级 LLMs 在 Spider 和 BIRD 数据集上超过了或匹配了现有最先进(SOTA)方法，尤其是在模型能力边界处取得了显著改进，类似于人类的进步学习。代码已开源，可进一步验证和应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Main Conference. Revised by ARR April and ARR June. 32\n  pages, 7 figures and 30 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.14082v1",
      "published_date": "2024-09-21 09:33:14 UTC",
      "updated_date": "2024-09-21 09:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:13:42.692815"
    },
    {
      "arxiv_id": "2409.15381v1",
      "title": "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "G M Shahariar",
        "Jia Chen",
        "Jiachen Li",
        "Yue Dong"
      ],
      "abstract": "Recent studies show that text-to-image (T2I) models are vulnerable to\nadversarial attacks, especially with noun perturbations in text prompts. In\nthis study, we investigate the impact of adversarial attacks on different POS\ntags within text prompts on the images generated by T2I models. We create a\nhigh-quality dataset for realistic POS tag token swapping and perform\ngradient-based attacks to find adversarial suffixes that mislead T2I models\ninto generating images with altered tokens. Our empirical results show that the\nattack success rate (ASR) varies significantly among different POS tag\ncategories, with nouns, proper nouns, and adjectives being the easiest to\nattack. We explore the mechanism behind the steering effect of adversarial\nsuffixes, finding that the number of critical tokens and content fusion vary\namong POS tags, while features like suffix transferability are consistent\nacross categories. We have made our implementation publicly available at -\nhttps://github.com/shahariar-shibli/Adversarial-Attack-on-POS-Tags.",
      "tldr_zh": "本研究调查了在文本到图像（T2I）模型中，对不同词性（POS）标签进行对抗攻击的影响，特别关注文本提示中的名词、专有名词和形容词。研究者创建了一个高质量数据集，并使用基于梯度的攻击方法来生成欺骗性后缀，导致T2I模型输出图像中的token改变。实验结果显示，攻击成功率（ASR）在POS标签类别间差异显著，名词等类别最易受攻击，且机制分析揭示了关键token数量和内容融合的差异；代码已公开在https://github.com/shahariar-shibli/Adversarial-Attack-on-POS-Tags。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Findings of the EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.15381v1",
      "published_date": "2024-09-21 09:19:55 UTC",
      "updated_date": "2024-09-21 09:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:13:56.227571"
    },
    {
      "arxiv_id": "2409.14071v2",
      "title": "N-Version Assessment and Enhancement of Generative AI",
      "title_zh": "N-Version 生成式 AI 的评估与增强",
      "authors": [
        "Marcus Kessel",
        "Colin Atkinson"
      ],
      "abstract": "Generative AI (GAI) holds great potential to improve software engineering\nproductivity, but its untrustworthy outputs, particularly in code synthesis,\npose significant challenges. The need for extensive verification and validation\n(V&V) of GAI-generated artifacts may undermine the potential productivity\ngains. This paper proposes a way of mitigating these risks by exploiting GAI's\nability to generate multiple versions of code and tests to facilitate\ncomparative analysis across versions. Rather than relying on the quality of a\nsingle test or code module, this \"differential GAI\" (D-GAI) approach promotes\nmore reliable quality evaluation through version diversity. We introduce the\nLarge-Scale Software Observatorium (LASSO), a platform that supports D-GAI by\nexecuting and analyzing large sets of code versions and tests. We discuss how\nLASSO enables rigorous evaluation of GAI-generated artifacts and propose its\napplication in both software development and GAI research.",
      "tldr_zh": "这篇论文探讨了 Generative AI (GAI) 在软件工程中的潜在问题，特别是其输出不可靠（如代码合成），导致需要大量 verification and validation (V&V) 工作从而削弱生产力收益。论文提出 differential GAI (D-GAI) 方法，利用 GAI 生成多个代码和测试版本进行比较分析，以通过版本多样性提升质量评估的可靠性。作者引入 Large-Scale Software Observatorium (LASSO) 平台，支持大规模执行和分析这些版本，帮助进行更严格的 GAI 生成工件的评估。最终，这为软件开发和 GAI 研究提供了更可信的增强框架。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.1; D.2.4; I.2.2; I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "This work has been accepted for publication in an upcoming issue of\n  IEEE Software. This work has been submitted to the IEEE for possible\n  publication",
      "pdf_url": "http://arxiv.org/pdf/2409.14071v2",
      "published_date": "2024-09-21 09:00:16 UTC",
      "updated_date": "2024-09-30 07:35:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:14:18.006040"
    },
    {
      "arxiv_id": "2409.14066v1",
      "title": "KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data",
      "title_zh": "翻译失败",
      "authors": [
        "Grace Tang",
        "Swetha Rajkumar",
        "Yifei Zhou",
        "Homer Rich Walke",
        "Sergey Levine",
        "Kuan Fang"
      ],
      "abstract": "Building generalist robotic systems involves effectively endowing robots with\nthe capabilities to handle novel objects in an open-world setting. Inspired by\nthe advances of large pre-trained models, we propose Keypoint Affordance\nLearning from Imagined Environments (KALIE), which adapts pre-trained Vision\nLanguage Models (VLMs) for robotic control in a scalable manner. Instead of\ndirectly producing motor commands, KALIE controls the robot by predicting\npoint-based affordance representations based on natural language instructions\nand visual observations of the scene. The VLM is trained on 2D images with\naffordances labeled by humans, bypassing the need for training data collected\non robotic systems. Through an affordance-aware data synthesis pipeline, KALIE\nautomatically creates massive high-quality training data based on limited\nexample data manually collected by humans. We demonstrate that KALIE can learn\nto robustly solve new manipulation tasks with unseen objects given only 50\nexample data points. Compared to baselines using pre-trained VLMs, our approach\nconsistently achieves superior performance.",
      "tldr_zh": "该研究提出KALIE方法，用于在不依赖机器人数据的情况下微调Vision Language Models (VLMs)，以实现开放世界环境下的机器人操控任务。KALIE通过预测基于关键点的affordance表示来控制机器人，结合自然语言指令和视觉观察，并利用一个affordance-aware数据合成管道，仅基于少量人类标注数据（如50个示例）自动生成海量高质量训练数据。实验结果显示，KALIE能稳健地处理新任务和未见物体，性能显著优于基线模型。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.14066v1",
      "published_date": "2024-09-21 08:45:16 UTC",
      "updated_date": "2024-09-21 08:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:14:29.414090"
    },
    {
      "arxiv_id": "2409.14051v1",
      "title": "GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion",
      "title_zh": "GroupDebate：使用群体讨论增强多智能体辩论的效率",
      "authors": [
        "Tongxuan Liu",
        "Xingyu Wang",
        "Weizhe Huang",
        "Wenjiang Xu",
        "Yuting Zeng",
        "Lei Jiang",
        "Hailong Yang",
        "Jing Li"
      ],
      "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across diverse NLP tasks. Extensive research has explored how to\nenhance the logical reasoning abilities such as Chain-of-Thought,\nChain-of-Thought with Self-Consistency, Tree-Of-Thoughts, and multi-agent\ndebates. In the context of multi-agent debates, significant performance\nimprovements can be achieved with an increasing number of agents and debate\nrounds. However, the escalation in the number of agents and debate rounds can\ndrastically raise the tokens cost of debates, thereby limiting the scalability\nof the multi-agent debate technique. To better harness the advantages of\nmulti-agent debates in logical reasoning tasks, this paper proposes a method to\nsignificantly reduce token cost in multi-agent debates. This approach involves\ndividing all agents into multiple debate groups, with agents engaging in\ndebates within their respective groups and sharing interim debate results\nbetween groups. Comparative experiments across multiple datasets have\ndemonstrated that this method can reduce the total tokens by up to 51.7% during\ndebates and while potentially enhancing accuracy by as much as 25%. Our method\nsignificantly enhances the performance and efficiency of interactions in the\nmulti-agent debate.",
      "tldr_zh": "这篇论文针对多智能体辩论(multi-agent debates)的高 token 成本问题，提出 GroupDebate 方法，通过将代理分成多个辩论组，让代理在组内进行讨论并分享中间结果，从而显著降低整体 token 使用。\n该方法在多个数据集上进行比较实验，成功减少了高达 51.7% 的 token 成本，同时可能将准确率提升至 25%。\nGroupDebate 增强了 LLMs 在逻辑推理任务中的性能和效率，为大规模应用提供了更可扩展的框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.14051v1",
      "published_date": "2024-09-21 07:49:38 UTC",
      "updated_date": "2024-09-21 07:49:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:14:31.645957"
    },
    {
      "arxiv_id": "2409.14050v1",
      "title": "The use of GPT-4o and Other Large Language Models for the Improvement and Design of Self-Assessment Scales for Measurement of Interpersonal Communication Skills",
      "title_zh": "翻译失败",
      "authors": [
        "Goran Bubaš"
      ],
      "abstract": "OpenAI's ChatGPT (GPT-4 and GPT-4o) and other Large Language Models (LLMs)\nlike Microsoft's Copilot, Google's Gemini 1.5 Pro, and Antrophic's Claude 3.5\nSonnet can be effectively used in various phases of scientific research. Their\nperformance in diverse verbal tasks and reasoning is close to or above the\naverage human level and rapidly increasing, providing those models with a\ncapacity that resembles a relatively high level of theory of mind. The current\nability of LLMs to process information about human psychology and communication\ncreates an opportunity for their scientific use in the fields of personality\npsychology and interpersonal communication skills. This article illustrates the\npossible uses of GPT-4o and other advanced LLMs for typical tasks in designing\nself-assessment scales for interpersonal communication skills measurement like\nthe selection and improvement of scale items and evaluation of content validity\nof scales. The potential for automated item generation and application is\nillustrated as well. The case study examples are accompanied by prompts for\nLLMs that can be useful for these purposes. Finally, a summary is provided of\nthe potential benefits of using LLMs in the process of evaluation, design, and\nimprovement of interpersonal communication skills self-assessment scales.",
      "tldr_zh": "这篇论文探讨了利用 GPT-4o 和其他大型语言模型（LLMs，如 Copilot、Gemini 1.5 Pro 和 Claude 3.5 Sonnet）来改进和设计人际沟通技能的自评量表（self-assessment scales）。研究展示了这些模型在量表开发过程中的应用，包括选择和优化量表项目、评估内容效度（content validity），以及自动生成项目，并提供了具体提示示例。结果表明，LLMs 的性能接近或超过人类平均水平，能显著提升量表设计的效率和质量，为人格心理学和人际沟通研究提供新工具。",
      "categories": [
        "cs.AI",
        "I.2.7; J.4"
      ],
      "primary_category": "cs.AI",
      "comment": "41 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.14050v1",
      "published_date": "2024-09-21 07:37:21 UTC",
      "updated_date": "2024-09-21 07:37:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:14:44.323079"
    },
    {
      "arxiv_id": "2409.16321v1",
      "title": "WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Junchao Gong",
        "Tao Han",
        "Kang Chen",
        "Lei Bai"
      ],
      "abstract": "Numerical Weather Prediction (NWP) system is an infrastructure that exerts\nconsiderable impacts on modern society.Traditional NWP system, however,\nresolves it by solving complex partial differential equations with a huge\ncomputing cluster, resulting in tons of carbon emission. Exploring efficient\nand eco-friendly solutions for NWP attracts interest from Artificial\nIntelligence (AI) and earth science communities. To narrow the performance gap\nbetween the AI-based methods and physic predictor, this work proposes a new\ntransformer-based NWP framework, termed as WeatherFormer, to model the complex\nspatio-temporal atmosphere dynamics and empowering the capability of\ndata-driven NWP. WeatherFormer innovatively introduces the space-time\nfactorized transformer blocks to decrease the parameters and memory\nconsumption, in which Position-aware Adaptive Fourier Neural Operator (PAFNO)\nis proposed for location sensible token mixing. Besides, two data augmentation\nstrategies are utilized to boost the performance and decrease training\nconsumption. Extensive experiments on WeatherBench dataset show WeatherFormer\nachieves superior performance over existing deep learning methods and further\napproaches the most advanced physical model.",
      "tldr_zh": "这篇论文提出 WeatherFormer，一种基于 Transformer 的框架，旨在通过数据驱动的方法提升全球数值天气预报 (NWP) 的效率和环保性，以解决传统 NWP 系统计算密集和碳排放高的难题。WeatherFormer 创新性地采用 space-time factorized transformer blocks 和 Position-aware Adaptive Fourier Neural Operator (PAFNO) 来建模复杂的时空大气动态，同时利用两种数据增强策略减少参数、内存消耗并提升性能。在 WeatherBench 数据集上的实验表明，该框架超越了现有深度学习方法，并接近最先进的物理预测模型。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.16321v1",
      "published_date": "2024-09-21 07:02:31 UTC",
      "updated_date": "2024-09-21 07:02:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:14:55.922535"
    },
    {
      "arxiv_id": "2409.14040v1",
      "title": "PepINVENT: Generative peptide design beyond the natural amino acids",
      "title_zh": "PepINVENT：超越天然氨基酸的生成式肽设计",
      "authors": [
        "Gökçe Geylan",
        "Jon Paul Janet",
        "Alessandro Tibo",
        "Jiazhen He",
        "Atanas Patronov",
        "Mikhail Kabeshov",
        "Florian David",
        "Werngard Czechtizky",
        "Ola Engkvist",
        "Leonardo De Maria"
      ],
      "abstract": "Peptides play a crucial role in the drug design and discovery whether as a\ntherapeutic modality or a delivery agent. Non-natural amino acids (NNAAs) have\nbeen used to enhance the peptide properties from binding affinity, plasma\nstability to permeability. Incorporating novel NNAAs facilitates the design of\nmore effective peptides with improved properties. The generative models used in\nthe field, have focused on navigating the peptide sequence space. The sequence\nspace is formed by combinations of a predefined set of amino acids. However,\nthere is still a need for a tool to explore the peptide landscape beyond this\nenumerated space to unlock and effectively incorporate de novo design of new\namino acids. To thoroughly explore the theoretical chemical space of the\npeptides, we present PepINVENT, a novel generative AI-based tool as an\nextension to the small molecule molecular design platform, REINVENT. PepINVENT\nnavigates the vast space of natural and non-natural amino acids to propose\nvalid, novel, and diverse peptide designs. The generative model can serve as a\ncentral tool for peptide-related tasks, as it was not trained on peptides with\nspecific properties or topologies. The prior was trained to understand the\ngranularity of peptides and to design amino acids for filling the masked\npositions within a peptide. PepINVENT coupled with reinforcement learning\nenables the goal-oriented design of peptides using its chemistry-informed\ngenerative capabilities. This study demonstrates PepINVENT's ability to explore\nthe peptide space with unique and novel designs, and its capacity for property\noptimization in the context of therapeutically relevant peptides. Our tool can\nbe employed for multi-parameter learning objectives, peptidomimetics, lead\noptimization, and variety of other tasks within the peptide domain.",
      "tldr_zh": "本文介绍了PepINVENT，一种基于AI的生成工具，扩展自REINVENT平台，用于超越自然氨基酸的肽设计，允许探索非天然氨基酸（NNAAs）空间以生成有效、新颖和多样的肽序列。PepINVENT通过强化学习（Reinforcement Learning）实现目标导向设计，能够理解肽的粒度并优化属性，如结合亲和力、血浆稳定性和渗透性，而无需针对特定属性进行训练。研究展示了该工具在治疗相关肽中的属性优化能力，并适用于多参数学习、peptidomimetics和lead optimization等任务。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14040v1",
      "published_date": "2024-09-21 06:53:03 UTC",
      "updated_date": "2024-09-21 06:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:15:08.585898"
    },
    {
      "arxiv_id": "2409.14038v5",
      "title": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Zhangcheng Qiang",
        "Kerry Taylor",
        "Weiqing Wang",
        "Jing Jiang"
      ],
      "abstract": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在本体匹配 (Ontology Matching, OM) 任务中常见的幻觉问题，开发了 OAEI-LLM 数据集作为 OAEI 数据集的扩展版，用于评估和理解这些幻觉。数据集通过构建方法和模式扩展来专门针对 LLMs 的特性进行设计，并提供了潜在用例示例以展示其应用。OAEI-LLM 的推出有助于提升 OM 任务的基准测试标准，促进更可靠的模型性能分析。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 figure, 1 table, 1 code snippet",
      "pdf_url": "http://arxiv.org/pdf/2409.14038v5",
      "published_date": "2024-09-21 06:49:34 UTC",
      "updated_date": "2025-02-01 06:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:15:20.539101"
    },
    {
      "arxiv_id": "2409.14037v1",
      "title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
      "title_zh": "翻译失败",
      "authors": [
        "Prasoon Bajpai",
        "Niladri Chatterjee",
        "Subhabrata Dutta",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Large Language Models (LLMs) and AI assistants driven by these models are\nexperiencing exponential growth in usage among both expert and amateur users.\nIn this work, we focus on evaluating the reliability of current LLMs as science\ncommunicators. Unlike existing benchmarks, our approach emphasizes assessing\nthese models on scientific questionanswering tasks that require a nuanced\nunderstanding and awareness of answerability. We introduce a novel dataset,\nSCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific\nconcepts, along with a benchmarking suite that evaluates LLMs for correctness\nand consistency across various criteria. We benchmark three proprietary LLMs\nfrom the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2,\nLlama-3, and Mistral families. While most open-access models significantly\nunderperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a\nstrong competitor, often surpassing GPT-4 Turbo in various evaluation aspects.\nWe also find that even the GPT models exhibit a general incompetence in\nreliably verifying LLM responses. Moreover, we observe an alarming trend where\nhuman evaluators are deceived by incorrect responses from GPT-4 Turbo.",
      "tldr_zh": "本文评估了大型语言模型（LLMs）作为科学传播者的可靠性，焦点在于其在复杂科学问答任务中的正确性、一致性和答案可回答性。研究引入了新数据集SCiPS-QA，包含742个Yes/No查询，并对OpenAI GPT系列的三个专有模型以及Meta Llama-2、Llama-3和Mistral系列的13个开源模型进行了基准测试。结果显示，大多数开源模型逊色于GPT-4 Turbo，但Llama-3-70B在某些方面表现更优；此外，LLMs在自验证响应上存在缺陷，且人类评估者容易被GPT-4 Turbo的错误回答误导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14037v1",
      "published_date": "2024-09-21 06:48:32 UTC",
      "updated_date": "2024-09-21 06:48:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:15:34.720770"
    },
    {
      "arxiv_id": "2409.14026v3",
      "title": "Uncovering Latent Chain of Thought Vectors in Language Models",
      "title_zh": "揭示语言模型中的潜在",
      "authors": [
        "Jason Zhang",
        "Scott Viteri"
      ],
      "abstract": "In this work, we examine how targeted perturbations in the activation space\nof Language Models (LMs) can encode complex reasoning patterns. We inject\nsteering vectors, derived from LM activations, into LMs during inference time\nand study whether these vectors can induce Chain-of-Thought (CoT) reasoning in\nLMs without the need for natural language prompting. We demonstrate this\napproach on Llama3 8B Instruct and Mistral 7B v0.2 Instruct and show that\nactivation-space interventions achieve competitive, if not superior,\nperformance compared to traditional CoT prompting across multiple reasoning\nbenchmarks, including GSM8k, MMLU, AGI Eval, and ARC AI2. These findings\nsuggest that neural network activations can encode reasoning patterns, offering\na new application of activation space manipulation as a tool for tuning model\nbehavior.",
      "tldr_zh": "本研究探讨了如何通过在语言模型激活空间中注入 steering vectors 来诱导 Chain-of-Thought (CoT) 推理，从而编码复杂推理模式，而无需依赖自然语言提示。实验在 Llama3 8B Instruct 和 Mistral 7B v0.2 Instruct 模型上进行，结果显示这种激活空间干预方法在 GSM8k、MMLU、AGI Eval 和 ARC AI2 等基准测试中，性能与传统 CoT 提示相当或更优。研究发现，神经网络激活可以隐含地编码推理模式，为模型行为调整提供一种新型工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This work was presented at the Workshop on Neural Network Weights as\n  a New Data Modality at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.14026v3",
      "published_date": "2024-09-21 05:58:07 UTC",
      "updated_date": "2025-03-20 20:41:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:15:46.942540"
    },
    {
      "arxiv_id": "2410.02810v3",
      "title": "StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolai Rozanov",
        "Marek Rei"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as autonomous agents,\ntackling tasks from robotics to web navigation. Their performance depends on\nthe underlying base agent. Existing methods, however, struggle with\nlong-context reasoning and goal adherence. We introduce StateAct, a novel and\nefficient base agent that enhances decision-making through (1) self-prompting,\nwhich reinforces task goals at every step, and (2) chain-of-states, an\nextension of chain-of-thought that tracks state information over time. StateAct\noutperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30%\non Textcraft, and 7% on Webshop across multiple frontier LLMs. We also\ndemonstrate that StateAct can be used as a drop-in replacement for ReAct with\nadvanced LLM agent methods such as test-time scaling, yielding an additional\n12% gain on Textcraft. By improving efficiency and long-range reasoning without\nrequiring additional training or retrieval, StateAct provides a scalable\nfoundation for LLM agents. We open source our code to support further research\nat https://github.com/ai-nikolai/stateact .",
      "tldr_zh": "本文提出 StateAct，一种新型基代理框架，用于提升大语言模型 (LLMs) 的决策能力，通过 self-prompting 在每个步骤强化任务目标，以及 chain-of-states 扩展 chain-of-thought 以跟踪状态信息随时间变化。相比于 ReAct，StateAct 在 Alfworld 上提升超过 10%、Textcraft 上 30%、Webshop 上 7%，并可作为 ReAct 的直接替换，与测试时缩放等高级方法结合，在 Textcraft 上额外获得 12% 性能提升。该框架提高了效率和长程推理能力，而无需额外训练或检索，并已开源以支持进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 pages appendix, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.02810v3",
      "published_date": "2024-09-21 05:54:35 UTC",
      "updated_date": "2025-04-08 06:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:16:00.298854"
    },
    {
      "arxiv_id": "2409.14023v2",
      "title": "FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs",
      "title_zh": "翻译失败",
      "authors": [
        "Ehsan Kabir",
        "Md. Arafat Kabir",
        "Austin R. J. Downey",
        "Jason D. Bakos",
        "David Andrews",
        "Miaoqing Huang"
      ],
      "abstract": "Transformer neural networks (TNNs) are being applied across a widening range\nof application domains, including natural language processing (NLP), machine\ntranslation, and computer vision (CV). Their popularity is largely attributed\nto the exceptional performance of their multi-head self-attention blocks when\nanalyzing sequential data and extracting features. To date, there are limited\nhardware accelerators tailored for this mechanism, which is the first step\nbefore designing an accelerator for a complete model. This paper proposes\n\\textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention\n(MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is\noptimized for high utilization of processing elements and on-chip memories to\nimprove parallelism and reduce latency. An efficient tiling of large matrices\nhas been employed to distribute memory and computing resources across different\nmodules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C\nand U200 data center cards containing Ultrascale+ FPGAs. Experimental results\nare presented that show that it can attain a maximum throughput, number of\nparallel attention heads, embedding dimension and tile size of 328 (giga\noperations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore,\nit is 3.28$\\times$ and 2.6$\\times$ faster than the Intel Xeon Gold 5220R CPU\nand NVIDIA V100 GPU respectively. It is also 1.3$\\times$ faster than the\nfastest state-of-the-art FPGA-based accelerator.",
      "tldr_zh": "该论文提出 FAMOUS，一种灵活的硬件加速器，针对 Transformer 神经网络的多头自注意力（MHA）机制，在 UltraScale+ FPGAs 上进行优化，以提高处理元素的利用率、片上内存效率并增强并行性。FAMOUS 通过高效矩阵平铺技术分配内存和计算资源，适用于自然语言处理（NLP）、机器翻译和计算机视觉（CV）等领域。实验结果显示，在 Xilinx Alveo U55C 上，它实现了 328 GOPS 的最高吞吐量、8 个并行注意力头和 768 的嵌入维度，比 Intel Xeon Gold 5220R CPU 快 3.28 倍、比 NVIDIA V100 GPU 快 2.6 倍，并比最先进的 FPGA 加速器快 1.3 倍。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "arXiv admin note: text overlap with arXiv:2409.13975",
      "pdf_url": "http://arxiv.org/pdf/2409.14023v2",
      "published_date": "2024-09-21 05:25:46 UTC",
      "updated_date": "2024-10-21 05:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:16:11.580829"
    },
    {
      "arxiv_id": "2409.14021v1",
      "title": "BrainDreamer: Reasoning-Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Ling Wang",
        "Chen Wu",
        "Lin Wang"
      ],
      "abstract": "Can we directly visualize what we imagine in our brain together with what we\ndescribe? The inherent nature of human perception reveals that, when we think,\nour body can combine language description and build a vivid picture in our\nbrain. Intuitively, generative models should also hold such versatility. In\nthis paper, we introduce BrainDreamer, a novel end-to-end language-guided\ngenerative framework that can mimic human reasoning and generate high-quality\nimages from electroencephalogram (EEG) brain signals. Our method is superior in\nits capacity to eliminate the noise introduced by non-invasive EEG data\nacquisition and meanwhile achieve a more precise mapping between the EEG and\nimage modality, thus leading to significantly better-generated images.\nSpecifically, BrainDreamer consists of two key learning stages: 1) modality\nalignment and 2) image generation. In the alignment stage, we propose a novel\nmask-based triple contrastive learning strategy to effectively align EEG, text,\nand image embeddings to learn a unified representation. In the generation\nstage, we inject the EEG embeddings into the pre-trained Stable Diffusion model\nby designing a learnable EEG adapter to generate high-quality\nreasoning-coherent images. Moreover, BrainDreamer can accept textual\ndescriptions (e.g., color, position, etc.) to achieve controllable image\ngeneration. Extensive experiments show that our method significantly\noutperforms prior arts in terms of generating quality and quantitative\nperformance.",
      "tldr_zh": "论文提出了 BrainDreamer，一种端到端的语言引导生成框架，能够从 EEG 脑信号生成高质量、推理一致的图像，并模仿人类结合语言描述的思维过程。框架包括两个关键阶段：首先，通过 mask-based triple contrastive learning 策略对齐 EEG、文本和图像嵌入，实现精确的模态映射并减少 EEG 数据噪声；其次，利用 learnable EEG adapter 将 EEG 嵌入注入预训练的 Stable Diffusion 模型，支持基于文本描述（如颜色、位置）的可控图像生成。实验结果表明，BrainDreamer 在生成质量和量化性能上显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14021v1",
      "published_date": "2024-09-21 05:16:31 UTC",
      "updated_date": "2024-09-21 05:16:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:16:22.724672"
    },
    {
      "arxiv_id": "2409.14019v1",
      "title": "MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors",
      "title_zh": "MOSE：利用 NeRF 提升的噪声先验进行单目语义重建",
      "authors": [
        "Zhenhua Du",
        "Binbin Xu",
        "Haoyu Zhang",
        "Kai Huo",
        "Shuaifeng Zhi"
      ],
      "abstract": "Accurately reconstructing dense and semantically annotated 3D meshes from\nmonocular images remains a challenging task due to the lack of geometry\nguidance and imperfect view-dependent 2D priors. Though we have witnessed\nrecent advancements in implicit neural scene representations enabling precise\n2D rendering simply from multi-view images, there have been few works\naddressing 3D scene understanding with monocular priors alone. In this paper,\nwe propose MOSE, a neural field semantic reconstruction approach to lift\ninferred image-level noisy priors to 3D, producing accurate semantics and\ngeometry in both 3D and 2D space. The key motivation for our method is to\nleverage generic class-agnostic segment masks as guidance to promote local\nconsistency of rendered semantics during training. With the help of semantics,\nwe further apply a smoothness regularization to texture-less regions for better\ngeometric quality, thus achieving mutual benefits of geometry and semantics.\nExperiments on the ScanNet dataset show that our MOSE outperforms relevant\nbaselines across all metrics on tasks of 3D semantic segmentation, 2D semantic\nsegmentation and 3D surface reconstruction.",
      "tldr_zh": "该论文提出MOSE，一种基于NeRF-lifted noisy priors的神经场语义重建方法，用于从单目图像中提升图像级噪声先验，从而实现准确的3D和2D语义及几何重建。MOSE的关键创新是利用通用的类无关分割掩码作为指导，促进训练期间渲染语义的局部一致性，并通过语义辅助的平滑正则化改善无纹理区域的几何质量，实现几何和语义的互利提升。在ScanNet数据集上的实验显示，MOSE在3D语义分割、2D语义分割和3D表面重建任务上优于相关基线模型的所有指标。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.14019v1",
      "published_date": "2024-09-21 05:12:13 UTC",
      "updated_date": "2024-09-21 05:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:16:34.539757"
    },
    {
      "arxiv_id": "2410.03688v1",
      "title": "LLM Agents as 6G Orchestrator: A Paradigm for Task-Oriented Physical-Layer Automation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoran Xiao",
        "Chenhui Ye",
        "Yunbo Hu",
        "Honggang Yuan",
        "Yihang Huang",
        "Yijia Feng",
        "Liyu Cai",
        "Jiang Chang"
      ],
      "abstract": "The rapid advancement in generative pre-training models is propelling a\nparadigm shift in technological progression from basic applications such as\nchatbots towards more sophisticated agent-based systems. It is with huge\npotential and necessity that the 6G system be combined with the copilot of\nlarge language model (LLM) agents and digital twins (DT) to manage the highly\ncomplicated communication system with new emerging features such as native AI\nservice and sensing. With the 6G-oriented agent, the base station could\nunderstand the transmission requirements of various dynamic upper-layer tasks,\nautomatically orchestrate the optimal system workflow. Through continuously get\nfeedback from the 6G DT for reinforcement, the agents can finally raise the\nperformance of practical system accordingly. Differing from existing LLM agents\ndesigned for general application, the 6G-oriented agent aims to make highly\nrigorous and precise planning with a vast amount of extra expert knowledge,\nwhich inevitably requires a specific system design from model training to\nimplementation. This paper proposes a novel comprehensive approach for building\ntask-oriented 6G LLM agents. We first propose a two-stage continual\npre-training and fine-tuning scheme to build the field basic model and\ndiversities of specialized expert models for meeting the requirements of\nvarious application scenarios. Further, a novel inference framework based on\nsemantic retrieval for leveraging the existing communication-related functions\nis proposed. Experiment results of exemplary tasks, such as physical-layer task\ndecomposition, show the proposed paradigm's feasibility and effectiveness.",
      "tldr_zh": "该论文提出了一种将 LLM Agents 作为 6G 系统编排器的全新范式，旨在实现任务导向的物理层自动化，通过结合 LLM Agents 和数字孪生 (DT) 来管理复杂通信系统。方法包括两阶段的持续预训练和微调，以构建领域基础模型和多种专家模型，并引入基于语义检索的推理框架来优化系统工作流和性能反馈。实验结果证明，该范式在物理层任务分解等示例任务上显示出显著的可行性和有效性，有望提升 6G 系统的整体效率。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03688v1",
      "published_date": "2024-09-21 05:08:29 UTC",
      "updated_date": "2024-09-21 05:08:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:16:48.769560"
    },
    {
      "arxiv_id": "2409.14016v1",
      "title": "Enhancing Multivariate Time Series-based Solar Flare Prediction with Multifaceted Preprocessing and Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "MohammadReza EskandariNasab",
        "Shah Muhammad Hamdi",
        "Soukaina Filali Boubrahimi"
      ],
      "abstract": "Accurate solar flare prediction is crucial due to the significant risks that\nintense solar flares pose to astronauts, space equipment, and satellite\ncommunication systems. Our research enhances solar flare prediction by\nutilizing advanced data preprocessing and classification methods on a\nmultivariate time series-based dataset of photospheric magnetic field\nparameters. First, our study employs a novel preprocessing pipeline that\nincludes missing value imputation, normalization, balanced sampling, near\ndecision boundary sample removal, and feature selection to significantly boost\nprediction accuracy. Second, we integrate contrastive learning with a GRU\nregression model to develop a novel classifier, termed ContReg, which employs\ndual learning methodologies, thereby further enhancing prediction performance.\nTo validate the effectiveness of our preprocessing pipeline, we compare and\ndemonstrate the performance gain of each step, and to demonstrate the efficacy\nof the ContReg classifier, we compare its performance to that of sequence-based\ndeep learning architectures, machine learning models, and findings from\nprevious studies. Our results illustrate exceptional True Skill Statistic (TSS)\nscores, surpassing previous methods and highlighting the critical role of\nprecise data preprocessing and classifier development in time series-based\nsolar flare prediction.",
      "tldr_zh": "本研究通过多方面预处理和对比学习（contrastive learning）方法，增强了基于多变量时间序列（multivariate time series）的太阳耀斑预测，以降低对宇航员、太空设备和卫星通信系统的风险。具体而言，该方法包括一个新颖的预处理管道——缺失值填充（missing value imputation）、归一化（normalization）、平衡采样（balanced sampling）、移除近决策边界样本（near decision boundary sample removal）和特征选择（feature selection），显著提高了预测准确性。随后，研究开发了 ContReg 分类器，将对比学习与 GRU 回归模型（GRU regression）结合，使用双重学习方法进一步优化性能。实验结果显示，该方法在 True Skill Statistic (TSS) 得分上超过了序列-based 深度学习模型、机器学习模型和先前研究，突显了精确数据预处理的必要性。",
      "categories": [
        "astro-ph.SR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "This work has been accepted at ICMLA 2024 on September 7, 2024, as a\n  regular paper for an oral presentation",
      "pdf_url": "http://arxiv.org/pdf/2409.14016v1",
      "published_date": "2024-09-21 05:00:34 UTC",
      "updated_date": "2024-09-21 05:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:16:58.862693"
    },
    {
      "arxiv_id": "2409.14014v1",
      "title": "Mitigating Exposure Bias in Score-Based Generation of Molecular Conformations",
      "title_zh": "翻译失败",
      "authors": [
        "Sijia Wang",
        "Chen Wang",
        "Zhenhao Zhao",
        "Jiqiang Zhang",
        "Weiran Cai"
      ],
      "abstract": "Molecular conformation generation poses a significant challenge in the field\nof computational chemistry. Recently, Diffusion Probabilistic Models (DPMs) and\nScore-Based Generative Models (SGMs) are effectively used due to their capacity\nfor generating accurate conformations far beyond conventional physics-based\napproaches. However, the discrepancy between training and inference rises a\ncritical problem known as the exposure bias. While this issue has been\nextensively investigated in DPMs, the existence of exposure bias in SGMs and\nits effective measurement remain unsolved, which hinders the use of\ncompensation methods for SGMs, including ConfGF and Torsional Diffusion as the\nrepresentatives. In this work, we first propose a method for measuring exposure\nbias in SGMs used for molecular conformation generation, which confirms the\nsignificant existence of exposure bias in these models and measures its value.\nWe design a new compensation algorithm Input Perturbation (IP), which is\nadapted from a method originally designed for DPMs only. Experimental results\nshow that by introducing IP, SGM-based molecular conformation models can\nsignificantly improve both the accuracy and diversity of the generated\nconformations. Especially by using the IP-enhanced Torsional Diffusion model,\nwe achieve new state-of-the-art performance on the GEOM-Drugs dataset and are\non par on GEOM-QM9. We provide the code publicly at\nhttps://github.com/jia-975/torsionalDiff-ip.",
      "tldr_zh": "本研究针对分子构象生成中的 exposure bias 问题，提出了一种测量 Score-Based Generative Models (SGMs) 中 exposure bias 的新方法，确认其在 SGMs 中的显著存在。该方法基于从 Diffusion Probabilistic Models (DPMs) 改编的 Input Perturbation (IP) 算法，来补偿 SGMs 的训练与推理不一致性，从而提升生成构象的准确性和多样性。实验结果显示，应用 IP 增强的 Torsional Diffusion 模型，在 GEOM-Drugs 数据集上实现了新的 state-of-the-art 性能，并在 GEOM-QM9 数据集上与最佳模型持平，为计算化学中的生成模型提供了重要改进。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "SMC 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.14014v1",
      "published_date": "2024-09-21 04:54:37 UTC",
      "updated_date": "2024-09-21 04:54:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:17:09.578997"
    },
    {
      "arxiv_id": "2409.14013v1",
      "title": "ChronoGAN: Supervised and Embedded Generative Adversarial Networks for Time Series Generation",
      "title_zh": "翻译失败",
      "authors": [
        "MohammadReza EskandariNasab",
        "Shah Muhammad Hamdi",
        "Soukaina Filali Boubrahimi"
      ],
      "abstract": "Generating time series data using Generative Adversarial Networks (GANs)\npresents several prevalent challenges, such as slow convergence, information\nloss in embedding spaces, instability, and performance variability depending on\nthe series length. To tackle these obstacles, we introduce a robust framework\naimed at addressing and mitigating these issues effectively. This advanced\nframework integrates the benefits of an Autoencoder-generated embedding space\nwith the adversarial training dynamics of GANs. This framework benefits from a\ntime series-based loss function and oversight from a supervisory network, both\nof which capture the stepwise conditional distributions of the data\neffectively. The generator functions within the latent space, while the\ndiscriminator offers essential feedback based on the feature space. Moreover,\nwe introduce an early generation algorithm and an improved neural network\narchitecture to enhance stability and ensure effective generalization across\nboth short and long time series. Through joint training, our framework\nconsistently outperforms existing benchmarks, generating high-quality time\nseries data across a range of real and synthetic datasets with diverse\ncharacteristics.",
      "tldr_zh": "本论文提出ChronoGAN框架，一种监督式和嵌入式GANs，用于解决时间序列生成中的挑战，如慢速收敛、信息丢失、不稳定性和序列长度依赖性。该框架整合Autoencoder生成的嵌入空间、GANs对抗训练、基于时间序列的损失函数以及监督网络，来捕捉数据的逐步条件分布，并通过早期生成算法和改进神经网络架构提升稳定性和泛化能力。实验结果表明，ChronoGAN通过联合训练，在多种真实和合成数据集上显著超越现有基准，生成高质量的时间序列数据。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted at ICMLA 2024 on September 7, 2024, as a\n  regular paper for an oral presentation",
      "pdf_url": "http://arxiv.org/pdf/2409.14013v1",
      "published_date": "2024-09-21 04:51:35 UTC",
      "updated_date": "2024-09-21 04:51:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:17:22.079926"
    },
    {
      "arxiv_id": "2409.14012v3",
      "title": "Test Time Learning for Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Panayiotis Christou",
        "Shichu Chen",
        "Xupeng Chen",
        "Parijat Dube"
      ],
      "abstract": "Time-series forecasting has seen significant advancements with the\nintroduction of token prediction mechanisms such as multi-head attention.\nHowever, these methods often struggle to achieve the same performance as in\nlanguage modeling, primarily due to the quadratic computational cost and the\ncomplexity of capturing long-range dependencies in time-series data.\nState-space models (SSMs), such as Mamba, have shown promise in addressing\nthese challenges by offering efficient solutions with linear RNNs capable of\nmodeling long sequences with larger context windows. However, there remains\nroom for improvement in accuracy and scalability.\n  We propose the use of Test-Time Training (TTT) modules in a parallel\narchitecture to enhance performance in long-term time series forecasting.\nThrough extensive experiments on standard benchmark datasets, we demonstrate\nthat TTT modules consistently outperform state-of-the-art models, including the\nMamba-based TimeMachine, particularly in scenarios involving extended sequence\nand prediction lengths. Our results show significant improvements in Mean\nSquared Error (MSE) and Mean Absolute Error (MAE), especially on larger\ndatasets such as Electricity, Traffic, and Weather, underscoring the\neffectiveness of TTT in capturing long-range dependencies. Additionally, we\nexplore various convolutional architectures within the TTT framework, showing\nthat even simple configurations like 1D convolution with small filters can\nachieve competitive results. This work sets a new benchmark for time-series\nforecasting and lays the groundwork for future research in scalable,\nhigh-performance forecasting models.",
      "tldr_zh": "这篇论文针对时间序列预测中的计算成本高和捕捉长程依赖性困难等问题，提出使用 Test-Time Training (TTT) 模块在并行架构中提升长期预测性能。相比于 State-space models (SSMs) 如 Mamba，TTT 通过实验在标准基准数据集上表现出色，尤其在长序列和长预测长度场景下，显著降低了 Mean Squared Error (MSE) 和 Mean Absolute Error (MAE)。结果显示，TTT 模块在大型数据集如 Electricity, Traffic 和 Weather 上比现有模型（如 Mamba-based TimeMachine）提高了29%以上，甚至简单的 1D 卷积配置也能取得竞争性效果。该工作设定了时间序列预测的新基准，并为开发可扩展、高性能的预测模型奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14012v3",
      "published_date": "2024-09-21 04:40:08 UTC",
      "updated_date": "2024-11-30 19:40:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:17:36.382792"
    },
    {
      "arxiv_id": "2409.16320v3",
      "title": "Developing a Thailand solar irradiance map using Himawari-8 satellite imageries and deep learning models",
      "title_zh": "翻译失败",
      "authors": [
        "Suwichaya Suwanwimolkul",
        "Natanon Tongamrak",
        "Nuttamon Thungka",
        "Naebboon Hoonchareon",
        "Jitkomut Songsiri"
      ],
      "abstract": "This paper presents an online platform showing Thailand solar irradiance map\nevery 30 minutes, available at https://www.cusolarforecast.com. The methodology\nfor estimating global horizontal irradiance (GHI) across Thailand relies on\ncloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky\nmodel with locally-tuned Linke turbidity, and machine learning models. The\nmethods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature\ndata from the MERRA-2 database, and date-time as inputs for GHI estimation\nmodels, including LightGBM, LSTM, Informer, and Transformer. These are\nbenchmarked with the estimate from a commercial service X by evaluation of\n15-minute ground GHI data from 53 ground stations over 1.5 years during\n2022-2023. The results show that the four models exhibit comparable overall MAE\nperformance to the service X. The best model is LightGBM with an overall MAE of\n78.58 W/sqm and RMSE of 118.97 W/sqm, while the service X achieves the lowest\nMAE, RMSE, and MBE in cloudy condition. Obtaining re-analyzed MERRA-2 data for\nthe whole Thailand region is not economically feasible for deployment. When\nremoving these features, the Informer model has a winning performance in MAE of\n78.67 W/sqm. The obtained performance aligns with existing literature by taking\nthe climate zone and time granularity of data into consideration. As the map\nshows an estimate of GHI over 93,000 grids with a frequent update, the paper\nalso describes a computational framework for displaying the entire map. It\ntests the runtime performance of deep learning models in the GHI estimation\nprocess.",
      "tldr_zh": "本论文开发了一个在线平台（https://www.cusolarforecast.com），利用Himawari-8卫星图像和深度学习模型，每30分钟生成泰国全球水平辐照度(GHI)地图。方法结合云指数提取、Ineichen clear-sky模型及本地调整的Linke浊度，作为输入训练LightGBM、LSTM、Informer和Transformer模型，并使用MERRA-2数据库的数据进行GHI估计。实验结果显示，LightGBM模型表现出色，整体MAE为78.58 W/sqm，与商业服务X相当；在移除MERRA-2数据后，Informer模型的MAE为78.67 W/sqm最佳，且该框架在93,000网格上实现了高效计算和频繁更新。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "23 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.16320v3",
      "published_date": "2024-09-21 03:45:05 UTC",
      "updated_date": "2024-12-05 07:14:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:17:48.289308"
    },
    {
      "arxiv_id": "2409.14001v1",
      "title": "Boolean Product Graph Neural Networks",
      "title_zh": "布尔乘积图神经网络",
      "authors": [
        "Ziyan Wang",
        "Bin Liu",
        "Ling Xiang"
      ],
      "abstract": "Graph Neural Networks (GNNs) have recently achieved significant success, with\na key operation involving the aggregation of information from neighboring\nnodes. Substantial researchers have focused on defining neighbors for\naggregation, predominantly based on observed adjacency matrices. However, in\nmany scenarios, the explicitly given graphs contain noise, which can be\namplified during the messages-passing process. Therefore, many researchers have\nturned their attention to latent graph inference, specifically learning a\nparametric graph. To mitigate fluctuations in latent graph structure learning,\nthis paper proposes a novel Boolean product-based graph residual connection in\nGNNs to link the latent graph and the original graph. It computes the Boolean\nproduct between the latent graph and the original graph at each layer to\ncorrect the learning process. The Boolean product between two adjacency\nmatrices is equivalent to triangle detection. Accordingly, the proposed Boolean\nproduct graph neural networks can be interpreted as discovering triangular\ncliques from the original and the latent graph. We validate the proposed method\nin benchmark datasets and demonstrate its ability to enhance the performance\nand robustness of GNNs.",
      "tldr_zh": "这篇论文针对图神经网络 (GNNs) 在邻居聚合过程中因图噪声而导致性能下降的问题，提出了一种新型 Boolean product-based graph residual connection 方法。该方法通过在每个层计算潜在图和原始图的 Boolean product（相当于三角形检测），来连接两者并修正学习过程，从而发现三角形团簇并增强模型的鲁棒性。实验在基准数据集上验证了该方法的有效性，显著提高了 GNNs 的性能和整体表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2407.10688",
      "pdf_url": "http://arxiv.org/pdf/2409.14001v1",
      "published_date": "2024-09-21 03:31:33 UTC",
      "updated_date": "2024-09-21 03:31:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:17:58.171425"
    },
    {
      "arxiv_id": "2409.14000v1",
      "title": "Graph Neural Network Framework for Sentiment Analysis Using Syntactic Feature",
      "title_zh": "基于句法特征的图神经网络框架用于情感分析",
      "authors": [
        "Linxiao Wu",
        "Yuanshuai Luo",
        "Binrong Zhu",
        "Guiran Liu",
        "Rui Wang",
        "Qian Yu"
      ],
      "abstract": "Amidst the swift evolution of social media platforms and e-commerce\necosystems, the domain of opinion mining has surged as a pivotal area of\nexploration within natural language processing. A specialized segment within\nthis field focuses on extracting nuanced evaluations tied to particular\nelements within textual contexts. This research advances a composite framework\nthat amalgamates the positional cues of topical descriptors. The proposed\nsystem converts syntactic structures into a matrix format, leveraging\nconvolutions and attention mechanisms within a graph to distill salient\ncharacteristics. Incorporating the positional relevance of descriptors relative\nto lexical items enhances the sequential integrity of the input. Trials have\nsubstantiated that this integrated graph-centric scheme markedly elevates the\nefficacy of evaluative categorization, showcasing preeminence.",
      "tldr_zh": "该研究提出了一种基于图神经网络(Graph Neural Network)的框架，用于利用句法特征(Syntactic Feature)进行情感分析(Sentiment Analysis)，旨在从文本中提取针对特定元素的细微评估。框架将句法结构转换为矩阵形式，通过图中的卷积(convolutions)和注意力机制(attention mechanisms)提取关键特征，并整合描述符相对于词汇的位置相关性以提升输入的顺序完整性。实验结果表明，该图中心方法显著提高了评估分类的有效性，在意见挖掘领域表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.14000v1",
      "published_date": "2024-09-21 03:30:59 UTC",
      "updated_date": "2024-09-21 03:30:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:18:10.026763"
    },
    {
      "arxiv_id": "2409.13998v2",
      "title": "Relevance-driven Decision Making for Safer and More Efficient Human Robot Collaboration",
      "title_zh": "相关性驱动的决策用于更安全和更",
      "authors": [
        "Xiaotong Zhang",
        "Dingcheng Huang",
        "Kamal Youcef-Toumi"
      ],
      "abstract": "Human brain possesses the ability to effectively focus on important\nenvironmental components, which enhances perception, learning, reasoning, and\ndecision-making. Inspired by this cognitive mechanism, we introduced a novel\nconcept termed relevance for Human-Robot Collaboration (HRC). Relevance is a\ndimensionality reduction process that incorporates a continuously operating\nperception module, evaluates cue sufficiency within the scene, and applies a\nflexible formulation and computation framework. In this paper, we present an\nenhanced two-loop framework that integrates real-time and asynchronous\nprocessing to quantify relevance and leverage it for safer and more efficient\nhuman-robot collaboration (HRC). The two-loop framework integrates an\nasynchronous loop, which leverages LLM world knowledge to quantify relevance,\nand a real-time loop, which performs scene understanding, human intent\nprediction, and decision-making based on relevance. HRC decision-making is\nenhanced by a relevance-based task allocation method, as well as a motion\ngeneration and collision avoidance approach that incorporates human trajectory\nprediction. Simulations and experiments show that our methodology for relevance\nquantification can accurately and robustly predict the human objective and\nrelevance, with an average accuracy of up to 0.90 for objective prediction and\nup to 0.96 for relevance prediction. Moreover, our motion generation\nmethodology reduces collision cases by 63.76% and collision frames by 44.74%\nwhen compared with a state-of-the-art (SOTA) collision avoidance method. Our\nframework and methodologies, with relevance, guide the robot on how to best\nassist humans and generate safer and more efficient actions for HRC.",
      "tldr_zh": "本文受人类大脑认知机制启发，引入了“relevance”概念作为一种降维过程，用于提升人机协作 (HRC) 的安全性和效率。研究提出一个增强的两环框架，包括异步环利用 LLM 的世界知识量化 relevance，以及实时环进行场景理解、人类意图预测和基于 relevance 的决策。决策方法通过 relevance 驱动的任务分配和运动生成，实现了碰撞避免；实验结果显示，目标预测准确率达 0.90，relevance 预测达 0.96，并分别减少了 63.76% 的碰撞案例和 44.74% 的碰撞帧。该框架使机器人能够更有效地辅助人类，提供更可靠的 HRC 行动。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13998v2",
      "published_date": "2024-09-21 03:20:53 UTC",
      "updated_date": "2025-04-18 18:40:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:18:23.344661"
    },
    {
      "arxiv_id": "2409.13997v1",
      "title": "Drift to Remember",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Du",
        "Xinhe Zhang",
        "Hao Shen",
        "Xun Xian",
        "Ganghua Wang",
        "Jiawei Zhang",
        "Yuhong Yang",
        "Na Li",
        "Jia Liu",
        "Jie Ding"
      ],
      "abstract": "Lifelong learning in artificial intelligence (AI) aims to mimic the\nbiological brain's ability to continuously learn and retain knowledge, yet it\nfaces challenges such as catastrophic forgetting. Recent neuroscience research\nsuggests that neural activity in biological systems undergoes representational\ndrift, where neural responses evolve over time, even with consistent inputs and\ntasks. We hypothesize that representational drift can alleviate catastrophic\nforgetting in AI during new task acquisition. To test this, we introduce\nDriftNet, a network designed to constantly explore various local minima in the\nloss landscape while dynamically retrieving relevant tasks. This approach\nensures efficient integration of new information and preserves existing\nknowledge. Experimental studies in image classification and natural language\nprocessing demonstrate that DriftNet outperforms existing models in lifelong\nlearning. Importantly, DriftNet is scalable in handling a sequence of tasks\nsuch as sentiment analysis and question answering using large language models\n(LLMs) with billions of parameters on a single Nvidia A100 GPU. DriftNet\nefficiently updates LLMs using only new data, avoiding the need for full\ndataset retraining. Tested on GPT-2 and RoBERTa, DriftNet is a robust,\ncost-effective solution for lifelong learning in LLMs. This study not only\nadvances AI systems to emulate biological learning, but also provides insights\ninto the adaptive mechanisms of biological neural systems, deepening our\nunderstanding of lifelong learning in nature.",
      "tldr_zh": "该研究探讨了人工智能（AI）中的终身学习（lifelong learning），受神经科学中表征漂移（representational drift）的启发，提出假设这种机制可缓解灾难性遗忘（catastrophic forgetting）。他们开发了DriftNet，一种网络设计，用于在损失景观中持续探索局部最小值并动态检索相关任务，从而高效整合新信息并保留现有知识。实验结果显示，DriftNet在图像分类和自然语言处理任务中优于现有模型，并在大型语言模型（LLMs）如GPT-2和RoBERTa上实现可扩展性，使用单个Nvidia A100 GPU仅需新数据更新，避免完整数据集重训。该方法不仅提升了AI的生物学习模拟能力，还为理解生物神经系统的适应机制提供了新洞见。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13997v1",
      "published_date": "2024-09-21 03:18:44 UTC",
      "updated_date": "2024-09-21 03:18:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:18:35.821592"
    },
    {
      "arxiv_id": "2409.13994v2",
      "title": "Contrastive Learning for Knowledge-Based Question Generation in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenhong Zhang",
        "Jiajing Chen",
        "Weiyan Shi",
        "Lingjie Yi",
        "Chihang Wang",
        "Qian Yu"
      ],
      "abstract": "With the rapid development of artificial intelligence technology, especially\nthe increasingly widespread application of question-and-answer systems,\nhigh-quality question generation has become a key component in supporting the\ndevelopment of these systems. This article focuses on knowledge-based question\ngeneration technology, which aims to enable computers to simulate the human\nquestioning process based on understanding specific texts or knowledge bases.\nIn light of the issues of hallucination and knowledge gaps present in\nlarge-scale language models when applied to knowledge-intensive tasks, this\npaper proposes an enhanced question generation method that incorporates\ncontrastive learning. This method utilizes multiple models to jointly mine\ndomain knowledge and uses contrastive learning to guide the model in reducing\nnoise and hallucinations in generation. Experimental results show that by\ndesigning prompts containing contrasting examples, the model's performance in\nquestion generation improves considerably, particularly when contrasting\ninstructions and examples are used simultaneously, leading to the highest\nquality of generated questions and improved accuracy. These results demonstrate\nthat the method proposed in this study, which combines contrasting context and\nchain-of-thought prompts, can effectively improve both the quality and the\npracticality of question generation.",
      "tldr_zh": "这篇论文针对大型语言模型（Large Language Models）在知识密集任务中存在的幻觉和知识缺口问题，提出了一种增强的知识为基础的问题生成方法，结合对比学习（Contrastive Learning）。该方法通过多个模型联合挖掘领域知识，并利用对比示例和提示设计（如对比指令和示例）来减少生成过程中的噪声和幻觉，从而提升模型性能。实验结果表明，这种方法显著提高了问题生成的质量和准确性，特别是当同时使用对比上下文和链式思维提示时，生成的问句更具实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13994v2",
      "published_date": "2024-09-21 03:09:10 UTC",
      "updated_date": "2024-09-26 22:24:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:18:49.284703"
    },
    {
      "arxiv_id": "2409.13989v1",
      "title": "ChemEval: A Comprehensive Multi-Level Chemical Evaluation for Large Language Models",
      "title_zh": "ChemEval：针对大型语言模型的全面多层次化学评估",
      "authors": [
        "Yuqing Huang",
        "Rongyang Zhang",
        "Xuesong He",
        "Xuyang Zhi",
        "Hao Wang",
        "Xin Li",
        "Feiyang Xu",
        "Deguang Liu",
        "Huadong Liang",
        "Yi Li",
        "Jian Cui",
        "Zimu Liu",
        "Shijin Wang",
        "Guoping Hu",
        "Guiquan Liu",
        "Qi Liu",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "There is a growing interest in the role that LLMs play in chemistry which\nlead to an increased focus on the development of LLMs benchmarks tailored to\nchemical domains to assess the performance of LLMs across a spectrum of\nchemical tasks varying in type and complexity. However, existing benchmarks in\nthis domain fail to adequately meet the specific requirements of chemical\nresearch professionals. To this end, we propose \\textbf{\\textit{ChemEval}},\nwhich provides a comprehensive assessment of the capabilities of LLMs across a\nwide range of chemical domain tasks. Specifically, ChemEval identified 4\ncrucial progressive levels in chemistry, assessing 12 dimensions of LLMs across\n42 distinct chemical tasks which are informed by open-source data and the data\nmeticulously crafted by chemical experts, ensuring that the tasks have\npractical value and can effectively evaluate the capabilities of LLMs. In the\nexperiment, we evaluate 12 mainstream LLMs on ChemEval under zero-shot and\nfew-shot learning contexts, which included carefully selected demonstration\nexamples and carefully designed prompts. The results show that while general\nLLMs like GPT-4 and Claude-3.5 excel in literature understanding and\ninstruction following, they fall short in tasks demanding advanced chemical\nknowledge. Conversely, specialized LLMs exhibit enhanced chemical competencies,\nalbeit with reduced literary comprehension. This suggests that LLMs have\nsignificant potential for enhancement when tackling sophisticated tasks in the\nfield of chemistry. We believe our work will facilitate the exploration of\ntheir potential to drive progress in chemistry. Our benchmark and analysis will\nbe available at {\\color{blue} \\url{https://github.com/USTC-StarTeam/ChemEval}}.",
      "tldr_zh": "这篇论文提出了 ChemEval，一种全面的多级化学评估基准，用于评估大型语言模型 (LLMs) 在化学领域的性能，旨在解决现有基准无法满足化学研究专业人士需求的问题。ChemEval 涵盖 4 个关键水平、12 个维度和 42 个任务，这些任务基于开源数据和化学专家精心设计，确保其实用性和有效性。在实验中，评估了 12 个主流 LLMs 在 zero-shot 和 few-shot 学习场景下，结果显示通用模型如 GPT-4 和 Claude-3.5 在文献理解和指令遵循上表现出色，但高级化学知识任务表现不足，而专业 LLMs 在化学能力上更强，表明 LLMs 在化学领域有显著提升潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "physics.chem-ph",
        "q-bio.BM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13989v1",
      "published_date": "2024-09-21 02:50:43 UTC",
      "updated_date": "2024-09-21 02:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:18:59.528251"
    },
    {
      "arxiv_id": "2409.13980v1",
      "title": "Enhancing Advanced Visual Reasoning Ability of Large Language Models",
      "title_zh": "增强大型语言模型的高级视觉推理能力",
      "authors": [
        "Zhiyuan Li",
        "Dongnan Liu",
        "Chaoyi Zhang",
        "Heng Wang",
        "Tengfei Xue",
        "Weidong Cai"
      ],
      "abstract": "Recent advancements in Vision-Language (VL) research have sparked new\nbenchmarks for complex visual reasoning, challenging models' advanced reasoning\nability. Traditional Vision-Language Models (VLMs) perform well in visual\nperception tasks while struggling with complex reasoning scenarios. Conversely,\nLarge Language Models (LLMs) demonstrate robust text reasoning capabilities;\nhowever, they lack visual acuity. To bridge this gap, we propose Complex Visual\nReasoning Large Language Models (CVR-LLM), capitalizing on VLMs' visual\nperception proficiency and LLMs' extensive reasoning capability. Unlike recent\nmultimodal large language models (MLLMs) that require a projection layer, our\napproach transforms images into detailed, context-aware descriptions using an\niterative self-refinement loop and leverages LLMs' text knowledge for accurate\npredictions without extra training. We also introduce a novel multi-modal\nin-context learning (ICL) methodology to enhance LLMs' contextual understanding\nand reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a\nstep-by-step comparison technique enabling contrasting various aspects of\npredictions. Our CVR-LLM presents the first comprehensive study across a wide\narray of complex visual reasoning tasks and achieves SOTA performance among\nall.",
      "tldr_zh": "本研究针对传统 Vision-Language Models (VLMs) 在复杂视觉推理上的不足，以及 Large Language Models (LLMs) 的视觉感知缺失，提出了一种 Complex Visual Reasoning Large Language Models (CVR-LLM) 框架，利用 VLMs 的视觉感知能力和 LLMs 的文本推理能力。CVR-LLM 通过迭代自精炼循环将图像转化为详细的上下文感知描述，并引入多模态 in-context learning (ICL) 和 Chain-of-Comparison (CoC) 技术，实现预测过程的准确性和逐步比较，而无需额外训练。该方法在广泛的复杂视觉推理任务上实现了 SOTA 性能，填补了现有模型的空白。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2409.13980v1",
      "published_date": "2024-09-21 02:10:19 UTC",
      "updated_date": "2024-09-21 02:10:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:19:11.363996"
    },
    {
      "arxiv_id": "2409.13976v2",
      "title": "Detecting Inpainted Video with Frequency Domain Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Quanhui Tang",
        "Jingtao Cao"
      ],
      "abstract": "Video inpainting enables seamless content removal and replacement within\nframes, posing ethical and legal risks when misused. To mitigate these risks,\ndetecting manipulated regions in inpainted videos is critical. Previous\ndetection methods often focus solely on the characteristics derived from\nspatial and temporal dimensions, which limits their effectiveness by\noverlooking the unique frequency characteristics of different inpainting\nalgorithms. In this paper, we propose the Frequency Domain Insights Network\n(FDIN), which significantly enhances detection accuracy by incorporating\ninsights from the frequency domain. Our network features an Adaptive Band\nSelective Response module to discern frequency characteristics specific to\nvarious inpainting techniques and a Fast Fourier Convolution-based Attention\nmodule for identifying periodic artifacts in inpainted regions. Utilizing 3D\nResBlocks for spatiotemporal analysis, FDIN progressively refines detection\nprecision from broad assessments to detailed localization. Experimental\nevaluations on public datasets demonstrate that FDIN achieves state-of-the-art\nperformance, setting a new benchmark in video inpainting detection.",
      "tldr_zh": "该论文针对视频修复（inpainting）可能带来的伦理和法律风险，提出了一种新型检测方法，以解决现有方法忽略频率域特性的局限。论文引入 Frequency Domain Insights Network (FDIN)，该网络通过 Adaptive Band Selective Response 模块识别不同修复算法的频率特性，以及 Fast Fourier Convolution-based Attention 模块检测修复区域的周期性伪影，同时利用 3D ResBlocks 进行时空分析，从整体评估到精确定位逐步提升检测精度。实验结果显示，FDIN 在公共数据集上实现了最先进性能，显著提高了视频篡改检测的准确性和基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.9; I.2.10; I.5.1; K.4.1"
      ],
      "primary_category": "cs.CV",
      "comment": "Unsatisfied with this job",
      "pdf_url": "http://arxiv.org/pdf/2409.13976v2",
      "published_date": "2024-09-21 01:51:07 UTC",
      "updated_date": "2024-12-22 14:03:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:19:22.981667"
    },
    {
      "arxiv_id": "2409.13975v1",
      "title": "ProTEA: Programmable Transformer Encoder Acceleration on FPGA",
      "title_zh": "翻译失败",
      "authors": [
        "Ehsan Kabir",
        "Jason D. Bakos",
        "David Andrews",
        "Miaoqing Huang"
      ],
      "abstract": "Transformer neural networks (TNN) have been widely utilized on a diverse\nrange of applications, including natural language processing (NLP), machine\ntranslation, and computer vision (CV). Their widespread adoption has been\nprimarily driven by the exceptional performance of their multi-head\nself-attention block used to extract key features from sequential data. The\nmulti-head self-attention block is followed by feedforward neural networks,\nwhich play a crucial role in introducing non-linearity to assist the model in\nlearning complex patterns. Despite the popularity of TNNs, there has been\nlimited numbers of hardware accelerators targeting these two critical blocks.\nMost prior works have concentrated on sparse architectures that are not\nflexible for popular TNN variants. This paper introduces \\textit{ProTEA}, a\nruntime programmable accelerator tailored for the dense computations of most of\nstate-of-the-art transformer encoders. \\textit{ProTEA} is designed to reduce\nlatency by maximizing parallelism. We introduce an efficient tiling of large\nmatrices that can distribute memory and computing resources across different\nhardware components within the FPGA. We provide run time evaluations of\n\\textit{ProTEA} on a Xilinx Alveo U55C high-performance data center accelerator\ncard. Experimental results demonstrate that \\textit{ProTEA} can host a wide\nrange of popular transformer networks and achieve near optimal performance with\na tile size of 64 in the multi-head self-attention block and 6 in the\nfeedforward networks block when configured with 8 parallel attention heads, 12\nlayers, and an embedding dimension of 768 on the U55C. Comparative results are\nprovided showing \\textit{ProTEA} is 2.5$\\times$ faster than an NVIDIA Titan XP\nGPU. Results also show that it achieves 1.3 -- 2.8$\\times$ speed up compared\nwith current state-of-the-art custom designed FPGA accelerators.",
      "tldr_zh": "本论文提出ProTEA，一种运行时可编程的FPGA加速器，针对Transformer神经网络的稠密计算，特别是multi-head self-attention和feedforward neural networks块，以提升处理效率。ProTEA通过高效矩阵平铺和最大化并行性来分配FPGA的内存和计算资源，从而减少延迟，并支持多种流行Transformer网络。实验结果显示，在Xilinx Alveo U55C上，ProTEA比NVIDIA Titan XP GPU快2.5倍，并比现有FPGA加速器实现1.3-2.8倍的速度提升。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13975v1",
      "published_date": "2024-09-21 01:44:13 UTC",
      "updated_date": "2024-09-21 01:44:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:19:35.384358"
    },
    {
      "arxiv_id": "2409.13959v1",
      "title": "One Model, Any Conjunctive Query: Graph Neural Networks for Answering Complex Queries over Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Krzysztof Olejniczak",
        "Xingyue Huang",
        "İsmail İlkan Ceylan",
        "Mikhail Galkin"
      ],
      "abstract": "Traditional query answering over knowledge graphs -- or broadly over\nrelational data -- is one of the most fundamental problems in data management.\nMotivated by the incompleteness of modern knowledge graphs, a new setup for\nquery answering has emerged, where the goal is to predict answers that do not\nnecessarily appear in the knowledge graph, but are present in its completion.\nIn this work, we propose AnyCQ, a graph neural network model that can classify\nanswers to any conjunctive query on any knowledge graph, following training. At\nthe core of our framework lies a graph neural network model trained using a\nreinforcement learning objective to answer Boolean queries. Our approach and\nproblem setup differ from existing query answering studies in multiple\ndimensions. First, we focus on the problem of query answer classification:\ngiven a query and a set of possible answers, classify these proposals as true\nor false relative to the complete knowledge graph. Second, we study the problem\nof query answer retrieval: given a query, retrieve an answer to the query\nrelative to the complete knowledge graph or decide that no correct solutions\nexist. Trained on simple, small instances, AnyCQ can generalize to large\nqueries of arbitrary structure, reliably classifying and retrieving answers to\nsamples where existing approaches fail, which is empirically validated on new\nand challenging benchmarks. Furthermore, we demonstrate that our AnyCQ models\neffectively transfer to out-of-distribution knowledge graphs, when equipped\nwith a relevant link predictor, highlighting their potential to serve as a\ngeneral engine for query answering.",
      "tldr_zh": "本研究提出 AnyCQ，一种基于 Graph Neural Networks 的模型，用于在知识图谱上回答任意 Conjunctive Query，包括分类查询答案（判断给定答案的真假）和检索查询答案（查找正确答案或确认无解）。该模型通过强化学习训练来处理布尔查询，并在简单小实例上训练后实现泛化，能够处理复杂查询结构。实验结果显示，AnyCQ 在新基准测试中显著优于现有方法，并在转移到其他知识图谱时表现出色，当配备相关链接预测器时，成为高效的通用查询引擎。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13959v1",
      "published_date": "2024-09-21 00:30:44 UTC",
      "updated_date": "2024-09-21 00:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:19:48.373278"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 52,
  "processed_papers_count": 52,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T02:20:05.891069"
}
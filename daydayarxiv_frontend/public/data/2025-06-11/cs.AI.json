{
  "date": "2025-06-11",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ ä»¬çš„ AI ç ”ç©¶å‘˜æœ‹å‹ã€‚æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-11 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å¯è°“æ˜¯ä¼—æ˜Ÿäº‘é›†ä¸”è¯é¢˜çˆ†ç‚¸ã€‚**Meta FAIR å›¢é˜Ÿï¼ˆYann LeCun é¢†è¡”ï¼‰å‘å¸ƒäº† V-JEPA 2**ï¼Œå‘ç€â€œä¸–ç•Œæ¨¡å‹â€çš„ç‰©ç†ç†è§£è¿ˆè¿›äº†ä¸€å¤§æ­¥ï¼›**OpenAI çš„ o3 æ¨¡å‹åœ¨çƒ­åŠ›å­¦è€ƒè¯•ä¸­å±•ç°äº†â€œè¶…å­¦ç”Ÿâ€çš„æ™ºåŠ›**ï¼Œå¼•å‘äº†å…³äº AI æ¨ç†èƒ½åŠ›çš„æ–°è®¨è®ºã€‚æ­¤å¤–ï¼Œå…³äº **System 2 æ€ç»´ï¼ˆæ…¢æ€è€ƒ/æ¨ç†ï¼‰**ã€**è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰** ä»¥åŠ **æœºå™¨é—å¿˜ï¼ˆUnlearningï¼‰** çš„è„†å¼±æ€§æ˜¯ä»Šå¤©å­¦æœ¯ç•Œå…³æ³¨çš„ç»å¯¹ç„¦ç‚¹ã€‚\n\n---\n\n### ğŸš€ é‡ç£…å¿…è¯» (Headliners)\n\n**1. [CV/World Model] V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning**\n*   **V-JEPA 2ï¼šè‡ªç›‘ç£è§†é¢‘æ¨¡å‹èµ‹èƒ½ç†è§£ã€é¢„æµ‹ä¸è§„åˆ’**\n*   **Authors:** Mido Assran, Yann LeCun, Nicolas Ballas et al. (Meta FAIR)\n*   **å…³é”®è¯:** V-JEPA 2, World Model, Self-Supervised Learning, Robotic Planning\n*   **TLDR:** Yann LeCun å›¢é˜Ÿæ–°ä½œã€‚æå‡ºäº† V-JEPA 2ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº**è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAï¼‰**çš„æ— åŠ¨ä½œï¼ˆaction-freeï¼‰è§†é¢‘æ¨¡å‹ã€‚å®ƒåœ¨ 100 ä¸‡å°æ—¶çš„äº’è”ç½‘è§†é¢‘ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç»“åˆå°‘é‡æœºå™¨äººäº¤äº’æ•°æ®ï¼Œå°±èƒ½åœ¨ç‰©ç†ä¸–ç•Œä¸­è¿›è¡Œè§„åˆ’ã€‚\n*   **è´¡çŒ®:** V-JEPA 2 ä¸ä»…åœ¨åŠ¨ä½œé¢„æµ‹ä¸Šåˆ·æ–° SOTAï¼Œè¿˜èƒ½é€šè¿‡é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰æ–¹å¼åœ¨ Franka æœºæ¢°è‡‚ä¸Šå®ç°æŠ“å–å’Œæ”¾ç½®ï¼Œ**æ— éœ€ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±æˆ–è®­ç»ƒæ•°æ®**ã€‚è¿™æ˜¯å‘é€šç”¨çš„ç‰©ç†ä¸–ç•Œæ¨¡å‹è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚\n\n**2. [Reasoning/Evaluation] Superstudent intelligence in thermodynamics**\n*   **çƒ­åŠ›å­¦ä¸­çš„â€œè¶…å­¦ç”Ÿâ€æ™ºèƒ½**\n*   **Authors:** Rebecca Loubet, Hans Hasse et al.\n*   **å…³é”®è¯:** OpenAI o3, Thermodynamics, Reasoning, Zero-shot\n*   **TLDR:** è¿™ç¯‡æ–‡ç« æŠ¥å‘Šäº†ä¸€ä¸ªæƒŠäººçš„ç»“æœï¼š**OpenAI çš„ o3 æ¨¡å‹åœ¨ä¸€åœºå¤§å­¦çƒ­åŠ›å­¦è€ƒè¯•ä¸­å‡»è´¥äº†æ‰€æœ‰äººç±»å­¦ç”Ÿ**ã€‚\n*   **å‘ç°:** çƒ­åŠ›å­¦è€ƒè¯•é€šå¸¸éœ€è¦åˆ›é€ æ€§åœ°ç»“åˆåŸç†è€Œéæ­»è®°ç¡¬èƒŒã€‚o3 åœ¨é›¶æ ·æœ¬æ¨¡å¼ä¸‹è§£å†³äº†æ‰€æœ‰é—®é¢˜ï¼Œå¾—åˆ†ä¸ä»…é«˜äºæœ¬å±Šæ‰€æœ‰å­¦ç”Ÿï¼Œä¹Ÿå¤„äºè¯¥æ ¡ 1985 å¹´ä»¥æ¥ 10,000 å¤šä»½è¯•å·ä¸­çš„æœ€é«˜åˆ†æ¢¯é˜Ÿã€‚è¿™æ„å‘³ç€ AI åœ¨å¤æ‚çš„ã€éæ¨¡å¼åŒ¹é…çš„ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šå·²è¾¾åˆ°äººç±»é¡¶å°–æ°´å¹³ã€‚\n\n**3. [Safety/Unlearning] Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods**\n*   **æç¤ºæ”»å‡»æ­ç¤ºäº†é—å¿˜æ–¹æ³•ä¸­ä»…æ˜¯è¡¨é¢çŸ¥è¯†çš„ç§»é™¤**\n*   **Authors:** Yeonwoo Jang, Diogo Cruz et al.\n*   **å…³é”®è¯:** Machine Unlearning, Prompt Attacks, Safety\n*   **TLDR:** æ³¼å†·æ°´çš„ç ”ç©¶ã€‚ä½œè€…å‘ç°ç›®å‰çš„æœºå™¨é—å¿˜ï¼ˆUnlearningï¼‰æŠ€æœ¯ï¼ˆå¦‚ ELMï¼‰éå¸¸è„†å¼±ã€‚\n*   **å‘ç°:** ä»…ä»…é€šè¿‡ç®€å•çš„æç¤ºæ”»å‡»ï¼ˆä¾‹å¦‚åœ¨æç¤ºè¯å‰åŠ å°åœ°è¯­å¡«å……æ–‡æœ¬ï¼‰ï¼Œå°±èƒ½æ¢å¤ 57.3% æœ¬è¯¥è¢«â€œé—å¿˜â€çš„çŸ¥è¯†ã€‚è¿™è¡¨æ˜å½“å‰çš„æ¨¡å‹å¯èƒ½å¹¶æ²¡æœ‰çœŸæ­£åˆ é™¤çŸ¥è¯†ï¼Œåªæ˜¯å‹åˆ¶äº†è¾“å‡ºï¼Œ**è¿™ä¹Ÿæ˜¯ç›®å‰å¼€æºæ¨¡å‹å®‰å…¨å¯¹é½çš„ä¸€å¤§éšæ‚£**ã€‚\n\n**4. [Multimodal/SOTA] Ming-Omni: A Unified Multimodal Model for Perception and Generation**\n*   **Ming-Omniï¼šç”¨äºæ„ŸçŸ¥å’Œç”Ÿæˆçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹**\n*   **Authors:** Inclusion AI Team\n*   **å…³é”®è¯:** Unified Multimodal, Any-to-Any, Image Generation, Speech Generation\n*   **TLDR:** è¿™æ˜¯ä¸€ä¸ªè¯•å›¾å¯¹æ ‡ GPT-4o çš„å¼€æºå°è¯•ã€‚Ming-Omni æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œå¹¶æ”¯æŒè¯­éŸ³å’Œå›¾åƒç”Ÿæˆã€‚\n*   **è´¡çŒ®:** æå‡ºäº†ç‰¹å®šçš„æ¨¡æ€è·¯ç”±ï¼ˆmodality-specific routersï¼‰å’Œ Ling MoE æ¶æ„ã€‚å®ƒæ˜¯ç›®å‰å¼€æºç•Œå°‘æ•°èƒ½åŒæ—¶æ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œé«˜è´¨é‡å›¾åƒ/è¯­éŸ³ç”Ÿæˆçš„å…¨èƒ½æ¨¡å‹ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€æ€ç»´é“¾ä¸ System 2 (Reasoning, CoT & System 2)\n\n**5. [Code Reasoning] CoRT: Code-integrated Reasoning within Thinking**\n*   **CoRTï¼šæ€ç»´ä¸­çš„ä»£ç é›†æˆæ¨ç†**\n*   **å…³é”®è¯:** Code Interpreter, Reasoning, System 2, Hint-Engineering\n*   **TLDR:** é’ˆå¯¹ DeepSeek-R1 è¿™ç±»æ¨ç†æ¨¡å‹åœ¨å¤æ‚è®¡ç®—ä¸Šçš„çŸ­æ¿ï¼Œæå‡ºäº† CoRT æ¡†æ¶ã€‚é€šè¿‡ Hint-Engineering æ•™æ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹ä¸­è°ƒç”¨ä»£ç è§£é‡Šå™¨ï¼ˆCode Interpreterï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä¸ä»…æå‡äº†å‡†ç¡®ç‡ï¼Œè¿˜å‡å°‘äº† 30%-50% çš„ token æ¶ˆè€—ã€‚\n\n**6. [PRM] Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**\n*   **Athenaï¼šç”¨æ•°æ®é«˜æ•ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¢å¼ºå¤šæ¨¡æ€æ¨ç†**\n*   **å…³é”®è¯:** Process Reward Model (PRM), Multimodal Reasoning, Test-time Scaling\n*   **TLDR:** åªæœ‰ 5000 ä¸ªæ ·æœ¬å°±èƒ½è®­ç»ƒå‡ºå¼ºå¤§çš„ PRMï¼ŸAthena åˆ©ç”¨å¼±å®Œæˆè€…å’Œå¼ºå®Œæˆè€…ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§æ¥ç”Ÿæˆè¿‡ç¨‹æ ‡ç­¾ã€‚ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œå®ƒæ˜¾è‘—æå‡äº† Qwen2.5-VL åœ¨æ•°å­¦å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„ Test-time Scaling èƒ½åŠ›ã€‚\n\n**7. [PRM Calibration] Know What You Don't Know: Uncertainty Calibration of Process Reward Models**\n*   **çŸ¥ä¹‹ä¸ºçŸ¥ä¹‹ï¼šè¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ ¡å‡†**\n*   **å…³é”®è¯:** PRM Calibration, Instance-adaptive Scaling\n*   **TLDR:** å‘ç°ç›®å‰çš„ PRM å¾€å¾€è¿‡åº¦è‡ªä¿¡ã€‚æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†ä½æ•°å›å½’æ ¡å‡† PRM çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨æ ¡å‡†åçš„ç½®ä¿¡åº¦è¿›è¡Œ**å®ä¾‹è‡ªé€‚åº”ç¼©æ”¾ï¼ˆInstance-adaptive Scalingï¼‰**ï¼Œåœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘æ¨ç†ç®—åŠ›ã€‚\n\n**8. [Safety] Benchmarking Gaslighting Negation Attacks Against Reasoning Models**\n*   **é’ˆå¯¹æ¨ç†æ¨¡å‹çš„â€œç…¤æ°”ç¯â€å¦å®šæ”»å‡»åŸºå‡†æµ‹è¯•**\n*   **å…³é”®è¯:** Gaslighting Attacks, Reasoning Robustness, O4-mini, Claude-3.7\n*   **TLDR:** å³ä½¿æ˜¯ o4-mini å’Œ Claude-3.7 è¿™æ ·å¼ºå¤§çš„æ¨ç†æ¨¡å‹ï¼Œé¢å¯¹â€œç…¤æ°”ç¯æ”»å‡»â€ï¼ˆå³ç”¨æˆ·è‡ªä¿¡åœ°å¦å®šæ­£ç¡®ç­”æ¡ˆï¼‰æ—¶ï¼Œå‡†ç¡®ç‡ä¹Ÿä¼šå¹³å‡ä¸‹é™ 25-29%ã€‚**æ¨ç†èƒ½åŠ›å¼ºå¹¶ä¸ä»£è¡¨ç«‹åœºåšå®š**ã€‚\n\n**9. [Cognitive Science] The Emergence of Abstract Thought in Large Language Models Beyond Any Language**\n*   **LLM ä¸­è¶…è¶Šè¯­è¨€çš„æŠ½è±¡æ€ç»´æ¶Œç°**\n*   **å…³é”®è¯:** Language-agnostic Neurons, Abstract Thought, Multilingual\n*   **TLDR:** LLM æ˜¯ç”¨è‹±è¯­æ€è€ƒå—ï¼Ÿç ”ç©¶å‘ç° LLM é€æ¸å‘å±•å‡ºä¸€ä¸ª**è¯­è¨€æ— å…³çš„æ ¸å¿ƒå‚æ•°ç©ºé—´**ã€‚éšç€è®­ç»ƒè¿›è¡Œï¼Œå…±äº«ç¥ç»å…ƒï¼ˆShared Neuronsï¼‰æ¯”ä¾‹å¢åŠ ï¼Œè¿™äº›ç¥ç»å…ƒæ„æˆäº†æŠ½è±¡æ€ç»´çš„åŸºç¡€ï¼Œä¸ç»‘å®šäºç‰¹å®šè¯­è¨€ã€‚\n\n---\n\n### ğŸ¥ è§†è§‰ç”Ÿæˆä¸ 3D (Vision Generation & 3D)\n\n**10. [3D Reconstruction] DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos**\n*   **DGS-LRMï¼šåŸºäºå•ç›®è§†é¢‘çš„å®æ—¶å¯å˜å½¢ 3D é«˜æ–¯é‡å»º**\n*   **å…³é”®è¯:** 3D Gaussian Splats, Dynamic Scene Reconstruction, Feed-forward\n*   **TLDR:** ç¬¬ä¸€ä¸ªå‰é¦ˆå¼ï¼ˆFeed-forwardï¼‰æ¨¡å‹ï¼Œèƒ½ä»å•ç›®è§†é¢‘é¢„æµ‹å¯å˜å½¢çš„ 3D é«˜æ–¯æ³¼æº…ï¼ˆGaussian Splatsï¼‰ã€‚å®ç°äº†å®æ—¶ã€å¯æ³›åŒ–çš„åŠ¨æ€åœºæ™¯é‡å»ºï¼Œè§£å†³äº†åŠ¨æ€ç‰©ä½“ä¸é™æ€èƒŒæ™¯åˆ†ç¦»åŠé•¿æ—¶é—´è·Ÿè¸ªçš„éš¾é¢˜ã€‚\n\n**11. [Video Generation] Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation**\n*   **ç”¨äºå®æ—¶äº¤äº’å¼è§†é¢‘ç”Ÿæˆçš„è‡ªå›å½’å¯¹æŠ—åè®­ç»ƒ**\n*   **å…³é”®è¯:** Real-time Video Gen, Adversarial Training, Interactive\n*   **TLDR:** æå‡º AAPTï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºå®æ—¶ç”Ÿæˆå™¨ã€‚èƒ½åœ¨å•å¼  H100 ä¸Šä»¥ 24fps çš„é€Ÿåº¦ç”Ÿæˆ 736x416 åˆ†è¾¨ç‡çš„è§†é¢‘æµï¼Œå¹¶æ”¯æŒç”¨æˆ·å®æ—¶äº¤äº’æ§åˆ¶ã€‚\n\n**12. [Image Editing] EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits**\n*   **EditInspectorï¼šæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„è¯„ä¼°åŸºå‡†**\n*   **å…³é”®è¯:** Image Editing Evaluation, Benchmark, Hallucination\n*   **TLDR:**ç›®å‰çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4Vï¼‰åœ¨è¯„ä¼°å›¾åƒç¼–è¾‘è´¨é‡æ—¶ç»å¸¸äº§ç”Ÿå¹»è§‰ã€‚EditInspector åŒ…å«å¤§é‡äººå·¥æ ‡æ³¨ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹ç¼–è¾‘ä¼ªå½±ã€ä¸€è‡´æ€§ç­‰æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†æ¯” SOTA æ›´å¥½çš„æ£€æµ‹æ–¹æ³•ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸å…·èº«æ™ºèƒ½ (Agents & Embodied AI)\n\n**13. [GUI Agents] LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization**\n*   **LPOï¼šé€šè¿‡ä½ç½®åå¥½ä¼˜åŒ–å®ç°ç²¾ç¡®çš„ GUI Agent äº¤äº’**\n*   **å…³é”®è¯:** GUI Agents, Location Preference Optimization, GRPO\n*   **TLDR:** é’ˆå¯¹ GUI Agent ç‚¹å‡»ä½ç½®ä¸å‡†çš„é—®é¢˜ï¼Œæå‡ºäº† LPOã€‚åˆ©ç”¨ä¿¡æ¯ç†µé¢„æµ‹äº¤äº’åŒºåŸŸï¼Œå¹¶å¼•å…¥åŸºäºç‰©ç†è·ç¦»çš„å¥–åŠ±å‡½æ•°ï¼Œç»“åˆ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ˜¾è‘—æå‡äº† Agent åœ¨å±å¹•ä¸Šçš„æ“ä½œç²¾åº¦ã€‚\n\n**14. [Agent Safety] Effective Red-Teaming of Policy-Adherent Agents**\n*   **é’ˆå¯¹éµå¾ªç­–ç•¥çš„ Agent çš„æœ‰æ•ˆçº¢é˜Ÿæµ‹è¯•**\n*   **å…³é”®è¯:** Red-Teaming, Task-oriented Agents, Jailbreak\n*   **TLDR:** å®¢æœç±» Agent å¾€å¾€æœ‰ä¸¥æ ¼çš„é€€æ¬¾/å–æ¶ˆæ”¿ç­–ã€‚æœ¬æ–‡æå‡ºäº† CRAFT ç³»ç»Ÿï¼Œåˆ©ç”¨å¤š Agent çº¢é˜Ÿæµ‹è¯•ï¼Œé€šè¿‡ç­–ç•¥æ„ŸçŸ¥çš„è¯´æœæŠ€å·§æ¥è¯±å¯¼å®¢æœ Agent è¿è§„ï¼Œæ•ˆæœä¼˜äºä¼ ç»Ÿçš„è¶Šç‹±æç¤ºã€‚\n\n**15. [Autonomous Dev] Autonomous Computer Vision Development with Agentic AI**\n*   **åˆ©ç”¨ Agentic AI è¿›è¡Œè‡ªä¸»è®¡ç®—æœºè§†è§‰å¼€å‘**\n*   **å…³é”®è¯:** Agentic AI, Automated Coding, Medical Imaging\n*   **TLDR:** å±•ç¤ºäº†ä¸€ä¸ª Agent ç³»ç»Ÿï¼Œä»…å‡­ä¸€å¥ \"é…ç½®è‚ºéƒ¨ã€å¿ƒè„å’Œè‚‹éª¨åˆ†å‰²\"ï¼Œå°±èƒ½è‡ªä¸»ç¼–å†™ YAML é…ç½®ã€è°ƒç”¨ SimpleMind æ¡†æ¶ã€è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•ï¼Œæœ€ç»ˆåœ¨èƒ¸éƒ¨ X å…‰ç‰‡ä¸Šè¾¾åˆ°äº†æé«˜çš„åˆ†å‰²ç²¾åº¦ã€‚\n\n---\n\n### ğŸ’Š AI for Science & Medicine\n\n**16. [Medical Reasoning] ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning**\n*   **ReasonMedï¼šç”¨äºæ¨è¿›åŒ»å­¦æ¨ç†çš„ 370K å¤šæ™ºèƒ½ä½“ç”Ÿæˆæ•°æ®é›†**\n*   **å…³é”®è¯:** Medical Reasoning, Dataset, CoT\n*   **TLDR:** å‘å¸ƒäº†è¿„ä»Šæœ€å¤§çš„åŒ»å­¦æ¨ç†æ•°æ®é›† ReasonMedï¼ˆ370k æ¡ï¼‰ã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“ç”Ÿæˆã€éªŒè¯å’Œä¿®æ­£æµç¨‹æ„å»ºã€‚åŸºäºæ­¤è®­ç»ƒçš„ 7B æ¨¡å‹åœ¨ PubMedQA ä¸Šç”šè‡³è¶…è¿‡äº† LLaMA3.1-70Bã€‚\n\n**17. [Analog Circuits] LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation**\n*   **LaMAGIC2ï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„æ¨¡æ‹Ÿæ‹“æ‰‘ç”Ÿæˆçš„é«˜çº§ç”µè·¯å…¬å¼**\n*   **å…³é”®è¯:** Analog Circuit Design, EDA, Language Models\n*   **TLDR:** ç”¨ LLM è®¾è®¡æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘ã€‚LaMAGIC2 æ”¹è¿›äº†ç”µè·¯çš„æ–‡æœ¬è¡¨ç¤ºï¼ˆToken length å¤æ‚åº¦é™ä½ï¼‰ï¼Œåœ¨ä¸¥æ ¼å…¬å·®ä¸‹æˆåŠŸç‡æ¯”å‰ä½œæé«˜äº† 34%ï¼Œæ˜¯è‡ªåŠ¨åŒ–èŠ¯ç‰‡è®¾è®¡é¢†åŸŸçš„é‡è¦è¿›å±•ã€‚\n\n---\n\n### ğŸ§© å…¶ä»–æœ‰è¶£çš„ç ”ç©¶ (Quick Hits)\n\n*   **[Music] Fine-Grained control over Music Generation with Activation Steering:** é€šè¿‡å¹²é¢„ MusicGen çš„æ¿€æ´»å±‚ï¼Œå®ç°äº†å¯¹éŸ³ä¹ç”Ÿæˆä¸­éŸ³è‰²ã€é£æ ¼çš„ç»†ç²’åº¦æ§åˆ¶ã€‚\n*   **[Quantum NLP] A quantum semantic framework for natural language processing:** è¿™æ˜¯ä¸€ä¸ªéå¸¸ç¡¬æ ¸çš„å°è¯•ï¼Œç”¨é‡å­åŠ›å­¦ä¸­çš„è´å°”ä¸ç­‰å¼æµ‹è¯•ï¼ˆBell inequality testï¼‰æ¥åˆ†æ LLM çš„è¯­ä¹‰æ­§ä¹‰ï¼Œè®¤ä¸ºè¯­è¨€æ„ä¹‰çš„ç¡®å®šç±»ä¼¼äºé‡å­åç¼©ã€‚\n*   **[Geology] Synthetic Geology:** åˆ©ç”¨æ·±åº¦å­¦ä¹ ç”Ÿæˆé€¼çœŸçš„ 3D åœ°è´¨æ¨¡å‹ï¼Œè§£å†³äº†åœ°ä¸‹çŸ¿äº§å‹˜æ¢æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚\n*   **[Diplo] DipLLM:** é’ˆå¯¹ã€Šå¤–äº¤ã€‹ï¼ˆDiplomacyï¼‰æ¸¸æˆçš„å¾®è°ƒ LLMï¼Œä»…ç”¨ Cicero æ¨¡å‹ 1.5% çš„æ•°æ®é‡å°±è¾¾åˆ°äº†è¶…è¶Šå…¶çš„æ€§èƒ½ã€‚\n\n---\n\nä»Šå¤©å°±èŠåˆ°è¿™é‡Œï¼Œå¸Œæœ›èƒ½å¸®ä½ åœ¨æµ·é‡è®ºæ–‡ä¸­æ‰¾åˆ°çµæ„Ÿï¼æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2506.10236v2",
      "title": "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods",
      "title_zh": "æç¤ºæ”»å‡»æ­ç¤ºæœºå™¨é—å¿˜æ–¹æ³•ä¸­çš„è¡¨å±‚çŸ¥è¯†ç§»é™¤ç°è±¡",
      "authors": [
        "Yeonwoo Jang",
        "Shariqah Hossain",
        "Ashwin Sreevatsa",
        "Diogo Cruz"
      ],
      "abstract": "In this work, we demonstrate that certain machine unlearning methods may fail under straightforward prompt attacks. We systematically evaluate eight unlearning techniques across three model families using output-based, logit-based, and probe analysis to assess the extent to which supposedly unlearned knowledge can be retrieved. While methods like RMU and TAR exhibit robust unlearning, ELM remains vulnerable to specific prompt attacks (e.g., prepending Hindi filler text to the original prompt recovers 57.3% accuracy). Our logit analysis further indicates that unlearned models are unlikely to hide knowledge through changes in answer formatting, given the strong correlation between output and logit accuracy. These findings challenge prevailing assumptions about unlearning effectiveness and highlight the need for evaluation frameworks that can reliably distinguish between genuine knowledge removal and superficial output suppression. To facilitate further research, we publicly release our evaluation framework to easily evaluate prompting techniques to retrieve unlearned knowledge.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é€šè¿‡ prompt attacks æ­ç¤ºäº†æŸäº›æœºå™¨å¸è½½ (machine unlearning) æ–¹æ³•åœ¨çŸ¥è¯†ç§»é™¤æ–¹é¢çš„å±€é™æ€§ï¼Œè¯æ˜å…¶å¯èƒ½ä»…å®ç°äº†è¡¨å±‚æŠ‘åˆ¶è€ŒéçœŸæ­£çš„çŸ¥è¯†æ“¦é™¤ã€‚ç ”ç©¶äººå‘˜ç³»ç»Ÿè¯„ä¼°äº†ä¸‰ä¸ªæ¨¡å‹ç³»åˆ—ä¸­çš„å…«ç§ unlearning æŠ€æœ¯ï¼Œåˆ©ç”¨åŸºäºè¾“å‡ºã€åŸºäº logit å’Œæ¢é’ˆåˆ†æ (probe analysis) è¡¡é‡è¢«åˆ é™¤çŸ¥è¯†çš„å¯æ¢å¤ç¨‹åº¦ã€‚å®éªŒå‘ç°è™½ç„¶ RMU å’Œ TAR è¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§ï¼Œä½† ELM æ–¹æ³•åœ¨ç‰¹å®šæç¤ºæ”»å‡»ï¼ˆå¦‚åœ¨æç¤ºå‰æ·»åŠ å°åœ°è¯­å¡«å……æ–‡æœ¬ï¼‰ä¸‹ï¼ŒåŸæœ¬è¢«å¸è½½çš„çŸ¥è¯†å‡†ç¡®ç‡å¯æ¢å¤è‡³ 57.3%ã€‚åŸºäº logit çš„åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¾“å‡ºå‡†ç¡®ç‡ä¸ logit å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œæ„å‘³ç€æ¨¡å‹å¹¶éç®€å•åœ°é€šè¿‡æ”¹å˜ç­”æ¡ˆæ ¼å¼æ¥éšè—ä¿¡æ¯ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å½“å‰å…³äº unlearning æœ‰æ•ˆæ€§çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘èƒ½åŒºåˆ†çœŸå®çŸ¥è¯†ç§»é™¤ä¸è¡¨é¢è¾“å‡ºæŠ‘åˆ¶ (superficial output suppression) çš„è¯„ä¼°æ¡†æ¶çš„é‡è¦æ€§ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œä½œè€…å…¬å¼€äº†è¯¥è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•å„ç§æç¤ºæŠ€æœ¯åœ¨æ£€ç´¢å·²å¸è½½çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "19 pages, 6 figures. Accepted at COLM 2025 SoLaR Workshop",
      "pdf_url": "https://arxiv.org/pdf/2506.10236v2",
      "published_date": "2025-06-11 23:36:30 UTC",
      "updated_date": "2025-08-14 05:03:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:01.204177+00:00"
    },
    {
      "arxiv_id": "2506.10235v2",
      "title": "LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation",
      "title_zh": "LaMAGIC2ï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘ç”Ÿæˆçš„é«˜çº§ç”µè·¯è¡¨è¿°æ–¹å¼",
      "authors": [
        "Chen-Chia Chang",
        "Wan-Hsuan Lin",
        "Yikang Shen",
        "Yiran Chen",
        "Xin Zhang"
      ],
      "abstract": "Automation of analog topology design is crucial due to customized requirements of modern applications with heavily manual engineering efforts. The state-of-the-art work applies a sequence-to-sequence approach and supervised finetuning on language models to generate topologies given user specifications. However, its circuit formulation is inefficient due to O(|V |2) token length and suffers from low precision sensitivity to numeric inputs. In this work, we introduce LaMAGIC2, a succinct float-input canonical formulation with identifier (SFCI) for language model-based analog topology generation. SFCI addresses these challenges by improving component-type recognition through identifier-based representations, reducing token length complexity to O(|V |), and enhancing numeric precision sensitivity for better performance under tight tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a prior method. LaMAGIC2 also exhibits better transferability for circuits with more vertices with up to 58.5% improvement. These advancements establish LaMAGIC2 as a robust framework for analog topology generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘è®¾è®¡è‡ªåŠ¨åŒ–ä¸­ç°æœ‰è¯­è¨€æ¨¡å‹(Language Model)æ–¹æ³•å­˜åœ¨çš„$O(|V|^2)$ä»¤ç‰Œé•¿åº¦(Token Length)å¤æ‚åº¦å’Œæ•°å€¼ç²¾åº¦æ•æ„Ÿæ€§ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºäº†LaMAGIC2æ¡†æ¶ã€‚LaMAGIC2å¼•å…¥äº†ä¸€ç§å¸¦æœ‰æ ‡è¯†ç¬¦çš„ç®€æ´æµ®ç‚¹è¾“å…¥è§„èŒƒåŒ–å…¬å¼(SFCI)ï¼Œé€šè¿‡åŸºäºæ ‡è¯†ç¬¦(Identifier)çš„è¡¨ç¤ºæ˜¾è‘—æå‡äº†ç»„ä»¶ç±»å‹çš„è¯†åˆ«æ•ˆç‡ã€‚è¯¥æ–¹æ³•æˆåŠŸå°†ä»¤ç‰Œé•¿åº¦å¤æ‚åº¦é™ä½è‡³$O(|V|)$ï¼Œå¹¶å¢å¼ºäº†å¯¹æ•°å€¼è¾“å…¥çš„ç²¾åº¦æ•æ„Ÿåº¦ï¼Œä»è€Œåœ¨ä¸¥æ ¼å…¬å·®ä¸‹å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨0.01çš„å…¬å·®è¦æ±‚ä¸‹ï¼ŒLaMAGIC2çš„ç”ŸæˆæˆåŠŸç‡æå‡äº†34%ï¼Œå‡æ–¹è¯¯å·®(MSE)é™ä½äº†10å€ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†å…·æœ‰æ›´å¤šé¡¶ç‚¹çš„ç”µè·¯æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„è¿ç§»æ€§ï¼Œæ€§èƒ½æå‡æœ€é«˜è¾¾58.5%ï¼Œä¸ºæ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘ç”Ÿæˆå»ºç«‹äº†ä¸€ä¸ªç¨³å¥ä¸”é«˜æ•ˆçš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at 42nd International Conference on Machine Learning (ICML) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10235v2",
      "published_date": "2025-06-11 23:28:00 UTC",
      "updated_date": "2025-07-26 01:50:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:04.900357+00:00"
    },
    {
      "arxiv_id": "2506.10226v2",
      "title": "ScoreMix: Synthetic Data Generation by Score Composition in Diffusion Models Improves Recognition",
      "title_zh": "ScoreMixï¼šæ‰©æ•£æ¨¡å‹ä¸­åŸºäºåˆ†å€¼ç»„åˆçš„åˆæˆæ•°æ®ç”Ÿæˆæå‡è¯†åˆ«æ€§èƒ½",
      "authors": [
        "Parsa Rahimi",
        "Sebastien Marcel"
      ],
      "abstract": "Synthetic data generation is increasingly used in machine learning for training and data augmentation. Yet, current strategies often rely on external foundation models or datasets, whose usage is restricted in many scenarios due to policy or legal constraints. We propose ScoreMix, a self-contained synthetic generation method to produce hard synthetic samples for recognition tasks by leveraging the score compositionality of diffusion models. The approach mixes class-conditioned scores along reverse diffusion trajectories, yielding domain-specific data augmentation without external resources. We systematically study class-selection strategies and find that mixing classes distant in the discriminator's embedding space yields larger gains, providing up to 3% additional average improvement, compared to selection based on proximity. Interestingly, we observe that condition and embedding spaces are largely uncorrelated under standard alignment metrics, and the generator's condition space has a negligible effect on downstream performance. Across 8 public face recognition benchmarks, ScoreMix improves accuracy by up to 7 percentage points, without hyperparameter search, highlighting both robustness and practicality. Our method provides a simple yet effective way to maximize discriminator performance using only the available dataset, without reliance on third-party resources. Paper website: https://parsa-ra.github.io/scoremix/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ScoreMixï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹(Diffusion Models)å¾—åˆ†ç»„åˆæ€§(Score Compositionality)ç”Ÿæˆç¡¬åˆæˆæ ·æœ¬çš„è‡ªåŒ…å«åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨é€†å‘æ‰©æ•£è½¨è¿¹ä¸­æ··åˆç±»æ¡ä»¶å¾—åˆ†ï¼Œå®ç°åœ¨æ— éœ€å¤–éƒ¨èµ„æºæˆ–åŸºç¡€æ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œç‰¹å®šé¢†åŸŸçš„å¢å¼ºã€‚é€šè¿‡ç³»ç»Ÿæ€§ç ”ç©¶ç±»é€‰æ‹©ç­–ç•¥ï¼Œç ”ç©¶å‘ç°æ··åˆé‰´åˆ«å™¨åµŒå…¥ç©ºé—´(Discriminator's Embedding Space)ä¸­è·ç¦»è¾ƒè¿œçš„ç±»åˆ«èƒ½å¸¦æ¥æ›´å¤§çš„å¢ç›Šï¼Œæ¯”åŸºäºé‚»è¿‘æ€§çš„é€‰æ‹©å¤šæä¾›é«˜è¾¾3%çš„é¢å¤–å¹³å‡æå‡ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç ”ç©¶è§‚å¯Ÿåˆ°åœ¨æ ‡å‡†å¯¹é½åº¦é‡ä¸‹ï¼Œæ¡ä»¶ç©ºé—´ä¸åµŒå…¥ç©ºé—´åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä¸ç›¸å…³çš„ï¼Œä¸”ç”Ÿæˆå™¨çš„æ¡ä»¶ç©ºé—´å¯¹ä¸‹æ¸¸æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ã€‚åœ¨8ä¸ªå…¬å¼€çš„äººè„¸è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­ï¼ŒScoreMixåœ¨æ— éœ€è¶…å‚æ•°æœç´¢çš„æƒ…å†µä¸‹å°†å‡†ç¡®ç‡æå‡äº†é«˜è¾¾7ä¸ªç™¾åˆ†ç‚¹ï¼Œå±•ç°äº†å…¶å¼ºå¤§çš„é²æ£’æ€§å’Œå®ç”¨æ€§ã€‚ScoreMixæä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œä»…åˆ©ç”¨ç°æœ‰æ•°æ®é›†å³å¯æœ€å¤§åŒ–é‰´åˆ«å™¨æ€§èƒ½ï¼Œå®Œå…¨æ‘†è„±äº†å¯¹ç¬¬ä¸‰æ–¹èµ„æºçš„ä¾èµ–ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Extended version of ICMLw25 Oral",
      "pdf_url": "https://arxiv.org/pdf/2506.10226v2",
      "published_date": "2025-06-11 23:06:44 UTC",
      "updated_date": "2025-10-24 13:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:04.554752+00:00"
    },
    {
      "arxiv_id": "2506.10225v1",
      "title": "Fine-Grained control over Music Generation with Activation Steering",
      "title_zh": "é€šè¿‡æ¿€æ´»è½¬å‘å®ç°éŸ³ä¹ç”Ÿæˆçš„ç»†ç²’åº¦æ§åˆ¶",
      "authors": [
        "Dipanshu Panda",
        "Jayden Koshy Joe",
        "Harshith M R",
        "Swathi Narashiman",
        "Pranay Mathur",
        "Anish Veerakumar",
        "Aniruddh Krishna",
        "Keerthiharan A"
      ],
      "abstract": "We present a method for fine-grained control over music generation through inference-time interventions on an autoregressive generative music transformer called MusicGen. Our approach enables timbre transfer, style transfer, and genre fusion by steering the residual stream using weights of linear probes trained on it, or by steering the attention layer activations in a similar manner. We observe that modelling this as a regression task provides improved performance, hypothesizing that the mean-squared-error better preserve meaningful directional information in the activation space. Combined with the global conditioning offered by text prompts in MusicGen, our method provides both global and local control over music generation. Audio samples illustrating our method are available at our demo page.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡æ¨ç†æ—¶å¹²é¢„(inference-time interventions)å®ç°éŸ³ä¹ç”Ÿæˆç»†ç²’åº¦æ§åˆ¶çš„æ–¹æ³•ï¼Œä¸»è¦é’ˆå¯¹è‡ªå›å½’ç”ŸæˆéŸ³ä¹Transformeræ¨¡å‹ MusicGenã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ¿€æ´»è½¬å‘(Activation Steering)ï¼Œé€šè¿‡ä½¿ç”¨åœ¨çº¿æ€§æ¢é’ˆ(linear probes)ä¸Šè®­ç»ƒçš„æƒé‡æ¥å¼•å¯¼æ®‹å·®æµ(residual stream)æˆ–æ³¨æ„åŠ›å±‚(attention layer)çš„æ¿€æ´»ã€‚ä½œè€…æŒ‡å‡ºå°†æ­¤è¿‡ç¨‹å»ºæ¨¡ä¸ºå›å½’ä»»åŠ¡(regression task)å¯è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¹¶å‡è®¾å‡æ–¹è¯¯å·®(Mean-Squared-Error)èƒ½æ›´æœ‰æ•ˆåœ°ä¿ç•™æ¿€æ´»ç©ºé—´å†…å…·æœ‰æ„ä¹‰çš„æ–¹å‘ä¿¡æ¯ã€‚è¯¥æ–¹æ¡ˆèƒ½å¤Ÿå®ç°éŸ³è‰²è½¬æ¢(timbre transfer)ã€é£æ ¼è½¬æ¢(style transfer)ä»¥åŠæµæ´¾èåˆ(genre fusion)ã€‚é€šè¿‡ç»“åˆ MusicGen åŸæœ‰çš„æ–‡æœ¬æç¤º(text prompts)å…¨å±€è°ƒèŠ‚èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•æˆåŠŸå®ç°äº†å¯¹éŸ³ä¹ç”Ÿæˆè¿‡ç¨‹çš„å…¨å±€ä¸å±€éƒ¨åŒé‡ç²¾ç¡®æ§åˆ¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10225v1",
      "published_date": "2025-06-11 23:02:39 UTC",
      "updated_date": "2025-06-11 23:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:18.966145+00:00"
    },
    {
      "arxiv_id": "2506.21560v2",
      "title": "Reinforcement learning fine-tuning of language model for instruction following and math reasoning",
      "title_zh": "é¢å‘æŒ‡ä»¤éµå¾ªä¸æ•°å­¦æ¨ç†çš„è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¾®è°ƒ",
      "authors": [
        "Yifu Han",
        "Geo Zhang"
      ],
      "abstract": "This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è½»é‡çº§è¯­è¨€æ¨¡å‹ Qwen2.5-0.5B Base ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¾®è°ƒæŠ€æœ¯ä»¥æå‡æŒ‡ä»¤éµå¾ªå’Œæ•°å­¦æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…å¯¹æ¯”äº†ç›‘ç£å¾®è°ƒ(SFT)ã€ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ä»¥åŠç»“åˆå¥–åŠ±æ¨¡å‹çš„ Reinforce Leave-One-Out (RLOO) ç­‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ DeBERTa å¥–åŠ±æ¨¡å‹çš„ RLOO åœ¨å¯¹é½ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œ DPO åˆ™å±•ç°äº†å¼ºåŠ²ä¸”ä¸€è‡´çš„æ€§èƒ½ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œé€šè¿‡åˆæˆæ•°æ®å¢å¼ºå’Œå¸¦æœ‰å¤–éƒ¨éªŒè¯å™¨çš„ Best-of-N é‡‡æ ·æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å¾®è°ƒä¸æ¨ç†æ—¶å·¥å…·ç»“åˆçš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘é«˜æ€§èƒ½ä¸”ä»»åŠ¡å¯¹é½çš„å°è§„æ¨¡è¯­è¨€æ¨¡å‹æä¾›äº†å…³é”®çš„æƒè¡¡åˆ†æä¸å®ç”¨ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21560v2",
      "published_date": "2025-06-11 22:49:42 UTC",
      "updated_date": "2025-07-27 00:45:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:11.111487+00:00"
    },
    {
      "arxiv_id": "2506.11166v1",
      "title": "Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning",
      "title_zh": "åŸºäºè§†è§‰è¯­è¨€æ¨ç†çš„é›¶æ ·æœ¬è¯Šæ–­æµ‹è¯•æ—¶ç¼©æ”¾",
      "authors": [
        "Ji Young Byun",
        "Young-Jin Park",
        "Navid Azizan",
        "Rama Chellappa"
      ],
      "abstract": "As a cornerstone of patient care, clinical decision-making significantly influences patient outcomes and can be enhanced by large language models (LLMs). Although LLMs have demonstrated remarkable performance, their application to visual question answering in medical imaging, particularly for reasoning-based diagnosis, remains largely unexplored. Furthermore, supervised fine-tuning for reasoning tasks is largely impractical due to limited data availability and high annotation costs. In this work, we introduce a zero-shot framework for reliable medical image diagnosis that enhances the reasoning capabilities of LLMs in clinical settings through test-time scaling. Given a medical image and a textual prompt, a vision-language model processes a medical image along with a corresponding textual prompt to generate multiple descriptions or interpretations of visual features. These interpretations are then fed to an LLM, where a test-time scaling strategy consolidates multiple candidate outputs into a reliable final diagnosis. We evaluate our approach across various medical imaging modalities -- including radiology, ophthalmology, and histopathology -- and demonstrate that the proposed test-time scaling strategy enhances diagnostic accuracy for both our and baseline methods. Additionally, we provide an empirical analysis showing that the proposed approach, which allows unbiased prompting in the first stage, improves the reliability of LLM-generated diagnoses and enhances classification accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒæ¨ç†è¯Šæ–­ä¸­ç›‘ç£å¾®è°ƒ(supervised fine-tuning)å› æ•°æ®æœ‰é™å’Œæ ‡æ³¨æˆæœ¬é«˜è€Œéš¾ä»¥å®ç°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºå¯é åŒ»ç–—å½±åƒè¯Šæ–­çš„é›¶æ ·æœ¬(Zero-Shot)æ¡†æ¶ï¼Œé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)æŠ€æœ¯å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸´åºŠç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å¤„ç†å½±åƒå¹¶ç”Ÿæˆå¤šé‡è§†è§‰ç‰¹å¾è§£é‡Šï¼Œéšåç”±LLMè¿ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥å°†è¿™äº›å€™é€‰è¾“å‡ºæ•´åˆä¸ºæœ€ç»ˆè¯Šæ–­ã€‚å®éªŒåœ¨æ”¾å°„å­¦(radiology)ã€çœ¼ç§‘å­¦(ophthalmology)å’Œç»„ç»‡ç—…ç†å­¦(histopathology)ç­‰å¤šç§æ¨¡æ€ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºè¯¥ç­–ç•¥æ˜¾è‘—æå‡äº†æœ¬æ–¹æ¡ˆåŠåŸºçº¿æ–¹æ³•çš„è¯Šæ–­å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œç»éªŒåˆ†æè¡¨æ˜è¯¥æ¡†æ¶é€šè¿‡ç¬¬ä¸€é˜¶æ®µçš„æ— åæç¤º(unbiased prompting)æå‡äº†LLMç”Ÿæˆè¯Šæ–­çš„å¯é æ€§ï¼Œå¹¶è¿›ä¸€æ­¥å¢å¼ºäº†åˆ†ç±»ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11166v1",
      "published_date": "2025-06-11 22:23:38 UTC",
      "updated_date": "2025-06-11 22:23:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:25.566150+00:00"
    },
    {
      "arxiv_id": "2506.10212v1",
      "title": "Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms",
      "title_zh": "ECG ä¸ PCG çš„äº¤å‰å­¦ä¹ ï¼šæ¢ç©¶åŒæ¨¡æ€å¿ƒè„æœºç”µæ³¢å½¢çš„å…±é€šä¸ç‹¬æœ‰ç‰¹å¾",
      "authors": [
        "Sajjad Karimi",
        "Amit J. Shah",
        "Gari D. Clifford",
        "Reza Sameni"
      ],
      "abstract": "Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a comprehensive, multimodal perspective on cardiac function by capturing the heart's electrical and mechanical activities, respectively. However, the distinct and overlapping information content of these signals, as well as their potential for mutual reconstruction and biomarker extraction, remains incompletely understood, especially under varying physiological conditions and across individuals.\n  In this study, we systematically investigate the common and exclusive characteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous ECG-PCG recordings during rest and exercise. We employ a suite of linear and nonlinear machine learning models, including non-causal LSTM networks, to reconstruct each modality from the other and analyze the influence of causality, physiological state, and cross-subject variability. Our results demonstrate that nonlinear models, particularly non-causal LSTM, provide superior reconstruction performance, with reconstructing ECG from PCG proving more tractable than the reverse. Exercise and cross-subject scenarios present significant challenges, but envelope-based modeling that utilizes instantaneous amplitude features substantially improves cross-subject generalizability for cross-modal learning. Furthermore, we demonstrate that clinically relevant ECG biomarkers, such as fiducial points and QT intervals, can be estimated from PCG in cross-subject settings.\n  These findings advance our understanding of the relationship between electromechanical cardiac modalities, in terms of both waveform characteristics and the timing of cardiac events, with potential applications in novel multimodal cardiac monitoring technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†å¿ƒç”µå›¾(ECG)ä¸å¿ƒéŸ³å›¾(PCG)ä¹‹é—´çš„äº¤å‰å­¦ä¹ ï¼Œæ—¨åœ¨æ­ç¤ºå¿ƒè„ç”µæ´»åŠ¨ä¸æœºæ¢°æ´»åŠ¨åŒæ¨¡æ€æ³¢å½¢ä¸­çš„å…±åŒä¸ç‹¬æœ‰ç‰¹å¾ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨EPHNOGRAMæ•°æ®é›†ï¼Œé‡‡ç”¨éå› æœLSTMç½‘ç»œç­‰çº¿æ€§ä¸éçº¿æ€§æœºå™¨å­¦ä¹ æ¨¡å‹å®ç°äº†ä¸¤ç§æ¨¡æ€çš„ç›¸äº’é‡å»ºï¼Œå¹¶æ·±å…¥åˆ†æäº†ç”Ÿç†çŠ¶æ€å’Œä¸ªä½“å·®å¼‚å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéçº¿æ€§æ¨¡å‹åœ¨é‡å»ºæ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä¸”ä»PCGé‡å»ºECGæ¯”åå‘é‡å»ºæ›´å…·å¯è¡Œæ€§ã€‚é’ˆå¯¹è·¨å—è¯•è€…åœºæ™¯å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å‘ç°åŸºäºç¬æ—¶æŒ¯å¹…ç‰¹å¾çš„åŒ…ç»œå»ºæ¨¡(envelope-based modeling)èƒ½æ˜¾è‘—å¢å¼ºè·¨æ¨¡æ€å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¯æ˜äº†åœ¨è·¨å—è¯•è€…è®¾ç½®ä¸‹ä»PCGç›´æ¥ä¼°è®¡QT intervalsç­‰ä¸´åºŠæ ¸å¿ƒç”Ÿç‰©æ ‡å¿—ç‰©çš„å¯èƒ½æ€§ã€‚è¿™äº›å‘ç°ä¸ºç†è§£å¿ƒè„ç”µæœºæ¢°æ¨¡æ€é—´çš„å¤æ‚å…³ç³»æä¾›äº†æ–°è§è§£ï¼Œå¹¶ä¸ºå¼€å‘æ–°å‹å¤šæ¨¡æ€å¿ƒè„ç›‘æµ‹æŠ€æœ¯å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10212v1",
      "published_date": "2025-06-11 22:16:59 UTC",
      "updated_date": "2025-06-11 22:16:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:19.696292+00:00"
    },
    {
      "arxiv_id": "2506.11165v1",
      "title": "Evaluating BiLSTM and CNN+GRU Approaches for Human Activity Recognition Using WiFi CSI Data",
      "title_zh": "åŸºäº WiFi CSI æ•°æ®çš„ BiLSTM ä¸ CNN+GRU äººä½“æ´»åŠ¨è¯†åˆ«æ–¹æ³•è¯„ä¼°",
      "authors": [
        "Almustapha A. Wakili",
        "Babajide J. Asaju",
        "Woosub Jung"
      ],
      "abstract": "This paper compares the performance of BiLSTM and CNN+GRU deep learning models for Human Activity Recognition (HAR) on two WiFi-based Channel State Information (CSI) datasets: UT-HAR and NTU-Fi HAR. The findings indicate that the CNN+GRU model has a higher accuracy on the UT-HAR dataset (95.20%) thanks to its ability to extract spatial features. In contrast, the BiLSTM model performs better on the high-resolution NTU-Fi HAR dataset (92.05%) by extracting long-term temporal dependencies more effectively. The findings strongly emphasize the critical role of dataset characteristics and preprocessing techniques in model performance improvement. We also show the real-world applicability of such models in applications like healthcare and intelligent home systems, highlighting their potential for unobtrusive activity recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†BiLSTMå’ŒCNN+GRUä¸¤ç§æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŸºäºWiFiä¿¡é“çŠ¶æ€ä¿¡æ¯(CSI)çš„äººä½“æ´»åŠ¨è¯†åˆ«(HAR)ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒåœ¨UT-HARå’ŒNTU-Fi HARä¸¤ä¸ªæ•°æ®é›†ä¸Šå±•å¼€ï¼Œé€šè¿‡è¯„ä¼°ä¸åŒæ¨¡å‹æ¶æ„åœ¨ç‰¹å¾æå–æ–¹é¢çš„å·®å¼‚ï¼Œæ¢è®¨äº†å…¶åœ¨æ— çº¿æ„ŸçŸ¥é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒCNN+GRUæ¨¡å‹å‡­å€Ÿå…¶å¯¹ç©ºé—´ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œåœ¨UT-HARæ•°æ®é›†ä¸Šå®ç°äº†95.20%çš„å‡†ç¡®ç‡ã€‚è€Œåœ¨é«˜åˆ†è¾¨ç‡çš„NTU-Fi HARæ•°æ®é›†ä¸­ï¼ŒBiLSTMæ¨¡å‹é€šè¿‡æœ‰æ•ˆæ•æ‰é•¿æ—¶åºåˆ—çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œå±•ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾åˆ°92.05%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ•°æ®é›†ç‰¹æ€§å’Œé¢„å¤„ç†æ–¹æ³•åœ¨æå‡è¯†åˆ«ç²¾åº¦ä¸­çš„å…³é”®ä½œç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†ç›¸å…³æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥åŠæ™ºèƒ½å®¶å±…ç­‰å®é™…åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ï¼ŒéªŒè¯äº†éä¾µå…¥å¼æ„ŸçŸ¥æŠ€æœ¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This Paper has been Accepted and will appear in the 23rd IEEE/ACIS International Conference on Software Engineering, Management and Applications (SERA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.11165v1",
      "published_date": "2025-06-11 22:10:34 UTC",
      "updated_date": "2025-06-11 22:10:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:38.438365+00:00"
    },
    {
      "arxiv_id": "2506.10209v1",
      "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games",
      "title_zh": "TTT-Benchï¼šé€šè¿‡ç®€å•ä¸”æ–°é¢–çš„äº•å­—æ£‹ç±»æ¸¸æˆè¯„ä¼°æ¨ç†èƒ½åŠ›çš„åŸºå‡†",
      "authors": [
        "Prakamya Mishra",
        "Jiang Liu",
        "Jialian Wu",
        "Xiaodong Yu",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "abstract": "Large reasoning models (LRMs) have demonstrated impressive reasoning capabilities across a broad range of tasks including Olympiad-level mathematical problems, indicating evidence of their complex reasoning abilities. While many reasoning benchmarks focus on the STEM domain, the ability of LRMs to reason correctly in broader task domains remains underexplored. In this work, we introduce \\textbf{TTT-Bench}, a new benchmark that is designed to evaluate basic strategic, spatial, and logical reasoning abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games that humans can effortlessly solve from a young age. We propose a simple yet scalable programmatic approach for generating verifiable two-player game problems for TTT-Bench. Although these games are trivial for humans, they require reasoning about the intentions of the opponent, as well as the game board's spatial configurations, to ensure a win. We evaluate a diverse set of state-of-the-art LRMs, and \\textbf{discover that the models that excel at hard math problems frequently fail at these simple reasoning games}. Further testing reveals that our evaluated reasoning models score on average $\\downarrow$ 41\\% \\& $\\downarrow$ 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024 respectively, with larger models achieving higher performance using shorter reasoning traces, where most of the models struggle on long-term strategic reasoning situations on simple and new TTT-Bench tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶å¼•å…¥äº† TTT-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡å››ç§ä¸¤ç©å®¶äº•å­—æ£‹é£æ ¼ (Tic-Tac-Toe-style) æ¸¸æˆè¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ (Large reasoning models, LRMs) åŸºç¡€æˆ˜ç•¥ã€ç©ºé—´å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„å…¨æ–°åŸºå‡†ã€‚è¯¥åŸºå‡†é‡‡ç”¨äº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„ç¨‹åºåŒ–æ–¹æ³•æ¥ç”Ÿæˆå¯éªŒè¯çš„åšå¼ˆé—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹åœ¨ç¡®ä¿è·èƒœçš„è¿‡ç¨‹ä¸­å¿…é¡»è€ƒè™‘å¯¹æ‰‹çš„æ„å›¾åŠæ£‹ç›˜çš„ç©ºé—´é…ç½®ã€‚å®éªŒè¯„ä¼°å‘ç°ï¼Œå³ä¾¿åœ¨äººç±»èƒ½è½»æ¾è§£å†³çš„ä»»åŠ¡ä¸­ï¼Œè®¸å¤šåœ¨ç¡¬æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ä¹Ÿé¢‘ç¹å¤±è´¥ï¼Œå…¶åœ¨ TTT-Bench ä¸Šçš„å¹³å‡å¾—åˆ†æ¯” MATH 500 ä½ 41%ï¼Œæ¯” AIME 2024 ä½ 5%ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨å¤„ç† TTT-Bench ä¸­ç®€å•ä¸”æ–°é¢–çš„é•¿æœŸæˆ˜ç•¥æ¨ç† (long-term strategic reasoning) å±€é¢æ—¶å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚å°½ç®¡è¾ƒå¤§è§„æ¨¡çš„æ¨¡å‹èƒ½åˆ©ç”¨æ›´çŸ­çš„æ¨ç†è½¨è¿¹ (reasoning traces) è·å¾—æ›´é«˜æ€§èƒ½ï¼Œä½†å®éªŒç»“æœè¡¨æ˜å½“å‰çš„æ¨ç†æ¨¡å‹åœ¨éæ•°å­¦é¢†åŸŸçš„å¹¿æ³›æ¨ç†èƒ½åŠ›ä»æœ‰å¾…æå‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10209v1",
      "published_date": "2025-06-11 22:03:19 UTC",
      "updated_date": "2025-06-11 22:03:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:01.532501+00:00"
    },
    {
      "arxiv_id": "2506.10192v2",
      "title": "Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems",
      "title_zh": "è¿ˆå‘è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ï¼šè‡ªä¸»ç³»ç»Ÿå®‰å…¨æ€§ã€å…¬å¹³æ€§ä¸é—®è´£åˆ¶çš„ç ”ç©¶è¿›å±•",
      "authors": [
        "Filip Cano"
      ],
      "abstract": "Ensuring responsible use of artificial intelligence (AI) has become imperative as autonomous systems increasingly influence critical societal domains. However, the concept of trustworthy AI remains broad and multi-faceted. This thesis advances knowledge in the safety, fairness, transparency, and accountability of AI systems. In safety, we extend classical deterministic shielding techniques to become resilient against delayed observations, enabling practical deployment in real-world conditions. We also implement both deterministic and probabilistic safety shields into simulated autonomous vehicles to prevent collisions with road users, validating the use of these techniques in realistic driving simulators. We introduce fairness shields, a novel post-processing approach to enforce group fairness in sequential decision-making settings over finite and periodic time horizons. By optimizing intervention costs while strictly ensuring fairness constraints, this method efficiently balances fairness with minimal interference. For transparency and accountability, we propose a formal framework for assessing intentional behaviour in probabilistic decision-making agents, introducing quantitative metrics of agency and intention quotient. We use these metrics to propose a retrospective analysis of intention, useful for determining responsibility when autonomous systems cause unintended harm. Finally, we unify these contributions through the ``reactive decision-making'' framework, providing a general formalization that consolidates previous approaches. Collectively, the advancements presented contribute practically to the realization of safer, fairer, and more accountable AI systems, laying the foundations for future research in trustworthy AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½(AI)åœ¨ç¤¾ä¼šå…³é”®é¢†åŸŸçš„å½±å“ï¼Œæ·±å…¥æ¢è®¨äº†è‡ªä¸»ç³»ç»Ÿçš„å®‰å…¨æ€§(safety)ã€å…¬å¹³æ€§(fairness)å’Œé—®è´£åˆ¶(accountability)ã€‚åœ¨å®‰å…¨æ€§æ–¹é¢ï¼Œç ”ç©¶å°†ä¼ ç»Ÿçš„ç¡®å®šæ€§å±è”½(deterministic shielding)æŠ€æœ¯æ‰©å±•è‡³èƒ½å¤Ÿåº”å¯¹å»¶è¿Ÿè§‚æµ‹ï¼Œå¹¶åœ¨è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿä¸­é€šè¿‡ç¡®å®šæ€§å’Œæ¦‚ç‡æ€§å®‰å…¨å±è”½éªŒè¯äº†å…¶é¢„é˜²ç¢°æ’çš„æœ‰æ•ˆæ€§ã€‚é’ˆå¯¹å…¬å¹³æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå…¬å¹³å±è”½(fairness shields)çš„æ–°å‹åå¤„ç†æ–¹æ³•ï¼Œä»¥åœ¨ç¡®ä¿é¡ºåºå†³ç­–ä¸­ç¾¤ä½“å…¬å¹³çš„åŒæ—¶æœ€å°åŒ–å¹²é¢„æˆæœ¬ã€‚åœ¨é€æ˜åº¦ä¸é—®è´£åˆ¶æ–¹é¢ï¼Œå»ºç«‹äº†è¯„ä¼°æ¦‚ç‡å†³ç­–æ™ºèƒ½ä½“æ„å›¾è¡Œä¸ºçš„æ­£å¼æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†æœºæ„(agency)å’Œæ„å›¾å•†æ•°(intention quotient)ç­‰é‡åŒ–æŒ‡æ ‡ï¼Œç”¨äºè‡ªä¸»ç³»ç»Ÿé€ æˆä¼¤å®³æ—¶çš„è¿½æº¯åˆ†æã€‚æœ€åï¼Œé€šè¿‡ååº”å¼å†³ç­–(reactive decision-making)æ¡†æ¶ç»Ÿä¸€äº†ä¸Šè¿°è´¡çŒ®ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨ã€å…¬å¹³ä¸”å¯ä¿¡çš„AIç³»ç»Ÿå¥ å®šäº†ç†è®ºä¸å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "204 pages, 38 figures, PhD Thesis",
      "pdf_url": "https://arxiv.org/pdf/2506.10192v2",
      "published_date": "2025-06-11 21:30:02 UTC",
      "updated_date": "2025-10-27 14:00:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:55:33.334661+00:00"
    },
    {
      "arxiv_id": "2506.10186v2",
      "title": "Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment",
      "title_zh": "åŸºäºæ—‹è½¬å¯¹é½çš„å¯æ‰©å±•éç­‰å˜ä¸‰ç»´åˆ†å­ç”Ÿæˆ",
      "authors": [
        "Yuhui Ding",
        "Thomas Hofmann"
      ],
      "abstract": "Equivariant diffusion models have achieved impressive performance in 3D molecule generation. These models incorporate Euclidean symmetries of 3D molecules by utilizing an SE(3)-equivariant denoising network. However, specialized equivariant architectures limit the scalability and efficiency of diffusion models. In this paper, we propose an approach that relaxes such equivariance constraints. Specifically, our approach learns a sample-dependent SO(3) transformation for each molecule to construct an aligned latent space. A non-equivariant diffusion model is then trained over the aligned representations. Experimental results demonstrate that our approach performs significantly better than previously reported non-equivariant models. It yields sample quality comparable to state-of-the-art equivariant diffusion models and offers improved training and sampling efficiency. Our code is available at https://github.com/skeletondyh/RADM",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D åˆ†å­ç”Ÿæˆ (3D Molecule Generation) é—®é¢˜æå‡ºäº†ä¸€ç§é€šè¿‡æ—‹è½¬å¯¹é½ (Rotational Alignment) å®ç°çš„å¯æ‰©å±•éç­‰å˜ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç­‰å˜æ‰©æ•£æ¨¡å‹ (Equivariant diffusion models) å› å¤æ‚çš„ç­‰å˜æ¶æ„è€Œé¢ä¸´çš„å¯æ‰©å±•æ€§ä¸æ•ˆç‡ç“¶é¢ˆã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ æ ·æœ¬ç›¸å…³çš„ SO(3) å˜æ¢ (SO(3) transformation) ä¸ºæ¯ä¸ªåˆ†å­æ„å»ºå¯¹é½çš„éšç©ºé—´ï¼Œå¹¶åœ¨è¯¥ç©ºé—´å†…è®­ç»ƒéç­‰å˜æ‰©æ•£æ¨¡å‹ (non-equivariant diffusion model)ï¼Œä»è€Œæœ‰æ•ˆæ”¾å®½äº†ç­‰å˜çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä»¥å¾€çš„éç­‰å˜æ¨¡å‹ï¼Œä¸”åœ¨ç”Ÿæˆè´¨é‡ä¸Šå¯ä¸å½“å‰æœ€å…ˆè¿›çš„ç­‰å˜æ¨¡å‹åª²ç¾ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨è®­ç»ƒå’Œé‡‡æ ·æ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„åˆ†å­å»ºæ¨¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025; added conditional generation results",
      "pdf_url": "https://arxiv.org/pdf/2506.10186v2",
      "published_date": "2025-06-11 21:23:56 UTC",
      "updated_date": "2025-06-29 17:18:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:32.372377+00:00"
    },
    {
      "arxiv_id": "2506.10184v1",
      "title": "Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition",
      "title_zh": "åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œä¼˜åŒ–é—ä¼ ç®—æ³•ï¼Œå¢å¼º TinyFace è¯†åˆ«",
      "authors": [
        "Mohammad Subhi Al-Batah",
        "Mowafaq Salem Alzboon",
        "Muhyeeddin Alqaraleh"
      ],
      "abstract": "This study conducts an empirical examination of MLP networks investigated through a rigorous methodical experimentation process involving three diverse datasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes three key methods: a) a baseline training using the default settings for the Multi-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA) based refinement c) Principal Component Analysis (PCA) based dimension reduction. The results show important information on how such techniques affect performance. While PCA had showed benefits in low-dimensional and noise-free datasets GA consistently increased accuracy in complex datasets by accurately identifying critical features. Comparison reveals that feature selection and dimensionality reduction play interdependent roles in enhancing MLP performance. The study contributes to the literature on feature engineering and neural network parameter optimization, offering practical guidelines for a wide range of machine learning tasks",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡é—ä¼ ç®—æ³•(Genetic Algorithm, GA)å’Œä¸»æˆåˆ†åˆ†æ(PCA)ä¼˜åŒ–å¤šå±‚æ„ŸçŸ¥å™¨(MLP)ç½‘ç»œï¼Œä»¥æå‡TinyFaceè¯†åˆ«ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜åœ¨TinyFaceã€Heart Diseaseå’ŒIrisä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿå®éªŒï¼Œå¯¹æ¯”äº†åŸºå‡†MLPè®­ç»ƒã€åŸºäºGAçš„ç‰¹å¾é€‰æ‹©ä»¥åŠåŸºäºPCAçš„ç»´åº¦ç¼©å‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡PCAåœ¨ä½ç»´ä¸”æ— å™ªå£°çš„æ•°æ®é›†ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†GAé€šè¿‡ç²¾å‡†è¯†åˆ«å…³é”®ç‰¹å¾ï¼Œåœ¨å¤„ç†å¤æ‚æ•°æ®é›†æ—¶èƒ½ä¸€è‡´æ€§åœ°æé«˜æ¨¡å‹å‡†ç¡®ç‡ã€‚è¯¥å¯¹æ¯”åˆ†ææ­ç¤ºäº†ç‰¹å¾é€‰æ‹©ä¸ç»´åº¦ç¼©å‡åœ¨å¢å¼ºMLPæ€§èƒ½ä¸­å…·æœ‰ç›¸äº’ä¾èµ–çš„ä½œç”¨ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç‰¹å¾å·¥ç¨‹(feature engineering)å’Œç¥ç»ç½‘ç»œå‚æ•°ä¼˜åŒ–æä¾›äº†é‡è¦è´¡çŒ®ï¼Œå¹¶ä¸ºå¹¿æ³›çš„æœºå™¨å­¦ä¹ ä»»åŠ¡æä¾›äº†å®ç”¨çš„æŒ‡å¯¼å‡†åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10184v1",
      "published_date": "2025-06-11 21:21:46 UTC",
      "updated_date": "2025-06-11 21:21:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:30.746291+00:00"
    },
    {
      "arxiv_id": "2506.10180v1",
      "title": "A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes",
      "title_zh": "æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨ç³–å°¿ç—…æ—©æœŸé¢„æµ‹ä¸­çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Mowafaq Salem Alzboon",
        "Mohammad Al-Batah",
        "Muhyeeddin Alqaraleh",
        "Ahmad Abuashour",
        "Ahmad Fuad Bader"
      ],
      "abstract": "In many nations, diabetes is becoming a significant health problem, and early identification and control are crucial. Using machine learning algorithms to predict diabetes has yielded encouraging results. Using the Pima Indians Diabetes dataset, this study attempts to evaluate the efficacy of several machine-learning methods for diabetes prediction. The collection includes information on 768 patients, such as their ages, BMIs, and glucose levels. The techniques assessed are Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The findings indicate that the Neural Network algorithm performed the best, with an accuracy of 78.57 percent, followed by the Random Forest method, with an accuracy of 76.30 percent. The study implies that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤šç§ Machine Learning æ–¹æ³•åœ¨ç³–å°¿ç—…æ—©æœŸé¢„æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥åº”å¯¹æ—¥ç›Šä¸¥å³»çš„å…¨çƒå¥åº·æŒ‘æˆ˜ã€‚ç ”ç©¶ä½¿ç”¨äº† Pima Indians Diabetes æ•°æ®é›†ï¼Œåˆ†æäº† 768 åæ‚£è€…çš„å¹´é¾„ã€BMI å’Œè‘¡è„ç³–æ°´å¹³ç­‰å…³é”®ç”Ÿç†æŒ‡æ ‡ã€‚è¯„ä¼°çš„æŠ€æœ¯æ¶µç›–äº† Logistic Regressionã€Decision Treeã€Random Forestã€k-Nearest Neighborsã€Naive Bayesã€Support Vector Machineã€Gradient Boosting å’Œ Neural Networkã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeural Network ç®—æ³•åœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡è¾¾åˆ° 78.57%ï¼Œå…¶æ¬¡æ˜¯ Random Forest ç®—æ³•ï¼Œå…¶å‡†ç¡®ç‡ä¸º 76.30%ã€‚è¯¥é¡¹å¯¹æ¯”ç ”ç©¶è¯å®äº† Machine Learning ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©ç³–å°¿ç—…é¢„æµ‹ï¼Œæ˜¯å®ç°ç–¾ç—…æ—©æœŸæ£€æµ‹çš„é«˜æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10180v1",
      "published_date": "2025-06-11 21:12:28 UTC",
      "updated_date": "2025-06-11 21:12:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:25.277988+00:00"
    },
    {
      "arxiv_id": "2506.10179v1",
      "title": "Correlation vs causation in Alzheimer's disease: an interpretability-driven study",
      "title_zh": "é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„ç›¸å…³æ€§ä¸å› æœå…³ç³»ï¼šåŸºäºå¯è§£é‡Šæ€§çš„ç ”ç©¶",
      "authors": [
        "Hamzah Dabool",
        "Raghad Mustafa"
      ],
      "abstract": "Understanding the distinction between causation and correlation is critical in Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and the identification of true disease drivers. This experiment investigates the relationships among clinical, cognitive, genetic, and biomarker features using a combination of correlation analysis, machine learning classification, and model interpretability techniques. Employing the XGBoost algorithm, we identified key features influencing AD classification, including cognitive scores and genetic risk factors. Correlation matrices revealed clusters of interrelated variables, while SHAP (SHapley Additive exPlanations) values provided detailed insights into feature contributions across disease stages. Our results highlight that strong correlations do not necessarily imply causation, emphasizing the need for careful interpretation of associative data. By integrating feature importance and interpretability with classical statistical analysis, this work lays groundwork for future causal inference studies aimed at uncovering true pathological mechanisms. Ultimately, distinguishing causal factors from correlated markers can lead to improved early diagnosis and targeted interventions for Alzheimer's disease.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é˜¿å°”èŒ¨æµ·é»˜ç—… (Alzheimer's disease) ç ”ç©¶ä¸­å› æœå…³ç³» (causation) ä¸ç›¸å…³æ€§ (correlation) ä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œæ—¨åœ¨è¯†åˆ«çœŸæ­£çš„ç–¾ç—…é©±åŠ¨å› ç´ ã€‚å®éªŒæ•´åˆäº†ä¸´åºŠã€è®¤çŸ¥ã€é—ä¼ å’Œç”Ÿç‰©æ ‡å¿—ç‰©ç‰¹å¾ï¼Œåˆ©ç”¨ XGBoost ç®—æ³•è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ç»“åˆç›¸å…³æ€§åˆ†æä¸ SHAP (SHapley Additive exPlanations) æ¨¡å‹è§£é‡Šæ€§æŠ€æœ¯ã€‚ç ”ç©¶è¯†åˆ«äº†å½±å“ AD åˆ†ç±»çš„æ ¸å¿ƒç‰¹å¾ï¼ŒåŒ…æ‹¬è®¤çŸ¥è¯„åˆ†å’Œé—ä¼ é£é™©å› å­ï¼Œå¹¶é€šè¿‡ç›¸å…³çŸ©é˜µæ­ç¤ºäº†å˜é‡é—´çš„å¤æ‚å…³è”ã€‚SHAP å€¼åˆ†æè¿›ä¸€æ­¥æä¾›äº†ä¸åŒç–¾ç—…é˜¶æ®µç‰¹å¾è´¡çŒ®åº¦çš„æ·±å…¥è§è§£ï¼Œè¯æ˜äº†å¼ºç›¸å…³æ€§å¹¶ä¸ä¸€å®šæ„å‘³ç€å› æœå…³ç³»ã€‚è¯¥å·¥ä½œé€šè¿‡å°†ç‰¹å¾é‡è¦æ€§å’Œå¯è§£é‡Šæ€§ä¸ç»å…¸ç»Ÿè®¡å­¦ç›¸ç»“åˆï¼Œä¸ºæœªæ¥çš„å› æœæ¨ç† (causal inference) ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚åŒºåˆ†å› æœå› ç´ ä¸ç›¸å…³æ ‡å¿—ç‰©å°†æœ‰åŠ©äºæ”¹å–„ AD çš„æ—©æœŸè¯Šæ–­å’Œé¶å‘å¹²é¢„ã€‚",
      "categories": [
        "cs.AI",
        "q-bio.QM",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10179v1",
      "published_date": "2025-06-11 21:10:57 UTC",
      "updated_date": "2025-06-11 21:10:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:34.795449+00:00"
    },
    {
      "arxiv_id": "2506.10173v2",
      "title": "SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in Diffusion Models via RKE Score",
      "title_zh": "SPARKEï¼šåŸºäº RKE åˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å¯æ‰©å±•æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§ä¸æ–°é¢–æ€§å¼•å¯¼",
      "authors": [
        "Mohammad Jalali",
        "Haoyu Lei",
        "Amin Gohari",
        "Farzan Farnia"
      ],
      "abstract": "Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware RÃ©ny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: https://mjalali.github.io/SPARKE",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPARKEï¼Œä¸€ç§åŸºäº RÃ©ny Kernel Entropy (RKE) çš„å¯æ‰©å±•æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§å¼•å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹ (Diffusion Models) åœ¨å¤„ç†è¯­ä¹‰å¹¿æ³›çš„æç¤ºæ—¶ç”Ÿæˆæ ·æœ¬å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚SPARKE åˆ©ç”¨æ¡ä»¶ç†µ (Conditional Entropy) è¿›è¡Œå¼•å¯¼ï¼Œé€šè¿‡åœ¨ç›¸ä¼¼æç¤ºä¸ŠåŠ¨æ€è°ƒèŠ‚å¤šæ ·æ€§æµ‹é‡ï¼Œå®ç°äº†ç²¾ç¡®çš„æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§æ§åˆ¶ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿç†µåŸºæ–¹æ³•åœ¨å¤§è§„æ¨¡ç”Ÿæˆä»»åŠ¡ä¸­çš„è®¡ç®—ç“¶é¢ˆï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†æ¡ä»¶æ½œåœ¨ RKE åˆ†æ•°å¼•å¯¼æŠ€æœ¯ï¼Œå°†è®¡ç®—å’Œæ¢¯åº¦ä¼˜åŒ–å¤æ‚åº¦ä» $O(n^3)$ æ˜¾è‘—é™ä½è‡³ $O(n)$ã€‚è¿™ç§æ•ˆç‡æå‡ä½¿å¾—åœ¨æˆåƒä¸Šä¸‡æ¬¡é‡‡æ ·ä¸­å®æ–½å¤šæ ·æ€§å¼•å¯¼æˆä¸ºå¯èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒSPARKE åœ¨å¤šç§æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image) æ¨¡å‹ä¸­å‡èƒ½åœ¨ä¸å¢åŠ æ˜¾è‘—è®¡ç®—æˆæœ¬çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæå‡ç”Ÿæˆæ•°æ®çš„æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10173v2",
      "published_date": "2025-06-11 20:53:45 UTC",
      "updated_date": "2025-10-30 07:25:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:38.365340+00:00"
    },
    {
      "arxiv_id": "2506.10172v1",
      "title": "A Navigation Framework Utilizing Vision-Language Models",
      "title_zh": "ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¼èˆªæ¡†æ¶",
      "authors": [
        "Yicheng Duan",
        "Kaiyu tang"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)çš„å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€å¯¼èˆª(Vision-and-Language Navigation, VLN)ä¸­è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç†è§£ä¸å¤æ‚ç¯å¢ƒå¯¼èˆªçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–ã€å³æ’å³ç”¨çš„è®¾è®¡ï¼Œå°†è§†è§‰è¯­è¨€ç†è§£ä¸åŠ¨ä½œè§„åˆ’è§£è€¦ï¼Œé€šè¿‡é›†æˆå†»ç»“çš„ Qwen2.5-VL-7B-Instruct æ¨¡å‹ä¸è½»é‡åŒ–è§„åˆ’é€»è¾‘ï¼Œå®ç°äº†æ— éœ€å¤§è§„æ¨¡å¾®è°ƒçš„çµæ´»å¯¼èˆªã€‚ç³»ç»Ÿåˆ©ç”¨æç¤ºå·¥ç¨‹(Prompt Engineering)ã€ç»“æ„åŒ–å†å²ç®¡ç†å’ŒåŒå¸§è§†è§‰è¾“å…¥ç­–ç•¥ï¼Œå¢å¼ºäº†å¯¼èˆªæ­¥éª¤é—´çš„å†³ç­–è¿ç»­æ€§ã€‚ç ”ç©¶åœ¨ Matterport3D æ•°æ®é›†å’Œ Habitat-Lab ä»¿çœŸç¯å¢ƒä¸‹çš„ Room-to-Room åŸºå‡†æµ‹è¯•(VLN-CE)ä¸­å¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨ä¸¥æ ¼è¯„ä¼°è®¾ç½®ä¸‹æ¨¡å‹åœ¨æœªçŸ¥ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†è¯¥æ¨¡å—åŒ–æ–¹æ³•ä¸ºæ„å»ºå¯æ‰©å±•ä¸”é«˜æ•ˆçš„å¯¼èˆªç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚è¿™ä¸€å·¥ä½œçªå‡ºäº†é€šè¿‡å¢å¼ºç¯å¢ƒå…ˆéªŒå’Œæ‰©å±•å¤šæ¨¡æ€è¾“å…¥é›†æˆæ¥æå‡å…·èº«æ™ºèƒ½å¯¼èˆªæ€§èƒ½çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10172v1",
      "published_date": "2025-06-11 20:51:58 UTC",
      "updated_date": "2025-06-11 20:51:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:38.826249+00:00"
    },
    {
      "arxiv_id": "2506.10171v3",
      "title": "Beyond Jailbreaking: Auditing Contextual Privacy in LLM Agents",
      "title_zh": "è¶…è¶Šè¶Šç‹±ï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸­çš„ä¸Šä¸‹æ–‡éšç§å®¡è®¡",
      "authors": [
        "Saswat Das",
        "Jameson Sandler",
        "Ferdinando Fioretto"
      ],
      "abstract": "LLM agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. Moreover, these disclosures go beyond mere explicit disclosure, leaving open avenues for gradual manipulation or sidechannel information leakage. This study proposes an auditing framework for conversational privacy that quantifies an agent's susceptibility to these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework is designed to stress-test agents that enforce strict privacy directives against an iterative probing strategy. Rather than focusing solely on a single disclosure event or purely explicit leakage, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrates the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses, along with an in-depth longitudinal study of the temporal dynamics of leakage, strategies adopted by adaptive adversaries, and the evolution of adversarial beliefs about sensitive targets. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ (LLM agents) åœ¨ä½œä¸ºä¸ªäººåŠ©ç†æˆ–åŒ»ç–—åŠ©æ‰‹æ—¶é¢ä¸´çš„éšç§æ³„éœ²é£é™©ï¼ŒæŒ‡å‡ºé™¤äº†æ˜¾æ€§æŠ«éœ²å¤–ï¼Œæ™ºèƒ½ä½“è¿˜ææ˜“å—åˆ°æ¸è¿›å¼æ“çºµæˆ–ä¾§ä¿¡é“ (side-channel) ä¿¡æ¯æ³„éœ²çš„å¨èƒã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸º CMPL (Conversational Manipulation for Privacy Leakage) çš„å®¡è®¡æ¡†æ¶ï¼Œç”¨äºé‡åŒ–æ™ºèƒ½ä½“åœ¨æ‰§è¡Œä¸¥æ ¼éšç§æŒ‡ä»¤æ—¶å¯¹è¿­ä»£æ¢æµ‹ç­–ç•¥çš„æ•æ„Ÿæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹ŸçœŸå®çš„å¤šè½®äº¤äº’ (multi-turn interactions) ç³»ç»Ÿæ€§åœ°æ­ç¤ºæ½œåœ¨æ¼æ´ï¼Œè€Œéä»…å±€é™äºå•æ¬¡æˆ–æ˜¾æ€§çš„æ³„éœ²äº‹ä»¶ã€‚å®éªŒè¯„ä¼°æ¶µç›–äº†å¤šç§é¢†åŸŸå’Œæ•°æ®æ¨¡æ€ï¼Œç»“æœè¡¨æ˜ç°æœ‰çš„å•è½®é˜²å¾¡ (single-turn defenses) æ‰‹æ®µéš¾ä»¥åº”å¯¹ CMPL æ­ç¤ºçš„é£é™©ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†æ³„éœ²çš„æ—¶é—´åŠ¨æ€ã€è‡ªé€‚åº”å¯¹æ‰‹çš„ç­–ç•¥æ¼”å˜ä»¥åŠå¯¹æŠ—æ€§ä¿¡å¿µçš„å½¢æˆè¿‡ç¨‹ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè´¡çŒ®äº†ä¸€å¥—åŸºäºå¯é‡åŒ–é£é™©æŒ‡æ ‡çš„å®¡è®¡ç¨‹åºï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å¯¹è¯éšç§çš„å¼€æ”¾åŸºå‡† (open benchmark)ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10171v3",
      "published_date": "2025-06-11 20:47:37 UTC",
      "updated_date": "2025-09-27 20:28:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:45.178776+00:00"
    },
    {
      "arxiv_id": "2506.11164v3",
      "title": "Synthetic Geology: Structural Geology Meets Deep Learning",
      "title_zh": "åˆæˆåœ°è´¨å­¦ï¼šæ„é€ åœ°è´¨å­¦ä¸æ·±åº¦å­¦ä¹ çš„ç»“åˆ",
      "authors": [
        "Simon Ghyselincks",
        "Valeriia Okhmak",
        "Stefano Zampini",
        "George Turkiyyah",
        "David Keyes",
        "Eldad Haber"
      ],
      "abstract": "Reconstructing the structural geology and mineral composition of the first few kilometers of the Earth's subsurface from sparse or indirect surface observations remains a long-standing challenge with critical applications in mineral exploration, geohazard assessment, and geotechnical engineering. This inherently ill-posed problem is often addressed by classical geophysical inversion methods, which typically yield a single maximum-likelihood model that fails to capture the full range of plausible geology. The adoption of modern deep learning methods has been limited by the lack of large 3D training datasets. We address this gap with \\textit{StructuralGeo}, a geological simulation engine that mimics eons of tectonic, magmatic, and sedimentary processes to generate a virtually limitless supply of realistic synthetic 3D lithological models. Using this dataset, we train both unconditional and conditional generative flow-matching models with a 3D attention U-Net architecture. The resulting foundation model can reconstruct multiple plausible 3D scenarios from surface topography and sparse borehole data, depicting structures such as layers, faults, folds, and dikes. By sampling many reconstructions from the same observations, we introduce a probabilistic framework for estimating the size and extent of subsurface features. While the realism of the output is bounded by the fidelity of the training data to true geology, this combination of simulation and generative AI functions offers a flexible prior for probabilistic modeling, regional fine-tuning, and use as an AI-based regularizer in traditional geophysical inversion workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»ç¨€ç–æˆ–é—´æ¥è§‚æµ‹ä¸­é‡å»ºåœ°çƒæ·±éƒ¨æ„é€ åœ°è´¨çš„éš¾é¢˜ï¼Œæå‡ºäº†ç»“åˆæ¨¡æ‹ŸæŠ€æœ¯ä¸æ·±åº¦å­¦ä¹ (Deep Learning)çš„Synthetic Geologyæ¡†æ¶ã€‚ç ”ç©¶è€…å¼€å‘äº†åœ°è´¨æ¨¡æ‹Ÿå¼•æ“StructuralGeoï¼Œé€šè¿‡æ¨¡æ‹Ÿé•¿æœŸçš„åœ°è´¨æ¼”åŒ–è¿‡ç¨‹ç”Ÿæˆäº†å¤§è§„æ¨¡çš„åˆæˆ3Då²©çŸ³æ¨¡å‹æ•°æ®é›†ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶è®­ç»ƒäº†é‡‡ç”¨3D Attention U-Netæ¶æ„çš„ç”Ÿæˆå¼Flow-MatchingåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®åœ°è¡¨åœ°å½¢å’Œç¨€ç–é’»å­”æ•°æ®é‡å»ºåŒ…å«å±‚ç†ã€æ–­å±‚ã€è¤¶çš±å’Œå²©è„‰ç­‰ç»“æ„çš„å¤šç§åˆç†åœ°è´¨åœºæ™¯ã€‚é€šè¿‡å¯¹å¤šä¸ªé‡å»ºæ–¹æ¡ˆè¿›è¡Œé‡‡æ ·ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç”¨äºä¼°è®¡åœ°ä¸‹ç‰¹å¾å°ºå¯¸å’ŒèŒƒå›´çš„æ¦‚ç‡åŒ–æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†åœ°çƒç‰©ç†åæ¼”ä¸­çš„å¤šè§£æ€§é—®é¢˜ã€‚è¿™ç§æ¨¡æ‹Ÿä¸ç”Ÿæˆå¼AIçš„ç»“åˆä¸ºåœ°è´¨å»ºæ¨¡æä¾›äº†çµæ´»çš„å…ˆéªŒçŸ¥è¯†ï¼Œä¸ä»…æ”¯æŒåŒºåŸŸæ€§çš„å¾®è°ƒï¼Œè¿˜å¯ä½œä¸ºä¼ ç»Ÿåœ°çƒç‰©ç†åæ¼”(Geophysical Inversion)å·¥ä½œæµä¸­çš„AIæ­£åˆ™åŒ–é¡¹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in Journal of Geophysical Research: Machine Learning and Computation (2026). 16 pages, 9 figures, geological simulation code at https://doi.org/10.5281/zenodo.15244035, generative AI code at https://github.com/chipnbits/flowtrain_stochastic_interpolation/releases/tag/v1.0.2",
      "pdf_url": "https://arxiv.org/pdf/2506.11164v3",
      "published_date": "2025-06-11 20:42:28 UTC",
      "updated_date": "2026-01-19 06:11:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:47.430937+00:00"
    },
    {
      "arxiv_id": "2506.10161v1",
      "title": "Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦ç”Ÿæˆä¼˜è´¨æ•…äº‹ï¼ŸåŸºäºå™äº‹è§„åˆ’è§†è§’çš„è§è§£ä¸æŒ‘æˆ˜",
      "authors": [
        "Yi Wang",
        "Max Kreminski"
      ],
      "abstract": "Story generation has been a prominent application of Large Language Models (LLMs). However, understanding LLMs' ability to produce high-quality stories remains limited due to challenges in automatic evaluation methods and the high cost and subjectivity of manual evaluation. Computational narratology offers valuable insights into what constitutes a good story, which has been applied in the symbolic narrative planning approach to story generation. This work aims to deepen the understanding of LLMs' story generation capabilities by using them to solve narrative planning problems. We present a benchmark for evaluating LLMs on narrative planning based on literature examples, focusing on causal soundness, character intentionality, and dramatic conflict. Our experiments show that GPT-4 tier LLMs can generate causally sound stories at small scales, but planning with character intentionality and dramatic conflict remains challenging, requiring LLMs trained with reinforcement learning for complex reasoning. The results offer insights on the scale of stories that LLMs can generate while maintaining quality from different aspects. Our findings also highlight interesting problem solving behaviors and shed lights on challenges and considerations for applying LLM narrative planning in game environments.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å™äº‹è§„åˆ’(narrative planning)çš„è§’åº¦æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆé«˜è´¨é‡æ•…äº‹çš„èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³å½“å‰è‡ªåŠ¨è¯„ä¼°æ–¹æ³•å—é™ä»¥åŠäººå·¥è¯„ä¼°æˆæœ¬é«˜ä¸”ä¸»è§‚çš„é—®é¢˜ã€‚ä½œè€…å€Ÿé‰´è®¡ç®—å™äº‹å­¦(computational narratology)çš„ç†è®ºï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ–‡å­¦èŒƒä¾‹çš„è¯„ä¼°åŸºå‡†ï¼Œé‡ç‚¹è€ƒå¯Ÿæ•…äº‹çš„å› æœåˆç†æ€§(causal soundness)ã€è§’è‰²æ„å›¾æ€§(character intentionality)å’Œæˆå‰§å†²çª(dramatic conflict)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»¥GPT-4ä¸ºä»£è¡¨çš„é«˜å±‚çº§LLMsåœ¨å°è§„æ¨¡æ•…äº‹ç”Ÿæˆä¸­èƒ½å¤Ÿä¿æŒè‰¯å¥½çš„å› æœåˆç†æ€§ï¼Œä½†åœ¨å¤„ç†æ¶‰åŠè§’è‰²æ„å›¾å’Œæˆå‰§å†²çªçš„å¤æ‚å™äº‹è§„åˆ’æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦é€šè¿‡å¼ºåŒ–å­¦ä¹ (reinforcement learning)è®­ç»ƒä»¥å¢å¼ºå…¶å¤æ‚æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨ä¿æŒä¸åŒç»´åº¦æ•…äº‹è´¨é‡æ—¶æ‰€èƒ½è¾¾åˆ°çš„è§„æ¨¡é™åˆ¶ï¼Œå¹¶ä¸ºåœ¨æ¸¸æˆç¯å¢ƒç­‰å®é™…åº”ç”¨åœºæ™¯ä¸­éƒ¨ç½²LLMå™äº‹è§„åˆ’æä¾›äº†é‡è¦è§è§£å’ŒæŒ‘æˆ˜è€ƒé‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "In 2025 IEEE Conference on Games (CoG)",
      "pdf_url": "https://arxiv.org/pdf/2506.10161v1",
      "published_date": "2025-06-11 20:27:08 UTC",
      "updated_date": "2025-06-11 20:27:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:56.925800+00:00"
    },
    {
      "arxiv_id": "2506.10157v3",
      "title": "One Patient, Many Contexts: Scaling Medical AI with Contextual Intelligence",
      "title_zh": "ä¸€æ‚£å¤šå¢ƒï¼šåˆ©ç”¨æƒ…å¢ƒæ™ºèƒ½å®ç°åŒ»ç–—äººå·¥æ™ºèƒ½çš„è§„æ¨¡åŒ–æ‰©å±•",
      "authors": [
        "Michelle M. Li",
        "Ben Y. Reis",
        "Adam Rodman",
        "Tianxi Cai",
        "Noa Dagan",
        "Ran D. Balicer",
        "Joseph Loscalzo",
        "Isaac S. Kohane",
        "Marinka Zitnik"
      ],
      "abstract": "Medical AI, including clinical language models, vision-language models, and multimodal health record models, already summarizes notes, answers questions, and supports decisions. Their adaptation to new populations, specialties, or care settings often relies on fine-tuning, prompting, or retrieval from external knowledge bases. These strategies can scale poorly and risk contextual errors: outputs that appear plausible but miss critical patient or situational information. We envision context switching as a solution. Context switching adjusts model reasoning at inference without retraining. Generative models can tailor outputs to patient biology, care setting, or disease. Multimodal models can reason on notes, laboratory results, imaging, and genomics, even when some data are missing or delayed. Agent models can coordinate tools and roles based on tasks and users. In each case, context switching enables medical AI to adapt across specialties, populations, and geographies. It requires advances in data design, model architectures, and evaluation frameworks, and establishes a foundation for medical AI that scales to infinitely many contexts while remaining reliable and suited to real-world care.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–— AI æ¨¡å‹åœ¨ä¸åŒäººç¾¤å’ŒåŒ»ç–—ç¯å¢ƒä¸­çš„æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿçš„å¾®è°ƒ(fine-tuning)å’Œæ£€ç´¢å¢å¼ºç­–ç•¥å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡é”™è¯¯(contextual errors)ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸Šä¸‹æ–‡åˆ‡æ¢(context switching)çš„æ¦‚å¿µï¼Œæ—¨åœ¨æ¨ç†é˜¶æ®µæ— éœ€é‡æ–°è®­ç»ƒå³å¯åŠ¨æ€è°ƒæ•´æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç”Ÿæˆå¼æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ‚£è€…ç”Ÿç‰©å­¦ç‰¹å¾ã€ç–¾ç—…ç±»å‹åŠå…·ä½“æŠ¤ç†ç¯å¢ƒå®šåˆ¶è¾“å‡ºï¼Œæ”¯æŒå¤šæ¨¡æ€(multimodal)æ¨¡å‹åœ¨æ•°æ®ç¼ºå¤±æˆ–å»¶è¿Ÿçš„æƒ…å†µä¸‹æ•´åˆç—…å†ã€å®éªŒç»“æœå’ŒåŸºå› ç»„å­¦ç­‰ä¿¡æ¯è¿›è¡Œæ·±åº¦æ¨ç†ã€‚æ­¤å¤–ï¼Œæ™ºèƒ½ä½“æ¨¡å‹(Agent models)å¯ä»¥æ ¹æ®ç‰¹å®šä»»åŠ¡å’Œç”¨æˆ·è§’è‰²åè°ƒä¸åŒå·¥å…·ï¼Œä½¿åŒ»ç–— AI èƒ½å¤Ÿè·¨ä¸“ç§‘ã€è·¨äººç¾¤å’Œè·¨åœ°åŸŸè¿›è¡Œè‡ªé€‚åº”æ‰©å±•ã€‚è¯¥ç ”ç©¶å¼ºè°ƒè¿™ä¸€èŒƒå¼è¦æ±‚åœ¨æ•°æ®è®¾è®¡ã€æ¨¡å‹æ¶æ„å’Œè¯„ä¼°æ¡†æ¶ä¸Šè¿›è¡ŒåŒæ­¥åˆ›æ–°ï¼Œä»è€Œä¸ºæ„å»ºå¯å¤§è§„æ¨¡æ‰©å±•ä¸”ç¬¦åˆçœŸå®ä¸´åºŠéœ€æ±‚çš„å¯é åŒ»ç–— AI å¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10157v3",
      "published_date": "2025-06-11 20:23:57 UTC",
      "updated_date": "2025-11-27 16:35:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:56:58.315128+00:00"
    },
    {
      "arxiv_id": "2506.10155v1",
      "title": "Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities",
      "title_zh": "ä¼ä¸šäººåŠ›èµ„æœ¬æŠ«éœ²çš„æµ‹åº¦ï¼šè¯åº“ã€æ•°æ®ã€ä»£ç ä¸ç ”ç©¶æœºé‡",
      "authors": [
        "Elizabeth Demers",
        "Victor Xiaoqi Wang",
        "Kean Wu"
      ],
      "abstract": "Human capital (HC) is increasingly important to corporate value creation. Unlike other assets, however, HC is not currently subject to well-defined measurement or disclosure rules. We use a machine learning algorithm (word2vec) trained on a confirmed set of HC disclosures to develop a comprehensive list of HC-related keywords classified into five subcategories (DEI; health and safety; labor relations and culture; compensation and benefits; and demographics and other) that capture the multidimensional nature of HC management. We share our lexicon, corporate HC disclosures, and the Python code used to develop the lexicon, and we provide detailed examples of using our data and code, including for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the code to capture another construct of interest) with their samples of corporate communications to address pertinent HC questions. We close with a discussion of future research opportunities related to HC management and disclosure.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†ä¼ä¸šäººåŠ›èµ„æœ¬(Human Capital, HC)æŠ«éœ²çš„é‡è¦æ€§ï¼Œå¹¶é’ˆå¯¹ç›®å‰ç¼ºä¹æ˜ç¡®è¡¡é‡æˆ–æŠ«éœ²è§„åˆ™çš„ç°çŠ¶æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ä½œè€…åˆ©ç”¨åœ¨å·²ç¡®è®¤çš„ HC æŠ«éœ²æ•°æ®é›†ä¸Šè®­ç»ƒçš„æœºå™¨å­¦ä¹ ç®—æ³• word2vecï¼Œå¼€å‘äº†ä¸€å¥—æ¶µç›–å¤šæ ·æ€§ã€å…¬å¹³ä¸åŒ…å®¹(DEI)ã€å¥åº·ä¸å®‰å…¨ã€åŠ³èµ„å…³ç³»ä¸æ–‡åŒ–ã€è–ªé…¬ç¦åˆ©ä»¥åŠäººå£ç»Ÿè®¡ç­‰äº”ä¸ªå­ç»´åº¦çš„ç»¼åˆæ€§å…³é”®è¯åˆ—è¡¨ã€‚ç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†è¯¥è¯åº“(lexicon)ã€ä¼ä¸š HC æŠ«éœ²æ•°æ®ä»¥åŠç”¨äºå¼€å‘è¯åº“çš„ Python ä»£ç ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™äº›èµ„æºè¿›è¡Œ BERT æ¨¡å‹çš„å¾®è°ƒã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¯¥ HC è¯åº“æˆ–é€šè¿‡ä¿®æ”¹ä»£ç æ¥åˆ†æå„ç±»ä¼ä¸šæ²Ÿé€šæ–‡æœ¬ï¼Œè¿›è€Œæ¢è®¨äººåŠ›èµ„æœ¬ç®¡ç†ä¸­çš„å…³é”®é—®é¢˜ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºè¡¡é‡äººåŠ›èµ„æœ¬æä¾›äº†å¤šç»´åº¦çš„é‡åŒ–å·¥å…·ï¼Œè¿˜ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "50 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.10155v1",
      "published_date": "2025-06-11 20:18:37 UTC",
      "updated_date": "2025-06-11 20:18:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:11.426643+00:00"
    },
    {
      "arxiv_id": "2506.10139v1",
      "title": "Unsupervised Elicitation of Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹çš„æ— ç›‘ç£èƒ½åŠ›å¯å‘",
      "authors": [
        "Jiaxin Wen",
        "Zachary Ankner",
        "Arushi Somani",
        "Peter Hase",
        "Samuel Marks",
        "Jacob Goldman-Wetzler",
        "Linda Petrini",
        "Henry Sleight",
        "Collin Burns",
        "He He",
        "Shi Feng",
        "Ethan Perez",
        "Jan Leike"
      ],
      "abstract": "To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \\emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Internal Coherence Maximization (ICM)ï¼Œä¸€ç§å…¨æ–°çš„æ— ç›‘ç£ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯­è¨€æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ ‡ç­¾è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå½»åº•æ‘†è„±å¯¹å¤–éƒ¨ç›‘ç£(External Supervision)çš„ä¾èµ–ã€‚ä¸ºäº†è§£å†³äººç±»éš¾ä»¥å¯¹å…·æœ‰è¶…äººç±»èƒ½åŠ›(Superhuman Capabilities)çš„æ¨¡å‹æä¾›é«˜è´¨é‡æŒ‡å¯¼çš„éš¾é¢˜ï¼ŒICMé€šè¿‡ä¼˜åŒ–æ¨¡å‹ç”Ÿæˆçš„æ ‡ç­¾æ¥æå‡å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨GSM8k-verificationã€TruthfulQAå’ŒAlpacaå¥–åŠ±æ¨¡å‹ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¸ä»…è¾¾åˆ°äº†é‡‘æ ‡å‡†ç›‘ç£çš„æ•ˆæœï¼Œè¿˜æ˜¾è‘—ä¼˜äºä¼—åŒ…çš„äººç±»ç›‘ç£ã€‚å½“ä»»åŠ¡éš¾åº¦è¶…è¿‡äººç±»æ°´å¹³æ—¶ï¼ŒICMèƒ½æ¯”äººç±»æ ‡ç­¾æ›´æœ‰æ•ˆåœ°æ¿€å‘æ¨¡å‹çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…åˆ©ç”¨è¯¥æ–¹æ³•è®­ç»ƒå‡ºçš„Claude 3.5 HaikuåŠ©æ‰‹å’Œå¥–åŠ±æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå‡è¶…è¶Šäº†ç”±äººç±»ç›‘ç£è®­ç»ƒçš„å¯¹åº”ç‰ˆæœ¬ï¼ŒéªŒè¯äº†æ— ç›‘ç£å¯å‘åœ¨å‰æ²¿è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10139v1",
      "published_date": "2025-06-11 19:40:08 UTC",
      "updated_date": "2025-06-11 19:40:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:20.025087+00:00"
    },
    {
      "arxiv_id": "2506.10138v2",
      "title": "Path Channels and Plan Extension Kernels: a Mechanistic Description of Planning in a Sokoban RNN",
      "title_zh": "è·¯å¾„é€šé“ä¸è§„åˆ’æ‰©å±•å·ç§¯æ ¸ï¼šSokoban RNN è§„åˆ’æœºåˆ¶çš„æœºç†æ€§æè¿°",
      "authors": [
        "Mohammad Taufeeque",
        "Aaron David Tucker",
        "Adam Gleave",
        "AdriÃ  Garriga-Alonso"
      ],
      "abstract": "We partially reverse-engineer a convolutional recurrent neural network (RNN) trained with model-free reinforcement learning to play the box-pushing game Sokoban. We find that the RNN stores future moves (plans) as activations in particular channels of the hidden state, which we call path channels. A high activation in a particular location means that, when a box is in that location, it will get pushed in the channel's assigned direction. We examine the convolutional kernels between path channels and find that they encode the change in position resulting from each possible action, thus representing part of a learned transition model. The RNN constructs plans by starting at the boxes and goals. These kernels extend activations in path channels forwards from boxes and backwards from the goal. Negative values are placed in channels at obstacles. This causes the extension kernels to propagate the negative value in reverse, thus pruning the last few steps and letting an alternative plan emerge; a form of backtracking. Our work shows that, a precise understanding of the plan representation allows us to directly understand the bidirectional planning-like algorithm learned by model-free training in more familiar terms.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡é€†å‘å·¥ç¨‹åˆ†æäº†ä¸€ä¸ªåˆ©ç”¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ (Model-Free Reinforcement Learning)è®­ç»ƒçš„å·ç§¯å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ï¼Œæ¢è®¨å…¶åœ¨è§£å†³æ¨ç®±å­æ¸¸æˆ(Sokoban)æ—¶çš„å†…éƒ¨æœºç†ã€‚ç ”ç©¶å‘ç°è¯¥ç½‘ç»œå°†æœªæ¥ç§»åŠ¨è®¡åˆ’å­˜å‚¨åœ¨éšè—çŠ¶æ€çš„ç‰¹å®šé€šé“ä¸­ï¼Œå¹¶å°†å…¶å‘½åä¸ºè·¯å¾„é€šé“(Path Channels)ï¼Œå…¶ä¸­é«˜æ¿€æ´»å€¼ä»£è¡¨äº†ç‰¹å®šä½ç½®ç®±å­çš„é¢„å®šæ¨é€æ–¹å‘ã€‚é€šè¿‡åˆ†æè·¯å¾„é€šé“é—´çš„å·ç§¯æ ¸(Convolutional Kernels)ï¼Œç ”ç©¶æ­ç¤ºäº†è¿™äº›æ ¸å¦‚ä½•ç¼–ç åŠ¨ä½œå¯¼è‡´çš„ä½ç§»ï¼Œä»è€Œæ„æˆäº†ä¸€ä¸ªå­¦ä¹ åˆ°çš„è½¬æ¢æ¨¡å‹(Transition Model)ã€‚ç½‘ç»œé€šè¿‡ä»ç®±å­å’Œç›®æ ‡ç‚¹å‡ºå‘ï¼Œåˆ©ç”¨è¿™äº›æ ¸åœ¨è·¯å¾„é€šé“ä¸­å‰åä¼ æ’­æ¿€æ´»ä¿¡å·æ¥æ„å»ºè§„åˆ’ã€‚å½“é‡åˆ°éšœç¢ç‰©æ—¶ï¼Œè´Ÿå€¼ä¿¡å·ä¼šåå‘ä¼ æ’­ä»¥å®ç°è·¯å¾„å‰ªæå’Œå›æº¯(Backtracking)ã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡å¯¹è®¡åˆ’è¡¨ç¤ºçš„ç²¾ç¡®ç†è§£ï¼Œå¯ä»¥ç›´è§‚åœ°æ­ç¤ºæ— æ¨¡å‹è®­ç»ƒå¦‚ä½•åœ¨ç¥ç»ç½‘ç»œä¸­å®ç°ç±»ä¼¼äºåŒå‘è§„åˆ’çš„ç®—æ³•é€»è¾‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at the Mechanistic Interpretability Workshop at NeurIPS 2025. 34 pages, 26 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10138v2",
      "published_date": "2025-06-11 19:36:17 UTC",
      "updated_date": "2025-12-04 18:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:27.396220+00:00"
    },
    {
      "arxiv_id": "2506.10137v2",
      "title": "Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning",
      "title_zh": "è¡Œä¸ºå…‹éš†ä¸­ç»„åˆæ³›åŒ–çš„è‡ªé¢„æµ‹è¡¨ç¤º",
      "authors": [
        "Daniel Lawson",
        "Adriana Hugessen",
        "Charlotte Cloutier",
        "Glen Berseth",
        "Khimya Khetarpal"
      ],
      "abstract": "While goal-conditioned behavior cloning (GCBC) methods can perform well on in-distribution training tasks, they do not necessarily generalize zero-shot to tasks that require conditioning on novel state-goal pairs, i.e. combinatorial generalization. In part, this limitation can be attributed to a lack of temporal consistency in the state representation learned by BC; if temporally correlated states are properly encoded to similar latent representations, then the out-of-distribution gap for novel state-goal pairs would be reduced. We formalize this notion by demonstrating how encouraging long-range temporal consistency via successor representations (SR) can facilitate generalization. We then propose a simple yet effective representation learning objective, $\\text{BYOL-}Î³$ for GCBC, which theoretically approximates the successor representation in the finite MDP case through self-predictive representations, and achieves competitive empirical performance across a suite of challenging tasks requiring combinatorial generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®æ ‡æ¡ä»¶è¡Œä¸ºå…‹éš†(Goal-conditioned behavior cloning, GCBC)åœ¨å¤„ç†æ–°é¢–çŠ¶æ€-ç›®æ ‡å¯¹æ—¶ï¼Œå³ç»„åˆæ³›åŒ–(combinatorial generalization)èƒ½åŠ›ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§å±€é™æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’å› äºBCå­¦ä¹ çš„çŠ¶æ€è¡¨ç¤ºç¼ºä¹æ—¶é—´ä¸€è‡´æ€§(temporal consistency)ã€‚ä½œè€…é€šè¿‡è¯æ˜åˆ©ç”¨ç»§ä»»è€…è¡¨ç¤º(successor representations, SR)æ¥é¼“åŠ±é•¿ç¨‹æ—¶é—´ä¸€è‡´æ€§å¯ä»¥æ˜¾è‘—ä¿ƒè¿›æ³›åŒ–ï¼Œä»è€Œå½¢å¼åŒ–äº†è¿™ä¸€è§‚ç‚¹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBYOL-$\\gamma$çš„è¡¨ç¤ºå­¦ä¹ ç›®æ ‡ï¼Œè¯¥ç›®æ ‡é€šè¿‡è‡ªé¢„æµ‹è¡¨ç¤º(self-predictive representations)åœ¨ç†è®ºä¸Šé€¼è¿‘æœ‰é™MDPæƒ…å½¢ä¸‹çš„ç»§ä»»è€…è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBYOL-$\\gamma$åœ¨å¤šé¡¹éœ€è¦ç»„åˆæ³›åŒ–çš„å¤æ‚ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10137v2",
      "published_date": "2025-06-11 19:32:41 UTC",
      "updated_date": "2025-10-15 15:29:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:19.223554+00:00"
    },
    {
      "arxiv_id": "2506.10130v3",
      "title": "A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI",
      "title_zh": "å…³äºç¬¦å·ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ç¡®å®šæ€§ä¸é€‚ç”¨èŒƒå›´é—´åŸºæœ¬æƒè¡¡çš„çŒœæƒ³",
      "authors": [
        "Luciano Floridi"
      ],
      "abstract": "This article introduces a conjecture that formalises a fundamental trade-off between provable correctness and broad data-mapping capacity in Artificial Intelligence (AI) systems. When an AI system is engineered for deductively watertight guarantees (demonstrable certainty about the error-free nature of its outputs) -- as in classical symbolic AI -- its operational domain must be narrowly circumscribed and pre-structured. Conversely, a system that can input high-dimensional data to produce rich information outputs -- as in contemporary generative models -- necessarily relinquishes the possibility of zero-error performance, incurring an irreducible risk of errors or misclassification. By making this previously implicit trade-off explicit and open to rigorous verification, the conjecture significantly reframes both engineering ambitions and philosophical expectations for AI. After reviewing the historical motivations for this tension, the article states the conjecture in information-theoretic form and contextualises it within broader debates in epistemology, formal verification, and the philosophy of technology. It then offers an analysis of its implications and consequences, drawing on notions of underdetermination, prudent epistemic risk, and moral responsibility. The discussion clarifies how, if correct, the conjecture would help reshape evaluation standards, governance frameworks, and hybrid system design. The conclusion underscores the importance of eventually proving or refuting the inequality for the future of trustworthy AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å¯è¯æ˜çš„æ­£ç¡®æ€§ï¼ˆCertaintyï¼‰ä¸å¹¿æ³›çš„æ•°æ®å¤„ç†èƒ½åŠ›ï¼ˆScopeï¼‰ä¹‹é—´å­˜åœ¨æ ¹æœ¬æƒè¡¡çš„çŒœæƒ³ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå½“AIç³»ç»Ÿè¿½æ±‚å¦‚ç»å…¸ç¬¦å·AIï¼ˆSymbolic AIï¼‰èˆ¬çš„æ¼”ç»ä¸¥å¯†æ€§æ—¶ï¼Œå…¶è¿è¡Œé¢†åŸŸå¿…é¡»å—åˆ°ä¸¥æ ¼é™åˆ¶ï¼›è€Œåƒç°ä»£ç”Ÿæˆå¼æ¨¡å‹ï¼ˆGenerative AIï¼‰è¿™æ ·èƒ½å¤Ÿå¤„ç†é«˜ç»´æ•°æ®å¹¶äº§ç”Ÿä¸°å¯Œä¿¡æ¯çš„ç³»ç»Ÿï¼Œåˆ™å¿…ç„¶æ”¾å¼ƒé›¶é”™è¯¯çš„å¯èƒ½æ€§ï¼Œå¹¶é¢ä¸´ä¸å¯è¿˜åŸçš„é”™è¯¯é£é™©ã€‚é€šè¿‡å°†è¿™ä¸€æƒè¡¡å½¢å¼åŒ–ä¸ºä¿¡æ¯è®ºï¼ˆInformation-theoreticï¼‰è¡¨è¾¾ï¼Œè¯¥çŒœæƒ³é‡æ–°å®¡è§†äº†AIå·¥ç¨‹ç›®æ ‡ä¸å“²å­¦é¢„æœŸï¼Œå¹¶å°†å…¶ç½®äºè®¤è¯†è®ºã€å½¢å¼éªŒè¯å’ŒæŠ€æœ¯å“²å­¦çš„å¹¿æ³›è®¨è®ºä¸­ã€‚æ–‡ä¸­è¿›ä¸€æ­¥åˆ†æäº†è¯¥çŒœæƒ³å¯¹è¯„ä¼°æ ‡å‡†ã€æ²»ç†æ¡†æ¶ä»¥åŠæ··åˆç³»ç»Ÿè®¾è®¡çš„æ·±è¿œå½±å“ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒäº†æ˜ç¡®è¿™ä¸€åŸºæœ¬é™åˆ¶å¯¹äºæœªæ¥æ„å»ºå¯ä¿¡AIï¼ˆTrustworthy AIï¼‰çš„å…³é”®æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "version 3",
      "pdf_url": "https://arxiv.org/pdf/2506.10130v3",
      "published_date": "2025-06-11 19:18:13 UTC",
      "updated_date": "2025-08-02 22:20:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:28.751949+00:00"
    },
    {
      "arxiv_id": "2506.10120v1",
      "title": "GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments",
      "title_zh": "GRAILï¼šé¢å‘åŠ¨æ€æ„ŸçŸ¥ç¯å¢ƒçš„å›¾ä¸»åŠ¨å­¦ä¹ åŸºå‡†",
      "authors": [
        "Maryam Khalid",
        "Akane Sano"
      ],
      "abstract": "Graph-based Active Learning (AL) leverages the structure of graphs to efficiently prioritize label queries, reducing labeling costs and user burden in applications like health monitoring, human behavior analysis, and sensor networks. By identifying strategically positioned nodes, graph AL minimizes data collection demands while maintaining model performance, making it a valuable tool for dynamic environments. Despite its potential, existing graph AL methods are often evaluated on static graph datasets and primarily focus on prediction accuracy, neglecting user-centric considerations such as sampling diversity, query fairness, and adaptability to dynamic settings. To bridge this gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate graph AL strategies in dynamic, real-world environments. GRAIL introduces novel metrics to assess sustained effectiveness, diversity, and user burden, enabling a comprehensive evaluation of AL methods under varying conditions. Extensive experiments on datasets featuring dynamic, real-life human sensor data reveal trade-offs between prediction performance and user burden, highlighting limitations in existing AL strategies. GRAIL demonstrates the importance of balancing node importance, query diversity, and network topology, providing an evaluation mechanism for graph AL solutions in dynamic environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†GRAILï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŠ¨æ€ã€çœŸå®æ„ŸçŸ¥è¯†ç¯å¢ƒä¸‹çš„å›¾ä¸»åŠ¨å­¦ä¹ (Graph Active Learning, AL)ç­–ç•¥çš„åŸºå‡†æ¡†æ¶ã€‚ç”±äºç°æœ‰çš„å›¾ALæ–¹æ³•å¤šåŸºäºé™æ€æ•°æ®é›†ä¸”è¿‡åº¦å…³æ³¨é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¿½ç•¥äº†é‡‡æ ·å¤šæ ·æ€§ã€æŸ¥è¯¢å…¬å¹³æ€§å’Œå¯¹åŠ¨æ€ç¯å¢ƒçš„é€‚åº”æ€§ç­‰ç”¨æˆ·ä¸­å¿ƒè§†è§’ï¼ŒGRAILé€šè¿‡å¼•å…¥è¯„ä¼°æŒç»­æœ‰æ•ˆæ€§ã€å¤šæ ·æ€§å’Œç”¨æˆ·è´Ÿæ‹…çš„æ–°æŒ‡æ ‡æ¥å¼¥è¡¥è¿™ä¸€ç¼ºå£ã€‚åœ¨åŒ…å«åŠ¨æ€äººç±»ä¼ æ„Ÿå™¨æ•°æ®çš„çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†é¢„æµ‹æ€§èƒ½ä¸ç”¨æˆ·è´Ÿæ‹…ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå¹¶å‡¸æ˜¾äº†ç°æœ‰ALç­–ç•¥çš„å±€é™æ€§ã€‚GRAILçš„å®éªŒç»“æœå¼ºè°ƒäº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¹³è¡¡èŠ‚ç‚¹é‡è¦æ€§ã€æŸ¥è¯¢å¤šæ ·æ€§ä»¥åŠç½‘ç»œæ‹“æ‰‘ç»“æ„çš„é‡è¦æ€§ï¼Œä¸ºå¼€å‘æ›´å…·é²æ£’æ€§çš„å›¾ALè§£å†³æ–¹æ¡ˆæä¾›äº†ä¸€å¥—å…¨é¢çš„è¯„ä¼°æœºåˆ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10120v1",
      "published_date": "2025-06-11 19:04:18 UTC",
      "updated_date": "2025-06-11 19:04:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:25.206872+00:00"
    },
    {
      "arxiv_id": "2506.10119v1",
      "title": "DetecÃ§Ã£o da PsorÃ­ase Utilizando VisÃ£o Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers",
      "title_zh": "åˆ©ç”¨è®¡ç®—æœºè§†è§‰è¿›è¡Œé“¶å±‘ç—…æ£€æµ‹ï¼šCNN ä¸ Vision Transformers çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Natanael Lucena",
        "FÃ¡bio S. da Silva",
        "Ricardo Rios"
      ],
      "abstract": "This paper presents a comparison of the performance of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying images containing lesions of psoriasis and diseases similar to it. Models pre-trained on ImageNet were adapted to a specific data set. Both achieved high predictive metrics, but the ViTs stood out for their superior performance with smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the best results, with an f1-score of 96.4%, and is recommended as the most efficient architecture for automated psoriasis detection. This article reinforces the potential of ViTs for medical image classification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) ä¸è§†è§‰è½¬æ¢å™¨ (Vision Transformers, ViTs) åœ¨é“¶å±‘ç—…åŠå…¶ç›¸ä¼¼çš®è‚¤ç—…æŸå›¾åƒçš„å¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç ”ç©¶é‡‡ç”¨äº†åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹å¹¶é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¯„ä¼°äº†ä¸åŒæ¶æ„åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸¤ç§æ¶æ„å‡å–å¾—äº†è¾ƒé«˜çš„é¢„æµ‹æŒ‡æ ‡ï¼Œä½† ViTs åœ¨æ¨¡å‹è§„æ¨¡è¾ƒå°æ—¶å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚å…¶ä¸­ï¼ŒDual Attention Vision Transformer-Base (DaViT-B) ä»¥ 96.4% çš„ f1-score å–å¾—äº†æœ€ä½³ç»“æœï¼Œè¢«æ¨èä¸ºç‰›çš®ç™£è‡ªåŠ¨æ£€æµ‹ä¸­æœ€æœ‰æ•ˆçš„æ¶æ„ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº† ViTs åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºè®¡ç®—æœºè¾…åŠ©çš®è‚¤ç—…è¯Šæ–­æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, in Portuguese language, 2 figures, 2 tables, and 4 formulas. To be published in the Proceedings of the LII Brazilian Integrated Software and Hardware Seminar 2025 (SEMISH 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.10119v1",
      "published_date": "2025-06-11 19:00:32 UTC",
      "updated_date": "2025-06-11 19:00:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:35.729696+00:00"
    },
    {
      "arxiv_id": "2506.10106v1",
      "title": "One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture",
      "title_zh": "One For Allï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç²¾å‡†å†œä¸šå¼‚æ„ä»»åŠ¡è§„åˆ’",
      "authors": [
        "Marcos Abel ZuzuÃ¡rregui",
        "Mustafa Melih Toslak",
        "Stefano Carpin"
      ],
      "abstract": "Artificial intelligence is transforming precision agriculture, offering farmers new tools to streamline their daily operations. While these technological advances promise increased efficiency, they often introduce additional complexity and steep learning curves that are particularly challenging for non-technical users who must balance tech adoption with existing workloads. In this paper, we present a natural language (NL) robotic mission planner that enables non-specialists to control heterogeneous robots through a common interface. By leveraging large language models (LLMs) and predefined primitives, our architecture seamlessly translates human language into intermediate descriptions that can be executed by different robotic platforms. With this system, users can formulate complex agricultural missions without writing any code. In the work presented in this paper, we extend our previous system tailored for wheeled robot mission planning through a new class of experiments involving robotic manipulation and computer vision tasks. Our results demonstrate that the architecture is both general enough to support a diverse set of robots and powerful enough to execute complex mission requests. This work represents a significant step toward making robotic automation in precision agriculture more accessible to non-technical users.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªç„¶è¯­è¨€(NL)æœºå™¨äººä»»åŠ¡è§„åˆ’å™¨ï¼Œæ—¨åœ¨è§£å†³ç²¾å‡†å†œä¸šä¸­å¼‚æ„æœºå™¨äººæ“ä½œå¤æ‚ä¸”å­¦ä¹ æ›²çº¿é™¡å³­çš„é—®é¢˜ã€‚è¯¥æ¶æ„é€šè¿‡ LLMs å’Œé¢„å®šä¹‰çš„ primitives å°†äººç±»æŒ‡ä»¤æ— ç¼è½¬åŒ–ä¸ºä¸­é—´æè¿°ï¼Œä½¿éæŠ€æœ¯ç”¨æˆ·æ— éœ€ç¼–å†™ä»£ç å³å¯æŒ‡æŒ¥å¤šç§æœºå™¨äººååŒå·¥ä½œã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å‰æœŸè½®å¼æœºå™¨äººè§„åˆ’çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†æ¶‰åŠæœºå™¨äººæ“çºµ(robotic manipulation)å’Œè®¡ç®—æœºè§†è§‰(computer vision)ä»»åŠ¡çš„åŠŸèƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¶æ„å…·æœ‰æ”¯æŒå¤šæ ·åŒ–æœºå™¨äººå¹³å°çš„é€šç”¨æ€§ï¼Œå¹¶èƒ½é«˜æ•ˆæ‰§è¡Œå¤æ‚çš„å†œä¸šä»»åŠ¡ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡é™ä½æŠ€æœ¯é—¨æ§›ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººè‡ªåŠ¨åŒ–åœ¨ç²¾å‡†å†œä¸šé¢†åŸŸçš„æ˜“ç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to International Federation of Automatic Control (IFAC) Sensing, Control and Automation Technologies for Agriculture - 8th AGRICONTROL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10106v1",
      "published_date": "2025-06-11 18:45:44 UTC",
      "updated_date": "2025-06-11 18:45:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:36.293243+00:00"
    },
    {
      "arxiv_id": "2506.10102v1",
      "title": "Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach",
      "title_zh": "å›¾ä¸Šåä½œå­¦ä¹ ï¼šä¸€ç§é€‰æ‹©æ€§è”é‚¦å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Ahmed Elbakary",
        "Chaouki Ben Issaid",
        "Mehdi Bennis"
      ],
      "abstract": "We present a novel federated multi-task learning method that leverages cross-client similarity to enable personalized learning for each client. To avoid transmitting the entire model to the parameter server, we propose a communication-efficient scheme that introduces a feature anchor, a compact vector representation that summarizes the features learned from the client's local classes. This feature anchor is shared with the server to account for local clients' distribution. In addition, the clients share the classification heads, a lightweight linear layer, and perform a graph-based regularization to enable collaboration among clients. By modeling collaboration between clients as a dynamic graph and continuously updating and refining this graph, we can account for any drift from the clients. To ensure beneficial knowledge transfer and prevent negative collaboration, we leverage a community detection-based approach that partitions this dynamic graph into homogeneous communities, maximizing the sum of task similarities, represented as the graph edges' weights, within each community. This mechanism restricts collaboration to highly similar clients within their formed communities, ensuring positive interaction and preserving personalization. Extensive experiments on two heterogeneous datasets demonstrate that our method significantly outperforms state-of-the-art baselines. Furthermore, we show that our method exhibits superior computation and communication efficiency and promotes fairness across clients.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ Federated Multi-Task Learning æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å®¢æˆ·ç«¯é—´çš„äº¤å‰ç›¸ä¼¼æ€§æ¥å®ç°ä¸ªæ€§åŒ–å­¦ä¹ ã€‚ä¸ºæå‡é€šä¿¡æ•ˆç‡ï¼Œè¯¥æ–¹æ¡ˆå¼•å…¥äº† Feature Anchor ä½œä¸ºç´§å‡‘çš„å‘é‡è¡¨ç¤ºï¼Œå¹¶é…åˆè½»é‡çº§çš„ Classification Heads å…±äº«åŠ Graph-based Regularization æœºåˆ¶ã€‚é€šè¿‡å°†åä½œå»ºæ¨¡ä¸ºåŠ¨æ€å›¾å¹¶é‡‡ç”¨åŸºäº Community Detection çš„åˆ’åˆ†ç­–ç•¥ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«åŒè´¨ç¤¾åŒºå¹¶é™åˆ¶åœ¨é«˜åº¦ç›¸ä¼¼çš„å®¢æˆ·ç«¯é—´è¿›è¡Œåä½œï¼Œä»è€Œé¿å…äº†è´Ÿé¢åä½œå¹¶ä¿æŒäº†ä¸ªæ€§åŒ–ã€‚åœ¨ä¸¤ä¸ªå¼‚æ„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†ï¼Œå¹¶åœ¨è®¡ç®—ã€é€šä¿¡æ•ˆç‡åŠå®¢æˆ·ç«¯å…¬å¹³æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10102v1",
      "published_date": "2025-06-11 18:39:18 UTC",
      "updated_date": "2025-06-11 18:39:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:39.405245+00:00"
    },
    {
      "arxiv_id": "2506.10093v1",
      "title": "Leveraging LLMs for Mission Planning in Precision Agriculture",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°ç²¾å‡†å†œä¸šä»»åŠ¡è§„åˆ’",
      "authors": [
        "Marcos Abel ZuzuÃ¡rregui",
        "Stefano Carpin"
      ],
      "abstract": "Robotics and artificial intelligence hold significant potential for advancing precision agriculture. While robotic systems have been successfully deployed for various tasks, adapting them to perform diverse missions remains challenging, particularly because end users often lack technical expertise. In this paper, we present an end-to-end system that leverages large language models (LLMs), specifically ChatGPT, to enable users to assign complex data collection tasks to autonomous robots using natural language instructions. To enhance reusability, mission plans are encoded using an existing IEEE task specification standard, and are executed on robots via ROS2 nodes that bridge high-level mission descriptions with existing ROS libraries. Through extensive experiments, we highlight the strengths and limitations of LLMs in this context, particularly regarding spatial reasoning and solving complex routing challenges, and show how our proposed implementation overcomes them.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç²¾å‡†å†œä¸š (Precision Agriculture) ä¸­éæŠ€æœ¯ç”¨æˆ·éš¾ä»¥æŒ‡æŒ¥æœºå™¨äººæ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä»¥ ChatGPT ä¸ºæ ¸å¿ƒï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å‘è‡ªä¸»æœºå™¨äººåˆ†é…å¤æ‚çš„æ•°æ®é‡‡é›†ä»»åŠ¡ã€‚ä¸ºäº†å¢å¼ºæ–¹æ¡ˆçš„é‡ç”¨æ€§ï¼Œç ”ç©¶é‡‡ç”¨ç°æœ‰çš„ IEEE ä»»åŠ¡è§„èŒƒæ ‡å‡†å¯¹ä»»åŠ¡è®¡åˆ’è¿›è¡Œç¼–ç ï¼Œå¹¶åˆ©ç”¨ ROS2 èŠ‚ç‚¹å®ç°äº†é«˜çº§ä»»åŠ¡æè¿°ä¸ ROS åº“ä¹‹é—´çš„æœ‰æ•ˆè¡”æ¥ã€‚å®éªŒç»“æœè¯¦ç»†åˆ†æäº† LLMs åœ¨å¤„ç†ç©ºé—´æ¨ç† (Spatial Reasoning) å’Œå¤æ‚è·¯å¾„è§„åˆ’æŒ‘æˆ˜æ—¶çš„ä¼˜ç¼ºç‚¹ã€‚ç ”ç©¶æœ€ç»ˆå±•ç¤ºäº†æ‰€ææ¡†æ¶å¦‚ä½•æœ‰æ•ˆå…‹æœè¿™äº›å±€é™æ€§ï¼Œä¸ºéä¸“å®¶ç”¨æˆ·åœ¨å†œä¸šç¯å¢ƒä¸­ä½¿ç”¨è‡ªä¸»æœºå™¨äººæä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Published in Proceedings of 2025 International Conference on Robotics and Automation (ICRA)",
      "pdf_url": "https://arxiv.org/pdf/2506.10093v1",
      "published_date": "2025-06-11 18:25:23 UTC",
      "updated_date": "2025-06-11 18:25:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:57:51.577655+00:00"
    },
    {
      "arxiv_id": "2506.10085v4",
      "title": "VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models",
      "title_zh": "VITAï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶è‡ªé€‚åº”çš„é›¶æ ·æœ¬ä»·å€¼å‡½æ•°",
      "authors": [
        "Christos Ziakas",
        "Alessandra Russo"
      ],
      "abstract": "Vision-Language Models (VLMs) show promise as zero-shot goal-conditioned value functions, but their frozen pre-trained representations limit generalization and temporal reasoning. We introduce VITA, a zero-shot value function learning method that enhances both capabilities via test-time adaptation. At inference, a lightweight adaptation module is updated via a gradient step on a meta-learned self-supervised loss, such that each test-time update improves value estimation. By updating sequentially over a trajectory, VITA encodes history into its parameters, addressing the temporal reasoning limitations. To mitigate shortcut learning, we propose a dissimilarity-based sampling strategy that selects semantically diverse segments of the trajectory during training. In real-world robotic manipulation tasks, VITA generalizes from a single training environment to diverse out-of-distribution tasks, environments, and embodiments, outperforming the state-of-the-art zero-shot method using autoregressive VLMs. Furthermore, we demonstrate that VITA's zero-shot value estimates can be utilized for reward shaping in offline reinforcement learning, resulting in multi-task policies on the Meta-World benchmark that exceed the performance of those trained with the simulation's fuzzy-logic dense rewards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VITAï¼Œä¸€ç§é€šè¿‡æµ‹è¯•æ—¶è‡ªé€‚åº”(Test-Time Adaptation)æå‡è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)ä½œä¸ºé›¶æ ·æœ¬(Zero-Shot)ç›®æ ‡æ¡ä»¶ä»·å€¼å‡½æ•°èƒ½åŠ›çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒæ¨¡å‹åœ¨æ³›åŒ–å’Œæ—¶é—´æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒVITAåˆ©ç”¨å…ƒå­¦ä¹ (Meta-learned)çš„è‡ªç›‘ç£æŸå¤±å¯¹è½»é‡åŒ–é€‚é…æ¨¡å—è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œé€šè¿‡åœ¨è½¨è¿¹ä¸Šçš„é¡ºåºæ›´æ–°å°†å†å²ä¿¡æ¯ç¼–ç è¿›å‚æ•°ä¸­ï¼Œæœ‰æ•ˆå…‹æœäº†æ—¶é—´æ¨ç†çš„ä¸è¶³ã€‚ä¸ºäº†é˜²æ­¢æ¨¡å‹å­¦ä¹ æ·å¾„ï¼Œç ”ç©¶è€…è¿˜å¼•å…¥äº†åŸºäºä¸ç›¸ä¼¼åº¦çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿åœ¨è®­ç»ƒæœŸé—´é€‰æ‹©è¯­ä¹‰å¤šæ ·åŒ–çš„è½¨è¿¹ç‰‡æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç°å®ä¸–ç•Œçš„æœºå™¨äººæ“çºµä»»åŠ¡ä¸­ï¼ŒVITAå±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿè·¨ä»»åŠ¡ã€ç¯å¢ƒå’Œå®ä½“è¿è¡Œï¼Œå…¶æ€§èƒ½ä¼˜äºç›®å‰å…ˆè¿›çš„è‡ªå›å½’VLMsæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒVITAç”Ÿæˆçš„ä»·å€¼ä¼°è®¡å¯ç”¨äºç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)çš„å¥–åŠ±å¡‘é€ (Reward Shaping)ï¼Œåœ¨Meta-WorldåŸºå‡†æµ‹è¯•ä¸­å®ç°çš„ç­–ç•¥æ€§èƒ½ç”šè‡³è¶…è¿‡äº†ä½¿ç”¨ä»¿çœŸå¯†é›†å¥–åŠ±è®­ç»ƒçš„æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10085v4",
      "published_date": "2025-06-11 18:05:33 UTC",
      "updated_date": "2025-11-26 20:19:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:05.836282+00:00"
    },
    {
      "arxiv_id": "2506.10077v2",
      "title": "A quantum semantic framework for natural language processing",
      "title_zh": "ä¸€ç§é¢å‘è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡å­è¯­ä¹‰æ¡†æ¶",
      "authors": [
        "Christopher J. Agostino",
        "Quan Le Thien",
        "Molly Apsel",
        "Denizhan Pak",
        "Elina Lesyk",
        "Ashabari Majumdar"
      ],
      "abstract": "Semantic degeneracy represents a fundamental property of natural language that extends beyond simple polysemy to encompass the combinatorial explosion of potential interpretations that emerges as semantic expressions increase in complexity. In this work, we argue this property imposes fundamental limitations on Large Language Models (LLMs) and other modern NLP systems, precisely because they operate within natural language itself. Using Kolmogorov complexity, we demonstrate that as an expression's complexity grows, the amount of contextual information required to reliably resolve its ambiguity explodes combinatorially. The computational intractability of recovering a single intended meaning for complex or ambiguous text therefore suggests that the classical view that linguistic forms possess intrinsic meaning in and of themselves is conceptually inadequate. We argue instead that meaning is dynamically actualized through an observer-dependent interpretive act, a process whose non-deterministic nature is most appropriately described by a non-classical, quantum-like logic. To test this hypothesis, we conducted a semantic Bell inequality test using diverse LLM agents. Our experiments yielded average CHSH expectation values from 1.2 to 2.8, with several runs producing values (e.g., 2.3-2.4) in significant violation of the classical boundary ($|S|\\leq2$), demonstrating that linguistic interpretation under ambiguity can exhibit non-classical contextuality, consistent with results from human cognition experiments. These results inherently imply that classical frequentist-based analytical approaches for natural language are necessarily lossy. Instead, we propose that Bayesian-style repeated sampling approaches can provide more practically useful and appropriate characterizations of linguistic meaning in context.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç„¶è¯­è¨€ä¸­çš„è¯­ä¹‰é€€åŒ–(Semantic degeneracy)åŠå…¶å¼•å‘çš„ç»„åˆçˆ†ç‚¸é—®é¢˜ï¼Œè®¤ä¸ºè¿™é™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¤„ç†å¤æ‚è¯­ä¹‰çš„èƒ½åŠ›ã€‚ä½œè€…åˆ©ç”¨æŸ¯å°”è«å“¥æ´›å¤«å¤æ‚åº¦(Kolmogorov complexity)è¯æ˜ï¼Œéšç€è¡¨è¾¾å¤æ‚åº¦çš„å¢åŠ ï¼Œæ¶ˆé™¤æ­§ä¹‰æ‰€éœ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å‘ˆçˆ†ç‚¸å¼å¢é•¿ï¼Œä½¿å¾—æ¢å¤å•ä¸€ç¡®åˆ‡å«ä¹‰åœ¨è®¡ç®—ä¸Šå˜å¾—ä¸å¯è¡Œã€‚ç ”ç©¶æå‡ºï¼Œæ„ä¹‰å¹¶éè¯­è¨€å½¢å¼æœ¬èº«å›ºæœ‰ï¼Œè€Œæ˜¯é€šè¿‡ä¾èµ–è§‚å¯Ÿè€…çš„è§£é‡Šè¡Œä¸ºåŠ¨æ€å®ç°çš„ï¼Œå…¶éç¡®å®šæ€§ç‰¹å¾æ›´é€‚åˆç”¨ç±»é‡å­é€»è¾‘(Quantum-like logic)æ¥æè¿°ã€‚é€šè¿‡å¯¹å¤šç§LLMæ™ºèƒ½ä½“è¿›è¡Œè¯­ä¹‰è´å°”ä¸ç­‰å¼(Bell inequality)æµ‹è¯•ï¼Œå®éªŒå‘ç°CHSHæœŸæœ›å€¼æ˜¾è‘—è¿åäº†ç»å…¸ç•Œé™($|S|\\leq2$)ï¼Œè¯æ˜äº†è¯­è¨€è§£é‡Šåœ¨æ¨¡ç³ŠçŠ¶æ€ä¸‹å…·æœ‰éç»å…¸è¯­å¢ƒæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ä¼ ç»Ÿçš„åŸºäºé¢‘ç‡çš„ç»å…¸åˆ†ææ–¹æ³•å¿…ç„¶å­˜åœ¨ä¿¡æ¯æŸå¤±ï¼Œå¹¶å»ºè®®é‡‡ç”¨è´å¶æ–¯å¼çš„é‡å¤é‡‡æ ·æ–¹æ³•æ¥æ›´æœ‰æ•ˆåœ°åˆ»ç”»è¯­å¢ƒä¸­çš„è¯­è¨€æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10077v2",
      "published_date": "2025-06-11 18:00:30 UTC",
      "updated_date": "2025-07-15 00:08:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:06.163021+00:00"
    },
    {
      "arxiv_id": "2506.10060v1",
      "title": "Textual Bayes: Quantifying Uncertainty in LLM-Based Systems",
      "title_zh": "Textual Bayesï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Brendan Leigh Ross",
        "NoÃ«l Vouitsis",
        "Atiyeh Ashari Ghomi",
        "Rasa Hosseinzadeh",
        "Ji Xin",
        "Zhaoyan Liu",
        "Yi Sui",
        "Shiyi Hou",
        "Kin Kwan Leung",
        "Gabriel Loaiza-Ganem",
        "Jesse C. Cresswell"
      ],
      "abstract": "Although large language models (LLMs) are becoming increasingly capable of solving challenging real-world tasks, accurately quantifying their uncertainty remains a critical open problem, which limits their applicability in high-stakes domains. This challenge is further compounded by the closed-source, black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can be highly sensitive to the prompts that bind them together, which often require significant manual tuning (i.e., prompt engineering). In this work, we address these challenges by viewing LLM-based systems through a Bayesian lens. We interpret prompts as textual parameters in a statistical model, allowing us to use a small training dataset to perform Bayesian inference over these prompts. This novel perspective enables principled uncertainty quantification over both the model's textual parameters and its downstream predictions, while also incorporating prior beliefs about these parameters expressed in free-form text. To perform Bayesian inference, a difficult problem even for well-studied data modalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt optimization techniques with standard MCMC methods. MHLP is a turnkey modification to existing LLM pipelines, including those that rely exclusively on closed-source models. Empirically, we demonstrate that our method yields improvements in both predictive accuracy and uncertainty quantification (UQ) on a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a viable path for incorporating methods from the rich Bayesian literature into the era of LLMs, paving the way for more reliable and calibrated LLM-based systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Textual Bayesæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é—­æºæ¨¡å‹å’Œæç¤ºå·¥ç¨‹(prompt engineering)çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶ä»è´å¶æ–¯è§†è§’å‡ºå‘ï¼Œå°†æç¤ºè¯(prompts)è§†ä¸ºç»Ÿè®¡æ¨¡å‹ä¸­çš„æ–‡æœ¬å‚æ•°(textual parameters)ï¼Œä»è€Œå®ç°å¯¹æç¤ºè¯åŠä¸‹æ¸¸é¢„æµ‹çš„åŸåˆ™æ€§ä¸ç¡®å®šæ€§é‡åŒ–ã€‚ä¸ºäº†æ‰§è¡Œé«˜æ•ˆæ¨ç†ï¼Œç ”ç©¶è€…å¼•å…¥äº†Metropolis-Hastings through LLM Proposals (MHLP)ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†æç¤ºè¯ä¼˜åŒ–æŠ€æœ¯ä¸æ ‡å‡†é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—(MCMC)æ–¹æ³•çš„æ–°å‹é‡‡æ ·ç®—æ³•ã€‚MHLPä½œä¸ºä¸€ç§å¼€ç®±å³ç”¨çš„æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„LLMæµæ°´çº¿ä¸­ï¼Œä¸”ä¸ä¾èµ–äºæ¨¡å‹æºä»£ç çš„å¼€æ”¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–çš„è´¨é‡ã€‚è¯¥å·¥ä½œä¸ºå°†è´å¶æ–¯æ–¹æ³•å¼•å…¥LLMé¢†åŸŸå¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œä¸ºæ„å»ºæ›´åŠ å¯é ä¸”æ ¡å‡†è‰¯å¥½çš„LLMç³»ç»Ÿæä¾›äº†ç†è®ºä¸æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10060v1",
      "published_date": "2025-06-11 18:00:00 UTC",
      "updated_date": "2025-06-11 18:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:12.004689+00:00"
    },
    {
      "arxiv_id": "2506.09997v1",
      "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos",
      "title_zh": "DGS-LRMï¼šåŸºäºå•ç›®è§†é¢‘çš„å®æ—¶å¯å˜å½¢ 3D é«˜æ–¯é‡å»º",
      "authors": [
        "Chieh Hubert Lin",
        "Zhaoyang Lv",
        "Songyin Wu",
        "Zhen Xu",
        "Thu Nguyen-Phuoc",
        "Hung-Yu Tseng",
        "Julian Straub",
        "Numair Khan",
        "Lei Xiao",
        "Ming-Hsuan Yang",
        "Yuheng Ren",
        "Richard Newcombe",
        "Zhao Dong",
        "Zhengqin Li"
      ],
      "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DGS-LRMï¼ˆDeformable Gaussian Splats Large Reconstruction Modelï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­å®æ—¶é¢„æµ‹å¯å˜å½¢3Dé«˜æ–¯åˆ†å¸ƒï¼ˆDeformable 3D Gaussian splatsï¼‰çš„å‰é¦ˆé‡å»ºæ¨¡å‹ã€‚ä¸ºäº†å…‹æœåŠ¨æ€åœºæ™¯ä¸­æ•°æ®ç¨€ç¼ºå’Œè®­ç»ƒèŒƒå¼çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰ç¨ å¯†3Dåœºæ™¯æµï¼ˆ3D scene flowï¼‰ç›‘ç£çš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†é€åƒç´ çš„å¯å˜å½¢3Dé«˜æ–¯è¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨å¤§å‹Transformerç½‘ç»œï¼ŒDGS-LRMå®ç°äº†å¯¹ä»»æ„åŠ¨æ€åœºæ™¯çš„é«˜è´¨é‡è§†å›¾åˆæˆï¼ˆDynamic view synthesisï¼‰å’Œå®æ—¶é‡å»ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šå¯åª²ç¾åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæ•°æ®çš„è¡¨ç°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é¢„æµ‹å¼åŠ¨æ€é‡å»ºæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œå…¶ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§3Då½¢å˜å¯ç›´æ¥åº”ç”¨äºé•¿ç¨‹3Dè¿½è¸ªä»»åŠ¡ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¸å½“å‰æœ€å…ˆè¿›çš„å•ç›®è§†é¢‘è¿½è¸ªæ–¹æ³•ç›¸å½“ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: https://hubert0527.github.io/dgslrm/",
      "pdf_url": "https://arxiv.org/pdf/2506.09997v1",
      "published_date": "2025-06-11 17:59:58 UTC",
      "updated_date": "2025-06-11 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:16.427436+00:00"
    },
    {
      "arxiv_id": "2506.09993v2",
      "title": "Text-Aware Image Restoration with Diffusion Models",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤",
      "authors": [
        "Jaewon Min",
        "Jin Hyeon Kim",
        "Paul Hyunbin Cho",
        "Jaeeun Lee",
        "Jihye Park",
        "Minkyu Park",
        "Sangpil Kim",
        "Hyunhee Park",
        "Seungryong Kim"
      ],
      "abstract": "Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤(Image Restoration)ä¸­éš¾ä»¥å‡†ç¡®é‡å»ºæ–‡æœ¬åŒºåŸŸå¹¶äº§ç”Ÿæ–‡æœ¬å›¾åƒå¹»è§‰(text-image hallucination)çš„é—®é¢˜ï¼Œæå‡ºäº†æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤(Text-Aware Image Restoration, TAIR)è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨åŒæ—¶å®ç°è§†è§‰å†…å®¹æ¢å¤ä¸æ–‡æœ¬ä¿çœŸã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†åŒ…å«10ä¸‡å¼ é«˜è´¨é‡åœºæ™¯å›¾åƒçš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†SA-Textï¼Œä¸ºå¤„ç†å¤æ‚æ–‡æœ¬å®ä¾‹æä¾›äº†å¯†é›†æ ‡æ³¨ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åä¸ºTeReDiffçš„å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡å°†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾é›†æˆåˆ°æ–‡æœ¬æ£€æµ‹(text-spotting)æ¨¡å—å¹¶è¿›è¡Œè”åˆè®­ç»ƒï¼Œå®ç°äº†ä¸°å¯Œæ–‡æœ¬è¡¨ç¤ºçš„æœ‰æ•ˆæå–ã€‚è¿™äº›æ–‡æœ¬è¡¨ç¤ºè¢«ç”¨ä½œåç»­å»å™ªæ­¥éª¤çš„æç¤º(prompts)ï¼Œç¡®ä¿äº†ä¿®å¤è¿‡ç¨‹ä¸­çš„æ–‡æœ¬å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeReDiffåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›ä¿®å¤æ–¹æ³•ï¼Œå¹¶åœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢å–å¾—äº†å¤§å¹…æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://cvlab-kaist.github.io/TAIR/",
      "pdf_url": "https://arxiv.org/pdf/2506.09993v2",
      "published_date": "2025-06-11 17:59:46 UTC",
      "updated_date": "2025-07-03 06:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:15.931808+00:00"
    },
    {
      "arxiv_id": "2506.09994v1",
      "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures",
      "title_zh": "eFleshï¼šåŸºäºæˆªæ–­å•å…ƒå¾®ç»“æ„çš„é«˜åº¦å¯å®šåˆ¶ç£æ€§è§¦è§‰æ„ŸçŸ¥",
      "authors": [
        "Venkatesh Pattabiraman",
        "Zizhou Huang",
        "Daniele Panozzo",
        "Denis Zorin",
        "Lerrel Pinto",
        "Raunaq Bhirangi"
      ],
      "abstract": "If human experience is any guide, operating effectively in unstructured environments -- like homes and offices -- requires robots to sense the forces during physical interaction. Yet, the lack of a versatile, accessible, and easily customizable tactile sensor has led to fragmented, sensor-specific solutions in robotic manipulation -- and in many cases, to force-unaware, sensorless approaches. With eFlesh, we bridge this gap by introducing a magnetic tactile sensor that is low-cost, easy to fabricate, and highly customizable. Building an eFlesh sensor requires only four components: a hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired shape, and a magnetometer circuit board. The sensor is constructed from tiled, parameterized microstructures, which allow for tuning the sensor's geometry and its mechanical response. We provide an open-source design tool that converts convex OBJ/STL files into 3D-printable STLs for fabrication. This modular design framework enables users to create application-specific sensors, and to adjust sensitivity depending on the task. Our sensor characterization experiments demonstrate the capabilities of eFlesh: contact localization RMSE of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for shear force. We also present a learned slip detection model that generalizes to unseen objects with 95% accuracy, and visuotactile control policies that improve manipulation performance by 40% over vision-only baselines -- achieving 91% average success rate for four precise tasks that require sub-mm accuracy for successful completion. All design files, code and the CAD-to-eFlesh STL conversion tool are open-sourced and available on https://e-flesh.com.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† eFleshï¼Œä¸€ç§åŸºäº Cut-Cell Microstructures çš„é«˜åº¦å¯å®šåˆ¶ç£æ€§è§¦è§‰ä¼ æ„Ÿå™¨ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å› ç¼ºä¹é€šç”¨åŠ›ä¼ æ„Ÿå™¨è€Œå¯¼è‡´çš„æ„ŸçŸ¥å—é™é—®é¢˜ã€‚è¯¥ä¼ æ„Ÿå™¨é€šè¿‡ 3D æ‰“å°ã€ä½æˆæœ¬ç£é“å’Œ Magnetometer ç”µè·¯æ¿æ„å»ºï¼Œåˆ©ç”¨å‚æ•°åŒ–å¾®ç»“æ„å®ç°å¯¹ä¼ æ„Ÿå™¨å‡ ä½•å½¢çŠ¶ä¸æœºæ¢°å“åº”çš„çµæ´»è°ƒæ§ã€‚ç ”ç©¶å›¢é˜Ÿæä¾›äº†ä¸€å¥—å¼€æºå·¥å…·ï¼Œå¯å°† CAD æ¨¡å‹è½¬æ¢ä¸ºå¯æ‰“å°çš„ STL æ–‡ä»¶ï¼Œæ”¯æŒç”¨æˆ·æ ¹æ®ç‰¹å®šä»»åŠ¡å®šåˆ¶çµæ•åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒeFlesh çš„æ¥è§¦å®šä½ RMSE ä¸º 0.5 mmï¼Œæ³•å‘åŠ›å’Œå‰ªåˆ‡åŠ›çš„é¢„æµ‹ç²¾åº¦åˆ†åˆ«è¾¾åˆ° 0.27 N å’Œ 0.12 Nã€‚æ­¤å¤–ï¼Œè¯¥ä¼ æ„Ÿå™¨çš„æ»‘ç§»æ£€æµ‹æ¨¡å‹åœ¨æœªçŸ¥ç‰©ä½“ä¸Šçš„å‡†ç¡®ç‡é«˜è¾¾ 95%ï¼Œå…¶ visuotactile æ§åˆ¶ç­–ç•¥åœ¨ç²¾å¯†æ“çºµä»»åŠ¡ä¸­æ¯”çº¯è§†è§‰æ–¹æ¡ˆæ€§èƒ½æå‡ 40%ï¼Œå¹³å‡æˆåŠŸç‡è¾¾ 91%ã€‚è¿™é¡¹å¼€æºæˆæœä¸ºæœºå™¨äººå®ç°ä½æˆæœ¬ã€é«˜ç²¾åº¦çš„ç‰©ç†äº¤äº’æ„ŸçŸ¥æä¾›äº†å…¨æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09994v1",
      "published_date": "2025-06-11 17:59:46 UTC",
      "updated_date": "2025-06-11 17:59:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:32.926273+00:00"
    },
    {
      "arxiv_id": "2506.09988v1",
      "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits",
      "title_zh": "EditInspectorï¼šæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘è¯„ä¼°åŸºå‡†",
      "authors": [
        "Ron Yosef",
        "Moran Yanuka",
        "Yonatan Bitton",
        "Dani Lischinski"
      ],
      "abstract": "Text-guided image editing, fueled by recent advancements in generative AI, is becoming increasingly widespread. This trend highlights the need for a comprehensive framework to verify text-guided edits and assess their quality. To address this need, we introduce EditInspector, a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models in assessing edits across various dimensions, including accuracy, artifact detection, visual quality, seamless integration with the image scene, adherence to common sense, and the ability to describe edit-induced changes. Our findings indicate that current models struggle to evaluate edits comprehensively and frequently hallucinate when describing the changes. To address these challenges, we propose two novel methods that outperform SoTA models in both artifact detection and difference caption generation.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† EditInspectorï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° Text-guided image editing çš„æ–°åŸºå‡†ï¼Œå…¶å»ºç«‹åœ¨é€šè¿‡è¯¦å°½æ¨¡æ¿æ”¶é›†çš„å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®ä¹‹ä¸Šã€‚è¯¥åŸºå‡†æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥éªŒè¯æ–‡æœ¬å¼•å¯¼çš„ç¼–è¾‘å¹¶è¯„ä¼°å…¶è´¨é‡ï¼Œè€ƒå¯Ÿç»´åº¦åŒ…æ‹¬å‡†ç¡®æ€§ã€Artifact detectionã€è§†è§‰è´¨é‡ã€åœºæ™¯é›†æˆã€Common sense ä»¥åŠæè¿°ç¼–è¾‘å¼•å‘å˜åŒ–çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨ EditInspector è¯„ä¼°å½“å‰æœ€å…ˆè¿›çš„ Vision and language modelsï¼Œç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨å…¨é¢è¯„ä¼°ç¼–è¾‘ä»»åŠ¡ä¸Šå­˜åœ¨æ˜æ˜¾å›°éš¾ï¼Œä¸”åœ¨ç”Ÿæˆå·®å¼‚æè¿°æ—¶ç»å¸¸å‡ºç° Hallucination ç°è±¡ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸¤ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œåœ¨ Artifact detection å’Œ Difference caption generation ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰ SoTA æ¨¡å‹ã€‚è¯¥åŸºå‡†å’Œæ–°æ–¹æ³•çš„æå‡ºä¸ºå¢å¼ºç”Ÿæˆå¼ AI å›¾åƒç¼–è¾‘çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·ä¸æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09988v1",
      "published_date": "2025-06-11 17:58:25 UTC",
      "updated_date": "2025-06-11 17:58:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:21.427120+00:00"
    },
    {
      "arxiv_id": "2506.10054v2",
      "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs",
      "title_zh": "Omni-DPOï¼šå¤§è¯­è¨€æ¨¡å‹åŠ¨æ€åå¥½å­¦ä¹ çš„åŒè§†è§’èŒƒå¼",
      "authors": [
        "Shangpin Peng",
        "Weinong Wang",
        "Zhuotao Tian",
        "Senqiao Yang",
        "Xing Wu",
        "Haotian Xu",
        "Chengquan Zhang",
        "Takashi Isobe",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at https://github.com/pspdada/Omni-DPO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Omni-DPOï¼Œä¸€ä¸ªç”¨äºå¤§è¯­è¨€æ¨¡å‹(LLMs)åŠ¨æ€åå¥½å­¦ä¹ çš„åŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)æ–¹æ³•åœ¨å¤„ç†åå¥½å¯¹æ—¶å¿½ç•¥å…¶å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨å·®å¼‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒæ—¶è€ƒè™‘åå¥½å¯¹çš„å†…åœ¨è´¨é‡ä»¥åŠæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è¿™äº›åå¥½å¯¹çš„åŠ¨æ€æ€§èƒ½æ¼”å˜ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨ã€‚é€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œå­¦ä¹ åŠ¨æ€è‡ªé€‚åº”åœ°è°ƒæ•´æ ·æœ¬æƒé‡ï¼ŒOmni-DPO åœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šå‡å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç» Omni-DPO å¾®è°ƒçš„ Gemma-2-9b-it åœ¨ Arena-Hard åŸºå‡†æµ‹è¯•ä¸­ä»¥ 6.7 åˆ†çš„æ˜¾è‘—ä¼˜åŠ¿è¶…è¶Šäº†é¢†å…ˆçš„ Claude 3 Opusã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿä¸€è‡´ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸ºåå¥½å­¦ä¹ çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§æä¾›äº†å¼ºæœ‰åŠ›çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10054v2",
      "published_date": "2025-06-11 17:58:05 UTC",
      "updated_date": "2025-08-15 15:40:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:28.166548+00:00"
    },
    {
      "arxiv_id": "2506.09984v1",
      "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions",
      "title_zh": "InterActHumanï¼šåŸºäºå¸ƒå±€å¯¹é½éŸ³é¢‘æ¡ä»¶çš„å¤šæ¦‚å¿µäººç‰©åŠ¨ç”»",
      "authors": [
        "Zhenzhi Wang",
        "Jiaqi Yang",
        "Jianwen Jiang",
        "Chao Liang",
        "Gaojie Lin",
        "Zerong Zheng",
        "Ceyuan Yang",
        "Dahua Lin"
      ],
      "abstract": "End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InterActHumanï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒå¤šæ¦‚å¿µäººç‰©åŠ¨ç”»çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šäººç‰©äº¤äº’å’Œäººç‰©-ç‰©ä½“äº¤äº’ï¼ˆhuman-object interactionsï¼‰æ—¶ç¼ºä¹ç²¾ç¡®èº«ä»½æ§åˆ¶çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•æ‘’å¼ƒäº†ä¼ ç»Ÿçš„å•å®ä½“å‡è®¾ï¼Œé€šè¿‡å°†å¤šæ¨¡æ€æ¡ä»¶ä¸æ¯ä¸ªèº«ä»½çš„æ—¶ç©ºè¶³è¿¹ï¼ˆspatiotemporal footprintï¼‰è¿›è¡Œå¼ºåŒºåŸŸç»‘å®šï¼Œå®ç°äº†å¯¹å¤šä¸ªæ¦‚å¿µçš„ç²¾ç¡®æ§åˆ¶ã€‚InterActHumanåˆ©ç”¨æ©ç é¢„æµ‹å™¨ï¼ˆmask predictorï¼‰è‡ªåŠ¨æ¨æ–­å¸ƒå±€ä¿¡æ¯ï¼Œé€šè¿‡åŒ¹é…å»å™ªè§†é¢‘ä¸å‚è€ƒå¤–è§‚ä¹‹é—´çš„è§†è§‰çº¿ç´¢æ¥ç¡®å®šç©ºé—´å¸ƒå±€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä»¥è¿­ä»£æ–¹å¼å°†å±€éƒ¨éŸ³é¢‘æ¡ä»¶æ³¨å…¥å…¶å¯¹åº”åŒºåŸŸï¼Œç¡®ä¿éŸ³é¢‘æ¨¡æ€ä¸å¸ƒå±€å¯¹é½ã€‚å®éªŒç»“æœå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œä¸éšå¼æ§åˆ¶æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€æ¡ä»¶çš„æ˜¾å¼å¸ƒå±€æ§åˆ¶æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå®ç°äº†é«˜è´¨é‡ã€å¯æ§çš„å¤šæ¦‚å¿µäººä½“ä¸­å¿ƒè§†é¢‘ç”Ÿæˆï¼Œä¸ºå¤æ‚äº¤äº’åœºæ™¯ä¸‹çš„è§†é¢‘åˆæˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CV",
      "comment": "TL;DR: The first multi-person dialogue video generation method from pairs of reference image and audio via explicit layout-aligned condition injection. See project page https://zhenzhiwang.github.io/interacthuman/ for more details",
      "pdf_url": "https://arxiv.org/pdf/2506.09984v1",
      "published_date": "2025-06-11 17:57:09 UTC",
      "updated_date": "2025-06-11 17:57:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:45.339760+00:00"
    },
    {
      "arxiv_id": "2506.09985v1",
      "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
      "title_zh": "V-JEPA 2ï¼šèµ‹èƒ½ç†è§£ã€é¢„æµ‹ä¸è§„åˆ’çš„è‡ªç›‘ç£è§†é¢‘æ¨¡å‹",
      "authors": [
        "Mido Assran",
        "Adrien Bardes",
        "David Fan",
        "Quentin Garrido",
        "Russell Howes",
        "Mojtaba",
        "Komeili",
        "Matthew Muckley",
        "Ammar Rizvi",
        "Claire Roberts",
        "Koustuv Sinha",
        "Artem Zholus",
        "Sergio Arnaud",
        "Abha Gejji",
        "Ada Martin",
        "Francois Robert Hogan",
        "Daniel Dugas",
        "Piotr Bojanowski",
        "Vasil Khalidov",
        "Patrick Labatut",
        "Francisco Massa",
        "Marc Szafraniec",
        "Kapil Krishnakumar",
        "Yong Li",
        "Xiaodong Ma",
        "Sarath Chandar",
        "Franziska Meier",
        "Yann LeCun",
        "Michael Rabbat",
        "Nicolas Ballas"
      ],
      "abstract": "A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† V-JEPA 2ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†äº’è”ç½‘è§„æ¨¡è§†é¢‘æ•°æ®ä¸å°‘é‡æœºå™¨äººäº¤äº’è½¨è¿¹çš„è‡ªç›‘ç£å­¦ä¹ (Self-Supervised)æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºèƒ½å¤Ÿç†è§£ã€é¢„æµ‹å¹¶äºç‰©ç†ä¸–ç•Œä¸­è¿›è¡Œè§„åˆ’çš„ä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ— åŠ¨ä½œçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„(Joint-Embedding-Predictive Architecture)ï¼Œåœ¨è¶…è¿‡100ä¸‡å°æ—¶çš„è§†é¢‘æ•°æ®ä¸Šé¢„è®­ç»ƒåï¼Œåœ¨ Something-Something v2 è¿åŠ¨ç†è§£å’Œ Epic-Kitchens-100 åŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚é€šè¿‡ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ï¼ŒV-JEPA 2 åœ¨ PerceptionTest å’Œ TempCompass ç­‰è§†é¢‘é—®ç­”(Video QA)ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†åŠ¨ä½œæ¡ä»¶ä¸‹çš„éšç©ºé—´ä¸–ç•Œæ¨¡å‹ V-JEPA 2-ACï¼Œå®ç°äº†åœ¨æ— ç‰¹å®šä»»åŠ¡è®­ç»ƒæˆ–å¥–åŠ±å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œå¯¹ Franka æœºå™¨äººæ‰‹è‡‚è¿›è¡Œé›¶æ ·æœ¬(Zero-shot)éƒ¨ç½²å¹¶å®Œæˆç‰©ä½“æŠ“å–è§„åˆ’ã€‚è¯¥å·¥ä½œè¯æ˜äº†åˆ©ç”¨æµ·é‡ç½‘ç»œæ•°æ®ç»“åˆæå°‘é‡é¢†åŸŸäº¤äº’æ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆåŸ¹è‚²å‡ºå…·å¤‡ç‰©ç†å¸¸è¯†ä¸å¤æ‚ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›çš„é€šç”¨ä¸–ç•Œæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "48 pages, 19 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09985v1",
      "published_date": "2025-06-11 17:57:09 UTC",
      "updated_date": "2025-06-11 17:57:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:58:44.130304+00:00"
    },
    {
      "arxiv_id": "2506.09977v1",
      "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies",
      "title_zh": "äººç±»å¦‚ä½•ä¿®æ­£ä¸ä¸€è‡´çš„ä¿¡å¿µï¼ŸåŸºäºç”¨æˆ·ç ”ç©¶çš„äººç±»ä¿¡å¿µä¿®æ­£æœºåˆ¶è€ƒå¯Ÿ",
      "authors": [
        "Stylianos Loukas Vasileiou",
        "Antonio Rago",
        "Maria Vanina Martinez",
        "William Yeoh"
      ],
      "abstract": "Understanding how humans revise their beliefs in light of new information is crucial for developing AI systems which can effectively model, and thus align with, human reasoning. While theoretical belief revision frameworks rely on a set of principles that establish how these operations are performed, empirical evidence from cognitive psychology suggests that people may follow different patterns when presented with conflicting information. In this paper, we present three comprehensive user studies showing that people consistently prefer explanation-based revisions, i.e., those which are guided by explanations, that result in changes to their belief systems that are not necessarily captured by classical belief change theory. Our experiments systematically investigate how people revise their beliefs with explanations for inconsistencies, whether they are provided with them or left to formulate them themselves, demonstrating a robust preference for what may seem non-minimal revisions across different types of scenarios. These findings have implications for AI systems designed to model human reasoning or interact with humans, suggesting that such systems should accommodate explanation-based, potentially non-minimal belief revision operators to better align with human cognitive processes.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é€šè¿‡ä¸‰é¡¹ç»¼åˆç”¨æˆ·ç ”ç©¶(user studies)æ¢è®¨äº†äººç±»åœ¨é¢å¯¹å†²çªä¿¡æ¯æ—¶å¦‚ä½•è¿›è¡Œä¿¡å¿µä¿®æ­£(belief revision)ï¼Œæ—¨åœ¨ä¸ºå¼€å‘èƒ½ä¸äººç±»æ¨ç†å¯¹é½çš„AIç³»ç»Ÿæä¾›ä¾æ®ã€‚ç ”ç©¶å‘ç°ï¼Œäººç±»åœ¨å¤„ç†ä¸ä¸€è‡´ä¿¡æ¯æ—¶å¾€å¾€éµå¾ªä¸ç»å…¸ä¿¡å¿µå˜åŒ–ç†è®º(classical belief change theory)ä¸åŒçš„æ¨¡å¼ï¼Œè¡¨ç°å‡ºå¯¹è§£é‡Šé©±åŠ¨å‹ä¿®æ­£(explanation-based revisions)çš„å¼ºçƒˆåå¥½ã€‚å®éªŒç³»ç»Ÿåœ°è°ƒæŸ¥äº†åœ¨ä¸åŒåœºæ™¯ä¸‹ï¼Œæ— è®ºæ˜¯è¢«æä¾›è§£é‡Šè¿˜æ˜¯è‡ªè¡Œç”Ÿæˆè§£é‡Šï¼Œäººä»¬éƒ½å€¾å‘äºè¿›è¡Œçœ‹ä¼¼éæœ€å°åŒ–(non-minimal)çš„ä¿¡å¿µè°ƒæ•´ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œäººç±»çš„ä¿¡å¿µä¿®æ­£è¿‡ç¨‹å¾€å¾€æ˜¯ç”±è§£é‡Šå¼•å¯¼çš„ï¼Œè€Œéä»…ä»…è¿½æ±‚å˜åŒ–çš„æœ€å°åŒ–ã€‚è¯¥ç ”ç©¶ç»“è®ºå¼ºè°ƒï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»æ¨ç†æˆ–ä¸äººäº’åŠ¨çš„AIç³»ç»Ÿåº”å½“å®¹çº³è¿™ç§åŸºäºè§£é‡Šçš„ä¿®æ­£ç®—å­(belief revision operators)ï¼Œä»¥æ›´å¥½åœ°ä¸äººç±»è®¤çŸ¥è¿‡ç¨‹ä¿æŒå¯¹é½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09977v1",
      "published_date": "2025-06-11 17:52:33 UTC",
      "updated_date": "2025-06-11 17:52:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:11.972888+00:00"
    },
    {
      "arxiv_id": "2507.00007v1",
      "title": "Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy",
      "title_zh": "å°†é€šç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¹³å°èå…¥æ•™å­¦å®éªŒä»¥åŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´ä¸æ•°å­—ç´ å…»",
      "authors": [
        "Vasiliy Znamenskiy",
        "Rafael Niyazov",
        "Joel Hernandez"
      ],
      "abstract": "This paper presents a new educational framework for integrating generative artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini into laboratory activities aimed at developing critical thinking and digital literacy among undergraduate students. Recognizing the limitations and risks of uncritical reliance on large language models (LLMs), the proposed pedagogical model reframes GenAI as a research subject and cognitive tool. Students formulate discipline-specific prompts and evaluate GenAI-generated responses in text, image, and video modalities. A pilot implementation in a general astronomy course for non-science majors demonstrated high levels of engagement and critical reflection, with many students continuing the activity after class and presenting results at a research symposium. The results highlight the importance of structured AI interactions in education and suggest that GenAI can improve learning outcomes when combined with reflective assessment methods. The study proposes a replicable model for interdisciplinary AI-integrated lab work, adaptable to scientific disciplines. See the guide to learning activities based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•™è‚²æ¡†æ¶ï¼Œæ—¨åœ¨å°† ChatGPTã€Claude å’Œ Gemini ç­‰é€šç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¹³å°æ•´åˆåˆ°å®éªŒå®¤æ´»åŠ¨ä¸­ï¼Œä»¥åŸ¹å…»æœ¬ç§‘ç”Ÿçš„æ‰¹åˆ¤æ€§æ€ç»´ (Critical Thinking) å’Œæ•°å­—ç´ å…» (Digital Literacy)ã€‚è¯¥æ•™å­¦æ¨¡å¼å°† GenAI é‡æ–°å®šä¹‰ä¸ºç ”ç©¶å¯¹è±¡å’Œè®¤çŸ¥å·¥å…·ï¼Œé€šè¿‡è®©å­¦ç”Ÿé’ˆå¯¹ç‰¹å®šå­¦ç§‘è®¾è®¡ Prompt å¹¶è¯„ä¼°ç³»ç»Ÿç”Ÿæˆçš„æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šæ¨¡æ€å›åº”ï¼Œæœ‰æ•ˆåº”å¯¹äº†å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç›²ç›®ä¾èµ–çš„é£é™©ã€‚åœ¨é’ˆå¯¹éç§‘å­¦ä¸“ä¸šå­¦ç”Ÿçš„å¤©æ–‡å­¦è¯¾ç¨‹è¯•ç‚¹ä¸­ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†å­¦ç”Ÿçš„å‚ä¸åº¦å’Œæ‰¹åˆ¤æ€§åæ€æ°´å¹³ï¼Œä¿ƒä½¿å­¦ç”Ÿåœ¨è¯¾åä¸»åŠ¨å»¶ç»­ç ”ç©¶å¹¶åœ¨å­¦æœ¯ç ”è®¨ä¼šä¸Šå±•ç¤ºæˆæœã€‚å®éªŒç»“æœå¼ºè°ƒäº†åœ¨æ•™è‚²ä¸­è¿›è¡Œç»“æ„åŒ– AI äº’åŠ¨çš„é‡è¦æ€§ï¼Œè¯æ˜äº† GenAI ä¸åæ€æ€§è¯„ä¼°ç»“åˆèƒ½æ˜¾è‘—æ”¹å–„å­¦ä¹ æˆæœã€‚è¯¥ç ”ç©¶æœ€ç»ˆæä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„è·¨å­¦ç§‘ AI æ•´åˆæ¨¡å‹ï¼Œä¸ºå„ç±»ç§‘å­¦å­¦ç§‘çš„æ•™è‚²å®è·µæä¾›äº†å…·æœ‰å‚è€ƒä»·å€¼çš„æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "http://doi.org/10.5121/ijci.2025.140302",
      "pdf_url": "https://arxiv.org/pdf/2507.00007v1",
      "published_date": "2025-06-11 17:45:51 UTC",
      "updated_date": "2025-06-11 17:45:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:09.234825+00:00"
    },
    {
      "arxiv_id": "2506.09967v2",
      "title": "Resa: Transparent Reasoning Models via SAEs",
      "title_zh": "Resaï¼šåŸºäº SAEs çš„é€æ˜æ¨ç†æ¨¡å‹",
      "authors": [
        "Shangshang Wang",
        "Julian Asilis",
        "Ã–mer Faruk AkgÃ¼l",
        "Enes Burak Bilgin",
        "Ollie Liu",
        "Deqing Fu",
        "Willie Neiswanger"
      ],
      "abstract": "How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\$1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around \\$1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Resa ç³»åˆ— 1.5B æ¨ç†æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¾®è°ƒ (SAE-Tuning) æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„åº•å±‚è¡¨ç¤ºæ¥é«˜æ•ˆæ¿€å‘å…¶æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒ SAE ä»æºæ¨¡å‹ä¸­æ•æ‰æ¨ç†ç‰¹å¾ï¼Œéšååˆ©ç”¨ SAE æŒ‡å¯¼ç›®æ ‡æ¨¡å‹çš„æ ‡å‡†ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning) è¿‡ç¨‹ï¼Œä¸”å…¨è¿‡ç¨‹ä»…éœ€éªŒè¯è¿‡çš„é—®ç­”æ•°æ®è€Œæ— éœ€æ¨ç†è¸ªè¿¹ (reasoning traces)ã€‚åœ¨ç‰¹å®šæ¨¡å‹ä¸Šåº”ç”¨æ—¶ï¼ŒSAE-Tuning åœ¨ä¿ç•™å¼ºåŒ–å­¦ä¹  (RL) è®­ç»ƒæ¨¡å‹ 97% ä»¥ä¸Šæ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®­ç»ƒæˆæœ¬é™ä½äº† 2000 å€ä»¥ä¸Šï¼ˆä»…çº¦ 1 ç¾å…ƒï¼‰ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­äº† 450 å€ã€‚å®éªŒè¡¨æ˜è¯¥æ¨¡å‹åœ¨ AIME24 å’Œ AMC23 ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåˆ†åˆ«å–å¾—äº† 43.33% å’Œ 90% çš„ Pass@1 å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é€šè¿‡ SAE æå–çš„æ¨ç†èƒ½åŠ›å…·æœ‰é€šç”¨æ€§ (Generality) å’Œæ¨¡å—åŒ– (Modularity) ç‰¹å¾ï¼Œå¯ç›´æ¥é™„åŠ åˆ° R1-Distill ç­‰æ¨¡å‹ä¸Šè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºä½æˆæœ¬ã€å¯è§£é‡Šä¸”å¯æ‰©å±•çš„æ¨ç†æ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒï¼Œä¸”æ‰€æœ‰æˆæœå‡å·²å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09967v2",
      "published_date": "2025-06-11 17:44:01 UTC",
      "updated_date": "2025-06-13 18:01:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:11.846348+00:00"
    },
    {
      "arxiv_id": "2506.09965v2",
      "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
      "title_zh": "é€šè¿‡æ€ç»´ä¸è§†è§‰ç»˜å›¾çš„äº¤ç»‡å¼ºåŒ–è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç©ºé—´æ¨ç†",
      "authors": [
        "Junfei Wu",
        "Jian Guan",
        "Kaituo Feng",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang",
        "Wei Wu",
        "Tieniu Tan"
      ],
      "abstract": "As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ä»…ä¾èµ–æ–‡æœ¬æ¨æ–­è€Œå¯¼è‡´çš„å‡ ä½•ç†è§£ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†â€œä»¥ç”»ä¿ƒæ€â€(drawing to reason in space)çš„æ–°èŒƒå¼ã€‚é€šè¿‡èµ‹äºˆæ¨¡å‹æ ‡æ³¨è¾¹ç•Œæ¡†(bounding boxes)å’Œç»˜åˆ¶è¾…åŠ©çº¿(auxiliary lines)ç­‰åŸºç¡€ç»˜å›¾æ“ä½œï¼ŒLVLMs èƒ½å¤Ÿç›´æ¥åœ¨è§†è§‰ç©ºé—´å†…è¡¨è¾¾å¹¶åˆ†æå¤æ‚çš„ç©ºé—´å…³ç³»ï¼Œæœ‰æ•ˆçªç ´äº†ä»¥å¾€å·¥å…·é›†æˆæ¨ç†æ–¹æ³•çš„æ€§èƒ½é™åˆ¶ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŒ…å«åˆæˆæ•°æ®å†·å¯åŠ¨ã€åæ€æ€§æ‹’ç»é‡‡æ ·(reflective rejection sampling)ä»¥åŠå¼ºåŒ–å­¦ä¹ (reinforcement learning)åœ¨å†…çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ„å»ºå‡ºåä¸º VILASR çš„æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVILASR åœ¨è¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€è§†é¢‘æ¨ç†åŠå¤šè§†è§’æ¨ç†ç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ¨¡å‹å®ç°äº†å¹³å‡ 18.4% çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—å¼ºåŒ–äº†æ¨¡å‹åœ¨å¤æ‚å‡ ä½•ç¯å¢ƒä¸‹çš„è¿ç»­ç©ºé—´è¿½è¸ªä¸å¯è§†åŒ–åˆ†æèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09965v2",
      "published_date": "2025-06-11 17:41:50 UTC",
      "updated_date": "2025-06-19 03:46:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:16.923430+00:00"
    },
    {
      "arxiv_id": "2506.09956v1",
      "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge",
      "title_zh": "LLMail-Injectï¼šæºè‡ªçœŸå®è‡ªé€‚åº”æç¤ºæ³¨å…¥æŒ‘æˆ˜èµ›çš„æ•°æ®é›†",
      "authors": [
        "Sahar Abdelnabi",
        "Aideen Fay",
        "Ahmed Salem",
        "Egor Zverev",
        "Kai-Chieh Liao",
        "Chi-Huang Liu",
        "Chun-Chih Kuo",
        "Jannis Weigend",
        "Danyael Manlangit",
        "Alex Apostolov",
        "Haris Umair",
        "JoÃ£o Donato",
        "Masayuki Kawakita",
        "Athar Mahboob",
        "Tran Huu Bach",
        "Tsun-Han Chiang",
        "Myeongjin Cho",
        "Hajin Choi",
        "Byeonghyeon Kim",
        "Hyeonjin Lee",
        "Benjamin Pannell",
        "Conor McCauley",
        "Mark Russinovich",
        "Andrew Paverd",
        "Giovanni Cherubin"
      ],
      "abstract": "Indirect Prompt Injection attacks exploit the inherent limitation of Large Language Models (LLMs) to distinguish between instructions and data in their inputs. Despite numerous defense proposals, the systematic evaluation against adaptive adversaries remains limited, even when successful attacks can have wide security and privacy implications, and many real-world LLM-based applications remain vulnerable. We present the results of LLMail-Inject, a public challenge simulating a realistic scenario in which participants adaptively attempted to inject malicious instructions into emails in order to trigger unauthorized tool calls in an LLM-based email assistant. The challenge spanned multiple defense strategies, LLM architectures, and retrieval configurations, resulting in a dataset of 208,095 unique attack submissions from 839 participants. We release the challenge code, the full dataset of submissions, and our analysis demonstrating how this data can provide new insights into the instruction-data separation problem. We hope this will serve as a foundation for future research towards practical structural solutions to prompt injection.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† LLMail-Injectï¼Œè¿™æ˜¯ä¸€ä¸ªæºè‡ªçœŸå®è‡ªé€‚åº” Prompt Injection æŒ‘æˆ˜çš„å¤§å‹æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éš¾ä»¥åŒºåˆ†è¾“å…¥æŒ‡ä»¤ä¸æ•°æ®çš„æ ¸å¿ƒå®‰å…¨é—®é¢˜ã€‚è¯¥æŒ‘æˆ˜æ¨¡æ‹Ÿäº†ä¸€ä¸ªç°å®çš„ç”µå­é‚®ä»¶åŠ©æ‰‹åœºæ™¯ï¼Œå‚ä¸è€…éœ€å°è¯•é€šè¿‡åœ¨é‚®ä»¶ä¸­æ³¨å…¥æ¶æ„æŒ‡ä»¤æ¥è§¦å‘æœªç»æˆæƒçš„ Tool Callsã€‚å®éªŒè¿‡ç¨‹æ¶µç›–äº†å¤šç§é˜²å¾¡ç­–ç•¥ã€LLM æ¶æ„åŠæ£€ç´¢é…ç½®ï¼Œæœ€ç»ˆä» 839 åå‚ä¸è€…ä¸­æ”¶é›†å¹¶æ•´ç†äº† 208,095 æ¡ç‹¬ç‰¹çš„æ”»å‡»æ ·æœ¬ã€‚ä½œè€…ä¸ä»…å‘å¸ƒäº†æŒ‘æˆ˜ä»£ç å’Œå®Œæ•´æ•°æ®é›†ï¼Œè¿˜é€šè¿‡æ·±å…¥åˆ†ææ­ç¤ºäº† Instruction-Data Separation é—®é¢˜çš„å¤æ‚æ€§ã€‚è¯¥ç ”ç©¶ä¸ºç³»ç»Ÿæ€§è¯„ä¼°é’ˆå¯¹è‡ªé€‚åº”å¯¹æ‰‹çš„æ”»å‡»æä¾›äº†å®è´µèµ„æºï¼Œå¹¶ä¸ºæœªæ¥æ¢ç´¢ Prompt Injection çš„ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Dataset at: https://huggingface.co/datasets/microsoft/llmail-inject-challenge",
      "pdf_url": "https://arxiv.org/pdf/2506.09956v1",
      "published_date": "2025-06-11 17:30:07 UTC",
      "updated_date": "2025-06-11 17:30:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:09.469593+00:00"
    },
    {
      "arxiv_id": "2506.09954v1",
      "title": "Vision Generalist Model: A Survey",
      "title_zh": "è§†è§‰é€šç”¨æ¨¡å‹ç»¼è¿°",
      "authors": [
        "Ziyi Wang",
        "Yongming Rao",
        "Shuofeng Sun",
        "Xinrun Liu",
        "Yi Wei",
        "Xumin Yu",
        "Zuyan Liu",
        "Yanbo Wang",
        "Hongmin Liu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process various downstream tasks simultaneously. Encouraged by their impressive performance, an increasing number of researchers are venturing into the realm of applying these models to computer vision tasks. However, the inputs and outputs of vision tasks are more diverse, and it is difficult to summarize them as a unified representation. In this paper, we provide a comprehensive overview of the vision generalist models, delving into their characteristics and capabilities within the field. First, we review the background, including the datasets, tasks, and benchmarks. Then, we dig into the design of frameworks that have been proposed in existing research, while also introducing the techniques employed to enhance their performance. To better help the researchers comprehend the area, we take a brief excursion into related domains, shedding light on their interconnections and potential synergies. To conclude, we provide some real-world application scenarios, undertake a thorough examination of the persistent challenges, and offer insights into possible directions for future research endeavors.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡å¯¹ Vision Generalist Model è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œé‡ç‚¹æ¢è®¨äº†åœ¨ NLP é€šç”¨æ¨¡å‹å–å¾—å·¨å¤§æˆåŠŸèƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•æ„å»ºèƒ½åŒæ—¶å¤„ç†å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚æ–‡ç« åˆ†æäº†è§†è§‰ä»»åŠ¡å› è¾“å…¥è¾“å‡ºå¤šæ ·åŒ–è€Œéš¾ä»¥å®ç°ç»Ÿä¸€è¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œå¹¶ç³»ç»Ÿæ€»ç»“äº†ç›¸å…³çš„ Datasetsã€Tasks ä»¥åŠ Benchmarksã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†ç°æœ‰ Frameworks çš„è®¾è®¡æ€è·¯åŠæå‡æ€§èƒ½çš„å„é¡¹ Techniquesï¼ŒåŒæ—¶é˜è¿°äº†è§†è§‰é€šç”¨æ¨¡å‹ä¸å…¶ä»–ç›¸å…³é¢†åŸŸçš„å†…åœ¨è”ç³»ä¸ååŒæ½œåŠ›ã€‚é€šè¿‡å¯¹ Real-world Application åœºæ™¯çš„å±•ç¤ºï¼Œè®ºæ–‡è¿›ä¸€æ­¥å‰–æäº†è¯¥é¢†åŸŸå½“å‰çš„æŒä¹…æ€§æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by International Journal of Computer Vision (IJCV)",
      "pdf_url": "https://arxiv.org/pdf/2506.09954v1",
      "published_date": "2025-06-11 17:23:41 UTC",
      "updated_date": "2025-06-11 17:23:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:25.454820+00:00"
    },
    {
      "arxiv_id": "2506.09953v1",
      "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos",
      "title_zh": "Outside Knowledge Conversational Video (OKCV) æ•°æ®é›†ï¼šåŸºäºè§†é¢‘çš„å¤–éƒ¨çŸ¥è¯†å¯¹è¯",
      "authors": [
        "Benjamin Reichman",
        "Constantin Patsch",
        "Jack Truxal",
        "Atishay Jain",
        "Larry Heck"
      ],
      "abstract": "In outside knowledge visual question answering (OK-VQA), the model must identify relevant visual information within an image and incorporate external knowledge to accurately respond to a question. Extending this task to a visually grounded dialogue setting based on videos, a conversational model must both recognize pertinent visual details over time and answer questions where the required information is not necessarily present in the visual information. Moreover, the context of the overall conversation must be considered for the subsequent dialogue. To explore this task, we introduce a dataset comprised of $2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$ interleaved dialogue turns. While the dialogue context is visually grounded in specific video segments, the questions further require external knowledge that is not visually present. Thus, the model not only has to identify relevant video parts but also leverage external knowledge to converse within the dialogue. We further provide several baselines evaluated on our dataset and show future challenges associated with this task. The dataset is made publicly available here: https://github.com/c-patsch/OKCV.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Outside Knowledge Conversational Video (OKCV) æ•°æ®é›†ï¼Œæ—¨åœ¨æ¢ç´¢åœ¨è§†é¢‘è¯­å¢ƒä¸‹ç»“åˆå¤–éƒ¨çŸ¥è¯†è¿›è¡Œè§†è§‰å®šä½å¯¹è¯çš„æ–°ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„ OK-VQA ä¸åŒï¼Œè¯¥ä»»åŠ¡è¦æ±‚å¯¹è¯æ¨¡å‹ä¸ä»…è¦åœ¨æ—¶é—´ç»´åº¦ä¸Šè¯†åˆ«ç›¸å…³çš„è§†é¢‘ç»†èŠ‚ï¼Œè¿˜å¿…é¡»å¤„ç†é‚£äº›æ‰€éœ€ä¿¡æ¯æœªç›´æ¥å‡ºç°åœ¨è§†è§‰ç”»é¢ä¸­çš„é—®é¢˜ã€‚OKCV æ•°æ®é›†åŒ…å« 2,017 ä¸ªè§†é¢‘å’Œ 5,986 ç»„ç”±äººå·¥æ ‡æ³¨çš„å¯¹è¯ï¼Œæ€»è®¡æ¶µç›– 40,954 ä¸ªäº¤æ›¿è¿›è¡Œçš„å¯¹è¯è½®æ¬¡ã€‚æ¨¡å‹åœ¨æ‰§è¡Œä»»åŠ¡æ—¶éœ€å……åˆ†è€ƒè™‘æ•´ä½“å¯¹è¯çš„ Contextï¼Œåœ¨ç²¾å‡†å®šä½è§†é¢‘ç‰‡æ®µçš„åŒæ—¶æœ‰æ•ˆåœ°åˆ©ç”¨ External Knowledge è¿›è¡Œå›å¤ã€‚ç ”ç©¶äººå‘˜è¿˜æä¾›äº†åœ¨è¯¥æ•°æ®é›†ä¸Šè¯„ä¼°çš„å¤šç§ Baseline æ¨¡å‹ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œä¸ºæ¨åŠ¨å…·æœ‰çŸ¥è¯†æ·±åº¦ä¸”å¯è§£é‡Šçš„è§†é¢‘å¯¹è¯è¾…åŠ©æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09953v1",
      "published_date": "2025-06-11 17:23:35 UTC",
      "updated_date": "2025-06-11 17:23:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:31.330260+00:00"
    },
    {
      "arxiv_id": "2506.09952v1",
      "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
      "title_zh": "UniPre3Dï¼šåŸºäºè·¨æ¨¡æ€é«˜æ–¯æ³¼æº…çš„ 3D ç‚¹äº‘æ¨¡å‹ç»Ÿä¸€é¢„è®­ç»ƒ",
      "authors": [
        "Ziyi Wang",
        "Yanran Zhang",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniPre3Dï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿæ— ç¼åº”ç”¨äºä»»ä½•å°ºåº¦ point cloud åŠä»»ä½•æ¶æ„ 3D models çš„ç»Ÿä¸€é¢„è®­ç»ƒæ–¹æ³•ã€‚ä¸ºäº†è§£å†³ç‚¹äº‘æ•°æ®å°ºåº¦å¤šæ ·æ€§å¸¦æ¥çš„è¡¨å¾å­¦ä¹ æŒ‘æˆ˜ï¼ŒUniPre3D å°†é¢„æµ‹ Gaussian primitives ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨ differentiable Gaussian splatting æ¸²æŸ“å›¾åƒï¼Œä»è€Œå®ç°ç²¾ç¡®çš„ pixel-level ç›‘ç£ä¸ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥é›†æˆäº†é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„ 2D features ä»¥å¼•å…¥æˆç†Ÿçš„çº¹ç†çŸ¥è¯†ï¼Œæœ‰æ•ˆè°ƒèŠ‚äº†é¢„è®­ç»ƒä»»åŠ¡çš„å¤æ‚åº¦å¹¶å¼•å¯¼æ¨¡å‹å…³æ³¨å‡ ä½•ç»“æ„ã€‚é€šè¿‡åœ¨å¤šç§ç‰©ä½“çº§å’Œåœºæ™¯çº§ä»»åŠ¡ä¸Šä½¿ç”¨ä¸åŒ point cloud models ä½œä¸º backbone è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ 3D è§†è§‰ç»Ÿä¸€è¡¨å¾å­¦ä¹ ä¸­çš„æ™®é€‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09952v1",
      "published_date": "2025-06-11 17:23:21 UTC",
      "updated_date": "2025-06-11 17:23:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:35.926011+00:00"
    },
    {
      "arxiv_id": "2506.09943v1",
      "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models",
      "title_zh": "CausalVQAï¼šé¢å‘è§†é¢‘æ¨¡å‹çš„ç‰©ç†å› æœæ¨ç†åŸºå‡†",
      "authors": [
        "Aaron Foss",
        "Chloe Evans",
        "Sasha Mitts",
        "Koustuv Sinha",
        "Ammar Rizvi",
        "Justine T. Kao"
      ],
      "abstract": "We introduce CausalVQA, a benchmark dataset for video question answering (VQA) composed of question-answer pairs that probe models' understanding of causality in the physical world. Existing VQA benchmarks either tend to focus on surface perceptual understanding of real-world videos, or on narrow physical reasoning questions created using simulation environments. CausalVQA fills an important gap by presenting challenging questions that are grounded in real-world scenarios, while focusing on models' ability to predict the likely outcomes of different actions and events through five question types: counterfactual, hypothetical, anticipation, planning and descriptive. We designed quality control mechanisms that prevent models from exploiting trivial shortcuts, requiring models to base their answers on deep visual understanding instead of linguistic cues. We find that current frontier multimodal models fall substantially below human performance on the benchmark, especially on anticipation and hypothetical questions. This highlights a challenge for current systems to leverage spatial-temporal reasoning, understanding of physical principles, and comprehension of possible alternatives to make accurate predictions in real-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†CausalVQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†é¢‘æ¨¡å‹å¯¹ç°å®ç‰©ç†ä¸–ç•Œå› æœå…³ç³»(Causality)ç†è§£èƒ½åŠ›çš„è§†é¢‘é—®ç­”(VQA)åŸºå‡†æ•°æ®é›†ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºè¡¨é¢æ„ŸçŸ¥æˆ–ç‹­çª„çš„æ¨¡æ‹Ÿç¯å¢ƒæ¨ç†ï¼Œè€ŒCausalVQAå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä¸“æ³¨äºæ¨¡å‹é¢„æµ‹ä¸åŒè¡ŒåŠ¨å’Œäº‹ä»¶ç»“æœçš„èƒ½åŠ›ï¼Œæ¶µç›–äº†åäº‹å®æ¨ç†(Counterfactual)ã€å‡è®¾æ€§æ¨ç†(Hypothetical)ã€é¢„æµ‹(Anticipation)ã€è§„åˆ’(Planning)å’Œæè¿°æ€§(Descriptive)äº”ç§é—®é¢˜ç±»å‹ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è´¨é‡æ§åˆ¶æœºåˆ¶é˜²æ­¢æ¨¡å‹åˆ©ç”¨è¯­è¨€æ·å¾„ï¼Œè¦æ±‚å…¶å¿…é¡»åŸºäºæ·±åº¦çš„è§†è§‰ç†è§£è¿›è¡Œåˆ¤æ–­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å°–ç«¯å¤šæ¨¡æ€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¿œä½äºäººç±»æ°´å¹³ï¼Œå°¤å…¶åœ¨é¢„æµ‹å’Œå‡è®¾æ€§é—®é¢˜ä¸Šè¡¨ç°æ¬ ä½³ã€‚è¿™çªæ˜¾äº†ç°æœ‰ç³»ç»Ÿåœ¨åˆ©ç”¨æ—¶ç©ºæ¨ç†(Spatial-temporal reasoning)å’Œç‰©ç†è§„å¾‹ç†è§£æ¥åšå‡ºå‡†ç¡®é¢„æµ‹æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track",
      "pdf_url": "https://arxiv.org/pdf/2506.09943v1",
      "published_date": "2025-06-11 17:10:36 UTC",
      "updated_date": "2025-06-11 17:10:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:31.577912+00:00"
    },
    {
      "arxiv_id": "2506.09942v1",
      "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following",
      "title_zh": "VerIFï¼šé¢å‘æŒ‡ä»¤éµå¾ªå¼ºåŒ–å­¦ä¹ çš„éªŒè¯å·¥ç¨‹",
      "authors": [
        "Hao Peng",
        "Yunjia Qi",
        "Xiaozhi Wang",
        "Bin Xu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒ‡ä»¤éµå¾ª (instruction following) é¢†åŸŸä¸­çš„éªŒè¯æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº† VerIF éªŒè¯æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºè§„åˆ™çš„ä»£ç éªŒè¯ (rule-based code verification) ä»¥åŠæ¥è‡ªå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚ QwQ-32Bï¼‰çš„ LLM-based verificationã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«çº¦ 22,000 ä¸ªå®ä¾‹åŠå…¶å…³è”éªŒè¯ä¿¡å·çš„é«˜è´¨é‡æ•°æ®é›† VerInstructã€‚é€šè¿‡åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸Šåº”ç”¨åŸºäº VerIF çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œç ”ç©¶å®ç°åœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå—è®­æ¨¡å‹åœ¨åŒè§„æ¨¡æ¨¡å‹ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå¹¶å¯¹æœªè§è¿‡çš„çº¦æŸè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¢å¼ºæŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„åŒæ—¶å¹¶æœªæŸå®³æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œè¯æ˜äº† VerIF å¯ä»¥æœ‰æ•ˆåœ°é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æµç¨‹ä¸­ä»¥æå‡æ•´ä½“è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09942v1",
      "published_date": "2025-06-11 17:10:36 UTC",
      "updated_date": "2025-06-11 17:10:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:34.401855+00:00"
    },
    {
      "arxiv_id": "2506.09940v1",
      "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability",
      "title_zh": "ä¿¡æ¯ä¸å¯¹ç§°ä¸çŸ¥è¯†å¯è¿ç§»æ€§ä¸‹çš„åœ¨çº¿ç­–ç•¥å†³ç­–æ ·æœ¬å¤æ‚åº¦",
      "authors": [
        "Jiachen Hu",
        "Rui Ai",
        "Han Zhong",
        "Xiaoyu Chen",
        "Liwei Wang",
        "Zhaoran Wang",
        "Zhuoran Yang"
      ],
      "abstract": "Information asymmetry is a pervasive feature of multi-agent systems, especially evident in economics and social sciences. In these settings, agents tailor their actions based on private information to maximize their rewards. These strategic behaviors often introduce complexities due to confounding variables. Simultaneously, knowledge transportability poses another significant challenge, arising from the difficulties of conducting experiments in target environments. It requires transferring knowledge from environments where empirical data is more readily available. Against these backdrops, this paper explores a fundamental question in online learning: Can we employ non-i.i.d. actions to learn about confounders even when requiring knowledge transfer? We present a sample-efficient algorithm designed to accurately identify system dynamics under information asymmetry and to navigate the challenges of knowledge transfer effectively in reinforcement learning, framed within an online strategic interaction model. Our method provably achieves learning of an $Îµ$-optimal policy with a tight sample complexity of $O(1/Îµ^2)$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿å­¦ä¹ (Online Learning)ä¸­ç”±äºä¿¡æ¯ä¸å¯¹ç§°(Information Asymmetry)å’ŒçŸ¥è¯†å¯è¿ç§»æ€§(Knowledge Transportability)å¸¦æ¥çš„å¤æ‚å†³ç­–æŒ‘æˆ˜ã€‚åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œä»£ç†æ ¹æ®ç§æœ‰ä¿¡æ¯é‡‡å–ç­–ç•¥è¡Œä¸ºå¾€å¾€ä¼šå¼•å…¥æ··æ‚å˜é‡(Confounders)ï¼Œè€Œç›®æ ‡ç¯å¢ƒå®éªŒçš„å±€é™æ€§åˆ™è¦æ±‚å®ç°æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚è®ºæ–‡é’ˆå¯¹èƒ½å¦åˆ©ç”¨éç‹¬ç«‹åŒåˆ†å¸ƒ(non-i.i.d.)è¡ŒåŠ¨åœ¨çŸ¥è¯†è¿ç§»è¿‡ç¨‹ä¸­å­¦ä¹ æ··æ‚å˜é‡è¿™ä¸€åŸºæœ¬é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é‡‡æ ·é«˜æ•ˆ(Sample-Efficient)çš„ç®—æ³•ã€‚è¯¥ç®—æ³•è¢«è®¾è®¡ç”¨äºåœ¨çº¿ç­–ç•¥äº¤äº’æ¨¡å‹ä¸‹çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å­˜åœ¨ä¿¡æ¯ä¸å¯¹ç§°æ—¶çš„ç³»ç»ŸåŠ¨æ€å¹¶æœ‰æ•ˆå¤„ç†çŸ¥è¯†è¿ç§»ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥ $O(1/\\epsilon^2)$ çš„ç´§å‡‘æ ·æœ¬å¤æ‚åº¦(Sample Complexity)å®ç° $\\epsilon$-æœ€ä¼˜ç­–ç•¥($\\epsilon$-optimal policy)çš„å­¦ä¹ ï¼Œä¸ºè§£å†³å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„æˆ˜ç•¥å†³ç­–é—®é¢˜æä¾›äº†ç†è®ºä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09940v1",
      "published_date": "2025-06-11 17:06:57 UTC",
      "updated_date": "2025-06-11 17:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:50.788092+00:00"
    },
    {
      "arxiv_id": "2506.09937v2",
      "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
      "title_zh": "SAFEï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¤šä»»åŠ¡å¤±è´¥æ£€æµ‹",
      "authors": [
        "Qiao Gu",
        "Yuanliang Ju",
        "Shengxiang Sun",
        "Igor Gilitschenski",
        "Haruki Nishimura",
        "Masha Itkina",
        "Florian Shkurti"
      ],
      "abstract": "While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out of the box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while generalist VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $Ï€_0$, and $Ï€_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results and code can be found at the project webpage: https://vla-safe.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Vision-Language-Action Models (VLAs)åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶æˆåŠŸç‡æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†å¤šä»»åŠ¡æ•…éšœæ£€æµ‹æ¡†æ¶SAFEã€‚ä½œè€…é€šè¿‡åˆ†æVLAçš„ç‰¹å¾ç©ºé—´ï¼Œå‘ç°å…¶å†…éƒ¨åŒ…å«äº†è·¨ä»»åŠ¡é€šç”¨çš„å…³äºæˆåŠŸä¸å¤±è´¥çš„é«˜å±‚çŸ¥è¯†ã€‚åŸºäºæ­¤å‘ç°ï¼ŒSAFEåˆ©ç”¨VLAçš„å†…éƒ¨ç‰¹å¾å­¦ä¹ å¹¶é¢„æµ‹è¡¨ç¤ºä»»åŠ¡å¤±è´¥æ¦‚ç‡çš„æ ‡é‡å€¼ï¼Œä»è€Œå®ç°åœ¨æœªè§ä»»åŠ¡å’Œæ–°ç¯å¢ƒä¸­çš„æ•…éšœé¢„è­¦ã€‚SAFEå…·æœ‰è‰¯å¥½çš„å…¼å®¹æ€§ï¼Œåœ¨OpenVLAã€$\\pi_{0}$ä»¥åŠ$\\pi_{0}$-FASTç­‰å¤šç§ç­–ç•¥æ¶æ„ä¸Šå‡è¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒçš„è¯„ä¼°ä¸­ï¼ŒSAFEå‡è¾¾åˆ°äº†State-of-the-Artçš„æ•…éšœæ£€æµ‹æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥Conformal PredictionæŠ€æœ¯ï¼ŒSAFEåœ¨æ£€æµ‹å‡†ç¡®æ€§ä¸å“åº”æ—¶é—´ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ï¼Œä¸ºé€šç”¨æœºå™¨äººç­–ç•¥çš„å®‰å…¨äº¤äº’æä¾›äº†é‡è¦ä¿éšœã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "NeurIPS 2025 camera ready. Project Page: https://vla-safe.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.09937v2",
      "published_date": "2025-06-11 16:59:13 UTC",
      "updated_date": "2025-10-30 07:02:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:01.138167+00:00"
    },
    {
      "arxiv_id": "2506.09932v2",
      "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations",
      "title_zh": "HadaNormï¼šåŸºäºå‡å€¼ä¸­å¿ƒåŒ–å˜æ¢çš„æ‰©æ•£ Transformer é‡åŒ–",
      "authors": [
        "Marco Federici",
        "Riccardo Del Chiaro",
        "Boris van Breugel",
        "Paul Whatmough",
        "Markus Nagel"
      ],
      "abstract": "Diffusion models represent the cutting edge in image generation, but their high memory and computational demands hinder deployment on resource-constrained devices. Post-Training Quantization (PTQ) offers a promising solution by reducing the bitwidth of matrix operations. However, standard PTQ methods struggle with outliers, and achieving higher compression often requires transforming model weights and activations before quantization. In this work, we propose HadaNorm, a novel linear transformation that extends existing approaches by both normalizing channels activations and applying Hadamard transforms to effectively mitigate outliers and enable aggressive activation quantization. We demonstrate that HadaNorm consistently reduces quantization error across the various components of transformer blocks, outperforming state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶é¢ä¸´çš„é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º HadaNorm çš„æ–°å‹çº¿æ€§å˜æ¢æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ– Diffusion Transformer çš„è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰è¡¨ç°ã€‚HadaNorm é€šè¿‡å¯¹é€šé“æ¿€æ´»å€¼è¿›è¡Œå½’ä¸€åŒ–å¤„ç†å¹¶ç»“åˆ Hadamard å˜æ¢ï¼ˆHadamard transformsï¼‰ï¼Œæœ‰æ•ˆç¼“è§£äº†ç°æœ‰ PTQ æ–¹æ³•éš¾ä»¥å¤„ç†çš„ç¦»ç¾¤å€¼ï¼ˆoutliersï¼‰é—®é¢˜ï¼Œä»è€Œæ”¯æŒæ›´æ¿€è¿›çš„æ¿€æ´»é‡åŒ–ï¼ˆaggressive activation quantizationï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å‡å€¼ä¸­å¿ƒåŒ–å˜æ¢æ‰©å±•äº†ç°æœ‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿä¸€è‡´åœ°é™ä½ Transformer å—ä¸­å„ä¸ªç»„ä»¶çš„é‡åŒ–è¯¯å·®ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒHadaNorm åœ¨å‡å°‘é‡åŒ–è¯¯å·®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¸€è‡´ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆSOTAï¼‰ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œç¡®ä¿åœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ç»´æŒç”Ÿæˆè´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 Pages, 6 Figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09932v2",
      "published_date": "2025-06-11 16:54:34 UTC",
      "updated_date": "2025-07-10 10:03:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:59:56.764796+00:00"
    },
    {
      "arxiv_id": "2506.21558v1",
      "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents",
      "title_zh": "Bench to the Futureï¼šé¢å‘é¢„æµ‹æ™ºèƒ½ä½“çš„å›æº¯é¢„æµ‹åŸºå‡†",
      "authors": [
        "FutureSearch",
        ":",
        "Jack Wildman",
        "Nikos I. Bosse",
        "Daniel Hnyk",
        "Peter MÃ¼hlbacher",
        "Finn Hambly",
        "Jon Evans",
        "Dan Schwarz",
        "Lawrence Phillips"
      ],
      "abstract": "Forecasting is a challenging task that offers a clearly measurable way to study AI systems. Forecasting requires a large amount of research on the internet, and evaluations require time for events to happen, making the development of forecasting benchmarks challenging. To date, no forecasting benchmark provides a realistic, hermetic, and repeatable environment for LLM forecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark with hundreds of high-quality questions for which the resolution is already known. Each question is accompanied by a large offline corpus of tens of thousands of relevant web pages, enabling a way to elicit realistic \"forecasts\" on past events from LLMs. Results suggest that our pastcasting environment can produce results comparable to those based on forecasts using the internet on at-the-time unresolved questions. We show results benchmarking agent and chain-of-thought forecasting approaches using several LLMs, including the recently-released Claude 4 models, and demonstrate BTF's ability to track steady forecasting capability progress over time. We intend this to be a living benchmark, with new questions added continually to account for increasing training data cutoff dates. We invite researchers to contact us at hello@futuresearch.ai to utilize our benchmark or tooling for their own research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Bench To the Future (BTF)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºé¢„æµ‹æ™ºèƒ½ä½“ (Forecasting Agents) è®¾è®¡çš„â€œå›æº¯é¢„æµ‹â€ (Pastcasting) åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé¢„æµ‹ä»»åŠ¡ä¸­è¯„ä¼°å‘¨æœŸé•¿ä¸”ç¼ºä¹å¯é‡å¤ã€å°é—­å¼ç¯å¢ƒçš„é—®é¢˜ã€‚BTF åŒ…å«æ•°ç™¾ä¸ªå·²çŸ¥ç»“æœçš„é«˜è´¨é‡é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜é…å¤‡äº†åŒ…å«æ•°ä¸‡ä¸ªç›¸å…³ç½‘é¡µçš„å¤§å‹ç¦»çº¿è¯­æ–™åº“ï¼Œä»è€Œä½¿å¤§è¯­è¨€æ¨¡å‹ (LLMs) èƒ½å¤Ÿåœ¨ç¦»çº¿çŠ¶æ€ä¸‹å¯¹è¿‡å»å‘ç”Ÿçš„äº‹ä»¶è¿›è¡Œé€¼çœŸçš„é¢„æµ‹æ¨¡æ‹Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å›æº¯é¢„æµ‹ç¯å¢ƒäº§ç”Ÿçš„è¯„ä¼°ç»“æœä¸åŸºäºäº’è”ç½‘å®æ—¶é¢„æµ‹æœªå‘ç”Ÿäº‹ä»¶çš„ç»“æœå…·æœ‰é«˜åº¦å¯æ¯”æ€§ã€‚ç ”ç©¶é€šè¿‡ Chain-of-Thought (CoT) å’Œæ™ºèƒ½ä½“æ–¹æ³•å¯¹åŒ…æ‹¬ Claude 4 åœ¨å†…çš„å¤šç§æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº† BTF èƒ½å¤Ÿæœ‰æ•ˆè¿½è¸ª AI é¢„æµ‹èƒ½åŠ›çš„æŒç»­è¿›æ­¥ã€‚ä½œä¸ºä¸€ä¸ªåŠ¨æ€æ›´æ–°çš„åŸºå‡†ï¼ŒBTF å°†ä¸æ–­å¢åŠ æ–°é—®é¢˜ä»¥é€‚åº”æ¨¡å‹è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸçš„å˜åŒ–ï¼Œä¸ºé¢„æµ‹ç³»ç»Ÿçš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªé«˜åº¦å¯æ§ä¸”å¯é‡å¤çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21558v1",
      "published_date": "2025-06-11 16:18:40 UTC",
      "updated_date": "2025-06-11 16:18:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:03.682939+00:00"
    },
    {
      "arxiv_id": "2506.09902v1",
      "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants",
      "title_zh": "PersonaLensï¼šå¯¹è¯å¼ AI åŠ©æ‰‹ä¸ªæ€§åŒ–èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Zheng Zhao",
        "Clara Vania",
        "Subhradeep Kayal",
        "Naila Khan",
        "Shay B. Cohen",
        "Emine Yilmaz"
      ],
      "abstract": "Large language models (LLMs) have advanced conversational AI assistants. However, systematically evaluating how well these assistants apply personalization--adapting to individual user preferences while completing tasks--remains challenging. Existing personalization benchmarks focus on chit-chat, non-conversational tasks, or narrow domains, failing to capture the complexities of personalized task-oriented assistance. To address this, we introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants. Our benchmark features diverse user profiles equipped with rich preferences and interaction histories, along with two specialized LLM-based agents: a user agent that engages in realistic task-oriented dialogues with AI assistants, and a judge agent that employs the LLM-as-a-Judge paradigm to assess personalization, response quality, and task success. Through extensive experiments with current LLM assistants across diverse tasks, we reveal significant variability in their personalization capabilities, providing crucial insights for advancing conversational AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PersonaLensï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä»»åŠ¡å¯¼å‘å‹ AI åŠ©æ‰‹ä¸ªæ€§åŒ–ï¼ˆpersonalizationï¼‰èƒ½åŠ›çš„ç»¼åˆåŸºå‡†ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†å¤šé›†ä¸­äºé—²èŠæˆ–ç‰¹å®šç‹­çª„é¢†åŸŸï¼Œéš¾ä»¥æ•æ‰å¤æ‚ä»»åŠ¡ä¸­ä¸ªæ€§åŒ–é€‚é…éœ€æ±‚çš„å±€é™æ€§ï¼ŒPersonaLens æä¾›äº†åŒ…å«ä¸°å¯Œåå¥½å’Œäº¤äº’å†å²çš„å¤šæ ·åŒ–ç”¨æˆ·ç”»åƒã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªä¸“é—¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“ï¼šä¸€ä¸ªæ˜¯è´Ÿè´£æ¨¡æ‹ŸçœŸå®ä»»åŠ¡å¯¹è¯çš„ user agentï¼Œå¦ä¸€ä¸ªæ˜¯é‡‡ç”¨ LLM-as-a-Judge èŒƒå¼å¯¹å“åº”è´¨é‡ã€ä»»åŠ¡æˆåŠŸç‡åŠä¸ªæ€§åŒ–ç¨‹åº¦è¿›è¡Œå¤šç»´åº¦è¯„ä¼°çš„ judge agentã€‚é€šè¿‡å¯¹å¤šç§ä¸»æµ LLM åŠ©æ‰‹çš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ä¸ªæ€§åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å‘ç°ä¸ºè¿›ä¸€æ­¥æ¨åŠ¨å¯¹è¯å¼ AI ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¸ªæ€§åŒ–å‘å±•æä¾›äº†å…³é”®çš„æ•°æ®æ”¯æŒå’Œè¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.09902v1",
      "published_date": "2025-06-11 16:16:07 UTC",
      "updated_date": "2025-06-11 16:16:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:07.932852+00:00"
    },
    {
      "arxiv_id": "2506.09891v2",
      "title": "Causal Climate Emulation with Bayesian Filtering",
      "title_zh": "åŸºäºè´å¶æ–¯æ»¤æ³¢çš„å› æœæ°”å€™ä»¿çœŸ",
      "authors": [
        "Sebastian Hickman",
        "Ilija Trajkovic",
        "Julia Kaltenborn",
        "Francis Pelletier",
        "Alex Archibald",
        "Yaniv Gurwicz",
        "Peer Nowack",
        "David Rolnick",
        "Julien Boussard"
      ],
      "abstract": "Traditional models of climate change use complex systems of coupled equations to simulate physical processes across the Earth system. These simulations are highly computationally expensive, limiting our predictions of climate change and analyses of its causes and effects. Machine learning has the potential to quickly emulate data from climate models, but current approaches are not able to incorporate physically-based causal relationships. Here, we develop an interpretable climate model emulator based on causal representation learning. We derive a novel approach including a Bayesian filter for stable long-term autoregressive emulation. We demonstrate that our emulator learns accurate climate dynamics, and we show the importance of each one of its components on a realistic synthetic dataset and data from two widely deployed climate models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ°”å€™æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ä¸”ç°æœ‰æœºå™¨å­¦ä¹ æ¨¡æ‹Ÿå™¨ç¼ºä¹ç‰©ç†å› æœå…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå› æœè¡¨ç¤ºå­¦ä¹ (causal representation learning)çš„å¯è§£é‡Šæ°”å€™æ¨¡å‹æ¨¡æ‹Ÿå™¨ã€‚ç ”ç©¶äººå‘˜æ¨å¯¼å¹¶å¼€å‘äº†ä¸€ç§åŒ…å«è´å¶æ–¯æ»¤æ³¢(Bayesian filter)çš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç¨³å®šçš„é•¿æœŸè‡ªå›å½’æ¨¡æ‹Ÿ(autoregressive emulation)ã€‚é€šè¿‡åœ¨ç°å®åˆæˆæ•°æ®é›†ä»¥åŠä¸¤ç§å¹¿æ³›åº”ç”¨çš„æ°”å€™æ¨¡å‹æ•°æ®ä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ¨¡æ‹Ÿå™¨è¢«è¯æ˜èƒ½å¤Ÿå‡†ç¡®å­¦ä¹ å¤æ‚çš„æ°”å€™åŠ¨åŠ›å­¦ç‰¹å¾ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹å„æ ¸å¿ƒç»„ä»¶å¯¹æ¨¡æ‹Ÿæ•ˆæœçš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”å…·å¤‡ç‰©ç†ä¸€è‡´æ€§çš„æ°”å€™æ¨¡æ‹Ÿå·¥å…·æä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 26 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09891v2",
      "published_date": "2025-06-11 16:00:55 UTC",
      "updated_date": "2025-10-24 17:57:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:04.614565+00:00"
    },
    {
      "arxiv_id": "2506.09890v1",
      "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­è¶…è¶Šç‰¹å®šè¯­è¨€çš„æŠ½è±¡æ€ç»´æ¶Œç°",
      "authors": [
        "Yuxin Chen",
        "Yiran Zhao",
        "Yang Zhang",
        "An Zhang",
        "Kenji Kawaguchi",
        "Shafiq Joty",
        "Junnan Li",
        "Tat-Seng Chua",
        "Michael Qizhe Shieh",
        "Wenxuan Zhang"
      ],
      "abstract": "As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may \"think\" in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space-a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model's ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons-those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦ä¸»è¦ä»¥è‹±è¯­è¿›è¡Œæ€ç»´ï¼Œè¿˜æ˜¯å…·å¤‡è¶…è¶Šç‰¹å®šè¯­è¨€çš„æŠ½è±¡æ€ç»´èƒ½åŠ›ã€‚ä½œè€…å‘ç°LLMsåœ¨å‘å±•è¿‡ç¨‹ä¸­ä¼šé€æ¸å½¢æˆä¸€ä¸ªæ ¸å¿ƒçš„language-agnosticå‚æ•°ç©ºé—´ï¼Œè™½ç„¶è¿™åªæ˜¯å‚æ•°ä¸­çš„æå°ä¸€éƒ¨åˆ†ï¼Œä½†å…¶åŠŸèƒ½å¯¹æ‰€æœ‰è¯­è¨€çš„æ€§èƒ½éƒ½è‡³å…³é‡è¦ã€‚ç ”ç©¶è¯†åˆ«å¹¶åˆ†ç±»äº†language-relatedç¥ç»å…ƒï¼Œå°†å…¶åˆ†ä¸ºåœ¨å¤šç§è¯­è¨€ä¸­æ¿€æ´»çš„sharedç¥ç»å…ƒå’Œä»…åœ¨ç‰¹å®šè¯­è¨€ä¸­æ¿€æ´»çš„exclusiveç¥ç»å…ƒã€‚å®éªŒè§‚å¯Ÿåˆ°ï¼Œéšç€æ¨¡å‹èƒ½åŠ›çš„æå‡ï¼Œsharedç¥ç»å…ƒçš„å æ¯”å’Œé‡è¦æ€§æ˜¾è‘—å¢åŠ ï¼Œè€Œexclusiveç¥ç»å…ƒçš„å½±å“åŠ›åˆ™é€æ¸å‡å¼±ã€‚è¿™äº›sharedç¥ç»å…ƒæ„æˆäº†æ ¸å¿ƒlanguage-agnosticå‚æ•°ç©ºé—´çš„éª¨å¹²ï¼Œæ”¯æŒäº†ä¸ä¾èµ–äºç‰¹å®šè¯­è¨€ç³»ç»Ÿçš„æŠ½è±¡æ€ç»´çš„æ¶Œç°ã€‚åŸºäºæ­¤è§è§£ï¼Œç ”ç©¶æå‡ºäº†é’ˆå¯¹æ¨¡å‹ä¸åŒé˜¶æ®µlanguage-agnosticæ°´å¹³çš„neuron-specificè®­ç»ƒç­–ç•¥ï¼Œå¹¶åœ¨å¤šä¸ªLLMå®¶æ—ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09890v1",
      "published_date": "2025-06-11 16:00:54 UTC",
      "updated_date": "2025-06-11 16:00:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:23.426872+00:00"
    },
    {
      "arxiv_id": "2506.09886v1",
      "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs",
      "title_zh": "åŸºäºæ³¨æ„åŠ›å¤´åµŒå…¥ä¸å¯è®­ç»ƒæ·±åº¦æ ¸çš„å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹",
      "authors": [
        "Rodion Oblovatny",
        "Alexandra Bazarova",
        "Alexey Zaytsev"
      ],
      "abstract": "We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hidden-state distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ£€æµ‹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¹»è§‰(Hallucination Detection)çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ†æPromptä¸Responseçš„éšè—çŠ¶æ€åˆ†å¸ƒ(Hidden-State Distributions)ä¹‹é—´çš„æ¦‚ç‡æ•£åº¦(Probabilistic Divergence)æ¥å®ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå¹»è§‰å“åº”ä¸æç¤ºè¯ä¹‹é—´çš„åå·®æ¯”çœŸå®å“åº”æ›´å°ï¼Œè¿™è¡¨æ˜å¹»è§‰å¾€å¾€æºäºè¡¨å±‚æ”¹å†™è€Œéå®è´¨æ€§æ¨ç†ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ¨¡å‹å†…åœ¨çš„æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å¸ƒè·ç¦»(Distributional Distances)ä½œä¸ºå¹»è§‰è¯„åˆ†æŒ‡æ ‡ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–è¾…åŠ©æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ•æ„Ÿåº¦ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†å¯è®­ç»ƒçš„æ·±åº¦æ ¸å‡½æ•°(Deep Learnable Kernels)æ¥è‡ªåŠ¨æ•æ‰åˆ†å¸ƒé—´çš„ç»†å¾®å‡ ä½•å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œä¸”åœ¨ä¸è¿›è¡Œæ ¸å‡½æ•°è®­ç»ƒçš„æƒ…å†µä¸‹ä¾ç„¶ä¿æŒç«äº‰åŠ›ï¼Œä¸ºå¹»è§‰æ£€æµ‹æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09886v1",
      "published_date": "2025-06-11 15:59:15 UTC",
      "updated_date": "2025-06-11 15:59:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:18.000493+00:00"
    },
    {
      "arxiv_id": "2506.09883v2",
      "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation",
      "title_zh": "åŸºäºå‡ ä½•è’¸é¦çš„3Dæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Seonho Lee",
        "Jiho Choi",
        "Inha Kang",
        "Jiwook Kim",
        "Junsung Park",
        "Hyunjung Shim"
      ],
      "abstract": "Vision-Language Models (VLMs) have shown remarkable performance on diverse visual and linguistic tasks, yet they remain fundamentally limited in their understanding of 3D spatial structures. We propose Geometric Distillation, a lightweight, annotation-free fine-tuning framework that injects human-inspired geometric cues into pretrained VLMs without modifying their architecture. By distilling (1) sparse correspondences, (2) relative depth relations, and (3) dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R, VGGT), our method shapes representations to be geometry-aware while remaining compatible with natural image-text inputs. Through extensive evaluations on 3D vision-language reasoning and 3D perception benchmarks, our method consistently outperforms prior approaches, achieving improved 3D spatial reasoning with significantly lower computational cost. Our work demonstrates a scalable and efficient path to bridge 2D-trained VLMs with 3D understanding, opening up wider use in spatially grounded multimodal tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨3Dç©ºé—´ç»“æ„ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGeometric Distillationçš„è½»é‡çº§ã€æ— éœ€æ ‡æ³¨çš„å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨ä¸ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹æ¶æ„çš„å‰æä¸‹ï¼Œé€šè¿‡ä»ç°æˆçš„3DåŸºç¡€æ¨¡å‹ï¼ˆå¦‚MASt3Rã€VGGTï¼‰ä¸­è’¸é¦å‡ºç¨€ç–å¯¹åº”å…³ç³»(sparse correspondences)ã€ç›¸å¯¹æ·±åº¦å…³ç³»(relative depth relations)ä»¥åŠå¯†é›†ä»£ä»·ä½“ç§¯(dense cost volumes)ï¼Œå°†å‡ ä½•çº¿ç´¢æ³¨å…¥æ¨¡å‹ä¸­ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹è¡¨å¾å…·å¤‡å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†ä¸è‡ªç„¶å›¾åƒ-æ–‡æœ¬è¾“å…¥çš„å…¼å®¹æ€§ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨3Dè§†è§‰è¯­è¨€æ¨ç†å’Œ3Dæ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†æ›´å¼ºçš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºè¿æ¥2Dè®­ç»ƒçš„VLMsä¸3Dç†è§£æä¾›äº†ä¸€æ¡å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è·¯å¾„ï¼Œä¸ºç©ºé—´å®šä½çš„å¤šæ¨¡æ€ä»»åŠ¡åº”ç”¨å¼€è¾Ÿäº†æ›´å¹¿é˜”çš„ç©ºé—´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09883v2",
      "published_date": "2025-06-11 15:56:59 UTC",
      "updated_date": "2025-11-17 10:19:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:22.922489+00:00"
    },
    {
      "arxiv_id": "2506.09873v1",
      "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice",
      "title_zh": "è´Ÿè´£ä»»äººå·¥æ™ºèƒ½å¼€å‘çš„åˆ©ç›Šç›¸å…³è€…å‚ä¸ï¼šæŒ‡å¯¼å‡†åˆ™ä¸å½“å‰å®è·µçš„è„±èŠ‚",
      "authors": [
        "Emma Kallina",
        "Thomas BohnÃ©",
        "Jat Singh"
      ],
      "abstract": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement (SHI) during AI development. At the same time, SHI is already common in commercial software development, but with potentially different foci. This study clarifies the extent to which established SHI practices are able to contribute to rAI efforts as well as potential disconnects -- essential insights to inform and tailor future interventions that further shift industry practice towards rAI efforts. First, we analysed 56 rAI guidance documents to identify why SHI is recommended (i.e. its expected benefits for rAI) and uncovered goals such as redistributing power, improving socio-technical understandings, anticipating risks, and enhancing public oversight. To understand why and how SHI is currently practised in commercial settings, we then conducted an online survey (n=130) and semi-structured interviews (n=10) with AI practitioners. Our findings reveal that SHI in practice is primarily driven by commercial priorities (e.g. customer value, compliance) and several factors currently discourage more rAI-aligned SHI practices. This suggests that established SHI practices are largely not contributing to rAI efforts. To address this disconnect, we propose interventions and research opportunities to advance rAI development in practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è´Ÿè´£ä»»äººå·¥æ™ºèƒ½(Responsible AI, rAI)å¼€å‘è¿‡ç¨‹ä¸­åˆ©å®³å…³ç³»äººå‚ä¸(Stakeholder Involvement, SHI)çš„ç°çŠ¶ï¼Œåˆ†æäº†ç°æœ‰æŒ‡å¯¼æ–¹é’ˆä¸è¡Œä¸šå®è·µä¹‹é—´çš„è„±èŠ‚ã€‚é€šè¿‡å¯¹56ä»½rAIæŒ‡å¯¼æ–‡ä»¶çš„åˆ†æï¼Œç ”ç©¶æ˜ç¡®äº†SHIåœ¨æƒåŠ›å†åˆ†é…ã€ç¤¾ä¼šæŠ€æœ¯ç†è§£ã€é£é™©é¢„æµ‹å’Œå…¬å…±ç›‘ç£ç­‰æ–¹é¢çš„é¢„æœŸæ•ˆç›Šã€‚éšåï¼Œç ”ç©¶äººå‘˜é’ˆå¯¹130åAIä»ä¸šè€…è¿›è¡Œäº†é—®å·è°ƒæŸ¥å¹¶å¼€å±•äº†10åœºåŠç»“æ„åŒ–è®¿è°ˆï¼Œå‘ç°å®é™…å·¥ä½œä¸­çš„SHIä¸»è¦å—å®¢æˆ·ä»·å€¼å’Œåˆè§„æ€§ç­‰å•†ä¸šä¼˜å…ˆçº§é©±åŠ¨ã€‚ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„SHIå®è·µåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªèƒ½æœ‰æ•ˆè´¡çŒ®äºrAIç›®æ ‡ï¼Œä¸”å­˜åœ¨å¤šç§é˜»ç¢å‘rAIå¯¹é½çš„ç°å®å› ç´ ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½å’Œç ”ç©¶æœºé‡ï¼Œæ—¨åœ¨å¼¥åˆç†è®ºä¸å®è·µçš„é¸¿æ²Ÿï¼Œæ¨åŠ¨å·¥ä¸šç•ŒçœŸæ­£å®ç°è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å¼€å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and Transparency FAccT'25",
      "pdf_url": "https://arxiv.org/pdf/2506.09873v1",
      "published_date": "2025-06-11 15:43:07 UTC",
      "updated_date": "2025-06-11 15:43:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:22.126088+00:00"
    },
    {
      "arxiv_id": "2506.09862v1",
      "title": "Guided Graph Compression for Quantum Graph Neural Networks",
      "title_zh": "é¢å‘é‡å­å›¾ç¥ç»ç½‘ç»œçš„å¼•å¯¼å¼å›¾å‹ç¼©",
      "authors": [
        "Mikel Casals",
        "Vasilis Belis",
        "Elias F. Combarro",
        "Eduard AlarcÃ³n",
        "Sofia Vallecorsa",
        "Michele Grossi"
      ],
      "abstract": "Graph Neural Networks (GNNs) are effective for processing graph-structured data but face challenges with large graphs due to high memory requirements and inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a promising avenue to address these issues and inspires new algorithmic approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been explored in recent literature. However, current quantum hardware limits the dimension of the data that can be effectively encoded. Existing approaches either simplify datasets manually or use artificial graph datasets. This work introduces the Guided Graph Compression (GGC) framework, which uses a graph autoencoder to reduce both the number of nodes and the dimensionality of node features. The compression is guided to enhance the performance of a downstream classification task, which can be applied either with a quantum or a classical classifier. The framework is evaluated on the Jet Tagging task, a classification problem of fundamental importance in high energy physics that involves distinguishing particle jets initiated by quarks from those by gluons. The GGC is compared against using the autoencoder as a standalone preprocessing step and against a baseline classical GNN classifier. Our numerical results demonstrate that GGC outperforms both alternatives, while also facilitating the testing of novel QGNN ansatzes on realistic datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Guided Graph Compression (GGC) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Graph Neural Networks (GNNs) åœ¨å¤„ç†å¤§è§„æ¨¡å›¾æ•°æ®æ—¶çš„è®¡ç®—æ•ˆç‡ç“¶é¢ˆï¼Œä»¥åŠ Quantum Graph Neural Networks (QGNNs) å—é™äºå½“å‰é‡å­ç¡¬ä»¶ç¼–ç ç»´åº¦çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å›¾è‡ªåŠ¨ç¼–ç å™¨ (graph autoencoder) åŒæ—¶å‡å°‘èŠ‚ç‚¹æ•°é‡å’ŒèŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼Œå¹¶é€šè¿‡ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡çš„å¼•å¯¼æ¥ä¼˜åŒ–å‹ç¼©è´¨é‡ã€‚GGC èƒ½å¤Ÿçµæ´»åº”ç”¨äºé‡å­æˆ–ç»å…¸åˆ†ç±»å™¨ï¼Œå¹¶åœ¨é«˜èƒ½ç‰©ç†é¢†åŸŸçš„å–·æ³¨æ ‡è®° (Jet Tagging) è¿™ä¸€å…³é”®åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æ•°å€¼è®¡ç®—ç»“æœè¡¨æ˜ï¼ŒGGC çš„æ€§èƒ½ä¼˜äºå•çº¯çš„è‡ªåŠ¨ç¼–ç å™¨é¢„å¤„ç†æ­¥å¥åŠåŸºå‡†ç»å…¸ GNN åˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†å¼•å¯¼å¼å‹ç¼©çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºåœ¨ç°å®æ•°æ®é›†ä¸Šæµ‹è¯•æ–°å‹ QGNN æ¶æ„æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-ex",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09862v1",
      "published_date": "2025-06-11 15:36:29 UTC",
      "updated_date": "2025-06-11 15:36:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:36.327075+00:00"
    },
    {
      "arxiv_id": "2506.09853v3",
      "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
      "title_zh": "å› æœå……åˆ†æ€§ä¸å¿…è¦æ€§æå‡æ€ç»´é“¾æ¨ç†",
      "authors": [
        "Xiangning Yu",
        "Zhuohan Wang",
        "Linyi Yang",
        "Haoxuan Li",
        "Anjie Liu",
        "Xiao Xue",
        "Jun Wang",
        "Mengyue Yang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ Chain-of-Thought (CoT) æ¨ç†ä¸­é¢ä¸´çš„ Sufficiencyï¼ˆå……åˆ†æ€§ï¼‰ä¸ Necessityï¼ˆå¿…è¦æ€§ï¼‰ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„ causal frameworkã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å› æœå……åˆ†æ€§ä¸å¿…è¦æ€§æ¦‚ç‡ï¼ˆProbability of Sufficiency and Necessityï¼‰ï¼Œä¸ä»…èƒ½è¯†åˆ«é€»è¾‘ä¸Šæ”¯æ’‘ç»“è®ºçš„å…³é”®æ­¥éª¤ï¼Œè¿˜èƒ½é‡åŒ–ä¸åŒå¹²é¢„åœºæ™¯ä¸‹å„æ¨ç†æ­¥éª¤å¯¹æœ€ç»ˆç»“æœçš„å®é™…å½±å“ã€‚åŸºäºè¿™ä¸€ç†è®ºï¼Œç ”ç©¶å®ç°äº†å¯¹æ¨ç†è¿‡ç¨‹ä¸­ç¼ºå¤±æ­¥éª¤çš„è‡ªåŠ¨è¡¥å……ä»¥åŠå†—ä½™æ­¥éª¤çš„ç²¾å‡†å‰ªæã€‚åœ¨å¤šä¸ªæ•°å­¦å’Œå¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé¢„æµ‹å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡å¹¶é™ä½äº† Token æ¶ˆè€—ã€‚è¯¥å·¥ä½œä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ä¸æˆæœ¬æ•ˆç›Šæä¾›äº†ä¸€ç§æå…·å‰æ™¯çš„å› æœæ¨æ–­è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "math.ST",
        "stat.ME"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09853v3",
      "published_date": "2025-06-11 15:22:09 UTC",
      "updated_date": "2025-10-25 10:01:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:39.726889+00:00"
    },
    {
      "arxiv_id": "2506.09847v1",
      "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment",
      "title_zh": "ç”¨äºåª’ä½“å…³è”æ€§è¯„ä¼°çš„å¸¦æº¯æºå…ƒæ•°æ®æ–°é—»æ–‡ç« æ•°æ®é›†",
      "authors": [
        "Tomas Peterka",
        "Matyas Bohacek"
      ],
      "abstract": "Out-of-context and misattributed imagery is the leading form of media manipulation in today's misinformation and disinformation landscape. The existing methods attempting to detect this practice often only consider whether the semantics of the imagery corresponds to the text narrative, missing manipulation so long as the depicted objects or scenes somewhat correspond to the narrative at hand. To tackle this, we introduce News Media Provenance Dataset, a dataset of news articles with provenance-tagged images. We formulate two tasks on this dataset, location of origin relevance (LOR) and date and time of origin relevance (DTOR), and present baseline results on six large language models (LLMs). We identify that, while the zero-shot performance on LOR is promising, the performance on DTOR hinders, leaving room for specialized architectures and future work.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“ä»Šè™šå‡ä¿¡æ¯é¢†åŸŸä¸­æ™®éå­˜åœ¨çš„è„±ç¦»è¯­å¢ƒå’Œè¯¯å½’å±å›¾åƒæ“çºµé—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ£€æµ‹æ–¹æ³•å› è¿‡åº¦ä¾èµ–å›¾åƒä¸æ–‡æœ¬çš„è¯­ä¹‰ä¸€è‡´æ€§è€Œéš¾ä»¥è¯†åˆ«æ­¤ç±»æ‰‹æ®µã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº† News Media Provenance Datasetï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å‡ºå¤„æ ‡è®°å›¾åƒçš„æ–°é—»æ–‡ç« æ•°æ®é›†ã€‚ç ”ç©¶è€…åŸºäºè¯¥æ•°æ®é›†å®šä¹‰äº†ä¸¤ä¸ªæ ¸å¿ƒè¯„ä¼°ä»»åŠ¡ï¼šLocation of Origin Relevance (LOR) å’Œ Date and Time of Origin Relevance (DTOR)ï¼Œå¹¶ä½¿ç”¨å…­ç§ Large Language Models (LLMs) è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ LLMs åœ¨ LOR ä»»åŠ¡ä¸Šçš„ Zero-shot æ€§èƒ½æå…·æ½œåŠ›ï¼Œä½†åœ¨ DTOR ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ä¸ç†æƒ³ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†å½“å‰é€šç”¨æ¨¡å‹åœ¨å¤„ç†ç²¾ç¡®æ—¶é—´ä¸ç©ºé—´ç›¸å…³æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸ºæœªæ¥å¼€å‘ä¸“é—¨çš„æ£€æµ‹æ¶æ„å’Œå­¦æœ¯ç ”ç©¶æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09847v1",
      "published_date": "2025-06-11 15:21:05 UTC",
      "updated_date": "2025-06-11 15:21:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:44.106834+00:00"
    },
    {
      "arxiv_id": "2506.09846v2",
      "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition",
      "title_zh": "å­¦ä¹ å¯¹é½ï¼šåº”å¯¹æ‰‹å†™æ–‡æœ¬è¯†åˆ«ä¸­çš„å­—ç¬¦é¢‘ç‡åˆ†å¸ƒåç§»",
      "authors": [
        "Panagiotis Kaliosis",
        "John Pavlopoulos"
      ],
      "abstract": "Handwritten text recognition aims to convert visual input into machine-readable text, and it remains challenging due to the evolving and context-dependent nature of handwriting. Character sets change over time, and character frequency distributions shift across historical periods or regions, often causing models trained on broad, heterogeneous corpora to underperform on specific subsets. To tackle this, we propose a novel loss function that incorporates the Wasserstein distance between the character frequency distribution of the predicted text and a target distribution empirically derived from training data. By penalizing divergence from expected distributions, our approach enhances both accuracy and robustness under temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that character distribution alignment can also improve existing models at inference time without requiring retraining by integrating it as a scoring function in a guided decoding scheme. Experimental results across multiple datasets and architectures confirm the effectiveness of our method in boosting generalization and performance. We open source our code at https://github.com/pkaliosis/fada.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹å†™æ–‡æœ¬è¯†åˆ« (Handwritten Text Recognition) ä¸­ç”±äºä¸åŒå†å²æ—¶æœŸæˆ–åœ°åŸŸå¯¼è‡´çš„å­—ç¬¦é¢‘ç‡åˆ†å¸ƒåç§» (Character Frequency Distribution Shifts) éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹é½èƒ½åŠ›çš„ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åˆ›æ–°çš„æŸå¤±å‡½æ•°ï¼Œé€šè¿‡è®¡ç®—é¢„æµ‹æ–‡æœ¬ä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„ Wasserstein distance æ¥æƒ©ç½šåç¦»é¢„æœŸåˆ†å¸ƒçš„è¡Œä¸ºï¼Œä»è€Œæœ‰æ•ˆæå‡æ¨¡å‹åœ¨é¢å¯¹æ•°æ®é›†å†…éƒ¨æ—¶ç©ºå’Œä¸Šä¸‹æ–‡åç§»æ—¶çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å­—ç¬¦åˆ†å¸ƒå¯¹é½æŠ€æœ¯è¿˜å¯ä»¥ä½œä¸ºè¯„åˆ†å‡½æ•°é›†æˆåˆ°å¼•å¯¼è§£ç  (Guided Decoding) æ–¹æ¡ˆä¸­ï¼Œå®ç°åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„å‰æä¸‹ä¼˜åŒ–ç°æœ‰æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„è¡¨ç°ã€‚è·¨å¤šä¸ªæ•°æ®é›†å’Œæ¶æ„çš„å®éªŒç»“æœä¸€è‡´è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¢å¼ºæ³›åŒ–æ€§èƒ½å’Œè¯†åˆ«å‡†ç¡®ç‡æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2025 Findings, 18 pages, 10 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.09846v2",
      "published_date": "2025-06-11 15:20:30 UTC",
      "updated_date": "2025-09-20 18:37:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:01:27.321972+00:00"
    },
    {
      "arxiv_id": "2506.09839v1",
      "title": "OctoNav: Towards Generalist Embodied Navigation",
      "title_zh": "OctoNavï¼šè¿ˆå‘é€šç”¨å…·èº«å¯¼èˆª",
      "authors": [
        "Chen Gao",
        "Liankai Jin",
        "Xingyu Peng",
        "Jiazhao Zhang",
        "Yue Deng",
        "Annan Li",
        "He Wang",
        "Si Liu"
      ],
      "abstract": "Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.",
      "tldr_zh": "é’ˆå¯¹ä»¥å¾€å…·èº«å¯¼èˆªç ”ç©¶åœ¨ ObjNavã€ImgNav å’Œ VLN ç­‰ä»»åŠ¡ä¸Šçš„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œè¯¥ç ”ç©¶è‡´åŠ›äºå¼€å‘èƒ½å¤Ÿéµå¾ªå¤šæ¨¡æ€è‡ªç”±æŒ‡ä»¤çš„é€šç”¨å‹å¯¼èˆªæ™ºèƒ½ä½“ã€‚ç ”ç©¶æå‡ºäº†å¤§è§„æ¨¡åŸºå‡†æµ‹è¯• OctoNav-Benchï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ ‡æ³¨æµæ°´çº¿æ„å»ºäº†åŒ…å«å¤šæ ·åŒ–è‡ªç”±æŒ‡ä»¤å’Œè½¨è¿¹å¯¹çš„è¿ç»­ç¯å¢ƒã€‚æ ¸å¿ƒæ¨¡å‹ OctoNav-R1 åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) æ„å»ºå¹¶é€‚é…ä¸º VLA ç±»å‹æ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ä»äºŒç»´è§†è§‰è§‚å¯Ÿä¸­ç”Ÿæˆåº•å±‚åŠ¨ä½œæŒ‡ä»¤ã€‚å— OpenAI-o1 å’Œ DeepSeek-R1 çš„å¯å‘ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†â€œè¡ŒåŠ¨å‰æ€è€ƒâ€(Think-Before-Action, TBA-CoT) æœºåˆ¶ï¼Œæ—¨åœ¨æ¢ç´¢å¦‚ä½•é€šè¿‡å¼ºåŒ–æ¨ç†è¿‡ç¨‹æ¥æå‡å¯¼èˆªæ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†åŒ…å« Action-/TBA-SFTã€Nav-GRPO å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹  (Online RL) çš„æ··åˆè®­ç»ƒèŒƒå¼ (Hybrid Training Paradigm, HTP)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOctoNav-R1 åœ¨å¯¼èˆªæ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå®ç°é€šç”¨çš„å…·èº«å¯¼èˆªæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages, 25 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09839v1",
      "published_date": "2025-06-11 15:15:17 UTC",
      "updated_date": "2025-06-11 15:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:52.227940+00:00"
    },
    {
      "arxiv_id": "2506.09836v1",
      "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction",
      "title_zh": "DynaSplatï¼šèåˆåˆ†å±‚è¿åŠ¨åˆ†è§£ä¸åŠ¨é™åˆ†ç¦»çš„é«˜æ–¯æ³¼æº…åœºæ™¯é‡å»º",
      "authors": [
        "Junli Deng",
        "Ping Shi",
        "Qipei Li",
        "Jinyang Guo"
      ],
      "abstract": "Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DynaSplatï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŠ¨æ€åœºæ™¯é‡å»ºçš„ Gaussian Splatting æ‰©å±•æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ¡ˆåœ¨å¤„ç†å¤æ‚çœŸå®ç¯å¢ƒåŠ¨æ€æ€§æ—¶çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡èåˆå˜å½¢åç§»ç»Ÿè®¡æ•°æ®(deformation offset statistics)ä¸ 2D è¿åŠ¨æµä¸€è‡´æ€§(2D motion flow consistency)ï¼Œå®ç°äº†åˆ›æ–°çš„åŠ¨é™æ€åˆ†ç¦»ï¼Œä»è€Œèƒ½å¤Ÿç²¾ç¡®èšç„¦äºè¿åŠ¨åŒºåŸŸã€‚DynaSplat å¼•å…¥äº†å±‚æ¬¡åŒ–è¿åŠ¨å»ºæ¨¡(hierarchical motion modeling)ç­–ç•¥ï¼Œé€šè¿‡ååŒæ•è·å…¨å±€ç²—ç•¥å˜æ¢ä¸ç»†ç²’åº¦å±€éƒ¨è¿åŠ¨ï¼Œæå‡äº†å¯¹å¤æ‚éåˆšæ€§è¿åŠ¨(non-rigid motions)çš„å¤„ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡é›†æˆåŸºäºç‰©ç†çš„é€æ˜åº¦ä¼°è®¡(physically-based opacity estimation)ï¼Œç³»ç»Ÿåœ¨é¢ä¸´é®æŒ¡å’Œé€è§†å˜åŒ–æ—¶ä»èƒ½ä¿æŒè§†è§‰é‡å»ºçš„è¿è´¯æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDynaSplat åœ¨å‡†ç¡®æ€§å’ŒçœŸå®æ„Ÿä¸Šå‡ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ï¼Œä¸ºåŠ¨æ€åœºæ™¯é‡å»ºæä¾›äº†ä¸€æ¡æ›´ç›´è§‚ã€ç´§å‡‘ä¸”é«˜æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09836v1",
      "published_date": "2025-06-11 15:13:35 UTC",
      "updated_date": "2025-06-11 15:13:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:51.333808+00:00"
    },
    {
      "arxiv_id": "2506.09827v3",
      "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection",
      "title_zh": "EmoNet-Voiceï¼šç»ä¸“å®¶éªŒè¯çš„ç»†ç²’åº¦è¯­éŸ³æƒ…æ„Ÿæ£€æµ‹åŸºå‡†",
      "authors": [
        "Christoph Schuhmann",
        "Robert Kaczmarczyk",
        "Gollam Rabby",
        "Felix Friedrich",
        "Maurice Kraus",
        "Kourosh Nadi",
        "Huu Nguyen",
        "Kristian Kersting",
        "SÃ¶ren Auer"
      ],
      "abstract": "Speech emotion recognition (SER) systems are constrained by existing datasets that typically cover only 6-10 basic emotions, lack scale and diversity, and face ethical challenges when collecting sensitive emotional states. We introduce EMONET-VOICE, a comprehensive resource addressing these limitations through two components: (1) EmoNet-Voice Big, a 5,000-hour multilingual pre-training dataset spanning 40 fine-grained emotion categories across 11 voices and 4 languages, and (2) EmoNet-Voice Bench, a rigorously validated benchmark of 4,7k samples with unanimous expert consensus on emotion presence and intensity levels. Using state-of-the-art synthetic voice generation, our privacy-preserving approach enables ethical inclusion of sensitive emotions (e.g., pain, shame) while maintaining controlled experimental conditions. Each sample underwent validation by three psychology experts. We demonstrate that our Empathic Insight models trained on our synthetic data achieve strong real-world dataset generalization, as tested on EmoDB and RAVDESS. Furthermore, our comprehensive evaluation reveals that while high-arousal emotions (e.g., anger: 95% accuracy) are readily detected, the benchmark successfully exposes the difficulty of distinguishing perceptually similar emotions (e.g., sadness vs. distress: 63% discrimination), providing quantifiable metrics for advancing nuanced emotion AI. EMONET-VOICE establishes a new paradigm for large-scale, ethically-sourced, fine-grained SER research.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« (Speech Emotion Recognition) ç°æœ‰æ•°æ®é›†è§„æ¨¡æœ‰é™ã€æƒ…æ„Ÿç±»åˆ«å•ä¸€åŠæ•æ„Ÿæ•°æ®é‡‡é›†ä¼¦ç†æŒ‘æˆ˜ç­‰é—®é¢˜ï¼Œæå‡ºäº† EmoNet-Voice è¿™ä¸€ç»†ç²’åº¦ã€ç»ä¸“å®¶éªŒè¯çš„åŸºå‡†èµ„æºã€‚è¯¥èµ„æºåŒ…å« 5,000 å°æ—¶ã€æ¶µç›– 40 ç§ç»†ç²’åº¦æƒ…æ„Ÿç±»åˆ«çš„å¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®é›† EmoNet-Voice Bigï¼Œä»¥åŠç»ä¸‰ä½å¿ƒç†å­¦ä¸“å®¶ä¸€è‡´æ€§éªŒè¯çš„æµ‹è¯•åŸºå‡† EmoNet-Voice Benchã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°é‡‡ç”¨åˆæˆè¯­éŸ³ç”ŸæˆæŠ€æœ¯ï¼Œä»¥éšç§ä¿æŠ¤çš„æ–¹å¼ä¼¦ç†åœ°çº³å…¥äº†ç–¼ç—› (pain) å’Œç¾è€» (shame) ç­‰æ•æ„Ÿæƒ…æ„ŸçŠ¶æ€ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„ Empathic Insight æ¨¡å‹åœ¨ EmoDB å’Œ RAVDESS ç­‰çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥åŸºå‡†åœ¨ä¿æŒé«˜å”¤é†’åº¦æƒ…æ„Ÿé«˜è¯†åˆ«ç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆæš´éœ²äº†åŒºåˆ†æ„ŸçŸ¥ç›¸è¿‘æƒ…æ„Ÿï¼ˆå¦‚ sadness ä¸ distressï¼‰çš„éš¾åº¦ã€‚EmoNet-Voice ä¸ºå¤§è§„æ¨¡ã€ä¼¦ç†åŒ–ä¸”ç»†ç²’åº¦çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09827v3",
      "published_date": "2025-06-11 15:06:59 UTC",
      "updated_date": "2026-01-05 18:59:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:56.830587+00:00"
    },
    {
      "arxiv_id": "2506.09822v1",
      "title": "Superstudent intelligence in thermodynamics",
      "title_zh": "çƒ­åŠ›å­¦ä¸­çš„â€œè¶…çº§å­¦ç”Ÿâ€æ™ºèƒ½",
      "authors": [
        "Rebecca Loubet",
        "Pascal Zittlau",
        "Marco Hoffmann",
        "Luisa Vollmer",
        "Sophie Fellenz",
        "Heike Leitte",
        "Fabian Jirasek",
        "Johannes Lenhard",
        "Hans Hasse"
      ],
      "abstract": "In this short note, we report and analyze a striking event: OpenAI's large language model o3 has outwitted all students in a university exam on thermodynamics. The thermodynamics exam is a difficult hurdle for most students, where they must show that they have mastered the fundamentals of this important topic. Consequently, the failure rates are very high, A-grades are rare - and they are considered proof of the students' exceptional intellectual abilities. This is because pattern learning does not help in the exam. The problems can only be solved by knowledgeably and creatively combining principles of thermodynamics. We have given our latest thermodynamics exam not only to the students but also to OpenAI's most powerful reasoning model, o3, and have assessed the answers of o3 exactly the same way as those of the students. In zero-shot mode, the model o3 solved all problems correctly, better than all students who took the exam; its overall score was in the range of the best scores we have seen in more than 10,000 similar exams since 1985. This is a turning point: machines now excel in complex tasks, usually taken as proof of human intellectual capabilities. We discuss the consequences this has for the work of engineers and the education of future engineers.",
      "tldr_zh": "æœ¬ç ”ç©¶æŠ¥å‘Šå¹¶åˆ†æäº†OpenAIçš„æ¨ç†æ¨¡å‹o3åœ¨å¤§å­¦çƒ­åŠ›å­¦(thermodynamics)è€ƒè¯•ä¸­çš„å“è¶Šè¡¨ç°ã€‚çƒ­åŠ›å­¦è€ƒè¯•é€šå¸¸è¢«è®¤ä¸ºæ˜¯å­¦ç”ŸæŒæ¡å­¦ç§‘åŸºç¡€çš„é‡éš¾ç‚¹ï¼Œè¦æ±‚è€ƒç”Ÿèƒ½å¤Ÿçµæ´»ä¸”å…·åˆ›é€ æ€§åœ°ç»“åˆç‰©ç†åŸç†ï¼Œè€Œéå•çº¯ä¾èµ–æ¨¡å¼å­¦ä¹ (pattern learning)ã€‚åœ¨zero-shot modeä¸‹ï¼Œo3æ¨¡å‹æ­£ç¡®è§£å†³äº†æ‰€æœ‰è€ƒé¢˜ï¼Œè¡¨ç°ä¼˜äºå‚åŠ æ­¤æ¬¡è€ƒè¯•çš„æ‰€æœ‰å­¦ç”Ÿï¼Œå…¶æ€»åˆ†è¾¾åˆ°äº†1985å¹´ä»¥æ¥ä¸€ä¸‡å¤šä»½ç›¸ä¼¼è¯•å·ä¸­çš„é¡¶çº§æ°´å¹³ã€‚è¿™ä¸€ç»“æœæ ‡å¿—ç€ä¸€ä¸ªè½¬æŠ˜ç‚¹ï¼Œå³æœºå™¨ç°å·²åœ¨é€šå¸¸è¢«è§†ä¸ºäººç±»æ™ºåŠ›è¯æ˜çš„å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶æœ€åè®¨è®ºäº†è¿™ä¸€è¿›å±•å¯¹å·¥ç¨‹å¸ˆå·¥ä½œåŠæœªæ¥å·¥ç¨‹æ•™è‚²å¯èƒ½äº§ç”Ÿçš„æ·±è¿œå½±å“ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "This document is the unedited Author's version of a yet to be Submitted Work to Physical Review Physics Education Research. 15 pages, 2 figures, Graphical Abstract, Highlights and SI available (12 pages)",
      "pdf_url": "https://arxiv.org/pdf/2506.09822v1",
      "published_date": "2025-06-11 15:01:41 UTC",
      "updated_date": "2025-06-11 15:01:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:00:58.527756+00:00"
    },
    {
      "arxiv_id": "2506.09820v2",
      "title": "CoRT: Code-integrated Reasoning within Thinking",
      "title_zh": "CoRTï¼šæ€ç»´ä¸­çš„ä»£ç é›†æˆæ¨ç†",
      "authors": [
        "Chengpeng Li",
        "Zhengyang Tang",
        "Ziniu Li",
        "Mingfeng Xue",
        "Keqin Bao",
        "Tian Ding",
        "Ruoyu Sun",
        "Benyou Wang",
        "Xiang Wang",
        "Junyang Lin",
        "Dayiheng Liu"
      ],
      "abstract": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\\% and 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoRTï¼Œä¸€ä¸ªæ—¨åœ¨è®©å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ›´é«˜æ•ˆåœ°åˆ©ç”¨ä»£ç è§£é‡Šå™¨ï¼ˆCode Interpreterï¼‰çš„åè®­ç»ƒæ¡†æ¶ï¼Œä»¥è§£å†³æ¨¡å‹åœ¨å¤æ‚æ•°å­¦è¿ç®—ä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚é’ˆå¯¹CIå¼•å…¥çš„å¤–éƒ¨çŸ¥è¯†ä¸æ¨¡å‹å†…éƒ¨è¡¨ç¤ºç»“åˆæ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ï¼ŒCoRTé€šè¿‡Hint-EngineeringæŠ€æœ¯åœ¨å…³é”®ä½ç½®ç­–ç•¥æ€§åœ°æ’å…¥æç¤ºï¼Œä»è€Œä¼˜åŒ–LRMä¸CIçš„äº¤äº’å¹¶åˆæˆé«˜è´¨é‡æ¨ç†æ•°æ®ã€‚ç ”ç©¶äººå‘˜åŸºäºæ‰‹åŠ¨åˆ›å»ºçš„æ ·æœ¬ï¼Œå¯¹è§„æ¨¡ä»1.5Båˆ°32Bçš„æ¨¡å‹è¿›è¡Œäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€æ‹’ç»å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoRTåœ¨äº”ä¸ªæŒ‘æˆ˜æ€§æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å‡†ç¡®ç‡ï¼Œå…¶ä¸­åœ¨DeepSeek-R1-Distill-Qwen-32Bä¸Šå®ç°äº†4%çš„ç»å¯¹æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¤§å¹…æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œ32Bæ¨¡å‹èŠ‚çœäº†çº¦30%çš„Tokenæ¶ˆè€—ï¼Œè€Œ1.5Bæ¨¡å‹åˆ™å‡å°‘äº†50%çš„Tokenä½¿ç”¨ï¼Œå®ç°äº†å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡çš„ååŒä¼˜åŒ–ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "work in progress",
      "pdf_url": "https://arxiv.org/pdf/2506.09820v2",
      "published_date": "2025-06-11 14:59:02 UTC",
      "updated_date": "2025-06-12 12:50:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:11.283638+00:00"
    },
    {
      "arxiv_id": "2506.13793v3",
      "title": "Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection",
      "title_zh": "Med-REFLï¼šåŸºäºè‡ªæ ¡æ­£ç»†ç²’åº¦åæ€çš„åŒ»ç–—æ¨ç†å¢å¼º",
      "authors": [
        "Zongxian Yang",
        "Jiayu Qian",
        "Zegao Peng",
        "Haoyu Zhang",
        "Yu-An Huang",
        "KC Tan",
        "Zhi-An Huang"
      ],
      "abstract": "Large reasoning models excel in domains like mathematics where intermediate reasoning is straightforward to verify, but struggle to self-correct in medicine fields where evaluating intermediate reasoning is cumbersome and expensive. This verification bottleneck hinders the development of reliable AI reasoners for high-stakes application. Here we propose Med-REFL, a novel framework that learns fine-grained reflection without human labels or model distillation. Med-REFL introduces a deterministic structural assessment of the reasoning space to automatically generate preference data for reflection. By globally evaluating all explored reasoning paths in a tree-of-thoughts, our method quantifies the value of corrective actions, enabling the automated construction of direct preference optimization pairs. This trains the model to recognize and amend its own reasoning fallacies. Extensive experiments show Med-REFL delivers robust gains across diverse models architectures and medical benchmarks, boosting a general-purpose Llama3.1-8B by +5.82% and the state-of-the-art Huatuo-o1 by +4.13% on the MedQA benchmark. Our Med-REFL-8B achieves state-of-the-art performance among 7-8B models while even competing with models twice its size. Crucially, targeted ablations prove its success generalizes to other domains such as logical reasoning and mitigates the `fake reflection' phenomenon in LRMs. Ultimately, our framework provides a scalable solution to the verification bottleneck, paving the way for more reliable AI reasoners in high-stakes domains like medicine. Med-REFL has been made publicly available in https://github.com/TianYin123/Med-REFL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Med-REFL æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸå› ä¸­é—´æ¨ç†æ­¥éª¤éªŒè¯æˆæœ¬é«˜æ˜‚è€Œéš¾ä»¥å®ç°è‡ªæˆ‘ä¿®æ­£çš„é—®é¢˜ã€‚Med-REFL å¼•å…¥äº†ä¸€ç§ç¡®å®šæ€§çš„æ¨ç†ç©ºé—´ç»“æ„åŒ–è¯„ä¼°æ–¹æ³•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–æ¨¡å‹è’¸é¦å³å¯å­¦ä¹ ç»†ç²’åº¦çš„ reflectionã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ€ç»´æ ‘ (tree-of-thoughts) ä¸­æ‰€æœ‰æ¢ç´¢è·¯å¾„è¿›è¡Œå…¨å±€è¯„ä¼°ï¼Œé‡åŒ–ä¿®æ­£è¡Œä¸ºçš„ä»·å€¼ï¼Œå¹¶è‡ªåŠ¨æ„å»ºç›´æ¥åå¥½ä¼˜åŒ– (Direct Preference Optimization) æ ·æœ¬ï¼Œä»è€Œè®­ç»ƒæ¨¡å‹è¯†åˆ«å¹¶ä¿®æ­£è‡ªèº«çš„æ¨ç†è°¬è¯¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMed-REFL åœ¨å¤šä¸ªåŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ï¼Œä½¿ Llama3.1-8B åœ¨ MedQA ä¸Šçš„å‡†ç¡®ç‡æå‡äº† 5.82%ï¼ŒMed-REFL-8B ç”šè‡³åœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜è¯¥æ–¹æ³•å¯æ¨å¹¿è‡³é€»è¾‘æ¨ç†ç­‰é¢†åŸŸï¼Œå¹¶èƒ½æœ‰æ•ˆç¼“è§£æ¨ç†æ¨¡å‹ä¸­çš„ fake reflection ç°è±¡ã€‚è¯¥æ¡†æ¶ä¸ºåŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸçš„ AI æ¨ç†å™¨æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„éªŒè¯è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆç¼“è§£äº†éªŒè¯ç“¶é¢ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13793v3",
      "published_date": "2025-06-11 14:58:38 UTC",
      "updated_date": "2025-12-12 16:49:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:01:13.337101+00:00"
    },
    {
      "arxiv_id": "2506.09785v4",
      "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data",
      "title_zh": "é’ˆå¯¹è¿ç»­ä¾èµ–æ•°æ®çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ç†è®ºæ¡†æ¶",
      "authors": [
        "Alexander Marusov",
        "Aleksandr Yugay",
        "Alexey Zaytsev"
      ],
      "abstract": "Self-supervised learning (SSL) has emerged as a powerful approach to learning representations, particularly in the field of computer vision. However, its application to dependent data, such as temporal and spatio-temporal domains, remains underexplored. Besides, traditional contrastive SSL methods often assume \\emph{semantic independence between samples}, which does not hold for dependent data exhibiting complex correlations. We propose a novel theoretical framework for contrastive SSL tailored to \\emph{continuous dependent data}, which allows the nearest samples to be semantically close to each other. In particular, we propose two possible \\textit{ground truth similarity measures} between objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive an analytical form for the \\textit{estimated similarity matrix} that accommodates both types of closeness between samples, thereby introducing dependency-aware loss functions. We validate our approach, \\emph{Dependent TS2Vec}, on temporal and spatio-temporal downstream problems. Given the dependency patterns presented in the data, our approach surpasses modern ones for dependent data, highlighting the effectiveness of our theoretically grounded loss functions for SSL in capturing spatio-temporal dependencies. Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with accuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on the drought classification task, which involves complex spatio-temporal patterns, our method achieves a $7$\\% higher ROC-AUC score.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning, SSL)å‡è®¾æ ·æœ¬é—´è¯­ä¹‰ç‹¬ç«‹ï¼Œä»è€Œéš¾ä»¥å¤„ç†å…·æœ‰å¤æ‚ç›¸å…³æ€§çš„è¿ç»­ä¾èµ–æ•°æ®ï¼ˆå¦‚æ—¶é—´ä¸æ—¶ç©ºæ•°æ®ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸“ä¸ºè¿ç»­ä¾èµ–æ•°æ®è®¾è®¡çš„åˆ›æ–°ç†è®ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸é‚»è¿‘æ ·æœ¬åœ¨è¯­ä¹‰ä¸Šç›¸äº’æ¥è¿‘ï¼Œå¹¶å¼•å…¥äº†â€œç¡¬â€ä¸â€œè½¯â€ä¸¤ç§åœ°æ ‡çœŸå€¼ç›¸ä¼¼åº¦åº¦é‡(ground truth similarity measures)ï¼Œç”±æ­¤æ¨å¯¼å‡ºåŒ…å«ä¾èµ–æ„ŸçŸ¥æŸå¤±å‡½æ•°(dependency-aware loss functions)çš„ä¼°è®¡ç›¸ä¼¼åº¦çŸ©é˜µã€‚é€šè¿‡åœ¨æ—¶é—´ä¸æ—¶ç©ºä¸‹æ¸¸é—®é¢˜ä¸Šå®æ–½Dependent TS2Vecæ¨¡å‹ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæ•è·äº†æ•°æ®ä¸­çš„æ—¶ç©ºä¾èµ–æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨UEAå’ŒUCRæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯”TS2Vecå‡†ç¡®ç‡æå‡äº†4.17%å’Œ2.08%ã€‚æ­¤å¤–ï¼Œåœ¨å¤æ‚çš„å¹²æ—±åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•çš„ROC-AUCåˆ†æ•°ä¹Ÿå®ç°äº†7%çš„æ˜¾è‘—å¢é•¿ï¼Œå……åˆ†éªŒè¯äº†å…¶ç†è®ºé©±åŠ¨çš„æŸå¤±å‡½æ•°åœ¨å¤„ç†ä¾èµ–æ•°æ®æ—¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09785v4",
      "published_date": "2025-06-11 14:23:47 UTC",
      "updated_date": "2025-09-30 07:56:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:24.216737+00:00"
    },
    {
      "arxiv_id": "2506.09782v2",
      "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2",
      "title_zh": "Q-SAM2ï¼šé’ˆå¯¹ Segment Anything Model 2 çš„é«˜ç²¾åº¦é‡åŒ–",
      "authors": [
        "Nicola Farronato",
        "Florian Scheidegger",
        "Mattia Rigotti",
        "Cristiano Malossi",
        "Michele Magno",
        "Haotong Qin"
      ],
      "abstract": "The Segment Anything Model 2 (SAM2) is a powerful foundation model for promptable segmentation. However, its high computational and memory costs are a major barrier to deployment on resource-constrained devices. In this paper, we present Q-SAM2, an accurate low-bit quantization method that achieves high compression and high fidelity. To address performance degradation arising from challenging weight and activation distributions during quantization, Q-SAM2 introduces two novel contributions: Variance-Reduced Calibration (VRC), an initialization method that reduces weight statistical variance by minimizing the Frobenius norm over a small calibration batch; and Learnable Statistical Clipping (LSC), a Quantization-Aware Training (QAT) method that learns momentum-stabilized clipping factors to manage outliers in weights and activations. Comprehensive experiments demonstrate that Q-SAM2 achieves highly accurate inference with substantial efficiency gains, significantly surpassing state-of-the-art general QAT schemes, particularly in the ultra-low 2-bit regime. Specifically, Q-SAM2 achieves an accuracy gain of up to 9.7 ppt in J&F on the video segmentation benchmark and 7.3 ppt in mIoU for instance segmentation over the best competing QAT model, all while achieving an 8x reduction in model size compared to the BF16 baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Q-SAM2ï¼Œä¸€ç§é’ˆå¯¹ Segment Anything Model 2 (SAM2) çš„é«˜ç²¾åº¦ä½æ¯”ç‰¹é‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¯¥å¤§æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶çš„è®¡ç®—ä¸å†…å­˜ç“¶é¢ˆã€‚Q-SAM2 å¼•å…¥äº†æ–¹å·®å‡å°‘æ ¡å‡† (Variance-Reduced Calibration, VRC) åˆå§‹æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ– Frobenius èŒƒæ•°æ¥é™ä½æƒé‡çš„ç»Ÿè®¡æ–¹å·®ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†å¯å­¦ä¹ ç»Ÿè®¡è£å‰ª (Learnable Statistical Clipping, LSC) çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization-Aware Training, QAT) æ–¹æ¡ˆï¼Œåˆ©ç”¨åŠ¨é‡ç¨³å®šçš„è£å‰ªå› å­æ¥ç®¡ç†æƒé‡å’Œæ¿€æ´»å€¼ä¸­çš„ç¦»ç¾¤å€¼ã€‚å®éªŒè¯æ˜ï¼ŒQ-SAM2 åœ¨è¶…ä½ 2-bit æ¨¡å¼ä¸‹è¡¨ç°å°¤ä¸ºå“è¶Šï¼Œåœ¨è§†é¢‘åˆ†å‰²ä»»åŠ¡çš„ J&F æŒ‡æ ‡å’Œå®ä¾‹åˆ†å‰²çš„ mIoU ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€ä¼˜ QAT æ¨¡å‹æå‡äº† 9.7 ppt å’Œ 7.3 pptã€‚åœ¨ä¿æŒé«˜ä¿çœŸæ¨ç†çš„åŒæ—¶ï¼ŒQ-SAM2 ç›¸æ¯” BF16 åŸºçº¿å®ç°äº† 8 å€çš„æ¨¡å‹ä½“ç§¯å‹ç¼©ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²åˆ†å‰²åŸºç¡€æ¨¡å‹æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.09782v2",
      "published_date": "2025-06-11 14:21:38 UTC",
      "updated_date": "2025-11-24 10:55:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:37.117654+00:00"
    },
    {
      "arxiv_id": "2506.09777v1",
      "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space",
      "title_zh": "åŸºäºç‰¹å¾è„¸ç©ºé—´é›¶é˜¶ä¼˜åŒ–çš„é»‘ç›’äººè„¸è¯†åˆ«ç³»ç»Ÿæ¨¡å‹åæ¼”",
      "authors": [
        "Anton Razzhigaev",
        "Matvey Mikhalchuk",
        "Klim Kireev",
        "Igor Udovichenko",
        "Andrey Kuznetsov",
        "Aleksandr Petiushko"
      ],
      "abstract": "Reconstructing facial images from black-box recognition models poses a significant privacy threat. While many methods require access to embeddings, we address the more challenging scenario of model inversion using only similarity scores. This paper introduces DarkerBB, a novel approach that reconstructs color faces by performing zero-order optimization within a PCA-derived eigenface space. Despite this highly limited information, experiments on LFW, AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves state-of-the-art verification accuracies in the similarity-only setting, with competitive query efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»é»‘ç›’(black-box)äººè„¸è¯†åˆ«æ¨¡å‹ä¸­é‡å»ºå›¾åƒæ‰€å¸¦æ¥çš„éšç§å¨èƒï¼Œæå‡ºäº†ä¸€ç§åä¸º DarkerBB çš„åˆ›æ–°æ¨¡å‹åè½¬(model inversion)æ–¹æ³•ã€‚ä¸ä»¥å¾€ä¾èµ–ç‰¹å¾åµŒå…¥(embeddings)çš„æ–¹æ³•ä¸åŒï¼ŒDarkerBB ä¸“æ³¨äºä»…èƒ½è·å–ç›¸ä¼¼åº¦åˆ†æ•°(similarity scores)çš„æ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨åŸºäº PCA å¯¼å‡ºçš„ç‰¹å¾è„¸ç©ºé—´(eigenface space)å†…æ‰§è¡Œé›¶é˜¶ä¼˜åŒ–(zero-order optimization)æ¥é‡å»ºå½©è‰²äººè„¸ã€‚åœ¨ LFWã€AgeDB-30 å’Œ CFP-FP ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDarkerBB åœ¨ä»…é™ç›¸ä¼¼åº¦çš„è®¾å®šä¸‹å®ç°äº†æœ€å…ˆè¿›(state-of-the-art)çš„éªŒè¯å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜é‡æ„è´¨é‡çš„åŒæ—¶ï¼Œè¿˜å…·å¤‡æå…·ç«äº‰åŠ›çš„æŸ¥è¯¢æ•ˆç‡(query efficiency)ï¼Œæ­ç¤ºäº†é»‘ç›’æ¨¡å‹åœ¨ä»…æä¾›ç›¸ä¼¼åº¦åé¦ˆæ—¶ä¾ç„¶å­˜åœ¨çš„å®‰å…¨é£é™©ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09777v1",
      "published_date": "2025-06-11 14:15:18 UTC",
      "updated_date": "2025-06-11 14:15:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:31.184201+00:00"
    },
    {
      "arxiv_id": "2506.09769v1",
      "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning",
      "title_zh": "é¢å‘æ¨¡å‹å¾ªç¯å¼å»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹ çš„è´Ÿè½½æ„ŸçŸ¥è®­ç»ƒè°ƒåº¦",
      "authors": [
        "Haruki Kainuma",
        "Takayuki Nishio"
      ],
      "abstract": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that introduces a training scheduling mechanism to minimize total training time in decentralized federated learning by accounting for both computational and communication loads. The scheduling problem is formulated as a global optimization task, which-though intractable in its original form-is made solvable by decomposing it into node-wise subproblems. To promote balanced data utilization under non-IID distributions, a variance constraint is introduced, while the overall training latency, including both computation and communication costs, is minimized through the objective function. Simulation results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly reduces training time and accelerates convergence compared to baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Load-aware Tram-FLï¼Œä½œä¸ºå¯¹ Tram-FL æ¡†æ¶çš„æ‰©å±•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥è®­ç»ƒè°ƒåº¦æœºåˆ¶æ¥æœ€å°åŒ–å»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹ ï¼ˆDecentralized Federated Learningï¼‰ä¸­çš„æ€»è®­ç»ƒæ—¶é—´ã€‚è¯¥æœºåˆ¶å°†è®¡ç®—è´Ÿè½½ï¼ˆComputational Loadï¼‰å’Œé€šä¿¡è´Ÿè½½ï¼ˆCommunication Loadï¼‰å…±åŒçº³å…¥è€ƒé‡ï¼Œå¹¶æŠŠè°ƒåº¦é—®é¢˜å»ºæ¨¡ä¸ºå…¨å±€ä¼˜åŒ–ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³åŸå§‹é—®é¢˜çš„è®¡ç®—å¤æ‚æ€§ï¼Œç ”ç©¶è€…å°†å…¶åˆ†è§£ä¸ºèŠ‚ç‚¹çº§å­é—®é¢˜ï¼ˆNode-wise subproblemsï¼‰è¿›è¡Œæ±‚è§£ã€‚é’ˆå¯¹éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®å¯¼è‡´çš„åˆ©ç”¨ä¸å‡é—®é¢˜ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†æ–¹å·®çº¦æŸï¼ˆVariance constraintï¼‰ä»¥ç¡®ä¿å‡è¡¡çš„æ•°æ®åˆ©ç”¨ã€‚å®éªŒåœ¨ MNIST å’Œ CIFAR-10 æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¯æ˜ Load-aware Tram-FL èƒ½å¤Ÿæ˜¾è‘—é™ä½æ•´ä½“è®­ç»ƒå»¶è¿Ÿï¼ˆTraining latencyï¼‰ï¼Œå¹¶æ¯”åŸºå‡†æ–¹æ³•æ›´æœ‰æ•ˆåœ°åŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)",
      "pdf_url": "https://arxiv.org/pdf/2506.09769v1",
      "published_date": "2025-06-11 14:09:53 UTC",
      "updated_date": "2025-06-11 14:09:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:32.731793+00:00"
    },
    {
      "arxiv_id": "2506.09755v2",
      "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era",
      "title_zh": "æ™ºèƒ½è®¾è®¡ 4.0ï¼šè¿ˆå‘æ™ºèƒ½ä½“ AI æ—¶ä»£çš„èŒƒå¼æ¼”è¿›",
      "authors": [
        "Shuo Jiang",
        "Min Xie",
        "Frank Youhua Chen",
        "Jian Ma",
        "Jianxi Luo"
      ],
      "abstract": "Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by foundation model-based agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of foundation model-based multi-agent collaboration. We propose an ontological framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss challenges and opportunities of ID 4.0, including perspectives on data foundations, agent collaboration mechanisms, and the formulation of design problems and objectives. In sum, these insights provide a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing the growing complexity of engineering design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Intelligent Design 4.0 (ID 4.0) è¿™ä¸€æ–°å…´èŒƒå¼ï¼Œå¼ºè°ƒç”±åŸºäºFoundation Models (FMs) çš„Agentic AIç³»ç»Ÿé©±åŠ¨å·¥ç¨‹è®¾è®¡çš„å˜é©ã€‚è®ºæ–‡ç³»ç»Ÿå›é¡¾äº†æ™ºèƒ½è®¾è®¡ä»Rule-based expert systemsã€Task-specific machine learning modelsã€Large-scale foundation AI modelsåˆ°Multi-agentåä½œçš„å››ä¸ªæ¼”è¿›é˜¶æ®µã€‚ä½œè€…æå‡ºäº†ID 4.0çš„Ontological frameworkï¼Œæ—¨åœ¨é€šè¿‡åè°ƒçš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®ç°å·¥ç¨‹è®¾è®¡æµç¨‹çš„End-to-end automationã€‚æ–‡ç« è¿˜æ·±å…¥æ¢è®¨äº†æ•°æ®åŸºç¡€ã€Agentåä½œæœºåˆ¶ä»¥åŠè®¾è®¡é—®é¢˜å»ºæ¨¡ç­‰å…³é”®é¢†åŸŸçš„æŒ‘æˆ˜ä¸æœºé‡ã€‚è¿™äº›è§è§£ä¸ºæ™ºèƒ½è®¾è®¡åœ¨å¤„ç†å¤æ‚å·¥ç¨‹ä»»åŠ¡æ—¶å®ç°æ›´é«˜çš„è‡ªé€‚åº”æ€§ã€è‡ªä¸»æ€§ä¸æœ‰æ•ˆæ€§å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "17 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09755v2",
      "published_date": "2025-06-11 13:57:26 UTC",
      "updated_date": "2025-10-29 04:00:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:10.332618+00:00"
    },
    {
      "arxiv_id": "2506.09749v2",
      "title": "Large Language Models for Combinatorial Optimization of Design Structure Matrix",
      "title_zh": "ç”¨äºè®¾è®¡ç»“æ„çŸ©é˜µç»„åˆä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Shuo Jiang",
        "Min Xie",
        "Jianxi Luo"
      ],
      "abstract": "In complex engineering systems, the dependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that rely solely on mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) to address such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM sequencing-a common CO problem. Experiments on various DSM cases demonstrate that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.",
      "tldr_zh": "åœ¨å¤æ‚å·¥ç¨‹ç³»ç»Ÿä¸­ï¼Œè®¾è®¡ç»“æ„çŸ©é˜µ(Design Structure Matrix, DSM)ç”¨äºå»ºæ¨¡ç»„ä»¶ä¾èµ–å…³ç³»ï¼Œä½†å¯¹å…¶è¿›è¡Œé‡ç»„ä»¥æœ€å°åŒ–åé¦ˆå›è·¯æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„ç»„åˆä¼˜åŒ–(Combinatorial Optimization, CO)é—®é¢˜ã€‚ä¼ ç»Ÿçš„æ•°å­¦å¯å‘å¼ç®—æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ã€å¤æ‚ä¾èµ–ç½‘ç»œæ—¶ï¼Œå¾€å¾€éš¾ä»¥æ•æ‰ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå¯¼è‡´æ±‚è§£æ•ˆç‡å’Œæ•ˆæœå—é™ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç½‘ç»œæ‹“æ‰‘ç»“æ„ä¸ä¸Šä¸‹æ–‡é¢†åŸŸçŸ¥è¯†ï¼Œå¯¹DSMåºåˆ—è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§DSMæ¡ˆä¾‹ä¸­æ¯”éšæœºå’Œç¡®å®šæ€§åŸºçº¿æ¨¡å‹å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ä¼˜çš„è§£è´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œæ— è®ºä½¿ç”¨ä½•ç§LLMä¸»å¹²ç½‘ç»œï¼Œå¼•å…¥ä¸Šä¸‹æ–‡é¢†åŸŸçŸ¥è¯†éƒ½èƒ½æ˜¾è‘—æå‡ä¼˜åŒ–æ€§èƒ½ã€‚è¿™ä¸€æˆæœè¯æ˜äº†LLMsé€šè¿‡ç»“åˆè¯­ä¹‰ä¸æ•°å­¦æ¨ç†è§£å†³å¤æ‚å·¥ç¨‹COé—®é¢˜çš„æ½œåŠ›ï¼Œä¸ºåŸºäºLLMçš„å·¥ç¨‹è®¾è®¡ä¼˜åŒ–å¼€è¾Ÿäº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "20 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09749v2",
      "published_date": "2025-06-11 13:53:35 UTC",
      "updated_date": "2025-10-31 02:18:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:38.509536+00:00"
    },
    {
      "arxiv_id": "2506.09742v1",
      "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring",
      "title_zh": "é¢å‘æ™ºèƒ½ä½“çš„ç‰¹å¾å·¥ç¨‹ï¼šä¸€ç§ç”¨äºå¯è§£é‡Šæœºå™¨å­¦ä¹ ç›‘æ§çš„è‡ªé€‚åº”è®¤çŸ¥æ¶æ„",
      "authors": [
        "Gusseppe Bravo-Rocca",
        "Peini Liu",
        "Jordi Guitart",
        "Rodrigo M Carrillo-Larco",
        "Ajay Dholakia",
        "David Ellison"
      ],
      "abstract": "Monitoring Machine Learning (ML) models in production environments is crucial, yet traditional approaches often yield verbose, low-interpretability outputs that hinder effective decision-making. We propose a cognitive architecture for ML monitoring that applies feature engineering principles to agents based on Large Language Models (LLMs), significantly enhancing the interpretability of monitoring outputs. Central to our approach is a Decision Procedure module that simulates feature engineering through three key steps: Refactor, Break Down, and Compile. The Refactor step improves data representation to better capture feature semantics, allowing the LLM to focus on salient aspects of the monitoring data while reducing noise and irrelevant information. Break Down decomposes complex information for detailed analysis, and Compile integrates sub-insights into clear, interpretable outputs. This process leads to a more deterministic planning approach, reducing dependence on LLM-generated planning, which can sometimes be inconsistent and overly general. The combination of feature engineering-driven planning and selective LLM utilization results in a robust decision support system, capable of providing highly interpretable and actionable insights. Experiments using multiple LLMs demonstrate the efficacy of our approach, achieving significantly higher accuracy compared to various baselines across several domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒä¸­ Machine Learning (ML) ç›‘æ§è¾“å‡ºå†—é•¿ä¸”å¯è§£é‡Šæ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†ç‰¹å¾å·¥ç¨‹ (Feature Engineering) åŸç†åº”ç”¨äº Large Language Models (LLM) æ™ºèƒ½ä½“çš„è‡ªé€‚åº”è®¤çŸ¥æ¶æ„ã€‚å…¶æ ¸å¿ƒ Decision Procedure æ¨¡å—é€šè¿‡ Refactorã€Break Down å’Œ Compile ä¸‰ä¸ªå…³é”®æ­¥éª¤æ¨¡æ‹Ÿç‰¹å¾å·¥ç¨‹ï¼Œæ—¨åœ¨ä¼˜åŒ–æ•°æ®è¡¨ç¤ºä»¥å‡å°‘å™ªå£°ã€åˆ†è§£å¤æ‚ä¿¡æ¯å¹¶æ•´åˆå¾—å‡ºå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„è§è§£ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥æ›´å…·ç¡®å®šæ€§çš„è§„åˆ’æœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘äº†å¯¹ LLM è‡ªä¸»ç”Ÿæˆè§„åˆ’çš„ä¾èµ–ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­è§„åˆ’ä¸ä¸€è‡´å’Œè¿‡äºå®½æ³›çš„ç¼ºé™·ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¶æ„åœ¨å¤šä¸ªé¢†åŸŸå‡å–å¾—äº†æ˜¾è‘—é«˜äºåŸºçº¿æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œæ„å»ºäº†ä¸€ä¸ªç¨³å¥ä¸”èƒ½å¤Ÿæä¾›è¡ŒåŠ¨æŒ‡å¯¼æ„ä¹‰ç›‘æ§æ´å¯Ÿçš„å†³ç­–æ”¯æŒç³»ç»Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAMAS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09742v1",
      "published_date": "2025-06-11 13:48:25 UTC",
      "updated_date": "2025-06-11 13:48:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:59.978654+00:00"
    },
    {
      "arxiv_id": "2506.09740v1",
      "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models",
      "title_zh": "ELBO-T2IAlignï¼šä¸€ç§åŸºäº ELBO çš„æ‰©æ•£æ¨¡å‹åƒç´ çº§æ–‡æœ¬-å›¾åƒå¯¹é½é€šç”¨æ ¡å‡†æ–¹æ³•",
      "authors": [
        "Qin Zhou",
        "Zhiyang Zhang",
        "Jinglong Wang",
        "Xiaobin Li",
        "Jing Zhang",
        "Qian Yu",
        "Lu Sheng",
        "Dong Xu"
      ],
      "abstract": "Diffusion models excel at image generation. Recent studies have shown that these models not only generate high-quality images but also encode text-image alignment information through attention maps or loss functions. This information is valuable for various downstream tasks, including segmentation, text-guided image editing, and compositional image generation. However, current methods heavily rely on the assumption of perfect text-image alignment in diffusion models, which is not the case. In this paper, we propose using zero-shot referring image segmentation as a proxy task to evaluate the pixel-level image and class-level text alignment of popular diffusion models. We conduct an in-depth analysis of pixel-text misalignment in diffusion models from the perspective of training data bias. We find that misalignment occurs in images with small sized, occluded, or rare object classes. Therefore, we propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text alignment in diffusion models based on the evidence lower bound (ELBO) of likelihood. Our method is training-free and generic, eliminating the need to identify the specific cause of misalignment and works well across various diffusion model architectures. Extensive experiments on commonly used benchmark datasets on image segmentation and generation have verified the effectiveness of our proposed calibration approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡é«˜ï¼Œä½†ç°æœ‰çš„åƒç´ çº§æ–‡æœ¬-å›¾åƒå¯¹é½(text-image alignment)æ–¹æ³•å¸¸ç”±äºè¯¯åˆ¤æ¨¡å‹å…·æœ‰å®Œç¾å¯¹é½èƒ½åŠ›è€Œå¤±æ•ˆã€‚ä½œè€…é€šè¿‡é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²(zero-shot referring image segmentation)ä»»åŠ¡ï¼Œæ·±å…¥åˆ†æäº†å› è®­ç»ƒæ•°æ®åå·®(training data bias)å¯¼è‡´çš„åƒç´ -æ–‡æœ¬å¤±é…ç°è±¡ï¼Œå‘ç°è¯¥é—®é¢˜åœ¨å¤„ç†å°å‹ã€é®æŒ¡æˆ–ç½•è§ç±»åˆ«æ—¶å°¤ä¸ºä¸¥é‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº† ELBO-T2IAlign æ–¹æ³•ï¼Œåˆ©ç”¨è¯æ®ä¸‹ç•Œ(ELBO)å®ç°å¯¹æ‰©æ•£æ¨¡å‹åƒç´ çº§å¯¹é½çš„æœ‰æ•ˆæ ¡å‡†ã€‚è¯¥æ–¹æ³•å…·æœ‰æ— éœ€è®­ç»ƒ(training-free)ä¸”é€šç”¨çš„ç‰¹æ€§ï¼Œå¯è·¨å¤šç§æ¨¡å‹æ¶æ„è¿è¡Œè€Œæ— éœ€è¯†åˆ«å…·ä½“çš„å¤±é…åŸå› ã€‚åœ¨å›¾åƒåˆ†å‰²å’Œç”Ÿæˆç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒELBO-T2IAlign æœ‰æ•ˆæå‡äº†å¯¹é½ç²¾åº¦ï¼Œä¸ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†ä¸‹æ¸¸è§†è§‰ä»»åŠ¡æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09740v1",
      "published_date": "2025-06-11 13:47:03 UTC",
      "updated_date": "2025-06-11 13:47:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:49.789346+00:00"
    },
    {
      "arxiv_id": "2506.13792v1",
      "title": "ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \\& a ML Ensemble on Longitudinal Identity Resolution",
      "title_zh": "ICE-IDï¼šä¸€ç§å¯¹æ¯” NARSã€LLMs ä¸æœºå™¨å­¦ä¹ é›†æˆåœ¨çºµå‘èº«ä»½è§£æä¸­è¡¨ç°çš„æ–°é¢–å†å²äººå£æ™®æŸ¥æ•°æ®åŸºå‡†",
      "authors": [
        "GonÃ§alo Hora de Carvalho",
        "Lazar S. Popov",
        "Sander Kaatee",
        "Kristinn R. ThÃ³risson",
        "Tangrui Li",
        "PÃ©tur HÃºni BjÃ¶rnsson",
        "Jilles S. Dibangoye"
      ],
      "abstract": "We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ICE-IDï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å†å²èº«ä»½è§£æ(Identity Resolution)è®¾è®¡çš„æ–°å‹åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«1703å¹´è‡³1920å¹´é—´è·¨è¶Š220å¹´çš„å†°å²›äººå£æ™®æŸ¥è®°å½•ã€‚ä½œä¸ºé¦–ä¸ªç”¨äºç ”ç©¶çœŸå®äººå£é•¿æœŸå®ä½“åŒ¹é…çš„å¤§è§„æ¨¡å¼€æºè¡¨æ ¼æ•°æ®é›†ï¼ŒICE-IDæ•æ‰äº†å¤šä»£äººçš„å§“åå˜ä½“ã€äººå£ç»Ÿè®¡æ¼”å˜åŠå¤æ‚çš„è°±ç³»å…³è”ã€‚ç ”ç©¶è€…åœ¨å®šä¹‰çš„èº«ä»½è§£æä»»åŠ¡ä¸Šï¼Œå¯¹æ¯”è¯„ä¼°äº†æ‰‹å·¥è§„åˆ™ã€æœºå™¨å­¦ä¹ é›†æˆæ¨¡å‹(ML ensemble)ã€å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»¥åŠéå…¬ç†æ¨ç†ç³»ç»Ÿ(NARS)çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºéå…¬ç†é€»è¾‘(Non-Axiomatic Logic)çš„NARSæ¡†æ¶åœ¨å¤„ç†æ­¤ç±»çŸ¥è¯†å—é™çš„æ¨ç†ä»»åŠ¡æ—¶æå…·ç«äº‰åŠ›ï¼Œå¹¶æˆåŠŸè¾¾åˆ°äº†å½“å‰å…ˆè¿›æ°´å¹³(SOTA)ã€‚é€šè¿‡å…¬å¼€æ•°æ®é›†ä¸ä»£ç ï¼Œè¯¥ç ”ç©¶ä¸ä»…ä¸ºçºµå‘èº«ä»½è§£ææä¾›äº†å¯é‡å¤çš„å®éªŒåŸºå‡†ï¼Œä¹Ÿä¸ºæ•°æ®å…³è”(Data Linkage)å’Œå†å²åˆ†æç­‰è·¨å­¦ç§‘ç ”ç©¶é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13792v1",
      "published_date": "2025-06-11 13:46:47 UTC",
      "updated_date": "2025-06-11 13:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:02:52.577348+00:00"
    },
    {
      "arxiv_id": "2506.09736v2",
      "title": "Revisiting Visual Understanding in Multimodal Reasoning through a Lens of Image Perturbation",
      "title_zh": "é€è¿‡å›¾åƒæ‰°åŠ¨è§†è§’é‡æ–°å®¡è§†å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰ç†è§£",
      "authors": [
        "Yuting Li",
        "Lai Wei",
        "Kaipeng Zheng",
        "Jingyuan Huang",
        "Guilin Li",
        "Bo Wang",
        "Linghe Kong",
        "Lichao Sun",
        "Weiran Huang"
      ],
      "abstract": "Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹è§†è§‰ä¿¡æ¯å¤„ç†çš„å¿½è§†ï¼ŒæŒ‡å‡ºä»…ä½¿ç”¨å›¾åƒæ ‡é¢˜çš„çº¯è¯­è¨€æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°ç”šè‡³ä¼˜äºå¤„ç†åŸå§‹è§†è§‰è¾“å…¥çš„ MLLMsï¼Œæš—ç¤ºç°æœ‰æ¨¡å‹æœªèƒ½æœ‰æ•ˆæ•´åˆè§†è§‰ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç®€å•çš„è§†è§‰æ‰°åŠ¨æ¡†æ¶ (visual perturbation framework)ï¼Œé€šè¿‡å¹²æ‰°é¡¹æ‹¼æ¥ (distractor concatenation)ã€ä¸»å¯¼æ€§ä¿æŒæ··åˆ (dominance-preserving mixup) å’Œéšæœºæ—‹è½¬ (random rotation) ä¸‰ç§é’ˆå¯¹æ€§æ‰°åŠ¨æ¥å¢å¼ºæ¨¡å‹çš„æ„ŸçŸ¥é²æ£’æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ° SFTã€DPO å’Œ GRPO ç­‰ç°æœ‰åè®­ç»ƒæµæ°´çº¿ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹ Qwen2.5-VL-7B è¿›è¡Œè§†è§‰æ‰°åŠ¨è®­ç»ƒï¼Œè¯¥ç ”ç©¶åœ¨å¼€æº 7B å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¨¡å‹ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ç»“æœã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†ä¸åŒæ‰°åŠ¨ç­–ç•¥å¯¹è§†è§‰æ¨ç†å„æ–¹é¢çš„ç‹¬ç‰¹è´¡çŒ®ï¼Œå¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­â€œæ›´å¥½çš„æ¨ç†å§‹äºæ›´å¥½çš„è§†è§‰è§‚å¯Ÿâ€è¿™ä¸€æ ¸å¿ƒè§‚ç‚¹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2506.09736v2",
      "published_date": "2025-06-11 13:39:46 UTC",
      "updated_date": "2025-09-28 03:21:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:03.119687+00:00"
    },
    {
      "arxiv_id": "2506.09733v3",
      "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale",
      "title_zh": "AtmosMJï¼šé‡æ–°å®¡è§†è¶…è¶Šå¹´å°ºåº¦çš„ AI å¤©æ°”é¢„æŠ¥é—¨æ§æœºåˆ¶",
      "authors": [
        "Minjong Cheon"
      ],
      "abstract": "The advent of Large Weather Models (LWMs) has marked a turning point in data-driven forecasting, with many models now outperforming traditional numerical systems in the medium range. However, achieving stable, long-range autoregressive forecasts beyond a few weeks remains a significant challenge. Prevailing state-of-the-art models that achieve year-long stability, such as SFNO and DLWP-HPX, have relied on transforming input data onto non-standard spatial domains like spherical harmonics or HEALPix meshes. This has led to the prevailing assumption that such representations are necessary to enforce physical consistency and long-term stability. This paper challenges that assumption by investigating whether comparable long-range performance can be achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep convolutional network that operates directly on ERA5 data without any spherical remapping. The model's stability is enabled by a novel Gated Residual Fusion (GRF) mechanism, which adaptively moderates feature updates to prevent error accumulation over long recursive simulations. Our results demonstrate that AtmosMJ produces stable and physically plausible forecasts for about 500 days. In quantitative evaluations, it achieves competitive 10-day forecast accuracy against models like Pangu-Weather and GraphCast, all while requiring a remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest that efficient architectural design, rather than non-standard data representation, can be the key to unlocking stable and computationally efficient long-range weather prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AtmosMJï¼Œä¸€ç§æ—¨åœ¨æŒ‘æˆ˜ç°æœ‰é•¿æœŸå¤©æ°”é¢„æŠ¥å‡è®¾çš„æ·±åº¦å·ç§¯ç½‘ç»œã€‚è™½ç„¶å½“å‰çš„Large Weather Modelsåœ¨çŸ­ä¸­æœŸé¢„æŠ¥ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é•¿æœŸè‡ªå›å½’é¢„æµ‹çš„ç¨³å®šæ€§é€šå¸¸è¢«è®¤ä¸ºä¾èµ–äºçƒè°å‡½æ•°æˆ–HEALPixç­‰éæ ‡å‡†ç©ºé—´è¡¨ç¤ºã€‚AtmosMJç›´æ¥åœ¨æ ‡å‡†ç»çº¬åº¦ç½‘æ ¼çš„ERA5æ•°æ®ä¸Šè¿è¡Œï¼Œé€šè¿‡å¼•å…¥åˆ›æ–°çš„é—¨æ§æ®‹å·®èåˆæœºåˆ¶(Gated Residual Fusion, GRF)æ¥è°ƒèŠ‚ç‰¹å¾æ›´æ–°ï¼Œä»è€Œæœ‰æ•ˆé˜²æ­¢äº†é•¿æœŸé€’å½’æ¨¡æ‹Ÿä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé•¿è¾¾çº¦500å¤©çš„ç¨³å®šä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„é¢„æŠ¥ã€‚åœ¨å®šé‡è¯„ä¼°ä¸­ï¼ŒAtmosMJåœ¨10å¤©é¢„æŠ¥å‡†ç¡®ç‡ä¸Šä¸Pangu-Weatherå’ŒGraphCastç­‰é¡¶å°–æ¨¡å‹ç›¸å½“ï¼Œä¸”åœ¨V100 GPUä¸Šçš„è®­ç»ƒæ—¶é—´ä»…éœ€5.7å¤©ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é«˜æ•ˆçš„æ¶æ„è®¾è®¡è€Œéæ•°æ®è¡¨ç¤ºå½¢å¼ï¼Œæ‰æ˜¯å®ç°ç¨³å®šä¸”ä½è®¡ç®—æˆæœ¬é•¿æœŸå¤©æ°”é¢„æµ‹çš„å…³é”®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "All authors of this manuscript have not reached a consensus on its submission to arXiv. Since at least one co-author does not agree with the current version being publicly available, we respectfully request the withdrawal of this preprint in accordance with the authors' collective decision",
      "pdf_url": "https://arxiv.org/pdf/2506.09733v3",
      "published_date": "2025-06-11 13:38:56 UTC",
      "updated_date": "2025-08-18 02:03:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:22.826892+00:00"
    },
    {
      "arxiv_id": "2506.09718v2",
      "title": "Non-Contact Health Monitoring During Daily Personal Care Routines",
      "title_zh": "æ—¥å¸¸ä¸ªäººæŠ¤ç†åœºæ™¯ä¸‹çš„éæ¥è§¦å¼å¥åº·ç›‘æµ‹",
      "authors": [
        "Xulin Ma",
        "Jiankai Tang",
        "Zhang Jiang",
        "Songqin Cheng",
        "Yuanchun Shi",
        "Dong LI",
        "Xin Liu",
        "Daniel McDuff",
        "Xiaojing Liu",
        "Yuntao Wang"
      ],
      "abstract": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring of physiological signals and offers a practical alternative to traditional health sensing methods. Although rPPG is promising for daily health monitoring, its application in long-term personal care scenarios, such as mirror-facing routines in high-altitude environments, remains challenging due to ambient lighting variations, frequent occlusions from hand movements, and dynamic facial postures. To address these challenges, we present LADH (Long-term Altitude Daily Health), the first long-term rPPG dataset containing 240 synchronized RGB and infrared (IR) facial videos from 21 participants across five common personal care scenarios, along with ground-truth PPG, respiration, and blood oxygen signals. Our experiments demonstrate that combining RGB and IR video inputs improves the accuracy and robustness of non-contact physiological monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate estimation. Furthermore, we find that multi-task learning enhances performance across multiple physiological indicators simultaneously. Dataset and code are open at https://github.com/McJackTang/FusionVitals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜æµ·æ‹”ç¯å¢ƒä¸‹ä¸ªäººæ—¥å¸¸æŠ¤ç†åœºæ™¯ä¸­ï¼Œéæ¥è§¦å¼ç”Ÿç†ä¿¡å·ç›‘æµ‹é¢ä¸´çš„å…‰ç…§å˜åŒ–ã€é¢‘ç¹é®æŒ¡åŠåŠ¨æ€é¢éƒ¨å§¿æ€ç­‰æŒ‘æˆ˜ï¼Œæ¢è®¨äº†è¿œç¨‹å…‰ç”µå®¹ç§¯è„‰ææ³¢æè®°æ³•(rPPG)çš„åº”ç”¨ã€‚ç ”ç©¶è€…æå‡ºäº†LADH (Long-term Altitude Daily Health)æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«21åå‚ä¸è€…åœ¨äº”ç§å¸¸è§ä¸ªäººæŠ¤ç†åœºæ™¯ä¸‹çš„240æ®µåŒæ­¥RGBå’Œçº¢å¤–(IR)é¢éƒ¨è§†é¢‘çš„é•¿å‘¨æœŸrPPGæ•°æ®é›†ï¼Œå¹¶æä¾›äº†åŒæ­¥é‡‡é›†çš„çœŸå®å€¼(ground-truth) PPGã€å‘¼å¸å’Œè¡€æ°§ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒèåˆRGBå’ŒIRè§†é¢‘è¾“å…¥æ˜¾è‘—æå‡äº†ç›‘æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œåœ¨å¿ƒç‡(heart rate)ä¼°ç®—ä¸­å®ç°äº†4.99 BPMçš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ (multi-task learning)èƒ½æœ‰æ•ˆå¢å¼ºå¤šä¸ªç”Ÿç†æŒ‡æ ‡çš„åŒæ­¥ç›‘æµ‹æ€§èƒ½ã€‚ç›®å‰ï¼Œè¯¥æ•°æ®é›†å’Œç›¸å…³ä»£ç å·²åœ¨GitHubå¹³å°å¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE BSN 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09718v2",
      "published_date": "2025-06-11 13:29:21 UTC",
      "updated_date": "2025-11-03 17:30:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:48.090982+00:00"
    },
    {
      "arxiv_id": "2506.09701v2",
      "title": "ABS: Enforcing Constraint Satisfaction On Generated Sequences Via Automata-Guided Beam Search",
      "title_zh": "ABSï¼šåŸºäºè‡ªåŠ¨æœºå¼•å¯¼é›†æŸæœç´¢çš„ç”Ÿæˆåºåˆ—å¼ºåˆ¶çº¦æŸæ»¡è¶³",
      "authors": [
        "Vincenzo Collura",
        "Karim Tit",
        "Laura Bussi",
        "Eleonora Giunchiglia",
        "Maxime Cordy"
      ],
      "abstract": "Sequence generation and prediction form a cornerstone of modern machine learning, with applications spanning natural language processing, program synthesis, and time-series forecasting. These tasks are typically modeled in an autoregressive fashion, where each token is generated conditional on the preceding ones, and beam search is commonly used to balance exploration and fluency during decoding. While deep learning models and Large Language Models (LLMs) excel at capturing statistical patterns in this setting, they remain ill-equipped to guarantee compliance with formal constraints. In this paper, we introduce ABS: a general and model-agnostic inference-time algorithm that guarantees compliance with any constraint that can be compiled into a Deterministic Finite Automaton (DFA), without requiring retraining. ABS leverages the DFA to guide a constrained variant of beam search: at each decoding step, transitions leading to violations are masked, while remaining paths are dynamically re-ranked according to both the model's probabilities and the automaton's acceptance structure. We formally prove that the resulting sequences are guaranteed to satisfy the given constraints, and we empirically demonstrate that ABS also improves output quality. We validate our approach on three distinct tasks: constrained image-stream classification, controlled text generation, and text infilling. In all settings, ABS achieves perfect constraint satisfaction, while outperforming or matching state-of-the-art baselines on standard quality metrics and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ABSï¼Œä¸€ç§é€šç”¨ä¸”ä¸æ¨¡å‹æ— å…³çš„æ¨ç†æ—¶ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨å›å½’ç”Ÿæˆä»»åŠ¡ä¸­éš¾ä»¥ä¿è¯ç¬¦åˆå½¢å¼åŒ–çº¦æŸçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†çº¦æŸç¼–è¯‘ä¸ºç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº(DFA)æ¥å¼•å¯¼ä¸€ç§æ”¹è¿›çš„é›†æŸæœç´¢(Beam Search)è¿‡ç¨‹ï¼Œåœ¨è§£ç çš„æ¯ä¸€æ­¥ä¸­åŠ¨æ€å±è”½è¿è§„è·¯å¾„å¹¶æ ¹æ®æ¨¡å‹æ¦‚ç‡ä¸è‡ªåŠ¨æœºç»“æ„è¿›è¡Œé‡æ–°æ’åºã€‚ç ”ç©¶äººå‘˜åœ¨ç†è®ºä¸Šè¯æ˜äº†ABSç”Ÿæˆçš„åºåˆ—èƒ½å¤Ÿå®Œå…¨æ»¡è¶³ç»™å®šçº¦æŸï¼Œå¹¶åœ¨å—é™å›¾åƒæµåˆ†ç±»ã€å—æ§æ–‡æœ¬ç”Ÿæˆå’Œæ–‡æœ¬å¡«å……ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒABSåœ¨å®ç°ç™¾åˆ†ä¹‹ç™¾çº¦æŸæ»¡è¶³ç‡çš„åŒæ—¶ï¼Œåœ¨è¾“å‡ºè´¨é‡å’Œè¿è¡Œæ•ˆç‡æ–¹é¢å‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09701v2",
      "published_date": "2025-06-11 13:14:01 UTC",
      "updated_date": "2025-11-04 13:52:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:36.358949+00:00"
    },
    {
      "arxiv_id": "2506.09695v3",
      "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model",
      "title_zh": "è¿ˆå‘å®ç”¨çš„é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ï¼šä¸€ç§è½»é‡çº§ä¸”å¯è§£é‡Šçš„è„‰å†²ç¥ç»æ¨¡å‹",
      "authors": [
        "Changwei Wu",
        "Yifei Chen",
        "Yuxin Du",
        "Jinying Zong",
        "Jie Dong",
        "Mingxuan Liu",
        "Feiwei Qin",
        "Yong Peng",
        "Jin Fan",
        "Changmiao Wang"
      ],
      "abstract": "Early diagnosis of Alzheimer's Disease (AD), particularly at the mild cognitive impairment stage, is essential for timely intervention. However, this process faces significant barriers, including reliance on subjective assessments and the high cost of advanced imaging techniques. While deep learning offers automated solutions to improve diagnostic accuracy, its widespread adoption remains constrained due to high energy requirements and computational demands, particularly in resource-limited settings. Spiking neural networks (SNNs) provide a promising alternative, as their brain-inspired design is well-suited to model the sparse and event-driven patterns characteristic of neural degeneration in AD. These networks offer the potential for developing interpretable, energy-efficient diagnostic tools. Despite their advantages, existing SNNs often suffer from limited expressiveness and challenges in stable training, which reduce their effectiveness in handling complex medical tasks. To address these shortcomings, we introduce FasterSNN, a hybrid neural architecture that combines biologically inspired Leaky Integrate-and-Fire (LIF) neurons with region-adaptive convolution and multi-scale spiking attention mechanisms. This approach facilitates efficient, sparse processing of 3D MRI data while maintaining high diagnostic accuracy. Experimental results on benchmark datasets reveal that FasterSNN delivers competitive performance with significantly enhanced efficiency and training stability, highlighting its potential for practical application in AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FasterSNNï¼Œä¸€ç§è½»é‡çº§ä¸”å¯è§£é‡Šçš„è„‰å†²ç¥ç»ç½‘ç»œ(Spiking neural networks, SNNs)æ¶æ„ï¼Œæ—¨åœ¨è§£å†³é˜¿å°”èŒ¨æµ·é»˜ç—‡(Alzheimer's Disease, AD)æ—©æœŸè¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½æ•ˆä½å’Œè®¡ç®—éœ€æ±‚é«˜çš„é—®é¢˜ã€‚è¯¥æ¶æ„å°†ç”Ÿç‰©å¯å‘çš„Leaky Integrate-and-Fire (LIF)ç¥ç»å…ƒä¸åŒºåŸŸè‡ªé€‚åº”å·ç§¯(region-adaptive convolution)åŠå¤šå°ºåº¦è„‰å†²æ³¨æ„åŠ›(multi-scale spiking attention)æœºåˆ¶ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹3D MRIæ•°æ®çš„é«˜æ•ˆç¨€ç–å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒFasterSNNåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå±•ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œè¿è¡Œæ•ˆç‡ã€‚é€šè¿‡å…‹æœä¼ ç»ŸSNNè¡¨è¾¾èƒ½åŠ›æœ‰é™çš„ç¼ºé™·ï¼Œè¯¥ç ”ç©¶ä¸ºå¼€å‘é€‚ç”¨äºå®é™…ä¸´åºŠç¯å¢ƒçš„ä½åŠŸè€—ã€é«˜æ•ˆç‡ADç­›æŸ¥å·¥å…·æä¾›äº†å…³é”®æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09695v3",
      "published_date": "2025-06-11 13:10:49 UTC",
      "updated_date": "2025-12-18 13:33:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:53.686376+00:00"
    },
    {
      "arxiv_id": "2506.09677v2",
      "title": "Benchmarking Gaslighting Negation Attacks Against Reasoning Models",
      "title_zh": "é’ˆå¯¹æ¨ç†æ¨¡å‹çš„ç…¤æ°”ç¯å¼å¦å®šæ”»å‡»åŸºå‡†æµ‹è¯•",
      "authors": [
        "Bin Zhu",
        "Hailong Yin",
        "Jingjing Chen",
        "Yu-Gang Jiang"
      ],
      "abstract": "Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†æ¨ç†ä¸­å¿ƒæ¨¡å‹ï¼ˆReasoning-centric modelsï¼‰åœ¨é¢å¯¹ Gaslighting Negation Attacksï¼ˆå³é€šè¿‡å¯¹æŠ—æ€§æç¤ºè‡ªä¿¡åœ°å¦å®šæ­£ç¡®ç­”æ¡ˆï¼‰æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ MMMUã€MathVista å’Œ CharXiv ä¸‰ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸Šå¯¹ OpenAI o4-miniã€Claude-3.7-Sonnet å’Œ Gemini-2.5-Flash è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°æ”»å‡»å¯¼è‡´æ¨¡å‹å‡†ç¡®ç‡å¹³å‡ä¸‹é™äº† 25-29%ã€‚ä¸ºæ·±å…¥æ¢è®¨è¿™ä¸€æ¼æ´ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æ¨å‡ºäº†åŒ…å« 1,025 ä¸ªæŒ‘æˆ˜æ€§æ ·æœ¬çš„è¯Šæ–­åŸºå‡† GaslightingBench-Rï¼Œåœ¨è¯¥åŸºå‡†ä¸‹æ¨¡å‹å‡†ç¡®ç‡ä¸‹é™å¹…åº¦ç”šè‡³è¶…è¿‡äº† 53%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å…·å¤‡ Chain-of-Thought æ¨ç†æœºåˆ¶ï¼Œä½†åœ¨æ“çºµæ€§åé¦ˆä¸‹ä»éš¾ä»¥åšæŒå…¶æ­£ç¡®åˆ¤æ–­ã€‚è¯¥å‘ç°æ­ç¤ºäº†é€»è¾‘æ¨ç†èƒ½åŠ›ä¸æŠ—å¯¹æŠ—æ“çºµèƒ½åŠ›ä¹‹é—´çš„æ ¹æœ¬å·®è·ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°å‹é²æ£’æ€§ç­–ç•¥ä»¥ä¿æŠ¤æ¨ç†æ¨¡å‹å…å— Gaslighting Negation Attacks ä¾µå®³çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09677v2",
      "published_date": "2025-06-11 12:52:25 UTC",
      "updated_date": "2025-12-16 19:28:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:41.491193+00:00"
    },
    {
      "arxiv_id": "2506.09672v1",
      "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data",
      "title_zh": "å¾®è°ƒæ˜¯å¦ä¸ºæœ‰æ•ˆæ–¹æ¡ˆï¼Ÿé‡æ–°è¯„ä¼°éç»“æ„åŒ–æ•°æ®çŸ¥è¯†ç¼–è¾‘",
      "authors": [
        "Hao Xiong",
        "Chuanyuan Tan",
        "Wenliang Chen"
      ],
      "abstract": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant knowledge of large language models (LLMs). It focuses on unstructured inputs, such as long or free-form texts, which are common forms of real-world knowledge. Although previous studies have proposed effective methods and tested them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2) Abnormal failure of fine-tuning (FT) based methods for UKE. To address these issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by extending two existing UKE datasets with locality test data from the unstructured and structured views. This enables a systematic evaluation of the Locality of post-edited models. Furthermore, we identify four factors that may affect the performance of FT-based methods. Based on these factors, we conduct experiments to determine how the well-performing FT-based methods should be trained for the UKE task, providing a training recipe for future research. Our experimental results indicate that the FT-based method with the optimal setting (FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art (SOTA). In batch editing scenarios, FT-UKE shows strong performance as well, with its advantage over SOTA methods increasing as the batch size grows, expanding the average metric lead from +6.78% to +10.80%",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éç»“æ„åŒ–çŸ¥è¯†ç¼–è¾‘ (Unstructured Knowledge Editing, UKE) ä»»åŠ¡ï¼Œç³»ç»Ÿæ€§åœ°é‡æ–°è¯„ä¼°äº†å¾®è°ƒ (Fine-Tuning, FT) æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…é€šè¿‡æ„å»º UnKEBench-Loc å’Œ AKEW-Loc (CF) ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œè§£å†³äº† UKE é¢†åŸŸç¼ºä¹å±€éƒ¨æ€§ (Locality) è¯„ä¼°çš„é—®é¢˜ï¼Œå®ç°äº†ä»ç»“æ„åŒ–å’Œéç»“æ„åŒ–è§†è§’å¯¹ç¼–è¾‘åæ¨¡å‹çš„å…¨é¢æµ‹è¯•ã€‚é€šè¿‡è¯†åˆ«å¹¶åˆ†æå½±å“å¾®è°ƒè¡¨ç°çš„å››ä¸ªå…³é”®å› ç´ ï¼Œè¯¥ç ”ç©¶æ€»ç»“å‡ºäº†ä¸€å¥—ä¼˜åŒ–çš„è®­ç»ƒèŒƒå¼ FT-UKEã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFT-UKE çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (SOTA) æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æ‰¹é‡ç¼–è¾‘åœºæ™¯ä¸‹ï¼ŒFT-UKE å±•ç°å‡ºæå¼ºçš„æ‰©å±•æ€§ï¼Œå…¶é¢†å…ˆä¼˜åŠ¿éš Batch Size çš„å¢åŠ ä» +6.78% æ‰©å¤§è‡³ +10.80%ã€‚è¯¥å·¥ä½œè¯æ˜äº†åœ¨é€‚å½“é…ç½®ä¸‹ï¼Œå¾®è°ƒæ˜¯è§£å†³éç»“æ„åŒ–æ•°æ®çŸ¥è¯†ç¼–è¾‘é—®é¢˜çš„å¼ºåŠ›æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è®­ç»ƒæŒ‡å—ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09672v1",
      "published_date": "2025-06-11 12:43:10 UTC",
      "updated_date": "2025-06-11 12:43:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:42.883756+00:00"
    },
    {
      "arxiv_id": "2506.09662v1",
      "title": "Empirical Quantification of Spurious Correlations in Malware Detection",
      "title_zh": "æ¶æ„è½¯ä»¶æ£€æµ‹ä¸­è™šå‡ç›¸å…³çš„å®è¯é‡åŒ–",
      "authors": [
        "Bianca Perasso",
        "Ludovico Lozza",
        "Andrea Ponte",
        "Luca Demetrio",
        "Luca Oneto",
        "Fabio Roli"
      ],
      "abstract": "End-to-end deep learning exhibits unmatched performance for detecting malware, but such an achievement is reached by exploiting spurious correlations -- features with high relevance at inference time, but known to be useless through domain knowledge. While previous work highlighted that deep networks mainly focus on metadata, none investigated the phenomenon further, without quantifying their impact on the decision. In this work, we deepen our understanding of how spurious correlation affects deep learning for malware detection by highlighting how much models rely on empty spaces left by the compiler, which diminishes the relevance of the compiled code. Through our seminal analysis on a small-scale balanced dataset, we introduce a ranking of two end-to-end models to better understand which is more suitable to be put in production.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹ä¸­åˆ©ç”¨ä¼ªç›¸å…³æ€§ï¼ˆSpurious Correlationsï¼‰â€”â€”å³åœ¨æ¨ç†é˜¶æ®µå…·æœ‰é«˜ç›¸å…³æ€§ä½†åœ¨é¢†åŸŸçŸ¥è¯†ä¸­è¢«è®¤ä¸ºæ— æ•ˆçš„ç‰¹å¾â€”â€”è¿™ä¸€ç°è±¡è¿›è¡Œäº†å®è¯é‡åŒ–ã€‚è™½ç„¶ä»¥å¾€ç ”ç©¶å‘ç°æ·±åº¦ç½‘ç»œä¸»è¦å…³æ³¨å…ƒæ•°æ®ï¼ˆMetadataï¼‰ï¼Œä½†è¯¥å·¥ä½œè¿›ä¸€æ­¥æ·±å…¥æ¢è®¨å¹¶é‡åŒ–äº†æ­¤ç±»ç›¸å…³æ€§å¯¹æ¨¡å‹å†³ç­–çš„å…·ä½“å½±å“ã€‚é€šè¿‡åœ¨å°è§„æ¨¡å¹³è¡¡æ•°æ®é›†ä¸Šçš„å¼€åˆ›æ€§åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹å¯¹ç¼–è¯‘å™¨ç•™ä¸‹çš„ç©ºç™½ç©ºé—´ï¼ˆEmpty Spacesï¼‰çš„è¿‡åº¦ä¾èµ–ï¼Œè¿™åœ¨å®¢è§‚ä¸Šé™ä½äº†å·²ç¼–è¯‘ä»£ç åœ¨è¯†åˆ«è¿‡ç¨‹ä¸­çš„æƒé‡ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†å¯¹ä¸¤ç§ç«¯åˆ°ç«¯ï¼ˆEnd-to-Endï¼‰æ¨¡å‹çš„æ€§èƒ½æ’åï¼Œæ—¨åœ¨ä¸ºé€‰æ‹©æ›´é€‚åˆç”Ÿäº§ï¼ˆProductionï¼‰ç¯å¢ƒçš„æ¨¡å‹æä¾›ç§‘å­¦ä¾æ®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09662v1",
      "published_date": "2025-06-11 12:32:06 UTC",
      "updated_date": "2025-06-11 12:32:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:03:53.292756+00:00"
    },
    {
      "arxiv_id": "2506.09659v1",
      "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model",
      "title_zh": "æ„å›¾å› å­åŒ–ç”Ÿæˆï¼šé‡Šæ”¾è¯­è¨€æ¨¡å‹çš„å¤šæ ·æ€§",
      "authors": [
        "Eltayeb Ahmed",
        "Uljad Berdica",
        "Martha Elliott",
        "Danijela Horak",
        "Jakob N. Foerster"
      ],
      "abstract": "Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response. This is problematic because it leads to poor exploration on reasoning problems and to unengaging, repetitive conversational agents. To address this we propose Intent Factored Generation (IFG), factorising the sampling process into two stages. First, we sample a semantically dense intent, e.g., a summary or keywords. Second, we sample the final response conditioning on both the original prompt and the intent from the first stage. This allows us to use a higher temperature during the intent step to promote conceptual diversity, and a lower temperature during the final generation to ensure the outputs are coherent and self-consistent. Additionally, we find that prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step is beneficial for reasoning tasks. We demonstrate our method's effectiveness across a diverse set of tasks. We show this method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. For instruction-tuning, we combine IFG with Direct Preference Optimisation to increase conversational diversity without sacrificing reward. Finally, we achieve higher diversity while maintaining the quality of generations on a general language modelling task, using a new dataset of reader comments and news articles that we collect and open-source. In summary, we present a simple method of increasing the sample diversity of LLMs while maintaining performance. This method can be implemented by changing the prompt and varying the temperature during generation, making it easy to integrate into many algorithms for gains across various applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå¤šæ ·åŒ–ä¸”é«˜è´¨é‡æ ·æœ¬æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†Intent Factored Generation (IFG)é‡‡æ ·æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºæ„å›¾é‡‡æ ·å’Œç»“æœé‡‡æ ·ä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡é«˜æ¸©åº¦(Temperature)é‡‡æ ·è¯­ä¹‰å¯†é›†çš„æ„å›¾(Intent)ä»¥æå‡æ¦‚å¿µå¤šæ ·æ€§ï¼Œéšååœ¨æç¤ºè¯ä¸æ„å›¾çš„å…±åŒçº¦æŸä¸‹åˆ©ç”¨ä½æ¸©åº¦ç¡®ä¿è¾“å‡ºçš„è¿è´¯ä¸è‡ªæ´½ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸­è¦æ±‚æ¨¡å‹å…ˆæ˜ç¡®é“¾å¼æ€ç»´(Chain-of-Thought)æ¯ä¸€æ­¥çš„æ„å›¾èƒ½æ˜¾è‘—ä¼˜åŒ–è¡¨ç°ï¼Œä¸”å®éªŒè¯æ˜IFGåœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸­æé«˜äº†pass@kåŠåŸºäºéªŒè¯å™¨åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLVF)æ•ˆæœã€‚åœ¨æŒ‡ä»¤å¾®è°ƒå’Œé€šç”¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ï¼ŒIFGèƒ½å¤Ÿä¸ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ç»“åˆï¼Œåœ¨ç»´æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—å¢åŠ æ ·æœ¬çš„å¤šæ ·æ€§ã€‚ä½œä¸ºä¸€ç§ä»…éœ€è°ƒæ•´æç¤ºè¯å’Œæ¸©åº¦æ§åˆ¶çš„ç®€ä¾¿æ–¹æ³•ï¼ŒIFGèƒ½å¤Ÿè½»æ¾é›†æˆè‡³ç°æœ‰ç®—æ³•ï¼Œä¸ºæå‡LLMsåœ¨å¤šåœºæ™¯ä¸‹çš„åº”ç”¨æ•ˆèƒ½æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09659v1",
      "published_date": "2025-06-11 12:26:45 UTC",
      "updated_date": "2025-06-11 12:26:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:17.710533+00:00"
    },
    {
      "arxiv_id": "2506.09656v2",
      "title": "Multi-level Value Alignment in Agentic AI Systems: Survey and Perspectives",
      "title_zh": "ä»£ç†å¼ AI ç³»ç»Ÿä¸­çš„å¤šå±‚æ¬¡ä»·å€¼å¯¹é½ï¼šç»¼è¿°ä¸å±•æœ›",
      "authors": [
        "Wei Zeng",
        "Hengshu Zhu",
        "Chuan Qin",
        "Han Wu",
        "Yihang Cheng",
        "Sirui Zhang",
        "Xiaowei Jin",
        "Yinuo Shen",
        "Zhenxing Wang",
        "Feimin Zhong",
        "Hui Xiong"
      ],
      "abstract": "The ongoing evolution of AI paradigms has propelled AI research into the agentic AI stage. Consequently, the focus of research has shifted from single agents and simple applications towards multi-agent autonomous decision-making and task collaboration in complex environments. As Large Language Models (LLMs) advance, their applications become more diverse and complex, leading to increasing situational and systemic risks. This has brought significant attention to value alignment for agentic AI systems, which aims to ensure that an agent's goals, preferences, and behaviors align with human values and societal norms. Addressing socio-governance demands through a Multi-level Value framework, this study comprehensively reviews value alignment in LLM-based multi-agent systems as the representative archetype of agentic AI systems. Our survey systematically examines three interconnected dimensions: First, value principles are structured via a top-down hierarchy across macro, meso, and micro levels. Second, application scenarios are categorized along a general-to-specific continuum explicitly mirroring these value tiers. Third, value alignment methods and evaluation are mapped to this tiered framework through systematic examination of benchmarking datasets and relevant methodologies. Additionally, we delve into value coordination among multiple agents within agentic AI systems. Finally, we propose several potential research directions in this field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIç ”ç©¶å‘agentic AIé˜¶æ®µæ¼”è¿›è¿‡ç¨‹ä¸­ï¼Œå¤šæ™ºèƒ½ä½“è‡ªä¸»å†³ç­–ä¸åä½œå¸¦æ¥çš„ç³»ç»Ÿæ€§é£é™©åŠä»·å€¼å¯¹é½ï¼ˆvalue alignmentï¼‰çš„é‡è¦æ€§ã€‚ä¸ºåº”å¯¹ç¤¾ä¼šæ²»ç†éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šå±‚æ¬¡ä»·å€¼æ¡†æ¶ï¼ˆMulti-level Value frameworkï¼‰ï¼Œå¯¹åŸºäºLarge Language Models (LLMs) çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä»·å€¼å¯¹é½è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚ç ”ç©¶ä»ä¸‰ä¸ªç»´åº¦å±•å¼€ï¼šé¦–å…ˆåœ¨å®è§‚ï¼ˆmacroï¼‰ã€ä¸­è§‚ï¼ˆmesoï¼‰å’Œå¾®è§‚ï¼ˆmicroï¼‰å±‚çº§ä¸Šæ„å»ºä»·å€¼åŸåˆ™ä½“ç³»ï¼›å…¶æ¬¡å°†åº”ç”¨åœºæ™¯ä¸è¿™äº›ä»·å€¼å±‚çº§è¿›è¡Œæ˜¾å¼å¯¹åº”ï¼›æœ€åå°†å¯¹é½æ–¹æ³•ä¸è¯„ä¼°åŸºå‡†æ˜ å°„è‡³è¯¥åˆ†å±‚æ¡†æ¶ä¸­ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ·±å…¥åˆ†æäº†æ™ºèƒ½ä½“ç³»ç»Ÿå†…éƒ¨å¤šæ™ºèƒ½ä½“ä¹‹é—´çš„ä»·å€¼åä½œï¼ˆvalue coordinationï¼‰æœºåˆ¶ã€‚æœ€åï¼Œç ”ç©¶è€…æ€»ç»“äº†å½“å‰æŒ‘æˆ˜å¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†å‰ç»æ€§è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09656v2",
      "published_date": "2025-06-11 12:25:38 UTC",
      "updated_date": "2025-08-07 08:42:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:04.988612+00:00"
    },
    {
      "arxiv_id": "2506.09655v2",
      "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy",
      "title_zh": "DipLLMï¼šé¢å‘ã€Šå¤–äº¤ã€‹æ¸¸æˆæˆ˜ç•¥å†³ç­–çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Kaixuan Xu",
        "Jiajun Chai",
        "Sicheng Li",
        "Yuqian Fu",
        "Yuanheng Zhu",
        "Dongbin Zhao"
      ],
      "abstract": "Diplomacy is a complex multiplayer game that requires both cooperation and competition, posing significant challenges for AI systems. Traditional methods rely on equilibrium search to generate extensive game data for training, which demands substantial computational resources. Large Language Models (LLMs) offer a promising alternative, leveraging pre-trained knowledge to achieve strong performance with relatively small-scale fine-tuning. However, applying LLMs to Diplomacy remains challenging due to the exponential growth of possible action combinations and the intricate strategic interactions among players. To address this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns equilibrium policies for Diplomacy. DipLLM employs an autoregressive factorization framework to simplify the complex task of multi-unit action assignment into a sequence of unit-level decisions. By defining an equilibrium policy within this framework as the learning objective, we fine-tune the model using only 1.5% of the data required by the state-of-the-art Cicero model, surpassing its performance. Our results demonstrate the potential of fine-tuned LLMs for tackling complex strategic decision-making in multiplayer games.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DipLLMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³å¤æ‚åšå¼ˆæ¸¸æˆã€Šå¤–äº¤ã€‹(Diplomacy)ä¸­çš„æˆ˜ç•¥å†³ç­–æŒ‘æˆ˜ã€‚é’ˆå¯¹è¯¥åšå¼ˆä¸­åŠ¨ä½œç»„åˆå‘ˆæŒ‡æ•°çº§å¢é•¿åŠç©å®¶é—´å¤æ‚çš„äº¤äº’éš¾é¢˜ï¼ŒDipLLMå¼•å…¥äº†è‡ªå›å½’åˆ†è§£æ¡†æ¶(autoregressive factorization framework)ï¼Œå°†å¤šå•ä½åŠ¨ä½œåˆ†é…ä»»åŠ¡è½¬åŒ–ä¸ºåºåˆ—åŒ–çš„å•ä½çº§å†³ç­–ã€‚é€šè¿‡åœ¨è¯¥æ¡†æ¶å†…å°†å‡è¡¡ç­–ç•¥(equilibrium policy)å®šä¹‰ä¸ºå­¦ä¹ ç›®æ ‡ï¼ŒDipLLMæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡å¹¶ç®€åŒ–äº†å¤æ‚å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä»…éœ€æœ€å…ˆè¿›æ¨¡å‹Ciceroæ‰€éœ€æ•°æ®é‡çš„1.5%å³å¯å®Œæˆå¾®è°ƒï¼Œå¹¶åœ¨å®é™…åšå¼ˆæ€§èƒ½ä¸Šå®ç°äº†è¶…è¶Šã€‚è¿™ä¸€æˆæœå……åˆ†å±•ç¤ºäº†å¾®è°ƒåçš„LLMsåœ¨å¤„ç†å¤šç©å®¶å¤æ‚æˆ˜ç•¥åšå¼ˆåŠå†³ç­–ä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the 42nd International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.09655v2",
      "published_date": "2025-06-11 12:25:32 UTC",
      "updated_date": "2025-06-23 07:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:16.892815+00:00"
    },
    {
      "arxiv_id": "2506.09644v2",
      "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning",
      "title_zh": "DGAEï¼šé¢å‘é«˜æ•ˆæ½œåœ¨è¡¨ç¤ºå­¦ä¹ çš„æ‰©æ•£å¼•å¯¼è‡ªç¼–ç å™¨",
      "authors": [
        "Dongxu Liu",
        "Jiahui Zhu",
        "Yuang Peng",
        "Haomiao Tang",
        "Yuwei Chen",
        "Chunrui Han",
        "Zheng Ge",
        "Daxin Jiang",
        "Mingxue Liao"
      ],
      "abstract": "Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoder's expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Autoencoders åœ¨é«˜å‹ç¼©ç‡ä¸‹æ€§èƒ½ä¸‹é™ä»¥åŠ GAN å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† DGAE (Diffusion-Guided Autoencoder)ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ Diffusion Model æ¥å¼•å¯¼ Decoderï¼Œæ—¨åœ¨æ¢å¤é‚£äº›æœªèƒ½ä»æ½œç©ºé—´ (Latent Representation) ä¸­å®Œå…¨è§£ç çš„ä¿¡æ¯ä¿¡å·ï¼Œä»è€Œå¤§å¹…å¢å¼ºäº†è§£ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒDGAE æœ‰æ•ˆç¼“è§£äº†é«˜ç©ºé—´å‹ç¼©ç‡å¸¦æ¥çš„è´¨é‡æŸå¤±ï¼Œå¹¶èƒ½åœ¨æ½œç©ºé—´ç»´åº¦ç¼©å° 2 å€çš„æƒ…å†µä¸‹å®ç° state-of-the-art æ€§èƒ½ã€‚åœ¨ ImageNet-1K å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒDGAE å±•ç°äº†å“è¶Šçš„ç«äº‰åŠ›ï¼Œä¸”å…¶ç”Ÿæˆçš„ç´§å‡‘æ½œç©ºé—´è¡¨ç¤ºæ˜¾è‘—åŠ é€Ÿäº†åç»­æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›ã€‚è¯¥æ–¹æ³•ä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´ç´§å‡‘çš„ç”Ÿæˆæ¨¡å‹è¡¨ç¤ºå­¦ä¹ æä¾›äº†å…¨æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09644v2",
      "published_date": "2025-06-11 12:01:03 UTC",
      "updated_date": "2026-01-13 18:26:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:40.101730+00:00"
    },
    {
      "arxiv_id": "2506.09634v1",
      "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding",
      "title_zh": "HSENetï¼šé¢å‘3DåŒ»å­¦è§†è§‰-è¯­è¨€ç†è§£çš„æ··åˆç©ºé—´ç¼–ç ç½‘ç»œ",
      "authors": [
        "Yanzhao Shi",
        "Xiaodan Zhang",
        "Junzhong Ji",
        "Haoning Jiang",
        "Chengxin Zheng",
        "Yinong Wang",
        "Liangqiong Qu"
      ],
      "abstract": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based decisions by enhancing diagnostic accuracy and workflow efficiency. While multimodal large language models (MLLMs) exhibit promising performance in visual-language understanding, existing methods mainly focus on 2D medical images, which fundamentally limits their ability to capture complex 3D anatomical structures. This limitation often leads to misinterpretation of subtle pathologies and causes diagnostic hallucinations. In this paper, we present Hybrid Spatial Encoding Network (HSENet), a framework that exploits enriched 3D medical visual cues by effective visual perception and projection for accurate and robust vision-language understanding. Specifically, HSENet employs dual-3D vision encoders to perceive both global volumetric contexts and fine-grained anatomical details, which are pre-trained by dual-stage alignment with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient multimodal projector that condenses high-resolution 3D spatial regions into a compact set of informative visual tokens via centroid-based compression. By assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly perceive and transfer hybrid visual representations to LLM's semantic space, facilitating accurate diagnostic text generation. Experimental results demonstrate that our method achieves state-of-the-art performance in 3D language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering (73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness. Our code is available at https://github.com/YanzhaoShi/HSENet.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ··åˆç©ºé—´ç¼–ç ç½‘ç»œï¼ˆHSENetï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†3DåŒ»å­¦å½±åƒæ—¶éš¾ä»¥æ•æ‰å¤æ‚è§£å‰–ç»“æ„å¹¶æ˜“äº§ç”Ÿè¯Šæ–­å¹»è§‰çš„é—®é¢˜ã€‚HSENet é‡‡ç”¨äº†åŒ3Dè§†è§‰ç¼–ç å™¨ï¼ˆdual-3D vision encodersï¼‰æ¥åŒæ—¶æ„ŸçŸ¥å…¨å±€ä½“ç§¯èƒŒæ™¯å’Œç»†ç²’åº¦çš„è§£å‰–ç»†èŠ‚ï¼Œå¹¶é€šè¿‡ä¸è¯Šæ–­æŠ¥å‘Šçš„åŒé˜¶æ®µå¯¹é½è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„ç‰¹å¾æŠ•å½±ï¼Œç ”ç©¶è®¾è®¡äº†ç©ºé—´æ‰“åŒ…å™¨ï¼ˆSpatial Packerï¼‰ï¼Œåˆ©ç”¨åŸºäºè´¨å¿ƒçš„å‹ç¼©æŠ€æœ¯å°†é«˜åˆ†è¾¨ç‡3Dç©ºé—´åŒºåŸŸè½¬åŒ–ä¸ºç²¾ç®€çš„è§†è§‰æ ‡è®°ï¼ˆvisual tokensï¼‰ï¼Œä»è€Œå°†æ··åˆè§†è§‰è¡¨ç¤ºæ— ç¼ä¼ è¾“åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHSENet åœ¨3Dè§†è§‰è¯­è¨€æ£€ç´¢ã€3DåŒ»ç–—æŠ¥å‘Šç”Ÿæˆå’Œ3Dè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ¨¡å‹å°† BLEU-4 æŒ‡æ ‡æå‡äº†8.01%ï¼Œæ˜¾è‘—å¢å¼ºäº†è‡ªåŠ¨åŒ–3D CTè¯Šæ–­çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "27 pages, 9 figures. arXiv admin note: text overlap with arXiv:2410.14200 by other authors",
      "pdf_url": "https://arxiv.org/pdf/2506.09634v1",
      "published_date": "2025-06-11 11:46:57 UTC",
      "updated_date": "2025-06-11 11:46:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:33.016946+00:00"
    },
    {
      "arxiv_id": "2506.13790v3",
      "title": "The NordDRG AI Benchmark for Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ NordDRG AI è¯„æµ‹åŸºå‡†",
      "authors": [
        "Tapio PitkÃ¤ranta"
      ],
      "abstract": "Large language models (LLMs) are being piloted for clinical coding and decision support, yet no open benchmark targets the hospital-funding layer where Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD systems, DRGs route a substantial share of multi-trillion-dollar health spending through governed grouper software, making transparency and auditability first-order concerns. We release NordDRG-AI-Benchmark, the first public, rule-complete test bed for DRG reasoning. The package includes (i) machine-readable approximately 20-sheet NordDRG definition tables and (ii) expert manuals and change-log templates that capture governance workflows. It exposes two suites: a 13-task Logic benchmark (code lookup, cross-table inference, grouping features, multilingual terminology, and CC/MCC validity checks) and a 13-task Grouper benchmark that requires full DRG grouper emulation with strict exact-match scoring on both the DRG and the triggering drg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable artefact-only evaluation. Under an artefact-only (no web) setting, on the 13 Logic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier models (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining models score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5 Thinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13, and all other tested endpoints score 0/13. To our knowledge, this is the first public report of an LLM partially emulating the complete NordDRG grouper logic with governance-grade traceability. Coupling a rule-complete release with exact-match tasks and open scoring provides a reproducible yardstick for head-to-head and longitudinal evaluation in hospital funding. Benchmark materials available in Github.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘å¸ƒäº†NordDRG-AI-Benchmarkï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è¯Šæ–­ç›¸å…³åˆ†ç»„(Diagnosis-Related Groups, DRGs)æ¨ç†çš„å…¬å¼€ã€è§„åˆ™å®Œå¤‡çš„æµ‹è¯•åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸´åºŠç¼–ç å’ŒåŒ»é™¢èµ„é‡‘å†³ç­–æ”¯æŒä¸­ç¼ºä¹é€æ˜åº¦çš„é—®é¢˜ã€‚è¯¥åŸºå‡†åŒ…æä¾›äº†æœºå™¨å¯è¯»çš„NordDRGå®šä¹‰è¡¨åŠä¸“å®¶æ‰‹å†Œï¼Œå¹¶è®¾ç«‹äº†æ¶‰åŠè·¨è¡¨æ¨ç†çš„Logicå’Œæ¶‰åŠå®Œæ•´åˆ†ç»„ä»¿çœŸçš„Grouperä¸¤ä¸ªè¯„ä»·å¥—ä»¶ã€‚é€šè¿‡ä½¿ç”¨LogicAgentå’ŒGrouperAgentè¿›è¡Œçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œè™½ç„¶GPT-5 Thinkingå’ŒOpus 4.1åœ¨é€»è¾‘ä»»åŠ¡ä¸­å–å¾—äº†æ»¡åˆ†ï¼Œä½†åœ¨å¤æ‚çš„Grouperä»¿çœŸä»»åŠ¡ä¸­ï¼Œä»…æœ‰å°‘æ•°æ¨¡å‹å¦‚GPT-5 Thinkingå’Œo3èƒ½å®ç°éƒ¨åˆ†ä»¿çœŸã€‚è¯¥ç ”ç©¶é¦–æ¬¡æŠ¥é“äº†LLMåœ¨å…·å¤‡æ²»ç†çº§å¯è¿½æº¯æ€§çš„æƒ…å†µä¸‹éƒ¨åˆ†æ¨¡æ‹ŸNordDRGå®Œæ•´åˆ†ç»„é€»è¾‘çš„èƒ½åŠ›ã€‚è¿™ä¸€æˆæœé€šè¿‡æä¾›è§„åˆ™å®Œå¤‡çš„å‘å¸ƒç‰ˆæœ¬å’Œç²¾ç¡®åŒ¹é…çš„ä»»åŠ¡ï¼Œä¸ºåŒ»ç–—èµ„é‡‘é¢†åŸŸçš„LLMæ€§èƒ½è¯„ä¼°å»ºç«‹äº†å¯å¤åˆ¶çš„è¡¡é‡æ ‡å°ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.13790v3",
      "published_date": "2025-06-11 11:40:11 UTC",
      "updated_date": "2025-08-20 13:47:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:41.347802+00:00"
    },
    {
      "arxiv_id": "2506.09600v3",
      "title": "Effective Red-Teaming of Policy-Adherent Agents",
      "title_zh": "é’ˆå¯¹éµå¾ªç­–ç•¥æ™ºèƒ½ä½“çš„æœ‰æ•ˆçº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Itay Nakash",
        "George Kour",
        "Koren Lazar",
        "Matan Vetzler",
        "Guy Uziel",
        "Ateret Anaby-Tavor"
      ],
      "abstract": "Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä»»åŠ¡å¯¼å‘å‹æ™ºèƒ½ä½“åœ¨é€€æ¬¾æˆ–å–æ¶ˆè§„åˆ™ç­‰ä¸¥æ ¼æ”¿ç­–é¢†åŸŸä¸­çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œé‡ç‚¹è§£å†³æ™ºèƒ½ä½“åœ¨ä¿æŒæœåŠ¡æ€§çš„åŒæ—¶å¦‚ä½•æ‹’ç»è¿è§„è¯·æ±‚ã€‚ä½œè€…æå‡ºäº†CRAFTï¼Œä¸€ç§åˆ©ç”¨æ”¿ç­–æ„ŸçŸ¥è¯´æœç­–ç•¥(Policy-aware persuasive strategies)çš„å¤šæ™ºèƒ½ä½“çº¢é˜Ÿæµ‹è¯•(Red-teaming)ç³»ç»Ÿï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå¯¹æŠ—æ€§ç”¨æˆ·é€šè¿‡æ“çºµæ‰‹æ®µè·å–ä¸ªäººåˆ©ç›Šçš„å¨èƒæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRAFTåœ¨è¯±å¯¼æ™ºèƒ½ä½“è¿åæ”¿ç­–æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºDANæç¤ºã€æƒ…æ„Ÿæ“çºµå’Œå¼ºåˆ¶æ€§æŒ‡ä»¤ç­‰ä¼ ç»Ÿè¶Šç‹±æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜ŸåŸºäºtau-benchæ¨å‡ºäº†å…¨æ–°çš„tau-breakåŸºå‡†æµ‹è¯•ï¼Œç”¨äºä¸¥è‹›è¯„ä¼°æ™ºèƒ½ä½“å¯¹æŠ—æ“çºµè¡Œä¸ºçš„é²æ£’æ€§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†å¤šç§é˜²å¾¡ç­–ç•¥ï¼Œå‘ç°ç°æœ‰æ‰‹æ®µè™½æœ‰ä¸€å®šä¿æŠ¤ä½œç”¨ä½†ä»æ˜¾ä¸è¶³ï¼Œå¼ºè°ƒäº†é’ˆå¯¹æ”¿ç­–æ‰§è¡Œå‹æ™ºèƒ½ä½“å¼€å‘æ›´å¼ºå¤§ã€ç ”ç©¶é©±åŠ¨å‹å®‰å…¨ä¿éšœæŠ€æœ¯çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09600v3",
      "published_date": "2025-06-11 10:59:47 UTC",
      "updated_date": "2025-08-23 09:21:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:45.153865+00:00"
    },
    {
      "arxiv_id": "2506.09566v1",
      "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies",
      "title_zh": "ä»ç¬¦å·åˆ°ç¥ç»çš„è·¨è¶Šä¸å›å½’ï¼šçŸ¥è¯†å›¾è°±ä¸å¤§è¯­è¨€æ¨¡å‹ååŒæ•ˆåº”æ¢ç©¶",
      "authors": [
        "BlaÅ¾ Å krlj",
        "Boshko Koloski",
        "Senja Pollak",
        "Nada LavraÄ"
      ],
      "abstract": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) enhances factual grounding and reasoning capabilities. This survey paper systematically examines the synergy between KGs and LLMs, categorizing existing approaches into two main groups: KG-enhanced LLMs, which improve reasoning, reduce hallucinations, and enable complex question answering; and LLM-augmented KGs, which facilitate KG construction, completion, and querying. Through comprehensive analysis, we identify critical gaps and highlight the mutual benefits of structured knowledge integration. Compared to existing surveys, our study uniquely emphasizes scalability, computational efficiency, and data quality. Finally, we propose future research directions, including neuro-symbolic integration, dynamic KG updating, data reliability, and ethical considerations, paving the way for intelligent systems capable of managing more complex real-world knowledge tasks.",
      "tldr_zh": "æœ¬ç»¼è¿°ç³»ç»Ÿåœ°æ¢è®¨äº†çŸ¥è¯†å›¾è°±(Knowledge Graphs, KGs)ä¸å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä¹‹é—´çš„ååŒä½œç”¨ï¼Œæ—¨åœ¨å¢å¼ºäº‹å®è½åœ°(factual grounding)å’Œæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å°†ç°æœ‰æ–¹æ³•å½’çº³ä¸ºä¸¤å¤§ç±»ï¼šä¸€æ˜¯åˆ©ç”¨KGså¢å¼ºLLMsä»¥å‡å°‘å¹»è§‰(hallucinations)å¹¶æå‡å¤æ‚é—®ç­”èƒ½åŠ›ï¼ŒäºŒæ˜¯åˆ©ç”¨LLMsè¾…åŠ©KGsçš„æ„å»ºã€è¡¥å…¨ä¸æŸ¥è¯¢ã€‚ç›¸è¾ƒäºåŒç±»ç»¼è¿°ï¼Œæœ¬æ–‡ç‹¬ç‰¹åœ°å¼ºè°ƒäº†å¯æ‰©å±•æ€§(scalability)ã€è®¡ç®—æ•ˆç‡(computational efficiency)å’Œæ•°æ®è´¨é‡(data quality)ç­‰å…³é”®ç»´åº¦ã€‚é€šè¿‡å…¨é¢åˆ†æï¼Œæ–‡ç« è¯†åˆ«äº†ç»“æ„åŒ–çŸ¥è¯†é›†æˆçš„äº’æƒ åˆ©ç›ŠåŠå…¶å½“å‰çš„æŠ€æœ¯å·®è·ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ç¥ç»ç¬¦å·é›†æˆ(neuro-symbolic integration)ã€åŠ¨æ€KGæ›´æ–°å’Œæ•°æ®å¯é æ€§ç­‰å‰ç»æ€§ç ”ç©¶æ–¹å‘ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºèƒ½é«˜æ•ˆå¤„ç†å¤æ‚ç°å®ä¸–ç•ŒçŸ¥è¯†ä»»åŠ¡çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç³»ç»Ÿæ€§çš„æŒ‡å¯¼ä¸å±•æœ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "To-appear as a book chapter",
      "pdf_url": "https://arxiv.org/pdf/2506.09566v1",
      "published_date": "2025-06-11 09:58:14 UTC",
      "updated_date": "2025-06-11 09:58:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:45.640108+00:00"
    },
    {
      "arxiv_id": "2506.09557v1",
      "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions",
      "title_zh": "AD^2-Benchï¼šé¢å‘æ¶åŠ£ç¯å¢ƒä¸‹è‡ªåŠ¨é©¾é©¶å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å±‚çº§åŒ–æ€ç»´é“¾è¯„æµ‹åŸºå‡†",
      "authors": [
        "Zhaoyang Wei",
        "Chenhui Qiang",
        "Bowen Jiang",
        "Xumeng Han",
        "Xuehui Yu",
        "Zhenjun Han"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to enhance the structured, multi-step decision-making capabilities of Multi-Modal Large Models (MLLMs), is particularly crucial for autonomous driving with adverse weather conditions and complex traffic environments. However, existing benchmarks have largely overlooked the need for rigorous evaluation of CoT processes in these specific and challenging scenarios. To address this critical gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically designed for autonomous driving with adverse weather and complex scenes. AD^2-Bench is meticulously constructed to fulfill three key criteria: comprehensive data coverage across diverse adverse environments, fine-grained annotations that support multi-step reasoning, and a dedicated evaluation framework tailored for assessing CoT performance. The core contribution of AD^2-Bench is its extensive collection of over 5.4k high-quality, manually annotated CoT instances. Each intermediate reasoning step in these annotations is treated as an atomic unit with explicit ground truth, enabling unprecedented fine-grained analysis of MLLMs' inferential processes under text-level, point-level, and region-level visual prompts. Our comprehensive evaluation of state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting the benchmark's difficulty and the need to advance robust, interpretable end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized evaluation platform, driving research forward by improving MLLMs' reasoning in autonomous driving, making it an invaluable resource.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†åœ¨æ¶åŠ£å¤©æ°”å’Œå¤æ‚äº¤é€šç¯å¢ƒä¸‹ç¼ºä¹å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹(MLLMs)æ¨ç†è¿‡ç¨‹è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªä¸“é—¨ä¸ºè¿™ç±»æŒ‘æˆ˜æ€§åœºæ™¯è®¾è®¡çš„é“¾å¼æ€ç»´(Chain-of-Thought)åŸºå‡†AD^2-Benchã€‚è¯¥åŸºå‡†å…·å¤‡å¹¿æ³›çš„æ•°æ®è¦†ç›–èŒƒå›´å’Œæ”¯æŒå¤šæ­¥æ¨ç†çš„ç»†ç²’åº¦æ ‡æ³¨ï¼Œæ ¸å¿ƒè´¡çŒ®åŒ…å«è¶…è¿‡5.4kä¸ªé«˜è´¨é‡çš„æ‰‹å·¥æ ‡æ³¨CoTå®ä¾‹ã€‚ç ”ç©¶å°†æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤è§†ä¸ºå…·æœ‰æ˜ç¡®çœŸå€¼(Ground Truth)çš„åŸå­å•ä½ï¼Œæ”¯æŒåœ¨æ–‡æœ¬ã€ç‚¹ä½å’ŒåŒºåŸŸçº§åˆ«è§†è§‰æç¤ºä¸‹å¯¹æ¨¡å‹çš„æ¨æ–­è¿‡ç¨‹è¿›è¡Œç»†ç²’åº¦åˆ†æã€‚å®éªŒæµ‹è¯•æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨AD^2-Benchä¸Šçš„å‡†ç¡®ç‡æ™®éä½äº60%ï¼Œåæ˜ å‡ºè¯¥åŸºå‡†æé«˜çš„æŒ‘æˆ˜æ€§ã€‚AD^2-Benchä¸ºæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°å¹³å°ï¼Œæ˜¯æ¨åŠ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç ”ç©¶çš„é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09557v1",
      "published_date": "2025-06-11 09:41:46 UTC",
      "updated_date": "2025-06-11 09:41:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:52.286153+00:00"
    },
    {
      "arxiv_id": "2506.09548v2",
      "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information",
      "title_zh": "èåˆè¶³ç«¯è§¦è§‰ä¿¡æ¯ä¸åœ¨çº¿å­¦ä¹ è…¿éƒ¨è¿åŠ¨å­¦çš„ç´§è€¦åˆæ¿€å…‰é›·è¾¾-æƒ¯æ€§-è¶³å¼é‡Œç¨‹è®¡",
      "authors": [
        "Taku Okawara",
        "Kenji Koide",
        "Aoki Takanose",
        "Shuji Oishi",
        "Masashi Yokozuka",
        "Kentaro Uno",
        "Kazuya Yoshida"
      ],
      "abstract": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is robust to challenging conditions such as featureless environments and deformable terrains. We developed an online learning-based leg kinematics model named the neural leg kinematics model, which incorporates tactile information (foot reaction force) to implicitly express the nonlinear dynamics between robot feet and the ground. Online training of this model enhances its adaptability to weight load changes of a robot (e.g., assuming delivery or transportation tasks) and terrain conditions. According to the \\textit{neural adaptive leg odometry factor} and online uncertainty estimation of the leg kinematics model-based motion predictions, we jointly solve online training of this kinematics model and odometry estimation on a unified factor graph to retain the consistency of both. The proposed method was verified through real experiments using a quadruped robot in two challenging situations: 1) a sandy beach, representing an extremely featureless area with a deformable terrain, and 2) a campus, including multiple featureless areas and terrain types of asphalt, gravel (deformable terrain), and grass. Experimental results showed that our odometry estimation incorporating the \\textit{neural leg kinematics model} outperforms state-of-the-art works. Our project page is available for further details: https://takuokawara.github.io/RAL2025_project_page/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç´§è€¦åˆçš„ LiDAR-IMU-leg odometry æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³å››è¶³æœºå™¨äººåœ¨ç‰¹å¾ç¼ºå¤±ç¯å¢ƒå’Œå¯å˜å½¢åœ°å½¢ä¸­çš„é²æ£’å®šä½æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åä¸º neural leg kinematics model çš„åœ¨çº¿å­¦ä¹ è…¿éƒ¨è¿åŠ¨å­¦æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆè§¦è§‰ä¿¡æ¯ï¼ˆfoot reaction forceï¼‰æ¥éšå¼è¡¨è¾¾æœºå™¨äººè¶³éƒ¨ä¸åœ°é¢ä¹‹é—´çš„å¤æ‚éçº¿æ€§åŠ¨åŠ›å­¦ã€‚è¯¥æ¨¡å‹çš„åœ¨çº¿è®­ç»ƒæœºåˆ¶æ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿå¯¹æœºå™¨äººè´Ÿè½½å˜åŒ–åŠä¸åŒåœ°å½¢æ¡ä»¶çš„é€‚åº”èƒ½åŠ›ã€‚é€šè¿‡åœ¨ç»Ÿä¸€çš„ factor graph ä¸­æ•´åˆ neural adaptive leg odometry factor å’Œè¿åŠ¨å­¦æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç ”ç©¶å®ç°äº†åœ¨çº¿æ¨¡å‹è®­ç»ƒä¸é‡Œç¨‹è®¡ä¼°è®¡çš„åŒæ­¥ä¼˜åŒ–ä¸ä¸€è‡´æ€§ä¿æŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ²™æ»©ã€ç ¾çŸ³å’Œè‰åœ°ç­‰å¤šç§æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹çš„å®šä½æ€§èƒ½å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›ç®—æ³•ï¼Œå±•ç°äº†åœ¨æä½ç‰¹å¾ç¯å¢ƒä¸‹çš„ä¼˜è¶Šé²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Robotics and Automation Letters, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09548v2",
      "published_date": "2025-06-11 09:28:07 UTC",
      "updated_date": "2025-07-02 04:53:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:04:53.984332+00:00"
    },
    {
      "arxiv_id": "2506.09532v4",
      "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models",
      "title_zh": "Athenaï¼šåˆ©ç”¨é«˜æ•°æ®æ•ˆç‡çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¢å¼ºå¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Shuai Wang",
        "Zhenhua Liu",
        "Jiaheng Wei",
        "Xuanwu Yin",
        "Dong Li",
        "Emad Barsoum"
      ],
      "abstract": "We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Athena-PRMï¼Œä¸€ç§ä¸“ä¸ºè¯„ä¼°å¤æ‚æ¨ç†é—®é¢˜ä¸­æ¯ä¸€æ­¥å¥–åŠ±åˆ†æ•°è€Œè®¾è®¡çš„å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(Process Reward Model)ã€‚é’ˆå¯¹ç°æœ‰PRMå¼€å‘ä¸­æ­¥éª¤çº§æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä»¥åŠä¼ ç»ŸMonte Carlo estimationæ–¹æ³•å­˜åœ¨å™ªå£°ç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºåˆ©ç”¨å¼±è¡¥å…¨å™¨(weak completer)ä¸å¼ºè¡¥å…¨å™¨(strong completer)ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§ä½œä¸ºç­›é€‰å¯é è¿‡ç¨‹æ ‡ç­¾çš„å‡†åˆ™ï¼Œä»…éœ€5,000ä¸ªæ ·æœ¬å³å¯å®ç°é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ORMåˆå§‹åŒ–å’Œè´Ÿæ•°æ®ä¸Šé‡‡æ ·ç­–ç•¥æ¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼ŒAthena-PRMåœ¨æµ‹è¯•æ—¶ç¼©æ”¾(test time scaling)ã€æ¨ç†æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°åŠå¥–åŠ±æ’åå¾®è°ƒ(reward ranked fine-tuning)ç­‰åœºæ™¯ä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚åœ¨é…åˆQwen2.5-VL-7Bè¿›è¡Œæµ‹è¯•æ—¶ç¼©æ”¾æ—¶ï¼Œå…¶åœ¨WeMathå’ŒMathVistaä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†10.2å’Œ7.1ä¸ªç™¾åˆ†ç‚¹ã€‚ç›®å‰Athena-PRMå·²åœ¨VisualProcessBenchä¸Šå–å¾—SoTAç»“æœï¼ŒF1åˆ†æ•°é¢†å…ˆå‰ä»£æ¨¡å‹3.9åˆ†ï¼Œå……åˆ†éªŒè¯äº†å…¶å¯¹æ¨ç†æ­¥éª¤è¯„ä¼°çš„ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09532v4",
      "published_date": "2025-06-11 09:01:59 UTC",
      "updated_date": "2025-12-04 18:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:04.561535+00:00"
    },
    {
      "arxiv_id": "2506.09526v1",
      "title": "Neural Functions for Learning Periodic Signal",
      "title_zh": "ç”¨äºå­¦ä¹ å‘¨æœŸä¿¡å·çš„ç¥ç»å‡½æ•°",
      "authors": [
        "Woojin Cho",
        "Minju Jo",
        "Kookjin Lee",
        "Noseong Park"
      ],
      "abstract": "As function approximators, deep neural networks have served as an effective tool to represent various signal types. Recent approaches utilize multi-layer perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its corresponding signal, facilitating the learning of continuous neural representations from discrete data points. Despite notable successes in learning diverse signal types, coordinate-based MLPs often face issues of overfitting and limited generalizability beyond the training region, resulting in subpar extrapolation performance. This study addresses scenarios where the underlying true signals exhibit periodic properties, either spatially or temporally. We propose a novel network architecture, which extracts periodic patterns from measurements and leverages this information to represent the signal, thereby enhancing generalization and improving extrapolation performance. We demonstrate the efficacy of the proposed method through comprehensive experiments, including the learning of the periodic solutions for differential equations, and time series imputation (interpolation) and forecasting (extrapolation) on real-world datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºåæ ‡çš„å¤šå±‚æ„ŸçŸ¥æœº(MLPs)åœ¨å­¦ä¹ è¿ç»­ç¥ç»è¡¨ç¤ºæ—¶å¸¸è§çš„è¿‡æ‹ŸåˆåŠå¤–æ¨(extrapolation)æ€§èƒ½å—é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºå­¦ä¹ å‘¨æœŸä¿¡å·çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥æ¶æ„æ—¨åœ¨å¤„ç†å…·æœ‰ç©ºé—´æˆ–æ—¶é—´å‘¨æœŸå±æ€§çš„ä¿¡å·ï¼Œé€šè¿‡ä»æµ‹é‡æ•°æ®ä¸­è‡ªåŠ¨æå–å‘¨æœŸæ¨¡å¼(periodic patterns)å¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯å¢å¼ºä¿¡å·è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—ç»¼åˆå®éªŒå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ±‚è§£å¾®åˆ†æ–¹ç¨‹çš„å‘¨æœŸè§£ã€æ—¶é—´åºåˆ—æ’å€¼(interpolation)ä»¥åŠåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„é¢„æµ‹ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•è·ä¿¡å·åº•å±‚å‘¨æœŸç‰¹æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½æœ‰æ•ˆæ”¹å–„æ¨¡å‹åœ¨è®­ç»ƒèŒƒå›´å¤–çš„å¤–æ¨ç²¾åº¦ã€‚è¿™ä¸€è´¡çŒ®ä¸ºç¥ç»å‡½æ•°è¡¨ç¤ºå‘¨æœŸæ€§ç‰©ç†ç°è±¡å’Œæ—¶é—´åºåˆ—æ•°æ®æä¾›äº†ä¸€ä¸ªæ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09526v1",
      "published_date": "2025-06-11 08:52:01 UTC",
      "updated_date": "2025-06-11 08:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:03.786873+00:00"
    },
    {
      "arxiv_id": "2506.09522v2",
      "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding",
      "title_zh": "é‡è®¿æ‰€è§ï¼šæ­ç¤ºLVLMè§£ç ä¸­è§†è§‰æ ‡è®°çš„è¯­è¨€å…ˆéªŒ",
      "authors": [
        "Beomsik Cho",
        "Jaehyung Kim"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance across multimodal tasks by integrating visual perception with language understanding. However, how vision information contributes to the model's decoding process remains under-explored, as reflected in frequent hallucinations. Through a series of analyses, we found that (i) vision tokens provide meaningful visual information even when hallucinations occur, and (ii) their semantics are encoded in the textual space and become explicit under appropriate vocabulary constraints. Building on these observations, we propose ReVisiT, a simple training-free decoding method that references vision tokens to guide text generation. Our approach leverages the semantic information embedded within vision tokens by projecting them into the text token distribution. Specifically, ReVisiT dynamically selects the most relevant vision token at each decoding step via context-aware constrained divergence minimization, and using its constrained projection to refine the output distribution to better incorporate visual semantics. Across five benchmarks on recent LVLMs, ReVisiT consistently enhances visual grounding with minimal computational overhead, and achieves competitive or superior results to state-of-the-art decoding baselines while reducing computational cost by up to $2\\times$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)è§£ç è¿‡ç¨‹ä¸­è§†è§‰ä¿¡æ¯çš„è´¡çŒ®ï¼Œæ—¨åœ¨è§£å†³é¢‘ç¹å‡ºç°çš„å¹»è§‰(hallucinations)é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡åˆ†æå‘ç°ï¼Œvision tokens å³ä½¿åœ¨å¹»è§‰å‘ç”Ÿæ—¶ä¹ŸåŒ…å«æœ‰æ„ä¹‰çš„è§†è§‰ä¿¡æ¯ï¼Œä¸”å…¶è¯­ä¹‰åœ¨æ–‡æœ¬ç©ºé—´ä¸­ä»¥æ˜¾å¼æ–¹å¼ç¼–ç ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œä½œè€…æå‡ºäº† ReVisiTï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç æ–¹æ³•ï¼Œé€šè¿‡å°† vision tokens æŠ•å½±åˆ°æ–‡æœ¬ token åˆ†å¸ƒæ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ReVisiT åˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„çº¦æŸæ•£åº¦æœ€å°åŒ–(context-aware constrained divergence minimization)æŠ€æœ¯åŠ¨æ€é€‰æ‹©ç›¸å…³çš„ vision token å¹¶ä¿®æ­£è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œå¼ºåŒ–è§†è§‰è¯­ä¹‰çš„ç»“åˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReVisiT æ˜¾è‘—å¢å¼ºäº†è§†è§‰å¯¹é½(visual grounding)ï¼Œåœ¨ä¿æŒç«äº‰æ€§æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®¡ç®—æˆæœ¬é™ä½äº†å¤šè¾¾ 2 å€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Code available at https://github.com/bscho333/ReVisiT",
      "pdf_url": "https://arxiv.org/pdf/2506.09522v2",
      "published_date": "2025-06-11 08:46:55 UTC",
      "updated_date": "2025-10-11 19:17:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:08.885713+00:00"
    },
    {
      "arxiv_id": "2506.09520v1",
      "title": "How attention simplifies mental representations for planning",
      "title_zh": "æ³¨æ„åŠ›å¦‚ä½•ç®€åŒ–è§„åˆ’ä¸­çš„å¿ƒç†è¡¨å¾",
      "authors": [
        "Jason da Silva Castanheira",
        "Nicholas Shea",
        "Stephen M. Fleming"
      ],
      "abstract": "Human planning is efficient -- it frugally deploys limited cognitive resources to accomplish difficult tasks -- and flexible -- adapting to novel problems and environments. Computational approaches suggest that people construct simplified mental representations of their environment, balancing the complexity of a task representation with its utility. These models imply a nested optimisation in which planning shapes perception, and perception shapes planning -- but the perceptual and attentional mechanisms governing how this interaction unfolds remain unknown. Here, we harness virtual maze navigation to characterise how spatial attention controls which aspects of a task representation enter subjective awareness and are available for planning. We find that spatial proximity governs which aspects of a maze are available for planning, and that when task-relevant information follows natural (lateralised) contours of attention, people can more easily construct simplified and useful maze representations. This influence of attention varies considerably across individuals, explaining differences in people's task representations and behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the effects of visuospatial attention into existing computational accounts of value-guided construal. Together, our work bridges computational perspectives on perception and decision-making to better understand how individuals represent their environments in aid of planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ³¨æ„åŠ›å¦‚ä½•ç®€åŒ–ç”¨äºè§„åˆ’çš„å¿ƒç†è¡¨å¾ï¼Œæ—¨åœ¨æ­ç¤ºäººç±»åœ¨å¤æ‚ä»»åŠ¡ä¸­é«˜æ•ˆä¸”çµæ´»åˆ†é…è®¤çŸ¥èµ„æºçš„æœºåˆ¶ã€‚ç ”ç©¶é€šè¿‡è™šæ‹Ÿè¿·å®«å¯¼èˆªä»»åŠ¡ï¼Œè¡¨å¾äº†ç©ºé—´æ³¨æ„åŠ›(Spatial Attention)å¦‚ä½•æ§åˆ¶ç¯å¢ƒä¿¡æ¯è¿›å…¥ä¸»è§‚æ„è¯†å¹¶ç”¨äºè§„åˆ’ã€‚å®éªŒå‘ç°ç©ºé—´é‚»è¿‘æ€§(Spatial Proximity)å†³å®šäº†å“ªäº›ç¯å¢ƒä¿¡æ¯å¯è¢«çº³å…¥è§„åˆ’è¿‡ç¨‹ï¼Œä¸”å½“ä»»åŠ¡ç›¸å…³ä¿¡æ¯ç¬¦åˆæ³¨æ„åŠ›çš„è‡ªç„¶ä¾§åŒ–è½®å»“æ—¶ï¼Œä¸ªä½“æ›´å®¹æ˜“æ„å»ºç®€åŒ–ä¸”æœ‰æ•ˆçš„è¿·å®«è¡¨å¾ã€‚è¿™ç§æ³¨æ„åŠ›çš„å½±å“åœ¨ä¸ªä½“é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè§£é‡Šäº†ä¸åŒäººåœ¨ä»»åŠ¡è¡¨å¾å’Œè¡Œä¸ºä¸Šçš„å¤šæ ·æ€§ã€‚å—â€œæ³¨æ„åŠ›èšå…‰ç¯â€(Spotlight of Attention)ç±»æ¯”çš„å¯å‘ï¼Œç ”ç©¶è¿›ä¸€æ­¥å°†è§†è§‰ç©ºé—´æ³¨æ„åŠ›çš„å½±å“æ•´åˆè¿›ç°æœ‰çš„ä»·å€¼å¯¼å‘è§£é‡Š(Value-Guided Construal)è®¡ç®—æ¨¡å‹ä¸­ã€‚è¯¥å·¥ä½œæ¡¥æ¥äº†æ„ŸçŸ¥ä¸å†³ç­–çš„è®¡ç®—è§†è§’ï¼Œä¸ºç†è§£ä¸ªä½“å¦‚ä½•ä¸ºè§„åˆ’ç›®çš„è€Œè¡¨å¾ç¯å¢ƒæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09520v1",
      "published_date": "2025-06-11 08:46:05 UTC",
      "updated_date": "2025-06-11 08:46:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:23.290518+00:00"
    },
    {
      "arxiv_id": "2506.09513v3",
      "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning",
      "title_zh": "ReasonMedï¼šæ—¨åœ¨æ¨è¿›åŒ»å­¦æ¨ç†èƒ½åŠ›çš„ 370K å¤šæ™ºèƒ½ä½“ç”Ÿæˆæ•°æ®é›†",
      "authors": [
        "Yu Sun",
        "Xingyu Qian",
        "Weiwen Xu",
        "Hao Zhang",
        "Chenghao Xiao",
        "Long Li",
        "Deli Zhao",
        "Wenbing Huang",
        "Tingyang Xu",
        "Qifeng Bai",
        "Yu Rong"
      ],
      "abstract": "Reasoning-based large language models have excelled in mathematics and programming, yet their potential in knowledge-intensive medical question answering remains underexplored and insufficiently validated in clinical contexts. To bridge this gap, we introduce ReasonMed, the largest medical reasoning dataset to date, comprising 370k high-quality examples distilled from 1.75 million initial reasoning paths generated by complementary LLMs and curated through a cost-efficient easy-medium-difficult (EMD) pipeline. ReasonMed is built through a multi-agent generation, verification, and refinement process, in which an Error Refiner improves reasoning paths by correcting error-prone steps identified by a verifier. Using ReasonMed, we investigate effective strategies for training medical reasoning models and find that integrating detailed CoT reasoning with concise answer summaries yields the most robust fine-tuning results. Models trained on ReasonMed set a new benchmark: ReasonMed-7B surpasses the prior best sub-10B models by 4.17% and even exceeds LLaMA3.1-70B on PubMedQA by 4.60%. When scaled to ReasonMed-14B, it remains highly competitive, underscoring consistent scaling potential. The codes and datasets are available at https://github.com/YuSun-Work/ReasonMed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ReasonMedï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥åŸºäºæ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨çŸ¥è¯†å¯†é›†å‹åŒ»å­¦é—®ç­”é¢†åŸŸåº”ç”¨ä¸è¶³çš„ç°çŠ¶ã€‚è¯¥æ•°æ®é›†é€šè¿‡é«˜æ•ˆçš„easy-medium-difficult (EMD) æµç¨‹ï¼Œä»175ä¸‡ä¸ªåˆå§‹æ¨ç†è·¯å¾„ä¸­æç‚¼å‡º370kä¸ªé«˜è´¨é‡æ ·æœ¬ã€‚æ„å»ºè¿‡ç¨‹é‡‡ç”¨äº†å¤šæ™ºèƒ½ä½“(multi-agent)ç”Ÿæˆã€éªŒè¯ä¸ç²¾ç‚¼æœºåˆ¶ï¼Œé€šè¿‡Error Refinerç»„ä»¶ä¿®æ­£éªŒè¯å™¨è¯†åˆ«çš„é”™è¯¯æ­¥éª¤ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†è·¯å¾„çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°†è¯¦ç»†çš„Chain-of-Thought (CoT) æ¨ç†ä¸ç®€æ´çš„ç­”æ¡ˆæ‘˜è¦ç›¸ç»“åˆæ˜¯è®­ç»ƒåŒ»å­¦æ¨ç†æ¨¡å‹æœ€æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒReasonMed-7Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æ­¤å‰æœ€ä½³çš„10Bä»¥ä¸‹æ¨¡å‹ï¼Œå¹¶åœ¨PubMedQAæµ‹è¯•ä¸­ä¼˜äºLLaMA3.1-70Bã€‚éšç€å‚æ•°è§„æ¨¡æ‰©å±•è‡³14Bï¼Œæ¨¡å‹ä¾ç„¶ä¿æŒäº†æå¼ºçš„ç«äº‰åŠ›ï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†åœ¨æ¨åŠ¨åŒ»ç–—AIæ¨ç†èƒ½åŠ›æ–¹é¢çš„æŒç»­æ‰©å±•æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 6 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.09513v3",
      "published_date": "2025-06-11 08:36:55 UTC",
      "updated_date": "2025-10-09 07:42:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:27.483948+00:00"
    },
    {
      "arxiv_id": "2506.09508v2",
      "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design",
      "title_zh": "é«˜æ•ˆçš„åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼šéšæœºæ¢ç´¢ä¸å®éªŒè®¾è®¡çš„ç»“åˆ",
      "authors": [
        "Andreas Schlaginhaufen",
        "Reda Ouhamma",
        "Maryam Kamgarpour"
      ],
      "abstract": "We study reinforcement learning from human feedback in general Markov decision processes, where agents learn from trajectory-level preference comparisons. A central challenge in this setting is to design algorithms that select informative preference queries to identify the underlying reward while ensuring theoretical guarantees. We propose a meta-algorithm based on randomized exploration, which avoids the computational challenges associated with optimistic approaches and remains tractable. We establish both regret and last-iterate guarantees under mild reinforcement learning oracle assumptions. To improve query complexity, we introduce and analyze an improved algorithm that collects batches of trajectory pairs and applies optimal experimental design to select informative comparison queries. The batch structure also enables parallelization of preference queries, which is relevant in practical deployment as feedback can be gathered concurrently. Empirical evaluation confirms that the proposed method is competitive with reward-based reinforcement learning while requiring a small number of preference queries.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov decision processes)ä¸­åŸºäºäººç±»åå¥½æ¯”è¾ƒçš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡è®¾è®¡é«˜æ•ˆçš„æŸ¥è¯¢æœºåˆ¶æ¥è¯†åˆ«åº•å±‚å¥–åŠ±å‡½æ•°ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¢ç´¢(Randomized Exploration)çš„å…ƒç®—æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆé¿å¼€äº†ä¼ ç»Ÿä¹è§‚æ–¹æ³•(Optimistic approaches)é¢ä¸´çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå¹¶åœ¨æ¸©å’Œçš„ç®—å­å‡è®¾ä¸‹æä¾›äº†é—æ†¾(Regret)å’Œæœ€åè¿­ä»£(Last-iterate)çš„ç†è®ºä¿è¯ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–æŸ¥è¯¢å¤æ‚åº¦(Query complexity)ï¼Œç ”ç©¶å¼•å…¥äº†ç»“åˆæœ€ä¼˜å®éªŒè®¾è®¡(Optimal experimental design)çš„æ‰¹é‡é‡‡æ ·æœºåˆ¶ï¼Œåˆ©ç”¨è½¨è¿¹å¯¹çš„æ‰¹é‡ç»“æ„å®ç°äº†åå¥½æŸ¥è¯¢çš„å¹¶è¡ŒåŒ–å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…éœ€å°‘é‡åå¥½æŸ¥è¯¢çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½è¡¨ç°å³å¯ä¸åŸºäºæ˜¾å¼å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ (Reward-based reinforcement learning)ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09508v2",
      "published_date": "2025-06-11 08:27:16 UTC",
      "updated_date": "2025-12-03 21:18:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:31.095498+00:00"
    },
    {
      "arxiv_id": "2506.09507v3",
      "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding",
      "title_zh": "TransXSSMï¼šé‡‡ç”¨ç»Ÿä¸€æ—‹è½¬ä½ç½®åµŒå…¥çš„æ··åˆ Transformer çŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Bingheng Wu",
        "Jingze Shi",
        "Yifan Wu",
        "Nan Tang",
        "Yuyu Luo"
      ],
      "abstract": "Transformers exhibit proficiency in capturing long-range dependencies, whereas State Space Models (SSMs) facilitate linear-time sequence modeling. Notwithstanding their synergistic potential, the integration of these architectures presents a significant challenge, primarily attributable to a fundamental incongr inuity their respective positional encoding mechanisms: Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs leverage implicit positional representations via convolutions. This divergence often precipitates discontinuities and suboptimal performance.To address this impediment, we propose a unified rotary position embedding (Unified RoPE) methodology, thereby establishing a consistent positional encoding framework for both self-attention and state-space components. Using this Unified RoPE, we introduce TransXSSM, a hybrid architecture that coherently integrates the Transformer and SSM layers under this unified positional encoding scheme. At a 4 sequenceK length, TransXSSM exhibits training and inference speeds that are 42.3% and 29.5% faster, respectively, relative to standard Transformer models. It also delivers higher accuracy: under comparable settings, it surpasses a Transformer baseline by over 4% on language modeling benchmarks.TransXSSM furthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average accuracy over its 320M version (versus about 6% gains for equivalent Transformers or SSMs). Our results show that unified positional encoding resolves positional incompatibility in hybrid models, enabling efficient, high-performance long-context modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformers æ•æ‰é•¿ç¨‹ä¾èµ–çš„èƒ½åŠ›ä¸ State Space Models (SSMs) çº¿æ€§æ—¶é—´å»ºæ¨¡ä¼˜åŠ¿ç»“åˆæ—¶ï¼Œå› ä½ç½®ç¼–ç æœºåˆ¶ï¼ˆRoPE ä¸éšå¼å·ç§¯è¡¨ç¤ºï¼‰ä¸ä¸€è‡´å¯¼è‡´çš„æ€§èƒ½å—é™é—®é¢˜ï¼Œæå‡ºäº†ç»Ÿä¸€æ—‹è½¬ä½ç½®åµŒå…¥ (Unified RoPE) æ–¹æ³•ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†æ··åˆæ¶æ„ TransXSSMï¼Œå®ç°äº†è‡ªæ³¨æ„åŠ›ä¸çŠ¶æ€ç©ºé—´å±‚åœ¨ä½ç½®ç¼–ç ä¸Šçš„é«˜åº¦ç»Ÿä¸€ã€‚åœ¨ 4K åºåˆ—é•¿åº¦çš„å®éªŒä¸­ï¼ŒTransXSSM çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦è¾ƒæ ‡å‡† Transformer åˆ†åˆ«æå‡äº† 42.3% å’Œ 29.5%ï¼Œä¸”åœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡é«˜å‡º 4% ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒTransXSSM-1.3B åœ¨æ¨¡å‹æ‰©å±•æ€§ä¸Šå±•ç°å‡ºæ¯”åŒç±»æ¨¡å‹æ›´æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼Œè¯æ˜äº†ç»Ÿä¸€ä½ç½®ç¼–ç åœ¨è§£å†³æ··åˆæ¶æ„å…¼å®¹æ€§åŠå®ç°é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09507v3",
      "published_date": "2025-06-11 08:26:51 UTC",
      "updated_date": "2025-06-18 05:38:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:38.287907+00:00"
    },
    {
      "arxiv_id": "2506.09499v1",
      "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes",
      "title_zh": "é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„ç»„åˆæ€§ã€æ¨¡å—åŒ–ä¸å¯è§£é‡Šæ€§ç»Ÿä¸€ç†è®º",
      "authors": [
        "Thomas J. Ringstrom",
        "Paul R. Schrater"
      ],
      "abstract": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free Markov Decision Process. Rather than a value function, OKBEs directly construct and optimize a predictive map called a state-time option kernel (STOK) to maximize the probability of completing a goal while avoiding constraint violations. STOKs are compositional, modular, and interpretable initiation-to-termination transition kernels for policies in the Options Framework of Reinforcement Learning. This means: 1) STOKs can be composed using Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple policies over long horizons, 2) high-dimensional STOKs can be represented and computed efficiently in a factorized and reconfigurable form, and 3) STOKs record the probabilities of semantically interpretable goal-success and constraint-violation events, needed for formal verification. Given a high-dimensional state-transition model for an intractable planning problem, we can decompose it with local STOKs and goal-conditioned policies that are aggregated into a factorized goal kernel, making it possible to forward-plan at the level of goals in high-dimensions to solve the problem. These properties lead to highly flexible agents that can rapidly synthesize meta-policies, reuse planning representations across many tasks, and justify goals using empowerment, an intrinsic motivation function. We argue that reward-maximization is in conflict with the properties of compositionality, modularity, and interpretability. Alternatively, OKBEs facilitate these properties to support verifiable long-horizon planning and intrinsic motivation that scales to dynamic high-dimensional world-models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Option Kernel Bellman Equations (OKBEs)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ— å¥–åŠ± Markov Decision Process (MDP) çš„ç»Ÿä¸€ç†è®ºæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ä»·å€¼å‡½æ•°ä¸åŒï¼ŒOKBEs ç›´æ¥æ„å»ºå¹¶ä¼˜åŒ–åä¸º state-time option kernel (STOK) çš„é¢„æµ‹æ˜ å°„ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ç›®æ ‡å®Œæˆæ¦‚ç‡çš„åŒæ—¶è§„é¿çº¦æŸå†²çªã€‚STOKs å…·å¤‡é«˜åº¦çš„ Compositionality å’Œ Modularityï¼Œèƒ½å¤Ÿåˆ©ç”¨ Chapman-Kolmogorov æ–¹ç¨‹è¿›è¡Œé•¿æ—¶ç¨‹çš„æ—¶ç©ºé¢„æµ‹ï¼Œå¹¶ä»¥å› å­åŒ–å½¢å¼é«˜æ•ˆå¤„ç†é«˜ç»´æ•°æ®ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ˜¾è‘—çš„ Interpretabilityï¼Œé€šè¿‡è®°å½•è¯­ä¹‰åŒ–çš„ç›®æ ‡æˆåŠŸä¸çº¦æŸè¿åäº‹ä»¶ï¼Œæ”¯æŒå¯¹å¤æ‚è§„åˆ’ä»»åŠ¡è¿›è¡Œæ­£å¼éªŒè¯ã€‚é€šè¿‡å°†é«˜ç»´çŠ¶æ€è½¬ç§»æ¨¡å‹åˆ†è§£ä¸ºå±€éƒ¨ STOKs å’Œç›®æ ‡æ¡ä»¶ç­–ç•¥ï¼Œè¯¥æ–¹æ³•å®ç°äº†åœ¨é«˜ç»´ç›®æ ‡å±‚é¢çš„é«˜æ•ˆå‰å‘è§„åˆ’ã€‚è¿™ç§æ–¹æ³•ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå¿«é€Ÿåˆæˆ Meta-policies å¹¶å¤ç”¨è§„åˆ’è¡¨ç¤ºï¼ŒåŒæ—¶åˆ©ç”¨ Empowerment ä½œä¸ºå†…åœ¨æ¿€åŠ±æ¥è®ºè¯ç›®æ ‡çš„åˆç†æ€§ã€‚è¯¥ç ”ç©¶æŒ‡å‡º OKBEs å…‹æœäº†ä¼ ç»Ÿå¥–åŠ±æœ€å¤§åŒ–åœ¨ç»„åˆæ€§ä¸å¯è§£é‡Šæ€§æ–¹é¢çš„å±€é™ï¼Œä¸ºæ„å»ºå¯éªŒè¯ä¸”å…·å¤‡å†…åœ¨æ¿€åŠ±çš„å¤§è§„æ¨¡é«˜ç»´ä¸–ç•Œæ¨¡å‹æä¾›äº†å¼ºæœ‰åŠ›çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 Pages",
      "pdf_url": "https://arxiv.org/pdf/2506.09499v1",
      "published_date": "2025-06-11 08:21:22 UTC",
      "updated_date": "2025-06-11 08:21:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:38.891533+00:00"
    },
    {
      "arxiv_id": "2506.09498v4",
      "title": "Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning",
      "title_zh": "Fast Monte Carlo Tree Diffusionï¼šé€šè¿‡å¹¶è¡Œç¨€ç–è§„åˆ’å®ç°100å€æé€Ÿ",
      "authors": [
        "Jaesik Yoon",
        "Hyeonseo Cho",
        "Yoshua Bengio",
        "Sungjin Ahn"
      ],
      "abstract": "Diffusion models have recently emerged as a powerful approach for trajectory planning. However, their inherently non-sequential nature limits their effectiveness in long-horizon reasoning tasks at test time. The recently proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by combining diffusion with tree-based search, achieving state-of-the-art performance on complex planning problems. Despite its strengths, our analysis shows that MCTD incurs substantial computational overhead due to the sequential nature of tree search and the cost of iterative denoising. To address this, we propose Fast-MCTD, a more efficient variant that preserves the strengths of MCTD while significantly improving its speed and scalability. Fast-MCTD integrates two techniques: Parallel MCTD, which enables parallel rollouts via delayed tree updates and redundancy-aware selection; and Sparse MCTD, which reduces rollout length through trajectory coarsening. Experiments show that Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or improving planning performance. Remarkably, it even outperforms Diffuser in inference speed on some tasks, despite Diffuser requiring no search and yielding weaker solutions. These results position Fast-MCTD as a practical and scalable solution for diffusion-based inference-time reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fast-MCTDï¼Œæ—¨åœ¨è§£å†³ Monte Carlo Tree Diffusion (MCTD) åœ¨å¤„ç†é•¿ç¨‹è§„åˆ’ä»»åŠ¡æ—¶ç”±äºæ ‘æœç´¢çš„é¡ºåºæ€§ä»¥åŠè¿­ä»£å»å™ªæˆæœ¬å¯¼è‡´çš„å·¨å¤§è®¡ç®—å¼€é”€ã€‚Fast-MCTD é›†æˆäº† Parallel MCTD å’Œ Sparse MCTD ä¸¤é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼Œåˆ†åˆ«é€šè¿‡å»¶è¿Ÿæ ‘æ›´æ–°ä¸å†—ä½™æ„ŸçŸ¥é€‰æ‹©å®ç°å¹¶è¡Œ Rolloutsï¼Œä»¥åŠé€šè¿‡è½¨è¿¹ç²—åŒ– (Trajectory Coarsening) æ˜¾è‘—ç¼©çŸ­ Rollout é•¿åº¦ã€‚å®éªŒè¯æ˜ï¼ŒFast-MCTD åœ¨ä¿æŒç”šè‡³æå‡è§„åˆ’æ€§èƒ½çš„å‰æä¸‹ï¼Œè¾ƒæ ‡å‡† MCTD å®ç°äº†é«˜è¾¾ 100 å€çš„åŠ é€Ÿã€‚è¯¥æ–¹æ³•åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­çš„æ¨ç†é€Ÿåº¦ç”šè‡³ä¼˜äºæ— éœ€æœç´¢çš„ Diffuserï¼ŒåŒæ—¶æä¾›äº†æ›´é«˜è´¨é‡çš„è·¯å¾„è§£ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†ä¾§æ¨ç† (Inference-time Reasoning) ä¸­çš„åº”ç”¨æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 6 figures, NeurIPS 25 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2506.09498v4",
      "published_date": "2025-06-11 08:17:40 UTC",
      "updated_date": "2025-10-24 09:38:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:48.688247+00:00"
    },
    {
      "arxiv_id": "2506.09496v1",
      "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization",
      "title_zh": "EnerBridge-DPOï¼šåŸºäºé©¬å°”å¯å¤«æ¡¥ä¸ç›´æ¥åå¥½ä¼˜åŒ–çš„èƒ½é‡å¼•å¯¼è›‹ç™½è´¨é€†æŠ˜å ",
      "authors": [
        "Dingyi Rong",
        "Haotian Lu",
        "Wenzhuo Zheng",
        "Fan Zhang",
        "Shuangjia Zheng",
        "Ning Liu"
      ],
      "abstract": "Designing protein sequences with optimal energetic stability is a key challenge in protein inverse folding, as current deep learning methods are primarily trained by maximizing sequence recovery rates, often neglecting the energy of the generated sequences. This work aims to overcome this limitation by developing a model that directly generates low-energy, stable protein sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused on generating low-energy, high-stability protein sequences. Our core innovation lies in: First, integrating Markov Bridges with Direct Preference Optimization (DPO), where energy-based preferences are used to fine-tune the Markov Bridge model. The Markov Bridge initiates optimization from an information-rich prior sequence, providing DPO with a pool of structurally plausible sequence candidates. Second, an explicit energy constraint loss is introduced, which enhances the energy-driven nature of DPO based on prior sequences, enabling the model to effectively learn energy representations from a wealth of prior knowledge and directly predict sequence energy values, thereby capturing quantitative features of the energy landscape. Our evaluations demonstrate that EnerBridge-DPO can design protein complex sequences with lower energy while maintaining sequence recovery rates comparable to state-of-the-art models, and accurately predicts $Î”Î”G$ values between various sequences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EnerBridge-DPOï¼Œä¸€ç§æ—¨åœ¨ç”Ÿæˆä½èƒ½é‡ã€é«˜ç¨³å®šæ€§è›‹ç™½è´¨åºåˆ—çš„æ–°å‹ Protein Inverse Folding æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•è¿‡åº¦å…³æ³¨åºåˆ—æ¢å¤ç‡è€Œå¿½è§†èƒ½é‡ç¨³å®šæ€§çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°† Markov Bridges ä¸ Direct Preference Optimization (DPO) ç›¸ç»“åˆï¼Œåˆ©ç”¨åŸºäºèƒ½é‡çš„åå¥½å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡ Markov Bridges ä»ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒåºåˆ—å¯åŠ¨ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶ä¸º DPO æä¾›äº†å¤§é‡ç»“æ„åˆç†çš„å€™é€‰åºåˆ—ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ˜¾å¼çš„èƒ½é‡çº¦æŸæŸå¤±ï¼Œå¢å¼ºäº† DPO çš„èƒ½é‡é©±åŠ¨ç‰¹æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ èƒ½é‡è¡¨ç¤ºå¹¶ç›´æ¥é¢„æµ‹åºåˆ—èƒ½é‡å€¼ï¼Œä»è€Œæ•æ‰èƒ½é‡æ™¯è§‚çš„å®šé‡ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEnerBridge-DPO åœ¨ä¿æŒä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„åºåˆ—æ¢å¤ç‡çš„åŒæ—¶ï¼Œèƒ½è®¾è®¡å‡ºèƒ½é‡æ›´ä½çš„è›‹ç™½è´¨å¤åˆç‰©åºåˆ—ã€‚è¯¥æ¨¡å‹è¿˜èƒ½å‡†ç¡®é¢„æµ‹ä¸åŒåºåˆ—é—´çš„ $Î”Î”G$ å€¼ï¼Œè¯æ˜äº†å…¶åœ¨è›‹ç™½è´¨è®¾è®¡ä¸ç¨³å®šæ€§è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09496v1",
      "published_date": "2025-06-11 08:12:26 UTC",
      "updated_date": "2025-06-11 08:12:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:50.520833+00:00"
    },
    {
      "arxiv_id": "2506.09487v2",
      "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation",
      "title_zh": "BemaGANv2ï¼šé¢å‘é•¿æ—¶éŸ³é¢‘ç”Ÿæˆçš„ GAN å£°ç å™¨æ•™ç¨‹ä¸å¯¹æ¯”ç»¼è¿°",
      "authors": [
        "Taesoo Park",
        "Mungwi Jeong",
        "Mingyu Park",
        "Narae Kim",
        "Junyoung Kim",
        "Mujung Kim",
        "Jisang Yoo",
        "Hoyun Lee",
        "Sanghoon Kim",
        "Soonchul Kwon"
      ],
      "abstract": "This paper presents a tutorial-style survey and implementation guide of BemaGANv2, an advanced GANbased vocoder designed for high-fidelity and long-term audio generation. Long-term audio generation is critical for applications in Text-to-Music (TTM) and Text-to-Audio (TTA) systems, where maintaining temporal coherence, prosodic consistency, and harmonic structure over extended durations remains a significant challenge. Built upon the original BemaGAN architecture, BemaGANv2 incorporates major architectural innovations by replacing traditional ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition (AMP) module, which internally applies the Snake activation function to better model periodic structures. In the discriminator framework, we integrate the Multi-Envelope Discriminator (MED), a novel architecture we proposed, to extract rich temporal envelope features crucial for periodicity detection. Coupled with the Multi-Resolution Discriminator (MRD), this combination enables more accurate modeling of long-range dependencies in audio. We systematically evaluate various discriminator configurations, including Multi-Scale Discriminator (MSD) + MED, MSD + MRD, and Multi-Period Discriminator (MPD) + MED + MRD, using objective metrics (FrÃ©chet Audio Distance (FAD), Structural Similarity Index (SSIM), Pearson Correlation Coefficient (PCC), Mel-Cepstral Distortion (MCD)) and subjective evaluations (MOS, SMOS). This paper also provides a comprehensive tutorial on the model architecture, training methodology, and implementation to promote reproducibility. The code and pre-trained models are available at: https://github.com/dinhoitt/BemaGANv2.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† BemaGANv2ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºé«˜ä¿çœŸå’Œé•¿æ—¶éŸ³é¢‘ç”Ÿæˆè®¾è®¡çš„ GAN-based vocoderï¼Œæ—¨åœ¨è§£å†³ Text-to-Music (TTM) å’Œ Text-to-Audio (TTA) ç³»ç»Ÿåœ¨ç»´æŒé•¿æ—¶è¿è´¯æ€§ã€éŸµå¾‹ä¸€è‡´æ€§åŠå’Œè°ç»“æ„æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚BemaGANv2 åœ¨ç”Ÿæˆå™¨ä¸­é€šè¿‡ Anti-aliased Multi-Periodicity composition (AMP) æ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿçš„ ResBlocksï¼Œå¹¶ç»“åˆ Snake æ¿€æ´»å‡½æ•°ä»¥æ›´å¥½åœ°å»ºæ¨¡éŸ³é¢‘çš„å‘¨æœŸæ€§ç»“æ„ã€‚åœ¨åˆ¤åˆ«å™¨æ¡†æ¶ä¸­ï¼Œç ”ç©¶è€…å¼•å…¥äº†æ–°é¢–çš„ Multi-Envelope Discriminator (MED) å¹¶ç»“åˆ Multi-Resolution Discriminator (MRD)ï¼Œä»è€Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æå–æ—¶é—´åŒ…ç»œç‰¹å¾å¹¶æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¯¹ FADã€SSIMã€MCD ç­‰å®¢è§‚æŒ‡æ ‡ä»¥åŠ MOS ä¸»è§‚è¯„åˆ†çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå®éªŒéªŒè¯äº†ä¸åŒåˆ¤åˆ«å™¨é…ç½®ä¸‹çš„æ€§èƒ½æå‡ã€‚è¯¥è®ºæ–‡è¿˜æä¾›äº†è¯¦ç»†çš„æ¶æ„æ•™ç¨‹ä¸å¼€æºä»£ç ï¼Œä¸ºé•¿æ—¶éŸ³é¢‘åˆæˆé¢†åŸŸçš„å¯é‡å¤æ€§ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under review at ICT Express as an extended version of our ICAIIC 2025 paper",
      "pdf_url": "https://arxiv.org/pdf/2506.09487v2",
      "published_date": "2025-06-11 07:57:05 UTC",
      "updated_date": "2025-11-22 03:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:05:47.682678+00:00"
    },
    {
      "arxiv_id": "2506.09485v2",
      "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation",
      "title_zh": "Adv-BMTï¼šé¢å‘å®‰å…¨å…³é”®å‹äº¤é€šåœºæ™¯ç”Ÿæˆçš„åŒå‘è¿åŠ¨ Transformer",
      "authors": [
        "Yuxin Liu",
        "Zhenghao Peng",
        "Xuanhao Cui",
        "Bolei Zhou"
      ],
      "abstract": "Scenario-based testing is essential for validating the performance of autonomous driving (AD) systems. However, such testing is limited by the scarcity of long-tailed, safety-critical scenarios in existing datasets collected in the real world. To tackle the data issue, we propose the Adv-BMT framework, which augments real-world scenarios with diverse and realistic adversarial traffic interactions. The core component of Adv-BMT is a bidirectional motion transformer (BMT) model to perform inverse traffic motion predictions, which takes agent information in the last time step of the scenario as input, and reconstructs the traffic in the inverse of chronological order until the initial time step. The Adv-BMT framework is a two-staged pipeline: it first conducts adversarial initializations and then inverse motion predictions. Different from previous work, we do not need any collision data for pretraining, and are able to generate realistic and diverse collision interactions. Our experimental results validate the quality of generated collision scenarios by Adv-BMT: training in our augmented dataset would reduce episode collision rates by 20%. Demo and code are available at: https://metadriverse.github.io/adv-bmt/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Adv-BMT æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ (Autonomous Driving) ç³»ç»ŸéªŒè¯ä¸­é¢ä¸´çš„å®‰å…¨å…³é”® (safety-critical) åœºæ™¯ç¨€ç¼ºé—®é¢˜ã€‚æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶æ˜¯åŒå‘è¿åŠ¨å˜æ¢å™¨ (Bidirectional Motion Transformer, BMT)ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨é€†å‘äº¤é€šè¿åŠ¨é¢„æµ‹ (inverse traffic motion predictions) æŠ€æœ¯ï¼Œä»åœºæ™¯çš„æœ€ç»ˆçŠ¶æ€åå‘é‡æ„è‡³åˆå§‹æ—¶é—´æ­¥ã€‚Adv-BMT é‡‡ç”¨å…ˆè¿›è¡Œå¯¹æŠ—æ€§åˆå§‹åŒ– (adversarial initializations) åè¿›è¡Œé€†å‘è¿åŠ¨é¢„æµ‹çš„ä¸¤é˜¶æ®µæµç¨‹ï¼Œä¸”æ— éœ€ä»»ä½•ç¢°æ’æ•°æ®è¿›è¡Œé¢„è®­ç»ƒå³å¯ç”ŸæˆçœŸå®ä¸”å¤šæ ·åŒ–çš„ç¢°æ’äº¤äº’åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ Adv-BMT ç”Ÿæˆçš„å¢å¼ºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ä½¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç‰‡æ®µç¢°æ’ç‡ (episode collision rates) é™ä½ 20%ï¼Œæœ‰æ•ˆæå‡äº†ç³»ç»Ÿå¤„ç†é•¿å°¾é£é™©åœºæ™¯çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09485v2",
      "published_date": "2025-06-11 07:54:50 UTC",
      "updated_date": "2025-11-04 02:15:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:20.773985+00:00"
    },
    {
      "arxiv_id": "2506.21803v1",
      "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining",
      "title_zh": "ä»æ ‡è®°åˆ°èŠ‚å¾‹ï¼šä¸€ç§å¤šå°ºåº¦å¿ƒç”µå›¾-è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•",
      "authors": [
        "Fuying Wang",
        "Jiacheng Xu",
        "Lequan Yu"
      ],
      "abstract": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and diagnosing heart diseases. However, traditional deep learning approaches for ECG analysis rely heavily on large-scale manual annotations, which are both time-consuming and resource-intensive to obtain. To overcome this limitation, self-supervised learning (SSL) has emerged as a promising alternative, enabling the extraction of robust ECG representations that can be efficiently transferred to various downstream tasks. While previous studies have explored SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail to capture the multi-scale nature of ECG signals. As a result, these methods struggle to learn generalized representations due to their inability to model the hierarchical structure of ECG data. To address this gap, we introduce MELP, a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages hierarchical supervision from ECG-text pairs. MELP first pretrains a cardiology-specific language model to enhance its understanding of clinical text. It then applies three levels of cross-modal supervision-at the token, beat, and rhythm levels-to align ECG signals with textual reports, capturing structured information across different time scales. We evaluate MELP on three public ECG datasets across multiple tasks, including zero-shot ECG classification, linear probing, and transfer learning. Experimental results demonstrate that MELP outperforms existing SSL methods, underscoring its effectiveness and adaptability across diverse clinical applications. Our code is available at https://github.com/HKU-MedAI/MELP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MELPï¼Œä¸€ç§æ–°å‹çš„å¤šå°ºåº¦å¿ƒç”µå›¾-è¯­è¨€é¢„è®­ç»ƒ(Multi-scale ECG-Language Pretraining)æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªç›‘ç£å­¦ä¹ (SSL)æ–¹æ³•æ— æ³•æ•æ‰ECGä¿¡å·å¤šå°ºåº¦ç‰¹æ€§å’Œå±‚çº§ç»“æ„çš„é—®é¢˜ã€‚MELPé€šè¿‡é¢„è®­ç»ƒå¿ƒè„ç—…å­¦ä¸“ç”¨çš„è¯­è¨€æ¨¡å‹æ¥å¢å¼ºå¯¹ä¸´åºŠæ–‡æœ¬çš„ç†è§£ï¼Œå¹¶å¼•å…¥äº†tokenã€beatå’Œrhythmä¸‰ä¸ªå±‚çº§çš„è·¨æ¨¡æ€ç›‘ç£ï¼Œä»è€Œå®ç°ECGä¿¡å·ä¸æ–‡æœ¬æŠ¥å‘Šåœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„ç²¾ç¡®å¯¹é½ã€‚å®éªŒåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†zero-shot ECGåˆ†ç±»ã€linear probingå’Œtransfer learningç­‰ä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤ºMELPçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„SSLåŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ³•è¯æ˜äº†åˆ©ç”¨å±‚çº§ç›‘ç£å­¦ä¹ æ³›åŒ–ECGè¡¨å¾çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†å¤æ‚çš„ç”Ÿç†ä¿¡å·ä¸è‡ªç„¶è¯­è¨€å¯¹é½ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21803v1",
      "published_date": "2025-06-11 07:22:17 UTC",
      "updated_date": "2025-06-11 07:22:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:03.373129+00:00"
    },
    {
      "arxiv_id": "2506.09455v1",
      "title": "Abstraction-Based Proof Production in Formal Verification of Neural Networks",
      "title_zh": "ç¥ç»ç½‘ç»œå½¢å¼åŒ–éªŒè¯ä¸­åŸºäºæŠ½è±¡çš„è¯æ˜ç”Ÿæˆ",
      "authors": [
        "Yizhak Yisrael Elboher",
        "Omri Isac",
        "Guy Katz",
        "Tobias Ladner",
        "Haoze Wu"
      ],
      "abstract": "Modern verification tools for deep neural networks (DNNs) increasingly rely on abstraction to scale to realistic architectures. In parallel, proof production is becoming a critical requirement for increasing the reliability of DNN verification results. However, current proofproducing verifiers do not support abstraction-based reasoning, creating a gap between scalability and provable guarantees. We address this gap by introducing a novel framework for proof-producing abstraction-based DNN verification. Our approach modularly separates the verification task into two components: (i) proving the correctness of an abstract network, and (ii) proving the soundness of the abstraction with respect to the original DNN. The former can be handled by existing proof-producing verifiers, whereas we propose the first method for generating formal proofs for the latter. This preliminary work aims to enable scalable and trustworthy verification by supporting common abstraction techniques within a formal proof framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)å½¢å¼åŒ–éªŒè¯ä¸­å¯æ‰©å±•æ€§ä¸å¯é æ€§ä¹‹é—´çš„æ–­å±‚ï¼Œæå‡ºäº†ä¸€ç§æ”¯æŒåŸºäºæŠ½è±¡(abstraction-based)æ¨ç†çš„è¯æ˜ç”Ÿæˆ(proof production)æ¡†æ¶ã€‚å½“å‰éªŒè¯å·¥å…·è™½ä¾èµ–æŠ½è±¡æŠ€æœ¯å®ç°è§„æ¨¡åŒ–ï¼Œä½†å¾€å¾€æ— æ³•æä¾›å½¢å¼åŒ–è¯æ˜ï¼Œå¯¼è‡´ç»“æœçš„å¯ä¿¡åº¦å—é™ã€‚è¯¥æ¡†æ¶å°†éªŒè¯ä»»åŠ¡æ¨¡å—åŒ–åœ°æ‹†åˆ†ä¸ºè¯æ˜æŠ½è±¡ç½‘ç»œçš„æ­£ç¡®æ€§ä»¥åŠè¯æ˜æŠ½è±¡è¿‡ç¨‹ç›¸å¯¹äºåŸå§‹DNNçš„å¯é æ€§(soundness)ä¸¤ä¸ªéƒ¨åˆ†ã€‚ä½œè€…ä¸ºåè€…å¼€å‘äº†é¦–ä¸ªç”Ÿæˆå½¢å¼åŒ–è¯æ˜çš„æ–¹æ³•ï¼Œè€Œå‰è€…åˆ™å¯ç”±ç°æœ‰çš„è¯æ˜ç”ŸæˆéªŒè¯å™¨å¤„ç†ã€‚è¯¥ç ”ç©¶é€šè¿‡åœ¨å½¢å¼åŒ–è¯æ˜æ¡†æ¶ä¸­æ”¯æŒå¸¸è§çš„æŠ½è±¡æŠ€æœ¯ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†éªŒè¯å·¥å…·åœ¨æ‰©å±•æ€§ä¸å¯ä¿¡ä¿è¯ä¹‹é—´çš„å·®è·ï¼Œä¸ºå®ç°å¤§è§„æ¨¡ä¸”é«˜å¯ä¿¡çš„ç¥ç»ç½‘ç»œéªŒè¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "To appear in SAIV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09455v1",
      "published_date": "2025-06-11 07:00:09 UTC",
      "updated_date": "2025-06-11 07:00:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:35.835223+00:00"
    },
    {
      "arxiv_id": "2506.09450v1",
      "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs",
      "title_zh": "UniToMBenchï¼šé›†æˆè§‚ç‚¹é‡‡æ‹©ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å¿ƒæ™ºç†è®ºèƒ½åŠ›",
      "authors": [
        "Prameshwar Thiyagarajan",
        "Vaishnavi Parimi",
        "Shamant Sai",
        "Soumil Garg",
        "Zhangir Meirbek",
        "Nitin Yarlagadda",
        "Kevin Zhu",
        "Chris Kim"
      ],
      "abstract": "Theory of Mind (ToM), the ability to understand the mental states of oneself and others, remains a challenging area for large language models (LLMs), which often fail to predict human mental states accurately. In this paper, we introduce UniToMBench, a unified benchmark that integrates the strengths of SimToM and TOMBENCH to systematically improve and assess ToM capabilities in LLMs by integrating multi-interaction task designs and evolving story scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios, UniToMBench combines perspective-taking techniques with diverse evaluation metrics to better stimulate social cognition in LLMs. Through evaluation, we observe that while models like GPT-4o and GPT-4o Mini show consistently high accuracy in tasks involving emotional and belief-related scenarios, with results usually above 80%, there is significant variability in their performance across knowledge-based tasks. These results highlight both the strengths and limitations of current LLMs in ToM-related tasks, underscoring the value of UniToMBench as a comprehensive tool for future development. Our code is publicly available here: https://github.com/Shamant/unifiedtombenchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniToMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡å’Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¿ƒæ™ºç†è®º (Theory of Mind, ToM) èƒ½åŠ›çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•ã€‚UniToMBench æ•´åˆäº† SimToM å’Œ TOMBENCH çš„ä¼˜åŠ¿ï¼Œé€šè¿‡ 1,000 å¤šä¸ªæ‰‹å·¥ç¼–å†™çš„æƒ…å¢ƒã€å¤šäº¤äº’ä»»åŠ¡è®¾è®¡ä»¥åŠæ¼”è¿›çš„æ•…äº‹æƒ…å¢ƒï¼Œç³»ç»Ÿåœ°è€ƒå¯Ÿæ¨¡å‹é¢„æµ‹äººç±»å¿ƒç†çŠ¶æ€çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†è§’é‡‡å– (Perspective-taking) æŠ€æœ¯ä¸å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ¿€å‘ LLMs çš„ç¤¾äº¤è®¤çŸ¥ã€‚å®éªŒè¯„ä¼°å‘ç°ï¼Œè™½ç„¶ GPT-4o å’Œ GPT-4o Mini åœ¨æƒ…ç»ªå’Œä¿¡å¿µç›¸å…³çš„ä»»åŠ¡ä¸­é€šå¸¸èƒ½è¾¾åˆ° 80% ä»¥ä¸Šçš„é«˜å‡†ç¡®ç‡ï¼Œä½†åœ¨åŸºäºçŸ¥è¯†çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„æ³¢åŠ¨æ€§ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å½“å‰ä¸»æµæ¨¡å‹åœ¨ ToM ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œå‡¸æ˜¾äº† UniToMBench ä½œä¸ºæ¨åŠ¨è¯¥é¢†åŸŸå‘å±•çš„ç»¼åˆæ€§è¯„ä¼°å·¥å…·çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Conference of the North American Chapter of the Association for Computational Linguistics, Student Research Workshop 2025 (NAACL SRW 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.09450v1",
      "published_date": "2025-06-11 06:55:40 UTC",
      "updated_date": "2025-06-11 06:55:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:36.273198+00:00"
    },
    {
      "arxiv_id": "2506.09445v1",
      "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision",
      "title_zh": "TOGAï¼šå¼±ç›‘ç£ä¸‹çš„æ—¶é—´å®šä½å¼€æ”¾å¼è§†é¢‘é—®ç­”",
      "authors": [
        "Ayush Gupta",
        "Anirban Roy",
        "Rama Chellappa",
        "Nathaniel D. Bastian",
        "Alvaro Velasquez",
        "Susmit Jha"
      ],
      "abstract": "We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TOGAï¼Œä¸€ç§ç”¨äºå¼±ç›‘ç£(Weak Supervision)ä¸‹å…·æœ‰æ—¶é—´å®šä½åŠŸèƒ½çš„å¼€æ”¾å¼è§†é¢‘é—®ç­”(Video QA)è§†è§‰è¯­è¨€æ¨¡å‹ã€‚é’ˆå¯¹åœ¨ç¼ºä¹æ—¶é—´æ ‡æ³¨(Temporal Annotations)çš„æƒ…å†µä¸‹å®ç°æ—¶é—´å®šä½è¿™ä¸€éš¾é¢˜ï¼ŒTOGAé€šè¿‡æŒ‡ä»¤å¾®è°ƒ(Instruct-tune)å®ç°äº†ç­”æ¡ˆç”Ÿæˆä¸æ—¶é—´å®šä½çš„è”åˆè¾“å‡ºã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¼ªæ ‡ç­¾(Pseudo Labels)æŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†ä¸€è‡´æ€§çº¦æŸ(Consistency Constraint)æ¥ç¡®ä¿å®šä½æ ‡ç­¾çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°†ç­”æ¡ˆç”Ÿæˆä¸æ—¶é—´å®šä½ä»»åŠ¡ç»“åˆèƒ½å¤ŸåŒæ—¶æå‡é—®ç­”ä¸å®šä½ä¸¤æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTOGAåœ¨NExT-GQAã€MSVD-QAå’ŒActivityNet-QAç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09445v1",
      "published_date": "2025-06-11 06:52:31 UTC",
      "updated_date": "2025-06-11 06:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:38.431357+00:00"
    },
    {
      "arxiv_id": "2506.09440v1",
      "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture",
      "title_zh": "GigaChat ç³»åˆ—ï¼šåŸºäºæ··åˆä¸“å®¶æ¶æ„çš„é«˜æ•ˆä¿„è¯­è¯­è¨€å»ºæ¨¡",
      "authors": [
        "GigaChat team",
        "Mamedov Valentin",
        "Evgenii Kosarev",
        "Gregory Leleytner",
        "Ilya Shchuckin",
        "Valeriy Berezovskiy",
        "Daniil Smirnov",
        "Dmitry Kozlov",
        "Sergei Averkiev",
        "Lukyanenko Ivan",
        "Aleksandr Proshunin",
        "Ainur Israfilova",
        "Ivan Baskov",
        "Artem Chervyakov",
        "Emil Shakirov",
        "Mikhail Kolesov",
        "Daria Khomich",
        "Darya Latortseva",
        "Sergei Porkhun",
        "Yury Fedorov",
        "Oleg Kutuzov",
        "Polina Kudriavtseva",
        "Sofiia Soldatova",
        "Kolodin Egor",
        "Stanislav Pyatkin",
        "Dzmitry Menshykh",
        "Grafov Sergei",
        "Eldar Damirov",
        "Karlov Vladimir",
        "Ruslan Gaitukiev",
        "Arkadiy Shatenov",
        "Alena Fenogenova",
        "Nikita Savushkin",
        "Fedor Minkin"
      ],
      "abstract": "Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (https://huggingface.co/ai-sage), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† GigaChat ç³»åˆ—ä¿„ç½—æ–¯è¯­å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œæ—¨åœ¨è§£å†³ä¿„è¯­åŸºç¡€æ¨¡å‹å¼€å‘ä¸­é¢ä¸´çš„é«˜è®¡ç®—èµ„æºæŒ‘æˆ˜ã€‚è¯¥ç³»åˆ—æ¨¡å‹é€šè¿‡ä¸“å®¶æ··åˆæ¶æ„ (Mixture of Experts) å®ç°é«˜æ•ˆå»ºæ¨¡ï¼Œæ¶µç›–äº†ä»åŸºç¡€æ¨¡å‹åˆ°æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬çš„å¤šç§è§„æ¨¡ã€‚è®ºæ–‡è¯¦ç»†æŠ¥å‘Šäº†å…¶æ¨¡å‹æ¶æ„ã€é¢„è®­ç»ƒè¿‡ç¨‹åŠå®éªŒè®¾è®¡ï¼Œå¹¶åœ¨ä¿„è¯­å’Œè‹±è¯­åŸºå‡†æµ‹è¯•ä¸­é€šè¿‡ä¸å¤šè¯­è¨€æ¨¡å‹çš„å¯¹æ¯”éªŒè¯äº†å…¶æ€§èƒ½ã€‚ç›®å‰ï¼Œå›¢é˜Ÿå·²åœ¨å¼€æºç¤¾åŒºå‘å¸ƒäº†ä¸‰ä¸ª GigaChat æ¨¡å‹ï¼Œå¹¶æä¾› API å’Œ Web ç•Œé¢ç­‰å¤šç§è®¿é—®æ¸ é“ã€‚è¿™ä¸€æˆæœæ˜¾è‘—æ‰©å±•äº†ä¿„è¯­è‡ªç„¶è¯­è¨€å¤„ç† (NLP) çš„ç ”ç©¶æœºä¼šï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸçš„å·¥ä¸šåŒ–è§£å†³æ–¹æ¡ˆæä¾›äº†å¼ºåŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL-2025 System Demo",
      "pdf_url": "https://arxiv.org/pdf/2506.09440v1",
      "published_date": "2025-06-11 06:46:49 UTC",
      "updated_date": "2025-06-11 06:46:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:40.884060+00:00"
    },
    {
      "arxiv_id": "2506.13983v1",
      "title": "SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine",
      "title_zh": "SANGAMï¼šåŸºäºè’™ç‰¹å¡æ´›æ ‘è‡ªæˆ‘ä¼˜åŒ–çš„ SystemVerilog æ–­è¨€ç”Ÿæˆ",
      "authors": [
        "Adarsh Gupta",
        "Bhabesh Mali",
        "Chandan Karfa"
      ],
      "abstract": "Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SANGAMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) æŒ‡å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ (Monte Carlo Tree Search) æ¡†æ¶ï¼Œæ—¨åœ¨ä»å·¥ä¸šçº§è§„èŒƒä¸­è‡ªåŠ¨ç”Ÿæˆ SystemVerilog Assertions (SVAs)ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡åŒ…å« Signal Mapperã€SPEC Analyzer å’Œ Waveform Analyzer çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“è¿›è¡Œè§„èŒƒå¤„ç†ã€‚éšåï¼Œåˆ©ç”¨ Monte Carlo Tree Self-Refine (MCTSr) ç®—æ³•å¯¹æ¯ä¸ªä¿¡å·çš„ SVA è¿›è¡Œè‡ªåŠ¨æ¨ç†ã€‚æœ€åï¼Œç³»ç»Ÿæ•´åˆ MCTSr ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ï¼Œä¸ºå„ä¸ªä¿¡å·ç”Ÿæˆæœ€ç»ˆçš„ SVA æ–­è¨€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSANGAM èƒ½å¤Ÿç”Ÿæˆé²æ£’çš„ SVA é›†åˆï¼Œåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºå¤æ‚ç¡¬ä»¶è®¾è®¡çš„è‡ªåŠ¨éªŒè¯æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Adarsh Gupta and Bhabesh Mali contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2506.13983v1",
      "published_date": "2025-06-11 06:43:24 UTC",
      "updated_date": "2025-06-11 06:43:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:48.529980+00:00"
    },
    {
      "arxiv_id": "2506.09434v3",
      "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?",
      "title_zh": "åˆä½œå¼å¤šæ™ºèƒ½ä½“å­¦ä¹ ä¸­ï¼Œå¤šæ ·æ€§ä½•æ—¶èƒ½è·å¾—å¥–åŠ±ï¼Ÿ",
      "authors": [
        "Michael Amir",
        "Matteo Bettini",
        "Amanda Prorok"
      ],
      "abstract": "The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åä½œå¤šæ™ºèƒ½ä½“å­¦ä¹ ä¸­ï¼Œå¤šæ ·æ€§(Diversity)ä½•æ—¶ä¼˜äºåŒè´¨åŒ–å›¢é˜Ÿè¿™ä¸€æ ¸å¿ƒé—®é¢˜ã€‚ç ”ç©¶è€…é¦–å…ˆä»å¥–åŠ±è®¾è®¡(Reward Design)çš„è§’åº¦ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå†…éƒ¨å’Œå¤–éƒ¨èšåˆç®—å­(Aggregation Operators)çš„ç¬æ—¶ä»»åŠ¡åˆ†é…æ¨¡å‹ï¼Œå¹¶è¯æ˜äº†è¿™äº›ç®—å­çš„æ›²ç‡(Curvature)æ˜¯å†³å®šå¼‚è´¨æ€§èƒ½å¦æé«˜å¥–åŠ±çš„å…³é”®å› ç´ ã€‚åœ¨å¹¿æ³›çš„å¥–åŠ±æ—ä¸­ï¼Œè¿™ä¸€ç»“è®ºå¯ç®€åŒ–ä¸ºç®€å•çš„å‡¸æ€§æµ‹è¯•(Convexity Test)ã€‚é’ˆå¯¹å…·èº«ä¸”æœ‰æ—¶å»¶çš„æ™ºèƒ½ä½“ï¼Œç ”ç©¶å¼•å…¥äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)èŒƒå¼ä¸‹çš„å¼‚è´¨æ€§å¢ç›Šå‚æ•°æœç´¢ç®—æ³•(Heterogeneity Gain Parameter Search, HetGPS)ï¼Œæ—¨åœ¨ä¼˜åŒ–ç¯å¢ƒå‚æ•°ä»¥å‘ç°å¼‚è´¨æ€§å…·æœ‰ä¼˜åŠ¿çš„åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHetGPS æˆåŠŸé‡ç°äº†ç†è®ºé¢„æµ‹çš„å¥–åŠ±æœºåˆ¶ï¼ŒéªŒè¯äº†ç†è®ºä¸è®¡ç®—æ¨¡å‹çš„ä¸€è‡´æ€§ã€‚è¯¥æˆæœä¸ºç†è§£è¡Œä¸ºå¤šæ ·æ€§åœ¨ä½•ç§æ¡ä»¶ä¸‹èƒ½å¸¦æ¥å¯è¡¡é‡çš„æ”¶ç›Šæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09434v3",
      "published_date": "2025-06-11 06:33:55 UTC",
      "updated_date": "2025-11-04 09:17:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:49.109983+00:00"
    },
    {
      "arxiv_id": "2506.09428v2",
      "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting",
      "title_zh": "ç¼“è§£å¤§è¯­è¨€æ¨¡å‹ç¾éš¾æ€§é—å¿˜çš„æ”¹è¿›æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•",
      "authors": [
        "Fei Ding",
        "Baiqiao Wang"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) is a critical step for enhancing the instruction-following capabilities of Large Language Models (LLMs) and adapting them to specialized domains. However, SFT often leads to a degradation of the model's general abilities, a phenomenon known as catastrophic forgetting. This problem is exacerbated when third-party practitioners fine-tune open-source models, as the original SFT data is typically not available. To address this challenge, we propose a novel and cost-effective SFT method that effectively mitigates catastrophic forgetting without requiring access to the original SFT data. Our approach first reconstructs the likely instruction distribution of the base model. It then employs a multi-model generation and filtering pipeline to synthesize a high-quality general-purpose dataset. This synthetic dataset is mixed with new, domain-specific data for fine-tuning. Experimental results show that our method not only preserves the model's capabilities in general domains but also improves task-specific performance, outperforming baselines that use publicly available SFT datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models) åœ¨è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT) æ—¶å‡ºç°çš„ç¾éš¾æ€§é—å¿˜ (Catastrophic Forgetting) ç°è±¡ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€åŸå§‹æ•°æ®æ”¯æŒçš„æ–°å‹ SFT æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡é‡æ„åŸºç¡€æ¨¡å‹çš„æŒ‡ä»¤åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨å¤šæ¨¡å‹ç”Ÿæˆä¸è¿‡æ»¤æµæ°´çº¿æ¥åˆæˆé«˜è´¨é‡çš„é€šç”¨æ•°æ®é›†ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œé€šè¿‡å°†è¿™äº›åˆæˆæ•°æ®ä¸æ–°çš„é¢†åŸŸç‰¹å®šæ•°æ®è¿›è¡Œæ··åˆï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹çš„ä¸“ä¸šåŒ–é€‚é…ä¸é€šç”¨èƒ½åŠ›ä¿ç•™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆä¸ä»…èƒ½å¤Ÿé˜²æ­¢é€šç”¨æ€§èƒ½é€€åŒ–ï¼Œè¿˜æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ä½¿ç”¨å…¬å¼€ SFT æ•°æ®é›†çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¸ºå¼€å‘è€…åœ¨ç¼ºä¹åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨¡å‹æä¾›äº†é«˜æ•ˆä¸”ç»æµçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09428v2",
      "published_date": "2025-06-11 06:23:50 UTC",
      "updated_date": "2025-06-28 02:26:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:06:55.952072+00:00"
    },
    {
      "arxiv_id": "2506.09427v1",
      "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation",
      "title_zh": "é¢å‘äº¤é”™å¼å›¾æ–‡ç”Ÿæˆçš„é«˜è´¨é‡æ•°æ®é›†ä¸å¯é è¯„ä¼°",
      "authors": [
        "Yukang Feng",
        "Jianwen Sun",
        "Chuanhao Li",
        "Zizhen Li",
        "Jiaxin Ai",
        "Fanrui Zhang",
        "Yifan Chang",
        "Sizhuo Zhou",
        "Shenglin Zhang",
        "Yu Dai",
        "Kaipeng Zhang"
      ],
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹(LMMs)åœ¨ç”Ÿæˆäº¤é”™å›¾æ–‡(interleaved image-text)è¾“å‡ºæ—¶é¢ä¸´çš„æ•°æ®è§„æ¨¡ä¸æŒ‡ä»¤ä¸°å¯Œåº¦ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæ¨å‡ºäº†å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†InterSynã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†SEIR (Self-Evaluation with Iterative Refinement) æ–¹æ³•ï¼Œé€šè¿‡å¤šè½®æŒ‡ä»¤é©±åŠ¨å¯¹è¯å’Œè¿­ä»£ç²¾ç‚¼æŠ€æœ¯ï¼Œç¡®ä¿äº†æ•°æ®é›†çš„é«˜è´¨é‡ä¸å¯¹è±¡å¤šæ ·æ€§ã€‚ä¸ºäº†å¼¥è¡¥å¯é è¯„ä¼°å·¥å…·çš„ç©ºç™½ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†SynJudgeè‡ªåŠ¨è¯„ä¼°æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬å†…å®¹ã€å›¾åƒå†…å®¹ã€å›¾åƒè´¨é‡å’Œå›¾æ–‡ååŒ(image-text synergy)å››ä¸ªç»´åº¦å¯¹å¤šæ¨¡æ€è¾“å‡ºè¿›è¡Œå®šé‡è¡¡é‡ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒSEIRæ–¹æ³•èƒ½æ˜¾è‘—ä¼˜åŒ–æ•°æ®è´¨é‡ï¼Œè€Œåœ¨InterSynä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½å¢ç›Šï¼Œæœ‰æ•ˆæ¨åŠ¨äº†ä¸‹ä¸€ä»£æŒ‡ä»¤éµå¾ªLMMsçš„å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09427v1",
      "published_date": "2025-06-11 06:21:20 UTC",
      "updated_date": "2025-06-11 06:21:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:34.139010+00:00"
    },
    {
      "arxiv_id": "2506.09420v1",
      "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy",
      "title_zh": "å‘¼ååä½œæ™ºèƒ½ï¼šä¸ºä½•äººæœºåä½œç³»ç»Ÿåº”ä¼˜å…ˆäºäººå·¥æ™ºèƒ½è‡ªä¸»",
      "authors": [
        "Henry Peng Zou",
        "Wei-Chieh Huang",
        "Yaozu Wu",
        "Chunyu Miao",
        "Dongyuan Li",
        "Aiwei Liu",
        "Yue Zhou",
        "Yankai Chen",
        "Weizhi Zhang",
        "Yangning Li",
        "Liancheng Fang",
        "Renhe Jiang",
        "Philip S. Yu"
      ],
      "abstract": "Recent improvements in large language models (LLMs) have led many researchers to focus on building fully autonomous AI agents. This position paper questions whether this approach is the right path forward, as these autonomous systems still have problems with reliability, transparency, and understanding the actual requirements of human. We suggest a different approach: LLM-based Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing them. By keeping human involved to provide guidance, answer questions, and maintain control, these systems can be more trustworthy and adaptable. Looking at examples from healthcare, finance, and software development, we show how human-AI teamwork can handle complex tasks better than AI working alone. We also discuss the challenges of building these collaborative systems and offer practical solutions. This paper argues that progress in AI should not be measured by how independent systems become, but by how well they can work with humans. The most promising future for AI is not in systems that take over human roles, but in those that enhance human capabilities through meaningful partnership.",
      "tldr_zh": "é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿½æ±‚å®Œå…¨è‡ªä¸»åŒ–è·¯å¾„ä¸­é¢ä¸´çš„å¯é æ€§ã€é€æ˜åº¦åŠäººç±»éœ€æ±‚å¯¹é½ç­‰å±€é™æ€§ï¼Œæœ¬æ–‡å¯¹å½“å‰çš„AIè‡ªä¸»åŒ–è¶‹åŠ¿æå‡ºäº†è´¨ç–‘ã€‚è¯¥ç«‹åœºè®ºæ–‡å€¡å¯¼å°†ç ”å‘é‡ç‚¹è½¬å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„äººæœºç³»ç»Ÿ(LLM-based Human-Agent Systems, LLM-HAS)ï¼Œå¼ºè°ƒAIåº”ä»¥åä½œè€Œéæ›¿ä»£çš„æ–¹å¼ä¸äººç±»å…±äº‹ã€‚é€šè¿‡ä¿æŒäººç±»åœ¨ç¯(Human-in-the-loop)ä»¥æä¾›æŒ‡å¯¼ã€è§£ç­”ç–‘é—®å¹¶ç»´æŒæœ€ç»ˆæ§åˆ¶æƒï¼Œè¿™äº›ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å¯ä¿¡åº¦ä¸é€‚åº”æ€§ã€‚é€šè¿‡åˆ†æåŒ»ç–—ã€é‡‘èå’Œè½¯ä»¶å¼€å‘é¢†åŸŸçš„æ¡ˆä¾‹ï¼Œç ”ç©¶è¯æ˜äº†äººæœºå›¢é˜Ÿåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ä¼˜äºå•ä¸€çš„AIè‡ªä¸»ç³»ç»Ÿã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†æ„å»ºåä½œç³»ç»Ÿæ‰€é¢ä¸´çš„æŒ‘æˆ˜å¹¶æä¾›äº†å®è·µè§£å†³æ–¹æ¡ˆã€‚æ–‡ç« æœ€åæŒ‡å‡ºï¼ŒAIçš„è¿›æ­¥ä¸åº”ç”±ç³»ç»Ÿçš„ç‹¬ç«‹ç¨‹åº¦æ¥è¡¡é‡ï¼Œè€Œåº”å–å†³äºå…¶å¢å¼ºäººç±»èƒ½åŠ›å¹¶ä¸ä¹‹å»ºç«‹æ·±å±‚ä¼™ä¼´å…³ç³»çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09420v1",
      "published_date": "2025-06-11 06:08:13 UTC",
      "updated_date": "2025-06-11 06:08:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:00.555223+00:00"
    },
    {
      "arxiv_id": "2506.09411v1",
      "title": "Synthetic Human Action Video Data Generation with Pose Transfer",
      "title_zh": "åŸºäºå§¿æ€è¿ç§»çš„åˆæˆäººä½“åŠ¨ä½œè§†é¢‘æ•°æ®ç”Ÿæˆ",
      "authors": [
        "Vaclav Knapp",
        "Matyas Bohacek"
      ],
      "abstract": "In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç†è§£ä»»åŠ¡ä¸­åˆæˆæ•°æ®å¸¸å‡ºç°çš„â€œææ€–è°·â€æ•ˆåº”ï¼ˆuncanny featuresï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Pose Transferï¼ˆå§¿æ€è¿ç§»ï¼‰çš„äººä½“åŠ¨ä½œè§†é¢‘æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å…·ä½“é‡‡ç”¨äº†å¯æ§çš„ 3D Gaussian avatar æ¨¡å‹ï¼Œå¹¶åœ¨ Toyota Smarthome å’Œ NTU RGB+D æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯èƒ½æ˜¾è‘—æå‡ Action Recognitionï¼ˆåŠ¨ä½œè¯†åˆ«ï¼‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰©å…… Few-shotï¼ˆå°‘æ ·æœ¬ï¼‰æ•°æ®é›†ï¼Œè§£å†³çœŸå®æ•°æ®ä¸­ç‰¹å®šç¾¤ä½“ä»£è¡¨æ€§ä¸è¶³åŠèƒŒæ™¯å•ä¸€çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¼€æºäº†ç”Ÿæˆæ–¹æ³•ä»¥åŠåŒ…å«å¤šæ ·åŒ–äººä½“èº«ä»½çš„ RANDOM People æ•°æ®é›†ï¼Œä¸ºäººä½“åŠ¨ä½œç†è§£é¢†åŸŸæä¾›äº†é«˜è´¨é‡çš„åˆæˆèµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09411v1",
      "published_date": "2025-06-11 05:52:39 UTC",
      "updated_date": "2025-06-11 05:52:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:00.407770+00:00"
    },
    {
      "arxiv_id": "2506.22441v1",
      "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation",
      "title_zh": "åŸºäºé˜ˆå€¼è·ç¦»åŠ æƒæŸå¤±çš„å¼ é‡éšå› å­åˆ†è§£äº¤é€šæ•°æ®ä¼°è®¡",
      "authors": [
        "Lei Yang"
      ],
      "abstract": "Intelligent transportation systems (ITS) rely heavily on complete and high-quality spatiotemporal traffic data to achieve optimal performance. Nevertheless, in real-word traffic data collection processes, issues such as communication failures and sensor malfunctions often lead to incomplete or corrupted datasets, thereby posing significant challenges to the advancement of ITS. Among various methods for imputing missing spatiotemporal traffic data, the latent factorization of tensors (LFT) model has emerged as a widely adopted and effective solution. However, conventional LFT models typically employ the standard L2-norm in their learning objective, which makes them vulnerable to the influence of outliers. To overcome this limitation, this paper proposes a threshold distance weighted (TDW) loss-incorporated Latent Factorization of Tensors (TDWLFT) model. The proposed loss function effectively reduces the model's sensitivity to outliers by assigning differentiated weights to individual samples. Extensive experiments conducted on two traffic speed datasets sourced from diverse urban environments confirm that the proposed TDWLFT model consistently outperforms state-of-the-art approaches in terms of both in both prediction accuracy and computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé˜ˆå€¼è·ç¦»åŠ æƒ(Threshold Distance Weighted, TDW)æŸå¤±å‡½æ•°çš„å¼ é‡æ½œåœ¨åˆ†è§£æ¨¡å‹(TDWLFT)ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½äº¤é€šç³»ç»Ÿ(ITS)åœ¨æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­å› ä¼ æ„Ÿå™¨æ•…éšœå¯¼è‡´çš„æ•°æ®ç¼ºå¤±åŠè´¨é‡ä¸‹é™é—®é¢˜ã€‚ä¼ ç»Ÿçš„å¼ é‡æ½œåœ¨åˆ†è§£(Latent Factorization of Tensors, LFT)æ¨¡å‹ç”±äºæ™®éé‡‡ç”¨L2èŒƒæ•°(L2-norm)ä½œä¸ºå­¦ä¹ ç›®æ ‡ï¼Œä½¿å…¶åœ¨é¢å¯¹ç¦»ç¾¤å€¼(outliers)æ—¶è¡¨ç°è„†å¼±ã€‚TDWLFTé€šè¿‡ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…å·®å¼‚åŒ–çš„æƒé‡ï¼Œæœ‰æ•ˆé™ä½äº†æ¨¡å‹å¯¹ç¦»ç¾¤å€¼çš„æ•æ„Ÿåº¦ï¼Œä»è€Œæå‡äº†æ—¶ç©ºäº¤é€šæ•°æ®ä¼°ç®—çš„é²æ£’æ€§ã€‚åœ¨ä¸¤ä¸ªä¸åŒåŸå¸‚ç¯å¢ƒçš„äº¤é€šé€Ÿåº¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›(State-of-the-art)æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22441v1",
      "published_date": "2025-06-11 05:36:13 UTC",
      "updated_date": "2025-06-11 05:36:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:50.714256+00:00"
    },
    {
      "arxiv_id": "2506.09408v1",
      "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models",
      "title_zh": "ä»¤ç‰Œçº¦æŸè§£ç æå‡å¤§è¯­è¨€æ¨¡å‹é—®ç­”ä»»åŠ¡çš„é²æ£’æ€§",
      "authors": [
        "Jui-Ming Yao",
        "Hao-Yuan Chen",
        "Zi-Xian Tang",
        "Bing-Jia Tan",
        "Sheng-Wei Peng",
        "Bing-Cheng Xie",
        "Shun-Feng Su"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on multiple-choice question answering (MCQA) benchmarks, yet they remain highly vulnerable to minor input perturbations. In this paper, we introduce and evaluate Token Constraint Decoding (TCD). This simple yet effective inference-time algorithm enforces alignment between token-level predictions to enhance robustness in noisy settings. Through extensive experiments on CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired with prompt engineering (PE) fixes, significantly restores performance degraded by input noise, yielding up to +39\\% absolute gains for weaker models like Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly regularizes overconfident outputs, with different models requiring distinct penalty schedules to maximize resilience. Our findings establish TCD as a practical, model-agnostic approach for improving reasoning stability under real-world imperfections and pave the way for more reliable deployment of LLMs in safety-critical or user-facing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ï¼ˆMCQAï¼‰åŸºå‡†æµ‹è¯•ä¸­å¯¹å¾®å°è¾“å…¥æ‰°åŠ¨çš„é«˜åº¦æ•æ„Ÿæ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº† Token Constraint Decoding (TCD) ç®—æ³•ã€‚TCD æ˜¯ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ¨ç†ç«¯ï¼ˆinference-timeï¼‰ç®—æ³•ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œæ ‡è®°çº§ï¼ˆtoken-levelï¼‰é¢„æµ‹ä¹‹é—´çš„å¯¹é½ï¼Œæ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚ç ”ç©¶åœ¨ CommonsenseQAã€MMLU å’Œ MMLU-Pro ç­‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤º TCD é…åˆæç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰ä¿®å¤èƒ½æœ‰æ•ˆæ¢å¤å› è¾“å…¥å™ªå£°å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¾ƒå¼±çš„æ¨¡å‹ï¼ˆå¦‚ Gemma3 1Bï¼‰ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è¾¾ 39% çš„ç»å¯¹å‡†ç¡®ç‡æå‡ã€‚æƒ©ç½šé¡¹åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼ŒTCD èƒ½å¤Ÿéšå¼åœ°è§„èŒƒè¿‡åº¦è‡ªä¿¡çš„è¾“å‡ºï¼Œä¸”ä¸åŒæ¨¡å‹éœ€è¦ç‰¹å®šçš„æƒ©ç½šç­–ç•¥ä»¥æœ€å¤§åŒ–æŠ—å¹²æ‰°èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°ç¡®ç«‹äº† TCD ä½œä¸ºä¸€ç§é€šç”¨çš„ã€ä¸æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç°å®ä¸–ç•Œä¸å®Œç¾è¾“å…¥ä¸‹çš„æ¨ç†ç¨³å®šæ€§ï¼Œä¸º LLMs åœ¨å®‰å…¨å…³é”®æˆ–é¢å‘ç”¨æˆ·çš„åº”ç”¨ä¸­çš„å¯é éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09408v1",
      "published_date": "2025-06-11 05:33:56 UTC",
      "updated_date": "2025-06-11 05:33:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:51.677988+00:00"
    },
    {
      "arxiv_id": "2506.09397v5",
      "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving",
      "title_zh": "SLEDï¼šé¢å‘é«˜æ•ˆè¾¹ç¼˜æœåŠ¡çš„å¤§è¯­è¨€æ¨¡å‹æŠ•æœºè§£ç æ¡†æ¶",
      "authors": [
        "Xiangchen Li",
        "Dimitrios Spatharakis",
        "Saeid Ghafouri",
        "Jiakun Fan",
        "Hans Vandierendonck",
        "Deepu John",
        "Bo Ji",
        "Dimitrios Nikolopoulos"
      ],
      "abstract": "The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \\acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§ä¸è¾¹ç¼˜è®¾å¤‡æœ‰é™è®¡ç®—èµ„æºä¹‹é—´çš„çŸ›ç›¾ï¼Œæå‡ºäº†SLEDæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„è¾¹ç¼˜ç«¯æ¨ç†æœåŠ¡ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†æŠ•æœºé‡‡æ ·(Speculative Decoding)æŠ€æœ¯é€‚é…äºè¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œé€šè¿‡åè°ƒå¼‚æ„è®¾å¤‡é—´çš„è®¡ç®—åˆ†å¸ƒæ¥æå‡æ€§èƒ½ã€‚åœ¨SLEDä¸­ï¼Œè½»é‡çº§è¾¹ç¼˜è®¾å¤‡åœ¨æœ¬åœ°åˆ©ç”¨å¤šæ ·åŒ–çš„è‰ç¨¿æ¨¡å‹(Draft Models)ç”Ÿæˆå€™é€‰ä»¤ç‰Œï¼Œè€Œå…±äº«çš„è¾¹ç¼˜æœåŠ¡å™¨åˆ™é€šè¿‡ç›®æ ‡æ¨¡å‹(Target Model)å¯¹æ¥è‡ªä¸åŒè®¾å¤‡çš„è¯·æ±‚è¿›è¡Œæ‰¹é‡éªŒè¯ï¼Œä»è€Œåœ¨æ”¯æŒè®¾å¤‡å¼‚æ„æ€§çš„åŒæ—¶é™ä½äº†æœåŠ¡å™¨ç«¯çš„æ˜¾å­˜å ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Jetson Orin Nanoå’ŒRaspberry Piç­‰å…¸å‹è¾¹ç¼˜ç¡¬ä»¶ä¸Šï¼ŒSLEDåœ¨ä¸æŸå¤±æ¨¡å‹å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œä½¿ç³»ç»Ÿååé‡æå‡äº†2.2å€ï¼Œç³»ç»Ÿå®¹é‡æå‡äº†2.8å€ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†è¾¹ç¼˜ç«¯LLMæœåŠ¡çš„éƒ¨ç½²æ•ˆç‡ä¸æˆæœ¬æ•ˆç›Šã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "8 pages, 8 figures, 2 tables, accepted by SEC 2025: Tenth ACM/IEEE Symposium on Edge Computing",
      "pdf_url": "https://arxiv.org/pdf/2506.09397v5",
      "published_date": "2025-06-11 04:55:54 UTC",
      "updated_date": "2025-11-04 21:34:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:59.347743+00:00"
    },
    {
      "arxiv_id": "2506.09396v1",
      "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models",
      "title_zh": "æ¨ç†å³èµ„æºï¼šä¼˜åŒ–ä»£ç ç”Ÿæˆæ¨¡å‹ä¸­çš„å¿«æ…¢æ€è€ƒ",
      "authors": [
        "Zongjie Li",
        "Shuai Wang"
      ],
      "abstract": "This position paper proposes a fundamental shift in designing code generation models: treating reasoning depth as a controllable resource. Rather than being an incidental byproduct of prompting, we argue that the trade-off between rapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought deliberation (\"slow thinking\") must be explicitly managed. We contend that optimizing reasoning budgets across the entire model lifecycle - from synthetic data creation and benchmarking to real-world deploymen - can unlock superior trade-offs among accuracy, latency, and cost. This paper outlines how adaptive control over reasoning can enrich supervision signals, motivate new multi-dimensional benchmarks, and inform cost-aware, security-conscious deployment policies. By viewing fast and slow thinking as complementary modes to be scheduled, we envision coding agents that think deep when necessary and act fast when possible.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºåœ¨è®¾è®¡ä»£ç ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œåº”å°†æ¨ç†æ·±åº¦(reasoning depth)è§†ä¸ºä¸€ç§å¯æ§èµ„æºï¼Œè€Œéæç¤ºè¯è§¦å‘çš„å¶ç„¶äº§ç‰©ã€‚è®ºæ–‡ä¸»å¼ æ˜¾å¼ç®¡ç†å¿«é€Ÿç›´æ¥å›ç­”(fast thinking)ä¸å¤æ‚çš„é“¾å¼æ€ç»´(slow thinking)ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®ç”Ÿæˆã€åŸºå‡†æµ‹è¯•åŠå®é™…éƒ¨ç½²ä¸­åŠ¨æ€åˆ†é…æ¨ç†é¢„ç®—(reasoning budgets)ï¼Œå¯ä»¥å®ç°åœ¨å‡†ç¡®ç‡ã€å»¶è¿Ÿä¸æˆæœ¬ä¹‹é—´çš„æ›´ä¼˜å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæ¨ç†çš„è‡ªé€‚åº”æ§åˆ¶è¿˜èƒ½ä¸°å¯Œç›‘ç£ä¿¡å·ï¼Œå¹¶ä¸ºæˆæœ¬æ„ŸçŸ¥åŠå®‰å…¨æ„è¯†å¼ºçš„éƒ¨ç½²ç­–ç•¥æä¾›æŒ‡å¯¼ã€‚è¿™ç§å°†å¿«æ…¢æ€è€ƒè§†ä¸ºäº’è¡¥è°ƒåº¦æ¨¡å¼çš„æ„¿æ™¯ï¼Œæ—¨åœ¨å¼€å‘å‡ºèƒ½åœ¨å¿…è¦æ—¶æ·±åº¦æ€è€ƒã€åœ¨å¯èƒ½æ—¶å¿«é€Ÿå“åº”çš„ä»£ç æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09396v1",
      "published_date": "2025-06-11 04:55:00 UTC",
      "updated_date": "2025-06-11 04:55:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:07:58.558383+00:00"
    },
    {
      "arxiv_id": "2506.09390v1",
      "title": "Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making",
      "title_zh": "è¶…è¶Šçº³ä»€å‡è¡¡ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åœ¨ç­–ç•¥å†³ç­–ä¸­çš„æœ‰é™ç†æ€§",
      "authors": [
        "Kehan Zheng",
        "Jinfeng Zhou",
        "Hongning Wang"
      ],
      "abstract": "Large language models are increasingly used in strategic decision-making settings, yet evidence shows that, like humans, they often deviate from full rationality. In this study, we compare LLMs and humans using experimental paradigms directly adapted from behavioral game-theory research. We focus on two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's Dilemma, which are well known for revealing systematic departures from rational play in human subjects. By placing LLMs in identical experimental conditions, we evaluate whether their behaviors exhibit the bounded rationality characteristic of humans. Our findings show that LLMs reproduce familiar human heuristics, such as outcome-based strategy switching and increased cooperation when future interaction is possible, but they apply these rules more rigidly and demonstrate weaker sensitivity to the dynamic changes in the game environment. Model-level analyses reveal distinctive architectural signatures in strategic behavior, and even reasoning models sometimes struggle to find effective strategies in adaptive situations. These results indicate that current LLMs capture only a partial form of human-like bounded rationality and highlight the need for training methods that encourage flexible opponent modeling and stronger context awareness.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¯”è¾ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åœ¨æˆ˜ç•¥å†³ç­–ä¸­çš„è¡¨ç°ï¼Œé‡ç‚¹æ¢è®¨äº†åœ¨Rock-Paper-Scissorså’ŒPrisoner's Dilemmaç­‰ç»å…¸ç­–ç•¥åšå¼ˆä¸­å±•ç°çš„æœ‰é™ç†æ€§ï¼ˆBounded Rationalityï¼‰ã€‚é€šè¿‡å°†LLMsç½®äºä¸äººç±»å®éªŒç›¸åŒçš„æ¡ä»¶ä¸‹ï¼Œç ”ç©¶å‘ç°LLMsèƒ½å¤ç°äººç±»å¸¸è§çš„å¯å‘å¼ï¼ˆHeuristicsï¼‰è¡Œä¸ºï¼Œå¦‚åŸºäºç»“æœçš„ç­–ç•¥åˆ‡æ¢ä»¥åŠåœ¨æœªæ¥å­˜åœ¨äº’åŠ¨å¯èƒ½æ—¶å¢åŠ åˆä½œå€¾å‘ã€‚ç„¶è€Œï¼ŒLLMsåœ¨åº”ç”¨è¿™äº›è§„åˆ™æ—¶è¡¨ç°å¾—æ›´åŠ åƒµåŒ–ï¼Œä¸”å¯¹åšå¼ˆç¯å¢ƒçš„åŠ¨æ€å˜åŒ–æ•æ„Ÿåº¦è¾ƒä½ã€‚æ¨¡å‹åˆ†ææ­ç¤ºäº†æˆ˜ç•¥è¡Œä¸ºä¸­ç‹¬ç‰¹çš„æ¶æ„ç‰¹å¾ï¼Œç”šè‡³ä¸€äº›æ¨ç†æ¨¡å‹åœ¨è‡ªé€‚åº”æƒ…å¢ƒä¸‹ä¹Ÿéš¾ä»¥æ‰¾åˆ°æœ‰æ•ˆç­–ç•¥ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMsä»…éƒ¨åˆ†æ•æ‰äº†ç±»ä¼¼äººç±»çš„æœ‰é™ç†æ€§ï¼Œå‡¸æ˜¾äº†å¼€å‘èƒ½å¤Ÿå®ç°çµæ´»å¯¹æ‰‹å»ºæ¨¡ï¼ˆOpponent Modelingï¼‰å’Œå¢å¼ºä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆContext Awarenessï¼‰è®­ç»ƒæ–¹æ³•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09390v1",
      "published_date": "2025-06-11 04:43:54 UTC",
      "updated_date": "2025-06-11 04:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:05.768764+00:00"
    },
    {
      "arxiv_id": "2506.13787v1",
      "title": "Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network",
      "title_zh": "åŸºäºå›¾ç¥ç»ç½‘ç»œçš„åŒ¿åç”¨æˆ·äº¤äº’å…³ç³»åˆ†æä¸å¹¿å‘Šåé¦ˆé¢„æµ‹",
      "authors": [
        "Yanjun Dai",
        "Haoyang Feng",
        "Yuan Gao"
      ],
      "abstract": "While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§£è€¦æ—¶åºå±‚æ¬¡å›¾ç¥ç»ç½‘ç»œ(DTH-GNN)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾æ¨¡å‹åœ¨æ•æ‰åŒ¿åç”¨æˆ·äº¤äº’ç½‘ç»œä¸­å¤šå°ºåº¦æ—¶é—´ã€è¯­ä¹‰åŠé«˜é˜¶ä¾èµ–ç‰¹å¾æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹é¦–å…ˆå¼•å…¥äº†æ—¶é—´è¾¹åˆ†è§£(temporal edge decomposition)æŠ€æœ¯ï¼Œå°†äº¤äº’åˆ’åˆ†ä¸ºçŸ­æœŸçˆ†å‘(short-term burst)ã€æ˜¼å¤œå‘¨æœŸ(diurnal cycle)å’Œé•¿ç¨‹è®°å¿†(long-range memory)ä¸‰ä¸ªé€šé“ï¼Œå¹¶åˆ©ç”¨å¹¶è¡Œç©ºæ´æ®‹å·®(parallel dilated residuals)å·ç§¯æ ¸è¿›è¡Œç‰¹å¾æå–ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å…ƒè·¯å¾„æ¡ä»¶Transformerç¼–ç å™¨æ„å»ºäº†å±‚æ¬¡åŒ–å¼‚æ„èšåˆ(hierarchical heterogeneous aggregation)æœºåˆ¶ï¼Œç»“åˆè·¨é€šé“è‡ªæ³¨æ„åŠ›ä¸é—¨æ§å…³ç³»é€‰æ‹©å™¨åŠ¨æ€æŠ‘åˆ¶å™ªå£°ç»“æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ¶å®šäº†åé¦ˆæ„ŸçŸ¥å¯¹æ¯”è§„åˆ™(contrast regularity of feedback perception)ï¼Œåˆ©ç”¨åŒåŠ¨é‡é˜Ÿåˆ—è’¸é¦(dual-momentum queue distillation)å’Œç­–ç•¥æ¢¯åº¦å±‚(strategy gradient layer)å¯¹èŠ‚ç‚¹è¡¨ç¤ºè¿›è¡Œé¢å‘æ•ˆç›Šçš„å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDTH-GNNåœ¨å¹¿å‘Šåé¦ˆé¢„æµ‹ä»»åŠ¡ä¸­çš„AUCæ¯”æœ€ä¼˜åŸºçº¿æ¨¡å‹æå‡äº†8.2%ï¼Œå¯¹æ•°æŸå¤±(logarithmic loss)é™ä½äº†5.7%ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹å¤æ‚åŒ¿åè¡Œä¸ºæ¨¡å¼çš„å»ºæ¨¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13787v1",
      "published_date": "2025-06-11 04:40:24 UTC",
      "updated_date": "2025-06-11 04:40:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:05.916993+00:00"
    },
    {
      "arxiv_id": "2506.09383v2",
      "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations",
      "title_zh": "ç»“åˆå…¨èº«è‚Œè‚‰éª¨éª¼ç«™ç«‹ä¸è·Œå€’æ¨¡æ‹Ÿçš„åŒè¶³å¹³è¡¡æ§åˆ¶",
      "authors": [
        "Chengtian Ma",
        "Yunyue Wei",
        "Chenhui Zuo",
        "Chen Zhang",
        "Yanan Sui"
      ],
      "abstract": "Balance control is important for human and bipedal robotic systems. While dynamic balance during locomotion has received considerable attention, quantitative understanding of static balance and falling remains limited. This work presents a hierarchical control pipeline for simulating human balance via a comprehensive whole-body musculoskeletal system. We identified spatiotemporal dynamics of balancing during stable standing, revealed the impact of muscle injury on balancing behavior, and generated fall contact patterns that aligned with clinical data. Furthermore, our simulated hip exoskeleton assistance demonstrated improvement in balance maintenance and reduced muscle effort under perturbation. This work offers unique muscle-level insights into human balance dynamics that are challenging to capture experimentally. It could provide a foundation for developing targeted interventions for individuals with balance impairments and support the advancement of humanoid robotic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ†å±‚æ§åˆ¶ç®¡çº¿(hierarchical control pipeline)ï¼Œæ—¨åœ¨é€šè¿‡å…¨èº«è‚Œè‚‰éª¨éª¼ç³»ç»Ÿ(whole-body musculoskeletal system)æ¨¡æ‹Ÿäººä½“çš„åŒè¶³å¹³è¡¡æ§åˆ¶ã€‚ç ”ç©¶å›¢é˜Ÿè¯†åˆ«äº†ç¨³å®šç«™ç«‹æœŸé—´å¹³è¡¡çš„æ—¶ç©ºåŠ¨åŠ›å­¦(spatiotemporal dynamics)ï¼Œæ­ç¤ºäº†è‚Œè‚‰æŸä¼¤å¯¹å¹³è¡¡è¡Œä¸ºçš„å½±å“ï¼Œå¹¶ç”Ÿæˆäº†ä¸ä¸´åºŠæ•°æ®ä¸€è‡´çš„è·Œå€’æ¥è§¦æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæ¨¡æ‹Ÿç»“æœæ˜¾ç¤ºé«‹éƒ¨å¤–éª¨éª¼(hip exoskeleton)è¾…åŠ©åœ¨å—åˆ°æ‰°åŠ¨æ—¶èƒ½æ˜¾è‘—æ”¹å–„å¹³è¡¡ç»´æŒèƒ½åŠ›å¹¶å‡å°‘è‚Œè‚‰è´Ÿè·ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†å®éªŒç¯å¢ƒä¸‹éš¾ä»¥è·å–çš„è‚Œè‚‰æ°´å¹³(muscle-level)åŠ¨åŠ›å­¦è§è§£ï¼Œä¸ºé’ˆå¯¹å¹³è¡¡éšœç¢ä¸ªä½“çš„ç²¾å‡†å¹²é¢„ä»¥åŠäººå½¢æœºå™¨äººç³»ç»Ÿ(humanoid robotic systems)çš„è¿›é˜¶ç ”å‘å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09383v2",
      "published_date": "2025-06-11 04:23:49 UTC",
      "updated_date": "2025-09-08 12:59:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:17.193818+00:00"
    },
    {
      "arxiv_id": "2506.13786v1",
      "title": "Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction",
      "title_zh": "èåˆæ•°æ®é›†æˆçš„å¢å¼ºå‹ Bagging é›†æˆå›å½’ç”¨äºæ—¶é—´åºåˆ—ç³–å°¿ç—…é¢„æµ‹",
      "authors": [
        "Vuong M. Ngo",
        "Tran Quang Vinh",
        "Patricia Kearney",
        "Mark Roantree"
      ],
      "abstract": "Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç³–å°¿ç—…é¢„æµ‹ä¸­æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„å»ºæ¨¡éš¾é¢˜ï¼Œé¦–å…ˆé€šè¿‡Data Engineeringè¿‡ç¨‹æ•´åˆäº†2011è‡³2021å¹´çš„ç³–å°¿ç—…ç›¸å…³æ•°æ®é›†ï¼Œæ„å»ºäº†å…¨é¢çš„ç‰¹å¾é›†ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹è¢‹è£…é›†æˆå›å½’æ¨¡å‹EBMBag+ï¼Œç”¨äºå¯¹ç¾å›½å„åŸå¸‚ç³–å°¿ç—…æ‚£ç—…ç‡è¿›è¡ŒTime Seriesé¢„æµ‹ã€‚é€šè¿‡ä¸SVMRegã€BDTreeã€LSBoostã€NNã€LSTMå’ŒERMBagç­‰åŸºçº¿æ¨¡å‹å¯¹æ¯”ï¼Œå®éªŒç»“æœæ˜¾ç¤ºEBMBag+åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¡¨ç°æœ€ä¼˜ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¨¡å‹å®ç°äº†0.41çš„MAEã€0.53çš„RMSEã€4.01çš„MAPEä»¥åŠ0.9çš„R2ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†ç³–å°¿ç—…é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºé«˜æ•ˆçš„åŒ»ç–—ä¿å¥è§„åˆ’å’Œå®šå‘å¹²é¢„æä¾›äº†æ•°æ®é©±åŠ¨çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17th International Conference on Computational Collective Intelligence, LNAI, Springer, 11 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.13786v1",
      "published_date": "2025-06-11 04:21:50 UTC",
      "updated_date": "2025-06-11 04:21:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:14.194285+00:00"
    },
    {
      "arxiv_id": "2506.09373v2",
      "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization",
      "title_zh": "LPOï¼šé€šè¿‡ä½ç½®åå¥½ä¼˜åŒ–å®ç°ç²¾å‡†çš„ GUI æ™ºèƒ½ä½“äº¤äº’",
      "authors": [
        "Jiaqi Tang",
        "Yu Xia",
        "Yi-Feng Wu",
        "Yuwei Hu",
        "Yuhui Chen",
        "Qing-Guo Chen",
        "Xiaogang Xu",
        "Xiangyu Wu",
        "Hao Lu",
        "Yanqing Ma",
        "Shiyin Lu",
        "Qifeng Chen"
      ],
      "abstract": "The advent of autonomous agents is transforming interactions with Graphical User Interfaces (GUIs) by employing natural language as a powerful intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods in current GUI agents for achieving spatial localization, these methods face substantial challenges due to their limited capacity to accurately perceive positional data. Existing strategies, such as reinforcement learning, often fail to assess positional accuracy effectively, thereby restricting their utility. In response, we introduce Location Preference Optimization (LPO), a novel approach that leverages locational data to optimize interaction preferences. LPO uses information entropy to predict interaction positions by focusing on zones rich in information. Besides, it further introduces a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Supported by Group Relative Preference Optimization (GRPO), LPO facilitates an extensive exploration of GUI environments and significantly enhances interaction precision. Comprehensive experiments demonstrate LPO's superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations. Our code will be made publicly available soon, at https://github.com/AIDC-AI/LPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)æ™ºèƒ½ä½“åœ¨ç©ºé—´å®šä½(spatial localization)å‡†ç¡®æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä½ç½®åå¥½ä¼˜åŒ–(Location Preference Optimization, LPO)æ¡†æ¶ã€‚LPOåˆ©ç”¨ä¿¡æ¯ç†µ(information entropy)æ¥é¢„æµ‹äº¤äº’ä½ç½®ï¼Œé€šè¿‡èšç„¦ä¿¡æ¯å¯†é›†åŒºåŸŸæå‡äº†æ¨¡å‹å¯¹ä½ç½®æ•°æ®çš„æ„ŸçŸ¥ç²¾åº¦ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºç‰©ç†è·ç¦»çš„åŠ¨æ€ä½ç½®å¥–åŠ±å‡½æ•°ï¼Œæœ‰æ•ˆè¡¡é‡äº†ä¸åŒäº¤äº’ä½ç½®çš„é‡è¦æ€§ã€‚ç»“åˆç»„ç›¸å¯¹åå¥½ä¼˜åŒ–(Group Relative Preference Optimization, GRPO)ï¼ŒLPOåœ¨å¹¿æ³›æ¢ç´¢GUIç¯å¢ƒçš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†äº¤äº’çš„ç²¾ç¡®åº¦ã€‚å®éªŒè¯æ˜ï¼ŒLPOåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’Œå®æ—¶åœ¨çº¿è¯„ä¼°ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09373v2",
      "published_date": "2025-06-11 03:43:30 UTC",
      "updated_date": "2025-06-15 07:25:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:22.627114+00:00"
    },
    {
      "arxiv_id": "2506.09368v1",
      "title": "Anomaly Detection and Generation with Diffusion Models: A Survey",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹ä¸ç”Ÿæˆï¼šç»¼è¿°",
      "authors": [
        "Yang Liu",
        "Jing Liu",
        "Chengfang Li",
        "Rui Xi",
        "Wenchao Li",
        "Liang Cao",
        "Jin Wang",
        "Laurence T. Yang",
        "Junsong Yuan",
        "Wei Zhou"
      ],
      "abstract": "Anomaly detection (AD) plays a pivotal role across diverse domains, including cybersecurity, finance, healthcare, and industrial manufacturing, by identifying unexpected patterns that deviate from established norms in real-world data. Recent advancements in deep learning, specifically diffusion models (DMs), have sparked significant interest due to their ability to learn complex data distributions and generate high-fidelity samples, offering a robust framework for unsupervised AD. In this survey, we comprehensively review anomaly detection and generation with diffusion models (ADGDM), presenting a tutorial-style analysis of the theoretical foundations and practical implementations and spanning images, videos, time series, tabular, and multimodal data. Crucially, unlike existing surveys that often treat anomaly detection and generation as separate problems, we highlight their inherent synergistic relationship. We reveal how DMs enable a reinforcing cycle where generation techniques directly address the fundamental challenge of anomaly data scarcity, while detection methods provide critical feedback to improve generation fidelity and relevance, advancing both capabilities beyond their individual potential. A detailed taxonomy categorizes ADGDM methods based on anomaly scoring mechanisms, conditioning strategies, and architectural designs, analyzing their strengths and limitations. We final discuss key challenges including scalability and computational efficiency, and outline promising future directions such as efficient architectures, conditioning strategies, and integration with foundation models (e.g., visual-language models and large language models). By synthesizing recent advances and outlining open research questions, this survey aims to guide researchers and practitioners in leveraging DMs for innovative AD solutions across diverse applications.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°å…¨é¢å›é¡¾äº†åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion Models, DMs)çš„å¼‚å¸¸æ£€æµ‹ä¸ç”Ÿæˆ(Anomaly Detection and Generation, ADGDM)æŠ€æœ¯ï¼Œæ¶µç›–äº†å›¾åƒã€è§†é¢‘ã€æ—¶é—´åºåˆ—ã€è¡¨æ ¼åŠå¤šæ¨¡æ€æ•°æ®ã€‚ä¸åŒäºä»¥å¾€å°†å¼‚å¸¸æ£€æµ‹ä¸ç”Ÿæˆè§†ä¸ºç‹¬ç«‹é—®é¢˜çš„ç ”ç©¶ï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†ä¸¤è€…ä¹‹é—´çš„ååŒå…³ç³»ï¼Œæ­ç¤ºäº†ç”ŸæˆæŠ€æœ¯å¦‚ä½•è§£å†³å¼‚å¸¸æ•°æ®ç¨€ç¼ºéš¾é¢˜ï¼Œä»¥åŠæ£€æµ‹æ–¹æ³•å¦‚ä½•åé¦ˆå¹¶æå‡ç”Ÿæˆä¿çœŸåº¦ã€‚æ–‡ç« æ ¹æ®å¼‚å¸¸è¯„åˆ†æœºåˆ¶(anomaly scoring mechanisms)ã€æ¡ä»¶ç­–ç•¥(conditioning strategies)å’Œæ¶æ„è®¾è®¡å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„åˆ†ç±»å­¦(taxonomy)å½’ç±»ï¼Œå¹¶åˆ†æäº†å…¶å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚æ­¤å¤–ï¼Œç»¼è¿°è¿˜è®¨è®ºäº†å¯æ‰©å±•æ€§(scalability)å’Œè®¡ç®—æ•ˆç‡ç­‰å…³é”®æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…æŒ‡å‡ºäº†é«˜æ•ˆæ¶æ„ã€æ¡ä»¶ç­–ç•¥ä»¥åŠä¸è§†è§‰è¯­è¨€æ¨¡å‹(Visual-Language Models)å’Œå¤§è¯­è¨€æ¨¡å‹(Large Language Models)é›†æˆç­‰å‰æ²¿ç ”ç©¶æ–¹å‘ï¼Œä¸ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºåˆ›æ–°å¼‚å¸¸æ£€æµ‹è§£å†³æ–¹æ¡ˆæä¾›äº†ç³»ç»Ÿæ€§æŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 11 figures, 13 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.09368v1",
      "published_date": "2025-06-11 03:29:18 UTC",
      "updated_date": "2025-06-11 03:29:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:21.077355+00:00"
    },
    {
      "arxiv_id": "2506.09367v1",
      "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content",
      "title_zh": "COGENTï¼šä¸€ç§é¢å‘è¯¾ç¨‹çš„å­¦æ®µé€‚é…å‹æ•™è‚²å†…å®¹ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Zhengyuan Liu",
        "Stella Xin Yin",
        "Dion Hoe-Lian Goh",
        "Nancy F. Chen"
      ],
      "abstract": "While Generative AI has demonstrated strong potential and versatility in content generation, its application to educational contexts presents several challenges. Models often fail to align with curriculum standards and maintain grade-appropriate reading levels consistently. Furthermore, STEM education poses additional challenges in balancing scientific explanations with everyday language when introducing complex and abstract ideas and phenomena to younger students. In this work, we propose COGENT, a curriculum-oriented framework for generating grade-appropriate educational content. We incorporate three curriculum components (science concepts, core ideas, and learning objectives), control readability through length, vocabulary, and sentence complexity, and adopt a ``wonder-based'' approach to increase student engagement and interest. We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human expert analysis. Experimental results show that COGENT consistently produces grade-appropriate passages that are comparable or superior to human references. Our work establishes a viable approach for scaling adaptive and high-quality learning resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† COGENTï¼Œä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆç¬¦åˆå¹´çº§æ°´å¹³æ•™è‚²å†…å®¹çš„è¯¾ç¨‹å¯¼å‘æ¡†æ¶ï¼Œä»¥è§£å†³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨è¯¾ç¨‹æ ‡å‡†å¯¹é½åŠ STEM æ•™è‚²ä¸­å¤æ‚æ¦‚å¿µé€šä¿—åŒ–è¡¨è¾¾çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆ science conceptsã€core ideas å’Œ learning objectives ä¸‰å¤§ç»„ä»¶ï¼Œå®ç°äº†å¯¹æ•™è‚²å†…å®¹çš„ç³»ç»ŸåŒ–æ„å»ºã€‚COGENT è¿›ä¸€æ­¥é€šè¿‡ç²¾ç»†åŒ–æ§åˆ¶é•¿åº¦ã€è¯æ±‡å’Œå¥å­å¤æ‚åº¦æ¥ç¡®ä¿ readability çš„é€‚é…æ€§ï¼Œå¹¶å¼•å…¥ â€œwonder-basedâ€ æ–¹æ³•ä»¥æå‡å­¦ç”Ÿçš„å­¦ä¹ å…´è¶£ä¸å‚ä¸æ„Ÿã€‚ç ”ç©¶é‡‡ç”¨äº† LLM-as-a-judge ä¸äººç±»ä¸“å®¶åˆ†æè¿›è¡Œå¤šç»´åº¦è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶ç”Ÿæˆçš„ç¯‡ç« åœ¨å¹´çº§é€‚é…æ€§ä¸Šä¸äººç±»å‚è€ƒæ°´å¹³ç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚è¯¥å·¥ä½œä¸ºè§„æ¨¡åŒ–ç”Ÿäº§é«˜è´¨é‡ã€è‡ªé€‚åº”çš„æ•™è‚²èµ„æºæä¾›äº†ä¸€ç§åˆ‡å®æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "BEA 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09367v1",
      "published_date": "2025-06-11 03:27:50 UTC",
      "updated_date": "2025-06-11 03:27:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:39.345946+00:00"
    },
    {
      "arxiv_id": "2506.09363v1",
      "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing",
      "title_zh": "SAGEï¼šåˆ©ç”¨è¯­ä¹‰å¢å¼ºæ“¦é™¤æ¢ç´¢ä¸å®‰å…¨æ¦‚å¿µåŸŸçš„è¾¹ç•Œ",
      "authors": [
        "Hongguang Zhu",
        "Yunchao Wei",
        "Mengyu Wang",
        "Siyu Jiao",
        "Yan Fang",
        "Jiannan Huang",
        "Yao Zhao"
      ],
      "abstract": "Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at https://github.com/KevinLight831/SAGE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAGEï¼Œä¸€ç§é€šè¿‡è¯­ä¹‰å¢å¼ºæ“¦é™¤ï¼ˆSemantic-Augment Erasingï¼‰æ¥æ¢ç´¢å’Œæ¶ˆé™¤æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰ä¸­ä¸å®‰å…¨æ¦‚å¿µé¢†åŸŸçš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ“¦é™¤æŠ€æœ¯é™·å…¥â€œè¯æ¦‚å¿µæ·±æ¸Šâ€å¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒSAGEåˆ©ç”¨å¾ªç¯è‡ªæ£€å’Œè‡ªæ“¦é™¤æœºåˆ¶ï¼Œå°†é’ˆå¯¹å•ä¸€è¯æ±‡çš„æ“¦é™¤æ‰©å±•ä¸ºå¯¹æ•´ä¸ªæ¦‚å¿µé¢†åŸŸçš„æ“¦é™¤ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æåŸå§‹æ¨¡å‹ä¸è®­ç»ƒæ¨¡å‹é—´çš„è¯­ä¹‰ç©ºé—´å…³ç³»ï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«å¹¶æ¶ˆé™¤æ¦‚å¿µé¢†åŸŸçš„è¾¹ç•Œè¡¨ç¤ºï¼Œä¸”æ— éœ€é¢å¤–çš„é¢„å¤„ç†æ•°æ®ã€‚ä¸ºäº†åœ¨æ“¦é™¤è¿‡ç¨‹ä¸­ä¿æŠ¤æ— å…³æ¦‚å¿µçš„å®Œæ•´æ€§ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†å…¨å±€-å±€éƒ¨åä½œä¿ç•™æœºåˆ¶ï¼ˆGlobal-local collaborative retention mechanismï¼‰ï¼Œå°†å…¨å±€è¯­ä¹‰å¯¹é½ä¸å±€éƒ¨é¢„æµ‹å™ªå£°ä¿ç•™ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGEåœ¨ç¡®ä¿ç”Ÿæˆå®‰å…¨æ€§çš„åŒæ—¶æœ‰æ•ˆå‡å°‘äº†å¯¹æ­£å¸¸æ€§èƒ½çš„æŸå®³ï¼Œå…¶ç»¼åˆè¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¦‚å¿µæ“¦é™¤æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.09363v1",
      "published_date": "2025-06-11 03:21:24 UTC",
      "updated_date": "2025-06-11 03:21:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:44.685016+00:00"
    },
    {
      "arxiv_id": "2506.09362v1",
      "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore",
      "title_zh": "â€œæˆ‘è¯´äº†é‚£äº›æˆ‘è‡ªå·±ä¹Ÿéœ€è¦å¬åˆ°çš„è¯â€ï¼šSingapore åŒä¼´æ”¯æŒçš„æƒ…æ„Ÿã€ç»„ç»‡ä¸ç¤¾ä¼šæŠ€æœ¯å®è·µ",
      "authors": [
        "Kellie Yu Hui Sim",
        "Kenny Tsu Wei Choo"
      ],
      "abstract": "Peer support plays a vital role in expanding access to mental health care by providing empathetic, community-based support outside formal clinical systems. As digital platforms increasingly mediate such support, the design and impact of these technologies remain under-examined, particularly in Asian contexts. This paper presents findings from an interview study with 20 peer supporters in Singapore, who operate across diverse online, offline, and hybrid environments. Through a thematic analysis, we unpack how participants start, conduct, and sustain peer support, highlighting their motivations, emotional labour, and the sociocultural dimensions shaping their practices. Building on this grounded understanding, we surface design directions for culturally responsive digital tools that scaffold rather than supplant relational care. Drawing insights from qualitative accounts, we offer a situated perspective on how AI might responsibly augment peer support. This research contributes to human-centred computing by articulating the lived realities of peer supporters and proposing design implications for trustworthy and context-sensitive AI in mental health.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨æ–°åŠ å¡èƒŒæ™¯ä¸‹ï¼Œæœ‹è¾ˆæ”¯æŒ (Peer support) ä½œä¸ºä¸€ç§æƒ…æ„Ÿã€ç»„ç»‡å’Œç¤¾ä¼šæŠ€æœ¯å®è·µçš„ç°çŠ¶ï¼Œç‰¹åˆ«å…³æ³¨æ•°å­—å¹³å°åœ¨å…¶ä¸­å‘æŒ¥çš„ä¸­ä»‹ä½œç”¨ã€‚ç ”ç©¶é€šè¿‡å¯¹20ååœ¨åœ¨çº¿ã€çº¿ä¸‹åŠæ··åˆç¯å¢ƒä¸­å·¥ä½œçš„æ–°åŠ å¡æœ‹è¾ˆæ”¯æŒè€…è¿›è¡Œè®¿è°ˆï¼Œå¹¶é‡‡ç”¨ä¸»é¢˜åˆ†æ (Thematic analysis) æ–¹æ³•ï¼Œæ·±å…¥å‰–æäº†ä»–ä»¬å¯åŠ¨ã€æ‰§è¡Œå’Œç»´æŒæ”¯æŒå·¥ä½œçš„è¿‡ç¨‹ã€‚è°ƒæŸ¥æ­ç¤ºäº†å‚ä¸è€…çš„åŠ¨æœºã€æ‰€æ‰¿æ‹…çš„æƒ…æ„ŸåŠ³åŠ¨ (Emotional labour) ä»¥åŠå¡‘é€ å…¶å®è·µçš„ç¤¾ä¼šæ–‡åŒ–ç»´åº¦ã€‚åŸºäºè¿™äº›å®è¯ç†è§£ï¼Œè®ºæ–‡æå‡ºäº†é’ˆå¯¹æ–‡åŒ–æ•æ„Ÿå‹ (Culturally responsive) æ•°å­—å·¥å…·çš„è®¾è®¡æ–¹å‘ï¼Œå¼ºè°ƒè¿™äº›å·¥å…·åº”è¾…åŠ©è€Œéå–ä»£å…³ç³»å‹æŠ¤ç† (Relational care)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä»å®šæ€§å™è¿°ä¸­æ±²å–è§è§£ï¼Œæå‡ºäº†äººå·¥æ™ºèƒ½ (AI) å¦‚ä½•è´Ÿè´£ä»»åœ°å¢å¼ºæœ‹è¾ˆæ”¯æŒçš„æœ¬åœ°åŒ–è§†è§’ã€‚è¯¥ç ”ç©¶é€šè¿‡é˜è¿°æœ‹è¾ˆæ”¯æŒè€…çš„çœŸå®ç”Ÿæ´»ç°çŠ¶ï¼Œä¸ºå¿ƒç†å¥åº·é¢†åŸŸä¸­æ„å»ºå¯ä¿¡ä¸”å…·å¤‡æƒ…å¢ƒæ•æ„Ÿæ€§ (Context-sensitive) çš„ AI æä¾›äº†ä»¥äººä¸ºæœ¬çš„è®¡ç®— (Human-centred computing) è®¾è®¡å¯ç¤ºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09362v1",
      "published_date": "2025-06-11 03:17:31 UTC",
      "updated_date": "2025-06-11 03:17:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:01.218004+00:00"
    },
    {
      "arxiv_id": "2506.09354v1",
      "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions",
      "title_zh": "â€œè¿™çœŸçš„æ˜¯äººç±»åŒä¼´æ”¯æŒè€…å—ï¼Ÿâ€ï¼šå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©äº’åŠ¨ä¸­åŒä¼´æ”¯æŒè€…ä¸ä¸“å®¶é—´çš„åˆ†æ­§",
      "authors": [
        "Kellie Yu Hui Sim",
        "Roy Ka-Wei Lee",
        "Kenny Tsu Wei Choo"
      ],
      "abstract": "Mental health is a growing global concern, prompting interest in AI-driven solutions to expand access to psychosocial support. Peer support, grounded in lived experience, offers a valuable complement to professional care. However, variability in training, effectiveness, and definitions raises concerns about quality, consistency, and safety. Large Language Models (LLMs) present new opportunities to enhance peer support interactions, particularly in real-time, text-based interactions. We present and evaluate an AI-supported system with an LLM-simulated distressed client, context-sensitive LLM-generated suggestions, and real-time emotion visualisations. 2 mixed-methods studies with 12 peer supporters and 5 mental health professionals (i.e., experts) examined the system's effectiveness and implications for practice. Both groups recognised its potential to enhance training and improve interaction quality. However, we found a key tension emerged: while peer supporters engaged meaningfully, experts consistently flagged critical issues in peer supporter responses, such as missed distress cues and premature advice-giving. This misalignment highlights potential limitations in current peer support training, especially in emotionally charged contexts where safety and fidelity to best practices are essential. Our findings underscore the need for standardised, psychologically grounded training, especially as peer support scales globally. They also demonstrate how LLM-supported systems can scaffold this development--if designed with care and guided by expert oversight. This work contributes to emerging conversations on responsible AI integration in mental health and the evolving role of LLMs in augmenting peer-delivered care.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨å¿ƒç†å¥åº·é¢†åŸŸä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å¢å¼ºåŒä¼´æ”¯æŒ(Peer Support)äº’åŠ¨çš„æ½œåŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŒ…å«LLMæ¨¡æ‹Ÿå—åŠ©è€…ã€æƒ…å¢ƒæ•æ„Ÿå»ºè®®å’Œå®æ—¶æƒ…ç»ªå¯è§†åŒ–çš„è¾…åŠ©ç³»ç»Ÿã€‚é€šè¿‡å¯¹12ååŒä¼´æ”¯æŒè€…å’Œ5åå¿ƒç†å¥åº·ä¸“å®¶çš„æ··åˆæ–¹æ³•ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºä¸¤ç»„äººå‘˜å‡è®¤å¯è¯¥ç³»ç»Ÿåœ¨æå‡åŸ¹è®­æ•ˆæœå’Œäº’åŠ¨è´¨é‡æ–¹é¢çš„ä»·å€¼ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°ä¸“å®¶é¢‘ç¹æŒ‡å‡ºåŒä¼´æ”¯æŒè€…åœ¨äº’åŠ¨ä¸­å­˜åœ¨å¿½è§†æ±‚åŠ©ä¿¡å·å’Œè¿‡æ—©æä¾›å»ºè®®ç­‰å…³é”®è´¨é‡é—®é¢˜ï¼Œæ­ç¤ºäº†ä¸¤è€…ä¹‹é—´åœ¨å¤„ç†æƒ…æ„Ÿå±æœºæ—¶å­˜åœ¨çš„æ˜¾è‘—è®¤çŸ¥åˆ†æ­§ã€‚è¿™ç§ä¸ä¸€è‡´æ€§å‡¸æ˜¾äº†å½“å‰åŒä¼´æ”¯æŒåŸ¹è®­åœ¨åº”å¯¹å¤æ‚æƒ…æ„Ÿåœºæ™¯å’Œéµå¾ªä¸“ä¸šè§„èŒƒæ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶æœ€åå¼ºè°ƒäº†åœ¨å…¨çƒæ‰©å±•åŒä¼´æ”¯æŒæœåŠ¡æ—¶å»ºç«‹æ ‡å‡†åŒ–ã€å…·å¤‡å¿ƒç†å­¦åŸºç¡€çš„åŸ¹è®­ä½“ç³»çš„å¿…è¦æ€§ï¼Œå¹¶æŒ‡å‡ºåœ¨ä¸“å®¶æŒ‡å¯¼ä¸‹ï¼ŒLLMæ”¯æŒç³»ç»Ÿå¯ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è¾…åŠ©æ•™å­¦å·¥å…·ï¼Œä¸ºå¿ƒç†å¥åº·é¢†åŸŸè´Ÿè´£ä»»çš„AIé›†æˆè´¡çŒ®äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09354v1",
      "published_date": "2025-06-11 03:06:41 UTC",
      "updated_date": "2025-06-11 03:06:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:46.013891+00:00"
    },
    {
      "arxiv_id": "2506.09350v2",
      "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
      "title_zh": "é¢å‘å®æ—¶äº¤äº’å¼è§†é¢‘ç”Ÿæˆçš„è‡ªå›å½’å¯¹æŠ—å¼åè®­ç»ƒ",
      "authors": [
        "Shanchuan Lin",
        "Ceyuan Yang",
        "Hao He",
        "Jianwen Jiang",
        "Yuxi Ren",
        "Xin Xia",
        "Yang Zhao",
        "Xuefeng Xiao",
        "Lu Jiang"
      ],
      "abstract": "Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªå›å½’å¯¹æŠ—åè®­ç»ƒ (Autoregressive Adversarial Post-Training, AAPT)ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„æ½œè§†é¢‘æ‰©æ•£æ¨¡å‹ (Latent Video Diffusion Model) è½¬åŒ–ä¸ºå®æ—¶ã€å¯äº¤äº’çš„è§†é¢‘ç”Ÿæˆå™¨ï¼Œä»¥è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å•æ¬¡ç¥ç»å‡½æ•°è¯„ä¼° (1NFE) å®ç°æ½œå¸§çš„é€å¸§è‡ªå›å½’ç”Ÿæˆï¼Œæ”¯æŒå®æ—¶æµå¼ä¼ è¾“å¹¶èƒ½æ ¹æ®ç”¨æˆ·äº¤äº’æŒ‡ä»¤åŠ¨æ€ç”Ÿæˆåç»­å†…å®¹ã€‚AAPT é¦–æ¬¡æ¢ç´¢å°†å¯¹æŠ—è®­ç»ƒ (Adversarial Training) ä½œä¸ºè‡ªå›å½’ç”Ÿæˆçš„æœ‰æ•ˆèŒƒå¼ï¼Œåœ¨å……åˆ†åˆ©ç”¨ KV cache çš„åŒæ—¶å®ç°äº†æé«˜çš„å•æ­¥ç”Ÿæˆæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨å­¦ç”Ÿå¼ºåˆ¶ (Student-forcing) æ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘äº†é•¿è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ 8B å‚æ•°è§„æ¨¡çš„æ¨¡å‹åœ¨å•å° H100 ä¸Šå¯å®ç° 24fpsã€736x416 åˆ†è¾¨ç‡çš„å®æ—¶è§†é¢‘ç”Ÿæˆï¼Œåœ¨å¤šå¡ç¯å¢ƒä¸‹æ›´å¯ç”Ÿæˆé•¿è¾¾ä¸€åˆ†é’Ÿï¼ˆ1440å¸§ï¼‰çš„ 720P é«˜æ¸…è§†é¢‘ã€‚è¿™é¡¹å·¥ä½œä¸ºå¯è§£é‡Šã€é«˜å“åº”çš„å®æ—¶äº¤äº’å¼è§†é¢‘ç”ŸæˆæŠ€æœ¯å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09350v2",
      "published_date": "2025-06-11 03:04:23 UTC",
      "updated_date": "2025-10-01 18:55:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:47.666178+00:00"
    },
    {
      "arxiv_id": "2506.09347v1",
      "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning",
      "title_zh": "ErrorEraserï¼šé€šè¿‡é—å¿˜æ•°æ®åå·®æå‡æŒç»­å­¦ä¹ æ€§èƒ½",
      "authors": [
        "Xuemei Cao",
        "Hanlin Gu",
        "Xin Yang",
        "Bingjun Wei",
        "Haoyang Liang",
        "Xiangkun Wang",
        "Tianrui Li"
      ],
      "abstract": "Continual Learning (CL) primarily aims to retain knowledge to prevent catastrophic forgetting and transfer knowledge to facilitate learning new tasks. Unlike traditional methods, we propose a novel perspective: CL not only needs to prevent forgetting, but also requires intentional forgetting.This arises from existing CL methods ignoring biases in real-world data, leading the model to learn spurious correlations that transfer and amplify across tasks. From feature extraction and prediction results, we find that data biases simultaneously reduce CL's ability to retain and transfer knowledge. To address this, we propose ErrorEraser, a universal plugin that removes erroneous memories caused by biases in CL, enhancing performance in both new and old tasks. ErrorEraser consists of two modules: Error Identification and Error Erasure. The former learns the probability density distribution of task data in the feature space without prior knowledge, enabling accurate identification of potentially biased samples. The latter ensures only erroneous knowledge is erased by shifting the decision space of representative outlier samples. Additionally, an incremental feature distribution learning strategy is designed to reduce the resource overhead during error identification in downstream tasks. Extensive experimental results show that ErrorEraser significantly mitigates the negative impact of data biases, achieving higher accuracy and lower forgetting rates across three types of CL methods. The code is available at https://github.com/diadai/ErrorEraser.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ErrorEraserï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ä¸»åŠ¨é—å¿˜æ•°æ®åå·®æ¥ä¼˜åŒ–æŒç»­å­¦ä¹ (Continual Learning)æ€§èƒ½çš„é€šç”¨æ’ä»¶ã€‚ç ”ç©¶å‘ç°ç°å®ä¸–ç•Œæ•°æ®ä¸­çš„åå·®ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°è™šå‡ç›¸å…³æ€§ï¼Œå¹¶åœ¨ä»»åŠ¡é—´ä¼ é€’æ”¾å¤§ï¼Œè¿›è€Œé˜»ç¢çŸ¥è¯†çš„ä¿ç•™ä¸è¿ç§»ã€‚ErrorEraserç”±é”™è¯¯è¯†åˆ«(Error Identification)å’Œé”™è¯¯æ“¦é™¤(Error Erasure)ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼Œèƒ½å¤Ÿåœ¨æ— éœ€å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹è¯†åˆ«å¹¶æ¸…é™¤åå·®äº§ç”Ÿçš„é”™è¯¯è®°å¿†ã€‚å…¶ä¸­è¯†åˆ«æ¨¡å—é€šè¿‡å­¦ä¹ ç‰¹å¾ç©ºé—´çš„æ¦‚ç‡å¯†åº¦åˆ†å¸ƒæ¥å®šä½æ½œåœ¨åå·®æ ·æœ¬ï¼Œè€Œæ“¦é™¤æ¨¡å—åˆ™é€šè¿‡å¹³ç§»ä»£è¡¨æ€§ç¦»ç¾¤æ ·æœ¬çš„å†³ç­–ç©ºé—´æ¥æ¶ˆé™¤é”™è¯¯çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å¢é‡ç‰¹å¾åˆ†å¸ƒå­¦ä¹ ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆé™ä½åœ¨å¤„ç†ä¸‹æ¸¸ä»»åŠ¡æ—¶çš„èµ„æºå¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒErrorEraseråœ¨å¤šç§ç±»å‹çš„CLæ–¹æ³•ä¸Šå‡æ˜¾è‘—æå‡äº†åˆ†ç±»å‡†ç¡®ç‡å¹¶é™ä½äº†é—å¿˜ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.09347v1",
      "published_date": "2025-06-11 02:54:29 UTC",
      "updated_date": "2025-06-11 02:54:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:08:54.393296+00:00"
    },
    {
      "arxiv_id": "2506.09344v1",
      "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
      "title_zh": "Ming-Omniï¼šé¢å‘æ„ŸçŸ¥ä¸ç”Ÿæˆçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹",
      "authors": [
        "Inclusion AI",
        "Biao Gong",
        "Cheng Zou",
        "Chuanyang Zheng",
        "Chunluan Zhou",
        "Canxiang Yan",
        "Chunxiang Jin",
        "Chunjie Shen",
        "Dandan Zheng",
        "Fudong Wang",
        "Furong Xu",
        "GuangMing Yao",
        "Jun Zhou",
        "Jingdong Chen",
        "Jianxin Sun",
        "Jiajia Liu",
        "Jianjiang Zhu",
        "Jun Peng",
        "Kaixiang Ji",
        "Kaiyou Song",
        "Kaimeng Ren",
        "Libin Wang",
        "Lixiang Ru",
        "Lele Xie",
        "Longhua Tan",
        "Lyuxin Xue",
        "Lan Wang",
        "Mochen Bai",
        "Ning Gao",
        "Pei Chen",
        "Qingpei Guo",
        "Qinglong Zhang",
        "Qiang Xu",
        "Rui Liu",
        "Ruijie Xiong",
        "Sirui Gao",
        "Tinghao Liu",
        "Taisong Li",
        "Weilong Chai",
        "Xinyu Xiao",
        "Xiaomei Wang",
        "Xiaoxue Chen",
        "Xiao Lu",
        "Xiaoyu Li",
        "Xingning Dong",
        "Xuzheng Yu",
        "Yi Yuan",
        "Yuting Gao",
        "Yunxiao Sun",
        "Yipeng Chen",
        "Yifei Wu",
        "Yongjie Lyu",
        "Ziping Ma",
        "Zipeng Feng",
        "Zhijiang Fang",
        "Zhihao Qiu",
        "Ziyuan Huang",
        "Zhengyu He"
      ],
      "abstract": "We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Ming-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œåœ¨æ„ŸçŸ¥ä¸ç”Ÿæˆæ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚æ¨¡å‹é€šè¿‡ä¸“é—¨çš„ç¼–ç å™¨æå–ä¸åŒæ¨¡æ€çš„ Tokenï¼Œå¹¶åˆ©ç”¨åä¸º Ling çš„æ··åˆä¸“å®¶æ¶æ„(MoE)åŠæ–°å¢çš„æ¨¡æ€ç‰¹å®šè·¯ç”±(modality-specific routers)å®ç°é«˜æ•ˆå¤„ç†ä¸èåˆã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨ç»Ÿä¸€æ¡†æ¶å†…å®Œæˆå¤šæ ·åŒ–ä»»åŠ¡ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ–ç»“æ„é‡æ„ã€‚Ming-Omni è¿›ä¸€æ­¥é›†æˆäº†é«˜çº§éŸ³é¢‘è§£ç å™¨å’Œ Ming-Lite-Uni æ¨¡å—ï¼Œä½¿å…¶å…·å¤‡è‡ªç„¶è¯­éŸ³åˆæˆã€é«˜è´¨é‡å›¾åƒç”Ÿæˆã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯åŠå›¾åƒç¼–è¾‘ç­‰åŠŸèƒ½ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ¨¡å‹ä¸ºè·¨æ¨¡æ€ç»Ÿä¸€æ„ŸçŸ¥ä¸ç”Ÿæˆæä¾›äº†å¼ºå¤§æ–¹æ¡ˆï¼Œæ˜¯é¦–ä¸ªåœ¨æ¨¡æ€æ”¯æŒä¸Šæ¯”è‚© GPT-4o çš„å¼€æºæ¨¡å‹ã€‚ç›®å‰è¯¥é¡¹ç›®çš„ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¨éƒ¨å¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›å­¦æœ¯ç¤¾åŒºçš„è¿›ä¸€æ­¥å¼€å‘ä¸ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages,8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09344v1",
      "published_date": "2025-06-11 02:50:49 UTC",
      "updated_date": "2025-06-11 02:50:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:00.853443+00:00"
    },
    {
      "arxiv_id": "2506.09342v2",
      "title": "Latent Multi-Head Attention for Small Language Models",
      "title_zh": "é¢å‘å°è¯­è¨€æ¨¡å‹çš„éšå¼å¤šå¤´æ³¨æ„åŠ›",
      "authors": [
        "Sushant Mehta",
        "Raj Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "We present the first comprehensive study of latent multi-head attention (MLA) for small language models, revealing interesting efficiency-quality trade-offs. Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory reduction while incurring only a 0.3% increase in validation loss (essentially matching MHA quality)- a Pareto improvement for memory constrained deployment. We further show that RoPE is crucial for MLA in small models: without it, MLA underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by 2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2 achieves a 1.4 times speedup over full-rank MLA while maintaining the memory savings. GPT-4 evaluations corroborate perplexity results, with ours achieving the highest quality scores (7.4/10) across grammar, creativity, and consistency metrics. Code and models will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å°è¯­è¨€æ¨¡å‹(Small Language Models)ä¸­çš„æ½œåœ¨å¤šå¤´æ³¨æ„åŠ›(Latent Multi-Head Attention, MLA)è¿›è¡Œäº†é¦–æ¬¡å…¨é¢æ¢è®¨ï¼Œæ­ç¤ºäº†å…¶åœ¨æ•ˆç‡ä¸è´¨é‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚é€šè¿‡åœ¨30Må‚æ•°çš„GPTæ¨¡å‹ä¸Šå¯¹æ¯”æ ‡å‡†MHAã€MLAä»¥åŠç»“åˆæ—‹è½¬ä½ç½®ç¼–ç çš„MLA+RoPEï¼Œç ”ç©¶å‘ç°é‡‡ç”¨åŠç§©æ½œç»´åº¦çš„MLA+RoPEèƒ½åœ¨ä»…å¢åŠ 0.3%éªŒè¯æŸå¤±çš„æƒ…å†µä¸‹ï¼Œå‡å°‘45%çš„KV-cacheå†…å­˜å ç”¨ã€‚å®éªŒè¯æ˜RoPEå¯¹å°æ¨¡å‹ä¸­çš„MLAè‡³å…³é‡è¦ï¼ŒåŠ å…¥RoPEåå…¶æ€§èƒ½å¯è¶…è¶ŠåŸå§‹æ³¨æ„åŠ›æœºåˆ¶2%ï¼Œè€Œç¼ºå¤±RoPEåˆ™ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æ¨ç†åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒåŠç§©MLAåœ¨ä¿æŒå†…å­˜èŠ‚çœçš„åŒæ—¶ï¼Œæ¯”å…¨ç§©ç‰ˆæœ¬å®ç°äº†1.4å€çš„æé€Ÿã€‚GPT-4çš„è¯„ä¼°ç»“æœè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¶æ„åœ¨è¯­æ³•ã€åˆ›æ„å’Œè¿è´¯æ€§ä¸Šçš„é«˜è´¨é‡è¡¨ç°ï¼Œä¸ºå†…å­˜å—é™çš„éƒ¨ç½²åœºæ™¯æä¾›äº†å¸•ç´¯æ‰˜æ”¹è¿›(Pareto improvement)æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 1 figure. 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.09342v2",
      "published_date": "2025-06-11 02:48:16 UTC",
      "updated_date": "2025-06-16 02:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:07.225024+00:00"
    },
    {
      "arxiv_id": "2506.09340v1",
      "title": "RePO: Replay-Enhanced Policy Optimization",
      "title_zh": "RePOï¼šå›æ”¾å¢å¼ºç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Siheng Li",
        "Zhanhui Zhou",
        "Wai Lam",
        "Chao Yang",
        "Chaochao Lu"
      ],
      "abstract": "Reinforcement learning (RL) is vital for optimizing large language models (LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages using multiple on-policy outputs per prompt, leading to high computational costs and low data efficiency. To address this, we introduce Replay-Enhanced Policy Optimization (RePO), which leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, allowing policy optimization based on a broader and more diverse set of samples for each prompt. Experiments on five LLMs across seven mathematical reasoning benchmarks demonstrate that RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further analysis indicates that RePO increases computational cost by $15\\%$ while raising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B, with both on-policy and off-policy sample numbers set to $8$. The repository can be accessed at https://github.com/SihengLi99/RePO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Replay-Enhanced Policy Optimization (RePO)ï¼Œæ—¨åœ¨è§£å†³Group Relative Policy Optimization (GRPO)åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œä½æ•°æ®æ•ˆç‡æŒ‘æˆ˜ã€‚RePOé€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„å›æ”¾ç­–ç•¥ï¼Œä»å›æ”¾ç¼“å†²åŒº(Replay Buffer)æ£€ç´¢ç¦»ç­–(Off-policy)æ ·æœ¬ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªæç¤º(Prompt)åˆ©ç”¨æ›´å¹¿æ³›ä¸”å¤šæ ·åŒ–çš„æ•°æ®è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚åœ¨äº”ä¸ªå¤§è¯­è¨€æ¨¡å‹å’Œä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRePOç›¸è¾ƒäºGRPOåœ¨Qwen2.5-Math-1.5Bå’ŒQwen3-1.7Bä¸Šåˆ†åˆ«å®ç°äº†18.4å’Œ4.1ä¸ªç™¾åˆ†ç‚¹çš„å¹³å‡æ€§èƒ½æå‡ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨åŒç­‰æ ·æœ¬é…ç½®ä¸‹ï¼ŒRePOè™½ç„¶ä»…å¢åŠ äº†15%çš„è®¡ç®—å¼€é”€ï¼Œå´å°†æœ‰æ•ˆä¼˜åŒ–æ­¥æ•°æ˜¾è‘—æå‡äº†48%ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¼˜åŒ–æ•ˆç‡æ–¹é¢çš„å“è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Page: https://github.com/SihengLi99/RePO",
      "pdf_url": "https://arxiv.org/pdf/2506.09340v1",
      "published_date": "2025-06-11 02:44:10 UTC",
      "updated_date": "2025-06-11 02:44:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:08.289410+00:00"
    },
    {
      "arxiv_id": "2506.09338v2",
      "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models",
      "title_zh": "çŸ¥å…¶æ‰€ä¸çŸ¥ï¼šè¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ ¡å‡†",
      "authors": [
        "Young-Jin Park",
        "Kristjan Greenewald",
        "Kaveh Alim",
        "Hao Wang",
        "Navid Azizan"
      ],
      "abstract": "Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach -- performed via quantile regression -- that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \\emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(Process Reward Models, PRMs)å­˜åœ¨çš„æ ¡å‡†åå·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåˆ†ä½æ•°å›å½’(quantile regression)çš„ç¡®å®šæ€§æ ¡å‡†æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰PRMså¾€å¾€ä¼šé«˜ä¼°éƒ¨åˆ†æ¨ç†æ­¥éª¤å¼•å¯¼è‡³æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå°æ¨¡å‹è¡¥å…¨æ¨ç†è½¨è¿¹æ—¶è¡¨ç°å°¤ä¸ºæ˜æ˜¾ã€‚é€šè¿‡è¯¥æ ¡å‡†æ–¹æ³•ï¼ŒPRMçš„è¾“å‡ºèƒ½æ›´å¥½åœ°ä¸çœŸå®æˆåŠŸæ¦‚ç‡å¯¹é½ï¼Œå¹¶ç”Ÿæˆç›¸å…³çš„ç½®ä¿¡åŒºé—´ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†å®ä¾‹è‡ªé€‚åº”ç¼©æ”¾(Instance-Adaptive Scaling, IAS)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯æ ¹æ®é¢„ä¼°çš„æˆåŠŸç‡åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œè€Œéé‡‡ç”¨ä¼ ç»Ÿçš„å›ºå®šè½¨è¿¹åˆ†é…æ–¹å¼ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ ¡å‡†æ–¹æ³•æ˜¾è‘—é™ä½äº†é¢„æµ‹è¯¯å·®ï¼Œä¸”IASç­–ç•¥èƒ½å¤Ÿåœ¨ç»´æŒç­”æ¡ˆå‡†ç¡®æ€§çš„å‰æä¸‹æœ‰æ•ˆå‡å°‘æ¨ç†æˆæœ¬ï¼Œå®ç°äº†è®¡ç®—èµ„æºçš„ä¼˜åŒ–é…ç½®ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09338v2",
      "published_date": "2025-06-11 02:39:26 UTC",
      "updated_date": "2025-11-07 14:35:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:07.365903+00:00"
    },
    {
      "arxiv_id": "2506.09335v1",
      "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds",
      "title_zh": "æ¶Œç°çŸ¥è¯†æ™ºèƒ½ç³»ç»Ÿï¼šäº¿ä¸‡å¿ƒæ™ºçš„ååŒæ¶æ„",
      "authors": [
        "Moshi Wei",
        "Sparks Li"
      ],
      "abstract": "The Intelligent System of Emergent Knowledge (ISEK) establishes a decentralized network where human and artificial intelligence agents collaborate as peers, forming a self-organizing cognitive ecosystem. Built on Web3 infrastructure, ISEK combines three fundamental principles: (1) a decentralized multi-agent architecture resistant to censorship, (2) symbiotic AI-human collaboration with equal participation rights, and (3) resilient self-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for dynamic task allocation, supported by robust fault tolerance and a multidimensional reputation system. Economic incentives are governed by the native $ISEK token, facilitating micropayments, governance participation, and reputation tracking, while agent sovereignty is maintained through NFT-based identity management.\n  This synthesis of blockchain technology, artificial intelligence, and incentive engineering creates an infrastructure that actively facilitates emergent intelligence. ISEK represents a paradigm shift from conventional platforms, enabling the organic development of large-scale, decentralized cognitive systems where autonomous agents collectively evolve beyond centralized constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç´§æ€¥çŸ¥è¯†æ™ºèƒ½ç³»ç»Ÿ(Intelligent System of Emergent Knowledge, ISEK)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºWeb3åŸºç¡€è®¾æ–½çš„å»ä¸­å¿ƒåŒ–ç½‘ç»œï¼Œæ—¨åœ¨è®©äººå‘˜ä¸äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(AI agents)ä½œä¸ºå¹³ç­‰ä¼™ä¼´è¿›è¡Œåä½œï¼Œå½¢æˆè‡ªç»„ç»‡çš„è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æŠ—å®¡æŸ¥çš„å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“æ¶æ„ã€äººæœºå…±ç”Ÿåä½œä»¥åŠé€šè¿‡åˆ†å¸ƒå¼å…±è¯†å®ç°çš„è‡ªé€‚åº”æ€§ç­‰æ ¸å¿ƒåŸåˆ™ã€‚ISEKå¼•å…¥äº†ä¸€ç§åŒ…å«å‘å¸ƒ(Publish)ã€å‘ç°(Discover)ã€æ‹›å‹Ÿ(Recruit)ã€æ‰§è¡Œ(Execute)ã€ç»“ç®—(Settle)å’Œåé¦ˆ(Feedback)å…­ä¸ªé˜¶æ®µçš„åˆ›æ–°åè°ƒåè®®ï¼Œä»¥æ”¯æŒåŠ¨æ€ä»»åŠ¡åˆ†é…å’Œå¤šç»´ä¿¡èª‰è¯„ä»·ã€‚ç³»ç»Ÿé€šè¿‡åŸç”Ÿä»£å¸$ISEKé©±åŠ¨ç»æµæ¿€åŠ±ä¸æ²»ç†ï¼Œå¹¶åˆ©ç”¨åŸºäºNFTçš„èº«ä»½ç®¡ç†æ¥ä¿éšœæ™ºèƒ½ä½“ä¸»æƒã€‚è¿™ç§æ•´åˆäº†åŒºå—é“¾æŠ€æœ¯ã€äººå·¥æ™ºèƒ½ä¸æ¿€åŠ±å·¥ç¨‹çš„æ¶æ„ï¼Œæ ‡å¿—ç€ä»ä¼ ç»Ÿä¸­å¿ƒåŒ–å¹³å°å‘å¤§è§„æ¨¡åˆ†å¸ƒå¼è®¤çŸ¥ç³»ç»Ÿçš„èŒƒå¼è½¬å˜ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ƒè¿›æ¶Œç°æ™ºèƒ½(emergent intelligence)çš„æœ‰æœºæ¼”åŒ–ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "11 pages, 1 figures,",
      "pdf_url": "https://arxiv.org/pdf/2506.09335v1",
      "published_date": "2025-06-11 02:28:05 UTC",
      "updated_date": "2025-06-11 02:28:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:27.787146+00:00"
    },
    {
      "arxiv_id": "2506.11140v3",
      "title": "Autonomous Computer Vision Development with Agentic AI",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“ AI çš„è‡ªä¸»è®¡ç®—æœºè§†è§‰å¼€å‘",
      "authors": [
        "Jin Kim",
        "Muhammad Wahi-Anwa",
        "Sangyun Park",
        "Shawn Shin",
        "John M. Hoffman",
        "Matthew S. Brown"
      ],
      "abstract": "Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, \"provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)\"), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“AI (Agentic AI) è‡ªä¸»æ„å»ºè®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„æ½œåŠ›ã€‚é€šè¿‡å°†å¼€æºè®¤çŸ¥AIç¯å¢ƒ SimpleMind (SM) ä¸åŸºäº OpenManus çš„æ™ºèƒ½ä½“ç›¸ç»“åˆï¼Œå®ç°äº†é’ˆå¯¹ç‰¹å®šè§†è§‰ä»»åŠ¡çš„è‡ªåŠ¨åŒ–è§„åˆ’ä¸å·¥å…·é…ç½®ã€‚æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªä¸»ç”Ÿæˆ YAML æ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼Œå¹¶å…¨è‡ªåŠ¨æ‰§è¡Œ SM-Learn è®­ç»ƒå’Œ SM-Think æ¨ç†æµç¨‹ã€‚åœ¨é’ˆå¯¹èƒ¸éƒ¨Xå…‰ç‰‡(CXR)çš„è‚ºéƒ¨ã€å¿ƒè„å’Œè‚‹éª¨åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨50å¼ å›¾åƒä¸Šåˆ†åˆ«å®ç°äº†0.96ã€0.82å’Œ0.83çš„å¹³å‡ Dice Scoreã€‚è¿™é¡¹å·¥ä½œæˆåŠŸå±•ç¤ºäº† Agentic AI åœ¨è‡ªä¸»è§„åˆ’å’Œå·¥å…·é…ç½®æ–¹é¢çš„èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨ä¼ ç»Ÿä¸Šç”±æ•°æ®ç§‘å­¦å®¶ä¸»å¯¼çš„è®¡ç®—æœºè§†è§‰åº”ç”¨å¼€å‘æµç¨‹ä¸­çš„æ›¿ä»£æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "The paper is 13 pages long and contains 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.11140v3",
      "published_date": "2025-06-11 02:21:19 UTC",
      "updated_date": "2025-06-19 21:19:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:23.432469+00:00"
    },
    {
      "arxiv_id": "2506.09331v2",
      "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation",
      "title_zh": "å¤šæ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹ï¼šæ¨è¿›åˆä½œã€åè°ƒä¸é€‚åº”æ€§",
      "authors": [
        "Arjun Vaithilingam Sudhakar"
      ],
      "abstract": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡å¿ƒç†ç†è®º(Theory of Mind)ï¼Œå³æ¨¡æ‹Ÿå’Œæ¨ç†ä»–äººæ„å›¾çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºå®ç°æœ‰æ•ˆçš„åä½œè‡³å…³é‡è¦ã€‚ä½œè€…ä»åˆä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Cooperative Multi-Agent Reinforcement Learning, MARL)çš„è§†è§’å‡ºå‘ï¼Œé€šè¿‡é‡å¤äº¤äº’ä½¿æ™ºèƒ½ä½“å­¦ä¹ å¦‚ä½•è¿›è¡Œåä½œï¼Œä»è€Œæ¨¡æ‹Ÿäººç±»çš„ç¤¾ä¼šæ¨ç†ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æå‡äººå·¥æ™ºèƒ½ä½“åœ¨ä¸äººç±»æˆ–å…¶å®ƒæ™ºèƒ½ä½“äº¤äº’è¿‡ç¨‹ä¸­çš„é€‚åº”æ€§ä¸åä½œæ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨æ”¯æŒè‡ªç„¶è¯­è¨€äº¤äº’çš„LLMæ™ºèƒ½ä½“ï¼Œè¯¥å·¥ä½œå±•ç¤ºäº†æ„å»ºæ— ç¼åä½œçš„æ··åˆäººæœºç³»ç»Ÿ(Hybrid Human-AI Systems)çš„æ½œåŠ›ã€‚è¿™ä¸€æ¢ç´¢ä¸ä»…æ·±åŒ–äº†å¯¹å¤§è¯­è¨€æ¨¡å‹ç¤¾ä¼šæ™ºèƒ½çš„ç†è§£ï¼Œä¹Ÿä¸ºæœªæ¥å¤æ‚çš„å¤šæ™ºèƒ½ä½“äº¤äº’åœºæ™¯æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687",
      "pdf_url": "https://arxiv.org/pdf/2506.09331v2",
      "published_date": "2025-06-11 02:12:34 UTC",
      "updated_date": "2025-06-17 23:22:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:31.470396+00:00"
    },
    {
      "arxiv_id": "2506.09315v1",
      "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models",
      "title_zh": "åŸºäºé…å¯¹å¤§è¯­è¨€æ¨¡å‹å›°æƒ‘åº¦çš„é˜¿å°”èŒ¨æµ·é»˜å‹ç—´å‘†æ£€æµ‹",
      "authors": [
        "Yao Xiao",
        "Heidi Christensen",
        "Stefan Goetze"
      ],
      "abstract": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive decline that commonly impacts language ability. This work extends the paired perplexity approach to detecting AD by using a recent large language model (LLM), the instruction-following version of Mistral-7B. We improve accuracy by an average of 3.33% over the best current paired perplexity method and by 6.35% over the top-ranked method from the ADReSS 2020 challenge benchmark. Our further analysis demonstrates that the proposed approach can effectively detect AD with a clear and interpretable decision boundary in contrast to other methods that suffer from opaque decision-making processes. Finally, by prompting the fine-tuned LLMs and comparing the model-generated responses to human responses, we illustrate that the LLMs have learned the special language patterns of AD speakers, which opens up possibilities for novel methods of model interpretation and data augmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æˆå¯¹å›°æƒ‘åº¦(paired perplexity)æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…(AD)çš„æ–°æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨äº†æŒ‡ä»¤éµå¾ªç‰ˆæœ¬çš„Mistral-7Bæ¨¡å‹è¿›è¡Œæ‰©å±•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ADReSS 2020åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåˆ†åˆ«æ¯”æœ€ä¼˜æˆå¯¹å›°æƒ‘åº¦æ³•å’ŒæŒ‘æˆ˜èµ›å† å†›æ–¹æ³•æå‡äº†3.33%å’Œ6.35%ã€‚ä¸åŒäºä»¥å¾€å†³ç­–è¿‡ç¨‹ä¸é€æ˜çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆæä¾›äº†æ¸…æ™°ä¸”å¯è§£é‡Šçš„å†³ç­–è¾¹ç•Œ(decision boundary)ã€‚é€šè¿‡åˆ†ææ¨¡å‹ç”Ÿæˆçš„å“åº”ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯å®LLMså·²æ•è·ADæ‚£è€…ç‰¹æœ‰çš„è¯­è¨€æ¨¡å¼(language patterns)ã€‚è¿™ä¸€æˆæœä¸ä»…æé«˜äº†æ£€æµ‹çš„ç²¾ç¡®åº¦ï¼Œè¿˜ä¸ºæ¨¡å‹è§£é‡Šå’Œæ•°æ®å¢å¼º(data augmentation)æä¾›äº†å…¨æ–°çš„è§†è§’ä¸å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in the proceedings of Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.09315v1",
      "published_date": "2025-06-11 01:15:13 UTC",
      "updated_date": "2025-06-11 01:15:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:09:33.612548+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 144,
  "processed_papers_count": 144,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T21:10:30.054233+00:00"
}
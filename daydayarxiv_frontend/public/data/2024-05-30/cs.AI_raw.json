[
  {
    "arxiv_id": "2405.20535v2",
    "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning",
    "authors": [
      "Xinlu Zhang",
      "Zhiyu Zoey Chen",
      "Xi Ye",
      "Xianjun Yang",
      "Lichang Chen",
      "William Yang Wang",
      "Linda Ruth Petzold"
    ],
    "abstract": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20535v2",
    "published_date": "2024-05-30 23:20:25 UTC",
    "updated_date": "2024-12-12 18:45:33 UTC"
  },
  {
    "arxiv_id": "2405.20529v1",
    "title": "An Automatic Question Usability Evaluation Toolkit",
    "authors": [
      "Steven Moore",
      "Eamon Costello",
      "Huy A. Nguyen",
      "John Stamper"
    ],
    "abstract": "Evaluating multiple-choice questions (MCQs) involves either labor intensive\nhuman assessments or automated methods that prioritize readability, often\noverlooking deeper question design flaws. To address this issue, we introduce\nthe Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an\nopen-source tool that leverages the Item-Writing Flaws (IWF) rubric for a\ncomprehensive and automated quality evaluation of MCQs. By harnessing the\nlatest in large language models such as GPT-4, advanced word embeddings, and\nTransformers designed to analyze textual complexity, SAQUET effectively\npinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the\ndiscrepancy between commonly used automated evaluation metrics and the human\nassessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs\nacross the five domains of Chemistry, Statistics, Computer Science, Humanities,\nand Healthcare, showing how it effectively distinguishes between flawed and\nflawless questions, providing a level of analysis beyond what is achievable\nwith traditional metrics. With an accuracy rate of over 94% in detecting the\npresence of flaws identified by human evaluators, our findings emphasize the\nlimitations of existing evaluation methods and showcase potential in improving\nthe quality of educational assessments.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Artificial Intelligence in Education 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20529v1",
    "published_date": "2024-05-30 23:04:53 UTC",
    "updated_date": "2024-05-30 23:04:53 UTC"
  },
  {
    "arxiv_id": "2405.20527v1",
    "title": "Towards Ontology-Enhanced Representation Learning for Large Language Models",
    "authors": [
      "Francesco Ronzano",
      "Jay Nanavati"
    ],
    "abstract": "Taking advantage of the widespread use of ontologies to organise and\nharmonize knowledge across several distinct domains, this paper proposes a\nnovel approach to improve an embedding-Large Language Model (embedding-LLM) of\ninterest by infusing the knowledge formalized by a reference ontology:\nontological knowledge infusion aims at boosting the ability of the considered\nLLM to effectively model the knowledge domain described by the infused\nontology. The linguistic information (i.e. concept synonyms and descriptions)\nand structural information (i.e. is-a relations) formalized by the ontology are\nutilized to compile a comprehensive set of concept definitions, with the\nassistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept\ndefinitions are then employed to fine-tune the target embedding-LLM using a\ncontrastive learning framework. To demonstrate and evaluate the proposed\napproach, we utilize the biomedical disease ontology MONDO. The results show\nthat embedding-LLMs enhanced by ontological disease knowledge exhibit an\nimproved capability to effectively evaluate the similarity of in-domain\nsentences from biomedical documents mentioning diseases, without compromising\ntheir out-of-domain performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7; I.2.6"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2405.20527v1",
    "published_date": "2024-05-30 23:01:10 UTC",
    "updated_date": "2024-05-30 23:01:10 UTC"
  },
  {
    "arxiv_id": "2405.20526v1",
    "title": "Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions",
    "authors": [
      "Steven Moore",
      "Robin Schmucker",
      "Tom Mitchell",
      "John Stamper"
    ],
    "abstract": "Knowledge Components (KCs) linked to assessments enhance the measurement of\nstudent learning, enrich analytics, and facilitate adaptivity. However,\ngenerating and linking KCs to assessment items requires significant effort and\ndomain-specific knowledge. To streamline this process for higher-education\ncourses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs)\nin Chemistry and E-Learning. We analyzed discrepancies between the KCs\ngenerated by the Large Language Model (LLM) and those made by humans through\nevaluation from three domain experts in each subject area. This evaluation\naimed to determine whether, in instances of non-matching KCs, evaluators showed\na preference for the LLM-generated KCs over their human-created counterparts.\nWe also developed an ontology induction algorithm to cluster questions that\nassess similar KCs based on their content. Our most effective LLM strategy\naccurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with\neven higher success when considering the top five KC suggestions. Human\nevaluators favored LLM-generated KCs, choosing them over human-assigned ones\napproximately two-thirds of the time, a preference that was statistically\nsignificant across both domains. Our clustering algorithm successfully grouped\nquestions by their underlying KCs without needing explicit labels or contextual\ninformation. This research advances the automation of KC generation and\nclassification for assessment items, alleviating the need for student data or\npredefined KC labels.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Learning @ Scale 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20526v1",
    "published_date": "2024-05-30 22:57:49 UTC",
    "updated_date": "2024-05-30 22:57:49 UTC"
  },
  {
    "arxiv_id": "2405.20519v1",
    "title": "Diffusion On Syntax Trees For Program Synthesis",
    "authors": [
      "Shreyas Kapur",
      "Erik Jenner",
      "Stuart Russell"
    ],
    "abstract": "Large language models generate code one token at a time. Their autoregressive\ngeneration process lacks the feedback of observing the program's output.\nTraining LLMs to suggest edits directly can be challenging due to the scarcity\nof rich edit data. To address these problems, we propose neural diffusion\nmodels that operate on syntax trees of any context-free grammar. Similar to\nimage diffusion models, our method also inverts ``noise'' applied to syntax\ntrees. Rather than generating code sequentially, we iteratively edit it while\npreserving syntactic validity, which makes it easy to combine this neural model\nwith search. We apply our approach to inverse graphics tasks, where our model\nlearns to convert images into programs that produce those images. Combined with\nsearch, our model is able to write graphics programs, see the execution result,\nand debug them to meet the required specifications. We additionally show how\nour system can write graphics programs for hand-drawn sketches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "https://tree-diffusion.github.io",
    "pdf_url": "http://arxiv.org/pdf/2405.20519v1",
    "published_date": "2024-05-30 22:31:16 UTC",
    "updated_date": "2024-05-30 22:31:16 UTC"
  },
  {
    "arxiv_id": "2405.20513v2",
    "title": "Deep Modeling of Non-Gaussian Aleatoric Uncertainty",
    "authors": [
      "Aastha Acharya",
      "Caleb Lee",
      "Marissa D'Alonzo",
      "Jared Shamwell",
      "Nisar R. Ahmed",
      "Rebecca Russell"
    ],
    "abstract": "Deep learning offers promising new ways to accurately model aleatoric\nuncertainty in robotic state estimation systems, particularly when the\nuncertainty distributions do not conform to traditional assumptions of being\nfixed and Gaussian. In this study, we formulate and evaluate three fundamental\ndeep learning approaches for conditional probability density modeling to\nquantify non-Gaussian aleatoric uncertainty: parametric, discretized, and\ngenerative modeling. We systematically compare the respective strengths and\nweaknesses of these three methods on simulated non-Gaussian densities as well\nas on real-world terrain-relative navigation data. Our results show that these\ndeep learning methods can accurately capture complex uncertainty patterns,\nhighlighting their potential for improving the reliability and robustness of\nestimation systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20513v2",
    "published_date": "2024-05-30 22:13:17 UTC",
    "updated_date": "2025-02-27 16:35:59 UTC"
  },
  {
    "arxiv_id": "2405.20501v1",
    "title": "ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane",
    "authors": [
      "Shivendra Agrawal",
      "Suresh Nayak",
      "Ashutosh Naik",
      "Bradley Hayes"
    ],
    "abstract": "The ability to shop independently, especially in grocery stores, is important\nfor maintaining a high quality of life. This can be particularly challenging\nfor people with visual impairments (PVI). Stores carry thousands of products,\nwith approximately 30,000 new products introduced each year in the US market\nalone, presenting a challenge even for modern computer vision solutions.\nThrough this work, we present a proof-of-concept socially assistive robotic\nsystem we call ShelfHelp, and propose novel technical solutions for enhancing\ninstrumented canes traditionally meant for navigation tasks with additional\ncapability within the domain of shopping. ShelfHelp includes a novel visual\nproduct locator algorithm designed for use in grocery stores and a novel\nplanner that autonomously issues verbal manipulation guidance commands to guide\nthe user during product retrieval. Through a human subjects study, we show the\nsystem's success in locating and providing effective manipulation guidance to\nretrieve desired products with novice users. We compare two autonomous verbal\nguidance modes achieving comparable performance to a human assistance baseline\nand present encouraging findings that validate our system's efficiency and\neffectiveness and through positive subjective metrics including competence,\nintelligence, and ease of use.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 14 figures and charts",
    "pdf_url": "http://arxiv.org/pdf/2405.20501v1",
    "published_date": "2024-05-30 21:42:54 UTC",
    "updated_date": "2024-05-30 21:42:54 UTC"
  },
  {
    "arxiv_id": "2405.20494v2",
    "title": "Slight Corruption in Pre-training Data Makes Better Diffusion Models",
    "authors": [
      "Hao Chen",
      "Yujin Han",
      "Diganta Misra",
      "Xiang Li",
      "Kai Hu",
      "Difan Zou",
      "Masashi Sugiyama",
      "Jindong Wang",
      "Bhiksha Raj"
    ],
    "abstract": "Diffusion models (DMs) have shown remarkable capabilities in generating\nrealistic high-quality images, audios, and videos. They benefit significantly\nfrom extensive pre-training on large-scale datasets, including web-crawled data\nwith paired data and conditions, such as image-text and image-class pairs.\nDespite rigorous filtering, these pre-training datasets often inevitably\ncontain corrupted pairs where conditions do not accurately describe the data.\nThis paper presents the first comprehensive study on the impact of such\ncorruption in pre-training data of DMs. We synthetically corrupt ImageNet-1K\nand CC3M to pre-train and evaluate over 50 conditional DMs. Our empirical\nfindings reveal that various types of slight corruption in pre-training can\nsignificantly enhance the quality, diversity, and fidelity of the generated\nimages across different DMs, both during pre-training and downstream adaptation\nstages. Theoretically, we consider a Gaussian mixture model and prove that\nslight corruption in the condition leads to higher entropy and a reduced\n2-Wasserstein distance to the ground truth of the data distribution generated\nby the corruptly trained DMs. Inspired by our analysis, we propose a simple\nmethod to improve the training of DMs on practical datasets by adding condition\nembedding perturbations (CEP). CEP significantly improves the performance of\nvarious DMs in both pre-training and downstream tasks. We hope that our study\nprovides new insights into understanding the data and pre-training processes of\nDMs and all models are released at https://huggingface.co/DiffusionNoise.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2405.20494v2",
    "published_date": "2024-05-30 21:35:48 UTC",
    "updated_date": "2024-10-30 13:52:56 UTC"
  },
  {
    "arxiv_id": "2405.20487v1",
    "title": "Probabilities of Causation for Continuous and Vector Variables",
    "authors": [
      "Yuta Kawakami",
      "Manabu Kuroki",
      "Jin Tian"
    ],
    "abstract": "Probabilities of causation (PoC) are valuable concepts for explainable\nartificial intelligence and practical decision-making. PoC are originally\ndefined for scalar binary variables. In this paper, we extend the concept of\nPoC to continuous treatment and outcome variables, and further generalize PoC\nto capture causal effects between multiple treatments and multiple outcomes. In\naddition, we consider PoC for a sub-population and PoC with multi-hypothetical\nterms to capture more sophisticated counterfactual information useful for\ndecision-making. We provide a nonparametric identification theorem for each\ntype of PoC we introduce. Finally, we illustrate the application of our results\non a real-world dataset about education.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20487v1",
    "published_date": "2024-05-30 21:22:26 UTC",
    "updated_date": "2024-05-30 21:22:26 UTC"
  },
  {
    "arxiv_id": "2406.02585v1",
    "title": "Contextual Counting: A Mechanistic Study of Transformers on a Quantitative Task",
    "authors": [
      "Siavash Golkar",
      "Alberto Bietti",
      "Mariel Pettee",
      "Michael Eickenberg",
      "Miles Cranmer",
      "Keiya Hirashima",
      "Geraud Krawezik",
      "Nicholas Lourie",
      "Michael McCabe",
      "Rudy Morel",
      "Ruben Ohana",
      "Liam Holden Parker",
      "Bruno Régaldo-Saint Blancard",
      "Kyunghyun Cho",
      "Shirley Ho"
    ],
    "abstract": "Transformers have revolutionized machine learning across diverse domains, yet\nunderstanding their behavior remains crucial, particularly in high-stakes\napplications. This paper introduces the contextual counting task, a novel toy\nproblem aimed at enhancing our understanding of Transformers in quantitative\nand scientific contexts. This task requires precise localization and\ncomputation within datasets, akin to object detection or region-based\nscientific analysis. We present theoretical and empirical analysis using both\ncausal and non-causal Transformer architectures, investigating the influence of\nvarious positional encodings on performance and interpretability. In\nparticular, we find that causal attention is much better suited for the task,\nand that no positional embeddings lead to the best accuracy, though rotary\nembeddings are competitive and easier to train. We also show that out of\ndistribution performance is tightly linked to which tokens it uses as a bias\nterm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02585v1",
    "published_date": "2024-05-30 20:52:23 UTC",
    "updated_date": "2024-05-30 20:52:23 UTC"
  },
  {
    "arxiv_id": "2406.02583v2",
    "title": "Exploring the Potential of Polynomial Basis Functions in Kolmogorov-Arnold Networks: A Comparative Study of Different Groups of Polynomials",
    "authors": [
      "Seyd Teymoor Seydi"
    ],
    "abstract": "This paper presents a comprehensive survey of 18 distinct polynomials and\ntheir potential applications in Kolmogorov-Arnold Network (KAN) models as an\nalternative to traditional spline-based methods. The polynomials are classified\ninto various groups based on their mathematical properties, such as orthogonal\npolynomials, hypergeometric polynomials, q-polynomials, Fibonacci-related\npolynomials, combinatorial polynomials, and number-theoretic polynomials. The\nstudy aims to investigate the suitability of these polynomials as basis\nfunctions in KAN models for complex tasks like handwritten digit classification\non the MNIST dataset. The performance metrics of the KAN models, including\noverall accuracy, Kappa, and F1 score, are evaluated and compared. The\nGottlieb-KAN model achieves the highest performance across all metrics,\nsuggesting its potential as a suitable choice for the given task. However,\nfurther analysis and tuning of these polynomials on more complex datasets are\nnecessary to fully understand their capabilities in KAN models. The source code\nfor the implementation of these KAN models is available at\nhttps://github.com/seydi1370/Basis_Functions .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02583v2",
    "published_date": "2024-05-30 20:40:16 UTC",
    "updated_date": "2024-10-14 01:58:57 UTC"
  },
  {
    "arxiv_id": "2405.20465v1",
    "title": "ENTIRe-ID: An Extensive and Diverse Dataset for Person Re-Identification",
    "authors": [
      "Serdar Yildiz",
      "Ahmet Nezih Kasim"
    ],
    "abstract": "The growing importance of person reidentification in computer vision has\nhighlighted the need for more extensive and diverse datasets. In response, we\nintroduce the ENTIRe-ID dataset, an extensive collection comprising over 4.45\nmillion images from 37 different cameras in varied environments. This dataset\nis uniquely designed to tackle the challenges of domain variability and model\ngeneralization, areas where existing datasets for person re-identification have\nfallen short. The ENTIRe-ID dataset stands out for its coverage of a wide array\nof real-world scenarios, encompassing various lighting conditions, angles of\nview, and diverse human activities. This design ensures a realistic and robust\ntraining platform for ReID models. The ENTIRe-ID dataset is publicly available\nat https://serdaryildiz.github.io/ENTIRe-ID",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 2024 18th International Conference on Automatic Face and\n  Gesture Recognition (FG)",
    "pdf_url": "http://arxiv.org/pdf/2405.20465v1",
    "published_date": "2024-05-30 20:26:47 UTC",
    "updated_date": "2024-05-30 20:26:47 UTC"
  },
  {
    "arxiv_id": "2405.20450v1",
    "title": "Decentralized AI: Permissionless LLM Inference on POKT Network",
    "authors": [
      "Daniel Olshansky",
      "Ramiro Rodriguez Colmeiro",
      "Bowen Li"
    ],
    "abstract": "POKT Network's decentralized Remote Procedure Call (RPC) infrastructure,\nsurpassing 740 billion requests since launching on MainNet in 2020, is\nwell-positioned to extend into providing AI inference services with minimal\ndesign or implementation modifications. This litepaper illustrates how the\nnetwork's open-source and permissionless design aligns incentives among model\nresearchers, hardware operators, API providers and users whom we term model\nSources, Suppliers, Gateways and Applications respectively. Through its Relay\nMining algorithm, POKT creates a transparent marketplace where costs and\nearnings directly reflect cryptographically verified usage. This decentralized\nframework offers large model AI researchers a new avenue to disseminate their\nwork and generate revenue without the complexities of maintaining\ninfrastructure or building end-user products. Supply scales naturally with\ndemand, as evidenced in recent years and the protocol's free market dynamics.\nPOKT Gateways facilitate network growth, evolution, adoption, and quality by\nacting as application-facing load balancers, providing value-added features\nwithout managing LLM nodes directly. This vertically decoupled network, battle\ntested over several years, is set up to accelerate the adoption, operation,\ninnovation and financialization of open-source models. It is the first mature\npermissionless network whose quality of service competes with centralized\nentities set up to provide application grade inference.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20450v1",
    "published_date": "2024-05-30 19:50:07 UTC",
    "updated_date": "2024-05-30 19:50:07 UTC"
  },
  {
    "arxiv_id": "2405.20446v3",
    "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
    "authors": [
      "Maya Anderson",
      "Guy Amit",
      "Abigail Goldsteen"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) systems have shown great promise in\nnatural language processing. However, their reliance on data stored in a\nretrieval database, which may contain proprietary or sensitive information,\nintroduces new privacy concerns. Specifically, an attacker may be able to infer\nwhether a certain text passage appears in the retrieval database by observing\nthe outputs of the RAG system, an attack known as a Membership Inference Attack\n(MIA). Despite the significance of this threat, MIAs against RAG systems have\nyet remained under-explored. This study addresses this gap by introducing an\nefficient and easy-to-use method for conducting MIA against RAG systems. We\ndemonstrate the effectiveness of our attack using two benchmark datasets and\nmultiple generative models, showing that the membership of a document in the\nretrieval database can be efficiently determined through the creation of an\nappropriate prompt in both black-box and gray-box settings. Moreover, we\nintroduce an initial defense strategy based on adding instructions to the RAG\ntemplate, which shows high effectiveness for some datasets and models. Our\nfindings highlight the importance of implementing security countermeasures in\ndeployed RAG systems and developing more advanced defenses to protect the\nprivacy and security of retrieval databases.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "I.2; K.6.5"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20446v3",
    "published_date": "2024-05-30 19:46:36 UTC",
    "updated_date": "2025-02-04 14:35:38 UTC"
  },
  {
    "arxiv_id": "2405.20441v4",
    "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
    "authors": [
      "Dipkamal Bhusal",
      "Md Tanvirul Alam",
      "Le Nguyen",
      "Ashim Mahara",
      "Zachary Lightcap",
      "Rodney Frazier",
      "Romy Fieblinger",
      "Grace Long Torales",
      "Benjamin A. Blakely",
      "Nidhi Rastogi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20441v4",
    "published_date": "2024-05-30 19:35:06 UTC",
    "updated_date": "2024-10-30 14:29:37 UTC"
  },
  {
    "arxiv_id": "2405.20434v1",
    "title": "Facilitating Human-LLM Collaboration through Factuality Scores and Source Attributions",
    "authors": [
      "Hyo Jin Do",
      "Rachel Ostrand",
      "Justin D. Weisz",
      "Casey Dugan",
      "Prasanna Sattigeri",
      "Dennis Wei",
      "Keerthiram Murugesan",
      "Werner Geyer"
    ],
    "abstract": "While humans increasingly rely on large language models (LLMs), they are\nsusceptible to generating inaccurate or false information, also known as\n\"hallucinations\". Technical advancements have been made in algorithms that\ndetect hallucinated content by assessing the factuality of the model's\nresponses and attributing sections of those responses to specific source\ndocuments. However, there is limited research on how to effectively communicate\nthis information to users in ways that will help them appropriately calibrate\ntheir trust toward LLMs. To address this issue, we conducted a scenario-based\nstudy (N=104) to systematically compare the impact of various design strategies\nfor communicating factuality and source attribution on participants' ratings of\ntrust, preferences, and ease in validating response accuracy. Our findings\nreveal that participants preferred a design in which phrases within a response\nwere color-coded based on the computed factuality scores. Additionally,\nparticipants increased their trust ratings when relevant sections of the source\nmaterial were highlighted or responses were annotated with reference numbers\ncorresponding to those sources, compared to when they received no annotation in\nthe source material. Our study offers practical design guidelines to facilitate\nhuman-LLM collaboration and it promotes a new human role to carefully evaluate\nand take responsibility for their use of LLM outputs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Submitted to the Trust and Reliance in Evolving Human-AI Workflows\n  (TREW) Workshop at CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20434v1",
    "published_date": "2024-05-30 19:23:14 UTC",
    "updated_date": "2024-05-30 19:23:14 UTC"
  },
  {
    "arxiv_id": "2406.02582v1",
    "title": "Spatiotemporal Predictions of Toxic Urban Plumes Using Deep Learning",
    "authors": [
      "Yinan Wang",
      "M. Giselle Fernández-Godino",
      "Nipun Gunawardena",
      "Donald D. Lucas",
      "Xiaowei Yue"
    ],
    "abstract": "Industrial accidents, chemical spills, and structural fires can release large\namounts of harmful materials that disperse into urban atmospheres and impact\npopulated areas. Computer models are typically used to predict the transport of\ntoxic plumes by solving fluid dynamical equations. However, these models can be\ncomputationally expensive due to the need for many grid cells to simulate\nturbulent flow and resolve individual buildings and streets. In emergency\nresponse situations, alternative methods are needed that can run quickly and\nadequately capture important spatiotemporal features. Here, we present a novel\ndeep learning model called ST-GasNet that was inspired by the mathematical\nequations that govern the behavior of plumes as they disperse through the\natmosphere. ST-GasNet learns the spatiotemporal dependencies from a limited set\nof temporal sequences of ground-level toxic urban plumes generated by a\nhigh-resolution large eddy simulation model. On independent sequences,\nST-GasNet accurately predicts the late-time spatiotemporal evolution, given the\nearly-time behavior as an input, even for cases when a building splits a large\nplume into smaller plumes. By incorporating large-scale wind boundary condition\ninformation, ST-GasNet achieves a prediction accuracy of at least 90% on test\ndata for the entire prediction period.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "86-08",
      "I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.02582v1",
    "published_date": "2024-05-30 19:18:20 UTC",
    "updated_date": "2024-05-30 19:18:20 UTC"
  },
  {
    "arxiv_id": "2405.20430v1",
    "title": "Enhancing Performance for Highly Imbalanced Medical Data via Data Regularization in a Federated Learning Setting",
    "authors": [
      "Georgios Tsoumplekas",
      "Ilias Siniosoglou",
      "Vasileios Argyriou",
      "Ioannis D. Moscholios",
      "Panagiotis Sarigiannidis"
    ],
    "abstract": "The increased availability of medical data has significantly impacted\nhealthcare by enabling the application of machine / deep learning approaches in\nvarious instances. However, medical datasets are usually small and scattered\nacross multiple providers, suffer from high class-imbalance, and are subject to\nstringent data privacy constraints. In this paper, the application of a data\nregularization algorithm, suitable for learning under high class-imbalance, in\na federated learning setting is proposed. Specifically, the goal of the\nproposed method is to enhance model performance for cardiovascular disease\nprediction by tackling the class-imbalance that typically characterizes\ndatasets used for this purpose, as well as by leveraging patient data available\nin different nodes of a federated ecosystem without compromising their privacy\nand enabling more resource sensitive allocation. The method is evaluated across\nfour datasets for cardiovascular disease prediction, which are scattered across\ndifferent clients, achieving improved performance. Meanwhile, its robustness\nunder various hyperparameter settings, as well as its ability to adapt to\ndifferent resource allocation scenarios, is verified.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20430v1",
    "published_date": "2024-05-30 19:15:38 UTC",
    "updated_date": "2024-05-30 19:15:38 UTC"
  },
  {
    "arxiv_id": "2405.20421v4",
    "title": "Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA",
    "authors": [
      "Qianqi Yan",
      "Xuehai He",
      "Xiang Yue",
      "Xin Eric Wang"
    ],
    "abstract": "Large Multimodal Models (LMMs) have shown remarkable progress in medical\nVisual Question Answering (Med-VQA), achieving high accuracy on existing\nbenchmarks. However, their reliability under robust evaluation is questionable.\nThis study reveals that when subjected to simple probing evaluation,\nstate-of-the-art models perform worse than random guessing on medical diagnosis\nquestions. To address this critical evaluation problem, we introduce the\nProbing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess\nLMM performance in medical imaging through probing evaluation and procedural\ndiagnosis. Particularly, probing evaluation features pairing original questions\nwith negation questions with hallucinated attributes, while procedural\ndiagnosis requires reasoning across various diagnostic dimensions for each\nimage, including modality recognition, organ identification, clinical findings,\nabnormalities, and positional grounding. Our evaluation reveals that\ntop-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than\nrandom guessing on specialized diagnostic questions, indicating significant\nlimitations in handling fine-grained medical inquiries. Besides, models like\nLLaVA-Med struggle even with more general questions, and results from CheXagent\ndemonstrate the transferability of expertise across different modalities of the\nsame organ, showing that specialized domain knowledge is still crucial for\nimproving performance. This study underscores the urgent need for more robust\nevaluation to ensure the reliability of LMMs in critical fields like medical\ndiagnosis, and current LMMs are still far from applicable to those fields.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20421v4",
    "published_date": "2024-05-30 18:56:01 UTC",
    "updated_date": "2024-10-05 00:09:21 UTC"
  },
  {
    "arxiv_id": "2405.20419v1",
    "title": "Enhancing Antibiotic Stewardship using a Natural Language Approach for Better Feature Representation",
    "authors": [
      "Simon A. Lee",
      "Trevor Brokowski",
      "Jeffrey N. Chiang"
    ],
    "abstract": "The rapid emergence of antibiotic-resistant bacteria is recognized as a\nglobal healthcare crisis, undermining the efficacy of life-saving antibiotics.\nThis crisis is driven by the improper and overuse of antibiotics, which\nescalates bacterial resistance. In response, this study explores the use of\nclinical decision support systems, enhanced through the integration of\nelectronic health records (EHRs), to improve antibiotic stewardship. However,\nEHR systems present numerous data-level challenges, complicating the effective\nsynthesis and utilization of data. In this work, we transform EHR data into a\nserialized textual representation and employ pretrained foundation models to\ndemonstrate how this enhanced feature representation can aid in antibiotic\nsusceptibility predictions. Our results suggest that this text representation,\ncombined with foundation models, provides a valuable tool to increase\ninterpretability and support antibiotic stewardship efforts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20419v1",
    "published_date": "2024-05-30 18:53:53 UTC",
    "updated_date": "2024-05-30 18:53:53 UTC"
  },
  {
    "arxiv_id": "2405.20410v1",
    "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
    "authors": [
      "Hongyu Gong",
      "Bandhav Veluri"
    ],
    "abstract": "Expressive speech-to-speech translation (S2ST) is a key research topic in\nseamless communication, which focuses on the preservation of semantics and\nspeaker vocal style in translated speech. Early works synthesized speaker style\naligned speech in order to directly learn the mapping from speech to target\nspeech spectrogram. Without reliance on style aligned data, recent studies\nleverage the advances of language modeling (LM) and build cascaded LMs on\nsemantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single\nspeech language model for expressive S2ST. We decompose the complex\nsource-to-target speech mapping into intermediate generation steps with\nchain-of-thought prompting. The model is first guided to translate target\nsemantic content and then transfer the speaker style to multi-stream acoustic\nunits. Evaluated on Spanish-to-English and Hungarian-to-English translations,\nSeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and\nstyle transfer, meanwhile achieving better parameter efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20410v1",
    "published_date": "2024-05-30 18:28:31 UTC",
    "updated_date": "2024-05-30 18:28:31 UTC"
  },
  {
    "arxiv_id": "2405.20389v1",
    "title": "Designing an Evaluation Framework for Large Language Models in Astronomy Research",
    "authors": [
      "John F. Wu",
      "Alina Hyk",
      "Kiera McCormick",
      "Christine Ye",
      "Simone Astarita",
      "Elina Baral",
      "Jo Ciuca",
      "Jesse Cranney",
      "Anjalie Field",
      "Kartheik Iyer",
      "Philipp Koehn",
      "Jenn Kotler",
      "Sandor Kruk",
      "Michelle Ntampaka",
      "Charles O'Neill",
      "Joshua E. G. Peek",
      "Sanjib Sharma",
      "Mikaeel Yunus"
    ],
    "abstract": "Large Language Models (LLMs) are shifting how scientific research is done. It\nis imperative to understand how researchers interact with these models and how\nscientific sub-communities like astronomy might benefit from them. However,\nthere is currently no standard for evaluating the use of LLMs in astronomy.\nTherefore, we present the experimental design for an evaluation study on how\nastronomy researchers interact with LLMs. We deploy a Slack chatbot that can\nanswer queries from users via Retrieval-Augmented Generation (RAG); these\nresponses are grounded in astronomy papers from arXiv. We record and anonymize\nuser questions and chatbot answers, user upvotes and downvotes to LLM\nresponses, user feedback to the LLM, and retrieved documents and similarity\nscores with the query. Our data collection method will enable future dynamic\nevaluations of LLM tools for astronomy.",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "7 pages, 3 figures. Code available at\n  https://github.com/jsalt2024-evaluating-llms-for-astronomy/astro-arxiv-bot",
    "pdf_url": "http://arxiv.org/pdf/2405.20389v1",
    "published_date": "2024-05-30 18:00:21 UTC",
    "updated_date": "2024-05-30 18:00:21 UTC"
  },
  {
    "arxiv_id": "2405.20380v1",
    "title": "Gradient Inversion of Federated Diffusion Models",
    "authors": [
      "Jiyue Huang",
      "Chi Hong",
      "Lydia Y. Chen",
      "Stefanie Roos"
    ],
    "abstract": "Diffusion models are becoming defector generative models, which generate\nexceptionally high-resolution image data. Training effective diffusion models\nrequire massive real data, which is privately owned by distributed parties.\nEach data party can collaboratively train diffusion models in a federated\nlearning manner by sharing gradients instead of the raw data. In this paper, we\nstudy the privacy leakage risk of gradient inversion attacks. First, we design\na two-phase fusion optimization, GIDM, to leverage the well-trained generative\nmodel itself as prior knowledge to constrain the inversion search (latent)\nspace, followed by pixel-wise fine-tuning. GIDM is shown to be able to\nreconstruct images almost identical to the original ones. Considering a more\nprivacy-preserving training scenario, we then argue that locally initialized\nprivate training noise $\\epsilon$ and sampling step t may raise additional\nchallenges for the inversion attack. To solve this, we propose a\ntriple-optimization GIDM+ that coordinates the optimization of the unknown\ndata, $\\epsilon$ and $t$. Our extensive evaluation results demonstrate the\nvulnerability of sharing gradient for data protection of diffusion models, even\nhigh-resolution images can be reconstructed with high quality.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20380v1",
    "published_date": "2024-05-30 18:00:03 UTC",
    "updated_date": "2024-05-30 18:00:03 UTC"
  },
  {
    "arxiv_id": "2405.20364v1",
    "title": "Learning 3D Robotics Perception using Inductive Priors",
    "authors": [
      "Muhammad Zubair Irshad"
    ],
    "abstract": "Recent advances in deep learning have led to a data-centric intelligence i.e.\nartificially intelligent models unlocking the potential to ingest a large\namount of data and be really good at performing digital tasks such as\ntext-to-image generation, machine-human conversation, and image recognition.\nThis thesis covers the topic of learning with structured inductive bias and\npriors to design approaches and algorithms unlocking the potential of\nprinciple-centric intelligence. Prior knowledge (priors for short), often\navailable in terms of past experience as well as assumptions of how the world\nworks, helps the autonomous agent generalize better and adapt their behavior\nbased on past experience. In this thesis, I demonstrate the use of prior\nknowledge in three different robotics perception problems. 1. object-centric 3D\nreconstruction, 2. vision and language for decision-making, and 3. 3D scene\nunderstanding. To solve these challenging problems, I propose various sources\nof prior knowledge including 1. geometry and appearance priors from synthetic\ndata, 2. modularity and semantic map priors and 3. semantic, structural, and\ncontextual priors. I study these priors for solving robotics 3D perception\ntasks and propose ways to efficiently encode them in deep learning models. Some\npriors are used to warm-start the network for transfer learning, others are\nused as hard constraints to restrict the action space of robotics agents. While\nclassical techniques are brittle and fail to generalize to unseen scenarios and\ndata-centric approaches require a large amount of labeled data, this thesis\naims to build intelligent agents which require very-less real-world data or\ndata acquired only from simulation to generalize to highly dynamic and\ncluttered environments in novel simulations (i.e. sim2sim) or real-world unseen\nenvironments (i.e. sim2real) for a holistic scene understanding of the 3D\nworld.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Georgia Tech Ph.D. Thesis, December 2023. For more details:\n  https://zubairirshad.com/",
    "pdf_url": "http://arxiv.org/pdf/2405.20364v1",
    "published_date": "2024-05-30 17:59:51 UTC",
    "updated_date": "2024-05-30 17:59:51 UTC"
  },
  {
    "arxiv_id": "2405.20337v1",
    "title": "OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving",
    "authors": [
      "Lening Wang",
      "Wenzhao Zheng",
      "Yilong Ren",
      "Han Jiang",
      "Zhiyong Cui",
      "Haiyang Yu",
      "Jiwen Lu"
    ],
    "abstract": "Understanding the evolution of 3D scenes is important for effective\nautonomous driving. While conventional methods mode scene development with the\nmotion of individual instances, world models emerge as a generative framework\nto describe the general scene dynamics. However, most existing methods adopt an\nautoregressive framework to perform next-token prediction, which suffer from\ninefficiency in modeling long-term temporal evolutions. To address this, we\npropose a diffusion-based 4D occupancy generation model, OccSora, to simulate\nthe development of the 3D world for autonomous driving. We employ a 4D scene\ntokenizer to obtain compact discrete spatial-temporal representations for 4D\noccupancy input and achieve high-quality reconstruction for long-sequence\noccupancy videos. We then learn a diffusion transformer on the spatial-temporal\nrepresentations and generate 4D occupancy conditioned on a trajectory prompt.\nWe conduct extensive experiments on the widely used nuScenes dataset with Occ3D\noccupancy annotations. OccSora can generate 16s-videos with authentic 3D layout\nand temporal consistency, demonstrating its ability to understand the spatial\nand temporal distributions of driving scenes. With trajectory-aware 4D\ngeneration, OccSora has the potential to serve as a world simulator for the\ndecision-making of autonomous driving. Code is available at:\nhttps://github.com/wzzheng/OccSora.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/wzzheng/OccSora",
    "pdf_url": "http://arxiv.org/pdf/2405.20337v1",
    "published_date": "2024-05-30 17:59:42 UTC",
    "updated_date": "2024-05-30 17:59:42 UTC"
  },
  {
    "arxiv_id": "2405.20331v2",
    "title": "CoSy: Evaluating Textual Explanations of Neurons",
    "authors": [
      "Laura Kopf",
      "Philine Lou Bommer",
      "Anna Hedström",
      "Sebastian Lapuschkin",
      "Marina M. -C. Höhne",
      "Kirill Bykov"
    ],
    "abstract": "A crucial aspect of understanding the complex nature of Deep Neural Networks\n(DNNs) is the ability to explain learned concepts within their latent\nrepresentations. While methods exist to connect neurons to human-understandable\ntextual descriptions, evaluating the quality of these explanations is\nchallenging due to the lack of a unified quantitative approach. We introduce\nCoSy (Concept Synthesis), a novel, architecture-agnostic framework for\nevaluating textual explanations of latent neurons. Given textual explanations,\nour proposed framework uses a generative model conditioned on textual input to\ncreate data points representing the explanations. By comparing the neuron's\nresponse to these generated data points and control data points, we can\nestimate the quality of the explanation. We validate our framework through\nsanity checks and benchmark various neuron description methods for Computer\nVision tasks, revealing significant differences in quality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20331v2",
    "published_date": "2024-05-30 17:59:04 UTC",
    "updated_date": "2024-12-05 15:48:24 UTC"
  },
  {
    "arxiv_id": "2405.20330v3",
    "title": "OmniHands: Towards Robust 4D Hand Mesh Recovery via A Versatile Transformer",
    "authors": [
      "Dixuan Lin",
      "Yuxiang Zhang",
      "Mengcheng Li",
      "Yebin Liu",
      "Wei Jing",
      "Qi Yan",
      "Qianying Wang",
      "Hongwen Zhang"
    ],
    "abstract": "In this paper, we introduce OmniHands, a universal approach to recovering\ninteractive hand meshes and their relative movement from monocular or\nmulti-view inputs. Our approach addresses two major limitations of previous\nmethods: lacking a unified solution for handling various hand image inputs and\nneglecting the positional relationship of two hands within images. To overcome\nthese challenges, we develop a universal architecture with novel tokenization\nand contextual feature fusion strategies, capable of adapting to a variety of\ntasks. Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT)\nmethod to embed positional relation information into the hand tokens. In this\nway, our network can handle both single-hand and two-hand inputs and explicitly\nleverage relative hand positions, facilitating the reconstruction of intricate\nhand interactions in real-world scenarios. As such tokenization indicates the\nrelative relationship of two hands, it also supports more effective feature\nfusion. To this end, we further develop a 4D Interaction Reasoning (FIR) module\nto fuse hand tokens in 4D with attention and decode them into 3D hand meshes\nand relative temporal movements. The efficacy of our approach is validated on\nseveral benchmark datasets. The results on in-the-wild videos and real-world\nscenarios demonstrate the superior performances of our approach for interactive\nhand reconstruction. More video results can be found on the project page:\nhttps://OmniHand.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "An extended journal version of 4DHands, featured with versatile\n  module that can adapt to temporal task and multi-view task. Additional\n  detailed comparison experiments and results presentation have been added.\n  More demo videos can be seen at our project page: https://OmniHand.github.io",
    "pdf_url": "http://arxiv.org/pdf/2405.20330v3",
    "published_date": "2024-05-30 17:59:02 UTC",
    "updated_date": "2024-10-01 15:04:23 UTC"
  },
  {
    "arxiv_id": "2405.20323v1",
    "title": "$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving",
    "authors": [
      "Nan Huang",
      "Xiaobao Wei",
      "Wenzhao Zheng",
      "Pengju An",
      "Ming Lu",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Shanghang Zhang"
    ],
    "abstract": "Photorealistic 3D reconstruction of street scenes is a critical technique for\ndeveloping real-world simulators for autonomous driving. Despite the efficacy\nof Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting\n(3DGS) emerges as a promising direction due to its faster speed and more\nexplicit representation. However, most existing street 3DGS methods require\ntracked 3D vehicle bounding boxes to decompose the static and dynamic elements\nfor effective reconstruction, limiting their applications for in-the-wild\nscenarios. To facilitate efficient 3D scene reconstruction without costly\nannotations, we propose a self-supervised street Gaussian\n($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from\n4D consistency. We represent each scene with 3D Gaussians to preserve the\nexplicitness and further accompany them with a spatial-temporal field network\nto compactly model the 4D dynamics. We conduct extensive experiments on the\nchallenging Waymo-Open dataset to evaluate the effectiveness of our method. Our\n$\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic\nscenes and achieves the best performance without using 3D annotations. Code is\navailable at: https://github.com/nnanhuang/S3Gaussian/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/nnanhuang/S3Gaussian/",
    "pdf_url": "http://arxiv.org/pdf/2405.20323v1",
    "published_date": "2024-05-30 17:57:08 UTC",
    "updated_date": "2024-05-30 17:57:08 UTC"
  },
  {
    "arxiv_id": "2405.20320v2",
    "title": "Improving the Training of Rectified Flows",
    "authors": [
      "Sangyun Lee",
      "Zinan Lin",
      "Giulia Fanti"
    ],
    "abstract": "Diffusion models have shown great promise for image and video generation, but\nsampling from state-of-the-art models requires expensive numerical integration\nof a generative ODE. One approach for tackling this problem is rectified flows,\nwhich iteratively learn smooth ODE paths that are less susceptible to\ntruncation error. However, rectified flows still require a relatively large\nnumber of function evaluations (NFEs). In this work, we propose improved\ntechniques for training rectified flows, allowing them to compete with\n\\emph{knowledge distillation} methods even in the low NFE setting. Our main\ninsight is that under realistic settings, a single iteration of the Reflow\nalgorithm for training rectified flows is sufficient to learn nearly straight\ntrajectories; hence, the current practice of using multiple Reflow iterations\nis unnecessary. We thus propose techniques to improve one-round training of\nrectified flows, including a U-shaped timestep distribution and LPIPS-Huber\npremetric. With these techniques, we improve the FID of the previous\n2-rectified flow by up to 75\\% in the 1 NFE setting on CIFAR-10. On ImageNet\n64$\\times$64, our improved rectified flow outperforms the state-of-the-art\ndistillation methods such as consistency distillation and progressive\ndistillation in both one-step and two-step settings and rivals the performance\nof improved consistency training (iCT) in FID. Code is available at\nhttps://github.com/sangyun884/rfpp.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20320v2",
    "published_date": "2024-05-30 17:56:04 UTC",
    "updated_date": "2024-10-08 21:40:13 UTC"
  },
  {
    "arxiv_id": "2405.20319v2",
    "title": "ParSEL: Parameterized Shape Editing with Language",
    "authors": [
      "Aditya Ganeshan",
      "Ryan Y. Huang",
      "Xianghao Xu",
      "R. Kenny Jones",
      "Daniel Ritchie"
    ],
    "abstract": "The ability to edit 3D assets from natural language presents a compelling\nparadigm to aid in the democratization of 3D content creation. However, while\nnatural language is often effective at communicating general intent, it is\npoorly suited for specifying precise manipulation. To address this gap, we\nintroduce ParSEL, a system that enables controllable editing of high-quality 3D\nassets from natural language. Given a segmented 3D mesh and an editing request,\nParSEL produces a parameterized editing program. Adjusting the program\nparameters allows users to explore shape variations with a precise control over\nthe magnitudes of edits. To infer editing programs which align with an input\nedit request, we leverage the abilities of large-language models (LLMs).\nHowever, while we find that LLMs excel at identifying initial edit operations,\nthey often fail to infer complete editing programs, and produce outputs that\nviolate shape semantics. To overcome this issue, we introduce Analytical Edit\nPropagation (AEP), an algorithm which extends a seed edit with additional\noperations until a complete editing program has been formed. Unlike prior\nmethods, AEP searches for analytical editing operations compatible with a range\nof possible user edits through the integration of computer algebra systems for\ngeometric analysis. Experimentally we demonstrate ParSEL's effectiveness in\nenabling controllable editing of 3D objects through natural language requests\nover alternative system designs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.SC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20319v2",
    "published_date": "2024-05-30 17:55:46 UTC",
    "updated_date": "2024-05-31 04:09:41 UTC"
  },
  {
    "arxiv_id": "2405.20318v3",
    "title": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through Curiosity-Driven Queries",
    "authors": [
      "Roberto Ceraolo",
      "Dmitrii Kharlapenko",
      "Ahmad Khan",
      "Amélie Reymond",
      "Rada Mihalcea",
      "Bernhard Schölkopf",
      "Mrinmaya Sachan",
      "Zhijing Jin"
    ],
    "abstract": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20318v3",
    "published_date": "2024-05-30 17:55:28 UTC",
    "updated_date": "2025-02-24 16:42:25 UTC"
  },
  {
    "arxiv_id": "2405.20315v1",
    "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
    "authors": [
      "Ziwei Ji",
      "Yuzhe Gu",
      "Wenwei Zhang",
      "Chengqi Lyu",
      "Dahua Lin",
      "Kai Chen"
    ],
    "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models\n(LLMs) is crucial for their wide applications. A comprehensive and fine-grained\nmeasurement of the hallucination is the first key step for the governance of\nthis issue but is under-explored in the community. Thus, we present\n$\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical\n$\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative\nQuestion Answering. Each answer sentence in our dataset undergoes rigorous\nannotation, involving the retrieval of a reference fragment, the judgment of\nthe hallucination type, and the correction of hallucinated content. ANAH\nconsists of ~12k sentence-level annotations for ~4.3k LLM responses covering\nover 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the\nfine granularity of the hallucination annotations, we can quantitatively\nconfirm that the hallucinations of LLMs progressively accumulate in the answer\nand use ANAH to train and evaluate hallucination annotators. We conduct\nextensive experiments on studying generative and discriminative annotators and\nshow that, although current open-source LLMs have difficulties in fine-grained\nhallucination annotation, the generative annotator trained with ANAH can\nsurpass all open-source LLMs and GPT-3.5, obtain performance competitive with\nGPT-4, and exhibits better generalization ability on unseen questions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20315v1",
    "published_date": "2024-05-30 17:54:40 UTC",
    "updated_date": "2024-05-30 17:54:40 UTC"
  },
  {
    "arxiv_id": "2405.20309v2",
    "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
    "authors": [
      "Ajay Patel",
      "Markus Hofmarcher",
      "Claudiu Leoveanu-Condrei",
      "Marius-Constantin Dinu",
      "Chris Callison-Burch",
      "Sepp Hochreiter"
    ],
    "abstract": "Training models to act as agents that can effectively navigate and perform\nactions in a complex environment, such as a web browser, has typically been\nchallenging due to lack of training data. Large language models (LLMs) have\nrecently demonstrated some capability to navigate novel environments as agents\nin a zero-shot or few-shot fashion, purely guided by natural language\ninstructions as prompts. Recent research has also demonstrated LLMs have the\ncapability to exceed their base performance through self-improvement, i.e.\nfine-tuning on data generated by the model itself. In this work, we explore the\nextent to which LLMs can self-improve their performance as agents in\nlong-horizon tasks in a complex environment using the WebArena benchmark. In\nWebArena, an agent must autonomously navigate and perform actions on web pages\nto achieve a specified objective. We explore fine-tuning on three distinct\nsynthetic training data mixtures and achieve a 31\\% improvement in task\ncompletion rate over the base model on the WebArena benchmark through a\nself-improvement procedure. We additionally contribute novel evaluation metrics\nfor assessing the performance, robustness, capabilities, and quality of\ntrajectories of our fine-tuned agent models to a greater degree than simple,\naggregate-level benchmark scores currently used to measure self-improvement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20309v2",
    "published_date": "2024-05-30 17:52:36 UTC",
    "updated_date": "2024-10-01 21:28:29 UTC"
  },
  {
    "arxiv_id": "2405.20289v1",
    "title": "DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation",
    "authors": [
      "Zachary Novack",
      "Julian McAuley",
      "Taylor Berg-Kirkpatrick",
      "Nicholas Bryan"
    ],
    "abstract": "Controllable music generation methods are critical for human-centered\nAI-based music creation, but are currently limited by speed, quality, and\ncontrol design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in\nparticular, offers state-of-the-art results, but is over 10x slower than\nreal-time, limiting practical use. We propose Distilled Diffusion\nInference-Time T -Optimization (or DITTO-2), a new method to speed up\ninference-time optimization-based control and unlock faster-than-real-time\ngeneration for a wide-variety of applications such as music inpainting,\noutpainting, intensity, melody, and musical structure control. Our method works\nby (1) distilling a pre-trained diffusion model for fast sampling via an\nefficient, modified consistency or consistency trajectory distillation process\n(2) performing inference-time optimization using our distilled model with\none-step sampling as an efficient surrogate optimization task and (3) running a\nfinal multi-step sampling generation (decoding) using our estimated noise\nlatents for best-quality, fast, controllable generation. Through thorough\nevaluation, we find our method not only speeds up generation over 10-20x, but\nsimultaneously improves control adherence and generation quality all at once.\nFurthermore, we apply our approach to a new application of maximizing text\nadherence (CLAP score) and show we can convert an unconditional diffusion model\nwithout text inputs into a model that yields state-of-the-art text control.\nSound examples can be found at https://ditto-music.github.io/ditto2/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20289v1",
    "published_date": "2024-05-30 17:40:11 UTC",
    "updated_date": "2024-05-30 17:40:11 UTC"
  },
  {
    "arxiv_id": "2405.20287v1",
    "title": "Flexible SE(2) graph neural networks with applications to PDE surrogates",
    "authors": [
      "Maria Bånkestad",
      "Olof Mogren",
      "Aleksis Pirinen"
    ],
    "abstract": "This paper presents a novel approach for constructing graph neural networks\nequivariant to 2D rotations and translations and leveraging them as PDE\nsurrogates on non-gridded domains. We show that aligning the representations\nwith the principal axis allows us to sidestep many constraints while preserving\nSE(2) equivariance. By applying our model as a surrogate for fluid flow\nsimulations and conducting thorough benchmarks against non-equivariant models,\nwe demonstrate significant gains in terms of both data efficiency and accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.20287v1",
    "published_date": "2024-05-30 17:39:15 UTC",
    "updated_date": "2024-05-30 17:39:15 UTC"
  },
  {
    "arxiv_id": "2405.20279v2",
    "title": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models",
    "authors": [
      "Sijie Zhao",
      "Yong Zhang",
      "Xiaodong Cun",
      "Shaoshu Yang",
      "Muyao Niu",
      "Xiaoyu Li",
      "Wenbo Hu",
      "Ying Shan"
    ],
    "abstract": "Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://ailab-cvc.github.io/cvvae/index.html",
    "pdf_url": "http://arxiv.org/pdf/2405.20279v2",
    "published_date": "2024-05-30 17:33:10 UTC",
    "updated_date": "2024-10-23 02:38:44 UTC"
  },
  {
    "arxiv_id": "2405.20278v2",
    "title": "Length independent generalization bounds for deep SSM architectures",
    "authors": [
      "Dániel Rácz",
      "Mihály Petreczky",
      "Bálint Daróczy"
    ],
    "abstract": "Many state-of-the-art models trained on long-range sequences, for example S4,\nS5 or LRU, are made of sequential blocks combining State-Space Models (SSMs)\nwith neural networks. In this paper we provide a PAC bound that holds for these\nkind of architectures with stable SSM blocks and does not depend on the length\nof the input sequence. Imposing stability of the SSM blocks is a standard\npractice in the literature, and it is known to help performance. Our results\nprovide a theoretical justification for the use of stable SSM blocks as the\nproposed PAC bound decreases as the degree of stability of the SSM blocks\nincreases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "68",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, no figures, accepted at ICML 2024 Next Generation of\n  Sequence Modeling Architectures Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.20278v2",
    "published_date": "2024-05-30 17:32:46 UTC",
    "updated_date": "2024-07-11 07:55:14 UTC"
  },
  {
    "arxiv_id": "2405.20274v2",
    "title": "ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection for ABSA",
    "authors": [
      "Siva Uday Sampreeth Chebolu",
      "Franck Dernoncourt",
      "Nedim Lipka",
      "Thamar Solorio"
    ],
    "abstract": "Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion\nand diversity due to various shared tasks spanning several languages and fields\nand organized via SemEval workshops and Germeval. Nonetheless, a few\nshortcomings still need to be addressed, such as the lack of low-resource\nlanguage evaluations and the emphasis on sentence-level analysis. To thoroughly\nassess ABSA techniques in the context of complete reviews, this research\npresents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST).\nROAST seeks to close the gap between sentence-level and text-level ABSA by\nidentifying every ABSA constituent at the review level. We extend the available\ndatasets to enable ROAST, addressing the drawbacks noted in previous research\nby incorporating low-resource languages, numerous languages, and a variety of\ntopics. Through this effort, ABSA research will be able to cover more ground\nand get a deeper comprehension of the task and its practical application in a\nvariety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2309.13297",
    "pdf_url": "http://arxiv.org/pdf/2405.20274v2",
    "published_date": "2024-05-30 17:29:15 UTC",
    "updated_date": "2024-07-18 18:05:04 UTC"
  },
  {
    "arxiv_id": "2405.20247v3",
    "title": "KerasCV and KerasNLP: Vision and Language Power-Ups",
    "authors": [
      "Matthew Watson",
      "Divyashree Shivakumar Sreepathihalli",
      "Francois Chollet",
      "Martin Gorner",
      "Kiranbir Sodhia",
      "Ramesh Sampath",
      "Tirth Patel",
      "Haifeng Jin",
      "Neel Kovelamudi",
      "Gabriel Rasskin",
      "Samaneh Saadat",
      "Luke Wood",
      "Chen Qian",
      "Jonathan Bischof",
      "Ian Stenbit",
      "Abheesht Sharma",
      "Anshuman Mishra"
    ],
    "abstract": "We present the Keras domain packages KerasCV and KerasNLP, extensions of the\nKeras API for Computer Vision and Natural Language Processing workflows,\ncapable of running on either JAX, TensorFlow, or PyTorch. These domain packages\nare designed to enable fast experimentation, with a focus on ease-of-use and\nperformance. We adopt a modular, layered design: at the library's lowest level\nof abstraction, we provide building blocks for creating models and data\npreprocessing pipelines, and at the library's highest level of abstraction, we\nprovide pretrained ``task\" models for popular architectures such as Stable\nDiffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc. Task models have\nbuilt-in preprocessing, pretrained weights, and can be fine-tuned on raw\ninputs. To enable efficient training, we support XLA compilation for all\nmodels, and run all preprocessing via a compiled graph of TensorFlow operations\nusing the tf.data API. The libraries are fully open-source (Apache 2.0 license)\nand available on GitHub.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SE",
      "I.2.5; I.2.7; I.2.10"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to Journal of Machine Learning Open Source Software",
    "pdf_url": "http://arxiv.org/pdf/2405.20247v3",
    "published_date": "2024-05-30 16:58:34 UTC",
    "updated_date": "2024-06-05 07:52:07 UTC"
  },
  {
    "arxiv_id": "2405.20245v1",
    "title": "Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use",
    "authors": [
      "Franz Louis Cesista",
      "Rui Aguiar",
      "Jason Kim",
      "Paolo Acilo"
    ],
    "abstract": "Business Document Information Extraction (BDIE) is the problem of\ntransforming a blob of unstructured information (raw text, scanned documents,\netc.) into a structured format that downstream systems can parse and use. It\nhas two main tasks: Key-Information Extraction (KIE) and Line Items Recognition\n(LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem,\nwhere the tools are these downstream systems. We then present Retrieval\nAugmented Structured Generation (RASG), a novel general framework for BDIE that\nachieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE\nbenchmarks.\n  The contributions of this paper are threefold: (1) We show, with ablation\nbenchmarks, that Large Language Models (LLMs) with RASG are already competitive\nwith or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on\nBDIE benchmarks. (2) We propose a new metric class for Line Items Recognition,\nGeneral Line Items Recognition Metric (GLIRM), that is more aligned with\npractical BDIE use cases compared to existing metrics, such as ANLS*, DocILE,\nand GriTS. (3) We provide a heuristic algorithm for backcalculating bounding\nboxes of predicted line items and tables without the need for vision encoders.\nFinally, we claim that, while LMMs might sometimes offer marginal performance\nbenefits, LLMs + RASG is oftentimes superior given real-world applications and\nconstraints of BDIE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IEEE 7th International Conference on Multimedia\n  Information Processing and Retrieval (MIPR), 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20245v1",
    "published_date": "2024-05-30 16:54:42 UTC",
    "updated_date": "2024-05-30 16:54:42 UTC"
  },
  {
    "arxiv_id": "2405.20237v1",
    "title": "Training-efficient density quantum machine learning",
    "authors": [
      "Brian Coyle",
      "El Amine Cherrat",
      "Nishant Jain",
      "Natansh Mathur",
      "Snehal Raj",
      "Skander Kazdaghli",
      "Iordanis Kerenidis"
    ],
    "abstract": "Quantum machine learning requires powerful, flexible and efficiently\ntrainable models to be successful in solving challenging problems. In this\nwork, we present density quantum neural networks, a learning model\nincorporating randomisation over a set of trainable unitaries. These models\ngeneralise quantum neural networks using parameterised quantum circuits, and\nallow a trade-off between expressibility and efficient trainability,\nparticularly on quantum hardware. We demonstrate the flexibility of the\nformalism by applying it to two recently proposed model families. The first are\ncommuting-block quantum neural networks (QNNs) which are efficiently trainable\nbut may be limited in expressibility. The second are orthogonal (Hamming-weight\npreserving) quantum neural networks which provide well-defined and\ninterpretable transformations on data but are challenging to train at scale on\nquantum devices. Density commuting QNNs improve capacity with minimal gradient\ncomplexity overhead, and density orthogonal neural networks admit a\nquadratic-to-constant gradient query advantage with minimal to no performance\nloss. We conduct numerical experiments on synthetic translationally invariant\ndata and MNIST image data with hyperparameter optimisation to support our\nfindings. Finally, we discuss the connection to post-variational quantum neural\nnetworks, measurement-based quantum machine learning and the dropout mechanism.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "17 pages main text, 9 pages appendices. 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20237v1",
    "published_date": "2024-05-30 16:40:28 UTC",
    "updated_date": "2024-05-30 16:40:28 UTC"
  },
  {
    "arxiv_id": "2405.20234v3",
    "title": "Hidden in Plain Sight: Exploring Chat History Tampering in Interactive Language Models",
    "authors": [
      "Cheng'an Wei",
      "Yue Zhao",
      "Yujia Gong",
      "Kai Chen",
      "Lu Xiang",
      "Shenchen Zhu"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling chat history tampering. This paper introduces a systematic methodology\nto inject user-supplied history into LLM conversations without any prior\nknowledge of the target model. The key is to utilize prompt templates that can\nwell organize the messages to be injected, leading the target LLM to interpret\nthem as genuine chat history. To automatically search for effective templates\nin a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm\n(LLMGA) that leverages an LLM to generate and iteratively optimize the\ntemplates. We apply the proposed method to popular real-world LLMs including\nChatGPT and Llama-2/3. The results show that chat history tampering can enhance\nthe malleability of the model's behavior over time and greatly influence the\nmodel output. For example, it can improve the success rate of disallowed\nresponse elicitation up to 97% on ChatGPT. Our findings provide insights into\nthe challenges associated with the real-world deployment of interactive LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20234v3",
    "published_date": "2024-05-30 16:36:47 UTC",
    "updated_date": "2024-09-06 02:41:35 UTC"
  },
  {
    "arxiv_id": "2405.20233v2",
    "title": "Grokfast: Accelerated Grokking by Amplifying Slow Gradients",
    "authors": [
      "Jaerin Lee",
      "Bong Gyun Kang",
      "Kihoon Kim",
      "Kyoung Mu Lee"
    ],
    "abstract": "One puzzling artifact in machine learning dubbed grokking is where delayed\ngeneralization is achieved tenfolds of iterations after near perfect\noverfitting to the training data. Focusing on the long delay itself on behalf\nof machine learning practitioners, our goal is to accelerate generalization of\na model under grokking phenomenon. By regarding a series of gradients of a\nparameter over training iterations as a random signal over time, we can\nspectrally decompose the parameter trajectories under gradient descent into two\ncomponents: the fast-varying, overfitting-yielding component and the\nslow-varying, generalization-inducing component. This analysis allows us to\naccelerate the grokking phenomenon more than $\\times 50$ with only a few lines\nof code that amplifies the slow-varying components of gradients. The\nexperiments show that our algorithm applies to diverse tasks involving images,\nlanguages, and graphs, enabling practical availability of this peculiar\nartifact of sudden generalization. Our code is available at\nhttps://github.com/ironjr/grokfast.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 13 figures. Typo fixed. Project page:\n  https://jaerinlee.com/research/grokfast",
    "pdf_url": "http://arxiv.org/pdf/2405.20233v2",
    "published_date": "2024-05-30 16:35:30 UTC",
    "updated_date": "2024-06-05 15:12:00 UTC"
  },
  {
    "arxiv_id": "2405.20231v3",
    "title": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",
    "authors": [
      "Derek Lim",
      "Theo Moe Putterman",
      "Robin Walters",
      "Haggai Maron",
      "Stefanie Jegelka"
    ],
    "abstract": "Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training. Our code\nis available at https://github.com/cptq/asymmetric-networks",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024. v2: added / updated some citations. v3 added link to\n  code, and some additional ablations",
    "pdf_url": "http://arxiv.org/pdf/2405.20231v3",
    "published_date": "2024-05-30 16:32:31 UTC",
    "updated_date": "2024-10-15 12:53:48 UTC"
  },
  {
    "arxiv_id": "2405.20222v3",
    "title": "MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model",
    "authors": [
      "Muyao Niu",
      "Xiaodong Cun",
      "Xintao Wang",
      "Yong Zhang",
      "Ying Shan",
      "Yinqiang Zheng"
    ],
    "abstract": "We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration. Project Page: https://myniuuu.github.io/MOFA_Video/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024 ; Project Page: https://myniuuu.github.io/MOFA_Video/ ;\n  Codes: https://github.com/MyNiuuu/MOFA-Video",
    "pdf_url": "http://arxiv.org/pdf/2405.20222v3",
    "published_date": "2024-05-30 16:22:22 UTC",
    "updated_date": "2024-07-11 16:26:03 UTC"
  },
  {
    "arxiv_id": "2405.20884v1",
    "title": "Effects of Dataset Sampling Rate for Noise Cancellation through Deep Learning",
    "authors": [
      "Brandon Colelough",
      "Andrew Zheng"
    ],
    "abstract": "Background: Active noise cancellation has been a subject of research for\ndecades. Traditional techniques, like the Fast Fourier Transform, have\nlimitations in certain scenarios. This research explores the use of deep neural\nnetworks (DNNs) as a superior alternative. Objective: The study aims to\ndetermine the effect sampling rate within training data has on lightweight,\nefficient DNNs that operate within the processing constraints of mobile\ndevices. Methods: We chose the ConvTasNET network for its proven efficiency in\nspeech separation and enhancement. ConvTasNET was trained on datasets such as\nWHAM!, LibriMix, and the MS-2023 DNS Challenge. The datasets were sampled at\nrates of 8kHz, 16kHz, and 48kHz to analyze the effect of sampling rate on noise\ncancellation efficiency and effectiveness. The model was tested on a core-i7\nIntel processor from 2023, assessing the network's ability to produce clear\naudio while filtering out background noise. Results: Models trained at higher\nsampling rates (48kHz) provided much better evaluation metrics against Total\nHarmonic Distortion (THD) and Quality Prediction For Generative Neural Speech\nCodecs (WARP-Q) values, indicating improved audio quality. However, a trade-off\nwas noted with the processing time being longer for higher sampling rates.\nConclusions: The Conv-TasNET network, trained on datasets sampled at higher\nrates like 48kHz, offers a robust solution for mobile devices in achieving\nnoise cancellation through speech separation and enhancement. Future work\ninvolves optimizing the model's efficiency further and testing on mobile\ndevices.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "16 pages, 8 pictures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.20884v1",
    "published_date": "2024-05-30 16:20:44 UTC",
    "updated_date": "2024-05-30 16:20:44 UTC"
  },
  {
    "arxiv_id": "2405.20218v1",
    "title": "ESG-FTSE: A corpus of news articles with ESG relevance labels and use cases",
    "authors": [
      "Mariya Pavlova",
      "Bernard Casey",
      "Miaosen Wang"
    ],
    "abstract": "We present ESG-FTSE, the first corpus comprised of news articles with\nEnvironmental, Social and Governance (ESG) relevance annotations. In recent\nyears, investors and regulators have pushed ESG investing to the mainstream due\nto the urgency of climate change. This has led to the rise of ESG scores to\nevaluate an investment's credentials as socially responsible. While demand for\nESG scores is high, their quality varies wildly. Quantitative techniques can be\napplied to improve ESG scores, thus, responsible investing. To contribute to\nresource building for ESG and financial text mining, we pioneer the ESG-FTSE\ncorpus. We further present the first of its kind ESG annotation schema. It has\nthree levels: a binary classification (relevant versus irrelevant news\narticles), ESG classification (ESG-related news articles), and target company.\nBoth supervised and unsupervised learning experiments for ESG relevance\ndetection were conducted to demonstrate that the corpus can be used in\ndifferent settings to derive accurate ESG predictions. Keywords: corpus\nannotation, ESG labels, annotation schema, news article, natural language\nprocessing",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "The corpus is available at\n  https://github.com/mariavpavlova/ESG-FTSE-Corpus.\n  https://aclanthology.org/2024.finnlp-1.14/",
    "pdf_url": "http://arxiv.org/pdf/2405.20218v1",
    "published_date": "2024-05-30 16:19:02 UTC",
    "updated_date": "2024-05-30 16:19:02 UTC"
  },
  {
    "arxiv_id": "2405.20216v3",
    "title": "Boost Your Human Image Generation Model via Direct Preference Optimization",
    "authors": [
      "Sanghyeon Na",
      "Yonggyu Kim",
      "Hyunjoon Lee"
    ],
    "abstract": "Human image generation is a key focus in image synthesis due to its broad\napplications, but even slight inaccuracies in anatomy, pose, or details can\ncompromise realism. To address these challenges, we explore Direct Preference\nOptimization (DPO), which trains models to generate preferred (winning) images\nwhile diverging from non-preferred (losing) ones. However, conventional DPO\nmethods use generated images as winning images, limiting realism. To overcome\nthis limitation, we propose an enhanced DPO approach that incorporates\nhigh-quality real images as winning images, encouraging outputs to resemble\nreal images rather than generated ones. However, implementing this concept is\nnot a trivial task. Therefore, our approach, HG-DPO (Human image Generation\nthrough DPO), employs a novel curriculum learning framework that gradually\nimproves the output of the model toward greater realism, making training more\nfeasible. Furthermore, HG-DPO effectively adapts to personalized text-to-image\ntasks, generating high-quality and identity-specific images, which highlights\nthe practical value of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025 as a highlight paper",
    "pdf_url": "http://arxiv.org/pdf/2405.20216v3",
    "published_date": "2024-05-30 16:18:05 UTC",
    "updated_date": "2025-04-09 06:55:52 UTC"
  },
  {
    "arxiv_id": "2405.20213v1",
    "title": "PostDoc: Generating Poster from a Long Multimodal Document Using Deep Submodular Optimization",
    "authors": [
      "Vijay Jaisankar",
      "Sambaran Bandyopadhyay",
      "Kalp Vyas",
      "Varre Chaitanya",
      "Shwetha Somasundaram"
    ],
    "abstract": "A poster from a long input document can be considered as a one-page\neasy-to-read multimodal (text and images) summary presented on a nice template\nwith good design elements. Automatic transformation of a long document into a\nposter is a very less studied but challenging task. It involves content\nsummarization of the input document followed by template generation and\nharmonization. In this work, we propose a novel deep submodular function which\ncan be trained on ground truth summaries to extract multimodal content from the\ndocument and explicitly ensures good coverage, diversity and alignment of text\nand images. Then, we use an LLM based paraphraser and propose to generate a\ntemplate with various design aspects conditioned on the input content. We show\nthe merits of our approach through extensive automated and human evaluations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20213v1",
    "published_date": "2024-05-30 16:16:25 UTC",
    "updated_date": "2024-05-30 16:16:25 UTC"
  },
  {
    "arxiv_id": "2405.20204v2",
    "title": "Jina CLIP: Your CLIP Model Is Also Your Text Retriever",
    "authors": [
      "Andreas Koukounas",
      "Georgios Mastrapas",
      "Michael Günther",
      "Bo Wang",
      "Scott Martens",
      "Isabelle Mohr",
      "Saba Sturua",
      "Mohammad Kalim Akram",
      "Joan Fontanals Martínez",
      "Saahil Ognawala",
      "Susana Guzman",
      "Maximilian Werk",
      "Nan Wang",
      "Han Xiao"
    ],
    "abstract": "Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, MFM-EAI@ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20204v2",
    "published_date": "2024-05-30 16:07:54 UTC",
    "updated_date": "2024-06-26 12:31:48 UTC"
  },
  {
    "arxiv_id": "2405.20202v1",
    "title": "One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments",
    "authors": [
      "Ke Yi",
      "Yuhui Xu",
      "Heng Chang",
      "Chen Tang",
      "Yuan Meng",
      "Tong Zhang",
      "Jia Li"
    ],
    "abstract": "Large Language Models (LLMs) have advanced rapidly but face significant\nmemory demands. While quantization has shown promise for LLMs, current methods\ntypically require lengthy training to alleviate the performance degradation\nfrom quantization loss. However, deploying LLMs across diverse scenarios with\ndifferent resource constraints, e.g., servers and personal computers, requires\nrepeated training per application, which amplifies the lengthy training\nproblem. Given that, it is advantageous to train a once-for-all (OFA) supernet\ncapable of yielding diverse optimal subnets for downstream applications through\none-shot training. Nonetheless, the scale of current language models impedes\nefficiency and amplifies interference from weight sharing between subnets. We\nmake an initial attempt to extend the once-for-all framework to large language\nmodels. Specifically, we decouple shared weights to eliminate the interference\nand incorporate Low-Rank adapters for training efficiency. Furthermore, we\nobserve the imbalance allocation of training resources from the traditional\nuniform sampling. A non-parametric scheduler is introduced to adjust the\nsampling rate for each quantization configuration, achieving a more balanced\nallocation among subnets with varying demands. We validate the approach on\nLLaMA2 families, and downstream evaluation confirms our ability to maintain\nhigh performance while significantly reducing deployment time faced with\nmultiple scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20202v1",
    "published_date": "2024-05-30 16:05:15 UTC",
    "updated_date": "2024-05-30 16:05:15 UTC"
  },
  {
    "arxiv_id": "2405.20189v1",
    "title": "Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory",
    "authors": [
      "Hangyeol Kang",
      "Maher Ben Moussa",
      "Nadia Magnenat-Thalmann"
    ],
    "abstract": "In this work, we describe our approach to developing an intelligent and\nrobust social robotic system for the Nadine social robot platform. We achieve\nthis by integrating Large Language Models (LLMs) and skilfully leveraging the\npowerful reasoning and instruction-following capabilities of these types of\nmodels to achieve advanced human-like affective and cognitive capabilities.\nThis approach is novel compared to the current state-of-the-art LLM-based\nagents which do not implement human-like long-term memory or sophisticated\nemotional appraisal. The naturalness of social robots, consisting of multiple\nmodules, highly depends on the performance and capabilities of each component\nof the system and the seamless integration of the components. We built a social\nrobot system that enables generating appropriate behaviours through multimodal\ninput processing, bringing episodic memories accordingly to the recognised\nuser, and simulating the emotional states of the robot induced by the\ninteraction with the human partner. In particular, we introduce an LLM-agent\nframe for social robots, SoR-ReAct, serving as a core component for the\ninteraction module in our system. This design has brought forth the advancement\nof social robots and aims to increase the quality of human-robot interaction.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20189v1",
    "published_date": "2024-05-30 15:55:41 UTC",
    "updated_date": "2024-05-30 15:55:41 UTC"
  },
  {
    "arxiv_id": "2405.20183v1",
    "title": "A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models",
    "authors": [
      "Eduard Frankford",
      "Ingo Höhn",
      "Clemens Sauerwein",
      "Ruth Breu"
    ],
    "abstract": "This paper analyzes Large Language Models (LLMs) with regard to their\nprogramming exercise generation capabilities. Through a survey study, we\ndefined the state of the art, extracted their strengths and weaknesses and\nfinally proposed an evaluation matrix, helping researchers and educators to\ndecide which LLM is the best fitting for the programming exercise generation\nuse case. We also found that multiple LLMs are capable of producing useful\nprogramming exercises. Nevertheless, there exist challenges like the ease with\nwhich LLMs might solve exercises generated by LLMs. This paper contributes to\nthe ongoing discourse on the integration of LLMs in education.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 0 figures, CSEE&T 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20183v1",
    "published_date": "2024-05-30 15:49:34 UTC",
    "updated_date": "2024-05-30 15:49:34 UTC"
  },
  {
    "arxiv_id": "2405.20180v1",
    "title": "Transformers and Slot Encoding for Sample Efficient Physical World Modelling",
    "authors": [
      "Francesco Petri",
      "Luigi Asprino",
      "Aldo Gangemi"
    ],
    "abstract": "World modelling, i.e. building a representation of the rules that govern the\nworld so as to predict its evolution, is an essential ability for any agent\ninteracting with the physical world. Recent applications of the Transformer\narchitecture to the problem of world modelling from video input show notable\nimprovements in sample efficiency. However, existing approaches tend to work\nonly at the image level thus disregarding that the environment is composed of\nobjects interacting with each other. In this paper, we propose an architecture\ncombining Transformers for world modelling with the slot-attention paradigm, an\napproach for learning representations of objects appearing in a scene. We\ndescribe the resulting neural architecture and report experimental results\nshowing an improvement over the existing solutions in terms of sample\nefficiency and a reduction of the variation of the performance over the\ntraining examples. The code for our architecture and experiments is available\nat https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20180v1",
    "published_date": "2024-05-30 15:48:04 UTC",
    "updated_date": "2024-05-30 15:48:04 UTC"
  },
  {
    "arxiv_id": "2405.20179v3",
    "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs",
    "authors": [
      "Zichao Hu",
      "Junyi Jessy Li",
      "Arjun Guha",
      "Joydeep Biswas"
    ],
    "abstract": "Code LLMs have shown promising results with converting tasks in natural\nlanguage to programs that can be executed by service robots. We are interested\nin finetuning small, specialized LLMs for this purpose, but collecting datasets\nof task-program pairs specific to each robot is time-consuming and expensive.\nWhile approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of\ngenerating novel tasks given a few examples, they are unable to provide the\ncorresponding programs that correctly abide by physical-world and\nrobot-constraints using the provided programming interface. Using a simulator\nis a natural potential solution to checking for such constraints, but building\nsimulation environments that can handle arbitrary tasks and their necessary\nobjects and locations, is challenging. To address these challenges, we\nintroduce ROBO-INSTRUCT, which synthesizes task-specific simulation\nenvironments on the fly during program execution, by opportunistically\ninferring entity properties and enforcing corresponding constraints based on\nhow the entities are used in the task program. Additionally, ROBO-INSTRUCT\nintegrates an LLM-aided post-processing procedure to refine instructions for\nbetter alignment with robot programs. We demonstrate the effectiveness of\nROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models\noutperform all baseline methods and even match or surpass the performance of\nseveral larger and proprietary models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20179v3",
    "published_date": "2024-05-30 15:47:54 UTC",
    "updated_date": "2025-04-11 19:55:48 UTC"
  },
  {
    "arxiv_id": "2405.20175v1",
    "title": "InstructionCP: A fast approach to transfer Large Language Models into target language",
    "authors": [
      "Kuang-Ming Chen",
      "Hung-yi Lee"
    ],
    "abstract": "The rapid development of large language models (LLMs) in recent years has\nlargely focused on English, resulting in models that respond exclusively in\nEnglish. To adapt these models to other languages, continual pre-training (CP)\nis often employed, followed by supervised fine-tuning (SFT) to maintain\nconversational abilities. However, CP and SFT can reduce a model's ability to\nfilter harmful content. We propose Instruction Continual Pre-training (InsCP),\nwhich integrates instruction tags into the CP process to prevent loss of\nconversational proficiency while acquiring new languages. Our experiments\ndemonstrate that InsCP retains conversational and Reinforcement Learning from\nHuman Feedback (RLHF) abilities. Empirical evaluations on language alignment,\nreliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,\nthis approach requires only 0.1 billion tokens of high-quality\ninstruction-following data, thereby reducing resource consumption.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2405.20175v1",
    "published_date": "2024-05-30 15:45:13 UTC",
    "updated_date": "2024-05-30 15:45:13 UTC"
  },
  {
    "arxiv_id": "2405.20172v3",
    "title": "Iterative Feature Boosting for Explainable Speech Emotion Recognition",
    "authors": [
      "Alaa Nfissi",
      "Wassim Bouachir",
      "Nizar Bouguila",
      "Brian Mishara"
    ],
    "abstract": "In speech emotion recognition (SER), using predefined features without\nconsidering their practical importance may lead to high dimensional datasets,\nincluding redundant and irrelevant information. Consequently, high-dimensional\nlearning often results in decreasing model accuracy while increasing\ncomputational complexity. Our work underlines the importance of carefully\nconsidering and analyzing features in order to build efficient SER systems. We\npresent a new supervised SER method based on an efficient feature engineering\napproach. We pay particular attention to the explainability of results to\nevaluate feature relevance and refine feature sets. This is performed\niteratively through feature evaluation loop, using Shapley values to boost\nfeature selection and improve overall framework performance. Our approach\nallows thus to balance the benefits between model performance and transparency.\nThe proposed method outperforms human-level performance (HLP) and\nstate-of-the-art machine learning methods in emotion recognition on the TESS\ndataset. The source code of this paper is publicly available at\nhttps://github.com/alaaNfissi/Iterative-Feature-Boosting-for-Explainable-Speech-Emotion-Recognition.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS",
      "I.2.7; I.2.6; I.2.1; I.2.8"
    ],
    "primary_category": "cs.SD",
    "comment": "Published in: 2023 International Conference on Machine Learning and\n  Applications (ICMLA)",
    "pdf_url": "http://arxiv.org/pdf/2405.20172v3",
    "published_date": "2024-05-30 15:44:27 UTC",
    "updated_date": "2024-06-05 22:28:13 UTC"
  },
  {
    "arxiv_id": "2405.20163v1",
    "title": "Reasoning about concepts with LLMs: Inconsistencies abound",
    "authors": [
      "Rosario Uceda-Sosa",
      "Karthikeyan Natesan Ramamurthy",
      "Maria Chang",
      "Moninder Singh"
    ],
    "abstract": "The ability to summarize and organize knowledge into abstract concepts is key\nto learning and reasoning. Many industrial applications rely on the consistent\nand systematic use of concepts, especially when dealing with decision-critical\nknowledge. However, we demonstrate that, when methodically questioned, large\nlanguage models (LLMs) often display and demonstrate significant\ninconsistencies in their knowledge. Computationally, the basic aspects of the\nconceptualization of a given domain can be represented as Is-A hierarchies in a\nknowledge graph (KG) or ontology, together with a few properties or axioms that\nenable straightforward reasoning. We show that even simple ontologies can be\nused to reveal conceptual inconsistencies across several LLMs. We also propose\nstrategies that domain experts can use to evaluate and improve the coverage of\nkey domain concepts in LLMs of various sizes. In particular, we have been able\nto significantly enhance the performance of LLMs of various sizes with openly\navailable weights using simple knowledge-graph (KG) based prompting strategies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.20163v1",
    "published_date": "2024-05-30 15:38:54 UTC",
    "updated_date": "2024-05-30 15:38:54 UTC"
  },
  {
    "arxiv_id": "2405.20142v2",
    "title": "MSSC-BiMamba: Multimodal Sleep Stage Classification and Early Diagnosis of Sleep Disorders with Bidirectional Mamba",
    "authors": [
      "Chao Zhang",
      "Weirong Cui",
      "Jingjing Guo"
    ],
    "abstract": "Monitoring sleep states is essential for evaluating sleep quality and\ndiagnosing sleep disorders. Traditional manual staging is time-consuming and\nprone to subjective bias, often resulting in inconsistent outcomes. Here, we\ndeveloped an automated model for sleep staging and disorder classification to\nenhance diagnostic accuracy and efficiency. Considering the characteristics of\npolysomnography (PSG) multi-lead sleep monitoring, we designed a multimodal\nsleep state classification model, MSSC-BiMamba, that combines an Efficient\nChannel Attention (ECA) mechanism with a Bidirectional State Space Model\n(BSSM). The ECA module allows for weighting data from different sensor\nchannels, thereby amplifying the influence of diverse sensor inputs.\nAdditionally, the implementation of bidirectional Mamba (BiMamba) enables the\nmodel to effectively capture the multidimensional features and long-range\ndependencies of PSG data. The developed model demonstrated impressive\nperformance on sleep stage classification tasks on both the ISRUC-S3 and\nISRUC-S1 datasets, respectively containing data with healthy and unhealthy\nsleep patterns. Also, the model exhibited a high accuracy for sleep health\nprediction when evaluated on a combined dataset consisting of ISRUC and\nSleep-EDF. Our model, which can effectively handle diverse sleep conditions, is\nthe first to apply BiMamba to sleep staging with multimodal PSG data, showing\nsubstantial gains in computational and memory efficiency over traditional\nTransformer-style models. This method enhances sleep health management by\nmaking monitoring more accessible and extending advanced healthcare through\ninnovative technology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.20142v2",
    "published_date": "2024-05-30 15:16:53 UTC",
    "updated_date": "2024-05-31 03:31:23 UTC"
  },
  {
    "arxiv_id": "2405.20139v1",
    "title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
    "authors": [
      "Costas Mavromatis",
      "George Karypis"
    ],
    "abstract": "Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form\nof triplets (head, relation, tail), which collectively form a graph. Question\nAnswering over KGs (KGQA) is the task of answering natural questions grounding\nthe reasoning to the information provided by the KG. Large Language Models\n(LLMs) are the state-of-the-art models for QA tasks due to their remarkable\nability to understand natural language. On the other hand, Graph Neural\nNetworks (GNNs) have been widely used for KGQA as they can handle the complex\ngraph information stored in the KG. In this work, we introduce GNN-RAG, a novel\nmethod for combining language understanding abilities of LLMs with the\nreasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.\nFirst, a GNN reasons over a dense KG subgraph to retrieve answer candidates for\na given question. Second, the shortest paths in the KG that connect question\nentities and answer candidates are extracted to represent KG reasoning paths.\nThe extracted paths are verbalized and given as input for LLM reasoning with\nRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to\nextract useful graph information, while the LLM leverages its natural language\nprocessing ability for ultimate KGQA. Furthermore, we develop a retrieval\naugmentation (RA) technique to further boost KGQA performance with GNN-RAG.\nExperimental results show that GNN-RAG achieves state-of-the-art performance in\ntwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching\nGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming competing approaches by 8.9--15.5%\npoints at answer F1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20139v1",
    "published_date": "2024-05-30 15:14:24 UTC",
    "updated_date": "2024-05-30 15:14:24 UTC"
  },
  {
    "arxiv_id": "2405.20138v2",
    "title": "Separation and Collapse of Equilibria Inequalities on AND-OR Trees without Shape Constraints",
    "authors": [
      "Fuki Ito",
      "Toshio Suzuki"
    ],
    "abstract": "Herein, we investigate the zero-error randomized complexity, which is the\nleast cost against the worst input, of AND-OR tree computation by imposing\nvarious restrictions on the algorithm to find the Boolean value of the root of\nthat tree and no restrictions on the tree shape. When a tree satisfies a\ncertain condition regarding its symmetry, directional algorithms proposed by\nSaks and Wigderson (1986), special randomized algorithms, are known to achieve\nthe randomized complexity. Furthermore, there is a known example of a tree that\nis so unbalanced that no directional algorithm achieves the randomized\ncomplexity (Vereshchagin 1998). In this study, we aim to identify where\ndeviations arise between the general randomized Boolean decision tree and its\nspecial case, directional algorithms. In this paper, we show that for any\nAND-OR tree, randomized depth-first algorithms, which form a broader class\ncompared with directional algorithms, have the same equilibrium as that of the\ndirectional algorithms. Thus, we get the collapse result on equilibria\ninequalities that holds for an arbitrary AND-OR tree. This implies that there\nexists a case where even depth-first algorithms cannot be the fastest, leading\nto the separation result on equilibria inequality. Additionally, a new\nalgorithm is introduced as a key concept for proof of the separation result.",
    "categories": [
      "cs.AI",
      "68T20, 68Q17, 03D15, 91A60",
      "I.2.8; F.2.2"
    ],
    "primary_category": "cs.AI",
    "comment": "42 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2405.20138v2",
    "published_date": "2024-05-30 15:13:46 UTC",
    "updated_date": "2024-10-01 09:11:53 UTC"
  },
  {
    "arxiv_id": "2405.20132v4",
    "title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics",
    "authors": [
      "Niki van Stein",
      "Thomas Bäck"
    ],
    "abstract": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at IEEE TEVC",
    "pdf_url": "http://arxiv.org/pdf/2405.20132v4",
    "published_date": "2024-05-30 15:10:59 UTC",
    "updated_date": "2025-01-30 08:54:54 UTC"
  },
  {
    "arxiv_id": "2405.20121v1",
    "title": "A Structure-Aware Lane Graph Transformer Model for Vehicle Trajectory Prediction",
    "authors": [
      "Sun Zhanbo",
      "Dong Caiyin",
      "Ji Ang",
      "Zhao Ruibin",
      "Zhao Yu"
    ],
    "abstract": "Accurate prediction of future trajectories for surrounding vehicles is vital\nfor the safe operation of autonomous vehicles. This study proposes a Lane Graph\nTransformer (LGT) model with structure-aware capabilities. Its key contribution\nlies in encoding the map topology structure into the attention mechanism. To\naddress variations in lane information from different directions, four Relative\nPositional Encoding (RPE) matrices are introduced to capture the local details\nof the map topology structure. Additionally, two Shortest Path Distance (SPD)\nmatrices are employed to capture distance information between two accessible\nlanes. Numerical results indicate that the proposed LGT model achieves a\nsignificantly higher prediction performance on the Argoverse 2 dataset.\nSpecifically, the minFDE$_6$ metric was decreased by 60.73% compared to the\nArgoverse 2 baseline model (Nearest Neighbor) and the b-minFDE$_6$ metric was\nreduced by 2.65% compared to the baseline LaneGCN model. Furthermore, ablation\nexperiments demonstrated that the consideration of map topology structure led\nto a 4.24% drop in the b-minFDE$_6$ metric, validating the effectiveness of\nthis model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20121v1",
    "published_date": "2024-05-30 14:57:16 UTC",
    "updated_date": "2024-05-30 14:57:16 UTC"
  },
  {
    "arxiv_id": "2405.20114v2",
    "title": "Towards Faster Decentralized Stochastic Optimization with Communication Compression",
    "authors": [
      "Rustem Islamov",
      "Yuan Gao",
      "Sebastian U. Stich"
    ],
    "abstract": "Communication efficiency has garnered significant attention as it is\nconsidered the main bottleneck for large-scale decentralized Machine Learning\napplications in distributed and federated settings. In this regime, clients are\nrestricted to transmitting small amounts of quantized information to their\nneighbors over a communication graph. Numerous endeavors have been made to\naddress this challenging problem by developing algorithms with compressed\ncommunication for decentralized non-convex optimization problems. Despite\nconsiderable efforts, the current results suffer from various issues such as\nnon-scalability with the number of clients, requirements for large batches, or\nbounded gradient assumption. In this paper, we introduce MoTEF, a novel\napproach that integrates communication compression with Momentum Tracking and\nError Feedback. Our analysis demonstrates that MoTEF achieves most of the\ndesired properties, and significantly outperforms existing methods under\narbitrary data heterogeneity. We provide numerical experiments to validate our\ntheoretical findings and confirm the practical superiority of MoTEF.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20114v2",
    "published_date": "2024-05-30 14:51:57 UTC",
    "updated_date": "2024-11-25 09:00:40 UTC"
  },
  {
    "arxiv_id": "2405.20082v3",
    "title": "Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations",
    "authors": [
      "Shivam Grover",
      "Amin Jalali",
      "Ali Etemad"
    ],
    "abstract": "Existing approaches for learning representations of time-series keep the\ntemporal arrangement of the time-steps intact with the presumption that the\noriginal order is the most optimal for learning. However, non-adjacent sections\nof real-world time-series may have strong dependencies. Accordingly, we raise\nthe question: Is there an alternative arrangement for time-series which could\nenable more effective representation learning? To address this, we propose a\nsimple plug-and-play neural network layer called Segment, Shuffle, and Stitch\n(S3) designed to improve representation learning in time-series models. S3\nworks by creating non-overlapping segments from the original sequence and\nshuffling them in a learned manner that is optimal for the task at hand. It\nthen re-attaches the shuffled segments back together and performs a learned\nweighted sum with the original input to capture both the newly shuffled\nsequence along with the original sequence. S3 is modular and can be stacked to\nachieve different levels of granularity, and can be added to many forms of\nneural architectures including CNNs or Transformers with negligible computation\noverhead. Through extensive experiments on several datasets and\nstate-of-the-art baselines, we show that incorporating S3 results in\nsignificant improvements for the tasks of time-series classification,\nforecasting, and anomaly detection, improving performance on certain datasets\nby up to 68\\%. We also show that S3 makes the learning more stable with a\nsmoother training loss curve and loss landscape compared to the original\nbaseline. The code is available at\nhttps://github.com/shivam-grover/S3-TimeSeries.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20082v3",
    "published_date": "2024-05-30 14:11:29 UTC",
    "updated_date": "2024-10-30 15:18:22 UTC"
  },
  {
    "arxiv_id": "2405.20081v2",
    "title": "NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models",
    "authors": [
      "Kai Wu",
      "Boyuan Jiang",
      "Zhengkai Jiang",
      "Qingdong He",
      "Donghao Luo",
      "Shengzhi Wang",
      "Qingwen Liu",
      "Chengjie Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) contribute a powerful mechanism to\nunderstanding visual information building on large language models. However,\nMLLMs are notorious for suffering from hallucinations, especially when\ngenerating lengthy, detailed descriptions for images. Our analysis reveals that\nhallucinations stem from the inherent summarization mechanism of large language\nmodels, leading to excessive dependence on linguistic tokens while neglecting\nvision information. In this paper, we propose NoiseBoost, a broadly applicable\nand simple method for alleviating hallucinations for MLLMs through the\nintegration of noise feature perturbations. Noise perturbation acts as a\nregularizer, facilitating a balanced distribution of attention weights among\nvisual and linguistic tokens. Despite its simplicity, NoiseBoost consistently\nenhances the performance of MLLMs across common training strategies, including\nsupervised fine-tuning and reinforcement learning. Further, NoiseBoost\npioneerly enables semi-supervised learning for MLLMs, unleashing the power of\nunlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves\ndense caption accuracy by 8.1% with human evaluation and achieves comparable\nresults with 50% of the data by mining unlabeled data. Code and models are\navailable at https://kaiwu5.github.io/noiseboost.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures with supplementary material",
    "pdf_url": "http://arxiv.org/pdf/2405.20081v2",
    "published_date": "2024-05-30 14:11:27 UTC",
    "updated_date": "2024-05-31 07:40:04 UTC"
  },
  {
    "arxiv_id": "2405.20059v1",
    "title": "Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation",
    "authors": [
      "Adam Sorrenti"
    ],
    "abstract": "Separating vocal elements from musical tracks is a longstanding challenge in\naudio signal processing. This study tackles the distinct separation of vocal\ncomponents from musical spectrograms. We employ the Short Time Fourier\nTransform (STFT) to extract audio waves into detailed frequency-time\nspectrograms, utilizing the benchmark MUSDB18 dataset for music separation.\nSubsequently, we implement a UNet neural network to segment the spectrogram\nimage, aiming to delineate and extract singing voice components accurately. We\nachieved noteworthy results in audio source separation using of our U-Net-based\nmodels. The combination of frequency-axis normalization with Min/Max scaling\nand the Mean Absolute Error (MAE) loss function achieved the highest\nSource-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy\nin preserving the quality of the original signal during separation. This setup\nalso recorded impressive Source-to-Interference Ratio (SIR) and\nSource-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively.\nThese values significantly outperformed other configurations, particularly\nthose using Quantile-based normalization or a Mean Squared Error (MSE) loss\nfunction. Our source code, model weights, and demo material can be found at the\nproject's GitHub repository: https://github.com/mbrotos/SoundSeg",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20059v1",
    "published_date": "2024-05-30 13:47:53 UTC",
    "updated_date": "2024-05-30 13:47:53 UTC"
  },
  {
    "arxiv_id": "2405.20053v1",
    "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
    "authors": [
      "Avelina Asada Hadji-Kyriacou",
      "Ognjen Arandjelovic"
    ],
    "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context\nlearning capabilities; however, their behaviors are often difficult to control.\nBy utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible\nto fine-tune unsupervised LMs to follow instructions and produce outputs that\nreflect human preferences. Despite its benefits, RLHF has been shown to\npotentially harm a language model's reasoning capabilities and introduce\nartifacts such as hallucinations where the model may fabricate facts. To\naddress this issue we introduce Direct Preference Heads (DPH), a fine-tuning\nframework that enables LMs to learn human preference signals through an\nauxiliary reward head without directly affecting the output distribution of the\nlanguage modeling head. We perform a theoretical analysis of our objective\nfunction and find strong ties to Conservative Direct Preference Optimization\n(cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All\nevaluation suite and demonstrate that our method produces models which achieve\nhigher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct\nPreference Optimization (DPO) alone.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20053v1",
    "published_date": "2024-05-30 13:38:52 UTC",
    "updated_date": "2024-05-30 13:38:52 UTC"
  },
  {
    "arxiv_id": "2405.20046v1",
    "title": "Cross-Training with Multi-View Knowledge Fusion for Heterogenous Federated Learning",
    "authors": [
      "Zhuang Qi",
      "Lei Meng",
      "Weihao He",
      "Ruohan Zhang",
      "Yu Wang",
      "Xin Qi",
      "Xiangxu Meng"
    ],
    "abstract": "Federated learning benefits from cross-training strategies, which enables\nmodels to train on data from distinct sources to improve the generalization\ncapability. However, the data heterogeneity between sources may lead models to\ngradually forget previously acquired knowledge when undergoing cross-training\nto adapt to new tasks or data sources. We argue that integrating personalized\nand global knowledge to gather information from multiple perspectives could\npotentially improve performance. To achieve this goal, this paper presents a\nnovel approach that enhances federated learning through a cross-training scheme\nincorporating multi-view information. Specifically, the proposed method, termed\nFedCT, includes three main modules, where the consistency-aware knowledge\nbroadcasting module aims to optimize model assignment strategies, which\nenhances collaborative advantages between clients and achieves an efficient\nfederated learning process. The multi-view knowledge-guided representation\nlearning module leverages fused prototypical knowledge from both global and\nlocal views to enhance the preservation of local knowledge before and after\nmodel exchange, as well as to ensure consistency between local and global\nknowledge. The mixup-based feature augmentation module aggregates rich\ninformation to further increase the diversity of feature spaces, which enables\nthe model to better discriminate complex samples. Extensive experiments were\nconducted on four datasets in terms of performance comparison, ablation study,\nin-depth analysis and case study. The results demonstrated that FedCT\nalleviates knowledge forgetting from both local and global views, which enables\nit outperform state-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20046v1",
    "published_date": "2024-05-30 13:27:30 UTC",
    "updated_date": "2024-05-30 13:27:30 UTC"
  },
  {
    "arxiv_id": "2405.20032v1",
    "title": "Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion",
    "authors": [
      "Jiangkai Wu",
      "Liming Liu",
      "Yunpeng Tan",
      "Junlin Hao",
      "Xinggong Zhang"
    ],
    "abstract": "With the exponential growth of video traffic, traditional video streaming\nsystems are approaching their limits in compression efficiency and\ncommunication capacity. To further reduce bitrate while maintaining quality, we\npropose Promptus, a disruptive novel system that streaming prompts instead of\nvideo content with Stable Diffusion, which converts video frames into a series\nof \"prompts\" for delivery. To ensure pixel alignment, a gradient descent-based\nprompt fitting framework is proposed. To achieve adaptive bitrate for prompts,\na low-rank decomposition-based bitrate control algorithm is introduced. For\ninter-frame compression of prompts, a temporal smoothing-based prompt\ninterpolation algorithm is proposed. Evaluations across various video domains\nand real network traces demonstrate Promptus can enhance the perceptual quality\nby 0.111 and 0.092 (in LPIPS) compared to VAE and H.265, respectively, and\ndecreases the ratio of severely distorted frames by 89.3% and 91.7%. Moreover,\nPromptus achieves real-time video generation from prompts at over 150 FPS. To\nthe best of our knowledge, Promptus is the first attempt to replace video\ncodecs with prompt inversion and the first to use prompt streaming instead of\nvideo streaming. Our work opens up a new paradigm for efficient video\ncommunication beyond the Shannon limit.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20032v1",
    "published_date": "2024-05-30 13:16:48 UTC",
    "updated_date": "2024-05-30 13:16:48 UTC"
  },
  {
    "arxiv_id": "2405.20024v2",
    "title": "Applications of Generative AI (GAI) for Mobile and Wireless Networking: A Survey",
    "authors": [
      "Thai-Hoc Vu",
      "Senthil Kumar Jagatheesaperumal",
      "Minh-Duong Nguyen",
      "Nguyen Van Huynh",
      "Sunghwan Kim",
      "Quoc-Viet Pham"
    ],
    "abstract": "The success of Artificial Intelligence (AI) in multiple disciplines and\nvertical domains in recent years has promoted the evolution of mobile\nnetworking and the future Internet toward an AI-integrated Internet-of-Things\n(IoT) era. Nevertheless, most AI techniques rely on data generated by physical\ndevices (e.g., mobile devices and network nodes) or specific applications\n(e.g., fitness trackers and mobile gaming). Therefore, Generative AI (GAI),\na.k.a. AI-generated content (AIGC), has emerged as a powerful AI paradigm;\nthanks to its ability to efficiently learn complex data distributions and\ngenerate synthetic data to represent the original data in various forms. This\nimpressive feature is projected to transform the management of mobile\nnetworking and diversify the current services and applications provided. On\nthis basis, this work presents a concise tutorial on the role of GAIs in mobile\nand wireless networking. In particular, this survey first provides the\nfundamentals of GAI and representative GAI models, serving as an essential\npreliminary to the understanding of GAI's applications in mobile and wireless\nnetworking. Then, this work provides a comprehensive review of state-of-the-art\nstudies and GAI applications in network management, wireless security, semantic\ncommunication, and lessons learned from the open literature. Finally, this work\nsummarizes the current research on GAI for mobile and wireless networking by\noutlining important challenges that need to be resolved to facilitate the\ndevelopment and applicability of GAI in this edge-cutting area.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "This work has been accepted for publication in the IEEE Internet of\n  Things Journal under ID number IoT-37996-2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20024v2",
    "published_date": "2024-05-30 13:06:40 UTC",
    "updated_date": "2024-10-19 12:21:30 UTC"
  },
  {
    "arxiv_id": "2405.20015v1",
    "title": "Efficient LLM-Jailbreaking by Introducing Visual Modality",
    "authors": [
      "Zhenxing Niu",
      "Yuyao Sun",
      "Haodong Ren",
      "Haoxuan Ji",
      "Quan Wang",
      "Xiaoke Ma",
      "Gang Hua",
      "Rong Jin"
    ],
    "abstract": "This paper focuses on jailbreaking attacks against large language models\n(LLMs), eliciting them to generate objectionable content in response to harmful\nuser queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our\napproach begins by constructing a multimodal large language model (MLLM)\nthrough the incorporation of a visual module into the target LLM. Subsequently,\nwe conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings\nembJS. Finally, we convert the embJS into text space to facilitate the\njailbreaking of the target LLM. Compared to direct LLM-jailbreaking, our\napproach is more efficient, as MLLMs are more vulnerable to jailbreaking than\npure LLM. Additionally, to improve the attack success rate (ASR) of\njailbreaking, we propose an image-text semantic matching scheme to identify a\nsuitable initial input. Extensive experiments demonstrate that our approach\nsurpasses current state-of-the-art methods in terms of both efficiency and\neffectiveness. Moreover, our approach exhibits superior cross-class\njailbreaking capabilities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20015v1",
    "published_date": "2024-05-30 12:50:32 UTC",
    "updated_date": "2024-05-30 12:50:32 UTC"
  },
  {
    "arxiv_id": "2405.20003v1",
    "title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities",
    "authors": [
      "Alexander Nikitin",
      "Jannik Kossen",
      "Yarin Gal",
      "Pekka Marttinen"
    ],
    "abstract": "Uncertainty quantification in Large Language Models (LLMs) is crucial for\napplications where safety and reliability are important. In particular,\nuncertainty can be used to improve the trustworthiness of LLMs by detecting\nfactually incorrect model responses, commonly called hallucinations.\nCritically, one should seek to capture the model's semantic uncertainty, i.e.,\nthe uncertainty over the meanings of LLM outputs, rather than uncertainty over\nlexical or syntactic variations that do not affect answer correctness. To\naddress this problem, we propose Kernel Language Entropy (KLE), a novel method\nfor uncertainty estimation in white- and black-box LLMs. KLE defines positive\nsemidefinite unit trace kernels to encode the semantic similarities of LLM\noutputs and quantifies uncertainty using the von Neumann entropy. It considers\npairwise semantic dependencies between answers (or semantic clusters),\nproviding more fine-grained uncertainty estimates than previous methods based\non hard clustering of answers. We theoretically prove that KLE generalizes the\nprevious state-of-the-art method called semantic entropy and empirically\ndemonstrate that it improves uncertainty quantification performance across\nmultiple natural language generation datasets and LLM architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20003v1",
    "published_date": "2024-05-30 12:42:05 UTC",
    "updated_date": "2024-05-30 12:42:05 UTC"
  },
  {
    "arxiv_id": "2405.19996v4",
    "title": "DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in the Wild",
    "authors": [
      "Honghao Fu",
      "Yufei Wang",
      "Wenhan Yang",
      "Bihan Wen"
    ],
    "abstract": "Blind image quality assessment (IQA) in the wild, which assesses the quality\nof images with complex authentic distortions and no reference images, presents\nsignificant challenges. Given the difficulty in collecting large-scale training\ndata, leveraging limited data to develop a model with strong generalization\nremains an open problem. Motivated by the robust image perception capabilities\nof pre-trained text-to-image (T2I) diffusion models, we propose a novel IQA\nmethod, diffusion priors-based IQA (DP-IQA), to utilize the T2I model's prior\nfor improved performance and generalization ability. Specifically, we utilize\npre-trained Stable Diffusion as the backbone, extracting multi-level features\nfrom the denoising U-Net guided by prompt embeddings through a tunable text\nadapter. Simultaneously, an image adapter compensates for information loss\nintroduced by the lossy pre-trained encoder. Unlike T2I models that require\nfull image distribution modeling, our approach targets image quality\nassessment, which inherently requires fewer parameters. To improve\napplicability, we distill the knowledge into a lightweight CNN-based student\nmodel, significantly reducing parameters while maintaining or even enhancing\ngeneralization performance. Experimental results demonstrate that DP-IQA\nachieves state-of-the-art performance on various in-the-wild datasets,\nhighlighting the superior generalization capability of T2I priors in blind IQA\ntasks. To our knowledge, DP-IQA is the first method to apply pre-trained\ndiffusion priors in blind IQA. Codes and checkpoints are available at\nhttps://github.com/RomGai/DP-IQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19996v4",
    "published_date": "2024-05-30 12:32:35 UTC",
    "updated_date": "2024-08-17 13:53:17 UTC"
  },
  {
    "arxiv_id": "2405.19988v2",
    "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics",
    "authors": [
      "Minttu Alakuijala",
      "Reginald McLean",
      "Isaac Woungang",
      "Nariman Farsad",
      "Samuel Kaski",
      "Pekka Marttinen",
      "Kai Yuan"
    ],
    "abstract": "Natural language is often the easiest and most convenient modality for humans\nto specify tasks for robots. However, learning to ground language to behavior\ntypically requires impractical amounts of diverse, language-annotated\ndemonstrations collected on each target robot. In this work, we aim to separate\nthe problem of what to accomplish from how to accomplish it, as the former can\nbenefit from substantial amounts of external observation-only data, and only\nthe latter depends on a specific robot embodiment. To this end, we propose\nVideo-Language Critic, a reward model that can be trained on readily available\ncross-embodiment data using contrastive learning and a temporal ranking\nobjective, and use it to score behavior traces from a separate actor. When\ntrained on Open X-Embodiment data, our reward model enables 2x more\nsample-efficient policy training on Meta-World tasks than a sparse reward only,\ndespite a significant domain gap. Using in-domain data but in a challenging\ntask generalization setting on Meta-World, we further demonstrate more\nsample-efficient training than is possible with prior language-conditioned\nreward models that are either trained with binary classification, use static\nimages, or do not leverage the temporal information present in video data.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages in the main text, 16 pages including references and\n  supplementary materials. 4 figures and 3 tables in the main text, 1 table in\n  supplementary materials",
    "pdf_url": "http://arxiv.org/pdf/2405.19988v2",
    "published_date": "2024-05-30 12:18:06 UTC",
    "updated_date": "2024-11-07 19:40:54 UTC"
  },
  {
    "arxiv_id": "2405.19982v1",
    "title": "A Deep Reinforcement Learning Approach for Trading Optimization in the Forex Market with Multi-Agent Asynchronous Distribution",
    "authors": [
      "Davoud Sarani",
      "Parviz Rashidi-Khazaee"
    ],
    "abstract": "In today's forex market traders increasingly turn to algorithmic trading,\nleveraging computers to seek more profits. Deep learning techniques as\ncutting-edge advancements in machine learning, capable of identifying patterns\nin financial data. Traders utilize these patterns to execute more effective\ntrades, adhering to algorithmic trading rules. Deep reinforcement learning\nmethods (DRL), by directly executing trades based on identified patterns and\nassessing their profitability, offer advantages over traditional DL approaches.\nThis research pioneers the application of a multi-agent (MA) RL framework with\nthe state-of-the-art Asynchronous Advantage Actor-Critic (A3C) algorithm. The\nproposed method employs parallel learning across multiple asynchronous workers,\neach specialized in trading across multiple currency pairs to explore the\npotential for nuanced strategies tailored to different market conditions and\ncurrency pairs. Two different A3C with lock and without lock MA model was\nproposed and trained on single currency and multi-currency. The results\nindicate that both model outperform on Proximal Policy Optimization model. A3C\nwith lock outperforms other in single currency training scenario and A3C\nwithout Lock outperforms other in multi-currency scenario. The findings\ndemonstrate that this approach facilitates broader and faster exploration of\ndifferent currency pairs, significantly enhancing trading returns.\nAdditionally, the agent can learn a more profitable trading strategy in a\nshorter time.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19982v1",
    "published_date": "2024-05-30 12:07:08 UTC",
    "updated_date": "2024-05-30 12:07:08 UTC"
  },
  {
    "arxiv_id": "2405.19973v1",
    "title": "A Triumvirate of AI Driven Theoretical Discovery",
    "authors": [
      "Yang-Hui He"
    ],
    "abstract": "Recent years have seen the dramatic rise of the usage of AI algorithms in\npure mathematics and fundamental sciences such as theoretical physics. This is\nperhaps counter-intuitive since mathematical sciences require the rigorous\ndefinitions, derivations, and proofs, in contrast to the experimental sciences\nwhich rely on the modelling of data with error-bars. In this Perspective, we\ncategorize the approaches to mathematical discovery as \"top-down\", \"bottom-up\"\nand \"meta-mathematics\", as inspired by historical examples. We review some of\nthe progress over the last few years, comparing and contrasting both the\nadvances and the short-comings in each approach. We argue that while the\ntheorist is in no way in danger of being replaced by AI in the near future, the\nhybrid of human expertise and AI algorithms will become an integral part of\ntheoretical discovery.",
    "categories": [
      "math.HO",
      "cs.AI",
      "hep-th",
      "physics.hist-ph"
    ],
    "primary_category": "math.HO",
    "comment": "14 pages, under consideration for Nature Review Physics",
    "pdf_url": "http://arxiv.org/pdf/2405.19973v1",
    "published_date": "2024-05-30 11:57:00 UTC",
    "updated_date": "2024-05-30 11:57:00 UTC"
  },
  {
    "arxiv_id": "2405.19970v1",
    "title": "Strategies to Counter Artificial Intelligence in Law Enforcement: Cross-Country Comparison of Citizens in Greece, Italy and Spain",
    "authors": [
      "Petra Saskia Bayerl",
      "Babak Akhgar",
      "Ernesto La Mattina",
      "Barbara Pirillo",
      "Ioana Cotoi",
      "Davide Ariu",
      "Matteo Mauri",
      "Jorge Garcia",
      "Dimitris Kavallieros",
      "Antonia Kardara",
      "Konstantina Karagiorgou"
    ],
    "abstract": "This paper investigates citizens' counter-strategies to the use of Artificial\nIntelligence (AI) by law enforcement agencies (LEAs). Based on information from\nthree countries (Greece, Italy and Spain) we demonstrate disparities in the\nlikelihood of ten specific counter-strategies. We further identified factors\nthat increase the propensity for counter-strategies. Our study provides an\nimportant new perspective to societal impacts of security-focused AI\napplications by illustrating the conscious, strategic choices by citizens when\nconfronted with AI capabilities for LEAs.",
    "categories": [
      "cs.AI",
      "I.2.0; K.4.1"
    ],
    "primary_category": "cs.AI",
    "comment": "20th International Conference on Information and Knowledge\n  Engineering (IKE'21), 3 papges, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2405.19970v1",
    "published_date": "2024-05-30 11:55:10 UTC",
    "updated_date": "2024-05-30 11:55:10 UTC"
  },
  {
    "arxiv_id": "2405.19967v2",
    "title": "Improved Out-of-Scope Intent Classification with Dual Encoding and Threshold-based Re-Classification",
    "authors": [
      "Hossam M. Zawbaa",
      "Wael Rashwan",
      "Sourav Dutta",
      "Haytham Assem"
    ],
    "abstract": "Detecting out-of-scope user utterances is essential for task-oriented\ndialogues and intent classification. Current methodologies face difficulties\nwith the unpredictable distribution of outliers and often rely on assumptions\nabout data distributions. We present the Dual Encoder for Threshold-Based\nRe-Classification (DETER) to address these challenges. This end-to-end\nframework efficiently detects out-of-scope intents without requiring\nassumptions on data distributions or additional post-processing steps. The core\nof DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and\nthe Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance\nembeddings, which are classified through a branched neural architecture.\nFurther, DETER generates synthetic outliers using self-supervision and\nincorporates out-of-scope phrases from open-domain datasets. This approach\nensures a comprehensive training set for out-of-scope detection. Additionally,\na threshold-based re-classification mechanism refines the model's initial\npredictions. Evaluations on the CLINC-150, Stackoverflow, and Banking77\ndatasets demonstrate DETER's efficacy. Our model outperforms previous\nbenchmarks, increasing up to 13% and 5% in F1 score for known and unknown\nintents on CLINC-150 and Stackoverflow, and 16% for known and 24% % for unknown\nintents on Banking77. The source code has been released at\nhttps://github.com/Hossam-Mohammed-tech/Intent_Classification_OOS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19967v2",
    "published_date": "2024-05-30 11:46:42 UTC",
    "updated_date": "2024-05-31 08:54:24 UTC"
  },
  {
    "arxiv_id": "2405.19958v1",
    "title": "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation",
    "authors": [
      "Yi Liu",
      "Xiangyu Liu",
      "Xiangrong Zhu",
      "Wei Hu"
    ],
    "abstract": "Multi-aspect controllable text generation aims to control the generated texts\nin attributes from multiple aspects (e.g., \"positive\" from sentiment and\n\"sport\" from topic). For ease of obtaining training samples, existing works\nneglect attribute correlations formed by the intertwining of different\nattributes. Particularly, the stereotype formed by imbalanced attribute\ncorrelations significantly affects multi-aspect control. In this paper, we\npropose MAGIC, a new multi-aspect controllable text generation method with\ndisentangled counterfactual augmentation. We alleviate the issue of imbalanced\nattribute correlations during training using counterfactual feature vectors in\nthe attribute latent space by disentanglement. During inference, we enhance\nattribute correlations by target-guided counterfactual augmentation to further\nimprove multi-aspect control. Experiments show that MAGIC outperforms\nstate-of-the-art baselines in both imbalanced and balanced attribute\ncorrelation scenarios. Our source code and data are available at\nhttps://github.com/nju-websoft/MAGIC.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in the 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.19958v1",
    "published_date": "2024-05-30 11:25:42 UTC",
    "updated_date": "2024-05-30 11:25:42 UTC"
  },
  {
    "arxiv_id": "2405.19957v4",
    "title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
    "authors": [
      "Qiaowei Miao",
      "JinSheng Quan",
      "Kehan Li",
      "Yawei Luo"
    ],
    "abstract": "Previous text-to-4D methods have leveraged multiple Score Distillation\nSampling (SDS) techniques, combining motion priors from video-based diffusion\nmodels (DMs) with geometric priors from multiview DMs to implicitly guide 4D\nrenderings. However, differences in these priors result in conflicting gradient\ndirections during optimization, causing trade-offs between motion fidelity and\ngeometry accuracy, and requiring substantial optimization time to reconcile the\nmodels. In this paper, we introduce \\textbf{P}ixel-\\textbf{L}evel\n\\textbf{A}lignment for text-driven \\textbf{4D} Gaussian splatting (PLA4D) to\nresolve this motion-geometry conflict. PLA4D provides an anchor reference,\ni.e., text-generated video, to align the rendering process conditioned by\ndifferent DMs in pixel space. For static alignment, our approach introduces a\nfocal alignment method and Gaussian-Mesh contrastive learning to iteratively\nadjust focal lengths and provide explicit geometric priors at each timestep. At\nthe dynamic level, a motion alignment technique and T-MV refinement method are\nemployed to enforce both pose alignment and motion continuity across unknown\nviewpoints, ensuring intrinsic geometric consistency across views. With such\npixel-level multi-DM alignment, our PLA4D framework is able to generate 4D\nobjects with superior geometric, motion, and semantic consistency. Fully\nimplemented with open-source tools, PLA4D offers an efficient and accessible\nsolution for high-quality 4D digital content creation with significantly\nreduced generation time.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19957v4",
    "published_date": "2024-05-30 11:23:01 UTC",
    "updated_date": "2024-11-19 02:12:54 UTC"
  },
  {
    "arxiv_id": "2405.19956v1",
    "title": "HOLMES: to Detect Adversarial Examples with Multiple Detectors",
    "authors": [
      "Jing Wen"
    ],
    "abstract": "Deep neural networks (DNNs) can easily be cheated by some imperceptible but\npurposeful noise added to images, and erroneously classify them. Previous\ndefensive work mostly focused on retraining the models or detecting the noise,\nbut has either shown limited success rates or been attacked by new adversarial\nexamples. Instead of focusing on adversarial images or the interior of DNN\nmodels, we observed that adversarial examples generated by different algorithms\ncan be identified based on the output of DNNs (logits). Logit can serve as an\nexterior feature to train detectors. Then, we propose HOLMES (Hierarchically\nOrganized Light-weight Multiple dEtector System) to reinforce DNNs by detecting\npotential adversarial examples to minimize the threats they may bring in\npractical. HOLMES is able to distinguish \\textit{unseen} adversarial examples\nfrom multiple attacks with high accuracy and low false positive rates than\nsingle detector systems even in an adaptive model. To ensure the diversity and\nrandomness of detectors in HOLMES, we use two methods: training dedicated\ndetectors for each label and training detectors with top-k logits. Our\neffective and inexpensive strategies neither modify original DNN models nor\nrequire its internal parameters. HOLMES is not only compatible with all kinds\nof learning models (even only with external APIs), but also complementary to\nother defenses to achieve higher detection rates (may also fully protect the\nsystem against various adversarial examples).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19956v1",
    "published_date": "2024-05-30 11:22:55 UTC",
    "updated_date": "2024-05-30 11:22:55 UTC"
  },
  {
    "arxiv_id": "2405.19950v2",
    "title": "Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine",
    "authors": [
      "Konstantin Hemker",
      "Nikola Simidjievski",
      "Mateja Jamnik"
    ],
    "abstract": "Learning holistic computational representations in physical, chemical or\nbiological systems requires the ability to process information from different\ndistributions and modalities within the same model. Thus, the demand for\nmultimodal machine learning models has sharply risen for modalities that go\nbeyond vision and language, such as sequences, graphs, time series, or tabular\ndata. While there are many available multimodal fusion and alignment\napproaches, most of them require end-to-end training, scale quadratically with\nthe number of modalities, cannot handle cases of high modality imbalance in the\ntraining set, or are highly topology-specific, making them too restrictive for\nmany biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego),\na general-purpose fusion framework to turn any set of encoders into a\ncompetitive multimodal model with no or minimal fine-tuning. We achieve this by\nintroducing a wrapper for any unimodal encoder that enforces shape consistency\nbetween modality representations. It harmonises these representations by\nlearning features in the frequency domain to enable model merging with little\nsignal interference. We show that MM-Lego 1) can be used as a model merging\nmethod which achieves competitive performance with end-to-end fusion models\nwithout any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a\nmodel fusion method that, with minimal fine-tuning, surpasses all benchmarks in\nfive out of seven datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19950v2",
    "published_date": "2024-05-30 11:14:01 UTC",
    "updated_date": "2025-04-16 16:43:35 UTC"
  },
  {
    "arxiv_id": "2406.15443v1",
    "title": "ExU: AI Models for Examining Multilingual Disinformation Narratives and Understanding their Spread",
    "authors": [
      "Jake Vasilakes",
      "Zhixue Zhao",
      "Ivan Vykopal",
      "Michal Gregor",
      "Martin Hyben",
      "Carolina Scarton"
    ],
    "abstract": "Addressing online disinformation requires analysing narratives across\nlanguages to help fact-checkers and journalists sift through large amounts of\ndata. The ExU project focuses on developing AI-based models for multilingual\ndisinformation analysis, addressing the tasks of rumour stance classification\nand claim retrieval. We describe the ExU project proposal and summarise the\nresults of a user requirements survey regarding the design of tools to support\nfact-checking.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at The 25th Annual Conference of The European Association\n  for Machine Translation (EAMT 24)",
    "pdf_url": "http://arxiv.org/pdf/2406.15443v1",
    "published_date": "2024-05-30 11:13:57 UTC",
    "updated_date": "2024-05-30 11:13:57 UTC"
  },
  {
    "arxiv_id": "2405.19946v2",
    "title": "Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf",
    "authors": [
      "Xuanfa Jin",
      "Ziyan Wang",
      "Yali Du",
      "Meng Fang",
      "Haifeng Zhang",
      "Jun Wang"
    ],
    "abstract": "Communication is a fundamental aspect of human society, facilitating the\nexchange of information and beliefs among people. Despite the advancements in\nlarge language models (LLMs), recent agents built with these often neglect the\ncontrol over discussion tactics, which are essential in communication scenarios\nand games. As a variant of the famous communication game Werewolf, One Night\nUltimate Werewolf (ONUW) requires players to develop strategic discussion\npolicies due to the potential role changes that increase the uncertainty and\ncomplexity of the game. In this work, we first present the existence of the\nPerfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with\ndiscussion and one without. The results showcase that the discussion greatly\nchanges players' utilities by affecting their beliefs, emphasizing the\nsignificance of discussion tactics. Based on the insights obtained from the\nanalyses, we propose an RL-instructed language agent framework, where a\ndiscussion policy trained by reinforcement learning (RL) is employed to\ndetermine appropriate discussion tactics to adopt. Our experimental results on\nseveral ONUW game settings demonstrate the effectiveness and generalizability\nof our proposed framework. The project page of our paper:\n$\\href{https://one-night-ultimate-werewolf.github.io}{one-night-ultimate-werewolf.github.io}$.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.19946v2",
    "published_date": "2024-05-30 11:07:06 UTC",
    "updated_date": "2025-01-12 08:25:44 UTC"
  },
  {
    "arxiv_id": "2405.19933v1",
    "title": "Learning Latent Graph Structures and their Uncertainty",
    "authors": [
      "Alessandro Manenti",
      "Daniele Zambon",
      "Cesare Alippi"
    ],
    "abstract": "Within a prediction task, Graph Neural Networks (GNNs) use relational\ninformation as an inductive bias to enhance the model's accuracy. As\ntask-relevant relations might be unknown, graph structure learning approaches\nhave been proposed to learn them while solving the downstream prediction task.\nIn this paper, we demonstrate that minimization of a point-prediction loss\nfunction, e.g., the mean absolute error, does not guarantee proper learning of\nthe latent relational information and its associated uncertainty. Conversely,\nwe prove that a suitable loss function on the stochastic model outputs\nsimultaneously grants (i) the unknown adjacency matrix latent distribution and\n(ii) optimal performance on the prediction task. Finally, we propose a\nsampling-based method that solves this joint learning task. Empirical results\nvalidate our theoretical claims and demonstrate the effectiveness of the\nproposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19933v1",
    "published_date": "2024-05-30 10:49:22 UTC",
    "updated_date": "2024-05-30 10:49:22 UTC"
  },
  {
    "arxiv_id": "2405.19931v1",
    "title": "Exploring Diffusion Models' Corruption Stage in Few-Shot Fine-tuning and Mitigating with Bayesian Neural Networks",
    "authors": [
      "Xiaoyu Wu",
      "Jiaru Zhang",
      "Yang Hua",
      "Bohan Lyu",
      "Hao Wang",
      "Tao Song",
      "Haibing Guan"
    ],
    "abstract": "Few-shot fine-tuning of Diffusion Models (DMs) is a key advancement,\nsignificantly reducing training costs and enabling personalized AI\napplications. However, we explore the training dynamics of DMs and observe an\nunanticipated phenomenon: during the training process, image fidelity initially\nimproves, then unexpectedly deteriorates with the emergence of noisy patterns,\nonly to recover later with severe overfitting. We term the stage with generated\nnoisy patterns as corruption stage. To understand this corruption stage, we\nbegin by theoretically modeling the one-shot fine-tuning scenario, and then\nextend this modeling to more general cases. Through this modeling, we identify\nthe primary cause of this corruption stage: a narrowed learning distribution\ninherent in the nature of few-shot fine-tuning. To tackle this, we apply\nBayesian Neural Networks (BNNs) on DMs with variational inference to implicitly\nbroaden the learned distribution, and present that the learning target of the\nBNNs can be naturally regarded as an expectation of the diffusion loss and a\nfurther regularization with the pretrained DMs. This approach is highly\ncompatible with current few-shot fine-tuning methods in DMs and does not\nintroduce any extra inference costs. Experimental results demonstrate that our\nmethod significantly mitigates corruption, and improves the fidelity, quality\nand diversity of the generated images in both object-driven and subject-driven\ngeneration tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2405.19931v1",
    "published_date": "2024-05-30 10:47:48 UTC",
    "updated_date": "2024-05-30 10:47:48 UTC"
  },
  {
    "arxiv_id": "2405.19915v1",
    "title": "P$^2$-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer",
    "authors": [
      "Huihong Shi",
      "Xin Cheng",
      "Wendong Mao",
      "Zhongfeng Wang"
    ],
    "abstract": "Vision Transformers (ViTs) have excelled in computer vision tasks but are\nmemory-consuming and computation-intensive, challenging their deployment on\nresource-constrained devices. To tackle this limitation, prior works have\nexplored ViT-tailored quantization algorithms but retained floating-point\nscaling factors, which yield non-negligible re-quantization overhead, limiting\nViTs' hardware efficiency and motivating more hardware-friendly solutions. To\nthis end, we propose \\emph{P$^2$-ViT}, the first \\underline{P}ower-of-Two (PoT)\n\\underline{p}ost-training quantization and acceleration framework to accelerate\nfully quantized ViTs. Specifically, {as for quantization,} we explore a\ndedicated quantization scheme to effectively quantize ViTs with PoT scaling\nfactors, thus minimizing the re-quantization overhead. Furthermore, we propose\ncoarse-to-fine automatic mixed-precision quantization to enable better\naccuracy-efficiency trade-offs. {In terms of hardware,} we develop {a dedicated\nchunk-based accelerator} featuring multiple tailored sub-processors to\nindividually handle ViTs' different types of operations, alleviating\nreconfigurable overhead. Additionally, we design {a tailored row-stationary\ndataflow} to seize the pipeline processing opportunity introduced by our PoT\nscaling factors, thereby enhancing throughput. Extensive experiments\nconsistently validate P$^2$-ViT's effectiveness. {Particularly, we offer\ncomparable or even superior quantization performance with PoT scaling factors\nwhen compared to the counterpart with floating-point scaling factors. Besides,\nwe achieve up to $\\mathbf{10.1\\times}$ speedup and $\\mathbf{36.8\\times}$ energy\nsaving over GPU's Turing Tensor Cores, and up to $\\mathbf{1.84\\times}$ higher\ncomputation utilization efficiency against SOTA quantization-based ViT\naccelerators. Codes are available at\n\\url{https://github.com/shihuihong214/P2-ViT}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19915v1",
    "published_date": "2024-05-30 10:26:36 UTC",
    "updated_date": "2024-05-30 10:26:36 UTC"
  },
  {
    "arxiv_id": "2405.19909v3",
    "title": "Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning",
    "authors": [
      "Tenglong Liu",
      "Yang Li",
      "Yixing Lan",
      "Hao Gao",
      "Wei Pan",
      "Xin Xu"
    ],
    "abstract": "In offline reinforcement learning, the challenge of out-of-distribution (OOD)\nis pronounced. To address this, existing methods often constrain the learned\npolicy through policy regularization. However, these methods often suffer from\nthe issue of unnecessary conservativeness, hampering policy improvement. This\noccurs due to the indiscriminate use of all actions from the behavior policy\nthat generates the offline dataset as constraints. The problem becomes\nparticularly noticeable when the quality of the dataset is suboptimal. Thus, we\npropose Adaptive Advantage-guided Policy Regularization (A2PR), obtaining\nhigh-advantage actions from an augmented behavior policy combined with VAE to\nguide the learned policy. A2PR can select high-advantage actions that differ\nfrom those present in the dataset, while still effectively maintaining\nconservatism from OOD actions. This is achieved by harnessing the VAE capacity\nto generate samples matching the distribution of the data points. We\ntheoretically prove that the improvement of the behavior policy is guaranteed.\nBesides, it effectively mitigates value overestimation with a bounded\nperformance gap. Empirically, we conduct a series of experiments on the D4RL\nbenchmark, where A2PR demonstrates state-of-the-art performance. Furthermore,\nexperimental results on additional suboptimal mixed datasets reveal that A2PR\nexhibits superior performance. Code is available at\nhttps://github.com/ltlhuuu/A2PR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024, 19 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.19909v3",
    "published_date": "2024-05-30 10:20:55 UTC",
    "updated_date": "2024-07-15 10:55:57 UTC"
  },
  {
    "arxiv_id": "2406.06553v1",
    "title": "Ensemble Model With Bert,Roberta and Xlnet For Molecular property prediction",
    "authors": [
      "Junling Hu"
    ],
    "abstract": "This paper presents a novel approach for predicting molecular properties with\nhigh accuracy without the need for extensive pre-training. Employing ensemble\nlearning and supervised fine-tuning of BERT, RoBERTa, and XLNet, our method\ndemonstrates significant effectiveness compared to existing advanced models.\nCrucially, it addresses the issue of limited computational resources faced by\nexperimental groups, enabling them to accurately predict molecular properties.\nThis innovation provides a cost-effective and resource-efficient solution,\npotentially advancing further research in the molecular domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages,7 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06553v1",
    "published_date": "2024-05-30 10:03:58 UTC",
    "updated_date": "2024-05-30 10:03:58 UTC"
  },
  {
    "arxiv_id": "2405.19899v1",
    "title": "Open-Set Domain Adaptation for Semantic Segmentation",
    "authors": [
      "Seun-An Choe",
      "Ah-Hyung Shin",
      "Keon-Hee Park",
      "Jinwoo Choi",
      "Gyeong-Moon Park"
    ],
    "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer the pixel-wise knowledge from the labeled source domain to the\nunlabeled target domain. However, current UDA methods typically assume a shared\nlabel space between source and target, limiting their applicability in\nreal-world scenarios where novel categories may emerge in the target domain. In\nthis paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation\n(OSDA-SS) for the first time, where the target domain includes unknown classes.\nWe identify two major problems in the OSDA-SS scenario as follows: 1) the\nexisting UDA methods struggle to predict the exact boundary of the unknown\nclasses, and 2) they fail to accurately predict the shape of the unknown\nclasses. To address these issues, we propose Boundary and Unknown Shape-Aware\nopen-set domain adaptation, coined BUS. Our BUS can accurately discern the\nboundaries between known and unknown classes in a contrastive manner using a\nnovel dilation-erosion-based contrastive loss. In addition, we propose\nOpenReMix, a new domain mixing augmentation method that guides our model to\neffectively learn domain and size-invariant features for improving the shape\ndetection of the known and unknown classes. Through extensive experiments, we\ndemonstrate that our proposed BUS effectively detects unknown classes in the\nchallenging OSDA-SS scenario compared to the previous methods by a large\nmargin. The code is available at https://github.com/KHU-AGI/BUS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures, 13 tables, CVPR 2024 Poster",
    "pdf_url": "http://arxiv.org/pdf/2405.19899v1",
    "published_date": "2024-05-30 09:55:19 UTC",
    "updated_date": "2024-05-30 09:55:19 UTC"
  },
  {
    "arxiv_id": "2405.19893v1",
    "title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts",
    "authors": [
      "Chunjing Gan",
      "Dan Yang",
      "Binbin Hu",
      "Hanxiao Zhang",
      "Siyuan Li",
      "Ziqi Liu",
      "Yue Shen",
      "Lin Ju",
      "Zhiqiang Zhang",
      "Jinjie Gu",
      "Lei Liang",
      "Jun Zhou"
    ],
    "abstract": "In recent years, large language models (LLMs) have made remarkable\nachievements in various domains. However, the untimeliness and cost of\nknowledge updates coupled with hallucination issues of LLMs have curtailed\ntheir applications in knowledge intensive tasks, where retrieval augmented\ngeneration (RAG) can be of help. Nevertheless, existing retrieval augmented\nmodels typically use similarity as a bridge between queries and documents and\nfollow a retrieve then read procedure. In this work, we argue that similarity\nis not always the panacea and totally relying on similarity would sometimes\ndegrade the performance of retrieval augmented generation. To this end, we\npropose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented\nGeneration framework. To begin with, beyond existing similarity oriented\nthought, we embrace a small scale utility model that draws supervision from an\nLLM for utility oriented thought and further come up with a smarter model by\ncomprehensively combining the similarity and utility oriented thoughts.\nFurthermore, given the fact that the retrieved document set tends to be huge\nand using them in isolation makes it difficult to capture the commonalities and\ncharacteristics among them, we propose to make an LLM as a task adaptive\nsummarizer to endow retrieval augmented generation with compactness-oriented\nthought. Finally, with multi layered thoughts from the precedent stages, an LLM\nis called for knowledge augmented generation. Extensive experiments on\nknowledge-intensive tasks have demonstrated the superiority of MetRag.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.19893v1",
    "published_date": "2024-05-30 09:50:38 UTC",
    "updated_date": "2024-05-30 09:50:38 UTC"
  },
  {
    "arxiv_id": "2405.19888v1",
    "title": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
    "authors": [
      "Chaofan Lin",
      "Zhenhua Han",
      "Chengruidong Zhang",
      "Yuqing Yang",
      "Fan Yang",
      "Chen Chen",
      "Lili Qiu"
    ],
    "abstract": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear on USENIX OSDI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19888v1",
    "published_date": "2024-05-30 09:46:36 UTC",
    "updated_date": "2024-05-30 09:46:36 UTC"
  },
  {
    "arxiv_id": "2405.19883v2",
    "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
    "authors": [
      "Jianliang He",
      "Siyu Chen",
      "Fengzhuo Zhang",
      "Zhuoran Yang"
    ],
    "abstract": "In this work, from a theoretical lens, we aim to understand why large\nlanguage model (LLM) empowered agents are able to solve decision-making\nproblems in the physical world. To this end, consider a hierarchical\nreinforcement learning (RL) model where the LLM Planner and the Actor perform\nhigh-level task planning and low-level execution, respectively. Under this\nmodel, the LLM Planner navigates a partially observable Markov decision process\n(POMDP) by iteratively generating language-based subgoals via prompting. Under\nproper assumptions on the pretraining data, we prove that the pretrained LLM\nPlanner effectively performs Bayesian aggregated imitation learning (BAIL)\nthrough in-context learning. Additionally, we highlight the necessity for\nexploration beyond the subgoals derived from BAIL by proving that naively\nexecuting the subgoals returned by LLM leads to a linear regret. As a remedy,\nwe introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven\nto incur sublinear regret when the pretraining error is small. Finally, we\nextend our theoretical framework to include scenarios where the LLM Planner\nserves as a world model for inferring the transition model of the environment\nand to multi-agent settings, enabling coordination among multiple Actors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "47 pages, accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19883v2",
    "published_date": "2024-05-30 09:42:54 UTC",
    "updated_date": "2024-07-20 06:00:22 UTC"
  },
  {
    "arxiv_id": "2405.19877v1",
    "title": "KNOW: A Real-World Ontology for Knowledge Capture with Large Language Models",
    "authors": [
      "Arto Bendiken"
    ],
    "abstract": "We present KNOW--the Knowledge Navigator Ontology for the World--the first\nontology designed to capture everyday knowledge to augment large language\nmodels (LLMs) in real-world generative AI use cases such as personal AI\nassistants. Our domain is human life, both its everyday concerns and its major\nmilestones. We have limited the initial scope of the modeled concepts to only\nestablished human universals: spacetime (places, events) plus social (people,\ngroups, organizations). The inclusion criteria for modeled concepts are\npragmatic, beginning with universality and utility. We compare and contrast\nprevious work such as Schema.org and Cyc--as well as attempts at a synthesis of\nknowledge graphs and language models--noting how LLMs already encode internally\nmuch of the commonsense tacit knowledge that took decades to capture in the Cyc\nproject. We also make available code-generated software libraries for the 12\nmost popular programming languages, enabling the direct use of ontology\nconcepts in software engineering. We emphasize simplicity and developer\nexperience in promoting AI interoperability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.4; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2405.19877v1",
    "published_date": "2024-05-30 09:32:14 UTC",
    "updated_date": "2024-05-30 09:32:14 UTC"
  },
  {
    "arxiv_id": "2405.19874v3",
    "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
    "authors": [
      "Hao Zhao",
      "Maksym Andriushchenko",
      "Francesco Croce",
      "Nicolas Flammarion"
    ],
    "abstract": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025. This camera-ready version v3 adds multi-turn\n  alignment via ICL, revisiting main results on instruct models, and simple\n  mechanistic study. Updates in the v2: experiment with decoding schemes,\n  scaling in-context alignment, ICL vs IFT for instruction following. Code at\n  https://github.com/tml-epfl/icl-alignment",
    "pdf_url": "http://arxiv.org/pdf/2405.19874v3",
    "published_date": "2024-05-30 09:28:56 UTC",
    "updated_date": "2025-04-18 12:31:18 UTC"
  },
  {
    "arxiv_id": "2405.19864v1",
    "title": "Out-of-distribution Reject Option Method for Dataset Shift Problem in Early Disease Onset Prediction",
    "authors": [
      "Taisei Tosaki",
      "Eiichiro Uchino",
      "Ryosuke Kojima",
      "Yohei Mineharu",
      "Mikio Arita",
      "Nobuyuki Miyai",
      "Yoshinori Tamada",
      "Tatsuya Mikami",
      "Koichi Murashita",
      "Shigeyuki Nakaji",
      "Yasushi Okuno"
    ],
    "abstract": "Machine learning is increasingly used to predict lifestyle-related disease\nonset using health and medical data. However, the prediction effectiveness is\nhindered by dataset shift, which involves discrepancies in data distribution\nbetween the training and testing datasets, misclassifying out-of-distribution\n(OOD) data. To diminish dataset shift effects, this paper proposes the\nout-of-distribution reject option for prediction (ODROP), which integrates OOD\ndetection models to preclude OOD data from the prediction phase. We\ninvestigated the efficacy of five OOD detection methods (variational\nautoencoder, neural network ensemble std, neural network ensemble epistemic,\nneural network energy, and neural network gaussian mixture based energy\nmeasurement) across two datasets, the Hirosaki and Wakayama health checkup\ndata, in the context of three disease onset prediction tasks: diabetes,\ndyslipidemia, and hypertension. To evaluate the ODROP method, we trained\ndisease onset prediction models and OOD detection models on Hirosaki data and\nused AUROC-rejection curve plots from Wakayama data. The variational\nautoencoder method showed superior stability and magnitude of improvement in\nArea Under the Receiver Operating Curve (AUROC) in five cases: AUROC in the\nWakayama data was improved from 0.80 to 0.90 at a 31.1% rejection rate for\ndiabetes onset and from 0.70 to 0.76 at a 34% rejection rate for dyslipidemia.\nWe categorized dataset shifts into two types using SHAP clustering - those that\nconsiderably affect predictions and those that do not. We expect that this\nclassification will help standardize measuring instruments. This study is the\nfirst to apply OOD detection to actual health and medical data, demonstrating\nits potential to substantially improve the accuracy and reliability of disease\nprediction models amidst dataset shift.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19864v1",
    "published_date": "2024-05-30 09:14:01 UTC",
    "updated_date": "2024-05-30 09:14:01 UTC"
  },
  {
    "arxiv_id": "2407.06157v1",
    "title": "Temporal Grounding of Activities using Multimodal Large Language Models",
    "authors": [
      "Young Chol Song"
    ],
    "abstract": "Temporal grounding of activities, the identification of specific time\nintervals of actions within a larger event context, is a critical task in video\nunderstanding. Recent advancements in multimodal large language models (LLMs)\noffer new opportunities for enhancing temporal reasoning capabilities. In this\npaper, we evaluate the effectiveness of combining image-based and text-based\nlarge language models (LLMs) in a two-stage approach for temporal activity\nlocalization. We demonstrate that our method outperforms existing video-based\nLLMs. Furthermore, we explore the impact of instruction-tuning on a smaller\nmultimodal LLM, showing that refining its ability to process action queries\nleads to more expressive and informative outputs, thereby enhancing its\nperformance in identifying specific time intervals of activities. Our\nexperimental results on the Charades-STA dataset highlight the potential of\nthis approach in advancing the field of temporal activity localization and\nvideo understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06157v1",
    "published_date": "2024-05-30 09:11:02 UTC",
    "updated_date": "2024-05-30 09:11:02 UTC"
  },
  {
    "arxiv_id": "2405.19861v1",
    "title": "Hierarchical Object-Centric Learning with Capsule Networks",
    "authors": [
      "Riccardo Renzulli"
    ],
    "abstract": "Capsule networks (CapsNets) were introduced to address convolutional neural\nnetworks limitations, learning object-centric representations that are more\nrobust, pose-aware, and interpretable. They organize neurons into groups called\ncapsules, where each capsule encodes the instantiation parameters of an object\nor one of its parts. Moreover, a routing algorithm connects capsules in\ndifferent layers, thereby capturing hierarchical part-whole relationships in\nthe data.\n  This thesis investigates the intriguing aspects of CapsNets and focuses on\nthree key questions to unlock their full potential. First, we explore the\neffectiveness of the routing algorithm, particularly in small-sized networks.\nWe propose a novel method that anneals the number of routing iterations during\ntraining, enhancing performance in architectures with fewer parameters.\n  Secondly, we investigate methods to extract more effective first-layer\ncapsules, also known as primary capsules. By exploiting pruned backbones, we\naim to improve computational efficiency by reducing the number of capsules\nwhile achieving high generalization. This approach reduces CapsNets memory\nrequirements and computational effort.\n  Third, we explore part-relationship learning in CapsNets. Through extensive\nresearch, we demonstrate that capsules with low entropy can extract more\nconcise and discriminative part-whole relationships compared to traditional\ncapsule networks, even with reasonable network sizes.\n  Lastly, we showcase how CapsNets can be utilized in real-world applications,\nincluding autonomous localization of unmanned aerial vehicles, quaternion-based\nrotations prediction in synthetic datasets, and lung nodule segmentation in\nbiomedical imaging.\n  The findings presented in this thesis contribute to a deeper understanding of\nCapsNets and highlight their potential to address complex computer vision\nchallenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Updated version of my PhD thesis (Nov 2023), with fixed typos. Will\n  keep updated as new typos are discovered!",
    "pdf_url": "http://arxiv.org/pdf/2405.19861v1",
    "published_date": "2024-05-30 09:10:33 UTC",
    "updated_date": "2024-05-30 09:10:33 UTC"
  },
  {
    "arxiv_id": "2405.19850v1",
    "title": "Deciphering Human Mobility: Inferring Semantics of Trajectories with Large Language Models",
    "authors": [
      "Yuxiao Luo",
      "Zhongcai Cao",
      "Xin Jin",
      "Kang Liu",
      "Ling Yin"
    ],
    "abstract": "Understanding human mobility patterns is essential for various applications,\nfrom urban planning to public safety. The individual trajectory such as mobile\nphone location data, while rich in spatio-temporal information, often lacks\nsemantic detail, limiting its utility for in-depth mobility analysis. Existing\nmethods can infer basic routine activity sequences from this data, lacking\ndepth in understanding complex human behaviors and users' characteristics.\nAdditionally, they struggle with the dependency on hard-to-obtain auxiliary\ndatasets like travel surveys. To address these limitations, this paper defines\ntrajectory semantic inference through three key dimensions: user occupation\ncategory, activity sequence, and trajectory description, and proposes the\nTrajectory Semantic Inference with Large Language Models (TSI-LLM) framework to\nleverage LLMs infer trajectory semantics comprehensively and deeply. We adopt\nspatio-temporal attributes enhanced data formatting (STFormat) and design a\ncontext-inclusive prompt, enabling LLMs to more effectively interpret and infer\nthe semantics of trajectory data. Experimental validation on real-world\ntrajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex\nhuman mobility patterns. This study explores the potential of LLMs in enhancing\nthe semantic analysis of trajectory data, paving the way for more sophisticated\nand accessible human mobility research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19850v1",
    "published_date": "2024-05-30 08:55:48 UTC",
    "updated_date": "2024-05-30 08:55:48 UTC"
  },
  {
    "arxiv_id": "2405.19846v7",
    "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
    "authors": [
      "Chaochen Gao",
      "Xing Wu",
      "Qi Fu",
      "Songlin Hu"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.19846v7",
    "published_date": "2024-05-30 08:50:55 UTC",
    "updated_date": "2025-02-11 06:22:30 UTC"
  },
  {
    "arxiv_id": "2405.19842v1",
    "title": "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation",
    "authors": [
      "Chengwei Dai",
      "Kun Li",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "abstract": "Large language models (LLMs) exhibit enhanced reasoning at larger scales,\ndriving efforts to distill these capabilities into smaller models via\nteacher-student learning. Previous works simply fine-tune student models on\nteachers' generated Chain-of-Thoughts (CoTs) data. Although these methods\nenhance in-domain (IND) reasoning performance, they struggle to generalize to\nout-of-domain (OOD) tasks. We believe that the widespread spurious correlations\nbetween questions and answers may lead the model to preset a specific answer\nwhich restricts the diversity and generalizability of its reasoning process. In\nthis paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to\naddress these issues by decomposing the traditional single-step learning\nprocess into two cascaded learning steps. Specifically, by restructuring the\ntraining objectives -- removing the answer from outputs and concatenating the\nquestion with the rationale as input -- CasCoD's two-step learning process\nensures that students focus on learning rationales without interference from\nthe preset answers, thus improving reasoning generalizability. Extensive\nexperiments demonstrate the effectiveness of CasCoD on both IND and OOD\nbenchmark reasoning datasets. Code can be found at\nhttps://github.com/C-W-D/CasCoD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19842v1",
    "published_date": "2024-05-30 08:49:34 UTC",
    "updated_date": "2024-05-30 08:49:34 UTC"
  },
  {
    "arxiv_id": "2405.19837v1",
    "title": "Lifelong learning challenges in the era of artificial intelligence: a computational thinking perspective",
    "authors": [
      "Margarida Romero"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) has brought significant\nchallenges to the education and workforce skills required to take advantage of\nAI for human-AI collaboration in the workplace. As AI continues to reshape\nindustries and job markets, the need to define how AI literacy can be\nconsidered in lifelong learning has become increasingly critical (Cetindamar et\nal., 2022; Laupichler et al., 2022; Romero et al., 2023). Like any new\ntechnology, AI is the subject of both hopes and fears, and what it entails\ntoday presents major challenges (Cugurullo \\& Acheampong, 2023; Villani et al.,\n2018). It also raises profound questions about our own humanity. Will the\nmachine surpass the intelligence of the humans who designed it? What will be\nthe relationship between so-called AI and our human intelligences? How could\nhuman-AI collaboration be regulated in a way that serves the Sustainable\nDevelopment Goals (SDGs)? This paper provides a review of the challenges of\nlifelong learning in the era of AI from a computational thinking, critical\nthinking, and creative competencies perspective, highlighting the implications\nfor management and leadership in organizations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19837v1",
    "published_date": "2024-05-30 08:46:11 UTC",
    "updated_date": "2024-05-30 08:46:11 UTC"
  },
  {
    "arxiv_id": "2405.19832v2",
    "title": "AI Safety: A Climb To Armageddon?",
    "authors": [
      "Herman Cappelen",
      "Josh Dever",
      "John Hawthorne"
    ],
    "abstract": "This paper presents an argument that certain AI safety measures, rather than\nmitigating existential risk, may instead exacerbate it. Under certain key\nassumptions - the inevitability of AI failure, the expected correlation between\nan AI system's power at the point of failure and the severity of the resulting\nharm, and the tendency of safety measures to enable AI systems to become more\npowerful before failing - safety efforts have negative expected utility. The\npaper examines three response strategies: Optimism, Mitigation, and Holism.\nEach faces challenges stemming from intrinsic features of the AI safety\nlandscape that we term Bottlenecking, the Perfection Barrier, and Equilibrium\nFluctuation. The surprising robustness of the argument forces a re-examination\nof core assumptions around AI safety and points to several avenues for further\nresearch.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 page article",
    "pdf_url": "http://arxiv.org/pdf/2405.19832v2",
    "published_date": "2024-05-30 08:41:54 UTC",
    "updated_date": "2024-06-02 22:32:46 UTC"
  },
  {
    "arxiv_id": "2405.19823v2",
    "title": "Joint Selective State Space Model and Detrending for Robust Time Series Anomaly Detection",
    "authors": [
      "Junqi Chen",
      "Xu Tan",
      "Sylwan Rahardja",
      "Jiawei Yang",
      "Susanto Rahardja"
    ],
    "abstract": "Deep learning-based sequence models are extensively employed in Time Series\nAnomaly Detection (TSAD) tasks due to their effective sequential modeling\ncapabilities. However, the ability of TSAD is limited by two key challenges:\n(i) the ability to model long-range dependency and (ii) the generalization\nissue in the presence of non-stationary data. To tackle these challenges, an\nanomaly detector that leverages the selective state space model known for its\nproficiency in capturing long-term dependencies across various domains is\nproposed. Additionally, a multi-stage detrending mechanism is introduced to\nmitigate the prominent trend component in non-stationary data to address the\ngeneralization issue. Extensive experiments conducted on realworld public\ndatasets demonstrate that the proposed methods surpass all 12 compared baseline\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE Signal Processing Letters.\n  DOI:10.1109/LSP.2024.3438078",
    "pdf_url": "http://arxiv.org/pdf/2405.19823v2",
    "published_date": "2024-05-30 08:31:18 UTC",
    "updated_date": "2024-08-20 08:00:02 UTC"
  },
  {
    "arxiv_id": "2405.19822v1",
    "title": "Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology",
    "authors": [
      "Frank A. Ruis",
      "Alma M. Liezenga",
      "Friso G. Heslinga",
      "Luca Ballan",
      "Thijs A. Eker",
      "Richard J. M. den Hollander",
      "Martin C. van Leeuwen",
      "Judith Dijk",
      "Wyke Huizinga"
    ],
    "abstract": "Collecting and annotating real-world data for the development of object\ndetection models is a time-consuming and expensive process. In the military\ndomain in particular, data collection can also be dangerous or infeasible.\nTraining models on synthetic data may provide a solution for cases where access\nto real-world training data is restricted. However, bridging the reality gap\nbetween synthetic and real data remains a challenge. Existing methods usually\nbuild on top of baseline Convolutional Neural Network (CNN) models that have\nbeen shown to perform well when trained on real data, but have limited ability\nto perform well when trained on synthetic data. For example, some architectures\nallow for fine-tuning with the expectation of large quantities of training data\nand are prone to overfitting on synthetic data. Related work usually ignores\nvarious best practices from object detection on real data, e.g. by training on\nsynthetic data from a single environment with relatively little variation. In\nthis paper we propose a methodology for improving the performance of a\npre-trained object detector when training on synthetic data. Our approach\nfocuses on extracting the salient information from synthetic data without\nforgetting useful features learned from pre-training on real images. Based on\nthe state of the art, we incorporate data augmentation methods and a\nTransformer backbone. Besides reaching relatively strong performance without\nany specialized synthetic data transfer methods, we show that our methods\nimprove the state of the art on synthetic data trained object detection for the\nRarePlanes and DGTA-VisDrone datasets, and reach near-perfect performance on an\nin-house vehicle detection dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to and presented at SPIE Defense + Commercial Sensing 2024,\n  13 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.19822v1",
    "published_date": "2024-05-30 08:31:01 UTC",
    "updated_date": "2024-05-30 08:31:01 UTC"
  },
  {
    "arxiv_id": "2405.19818v1",
    "title": "WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark",
    "authors": [
      "Chunhui Zhang",
      "Li Liu",
      "Guanjie Huang",
      "Hao Wen",
      "Xi Zhou",
      "Yanfeng Wang"
    ],
    "abstract": "Underwater object tracking (UOT) is a foundational task for identifying and\ntracing submerged entities in underwater video sequences. However, current UOT\ndatasets suffer from limitations in scale, diversity of target categories and\nscenarios covered, hindering the training and evaluation of modern tracking\nalgorithms. To bridge this gap, we take the first step and introduce WebUOT-1M,\n\\ie, the largest public UOT benchmark to date, sourced from complex and\nrealistic underwater environments. It comprises 1.1 million frames across 1,500\nvideo clips filtered from 408 target categories, largely surpassing previous\nUOT datasets, \\eg, UVOT400. Through meticulous manual annotation and\nverification, we provide high-quality bounding boxes for underwater targets.\nAdditionally, WebUOT-1M includes language prompts for video sequences,\nexpanding its application areas, \\eg, underwater vision-language tracking. Most\nexisting trackers are tailored for open-air environments, leading to\nperformance degradation when applied to UOT due to domain gaps. Retraining and\nfine-tuning these trackers are challenging due to sample imbalances and limited\nreal-world underwater datasets. To tackle these challenges, we propose a novel\nomni-knowledge distillation framework based on WebUOT-1M, incorporating various\nstrategies to guide the learning of the student Transformer. To the best of our\nknowledge, this framework is the first to effectively transfer open-air domain\nknowledge to the UOT model through knowledge distillation, as demonstrated by\nresults on both existing UOT datasets and the newly proposed WebUOT-1M.\nFurthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers,\nshowcasing its value as a benchmark for UOT research by presenting new\nchallenges and opportunities for future studies. The complete dataset, codes\nand tracking results, will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "GitHub project:\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking",
    "pdf_url": "http://arxiv.org/pdf/2405.19818v1",
    "published_date": "2024-05-30 08:25:21 UTC",
    "updated_date": "2024-05-30 08:25:21 UTC"
  },
  {
    "arxiv_id": "2405.19816v2",
    "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally",
    "authors": [
      "Manon Verbockhaven",
      "Sylvain Chevallier",
      "Guillaume Charpiat",
      "Théo Rudkiewicz"
    ],
    "abstract": "Machine learning tasks are generally formulated as optimization problems,\nwhere one searches for an optimal function within a certain functional space.\nIn practice, parameterized functional spaces are considered, in order to be\nable to perform gradient descent. Typically, a neural network architecture is\nchosen and fixed, and its parameters (connection weights) are optimized,\nyielding an architecture-dependent result. This way of proceeding however\nforces the evolution of the function during training to lie within the realm of\nwhat is expressible with the chosen architecture, and prevents any optimization\nacross architectures. Costly architectural hyper-parameter optimization is\noften performed to compensate for this. Instead, we propose to adapt the\narchitecture on the fly during training. We show that the information about\ndesirable architectural changes, due to expressivity bottlenecks when\nattempting to follow the functional gradient, can be extracted from\nbackpropagation. To do this, we propose a mathematical definition of\nexpressivity bottlenecks, which enables us to detect, quantify and solve them\nwhile training, by adding suitable neurons. Thus, while the standard approach\nrequires large networks, in terms of number of neurons per layer, for\nexpressivity and optimization reasons, we provide tools and properties to\ndevelop an architecture starting with a very small number of neurons. As a\nproof of concept, we show results~on the CIFAR dataset, matching large neural\nnetwork accuracy, with competitive training time, while removing the need for\nstandard architectural hyper-parameter search.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19816v2",
    "published_date": "2024-05-30 08:23:56 UTC",
    "updated_date": "2024-12-12 10:36:19 UTC"
  },
  {
    "arxiv_id": "2405.19815v1",
    "title": "Efficient Stimuli Generation using Reinforcement Learning in Design Verification",
    "authors": [
      "Deepak Narayan Gadde",
      "Thomas Nalapat",
      "Aman Kumar",
      "Djones Lettnin",
      "Wolfgang Kunz",
      "Sebastian Simon"
    ],
    "abstract": "The increasing design complexity of System-on-Chips (SoCs) has led to\nsignificant verification challenges, particularly in meeting coverage targets\nwithin a timely manner. At present, coverage closure is heavily dependent on\nconstrained random and coverage driven verification methodologies where the\nrandomized stimuli are bounded to verify certain scenarios and to reach\ncoverage goals. This process is said to be exhaustive and to consume a lot of\nproject time. In this paper, a novel methodology is proposed to generate\nefficient stimuli with the help of Reinforcement Learning (RL) to reach the\nmaximum code coverage of the Design Under Verification (DUV). Additionally, an\nautomated framework is created using metamodeling to generate a SystemVerilog\ntestbench and an RL environment for any given design. The proposed approach is\napplied to various designs and the produced results proves that the RL agent\nprovides effective stimuli to achieve code coverage faster in comparison with\nbaseline random simulations. Furthermore, various RL agents and reward schemes\nare analyzed in our work.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at the 20th International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD'24), Jul 2-5 2024, Volos, Greece",
    "pdf_url": "http://arxiv.org/pdf/2405.19815v1",
    "published_date": "2024-05-30 08:23:04 UTC",
    "updated_date": "2024-05-30 08:23:04 UTC"
  },
  {
    "arxiv_id": "2405.19808v2",
    "title": "AI with Alien Content and Alien Metasemantics",
    "authors": [
      "Herman Cappelen",
      "Josh Dever"
    ],
    "abstract": "AlphaGo plays chess and Go in a creative and novel way. It is natural for us\nto attribute contents to it, such as that it doesn't view being several pawns\nbehind, if it has more board space, as bad. The framework introduced in\nCappelen and Dever (2021) provides a way of thinking about the semantics and\nthe metasemantics of AI content: does AlphaGo entertain contents like this, and\nif so, in virtue of what does a given state of the program mean that particular\ncontent? One salient question Cappelen and Dever didn't consider was the\npossibility of alien content. Alien content is content that is not or cannot be\nexpressed by human beings. It's highly plausible that AlphaGo, or any other\nsophisticated AI system, expresses alien contents. That this is so, moreover,\nis plausibly a metasemantic fact: a fact that has to do with how AI comes to\nentertain content in the first place, one that will heed the vastly different\netiology of AI and human content. This chapter explores the question of alien\ncontent in AI from a semantic and metasemantic perspective. It lays out the\nlogical space of possible responses to the semantic and metasemantic questions\nalien content poses, considers whether and how we humans could communicate with\nentities who express alien content, and points out that getting clear about\nsuch questions might be important for more 'applied' issues in the philosophy\nof AI, such as existential risk and XAI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, book chapter",
    "pdf_url": "http://arxiv.org/pdf/2405.19808v2",
    "published_date": "2024-05-30 08:17:15 UTC",
    "updated_date": "2024-06-02 22:27:50 UTC"
  },
  {
    "arxiv_id": "2405.19796v1",
    "title": "Explainable Attribute-Based Speaker Verification",
    "authors": [
      "Xiaoliang Wu",
      "Chau Luu",
      "Peter Bell",
      "Ajitha Rajan"
    ],
    "abstract": "This paper proposes a fully explainable approach to speaker verification\n(SV), a task that fundamentally relies on individual speaker characteristics.\nThe opaque use of speaker attributes in current SV systems raises concerns of\ntrust. Addressing this, we propose an attribute-based explainable SV system\nthat identifies speakers by comparing personal attributes such as gender,\nnationality, and age extracted automatically from voice recordings. We believe\nthis approach better aligns with human reasoning, making it more understandable\nthan traditional methods. Evaluated on the Voxceleb1 test set, the best\nperformance of our system is comparable with the ground truth established when\nusing all correct attributes, proving its efficacy. Whilst our approach\nsacrifices some performance compared to non-explainable methods, we believe\nthat it moves us closer to the goal of transparent, interpretable AI and lays\nthe groundwork for future enhancements through attribute expansion.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19796v1",
    "published_date": "2024-05-30 08:04:28 UTC",
    "updated_date": "2024-05-30 08:04:28 UTC"
  },
  {
    "arxiv_id": "2405.19795v1",
    "title": "SLM as Guardian: Pioneering AI Safety with Small Language Models",
    "authors": [
      "Ohjoon Kwon",
      "Donghyeon Jeon",
      "Nayoung Choi",
      "Gyu-Hwung Cho",
      "Changbong Kim",
      "Hyunwoo Lee",
      "Inho Kang",
      "Sun Kim",
      "Taiwoo Park"
    ],
    "abstract": "Most prior safety research of large language models (LLMs) has focused on\nenhancing the alignment of LLMs to better suit the safety requirements of\nhumans. However, internalizing such safeguard features into larger models\nbrought challenges of higher training cost and unintended degradation of\nhelpfulness. To overcome such challenges, a modular approach employing a\nsmaller LLM to detect harmful user queries is regarded as a convenient solution\nin designing LLM-based system with safety requirements.\n  In this paper, we leverage a smaller LLM for both harmful query detection and\nsafeguard response generation. We introduce our safety requirements and the\ntaxonomy of harmfulness categories, and then propose a multi-task learning\nmechanism fusing the two tasks into a single model. We demonstrate the\neffectiveness of our approach, providing on par or surpassing harmful query\ndetection and safeguard response performance compared to the publicly available\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19795v1",
    "published_date": "2024-05-30 08:03:15 UTC",
    "updated_date": "2024-05-30 08:03:15 UTC"
  },
  {
    "arxiv_id": "2405.19787v2",
    "title": "From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers",
    "authors": [
      "Dylan Zhang",
      "Justin Wang",
      "Francois Charton"
    ],
    "abstract": "Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.PL"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19787v2",
    "published_date": "2024-05-30 07:54:07 UTC",
    "updated_date": "2024-05-31 01:23:41 UTC"
  },
  {
    "arxiv_id": "2405.19784v2",
    "title": "PixelsDB: Serverless and NL-Aided Data Analytics with Flexible Service Levels and Prices",
    "authors": [
      "Haoqiong Bian",
      "Dongyang Geng",
      "Haoyang Li",
      "Yunpeng Chai",
      "Anastasia Ailamaki"
    ],
    "abstract": "Serverless query processing has become increasingly popular due to its\nadvantages, including automated resource management, high elasticity, and\npay-as-you-go pricing. For users who are not system experts, serverless query\nprocessing greatly reduces the cost of owning a data analytic system. However,\nit is still a significant challenge for non-expert users to transform their\ncomplex and evolving data analytic needs into proper SQL queries and select a\nserverless query service that delivers satisfactory performance and price for\neach type of query.\n  This paper presents PixelsDB, an open-source data analytic system that allows\nusers who lack system or SQL expertise to explore data efficiently. It allows\nusers to generate and debug SQL queries using a natural language interface\npowered by fine-tuned language models. The queries are then executed by a\nserverless query engine that offers varying prices for different performance\nservice levels (SLAs). The performance SLAs are natively supported by dedicated\narchitecture design and heterogeneous resource scheduling that can apply\ncost-efficient resources to process non-urgent queries. We demonstrate that the\ncombination of a serverless paradigm, a natural-language-aided interface, and\nflexible SLAs and prices will substantially improve the usability of cloud data\nanalytic systems.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.DC",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "4 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.19784v2",
    "published_date": "2024-05-30 07:48:43 UTC",
    "updated_date": "2024-12-23 06:44:10 UTC"
  },
  {
    "arxiv_id": "2405.19783v2",
    "title": "Instruction-Guided Visual Masking",
    "authors": [
      "Jinliang Zheng",
      "Jianxiong Li",
      "Sijie Cheng",
      "Yinan Zheng",
      "Jiaming Li",
      "Jihao Liu",
      "Yu Liu",
      "Jingjing Liu",
      "Xianyuan Zhan"
    ],
    "abstract": "Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19783v2",
    "published_date": "2024-05-30 07:48:32 UTC",
    "updated_date": "2024-10-16 09:28:22 UTC"
  },
  {
    "arxiv_id": "2405.19778v5",
    "title": "CharacterGPT: A Persona Reconstruction Framework for Role-Playing Agents",
    "authors": [
      "Jeiyoon Park",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "The recent introduction of the Assistants API highlights its potential for\nlarge language models (LLMs) in role-playing agents (RPA). However, maintaining\nconsistent character personas remains a significant challenge due to\nvariability in information extraction, which frequently omits critical elements\nsuch as backstory or interpersonal relationships. To address this limitation,\nwe introduce CharacterGPT, a framework designed to dynamically reconstruct\ncharacter personas through Character Persona Training (CPT). This approach\nincrementally updates personas by extracting traits from chapter-wise novel\nsummaries, reflecting the progression of the narrative. Our framework is\nevaluated through Big Five personality evaluations and creative tasks, in which\ncharacters generate original narratives, demonstrating the efficacy of\nCharacterGPT in preserving persona consistency. The code and results are\navailable at https://github.com/Jeiyoon/charactergpt",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Industry Track (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2405.19778v5",
    "published_date": "2024-05-30 07:44:16 UTC",
    "updated_date": "2025-02-23 04:46:27 UTC"
  },
  {
    "arxiv_id": "2405.19765v1",
    "title": "Towards Unified Multi-granularity Text Detection with Interactive Attention",
    "authors": [
      "Xingyu Wan",
      "Chengquan Zhang",
      "Pengyuan Lyu",
      "Sen Fan",
      "Zihan Ni",
      "Kun Yao",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "Existing OCR engines or document image analysis systems typically rely on\ntraining separate models for text detection in varying scenarios and\ngranularities, leading to significant computational complexity and resource\ndemands. In this paper, we introduce \"Detect Any Text\" (DAT), an advanced\nparadigm that seamlessly unifies scene text detection, layout analysis, and\ndocument page detection into a cohesive, end-to-end model. This design enables\nDAT to efficiently manage text instances at different granularities, including\n*word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the\nacross-granularity interactive attention module, which significantly enhances\nthe representation learning of text instances at varying granularities by\ncorrelating structural information across different text queries. As a result,\nit enables the model to achieve mutually beneficial detection performances\nacross multiple text granularities. Additionally, a prompt-based segmentation\nmodule refines detection outcomes for texts of arbitrary curvature and complex\nlayouts, thereby improving DAT's accuracy and expanding its real-world\napplicability. Experimental results demonstrate that DAT achieves\nstate-of-the-art performances across a variety of text-related benchmarks,\nincluding multi-oriented/arbitrarily-shaped scene text detection, document\nlayout analysis and page detection tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19765v1",
    "published_date": "2024-05-30 07:25:23 UTC",
    "updated_date": "2024-05-30 07:25:23 UTC"
  },
  {
    "arxiv_id": "2405.19761v2",
    "title": "Revisiting CNNs for Trajectory Similarity Learning",
    "authors": [
      "Zhihao Chang",
      "Linzhu Yu",
      "Huan Li",
      "Sai Wu",
      "Gang Chen",
      "Dongxiang Zhang"
    ],
    "abstract": "Similarity search is a fundamental but expensive operator in querying\ntrajectory data, due to its quadratic complexity of distance computation. To\nmitigate the computational burden for long trajectories, neural networks have\nbeen widely employed for similarity learning and each trajectory is encoded as\na high-dimensional vector for similarity search with linear complexity. Given\nthe sequential nature of trajectory data, previous efforts have been primarily\ndevoted to the utilization of RNNs or Transformers.\n  In this paper, we argue that the common practice of treating trajectory as\nsequential data results in excessive attention to capturing long-term global\ndependency between two sequences. Instead, our investigation reveals the\npivotal role of local similarity, prompting a revisit of simple CNNs for\ntrajectory similarity learning. We introduce ConvTraj, incorporating both 1D\nand 2D convolutions to capture sequential and geo-distribution features of\ntrajectories, respectively. In addition, we conduct a series of theoretical\nanalyses to justify the effectiveness of ConvTraj. Experimental results on four\nreal-world large-scale datasets demonstrate that ConvTraj achieves\nstate-of-the-art accuracy in trajectory similarity search. Owing to the simple\nnetwork structure of ConvTraj, the training and inference speed on the Porto\ndataset with 1.6 million trajectories are increased by at least $240$x and\n$2.16$x, respectively. The source code and dataset can be found at\n\\textit{\\url{https://github.com/Proudc/ConvTraj}}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19761v2",
    "published_date": "2024-05-30 07:16:03 UTC",
    "updated_date": "2024-11-05 05:25:17 UTC"
  },
  {
    "arxiv_id": "2405.19757v3",
    "title": "Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise Filtering",
    "authors": [
      "Sungchul Hong",
      "Seunghwan An",
      "Jong-June Jeon"
    ],
    "abstract": "Recent advances in a generative neural network model extend the development\nof data augmentation methods. However, the augmentation methods based on the\nmodern generative models fail to achieve notable performance for class\nimbalance data compared to the conventional model, Synthetic Minority\nOversampling Technique (SMOTE). We investigate the problem of the generative\nmodel for imbalanced classification and introduce a framework to enhance the\nSMOTE algorithm using Variational Autoencoders (VAE). Our approach\nsystematically quantifies the density of data points in a low-dimensional\nlatent space using the VAE, simultaneously incorporating information on class\nlabels and classification difficulty. Then, the data points potentially\ndegrading the augmentation are systematically excluded, and the neighboring\nobservations are directly augmented on the data space. Empirical studies on\nseveral imbalanced datasets represent that this simple process innovatively\nimproves the conventional SMOTE algorithm over the deep learning models.\nConsequently, we conclude that the selection of minority data and the\ninterpolation in the data space are beneficial for imbalanced classification\nproblems with a relatively small number of data points.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19757v3",
    "published_date": "2024-05-30 07:06:02 UTC",
    "updated_date": "2024-08-26 05:54:22 UTC"
  },
  {
    "arxiv_id": "2405.19754v1",
    "title": "Mitigating annotation shift in cancer classification using single image generative models",
    "authors": [
      "Marta Buetas Arcas",
      "Richard Osuala",
      "Karim Lekadir",
      "Oliver Díaz"
    ],
    "abstract": "Artificial Intelligence (AI) has emerged as a valuable tool for assisting\nradiologists in breast cancer detection and diagnosis. However, the success of\nAI applications in this domain is restricted by the quantity and quality of\navailable data, posing challenges due to limited and costly data annotation\nprocedures that often lead to annotation shifts. This study simulates, analyses\nand mitigates annotation shifts in cancer classification in the breast\nmammography domain. First, a high-accuracy cancer risk prediction model is\ndeveloped, which effectively distinguishes benign from malignant lesions. Next,\nmodel performance is used to quantify the impact of annotation shift. We\nuncover a substantial impact of annotation shift on multiclass classification\nperformance particularly for malignant lesions. We thus propose a training data\naugmentation approach based on single-image generative models for the affected\nclass, requiring as few as four in-domain annotations to considerably mitigate\nannotation shift, while also addressing dataset imbalance. Lastly, we further\nincrease performance by proposing and validating an ensemble architecture based\non multiple models trained under different data augmentation regimes. Our study\noffers key insights into annotation shift in deep learning breast cancer\nclassification and explores the potential of single-image generative models to\novercome domain shift challenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint of paper accepted at SPIE IWBI 2024 Conference",
    "pdf_url": "http://arxiv.org/pdf/2405.19754v1",
    "published_date": "2024-05-30 07:02:50 UTC",
    "updated_date": "2024-05-30 07:02:50 UTC"
  },
  {
    "arxiv_id": "2405.19751v2",
    "title": "HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization",
    "authors": [
      "Wenxuan Liu",
      "Sai Qian Zhang"
    ],
    "abstract": "Diffusion Transformers (DiTs) have recently gained substantial attention in\nboth industrial and academic fields for their superior visual generation\ncapabilities, outperforming traditional diffusion models that use U-Net.\nHowever,the enhanced performance of DiTs also comes with high parameter counts\nand implementation costs, seriously restricting their use on resource-limited\ndevices such as mobile phones. To address these challenges, we introduce the\nHybrid Floating-point Quantization for DiT(HQ-DiT), an efficient post-training\nquantization method that utilizes 4-bit floating-point (FP) precision on both\nweights and activations for DiT inference. Compared to fixed-point quantization\n(e.g., INT8), FP quantization, complemented by our proposed clipping range\nselection mechanism, naturally aligns with the data distribution within DiT,\nresulting in a minimal quantization error. Furthermore, HQ-DiT also implements\na universal identity mathematical transform to mitigate the serious\nquantization error caused by the outliers. The experimental results demonstrate\nthat DiT can achieve extremely low-precision quantization (i.e., 4 bits) with\nnegligible impact on performance. Our approach marks the first instance where\nboth weights and activations in DiTs are quantized to just 4 bits, with only a\n0.12 increase in sFID on ImageNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19751v2",
    "published_date": "2024-05-30 06:56:11 UTC",
    "updated_date": "2024-05-31 15:48:05 UTC"
  },
  {
    "arxiv_id": "2405.19744v1",
    "title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions",
    "authors": [
      "Chong Li",
      "Wen Yang",
      "Jiajun Zhang",
      "Jinliang Lu",
      "Shaonan Wang",
      "Chengqing Zong"
    ],
    "abstract": "Large language models respond well in high-resource languages like English\nbut struggle in low-resource languages. It may arise from the lack of\nhigh-quality instruction following data in these languages. Directly\ntranslating English samples into these languages can be a solution but\nunreliable, leading to responses with translation errors and lacking\nlanguage-specific or cultural knowledge. To address this issue, we propose a\nnovel method to construct cross-lingual instruction following samples with\ninstruction in English and response in low-resource languages. Specifically,\nthe language model first learns to generate appropriate English instructions\naccording to the natural web texts in other languages as responses. The\ncandidate cross-lingual instruction tuning samples are further refined and\ndiversified. We have employed this method to build a large-scale cross-lingual\ninstruction tuning dataset on 10 languages, namely X-Instruction. The\ninstruction data built using our method incorporate more language-specific\nknowledge compared with the naive translation method. Experimental results have\nshown that the response quality of the model tuned on X-Instruction greatly\nexceeds the model distilled from a powerful teacher model, reaching or even\nsurpassing the ones of ChatGPT. In addition, we find that models tuned on\ncross-lingual instruction following samples can follow the instruction in the\noutput language without further tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024. Our codes, data and model weights are available at\n  https://github.com/ZNLP/X-Instruction",
    "pdf_url": "http://arxiv.org/pdf/2405.19744v1",
    "published_date": "2024-05-30 06:45:23 UTC",
    "updated_date": "2024-05-30 06:45:23 UTC"
  },
  {
    "arxiv_id": "2405.19743v1",
    "title": "May the Dance be with You: Dance Generation Framework for Non-Humanoids",
    "authors": [
      "Hyemin Ahn"
    ],
    "abstract": "We hypothesize dance as a motion that forms a visual rhythm from music, where\nthe visual rhythm can be perceived from an optical flow. If an agent can\nrecognize the relationship between visual rhythm and music, it will be able to\ndance by generating a motion to create a visual rhythm that matches the music.\nBased on this, we propose a framework for any kind of non-humanoid agents to\nlearn how to dance from human videos. Our framework works in two processes: (1)\ntraining a reward model which perceives the relationship between optical flow\n(visual rhythm) and music from human dance videos, (2) training the\nnon-humanoid dancer based on that reward model, and reinforcement learning. Our\nreward model consists of two feature encoders for optical flow and music. They\nare trained based on contrastive learning which makes the higher similarity\nbetween concurrent optical flow and music features. With this reward model, the\nagent learns dancing by getting a higher reward when its action creates an\noptical flow whose feature has a higher similarity with the given music\nfeature. Experiment results show that generated dance motion can align with the\nmusic beat properly, and user study result indicates that our framework is more\npreferred by humans compared to the baselines. To the best of our knowledge,\nour work of non-humanoid agents which learn dance from human videos is\nunprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 6 Figures, Rejected at Neurips 2023",
    "pdf_url": "http://arxiv.org/pdf/2405.19743v1",
    "published_date": "2024-05-30 06:43:55 UTC",
    "updated_date": "2024-05-30 06:43:55 UTC"
  },
  {
    "arxiv_id": "2405.19740v2",
    "title": "PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations",
    "authors": [
      "Jiatong Li",
      "Renjun Hu",
      "Kunzhe Huang",
      "Yan Zhuang",
      "Qi Liu",
      "Mengxiao Zhu",
      "Xing Shi",
      "Wei Lin"
    ],
    "abstract": "Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2405.19740v2",
    "published_date": "2024-05-30 06:38:32 UTC",
    "updated_date": "2024-10-18 06:57:08 UTC"
  },
  {
    "arxiv_id": "2405.19737v1",
    "title": "Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation",
    "authors": [
      "Chengwei Dai",
      "Kun Li",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "abstract": "As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts\n(CoTs) reasoning abilities, practical resource constraints drive efforts to\ndistill these capabilities into more compact Smaller Language Models (SLMs). We\nfind that CoTs consist mainly of simple reasoning forms, with a small\nproportion ($\\approx 4.7\\%$) of key reasoning steps that truly impact\nconclusions. However, previous distillation methods typically involve\nsupervised fine-tuning student SLMs only on correct CoTs data produced by\nteacher LLMs, resulting in students struggling to learn the key reasoning\nsteps, instead imitating the teacher's reasoning forms and making errors or\nomissions on these steps. To address these issues, drawing an analogy to human\nlearning, where analyzing mistakes according to correct solutions often reveals\nthe crucial steps leading to successes or failures, we propose\nmistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step\ndistilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs\nlearning key reasoning steps rather than mere simple fine-tuning. Firstly, to\nexpose these crucial steps in CoTs, we design specific prompts to generate dual\nCoTs data with similar reasoning paths but divergent conclusions. Then, we\napply the minimum edit distance algorithm on the dual CoTs data to locate these\nkey steps and optimize the likelihood of these steps. Extensive experiments\nvalidate the effectiveness of EDIT across both in-domain and out-of-domain\nbenchmark reasoning datasets. Further analysis shows that EDIT can generate\nhigh-quality CoTs with more correct key reasoning steps. Notably, we also\nexplore how different mistake patterns affect performance and find that EDIT\nbenefits more from logical errors than from knowledge or mathematical\ncalculation errors in dual CoTs\\footnote{Code can be found at\n\\url{https://github.com/C-W-D/EDIT}}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19737v1",
    "published_date": "2024-05-30 06:32:11 UTC",
    "updated_date": "2024-05-30 06:32:11 UTC"
  },
  {
    "arxiv_id": "2405.19736v2",
    "title": "Intrinsic Dynamics-Driven Generalizable Scene Representations for Vision-Oriented Decision-Making Applications",
    "authors": [
      "Dayang Liang",
      "Jinyang Lai",
      "Yunlong Liu"
    ],
    "abstract": "How to improve the ability of scene representation is a key issue in\nvision-oriented decision-making applications, and current approaches usually\nlearn task-relevant state representations within visual reinforcement learning\nto address this problem. While prior work typically introduces one-step\nbehavioral similarity metrics with elements (e.g., rewards and actions) to\nextract task-relevant state information from observations, they often ignore\nthe inherent dynamics relationships among the elements that are essential for\nlearning accurate representations, which further impedes the discrimination of\nshort-term similar task/behavior information in long-term dynamics transitions.\nTo alleviate this problem, we propose an intrinsic dynamics-driven\nrepresentation learning method with sequence models in visual reinforcement\nlearning, namely DSR. Concretely, DSR optimizes the parameterized encoder by\nthe state-transition dynamics of the underlying system, which prompts the\nlatent encoding information to satisfy the state-transition process and then\nthe state space and the noise space can be distinguished. In the implementation\nand to further improve the representation ability of DSR on encoding similar\ntasks, sequential elements' frequency domain and multi-step prediction are\nadopted for sequentially modeling the inherent dynamics. Finally, experimental\nresults show that DSR has achieved significant performance improvements in the\nvisual Distracting DMControl control tasks, especially with an average of\n78.9\\% over the backbone baseline. Further results indicate that it also\nachieves the best performances in real-world autonomous driving applications on\nthe CARLA simulator. Moreover, qualitative analysis results validate that our\nmethod possesses the superior ability to learn generalizable scene\nrepresentations on visual tasks. The source code is available at\nhttps://github.com/DMU-XMU/DSR.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2405.19736v2",
    "published_date": "2024-05-30 06:31:03 UTC",
    "updated_date": "2024-06-30 06:25:39 UTC"
  },
  {
    "arxiv_id": "2405.19730v5",
    "title": "Research on the Spatial Data Intelligent Foundation Model",
    "authors": [
      "Shaohua Wang",
      "Xing Xie",
      "Yong Li",
      "Danhuai Guo",
      "Zhi Cai",
      "Yu Liu",
      "Yang Yue",
      "Xiao Pan",
      "Feng Lu",
      "Huayi Wu",
      "Zhipeng Gui",
      "Zhiming Ding",
      "Bolong Zheng",
      "Fuzheng Zhang",
      "Jingyuan Wang",
      "Zhengchao Chen",
      "Hao Lu",
      "Jiayi Li",
      "Peng Yue",
      "Wenhao Yu",
      "Yao Yao",
      "Leilei Sun",
      "Yong Zhang",
      "Longbiao Chen",
      "Xiaoping Du",
      "Xiang Li",
      "Xueying Zhang",
      "Kun Qin",
      "Zhaoya Gong",
      "Weihua Dong",
      "Xiaofeng Meng"
    ],
    "abstract": "This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "V1 and V2 are in Chinese language, other versions are in English",
    "pdf_url": "http://arxiv.org/pdf/2405.19730v5",
    "published_date": "2024-05-30 06:21:34 UTC",
    "updated_date": "2024-08-28 13:05:41 UTC"
  },
  {
    "arxiv_id": "2405.19729v1",
    "title": "Dynamic feature selection in medical predictive monitoring by reinforcement learning",
    "authors": [
      "Yutong Chen",
      "Jiandong Gao",
      "Ji Wu"
    ],
    "abstract": "In this paper, we investigate dynamic feature selection within multivariate\ntime-series scenario, a common occurrence in clinical prediction monitoring\nwhere each feature corresponds to a bio-test result. Many existing feature\nselection methods fall short in effectively leveraging time-series information,\nprimarily because they are designed for static data. Our approach addresses\nthis limitation by enabling the selection of time-varying feature subsets for\neach patient. Specifically, we employ reinforcement learning to optimize a\npolicy under maximum cost restrictions. The prediction model is subsequently\nupdated using synthetic data generated by trained policy. Our method can\nseamlessly integrate with non-differentiable prediction models. We conducted\nexperiments on a sizable clinical dataset encompassing regression and\nclassification tasks. The results demonstrate that our approach outperforms\nstrong feature selection baselines, particularly when subjected to stringent\ncost limitations. Code will be released once paper is accepted.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "preview version",
    "pdf_url": "http://arxiv.org/pdf/2405.19729v1",
    "published_date": "2024-05-30 06:21:11 UTC",
    "updated_date": "2024-05-30 06:21:11 UTC"
  },
  {
    "arxiv_id": "2405.19723v3",
    "title": "Encoding and Controlling Global Semantics for Long-form Video Question Answering",
    "authors": [
      "Thong Thanh Nguyen",
      "Zhiyuan Hu",
      "Xiaobao Wu",
      "Cong-Duy T Nguyen",
      "See-Kiong Ng",
      "Anh Tuan Luu"
    ],
    "abstract": "Seeking answers effectively for long videos is essential to build video\nquestion answering (videoQA) systems. Previous methods adaptively select frames\nand regions from long videos to save computations. However, this fails to\nreason over the whole sequence of video, leading to sub-optimal performance. To\naddress this problem, we introduce a state space layer (SSL) into multi-modal\nTransformer to efficiently integrate global semantics of the video, which\nmitigates the video information loss caused by frame and region selection\nmodules. Our SSL includes a gating unit to enable controllability over the flow\nof global semantics into visual representations. To further enhance the\ncontrollability, we introduce a cross-modal compositional congruence (C^3)\nobjective to encourage global semantics aligned with the question. To\nrigorously evaluate long-form videoQA capacity, we construct two new benchmarks\nEgo-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5\nminutes and 1.9 hours, respectively. Extensive experiments demonstrate the\nsuperiority of our framework on these new as well as existing datasets. The\ncode, model, and data have been made available at\nhttps://nguyentthong.github.io/Long_form_VideoQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to the main EMNLP 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2405.19723v3",
    "published_date": "2024-05-30 06:10:10 UTC",
    "updated_date": "2024-10-05 14:02:31 UTC"
  },
  {
    "arxiv_id": "2405.19715v2",
    "title": "SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths",
    "authors": [
      "Kaixuan Huang",
      "Xudong Guo",
      "Mengdi Wang"
    ],
    "abstract": "Speculative decoding reduces the inference latency of a target large language\nmodel via utilizing a smaller and faster draft model. Its performance depends\non a hyperparameter K -- the candidate length, i.e., the number of candidate\ntokens for the target model to verify in each round. However, previous methods\noften use simple heuristics to choose K, which may result in sub-optimal\nperformance. We study the choice of the candidate length K and formulate it as\na Markov Decision Process. We theoretically show that the optimal policy of\nthis Markov decision process takes the form of a threshold policy, i.e., the\ncurrent speculation should stop and be verified when the probability of getting\na rejection exceeds a threshold value. Motivated by this theory, we propose\nSpecDec++, an enhanced version of speculative decoding that adaptively\ndetermines the candidate length on the fly. We augment the draft model with a\ntrained acceptance prediction head to predict the conditional acceptance\nprobability of the candidate tokens. SpecDec++ will stop the current\nspeculation when the predicted probability that at least one token gets\nrejected exceeds a threshold. We implement SpecDec++ and apply it to the\nllama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup\non the Alpaca dataset (an additional 7.2% improvement over the baseline\nspeculative decoding). On the GSM8K and HumanEval datasets, our method achieves\na 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement),\nrespectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "v2: fix Table 1",
    "pdf_url": "http://arxiv.org/pdf/2405.19715v2",
    "published_date": "2024-05-30 05:49:38 UTC",
    "updated_date": "2024-06-21 01:01:42 UTC"
  },
  {
    "arxiv_id": "2405.19708v1",
    "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
    "authors": [
      "Jia Li",
      "Lijie Hu",
      "Zhixian He",
      "Jingfeng Zhang",
      "Tianhang Zheng",
      "Di Wang"
    ],
    "abstract": "With the advancement of image-to-image diffusion models guided by text,\nsignificant progress has been made in image editing. However, a persistent\nchallenge remains in seamlessly incorporating objects into images based on\ntextual instructions, without relying on extra user-provided guidance. Text and\nimages are inherently distinct modalities, bringing out difficulties in fully\ncapturing the semantic intent conveyed through language and accurately\ntranslating that into the desired visual modifications. Therefore, text-guided\nimage editing models often produce generations with residual object attributes\nthat do not fully align with human expectations. To address this challenge, the\nmodels should comprehend the image content effectively away from a disconnect\nbetween the provided textual editing prompts and the actual modifications made\nto the image. In our paper, we propose a novel method called Locate and Forget\n(LaF), which effectively locates potential target concepts in the image for\nmodification by comparing the syntactic trees of the target prompt and scene\ndescriptions in the input image, intending to forget their existence clues in\nthe generated image. Compared to the baselines, our method demonstrates its\nsuperiority in text-guided image editing tasks both qualitatively and\nquantitatively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19708v1",
    "published_date": "2024-05-30 05:36:32 UTC",
    "updated_date": "2024-05-30 05:36:32 UTC"
  },
  {
    "arxiv_id": "2405.19701v2",
    "title": "Significance of Chain of Thought in Gender Bias Mitigation for English-Dravidian Machine Translation",
    "authors": [
      "Lavanya Prahallad",
      "Radhika Mamidi"
    ],
    "abstract": "Gender bias in machine translation (MT) sys- tems poses a significant\nchallenge to achieving accurate and inclusive translations. This paper examines\ngender bias in machine translation systems for languages such as Telugu and\nKan- nada from the Dravidian family, analyzing how gender inflections affect\ntranslation accuracy and neutrality using Google Translate and Chat- GPT. It\nfinds that while plural forms can reduce bias, individual-centric sentences\noften main- tain the bias due to historical stereotypes. The study evaluates\nthe Chain of Thought process- ing, noting significant bias mitigation from 80%\nto 4% in Telugu and from 40% to 0% in Kan- nada. It also compares Telugu and\nKannada translations, emphasizing the need for language specific strategies to\naddress these challenges and suggesting directions for future research to\nenhance fairness in both data preparation and prompts during inference.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.19701v2",
    "published_date": "2024-05-30 05:26:57 UTC",
    "updated_date": "2024-06-03 15:59:34 UTC"
  },
  {
    "arxiv_id": "2405.19697v2",
    "title": "Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity",
    "authors": [
      "Yan Yang",
      "Bin Gao",
      "Ya-xiang Yuan"
    ],
    "abstract": "Bilevel reinforcement learning (RL), which features intertwined two-level\nproblems, has attracted growing interest recently. The inherent non-convexity\nof the lower-level RL problem is, however, to be an impediment to developing\nbilevel optimization methods. By employing the fixed point equation associated\nwith the regularized RL, we characterize the hyper-gradient via fully\nfirst-order information, thus circumventing the assumption of lower-level\nconvexity. This, remarkably, distinguishes our development of hyper-gradient\nfrom the general AID-based bilevel frameworks since we take advantage of the\nspecific structure of RL problems. Moreover, we design both model-based and\nmodel-free bilevel reinforcement learning algorithms, facilitated by access to\nthe fully first-order hyper-gradient. Both algorithms enjoy the convergence\nrate $O(\\epsilon^{-1})$. To extend the applicability, a stochastic version of\nthe model-free algorithm is proposed, along with results on its iteration and\nsample complexity. In addition, numerical experiments demonstrate that the\nhyper-gradient indeed serves as an integration of exploitation and exploration.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "This v2 is a camera-ready version of AISTATS 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.19697v2",
    "published_date": "2024-05-30 05:24:20 UTC",
    "updated_date": "2025-02-27 06:52:19 UTC"
  },
  {
    "arxiv_id": "2405.19694v1",
    "title": "Grade Like a Human: Rethinking Automated Assessment with Large Language Models",
    "authors": [
      "Wenjing Xie",
      "Juxin Niu",
      "Chun Jason Xue",
      "Nan Guan"
    ],
    "abstract": "While large language models (LLMs) have been used for automated grading, they\nhave not yet achieved the same level of performance as humans, especially when\nit comes to grading complex questions. Existing research on this topic focuses\non a particular step in the grading procedure: grading using predefined\nrubrics. However, grading is a multifaceted procedure that encompasses other\ncrucial steps, such as grading rubrics design and post-grading review. There\nhas been a lack of systematic research exploring the potential of LLMs to\nenhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the\nentire grading procedure, including the following key components: 1) Developing\ngrading rubrics that not only consider the questions but also the student\nanswers, which can more accurately reflect students' performance. 2) Under the\nguidance of grading rubrics, providing accurate and consistent scores for each\nstudent, along with customized feedback. 3) Conducting post-grading review to\nbetter ensure accuracy and fairness. Additionally, we collected a new dataset\nnamed OS from a university operating system course and conducted extensive\nexperiments on both our new dataset and the widely used Mohler dataset.\nExperiments demonstrate the effectiveness of our proposed approach, providing\nsome new insights for developing automated grading systems based on LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19694v1",
    "published_date": "2024-05-30 05:08:15 UTC",
    "updated_date": "2024-05-30 05:08:15 UTC"
  },
  {
    "arxiv_id": "2405.19690v3",
    "title": "Diffusion Policies creating a Trust Region for Offline Reinforcement Learning",
    "authors": [
      "Tianyu Chen",
      "Zhendong Wang",
      "Mingyuan Zhou"
    ],
    "abstract": "Offline reinforcement learning (RL) leverages pre-collected datasets to train\noptimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a\npowerful and expressive policy class, significantly boosts the performance of\noffline RL. However, its reliance on iterative denoising sampling to generate\nactions slows down both training and inference. While several recent attempts\nhave tried to accelerate diffusion-QL, the improvement in training and/or\ninference speed often results in degraded performance. In this paper, we\nintroduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which\ncomprises a diffusion policy for pure behavior cloning and a practical one-step\npolicy. We bridge the two polices by a newly introduced diffusion trust region\nloss. The diffusion policy maintains expressiveness, while the trust region\nloss directs the one-step policy to explore freely and seek modes within the\nregion defined by the diffusion policy. DTQL eliminates the need for iterative\ndenoising sampling during both training and inference, making it remarkably\ncomputationally efficient. We evaluate its effectiveness and algorithmic\ncharacteristics against popular Kullback--Leibler divergence-based distillation\nmethods in 2D bandit scenarios and gym tasks. We then show that DTQL could not\nonly outperform other methods on the majority of the D4RL benchmark tasks but\nalso demonstrate efficiency in training and inference speeds. The PyTorch\nimplementation is available at\nhttps://github.com/TianyuCodings/Diffusion_Trusted_Q_Learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19690v3",
    "published_date": "2024-05-30 05:04:33 UTC",
    "updated_date": "2024-10-31 18:09:38 UTC"
  },
  {
    "arxiv_id": "2405.19686v1",
    "title": "Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback",
    "authors": [
      "Jingwei Sun",
      "Zhixu Du",
      "Yiran Chen"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nrange of natural language processing tasks. Once deployed, LLMs encounter users\nwith personalized factual knowledge, and such personalized knowledge is\nconsistently reflected through users' interactions with the LLMs. To enhance\nuser experience, real-time model personalization is essential, allowing LLMs to\nadapt user-specific knowledge based on user feedback during human-LLM\ninteractions. Existing methods mostly require back-propagation to finetune the\nmodel parameters, which incurs high computational and memory costs. In\naddition, these methods suffer from low interpretability, which will cause\nunforeseen impacts on model performance during long-term use, where the user's\npersonalized knowledge is accumulated extensively.To address these challenges,\nwe propose Knowledge Graph Tuning (KGT), a novel approach that leverages\nknowledge graphs (KGs) to personalize LLMs. KGT extracts personalized factual\nknowledge triples from users' queries and feedback and optimizes KGs without\nmodifying the LLM parameters. Our method improves computational and memory\nefficiency by avoiding back-propagation and ensures interpretability by making\nthe KG adjustments comprehensible to humans.Experiments with state-of-the-art\nLLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves\npersonalization performance while reducing latency and GPU memory costs.\nUltimately, KGT offers a promising solution of effective, efficient, and\ninterpretable real-time LLM personalization during user interactions with the\nLLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19686v1",
    "published_date": "2024-05-30 04:57:03 UTC",
    "updated_date": "2024-05-30 04:57:03 UTC"
  },
  {
    "arxiv_id": "2405.19678v2",
    "title": "View-Consistent Hierarchical 3D Segmentation Using Ultrametric Feature Fields",
    "authors": [
      "Haodi He",
      "Colton Stearns",
      "Adam W. Harley",
      "Leonidas J. Guibas"
    ],
    "abstract": "Large-scale vision foundation models such as Segment Anything (SAM)\ndemonstrate impressive performance in zero-shot image segmentation at multiple\nlevels of granularity. However, these zero-shot predictions are rarely\n3D-consistent. As the camera viewpoint changes in a scene, so do the\nsegmentation predictions, as well as the characterizations of \"coarse\" or\n\"fine\" granularity. In this work, we address the challenging task of lifting\nmulti-granular and view-inconsistent image segmentations into a hierarchical\nand 3D-consistent representation. We learn a novel feature field within a\nNeural Radiance Field (NeRF) representing a 3D scene, whose segmentation\nstructure can be revealed at different scales by simply using different\nthresholds on feature distance. Our key idea is to learn an ultrametric feature\nspace, which unlike a Euclidean space, exhibits transitivity in distance-based\ngrouping, naturally leading to a hierarchical clustering. Put together, our\nmethod takes view-inconsistent multi-granularity 2D segmentations as input and\nproduces a hierarchy of 3D-consistent segmentations as output. We evaluate our\nmethod and several baselines on synthetic datasets with multi-view images and\nmulti-granular segmentation, showcasing improved accuracy and\nviewpoint-consistency. We additionally provide qualitative examples of our\nmodel's 3D hierarchical segmentations in real world scenes. The code and\ndataset are available at https://github.com/hardyho/ultrametric_feature_fields",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19678v2",
    "published_date": "2024-05-30 04:14:58 UTC",
    "updated_date": "2024-07-18 02:28:14 UTC"
  },
  {
    "arxiv_id": "2405.19677v1",
    "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
    "authors": [
      "Zhaoxi Zhang",
      "Xiaomei Zhang",
      "Yanjun Zhang",
      "Leo Yu Zhang",
      "Chao Chen",
      "Shengshan Hu",
      "Asif Gill",
      "Shirui Pan"
    ],
    "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that\nshows promise in addressing concerns surrounding LLM copyright, monitoring\nAI-generated text, and preventing its misuse. The LLM watermark scheme commonly\nincludes generating secret keys to partition the vocabulary into green and red\nlists, applying a perturbation to the logits of tokens in the green list to\nincrease their sampling likelihood, thus facilitating watermark detection to\nidentify AI-generated text if the proportion of green tokens exceeds a\nthreshold. However, recent research indicates that watermarking methods using\nnumerous keys are susceptible to removal attacks, such as token editing,\nsynonym substitution, and paraphrasing, with robustness declining as the number\nof keys increases. Therefore, the state-of-the-art watermark schemes that\nemploy fewer or single keys have been demonstrated to be more robust against\ntext editing and paraphrasing. In this paper, we propose a novel green list\nstealing attack against the state-of-the-art LLM watermark scheme and\nsystematically examine its vulnerability to this attack. We formalize the\nattack as a mixed integer programming problem with constraints. We evaluate our\nattack under a comprehensive threat model, including an extreme scenario where\nthe attacker has no prior knowledge, lacks access to the watermark detector\nAPI, and possesses no information about the LLM's parameter settings or\nwatermark injection/detection scheme. Extensive experiments on LLMs, such as\nOPT and LLaMA, demonstrate that our attack can successfully steal the green\nlist and remove the watermark across all settings.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.19677v1",
    "published_date": "2024-05-30 04:11:17 UTC",
    "updated_date": "2024-05-30 04:11:17 UTC"
  },
  {
    "arxiv_id": "2405.19673v2",
    "title": "Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models",
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Ehsan Hajiramezanali",
      "Gabriele Scalia",
      "Gökcen Eraslan",
      "Avantika Lal",
      "Sergey Levine",
      "Tommaso Biancalani"
    ],
    "abstract": "AI-driven design problems, such as DNA/protein sequence design, are commonly\ntackled from two angles: generative modeling, which efficiently captures the\nfeasible design space (e.g., natural images or biological sequences), and\nmodel-based optimization, which utilizes reward models for extrapolation. To\ncombine the strengths of both approaches, we adopt a hybrid method that\nfine-tunes cutting-edge diffusion models by optimizing reward models through\nRL. Although prior work has explored similar avenues, they primarily focus on\nscenarios where accurate reward models are accessible. In contrast, we\nconcentrate on an offline setting where a reward model is unknown, and we must\nlearn from static offline datasets, a common scenario in scientific domains. In\noffline scenarios, existing approaches tend to suffer from overoptimization, as\nthey may be misled by the reward model in out-of-distribution regions. To\naddress this, we introduce a conservative fine-tuning approach, BRAID, by\noptimizing a conservative reward model, which includes additional penalization\noutside of offline data distributions. Through empirical and theoretical\nanalysis, we demonstrate the capability of our approach to outperform the best\ndesigns in offline data, leveraging the extrapolation capabilities of reward\nmodels while avoiding the generation of invalid designs through pre-trained\ndiffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2405.19673v2",
    "published_date": "2024-05-30 03:57:29 UTC",
    "updated_date": "2024-05-31 18:34:35 UTC"
  },
  {
    "arxiv_id": "2405.19667v1",
    "title": "Reconciling Model Multiplicity for Downstream Decision Making",
    "authors": [
      "Ally Yalei Du",
      "Dung Daniel Ngo",
      "Zhiwei Steven Wu"
    ],
    "abstract": "We consider the problem of model multiplicity in downstream decision-making,\na setting where two predictive models of equivalent accuracy cannot agree on\nthe best-response action for a downstream loss function. We show that even when\nthe two predictive models approximately agree on their individual predictions\nalmost everywhere, it is still possible for their induced best-response actions\nto differ on a substantial portion of the population. We address this issue by\nproposing a framework that calibrates the predictive models with regard to both\nthe downstream decision-making problem and the individual probability\nprediction. Specifically, leveraging tools from multi-calibration, we provide\nan algorithm that, at each time-step, first reconciles the differences in\nindividual probability prediction, then calibrates the updated models such that\nthey are indistinguishable from the true probability distribution to the\ndecision-maker. We extend our results to the setting where one does not have\ndirect access to the true probability distribution and instead relies on a set\nof i.i.d data to be the empirical distribution. Finally, we provide a set of\nexperiments to empirically evaluate our methods: compared to existing work, our\nproposed algorithm creates a pair of predictive models with both improved\ndownstream decision-making losses and agrees on their best-response actions\nalmost everywhere.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages main body, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.19667v1",
    "published_date": "2024-05-30 03:36:46 UTC",
    "updated_date": "2024-05-30 03:36:46 UTC"
  },
  {
    "arxiv_id": "2405.19665v1",
    "title": "A novel fault localization with data refinement for hydroelectric units",
    "authors": [
      "Jialong Huang",
      "Junlin Song",
      "Penglong Lian",
      "Mengjie Gan",
      "Zhiheng Su",
      "Benhao Wang",
      "Wenji Zhu",
      "Xiaomin Pu",
      "Jianxiao Zou",
      "Shicai Fan"
    ],
    "abstract": "Due to the scarcity of fault samples and the complexity of non-linear and\nnon-smooth characteristics data in hydroelectric units, most of the traditional\nhydroelectric unit fault localization methods are difficult to carry out\naccurate localization. To address these problems, a sparse autoencoder\n(SAE)-generative adversarial network (GAN)-wavelet noise reduction (WNR)-\nmanifold-boosted deep learning (SG-WMBDL) based fault localization method for\nhydroelectric units is proposed. To overcome the data scarcity, a SAE is\nembedded into the GAN to generate more high-quality samples in the data\ngeneration module. Considering the signals involving non-linear and non-smooth\ncharacteristics, the improved WNR which combining both soft and hard\nthresholding and local linear embedding (LLE) are utilized to the data\npreprocessing module in order to reduce the noise and effectively capture the\nlocal features. In addition, to seek higher performance, the novel Adaptive\nBoost (AdaBoost) combined with multi deep learning is proposed to achieve\naccurate fault localization. The experimental results show that the SG-WMBDL\ncan locate faults for hydroelectric units under a small number of fault samples\nwith non-linear and non-smooth characteristics on higher precision and accuracy\ncompared to other frontier methods, which verifies the effectiveness and\npracticality of the proposed method.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "6pages,4 figures,Conference on Decision and Control(CDC) conference",
    "pdf_url": "http://arxiv.org/pdf/2405.19665v1",
    "published_date": "2024-05-30 03:33:49 UTC",
    "updated_date": "2024-05-30 03:33:49 UTC"
  },
  {
    "arxiv_id": "2405.19657v1",
    "title": "Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D Gaussian",
    "authors": [
      "Wei Sun",
      "Qi Zhang",
      "Yanzhao Zhou",
      "Qixiang Ye",
      "Jianbin Jiao",
      "Yuan Li"
    ],
    "abstract": "3D Gaussian splatting has demonstrated impressive performance in real-time\nnovel view synthesis. However, achieving successful reconstruction from RGB\nimages generally requires multiple input views captured under static\nconditions. To address the challenge of sparse input views, previous approaches\nhave incorporated depth supervision into the training of 3D Gaussians to\nmitigate overfitting, using dense predictions from pretrained depth networks as\npseudo-ground truth. Nevertheless, depth predictions from monocular depth\nestimation models inherently exhibit significant uncertainty in specific areas.\nRelying solely on pixel-wise L2 loss may inadvertently incorporate detrimental\nnoise from these uncertain areas. In this work, we introduce a novel method to\nsupervise the depth distribution of 3D Gaussians, utilizing depth priors with\nintegrated uncertainty estimates. To address these localized errors in depth\npredictions, we integrate a patch-wise optimal transport strategy to complement\ntraditional L2 loss in depth supervision. Extensive experiments conducted on\nthe LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT,\nachieves superior novel view synthesis and consistently outperforms\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10pages",
    "pdf_url": "http://arxiv.org/pdf/2405.19657v1",
    "published_date": "2024-05-30 03:18:30 UTC",
    "updated_date": "2024-05-30 03:18:30 UTC"
  },
  {
    "arxiv_id": "2405.19656v1",
    "title": "Accurate and Reliable Predictions with Mutual-Transport Ensemble",
    "authors": [
      "Han Liu",
      "Peng Cui",
      "Bingning Wang",
      "Jun Zhu",
      "Xiaolin Hu"
    ],
    "abstract": "Deep Neural Networks (DNNs) have achieved remarkable success in a variety of\ntasks, especially when it comes to prediction accuracy. However, in complex\nreal-world scenarios, particularly in safety-critical applications, high\naccuracy alone is not enough. Reliable uncertainty estimates are crucial.\nModern DNNs, often trained with cross-entropy loss, tend to be overconfident,\nespecially with ambiguous samples. To improve uncertainty calibration, many\ntechniques have been developed, but they often compromise prediction accuracy.\nTo tackle this challenge, we propose the ``mutual-transport ensemble'' (MTE).\nThis approach introduces a co-trained auxiliary model and adaptively\nregularizes the cross-entropy loss using Kullback-Leibler (KL) divergence\nbetween the prediction distributions of the primary and auxiliary models. We\nconducted extensive studies on various benchmarks to validate the effectiveness\nof our method. The results show that MTE can simultaneously enhance both\naccuracy and uncertainty calibration. For example, on the CIFAR-100 dataset,\nour MTE method on ResNet34/50 achieved significant improvements compared to\nprevious state-of-the-art method, with absolute accuracy increases of\n2.4%/3.7%, relative reductions in ECE of $42.3%/29.4%, and relative reductions\nin classwise-ECE of 11.6%/15.3%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19656v1",
    "published_date": "2024-05-30 03:15:59 UTC",
    "updated_date": "2024-05-30 03:15:59 UTC"
  },
  {
    "arxiv_id": "2405.19654v1",
    "title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training",
    "authors": [
      "Jinxia Yang",
      "Bing Su",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "abstract": "Medical vision-language pre-training methods mainly leverage the\ncorrespondence between paired medical images and radiological reports. Although\nmulti-view spatial images and temporal sequences of image-report pairs are\navailable in off-the-shelf multi-modal medical datasets, most existing methods\nhave not thoroughly tapped into such extensive supervision signals. In this\npaper, we introduce the Med-ST framework for fine-grained spatial and temporal\nmodeling to exploit information from multiple spatial views of chest\nradiographs and temporal historical records. For spatial modeling, Med-ST\nemploys the Mixture of View Expert (MoVE) architecture to integrate different\nvisual features from both frontal and lateral views. To achieve a more\ncomprehensive alignment, Med-ST not only establishes the global alignment\nbetween whole images and texts but also introduces modality-weighted local\nalignment between text tokens and spatial regions of images. For temporal\nmodeling, we propose a novel cross-modal bidirectional cycle consistency\nobjective by forward mapping classification (FMC) and reverse mapping\nregression (RMR). By perceiving temporal information from simple to complex,\nMed-ST can learn temporal semantics. Experimental results across four distinct\ntasks demonstrate the effectiveness of Med-ST, especially in temporal\nclassification tasks. Our code and model are available at\nhttps://github.com/SVT-Yang/MedST.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19654v1",
    "published_date": "2024-05-30 03:15:09 UTC",
    "updated_date": "2024-05-30 03:15:09 UTC"
  },
  {
    "arxiv_id": "2405.19650v2",
    "title": "Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization",
    "authors": [
      "Xi Lin",
      "Yilu Liu",
      "Xiaoyuan Zhang",
      "Fei Liu",
      "Zhenkun Wang",
      "Qingfu Zhang"
    ],
    "abstract": "Multi-objective optimization can be found in many real-world applications\nwhere some conflicting objectives can not be optimized by a single solution.\nExisting optimization methods often focus on finding a set of Pareto solutions\nwith different optimal trade-offs among the objectives. However, the required\nnumber of solutions to well approximate the whole Pareto optimal set could be\nexponentially large with respect to the number of objectives, which makes these\nmethods unsuitable for handling many optimization objectives. In this work,\ninstead of finding a dense set of Pareto solutions, we propose a novel\nTchebycheff set scalarization method to find a few representative solutions\n(e.g., 5) to cover a large number of objectives (e.g., $>100$) in a\ncollaborative and complementary manner. In this way, each objective can be well\naddressed by at least one solution in the small solution set. In addition, we\nfurther develop a smooth Tchebycheff set scalarization approach for efficient\noptimization with good theoretical guarantees. Experimental studies on\ndifferent problems with many optimization objectives demonstrate the\neffectiveness of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19650v2",
    "published_date": "2024-05-30 03:04:57 UTC",
    "updated_date": "2024-10-15 09:57:16 UTC"
  },
  {
    "arxiv_id": "2405.19648v1",
    "title": "Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach",
    "authors": [
      "Ernesto Quevedo",
      "Jorge Yero",
      "Rachel Koerner",
      "Pablo Rivas",
      "Tomas Cerny"
    ],
    "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce\ninaccurate outputs, also known as hallucinations, have escalated. Detecting\nthem is vital for ensuring the reliability of applications relying on\nLLM-generated content. Current methods often demand substantial resources and\nrely on extensive LLMs or employ supervised learning with multidimensional\nfeatures or intricate linguistic and semantic analyses difficult to reproduce\nand largely depend on using the same LLM that hallucinated. This paper\nintroduces a supervised learning approach employing two simple classifiers\nutilizing only four numerical features derived from tokens and vocabulary\nprobabilities obtained from other LLM evaluators, which are not necessarily the\nsame. The method yields promising results, surpassing state-of-the-art outcomes\nin multiple tasks across three different benchmarks. Additionally, we provide a\ncomprehensive examination of the strengths and weaknesses of our approach,\nhighlighting the significance of the features utilized and the LLM employed as\nan evaluator. We have released our code publicly at\nhttps://github.com/Baylor-AI/HalluDetect.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "ICAI'24 - The 26th Int'l Conf on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2405.19648v1",
    "published_date": "2024-05-30 03:00:47 UTC",
    "updated_date": "2024-05-30 03:00:47 UTC"
  },
  {
    "arxiv_id": "2405.20354v2",
    "title": "Efficient Systematic Reviews: Literature Filtering with Transformers & Transfer Learning",
    "authors": [
      "John Hawkins",
      "David Tivey"
    ],
    "abstract": "Identifying critical research within the growing body of academic work is an\nintrinsic aspect of conducting quality research. Systematic review processes\nused in evidence-based medicine formalise this as a procedure that must be\nfollowed in a research program. However, it comes with an increasing burden in\nterms of the time required to identify the important articles of research for a\ngiven topic. In this work, we develop a method for building a general-purpose\nfiltering system that matches a research question, posed as a natural language\ndescription of the required content, against a candidate set of articles\nobtained via the application of broad search terms. Our results demonstrate\nthat transformer models, pre-trained on biomedical literature, and then fine\ntuned for the specific task, offer a promising solution to this problem. The\nmodel can remove large volumes of irrelevant articles for most research\nquestions. Furthermore, analysis of the specific research questions in our\ntraining data suggest natural avenues for further improvement.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.DL",
    "comment": "Paper Submitted to `Multimedia Tools and Applications`",
    "pdf_url": "http://arxiv.org/pdf/2405.20354v2",
    "published_date": "2024-05-30 02:55:49 UTC",
    "updated_date": "2024-10-10 23:20:34 UTC"
  },
  {
    "arxiv_id": "2405.19644v3",
    "title": "EgoSurgery-Phase: A Dataset of Surgical Phase Recognition from Egocentric Open Surgery Videos",
    "authors": [
      "Ryo Fujii",
      "Masashi Hatano",
      "Hideo Saito",
      "Hiroki Kajita"
    ],
    "abstract": "Surgical phase recognition has gained significant attention due to its\npotential to offer solutions to numerous demands of the modern operating room.\nHowever, most existing methods concentrate on minimally invasive surgery (MIS),\nleaving surgical phase recognition for open surgery understudied. This\ndiscrepancy is primarily attributed to the scarcity of publicly available open\nsurgery video datasets for surgical phase recognition. To address this issue,\nwe introduce a new egocentric open surgery video dataset for phase recognition,\nnamed EgoSurgery-Phase. This dataset comprises 15 hours of real open surgery\nvideos spanning 9 distinct surgical phases all captured using an egocentric\ncamera attached to the surgeon's head. In addition to video, the\nEgoSurgery-Phase offers eye gaze. As far as we know, it is the first real open\nsurgery video dataset for surgical phase recognition publicly available.\nFurthermore, inspired by the notable success of masked autoencoders (MAEs) in\nvideo understanding tasks (e.g., action recognition), we propose a gaze-guided\nmasked autoencoder (GGMAE). Considering the regions where surgeons' gaze\nfocuses are often critical for surgical phase recognition (e.g., surgical\nfield), in our GGMAE, the gaze information acts as an empirical semantic\nrichness prior to guiding the masking process, promoting better attention to\nsemantically rich spatial regions. GGMAE significantly improves the previous\nstate-of-the-art recognition method (6.4% in Jaccard) and the masked\nautoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase. The dataset is\nreleased at https://github.com/Fujiry0/EgoSurgery.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Early accepted by MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.19644v3",
    "published_date": "2024-05-30 02:53:19 UTC",
    "updated_date": "2024-11-27 04:52:51 UTC"
  },
  {
    "arxiv_id": "2405.19642v1",
    "title": "Few-shot fault diagnosis based on multi-scale graph convolution filtering for industry",
    "authors": [
      "Mengjie Gan",
      "Penglong Lian",
      "Zhiheng Su",
      "Jiyang Zhang",
      "Jialong Huang",
      "Benhao Wang",
      "Jianxiao Zou",
      "Shicai Fan"
    ],
    "abstract": "Industrial equipment fault diagnosis often encounter challenges such as the\nscarcity of fault data, complex operating conditions, and varied types of\nfailures. Signal analysis, data statistical learning, and conventional deep\nlearning techniques face constraints under these conditions due to their\nsubstantial data requirements and the necessity for transfer learning to\naccommodate new failure modes. To effectively leverage information and extract\nthe intrinsic characteristics of faults across different domains under limited\nsample conditions, this paper introduces a fault diagnosis approach employing\nMulti-Scale Graph Convolution Filtering (MSGCF). MSGCF enhances the traditional\nGraph Neural Network (GNN) framework by integrating both local and global\ninformation fusion modules within the graph convolution filter block. This\nadvancement effectively mitigates the over-smoothing issue associated with\nexcessive layering of graph convolutional layers while preserving a broad\nreceptive field. It also reduces the risk of overfitting in few-shot diagnosis,\nthereby augmenting the model's representational capacity. Experiments on the\nUniversity of Paderborn bearing dataset (PU) demonstrate that the MSGCF method\nproposed herein surpasses alternative approaches in accuracy, thereby offering\nvaluable insights for industrial fault diagnosis in few-shot learning\nscenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures, 2 tables, 63rd IEEE Conference on Decision and\n  Control",
    "pdf_url": "http://arxiv.org/pdf/2405.19642v1",
    "published_date": "2024-05-30 02:51:29 UTC",
    "updated_date": "2024-05-30 02:51:29 UTC"
  },
  {
    "arxiv_id": "2405.19631v1",
    "title": "Leveraging Open-Source Large Language Models for encoding Social Determinants of Health using an Intelligent Router",
    "authors": [
      "Akul Goel",
      "Surya Narayanan Hari",
      "Belinda Waltman",
      "Matt Thomson"
    ],
    "abstract": "Social Determinants of Health (SDOH) play a significant role in patient\nhealth outcomes. The Center of Disease Control (CDC) introduced a subset of\nICD-10 codes called Z-codes in an attempt to officially recognize and measure\nSDOH in the health care system. However, these codes are rarely annotated in a\npatient's Electronic Health Record (EHR), and instead, in many cases, need to\nbe inferred from clinical notes. Previous research has shown that large\nlanguage models (LLMs) show promise on extracting unstructured data from EHRs.\nHowever, with thousands of models to choose from with unique architectures and\ntraining sets, it's difficult to choose one model that performs the best on\ncoding tasks. Further, clinical notes contain trusted health information making\nthe use of closed-source language models from commercial vendors difficult, so\nthe identification of open source LLMs that can be run within health\norganizations and exhibits high performance on SDOH tasks is an urgent problem.\nHere, we introduce an intelligent routing system for SDOH coding that uses a\nlanguage model router to direct medical record data to open source LLMs that\ndemonstrate optimal performance on specific SDOH codes. The intelligent routing\nsystem exhibits state of the art performance of 97.4% accuracy averaged across\n5 codes, including homelessness and food insecurity, on par with closed models\nsuch as GPT-4o. In order to train the routing system and validate models, we\nalso introduce a synthetic data generation and validation paradigm to increase\nthe scale of training data without needing privacy protected medical records.\nTogether, we demonstrate an architecture for intelligent routing of inputs to\ntask-optimal language models to achieve high performance across a set of\nmedical coding sub-tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19631v1",
    "published_date": "2024-05-30 02:33:28 UTC",
    "updated_date": "2024-05-30 02:33:28 UTC"
  },
  {
    "arxiv_id": "2405.19616v2",
    "title": "Easy Problems That LLMs Get Wrong",
    "authors": [
      "Sean Williams",
      "James Huckle"
    ],
    "abstract": "We introduce a comprehensive Linguistic Benchmark designed to evaluate the\nlimitations of Large Language Models (LLMs) in domains such as logical\nreasoning, spatial intelligence, and linguistic understanding, among others.\nThrough a series of straightforward questions, it uncovers the significant\nlimitations of well-regarded models to perform tasks that humans manage with\nease. It also highlights the potential of prompt engineering to mitigate some\nerrors and underscores the necessity for better training methodologies. Our\nfindings stress the importance of grounding LLMs with human reasoning and\ncommon sense, emphasising the need for human-in-the-loop for enterprise\napplications. We hope this work paves the way for future research to enhance\nthe usefulness and reliability of new models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AutogenAI Ltd. GitHub Repo:\n  https://github.com/autogenai/easy-problems-that-llms-get-wrong",
    "pdf_url": "http://arxiv.org/pdf/2405.19616v2",
    "published_date": "2024-05-30 02:09:51 UTC",
    "updated_date": "2024-06-01 03:00:37 UTC"
  },
  {
    "arxiv_id": "2405.19606v2",
    "title": "Relation Modeling and Distillation for Learning with Noisy Labels",
    "authors": [
      "Xiaming Che",
      "Junlin Zhang",
      "Zhuang Qi",
      "Xin Qi"
    ],
    "abstract": "Learning with noisy labels has become an effective strategy for enhancing the\nrobustness of models, which enables models to better tolerate inaccurate data.\nExisting methods either focus on optimizing the loss function to mitigate the\ninterference from noise, or design procedures to detect potential noise and\ncorrect errors. However, their effectiveness is often compromised in\nrepresentation learning due to the dilemma where models overfit to noisy\nlabels. To address this issue, this paper proposes a relation modeling and\ndistillation framework that models inter-sample relationships via\nself-supervised learning and employs knowledge distillation to enhance\nunderstanding of latent associations, which mitigate the impact of noisy\nlabels. Specifically, the proposed method, termed RMDNet, includes two main\nmodules, where the relation modeling (RM) module implements the contrastive\nlearning technique to learn representations of all data, an unsupervised\napproach that effectively eliminates the interference of noisy tags on feature\nextraction. The relation-guided representation learning (RGRL) module utilizes\ninter-sample relation learned from the RM module to calibrate the\nrepresentation distribution for noisy samples, which is capable of improving\nthe generalization of the model in the inference phase. Notably, the proposed\nRMDNet is a plug-and-play framework that can integrate multiple methods to its\nadvantage. Extensive experiments were conducted on two datasets, including\nperformance comparison, ablation study, in-depth analysis and case study. The\nresults show that RMDNet can learn discriminative representations for noisy\ndata, which results in superior performance than the existing methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19606v2",
    "published_date": "2024-05-30 01:47:27 UTC",
    "updated_date": "2024-06-02 01:59:09 UTC"
  },
  {
    "arxiv_id": "2405.19600v2",
    "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
    "authors": [
      "Xiangru Jian",
      "Xinjian Zhao",
      "Wei Pang",
      "Chaolong Ying",
      "Yimu Wang",
      "Yaoyao Xu",
      "Tianshu Yu"
    ],
    "abstract": "The recent surge in contrast-based graph self-supervised learning has\nprominently featured an intensified exploration of spectral cues. Spectral\naugmentation, which involves modifying a graph's spectral properties such as\neigenvalues or eigenvectors, is widely believed to enhance model performance.\nHowever, an intriguing paradox emerges, as methods grounded in seemingly\nconflicting assumptions regarding the spectral domain demonstrate notable\nenhancements in learning performance. Through extensive empirical studies, we\nfind that simple edge perturbations - random edge dropping for node-level and\nrandom edge adding for graph-level self-supervised learning - consistently\nyield comparable or superior performance while being significantly more\ncomputationally efficient. This suggests that the computational overhead of\nsophisticated spectral augmentations may not justify their practical benefits.\nOur theoretical analysis of the InfoNCE loss bounds for shallow GNNs further\nsupports this observation. The proposed insights represent a significant leap\nforward in the field, potentially refining the understanding and implementation\nof graph self-supervised learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19600v2",
    "published_date": "2024-05-30 01:30:34 UTC",
    "updated_date": "2024-12-04 04:41:49 UTC"
  },
  {
    "arxiv_id": "2405.19597v1",
    "title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",
    "authors": [
      "Vijay Lingam",
      "Atula Tejaswi",
      "Aditya Vavre",
      "Aneesh Shetty",
      "Gautham Krishna Gudur",
      "Joydeep Ghosh",
      "Alex Dimakis",
      "Eunsol Choi",
      "Aleksandar Bojchevski",
      "Sujay Sanghavi"
    ],
    "abstract": "Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its\nvariants, freeze pre-trained model weights \\(W\\) and inject learnable matrices\n\\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient\nparameterization, often using techniques like low-rank approximations or\nscaling vectors. However, these methods typically show a performance gap\ncompared to full fine-tuning. Although recent PEFT methods have narrowed this\ngap, they do so at the cost of additional learnable parameters. We propose\nSVFT, a simple approach that fundamentally differs from existing methods: the\nstructure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).\nSpecifically, SVFT updates \\(W\\) as a sparse combination of outer products of\nits singular vectors, training only the coefficients (scales) of these sparse\ncombinations. This approach allows fine-grained control over expressivity\nthrough the number of coefficients. Extensive experiments on language and\nvision benchmarks show that SVFT recovers up to 96% of full fine-tuning\nperformance while training only 0.006 to 0.25% of parameters, outperforming\nexisting methods that only recover up to 85% performance using 0.03 to 0.8% of\nthe trainable parameter budget.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 5 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.19597v1",
    "published_date": "2024-05-30 01:27:43 UTC",
    "updated_date": "2024-05-30 01:27:43 UTC"
  },
  {
    "arxiv_id": "2405.19592v1",
    "title": "Why Larger Language Models Do In-context Learning Differently?",
    "authors": [
      "Zhenmei Shi",
      "Junyi Wei",
      "Zhuoyan Xu",
      "Yingyu Liang"
    ],
    "abstract": "Large language models (LLM) have emerged as a powerful tool for AI, with the\nkey ability of in-context learning (ICL), where they can perform well on unseen\ntasks based on a brief series of task examples without necessitating any\nadjustments to the model parameters. One recent interesting mysterious\nobservation is that models of different scales may have different ICL\nbehaviors: larger models tend to be more sensitive to noise in the test\ncontext. This work studies this observation theoretically aiming to improve the\nunderstanding of LLM and ICL. We analyze two stylized settings: (1) linear\nregression with one-layer single-head linear transformers and (2) parity\nclassification with two-layer multiple attention heads transformers (non-linear\ndata and non-linear model). In both settings, we give closed-form optimal\nsolutions and find that smaller models emphasize important hidden features\nwhile larger ones cover more hidden features; thus, smaller models are more\nrobust to noise while larger ones are more easily distracted, leading to\ndifferent ICL behaviors. This sheds light on where transformers pay attention\nto and how that affects ICL. Preliminary experimental results on large base and\nchat models provide positive support for our analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19592v1",
    "published_date": "2024-05-30 01:11:35 UTC",
    "updated_date": "2024-05-30 01:11:35 UTC"
  },
  {
    "arxiv_id": "2405.19581v2",
    "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
    "authors": [
      "Zian Su",
      "Xiangzhe Xu",
      "Ziyang Huang",
      "Kaiyuan Zhang",
      "Xiangyu Zhang"
    ],
    "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.19581v2",
    "published_date": "2024-05-30 00:17:44 UTC",
    "updated_date": "2024-10-30 16:12:36 UTC"
  }
]
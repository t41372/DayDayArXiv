{
  "date": "2024-05-24",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-24 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 领域的创新，包括大语言模型（LLM）的安全、对齐和迁移、强化学习（RL）的攻击防御与优化、图像生成与处理，以及跨领域应用如医疗和文化理解，亮点包括 Jeff Clune 等知名学者的探索算法与 LLM 结合，以及 Elias Bareinboim 的因果公平性研究，这些工作展示了 AI 在实际问题中的潜力。\n\n下面，我挑选并简要讨论几篇重要的、话题度高的论文，先从 LLM 和 RL 等核心领域入手，再快速掠过其他相关或次要工作。限于篇幅，我会聚焦于关键贡献，避免冗长描述。\n\n### 关键论文讨论\n\n**1. CulturePark: Boosting Cross-cultural Understanding in Large Language Models（文化公园：提升大语言模型的跨文化理解）**  \n   作者包括 Cheng Li 和 Juanzi Li。这篇论文提出 CulturePark，一个多代理通信框架，使用 LLM 模拟跨文化对话生成数据集，并微调模型以提升文化敏感性。主要贡献是通过生成高质量文化数据，显著改善 LLM 在内容审查、文化对齐和教育任务上的性能，实验显示在 TruthfulQA 等基准上超越 GPT-4，强调了 LLM 在文化包容性中的应用潜力。\n\n**2. Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models（智能 Go-Explore：基于巨型基础模型的探索算法）**  \n   作者包括 Jeff Clune，这位知名学者参与的工作令人印象深刻。论文将 LLM 与 Go-Explore 算法结合，创建智能代理来处理复杂探索任务。主要发现是，代理能识别有价值的意外发现，提升探索效率，在语言和视觉任务中超越传统 RL 和 FM 代理，展示了 LLM 在强化学习中的通用性。\n\n**3. Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee（带理论保证的分散强化学习中的合作后门攻击）**  \n   作者如 Mengtong Gao 和 Dongxiao Yu。论文探索后门攻击在分散 RL 中的合作机制，通过分解后门行为到不同代理，实现更隐蔽的攻击。主要贡献是提供理论证明和实验验证，证明这种方法在 Atari 环境中高效且隐蔽，提醒 RL 系统需加强防御。\n\n**4. Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation（使用双向状态空间模型扩展扩散 Mamba 以高效生成图像和视频）**  \n   作者如 Shentong Mo。论文提出 Diffusion Mamba 架构，使用双向状态空间模型（SSMs）优化图像生成，复杂度线性增长。主要发现是，该模型在图像和视频任务中超越传统扩散变压器，显著提升效率和性能，适用于高分辨率生成。\n\n**5. Fairness-Accuracy Trade-Offs: A Causal Perspective（公平性与准确性权衡：从因果视角分析）**  \n   作者包括 Elias Bareinboim，这位因果推理专家的作品值得关注。论文从因果角度分析 AI 决策中的公平与准确权衡，引入路径特定过量损失（PSEL）概念。主要贡献是提出新公平指标和算法，证明因果约束能平衡性能，并在实验中验证其有效性。\n\n**6. Weak-to-Strong Generalization in Large Language Models（大语言模型中的弱到强泛化）**  \n   作者如 Moses Charikar。论文量化 LLM 从弱监督到强泛化的性能提升，通过理论框架解释这种现象。主要发现是，强模型的改进受拟合误差影响，实验验证了 LLM 在复杂任务中的鲁棒性。\n\n其他论文中，医疗 AI 如 \"Risk Factor Identification In Osteoporosis Using Unsupervised Machine Learning Techniques（使用无监督机器学习识别骨质疏松风险因素）\" 提出聚类框架识别风险因素；图像处理如 \"Enhancing Visual-Language Modality Alignment in Large Vision Language Models（增强大视觉语言模型的多模态对齐）\" 使用自监督方法改善对齐；RL 优化如 \"Online Prompt Pricing based on Combinatorial Multi-Armed Bandit（基于组合多臂赌博机的在线提示定价）\" 探索多任务定价策略。这些工作虽有价值，但相对常规，我仅快速提及其核心：它们提供实用工具，但影响力不如上述重点论文。\n\n总之，今天的 arXiv 展示了 AI 领域的多样创新，LLM 和 RL 的安全与扩展尤为突出，值得跟踪。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2405.15985v1",
      "title": "The Impact and Opportunities of Generative AI in Fact-Checking",
      "title_zh": "生成式 AI 在事实核查中的影响和机会",
      "authors": [
        "Robert Wolfe",
        "Tanushree Mitra"
      ],
      "abstract": "Generative AI appears poised to transform white collar professions, with more\nthan 90% of Fortune 500 companies using OpenAI's flagship GPT models, which\nhave been characterized as \"general purpose technologies\" capable of effecting\nepochal changes in the economy. But how will such technologies impact\norganizations whose job is to verify and report factual information, and to\nensure the health of the information ecosystem? To investigate this question,\nwe conducted 30 interviews with N=38 participants working at 29 fact-checking\norganizations across six continents, asking about how they use generative AI\nand the opportunities and challenges they see in the technology. We found that\nuses of generative AI envisioned by fact-checkers differ based on\norganizational infrastructure, with applications for quality assurance in\nEditing, for trend analysis in Investigation, and for information literacy in\nAdvocacy. We used the TOE framework to describe participant concerns ranging\nfrom the Technological (lack of transparency), to the Organizational (resource\nconstraints), to the Environmental (uncertain and evolving policy). Building on\nthe insights of our participants, we describe value tensions between\nfact-checking and generative AI, and propose a novel Verification dimension to\nthe design space of generative models for information verification work.\nFinally, we outline an agenda for fairness, accountability, and transparency\nresearch to support the responsible use of generative AI in fact-checking.\nThroughout, we highlight the importance of human infrastructure and labor in\nproducing verified information in collaboration with AI. We expect that this\nwork will inform not only the scientific literature on fact-checking, but also\ncontribute to understanding of organizational adaptation to a powerful but\nunreliable new technology.",
      "tldr_zh": "本研究调查了生成式 AI 对事实查证组织的影响和机会，通过对 38 名参与者（来自 29 个组织、六个大陆）的 30 次采访，探讨了其在编辑、调查和倡导中的应用，例如用于质量保证、趋势分析和信息素养。研究发现，生成式 AI 的使用受组织基础设施影响，同时面临 TOE framework 中的挑战，包括技术层面的缺乏透明度、组织层面的资源限制，以及环境层面的政策不确定性。论文突出了事实查证与生成式 AI 之间的价值冲突，提出一个新的 Verification 维度，并概述了公平性、问责性和透明性研究议程，以支持人类与 AI 的协作，确保信息生态系统的健康。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "To be published at the ACM Conference on Fairness, Accountability,\n  and Transparency (FAccT) 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15985v1",
      "published_date": "2024-05-24 23:58:01 UTC",
      "updated_date": "2024-05-24 23:58:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:24:15.644868"
    },
    {
      "arxiv_id": "2405.15984v4",
      "title": "Evaluating and Safeguarding the Adversarial Robustness of Retrieval-Based In-Context Learning",
      "title_zh": "基于检索的上下文学习的对抗鲁棒性评估与保护",
      "authors": [
        "Simon Yu",
        "Jie He",
        "Pasquale Minervini",
        "Jeff Z. Pan"
      ],
      "abstract": "With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl",
      "tldr_zh": "本研究评估了基于检索增强的 In-Context Learning (ICL) 方法在对抗攻击下的鲁棒性，发现这种方法能提升对测试样本攻击的抵抗力，比传统 ICL 降低了 Attack Success Rate (ASR) 4.87%，但在演示攻击上表现出过自信，导致 ASR 增加 2%。为了改善这一问题，论文提出了一种无需训练的防御方法 DARD，通过在示例池中加入攻击样本，显著提升了模型性能和鲁棒性，使 ASR 较基线减少 15%。这项工作强调了检索增强 ICL 的潜在优势，并提供了开源代码以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLM 2024, 31 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15984v4",
      "published_date": "2024-05-24 23:56:36 UTC",
      "updated_date": "2024-10-08 18:08:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:24:27.695914"
    },
    {
      "arxiv_id": "2405.15973v4",
      "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement",
      "title_zh": "翻译失败",
      "authors": [
        "Xiyao Wang",
        "Jiuhai Chen",
        "Zhaoyang Wang",
        "Yuhang Zhou",
        "Yiyang Zhou",
        "Huaxiu Yao",
        "Tianyi Zhou",
        "Tom Goldstein",
        "Parminder Bhatia",
        "Furong Huang",
        "Cao Xiao"
      ],
      "abstract": "Large vision-language models (LVLMs) have achieved impressive results in\nvisual question-answering and reasoning tasks through vision instruction tuning\non specific datasets. However, there remains significant room for improvement\nin aligning visual and language modalities. Existing methods often depend on\nexternal models or data, leading to uncontrollable and unstable alignment\nresults. In this paper, we propose SIMA, a self-improvement framework that\nenhances visual and language modality alignment without external dependencies.\nSIMA leverages existing vision instruction tuning datasets to self-generate\nresponses, incorporating an in-context self-critic mechanism that constructs\npreference pairs for tuning. Crucially, our approach allows LVLMs to act as\ncritics by designing effective critic prompts, eliminating the need for\nadditional fine-tuning with external instruction data. We introduce three novel\nvisual metrics within the self-critic process to guide judgment, significantly\nimproving the accuracy of self-critic. Through extensive experiments across 14\nhallucination and comprehensive benchmarks, we demonstrate that SIMA\nsignificantly improves LVLM's performance and outperforms previous approaches,\nachieving superior modality alignment.",
      "tldr_zh": "本研究针对 Large Vision-Language Models (LVLMs) 在视觉问答和推理任务中视觉与语言模态对齐不足的问题，提出了一种自提升框架 SIMA，无需依赖外部模型或数据。\nSIMA 利用现有视觉指令调优数据集自我生成响应，并引入 in-context self-critic 机制构建偏好对进行调优，同时设计有效的批评提示和三个新颖的视觉指标来提升自批评的准确性。\n通过在14个幻觉和综合基准上的广泛实验，SIMA 显著提高了 LVLMs 的性能，超越了先前方法，实现更优的模态对齐。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NAACL 2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2405.15973v4",
      "published_date": "2024-05-24 23:09:27 UTC",
      "updated_date": "2025-02-08 21:50:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:24:40.894167"
    },
    {
      "arxiv_id": "2405.15960v1",
      "title": "Human-Centered Automation",
      "title_zh": "以人为中心的自动化",
      "authors": [
        "Carlos Toxtli"
      ],
      "abstract": "The rapid advancement of Generative Artificial Intelligence (AI), such as\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLM), has\nthe potential to revolutionize the way we work and interact with digital\nsystems across various industries. However, the current state of software\nautomation, such as Robotic Process Automation (RPA) frameworks, often requires\ndomain expertise and lacks visibility and intuitive interfaces, making it\nchallenging for users to fully leverage these technologies. This position paper\nargues for the emerging area of Human-Centered Automation (HCA), which\nprioritizes user needs and preferences in the design and development of\nautomation systems. Drawing on empirical evidence from human-computer\ninteraction research and case studies, we highlight the importance of\nconsidering user perspectives in automation and propose a framework for\ndesigning human-centric automation solutions. The paper discusses the\nlimitations of existing automation approaches, the challenges in integrating AI\nand RPA, and the benefits of human-centered automation for productivity,\ninnovation, and democratizing access to these technologies. We emphasize the\nimportance of open-source solutions and provide examples of how HCA can empower\nindividuals and organizations in the era of rapidly progressing AI, helping\nthem remain competitive. The paper also explores pathways to achieve more\nadvanced and context-aware automation solutions. We conclude with a call to\naction for researchers and practitioners to focus on developing automation\ntechnologies that adapt to user needs, provide intuitive interfaces, and\nleverage the capabilities of high-end AI to create a more accessible and\nuser-friendly future of automation.",
      "tldr_zh": "这篇论文讨论了生成式人工智能（AI），如 Large Language Models (LLMs) 和 Multimodal Large Language Models (MLLM)，在工作和数字系统中的革命性潜力，但指出现有 Robotic Process Automation (RPA) 框架因需领域专业知识和缺乏直观界面而存在局限性。论文提出 Human-Centered Automation (HCA) 概念和框架，强调在自动化系统设计中优先考虑用户需求和偏好，并基于人类-计算机交互研究及案例研究，提供证据支持。HCA 可提升生产力、创新，并通过开源解决方案普及 AI 技术，最终呼吁研究者和从业者开发适应用户需求的、上下文感知的自动化系统。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "12 pages, 0 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15960v1",
      "published_date": "2024-05-24 22:12:28 UTC",
      "updated_date": "2024-05-24 22:12:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:24:53.299307"
    },
    {
      "arxiv_id": "2405.15956v1",
      "title": "CFGs: Causality Constrained Counterfactual Explanations using goal-directed ASP",
      "title_zh": "翻译失败",
      "authors": [
        "Sopam Dasgupta",
        "Joaquín Arias",
        "Elmer Salazar",
        "Gopal Gupta"
      ],
      "abstract": "Machine learning models that automate decision-making are increasingly used\nin consequential areas such as loan approvals, pretrial bail approval, and\nhiring. Unfortunately, most of these models are black boxes, i.e., they are\nunable to reveal how they reach these prediction decisions. A need for\ntransparency demands justification for such predictions. An affected individual\nmight also desire explanations to understand why a decision was made. Ethical\nand legal considerations require informing the individual of changes in the\ninput attribute (s) that could be made to produce a desirable outcome. Our work\nfocuses on the latter problem of generating counterfactual explanations by\nconsidering the causal dependencies between features. In this paper, we present\nthe framework CFGs, CounterFactual Generation with s(CASP), which utilizes the\ngoal-directed Answer Set Programming (ASP) system s(CASP) to automatically\ngenerate counterfactual explanations from models generated by rule-based\nmachine learning algorithms in particular. We benchmark CFGs with the FOLD-SE\nmodel. Reaching the counterfactual state from the initial state is planned and\nachieved using a series of interventions. To validate our proposal, we show how\ncounterfactual explanations are computed and justified by imagining worlds\nwhere some or all factual assumptions are altered/changed. More importantly, we\nshow how CFGs navigates between these worlds, namely, go from our initial state\nwhere we obtain an undesired outcome to the imagined goal state where we obtain\nthe desired decision, taking into account the causal relationships among\nfeatures.",
      "tldr_zh": "该论文针对机器学习模型在决策领域的黑箱问题（如贷款审批），提出 CFGs 框架，利用 goal-directed Answer Set Programming (s(CASP)) 生成考虑因果关系的 Counterfactual Explanations。CFGs 通过规划一系列干预序列，从初始状态（ undesired outcome）导航到目标状态（desired outcome），同时尊重特征间的因果依赖。实验benchmark 与 FOLD-SE 模型比较，展示了框架如何通过修改假设世界来计算和验证反事实解释，从而提升模型的透明性和可解释性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2402.04382",
      "pdf_url": "http://arxiv.org/pdf/2405.15956v1",
      "published_date": "2024-05-24 21:47:58 UTC",
      "updated_date": "2024-05-24 21:47:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:25:13.511236"
    },
    {
      "arxiv_id": "2405.15936v1",
      "title": "Zero-Shot Spam Email Classification Using Pre-trained Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sergio Rojas-Galeano"
      ],
      "abstract": "This paper investigates the application of pre-trained large language models\n(LLMs) for spam email classification using zero-shot prompting. We evaluate the\nperformance of both open-source (Flan-T5) and proprietary LLMs (ChatGPT, GPT-4)\non the well-known SpamAssassin dataset. Two classification approaches are\nexplored: (1) truncated raw content from email subject and body, and (2)\nclassification based on summaries generated by ChatGPT. Our empirical analysis,\nleveraging the entire dataset for evaluation without further training, reveals\npromising results. Flan-T5 achieves a 90% F1-score on the truncated content\napproach, while GPT-4 reaches a 95% F1-score using summaries. While these\ninitial findings on a single dataset suggest the potential for classification\npipelines of LLM-based subtasks (e.g., summarisation and classification),\nfurther validation on diverse datasets is necessary. The high operational costs\nof proprietary models, coupled with the general inference costs of LLMs, could\nsignificantly hinder real-world deployment for spam filtering.",
      "tldr_zh": "本研究探讨了使用预训练的大型语言模型（LLMs）进行零样本提示（zero-shot prompting）的垃圾邮件分类，评估了开源模型 Flan-T5 和专有模型 ChatGPT、GPT-4 在 SpamAssassin 数据集上的表现。研究采用了两种方法：一是基于邮件主题和正文的截断原始内容，二是基于 ChatGPT 生成的摘要进行分类。结果显示，Flan-T5 在截断内容方法上达到了 90% 的 F1-score，而 GPT-4 在使用摘要方法上达到了 95% 的 F1-score。这些初步发现表明，LLMs 在子任务（如总结和分类）管道中的潜力，但高运营成本和推理开销可能限制实际部署，需要在更多数据集上进一步验证。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15936v1",
      "published_date": "2024-05-24 20:55:49 UTC",
      "updated_date": "2024-05-24 20:55:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:25:17.474980"
    },
    {
      "arxiv_id": "2405.15928v1",
      "title": "PatchProt: Hydrophobic patch prediction using protein foundation models",
      "title_zh": "PatchProt：使用蛋白质基础模型的疏水斑块预测",
      "authors": [
        "Dea Gogishvili",
        "Emmanuel Minois-Genin",
        "Jan van Eck",
        "Sanne Abeln"
      ],
      "abstract": "Hydrophobic patches on protein surfaces play important functional roles in\nprotein-protein and protein-ligand interactions. Large hydrophobic surfaces are\nalso involved in the progression of aggregation diseases. Predicting exposed\nhydrophobic patches from a protein sequence has been shown to be a difficult\ntask. Fine-tuning foundation models allows for adapting a model to the specific\nnuances of a new task using a much smaller dataset. Additionally, multi-task\ndeep learning offers a promising solution for addressing data gaps,\nsimultaneously outperforming single-task methods. In this study, we harnessed a\nrecently released leading large language model ESM-2. Efficient fine-tuning of\nESM-2 was achieved by leveraging a recently developed parameter-efficient\nfine-tuning method. This approach enabled comprehensive training of model\nlayers without excessive parameters and without the need to include a\ncomputationally expensive multiple sequence analysis. We explored several\nrelated tasks, at local (residue) and global (protein) levels, to improve the\nrepresentation of the model. As a result, our fine-tuned ESM-2 model,\nPatchProt, cannot only predict hydrophobic patch areas but also outperforms\nexisting methods at predicting primary tasks, including secondary structure and\nsurface accessibility predictions. Importantly, our analysis shows that\nincluding related local tasks can improve predictions on more difficult global\ntasks. This research sets a new standard for sequence-based protein property\nprediction and highlights the remarkable potential of fine-tuning foundation\nmodels enriching the model representation by training over related tasks.",
      "tldr_zh": "本研究针对蛋白质表面的疏水斑块（hydrophobic patches）预测问题，提出了一种基于 ESM-2 模型的 PatchProt 方法，这些斑块在蛋白质-蛋白质和蛋白质-配体相互作用中扮演关键角色，并与聚集性疾病相关。研究采用参数高效微调（parameter-efficient fine-tuning）技术结合多任务学习，探索局部（residue 级别）和全局（protein 级别）任务，以提升模型在小数据集上的适应性和性能。结果显示，PatchProt 不仅在预测疏水斑块方面表现出色，还优于现有方法在次要任务如二级结构和表面可及性预测上；此外，纳入相关局部任务显著改善了更复杂的全局任务预测，为基于序列的蛋白质属性预测树立了新标准。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15928v1",
      "published_date": "2024-05-24 20:37:02 UTC",
      "updated_date": "2024-05-24 20:37:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:25:29.141037"
    },
    {
      "arxiv_id": "2405.15908v1",
      "title": "Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanliang Li",
        "Hanzheng Dai",
        "Jun Yan"
      ],
      "abstract": "Automated penetration testing (AutoPT) based on reinforcement learning (RL)\nhas proven its ability to improve the efficiency of vulnerability\nidentification in information systems. However, RL-based PT encounters several\nchallenges, including poor sampling efficiency, intricate reward specification,\nand limited interpretability. To address these issues, we propose a\nknowledge-informed AutoPT framework called DRLRM-PT, which leverages reward\nmachines (RMs) to encode domain knowledge as guidelines for training a PT\npolicy. In our study, we specifically focus on lateral movement as a PT case\nstudy and formulate it as a partially observable Markov decision process\n(POMDP) guided by RMs. We design two RMs based on the MITRE ATT\\&CK knowledge\nbase for lateral movement. To solve the POMDP and optimize the PT policy, we\nemploy the deep Q-learning algorithm with RM (DQRM). The experimental results\ndemonstrate that the DQRM agent exhibits higher training efficiency in PT\ncompared to agents without knowledge embedding. Moreover, RMs encoding more\ndetailed domain knowledge demonstrated better PT performance compared to RMs\nwith simpler knowledge.",
      "tldr_zh": "本研究针对基于强化学习（Reinforcement Learning, RL）的自动渗透测试（AutoPT）面临的采样效率低、奖励指定复杂和可解释性有限等问题，提出了一种知识驱动框架DRLRM-PT，利用奖励机器（Reward Machines, RMs）编码领域知识来指导PT策略训练。框架将横向移动（lateral movement）作为案例研究，构建为部分可观测Markov决策过程（POMDP），并基于MITRE ATT&CK知识库设计两个RMs。实验结果显示，采用深度Q学习算法与RM（DQRM）的代理在训练效率上显著高于无知识嵌入的代理，且编码更详细领域知识的RMs表现出更好的PT性能。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15908v1",
      "published_date": "2024-05-24 20:05:12 UTC",
      "updated_date": "2024-05-24 20:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:25:39.631019"
    },
    {
      "arxiv_id": "2405.15907v2",
      "title": "Belief-State Query Policies for User-Aligned POMDPs",
      "title_zh": "用户对齐 POMDPs 的信念状态查询策略",
      "authors": [
        "Daniel Bramblett",
        "Siddharth Srivastava"
      ],
      "abstract": "Planning in real-world settings often entails addressing partial\nobservability while aligning with users' requirements. We present a novel\nframework for expressing users' constraints and preferences about agent\nbehavior in a partially observable setting using parameterized belief-state\nquery (BSQ) policies in the setting of goal-oriented partially observable\nMarkov decision processes (gPOMDPs). We present the first formal analysis of\nsuch constraints and prove that while the expected cost function of a\nparameterized BSQ policy w.r.t its parameters is not convex, it is piecewise\nconstant and yields an implicit discrete parameter search space that is finite\nfor finite horizons. This theoretical result leads to novel algorithms that\noptimize gPOMDP agent behavior with guaranteed user alignment. Analysis proves\nthat our algorithms converge to the optimal user-aligned behavior in the limit.\nEmpirical results show that parameterized BSQ policies provide a\ncomputationally feasible approach for user-aligned planning in partially\nobservable settings.",
      "tldr_zh": "本论文提出了一种新框架，使用参数化的信念状态查询 (BSQ) 策略来处理部分可观测 Markov 决策过程 (POMDPs) 中的目标导向场景 (gPOMDPs)，以确保代理行为与用户的约束和偏好对齐。研究首次对 BSQ 策略的期望成本函数进行了正式分析，证明其虽非凸函数，但为分段常数，并生成有限的离散参数搜索空间，从而支持有限地平线的优化。基于此，论文开发了新算法，这些算法能优化代理行为并保证用户对齐，且理论分析证明算法在极限情况下收敛到最优解。经验结果显示，该方法在部分可观测环境中提供了一种计算可行的用户对齐规划方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15907v2",
      "published_date": "2024-05-24 20:04:51 UTC",
      "updated_date": "2025-04-15 17:47:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:25:54.311315"
    },
    {
      "arxiv_id": "2405.15902v1",
      "title": "Hacc-Man: An Arcade Game for Jailbreaking LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Matheus Valentim",
        "Jeanette Falk",
        "Nanna Inie"
      ],
      "abstract": "The recent leaps in complexity and fluency of Large Language Models (LLMs)\nmean that, for the first time in human history, people can interact with\ncomputers using natural language alone. This creates monumental possibilities\nof automation and accessibility of computing, but also raises severe security\nand safety threats: When everyone can interact with LLMs, everyone can\npotentially break into the systems running LLMs. All it takes is creative use\nof language. This paper presents Hacc-Man, a game which challenges its players\nto \"jailbreak\" an LLM: subvert the LLM to output something that it is not\nintended to. Jailbreaking is at the intersection between creative problem\nsolving and LLM security. The purpose of the game is threefold: 1. To heighten\nawareness of the risks of deploying fragile LLMs in everyday systems, 2. To\nheighten people's self-efficacy in interacting with LLMs, and 3. To discover\nthe creative problem solving strategies, people deploy in this novel context.",
      "tldr_zh": "这篇论文介绍了 Hacc-Man，一种街机游戏，旨在通过挑战玩家使用创造性语言来 jailbreaking LLMs（即让大型语言模型输出不意图的内容），从而揭示 LLMs 的安全漏洞。游戏将 jailbreaking 与创造性问题解决相结合，作为一种互动方式，以提高人们对部署脆弱 LLMs 的风险意识，并增强用户与 LLMs 互动的自我效能感。同时，该游戏有助于发现人们在这种新环境中采用的创新策略，为 LLMs 安全研究提供新视角。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15902v1",
      "published_date": "2024-05-24 19:55:20 UTC",
      "updated_date": "2024-05-24 19:55:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:26:03.161216"
    },
    {
      "arxiv_id": "2405.15882v1",
      "title": "Risk Factor Identification In Osteoporosis Using Unsupervised Machine Learning Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Mikayla Calitis"
      ],
      "abstract": "In this study, the reliability of identified risk factors associated with\nosteoporosis is investigated using a new clustering-based method on electronic\nmedical records. This study proposes utilizing a new CLustering Iterations\nFramework (CLIF) that includes an iterative clustering framework that can adapt\nany of the following three components: clustering, feature selection, and\nprincipal feature identification. The study proposes using Wasserstein distance\nto identify principal features, borrowing concepts from the optimal transport\ntheory. The study also suggests using a combination of ANOVA and ablation tests\nto select influential features from a data set. Some risk factors presented in\nexisting works are endorsed by our identified significant clusters, while the\nreliability of some other risk factors is weakened.",
      "tldr_zh": "这篇论文使用无监督机器学习技术调查骨质疏松症的风险因素，提出了一种新的CLustering Iterations Framework (CLIF)框架，通过迭代聚类来分析电子病历数据。CLIF允许适应聚类、特征选择和主要特征识别，并利用Wasserstein distance从最优传输理论中借用概念来识别关键特征，同时结合ANOVA和ablation tests筛选影响性特征。研究结果验证了一些现有风险因素的可靠性，但也削弱了其他因素的可靠性，为风险因素识别提供了更可靠的方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 10 figures, 4 algorithms",
      "pdf_url": "http://arxiv.org/pdf/2405.15882v1",
      "published_date": "2024-05-24 18:53:28 UTC",
      "updated_date": "2024-05-24 18:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:26:15.125256"
    },
    {
      "arxiv_id": "2405.15881v1",
      "title": "Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation",
      "title_zh": "使用双向 SSMs 扩展 Diffusion Mamba 以实现高效图像和视频生成",
      "authors": [
        "Shentong Mo",
        "Yapeng Tian"
      ],
      "abstract": "In recent developments, the Mamba architecture, known for its selective state\nspace approach, has shown potential in the efficient modeling of long\nsequences. However, its application in image generation remains underexplored.\nTraditional diffusion transformers (DiT), which utilize self-attention blocks,\nare effective but their computational complexity scales quadratically with the\ninput length, limiting their use for high-resolution images. To address this\nchallenge, we introduce a novel diffusion architecture, Diffusion Mamba (DiM),\nwhich foregoes traditional attention mechanisms in favor of a scalable\nalternative. By harnessing the inherent efficiency of the Mamba architecture,\nDiM achieves rapid inference times and reduced computational load, maintaining\nlinear complexity with respect to sequence length. Our architecture not only\nscales effectively but also outperforms existing diffusion transformers in both\nimage and video generation tasks. The results affirm the scalability and\nefficiency of DiM, establishing a new benchmark for image and video generation\ntechniques. This work advances the field of generative models and paves the way\nfor further applications of scalable architectures.",
      "tldr_zh": "这篇论文引入了Diffusion Mamba (DiM)，一种基于Mamba架构和bidirectional SSMs的双向状态空间模型，用于高效的图像和视频生成。DiM放弃了传统diffusion transformers (DiT)的自注意力机制，将计算复杂度从二次降低为线性，从而实现快速推理和减少计算负载。相比基线模型，DiM在图像和视频生成任务中表现出色，显著提升了性能。实验结果证明了DiM的可扩展性，并为生成模型领域设定了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15881v1",
      "published_date": "2024-05-24 18:50:27 UTC",
      "updated_date": "2024-05-24 18:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:26:27.633845"
    },
    {
      "arxiv_id": "2405.15880v2",
      "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Shraddha Barke",
        "Emmanuel Anaya Gonzalez",
        "Saketh Ram Kasibatla",
        "Taylor Berg-Kirkpatrick",
        "Nadia Polikarpova"
      ],
      "abstract": "Many structured prediction and reasoning tasks can be framed as program\nsynthesis problems, where the goal is to generate a program in a\ndomain-specific language (DSL) that transforms input data into the desired\noutput. Unfortunately, purely neural approaches, such as large language models\n(LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while\npurely symbolic methods based on combinatorial search scale poorly to complex\nproblems. Motivated by these limitations, we introduce a hybrid approach, where\nLLM completions for a given task are used to learn a task-specific,\ncontext-free surrogate model, which is then used to guide program synthesis. We\nevaluate this hybrid approach on three domains, and show that it outperforms\nboth unguided search and direct sampling from LLMs, as well as existing program\nsynthesizers.",
      "tldr_zh": "本研究提出 HYSYNTH，一种混合方法，用于指导程序合成（program synthesis），它利用 LLM（Large Language Models）的完成结果学习一个任务特定的、context-free 代理模型，以克服纯神经方法在不熟悉的 DSL（Domain-Specific Language）中生成不准确程序，以及纯符号方法在复杂问题上扩展性差的问题。\nHYSYNTH 通过将 LLM 输出作为基础，构建代理模型来引导合成过程，确保更高效的程序生成。\n实验在三个领域上验证了该方法的有效性，表现出色，优于无指导搜索、直接从 LLM 采样以及现有程序合成器。",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "Accepted at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15880v2",
      "published_date": "2024-05-24 18:45:51 UTC",
      "updated_date": "2024-11-01 02:46:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:26:50.524763"
    },
    {
      "arxiv_id": "2405.17485v2",
      "title": "Comet: A Communication-efficient and Performant Approximation for Private Transformer Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangrui Xu",
        "Qiao Zhang",
        "Rui Ning",
        "Chunsheng Xin",
        "Hongyi Wu"
      ],
      "abstract": "The prevalent use of Transformer-like models, exemplified by ChatGPT in\nmodern language processing applications, underscores the critical need for\nenabling private inference essential for many cloud-based services reliant on\nsuch models. However, current privacy-preserving frameworks impose significant\ncommunication burden, especially for non-linear computation in Transformer\nmodel. In this paper, we introduce a novel plug-in method Comet to effectively\nreduce the communication cost without compromising the inference performance.\nWe second introduce an efficient approximation method to eliminate the heavy\ncommunication in finding good initial approximation. We evaluate our Comet on\nBert and RoBERTa models with GLUE benchmark datasets, showing up to 3.9$\\times$\nless communication and 3.5$\\times$ speedups while keep competitive model\nperformance compared to the prior art.",
      "tldr_zh": "本文提出 Comet，一种高效的插件方法，用于优化 Transformer 模型的私有推理过程，显著减少通信负担，同时保持推理性能。该方法引入高效近似技术，消除寻找初始近似的繁重通信开销。在 Bert 和 RoBERTa 模型上使用 GLUE 基准数据集进行评估，Comet 实现了高达 3.9 倍的通信减少和 3.5 倍的速度提升，同时保持竞争性的模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17485v2",
      "published_date": "2024-05-24 18:43:00 UTC",
      "updated_date": "2024-09-07 13:07:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:26:51.608415"
    },
    {
      "arxiv_id": "2405.15871v1",
      "title": "CausalConceptTS: Causal Attributions for Time Series Classification using High Fidelity Diffusion Models",
      "title_zh": "CausalConceptTS：使用高保真度扩散模型的时间序列分类因果归因",
      "authors": [
        "Juan Miguel Lopez Alcaraz",
        "Nils Strodthoff"
      ],
      "abstract": "Despite the excelling performance of machine learning models, understanding\nthe decisions of machine learning models remains a long-standing goal. While\ncommonly used attribution methods in explainable AI attempt to address this\nissue, they typically rely on associational rather than causal relationships.\nIn this study, within the context of time series classification, we introduce a\nnovel framework to assess the causal effect of concepts, i.e., predefined\nsegments within a time series, on specific classification outcomes. To achieve\nthis, we leverage state-of-the-art diffusion-based generative models to\nestimate counterfactual outcomes. Our approach compares these causal\nattributions with closely related associational attributions, both\ntheoretically and empirically. We demonstrate the insights gained by our\napproach for a diverse set of qualitatively different time series\nclassification tasks. Although causal and associational attributions might\noften share some similarities, in all cases they differ in important details,\nunderscoring the risks associated with drawing causal conclusions from\nassociational data alone. We believe that the proposed approach is widely\napplicable also in other domains, particularly where predefined segmentations\nare available, to shed some light on the limits of associational attributions.",
      "tldr_zh": "本研究提出CausalConceptTS框架，用于评估时间序列分类中预定义概念（如时间序列段）对分类结果的因果归因，而不是依赖传统的关联归因。框架利用高保真扩散模型估计反事实 outcomes，从而比较因果归因与关联归因的差异，并在理论和实证上进行验证。实验结果显示，虽然两者在某些方面相似，但因果归因揭示了重要细节差异，突显了从关联数据中推断因果的潜在风险，并表明该方法可扩展到其他有预定义分割的领域，以提升模型解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 8 figures. Source code under\n  https://github.com/AI4HealthUOL/CausalConceptTS",
      "pdf_url": "http://arxiv.org/pdf/2405.15871v1",
      "published_date": "2024-05-24 18:33:18 UTC",
      "updated_date": "2024-05-24 18:33:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:27:03.684035"
    },
    {
      "arxiv_id": "2405.15868v2",
      "title": "LLS: Local Learning Rule for Deep Neural Networks Inspired by Neural Activity Synchronization",
      "title_zh": "LLS：受神经活动同步启发的深度神经网络局部学习规则",
      "authors": [
        "Marco Paul E. Apolinario",
        "Arani Roy",
        "Kaushik Roy"
      ],
      "abstract": "Training deep neural networks (DNNs) using traditional backpropagation (BP)\npresents challenges in terms of computational complexity and energy\nconsumption, particularly for on-device learning where computational resources\nare limited. Various alternatives to BP, including random feedback alignment,\nforward-forward, and local classifiers, have been explored to address these\nchallenges. These methods have their advantages, but they can encounter\ndifficulties when dealing with intricate visual tasks or demand considerable\ncomputational resources. In this paper, we propose a novel Local Learning rule\ninspired by neural activity Synchronization phenomena (LLS) observed in the\nbrain. LLS utilizes fixed periodic basis vectors to synchronize neuron activity\nwithin each layer, enabling efficient training without the need for additional\ntrainable parameters. We demonstrate the effectiveness of LLS and its\nvariations, LLS-M and LLS-MxM, on multiple image classification datasets,\nachieving accuracy comparable to BP with reduced computational complexity and\nminimal additional parameters. Specifically, LLS achieves comparable\nperformance with up to $300 \\times$ fewer multiply-accumulate (MAC) operations\nand half the memory requirements of BP. Furthermore, the performance of LLS on\nthe Visual Wake Word (VWW) dataset highlights its suitability for on-device\nlearning tasks, making it a promising candidate for edge hardware\nimplementations.",
      "tldr_zh": "本研究提出了一种新型本地学习规则LLS（Local Learning Rule），灵感来源于大脑神经活动同步现象，用于训练深度神经网络（DNNs），以解决传统backpropagation (BP)算法在计算复杂性和能耗方面的挑战，特别是适用于资源有限的设备端学习。LLS通过使用固定周期基向量来同步各层神经元活动，实现高效训练，而无需额外可训练参数。实验结果显示，LLS及其变体LLS-M和LLS-MxM在多个图像分类数据集上，实现了与BP相当的准确性，同时减少多达300倍的multiply-accumulate (MAC)操作和一半的内存需求。在Visual Wake Word (VWW)数据集上的表现进一步证明了其适合于设备端学习和边缘硬件应用。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15868v2",
      "published_date": "2024-05-24 18:24:24 UTC",
      "updated_date": "2024-10-29 16:35:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:27:16.394150"
    },
    {
      "arxiv_id": "2406.00030v1",
      "title": "Large Language Model Pruning",
      "title_zh": "大型语言模型剪枝",
      "authors": [
        "Hanjuan Huang",
        "Hao-Jia Song",
        "Hsing-Kuo Pao"
      ],
      "abstract": "We surely enjoy the larger the better models for their superior performance\nin the last couple of years when both the hardware and software support the\nbirth of such extremely huge models. The applied fields include text mining and\nothers. In particular, the success of LLMs on text understanding and text\ngeneration draws attention from researchers who have worked on NLP and related\nareas for years or even decades. On the side, LLMs may suffer from problems\nlike model overfitting, hallucination, and device limitation to name a few. In\nthis work, we suggest a model pruning technique specifically focused on LLMs.\nThe proposed methodology emphasizes the explainability of deep learning models.\nBy having the theoretical foundation, we obtain a trustworthy deep model so\nthat huge models with a massive number of model parameters become not quite\nnecessary. A mutual information-based estimation is adopted to find neurons\nwith redundancy to eliminate. Moreover, an estimator with well-tuned parameters\nhelps to find precise estimation to guide the pruning procedure. At the same\ntime, we also explore the difference between pruning on large-scale models vs.\npruning on small-scale models. The choice of pruning criteria is sensitive in\nsmall models but not for large-scale models. It is a novel finding through this\nwork. Overall, we demonstrate the superiority of the proposed model to the\nstate-of-the-art models.",
      "tldr_zh": "这篇论文针对大型语言模型（Large Language Models, LLMs）的常见问题，如过拟合、幻觉和设备限制，提出了一种基于互信息估计的模型修剪技术，以提升模型的可解释性和可信度。方法通过识别冗余神经元并使用参数调整良好的估计器指导修剪过程，减少了对庞大参数的需求，同时探讨了修剪标准在小规模模型和大型模型间的差异，发现标准对小模型敏感但对大模型不敏感。实验结果表明，该技术优于现有最先进模型，在保持性能的同时实现了更高效的模型设计。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.00030v1",
      "published_date": "2024-05-24 18:22:15 UTC",
      "updated_date": "2024-05-24 18:22:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:27:29.012748"
    },
    {
      "arxiv_id": "2405.15863v3",
      "title": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation",
      "title_zh": "QA-MDT：质量感知掩码扩散 Transformer 用于增强音乐生成",
      "authors": [
        "Chang Li",
        "Ruoyu Wang",
        "Lijuan Liu",
        "Jun Du",
        "Yixuan Sun",
        "Zilu Guo",
        "Zhenrong Zhang",
        "Yuan Jiang",
        "Jianqing Gao",
        "Feng Ma"
      ],
      "abstract": "Text-to-music (TTM) generation, which converts textual descriptions into\naudio, opens up innovative avenues for multimedia creation. Achieving high\nquality and diversity in this process demands extensive, high-quality data,\nwhich are often scarce in available datasets. Most open-source datasets\nfrequently suffer from issues like low-quality waveforms and low text-audio\nconsistency, hindering the advancement of music generation models. To address\nthese challenges, we propose a novel quality-aware training paradigm for\ngenerating high-quality, high-musicality music from large-scale,\nquality-imbalanced datasets. Additionally, by leveraging unique properties in\nthe latent space of musical signals, we adapt and implement a masked diffusion\ntransformer (MDT) model for the TTM task, showcasing its capacity for quality\ncontrol and enhanced musicality. Furthermore, we introduce a three-stage\ncaption refinement approach to address low-quality captions' issue. Experiments\nshow state-of-the-art (SOTA) performance on benchmark datasets including\nMusicCaps and the Song-Describer Dataset with both objective and subjective\nmetrics. Demo audio samples are available at https://qa-mdt.github.io/, code\nand pretrained checkpoints are open-sourced at\nhttps://github.com/ivcylc/OpenMusic.",
      "tldr_zh": "本研究针对文本到音乐 (TTM) 生成面临的挑战，如数据集质量低和文本-音频一致性差，提出了一种质量感知训练范式，利用大规模质量不均衡数据集生成高质量、高音乐性的音乐。论文引入 QA-MDT 模型，即质量感知的 Masked Diffusion Transformer，通过利用音乐信号的潜在空间特性，实现对生成音乐的质量控制和音乐性增强，并采用三阶段标题精炼方法来优化低质量标题。实验结果显示，QA-MDT 在 MusicCaps 和 Song-Describer Dataset 等基准数据集上达到了 SOTA 性能，在客观和主观指标上表现出色，并公开了代码和预训练模型以促进进一步研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "2025 International Joint Conference on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2405.15863v3",
      "published_date": "2024-05-24 18:09:27 UTC",
      "updated_date": "2025-04-29 04:06:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:27:40.215764"
    },
    {
      "arxiv_id": "2405.15860v1",
      "title": "Free Performance Gain from Mixing Multiple Partially Labeled Samples in Multi-label Image Classification",
      "title_zh": "在多标签图像分类中，通过混合多个部分标记样本实现免费性能提升",
      "authors": [
        "Chak Fong Chong",
        "Jielong Guo",
        "Xu Yang",
        "Wei Ke",
        "Yapeng Wang"
      ],
      "abstract": "Multi-label image classification datasets are often partially labeled where\nmany labels are missing, posing a significant challenge to training accurate\ndeep classifiers. However, the powerful Mixup sample-mixing data augmentation\ncannot be well utilized to address this challenge, as it cannot perform linear\ninterpolation on the unknown labels to construct augmented samples. In this\npaper, we propose LogicMix, a Mixup variant designed for such partially labeled\ndatasets. LogicMix mixes the sample labels by logical OR so that the unknown\nlabels can be correctly mixed by utilizing OR's logical equivalences, including\nthe domination and identity laws. Unlike Mixup, which mixes exactly two\nsamples, LogicMix can mix multiple ($\\geq2$) partially labeled samples,\nconstructing visually more confused augmented samples to regularize training.\nLogicMix is more general and effective than other compared Mixup variants in\nthe experiments on various partially labeled dataset scenarios. Moreover, it is\nplug-and-play and only requires minimal computation, hence it can be easily\ninserted into existing frameworks to collaborate with other methods to improve\nmodel performance with a negligible impact on training time, as demonstrated\nthrough extensive experiments. In particular, through the collaboration of\nLogicMix, RandAugment, Curriculum Labeling, and Category-wise Fine-Tuning, we\nattain state-of-the-art performance on MS-COCO, VG-200, and Pascal VOC 2007\nbenchmarking datasets. The remarkable generality, effectiveness, collaboration,\nand simplicity suggest that LogicMix promises to be a popular and vital data\naugmentation method.",
      "tldr_zh": "本研究针对多-label image classification 中部分标记数据集的标签缺失问题，提出了一种名为 LogicMix 的数据增强方法。该方法通过逻辑 OR 操作混合多个 (≥2) 部分标记样本，利用 OR 的逻辑等价性（如 domination 和 identity laws）来正确处理未知标签，从而创建更混淆的增强样本以提升模型训练的鲁棒性。与传统 Mixup 相比，LogicMix 更通用且高效，仅需少量计算即可即插即用，与 RandAugment、Curriculum Labeling 和 Category-wise Fine-Tuning 等方法协作，在 MS-COCO、VG-200 和 Pascal VOC 2007 等基准数据集上实现了最先进性能。这种简单有效的设计有望成为多-label 图像分类中的流行数据增强技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15860v1",
      "published_date": "2024-05-24 18:05:09 UTC",
      "updated_date": "2024-05-24 18:05:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:27:53.295921"
    },
    {
      "arxiv_id": "2405.15768v1",
      "title": "Canonical Variates in Wasserstein Metric Space",
      "title_zh": "Wasserstein 度量空间中的规范变量",
      "authors": [
        "Jia Li",
        "Lin Lin"
      ],
      "abstract": "In this paper, we address the classification of instances each characterized\nnot by a singular point, but by a distribution on a vector space. We employ the\nWasserstein metric to measure distances between distributions, which are then\nused by distance-based classification algorithms such as k-nearest neighbors,\nk-means, and pseudo-mixture modeling. Central to our investigation is dimension\nreduction within the Wasserstein metric space to enhance classification\naccuracy. We introduce a novel approach grounded in the principle of maximizing\nFisher's ratio, defined as the quotient of between-class variation to\nwithin-class variation. The directions in which this ratio is maximized are\ntermed discriminant coordinates or canonical variates axes. In practice, we\ndefine both between-class and within-class variations as the average squared\ndistances between pairs of instances, with the pairs either belonging to the\nsame class or to different classes. This ratio optimization is achieved through\nan iterative algorithm, which alternates between optimal transport and\nmaximization steps within the vector space. We conduct empirical studies to\nassess the algorithm's convergence and, through experimental validation,\ndemonstrate that our dimension reduction technique substantially enhances\nclassification performance. Moreover, our method outperforms well-established\nalgorithms that operate on vector representations derived from distributional\ndata. It also exhibits robustness against variations in the distributional\nrepresentations of data clouds.",
      "tldr_zh": "本论文探讨了基于分布的实例分类问题，使用Wasserstein metric度量分布间的距离，并将其应用于k-nearest neighbors、k-means和pseudo-mixture modeling等算法。作者提出了一种新颖的降维方法，通过最大化Fisher's ratio（类间变异与类内变异的商）来确定discriminant coordinates或canonical variates axes，其中类间和类内变异定义为成对实例的平均平方距离。优化过程采用一个交替算法，结合optimal transport和maximization步骤；实验结果显示，该方法提升了分类准确率，优于基于向量表示的传统算法，并对数据分布变化表现出鲁棒性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "double space 37 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15768v1",
      "published_date": "2024-05-24 17:59:21 UTC",
      "updated_date": "2024-05-24 17:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:28:04.235696"
    },
    {
      "arxiv_id": "2405.15766v2",
      "title": "Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development",
      "title_zh": "翻译失败",
      "authors": [
        "Pranab Sahoo",
        "Ayush Kumar Singh",
        "Sriparna Saha",
        "Aman Chadha",
        "Samrat Mondal"
      ],
      "abstract": "The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance,\nenhancing patient safety by identifying potential risks associated with\nmedications, facilitating early detection of adverse events, and guiding\nregulatory decision-making. Traditional ADE detection methods are reliable but\nslow, not easily adaptable to large-scale operations, and offer limited\ninformation. With the exponential increase in data sources like social media\ncontent, biomedical literature, and Electronic Medical Records (EMR),\nextracting relevant ADE-related information from these unstructured texts is\nimperative. Previous ADE mining studies have focused on text-based\nmethodologies, overlooking visual cues, limiting contextual comprehension, and\nhindering accurate interpretation. To address this gap, we present a MultiModal\nAdverse Drug Event (MMADE) detection dataset, merging ADE-related textual\ninformation with visual aids. Additionally, we introduce a framework that\nleverages the capabilities of LLMs and VLMs for ADE detection by generating\ndetailed descriptions of medical images depicting ADEs, aiding healthcare\nprofessionals in visually identifying adverse events. Using our MMADE dataset,\nwe showcase the significance of integrating visual cues from images to enhance\noverall performance. This approach holds promise for patient safety, ADE\nawareness, and healthcare accessibility, paving the way for further exploration\nin personalized healthcare.",
      "tldr_zh": "本文针对药物不良事件(ADE)检测的传统方法缓慢且忽略视觉线索的问题，创建了多模态数据集(MMADE)，将ADE相关的文本信息与视觉辅助相结合。研究团队开发了一个框架，利用大型语言模型(LLMs)和视觉语言模型(VLMs)生成医疗图像的详细描述，帮助医疗专业人士更准确地识别不良事件。实验结果显示，整合视觉线索显著提升了ADE检测的整体性能。最终，该方法有望提高患者安全、增强ADE意识，并推动个性化医疗的发展。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "ACL Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15766v2",
      "published_date": "2024-05-24 17:58:42 UTC",
      "updated_date": "2024-05-27 02:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:28:17.561261"
    },
    {
      "arxiv_id": "2405.15758v1",
      "title": "InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchi Wang",
        "Junliang Guo",
        "Jianhong Bai",
        "Runyi Yu",
        "Tianyu He",
        "Xu Tan",
        "Xu Sun",
        "Jiang Bian"
      ],
      "abstract": "Recent talking avatar generation models have made strides in achieving\nrealistic and accurate lip synchronization with the audio, but often fall short\nin controlling and conveying detailed expressions and emotions of the avatar,\nmaking the generated video less vivid and controllable. In this paper, we\npropose a novel text-guided approach for generating emotionally expressive 2D\navatars, offering fine-grained control, improved interactivity, and\ngeneralizability to the resulting video. Our framework, named InstructAvatar,\nleverages a natural language interface to control the emotion as well as the\nfacial motion of avatars. Technically, we design an automatic annotation\npipeline to construct an instruction-video paired training dataset, equipped\nwith a novel two-branch diffusion-based generator to predict avatars with audio\nand text instructions at the same time. Experimental results demonstrate that\nInstructAvatar produces results that align well with both conditions, and\noutperforms existing methods in fine-grained emotion control, lip-sync quality,\nand naturalness. Our project page is\nhttps://wangyuchi369.github.io/InstructAvatar/.",
      "tldr_zh": "本文提出InstructAvatar，一种文本指导的框架，用于生成情感丰富的2D avatars，实现对表情和动作的精细控制，从而提升视频的生动性和交互性。该框架通过一个自动标注管道构建指令-视频配对的训练数据集，并采用两分支diffusion-based generator同时处理音频和文本指令。相比现有方法，InstructAvatar在精细情感控制、唇同步质量和自然性方面表现出色，实验结果显示其生成的视频与指令高度一致。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://wangyuchi369.github.io/InstructAvatar/",
      "pdf_url": "http://arxiv.org/pdf/2405.15758v1",
      "published_date": "2024-05-24 17:53:54 UTC",
      "updated_date": "2024-05-24 17:53:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:28:28.349085"
    },
    {
      "arxiv_id": "2405.15756v4",
      "title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity",
      "title_zh": "Wasserstein 距离、神经元纠缠和稀疏性",
      "authors": [
        "Shashata Sawmya",
        "Linghao Kong",
        "Ilia Markov",
        "Dan Alistarh",
        "Nir Shavit"
      ],
      "abstract": "Disentangling polysemantic neurons is at the core of many current approaches\nto interpretability of large language models. Here we attempt to study how\ndisentanglement can be used to understand performance, particularly under\nweight sparsity, a leading post-training optimization technique. We suggest a\nnovel measure for estimating neuronal entanglement: the Wasserstein distance of\na neuron's output distribution to a Gaussian. Moreover, we show the existence\nof a small number of highly entangled \"Wasserstein Neurons\" in each linear\nlayer of an LLM, characterized by their highly non-Gaussian output\ndistributions, their role in mapping similar inputs to dissimilar outputs, and\ntheir significant impact on model accuracy. To study these phenomena, we\npropose a new experimental framework for disentangling polysemantic neurons.\nOur framework separates each layer's inputs to create a mixture of experts\nwhere each neuron's output is computed by a mixture of neurons of lower\nWasserstein distance, each better at maintaining accuracy when sparsified\nwithout retraining. We provide strong evidence that this is because the mixture\nof sparse experts is effectively disentangling the input-output relationship of\nindividual neurons, in particular the difficult Wasserstein neurons.",
      "tldr_zh": "本论文探讨了Wasserstein Distances在评估神经元纠缠（neuronal entanglement）及其与模型性能关系中的作用，特别是针对大语言模型（LLMs）在权重稀疏化（sparsity）下的表现。研究者提出一种新度量方法，使用Wasserstein距离测量神经元输出分布与高斯分布的差异，并发现每个线性层中存在少量高度纠缠的\"Wasserstein Neurons\"，这些神经元导致类似输入映射到不同输出，并显著影响模型准确性。为解决这一问题，论文引入了一个实验框架，通过分离层输入创建mixture of experts系统，使每个神经元的输出由低Wasserstein距离的专家混合计算，从而在不重新训练的情况下提升稀疏化后的准确性。整体结果表明，这种解耦方法有效地改善了神经元的输入-输出关系，尤其针对困难的Wasserstein Neurons。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15756v4",
      "published_date": "2024-05-24 17:51:39 UTC",
      "updated_date": "2025-02-26 17:32:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:28:41.437791"
    },
    {
      "arxiv_id": "2405.15750v2",
      "title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
      "title_zh": "翻译失败",
      "authors": [
        "Abhinav Patil",
        "Jaap Jumelet",
        "Yu Ying Chiu",
        "Andy Lapastora",
        "Peter Shen",
        "Lexie Wang",
        "Clevis Willrich",
        "Shane Steinert-Threlkeld"
      ],
      "abstract": "This paper introduces Filtered Corpus Training, a method that trains language\nmodels (LMs) on corpora with certain linguistic constructions filtered out from\nthe training data, and uses it to measure the ability of LMs to perform\nlinguistic generalization on the basis of indirect evidence. We apply the\nmethod to both LSTM and Transformer LMs (of roughly comparable size),\ndeveloping filtered corpora that target a wide range of linguistic phenomena.\nOur results show that while transformers are better qua LMs (as measured by\nperplexity), both models perform equally and surprisingly well on linguistic\ngeneralization measures, suggesting that they are capable of generalizing from\nindirect evidence.",
      "tldr_zh": "本论文引入了 Filtered Corpus Training (FiCT) 方法，该方法通过从训练语料库中过滤特定语言结构，来评估语言模型 (LMs) 基于间接证据进行语言泛化的能力。\n研究者将 FiCT 应用于 LSTM 和 Transformer LMs，针对多种语言现象进行测试。\n结果显示，虽然 Transformer 在困惑度方面表现优于 LSTM，但两类模型在语言泛化任务上表现相当且出人意料地出色，表明 LMs 能够有效从间接证据中进行泛化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Forthcoming in Transactions of the Association for Computational\n  Linguistics (TACL). This is a pre-MIT Press publication version. For code and\n  trained models, see http://github.com/CLMBRs/corpus-filtering",
      "pdf_url": "http://arxiv.org/pdf/2405.15750v2",
      "published_date": "2024-05-24 17:47:20 UTC",
      "updated_date": "2024-08-06 22:29:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:28:53.342605"
    },
    {
      "arxiv_id": "2405.15739v3",
      "title": "Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias",
      "title_zh": "翻译失败",
      "authors": [
        "Andres Algaba",
        "Carmen Mazijn",
        "Vincent Holst",
        "Floriano Tori",
        "Sylvia Wenmackers",
        "Vincent Ginis"
      ],
      "abstract": "Citation practices are crucial in shaping the structure of scientific\nknowledge, yet they are often influenced by contemporary norms and biases. The\nemergence of Large Language Models (LLMs) introduces a new dynamic to these\npractices. Interestingly, the characteristics and potential biases of\nreferences recommended by LLMs that entirely rely on their parametric\nknowledge, and not on search or retrieval-augmented generation, remain\nunexplored. Here, we analyze these characteristics in an experiment using a\ndataset from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge\ncut-off date. In our experiment, LLMs are tasked with suggesting scholarly\nreferences for the anonymized in-text citations within these papers. Our\nfindings reveal a remarkable similarity between human and LLM citation\npatterns, but with a more pronounced high citation bias, which persists even\nafter controlling for publication year, title length, number of authors, and\nvenue. The results hold for both GPT-4, and the more capable models GPT-4o and\nClaude 3.5 where the papers are part of the training data. Additionally, we\nobserve a large consistency between the characteristics of LLM's existing and\nnon-existent generated references, indicating the model's internalization of\ncitation patterns. By analyzing citation graphs, we show that the references\nrecommended are embedded in the relevant citation context, suggesting an even\ndeeper conceptual internalization of the citation networks. While LLMs can aid\nin citation generation, they may also amplify existing biases, such as the\nMatthew effect, and introduce new ones, potentially skewing scientific\nknowledge dissemination.",
      "tldr_zh": "这篇论文研究了大型语言模型 (LLMs) 在引用推荐中的模式，发现它们模仿人类引用习惯，但表现出更明显的引用偏见 (citation bias)。研究采用实验方法，使用 AAAI、NeurIPS、ICML 和 ICLR 的数据集，让 LLMs 基于其参数知识为匿名引用建议参考，并控制变量如出版年份、标题长度和作者数量。结果显示，GPT-4、GPT-4o 和 Claude 3.5 模型均存在高引用偏见，且生成的引用特性高度一致，表明模型已内化引用网络。作者警告，这种偏见可能放大马太效应 (Matthew effect)，并引入新问题，影响科学知识的传播和公平性。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.DL",
      "comment": "30 pages, 13 figures, 4 tables. Added GPT-4o and Claude 3.5 results",
      "pdf_url": "http://arxiv.org/pdf/2405.15739v3",
      "published_date": "2024-05-24 17:34:32 UTC",
      "updated_date": "2024-08-24 12:04:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:29:05.935062"
    },
    {
      "arxiv_id": "2405.15843v1",
      "title": "SpotNet: An Image Centric, Lidar Anchored Approach To Long Range Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Louis Foucard",
        "Samar Khanna",
        "Yi Shi",
        "Chi-Kuei Liu",
        "Quinn Z Shen",
        "Thuyen Ngo",
        "Zi-Xiang Xia"
      ],
      "abstract": "In this paper, we propose SpotNet: a fast, single stage, image-centric but\nLiDAR anchored approach for long range 3D object detection. We demonstrate that\nour approach to LiDAR/image sensor fusion, combined with the joint learning of\n2D and 3D detection tasks, can lead to accurate 3D object detection with very\nsparse LiDAR support. Unlike more recent bird's-eye-view (BEV) sensor-fusion\nmethods which scale with range $r$ as $O(r^2)$, SpotNet scales as $O(1)$ with\nrange. We argue that such an architecture is ideally suited to leverage each\nsensor's strength, i.e. semantic understanding from images and accurate range\nfinding from LiDAR data. Finally we show that anchoring detections on LiDAR\npoints removes the need to regress distances, and so the architecture is able\nto transfer from 2MP to 8MP resolution images without re-training.",
      "tldr_zh": "本论文提出SpotNet，一种快速的单阶段方法，用于长距离3D物体检测，该方法以图像为中心但锚定于LiDAR，通过LiDAR和图像传感器融合以及2D和3D检测任务的联合学习，在稀疏LiDAR支持下实现高准确性。SpotNet的计算复杂度为O(1)，与传统bird's-eye-view (BEV)方法相比不受范围影响，从而充分利用图像的语义理解和LiDAR的精确测距优势。该框架通过在LiDAR点上锚定检测，消除了距离回归的需要，能在不重新训练的情况下从2MP分辨率图像转移到8MP分辨率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15843v1",
      "published_date": "2024-05-24 17:25:48 UTC",
      "updated_date": "2024-05-24 17:25:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:29:17.806835"
    },
    {
      "arxiv_id": "2405.15731v3",
      "title": "Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Jerome Sieber",
        "Carmen Amo Alonso",
        "Alexandre Didier",
        "Melanie N. Zeilinger",
        "Antonio Orvieto"
      ],
      "abstract": "Softmax attention is the principle backbone of foundation models for various\nartificial intelligence applications, yet its quadratic complexity in sequence\nlength can limit its inference throughput in long-context settings. To address\nthis challenge, alternative architectures such as linear attention, State Space\nModels (SSMs), and Recurrent Neural Networks (RNNs) have been considered as\nmore efficient alternatives. While connections between these approaches exist,\nsuch models are commonly developed in isolation and there is a lack of\ntheoretical understanding of the shared principles underpinning these\narchitectures and their subtle differences, greatly influencing performance and\nscalability. In this paper, we introduce the Dynamical Systems Framework (DSF),\nwhich allows a principled investigation of all these architectures in a common\nrepresentation. Our framework facilitates rigorous comparisons, providing new\ninsights on the distinctive characteristics of each model class. For instance,\nwe compare linear attention and selective SSMs, detailing their differences and\nconditions under which both are equivalent. We also provide principled\ncomparisons between softmax attention and other model classes, discussing the\ntheoretical conditions under which softmax attention can be approximated.\nAdditionally, we substantiate these new insights with empirical validations and\nmathematical arguments. This shows the DSF's potential to guide the systematic\ndevelopment of future more efficient and scalable foundation models.",
      "tldr_zh": "这篇论文探讨了基础模型（Foundation Models）中不同架构的差异，包括Softmax attention、State Space Models (SSMs) 和 Recurrent Neural Networks (RNNs)。作者引入了Dynamical Systems Framework (DSF)，一个统一的框架，用于系统地比较这些模型的原理、性能和可扩展性，例如比较线性attention与selective SSMs的等价条件，以及Softmax attention的近似理论条件。实验和数学论证验证了这些见解，证明DSF有助于指导开发更高效、可扩展的基础模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15731v3",
      "published_date": "2024-05-24 17:19:57 UTC",
      "updated_date": "2024-12-08 05:25:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:29:32.327755"
    },
    {
      "arxiv_id": "2405.19363v2",
      "title": "Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Yihe Wang",
        "Nan Huang",
        "Taida Li",
        "Yujun Yan",
        "Xiang Zhang"
      ],
      "abstract": "Medical time series (MedTS) data, such as Electroencephalography (EEG) and\nElectrocardiography (ECG), play a crucial role in healthcare, such as\ndiagnosing brain and heart diseases. Existing methods for MedTS classification\nprimarily rely on handcrafted biomarkers extraction and CNN-based models, with\nlimited exploration of transformer-based models. In this paper, we introduce\nMedformer, a multi-granularity patching transformer tailored specifically for\nMedTS classification. Our method incorporates three novel mechanisms to\nleverage the unique characteristics of MedTS: cross-channel patching to\nleverage inter-channel correlations, multi-granularity embedding for capturing\nfeatures at different scales, and two-stage (intra- and inter-granularity)\nmulti-granularity self-attention for learning features and correlations within\nand among granularities. We conduct extensive experiments on five public\ndatasets under both subject-dependent and challenging subject-independent\nsetups. Results demonstrate Medformer's superiority over 10 baselines,\nachieving top averaged ranking across five datasets on all six evaluation\nmetrics. These findings underscore the significant impact of our method on\nhealthcare applications, such as diagnosing Myocardial Infarction, Alzheimer's,\nand Parkinson's disease. We release the source code at\nhttps://github.com/DL4mHealth/Medformer.",
      "tldr_zh": "该研究引入了 Medformer，一种专为医疗时间序列 (MedTS) 分类设计的多粒度 patching Transformer，针对 EEG 和 ECG 等数据，解决了现有方法依赖手工提取生物标记和 CNN 模型的局限性。Medformer 整合了三个创新机制：cross-channel patching 以利用通道间相关性、multi-granularity embedding 以捕捉不同尺度的特征，以及两阶段（intra- 和 inter-granularity）multi-granularity self-attention 以学习特征和相关性。在五个公共数据集上的实验中，Medformer 在 subject-dependent 和 subject-independent 设置下优于 10 个基线模型，在所有六种评估指标上获得最高平均排名。这些结果突显了 Medformer 在诊断心肌梗塞、阿尔茨海默病和帕金森病等医疗应用中的重要潜力，并已开源代码。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "21 pages (15 pages main paper + 6 pages supplementary materials)",
      "pdf_url": "http://arxiv.org/pdf/2405.19363v2",
      "published_date": "2024-05-24 16:51:10 UTC",
      "updated_date": "2024-10-19 06:47:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:29:41.857846"
    },
    {
      "arxiv_id": "2406.00029v1",
      "title": "Clustered Retrieved Augmented Generation (CRAG)",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Akesson",
        "Frances A. Santos"
      ],
      "abstract": "Providing external knowledge to Large Language Models (LLMs) is a key point\nfor using these models in real-world applications for several reasons, such as\nincorporating up-to-date content in a real-time manner, providing access to\ndomain-specific knowledge, and contributing to hallucination prevention. The\nvector database-based Retrieval Augmented Generation (RAG) approach has been\nwidely adopted to this end. Thus, any part of external knowledge can be\nretrieved and provided to some LLM as the input context. Despite RAG approach's\nsuccess, it still might be unfeasible for some applications, because the\ncontext retrieved can demand a longer context window than the size supported by\nLLM. Even when the context retrieved fits into the context window size, the\nnumber of tokens might be expressive and, consequently, impact costs and\nprocessing time, becoming impractical for most applications. To address these,\nwe propose CRAG, a novel approach able to effectively reduce the number of\nprompting tokens without degrading the quality of the response generated\ncompared to a solution using RAG. Through our experiments, we show that CRAG\ncan reduce the number of tokens by at least 46\\%, achieving more than 90\\% in\nsome cases, compared to RAG. Moreover, the number of tokens with CRAG does not\nincrease considerably when the number of reviews analyzed is higher, unlike\nRAG, where the number of tokens is almost 9x higher when there are 75 reviews\ncompared to 4 reviews.",
      "tldr_zh": "该论文提出 Clustered Retrieved Augmented Generation (CRAG)，一种改进 Retrieval Augmented Generation (RAG) 的方法，用于为 Large Language Models (LLMs) 提供外部知识，以解决 RAG 在上下文窗口大小、处理成本和时间方面的局限性。CRAG 通过聚类技术有效减少提示 token 数量，同时不降低响应质量。实验结果显示，CRAG 至少可减少 46% 的 token 数量，在某些情况下超过 90%，并且在处理更多评论时，其 token 消耗远低于 RAG（如处理 75 条评论时，RAG 的 token 数量几乎是处理 4 条评论的 9 倍）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00029v1",
      "published_date": "2024-05-24 16:36:47 UTC",
      "updated_date": "2024-05-24 16:36:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:30:03.565963"
    },
    {
      "arxiv_id": "2405.15684v1",
      "title": "Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Zhang",
        "Hehe Fan",
        "Yi Yang"
      ],
      "abstract": "To bridge the gap between vision and language modalities, Multimodal Large\nLanguage Models (MLLMs) usually learn an adapter that converts visual inputs to\nunderstandable tokens for Large Language Models (LLMs). However, most adapters\ngenerate consistent visual tokens, regardless of the specific objects of\ninterest mentioned in the prompt. Since these adapters distribute equal\nattention to every detail in the image and focus on the entire scene, they may\nincrease the cognitive load for LLMs, particularly when processing complex\nscenes. To alleviate this problem, we propose prompt-aware adapters. These\nadapters are designed with the capability to dynamically embed visual inputs\nbased on the specific focus of the prompt. Specifically, prompt-aware adapters\nutilize both global and local textual features to capture the most relevant\nvisual clues from the prompt at both coarse and fine granularity levels. This\napproach significantly enhances the ability of LLMs to understand and interpret\nvisual content. Experiments on various visual question answering tasks, such as\ncounting and position reasoning, demonstrate the effectiveness of prompt-aware\nadapters.",
      "tldr_zh": "该论文针对多模态大语言模型(MLLMs)中视觉适配器的问题，提出了一种prompt-aware adapter，以动态调整视觉标记生成，使其根据提示中指定的对象关注相关细节，避免对整个图像均匀分配注意力。prompt-aware adapter 通过利用全局和局部文本特征，在粗粒度和细粒度级别捕获最相关的视觉线索，从而减轻Large Language Models (LLMs)的认知负担。实验结果显示，该方法在视觉问答任务（如计数和位置推理）上显著提升了模型的理解和解释能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15684v1",
      "published_date": "2024-05-24 16:24:10 UTC",
      "updated_date": "2024-05-24 16:24:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:30:06.022161"
    },
    {
      "arxiv_id": "2405.15683v3",
      "title": "Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs",
      "title_zh": "视觉描述接地减少幻觉并提升 LVLMs 中的推理能力",
      "authors": [
        "Sreyan Ghosh",
        "Chandra Kiran Reddy Evuru",
        "Sonal Kumar",
        "Utkarsh Tyagi",
        "Oriol Nieto",
        "Zeyu Jin",
        "Dinesh Manocha"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) often produce responses that misalign\nwith factual information, a phenomenon known as hallucinations. While\nhallucinations are well-studied, the exact causes behind them remain\nunderexplored. In this paper, we first investigate the root causes of\nhallucinations in LVLMs. Our findings reveal that existing mitigation\ntechniques primarily reduce hallucinations for visual recognition prompts-those\nthat require simple descriptions of visual elements-but fail for cognitive\nprompts that demand deliberate reasoning. We identify the core issue as a lack\nof true visual perception in LVLMs: although they can accurately recognize\nvisual elements, they struggle to fully interpret these elements in the context\nof the input prompt and effectively link this recognition to their internal\nknowledge, which is critical for reasoning. To address this gap, we introduce\nVisual Description Grounded Decoding (VDGD), a simple, robust, and\ntraining-free method designed to enhance visual perception and improve\nreasoning capabilities in LVLMs. VDGD works by first generating a detailed\ndescription of the image and appending it as a prefix to the instruction.\nDuring response generation, tokens are sampled based on their KL divergence to\nthe description, favoring candidates with lower divergence. Experimental\nresults on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD\nconsistently outperforms existing baselines 2% - 33%. Finally, we introduce\nVaLLu, a benchmark designed for comprehensive evaluation of the cognitive\ncapabilities of LVLMs.",
      "tldr_zh": "本研究调查了大型视觉语言模型（LVLMs）中的幻觉（hallucinations）问题，发现其根因在于模型缺乏真正的视觉感知，无法有效将视觉元素与内部知识链接，尤其在需要推理的认知提示上。论文提出 Visual Description Grounded Decoding (VDGD)，一个简单、无需训练的方法：先生成图像的详细描述作为指令前缀，然后通过计算与描述的 KL divergence 来采样响应标记，从而提升视觉感知和推理能力。实验结果显示，VDGD 在多个视觉推理基准上比现有基线提高了 2% - 33%。此外，论文还引入了 VaLLu 基准，用于全面评估 LVLMs 的认知能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025. Project: https://sreyan88.github.io/VDGD/",
      "pdf_url": "http://arxiv.org/pdf/2405.15683v3",
      "published_date": "2024-05-24 16:21:59 UTC",
      "updated_date": "2025-03-06 03:59:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:30:21.816940"
    },
    {
      "arxiv_id": "2405.15682v4",
      "title": "The Road Less Scheduled",
      "title_zh": "翻译失败",
      "authors": [
        "Aaron Defazio",
        "Xingyu Alice Yang",
        "Harsh Mehta",
        "Konstantin Mishchenko",
        "Ahmed Khaled",
        "Ashok Cutkosky"
      ],
      "abstract": "Existing learning rate schedules that do not require specification of the\noptimization stopping step T are greatly out-performed by learning rate\nschedules that depend on T. We propose an approach that avoids the need for\nthis stopping time by eschewing the use of schedules entirely, while exhibiting\nstate-of-the-art performance compared to schedules across a wide family of\nproblems ranging from convex problems to large-scale deep learning problems.\nOur Schedule-Free approach introduces no additional hyper-parameters over\nstandard optimizers with momentum. Our method is a direct consequence of a new\ntheory we develop that unifies scheduling and iterate averaging. An open source\nimplementation of our method is available at\nhttps://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the\ncore algorithm behind our winning entry to the MLCommons 2024 AlgoPerf\nAlgorithmic Efficiency Challenge Self-Tuning track.",
      "tldr_zh": "本文研究发现，现有的学习率 schedules 需要指定优化停止步数 T，这导致其性能远逊于依赖 T 的方法。为解决这一问题，作者提出 Schedule-Free approach，一种无需 schedules 的优化策略，在从凸问题到大规模深度学习任务的广泛场景中，展现出 state-of-the-art 性能，且不引入额外超参数。该方法基于一个新理论，将 scheduling 和 iterate averaging 统一起来，并通过开源实现（https://github.com/facebookresearch/schedule_free）证明其有效性，最终成为 MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning 赛道的获胜算法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15682v4",
      "published_date": "2024-05-24 16:20:46 UTC",
      "updated_date": "2024-10-29 22:40:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:30:33.775969"
    },
    {
      "arxiv_id": "2405.15673v2",
      "title": "Consistency of Neural Causal Partial Identification",
      "title_zh": "神经因果部分识别的一致性",
      "authors": [
        "Jiyuan Tan",
        "Jose Blanchet",
        "Vasilis Syrgkanis"
      ],
      "abstract": "Recent progress in Neural Causal Models (NCMs) showcased how identification\nand partial identification of causal effects can be automatically carried out\nvia training of neural generative models that respect the constraints encoded\nin a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However,\nformal consistency of these methods has only been proven for the case of\ndiscrete variables or only for linear causal models. In this work, we prove the\nconsistency of partial identification via NCMs in a general setting with both\ncontinuous and categorical variables. Further, our results highlight the impact\nof the design of the underlying neural network architecture in terms of depth\nand connectivity as well as the importance of applying Lipschitz regularization\nin the training phase. In particular, we provide a counterexample showing that\nwithout Lipschitz regularization this method may not be asymptotically\nconsistent. Our results are enabled by new results on the approximability of\nStructural Causal Models (SCMs) via neural generative models, together with an\nanalysis of the sample complexity of the resulting architectures and how that\ntranslates into an error in the constrained optimization problem that defines\nthe partial identification bounds.",
      "tldr_zh": "本研究证明了 Neural Causal Models (NCMs) 在一般设置下进行因果效应的部分识别时的一致性，包括连续和分类变量，而非仅限于之前的离散变量或线性模型场景。作者通过训练尊重因果图约束的神经生成模型，并强调神经网络架构（如深度和连通性）的设计以及 Lipschitz regularization 的应用，来确保方法的渐进一致性；他们还提供了一个反例，表明缺少 Lipschitz regularization 可能导致不一致。最终，这些结果基于对 Structural Causal Models (SCMs) 的神经近似分析，以及架构的样本复杂度和约束优化问题的错误评估，为 NCMs 的可靠应用奠定了理论基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "61 pages, 8 figures, accepted by Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15673v2",
      "published_date": "2024-05-24 16:12:39 UTC",
      "updated_date": "2024-11-08 04:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:30:43.098899"
    },
    {
      "arxiv_id": "2405.15661v2",
      "title": "Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables",
      "title_zh": "翻译失败",
      "authors": [
        "James Hinns",
        "David Martens"
      ],
      "abstract": "The rise of deep learning in image classification has brought unprecedented\naccuracy but also highlighted a key issue: the use of 'shortcuts' by models.\nSuch shortcuts are easy-to-learn patterns from the training data that fail to\ngeneralise to new data. Examples include the use of a copyright watermark to\nrecognise horses, snowy background to recognise huskies, or ink markings to\ndetect malignant skin lesions. The explainable AI (XAI) community has suggested\nusing instance-level explanations to detect shortcuts without external data,\nbut this requires the examination of many explanations to confirm the presence\nof such shortcuts, making it a labour-intensive process. To address these\nchallenges, we introduce Counterfactual Frequency (CoF) tables, a novel\napproach that aggregates instance-based explanations into global insights, and\nexposes shortcuts. The aggregation implies the need for some semantic concepts\nto be used in the explanations, which we solve by labelling the segments of an\nimage. We demonstrate the utility of CoF tables across several datasets,\nrevealing the shortcuts learned from them.",
      "tldr_zh": "该研究揭示了图像分类模型在使用“shortcuts”（捷径）时存在的泛化问题，这些捷径是模型从训练数据中学习到的简单模式，如版权水印识别马匹或雪景背景识别哈士奇。作者引入了Counterfactual Frequency (CoF) tables，一种新颖方法，将实例级解释聚合为全局insights，从而简化shortcuts的检测过程，并通过标记图像段来处理所需的语义概念。实验在多个数据集上验证了CoF tables的有效性，成功暴露了模型学到的shortcuts，为Explainable AI (XAI)提供了更高效的工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15661v2",
      "published_date": "2024-05-24 15:58:02 UTC",
      "updated_date": "2025-01-29 11:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:30:55.345661"
    },
    {
      "arxiv_id": "2405.15658v2",
      "title": "CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoyan Luo",
        "Yinghao Wu",
        "Tianheng Cheng",
        "Yong Liu",
        "Yicheng Xiao",
        "Hongfa Wang",
        "Xiao-Ping Zhang",
        "Yujiu Yang"
      ],
      "abstract": "The newly proposed Generalized Referring Expression Segmentation (GRES)\namplifies the formulation of classic RES by involving complex\nmultiple/non-target scenarios. Recent approaches address GRES by directly\nextending the well-adopted RES frameworks with object-existence identification.\nHowever, these approaches tend to encode multi-granularity object information\ninto a single representation, which makes it difficult to precisely represent\ncomprehensive objects of different granularity. Moreover, the simple binary\nobject-existence identification across all referent scenarios fails to specify\ntheir inherent differences, incurring ambiguity in object understanding. To\ntackle the above issues, we propose a \\textbf{Co}unting-Aware\n\\textbf{H}ierarchical \\textbf{D}ecoding framework (CoHD) for GRES. By\ndecoupling the intricate referring semantics into different granularity with a\nvisual-linguistic hierarchy, and dynamic aggregating it with intra- and\ninter-selection, CoHD boosts multi-granularity comprehension with the\nreciprocal benefit of the hierarchical nature. Furthermore, we incorporate the\ncounting ability by embodying multiple/single/non-target scenarios into count-\nand category-level supervision, facilitating comprehensive object perception.\nExperimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarks\ndemonstrate the effectiveness and rationality of CoHD which outperforms\nstate-of-the-art GRES methods by a remarkable margin. Code is available at\n\\href{https://github.com/RobertLuo1/CoHD}{here}.",
      "tldr_zh": "这篇论文针对 Generalized Referring Expression Segmentation (GRES) 的挑战，提出了一种计数感知的层次解码框架 CoHD，以处理多粒度对象信息编码和对象存在识别的歧义问题。CoHD 通过视觉-语言层次结构解耦复杂的引用语义，并动态聚合 intra- 和 inter-selection 信息，同时融入计数和类别级监督来提升对多个/单个/非目标场景的理解。实验结果显示，CoHD 在 gRefCOCO、Ref-ZOM、R-RefCOCO 和 RefCOCO 等基准上显著优于最先进方法，证明了其有效性和合理性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15658v2",
      "published_date": "2024-05-24 15:53:59 UTC",
      "updated_date": "2024-11-25 17:14:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:31:06.420197"
    },
    {
      "arxiv_id": "2405.15642v1",
      "title": "Effective Confidence Region Prediction Using Probability Forecasters",
      "title_zh": "翻译失败",
      "authors": [
        "David Lindsay",
        "Sian Lindsay"
      ],
      "abstract": "Confidence region prediction is a practically useful extension to the\ncommonly studied pattern recognition problem. Instead of predicting a single\nlabel, the constraint is relaxed to allow prediction of a subset of labels\ngiven a desired confidence level 1-delta. Ideally, effective region predictions\nshould be (1) well calibrated - predictive regions at confidence level 1-delta\nshould err with relative frequency at most delta and (2) be as narrow (or\ncertain) as possible. We present a simple technique to generate confidence\nregion predictions from conditional probability estimates (probability\nforecasts). We use this 'conversion' technique to generate confidence region\npredictions from probability forecasts output by standard machine learning\nalgorithms when tested on 15 multi-class datasets. Our results show that\napproximately 44% of experiments demonstrate well-calibrated confidence region\npredictions, with the K-Nearest Neighbour algorithm tending to perform\nconsistently well across all data. Our results illustrate the practical\nbenefits of effective confidence region prediction with respect to medical\ndiagnostics, where guarantees of capturing the true disease label can be given.",
      "tldr_zh": "该论文探讨了置信区域预测（Confidence Region Prediction），这是一种扩展的模式识别方法，允许预测标签子集而非单一标签，以满足给定置信水平 1-delta 的要求。研究提出了一种简单技术，通过条件概率估计（Probability Forecasters）生成置信区域预测，确保预测区域校准良好（错误频率不超过 delta）和尽可能窄。在 15 个多类数据集上的实验显示，约 44% 的测试结果表现出良好校准，其中 K-Nearest Neighbour 算法表现稳定，并突出了该方法在医疗诊断中的实际价值，如提供捕获真实疾病标签的保证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, originally posted in 2005",
      "pdf_url": "http://arxiv.org/pdf/2405.15642v1",
      "published_date": "2024-05-24 15:33:08 UTC",
      "updated_date": "2024-05-24 15:33:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:31:18.430503"
    },
    {
      "arxiv_id": "2405.15640v1",
      "title": "GECKO: Generative Language Model for English, Code and Korean",
      "title_zh": "GECKO：用于英语、代码和韩语的生成式语言模型",
      "authors": [
        "Sungwoo Oh",
        "Donggyu Kim"
      ],
      "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for\nKorean and English, along with programming languages. GECKO is pretrained on\nthe balanced, high-quality corpus of Korean and English employing LLaMA\narchitecture. In this report, we share the experiences of several efforts to\nbuild a better data pipeline for the corpus and to train our model. GECKO shows\ngreat efficiency in token generations for both Korean and English, despite its\nsmall size of vocabulary. We measure the performance on the representative\nbenchmarks in terms of Korean, English and Code, and it exhibits great\nperformance on KMMLU (Korean MMLU) and modest performance in English and Code,\neven with its smaller number of trained tokens compared to English-focused\nLLMs. GECKO is available to the open-source community under a permissive\nlicense. We hope our work offers a research baseline and practical insights for\nKorean LLM research. The model can be found at:\nhttps://huggingface.co/kifai/GECKO-7B",
      "tldr_zh": "我们介绍了 GECKO，一种针对英语、代码和韩语的双语大型语言模型 (LLM)，它基于 LLaMA 架构在平衡的高质量语料上预训练，并通过优化数据管道提升了训练效率。GECKO 在韩语基准测试如 KMMLU 上表现出色，在英语和代码任务上表现适中，同时实现了高效的 token 生成，尽管其词汇量较小。相比专注于英语的 LLM，即使训练 token 数量更少，GECKO 仍提供了可比拟的性能，并以 permissive license 开源，为韩语 LLM 研究提供了研究基准和实用见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15640v1",
      "published_date": "2024-05-24 15:30:41 UTC",
      "updated_date": "2024-05-24 15:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:31:31.000911"
    },
    {
      "arxiv_id": "2406.15430v1",
      "title": "Automated Parking Planning with Vision-Based BEV Approach",
      "title_zh": "基于视觉的 BEV 方法的自动停车规划",
      "authors": [
        "Yuxuan Zhao"
      ],
      "abstract": "Automated Valet Parking (AVP) is a crucial component of advanced autonomous\ndriving systems, focusing on the endpoint task within the \"human-vehicle\ninteraction\" process to tackle the challenges of the \"last mile\".The perception\nmodule of the automated parking algorithm has evolved from local perception\nusing ultrasonic radar and global scenario precise map matching for\nlocalization to a high-level map-free Birds Eye View (BEV) perception\nsolution.The BEV scene places higher demands on the real-time performance and\nsafety of automated parking planning tasks. This paper proposes an improved\nautomated parking algorithm based on the A* algorithm, integrating vehicle\nkinematic models, heuristic function optimization, bidirectional search, and\nBezier curve optimization to enhance the computational speed and real-time\ncapabilities of the planning algorithm.Numerical optimization methods are\nemployed to generate the final parking trajectory, ensuring the safety of the\nparking path. The proposed approach is experimentally validated in the commonly\nused industrial CARLA-ROS joint simulation environment. Compared to traditional\nalgorithms, this approach demonstrates reduced computation time with more\nchallenging collision-risk test cases and improved performance in comfort\nmetrics.",
      "tldr_zh": "该论文针对 Automated Valet Parking (AVP) 在自动驾驶系统中的“最后一英里”挑战，提出了一种基于 Birds Eye View (BEV) 的视觉感知方法，以提升停车规划的实时性和安全性。研究改进 A* 算法，整合车辆运动学模型、启发式函数优化、双向搜索和 Bezier 曲线优化，生成安全高效的停车轨迹，并通过数值优化确保路径可靠性。在 CARLA-ROS 模拟环境中实验验证，该方法相较传统算法显著减少计算时间，提升碰撞风险处理能力和舒适性指标。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.15430v1",
      "published_date": "2024-05-24 15:26:09 UTC",
      "updated_date": "2024-05-24 15:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:31:42.616184"
    },
    {
      "arxiv_id": "2406.15429v1",
      "title": "Automatic parking planning control method based on improved A* algorithm",
      "title_zh": "基于改进的 A* 算法的自动泊车规划控制方法",
      "authors": [
        "Yuxuan Zhao"
      ],
      "abstract": "As the trend of moving away from high-precision maps gradually emerges in the\nautonomous driving industry,traditional planning algorithms are gradually\nexposing some problems. To address the high real-time, high precision, and high\ntrajectory quality requirements posed by the automatic parking task under\nreal-time perceived local maps,this paper proposes an improved automatic\nparking planning algorithm based on the A* algorithm, and uses Model Predictive\nControl (MPC) as the control module for automatic parking.The algorithm\nenhances the planning real-time performance by optimizing heuristic functions,\nbinary heap optimization, and bidirectional search; it calculates the\npassability of narrow areas by dynamically loading obstacles and introduces the\nvehicle's own volume during planning; it improves trajectory quality by using\nneighborhood expansion and Bezier curve optimization methods to meet the high\ntrajectory quality requirements of the parking task. After obtaining the output\nresults of the planning algorithm, a loss function is designed according to the\ncharacteristics of the automatic parking task under local maps, and the MPC\nalgorithm is used to output control commands to drive the car along the planned\ntrajectory. This paper uses the perception results of real driving environments\nconverted into maps as planning inputs to conduct simulation tests and ablation\nexperiments on the algorithm. Experimental results show that the improved\nalgorithm proposed in this paper can effectively meet the special requirements\nof automatic parking under local maps and complete the automatic parking\nplanning and control tasks.",
      "tldr_zh": "这篇论文提出了一种基于改进 A* algorithm 的自动停车规划控制方法，针对自主驾驶中脱离高精度地图的挑战，通过优化启发式函数、二进制堆和双向搜索提升实时性能；动态加载障碍物并考虑车辆体积来评估狭窄区域可通行性；并采用邻域扩展和 Bezier curve 优化提高轨迹质量。算法结合 Model Predictive Control (MPC) 作为控制模块，根据规划结果输出控制命令，实现车辆沿规划轨迹行驶。实验在真实驾驶环境感知数据上进行模拟测试，结果显示该方法有效满足自动停车任务的实时、高精度和高轨迹质量要求。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.15429v1",
      "published_date": "2024-05-24 15:26:07 UTC",
      "updated_date": "2024-05-24 15:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:31:55.346768"
    },
    {
      "arxiv_id": "2405.15633v1",
      "title": "Less is more: Summarizing Patch Tokens for efficient Multi-Label Class-Incremental Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas De Min",
        "Massimiliano Mancini",
        "Stéphane Lathuilière",
        "Subhankar Roy",
        "Elisa Ricci"
      ],
      "abstract": "Prompt tuning has emerged as an effective rehearsal-free technique for\nclass-incremental learning (CIL) that learns a tiny set of task-specific\nparameters (or prompts) to instruct a pre-trained transformer to learn on a\nsequence of tasks. Albeit effective, prompt tuning methods do not lend well in\nthe multi-label class incremental learning (MLCIL) scenario (where an image\ncontains multiple foreground classes) due to the ambiguity in selecting the\ncorrect prompt(s) corresponding to different foreground objects belonging to\nmultiple tasks. To circumvent this issue we propose to eliminate the prompt\nselection mechanism by maintaining task-specific pathways, which allow us to\nlearn representations that do not interact with the ones from the other tasks.\nSince independent pathways in truly incremental scenarios will result in an\nexplosion of computation due to the quadratically complex multi-head\nself-attention (MSA) operation in prompt tuning, we propose to reduce the\noriginal patch token embeddings into summarized tokens. Prompt tuning is then\napplied to these fewer summarized tokens to compute the final representation.\nOur proposed method Multi-Label class incremental learning via summarising\npAtch tokeN Embeddings (MULTI-LANE) enables learning disentangled task-specific\nrepresentations in MLCIL while ensuring fast inference. We conduct experiments\nin common benchmarks and demonstrate that our MULTI-LANE achieves a new\nstate-of-the-art in MLCIL. Additionally, we show that MULTI-LANE is also\ncompetitive in the CIL setting. Source code available at\nhttps://github.com/tdemin16/multi-lane",
      "tldr_zh": "该研究针对多标签类增量学习（MLCIL）中提示调整（prompt tuning）的提示选择模糊问题，提出了一种新方法 MULTI-LANE，通过维护任务特定的路径（task-specific pathways）来学习互不干扰的表示。方法的关键在于将原始的 patch token 嵌入总结成更少的 summarized tokens，从而减少多头自注意力（MSA）操作的计算开销，实现高效推理。在常见基准实验中，MULTI-LANE 达到了 MLCIL 的新 state-of-the-art 性能，并在标准类增量学习（CIL）设置中表现出竞争力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15633v1",
      "published_date": "2024-05-24 15:18:27 UTC",
      "updated_date": "2024-05-24 15:18:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:32:06.416885"
    },
    {
      "arxiv_id": "2405.15624v2",
      "title": "Inverse-RLignment: Large Language Model Alignment from Demonstrations through Inverse Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Sun",
        "Mihaela van der Schaar"
      ],
      "abstract": "Aligning Large Language Models (LLMs) is crucial for enhancing their safety\nand utility. However, existing methods, primarily based on preference datasets,\nface challenges such as noisy labels, high annotation costs, and privacy\nconcerns. In this work, we introduce Alignment from Demonstrations (AfD), a\nnovel approach leveraging high-quality demonstration data to overcome these\nchallenges. We formalize AfD within a sequential decision-making framework,\nhighlighting its unique challenge of missing reward signals. Drawing insights\nfrom forward and inverse reinforcement learning, we introduce divergence\nminimization objectives for AfD. Analytically, we elucidate the mass-covering\nand mode-seeking behaviors of various approaches, explaining when and why\ncertain methods are superior. Practically, we propose a computationally\nefficient algorithm that extrapolates over a tailored reward model for AfD. We\nvalidate our key insights through experiments on the Harmless and Helpful\ntasks, demonstrating their strong empirical performance while maintaining\nsimplicity.",
      "tldr_zh": "该论文提出了一种名为 Inverse-RLignment 的新方法，通过逆强化学习（Inverse Reinforcement Learning）从高质量演示数据中实现 Large Language Models (LLMs) 的对齐（Alignment），以解决现有偏好数据集方法的噪音标签、高成本和隐私问题。作者将 Alignment from Demonstrations (AfD) 形式化为顺序决策框架，并引入发散最小化目标来处理缺少奖励信号的问题。分析显示，该方法在质量覆盖和模式寻找方面表现出优势，并提出了一种高效算法，使用定制的奖励模型进行外推。实验在 Harmless and Helpful 任务上验证了该方法的强性能，证明其简单性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15624v2",
      "published_date": "2024-05-24 15:13:53 UTC",
      "updated_date": "2025-01-25 11:10:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:32:20.199764"
    },
    {
      "arxiv_id": "2405.15616v1",
      "title": "Neuromorphic dreaming: A pathway to efficient learning in artificial agents",
      "title_zh": "翻译失败",
      "authors": [
        "Ingo Blakowski",
        "Dmitrii Zendrikov",
        "Cristiano Capone",
        "Giacomo Indiveri"
      ],
      "abstract": "Achieving energy efficiency in learning is a key challenge for artificial\nintelligence (AI) computing platforms. Biological systems demonstrate\nremarkable abilities to learn complex skills quickly and efficiently. Inspired\nby this, we present a hardware implementation of model-based reinforcement\nlearning (MBRL) using spiking neural networks (SNNs) on mixed-signal\nanalog/digital neuromorphic hardware. This approach leverages the energy\nefficiency of mixed-signal neuromorphic chips while achieving high sample\nefficiency through an alternation of online learning, referred to as the\n\"awake\" phase, and offline learning, known as the \"dreaming\" phase. The model\nproposed includes two symbiotic networks: an agent network that learns by\ncombining real and simulated experiences, and a learned world model network\nthat generates the simulated experiences. We validate the model by training the\nhardware implementation to play the Atari game Pong. We start from a baseline\nconsisting of an agent network learning without a world model and dreaming,\nwhich successfully learns to play the game. By incorporating dreaming, the\nnumber of required real game experiences are reduced significantly compared to\nthe baseline. The networks are implemented using a mixed-signal neuromorphic\nprocessor, with the readout layers trained using a computer in-the-loop, while\nthe other layers remain fixed. These results pave the way toward\nenergy-efficient neuromorphic learning systems capable of rapid learning in\nreal world applications and use-cases.",
      "tldr_zh": "该研究提出了一种基于神经形态硬件的模型，以实现人工智能代理的能源高效学习，灵感来源于生物系统的快速学习机制。方法采用尖峰神经网络 (SNNs) 实现基于模型的强化学习 (MBRL)，通过交替“觉醒 (awake)”阶段的在线学习和“梦境 (dreaming)”阶段的离线学习，结合代理网络和世界模型网络来减少真实经验需求。在 Atari 游戏 Pong 的实验中，该框架显著降低了所需真实游戏样本，同时在混合信号神经形态处理器上验证了其可行性，为能源高效的神经形态学习系统在实际应用中实现快速学习铺平了道路。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15616v1",
      "published_date": "2024-05-24 15:03:56 UTC",
      "updated_date": "2024-05-24 15:03:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:32:33.898680"
    },
    {
      "arxiv_id": "2405.15614v1",
      "title": "Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study",
      "title_zh": "利用大语言模型进行软件漏洞检测：一个全面的基准测试研究",
      "authors": [
        "Karl Tamberg",
        "Hayretdin Bahsi"
      ],
      "abstract": "Despite various approaches being employed to detect vulnerabilities, the\nnumber of reported vulnerabilities shows an upward trend over the years. This\nsuggests the problems are not caught before the code is released, which could\nbe caused by many factors, like lack of awareness, limited efficacy of the\nexisting vulnerability detection tools or the tools not being user-friendly. To\nhelp combat some issues with traditional vulnerability detection tools, we\npropose using large language models (LLMs) to assist in finding vulnerabilities\nin source code. LLMs have shown a remarkable ability to understand and generate\ncode, underlining their potential in code-related tasks. The aim is to test\nmultiple state-of-the-art LLMs and identify the best prompting strategies,\nallowing extraction of the best value from the LLMs. We provide an overview of\nthe strengths and weaknesses of the LLM-based approach and compare the results\nto those of traditional static analysis tools. We find that LLMs can pinpoint\nmany more issues than traditional static analysis tools, outperforming\ntraditional tools in terms of recall and F1 scores. The results should benefit\nsoftware developers and security analysts responsible for ensuring that the\ncode is free of vulnerabilities.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型 (LLMs) 来检测软件漏洞的问题，通过全面基准测试评估多种 state-of-the-art LLMs 和最佳 prompting strategies。研究比较了 LLMs 方法与传统静态分析工具的性能，发现 LLMs 在召回率和 F1 scores 上表现出色，能够识别更多漏洞。总体而言，该方法突出了 LLMs 的优势，并为软件开发者和安全分析师提供更有效的工具，以确保代码安全。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15614v1",
      "published_date": "2024-05-24 14:59:19 UTC",
      "updated_date": "2024-05-24 14:59:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:32:42.400229"
    },
    {
      "arxiv_id": "2405.15613v2",
      "title": "Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach",
      "title_zh": "自动数据策展用于自监督学习：一种基于聚类的方法",
      "authors": [
        "Huy V. Vo",
        "Vasil Khalidov",
        "Timothée Darcet",
        "Théo Moutakanni",
        "Nikita Smetanin",
        "Marc Szafraniec",
        "Hugo Touvron",
        "Camille Couprie",
        "Maxime Oquab",
        "Armand Joulin",
        "Hervé Jégou",
        "Patrick Labatut",
        "Piotr Bojanowski"
      ],
      "abstract": "Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data. Code is\navailable at https://github.com/facebookresearch/ssl-data-curation.",
      "tldr_zh": "这篇论文针对自监督学习（self-supervised learning）的数据整理问题，提出了一种基于聚类的自动数据整理方法，以解决手动整理的成本高、耗时长和扩展性差等问题。该方法通过层次化的 k-means 聚类在大型数据仓库中生成均匀分布的集群，并结合平衡采样步骤，确保数据集具备规模、多样性和平衡性。实验结果显示，在网页图像、卫星图像和文本领域，使用自动整理的数据集训练的特征表现优于未整理数据，甚至与手动整理数据集相当或更好。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15613v2",
      "published_date": "2024-05-24 14:58:51 UTC",
      "updated_date": "2024-06-28 09:22:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:32:54.588868"
    },
    {
      "arxiv_id": "2405.15598v5",
      "title": "MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel Data Fusion Network Model",
      "title_zh": "MCDFN：一种可解释的多通道数据融合",
      "authors": [
        "Md Abrar Jahin",
        "Asef Shahriar",
        "Md Al Amin"
      ],
      "abstract": "Accurate demand forecasting is crucial for optimizing supply chain\nmanagement. Traditional methods often fail to capture complex patterns from\nseasonal variability and special events. Despite advancements in deep learning,\ninterpretable forecasting models remain a challenge. To address this, we\nintroduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture\nthat integrates Convolutional Neural Networks (CNN), Long Short-Term Memory\nnetworks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive\nperformance by extracting spatial and temporal features from time series data.\nOur comparative benchmarking demonstrates that MCDFN outperforms seven other\ndeep-learning models, achieving superior metrics: MSE (23.5738), RMSE (4.8553),\nMAE (3.9991), and MAPE (20.1575%). Theil's U statistic of 0.1181 (U<1) of MCDFN\nindicates its superiority over the naive forecasting approach, and a 10-fold\ncross-validated statistical paired t-test with a p-value of 5% indicated no\nsignificant difference between MCDFN's predictions and actual values. We apply\nexplainable AI techniques like ShapTime and Permutation Feature Importance to\nenhance interpretability. This research advances demand forecasting\nmethodologies and offers practical guidelines for integrating MCDFN into supply\nchain systems, highlighting future research directions for scalability and\nuser-friendly deployment.",
      "tldr_zh": "这篇论文提出 MCDFN（Multi-Channel Data Fusion Network）模型，用于提升供应链需求预测的准确性，通过整合 CNN、LSTM 和 GRU 从时间序列数据中提取空间和时间特征，解决传统方法对季节性和特殊事件模式捕捉不足的问题。实验结果显示，MCDFN 在基准测试中优于其他七个深度学习模型，取得优异指标如 MSE (23.5738)、RMSE (4.8553)、MAE (3.9991) 和 MAPE (20.1575%)，并通过 Theil's U 统计量 (0.1181) 和 t 检验验证其预测可靠性。论文还应用 ShapTime 和 Permutation Feature Importance 等可解释 AI 技术增强模型的可解释性，并提供将 MCDFN 整合到供应链系统的实用指南，为未来可扩展性和部署指明方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15598v5",
      "published_date": "2024-05-24 14:30:00 UTC",
      "updated_date": "2025-03-01 19:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:33:10.834846"
    },
    {
      "arxiv_id": "2405.15579v1",
      "title": "Generating density nowcasts for U.S. GDP growth with deep learning: Bayes by Backprop and Monte Carlo dropout",
      "title_zh": "翻译失败",
      "authors": [
        "Kristóf Németh",
        "Dániel Hadházi"
      ],
      "abstract": "Recent results in the literature indicate that artificial neural networks\n(ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy\nof GDP nowcasts. Compared to the DFM, the performance advantage of these highly\nflexible, nonlinear estimators is particularly evident in periods of recessions\nand structural breaks. From the perspective of policy-makers, however, nowcasts\nare the most useful when they are conveyed with uncertainty attached to them.\nWhile the DFM and other classical time series approaches analytically derive\nthe predictive (conditional) distribution for GDP growth, ANNs can only produce\npoint nowcasts based on their default training procedure (backpropagation). To\nfill this gap, first in the literature, we adapt two different deep learning\nalgorithms that enable ANNs to generate density nowcasts for U.S. GDP growth:\nBayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts,\ndefined as the mean of the empirical predictive distribution, is evaluated\nrelative to a naive constant growth model for GDP and a benchmark DFM\nspecification. Using a 1D CNN as the underlying ANN architecture, both\nalgorithms outperform those benchmarks during the evaluation period (2012:Q1 --\n2022:Q4). Furthermore, both algorithms are able to dynamically adjust the\nlocation (mean), scale (variance), and shape (skew) of the empirical predictive\ndistribution. The results indicate that both Bayes by Backprop and Monte Carlo\ndropout can effectively augment the scope and functionality of ANNs, rendering\nthem a fully compatible and competitive alternative for classical time series\napproaches.",
      "tldr_zh": "本文研究使用深度学习方法生成美国 GDP 增长的密度 nowcasts，首次引入 Bayes by Backprop 和 Monte Carlo dropout 算法，以克服传统人工神经网络 (ANNs) 仅能提供点预测的局限性。基于 1D CNN 架构，这些算法在 2012:Q1 至 2022:Q4 评估期内，点预测准确性优于动态因子模型 (DFM) 和简单基准模型，并能动态调整预测分布的均值、方差和偏斜。结果表明，此方法增强了 ANNs 的功能，使其成为经典时间序列方法的强大竞争者。",
      "categories": [
        "econ.EM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15579v1",
      "published_date": "2024-05-24 14:06:08 UTC",
      "updated_date": "2024-05-24 14:06:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:33:20.193762"
    },
    {
      "arxiv_id": "2405.15569v1",
      "title": "Randomized heuristic repair for large-scale multidimensional knapsack problem",
      "title_zh": "翻译失败",
      "authors": [
        "Jean P. Martins"
      ],
      "abstract": "The multidimensional knapsack problem (MKP) is an NP-hard combinatorial\noptimization problem whose solution is determining a subset of maximum total\nprofit items that do not violate capacity constraints. Due to its hardness,\nlarge-scale MKP instances are usually a target for metaheuristics, a context in\nwhich effective feasibility maintenance strategies are crucial. In 1998, Chu\nand Beasley proposed an effective heuristic repair that is still relevant for\nrecent metaheuristics. However, due to its deterministic nature, the diversity\nof solutions such heuristic provides is insufficient for long runs. As a\nresult, the search for new solutions ceases after a while. This paper proposes\nan efficiency-based randomization strategy for the heuristic repair that\nincreases the variability of the repaired solutions without deteriorating\nquality and improves the overall results.",
      "tldr_zh": "多维背包问题（MKP）是一个 NP-hard 的组合优化问题，旨在选择最大利润的物品子集而不违反容量约束，而现有方法如 Chu 和 Beasley 的确定性启发式修复因缺乏解决方案多样性导致搜索效率低下。本文提出了一种基于效率的随机化策略，用于改进启发式修复过程，该策略通过增加修复解决方案的变异性，同时保持质量水平，提升了整体优化性能。该方法为处理大规模 MKP 实例提供了更有效的元启发式框架。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15569v1",
      "published_date": "2024-05-24 14:01:05 UTC",
      "updated_date": "2024-05-24 14:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:33:30.547689"
    },
    {
      "arxiv_id": "2405.15568v3",
      "title": "OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code",
      "title_zh": "翻译失败",
      "authors": [
        "Maxence Faldor",
        "Jenny Zhang",
        "Antoine Cully",
        "Jeff Clune"
      ],
      "abstract": "Open-ended and AI-generating algorithms aim to continuously generate and\nsolve increasingly complex tasks indefinitely, offering a promising path toward\nmore general intelligence. To accomplish this grand vision, learning must occur\nwithin a vast array of potential tasks. Existing approaches to automatically\ngenerating environments are constrained within manually predefined, often\nnarrow distributions of environment, limiting their ability to create any\nlearning environment. To address this limitation, we introduce a novel\nframework, OMNI-EPIC, that augments previous work in Open-endedness via Models\nof human Notions of Interestingness (OMNI) with Environments Programmed in Code\n(EPIC). OMNI-EPIC leverages foundation models to autonomously generate code\nspecifying the next learnable (i.e., not too easy or difficult for the agent's\ncurrent skill set) and interesting (e.g., worthwhile and novel) tasks.\nOMNI-EPIC generates both environments (e.g., an obstacle course) and reward\nfunctions (e.g., progress through the obstacle course quickly without touching\nred objects), enabling it, in principle, to create any simulatable learning\ntask. We showcase the explosive creativity of OMNI-EPIC, which continuously\ninnovates to suggest new, interesting learning challenges. We also highlight\nhow OMNI-EPIC can adapt to reinforcement learning agents' learning progress,\ngenerating tasks that are of suitable difficulty. Overall, OMNI-EPIC can\nendlessly create learnable and interesting environments, further propelling the\ndevelopment of self-improving AI systems and AI-Generating Algorithms. Project\nwebsite with videos: https://dub.sh/omniepic",
      "tldr_zh": "该研究提出 OMNI-EPIC 框架，以解决现有开放式 AI 生成算法受限于手动预定义狭窄环境分布的问题。OMNI-EPIC 扩展了 OMNI（Open-endedness via Models of human Notions of Interestingness），并结合 EPIC（Environments Programmed in Code），利用 foundation models 自主生成代码来创建可学习的任务，包括环境（如障碍课程）和奖励函数（如快速通过而不触碰红色物体）。该框架能根据 reinforcement learning agents 的技能水平动态调整任务难度，确保任务既不过于简单或困难，又富有新意和价值。总体上，OMNI-EPIC 展示了爆炸性的创造力，推动了自提升 AI 系统和 AI-Generating Algorithms 的发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15568v3",
      "published_date": "2024-05-24 13:57:32 UTC",
      "updated_date": "2025-02-14 14:24:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:33:43.776022"
    },
    {
      "arxiv_id": "2405.15564v2",
      "title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Miao",
        "Kaixiong Zhou",
        "Yili Wang",
        "Ninghao Liu",
        "Ying Wang",
        "Xin Wang"
      ],
      "abstract": "Graph neural networks (GNNs) have exhibited prominent performance in learning\ngraph-structured data. Considering node classification task, based on the i.i.d\nassumption among node labels, the traditional supervised learning simply sums\nup cross-entropy losses of the independent training nodes and applies the\naverage loss to optimize GNNs' weights. But different from other data formats,\nthe nodes are naturally connected. It is found that the independent\ndistribution modeling of node labels restricts GNNs' capability to generalize\nover the entire graph and defend adversarial attacks. In this work, we propose\na new framework, termed joint-cluster supervised learning, to model the joint\ndistribution of each node with its corresponding cluster. We learn the joint\ndistribution of node and cluster labels conditioned on their representations,\nand train GNNs with the obtained joint loss. In this way, the data-label\nreference signals extracted from the local cluster explicitly strengthen the\ndiscrimination ability on the target node. The extensive experiments\ndemonstrate that our joint-cluster supervised learning can effectively bolster\nGNNs' node classification accuracy. Furthermore, being benefited from the\nreference signals which may be free from spiteful interference, our learning\nparadigm significantly protects the node classification from being affected by\nthe adversarial attack.",
      "tldr_zh": "传统图神经网络(GNNs)在使用独立交叉熵损失(cross-entropy loss)进行节点分类时，假设节点标签独立分布，但忽略了图结构中节点间的自然连接，导致模型泛化能力不足和易受对抗攻击(adversarial attacks)影响。\n\n本文提出一种新框架——联合集群监督学习(joint-cluster supervised learning)，通过学习节点与其对应集群标签的联合分布，并使用联合损失训练GNNs，从而增强节点的区分能力。\n\n实验结果显示，该方法在节点分类任务上显著提高了准确率，并有效提升了对对抗攻击的防御能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15564v2",
      "published_date": "2024-05-24 13:52:41 UTC",
      "updated_date": "2024-05-27 01:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:33:54.573092"
    },
    {
      "arxiv_id": "2405.15561v1",
      "title": "When Generative AI Meets Workplace Learning: Creating A Realistic & Motivating Learning Experience With A Generative PCA",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Bucher",
        "Birgit Schenk",
        "Mateusz Dolata",
        "Gerhard Schwabe"
      ],
      "abstract": "Workplace learning is used to train employees systematically, e.g., via\ne-learning or in 1:1 training. However, this is often deemed ineffective and\ncostly. Whereas pure e-learning lacks the possibility of conversational\nexercise and personal contact, 1:1 training with human instructors involves a\nhigh level of personnel and organizational costs. Hence, pedagogical\nconversational agents (PCAs), based on generative AI, seem to compensate for\nthe disadvantages of both forms. Following Action Design Research, this paper\ndescribes an organizational communication training with a Generative PCA\n(GenPCA). The evaluation shows promising results: the agent was perceived\npositively among employees and contributed to an improvement in self-determined\nlearning. However, the integration of such agent comes not without limitations.\nWe conclude with suggestions concerning the didactical methods, which are\nsupported by a GenPCA, and possible improvements of such an agent for workplace\nlearning.",
      "tldr_zh": "本研究探讨了生成式 AI 在工作场所学习中的应用，提出使用 Pedagogical Conversational Agents (PCAs) 特别是 Generative PCA (GenPCA)，以解决 e-learning 缺乏互动性和 1:1 训练的高成本问题。论文通过 Action Design Research 方法，设计并评估了一个组织沟通训练程序，结果显示 GenPCA 获得了员工的积极反馈，并提升了自主学习体验。尽管存在整合局限性，如技术挑战，该框架仍证明了其潜力。作者建议优化 GenPCA 的教学方法和功能，以进一步改善工作场所学习效果。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15561v1",
      "published_date": "2024-05-24 13:49:18 UTC",
      "updated_date": "2024-05-24 13:49:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:34:06.252712"
    },
    {
      "arxiv_id": "2405.17483v1",
      "title": "Concept-based Explainable Malignancy Scoring on Pulmonary Nodules in CT Images",
      "title_zh": "翻译失败",
      "authors": [
        "Rinat I. Dumaev",
        "Sergei A. Molodyakov",
        "Lev V. Utkin"
      ],
      "abstract": "To increase the transparency of modern computer-aided diagnosis (CAD) systems\nfor assessing the malignancy of lung nodules, an interpretable model based on\napplying the generalized additive models and the concept-based learning is\nproposed. The model detects a set of clinically significant attributes in\naddition to the final malignancy regression score and learns the association\nbetween the lung nodule attributes and a final diagnosis decision as well as\ntheir contributions into the decision. The proposed concept-based learning\nframework provides human-readable explanations in terms of different concepts\n(numerical and categorical), their values, and their contribution to the final\nprediction. Numerical experiments with the LIDC-IDRI dataset demonstrate that\nthe diagnosis results obtained using the proposed model, which explicitly\nexplores internal relationships, are in line with similar patterns observed in\nclinical practice. Additionally, the proposed model shows the competitive\nclassification and the nodule attribute scoring performance, highlighting its\npotential for effective decision-making in the lung nodule diagnosis.",
      "tldr_zh": "这篇论文提出了一种基于 generalized additive models 和 concept-based learning 的可解释模型，用于评估 CT 图像中肺结节的恶性度。该模型不仅输出最终的恶性度评分，还检测临床重要属性并分析这些属性对诊断决策的贡献，提供人类可读解释，包括数值和分类概念及其影响。实验在 LIDC-IDRI dataset 上表明，该模型的诊断结果与临床实践一致，并在分类和属性评分性能上表现出色，具有提升决策支持潜力的优势。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17483v1",
      "published_date": "2024-05-24 13:36:44 UTC",
      "updated_date": "2024-05-24 13:36:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:34:22.061985"
    },
    {
      "arxiv_id": "2406.03199v4",
      "title": "Bayesian WeakS-to-Strong from Text Classification to Generation",
      "title_zh": "贝叶斯 WeakS-to-Strong：从文本分类到生成",
      "authors": [
        "Ziyun Cui",
        "Ziyang Zhang",
        "Guangzhi Sun",
        "Wen Wu",
        "Chao Zhang"
      ],
      "abstract": "Advances in large language models raise the question of how alignment\ntechniques will adapt as models become increasingly complex and humans will\nonly be able to supervise them weakly. Weak-to-Strong mimics such a scenario\nwhere weak model supervision attempts to harness the full capabilities of a\nmuch stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by\nexploring an ensemble of weak models which simulate the variability in human\nopinions. Confidence scores are estimated using a Bayesian approach to guide\nthe WeakS-to-Strong generalization. Furthermore, we extend the application of\nWeakS-to-Strong from text classification tasks to text generation tasks where\nmore advanced strategies are investigated for supervision. Moreover, direct\npreference optimization is applied to advance the student model's preference\nlearning, beyond the basic learning framework of teacher forcing. Results\ndemonstrate the effectiveness of the proposed approach for the reliability of a\nstrong student model, showing potential for superalignment.",
      "tldr_zh": "这项研究探讨了在大型语言模型（LLMs）不断复杂化的背景下，如何通过Bayesian WeakS-to-Strong方法实现模型对齐，该方法使用弱模型的集合模拟人类意见差异，并采用贝叶斯置信度估计来指导泛化。研究将WeakS-to-Strong从文本分类任务扩展到文本生成任务，引入高级监督策略和直接偏好优化（direct preference optimization），以提升学生模型的偏好学习。实验结果证明，该方法显著提高了强学生模型的可靠性和superalignment潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICLR2025",
      "pdf_url": "http://arxiv.org/pdf/2406.03199v4",
      "published_date": "2024-05-24 13:33:11 UTC",
      "updated_date": "2025-03-12 07:57:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:34:31.445576"
    },
    {
      "arxiv_id": "2405.15544v1",
      "title": "Knowledge-enhanced Relation Graph and Task Sampling for Few-shot Molecular Property Prediction",
      "title_zh": "知识增强关系图和任务采样用于少样本",
      "authors": [
        "Zeyu Wang",
        "Tianyi Jiang",
        "Yao Lu",
        "Xiaoze Bao",
        "Shanqing Yu",
        "Bin Wei",
        "Qi Xuan"
      ],
      "abstract": "Recently, few-shot molecular property prediction (FSMPP) has garnered\nincreasing attention. Despite impressive breakthroughs achieved by existing\nmethods, they often overlook the inherent many-to-many relationships between\nmolecules and properties, which limits their performance. For instance, similar\nsubstructures of molecules can inspire the exploration of new compounds.\nAdditionally, the relationships between properties can be quantified, with\nhigh-related properties providing more information in exploring the target\nproperty than those low-related. To this end, this paper proposes a novel\nmeta-learning FSMPP framework (KRGTS), which comprises the Knowledge-enhanced\nRelation Graph module and the Task Sampling module. The knowledge-enhanced\nrelation graph module constructs the molecule-property multi-relation graph\n(MPMRG) to capture the many-to-many relationships between molecules and\nproperties. The task sampling module includes a meta-training task sampler and\nan auxiliary task sampler, responsible for scheduling the meta-training process\nand sampling high-related auxiliary tasks, respectively, thereby achieving\nefficient meta-knowledge learning and reducing noise introduction. Empirically,\nextensive experiments on five datasets demonstrate the superiority of KRGTS\nover a variety of state-of-the-art methods. The code is available in\nhttps://github.com/Vencent-Won/KRGTS-public.",
      "tldr_zh": "该论文针对 Few-shot Molecular Property Prediction (FSMPP) 的问题，提出了一种新型元学习框架 KRGTS，以解决现有方法忽略分子和属性之间多对多关系的局限性。KRGTS 包括 Knowledge-enhanced Relation Graph 模块，该模块构建 Molecule-Property Multi-Relation Graph (MPMRG) 来捕捉分子相似子结构和属性间相关性；以及 Task Sampling 模块，通过 meta-training task sampler 和 auxiliary task sampler 优化训练过程，采样高相关辅助任务以减少噪声干扰。实验结果显示，KRGTS 在五个数据集上优于多种最先进方法，证明了其有效性，并提供了开源代码。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15544v1",
      "published_date": "2024-05-24 13:31:19 UTC",
      "updated_date": "2024-05-24 13:31:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:34:43.733789"
    },
    {
      "arxiv_id": "2405.15521v1",
      "title": "A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search",
      "title_zh": "翻译失败",
      "authors": [
        "Huimu Wang",
        "Mingming Li",
        "Dadong Miao",
        "Songlin Wang",
        "Guoyu Tang",
        "Lin Liu",
        "Sulong Xu",
        "Jinghe Hu"
      ],
      "abstract": "Re-ranking is a process of rearranging ranking list to more effectively meet\nuser demands by accounting for the interrelationships between items. Existing\nmethods predominantly enhance the precision of search results, often at the\nexpense of diversity, leading to outcomes that may not fulfill the varied needs\nof users. Conversely, methods designed to promote diversity might compromise\nthe precision of the results, failing to satisfy the users' requirements for\naccuracy. To alleviate the above problems, this paper proposes a\nPreference-oriented Diversity Model Based on Mutual-information (PODM-MI),\nwhich consider both accuracy and diversity in the re-ranking process.\nSpecifically, PODM-MI adopts Multidimensional Gaussian distributions based on\nvariational inference to capture users' diversity preferences with uncertainty.\nThen we maximize the mutual information between the diversity preferences of\nthe users and the candidate items using the maximum variational inference lower\nbound to enhance their correlations. Subsequently, we derive a utility matrix\nbased on the correlations, enabling the adaptive ranking of items in line with\nuser preferences and establishing a balance between the aforementioned\nobjectives. Experimental results on real-world online e-commerce systems\ndemonstrate the significant improvements of PODM-MI, and we have successfully\ndeployed PODM-MI on an e-commerce search platform.",
      "tldr_zh": "本论文针对电子商务搜索中的重新排序（Re-ranking）问题，提出了一种基于互信息（Mutual-information）的偏好导向多样性模型（PODM-MI），旨在同时平衡精确性（precision）和多样性（diversity），以更好地满足用户需求。具体而言，PODM-MI 使用基于变分推理（Variational inference）的多维高斯分布来捕捉用户多样性偏好的不确定性，并通过最大化用户偏好与候选物品之间的互信息来增强相关性，从而派生效用矩阵（Utility matrix）实现自适应排名。实验结果显示，该模型在真实在线电商系统中显著提升了搜索性能，并已成功部署在电商搜索平台上。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15521v1",
      "published_date": "2024-05-24 13:03:34 UTC",
      "updated_date": "2024-05-24 13:03:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:34:56.569273"
    },
    {
      "arxiv_id": "2405.15514v1",
      "title": "On the Convexity and Reliability of the Bethe Free Energy Approximation",
      "title_zh": "翻译失败",
      "authors": [
        "Harald Leisenberger",
        "Christian Knoll",
        "Franz Pernkopf"
      ],
      "abstract": "The Bethe free energy approximation provides an effective way for relaxing\nNP-hard problems of probabilistic inference. However, its accuracy depends on\nthe model parameters and particularly degrades if a phase transition in the\nmodel occurs. In this work, we analyze when the Bethe approximation is reliable\nand how this can be verified. We argue and show by experiment that it is mostly\naccurate if it is convex on a submanifold of its domain, the 'Bethe box'. For\nverifying its convexity, we derive two sufficient conditions that are based on\nthe definiteness properties of the Bethe Hessian matrix: the first uses the\nconcept of diagonal dominance, and the second decomposes the Bethe Hessian\nmatrix into a sum of sparse matrices and characterizes the definiteness\nproperties of the individual matrices in that sum. These theoretical results\nprovide a simple way to estimate the critical phase transition temperature of a\nmodel. As a practical contribution we propose $\\texttt{BETHE-MIN}$, a projected\nquasi-Newton method to efficiently find a minimum of the Bethe free energy.",
      "tldr_zh": "本文研究了 Bethe free energy approximation 的凸性和可靠性，作为一种松弛 NP-hard 概率推断问题的有效方法。作者证明，当该近似在 'Bethe box' 子集上凸时，其准确性最高，并推导了两个基于 Bethe Hessian 矩阵正定性的充分条件：一个利用对角占优概念，另一个通过分解为稀疏矩阵之和来分析。实验验证显示，这些条件可用于估计模型的临界相变温度，并提出 $\\texttt{BETHE-MIN}$ 算法，一种高效的投影准牛顿方法，用于最小化 Bethe free energy。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "This work has been submitted to the Journal of Machine Learning\n  Research (JMLR) for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2405.15514v1",
      "published_date": "2024-05-24 12:57:40 UTC",
      "updated_date": "2024-05-24 12:57:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:35:07.843818"
    },
    {
      "arxiv_id": "2405.15512v2",
      "title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
      "title_zh": "翻译失败",
      "authors": [
        "Marc Oedingen",
        "Raphael C. Engelhardt",
        "Robin Denz",
        "Maximilian Hammer",
        "Wolfgang Konen"
      ],
      "abstract": "In recent times, large language models (LLMs) have made significant strides\nin generating computer code, blurring the lines between code created by humans\nand code produced by artificial intelligence (AI). As these technologies evolve\nrapidly, it is crucial to explore how they influence code generation,\nespecially given the risk of misuse in areas like higher education. This paper\nexplores this issue by using advanced classification techniques to\ndifferentiate between code written by humans and that generated by ChatGPT, a\ntype of LLM. We employ a new approach that combines powerful embedding features\n(black-box) with supervised learning algorithms - including Deep Neural\nNetworks, Random Forests, and Extreme Gradient Boosting - to achieve this\ndifferentiation with an impressive accuracy of 98%. For the successful\ncombinations, we also examine their model calibration, showing that some of the\nmodels are extremely well calibrated. Additionally, we present white-box\nfeatures and an interpretable Bayes classifier to elucidate critical\ndifferences between the code sources, enhancing the explainability and\ntransparency of our approach. Both approaches work well but provide at most\n85-88% accuracy. We also show that untrained humans solve the same task not\nbetter than random guessing. This study is crucial in understanding and\nmitigating the potential risks associated with using AI in code generation,\nparticularly in the context of higher education, software development, and\ncompetitive programming.",
      "tldr_zh": "这篇论文探讨了使用高级分类技术区分人类编写的代码与 ChatGPT 生成的代码，以应对大型语言模型 (LLMs) 在代码生成中的潜在风险，如高等教育中的滥用。研究方法结合了黑盒嵌入特征 (embedding features) 与监督学习算法，包括 Deep Neural Networks、Random Forests 和 Extreme Gradient Boosting，实现了98%的准确率，并评估了模型校准 (model calibration) 的可靠性。作者还引入了白盒特征 (white-box features) 和可解释的 Bayes 分类器，提供85-88%的准确率，并发现 untrained humans 在此任务上表现不佳，仅略好于随机猜测。该工作为软件开发和竞争编程等领域提供了关键工具，以增强 AI 代码生成的透明度和风险管理。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in MDPI AI Journal",
      "pdf_url": "http://arxiv.org/pdf/2405.15512v2",
      "published_date": "2024-05-24 12:56:18 UTC",
      "updated_date": "2024-07-03 10:23:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:35:20.371629"
    },
    {
      "arxiv_id": "2405.15505v1",
      "title": "Revisiting Counterfactual Regression through the Lens of Gromov-Wasserstein Information Bottleneck",
      "title_zh": "通过 Gromov-Wasserstein 信息瓶颈的视角重新审视反事实回归",
      "authors": [
        "Hao Yang",
        "Zexu Sun",
        "Hongteng Xu",
        "Xu Chen"
      ],
      "abstract": "As a promising individualized treatment effect (ITE) estimation method,\ncounterfactual regression (CFR) maps individuals' covariates to a latent space\nand predicts their counterfactual outcomes. However, the selection bias between\ncontrol and treatment groups often imbalances the two groups' latent\ndistributions and negatively impacts this method's performance. In this study,\nwe revisit counterfactual regression through the lens of information bottleneck\nand propose a novel learning paradigm called Gromov-Wasserstein information\nbottleneck (GWIB). In this paradigm, we learn CFR by maximizing the mutual\ninformation between covariates' latent representations and outcomes while\npenalizing the kernelized mutual information between the latent representations\nand the covariates. We demonstrate that the upper bound of the penalty term can\nbe implemented as a new regularizer consisting of $i)$ the fused\nGromov-Wasserstein distance between the latent representations of different\ngroups and $ii)$ the gap between the transport cost generated by the model and\nthe cross-group Gromov-Wasserstein distance between the latent representations\nand the covariates. GWIB effectively learns the CFR model through alternating\noptimization, suppressing selection bias while avoiding trivial latent\ndistributions. Experiments on ITE estimation tasks show that GWIB consistently\noutperforms state-of-the-art CFR methods. To promote the research community, we\nrelease our project at https://github.com/peteryang1031/Causal-GWIB.",
      "tldr_zh": "这篇论文通过 Gromov-Wasserstein Information Bottleneck (GWIB) 框架重新审视 Counterfactual Regression (CFR)，旨在解决控制组和治疗组潜在分布不平衡导致的选择偏差问题。GWIB 提出了一种新范式，通过最大化协变量潜在表示与结果之间的 Mutual Information，同时惩罚潜在表示与协变量之间的 Kernelized Mutual Information，并使用 Fused Gromov-Wasserstein Distance 作为正则化器来抑制偏差并避免平凡分布。实验结果表明，GWIB 在 Individualized Treatment Effect (ITE) 估计任务中 consistently outperforms 现有 CFR 方法，并开源了项目代码以推动研究社区发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.15505v1",
      "published_date": "2024-05-24 12:48:24 UTC",
      "updated_date": "2024-05-24 12:48:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:35:34.416826"
    },
    {
      "arxiv_id": "2405.15485v1",
      "title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
      "title_zh": "超越模式匹配的学习？评估 LLMs 中的数学理解",
      "authors": [
        "Siyuan Guo",
        "Aniket Didolkar",
        "Nan Rosemary Ke",
        "Anirudh Goyal",
        "Ferenc Huszár",
        "Bernhard Schölkopf"
      ],
      "abstract": "We are beginning to see progress in language model assisted scientific\ndiscovery. Motivated by the use of LLMs as a general scientific assistant, this\npaper assesses the domain knowledge of LLMs through its understanding of\ndifferent mathematical skills required to solve problems. In particular, we\nlook at not just what the pre-trained model already knows, but how it learned\nto learn from information during in-context learning or instruction-tuning\nthrough exploiting the complex knowledge structure within mathematics.\nMotivated by the Neural Tangent Kernel (NTK), we propose \\textit{NTKEval} to\nassess changes in LLM's probability distribution via training on different\nkinds of math data. Our systematic analysis finds evidence of domain\nunderstanding during in-context learning. By contrast, certain\ninstruction-tuning leads to similar performance changes irrespective of\ntraining on different data, suggesting a lack of domain understanding across\ndifferent skills.",
      "tldr_zh": "本研究评估大型语言模型（LLMs）在数学理解方面的能力，超越单纯的模式匹配，探讨其通过in-context learning和instruction-tuning从复杂数学知识结构中学习的方式。主要贡献是提出NTKEval方法，该方法基于Neural Tangent Kernel (NTK)，用于分析LLMs在不同数学数据上训练后的概率分布变化。结果显示，in-context learning过程中存在领域理解的证据，而某些instruction-tuning导致的性能提升与训练数据无关，表明LLMs在跨技能领域理解上存在局限性。该工作为提升LLMs作为科学助理的潜力提供了重要洞见。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15485v1",
      "published_date": "2024-05-24 12:04:54 UTC",
      "updated_date": "2024-05-24 12:04:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:35:42.589744"
    },
    {
      "arxiv_id": "2405.15476v3",
      "title": "Editable Concept Bottleneck Models",
      "title_zh": "可编辑的概念瓶颈模型",
      "authors": [
        "Lijie Hu",
        "Chenyang Ren",
        "Zhengyu Hu",
        "Hongbin Lin",
        "Cheng-Long Wang",
        "Hui Xiong",
        "Jingfeng Zhang",
        "Di Wang"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) have garnered much attention for their\nability to elucidate the prediction process through a humanunderstandable\nconcept layer. However, most previous studies focused on cases where the data,\nincluding concepts, are clean. In many scenarios, we often need to\nremove/insert some training data or new concepts from trained CBMs for reasons\nsuch as privacy concerns, data mislabelling, spurious concepts, and concept\nannotation errors. Thus, deriving efficient editable CBMs without retraining\nfrom scratch remains a challenge, particularly in large-scale applications. To\naddress these challenges, we propose Editable Concept Bottleneck Models\n(ECBMs). Specifically, ECBMs support three different levels of data removal:\nconcept-label-level, concept-level, and data-level. ECBMs enjoy mathematically\nrigorous closed-form approximations derived from influence functions that\nobviate the need for retraining. Experimental results demonstrate the\nefficiency and adaptability of our ECBMs, affirming their practical value in\nCBMs.",
      "tldr_zh": "Concept Bottleneck Models (CBMs) 能够通过人类可理解的概念层解释预测过程，但现有研究多假设数据干净，而实际场景中需处理数据移除或插入问题，如隐私担忧或标注错误。研究提出 Editable Concept Bottleneck Models (ECBMs)，支持三种级别的数据编辑：concept-label-level、concept-level 和 data-level，并利用 influence functions 派生的数学严格闭式形式近似，避免从零重新训练。该方法在实验中证明了高效性和适应性，为大规模 CBMs 应用提供了实际价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "49 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.15476v3",
      "published_date": "2024-05-24 11:55:46 UTC",
      "updated_date": "2025-02-01 12:03:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:35:54.908906"
    },
    {
      "arxiv_id": "2405.15453v2",
      "title": "Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks",
      "title_zh": "预训练大语言模型在乌尔都语自然语言处理任务中的性能基准测试",
      "authors": [
        "Munief Hassan Tahir",
        "Sana Shams",
        "Layba Fiaz",
        "Farah Adeeba",
        "Sarmad Hussain"
      ],
      "abstract": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,\nBloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across\n17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and\ntheir performance against state-of-the-art (SOTA) models, has been compared and\nanalyzed. Our experiments show that SOTA models currently outperform\nencoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.\nHowever, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can\ndeduce that with improved language coverage, LLMs can surpass these SOTA\nmodels. Our results emphasize that models with fewer parameters but richer\nlanguage-specific data, like Llama 3.1-8B, often outperform larger models with\nlower language diversity, such as GPT-3.5, in several tasks.",
      "tldr_zh": "本研究评估了7个预训练大型语言模型（LLMs），包括GPT-3.5-turbo、Llama 2-7B-Chat和Llama 3.1-8B等，在乌尔都语自然语言处理（NLP）任务上的性能，使用22个数据集和17个任务进行零样本（zero-shot）基准测试，总共涉及13.8小时的语音数据。结果显示，现有的最先进（SOTA）模型在大多数乌尔都语NLP任务中优于编码器-解码器模型。相比之下，Llama 3.1-8B凭借更好的语言覆盖超越了其前身Llama 2-7B-Chat，并证明参数较少但语言特定数据更丰富的模型（如Llama 3.1-8B）在多个任务中表现优于参数较多但语言多样性较低的模型（如GPT-3.5）。这项工作强调了改进语言覆盖对提升LLMs性能的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15453v2",
      "published_date": "2024-05-24 11:30:37 UTC",
      "updated_date": "2024-12-31 09:13:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:36:07.538800"
    },
    {
      "arxiv_id": "2405.15452v2",
      "title": "Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top",
      "title_zh": "翻译失败",
      "authors": [
        "Keyuan Cheng",
        "Muhammad Asif Ali",
        "Shu Yang",
        "Gang Lin",
        "Yuxuan Zhai",
        "Haoyang Fei",
        "Ke Xu",
        "Lu Yu",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key\nchallenge in Large Language Models (LLMs). While best-performing solutions in\nthis domain use a plan and solve paradigm to split a question into\nsub-questions followed by response generation, we claim that this approach is\nsub-optimal as it fails for hard to decompose questions, and it does not\nexplicitly cater to correlated knowledge updates resulting as a consequence of\nknowledge edits. This has a detrimental impact on the overall consistency of\nthe updated knowledge. To address these issues, in this paper, we propose a\nnovel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a\ncherry on the top for augmenting the performance of all existing MQA methods\nunder KE. Specifically, RULE-KE leverages rule discovery to discover a set of\nlogical rules. Then, it uses these discovered rules to update knowledge about\nfacts highly correlated with the edit. Experimental evaluation using existing\nand newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment\nboth performances of parameter-based and memory-based solutions up to 92% and\n112.9%, respectively.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 中的 Multi-hop Question Answering (MQA) 在 Knowledge Editing (KE) 下的挑战，指出现有“计划和解决”范式存在问题，如难以分解问题和知识更新不一致。论文提出 RULE-KE 框架，通过 rule discovery 发现逻辑规则，并利用这些规则更新与编辑高度相关的知识，以提升整体知识一致性。实验结果显示，RULE-KE 可将基于参数和基于内存的解决方案性能分别提高高达 92% 和 112.9%，并在现有和新数据集 (RKE-EVAL) 上验证了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.15452v2",
      "published_date": "2024-05-24 11:30:00 UTC",
      "updated_date": "2024-05-27 11:24:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:36:31.056965"
    },
    {
      "arxiv_id": "2405.15446v1",
      "title": "Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Drago Plecko",
        "Elias Bareinboim"
      ],
      "abstract": "Investigating fairness and equity of automated systems has become a critical\nfield of inquiry. Most of the literature in fair machine learning focuses on\ndefining and achieving fairness criteria in the context of prediction, while\nnot explicitly focusing on how these predictions may be used later on in the\npipeline. For instance, if commonly used criteria, such as independence or\nsufficiency, are satisfied for a prediction score $S$ used for binary\nclassification, they need not be satisfied after an application of a simple\nthresholding operation on $S$ (as commonly used in practice). In this paper, we\ntake an important step to address this issue in numerous statistical and causal\nnotions of fairness. We introduce the notion of a margin complement, which\nmeasures how much a prediction score $S$ changes due to a thresholding\noperation. We then demonstrate that the marginal difference in the optimal 0/1\npredictor $\\widehat Y$ between groups, written $P(\\hat y \\mid x_1) - P(\\hat y\n\\mid x_0)$, can be causally decomposed into the influences of $X$ on the\n$L_2$-optimal prediction score $S$ and the influences of $X$ on the margin\ncomplement $M$, along different causal pathways (direct, indirect, spurious).\nWe then show that under suitable causal assumptions, the influences of $X$ on\nthe prediction score $S$ are equal to the influences of $X$ on the true outcome\n$Y$. This yields a new decomposition of the disparity in the predictor\n$\\widehat Y$ that allows us to disentangle causal differences inherited from\nthe true outcome $Y$ that exists in the real world vs. those coming from the\noptimization procedure itself. This observation highlights the need for more\nregulatory oversight due to the potential for bias amplification, and to\naddress this issue we introduce new notions of weak and strong business\nnecessity, together with an algorithm for assessing whether these notions are\nsatisfied.",
      "tldr_zh": "这篇论文从因果视角探讨了预测和决策过程中的偏差放大问题，指出现有公平机器学习标准（如independence或sufficiency）在阈值操作后可能失效，导致群体间预测差异。作者引入了margin complement的概念，并通过因果分解分析，将预测差异分解为变量X对$L_2$-optimal预测分数S的影响和对margin complement M的影响，从而区分真实结果Y的固有偏差与优化过程引入的偏差。最终，论文提出weak and strong business necessity的新概念，并提供一个评估算法，以强调监管必要性并缓解偏差放大风险。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15446v1",
      "published_date": "2024-05-24 11:22:19 UTC",
      "updated_date": "2024-05-24 11:22:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:36:32.227792"
    },
    {
      "arxiv_id": "2405.15444v4",
      "title": "HINT: Hypernetwork Approach to Training Weight Interval Regions in Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Patryk Krukowski",
        "Anna Bielawska",
        "Kamil Książek",
        "Paweł Wawrzyński",
        "Paweł Batorski",
        "Przemysław Spurek"
      ],
      "abstract": "Recently, a new Continual Learning (CL) paradigm was presented to control\ncatastrophic forgetting, called Interval Continual Learning (InterContiNet),\nwhich relies on enforcing interval constraints on the neural network parameter\nspace. Unfortunately, InterContiNet training is challenging due to the high\ndimensionality of the weight space, making intervals difficult to manage. To\naddress this issue, we introduce HINT, a technique that employs interval\narithmetic within the embedding space and utilizes a hypernetwork to map these\nintervals to the target network parameter space. We train interval embeddings\nfor consecutive tasks and train a hypernetwork to transform these embeddings\ninto weights of the target network. An embedding for a given task is trained\nalong with the hypernetwork, preserving the response of the target network for\nthe previous task embeddings. Interval arithmetic works with a more manageable,\nlower-dimensional embedding space rather than directly preparing intervals in a\nhigh-dimensional weight space. Our model allows faster and more efficient\ntraining. Furthermore, HINT maintains the guarantee of not forgetting. At the\nend of training, we can choose one universal embedding to produce a single\nnetwork dedicated to all tasks. In such a framework, hypernetwork is used only\nfor training and, finally, we can utilize one set of weights. HINT obtains\nsignificantly better results than InterContiNet and gives SOTA results on\nseveral benchmarks.",
      "tldr_zh": "这篇论文提出了 HINT，一种基于 hypernetwork 的方法，用于改进 Continual Learning 中的 Interval Continual Learning (InterContiNet)，以解决权重空间高维度导致的训练挑战。HINT 通过在嵌入空间应用 interval arithmetic，并利用 hypernetwork 将这些区间映射到目标网络参数空间，从而实现更高效的训练，同时保持 catastrophic forgetting 的控制。最终，HINT 允许选择一个通用嵌入产生单一网络，并在多个基准上取得了比 InterContiNet 显著更好的 SOTA 结果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15444v4",
      "published_date": "2024-05-24 11:20:41 UTC",
      "updated_date": "2025-05-06 11:52:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:36:45.471346"
    },
    {
      "arxiv_id": "2405.15443v2",
      "title": "Fairness-Accuracy Trade-Offs: A Causal Perspective",
      "title_zh": "公平性与准确性的权衡：一个因果视角",
      "authors": [
        "Drago Plecko",
        "Elias Bareinboim"
      ],
      "abstract": "Systems based on machine learning may exhibit discriminatory behavior based\non sensitive characteristics such as gender, sex, religion, or race. In light\nof this, various notions of fairness and methods to quantify discrimination\nwere proposed, leading to the development of numerous approaches for\nconstructing fair predictors. At the same time, imposing fairness constraints\nmay decrease the utility of the decision-maker, highlighting a tension between\nfairness and utility. This tension is also recognized in legal frameworks, for\ninstance in the disparate impact doctrine of Title VII of the Civil Rights Act\nof 1964 -- in which specific attention is given to considerations of business\nnecessity -- possibly allowing the usage of proxy variables associated with the\nsensitive attribute in case a high-enough utility cannot be achieved without\nthem. In this work, we analyze the tension between fairness and accuracy from a\ncausal lens for the first time. We introduce the notion of a path-specific\nexcess loss (PSEL) that captures how much the predictor's loss increases when a\ncausal fairness constraint is enforced. We then show that the total excess loss\n(TEL), defined as the difference between the loss of predictor fair along all\ncausal pathways vs. an unconstrained predictor, can be decomposed into a sum of\nmore local PSELs. At the same time, enforcing a causal constraint often reduces\nthe disparity between demographic groups. Thus, we introduce a quantity that\nsummarizes the fairness-utility trade-off, called the causal fairness/utility\nratio, defined as the ratio of the reduction in discrimination vs. the excess\nloss from constraining a causal pathway. This quantity is suitable for\ncomparing the fairness-utility trade-off across causal pathways. Finally, as\nour approach requires causally-constrained fair predictors, we introduce a new\nneural approach for causally-constrained fair learning.",
      "tldr_zh": "这篇论文从因果视角首次分析了机器学习中公平性和准确性（效用）的权衡，探讨了强制公平约束可能导致决策效用下降的问题。作者引入了路径特定超额损失 (PSEL) 和总超额损失 (TEL) 等概念，将TEL分解为局部PSEL的和，以量化因果公平约束对预测器损失的影响。同时，定义了因果公平/效用比率，用于比较不同因果路径中减少歧视与增加损失的trade-off。最后，论文提出了一种新的神经网络方法，支持因果约束的公平学习。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15443v2",
      "published_date": "2024-05-24 11:19:52 UTC",
      "updated_date": "2024-12-20 09:47:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:36:59.320487"
    },
    {
      "arxiv_id": "2405.15439v1",
      "title": "Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Zichen Geng",
        "Caren Han",
        "Zeeshan Hayder",
        "Jian Liu",
        "Mubarak Shah",
        "Ajmal Mian"
      ],
      "abstract": "Text-driven human motion generation is an emerging task in animation and\nhumanoid robot design. Existing algorithms directly generate the full sequence\nwhich is computationally expensive and prone to errors as it does not pay\nspecial attention to key poses, a process that has been the cornerstone of\nanimation for decades. We propose KeyMotion, that generates plausible human\nmotion sequences corresponding to input text by first generating keyframes\nfollowed by in-filling. We use a Variational Autoencoder (VAE) with\nKullback-Leibler regularization to project the keyframes into a latent space to\nreduce dimensionality and further accelerate the subsequent diffusion process.\nFor the reverse diffusion, we propose a novel Parallel Skip Transformer that\nperforms cross-modal attention between the keyframe latents and text condition.\nTo complete the motion sequence, we propose a text-guided Transformer designed\nto perform motion-in-filling, ensuring the preservation of both fidelity and\nadherence to the physical constraints of human motion. Experiments show that\nour method achieves state-of-theart results on the HumanML3D dataset\noutperforming others on all R-precision metrics and MultiModal Distance.\nKeyMotion also achieves competitive performance on the KIT dataset, achieving\nthe best results on Top3 R-precision, FID, and Diversity metrics.",
      "tldr_zh": "本文提出 KeyMotion 方法，用于文本引导的 3D 人类动作生成，通过先生成关键帧再进行填充，解决了传统直接生成序列的计算开销和错误问题。方法结合 Variational Autoencoder (VAE) 将关键帧投影到潜在空间以降低维度，并引入 Parallel Skip Transformer 进行关键帧潜在空间与文本条件的跨模态注意力；随后使用文本引导 Transformer 完成动作填充，确保动作的保真度和物理约束。实验结果显示，KeyMotion 在 HumanML3D 数据集上在所有 R-precision 指标和 MultiModal Distance 上达到最先进水平，并在 KIT 数据集上在 Top3 R-precision、FID 和 Diversity 指标上取得最佳性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15439v1",
      "published_date": "2024-05-24 11:12:37 UTC",
      "updated_date": "2024-05-24 11:12:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:37:08.883868"
    },
    {
      "arxiv_id": "2405.15436v1",
      "title": "Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance",
      "title_zh": "混合上下文",
      "authors": [
        "Candace Edwards"
      ],
      "abstract": "In higher education, accreditation is a quality assurance process, where an\ninstitution demonstrates a commitment to delivering high quality programs and\nservices to their students. For business schools nationally and internationally\nthe Association to Advance Collegiate Schools of Business (AACSB) accreditation\nis the gold standard. For a business school to receive and subsequently\nmaintain accreditation, the school must undertake a rigorous, time consuming\nreporting and peer review process, to demonstrate alignment with the AACSB\nStandards. For this project we create a hybrid context retrieval augmented\ngeneration pipeline that can assist in the documentation alignment and\nreporting process necessary for accreditation. We implement both a vector\ndatabase and knowledge graph, as knowledge stores containing both institutional\ndata and AACSB Standard data. The output of the pipeline can be used by\ninstitution stakeholders to build their accreditation report, dually grounded\nby the context from the knowledge stores. To develop our knowledge graphs we\nutilized both a manual construction process as well as an LLM Augmented\nKnowledge Graph approach. We evaluated the pipeline using the RAGAs framework\nand observed optimal performance on answer relevancy and answer correctness\nmetrics.",
      "tldr_zh": "该论文提出了一种Hybrid Context Retrieval Augmented Generation Pipeline，用于辅助高等教育机构，尤其是商学院的AACSB认证报告过程，通过整合向量数据库和LLM-Augmented Knowledge Graphs作为知识存储，处理机构数据和认证标准。管道采用手动和LLM-Augmented Knowledge Graph方法构建知识图谱，确保报告内容基于可靠的上下文。实验结果显示，使用RAGAs框架评估，该系统在答案相关性和正确性指标上表现出优异性能，为简化认证文档对齐和报告提供了高效工具。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "17 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15436v1",
      "published_date": "2024-05-24 11:05:45 UTC",
      "updated_date": "2024-05-24 11:05:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:37:19.915753"
    },
    {
      "arxiv_id": "2405.15414v1",
      "title": "Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Guo",
        "Shaohui Peng",
        "Jiaming Guo",
        "Di Huang",
        "Xishan Zhang",
        "Rui Zhang",
        "Yifan Hao",
        "Ling Li",
        "Zikang Tian",
        "Mingju Gao",
        "Yutai Li",
        "Yiming Gan",
        "Shuai Liang",
        "Zihao Zhang",
        "Zidong Du",
        "Qi Guo",
        "Xing Hu",
        "Yunji Chen"
      ],
      "abstract": "Building open agents has always been the ultimate goal in AI research, and\ncreative agents are the more enticing. Existing LLM agents excel at\nlong-horizon tasks with well-defined goals (e.g., `mine diamonds' in\nMinecraft). However, they encounter difficulties on creative tasks with open\ngoals and abstract criteria due to the inability to bridge the gap between\nthem, thus lacking feedback for self-improvement in solving the task. In this\nwork, we introduce autonomous embodied verification techniques for agents to\nfill the gap, laying the groundwork for creative tasks. Specifically, we\npropose the Luban agent target creative building tasks in Minecraft, which\nequips with two-level autonomous embodied verification inspired by human design\npractices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality\nprograms based on the abstract criteria. Extensive multi-dimensional human\nstudies and Elo ratings show that the Luban completes diverse creative building\ntasks in our proposed benchmark and outperforms other baselines ($33\\%$ to\n$100\\%$) in both visualization and pragmatism. Additional demos on the\nreal-world robotic arm show the creation potential of the Luban in the physical\nworld.",
      "tldr_zh": "本研究提出Luban代理，通过自主化身验证（autonomous embodied verification）技术，构建开放式创造性代理，以解决现有LLM agents在处理开放目标和抽象标准任务（如Minecraft中的创造性建筑）时的反馈缺失问题。主要方法包括两级验证：（1）视觉验证，利用代理合成的CAD modeling程序对3D结构进行推测；（2）实用验证，通过生成并验证环境相关功能程序来评估抽象标准。实验结果显示，Luban在多维度人类评估和Elo评分中优于基线模型（提升33%至100%），并在真实世界机器人臂演示中展现了物理世界的创造潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15414v1",
      "published_date": "2024-05-24 10:25:59 UTC",
      "updated_date": "2024-05-24 10:25:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:37:32.147759"
    },
    {
      "arxiv_id": "2405.15412v2",
      "title": "Data-driven Global Ocean Modeling for Seasonal to Decadal Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zijie Guo",
        "Pumeng Lyu",
        "Fenghua Ling",
        "Lei Bai",
        "Jing-Jia Luo",
        "Niklas Boers",
        "Toshio Yamagata",
        "Takeshi Izumo",
        "Sophie Cravatte",
        "Antonietta Capotondi",
        "Wanli Ouyang"
      ],
      "abstract": "Accurate ocean dynamics modeling is crucial for enhancing understanding of\nocean circulation, predicting climate variability, and tackling challenges\nposed by climate change. Despite improvements in traditional numerical models,\npredicting global ocean variability over multi-year scales remains challenging.\nHere, we propose ORCA-DL (Oceanic Reliable foreCAst via Deep Learning), the\nfirst data-driven 3D ocean model for seasonal to decadal prediction of global\nocean circulation. ORCA-DL accurately simulates three-dimensional ocean\ndynamics and outperforms state-of-the-art dynamical models in capturing extreme\nevents, including El Ni\\~no-Southern Oscillation and upper ocean heatwaves.\nThis demonstrates the high potential of data-driven models for efficient and\naccurate global ocean forecasting. Moreover, ORCA-DL stably emulates ocean\ndynamics at decadal timescales, demonstrating its potential even for skillful\ndecadal predictions and climate projections.",
      "tldr_zh": "这篇论文提出 ORCA-DL，一种基于 Deep Learning 的数据驱动 3D 海洋模型，用于季节到十年级的全球海洋循环预测，以解决传统数值模型在长期变异性预测上的挑战。ORCA-DL 能够准确模拟三维海洋动力学，并在捕捉极端事件如 El Niño-Southern Oscillation 和上层海洋热浪方面优于现有动态模型。实验结果显示，该模型在高效性和准确性上表现出色，并展现出在十年级预测和气候投影中的稳定潜力。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15412v2",
      "published_date": "2024-05-24 10:23:17 UTC",
      "updated_date": "2024-10-29 06:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:37:45.878833"
    },
    {
      "arxiv_id": "2407.13053v1",
      "title": "E2Vec: Feature Embedding with Temporal Information for Analyzing Student Actions in E-Book Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yuma Miyazaki",
        "Valdemar Švábenský",
        "Yuta Taniguchi",
        "Fumiya Okubo",
        "Tsubasa Minematsu",
        "Atsushi Shimada"
      ],
      "abstract": "Digital textbook (e-book) systems record student interactions with textbooks\nas a sequence of events called EventStream data. In the past, researchers\nextracted meaningful features from EventStream, and utilized them as inputs for\ndownstream tasks such as grade prediction and modeling of student behavior.\nPrevious research evaluated models that mainly used statistical-based features\nderived from EventStream logs, such as the number of operation types or access\nfrequencies. While these features are useful for providing certain insights,\nthey lack temporal information that captures fine-grained differences in\nlearning behaviors among different students. This study proposes E2Vec, a novel\nfeature representation method based on word embeddings. The proposed method\nregards operation logs and their time intervals for each student as a string\nsequence of characters and generates a student vector of learning activity\nfeatures that incorporates time information. We applied fastText to generate an\nembedding vector for each of 305 students in a dataset from two years of\ncomputer science courses. Then, we investigated the effectiveness of E2Vec in\nan at-risk detection task, demonstrating potential for generalizability and\nperformance.",
      "tldr_zh": "本文提出E2Vec，一种新型特征嵌入方法，用于分析电子书系统中的学生操作日志（EventStream），它通过整合时间信息来捕捉学生学习行为的细微差异，弥补了传统基于统计特征（如操作类型数量或访问频率）的不足。E2Vec将学生的操作日志和时间间隔视为字符串序列，并利用fastText生成包含时间信息的学习活动嵌入向量。实验在305名计算机科学课程学生的两年数据集上进行，结果表明E2Vec在风险检测任务中表现出色，具有良好的泛化性和性能潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Published in proceedings of the 17th Educational Data Mining\n  Conference (EDM 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.13053v1",
      "published_date": "2024-05-24 10:17:43 UTC",
      "updated_date": "2024-05-24 10:17:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:37:57.247220"
    },
    {
      "arxiv_id": "2405.15398v1",
      "title": "PriCE: Privacy-Preserving and Cost-Effective Scheduling for Parallelizing the Large Medical Image Processing Workflow over Hybrid Clouds",
      "title_zh": "PriCE: 隐私保护且成本有效的调度，用于",
      "authors": [
        "Yuandou Wang",
        "Neel Kanwal",
        "Kjersti Engan",
        "Chunming Rong",
        "Paola Grosso",
        "Zhiming Zhao"
      ],
      "abstract": "Running deep neural networks for large medical images is a resource-hungry\nand time-consuming task with centralized computing. Outsourcing such medical\nimage processing tasks to hybrid clouds has benefits, such as a significant\nreduction of execution time and monetary cost. However, due to privacy\nconcerns, it is still challenging to process sensitive medical images over\nclouds, which would hinder their deployment in many real-world applications. To\novercome this, we first formulate the overall optimization objectives of the\nprivacy-preserving distributed system model, i.e., minimizing the amount of\ninformation about the private data learned by the adversaries throughout the\nprocess, reducing the maximum execution time and cost under the user budget\nconstraint. We propose a novel privacy-preserving and cost-effective method\ncalled PriCE to solve this multi-objective optimization problem. We performed\nextensive simulation experiments for artifact detection tasks on medical images\nusing an ensemble of five deep convolutional neural network inferences as the\nworkflow task. Experimental results show that PriCE successfully splits a wide\nrange of input gigapixel medical images with graph-coloring-based strategies,\nyielding desired output utility and lowering the privacy risk, makespan, and\nmonetary cost under user's budget.",
      "tldr_zh": "该研究针对大型医疗图像处理任务在集中式计算中的资源密集和时间消耗问题，提出了一种隐私保护且成本有效的调度方法PriCE，用于在混合云上并行化工作流。PriCE通过多目标优化，旨在最小化对手从私有数据中获取的信息量、减少最大执行时间(makespan)和货币成本，同时满足用户预算约束。实验结果显示，在使用五种深度卷积神经网络集成进行医疗图像工件检测任务的模拟中，PriCE成功利用基于图着色的策略分割海量图像，显著降低了隐私风险、makespan和成本，同时保持了输出效用。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CV",
        "cs.DC",
        "cs.ET"
      ],
      "primary_category": "cs.CE",
      "comment": "Acccepted at Europar 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15398v1",
      "published_date": "2024-05-24 09:52:00 UTC",
      "updated_date": "2024-05-24 09:52:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:38:08.233500"
    },
    {
      "arxiv_id": "2405.15388v1",
      "title": "Language-Driven Interactive Traffic Trajectory Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Junkai Xia",
        "Chenxin Xu",
        "Qingyao Xu",
        "Chen Xie",
        "Yanfeng Wang",
        "Siheng Chen"
      ],
      "abstract": "Realistic trajectory generation with natural language control is pivotal for\nadvancing autonomous vehicle technology. However, previous methods focus on\nindividual traffic participant trajectory generation, thus failing to account\nfor the complexity of interactive traffic dynamics. In this work, we propose\nInteractTraj, the first language-driven traffic trajectory generator that can\ngenerate interactive traffic trajectories. InteractTraj interprets abstract\ntrajectory descriptions into concrete formatted interaction-aware numerical\ncodes and learns a mapping between these formatted codes and the final\ninteractive trajectories. To interpret language descriptions, we propose a\nlanguage-to-code encoder with a novel interaction-aware encoding strategy. To\nproduce interactive traffic trajectories, we propose a code-to-trajectory\ndecoder with interaction-aware feature aggregation that synergizes vehicle\ninteractions with the environmental map and the vehicle moves. Extensive\nexperiments show our method demonstrates superior performance over previous\nSoTA methods, offering a more realistic generation of interactive traffic\ntrajectories with high controllability via diverse natural language commands.\nOur code is available at https://github.com/X1a-jk/InteractTraj.git",
      "tldr_zh": "该论文提出InteractTraj，一种首创的语言驱动交互式交通轨迹生成方法，旨在解决现有方法忽略交通参与者间复杂交互的问题。InteractTraj 通过language-to-code encoder采用interaction-aware encoding策略，将抽象的语言描述转化为具体的交互感知数字代码；随后，code-to-trajectory decoder利用interaction-aware feature aggregation整合车辆交互、环境地图和运动信息，生成真实的交互轨迹。实验结果显示，该方法在多样化自然语言命令下比现有SoTA方法性能更优，提升了轨迹生成的真实性和可控性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15388v1",
      "published_date": "2024-05-24 09:38:36 UTC",
      "updated_date": "2024-05-24 09:38:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:38:24.701818"
    },
    {
      "arxiv_id": "2405.15383v2",
      "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
      "title_zh": "使用蒙特卡洛树搜索指导的大型语言模型生成代码世界模型",
      "authors": [
        "Nicola Dainese",
        "Matteo Merler",
        "Minttu Alakuijala",
        "Pekka Marttinen"
      ],
      "abstract": "In this work we consider Code World Models, world models generated by a Large\nLanguage Model (LLM) in the form of Python code for model-based Reinforcement\nLearning (RL). Calling code instead of LLMs for planning has potential to be\nmore precise, reliable, interpretable, and extremely efficient. However,\nwriting appropriate Code World Models requires the ability to understand\ncomplex instructions, to generate exact code with non-trivial logic and to\nself-debug a long program with feedback from unit tests and environment\ntrajectories. To address these challenges, we propose Generate, Improve and Fix\nwith Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for\nLLMs. To test our approach in an offline RL setting, we introduce the Code\nWorld Models Benchmark (CWMB), a suite of program synthesis and planning tasks\ncomprised of 18 diverse RL environments paired with corresponding textual\ndescriptions and curated trajectories. GIF-MCTS surpasses all baselines on the\nCWMB and two other benchmarks, and we show that the Code World Models\nsynthesized with it can be successfully used for planning, resulting in\nmodel-based RL agents with greatly improved sample efficiency and inference\nspeed.",
      "tldr_zh": "这篇论文提出了一种名为 GIF-MCTS（Generate, Improve and Fix with Monte Carlo Tree Search）的策略，利用 Large Language Models (LLMs) 生成 Code World Models，以 Python 代码形式支持模型-based Reinforcement Learning (RL)，使其更精确、可靠和高效。GIF-MCTS 通过 Monte Carlo Tree Search 指导代码的生成、改进和修复，解决了理解复杂指令、编写非平凡逻辑以及基于单元测试和环境轨迹的自调试挑战。研究引入了 Code World Models Benchmark (CWMB)，一个包含18个多样化 RL 环境的基准测试，结果显示 GIF-MCTS 超越所有基线，在规划任务中显著提升了 RL 代理的样本效率和推理速度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at NeurIPS 2024, Main Track. 11 pages in main text, 40 pages\n  including references and supplementary materials. 2 figures and 3 tables in\n  the main text, 9 figures and 12 tables when including the supplementary\n  materials. Website at https://sites.google.com/view/code-world-models/home",
      "pdf_url": "http://arxiv.org/pdf/2405.15383v2",
      "published_date": "2024-05-24 09:31:26 UTC",
      "updated_date": "2024-10-30 14:19:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:38:33.758014"
    },
    {
      "arxiv_id": "2407.00033v1",
      "title": "Uncovering cognitive taskonomy through transfer learning in masked autoencoder-based fMRI reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Youzhi Qu",
        "Junfeng Xia",
        "Xinyao Jian",
        "Wendu Li",
        "Kaining Peng",
        "Zhichao Liang",
        "Haiyan Wu",
        "Quanying Liu"
      ],
      "abstract": "Data reconstruction is a widely used pre-training task to learn the\ngeneralized features for many downstream tasks. Although reconstruction tasks\nhave been applied to neural signal completion and denoising, neural signal\nreconstruction is less studied. Here, we employ the masked autoencoder (MAE)\nmodel to reconstruct functional magnetic resonance imaging (fMRI) data, and\nutilize a transfer learning framework to obtain the cognitive taskonomy, a\nmatrix to quantify the similarity between cognitive tasks. Our experimental\nresults demonstrate that the MAE model effectively captures the temporal\ndynamics patterns and interactions within the brain regions, enabling robust\ncross-subject fMRI signal reconstruction. The cognitive taskonomy derived from\nthe transfer learning framework reveals the relationships among cognitive\ntasks, highlighting subtask correlations within motor tasks and similarities\nbetween emotion, social, and gambling tasks. Our study suggests that the fMRI\nreconstruction with MAE model can uncover the latent representation and the\nobtained taskonomy offers guidance for selecting source tasks in neural\ndecoding tasks for improving the decoding performance on target tasks.",
      "tldr_zh": "这篇论文使用Masked Autoencoder (MAE) 模型重建 functional magnetic resonance imaging (fMRI) 数据，并通过 transfer learning 框架获取 cognitive taskonomy，这是一个量化认知任务之间相似性的矩阵。实验结果表明，MAE 模型能有效捕获脑区的 temporal dynamics patterns 和互动，支持跨主体 fMRI 信号的鲁棒重建。获得的 cognitive taskonomy 揭示了运动任务中的子任务相关性，以及情绪、社会和赌博任务之间的相似性。该研究证明，fMRI 重建有助于揭示潜在神经表示，并为神经解码任务提供指导，以优化源任务选择并提升目标任务的解码性能。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00033v1",
      "published_date": "2024-05-24 09:29:16 UTC",
      "updated_date": "2024-05-24 09:29:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:38:46.145809"
    },
    {
      "arxiv_id": "2405.15380v1",
      "title": "Full-stack evaluation of Machine Learning inference workloads for RISC-V systems",
      "title_zh": "翻译失败",
      "authors": [
        "Debjyoti Bhattacharjee",
        "Anmol",
        "Tommaso Marinelli",
        "Karan Pathak",
        "Peter Kourzanov"
      ],
      "abstract": "Architectural simulators hold a vital role in RISC-V research, providing a\ncrucial platform for workload evaluation without the need for costly physical\nprototypes. They serve as a dynamic environment for exploring innovative\narchitectural concepts, enabling swift iteration and thorough analysis of\nperformance metrics. As deep learning algorithms become increasingly pervasive,\nit is essential to benchmark new architectures with machine learning workloads.\nThe diverse computational kernels used in deep learning algorithms highlight\nthe necessity for a comprehensive compilation toolchain to map to target\nhardware platforms. This study evaluates the performance of a wide array of\nmachine learning workloads on RISC-V architectures using gem5, an open-source\narchitectural simulator. Leveraging an open-source compilation toolchain based\non Multi-Level Intermediate Representation (MLIR), the research presents\nbenchmarking results specifically focused on deep learning inference workloads.\nAdditionally, the study sheds light on current limitations of gem5 when\nsimulating RISC-V architectures, offering insights for future development and\nrefinement.",
      "tldr_zh": "这篇论文评估了机器学习推理工作负载在 RISC-V 系统的全栈性能，使用开源建筑模拟器 gem5 作为评估平台，避免了昂贵的物理原型需求。研究采用基于 Multi-Level Intermediate Representation (MLIR) 的开源编译工具链，针对深度学习算法的多样化计算内核进行基准测试。结果显示了 RISC-V 架构在处理这些工作负载时的性能指标，并揭示了 gem5 在模拟 RISC-V 时存在的限制，为未来架构优化和开发提供了宝贵见解。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "RISC-V Summit Europe 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15380v1",
      "published_date": "2024-05-24 09:24:46 UTC",
      "updated_date": "2024-05-24 09:24:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:38:56.828961"
    },
    {
      "arxiv_id": "2405.15375v1",
      "title": "A Planet Scale Spatial-Temporal Knowledge Graph Based On OpenStreetMap And H3 Grid",
      "title_zh": "基于 OpenStreetMap 和 H3 Grid 的行星规模空间-时间知识图谱",
      "authors": [
        "Martin Böckling",
        "Heiko Paulheim",
        "Sarah Detzler"
      ],
      "abstract": "Geospatial data plays a central role in modeling our world, for which\nOpenStreetMap (OSM) provides a rich source of such data. While often spatial\ndata is represented in a tabular format, a graph based representation provides\nthe possibility to interconnect entities which would have been separated in a\ntabular representation. We propose in our paper a framework which supports a\nplanet scale transformation of OpenStreetMap data into a Spatial Temporal\nKnowledge Graph. In addition to OpenStreetMap data, we align the different\nOpenStreetMap geometries on individual h3 grid cells. We compare our\nconstructed spatial knowledge graph to other spatial knowledge graphs and\noutline our contribution in this paper. As a basis for our computation, we use\nApache Sedona as a computational framework for our Spatial Temporal Knowledge\nGraph construction",
      "tldr_zh": "该论文提出一个框架，将 OpenStreetMap (OSM) 数据转化为行星规模的空间-时间知识图 (Spatial Temporal Knowledge Graph)，以图表示形式实现实体间的互联，从而更好地建模地理空间数据。该框架通过将 OSM 几何体对齐到 H3 Grid 单元上，并使用 Apache Sedona 作为计算基础，实现了高效的数据处理和转换。相比其他空间知识图，该方法突出了其在全球规模上的优势和贡献。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 2 figures, GeoLD2024: 6th Geospatial Linked Data Workshop,\n  May 26, 2024, Hersonissos, Greece",
      "pdf_url": "http://arxiv.org/pdf/2405.15375v1",
      "published_date": "2024-05-24 09:22:20 UTC",
      "updated_date": "2024-05-24 09:22:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:39:08.070169"
    },
    {
      "arxiv_id": "2405.15374v1",
      "title": "Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph",
      "title_zh": "利用大型语言模型在学术知识图谱中进行语义查询处理",
      "authors": [
        "Runsong Jia",
        "Bowen Zhang",
        "Sergio J. Rodríguez Méndez",
        "Pouya G. Omran"
      ],
      "abstract": "The proposed research aims to develop an innovative semantic query processing\nsystem that enables users to obtain comprehensive information about research\nworks produced by Computer Science (CS) researchers at the Australian National\nUniversity (ANU). The system integrates Large Language Models (LLMs) with the\nANU Scholarly Knowledge Graph (ASKG), a structured repository of all\nresearch-related artifacts produced at ANU in the CS field. Each artifact and\nits parts are represented as textual nodes stored in a Knowledge Graph (KG).\n  To address the limitations of traditional scholarly KG construction and\nutilization methods, which often fail to capture fine-grained details, we\npropose a novel framework that integrates the Deep Document Model (DDM) for\ncomprehensive document representation and the KG-enhanced Query Processing\n(KGQP) for optimized complex query handling. DDM enables a fine-grained\nrepresentation of the hierarchical structure and semantic relationships within\nacademic papers, while KGQP leverages the KG structure to improve query\naccuracy and efficiency with LLMs.\n  By combining the ASKG with LLMs, our approach enhances knowledge utilization\nand natural language understanding capabilities. The proposed system employs an\nautomatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from\nthe ASKG. Initial experiments demonstrate that our framework is superior to\nbaseline methods in terms of accuracy retrieval and query efficiency.\n  We showcase the practical application of our framework in academic research\nscenarios, highlighting its potential to revolutionize scholarly knowledge\nmanagement and discovery. This work empowers researchers to acquire and utilize\nknowledge from documents more effectively and provides a foundation for\ndeveloping precise and reliable interactions with LLMs.",
      "tldr_zh": "本研究提出了一种创新的语义查询处理系统，利用 Large Language Models (LLMs) 与 ANU Scholarly Knowledge Graph (ASKG) 整合，旨在帮助用户获取澳大利亚国立大学 (ANU) 计算机科学 (CS) 领域的研究作品信息。系统采用 Deep Document Model (DDM) 来实现学术论文的细粒度表示，包括层次结构和语义关系，并通过 KG-enhanced Query Processing (KGQP) 优化复杂查询的准确性和效率。框架还引入自动 LLM-SPARQL 融合技术，从 ASKG 中检索相关事实和文本节点，显著提升了知识利用和自然语言理解能力。初步实验显示，该系统在准确检索和查询效率上优于基线方法，并展示了在学术研究场景中的实际应用潜力，有望革新学术知识管理和发现过程。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "H.3.3; I.2.4; I.7.5; I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "for the associated repository, see http://w3id.org/kgcp/KGQP",
      "pdf_url": "http://arxiv.org/pdf/2405.15374v1",
      "published_date": "2024-05-24 09:19:45 UTC",
      "updated_date": "2024-05-24 09:19:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:39:22.336801"
    },
    {
      "arxiv_id": "2405.15835v1",
      "title": "Analyzing the Impact of Climate Change With Major Emphasis on Pollution: A Comparative Study of ML and Statistical Models in Time Series Data",
      "title_zh": "翻译失败",
      "authors": [
        "Anurag Mishra",
        "Ronen Gold",
        "Sanjeev Vijayakumar"
      ],
      "abstract": "Industrial operations have grown exponentially over the last century, driving\nadvancements in energy utilization through vehicles and machinery.This growth\nhas significant environmental implications, necessitating the use of\nsophisticated technology to monitor and analyze climate data.The surge in\nindustrial activities presents a complex challenge in forecasting its diverse\nenvironmental impacts, which vary greatly across different regions.Aim to\nunderstand these dynamics more deeply to predict and mitigate the environmental\nimpacts of industrial activities.",
      "tldr_zh": "这篇论文分析了气候变化的影响，特别是对污染的重点关注，通过比较机器学习(ML)和统计模型在时间序列数据上的应用。研究背景在于工业活动指数级增长带来的环境挑战，如能源利用增加和区域差异，需要先进技术进行监测和预测。论文旨在通过这一比较研究，更深入地理解工业活动的多样化环境影响，从而为预测和缓解策略提供指导。",
      "categories": [
        "stat.AP",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "stat.AP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15835v1",
      "published_date": "2024-05-24 09:18:17 UTC",
      "updated_date": "2024-05-24 09:18:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:39:32.165879"
    },
    {
      "arxiv_id": "2405.15373v1",
      "title": "Autonomous Quilt Spreading for Caregiving Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchun Guo",
        "Zhiqing Lu",
        "Yanling Zhou",
        "Xin Jiang"
      ],
      "abstract": "In this work, we propose a novel strategy to ensure infants, who\ninadvertently displace their quilts during sleep, are promptly and accurately\nre-covered. Our approach is formulated into two subsequent steps: interference\nresolution and quilt spreading. By leveraging the DWPose human skeletal\ndetection and the Segment Anything instance segmentation models, the proposed\nmethod can accurately recognize the states of the infant and the quilt over\nher, which involves addressing the interferences resulted from an infant's\nlimbs laid on part of the quilt. Building upon prior research, the EM*D deep\nlearning model is employed to forecast quilt state transitions before and after\nquilt spreading actions. To improve the sensitivity of the network in\ndistinguishing state variation of the handled quilt, we introduce an enhanced\nloss function that translates the voxelized quilt state into a more\nrepresentative one. Both simulation and real-world experiments validate the\nefficacy of our method, in spreading and recover a quilt over an infant.",
      "tldr_zh": "该研究提出了一种自动被子铺展策略，针对照顾机器人，确保婴儿在睡眠中移开被子后及时准确覆盖。方法分为两个步骤：干扰解决和被子铺展，利用DWPose人体骨骼检测和Segment Anything实例分割模型识别婴儿及被子状态，同时处理婴儿肢体干扰；此外，采用EM*D深度学习模型预测被子状态变化，并引入增强损失函数提升网络对状态变异的敏感性。模拟和真实实验验证了该方法的有效性，在被子铺展和覆盖婴儿方面表现出色。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15373v1",
      "published_date": "2024-05-24 09:11:29 UTC",
      "updated_date": "2024-05-24 09:11:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:39:43.753950"
    },
    {
      "arxiv_id": "2405.15369v1",
      "title": "Cross-Domain Policy Adaptation by Capturing Representation Mismatch",
      "title_zh": "翻译失败",
      "authors": [
        "Jiafei Lyu",
        "Chenjia Bai",
        "Jingwen Yang",
        "Zongqing Lu",
        "Xiu Li"
      ],
      "abstract": "It is vital to learn effective policies that can be transferred to different\ndomains with dynamics discrepancies in reinforcement learning (RL). In this\npaper, we consider dynamics adaptation settings where there exists dynamics\nmismatch between the source domain and the target domain, and one can get\naccess to sufficient source domain data, while can only have limited\ninteractions with the target domain. Existing methods address this problem by\nlearning domain classifiers, performing data filtering from a value discrepancy\nperspective, etc. Instead, we tackle this challenge from a decoupled\nrepresentation learning perspective. We perform representation learning only in\nthe target domain and measure the representation deviations on the transitions\nfrom the source domain, which we show can be a signal of dynamics mismatch. We\nalso show that representation deviation upper bounds performance difference of\na given policy in the source domain and target domain, which motivates us to\nadopt representation deviation as a reward penalty. The produced\nrepresentations are not involved in either policy or value function, but only\nserve as a reward penalizer. We conduct extensive experiments on environments\nwith kinematic and morphology mismatch, and the results show that our method\nexhibits strong performance on many tasks. Our code is publicly available at\nhttps://github.com/dmksjfl/PAR.",
      "tldr_zh": "这篇论文针对强化学习（RL）中的跨域策略适应问题，提出了一种通过捕获表示不匹配（representation mismatch）的方法，处理源域和目标域动态差异的挑战。作者在目标域进行解耦的表示学习（representation learning），并使用源域转换的表示偏差作为动态不匹配的信号，同时证明表示偏差可上界策略性能差异，并将其作为奖励惩罚。实验结果显示，该方法在运动学和形态不匹配的环境中表现出强有力的性能提升，代码已在GitHub上公开。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15369v1",
      "published_date": "2024-05-24 09:06:12 UTC",
      "updated_date": "2024-05-24 09:06:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:40:06.912994"
    },
    {
      "arxiv_id": "2405.15346v1",
      "title": "BiSup: Bidirectional Quantization Error Suppression for Large Language Models",
      "title_zh": "BiSup：大语言模型的双向量化错误抑制技术",
      "authors": [
        "Minghui Zou",
        "Ronghui Guo",
        "Sai Zhang",
        "Xiaowang Zhang",
        "Zhiyong Feng"
      ],
      "abstract": "As the size and context length of Large Language Models (LLMs) grow,\nweight-activation quantization has emerged as a crucial technique for efficient\ndeployment of LLMs. Compared to weight-only quantization, weight-activation\nquantization presents greater challenges due to the presence of outliers in\nactivations. Existing methods have made significant progress by exploring\nmixed-precision quantization and outlier suppression. However, these methods\nprimarily focus on optimizing the results of single matrix multiplication,\nneglecting the bidirectional propagation of quantization errors in LLMs.\nSpecifically, errors accumulate vertically within the same token through\nlayers, and diffuse horizontally across different tokens due to self-attention\nmechanisms. To address this issue, we introduce BiSup, a Bidirectional\nquantization error Suppression method. By constructing appropriate optimizable\nparameter spaces, BiSup utilizes a small amount of data for quantization-aware\nparameter-efficient fine-tuning to suppress the error vertical accumulation.\nBesides, BiSup employs prompt mixed-precision quantization strategy, which\npreserves high precision for the key-value cache of system prompts, to mitigate\nthe error horizontal diffusion. Extensive experiments on Llama and Qwen\nfamilies demonstrate that BiSup can improve performance over two\nstate-of-the-art methods (the average WikiText2 perplexity decreases from 13.26\nto 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128\nconfiguration), further facilitating the practical applications of low-bit\nweight-activation quantization.",
      "tldr_zh": "该论文提出 BiSup，一种双向量化错误抑制方法，用于优化 Large Language Models (LLMs) 的 weight-activation 量化，解决现有方法忽略的量化错误双向传播问题，包括层间垂直积累和自注意力机制下的水平扩散。BiSup 通过构建可优化参数空间进行量化感知的参数高效微调来抑制垂直错误积累，并采用 prompt mixed-precision 量化策略为系统提示的 key-value cache 保留高精度，以缓解水平错误扩散。在 Llama 和 Qwen 系列模型的实验中，BiSup 显著提升了性能，例如在 W3A3-g128 配置下，WikiText2 的 perplexity 由 Atom 的 13.26 降至 9.41，以及由 QuaRot 的 14.33 降至 7.85，从而推动低位 weight-activation 量化的实际应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15346v1",
      "published_date": "2024-05-24 08:39:27 UTC",
      "updated_date": "2024-05-24 08:39:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:40:10.636949"
    },
    {
      "arxiv_id": "2405.15832v1",
      "title": "DETECTA 2.0: Research into non-intrusive methodologies supported by Industry 4.0 enabling technologies for predictive and cyber-secure maintenance in SMEs",
      "title_zh": "翻译失败",
      "authors": [
        "Álvaro Huertas-García",
        "Javier Muñoz",
        "Enrique De Miguel Ambite",
        "Marcos Avilés Camarmas",
        "José Félix Ovejero"
      ],
      "abstract": "The integration of predictive maintenance and cybersecurity represents a\ntransformative advancement for small and medium-sized enterprises (SMEs)\noperating within the Industry 4.0 paradigm. Despite their economic importance,\nSMEs often face significant challenges in adopting advanced technologies due to\nresource constraints and knowledge gaps. The DETECTA 2.0 project addresses\nthese hurdles by developing an innovative system that harmonizes real-time\nanomaly detection, sophisticated analytics, and predictive forecasting\ncapabilities.\n  The system employs a semi-supervised methodology, combining unsupervised\nanomaly detection with supervised learning techniques. This approach enables\nmore agile and cost-effective development of AI detection systems,\nsignificantly reducing the time required for manual case review.\n  At the core lies a Digital Twin interface, providing intuitive real-time\nvisualizations of machine states and detected anomalies. Leveraging\ncutting-edge AI engines, the system intelligently categorizes anomalies based\non observed patterns, differentiating between technical errors and potential\ncybersecurity incidents. This discernment is fortified by detailed analytics,\nincluding certainty levels that enhance alert reliability and minimize false\npositives.\n  The predictive engine uses advanced time series algorithms like N-HiTS to\nforecast future machine utilization trends. This proactive approach optimizes\nmaintenance planning, enhances cybersecurity measures, and minimizes unplanned\ndowntimes despite variable production processes.\n  With its modular architecture enabling seamless integration across industrial\nsetups and low implementation costs, DETECTA 2.0 presents an attractive\nsolution for SMEs to strengthen their predictive maintenance and cybersecurity\nstrategies.",
      "tldr_zh": "DETECTA 2.0 项目针对中小企业(SMEs)在 Industry 4.0 环境中面临的资源限制和知识缺口，开发了一个创新系统，整合预测维护和网络安全功能。该系统采用半监督方法，结合无监督异常检测和监督学习，通过 Digital Twin 接口实现实时机器状态可视化和 AI 引擎对异常的智能分类，包括区分技术错误与潜在网络安全事件，并使用 N-HiTS 等时间序列算法进行预测性趋势分析。最终，该系统以模块化架构和低实施成本，帮助 SMEs 优化维护规划、减少停机时间并提升网络安全可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15832v1",
      "published_date": "2024-05-24 08:38:38 UTC",
      "updated_date": "2024-05-24 08:38:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:40:23.579637"
    },
    {
      "arxiv_id": "2405.15341v2",
      "title": "V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Abdur Rahman",
        "Rajat Chawla",
        "Muskaan Kumar",
        "Arkajit Datta",
        "Adarsh Jha",
        "Mukunda NS",
        "Ishaan Bhola"
      ],
      "abstract": "In the rapidly evolving landscape of AI research and application, Multimodal\nLarge Language Models (MLLMs) have emerged as a transformative force, adept at\ninterpreting and integrating information from diverse modalities such as text,\nimages, and Graphical User Interfaces (GUIs). Despite these advancements, the\nnuanced interaction and understanding of GUIs pose a significant challenge,\nlimiting the potential of existing models to enhance automation levels. To\nbridge this gap, this paper presents V-Zen, an innovative Multimodal Large\nLanguage Model (MLLM) meticulously crafted to revolutionise the domain of GUI\nunderstanding and grounding. Equipped with dual-resolution image encoders,\nV-Zen establishes new benchmarks in efficient grounding and next-action\nprediction, thereby laying the groundwork for self-operating computer systems.\nComplementing V-Zen is the GUIDE dataset, an extensive collection of real-world\nGUI elements and task-based sequences, serving as a catalyst for specialised\nfine-tuning. The successful integration of V-Zen and GUIDE marks the dawn of a\nnew era in multimodal AI research, opening the door to intelligent, autonomous\ncomputing experiences. This paper extends an invitation to the research\ncommunity to join this exciting journey, shaping the future of GUI automation.\nIn the spirit of open science, our code, data, and model will be made publicly\navailable, paving the way for multimodal dialogue scenarios with intricate and\nprecise interactions.",
      "tldr_zh": "该论文介绍了 V-Zen，一种创新的 Multimodal LLM，旨在提升图形用户界面 (GUI) 的理解和精确 grounding，解决现有模型在处理 GUI 交互方面的挑战。V-Zen 采用双分辨率图像编码器，实现高效的 grounding 和下一个动作预测，在自主计算系统中设立新基准。同时，论文提出 GUIDE 数据集，这是一个包含真实世界 GUI 元素和任务序列的集合，用于专门的微调训练。最终，V-Zen 的整合推动了多模态 AI 研究的发展，并通过开源代码、数据和模型，促进智能自主计算的未来探索。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.15341v2",
      "published_date": "2024-05-24 08:21:45 UTC",
      "updated_date": "2024-07-21 07:34:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:40:36.215559"
    },
    {
      "arxiv_id": "2405.15831v1",
      "title": "Transmission Interface Power Flow Adjustment: A Deep Reinforcement Learning Approach based on Multi-task Attribution Map",
      "title_zh": "输电接口功率流调整：基于多任务归因图的深度强化",
      "authors": [
        "Shunyu Liu",
        "Wei Luo",
        "Yanzhen Zhou",
        "Kaixuan Chen",
        "Quan Zhang",
        "Huating Xu",
        "Qinglai Guo",
        "Mingli Song"
      ],
      "abstract": "Transmission interface power flow adjustment is a critical measure to ensure\nthe security and economy operation of power systems. However, conventional\nmodel-based adjustment schemes are limited by the increasing variations and\nuncertainties occur in power systems, where the adjustment problems of\ndifferent transmission interfaces are often treated as several independent\ntasks, ignoring their coupling relationship and even leading to conflict\ndecisions. In this paper, we introduce a novel data-driven deep reinforcement\nlearning (DRL) approach, to handle multiple power flow adjustment tasks jointly\ninstead of learning each task from scratch. At the heart of the proposed method\nis a multi-task attribution map (MAM), which enables the DRL agent to\nexplicitly attribute each transmission interface task to different power system\nnodes with task-adaptive attention weights. Based on this MAM, the agent can\nfurther provide effective strategies to solve the multi-task adjustment problem\nwith a near-optimal operation cost. Simulation results on the IEEE 118-bus\nsystem, a realistic 300-bus system in China, and a very large European system\nwith 9241 buses demonstrate that the proposed method significantly improves the\nperformance compared with several baseline methods, and exhibits high\ninterpretability with the learnable MAM.",
      "tldr_zh": "本文提出了一种基于深度强化学习 (DRL) 的方法，用于电力系统的传输接口功率流调整问题，以解决传统模型方法忽略任务耦合关系而导致冲突决策的局限性。该方法的核心是多任务归因映射 (Multi-task Attribution Map, MAM)，它通过任务自适应注意力权重将每个传输接口任务明确归因到电力系统节点，并联合处理多个调整任务以实现接近最优的操作成本。在 IEEE 118-bus 系统、中国 300-bus 系统和欧洲 9241-bus 系统上的模拟结果表明，该方法显著优于基线方法，并展示了高解释性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted by IEEE Transactions on Power Systems",
      "pdf_url": "http://arxiv.org/pdf/2405.15831v1",
      "published_date": "2024-05-24 08:20:53 UTC",
      "updated_date": "2024-05-24 08:20:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:40:48.746653"
    },
    {
      "arxiv_id": "2406.00025v1",
      "title": "SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxing Li",
        "Chi Xu",
        "Feng Wang",
        "Isaac M von Riedemann",
        "Cong Zhang",
        "Jiangchuan Liu"
      ],
      "abstract": "Large Language Models (LLMs) have become increasingly popular, transforming a\nwide range of applications across various domains. However, the real-world\neffectiveness of their query cache systems has not been thoroughly\ninvestigated. In this work, we for the first time conducted an analysis on\nreal-world human-to-LLM interaction data, identifying key challenges in\nexisting caching solutions for LLM-based chat services. Our findings reveal\nthat current caching methods fail to leverage semantic connections, leading to\ninefficient cache performance and extra token costs. To address these issues,\nwe propose SCALM, a new cache architecture that emphasizes semantic analysis\nand identifies significant cache entries and patterns. We also detail the\nimplementations of the corresponding cache storage and eviction strategies. Our\nevaluations show that SCALM increases cache hit ratios and reduces operational\ncosts for LLMChat services. Compared with other state-of-the-art solutions in\nGPTCache, SCALM shows, on average, a relative increase of 63% in cache hit\nratio and a relative improvement of 77% in tokens savings.",
      "tldr_zh": "这项研究分析了 Large Language Models (LLMs) 在聊天服务中的查询缓存系统，基于真实的人机交互数据发现现有方法未能利用语义连接，导致缓存性能低下和额外 token 成本。  \n为解决这些问题，作者提出 SCALM 框架，该框架强调语义分析，识别关键缓存条目和模式，并设计了相应的缓存存储和驱逐策略。  \n实验评估显示，SCALM 与 GPTCache 中的其他先进解决方案相比，平均提高了 63% 的缓存命中率和 77% 的 token 节省，从而提升了 LLM 聊天服务的效率和成本效益。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00025v1",
      "published_date": "2024-05-24 08:16:22 UTC",
      "updated_date": "2024-05-24 08:16:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:40:59.725016"
    },
    {
      "arxiv_id": "2407.03133v3",
      "title": "Quantifying the Cross-sectoral Intersecting Discrepancies within Multiple Groups Using Latent Class Analysis Towards Fairness",
      "title_zh": "翻译失败",
      "authors": [
        "Yingfang Yuan",
        "Kefan Chen",
        "Mehdi Rizvi",
        "Lynne Baillie",
        "Wei Pang"
      ],
      "abstract": "The growing interest in fair AI development is evident. The ''Leave No One\nBehind'' initiative urges us to address multiple and intersecting forms of\ninequality in accessing services, resources, and opportunities, emphasising the\nsignificance of fairness in AI. This is particularly relevant as an increasing\nnumber of AI tools are applied to decision-making processes, such as resource\nallocation and service scheme development, across various sectors such as\nhealth, energy, and housing. Therefore, exploring joint inequalities in these\nsectors is significant and valuable for thoroughly understanding overall\ninequality and unfairness. This research introduces an innovative approach to\nquantify cross-sectoral intersecting discrepancies among user-defined groups\nusing latent class analysis. These discrepancies can be used to approximate\ninequality and provide valuable insights to fairness issues. We validate our\napproach using both proprietary and public datasets, including both EVENS and\nCensus 2021 (England & Wales) datasets, to examine cross-sectoral intersecting\ndiscrepancies among different ethnic groups. We also verify the reliability of\nthe quantified discrepancy by conducting a correlation analysis with a\ngovernment public metric. Our findings reveal significant discrepancies both\namong minority ethnic groups and between minority ethnic groups and\nnon-minority ethnic groups, emphasising the need for targeted interventions in\npolicy-making processes. Furthermore, we demonstrate how the proposed approach\ncan provide valuable insights into ensuring fairness in machine learning\nsystems.",
      "tldr_zh": "本研究关注公平 AI 发展，提出一种创新方法，使用 Latent Class Analysis 来量化用户定义群体（如不同民族群体）之间的跨部门交叉差异（cross-sectoral intersecting discrepancies），以评估不平等和公平性问题。研究利用 EVENS 和 Census 2021（England & Wales）数据集进行验证，并通过与政府公共指标的相关分析确认了这些差异的可靠性，结果显示少数民族群体间以及与非少数民族群体间的显著差异，强调了政策干预的必要性。该方法为确保机器学习系统的公平性提供了宝贵洞见，并支持“Leave No One Behind”倡议。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03133v3",
      "published_date": "2024-05-24 08:10:31 UTC",
      "updated_date": "2025-02-08 19:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:41:10.210912"
    },
    {
      "arxiv_id": "2405.15324v2",
      "title": "Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Jianbiao Mei",
        "Yukai Ma",
        "Xuemeng Yang",
        "Licheng Wen",
        "Xinyu Cai",
        "Xin Li",
        "Daocheng Fu",
        "Bo Zhang",
        "Pinlong Cai",
        "Min Dou",
        "Botian Shi",
        "Liang He",
        "Yong Liu",
        "Yu Qiao"
      ],
      "abstract": "Autonomous driving has advanced significantly due to sensors, machine\nlearning, and artificial intelligence improvements. However, prevailing methods\nstruggle with intricate scenarios and causal relationships, hindering\nadaptability and interpretability in varied environments. To address the above\nproblems, we introduce LeapAD, a novel paradigm for autonomous driving inspired\nby the human cognitive process. Specifically, LeapAD emulates human attention\nby selecting critical objects relevant to driving decisions, simplifying\nenvironmental interpretation, and mitigating decision-making complexities.\nAdditionally, LeapAD incorporates an innovative dual-process decision-making\nmodule, which consists of an Analytic Process (System-II) for thorough analysis\nand reasoning, along with a Heuristic Process (System-I) for swift and\nempirical processing. The Analytic Process leverages its logical reasoning to\naccumulate linguistic driving experience, which is then transferred to the\nHeuristic Process by supervised fine-tuning. Through reflection mechanisms and\na growing memory bank, LeapAD continuously improves itself from past mistakes\nin a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD\noutperforms all methods relying solely on camera input, requiring 1-2 orders of\nmagnitude less labeled data. Experiments also demonstrate that as the memory\nbank expands, the Heuristic Process with only 1.8B parameters can inherit the\nknowledge from a GPT-4 powered Analytic Process and achieve continuous\nperformance improvement. Project page: https://pjlab-adg.github.io/LeapAD.",
      "tldr_zh": "这篇论文提出 LeapAD，一种受人类认知过程启发的自动驾驶范式，旨在解决现有方法在复杂场景和因果关系处理上的适应性和可解释性问题。LeapAD 通过模拟人类注意力来选择关键对象简化环境解读，并引入双过程决策模块：Analytic Process (System-II) 负责深入分析和积累语言驾驶经验，Heuristic Process (System-I) 则处理快速经验决策，并通过监督微调继承知识。系统还采用反思机制和不断增长的记忆库，实现从过去错误中持续学习和改进。在 CARLA 的闭环测试中，LeapAD 优于仅依赖摄像头输入的基线方法，需要 1-2 数量级的更少标注数据，且其仅 1.8B 参数的 Heuristic Process 可从 GPT-4 驱动的 Analytic Process 中继承知识，实现性能持续提升。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15324v2",
      "published_date": "2024-05-24 08:07:28 UTC",
      "updated_date": "2024-10-25 16:00:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:41:26.397238"
    },
    {
      "arxiv_id": "2405.15320v1",
      "title": "Organic Data-Driven Approach for Turkish Grammatical Error Correction and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Asım Ersoy",
        "Olcay Taner Yıldız"
      ],
      "abstract": "Grammatical Error Correction has seen significant progress with the recent\nadvancements in deep learning. As those methods require huge amounts of data,\nsynthetic datasets are being built to fill this gap. Unfortunately, synthetic\ndatasets are not organic enough in some cases and even require clean data to\nstart with. Furthermore, most of the work that has been done is focused mostly\non English. In this work, we introduce a new organic data-driven approach,\nclean insertions, to build parallel Turkish Grammatical Error Correction\ndatasets from any organic data, and to clean the data used for training Large\nLanguage Models. We achieve state-of-the-art results on two Turkish Grammatical\nError Correction test sets out of the three publicly available ones. We also\nshow the effectiveness of our method on the training losses of training\nlanguage models.",
      "tldr_zh": "该研究提出了一种有机数据驱动方法“clean insertions”，用于从任何有机数据构建土耳其语Grammatical Error Correction（语法错误修正）平行数据集，同时清理Large Language Models（LLMs）的训练数据，以解决传统合成数据集的局限性。不同于依赖干净数据或主要针对英语的现有方法，该方法能有效处理土耳其语的语法错误。实验结果显示，该方法在三个公开土耳其语测试集中的两个上达到了state-of-the-art性能，并显著降低了语言模型的训练损失。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15320v1",
      "published_date": "2024-05-24 08:00:24 UTC",
      "updated_date": "2024-05-24 08:00:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:41:34.413857"
    },
    {
      "arxiv_id": "2405.15319v2",
      "title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyu Du",
        "Tongxu Luo",
        "Zihan Qiu",
        "Zeyu Huang",
        "Yikang Shen",
        "Reynold Cheng",
        "Yike Guo",
        "Jie Fu"
      ],
      "abstract": "LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at https://llm-stacking.github.io.",
      "tldr_zh": "这篇论文探讨了模型增长（model growth）方法，以提高大型语言模型（LLMs）的预训练效率，通过利用小模型加速大模型训练。研究者识别了三个关键障碍：缺乏全面评估（O1）、未测试可扩展性（O2）和缺少经验指南（O3），并将现有方法总结为四个原子增长操作符，在标准化LLM预训练设置中进行系统评估。结果显示，depthwise stacking操作符（G_stack）显著加速训练，减少损失，并在八个标准NLP benchmarks上提升性能；例如，与传统训练的7B模型相比，G_stack仅需194B tokens即可达到相同损失，实现54.6%的加速。论文还证明了G_stack的可扩展性（适用于7B LLMs和750B tokens预训练），并提供了实用经验指南，包括增长时机和增长因子，以指导一般LLM预训练。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2405.15319v2",
      "published_date": "2024-05-24 08:00:00 UTC",
      "updated_date": "2024-10-22 10:31:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:41:48.839244"
    },
    {
      "arxiv_id": "2405.15318v1",
      "title": "Are Long-LLMs A Necessity For Long-Context Tasks?",
      "title_zh": "长上下文任务是否需要 Long-LLMs？",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu",
        "Peitian Zhang",
        "Kelong Mao",
        "Yujia Zhou",
        "Xu Chen",
        "Zhicheng Dou"
      ],
      "abstract": "The learning and deployment of long-LLMs remains a challenging problem\ndespite recent progresses. In this work, we argue that the long-LLMs are not a\nnecessity to solve long-context tasks, as common long-context tasks are\nshort-context solvable, i.e. they can be solved by purely working with oracle\nshort-contexts within the long-context tasks' inputs. On top of this argument,\nwe propose a framework called LC-Boost (Long-Context Bootstrapper), which\nenables a short-LLM to address the long-context tasks in a bootstrapping\nmanner. In our framework, the short-LLM prompts itself to reason for two\ncritical decisions: 1) how to access to the appropriate part of context within\nthe input, 2) how to make effective use of the accessed context. By adaptively\naccessing and utilizing the context based on the presented tasks, LC-Boost can\nserve as a general framework to handle diversified long-context processing\nproblems. We comprehensively evaluate different types of tasks from popular\nlong-context benchmarks, where LC-Boost is able to achieve a substantially\nimproved performance with a much smaller consumption of resource.",
      "tldr_zh": "本研究质疑了长LLM（long-LLMs）是否是处理长上下文任务（long-context tasks）的必要工具，主张许多此类任务可以通过短上下文（short-contexts）解决。作者提出LC-Boost（Long-Context Bootstrapper）框架，该框架让短LLM（short-LLM）通过自引导方式处理长上下文任务，包括两个关键决策：1) 如何访问输入中的适当上下文部分，2) 如何有效利用这些上下文。LC-Boost通过自适应访问和利用上下文，适用于多种长上下文处理问题。实验结果显示，在流行长上下文基准测试的不同任务上，LC-Boost实现了显著性能提升，同时资源消耗大幅降低。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.15318v1",
      "published_date": "2024-05-24 07:59:30 UTC",
      "updated_date": "2024-05-24 07:59:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:41:59.352748"
    },
    {
      "arxiv_id": "2405.15317v3",
      "title": "NuwaTS: a Foundation Model Mending Every Incomplete Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Jinguo Cheng",
        "Chunwei Yang",
        "Wanlin Cai",
        "Yuxuan Liang",
        "Qingsong Wen",
        "Yuankai Wu"
      ],
      "abstract": "Time series imputation is critical for many real-world applications and has\nbeen widely studied. However, existing models often require specialized designs\ntailored to specific missing patterns, variables, or domains which limits their\ngeneralizability. In addition, current evaluation frameworks primarily focus on\ndomain-specific tasks and often rely on time-wise train/validation/test data\nsplits, which fail to rigorously assess a model's ability to generalize across\nunseen variables or domains. In this paper, we present \\textbf{NuwaTS}, a novel\nframework that repurposes Pre-trained Language Models (PLMs) for general time\nseries imputation. Once trained, NuwaTS can be applied to impute missing data\nacross any domain. We introduce specialized embeddings for each sub-series\npatch, capturing information about the patch, its missing data patterns, and\nits statistical characteristics. By combining contrastive learning with the\nimputation task, we train PLMs to create a versatile, one-for-all imputation\nmodel. Additionally, we employ a plug-and-play fine-tuning approach, enabling\nefficient adaptation to domain-specific tasks with minimal adjustments. To\nevaluate cross-variable and cross-domain generalization, we propose a new\nbenchmarking protocol that partitions the datasets along the variable\ndimension. Experimental results on over seventeen million time series samples\nfrom diverse domains demonstrate that NuwaTS outperforms state-of-the-art\ndomain-specific models across various datasets under the proposed benchmarking\nprotocol. Furthermore, we show that NuwaTS generalizes to other time series\ntasks, such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.",
      "tldr_zh": "该论文提出 NuwaTS，一种基于 Pre-trained Language Models (PLMs) 的通用框架，用于处理时间序列插值问题，克服了现有模型对特定缺失模式、变量或领域的依赖限制。NuwaTS 通过引入子序列补丁的专用嵌入、结合对比学习（contrastive learning）和插值任务训练，创建一个通用的模型，并支持即插即用的细调以适应特定任务。实验结果显示，该框架在超过 1700 万样本的多个数据集上优于最先进领域特定模型，并在跨变量和跨领域泛化方面表现出色，还能扩展到时间序列预测任务。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15317v3",
      "published_date": "2024-05-24 07:59:02 UTC",
      "updated_date": "2024-10-02 14:34:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:42:12.524704"
    },
    {
      "arxiv_id": "2405.15311v3",
      "title": "Retro: Reusing teacher projection head for efficient embedding distillation on Lightweight Models via Self-supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Khanh-Binh Nguyen",
        "Chae Jung Park"
      ],
      "abstract": "Self-supervised learning (SSL) is gaining attention for its ability to learn\neffective representations with large amounts of unlabeled data. Lightweight\nmodels can be distilled from larger self-supervised pre-trained models using\ncontrastive and consistency constraints. Still, the different sizes of the\nprojection heads make it challenging for students to mimic the teacher's\nembedding accurately. We propose \\textsc{Retro}, which reuses the teacher's\nprojection head for students, and our experimental results demonstrate\nsignificant improvements over the state-of-the-art on all lightweight models.\nFor instance, when training EfficientNet-B0 using ResNet-50/101/152 as\nteachers, our approach improves the linear result on ImageNet to $66.9\\%$,\n$69.3\\%$, and $69.8\\%$, respectively, with significantly fewer parameters.",
      "tldr_zh": "这篇论文提出 Retro 方法，通过重用老师的 projection head 来提升自监督学习（SSL）中轻量级模型的嵌入蒸馏效率，从而解决学生模型难以准确模仿老师嵌入的挑战。Retro 利用对比和一致性约束，从更大的预训练模型（如 ResNet-50/101/152）中蒸馏轻量级模型，例如训练 EfficientNet-B0 时，ImageNet 的线性准确率分别达到 66.9%、69.3% 和 69.8%，并显著减少了参数数量。实验结果表明，该方法在所有轻量级模型上均优于现有最先进技术，为高效的模型蒸馏提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at BMVC 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15311v3",
      "published_date": "2024-05-24 07:53:09 UTC",
      "updated_date": "2024-08-24 13:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:42:25.945707"
    },
    {
      "arxiv_id": "2405.19360v3",
      "title": "ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users",
      "title_zh": "翻译失败",
      "authors": [
        "Guanlin Li",
        "Kangjie Chen",
        "Shudong Zhang",
        "Jie Zhang",
        "Tianwei Zhang"
      ],
      "abstract": "Large-scale pre-trained generative models are taking the world by storm, due\nto their abilities in generating creative content. Meanwhile, safeguards for\nthese generative models are developed, to protect users' rights and safety,\nmost of which are designed for large language models. Existing methods\nprimarily focus on jailbreak and adversarial attacks, which mainly evaluate the\nmodel's safety under malicious prompts. Recent work found that manually crafted\nsafe prompts can unintentionally trigger unsafe generations. To further\nsystematically evaluate the safety risks of text-to-image models, we propose a\nnovel Automatic Red-Teaming framework, ART. Our method leverages both vision\nlanguage model and large language model to establish a connection between\nunsafe generations and their prompts, thereby more efficiently identifying the\nmodel's vulnerabilities. With our comprehensive experiments, we reveal the\ntoxicity of the popular open-source text-to-image models. The experiments also\nvalidate the effectiveness, adaptability, and great diversity of ART.\nAdditionally, we introduce three large-scale red-teaming datasets for studying\nthe safety risks associated with text-to-image models. Datasets and models can\nbe found in https://github.com/GuanlinLee/ART.",
      "tldr_zh": "该研究针对文本到图像模型（Text-to-Image Models）的安全风险，提出了一种自动红队测试框架ART（Automatic Red-Teaming），旨在保护良性用户免受意外不安全生成的影响。ART结合视觉语言模型（VLM）和大型语言模型（LLM），通过建立不安全生成与提示之间的联系，更高效地识别模型漏洞，并在实验中验证了其有效性、适应性和多样性。实验结果揭示了流行开源文本到图像模型的毒性问题，并引入了三个大规模红队测试数据集，以促进相关安全研究。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19360v3",
      "published_date": "2024-05-24 07:44:27 UTC",
      "updated_date": "2024-10-11 06:52:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:42:36.337128"
    },
    {
      "arxiv_id": "2405.15302v2",
      "title": "The Buffer Mechanism for Multi-Step Information Reasoning in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiwei Wang",
        "Yunji Wang",
        "Zhongwang Zhang",
        "Zhangchen Zhou",
        "Hui Jin",
        "Tianyang Hu",
        "Jiacheng Sun",
        "Zhenguo Li",
        "Yaoyu Zhang",
        "Zhi-Qin John Xu"
      ],
      "abstract": "Large language models have consistently struggled with complex reasoning\ntasks, such as mathematical problem-solving. Investigating the internal\nreasoning mechanisms of these models can help us design better model\narchitectures and training strategies, ultimately enhancing their reasoning\ncapability. In this study, we constructed a symbolic dataset to investigate the\nmechanisms by which Transformer models employ vertical thinking strategy based\non their inherent structure and horizontal thinking strategy based on Chain of\nThought to achieve multi-step reasoning. We introduced the concept of buffer\nmechanism: the model stores various information in distinct buffers and\nselectively extracts them through the query-key matrix. We proposed a random\nmatrix-based algorithm to enhance the model's reasoning ability, resulting in a\n75% reduction in the training time required for the GPT-2 model to achieve\ngeneralization capability on the PrOntoQA dataset. These findings provide new\ninsights into understanding the mechanisms of large language models.",
      "tldr_zh": "本研究探讨了大型语言模型（Large Language Models）在多步推理任务（如数学问题）上的挑战，通过构建一个符号数据集来分析Transformer模型的内部机制，包括基于内在结构的垂直思考（vertical thinking）和基于Chain of Thought的水平思考策略。研究引入了buffer mechanism的概念，即模型在不同缓冲区存储信息，并通过query-key matrix选择性提取这些信息，以提升推理能力。作者提出了一种基于随机矩阵的算法，使GPT-2模型在PrOntoQA数据集上实现泛化能力的训练时间减少75%。这些发现为设计更好的模型架构和训练策略提供了新见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15302v2",
      "published_date": "2024-05-24 07:41:26 UTC",
      "updated_date": "2024-10-15 07:26:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:42:49.345883"
    },
    {
      "arxiv_id": "2405.15294v2",
      "title": "Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Dietrich",
        "Julian Rodemann",
        "Christoph Jansen"
      ],
      "abstract": "We provide a theoretical and computational investigation of the Gamma-Maximin\nmethod with soft revision, which was recently proposed as a robust criterion\nfor pseudo-label selection (PLS) in semi-supervised learning. Opposed to\ntraditional methods for PLS we use credal sets of priors (\"generalized Bayes\")\nto represent the epistemic modeling uncertainty. These latter are then updated\nby the Gamma-Maximin method with soft revision. We eventually select\npseudo-labeled data that are most likely in light of the least favorable\ndistribution from the so updated credal set. We formalize the task of finding\noptimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision\nas an optimization problem. A concrete implementation for the class of logistic\nmodels then allows us to compare the predictive power of the method with\ncompeting approaches. It is observed that the Gamma-Maximin method with soft\nrevision can achieve very promising results, especially when the proportion of\nlabeled data is low.",
      "tldr_zh": "这篇论文探讨了在半监督学习(Semi-Supervised Learning)中，使用 Gamma-Maximin 方法结合软修正(Soft Revision)作为伪标签选择(PLS)的鲁棒标准。方法通过 credal sets of priors 表示认知建模不确定性，并基于广义贝叶斯规则(Generalized Bayes Rule)进行更新，以选择在最不利分布下最可能的伪标签数据。实验结果表明，对于 logistic 模型，该方法在标记数据比例低时，预测性能显著优于竞争方法。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH",
        "62C12 62C10",
        "I.2.6; G.3"
      ],
      "primary_category": "stat.ML",
      "comment": "Accepted at the 11th International Conference on Soft Methods in\n  Probability and Statistics (SMPS) 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15294v2",
      "published_date": "2024-05-24 07:30:45 UTC",
      "updated_date": "2024-06-04 15:28:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:43:00.428781"
    },
    {
      "arxiv_id": "2405.15292v1",
      "title": "Towards a Probabilistic Fusion Approach for Robust Battery Prognostics",
      "title_zh": "翻译失败",
      "authors": [
        "Jokin Alcibar",
        "Jose I. Aizpurua",
        "Ekhi Zugasti"
      ],
      "abstract": "Batteries are a key enabling technology for the decarbonization of transport\nand energy sectors. The safe and reliable operation of batteries is crucial for\nbattery-powered systems. In this direction, the development of accurate and\nrobust battery state-of-health prognostics models can unlock the potential of\nautonomous systems for complex, remote and reliable operations. The combination\nof Neural Networks, Bayesian modelling concepts and ensemble learning\nstrategies, form a valuable prognostics framework to combine uncertainty in a\nrobust and accurate manner. Accordingly, this paper introduces a Bayesian\nensemble learning approach to predict the capacity depletion of lithium-ion\nbatteries. The approach accurately predicts the capacity fade and quantifies\nthe uncertainty associated with battery design and degradation processes. The\nproposed Bayesian ensemble methodology employs a stacking technique,\nintegrating multiple Bayesian neural networks (BNNs) as base learners, which\nhave been trained on data diversity. The proposed method has been validated\nusing a battery aging dataset collected by the NASA Ames Prognostics Center of\nExcellence. Obtained results demonstrate the improved accuracy and robustness\nof the proposed probabilistic fusion approach with respect to (i) a single BNN\nmodel and (ii) a classical stacking strategy based on different BNNs.",
      "tldr_zh": "这篇论文提出了一种基于贝叶斯集成学习（Bayesian ensemble learning）的概率融合方法，用于提升锂离子电池健康状态预测的准确性和稳健性。方法采用堆叠技术（stacking technique）整合多个基于数据多样性训练的Bayesian neural networks (BNNs)作为基学习器，以准确预测电池容量衰减并量化设计和退化过程中的不确定性。通过NASA Ames Prognostics Center of Excellence的电池老化数据集验证，结果显示该方法比单个BNN模型和经典堆叠策略在准确性和稳健性方面均有显著改进。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15292v1",
      "published_date": "2024-05-24 07:26:36 UTC",
      "updated_date": "2024-05-24 07:26:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:43:13.085715"
    },
    {
      "arxiv_id": "2405.20770v4",
      "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification",
      "title_zh": "翻译失败",
      "authors": [
        "Guang Lin",
        "Toshihisa Tanaka",
        "Qibin Zhao"
      ],
      "abstract": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
      "tldr_zh": "本研究提出了一种名为 Large Language Model Sentinel (LLAMOS) 的新防御技术，用于提升大型语言模型 (LLMs) 的对抗鲁棒性，通过在输入对抗文本前进行净化处理。LLAMOS 包括两个核心组件：Agent instruction，用于模拟防御代理以最小化字符修改、保持原句含义的同时抵御攻击；以及 Defense guidance，提供修改策略确保有效防御。实验结果显示，该方法即使不依赖对抗样本训练，也能在开源和闭源 LLMs 上显著抵御 adversarial attacks，并在防御与攻击代理的对抗实验中展现出均衡性能，从而增强了 LLMs 的整体安全性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20770v4",
      "published_date": "2024-05-24 07:23:56 UTC",
      "updated_date": "2025-04-23 05:12:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:43:24.752626"
    },
    {
      "arxiv_id": "2405.15282v2",
      "title": "Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Abhinav Jain",
        "Swarat Chaudhuri",
        "Thomas Reps",
        "Chris Jermaine"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has become the standard for\ncustomising Foundation Models (FMs) to user-specific downstream tasks. However,\ntypical PEFT methods require storing multiple task-specific adapters, creating\nscalability issues as these adapters must be housed and run at the FM server.\nTraditional prompt tuning offers a potential solution by customising them\nthrough task-specific input prefixes, but it under-performs compared to other\nPEFT methods like LoRA. To address this gap, we propose Low-Rank Prompt\nAdaptation (LoPA), a prompt-tuning-based approach that performs on par with\nstate-of-the-art PEFT methods and full fine-tuning while being more\nparameter-efficient and not requiring a server-based adapter. LoPA generates\nsoft prompts by balancing between sharing task-specific information across\ninstances and customization for each instance. It uses a low-rank decomposition\nof the soft-prompt component encoded for each instance to achieve parameter\nefficiency. We provide a comprehensive evaluation on multiple natural language\nunderstanding and code generation and understanding tasks across a wide range\nof foundation models with varying sizes.",
      "tldr_zh": "该研究针对参数高效微调(PEFT)方法中存储多任务适配器的可扩展性问题，提出了一种基于低秩提示适配(LoPA)的prompt tuning方法。LoPA通过低秩分解生成软提示，实现任务间信息共享与实例定制的平衡，从而在不需服务器存储适配器的情况下，与LoRA等最先进PEFT方法和全微调性能相当。实验在多种自然语言理解、代码生成和理解任务上进行，证明了LoPA在不同规模基础模型(FMs)上的参数效率和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 8 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.15282v2",
      "published_date": "2024-05-24 07:11:42 UTC",
      "updated_date": "2024-10-31 22:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:43:36.210811"
    },
    {
      "arxiv_id": "2405.15280v1",
      "title": "DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Yiqing Wu",
        "Ruobing Xie",
        "Zhao Zhang",
        "Xu Zhang",
        "Fuzhen Zhuang",
        "Leyu Lin",
        "Zhanhui Kang",
        "Yongjun Xu"
      ],
      "abstract": "The graph-based recommendation has achieved great success in recent years.\nHowever, most existing graph-based recommendations focus on capturing user\npreference based on positive edges/feedback, while ignoring negative\nedges/feedback (e.g., dislike, low rating) that widely exist in real-world\nrecommender systems. How to utilize negative feedback in graph-based\nrecommendations still remains underexplored. In this study, we first conducted\na comprehensive experimental analysis and found that (1) existing graph neural\nnetworks are not well-suited for modeling negative feedback, which acts as a\nhigh-frequency signal in a user-item graph. (2) The graph-based recommendation\nsuffers from the representation degeneration problem. Based on the two\nobservations, we propose a novel model that models positive and negative\nfeedback from a frequency filter perspective called Dual-frequency Graph Neural\nNetwork for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, the\ndesigned dual-frequency graph filter (DGF) captures both low-frequency and\nhigh-frequency signals that contain positive and negative feedback.\nFurthermore, the proposed signed graph regularization is applied to maintain\nthe user/item embedding uniform in the embedding space to alleviate the\nrepresentation degeneration problem. Additionally, we conduct extensive\nexperiments on real-world datasets and demonstrate the effectiveness of the\nproposed model. Codes of our model will be released upon acceptance.",
      "tldr_zh": "本文研究发现，现有的图-based推荐系统主要关注正向反馈（positive edges），而忽略负向反馈（negative edges），导致Graph Neural Networks在处理高频信号时表现不佳，并存在表示退化问题（representation degeneration）。为此，作者提出DFGNN模型，通过dual-frequency graph filter (DGF)从频率过滤视角捕捉正负反馈的低频和高频信号，并引入signed graph regularization来维持嵌入空间的均匀性。实验结果显示，该模型在真实数据集上显著提升了推荐性能，有效利用了负向反馈。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by KDD 2024 Research Track",
      "pdf_url": "http://arxiv.org/pdf/2405.15280v1",
      "published_date": "2024-05-24 07:07:41 UTC",
      "updated_date": "2024-05-24 07:07:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:43:49.456936"
    },
    {
      "arxiv_id": "2405.15264v1",
      "title": "Self-Contrastive Weakly Supervised Learning Framework for Prognostic Prediction Using Whole Slide Images",
      "title_zh": "翻译失败",
      "authors": [
        "Saul Fuster",
        "Farbod Khoraminia",
        "Julio Silva-Rodríguez",
        "Umay Kiraz",
        "Geert J. L. H. van Leenders",
        "Trygve Eftestøl",
        "Valery Naranjo",
        "Emiel A. M. Janssen",
        "Tahlita C. M. Zuiverloon",
        "Kjersti Engan"
      ],
      "abstract": "We present a pioneering investigation into the application of deep learning\ntechniques to analyze histopathological images for addressing the substantial\nchallenge of automated prognostic prediction. Prognostic prediction poses a\nunique challenge as the ground truth labels are inherently weak, and the model\nmust anticipate future events that are not directly observable in the image. To\naddress this challenge, we propose a novel three-part framework comprising of a\nconvolutional network based tissue segmentation algorithm for region of\ninterest delineation, a contrastive learning module for feature extraction, and\na nested multiple instance learning classification module. Our study explores\nthe significance of various regions of interest within the histopathological\nslides and exploits diverse learning scenarios. The pipeline is initially\nvalidated on artificially generated data and a simpler diagnostic task.\nTransitioning to prognostic prediction, tasks become more challenging.\nEmploying bladder cancer as use case, our best models yield an AUC of 0.721 and\n0.678 for recurrence and treatment outcome prediction respectively.",
      "tldr_zh": "我们提出一个自对比弱监督学习框架，用于利用全切片图像（Whole Slide Images）进行预后预测，该框架包括基于卷积网络的组织分割算法、对比学习模块和嵌套的多实例学习（Multiple Instance Learning）分类模块，以应对标签弱监督和未来事件预测的挑战。该框架首先在人工生成数据和简单诊断任务上验证有效性，然后应用于膀胱癌预后预测任务，分别在复发和治疗结果预测上获得 AUC 0.721 和 0.678 的性能。实验结果突显了不同感兴趣区域的重要性，并为自动化预后分析提供了新方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://github.com/Biomedical-Data-Analysis-Laboratory/HistoPrognostics",
      "pdf_url": "http://arxiv.org/pdf/2405.15264v1",
      "published_date": "2024-05-24 06:45:36 UTC",
      "updated_date": "2024-05-24 06:45:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:44:01.948024"
    },
    {
      "arxiv_id": "2405.15254v1",
      "title": "Novel Kernel Models and Exact Representor Theory for Neural Networks Beyond the Over-Parameterized Regime",
      "title_zh": "翻译失败",
      "authors": [
        "Alistair Shilton",
        "Sunil Gupta",
        "Santu Rana",
        "Svetha Venkatesh"
      ],
      "abstract": "This paper presents two models of neural-networks and their training\napplicable to neural networks of arbitrary width, depth and topology, assuming\nonly finite-energy neural activations; and a novel representor theory for\nneural networks in terms of a matrix-valued kernel. The first model is exact\n(un-approximated) and global, casting the neural network as an elements in a\nreproducing kernel Banach space (RKBS); we use this model to provide tight\nbounds on Rademacher complexity. The second model is exact and local, casting\nthe change in neural network function resulting from a bounded change in\nweights and biases (ie. a training step) in reproducing kernel Hilbert space\n(RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model\nprovides insight into model adaptation through tight bounds on Rademacher\ncomplexity of network adaptation. We also prove that the neural tangent kernel\n(NTK) is a first-order approximation of the LiNK kernel. Finally, and noting\nthat the LiNK does not provide a representor theory for technical reasons, we\npresent an exact novel representor theory for layer-wise neural network\ntraining with unregularized gradient descent in terms of a local-extrinsic\nneural kernel (LeNK). This representor theory gives insight into the role of\nhigher-order statistics in neural network training and the effect of kernel\nevolution in neural-network kernel models. Throughout the paper (a) feedforward\nReLU networks and (b) residual networks (ResNet) are used as illustrative\nexamples.",
      "tldr_zh": "这篇论文提出了两种适用于任意宽度、深度和拓扑的神经网络模型及其训练方法，仅假设有限能量神经激活。第一种模型将神经网络视为再现核 Banach 空间 (RKBS) 中的元素，提供全局精确表示并给出 Rademacher complexity 的紧密边界。第二种模型使用局部内在神经核 (LiNK) 在再现核 Hilbert 空间 (RKHS) 中描述权重变化导致的网络函数变化，并证明神经切线核 (NTK) 是 LiNK 的第一阶近似。此外，论文引入了基于局部外在神经核 (LeNK) 的新 representor 理论，解释了更高阶统计在无正则化梯度下降训练中的作用，并以前馈 ReLU 网络和 ResNet 为示例。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15254v1",
      "published_date": "2024-05-24 06:30:36 UTC",
      "updated_date": "2024-05-24 06:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:44:16.214229"
    },
    {
      "arxiv_id": "2405.15250v1",
      "title": "Coaching Copilot: Blended Form of an LLM-Powered Chatbot and a Human Coach to Effectively Support Self-Reflection for Leadership Growth",
      "title_zh": "翻译失败",
      "authors": [
        "Riku Arakawa",
        "Hiromu Yakura"
      ],
      "abstract": "Chatbots' role in fostering self-reflection is now widely recognized,\nespecially in inducing users' behavior change. While the benefits of 24/7\navailability, scalability, and consistent responses have been demonstrated in\ncontexts such as healthcare and tutoring to help one form a new habit, their\nutilization in coaching necessitating deeper introspective dialogue to induce\nleadership growth remains unexplored. This paper explores the potential of such\na chatbot powered by recent Large Language Models (LLMs) in collaboration with\nprofessional coaches in the field of executive coaching. Through a design\nworkshop with them and two weeks of user study involving ten coach-client\npairs, we explored the feasibility and nuances of integrating chatbots to\ncomplement human coaches. Our findings highlight the benefits of chatbots'\nubiquity and reasoning capabilities enabled by LLMs while identifying their\nlimitations and design necessities for effective collaboration between human\ncoaches and chatbots. By doing so, this work contributes to the foundation for\naugmenting one's self-reflective process with prevalent conversational agents\nthrough the human-in-the-loop approach.",
      "tldr_zh": "这篇论文提出“Coaching Copilot”，一种结合LLM驱动聊天机器人和人类教练的混合形式，旨在有效支持领导力成长的自我反思过程。通过设计工作坊和为期两周的用户研究（涉及十对教练-客户），探讨了这种整合的可行性和细节。研究发现，chatbots 的24/7可用性和LLM推理能力能增强内省对话，但也暴露了其局限性，并强调了human-in-the-loop设计的需求。该工作为利用对话代理增强自我反思奠定了基础。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted by the International ACM Conversational User Interfaces\n  Conference (CUI '24)",
      "pdf_url": "http://arxiv.org/pdf/2405.15250v1",
      "published_date": "2024-05-24 06:20:56 UTC",
      "updated_date": "2024-05-24 06:20:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:44:29.537166"
    },
    {
      "arxiv_id": "2405.15245v1",
      "title": "Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee",
      "title_zh": "翻译失败",
      "authors": [
        "Mengtong Gao",
        "Yifei Zou",
        "Zuyuan Zhang",
        "Xiuzhen Cheng",
        "Dongxiao Yu"
      ],
      "abstract": "The safety of decentralized reinforcement learning (RL) is a challenging\nproblem since malicious agents can share their poisoned policies with benign\nagents. The paper investigates a cooperative backdoor attack in a decentralized\nreinforcement learning scenario. Differing from the existing methods that hide\na whole backdoor attack behind their shared policies, our method decomposes the\nbackdoor behavior into multiple components according to the state space of RL.\nEach malicious agent hides one component in its policy and shares its policy\nwith the benign agents. When a benign agent learns all the poisoned policies,\nthe backdoor attack is assembled in its policy. The theoretical proof is given\nto show that our cooperative method can successfully inject the backdoor into\nthe RL policies of benign agents. Compared with the existing backdoor attacks,\nour cooperative method is more covert since the policy from each attacker only\ncontains a component of the backdoor attack and is harder to detect. Extensive\nsimulations are conducted based on Atari environments to demonstrate the\nefficiency and covertness of our method. To the best of our knowledge, this is\nthe first paper presenting a provable cooperative backdoor attack in\ndecentralized reinforcement learning.",
      "tldr_zh": "这篇论文探讨了去中心化强化学习(decentralized reinforcement learning)中的安全问题，提出了一种合作后门攻击(cooperative backdoor attack)，其中恶意代理将后门行为分解成多个组件，每个组件隐藏在其共享的poisoned policies中。良性代理(benign agents)在学习这些策略时，会无意中组装完整的后门攻击。论文提供了理论证明，证明该方法能成功注入后门，并通过Atari环境下的广泛模拟实验，展示了其比现有方法更隐蔽和高效的优势。这是首篇在decentralized reinforcement learning中呈现可证明合作后门攻击的论文。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15245v1",
      "published_date": "2024-05-24 06:13:31 UTC",
      "updated_date": "2024-05-24 06:13:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:44:42.050749"
    },
    {
      "arxiv_id": "2406.00024v2",
      "title": "Embedding-Aligned Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Guy Tennenholtz",
        "Yinlam Chow",
        "Chih-Wei Hsu",
        "Lior Shani",
        "Ethan Liang",
        "Craig Boutilier"
      ],
      "abstract": "We propose a novel approach for training large language models (LLMs) to\nadhere to objectives defined within a latent embedding space. Our method\nleverages reinforcement learning (RL), treating a pre-trained LLM as an\nenvironment. Our embedding-aligned guided language (EAGLE) agent is trained to\niteratively steer the LLM's generation towards optimal regions of the latent\nembedding space, w.r.t. some predefined criterion. We demonstrate the\neffectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review\ndatasets to surface content gaps that satisfy latent user demand. We also\ndemonstrate the benefit of using an optimal design of a state-dependent action\nset to improve EAGLE's efficiency. Our work paves the way for controlled and\ngrounded text generation using LLMs, ensuring consistency with domain-specific\nknowledge and data representations.",
      "tldr_zh": "我们提出了一种新方法，使用强化学习 (RL) 来训练大型语言模型 (LLMs)，使它们遵守嵌入空间中的预定义目标。该方法引入了 EAGLE 代理，通过迭代引导 LLM 的生成朝向最优嵌入区域，从而填补潜在用户需求的内容缺口。实验在 MovieLens 25M 和 Amazon Review 数据集上证明了 EAGLE 的有效性，并通过优化状态依赖动作集提高了其效率。该研究为受控且与领域特定知识一致的文本生成铺平了道路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00024v2",
      "published_date": "2024-05-24 06:11:17 UTC",
      "updated_date": "2024-10-28 06:30:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:44:52.423777"
    },
    {
      "arxiv_id": "2405.15230v2",
      "title": "$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Long Tan Le",
        "Han Shu",
        "Tung-Anh Nguyen",
        "Choong Seon Hong",
        "Nguyen H. Tran"
      ],
      "abstract": "While astonishingly capable, large Language Models (LLM) can sometimes\nproduce outputs that deviate from human expectations. Such deviations\nnecessitate an alignment phase to prevent disseminating untruthful, toxic, or\nbiased information. Traditional alignment methods based on reinforcement\nlearning often struggle with the identified instability, whereas preference\noptimization methods are limited by their overfitting to pre-collected\nhard-label datasets. In this paper, we propose a novel LLM alignment framework\nnamed $i$REPO, which utilizes implicit Reward pairwise difference regression\nfor Empirical Preference Optimization. Particularly, $i$REPO employs\nself-generated datasets labeled by empirical human (or AI annotator) preference\nto iteratively refine the aligned policy through a novel regression-based loss\nfunction. Furthermore, we introduce an innovative algorithm backed by\ntheoretical guarantees for achieving optimal results under ideal assumptions\nand providing a practical performance-gap result without such assumptions.\nExperimental results with Phi-2 and Mistral-7B demonstrate that $i$REPO\neffectively achieves self-alignment using soft-label, self-generated responses\nand the logit of empirical AI annotators. Furthermore, our approach surpasses\npreference optimization baselines in evaluations using the Language Model\nEvaluation Harness and Multi-turn benchmarks.",
      "tldr_zh": "该论文提出了一种新型框架 $i$REPO（基于隐式奖励对差异回归的经验偏好优化），旨在解决大型语言模型 (LLM) 输出偏离人类期望的问题，克服了传统强化学习方法的不稳定性以及偏好优化方法的过拟合问题。$i$REPO 通过自生成数据集和经验人类（或 AI 标注者）偏好进行迭代优化，引入一个新的回归损失函数，并提供理论保证下的最优结果以及实际性能差距分析。实验结果显示，在 Phi-2 和 Mistral-7B 模型上，$i$REPO 超过了基线偏好优化方法，在 Language Model Evaluation Harness 和 Multi-turn 基准测试中表现出色。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2405.15230v2",
      "published_date": "2024-05-24 05:42:11 UTC",
      "updated_date": "2024-10-29 00:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:45:04.482843"
    },
    {
      "arxiv_id": "2405.15222v2",
      "title": "Leveraging Unknown Objects to Construct Labeled-Unlabeled Meta-Relationships for Zero-Shot Object Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Yanwei Zheng",
        "Changrui Li",
        "Chuanlin Lan",
        "Yaling Li",
        "Xiao Zhang",
        "Yifei Zou",
        "Dongxiao Yu",
        "Zhipeng Cai"
      ],
      "abstract": "Zero-shot object navigation (ZSON) addresses situation where an agent\nnavigates to an unseen object that does not present in the training set.\nPrevious works mainly train agent using seen objects with known labels, and\nignore the seen objects without labels. In this paper, we introduce seen\nobjects without labels, herein termed as ``unknown objects'', into training\nprocedure to enrich the agent's knowledge base with distinguishable but\npreviously overlooked information. Furthermore, we propose the label-wise\nmeta-correlation module (LWMCM) to harness relationships among objects with and\nwithout labels, and obtain enhanced objects information. Specially, we propose\ntarget feature generator (TFG) to generate the features representation of the\nunlabeled target objects. Subsequently, the unlabeled object identifier (UOI)\nmodule assesses whether the unlabeled target object appears in the current\nobservation frame captured by the camera and produces an adapted target\nfeatures representation specific to the observed context. In meta contrastive\nfeature modifier (MCFM), the target features is modified via approaching the\nfeatures of objects within the observation frame while distancing itself from\nfeatures of unobserved objects. Finally, the meta object-graph learner (MOGL)\nmodule is utilized to calculate the relationships among objects based on the\nfeatures. Experiments conducted on AI2THOR and RoboTHOR platforms demonstrate\nthe effectiveness of our proposed method.",
      "tldr_zh": "该论文针对 Zero-Shot Object Navigation (ZSON) 问题，提出一种利用 unknown objects（无标签的已见物体）来构建有标签和无标签物体之间 meta-relationships 的方法，以丰富代理的知识库。核心创新包括 label-wise meta-correlation module (LWMCM)，它结合 target feature generator (TFG) 生成无标签目标特征、unlabeled object identifier (UOI) 检测物体出现、meta contrastive feature modifier (MCFM) 通过对比修改特征，以及 meta object-graph learner (MOGL) 计算物体关系，从而提升导航性能。在 AI2THOR 和 RoboTHOR 平台上的实验结果证明，该方法显著提高了代理的导航准确性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15222v2",
      "published_date": "2024-05-24 05:26:18 UTC",
      "updated_date": "2024-05-27 02:39:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:45:18.081579"
    },
    {
      "arxiv_id": "2406.06549v1",
      "title": "Large Language Model (LLM) for Standard Cell Layout Design Optimization",
      "title_zh": "大语言模型 (LLM) 用于标准单元布局设计优化",
      "authors": [
        "Chia-Tung Ho",
        "Haoxing Ren"
      ],
      "abstract": "Standard cells are essential components of modern digital circuit designs.\nWith process technologies advancing toward 2nm, more routability issues have\narisen due to the decreasing number of routing tracks, increasing number and\ncomplexity of design rules, and strict patterning rules. The state-of-the-art\nstandard cell design automation framework is able to automatically design\nstandard cell layouts in advanced nodes, but it is still struggling to generate\nhighly competitive Performance-Power-Area (PPA) and routable cell layouts for\ncomplex sequential cell designs. Consequently, a novel and efficient\nmethodology incorporating the expertise of experienced human designers to\nincrementally optimize the PPA of cell layouts is highly necessary and\nessential. High-quality device clustering, with consideration of netlist\ntopology, diffusion sharing/break and routability in the layouts, can reduce\ncomplexity and assist in finding highly competitive PPA, and routable layouts\nfaster. In this paper, we leverage the natural language and reasoning ability\nof Large Language Model (LLM) to generate high-quality cluster constraints\nincrementally to optimize the cell layout PPA and debug the routability with\nReAct prompting. On a benchmark of sequential standard cells in 2nm, we\ndemonstrate that the proposed method not only achieves up to 19.4% smaller cell\narea, but also generates 23.5% more LVS/DRC clean cell layouts than previous\nwork. In summary, the proposed method not only successfully reduces cell area\nby 4.65% on average, but also is able to fix routability in the cell layout\ndesigns.",
      "tldr_zh": "这篇论文提出了一种利用Large Language Model (LLM)优化标准单元布局设计的方法，针对2nm工艺下路由问题增多和PPA（Performance-Power-Area）优化挑战。方法通过LLM的自然语言和推理能力生成高质量的集群约束，并采用ReAct prompting逐步调试路由性和优化布局。实验结果显示，该方法在基准测试中使细胞面积减少多达19.4%，生成23.5%更多LVS/DRC干净布局，并平均减少4.65%面积，同时有效修复路由问题。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "6 pages, 8 figures, IEEE International Workshop on LLM-Aided Design\n  (LAD'24)",
      "pdf_url": "http://arxiv.org/pdf/2406.06549v1",
      "published_date": "2024-05-24 04:59:58 UTC",
      "updated_date": "2024-05-24 04:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:45:28.962671"
    },
    {
      "arxiv_id": "2405.17477v3",
      "title": "OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning",
      "title_zh": "OLLIE：从离线预训练到在线微调的模仿学习",
      "authors": [
        "Sheng Yue",
        "Xingyuan Hua",
        "Ju Ren",
        "Sen Lin",
        "Junshan Zhang",
        "Yaoxue Zhang"
      ],
      "abstract": "In this paper, we study offline-to-online Imitation Learning (IL) that\npretrains an imitation policy from static demonstration data, followed by fast\nfinetuning with minimal environmental interaction. We find the na\\\"ive\ncombination of existing offline IL and online IL methods tends to behave poorly\nin this context, because the initial discriminator (often used in online IL)\noperates randomly and discordantly against the policy initialization, leading\nto misguided policy optimization and $\\textit{unlearning}$ of pretraining\nknowledge. To overcome this challenge, we propose a principled\noffline-to-online IL method, named $\\texttt{OLLIE}$, that simultaneously learns\na near-expert policy initialization along with an $\\textit{aligned\ndiscriminator initialization}$, which can be seamlessly integrated into online\nIL, achieving smooth and fast finetuning. Empirically, $\\texttt{OLLIE}$\nconsistently and significantly outperforms the baseline methods in\n$\\textbf{20}$ challenging tasks, from continuous control to vision-based\ndomains, in terms of performance, demonstration efficiency, and convergence\nspeed. This work may serve as a foundation for further exploration of\npretraining and finetuning in the context of IL.",
      "tldr_zh": "本研究探讨了从离线预训练到在线微调的模仿学习(Imitation Learning, IL)，旨在从静态演示数据中预训练策略，然后通过最少的环境交互实现快速微调。传统方法存在问题，因为初始鉴别器(discriminator)随机运作，导致策略优化失误和预训练知识的遗忘(unlearning)。为此，提出OLLIE方法，通过同时学习接近专家的策略初始化和对齐的鉴别器初始化，实现无缝整合到在线IL中，确保平滑快速的微调。实验结果显示，OLLIE在20个挑战性任务中（如连续控制和基于视觉的领域）显著优于基线方法，在性能、演示效率和收敛速度方面表现出色，为IL中的预训练和微调提供坚实基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "International Conference on Machine Learning (ICML)",
      "pdf_url": "http://arxiv.org/pdf/2405.17477v3",
      "published_date": "2024-05-24 04:57:25 UTC",
      "updated_date": "2024-05-30 17:11:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:45:41.208032"
    },
    {
      "arxiv_id": "2405.17476v3",
      "title": "How to Leverage Diverse Demonstrations in Offline Imitation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sheng Yue",
        "Jiani Liu",
        "Xingyuan Hua",
        "Ju Ren",
        "Sen Lin",
        "Junshan Zhang",
        "Yaoxue Zhang"
      ],
      "abstract": "Offline Imitation Learning (IL) with imperfect demonstrations has garnered\nincreasing attention owing to the scarcity of expert data in many real-world\ndomains. A fundamental problem in this scenario is how to extract positive\nbehaviors from noisy data. In general, current approaches to the problem select\ndata building on state-action similarity to given expert demonstrations,\nneglecting precious information in (potentially abundant) $\\textit{diverse}$\nstate-actions that deviate from expert ones. In this paper, we introduce a\nsimple yet effective data selection method that identifies positive behaviors\nbased on their resultant states -- a more informative criterion enabling\nexplicit utilization of dynamics information and effective extraction of both\nexpert and beneficial diverse behaviors. Further, we devise a lightweight\nbehavior cloning algorithm capable of leveraging the expert and selected data\ncorrectly. In the experiments, we evaluate our method on a suite of complex and\nhigh-dimensional offline IL benchmarks, including continuous-control and\nvision-based tasks. The results demonstrate that our method achieves\nstate-of-the-art performance, outperforming existing methods on\n$\\textbf{20/21}$ benchmarks, typically by $\\textbf{2-5x}$, while maintaining a\ncomparable runtime to Behavior Cloning ($\\texttt{BC}$).",
      "tldr_zh": "本文针对 Offline Imitation Learning (IL) 中不完美演示数据的问题，提出了一种简单有效的数据选择方法，该方法基于结果状态（resultant states）来识别积极行为，从而充分利用动态信息并提取专家演示和有益的多样化状态-动作数据。同时，作者设计了一个轻量级的行为克隆算法（Behavior Cloning），能够正确整合这些数据进行训练。在实验中，该方法在复杂的高维基准（如连续控制和基于视觉的任务）上表现突出，在 20/21 个基准上优于现有方法，通常提升 2-5 倍，同时保持与 Behavior Cloning 相当的运行时间。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "International Conference on Machine Learning (ICML)",
      "pdf_url": "http://arxiv.org/pdf/2405.17476v3",
      "published_date": "2024-05-24 04:56:39 UTC",
      "updated_date": "2024-05-30 17:15:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:45:53.598740"
    },
    {
      "arxiv_id": "2405.19358v2",
      "title": "Robustifying Safety-Aligned Large Language Models through Clean Data Curation",
      "title_zh": "通过清洁数据策展强化安全对齐的大型语言模型",
      "authors": [
        "Xiaoqun Liu",
        "Jiacheng Liang",
        "Muchao Ye",
        "Zhaohan Xi"
      ],
      "abstract": "Large language models (LLMs) are vulnerable when trained on datasets\ncontaining harmful content, which leads to potential jailbreaking attacks in\ntwo scenarios: the integration of harmful texts within crowdsourced data used\nfor pre-training and direct tampering with LLMs through fine-tuning. In both\nscenarios, adversaries can compromise the safety alignment of LLMs,\nexacerbating malfunctions. Motivated by the need to mitigate these adversarial\ninfluences, our research aims to enhance safety alignment by either\nneutralizing the impact of malicious texts in pre-training datasets or\nincreasing the difficulty of jailbreaking during downstream fine-tuning. In\nthis paper, we propose a data curation framework designed to counter\nadversarial impacts in both scenarios. Our method operates under the assumption\nthat we have no prior knowledge of attack details, focusing solely on curating\nclean texts. We introduce an iterative process aimed at revising texts to\nreduce their perplexity as perceived by LLMs, while simultaneously preserving\ntheir text quality. By pre-training or fine-tuning LLMs with curated clean\ntexts, we observe a notable improvement in LLM robustness regarding safety\nalignment against harmful queries. For instance, when pre-training LLMs using a\ncrowdsourced dataset containing 5\\% harmful instances, adding an equivalent\namount of curated texts significantly mitigates the likelihood of providing\nharmful responses in LLMs and reduces the attack success rate by 71\\%. Our\nstudy represents a significant step towards mitigating the risks associated\nwith training-based jailbreaking and fortifying the secure utilization of LLMs.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在训练数据集中的有害内容问题，这些内容可能导致安全对齐（safety alignment）失效，并引发jailbreaking攻击。作者提出一个数据净化框架，通过迭代过程修订文本以降低LLMs的困惑度（perplexity），同时保持文本质量，从而增强模型在预训练和微调阶段的鲁棒性。在实验中，使用包含5%有害实例的预训练数据集时，添加等量净化文本可显著减少有害响应并降低攻击成功率71%，为缓解训练-based jailbreaking风险提供了重要方法。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19358v2",
      "published_date": "2024-05-24 04:50:38 UTC",
      "updated_date": "2024-05-31 02:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:46:04.014587"
    },
    {
      "arxiv_id": "2405.17475v2",
      "title": "How Culturally Aware are Vision-Language Models?",
      "title_zh": "视觉语言模型的文化意识有多高？",
      "authors": [
        "Olena Burda-Lassen",
        "Aman Chadha",
        "Shashank Goswami",
        "Vinija Jain"
      ],
      "abstract": "An image is often considered worth a thousand words, and certain images can\ntell rich and insightful stories. Can these stories be told via image\ncaptioning? Images from folklore genres, such as mythology, folk dance,\ncultural signs, and symbols, are vital to every culture. Our research compares\nthe performance of four popular vision-language models (GPT-4V, Gemini Pro\nVision, LLaVA, and OpenFlamingo) in identifying culturally specific information\nin such images and creating accurate and culturally sensitive image captions.\nWe also propose a new evaluation metric, the Cultural Awareness Score (CAS),\nwhich measures the degree of cultural awareness in image captions. We provide a\ndataset MOSAIC-1.5k labeled with ground truth for images containing cultural\nbackground and context and a labeled dataset with assigned Cultural Awareness\nScores that can be used with unseen data. Creating culturally appropriate image\ncaptions is valuable for scientific research and can be beneficial for many\npractical applications. We envision our work will promote a deeper integration\nof cultural sensitivity in AI applications worldwide. By making the dataset and\nCultural Awareness Score available to the public, we aim to facilitate further\nresearch in this area, encouraging the development of more culturally aware AI\nsystems that respect and celebrate global diversity.",
      "tldr_zh": "本研究评估了视觉语言模型（VLMs）在处理文化相关图像（如神话、民间舞蹈和符号）时的文化意识水平，比较了 GPT-4V、Gemini Pro Vision、LLaVA 和 OpenFlamingo 等四种流行模型的性能。论文引入了新的评价指标 Cultural Awareness Score (CAS) 来量化图像描述中的文化敏感度，并提供了标注数据集 MOSAIC-1.5k，其中包含文化背景和上下文的标签，以支持进一步测试。实验结果突显了模型在文化特定信息识别上的不足，并强调了提升文化意识的重要性，以促进更具包容性的 AI 应用和全球多样性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17475v2",
      "published_date": "2024-05-24 04:45:14 UTC",
      "updated_date": "2025-02-08 18:49:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:46:17.210834"
    },
    {
      "arxiv_id": "2405.15208v1",
      "title": "Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs",
      "title_zh": "以思维速度解码：驾驭词汇单位的平行解码用于 LLMs",
      "authors": [
        "Chenxi Sun",
        "Hongzhi Zhang",
        "Zijia Lin",
        "Jingyuan Zhang",
        "Fuzheng Zhang",
        "Zhongyuan Wang",
        "Bin Chen",
        "Chengru Song",
        "Di Zhang",
        "Kun Gai",
        "Deyi Xiong"
      ],
      "abstract": "Large language models have demonstrated exceptional capability in natural\nlanguage understanding and generation. However, their generation speed is\nlimited by the inherently sequential nature of their decoding process, posing\nchallenges for real-time applications. This paper introduces Lexical Unit\nDecoding (LUD), a novel decoding methodology implemented in a data-driven\nmanner, accelerating the decoding process without sacrificing output quality.\nThe core of our approach is the observation that a pre-trained language model\ncan confidently predict multiple contiguous tokens, forming the basis for a\n\\textit{lexical unit}, in which these contiguous tokens could be decoded in\nparallel. Extensive experiments validate that our method substantially reduces\ndecoding time while maintaining generation quality, i.e., 33\\% speed up on\nnatural language generation with no quality loss, and 30\\% speed up on code\ngeneration with a negligible quality loss of 3\\%. Distinctively, LUD requires\nno auxiliary models and does not require changes to existing architectures. It\ncan also be integrated with other decoding acceleration methods, thus achieving\nan even more pronounced inference efficiency boost. We posit that the\nfoundational principles of LUD could define a new decoding paradigm for future\nlanguage models, enhancing their applicability for a broader spectrum of\napplications. All codes are be publicly available at\nhttps://github.com/tjunlp-lab/Lexical-Unit-Decoding-LUD-. Keywords: Parallel\nDecoding, Lexical Unit Decoding, Large Language Model",
      "tldr_zh": "这篇论文针对大语言模型(LLMs)的顺序解码问题，提出了一种数据驱动的Lexical Unit Decoding (LUD)方法，通过并行预测多个连续tokens（称为lexical unit）来加速生成过程，而不牺牲输出质量。LUD的核心在于利用预训练模型的自信预测能力，实现高效的Parallel Decoding。实验结果显示，该方法在自然语言生成任务上实现了33%的速度提升且质量无损，在代码生成上实现了30%的速度提升，仅伴随3%的微小质量损失。此外，LUD无需辅助模型或架构修改，并可与其他解码加速技术结合，潜在地为未来LLMs的解码范式提供新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15208v1",
      "published_date": "2024-05-24 04:35:13 UTC",
      "updated_date": "2024-05-24 04:35:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:46:30.528834"
    },
    {
      "arxiv_id": "2405.17474v2",
      "title": "Federated Offline Policy Optimization with Dual Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Sheng Yue",
        "Zerui Qin",
        "Xingyuan Hua",
        "Yongheng Deng",
        "Ju Ren"
      ],
      "abstract": "Federated Reinforcement Learning (FRL) has been deemed as a promising\nsolution for intelligent decision-making in the era of Artificial Internet of\nThings. However, existing FRL approaches often entail repeated interactions\nwith the environment during local updating, which can be prohibitively\nexpensive or even infeasible in many real-world domains. To overcome this\nchallenge, this paper proposes a novel offline federated policy optimization\nalgorithm, named $\\texttt{DRPO}$, which enables distributed agents to\ncollaboratively learn a decision policy only from private and static data\nwithout further environmental interactions. $\\texttt{DRPO}$ leverages dual\nregularization, incorporating both the local behavioral policy and the global\naggregated policy, to judiciously cope with the intrinsic two-tier\ndistributional shifts in offline FRL. Theoretical analysis characterizes the\nimpact of the dual regularization on performance, demonstrating that by\nachieving the right balance thereof, $\\texttt{DRPO}$ can effectively counteract\ndistributional shifts and ensure strict policy improvement in each federative\nlearning round. Extensive experiments validate the significant performance\ngains of $\\texttt{DRPO}$ over baseline methods.",
      "tldr_zh": "本论文提出了一种名为 $\\texttt{DRPO}$ 的离线联邦强化学习(Federated Reinforcement Learning)算法，允许分布式代理从私有静态数据中协作学习决策策略，而无需进一步环境互动，从而解决传统方法的高成本问题。\n$\\texttt{DRPO}$ 通过双重正则化(Dual Regularization)，即结合本地行为策略和全局聚合策略，来有效处理离线 FRL 中的两层分布偏移，确保每个联邦学习轮次的严格策略改进。\n实验结果显示，$\\texttt{DRPO}$ 在广泛测试中比基线方法取得了显著性能提升，为智能决策在 Artificial Internet of Things 领域的应用提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IEEE International Conference on Computer Communications (INFOCOM)",
      "pdf_url": "http://arxiv.org/pdf/2405.17474v2",
      "published_date": "2024-05-24 04:24:03 UTC",
      "updated_date": "2024-05-29 01:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:46:52.952761"
    },
    {
      "arxiv_id": "2405.15194v2",
      "title": "Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Siddhant Bhambri",
        "Amrita Bhattacharjee",
        "Durgesh Kalwar",
        "Lin Guan",
        "Huan Liu",
        "Subbarao Kambhampati"
      ],
      "abstract": "Reinforcement Learning (RL) suffers from sample inefficiency in sparse reward\ndomains, and the problem is further pronounced in case of stochastic\ntransitions. To improve the sample efficiency, reward shaping is a well-studied\napproach to introduce intrinsic rewards that can help the RL agent converge to\nan optimal policy faster. However, designing a useful reward shaping function\nfor all desirable states in the Markov Decision Process (MDP) is challenging,\neven for domain experts. Given that Large Language Models (LLMs) have\ndemonstrated impressive performance across a magnitude of natural language\ntasks, we aim to answer the following question: `Can we obtain heuristics using\nLLMs for constructing a reward shaping function that can boost an RL agent's\nsample efficiency?' To this end, we aim to leverage off-the-shelf LLMs to\ngenerate a plan for an abstraction of the underlying MDP. We further use this\nLLM-generated plan as a heuristic to construct the reward shaping signal for\nthe downstream RL agent. By characterizing the type of abstraction based on the\nMDP horizon length, we analyze the quality of heuristics when generated using\nan LLM, with and without a verifier in the loop. Our experiments across\nmultiple domains with varying horizon length and number of sub-goals from the\nBabyAI environment suite, Household, Mario, and, Minecraft domain, show 1) the\nadvantages and limitations of querying LLMs with and without a verifier to\ngenerate a reward shaping heuristic, and, 2) a significant improvement in the\nsample efficiency of PPO, A2C, and Q-learning when guided by the LLM-generated\nheuristics.",
      "tldr_zh": "该研究探讨了 Reinforcement Learning (RL) 在稀疏奖励和随机转移环境中样本效率低下的问题，通过利用 Large Language Models (LLMs) 提取启发式来构建奖励整形函数，从而加速 RL 代理的收敛。方法涉及使用 LLMs 生成 Markov Decision Process (MDP) 的抽象计划，并将其作为启发式信号，结合或不结合验证器来优化奖励。实验在 BabyAI、Household、Mario 和 Minecraft 等多个领域显示，这种方法显著提升了 PPO、A2C 和 Q-learning 的样本效率，并分析了不同抽象类型和验证器的影响。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15194v2",
      "published_date": "2024-05-24 03:53:57 UTC",
      "updated_date": "2024-10-07 19:33:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:46:53.344060"
    },
    {
      "arxiv_id": "2405.15185v1",
      "title": "An Evaluation of Estimative Uncertainty in Large Language Models",
      "title_zh": "对大语言模型中估计不确定性的评估",
      "authors": [
        "Zhisheng Tang",
        "Ke Shen",
        "Mayank Kejriwal"
      ],
      "abstract": "Words of estimative probability (WEPs), such as ''maybe'' or ''probably not''\nare ubiquitous in natural language for communicating estimative uncertainty,\ncompared with direct statements involving numerical probability. Human\nestimative uncertainty, and its calibration with numerical estimates, has long\nbeen an area of study -- including by intelligence agencies like the CIA. This\nstudy compares estimative uncertainty in commonly used large language models\n(LLMs) like GPT-4 and ERNIE-4 to that of humans, and to each other. Here we\nshow that LLMs like GPT-3.5 and GPT-4 align with human estimates for some, but\nnot all, WEPs presented in English. Divergence is also observed when the LLM is\npresented with gendered roles and Chinese contexts. Further study shows that an\nadvanced LLM like GPT-4 can consistently map between statistical and estimative\nuncertainty, but a significant performance gap remains. The results contribute\nto a growing body of research on human-LLM alignment.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）如 GPT-4 和 ERNIE-4 在表达估测不确定性（estimative uncertainty）方面的表现，特别是通过 Words of Estimative Probability (WEPs) 如 “maybe” 或 “probably not” 与人类估测的比较。研究发现，LLMs 在某些英语 WEPs 上与人类一致，但涉及性别角色或中文语境时出现差异，且 GPT-4 虽能较好地映射统计不确定性和估测不确定性，但仍存在显著性能差距。这些结果为人类-LLM 对齐（human-LLM alignment）研究提供了重要贡献。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15185v1",
      "published_date": "2024-05-24 03:39:31 UTC",
      "updated_date": "2024-05-24 03:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:47:06.510935"
    },
    {
      "arxiv_id": "2405.15182v2",
      "title": "RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Peihua Mai",
        "Ran Yan",
        "Yan Pang"
      ],
      "abstract": "Federated learning (FL) allows multiple devices to train a model\ncollaboratively without sharing their data. Despite its benefits, FL is\nvulnerable to privacy leakage and poisoning attacks. To address the privacy\nconcern, secure aggregation (SecAgg) is often used to obtain the aggregation of\ngradients on sever without inspecting individual user updates. Unfortunately,\nexisting defense strategies against poisoning attacks rely on the analysis of\nlocal updates in plaintext, making them incompatible with SecAgg. To reconcile\nthe conflicts, we propose a robust federated learning framework against\npoisoning attacks (RFLPA) based on SecAgg protocol. Our framework computes the\ncosine similarity between local updates and server updates to conduct robust\naggregation. Furthermore, we leverage verifiable packed Shamir secret sharing\nto achieve reduced communication cost of $O(M+N)$ per user, and design a novel\ndot product aggregation algorithm to resolve the issue of increased information\nleakage. Our experimental results show that RFLPA significantly reduces\ncommunication and computation overhead by over $75\\%$ compared to the\nstate-of-the-art secret sharing method, BREA, while maintaining competitive\naccuracy.",
      "tldr_zh": "该研究提出RFLPA框架，一种针对投毒攻击(Poisoning Attacks)的鲁棒Federated Learning (FL)系统，结合Secure Aggregation (SecAgg)协议来保护隐私。RFLPA通过计算本地更新和服务器更新的余弦相似度(Cosine Similarity)进行鲁棒聚合，并利用可验证的Packed Shamir Secret Sharing减少通信成本至每用户O(M+N)，同时设计了一个新的点积聚合(Dot Product Aggregation)算法来缓解信息泄露问题。实验结果显示，RFLPA相较于最先进方法BREA降低了超过75%的通信和计算开销，同时保持了竞争性的模型准确率。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "E.4"
      ],
      "primary_category": "cs.CR",
      "comment": "accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15182v2",
      "published_date": "2024-05-24 03:31:10 UTC",
      "updated_date": "2024-10-26 03:42:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:47:17.922514"
    },
    {
      "arxiv_id": "2405.17473v2",
      "title": "Repeat-Aware Neighbor Sampling for Dynamic Graph Learning",
      "title_zh": "针对动态图学习的重复感知邻居采样",
      "authors": [
        "Tao Zou",
        "Yuhao Mao",
        "Junchen Ye",
        "Bowen Du"
      ],
      "abstract": "Dynamic graph learning equips the edges with time attributes and allows\nmultiple links between two nodes, which is a crucial technology for\nunderstanding evolving data scenarios like traffic prediction and\nrecommendation systems. Existing works obtain the evolving patterns mainly\ndepending on the most recent neighbor sequences. However, we argue that whether\ntwo nodes will have interaction with each other in the future is highly\ncorrelated with the same interaction that happened in the past. Only\nconsidering the recent neighbors overlooks the phenomenon of repeat behavior\nand fails to accurately capture the temporal evolution of interactions. To fill\nthis gap, this paper presents RepeatMixer, which considers evolving patterns of\nfirst and high-order repeat behavior in the neighbor sampling strategy and\ntemporal information learning. Firstly, we define the first-order repeat-aware\nnodes of the source node as the destination nodes that have interacted\nhistorically and extend this concept to high orders as nodes in the destination\nnode's high-order neighbors. Then, we extract neighbors of the source node that\ninteracted before the appearance of repeat-aware nodes with a slide window\nstrategy as its neighbor sequence. Next, we leverage both the first and\nhigh-order neighbor sequences of source and destination nodes to learn temporal\npatterns of interactions via an MLP-based encoder. Furthermore, considering the\nvarying temporal patterns on different orders, we introduce a time-aware\naggregation mechanism that adaptively aggregates the temporal representations\nfrom different orders based on the significance of their interaction time\nsequences. Experimental results demonstrate the superiority of RepeatMixer over\nstate-of-the-art models in link prediction tasks, underscoring the\neffectiveness of the proposed repeat-aware neighbor sampling strategy.",
      "tldr_zh": "本研究针对动态图学习(Dynamic Graph Learning)中现有方法忽略历史重复交互的问题，提出了RepeatMixer框架，通过repeat-aware neighbor sampling策略来捕捉一阶(first-order)和高阶(high-order)重复行为。框架首先定义repeat-aware节点并使用滑动窗口(slide window)策略提取邻居序列，然后通过MLP-based encoder学习交互的时序模式，并引入time-aware aggregation机制根据交互时间的重要性自适应聚合不同阶的表示。实验结果显示，RepeatMixer在链接预测(link prediction)任务上优于最先进模型，证明了repeat-aware策略的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD 2024, Research Track",
      "pdf_url": "http://arxiv.org/pdf/2405.17473v2",
      "published_date": "2024-05-24 03:24:29 UTC",
      "updated_date": "2024-06-20 05:23:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:47:28.616835"
    },
    {
      "arxiv_id": "2405.17472v2",
      "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Huang",
        "Haoming Wang",
        "Wei Gao"
      ],
      "abstract": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt\nto specific user preferences, but such adaptability has also been utilized for\nillegal purposes, such as forging public figures' portraits, duplicating\ncopyrighted artworks and generating explicit contents. Existing work focused on\ndetecting the illegally generated contents, but cannot prevent or mitigate\nillegal adaptations of diffusion models. Other schemes of model unlearning and\nreinitialization, similarly, cannot prevent users from relearning the knowledge\nof illegal model adaptation with custom data. In this paper, we present\nFreezeAsGuard, a new technique that addresses these limitations and enables\nirreversible mitigation of illegal adaptations of diffusion models. Our\napproach is that the model publisher selectively freezes tensors in pre-trained\ndiffusion models that are critical to illegal model adaptations, to mitigate\nthe fine-tuned model's representation power in illegal adaptations, but\nminimize the impact on other legal adaptations. Experiment results in multiple\ntext-to-image application domains show that FreezeAsGuard provides 37% stronger\npower in mitigating illegal model adaptations compared to competitive\nbaselines, while incurring less than 5% impact on legal model adaptations. The\nsource code is available at: https://github.com/pittisl/FreezeAsGuard.",
      "tldr_zh": "该论文提出 FreezeAsGuard，一种通过 Selective Tensor Freezing 技术来缓解文本到图像 diffusion models 非法适应的方法，该方法选择性地冻结预训练模型中关键张量，以减少非法微调（如伪造肖像或生成不当内容）的表示能力，同时最小化对合法适应的影响。不同于现有检测或模型遗忘方案，FreezeAsGuard 实现了不可逆的缓解机制，避免用户重新学习非法知识。实验结果显示，在多个文本到图像应用领域，该方法比基线方案提高了37%的缓解效果，而对合法适应的影响不到5%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.17472v2",
      "published_date": "2024-05-24 03:23:51 UTC",
      "updated_date": "2024-11-27 04:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:47:42.131811"
    },
    {
      "arxiv_id": "2405.17471v2",
      "title": "Momentum-Based Federated Reinforcement Learning with Interaction and Communication Efficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Sheng Yue",
        "Xingyuan Hua",
        "Lili Chen",
        "Ju Ren"
      ],
      "abstract": "Federated Reinforcement Learning (FRL) has garnered increasing attention\nrecently. However, due to the intrinsic spatio-temporal non-stationarity of\ndata distributions, the current approaches typically suffer from high\ninteraction and communication costs. In this paper, we introduce a new FRL\nalgorithm, named $\\texttt{MFPO}$, that utilizes momentum, importance sampling,\nand additional server-side adjustment to control the shift of stochastic policy\ngradients and enhance the efficiency of data utilization. We prove that by\nproper selection of momentum parameters and interaction frequency,\n$\\texttt{MFPO}$ can achieve $\\tilde{\\mathcal{O}}(H N^{-1}\\epsilon^{-3/2})$ and\n$\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ interaction and communication complexities\n($N$ represents the number of agents), where the interaction complexity\nachieves linear speedup with the number of agents, and the communication\ncomplexity aligns the best achievable of existing first-order FL algorithms.\nExtensive experiments corroborate the substantial performance gains of\n$\\texttt{MFPO}$ over existing methods on a suite of complex and\nhigh-dimensional benchmarks.",
      "tldr_zh": "该论文针对Federated Reinforcement Learning (FRL)中数据分布的时空非平稳性导致的高交互和通信成本问题，提出了一种新算法MFPO。该算法通过整合momentum、importance sampling和server-side adjustment来控制随机策略梯度的偏移，提高数据利用效率。理论证明显示，MFPO的交互复杂度达到\\(\\tilde{\\mathcal{O}}(H N^{-1}\\epsilon^{-3/2})\\)，实现了与代理数量N的线性加速，而通信复杂度为\\(\\tilde{\\mathcal{O}}(\\epsilon^{-1})\\)，与现有一流FL算法相当。实验结果在复杂的高维基准上验证了MFPO的显著性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IEEE International Conference on Computer Communications (INFOCOM)",
      "pdf_url": "http://arxiv.org/pdf/2405.17471v2",
      "published_date": "2024-05-24 03:23:37 UTC",
      "updated_date": "2024-05-29 01:36:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:47:53.045812"
    },
    {
      "arxiv_id": "2405.15177v5",
      "title": "Diffusion Actor-Critic with Entropy Regulator",
      "title_zh": "翻译失败",
      "authors": [
        "Yinuo Wang",
        "Likun Wang",
        "Yuxuan Jiang",
        "Wenjun Zou",
        "Tong Liu",
        "Xujie Song",
        "Wenxuan Wang",
        "Liming Xiao",
        "Jiang Wu",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "abstract": "Reinforcement learning (RL) has proven highly effective in addressing complex\ndecision-making and control tasks. However, in most traditional RL algorithms,\nthe policy is typically parameterized as a diagonal Gaussian distribution with\nlearned mean and variance, which constrains their capability to acquire complex\npolicies. In response to this problem, we propose an online RL algorithm termed\ndiffusion actor-critic with entropy regulator (DACER). This algorithm\nconceptualizes the reverse process of the diffusion model as a novel policy\nfunction and leverages the capability of the diffusion model to fit multimodal\ndistributions, thereby enhancing the representational capacity of the policy.\nSince the distribution of the diffusion policy lacks an analytical expression,\nits entropy cannot be determined analytically. To mitigate this, we propose a\nmethod to estimate the entropy of the diffusion policy utilizing Gaussian\nmixture model. Building on the estimated entropy, we can learn a parameter\n$\\alpha$ that modulates the degree of exploration and exploitation. Parameter\n$\\alpha$ will be employed to adaptively regulate the variance of the added\nnoise, which is applied to the action output by the diffusion model.\nExperimental trials on MuJoCo benchmarks and a multimodal task demonstrate that\nthe DACER algorithm achieves state-of-the-art (SOTA) performance in most MuJoCo\ncontrol tasks while exhibiting a stronger representational capacity of the\ndiffusion policy.",
      "tldr_zh": "本研究针对传统强化学习（RL）算法中政策参数化受限的问题，提出了一种在线RL算法Diffusion Actor-Critic with Entropy Regulator (DACER)。该算法将扩散模型的逆过程作为新型政策函数，利用扩散模型拟合多模态分布，从而提升政策的表示能力；同时，通过高斯混合模型估计扩散政策的熵，并学习参数α来动态调节探索与利用的平衡。实验结果显示，DACER在MuJoCo基准任务和多模态任务上实现了最先进（SOTA）性能，并在大多数MuJoCo控制任务中表现出更强的政策表示能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS2024 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2405.15177v5",
      "published_date": "2024-05-24 03:23:27 UTC",
      "updated_date": "2024-12-21 02:23:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:48:04.971858"
    },
    {
      "arxiv_id": "2405.17470v1",
      "title": "Athena: Efficient Block-Wise Post-Training Quantization for Large Language Models Using Second-Order Matrix Derivative Information",
      "title_zh": "翻译失败",
      "authors": [
        "Yanshu Wang",
        "Wenyang He",
        "Tong Yang"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing tasks such as machine translation, text generation, and sentiment\nanalysis. However, their large size, often consisting of billions of\nparameters, poses challenges for storage, computation, and deployment,\nparticularly in resource-constrained environments like mobile devices and edge\ncomputing platforms. Effective compression and quantization techniques are\ncrucial for addressing these issues, reducing memory footprint and\ncomputational requirements without significantly compromising performance.\nTraditional methods that uniformly map parameters to compressed spaces fail to\naccount for the uneven distribution of parameters, leading to substantial\naccuracy loss. In this work, we propose Athena, a novel algorithm for efficient\nblock-wise post-training quantization of LLMs. Athena leverages Second-Order\nMatrix Derivative Information to guide the quantization process using the\ncurvature information of the loss landscape. By grouping parameters by columns\nor rows and iteratively optimizing the quantization process, Athena updates the\nmodel parameters and Hessian matrix to achieve significant compression while\nmaintaining high accuracy. This makes Athena a practical solution for deploying\nLLMs in various settings.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 的存储和计算挑战，提出了一种高效的块级后训练量化算法Athena，以解决传统方法导致的准确性损失问题。Athena利用Second-Order Matrix Derivative Information（二阶矩阵导数信息）来指导量化过程，通过按列或行分组参数并迭代优化模型参数和Hessian矩阵，实现显著压缩的同时保持高准确性。该方法使LLMs更适合资源受限的环境，如移动设备和边缘计算平台，促进其实际部署。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17470v1",
      "published_date": "2024-05-24 03:14:29 UTC",
      "updated_date": "2024-05-24 03:14:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:48:17.560145"
    },
    {
      "arxiv_id": "2407.11974v1",
      "title": "Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone",
      "title_zh": "翻译失败",
      "authors": [
        "Catalina Gomez",
        "Ruolin Wang",
        "Katharina Breininger",
        "Corinne Casey",
        "Chris Bradley",
        "Mitchell Pavlak",
        "Alex Pham",
        "Jithin Yohannan",
        "Mathias Unberath"
      ],
      "abstract": "Primary care providers are vital for initial triage and referrals to\nspecialty care. In glaucoma, asymptomatic and fast progression can lead to\nvision loss, necessitating timely referrals to specialists. However, primary\neye care providers may not identify urgent cases, potentially delaying care.\nArtificial Intelligence (AI) offering explanations could enhance their referral\ndecisions. We investigate how various AI explanations help providers\ndistinguish between patients needing immediate or non-urgent specialist\nreferrals. We built explainable AI algorithms to predict glaucoma surgery needs\nfrom routine eyecare data as a proxy for identifying high-risk patients. We\nincorporated intrinsic and post-hoc explainability and conducted an online\nstudy with optometrists to assess human-AI team performance, measuring referral\naccuracy and analyzing interactions with AI, including agreement rates, task\ntime, and user experience perceptions. AI support enhanced referral accuracy\namong 87 participants (59.9%/50.8% with/without AI), though Human-AI teams\nunderperformed compared to AI alone. Participants believed they included AI\nadvice more when using the intrinsic model, and perceived it more useful and\npromising. Without explanations, deviations from AI recommendations increased.\nAI support did not increase workload, confidence, and trust, but reduced\nchallenges. On a separate test set, our black-box and intrinsic models achieved\nan accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We\nidentify opportunities of human-AI teaming for glaucoma management in primary\neye care, noting that while AI enhances referral accuracy, it also shows a\nperformance gap compared to AI alone, even with explanations. Human involvement\nremains essential in medical decision making, underscoring the need for future\nresearch to optimize collaboration, ensuring positive experiences and safe AI\nuse.",
      "tldr_zh": "这篇论文探讨了可解释 AI 在青光眼转诊中的作用，旨在帮助初级眼科提供者识别高风险患者以避免视力损失。研究构建了内在和后验可解释 AI 算法，并通过在线实验评估87名参与者的转诊准确性，结果显示 AI 支持将准确率从50.8%提高到59.9%，但人类-AI 团队的表现仍低于 AI 单独（黑箱模型77%、内在模型71%）。尽管参与者认为可解释 AI 更易用且未增加工作量，该研究强调了优化人类-AI 协作的重要性，以提升医疗决策的安全性和有效性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "5 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.11974v1",
      "published_date": "2024-05-24 03:01:20 UTC",
      "updated_date": "2024-05-24 03:01:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:48:30.801862"
    },
    {
      "arxiv_id": "2405.17469v1",
      "title": "A Dataset for Research on Water Sustainability",
      "title_zh": "翻译失败",
      "authors": [
        "Pranjol Sen Gupta",
        "Md Rajib Hossen",
        "Pengfei Li",
        "Shaolei Ren",
        "Mohammad A. Islam"
      ],
      "abstract": "Freshwater scarcity is a global problem that requires collective efforts\nacross all industry sectors. Nevertheless, a lack of access to operational\nwater footprint data bars many applications from exploring optimization\nopportunities hidden within the temporal and spatial variations. To break this\nbarrier into research in water sustainability, we build a dataset for operation\ndirect water usage in the cooling systems and indirect water embedded in\nelectricity generation. Our dataset consists of the hourly water efficiency of\nmajor U.S. cities and states from 2019 to 2023. We also offer cooling system\nmodels that capture the impact of weather on water efficiency. We present a\npreliminary analysis of our dataset and discuss three potential applications\nthat can benefit from it. Our dataset is publicly available at Open Science\nFramework (OSF)",
      "tldr_zh": "这篇论文构建了一个针对水可持续性研究的数据集，以解决全球淡水短缺问题中操作水足迹数据缺失的障碍。数据集涵盖2019-2023年间美国主要城市和州的每小时水效率，包括冷却系统中的直接水使用和电力生成中的间接水使用，并提供了捕捉天气影响的冷却系统模型。作者进行了初步分析，并讨论了三个潜在应用，如优化水资源管理。该数据集已公开可用于 Open Science Framework (OSF)，有助于推动相关研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ACM e-Energy 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17469v1",
      "published_date": "2024-05-24 02:59:52 UTC",
      "updated_date": "2024-05-24 02:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:48:42.207096"
    },
    {
      "arxiv_id": "2405.15165v1",
      "title": "A Solution-based LLM API-using Methodology for Academic Information Seeking",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanchun Wang",
        "Jifan Yu",
        "Zijun Yao",
        "Jing Zhang",
        "Yuyang Xie",
        "Shangqing Tu",
        "Yiyang Fu",
        "Youhe Feng",
        "Jinkai Zhang",
        "Jingyao Zhang",
        "Bowen Huang",
        "Yuanyao Li",
        "Huihui Yuan",
        "Lei Hou",
        "Juanzi Li",
        "Jie Tang"
      ],
      "abstract": "Applying large language models (LLMs) for academic API usage shows promise in\nreducing researchers' academic information seeking efforts. However, current\nLLM API-using methods struggle with complex API coupling commonly encountered\nin academic queries. To address this, we introduce SoAy, a solution-based LLM\nAPI-using methodology for academic information seeking. It uses code with a\nsolution as the reasoning method, where a solution is a pre-constructed API\ncalling sequence. The addition of the solution reduces the difficulty for the\nmodel to understand the complex relationships between APIs. Code improves the\nefficiency of reasoning.\n  To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied\nby SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental\nresults demonstrate a 34.58-75.99\\% performance improvement compared to\nstate-of-the-art LLM API-based baselines. All datasets, codes, tuned models,\nand deployed online services are publicly accessible at\nhttps://github.com/RUCKBReasoning/SoAy.",
      "tldr_zh": "该论文提出 SoAy，一种基于解决方案的 LLM API-using 方法，用于简化学术信息搜索中复杂的 API 耦合问题，通过使用代码和预构建的 API 调用序列作为推理手段，提高模型理解效率和整体性能。SoAy 结合了代码推理的优势，并在 SoAyBench 基准上使用 SoAyEval 进行评估，该基准基于 AMiner 的 API 克隆环境。实验结果显示，SoAy 相较于最先进基线提升了 34.58-75.99% 的性能，所有数据集、代码和模型均在 GitHub 上公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15165v1",
      "published_date": "2024-05-24 02:44:14 UTC",
      "updated_date": "2024-05-24 02:44:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:48:54.184371"
    },
    {
      "arxiv_id": "2405.15164v1",
      "title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
      "title_zh": "翻译失败",
      "authors": [
        "Jacob Russin",
        "Sam Whitman McGrath",
        "Danielle J. Williams",
        "Lotem Elber-Dorozko"
      ],
      "abstract": "Compositionality has long been considered a key explanatory property\nunderlying human intelligence: arbitrary concepts can be composed into novel\ncomplex combinations, permitting the acquisition of an open ended, potentially\ninfinite expressive capacity from finite learning experiences. Influential\narguments have held that neural networks fail to explain this aspect of\nbehavior, leading many to dismiss them as viable models of human cognition.\nOver the last decade, however, modern deep neural networks (DNNs), which share\nthe same fundamental design principles as their predecessors, have come to\ndominate artificial intelligence, exhibiting the most advanced cognitive\nbehaviors ever demonstrated in machines. In particular, large language models\n(LLMs), DNNs trained to predict the next word on a large corpus of text, have\nproven capable of sophisticated behaviors such as writing syntactically complex\nsentences without grammatical errors, producing cogent chains of reasoning, and\neven writing original computer programs -- all behaviors thought to require\ncompositional processing. In this chapter, we survey recent empirical work from\nmachine learning for a broad audience in philosophy, cognitive science, and\nneuroscience, situating recent breakthroughs within the broader context of\nphilosophical arguments about compositionality. In particular, our review\nemphasizes two approaches to endowing neural networks with compositional\ngeneralization capabilities: (1) architectural inductive biases, and (2)\nmetalearning, or learning to learn. We also present findings suggesting that\nLLM pretraining can be understood as a kind of metalearning, and can thereby\nequip DNNs with compositional generalization abilities in a similar way. We\nconclude by discussing the implications that these findings may have for the\nstudy of compositionality in human cognition and by suggesting avenues for\nfuture research.",
      "tldr_zh": "这篇论文探讨了组合性（compositionality）作为人类智能的核心属性，即从有限学习经验中生成无限表达能力的机制，并审视深度神经网络（DNNs）是否能解释这一特性。作者回顾了近年来机器学习领域的实证研究，强调两种赋予神经网络组合性泛化能力的策略：(1) 架构诱导偏差（architectural inductive biases），和 (2) 元学习（metalearning）。此外，论文指出大型语言模型（LLMs）的预训练可视为一种元学习形式，从而提升DNNs的组合性能力，并讨论这些发现对哲学、认知科学和神经科学研究的启示，为未来探究人类认知提供新方向。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "32 pages (50 pages including references), 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15164v1",
      "published_date": "2024-05-24 02:36:07 UTC",
      "updated_date": "2024-05-24 02:36:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:49:06.284810"
    },
    {
      "arxiv_id": "2405.15829v1",
      "title": "Spatio-temporal Value Semantics-based Abstraction for Dense Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jihui Nie",
        "Dehui Du",
        "Jiangnan Zhao"
      ],
      "abstract": "Intelligent Cyber-Physical Systems (ICPS) represent a specialized form of\nCyber-Physical System (CPS) that incorporates intelligent components, notably\nConvolutional Neural Networks (CNNs) and Deep Reinforcement Learning (DRL), to\nundertake multifaceted tasks encompassing perception, decision-making, and\ncontrol. The utilization of DRL for decision-making facilitates dynamic\ninteraction with the environment, generating control actions aimed at\nmaximizing cumulative rewards. Nevertheless, the inherent uncertainty of the\noperational environment and the intricate nature of ICPS necessitate\nexploration within complex and dynamic state spaces during the learning phase.\nDRL confronts challenges in terms of efficiency, generalization capabilities,\nand data scarcity during decision-making process. In response to these\nchallenges, we propose an innovative abstract modeling approach grounded in\nspatial-temporal value semantics, capturing the evolution in the distribution\nof semantic value across time and space. A semantics-based abstraction is\nintroduced to construct an abstract Markov Decision Process (MDP) for the DRL\nlearning process. Furthermore, optimization techniques for abstraction are\ndelineated, aiming to refine the abstract model and mitigate semantic gaps\nbetween abstract and concrete states. The efficacy of the abstract modeling is\nassessed through the evaluation and analysis of the abstract MDP model using\nPRISM. A series of experiments are conducted, involving diverse scenarios such\nas lane-keeping, adaptive cruise control, and intersection crossroad\nassistance, to demonstrate the effectiveness of our abstracting approach.",
      "tldr_zh": "该论文针对智能网络物理系统 (ICPS) 中 Deep Reinforcement Learning (DRL) 的效率、泛化能力和数据稀缺挑战，提出了一种基于时空价值语义的抽象建模方法，以捕捉语义价值在时间和空间上的演变。方法通过构建抽象的 Markov Decision Process (MDP) 来优化 DRL 学习过程，并采用优化技术减少抽象状态与具体状态之间的语义差距。论文使用 PRISM 工具评估了抽象 MDP 模型的有效性，并在车道保持、自适应巡航控制和交叉路口辅助等多样场景中进行实验，证明了该方法显著提升了 DRL 的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68N30",
        "D.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 7 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2405.15829v1",
      "published_date": "2024-05-24 02:21:10 UTC",
      "updated_date": "2024-05-24 02:21:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:49:18.881564"
    },
    {
      "arxiv_id": "2405.15154v2",
      "title": "Online Prompt Pricing based on Combinatorial Multi-Armed Bandit and Hierarchical Stackelberg Game",
      "title_zh": "基于组合多臂赌博机和分层斯塔克尔伯格博弈的在线提示定价",
      "authors": [
        "Meiling Li",
        "Hongrun Ren",
        "Haixu Xiong",
        "Zhenxing Qian",
        "Xinpeng Zhang"
      ],
      "abstract": "Generation models have shown promising performance in various tasks, making\ntrading around machine learning models possible. In this paper, we aim at a\nnovel prompt trading scenario, prompt bundle trading (PBT) system, and propose\nan online pricing mechanism. Based on the combinatorial multi-armed bandit\n(CMAB) and three-stage hierarchical Stackelburg (HS) game, our pricing\nmechanism considers the profits of the consumer, platform, and seller,\nsimultaneously achieving the profit satisfaction of these three participants.\nWe break down the pricing issue into two steps, namely unknown category\nselection and incentive strategy optimization. The former step is to select a\nset of categories with the highest qualities, and the latter is to derive the\noptimal strategy for each participant based on the chosen categories. Unlike\nthe existing fixed pricing mode, the PBT pricing mechanism we propose is more\nflexible and diverse, which is more in accord with the transaction needs of\nreal-world scenarios. We test our method on a simulated text-to-image dataset.\nThe experimental results demonstrate the effectiveness of our algorithm, which\nprovides a feasible price-setting standard for the prompt marketplaces.",
      "tldr_zh": "本论文提出了一种基于 Combinatorial Multi-Armed Bandit (CMAB) 和 Hierarchical Stackelberg Game (HS) 的在线定价机制，针对提示捆绑交易（PBT）系统，旨在同时满足消费者、平台和卖家的利润需求。定价问题分为两步：首先选择高质量的类别集合，其次优化每个参与者的激励策略，以实现更灵活多样的定价模式。实验结果在模拟文本到图像数据集上证明了该算法的有效性，为真实世界提示市场提供可行的定价标准。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15154v2",
      "published_date": "2024-05-24 02:13:46 UTC",
      "updated_date": "2024-05-31 14:01:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:49:30.079927"
    },
    {
      "arxiv_id": "2405.15152v1",
      "title": "Machine Unlearning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Saaketh Koundinya Gundavarapu",
        "Shreya Agarwal",
        "Arushi Arora",
        "Chandana Thimmalapura Jagadeeshaiah"
      ],
      "abstract": "Machine unlearning, a novel area within artificial intelligence, focuses on\naddressing the challenge of selectively forgetting or reducing undesirable\nknowledge or behaviors in machine learning models, particularly in the context\nof large language models (LLMs). This paper introduces a methodology to align\nLLMs, such as Open Pre-trained Transformer Language Models, with ethical,\nprivacy, and safety standards by leveraging the gradient ascent algorithm for\nknowledge unlearning. Our approach aims to selectively erase or modify learned\ninformation in LLMs, targeting harmful responses and copyrighted content. This\npaper presents a dual-pronged approach to enhance the ethical and safe behavior\nof large language models (LLMs) by addressing the issues of harmful responses\nand copyrighted content. To mitigate harmful responses, we applied gradient\nascent on the PKU dataset, achieving a 75\\% reduction in harmful responses for\nOpen Pre-trained Transformer Language Models (OPT1.3b and OPT2.7b)\n\\citet{zhang2022opt} while retaining previous knowledge using the TruthfulQA\ndataset \\citet{DBLP:journals/corr/abs-2109-07958}. For handling copyrighted\ncontent, we constructed a custom dataset based on the Lord of the Rings corpus\nand aligned LLMs (OPT1.3b and OPT2.7b) \\citet{zhang2022opt} through LoRA:\nLow-Rank Adaptation of Large Language Models\n\\citet{DBLP:journals/corr/abs-2106-09685} finetuning. Subsequently, we employed\ngradient ascent to unlearn the Lord of the Rings content, resulting in a\nremarkable reduction in the presence of copyrighted material. To maintain a\ndiverse knowledge base, we utilized the Book Corpus dataset. Additionally, we\npropose a new evaluation technique for assessing the effectiveness of harmful\nunlearning.",
      "tldr_zh": "本论文探讨了 Machine Unlearning 在 Large Language Models (LLMs) 中的应用，旨在通过梯度上升算法选择性地消除模型中的不良知识，如有害响应和版权内容，以提升伦理、隐私和安全标准。研究采用双重策略：针对有害响应，使用 PKU 数据集在 OPT1.3b 和 OPT2.7b 模型上进行梯度上升处理，实现了 75% 的有害响应减少，同时通过 TruthfulQA 数据集保留其他知识；针对版权内容，则构建自定义数据集（如《指环王》语料）并结合 LoRA 微调后应用梯度上升，显著降低了版权材料的出现。实验结果显示，该方法在保持知识多样性（如使用 Book Corpus 数据集）的同时，引入了新的评估技术来衡量 Machine Unlearning 的有效性，为更安全可靠的 LLMs 发展提供了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.15152v1",
      "published_date": "2024-05-24 02:12:51 UTC",
      "updated_date": "2024-05-24 02:12:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:49:42.833226"
    },
    {
      "arxiv_id": "2405.17468v2",
      "title": "Deep Activity Model: A Generative Approach for Human Mobility Pattern Synthesis",
      "title_zh": "深度活动模型：一种用于人类移动模式合成的生成式方法",
      "authors": [
        "Xishun Liao",
        "Qinhua Jiang",
        "Brian Yueshuai He",
        "Yifan Liu",
        "Chenchen Kuai",
        "Jiaqi Ma"
      ],
      "abstract": "Human mobility plays a crucial role in transportation, urban planning, and\npublic health. Advances in deep learning and the availability of diverse\nmobility data have transformed mobility modeling. However, existing deep\nlearning models often focus on spatio-temporal patterns and struggle to capture\nthe semantic interdependencies among activities, while also being limited by\nspecific data sources. These challenges reduce their realism and adaptability.\nTraditional activity-based models (ABMs) face issues as well, relying on rigid\nassumptions and requiring extensive data, making them costly and difficult to\nadapt to new regions, especially those with limited conventional travel data.\nTo address these limitations, we develop a novel generative deep learning\napproach for human mobility modeling and synthesis that incorporates both\nactivity patterns and location trajectories using open-source data. The model\ncan be fine-tuned with local data, allowing it to adapt to and accurately\nrepresent mobility patterns across diverse regions. The model is evaluated on a\nnationwide dataset of the United States, where it demonstrates superior\nperformance in generating activity-location chains that closely follow ground\ntruth distributions. Further tests using state- or city-specific datasets from\nCalifornia, Washington, and Mexico City confirm its transferability. This\ninnovative approach offers substantial potential to advance mobility modeling\nresearch, particularly in generating synthetic human mobility data. This can\nprovide urban planners and policymakers with enhanced tools for simulating\nmobility in diverse regions and better informing decisions related to\ntransportation, urban development, and public health.",
      "tldr_zh": "这篇论文提出了一种名为Deep Activity Model的生成式深度学习方法，用于合成人类移动性模式，旨在解决现有模型无法捕捉活动间语义互依赖并受限于特定数据源的问题。相比传统activity-based models (ABMs)，该方法整合活动模式和位置轨迹，利用开源数据，并支持通过本地数据微调以适应不同区域。实验结果显示，该模型在全美数据集上生成的高质量活动-位置链接近真实分布，并在加州、华盛顿和墨西哥城的特定数据集上验证了其可转移性。这种创新方法可为城市规划、交通和公共健康等领域提供更可靠的合成数据支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17468v2",
      "published_date": "2024-05-24 02:04:10 UTC",
      "updated_date": "2024-11-03 06:38:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:49:55.317293"
    },
    {
      "arxiv_id": "2405.15145v3",
      "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language Models",
      "title_zh": "CulturePark：提升大型语言模型中的跨文化理解",
      "authors": [
        "Cheng Li",
        "Damien Teney",
        "Linyi Yang",
        "Qingsong Wen",
        "Xing Xie",
        "Jindong Wang"
      ],
      "abstract": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)中的文化偏见问题，引入了CulturePark框架，这是一个基于LLM的多智能体通信系统，用于模拟跨文化对话并生成高质量样本。CulturePark通过代理角色扮演不同文化，成功生成了41,000个文化对话样本，并用于微调八个特定文化的LLMs。在下游任务评估中，这些模型在内容审核任务上匹配或超过GPT-4，在Hofstede's VSM文化一致性框架上表现优于GPT-4，并在文化教育任务中显示出更好的学习效果和用户体验，从而推动AI的民主化和文化包容性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2024; Code is released at\n  https://github.com/Scarelette/CulturePark. arXiv admin note: substantial text\n  overlap with arXiv:2402.10946",
      "pdf_url": "http://arxiv.org/pdf/2405.15145v3",
      "published_date": "2024-05-24 01:49:02 UTC",
      "updated_date": "2024-11-21 10:52:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:50:07.093983"
    },
    {
      "arxiv_id": "2405.15143v4",
      "title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models",
      "title_zh": "Intelligent Go-Explore：站在巨型基础模型的肩膀上",
      "authors": [
        "Cong Lu",
        "Shengran Hu",
        "Jeff Clune"
      ],
      "abstract": "Go-Explore is a powerful family of algorithms designed to solve\nhard-exploration problems built on the principle of archiving discovered\nstates, and iteratively returning to and exploring from the most promising\nstates. This approach has led to superhuman performance across a wide variety\nof challenging problems including Atari games and robotic control, but requires\nmanually designing heuristics to guide exploration (i.e., determine which\nstates to save and explore from, and what actions to consider next), which is\ntime-consuming and infeasible in general. To resolve this, we propose\nIntelligent Go-Explore (IGE) which greatly extends the scope of the original\nGo-Explore by replacing these handcrafted heuristics with the intelligence and\ninternalized human notions of interestingness captured by giant pretrained\nfoundation models (FMs). This provides IGE with a human-like ability to\ninstinctively identify how interesting or promising any new state is (e.g.,\ndiscovering new objects, locations, or behaviors), even in complex environments\nwhere heuristics are hard to define. Moreover, IGE offers the exciting\nopportunity to recognize and capitalize on serendipitous discoveries -- states\nencountered during exploration that are valuable in terms of exploration, yet\nwhere what makes them interesting was not anticipated by the human user. We\nevaluate our algorithm on a diverse range of language and vision-based tasks\nthat require search and exploration. Across these tasks, IGE strongly exceeds\nclassic reinforcement learning and graph search baselines, and also succeeds\nwhere prior state-of-the-art FM agents like Reflexion completely fail. Overall,\nIntelligent Go-Explore combines the tremendous strengths of FMs and the\npowerful Go-Explore algorithm, opening up a new frontier of research into\ncreating more generally capable agents with impressive exploration\ncapabilities.",
      "tldr_zh": "该研究提出Intelligent Go-Explore (IGE)，一种扩展Go-Explore算法的方法，通过利用巨型预训练基础模型 (FMs) 取代手动设计的启发式规则，实现更智能的探索决策。IGE能像人类一样评估状态的有趣性（如发现新对象或行为），并利用意外发现来提升探索效率，在复杂环境中表现出色。实验结果显示，IGE在各种语言和视觉任务上超越传统强化学习、图搜索基线，以及其他FM代理如Reflexion，开启了开发更通用智能代理的新研究方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.15143v4",
      "published_date": "2024-05-24 01:45:27 UTC",
      "updated_date": "2025-02-07 11:10:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:50:18.691913"
    },
    {
      "arxiv_id": "2405.15137v1",
      "title": "An Approximate Dynamic Programming Framework for Occlusion-Robust Multi-Object Tracking",
      "title_zh": "遮挡鲁棒多对象跟踪的近似动态规划框架",
      "authors": [
        "Pratyusha Musunuru",
        "Yuchao Li",
        "Jamison Weber",
        "Dimitri Bertsekas"
      ],
      "abstract": "In this work, we consider data association problems involving multi-object\ntracking (MOT). In particular, we address the challenges arising from object\nocclusions. We propose a framework called approximate dynamic programming track\n(ADPTrack), which applies dynamic programming principles to improve an existing\nmethod called the base heuristic. Given a set of tracks and the next target\nframe, the base heuristic extends the tracks by matching them to the objects of\nthis target frame directly. In contrast, ADPTrack first processes a few\nsubsequent frames and applies the base heuristic starting from the next target\nframe to obtain tentative tracks. It then leverages the tentative tracks to\nmatch the objects of the target frame. This tends to reduce the occlusion-based\nerrors and leads to an improvement over the base heuristic. When tested on the\nMOT17 video dataset, the proposed method demonstrates a 0.7% improvement in the\nassociation accuracy (IDF1 metric) over a state-of-the-art method that is used\nas the base heuristic. It also obtains improvements with respect to all the\nother standard metrics. Empirically, we found that the improvements are\nparticularly pronounced in scenarios where the video data is obtained by\nfixed-position cameras.",
      "tldr_zh": "本文提出了一种基于近似动态编程 (Approximate Dynamic Programming) 的框架 ADPTrack，用于提升多对象跟踪 (Multi-Object Tracking) 的遮挡鲁棒性 (Occlusion-Robust)。该框架改进基础启发式方法 (base heuristic)，通过先处理后续帧生成临时轨迹，然后利用这些轨迹匹配目标帧的对象，从而减少遮挡导致的关联错误。在 MOT17 数据集上，ADPTrack 比最先进方法在关联准确率 (IDF1 指标) 上提高了 0.7%，并在其他标准指标上均有改善，尤其在固定位置摄像机拍摄的场景中表现突出。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15137v1",
      "published_date": "2024-05-24 01:27:14 UTC",
      "updated_date": "2024-05-24 01:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:50:31.662884"
    },
    {
      "arxiv_id": "2405.15127v1",
      "title": "Benchmarking Hierarchical Image Pyramid Transformer for the classification of colon biopsies and polyps in histopathology images",
      "title_zh": "翻译失败",
      "authors": [
        "Nohemi Sofia Leon Contreras",
        "Marina D'Amato",
        "Francesco Ciompi",
        "Clement Grisi",
        "Witali Aswolinskiy",
        "Simona Vatrano",
        "Filippo Fraggetta",
        "Iris Nagtegaal"
      ],
      "abstract": "Training neural networks with high-quality pixel-level annotation in\nhistopathology whole-slide images (WSI) is an expensive process due to\ngigapixel resolution of WSIs. However, recent advances in self-supervised\nlearning have shown that highly descriptive image representations can be\nlearned without the need for annotations. We investigate the application of the\nrecent Hierarchical Image Pyramid Transformer (HIPT) model for the specific\ntask of classification of colorectal biopsies and polyps. After evaluating the\neffectiveness of TCGA-learned features in the original HIPT model, we\nincorporate colon biopsy image information into HIPT's pretraining using two\ndistinct strategies: (1) fine-tuning HIPT from the existing TCGA weights and\n(2) pretraining HIPT from random weight initialization. We compare the\nperformance of these pretraining regimes on two colorectal biopsy\nclassification tasks: binary and multiclass classification.",
      "tldr_zh": "本研究评估了 Hierarchical Image Pyramid Transformer (HIPT) 模型在组织病理图像中分类结肠活检和息肉的任务中表现，以解决全滑微镜图像 (WSI) 高分辨率导致的标注成本问题。研究者利用自监督学习，测试了从 TCGA 学到的特征效果，并引入两种策略将结肠活检图像信息融入 HIPT 的预训练：（1）从现有 TCGA 权重微调模型，（2）从随机权重初始化预训练。实验结果通过在二元分类和多类分类任务上的性能比较，展示了这些方法的有效性，为无标注训练的组织病理学应用提供了新见解。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "4 pages, 3 figures, to be published in the 2024 IEEE International\n  Symposium on Biomedical Imaging (ISBI) proceedings",
      "pdf_url": "http://arxiv.org/pdf/2405.15127v1",
      "published_date": "2024-05-24 00:59:30 UTC",
      "updated_date": "2024-05-24 00:59:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:50:44.241097"
    },
    {
      "arxiv_id": "2405.15124v4",
      "title": "Scaling Law for Time Series Forecasting",
      "title_zh": "时间序列预测的缩放定律",
      "authors": [
        "Jingzhe Shi",
        "Qinwei Ma",
        "Huan Ma",
        "Lei Li"
      ],
      "abstract": "Scaling law that rewards large datasets, complex models and enhanced data\ngranularity has been observed in various fields of deep learning. Yet, studies\non time series forecasting have cast doubt on scaling behaviors of deep\nlearning methods for time series forecasting: while more training data improves\nperformance, more capable models do not always outperform less capable models,\nand longer input horizons may hurt performance for some models. We propose a\ntheory for scaling law for time series forecasting that can explain these\nseemingly abnormal behaviors. We take into account the impact of dataset size\nand model complexity, as well as time series data granularity, particularly\nfocusing on the look-back horizon, an aspect that has been unexplored in\nprevious theories. Furthermore, we empirically evaluate various models using a\ndiverse set of time series forecasting datasets, which (1) verifies the\nvalidity of scaling law on dataset size and model complexity within the realm\nof time series forecasting, and (2) validates our theoretical framework,\nparticularly regarding the influence of look back horizon. We hope our findings\nmay inspire new models targeting time series forecasting datasets of limited\nsize, as well as large foundational datasets and models for time series\nforecasting in future work. Code for our experiments has been made public at\nhttps://github.com/JingzheShi/ScalingLawForTimeSeriesForecasting.",
      "tldr_zh": "这篇论文探讨了时间序列预测（time series forecasting）中的扩展法则（scaling law），解释了为什么更大数据集能提升性能，但更复杂模型或更长输入 horizon 不总能带来好处。作者提出一个新理论框架，考虑数据集大小、模型复杂度和数据粒度，特别是未被先前理论探索的回顾窗口（look-back horizon）影响。通过实验在多种时间序列数据集上验证，该框架证实了数据集规模和模型复杂度的扩展效应，并强调回顾窗口的作用。这些发现有望激发针对有限数据集的新模型设计，以及未来大型基础数据集和模型的开发。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15124v4",
      "published_date": "2024-05-24 00:46:27 UTC",
      "updated_date": "2024-11-09 19:21:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:50:55.046831"
    },
    {
      "arxiv_id": "2405.15828v1",
      "title": "Oil & Water? Diffusion of AI Within and Across Scientific Fields",
      "title_zh": "油与水？AI 在科学领域内部与跨领域的扩散",
      "authors": [
        "Eamon Duede",
        "William Dolan",
        "André Bauer",
        "Ian Foster",
        "Karim Lakhani"
      ],
      "abstract": "This study empirically investigates claims of the increasing ubiquity of\nartificial intelligence (AI) within roughly 80 million research publications\nacross 20 diverse scientific fields, by examining the change in scholarly\nengagement with AI from 1985 through 2022. We observe exponential growth, with\nAI-engaged publications increasing approximately thirteenfold (13x) across all\nfields, suggesting a dramatic shift from niche to mainstream. Moreover, we\nprovide the first empirical examination of the distribution of AI-engaged\npublications across publication venues within individual fields, with results\nthat reveal a broadening of AI engagement within disciplines. While this\nbroadening engagement suggests a move toward greater disciplinary integration\nin every field, increased ubiquity is associated with a semantic tension\nbetween AI-engaged research and more traditional disciplinary research. Through\nan analysis of tens of millions of document embeddings, we observe a complex\ninterplay between AI-engaged and non-AI-engaged research within and across\nfields, suggesting that increasing ubiquity is something of an oil-and-water\nphenomenon -- AI-engaged work is spreading out over fields, but not mixing well\nwith non-AI-engaged work.",
      "tldr_zh": "这篇论文实证调查了人工智能（AI）在20个科学领域约80百万研究出版物中的扩散情况，从1985年至2022年。研究发现，AI相关出版物呈指数级增长，约增加了13倍，从利基领域扩展到主流，并在各学科内出版场所分布更趋广泛。虽有迹象显示AI参与促进了学科整合，但通过分析数千万文档embeddings，论文揭示了AI相关研究与传统研究之间存在的语义张力，类似于“油和水”现象——AI工作在跨领域扩散，但未充分混合。总的来说，这突显了AI在科学界日益普及的复杂性。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15828v1",
      "published_date": "2024-05-24 00:39:32 UTC",
      "updated_date": "2024-05-24 00:39:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:51:08.375967"
    },
    {
      "arxiv_id": "2405.15116v2",
      "title": "Quantifying the Gain in Weak-to-Strong Generalization",
      "title_zh": "量化弱到强泛化的增益",
      "authors": [
        "Moses Charikar",
        "Chirag Pabbaraju",
        "Kirankumar Shiragur"
      ],
      "abstract": "Recent advances in large language models have shown capabilities that are\nextraordinary and near-superhuman. These models operate with such complexity\nthat reliably evaluating and aligning them proves challenging for humans. This\nleads to the natural question: can guidance from weak models (like humans)\nadequately direct the capabilities of strong models? In a recent and somewhat\nsurprising work, Burns et al. (2023) empirically demonstrated that when strong\nmodels (like GPT-4) are finetuned using labels generated by weak supervisors\n(like GPT-2), the strong models outperform their weaker counterparts -- a\nphenomenon they term weak-to-strong generalization.\n  In this work, we present a theoretical framework for understanding\nweak-to-strong generalization. Specifically, we show that the improvement in\nperformance achieved by strong models over their weaker counterparts is\nquantified by the misfit error incurred by the strong model on labels generated\nby the weaker model. Our theory reveals several curious algorithmic insights.\nFor instance, we can predict the amount by which the strong model will improve\nover the weak model, and also choose among different weak models to train the\nstrong model, based on its misfit error. We validate our theoretical findings\nthrough various empirical assessments.",
      "tldr_zh": "这篇论文探讨了弱模型（如人类或较弱AI）指导强模型（如GPT-4）的现象，即weak-to-strong generalization，量化了这种指导带来的性能提升。作者提出一个理论框架，表明强模型的改进量取决于其对弱模型生成标签的misfit error（不匹配误差），并揭示了算法洞见，如预测提升幅度和选择最佳弱模型。实验验证显示，该框架能有效指导模型优化，为理解和应用弱监督训练提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages; NeurIPS 2024 camera-ready version with additional\n  experiments, references and discussion",
      "pdf_url": "http://arxiv.org/pdf/2405.15116v2",
      "published_date": "2024-05-24 00:14:16 UTC",
      "updated_date": "2024-10-23 03:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T11:51:18.570683"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 135,
  "processed_papers_count": 135,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T11:51:39.422254"
}
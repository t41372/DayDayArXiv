{
  "date": "2024-01-25",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-01-25 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和大型语言模型（LLM）的创新应用，包括提示工程、代理系统、多模态生成和跨领域学习等领域，其中最令人印象深刻的是 LLM 在情感分析、临床预测和代理自我演化的新框架，如 Investigate-Consolidate-Exploit，以及多篇涉及知名模型（如 ChatGPT 和 GPT-4）的比较研究。\n\n### 重点论文讨论\n我挑选了最具话题性和影响力的论文进行简要分析，先从 LLM 相关主题入手，再扩展到代理和生成模型领域。其他论文如常规优化或特定领域应用（如交通数据或音频处理），由于相对常规或不那么突破性，我会快速掠过。\n\n1. **ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis (ChatGPT 与 Gemini 与 LLaMA 在多语言情感分析中的比较)**  \n   这篇论文比较了 ChatGPT、Gemini 和 LLaMA 在处理模糊和讽刺文本的多语言情感分析中的性能。主要贡献是通过构建复杂场景并与人工验证对比，发现这些 LLM 在处理歧义时表现出偏差，但 ChatGPT 和 Gemini 整体更稳定。发现：LLM 在情感分析中存在语言偏置，呼吁改进算法以提升鲁棒性。\n\n2. **Relative Value Biases in Large Language Models (大型语言模型中的相对价值偏差)**  \n   作者包括 Stefano Palminteri 等知名学者，探讨 LLM（如 GPT-4 Turbo）在决策中的偏差，类似于人类强化学习。主要贡献：通过实验证明 LLM 偏好相对更好过去的选项，并通过提示调整可消除偏差。发现：这揭示了 LLM 在上下文依赖决策中的潜在机制，对 AI 伦理和决策系统有重要启示。\n\n3. **Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data (使用结构化纵向电子健康记录数据提示大型语言模型进行零-shot 临床预测)**  \n   这篇论文展示了 LLM（如 GPT-4）在医疗领域的潜力。主要贡献：提出一种提示框架，使 LLM 在零-shot 设置下提升临床预测性能（如死亡率预测），比传统 ML 模型高 35%。发现：LLM 可处理新兴疾病决策，但需考虑 EHR 的稀疏性和上下文对齐。\n\n4. **Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms (迈向集体超级智能：使用对话群组放大群体 IQ)**  \n   作者包括 Louis Rosenberg 等专家，探索 LLM 在提升群体智能中的作用。主要贡献：使用对话群组平台（如 Thinkscape），使群体在 IQ 测试中提升 28 分。发现：这为 LLM 驱动的集体决策提供新路径，强调可扩展性。\n\n5. **Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution (调查-整合-利用：一种用于跨任务代理自我演化的通用策略)**  \n   这篇论文提出一个新框架，用于 AI 代理在动态环境中的自我演化。主要贡献：通过两层结构（高层策略由 LLM 生成，低层策略优化执行），显著提升代理在任务转移中的效率。发现：在实验中，代理性能超过基线，展示了 LLM 在强化学习中的潜力。\n\n6. **BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models (BootPIG：为预训练扩散模型引导零-shot 个性化图像生成能力)**  \n   这篇论文聚焦图像生成。主要贡献：提出一个框架，使用参考图像引导扩散模型进行个性化生成，提升生成质量。发现：在实验中，模型在零-shot 设置下显著优于基线，适用于各种任务。\n\n7. **WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models (WebVoyager：使用大型多模态模型构建端到端 Web 代理)**  \n   这篇论文构建了一个多模态 Web 代理系统。主要贡献：WebVoyager 可处理真实网站任务，结合 LLM 和视觉输入，实现高成功率（59.1%）。发现：代理在实际场景中超越 GPT-4，强调多模态融合的重要性。\n\n其他论文，如常规的交通数据收集或音频处理方法（如第12和第30篇），虽有贡献但不那么引人注目，我仅快速提及：它们优化了特定领域效率，但未带来革命性突破。总之，今天的 arXiv 突显了 LLM 在实际应用中的潜力，推动 AI 向更智能、更可解释的方向发展。",
  "papers": [
    {
      "arxiv_id": "2401.17319v1",
      "title": "Decentralized Federated Learning: A Survey on Security and Privacy",
      "title_zh": "去中心化联邦学习：关于安全性和隐私性的调查",
      "authors": [
        "Ehsan Hallaji",
        "Roozbeh Razavi-Far",
        "Mehrdad Saif",
        "Boyu Wang",
        "Qiang Yang"
      ],
      "abstract": "Federated learning has been rapidly evolving and gaining popularity in recent\nyears due to its privacy-preserving features, among other advantages.\nNevertheless, the exchange of model updates and gradients in this architecture\nprovides new attack surfaces for malicious users of the network which may\njeopardize the model performance and user and data privacy. For this reason,\none of the main motivations for decentralized federated learning is to\neliminate server-related threats by removing the server from the network and\ncompensating for it through technologies such as blockchain. However, this\nadvantage comes at the cost of challenging the system with new privacy threats.\nThus, performing a thorough security analysis in this new paradigm is\nnecessary. This survey studies possible variations of threats and adversaries\nin decentralized federated learning and overviews the potential defense\nmechanisms. Trustability and verifiability of decentralized federated learning\nare also considered in this study.",
      "tldr_zh": "这篇调查论文探讨了去中心化联邦学习（Decentralized Federated Learning）中的安全性和隐私问题，强调了联邦学习虽具有隐私保护优势，但模型更新交换易受恶意攻击影响。论文分析了去中心化方法（如使用区块链移除服务器）如何减少服务器相关威胁，却同时引入新隐私风险，并概述了可能的威胁变体、攻击者类型和防御机制。最终，它强调了对可信性和可验证性的考虑，为构建更安全可靠的去中心化系统提供了全面分析。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted for publication in IEEE Transactions on Big Data",
      "pdf_url": "http://arxiv.org/pdf/2401.17319v1",
      "published_date": "2024-01-25 23:35:47 UTC",
      "updated_date": "2024-01-25 23:35:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:13:07.877768"
    },
    {
      "arxiv_id": "2402.01715v1",
      "title": "ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Alessio Buscemi",
        "Daniele Proverbio"
      ],
      "abstract": "Automated sentiment analysis using Large Language Model (LLM)-based models\nlike ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic\nresearch and in industrial applications. However, assessment and validation of\ntheir performance in case of ambiguous or ironic text is still poor. In this\nstudy, we constructed nuanced and ambiguous scenarios, we translated them in 10\nlanguages, and we predicted their associated sentiment using popular LLMs. The\nresults are validated against post-hoc human responses. Ambiguous scenarios are\noften well-coped by ChatGPT and Gemini, but we recognise significant biases and\ninconsistent performance across models and evaluated human languages. This work\nprovides a standardised methodology for automated sentiment analysis evaluation\nand makes a call for action to further improve the algorithms and their\nunderlying data, to improve their performance, interpretability and\napplicability.",
      "tldr_zh": "本研究比较了 ChatGPT、Gemini 和 LLaMA2 在多语言情感分析中的性能，特别关注模糊或讽刺文本的处理。研究者构建了细微和模糊场景，将其翻译成10种语言，使用这些 Large Language Model (LLM) 进行情感预测，并与人类响应进行验证。结果显示，ChatGPT 和 Gemini 能较好应对模糊场景，但存在显著偏差和模型间、语言间的性能不一致性。该工作提出了一种标准化的自动情感分析评估方法，并呼吁改进算法和底层数据，以提升模型的性能、可解释性和适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01715v1",
      "published_date": "2024-01-25 23:15:45 UTC",
      "updated_date": "2024-01-25 23:15:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:13:21.634961"
    },
    {
      "arxiv_id": "2401.14559v1",
      "title": "Language Modelling Approaches to Adaptive Machine Translation",
      "title_zh": "语言建模方法在自适应机器翻译中的应用",
      "authors": [
        "Yasmin Moslem"
      ],
      "abstract": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nin-domain data scarcity is common in translation settings, due to the lack of\nspecialised datasets and terminology, or inconsistency and inaccuracy of\navailable in-domain translations. In such scenarios where there is insufficient\nin-domain data to fine-tune MT models, producing translations that are\nconsistent with the relevant context is challenging. While real-time adaptation\ncan make use of smaller amounts of in-domain data to improve the translation on\nthe fly, it remains challenging due to supported context limitations and\nefficiency constraints. Large language models (LLMs) have recently shown\ninteresting capabilities of in-context learning, where they learn to replicate\ncertain input-output text generation patterns, without further fine-tuning.\nSuch capabilities have opened new horizons for domain-specific data\naugmentation and real-time adaptive MT. This work attempts to address two main\nrelevant questions: 1) in scenarios involving human interaction and continuous\nfeedback, can we employ language models to improve the quality of adaptive MT\nat inference time? and 2) in the absence of sufficient in-domain data, can we\nuse pre-trained large-scale language models to improve the process of MT domain\nadaptation?",
      "tldr_zh": "这篇论文探讨了机器翻译（MT）中保持一致性的挑战，特别是适应预先批准的术语和更正翻译的问题，尤其在领域数据稀缺的情况下。作者提出利用大型语言模型（LLMs）的 in-context learning 能力，进行实时适应和领域特定数据增强，从而在无需进一步微调模型的情况下提升翻译质量。论文针对两个关键问题进行研究：1) 在人类互动和持续反馈的场景中，LLMs 是否能实时改进自适应 MT；2) 在缺乏足够领域数据时，预训练的 LLMs 是否能优化 MT 领域适应过程。这些方法为高效、灵活的机器翻译应用提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD thesis",
      "pdf_url": "http://arxiv.org/pdf/2401.14559v1",
      "published_date": "2024-01-25 23:02:54 UTC",
      "updated_date": "2024-01-25 23:02:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:13:32.237855"
    },
    {
      "arxiv_id": "2402.06640v1",
      "title": "Modeling and Optimization of Epidemiological Control Policies Through Reinforcement Learning",
      "title_zh": "通过强化学习对流行病学控制策略进行建模和优化",
      "authors": [
        "Ishir Rao"
      ],
      "abstract": "Pandemics involve the high transmission of a disease that impacts global and\nlocal health and economic patterns. The impact of a pandemic can be minimized\nby enforcing certain restrictions on a community. However, while minimizing\ninfection and death rates, these restrictions can also lead to economic crises.\nEpidemiological models help propose pandemic control strategies based on\nnon-pharmaceutical interventions such as social distancing, curfews, and\nlockdowns, reducing the economic impact of these restrictions. However,\ndesigning manual control strategies while considering disease spread and\neconomic status is non-trivial. Optimal strategies can be designed through\nmulti-objective reinforcement learning (MORL) models, which demonstrate how\nrestrictions can be used to optimize the outcome of a pandemic. In this\nresearch, we utilized an epidemiological Susceptible, Exposed, Infected,\nRecovered, Deceased (SEIRD) model: a compartmental model for virtually\nsimulating a pandemic day by day. We combined the SEIRD model with a deep\ndouble recurrent Q-network to train a reinforcement learning agent to enforce\nthe optimal restriction on the SEIRD simulation based on a reward function. We\ntested two agents with unique reward functions and pandemic goals to obtain two\nstrategies. The first agent placed long lockdowns to reduce the initial spread\nof the disease, followed by cyclical and shorter lockdowns to mitigate the\nresurgence of the disease. The second agent provided similar infection rates\nbut an improved economy by implementing a 10-day lockdown and 20-day\nno-restriction cycle. This use of reinforcement learning and epidemiological\nmodeling allowed for both economic and infection mitigation in multiple\npandemic scenarios.",
      "tldr_zh": "这篇论文使用强化学习（Reinforcement Learning）来建模和优化流行病控制策略，结合SEIRD模型模拟大流行病传播，并通过多目标强化学习（MORL）平衡感染率和经济影响。研究训练了两个RL代理：第一个采用长封锁减少初始传播后循环短封锁，第二个实施10天封锁加20天无限制周期，以实现类似感染控制但更好的经济表现。实验结果证明，这种方法在多种大流行场景中有效缓解了感染和经济双重风险，为制定自动控制策略提供了新途径。",
      "categories": [
        "cs.AI",
        "q-bio.PE"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.06640v1",
      "published_date": "2024-01-25 22:39:39 UTC",
      "updated_date": "2024-01-25 22:39:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:13:44.535126"
    },
    {
      "arxiv_id": "2401.14542v1",
      "title": "Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model",
      "title_zh": "翻译失败",
      "authors": [
        "Julia Barnett",
        "Hugo Flores Garcia",
        "Bryan Pardo"
      ],
      "abstract": "Every artist has a creative process that draws inspiration from previous\nartists and their works. Today, \"inspiration\" has been automated by generative\nmusic models. The black box nature of these models obscures the identity of the\nworks that influence their creative output. As a result, users may\ninadvertently appropriate, misuse, or copy existing artists' works. We\nestablish a replicable methodology to systematically identify similar pieces of\nmusic audio in a manner that is useful for understanding training data\nattribution. A key aspect of our approach is to harness an effective music\naudio similarity measure. We compare the effect of applying CLMR and CLAP\nembeddings to similarity measurement in a set of 5 million audio clips used to\ntrain VampNet, a recent open source generative music model. We validate this\napproach with a human listening study. We also explore the effect that\nmodifications of an audio example (e.g., pitch shifting, time stretching,\nbackground noise) have on similarity measurements. This work is foundational to\nincorporating automated influence attribution into generative modeling, which\npromises to let model creators and users move from ignorant appropriation to\ninformed creation. Audio samples that accompany this paper are available at\nhttps://tinyurl.com/exploring-musical-roots.",
      "tldr_zh": "本研究探讨了如何通过音频嵌入技术（如 CLMR 和 CLAP embeddings）实现生成音乐模型（如 VampNet）的训练数据归属，旨在识别模型对现有音乐作品的影响来源。该方法在500万音频片段上测试音频相似性测量，并通过人类听力研究验证其有效性，同时分析音频修改（如 pitch shifting、time stretching 和 background noise）对相似性的影响。实验结果为将自动影响归属整合到生成模型中提供了基础，促进用户从无知挪用到知情创作。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "14 pages + references. Under conference review",
      "pdf_url": "http://arxiv.org/pdf/2401.14542v1",
      "published_date": "2024-01-25 22:20:42 UTC",
      "updated_date": "2024-01-25 22:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:13:54.881179"
    },
    {
      "arxiv_id": "2401.14530v1",
      "title": "Relative Value Biases in Large Language Models",
      "title_zh": "大语言模型中的相对价值偏差",
      "authors": [
        "William M. Hayes",
        "Nicolas Yax",
        "Stefano Palminteri"
      ],
      "abstract": "Studies of reinforcement learning in humans and animals have demonstrated a\npreference for options that yielded relatively better outcomes in the past,\neven when those options are associated with lower absolute reward. The present\nstudy tested whether large language models would exhibit a similar bias. We had\ngpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between\npairs of options with the goal of maximizing payoffs. A complete record of\nprevious outcomes was included in each prompt. Both models exhibited relative\nvalue decision biases similar to those observed in humans and animals. Making\nrelative comparisons among outcomes more explicit magnified the bias, whereas\nprompting the models to estimate expected outcomes caused the bias to\ndisappear. These results have implications for the potential mechanisms that\ncontribute to context-dependent choice in human agents.",
      "tldr_zh": "这篇论文研究了大型语言模型（Large Language Models）是否像人类和动物一样存在相对价值偏见（Relative Value Biases），即偏好过去相对更好选项，即使绝对回报较低。研究者通过让 GPT-4 Turbo 和 Llama-2-70B 在包含历史结果的提示下进行重复选择实验，以最大化回报。结果显示，两个模型都表现出类似于人类和动物的决策偏见；更明确地比较相对结果会放大偏见，而提示模型估计预期结果则会消除偏见。这些发现为理解人类代理中的上下文依赖选择（context-dependent choice）机制提供了重要启示。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14530v1",
      "published_date": "2024-01-25 21:49:32 UTC",
      "updated_date": "2024-01-25 21:49:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:14:08.279446"
    },
    {
      "arxiv_id": "2401.14524v1",
      "title": "Evaluating GPT-3.5's Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics",
      "title_zh": "翻译失败",
      "authors": [
        "Candida M. Greco",
        "A. Tagarelli"
      ],
      "abstract": "Constitutions are foundational legal documents that underpin the governmental\nand societal structures. As such, they are a reflection of a nation's cultural\nand social uniqueness, but also contribute to establish topics of universal\nimportance, like citizens' rights and duties (RD). In this work, using the\nrenowned GPT-3.5, we leverage generative large language models to understand\nconstitutional passages that transcend national boundaries. A key contribution\nof our study is the introduction of a novel application of abstractive\nsummarization on a multi-source collection of constitutional texts, with a\nfocus on European countries' constitution passages related to RD topics. Our\nresults show the meaningfulness of GPT-3.5 to produce informative, coherent and\nfaithful summaries capturing RD topics across European countries.",
      "tldr_zh": "本研究评估了GPT-3.5在处理欧洲宪法文本中的感知和总结能力，重点关注共享主题如公民权利和义务（RD）。研究引入了一种新颖的抽象总结（abstractive summarization）应用，针对多源欧洲国家宪法段落，旨在理解超越国界的普遍主题。结果显示，GPT-3.5能够生成信息丰富、一致且忠实的总结，有效捕捉欧洲国家间的RD主题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.DL",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14524v1",
      "published_date": "2024-01-25 21:34:53 UTC",
      "updated_date": "2024-01-25 21:34:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:14:18.656991"
    },
    {
      "arxiv_id": "2401.14523v1",
      "title": "Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do",
      "title_zh": "翻译失败",
      "authors": [
        "William Kidder",
        "Jason D'Cruz",
        "Kush R. Varshney"
      ],
      "abstract": "Advances in the performance of large language models (LLMs) have led some\nresearchers to propose the emergence of theory of mind (ToM) in artificial\nintelligence (AI). LLMs can attribute beliefs, desires, intentions, and\nemotions, and they will improve in their accuracy. Rather than employing the\ncharacteristically human method of empathy, they learn to attribute mental\nstates by recognizing linguistic patterns in a dataset that typically do not\ninclude that individual. We ask whether LLMs' inability to empathize precludes\nthem from honoring an individual's right to be an exception, that is, from\nmaking assessments of character and predictions of behavior that reflect\nappropriate sensitivity to a person's individuality. Can LLMs seriously\nconsider an individual's claim that their case is different based on internal\nmental states like beliefs, desires, and intentions, or are they limited to\njudging that case based on its similarities to others? We propose that the\nmethod of empathy has special significance for honoring the right to be an\nexception that is distinct from the value of predictive accuracy, at which LLMs\nexcel. We conclude by considering whether using empathy to consider exceptional\ncases has intrinsic or merely practical value and we introduce conceptual and\nempirical avenues for advancing this investigation.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在理论思维（ToM）方面的进展及其局限性，指出 LLMs 能通过识别语言模式来归因信念、欲望、意图和情绪，但缺乏人类的移情方法。作者论证了 LLMs 无法真正尊重个体的“例外权”，即基于个人内部心理状态进行个性化评估，而倾向于依赖案例相似性。论文强调，移情在处理独特案例时具有独特价值，超越了 LLMs 的预测准确性，并提出概念和实证路径来进一步研究移情的作用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14523v1",
      "published_date": "2024-01-25 21:30:06 UTC",
      "updated_date": "2024-01-25 21:30:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:14:31.984516"
    },
    {
      "arxiv_id": "2401.14521v4",
      "title": "Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan-Heng Wang",
        "Hoshin V. Gupta"
      ],
      "abstract": "We investigate the applicability of machine learning technologies to the\ndevelopment of parsimonious, interpretable, catchment-scale hydrologic models\nusing directed-graph architectures based on the mass-conserving perceptron\n(MCP) as the fundamental computational unit. Here, we focus on architectural\ncomplexity (depth) at a single location, rather than universal applicability\n(breadth) across large samples of catchments. The goal is to discover a minimal\nrepresentation (numbers of cell-states and flow paths) that represents the\ndominant processes that can explain the input-state-output behaviors of a given\ncatchment, with particular emphasis given to simulating the full range (high,\nmedium, and low) of flow dynamics. We find that a HyMod Like architecture with\nthree cell-states and two major flow pathways achieves such a representation at\nour study location, but that the additional incorporation of an input-bypass\nmechanism significantly improves the timing and shape of the hydrograph, while\nthe inclusion of bi-directional groundwater mass exchanges significantly\nenhances the simulation of baseflow. Overall, our results demonstrate the\nimportance of using multiple diagnostic metrics for model evaluation, while\nhighlighting the need for properly selecting and designing the training metrics\nbased on information-theoretic foundations that are better suited to extracting\ninformation across the full range of flow dynamics. This study sets the stage\nfor interpretable regional-scale MCP-based hydrological modeling (using large\nsample data) by using neural architecture search to determine appropriate\nminimal representations for catchments in different hydroclimatic regimes.",
      "tldr_zh": "本研究探讨了使用 mass-conserving perceptron (MCP) 作为基本计算单元的定向图架构，来开发简约且可解释的流域规模水文模型，重点关注单一地点的架构深度而非跨流域普适性。研究目标是寻找最小表示（包括细胞状态和流路径数量），以准确模拟流域的输入-状态-输出行为，特别是全范围流量动态（如高、中、低流量）。结果显示，一个类似于 HyMod 的架构（三个细胞状态和两个主要流路径）能有效捕捉主导过程，但添加输入旁路机制和双向地下水交换显著改善了流量图的时序和形状，以及基流的模拟。总体上，该研究强调使用多种诊断指标和基于信息理论的训练指标进行模型评估，并为通过 neural architecture search 实现区域规模 MCP 基水文建模奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "65 pages, 8 Figures, 4 Tables, 1 Supplementary Material",
      "pdf_url": "http://arxiv.org/pdf/2401.14521v4",
      "published_date": "2024-01-25 21:26:49 UTC",
      "updated_date": "2024-07-28 22:59:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:14:44.595697"
    },
    {
      "arxiv_id": "2402.10920v1",
      "title": "Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Tomlinson",
        "Joe Li",
        "Andreas Andreou"
      ],
      "abstract": "Large language models (LLMs) have made headlines for synthesizing\ncorrect-sounding responses to a variety of prompts, including code generation.\nIn this paper, we present the prompts used to guide ChatGPT4 to produce a\nsynthesizable and functional verilog description for the entirety of a\nprogrammable Spiking Neuron Array ASIC. This design flow showcases the current\nstate of using ChatGPT4 for natural language driven hardware design. The\nAI-generated design was verified in simulation using handcrafted testbenches\nand has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5\nusing an open-source EDA flow.",
      "tldr_zh": "该研究探索了使用大型语言模型(LLMs)如ChatGPT4自动生成硬件设计代码，具体针对一个可编程的Spiking Neuron Array ASIC。研究者设计了特定提示来引导ChatGPT4产生可合成和功能性的Verilog描述，从而实现自然语言驱动的硬件设计流程。该生成的代码通过手工测试台在模拟中验证，并已提交至Skywater 130nm工艺进行制造，使用开源EDA流程，展示了LLMs在硅脑设计领域的潜力。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10920v1",
      "published_date": "2024-01-25 21:21:38 UTC",
      "updated_date": "2024-01-25 21:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:14:54.899442"
    },
    {
      "arxiv_id": "2401.14511v1",
      "title": "Automated legal reasoning with discretion to act using s(LAW)",
      "title_zh": "翻译失败",
      "authors": [
        "Joaquín Arias",
        "Mar Moreno-Rebato",
        "José A. Rodríguez-García",
        "Sascha Ossowski"
      ],
      "abstract": "Automated legal reasoning and its application in smart contracts and\nautomated decisions are increasingly attracting interest. In this context,\nethical and legal concerns make it necessary for automated reasoners to justify\nin human-understandable terms the advice given. Logic Programming, specially\nAnswer Set Programming, has a rich semantics and has been used to very\nconcisely express complex knowledge. However, modelling discretionality to act\nand other vague concepts such as ambiguity cannot be expressed in top-down\nexecution models based on Prolog, and in bottom-up execution models based on\nASP the justifications are incomplete and/or not scalable. We propose to use\ns(CASP), a top-down execution model for predicate ASP, to model vague concepts\nfollowing a set of patterns. We have implemented a framework, called s(LAW), to\nmodel, reason, and justify the applicable legislation and validate it by\ntranslating (and benchmarking) a representative use case, the criteria for the\nadmission of students in the \"Comunidad de Madrid\".",
      "tldr_zh": "本研究探讨了自动化法律推理在智能合约和决策中的应用，强调需要以人类可理解方式提供理由，但现有模型如Answer Set Programming (ASP) 在处理自由裁量权(discretionality to act)和模糊概念时存在局限。论文提出使用s(CASP)，一种基于ASP的顶层执行模型，通过一组模式来建模这些模糊概念，并开发了s(LAW)框架，用于建模、推理和证明适用的法律。作者通过翻译并基准测试马德里社区学生录取标准这一代表性用例，验证了框架的有效性，提供更完整且可扩展的理由。",
      "categories": [
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14511v1",
      "published_date": "2024-01-25 21:11:08 UTC",
      "updated_date": "2024-01-25 21:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:15:09.864909"
    },
    {
      "arxiv_id": "2401.14504v1",
      "title": "Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Ruixuan Zhang",
        "Wenyu Han",
        "Zilin Bian",
        "Kaan Ozbay",
        "Chen Feng"
      ],
      "abstract": "Collecting traffic data is crucial for transportation systems and urban\nplanning, and is often more desirable through easy-to-deploy but\npower-constrained devices, due to the unavailability or high cost of power and\nnetwork infrastructure. The limited power means an inevitable trade-off between\ndata collection duration and accuracy/resolution. We introduce a novel\nlearning-based framework that strategically decides observation timings for\nbattery-powered devices and reconstructs the full data stream from sparsely\nsampled observations, resulting in minimal performance loss and a significantly\nprolonged system lifetime. Our framework comprises a predictor, a controller,\nand an estimator. The predictor utilizes historical data to forecast future\ntrends within a fixed time horizon. The controller uses the forecasts to\ndetermine the next optimal timing for data collection. Finally, the estimator\nreconstructs the complete data profile from the sampled observations. We\nevaluate the performance of the proposed method on PeMS data by an RNN\n(Recurrent Neural Network) predictor and estimator, and a DRQN (Deep Recurrent\nQ-Network) controller, and compare it against the baseline that uses Kalman\nfilter and uniform sampling. The results indicate that our method outperforms\nthe baseline, primarily due to the inclusion of more representative data points\nin the profile, resulting in an overall 10\\% improvement in estimation\naccuracy. Source code will be publicly available.",
      "tldr_zh": "这篇论文提出了一种学习-based框架，用于在电力受限设备上优化长期交通数据收集，通过战略决定观察时机并从稀疏采样重建完整数据流，从而在延长系统寿命的同时最小化准确性损失。框架由预测器（使用RNN基于历史数据预测未来趋势）、控制器（采用DRQN确定最佳数据收集时机）和估计器（RNN重建完整数据）组成。与基线方法（Kalman filter和均匀采样）相比，实验在PeMS数据上显示，该框架提高了10%的估计准确率，为资源受限环境下的数据收集提供了高效解决方案。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted by IEEE 26th International Conference on Intelligent\n  Transportation Systems",
      "pdf_url": "http://arxiv.org/pdf/2401.14504v1",
      "published_date": "2024-01-25 20:50:34 UTC",
      "updated_date": "2024-01-25 20:50:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:15:20.608746"
    },
    {
      "arxiv_id": "2402.01714v1",
      "title": "TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy",
      "title_zh": "翻译失败",
      "authors": [
        "Vibhav Agarwal",
        "Sourav Ghosh",
        "Harichandana BSS",
        "Himanshu Arora",
        "Barath Raj Kandur Raja"
      ],
      "abstract": "Data-to-text (D2T) generation is a crucial task in many natural language\nunderstanding (NLU) applications and forms the foundation of task-oriented\ndialog systems. In the context of conversational AI solutions that can work\ndirectly with local data on the user's device, architectures utilizing large\npre-trained language models (PLMs) are impractical for on-device deployment due\nto a high memory footprint. To this end, we propose TrICy, a novel lightweight\nframework for an enhanced D2T task that generates text sequences based on the\nintent in context and may further be guided by user-provided triggers. We\nleverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words\naccurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L:\n70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom\ndataset related to text messaging applications, showcase our architecture's\neffectiveness. Moreover, we show that by leveraging an optional trigger input,\ndata-to-text generation quality increases significantly and achieves the new\nSOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that\nTrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively\nover LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some\nscenarios, performance improvement due to triggers is observed even when they\nare absent in training.",
      "tldr_zh": "该研究提出了一种轻量级框架 TrICy，用于触发器引导的数据到文本 (D2T) 生成任务，通过 intent aware attention-copy 机制来准确处理词汇外单词 (OOV)，并基于上下文意图生成文本序列。TrICy 旨在解决设备端部署的内存问题，在 E2E NLG 数据集上实现 BLEU 66.43% 和 ROUGE-L 70.14%，而在使用触发器时进一步提升至新 SOTA BLEU 69.29%。与其他大型语言模型如 GPT-3 和 ChatGPT 相比，TrICy 在 BLEU 和 METEOR 指标上分别提高了至少 24% 和 3%，且即使训练中无触发器，在某些场景下也能观察到性能提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in the IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. (Sourav Ghosh and Vibhav Agarwal contributed equally to this\n  work.)",
      "pdf_url": "http://arxiv.org/pdf/2402.01714v1",
      "published_date": "2024-01-25 20:17:06 UTC",
      "updated_date": "2024-01-25 20:17:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:15:33.370533"
    },
    {
      "arxiv_id": "2402.01713v2",
      "title": "Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data",
      "title_zh": "针对结构化纵向电子健康记录数据，提示大型语言模型进行零样本临床预测",
      "authors": [
        "Yinghao Zhu",
        "Zixiang Wang",
        "Junyi Gao",
        "Yuning Tong",
        "Jingkun An",
        "Weibin Liao",
        "Ewen M. Harrison",
        "Liantao Ma",
        "Chengwei Pan"
      ],
      "abstract": "The inherent complexity of structured longitudinal Electronic Health Records\n(EHR) data poses a significant challenge when integrated with Large Language\nModels (LLMs), which are traditionally tailored for natural language\nprocessing. Motivated by the urgent need for swift decision-making during new\ndisease outbreaks, where traditional predictive models often fail due to a lack\nof historical data, this research investigates the adaptability of LLMs, like\nGPT-4, to EHR data. We particularly focus on their zero-shot capabilities,\nwhich enable them to make predictions in scenarios in which they haven't been\nexplicitly trained. In response to the longitudinal, sparse, and\nknowledge-infused nature of EHR data, our prompting approach involves taking\ninto account specific EHR characteristics such as units and reference ranges,\nand employing an in-context learning strategy that aligns with clinical\ncontexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets\ndemonstrate that with our elaborately designed prompting framework, LLMs can\nimprove prediction performance in key tasks such as mortality, length-of-stay,\nand 30-day readmission by about 35\\%, surpassing ML models in few-shot\nsettings. Our research underscores the potential of LLMs in enhancing clinical\ndecision-making, especially in urgent healthcare situations like the outbreak\nof emerging diseases with no labeled data. The code is publicly available at\nhttps://github.com/yhzhu99/llm4healthcare for reproducibility.",
      "tldr_zh": "本研究探讨了如何通过提示策略（prompting）使大型语言模型（LLMs）如 GPT-4 在零样本（zero-shot）场景下处理结构化纵向电子健康记录（EHR）数据，以实现临床预测。针对 EHR 的复杂性（如稀疏性和知识密集特性），研究设计了考虑单位、参考范围和 in-context learning 的提示框架，使 LLMs 能够更好地适应临床上下文。实验在 MIMIC-IV 和 TJH 数据集上显示，该框架将 LLMs 在死亡率、住院时间和 30 天再入院等任务的预测性能提高了约 35%，优于传统少样本机器学习模型。整体结果强调了 LLMs 在紧急医疗决策中的潜力，例如新疾病爆发时的快速响应，并提供了公开代码以支持可复现性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01713v2",
      "published_date": "2024-01-25 20:14:50 UTC",
      "updated_date": "2024-02-10 16:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:15:44.426833"
    },
    {
      "arxiv_id": "2401.14489v2",
      "title": "The Case for Co-Designing Model Architectures with Hardware",
      "title_zh": "翻译失败",
      "authors": [
        "Quentin Anthony",
        "Jacob Hatef",
        "Deepak Narayanan",
        "Stella Biderman",
        "Stas Bekman",
        "Junqi Yin",
        "Aamir Shafi",
        "Hari Subramoni",
        "Dhabaleswar Panda"
      ],
      "abstract": "While GPUs are responsible for training the vast majority of state-of-the-art\ndeep learning models, the implications of their architecture are often\noverlooked when designing new deep learning (DL) models. As a consequence,\nmodifying a DL model to be more amenable to the target hardware can\nsignificantly improve the runtime performance of DL training and inference. In\nthis paper, we provide a set of guidelines for users to maximize the runtime\nperformance of their transformer models. These guidelines have been created by\ncarefully considering the impact of various model hyperparameters controlling\nmodel shape on the efficiency of the underlying computation kernels executed on\nthe GPU. We find the throughput of models with efficient model shapes is up to\n39\\% higher while preserving accuracy compared to models with a similar number\nof parameters but with unoptimized shapes.",
      "tldr_zh": "本论文主张在设计深度学习(DL)模型时，应考虑GPU架构的影响，以提升训练和推理的运行时性能。作者通过分析模型超参数（如控制Transformer模型形状的因素）对GPU计算内核效率的影响，提供了一套指导方针，帮助用户优化模型设计。结果显示，使用高效模型形状的模型在保持准确性的前提下，吞吐量可比参数数相似的未优化模型高出高达39%。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14489v2",
      "published_date": "2024-01-25 19:50:31 UTC",
      "updated_date": "2024-01-30 21:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:15:56.032952"
    },
    {
      "arxiv_id": "2401.14488v1",
      "title": "Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Dohmen",
        "Frank Röder",
        "Manfred Eppe"
      ],
      "abstract": "One problem with researching cognitive modeling and reinforcement learning\n(RL) is that researchers spend too much time on setting up an appropriate\ncomputational framework for their experiments. Many open source implementations\nof current RL algorithms exist, but there is a lack of a modular suite of tools\ncombining different robotic simulators and platforms, data visualization,\nhyperparameter optimization, and baseline experiments. To address this problem,\nwe present Scilab-RL, a software framework for efficient research in cognitive\nmodeling and reinforcement learning for robotic agents. The framework focuses\non goal-conditioned reinforcement learning using Stable Baselines 3 and the\nOpenAI gym interface. It enables native possibilities for experiment\nvisualizations and hyperparameter optimization. We describe how these features\nenable researchers to conduct experiments with minimal time effort, thus\nmaximizing research output.",
      "tldr_zh": "本文提出 Scilab-RL，这是一个高效的软件框架，旨在解决认知建模和 reinforcement learning (RL) 研究中实验框架设置耗时的问题。框架结合 Stable Baselines 3 和 OpenAI gym 接口，专注于目标条件强化学习，并提供模块化的工具支持，包括机器人模拟器集成、数据可视化以及 hyperparameter optimization。Scilab-RL 通过这些功能，帮助研究者以最少的努力进行实验，从而最大化研究输出。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14488v1",
      "published_date": "2024-01-25 19:49:02 UTC",
      "updated_date": "2024-01-25 19:49:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:16:07.818759"
    },
    {
      "arxiv_id": "2401.15109v1",
      "title": "Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms",
      "title_zh": "迈向集体超级智能",
      "authors": [
        "Louis Rosenberg",
        "Gregg Willcox",
        "Hans Schumann",
        "Ganesh Mani"
      ],
      "abstract": "Swarm Intelligence (SI) is a natural phenomenon that enables biological\ngroups to amplify their combined intellect by forming real-time systems.\nArtificial Swarm Intelligence (or Swarm AI) is a technology that enables\nnetworked human groups to amplify their combined intelligence by forming\nsimilar systems. In the past, swarm-based methods were constrained to narrowly\ndefined tasks like probabilistic forecasting and multiple-choice decision\nmaking. A new technology called Conversational Swarm Intelligence (CSI) was\ndeveloped in 2023 that amplifies the decision-making accuracy of networked\nhuman groups through natural conversational deliberations. The current study\nevaluated the ability of real-time groups using a CSI platform to take a common\nIQ test known as Raven's Advanced Progressive Matrices (RAPM). First, a\nbaseline group of participants took the Raven's IQ test by traditional survey.\nThis group averaged 45.6% correct. Then, groups of approximately 35 individuals\nanswered IQ test questions together using a CSI platform called Thinkscape.\nThese groups averaged 80.5% correct. This places the CSI groups in the 97th\npercentile of IQ test-takers and corresponds to an effective IQ increase of 28\npoints (p<0.001). This is an encouraging result and suggests that CSI is a\npowerful method for enabling conversational collective intelligence in large,\nnetworked groups. In addition, because CSI is scalable across groups of\npotentially any size, this technology may provide a viable pathway to building\na Collective Superintelligence.",
      "tldr_zh": "本文研究了Conversational Swarm Intelligence (CSI)技术如何通过自然对话增强网络化人类群体的集体智能。实验中，参与者先通过传统方式回答Raven's Advanced Progressive Matrices (RAPM) IQ测试，平均正确率为45.6%；随后，使用CSI平台（如Thinkscape）的群体（约35人）协作回答相同测试，平均正确率提升至80.5%，相当于IQ增加28点（p<0.001）。这一结果证明CSI能显著放大群体决策准确性，并为构建Collective Superintelligence提供可行路径。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.15109v1",
      "published_date": "2024-01-25 19:43:35 UTC",
      "updated_date": "2024-01-25 19:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:16:20.596044"
    },
    {
      "arxiv_id": "2401.14484v1",
      "title": "Design Principles for Generative AI Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Justin D. Weisz",
        "Jessica He",
        "Michael Muller",
        "Gabriela Hoefer",
        "Rachel Miles",
        "Werner Geyer"
      ],
      "abstract": "Generative AI applications present unique design challenges. As generative AI\ntechnologies are increasingly being incorporated into mainstream applications,\nthere is an urgent need for guidance on how to design user experiences that\nfoster effective and safe use. We present six principles for the design of\ngenerative AI applications that address unique characteristics of generative AI\nUX and offer new interpretations and extensions of known issues in the design\nof AI applications. Each principle is coupled with a set of design strategies\nfor implementing that principle via UX capabilities or through the design\nprocess. The principles and strategies were developed through an iterative\nprocess involving literature review, feedback from design practitioners,\nvalidation against real-world generative AI applications, and incorporation\ninto the design process of two generative AI applications. We anticipate the\nprinciples to usefully inform the design of generative AI applications by\ndriving actionable design recommendations.",
      "tldr_zh": "该论文探讨了生成式 AI 应用的独特设计挑战，并提出了六个设计原则，以应对生成式 AI UX 的特性，提供新解释和扩展现有 AI 设计问题。每个原则配有具体设计策略，通过 UX 能力或设计过程来实施。这些原则是通过文献综述、设计从业者反馈、对真实世界应用的验证以及整合到两个生成式 AI 应用的设计中迭代开发的，最终旨在为生成式 AI 应用的开发提供可操作的推荐，以促进有效和安全的用户体验。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "34 pages, 4 figures. To be published in CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14484v1",
      "published_date": "2024-01-25 19:38:21 UTC",
      "updated_date": "2024-01-25 19:38:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:16:31.212249"
    },
    {
      "arxiv_id": "2401.14469v1",
      "title": "Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels",
      "title_zh": "揭示未见之物：训练过的深度卷积核中的可识别集群",
      "authors": [
        "Zahra Babaiee",
        "Peyman M. Kiasari",
        "Daniela Rus",
        "Radu Grosu"
      ],
      "abstract": "Recent advances in depthwise-separable convolutional neural networks\n(DS-CNNs) have led to novel architectures, that surpass the performance of\nclassical CNNs, by a considerable scalability and accuracy margin. This paper\nreveals another striking property of DS-CNN architectures: discernible and\nexplainable patterns emerge in their trained depthwise convolutional kernels in\nall layers. Through an extensive analysis of millions of trained filters, with\ndifferent sizes and from various models, we employed unsupervised clustering\nwith autoencoders, to categorize these filters. Astonishingly, the patterns\nconverged into a few main clusters, each resembling the difference of Gaussian\n(DoG) functions, and their first and second-order derivatives. Notably, we were\nable to classify over 95\\% and 90\\% of the filters from state-of-the-art\nConvNextV2 and ConvNeXt models, respectively. This finding is not merely a\ntechnological curiosity; it echoes the foundational models neuroscientists have\nlong proposed for the vision systems of mammals. Our results thus deepen our\nunderstanding of the emergent properties of trained DS-CNNs and provide a\nbridge between artificial and biological visual processing systems. More\nbroadly, they pave the way for more interpretable and biologically-inspired\nneural network designs in the future.",
      "tldr_zh": "本研究揭示了深度可分离卷积神经网络 (DS-CNNs) 的一个关键特性：在训练后的深度卷积核中，出现了可辨识且可解释的模式。研究者通过对数百万滤波器进行无监督聚类和 autoencoders 分析，将这些模式归类为几个主要集群，这些集群类似于差高斯函数 (DoG) 及其一阶和二阶导数，并在 ConvNextV2 和 ConvNeXt 模型中成功分类超过95%和90%的滤波器。这样的发现不仅加深了对训练 DS-CNNs 的理解，还桥接了人工和生物视觉处理系统，为开发更可解释和生物启发的神经网络设计铺平了道路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14469v1",
      "published_date": "2024-01-25 19:05:53 UTC",
      "updated_date": "2024-01-25 19:05:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:16:45.450904"
    },
    {
      "arxiv_id": "2401.14461v2",
      "title": "Marabou 2.0: A Versatile Formal Analyzer of Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Haoze Wu",
        "Omri Isac",
        "Aleksandar Zeljić",
        "Teruhiro Tagomori",
        "Matthew Daggitt",
        "Wen Kokke",
        "Idan Refaeli",
        "Guy Amir",
        "Kyle Julian",
        "Shahaf Bassan",
        "Pei Huang",
        "Ori Lahav",
        "Min Wu",
        "Min Zhang",
        "Ekaterina Komendantskaya",
        "Guy Katz",
        "Clark Barrett"
      ],
      "abstract": "This paper serves as a comprehensive system description of version 2.0 of the\nMarabou framework for formal analysis of neural networks. We discuss the tool's\narchitectural design and highlight the major features and components introduced\nsince its initial release.",
      "tldr_zh": "这篇论文对 Marabou 2.0 框架进行了全面系统描述，该框架是一种多功能工具，用于神经网络的 formal analysis。论文重点讨论了工具的架构设计，并强调自初次发布以来新增的主要功能和组件，如增强的分析能力。Marabou 2.0 的改进有助于提升神经网络的可靠性和安全性，为形式验证应用提供了更坚实的基础。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Condensed version accepted at CAV'24",
      "pdf_url": "http://arxiv.org/pdf/2401.14461v2",
      "published_date": "2024-01-25 19:00:25 UTC",
      "updated_date": "2024-05-20 05:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:16:54.951291"
    },
    {
      "arxiv_id": "2401.14405v2",
      "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
      "title_zh": "翻译失败",
      "authors": [
        "Yiyuan Zhang",
        "Xiaohan Ding",
        "Kaixiong Gong",
        "Yixiao Ge",
        "Ying Shan",
        "Xiangyu Yue"
      ],
      "abstract": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
      "tldr_zh": "我们提出Multimodal Pathway方法，用于利用其他模态的无关数据（如音频或点云数据集）来提升特定模态的Transformer模型性能，例如改进ImageNet模型，而非依赖配对或交错数据。  \n该方法通过一个辅助Transformer和构建的连接路径，使目标模态数据能同时由两个模型处理，并采用Cross-Modal Re-parameterization技术来整合辅助权重，而不增加推理成本。  \n实验结果显示，在图像、点云、视频和音频识别任务上，使用无关模态数据后，模型性能实现了显著且一致的提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Code and models are available at\n  https://github.com/AILab-CVC/M2PT",
      "pdf_url": "http://arxiv.org/pdf/2401.14405v2",
      "published_date": "2024-01-25 18:59:58 UTC",
      "updated_date": "2024-03-18 08:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:17:08.088343"
    },
    {
      "arxiv_id": "2401.14403v2",
      "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
      "title_zh": "开放世界中铰接物体的自适应移动操纵",
      "authors": [
        "Haoyu Xiong",
        "Russell Mendonca",
        "Kenneth Shaw",
        "Deepak Pathak"
      ],
      "abstract": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
      "tldr_zh": "本论文提出Open-World Mobile Manipulation System，一种全栈方法，用于在开放非结构化环境中处理铰接物体（如门、柜子、抽屉和冰箱）的移动操作。系统采用自适应学习框架，先通过behavior cloning从少量数据初始学习，然后利用online practice在实际环境中适应新物体，并开发了一个低成本（约20,000 USD）的硬件平台以确保安全自主操作。实验在CMU校园的20个铰接物体上进行，通过不到一小时的在线学习，将成功率从behavior cloning预训练的50%提高到95%。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Website at https://open-world-mobilemanip.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2401.14403v2",
      "published_date": "2024-01-25 18:59:44 UTC",
      "updated_date": "2024-01-28 18:58:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:17:21.305713"
    },
    {
      "arxiv_id": "2401.14447v1",
      "title": "Wordflow: Social Prompt Engineering for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zijie J. Wang",
        "Aishwarya Chakravarthy",
        "David Munechika",
        "Duen Horng Chau"
      ],
      "abstract": "Large language models (LLMs) require well-crafted prompts for effective use.\nPrompt engineering, the process of designing prompts, is challenging,\nparticularly for non-experts who are less familiar with AI technologies. While\nresearchers have proposed techniques and tools to assist LLM users in prompt\ndesign, these works primarily target AI application developers rather than\nnon-experts. To address this research gap, we propose social prompt\nengineering, a novel paradigm that leverages social computing techniques to\nfacilitate collaborative prompt design. To investigate social prompt\nengineering, we introduce Wordflow, an open-source and social text editor that\nenables everyday users to easily create, run, share, and discover LLM prompts.\nAdditionally, by leveraging modern web technologies, Wordflow allows users to\nrun LLMs locally and privately in their browsers. Two usage scenarios highlight\nhow social prompt engineering and our tool can enhance laypeople's interaction\nwith LLMs. Wordflow is publicly accessible at\nhttps://poloclub.github.io/wordflow.",
      "tldr_zh": "该论文针对大型语言模型(LLMs)的提示工程(prompt engineering)挑战，提出社会提示工程(social prompt engineering)范式，利用社会计算技术促进非专家用户之间的协作提示设计。研究团队开发了Wordflow，一个开源社会文本编辑器，允许用户轻松创建、运行、共享和发现LLM提示，同时通过现代网络技术实现浏览器本地私有运行。论文通过两个使用场景展示了这一方法如何提升普通用户与LLMs的互动，并提供了工具的公开访问链接。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "8 pages, 7 figures. Wordflow is available at:\n  https://poloclub.github.io/wordflow. The code is available at:\n  https://github.com/poloclub/wordflow/. For a demo video, see:\n  https://youtu.be/3dOcVuofGVo",
      "pdf_url": "http://arxiv.org/pdf/2401.14447v1",
      "published_date": "2024-01-25 18:58:11 UTC",
      "updated_date": "2024-01-25 18:58:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:17:32.483819"
    },
    {
      "arxiv_id": "2401.14446v3",
      "title": "Black-Box Access is Insufficient for Rigorous AI Audits",
      "title_zh": "黑箱访问不足以进行严格的 AI 审计",
      "authors": [
        "Stephen Casper",
        "Carson Ezell",
        "Charlotte Siegmann",
        "Noam Kolt",
        "Taylor Lynn Curtis",
        "Benjamin Bucknall",
        "Andreas Haupt",
        "Kevin Wei",
        "Jérémy Scheurer",
        "Marius Hobbhahn",
        "Lee Sharkey",
        "Satyapriya Krishna",
        "Marvin Von Hagen",
        "Silas Alberti",
        "Alan Chan",
        "Qinyi Sun",
        "Michael Gerovitch",
        "David Bau",
        "Max Tegmark",
        "David Krueger",
        "Dylan Hadfield-Menell"
      ],
      "abstract": "External audits of AI systems are increasingly recognized as a key mechanism\nfor AI governance. The effectiveness of an audit, however, depends on the\ndegree of access granted to auditors. Recent audits of state-of-the-art AI\nsystems have primarily relied on black-box access, in which auditors can only\nquery the system and observe its outputs. However, white-box access to the\nsystem's inner workings (e.g., weights, activations, gradients) allows an\nauditor to perform stronger attacks, more thoroughly interpret models, and\nconduct fine-tuning. Meanwhile, outside-the-box access to training and\ndeployment information (e.g., methodology, code, documentation, data,\ndeployment details, findings from internal evaluations) allows auditors to\nscrutinize the development process and design more targeted evaluations. In\nthis paper, we examine the limitations of black-box audits and the advantages\nof white- and outside-the-box audits. We also discuss technical, physical, and\nlegal safeguards for performing these audits with minimal security risks. Given\nthat different forms of access can lead to very different levels of evaluation,\nwe conclude that (1) transparency regarding the access and methods used by\nauditors is necessary to properly interpret audit results, and (2) white- and\noutside-the-box access allow for substantially more scrutiny than black-box\naccess alone.",
      "tldr_zh": "这篇论文论证了黑箱访问（black-box access）不足以进行严格的 AI 审计，强调外部审计的有效性依赖于审计者对系统的访问权限。论文比较了不同访问级别，包括白箱访问（white-box access）允许更深入的模型分析（如权重、激活和梯度）和外部访问（outside-the-box access）让审计者审视训练、部署细节及开发过程，从而实现更强的攻击、更彻底的解释和针对性评估。作者讨论了技术、物理和法律保障，以最小化审计中的安全风险。最终结论是，审计结果的正确解读需要透明化访问权限和方法，而白箱及外部访问能提供远超黑箱的审查深度。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "FAccT 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14446v3",
      "published_date": "2024-01-25 18:58:05 UTC",
      "updated_date": "2024-05-29 13:56:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:17:46.363007"
    },
    {
      "arxiv_id": "2402.01712v1",
      "title": "Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hamideh Ghanadian",
        "Isar Nejadgholi",
        "Hussein Al Osman"
      ],
      "abstract": "Suicidal ideation detection is a vital research area that holds great\npotential for improving mental health support systems. However, the sensitivity\nsurrounding suicide-related data poses challenges in accessing large-scale,\nannotated datasets necessary for training effective machine learning models. To\naddress this limitation, we introduce an innovative strategy that leverages the\ncapabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to\ncreate synthetic data for suicidal ideation detection. Our data generation\napproach is grounded in social factors extracted from psychology literature and\naims to ensure coverage of essential information related to suicidal ideation.\nIn our study, we benchmarked against state-of-the-art NLP classification\nmodels, specifically, those centered around the BERT family structures. When\ntrained on the real-world dataset, UMD, these conventional models tend to yield\nF1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed\nby social factors, offers consistent F1-scores of 0.82 for both models,\nsuggesting that the richness of topics in synthetic data can bridge the\nperformance gap across different model complexities. Most impressively, when we\ncombined a mere 30% of the UMD dataset with our synthetic data, we witnessed a\nsubstantial increase in performance, achieving an F1-score of 0.88 on the UMD\ntest set. Such results underscore the cost-effectiveness and potential of our\napproach in confronting major challenges in the field, such as data scarcity\nand the quest for diversity in data representation.",
      "tldr_zh": "这篇论文提出了一种基于社会因素的合成数据生成策略，利用Large Language Models（如ChatGPT、Flan-T5和Llama）来解决自杀意念检测中数据稀缺的问题，该策略从心理学文献中提取关键社会因素，确保合成数据覆盖相关信息。实验结果显示，在UMD真实数据集上，训练BERT家族模型的F1分数为0.75-0.87，而使用合成数据驱动的方法可实现0.82的F1分数，且仅结合30%的UMD数据集即可将性能提升至0.88。总体而言，此方法证明了合成数据在提升模型性能、增加数据多样性和成本效益方面的潜力，为自杀意念检测领域提供了创新解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01712v1",
      "published_date": "2024-01-25 18:25:05 UTC",
      "updated_date": "2024-01-25 18:25:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:17:57.829798"
    },
    {
      "arxiv_id": "2401.14373v1",
      "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Gökçe Uludoğan",
        "Zeynep Yirmibeşoğlu Balal",
        "Furkan Akkurt",
        "Melikşah Türker",
        "Onur Güngör",
        "Susan Üsküdarlı"
      ],
      "abstract": "The recent advances in natural language processing have predominantly favored\nwell-resourced English-centric models, resulting in a significant gap with\nlow-resource languages. In this work, we introduce the language model TURNA,\nwhich is developed for the low-resource language Turkish and is capable of both\nnatural language understanding and generation tasks. TURNA is pretrained with\nan encoder-decoder architecture based on the unified framework UL2 with a\ndiverse corpus that we specifically curated for this purpose. We evaluated\nTURNA with three generation tasks and five understanding tasks for Turkish. The\nresults show that TURNA outperforms several multilingual models in both\nunderstanding and generation tasks, and competes with monolingual Turkish\nmodels in understanding tasks. TURNA is made available at\nhttps://huggingface.co/boun-tabi-LMG/TURNA .",
      "tldr_zh": "本研究针对自然语言处理中英语模型主导导致低资源语言（如土耳其语）落后的问题，引入了 TURNA，一种基于 encoder-decoder 架构的土耳其语语言模型。TURNA 采用 UL2 框架进行预训练，并使用专门 curated 的语料库，支持自然语言 understanding 和 generation 任务。在评估中，TURNA 在土耳其语的三个 generation 任务和五个 understanding 任务上优于多语言模型，并在 understanding 任务中与单语言模型竞争；该模型已开源于 https://huggingface.co/boun-tabi-LMG/TURNA。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14373v1",
      "published_date": "2024-01-25 18:24:13 UTC",
      "updated_date": "2024-01-25 18:24:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:18:09.398284"
    },
    {
      "arxiv_id": "2401.14371v1",
      "title": "Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input",
      "title_zh": "翻译失败",
      "authors": [
        "Enrico Picco",
        "Lina Jaurigue",
        "Kathy Lüdge",
        "Serge Massar"
      ],
      "abstract": "We present an experimental validation of a recently proposed optimization\ntechnique for reservoir computing, using an optoelectronic setup. Reservoir\ncomputing is a robust framework for signal processing applications, and the\ndevelopment of efficient optimization approaches remains a key challenge. The\ntechnique we address leverages solely a delayed version of the input signal to\nidentify the optimal operational region of the reservoir, simplifying the\ntraditionally time-consuming task of hyperparameter tuning. We verify the\neffectiveness of this approach on different benchmark tasks and reservoir\noperating conditions.",
      "tldr_zh": "本研究实验验证了一种针对物理reservoir computing的优化技术，该方法仅使用输入信号的延迟版本来识别水库的最佳操作区域，从而简化了传统耗时的超参数调整过程。通过光电设备进行测试，该技术在不同基准任务和reservoir运行条件下表现出有效性，为信号处理应用提供了更高效的框架。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.NE",
        "physics.optics"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14371v1",
      "published_date": "2024-01-25 18:20:37 UTC",
      "updated_date": "2024-01-25 18:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:18:19.232715"
    },
    {
      "arxiv_id": "2401.14367v1",
      "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Asaf Yehudai",
        "Boaz Carmeli",
        "Yosi Mass",
        "Ofir Arviv",
        "Nathaniel Mills",
        "Assaf Toledo",
        "Eyal Shnarch",
        "Leshem Choshen"
      ],
      "abstract": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.",
      "tldr_zh": "该研究提出Genie，一种新型方法，用于自动生成高质量的内容基础数据集，以解决内容导向生成任务数据短缺的问题。Genie包括三个阶段：内容准备、生成任务特定示例（如问答对或摘要），以及过滤机制以确保数据质量和忠实度。通过生成大规模合成数据用于Long-Form Question-Answering (LFQA)、摘要和信息提取，人类评估显示这些数据自然且高质量。实验结果表明，使用Genie数据训练的模型在性能上与基于人类数据（如ELI5、ASQA和CNN-DailyMail）的模型相当或更优，尤其在忠实度方面，并成功扩展到医疗领域的LFQA数据生成。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICLR24",
      "pdf_url": "http://arxiv.org/pdf/2401.14367v1",
      "published_date": "2024-01-25 18:14:57 UTC",
      "updated_date": "2024-01-25 18:14:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:18:32.923684"
    },
    {
      "arxiv_id": "2401.14362v3",
      "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support",
      "title_zh": "翻译失败",
      "authors": [
        "Inhwa Song",
        "Sachin R. Pendse",
        "Neha Kumar",
        "Munmun De Choudhury"
      ],
      "abstract": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.",
      "tldr_zh": "这篇论文探讨了人们使用 Large Language Model (LLM) 聊天机器人作为心理健康支持工具的亲身经历，通过对21位来自全球多样背景个体的访谈进行分析。研究发现，用户会为聊天机器人赋予独特支持角色，以填补日常护理的空白，同时需应对文化限制和潜在风险，如不当设计可能危害用户福祉。论文引入 therapeutic alignment 的概念，强调将 AI 与心理治疗价值观对齐，并为设计师提供建议，以促进 LLM 聊天机器人等工具在心理健康领域的道德和有效应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "The first two authors contributed equally to this work; typos\n  corrected and post-review revisions incorporated",
      "pdf_url": "http://arxiv.org/pdf/2401.14362v3",
      "published_date": "2024-01-25 18:08:53 UTC",
      "updated_date": "2025-05-09 15:24:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:18:45.847194"
    },
    {
      "arxiv_id": "2401.14444v1",
      "title": "ICASSP 2024 Speech Signal Improvement Challenge",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolae Catalin Ristea",
        "Ando Saabas",
        "Ross Cutler",
        "Babak Naderi",
        "Sebastian Braun",
        "Solomiya Branets"
      ],
      "abstract": "The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to\nstimulate research in the area of improving the speech signal quality in\ncommunication systems. This marks our second challenge, building upon the\nsuccess from the previous ICASSP 2023 Grand Challenge. We enhance the\ncompetition by introducing a dataset synthesizer, enabling all participating\nteams to start at a higher baseline, an objective metric for our extended P.804\ntests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a\nmetric. We evaluate a total of 13 systems in the real-time track and 11 systems\nin the non-real-time track using both subjective P.804 and objective Word\nAccuracy metrics.",
      "tldr_zh": "ICASSP 2024 Speech Signal Improvement Challenge 旨在推动通信系统中演讲信号质量改进的研究，这是基于ICASSP 2023挑战赛成功后的第二届版本。挑战赛新增了数据集合成器、扩展的P.804测试指标、2023测试集的转录，以及Word Accuracy (WAcc)作为新评估指标，以帮助参赛团队从更高起点出发。总计评估了13个实时系统和11个非实时系统，使用主观P.804和客观WAcc指标进行测试，刺激了相关领域的创新和发展。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14444v1",
      "published_date": "2024-01-25 18:08:00 UTC",
      "updated_date": "2024-01-25 18:08:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:18:56.924712"
    },
    {
      "arxiv_id": "2402.01711v1",
      "title": "LLM on FHIR -- Demystifying Health Records",
      "title_zh": "LLM on FHIR -- 揭开健康记录的神秘面纱",
      "authors": [
        "Paul Schmiedmayer",
        "Adrit Rao",
        "Philipp Zagar",
        "Vishnu Ravi",
        "Aydin Zahedivash",
        "Arash Fereydooni",
        "Oliver Aalami"
      ],
      "abstract": "Objective: To enhance health literacy and accessibility of health information\nfor a diverse patient population by developing a patient-centered artificial\nintelligence (AI) solution using large language models (LLMs) and Fast\nHealthcare Interoperability Resources (FHIR) application programming interfaces\n(APIs). Materials and Methods: The research involved developing LLM on FHIR, an\nopen-source mobile application allowing users to interact with their health\nrecords using LLMs. The app is built on Stanford's Spezi ecosystem and uses\nOpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient\ndataset and evaluated by medical experts to assess the app's effectiveness in\nincreasing health literacy. The evaluation focused on the accuracy, relevance,\nand understandability of the LLM's responses to common patient questions.\nResults: LLM on FHIR demonstrated varying but generally high degrees of\naccuracy and relevance in providing understandable health information to\npatients. The app effectively translated medical data into patient-friendly\nlanguage and was able to adapt its responses to different patient profiles.\nHowever, challenges included variability in LLM responses and the need for\nprecise filtering of health data. Discussion and Conclusion: LLMs offer\nsignificant potential in improving health literacy and making health records\nmore accessible. LLM on FHIR, as a pioneering application in this field,\ndemonstrates the feasibility and challenges of integrating LLMs into patient\ncare. While promising, the implementation and pilot also highlight risks such\nas inconsistent responses and the importance of replicable output. Future\ndirections include better resource identification mechanisms and executing LLMs\non-device to enhance privacy and reduce costs.",
      "tldr_zh": "这篇论文介绍了LLM on FHIR，一个开源移动应用，使用大型语言模型(LLMs)如OpenAI's GPT-4和Fast Healthcare Interoperability Resources (FHIR) APIs，旨在提升患者健康素养和健康信息可访问性。研究方法包括基于Stanford's Spezi生态系统开发该应用，并通过SyntheticMass患者数据集的试点研究，由医疗专家评估LLMs响应的准确性、相关性和易懂性。结果显示，应用能有效将医疗数据转化为患者友好的语言并适应不同用户，但存在响应变异性和数据过滤挑战。总体而言，该工作证明了整合LLMs到患者护理的可行性，并强调了未来需改进隐私保护和资源识别机制。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Pre-print of the paper submitted to the Call for Papers for the\n  Special Focus Issue on ChatGPT and Large Language Models (LLMs) in\n  Biomedicine and Health at the Journal of the American Medical Informatics\n  Association:\n  https://academic.oup.com/jamia/pages/call-for-papers-for-special-focus-issue",
      "pdf_url": "http://arxiv.org/pdf/2402.01711v1",
      "published_date": "2024-01-25 17:45:34 UTC",
      "updated_date": "2024-01-25 17:45:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:19:11.644744"
    },
    {
      "arxiv_id": "2401.14336v1",
      "title": "Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Dichao Liu"
      ],
      "abstract": "Fine-grained vehicle recognition (FGVR) is an essential fundamental\ntechnology for intelligent transportation systems, but very difficult because\nof its inherent intra-class variation. Most previous FGVR studies only focus on\nthe intra-class variation caused by different shooting angles, positions, etc.,\nwhile the intra-class variation caused by image noise has received little\nattention. This paper proposes a progressive multi-task anti-noise learning\n(PMAL) framework and a progressive multi-task distilling (PMD) framework to\nsolve the intra-class variation problem in FGVR due to image noise. The PMAL\nframework achieves high recognition accuracy by treating image denoising as an\nadditional task in image recognition and progressively forcing a model to learn\nnoise invariance. The PMD framework transfers the knowledge of the PMAL-trained\nmodel into the original backbone network, which produces a model with about the\nsame recognition accuracy as the PMAL-trained model, but without any additional\noverheads over the original backbone network. Combining the two frameworks, we\nobtain models that significantly exceed previous state-of-the-art methods in\nrecognition accuracy on two widely-used, standard FGVR datasets, namely\nStanford Cars, and CompCars, as well as three additional surveillance\nimage-based vehicle-type classification datasets, namely Beijing Institute of\nTechnology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images\nDataset for Make Model Recognition (VIDMMR), without any additional overheads\nover the original backbone networks. The source code is available at\nhttps://github.com/Dichao-Liu/Anti-noise_FGVR",
      "tldr_zh": "本文针对细粒度车辆识别 (FGVR) 中图像噪声引起的类内变异问题，提出了 Progressive Multi-task Anti-Noise Learning (PMAL) 和 Progressive Multi-task Distilling (PMD) 框架。PMAL 通过将图像去噪作为额外任务，与识别任务结合进行逐步训练，帮助模型实现噪声不变性，从而提升识别准确率；PMD 则将 PMAL 训练的知识转移到原始骨干网络中，保持相近的性能而不增加任何额外开销。实验结果显示，该方法在 Stanford Cars、CompCars 等标准数据集上显著超过了现有最先进方法，并在 BIT-Vehicle、VTID2 和 VIDMMR 等监控图像数据集上表现出色，提供开源代码以便复现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14336v1",
      "published_date": "2024-01-25 17:34:34 UTC",
      "updated_date": "2024-01-25 17:34:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:19:22.840762"
    },
    {
      "arxiv_id": "2401.15108v2",
      "title": "Tacit algorithmic collusion in deep reinforcement learning guided price competition: A study using EV charge pricing game",
      "title_zh": "翻译失败",
      "authors": [
        "Diwas Paudel",
        "Tapas K. Das"
      ],
      "abstract": "Players in pricing games with complex structures are increasingly adopting\nartificial intelligence (AI) aided learning algorithms to make pricing\ndecisions for maximizing profits. This is raising concern for the antitrust\nagencies as the practice of using AI may promote tacit algorithmic collusion\namong otherwise independent players. Recent studies of games in canonical forms\nhave shown contrasting claims ranging from none to a high level of tacit\ncollusion among AI-guided players. In this paper, we examine the concern for\ntacit collusion by considering a practical game where EV charging hubs compete\nby dynamically varying their prices. Such a game is likely to be commonplace in\nthe near future as EV adoption grows in all sectors of transportation. The hubs\nsource power from the day-ahead (DA) and real-time (RT) electricity markets as\nwell as from in-house battery storage systems. Their goal is to maximize\nprofits via pricing and efficiently managing the cost of power usage. To aid\nour examination, we develop a two-step data-driven methodology. The first step\nobtains the DA commitment by solving a stochastic model. The second step\ngenerates the pricing strategies by solving a competitive Markov decision\nprocess model using a multi-agent deep reinforcement learning (MADRL)\nframework. We evaluate the resulting pricing strategies using an index for the\nlevel of tacit algorithmic collusion. An index value of zero indicates no\ncollusion (perfect competition) and one indicates full collusion (monopolistic\nbehavior). Results from our numerical case study yield collusion index values\nbetween 0.14 and 0.45, suggesting a low to moderate level of collusion.",
      "tldr_zh": "这篇论文探讨了在复杂定价游戏中，使用深度强化学习指导的价格竞争，可能导致的隐性算法勾结（tacit algorithmic collusion），以电动车（EV）充电定价游戏为例，评估其对反垄断的影响。作者开发了一个两步数据驱动方法：首先通过解决随机模型获取日-ahead（DA）承诺，其次利用多智能体深度强化学习（MADRL）框架解决竞争Markov决策过程模型，以生成优化定价策略。研究结果显示，在数值案例中，勾结指数（collusion index）介于0.14至0.45之间，表明AI指导的定价行为存在低到中度的隐性勾结，但未达到完全垄断水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "econ.GN",
        "eess.SY",
        "q-fin.EC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.15108v2",
      "published_date": "2024-01-25 16:51:52 UTC",
      "updated_date": "2024-05-10 10:24:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:19:35.474727"
    },
    {
      "arxiv_id": "2401.14295v4",
      "title": "Demystifying Chains, Trees, and Graphs of Thoughts",
      "title_zh": "翻译失败",
      "authors": [
        "Maciej Besta",
        "Florim Memedi",
        "Zhenyu Zhang",
        "Robert Gerstenberger",
        "Guangyuan Piao",
        "Nils Blach",
        "Piotr Nyczyk",
        "Marcin Copik",
        "Grzegorz Kwaśniewski",
        "Jürgen Müller",
        "Lukas Gianinazzi",
        "Ales Kubicek",
        "Hubert Niewiadomski",
        "Aidan O'Mahony",
        "Onur Mutlu",
        "Torsten Hoefler"
      ],
      "abstract": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.",
      "tldr_zh": "这篇论文探讨了自然语言处理(NLP)领域中，通过结构化提示技术提升大型语言模型(LLM)性能的创新方法，包括Chain-of-Thought、Tree of Thoughts和Graph of Thoughts等设计，这些结构能指导LLM进行逻辑、数学推理、规划和创意写作等任务。作者提出一个通用蓝图，并构建了首个分类法(taxonomy)，对这些推理拓扑(reasoning topologies)的表示、算法和执行管道进行深入分析。论文比较了不同提示方案的设计选择，揭示了它们如何影响性能和成本，并讨论了理论基础、与知识库的关系以及未来研究挑战，以推动提示工程技术的进步。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14295v4",
      "published_date": "2024-01-25 16:34:00 UTC",
      "updated_date": "2025-02-08 09:02:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:19:46.951157"
    },
    {
      "arxiv_id": "2401.14292v2",
      "title": "Single and bi-layered 2-D acoustic soft tactile skin (AST2)",
      "title_zh": "翻译失败",
      "authors": [
        "Vishnu Rajendran",
        "Simon Parsons",
        "Amir Ghalamzan E"
      ],
      "abstract": "This paper aims to present an innovative and cost-effective design for\nAcoustic Soft Tactile (AST) Skin, with the primary goal of significantly\nenhancing the accuracy of 2-D tactile feature estimation. The existing\nchallenge lies in achieving precise tactile feature estimation, especially\nconcerning contact geometry characteristics, using cost-effective solutions. We\nhypothesise that by harnessing acoustic energy through dedicated acoustic\nchannels in 2 layers beneath the sensing surface and analysing amplitude\nmodulation, we can effectively decode interactions on the sensory surface,\nthereby improving tactile feature estimation. Our approach involves the\ndistinct separation of hardware components responsible for emitting and\nreceiving acoustic signals, resulting in a modular and highly customizable skin\ndesign. Practical tests demonstrate the effectiveness of this novel design,\nachieving remarkable precision in estimating contact normal forces (MAE < 0.8\nN), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE <\n0.3 mm). In conclusion, the AST skin, with its innovative design and modular\narchitecture, successfully addresses the challenge of tactile feature\nestimation. The presented results showcase its ability to precisely estimate\nvarious tactile features, making it a practical and cost-effective solution for\nrobotic applications.",
      "tldr_zh": "本文提出了一种创新且成本有效的 Acoustic Soft Tactile (AST) Skin 设计，旨在提升 2-D 触觉特征估计的准确性，特别是针对接触几何特性的挑战。该方法通过双层声学通道捕获和分析振幅调制（amplitude modulation），并将发射和接收硬件组件分离，实现模块化和高度可定制的结构。实验结果显示，该设计在估算接触法向力（MAE < 0.8 N）、2D 接触定位（MAE < 0.7 mm）和接触表面直径（MAE < 0.3 mm）方面表现出色，为机器人应用提供了一个实用且经济的触觉解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "IEEE Robosoft conference 2024 (accepted)",
      "pdf_url": "http://arxiv.org/pdf/2401.14292v2",
      "published_date": "2024-01-25 16:30:22 UTC",
      "updated_date": "2024-02-29 17:23:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:19:59.673795"
    },
    {
      "arxiv_id": "2401.15106v6",
      "title": "Underspecified Human Decision Experiments Considered Harmful",
      "title_zh": "未充分指定的人类决策实验被认为有害",
      "authors": [
        "Jessica Hullman",
        "Alex Kale",
        "Jason Hartline"
      ],
      "abstract": "Decision-making with information displays is a key focus of research in areas\nlike human-AI collaboration and data visualization. However, what constitutes a\ndecision problem, and what is required for an experiment to conclude that\ndecisions are flawed, remain imprecise. We present a widely applicable\ndefinition of a decision problem synthesized from statistical decision theory\nand information economics. We claim that to attribute loss in human performance\nto bias, an experiment must provide the information that a rational agent would\nneed to identify the normative decision. We evaluate whether recent empirical\nresearch on AI-assisted decisions achieves this standard. We find that only 10\n(26%) of 39 studies that claim to identify biased behavior presented\nparticipants with sufficient information to make this claim in at least one\ntreatment condition. We motivate the value of studying well-defined decision\nproblems by describing a characterization of performance losses they allow to\nbe conceived.",
      "tldr_zh": "本文批评了人类决策实验在定义决策问题和判断决策缺陷方面存在的模糊性，并从统计决策理论和信息经济学中提出一个通用的决策问题定义。作者主张，要将人类表现损失归因于偏差（如偏见），实验必须提供理性代理所需的规范决策信息。评估39个AI辅助决策研究的实证证据显示，只有10个（26%）的研究在至少一个处理条件下满足这一标准。该方法有助于更好地理解和表征性能损失，从而改进决策研究设计。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.15106v6",
      "published_date": "2024-01-25 16:21:37 UTC",
      "updated_date": "2025-05-02 13:21:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:20:10.454570"
    },
    {
      "arxiv_id": "2401.14285v1",
      "title": "POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation",
      "title_zh": "POUR-Net：一种基于种群先验辅助的过度-不足表示网络，用于低计数",
      "authors": [
        "Bo Zhou",
        "Jun Hou",
        "Tianqi Chen",
        "Yinchi Zhou",
        "Xiongchao Chen",
        "Huidong Xie",
        "Qiong Liu",
        "Xueqi Guo",
        "Yu-Jung Tsai",
        "Vladimir Y. Panin",
        "Takuya Toyonaga",
        "James S. Duncan",
        "Chi Liu"
      ],
      "abstract": "Low-dose PET offers a valuable means of minimizing radiation exposure in PET\nimaging. However, the prevalent practice of employing additional CT scans for\ngenerating attenuation maps (u-map) for PET attenuation correction\nsignificantly elevates radiation doses. To address this concern and further\nmitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an\ninnovative population-prior-aided over-under-representation network that aims\nfor high-quality attenuation map generation from low-dose PET. First, POUR-Net\nincorporates an over-under-representation network (OUR-Net) to facilitate\nefficient feature extraction, encompassing both low-resolution abstracted and\nfine-detail features, for assisting deep generation on the full-resolution\nlevel. Second, complementing OUR-Net, a population prior generation machine\n(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional\nprior information to aid OUR-Net generation. The integration of OUR-Net and\nPPGM within a cascade framework enables iterative refinement of $\\mu$-map\ngeneration, resulting in the production of high-quality $\\mu$-maps.\nExperimental results underscore the effectiveness of POUR-Net, showing it as a\npromising solution for accurate CT-free low-count PET attenuation correction,\nwhich also surpasses the performance of previous baseline methods.",
      "tldr_zh": "本研究提出 POUR-Net，一种结合人口先验的过度-欠表示网络（OUR-Net），旨在从低计数 PET 图像生成高质量衰减图（u-map），从而避免额外 CT 扫描并减少辐射暴露。POUR-Net 通过 OUR-Net 提取低分辨率抽象特征和高分辨率细节特征，并利用人口先验生成机（PPGM）基于全面 CT 衍生的 u-map 数据集提供辅助信息，在级联框架中实现衰减图的迭代精炼。实验结果表明，该方法在 CT-free 的低剂量 PET 衰减校正中超过了现有基线模型，展现出显著的性能优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.14285v1",
      "published_date": "2024-01-25 16:18:11 UTC",
      "updated_date": "2024-01-25 16:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:20:23.118574"
    },
    {
      "arxiv_id": "2401.14280v3",
      "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization",
      "title_zh": "RomanSetu：通过罗马化高效解锁大型语言模型的多语言能力",
      "authors": [
        "Jaavid Aktar Husain",
        "Raj Dabre",
        "Aswanth Kumar",
        "Jay Gala",
        "Thanmay Jayakumar",
        "Ratish Puduppully",
        "Anoop Kunchukuttan"
      ],
      "abstract": "This study addresses the challenge of extending Large Language Models (LLMs)\nto non-English languages that use non-Roman scripts. We propose an approach\nthat utilizes the romanized form of text as an interface for LLMs,\nhypothesizing that its frequent informal use and shared tokens with English\nenhance cross-lingual alignment. Our approach involves the continual\npretraining of an English LLM like Llama 2 on romanized text of non-English,\nnon-Roman script languages, followed by instruction tuning on romanized data.\nThe results indicate that romanized text not only reduces token fertility by\n2x-4x but also matches or outperforms native script representation across\nvarious NLU, NLG, and MT tasks. Moreover, the embeddings computed on romanized\ntext exhibit closer alignment with their English translations than those from\nthe native script. Our approach presents a promising direction for leveraging\nthe power of English LLMs in languages traditionally underrepresented in NLP.\nOur code is available on https://github.com/AI4Bharat/romansetu.",
      "tldr_zh": "本研究针对Large Language Models (LLMs)扩展到使用非罗马脚本的非英语语言的挑战，提出RomanSetu方法，通过利用文本的罗马化形式作为接口来提升多语言能力。该方法包括对英语LLMs如Llama 2进行持续预训练和指令调整，使用罗马化文本，从而减少标记生育率(token fertility)2x-4x，并在NLU、NLG和MT任务中匹配或优于原生脚本表示，同时使嵌入与英语翻译更紧密对齐。RomanSetu为利用英语LLMs处理NLP中代表性不足语言提供了高效且有前景的方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14280v3",
      "published_date": "2024-01-25 16:11:41 UTC",
      "updated_date": "2024-06-23 11:40:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:20:35.920192"
    },
    {
      "arxiv_id": "2401.14279v3",
      "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Azmain Kabir",
        "Shaowei Wang",
        "Yuan Tian",
        "Tse-Hsun Chen",
        "Muhammad Asaduzzaman",
        "Wenbin Zhang"
      ],
      "abstract": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.",
      "tldr_zh": "该研究提出ZS4C，一种轻量级零样本方法，使用大型语言模型(LLMs)如GPT-3.5，从不完整的代码片段中合成可编译代码，以解决技术Q&A站点上代码不可编译的问题。ZS4C采用两阶段过程：首先，LLMs识别缺失的import语句；其次，通过与编译器等验证器合作修复语法和import错误。实验在StatType-SO基准和新的Python-SO数据集上显示，ZS4C将编译率从63%提高到95.1%，比最先进方法SnR改善50.1%，并在import语句准确率上以F1 score 0.98实现8.5%的提升。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper has been accepted and published in ACM Transactions on\n  Software Engineering and Methodology (TOSEM), [2024],\n  [https://dl.acm.org/doi/10.1145/3702979]",
      "pdf_url": "http://arxiv.org/pdf/2401.14279v3",
      "published_date": "2024-01-25 16:10:33 UTC",
      "updated_date": "2024-12-09 18:41:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:20:46.388982"
    },
    {
      "arxiv_id": "2401.14267v3",
      "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across Time",
      "title_zh": "翻译失败",
      "authors": [
        "Lyle Muller",
        "Patricia S. Churchland",
        "Terrence J. Sejnowski"
      ],
      "abstract": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers.",
      "tldr_zh": "该论文探讨了 Transformers（如 ChatGPT 和 LLMs）如何通过自注意力(self-attention)机制将输入序列转化为编码向量，从而学习长程时间依赖性。作者提出，大脑皮层波(cortical waves)可能采用类似原理，将感官输入序列的近期历史封装成单一空间模式，以提取时间上下文。研究结果表明，这种类比为理解大脑如何处理自然序列提供了新视角，桥接了人工智能和神经科学领域。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.14267v3",
      "published_date": "2024-01-25 16:01:49 UTC",
      "updated_date": "2024-08-16 14:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:20:56.898146"
    },
    {
      "arxiv_id": "2401.14257v2",
      "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Minglin Chen",
        "Weihao Yuan",
        "Yukun Wang",
        "Zhe Sheng",
        "Yisheng He",
        "Zilong Dong",
        "Liefeng Bo",
        "Yulan Guo"
      ],
      "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.",
      "tldr_zh": "本文提出了一种多视图草图引导的文本到3D生成框架，名为Sketch2NeRF，以解决现有Text-to-3D方法生成的3D内容随机性高、缺乏细粒度控制的问题。该框架利用预训练的2D扩散模型（如Stable Diffusion和ControlNet）来监督NeRF（神经辐射场）的优化，并引入同步生成和重建方法，确保草图的抽象性得到有效处理。实验基于两种多视图草图数据集表明，Sketch2NeRF能合成与草图高度一致且文本提示高度对齐的3D内容，在草图相似性和文本对齐方面达到了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.14257v2",
      "published_date": "2024-01-25 15:49:12 UTC",
      "updated_date": "2024-01-27 07:22:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:21:09.968363"
    },
    {
      "arxiv_id": "2402.00053v1",
      "title": "Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors",
      "title_zh": "我们是在浪费时间",
      "authors": [
        "Filip Cornell",
        "Yifei Jin",
        "Jussi Karlgren",
        "Sarunas Girdzijauskas"
      ],
      "abstract": "The standard evaluation protocol for measuring the quality of Knowledge Graph\nCompletion methods - the task of inferring new links to be added to a graph -\ntypically involves a step which ranks every entity of a Knowledge Graph to\nassess their fit as a head or tail of a candidate link to be added. In\nKnowledge Graphs on a larger scale, this task rapidly becomes prohibitively\nheavy. Previous approaches mitigate this problem by using random sampling of\nentities to assess the quality of links predicted or suggested by a method.\nHowever, we show that this approach has serious limitations since the ranking\nmetrics produced do not properly reflect true outcomes. In this paper, we\npresent a thorough analysis of these effects along with the following findings.\nFirst, we empirically find and theoretically motivate why sampling uniformly at\nrandom vastly overestimates the ranking performance of a method. We show that\nthis can be attributed to the effect of easy versus hard negative candidates.\nSecond, we propose a framework that uses relational recommenders to guide the\nselection of candidates for evaluation. We provide both theoretical and\nempirical justification of our methodology, and find that simple and fast\nmethods can work extremely well, and that they match advanced neural\napproaches. Even when a large portion of true candidates for a property are\nmissed, the estimation barely deteriorates. With our proposed framework, we can\nreduce the time and computation needed similar to random sampling strategies\nwhile vastly improving the estimation; on ogbl-wikikg2, we show that accurate\nestimations of the full, filtered ranking can be obtained in 20 seconds instead\nof 30 minutes. We conclude that considerable computational effort can be saved\nby effective preprocessing and sampling methods and still reliably predict\nperformance accurately of the true performance for the entire ranking\nprocedure.",
      "tldr_zh": "该论文分析了知识图谱(Knowledge Graph)链接预测的标准评估协议问题，指出其排名所有实体的过程计算量巨大，而随机采样方法会高估性能，因为忽略了易负和难负候选的差异。研究提出一个快速准确的框架，使用关系推荐器(Relational Recommenders)指导候选实体选择，并通过理论和经验证据证明简单方法即可实现高效评估。即使遗漏部分真实候选，估算结果也保持稳定。实验在 ogbl-wikikg2 数据集上显示，该框架将评估时间从30分钟缩短到20秒，同时提供可靠的性能估算。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00053v1",
      "published_date": "2024-01-25 15:44:46 UTC",
      "updated_date": "2024-01-25 15:44:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:21:22.087310"
    },
    {
      "arxiv_id": "2401.14228v1",
      "title": "Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods",
      "title_zh": "评估由参数高效微调方法训练的参数矩阵可移植性",
      "authors": [
        "Mohammed Sabry",
        "Anya Belz"
      ],
      "abstract": "As the cost of training ever larger language models has grown, so has the\ninterest in reusing previously learnt knowledge. Transfer learning methods have\nshown how reusing non-task-specific knowledge can help in subsequent\ntask-specific learning. In this paper, we investigate the inverse: porting\nwhole functional modules that encode task-specific knowledge from one model to\nanother. We designed a study comprising 1,440 training/testing runs to test the\nportability of modules trained by parameter-efficient finetuning (PEFT)\ntechniques, using sentiment analysis as an example task. We test portability in\na wide range of scenarios, involving different PEFT techniques and different\npretrained host models, among other dimensions. We compare the performance of\nported modules with that of equivalent modules trained (i) from scratch, and\n(ii) from parameters sampled from the same distribution as the ported module.\nWe find that the ported modules far outperform the two alternatives tested, but\nthat there are interesting performance differences between the four PEFT\ntechniques. We conclude that task-specific knowledge in the form of\nstructurally modular sets of parameters as produced by PEFT techniques is\nhighly portable, but that degree of success depends on type of PEFT and on\ndifferences between originating and receiving pretrained models.",
      "tldr_zh": "该论文评估了通过参数高效微调(PEFT)方法训练的参数矩阵在不同模型间的可移植性，焦点是移植任务特定知识模块。研究设计了包括1440次训练/测试运行的实验，使用情感分析作为示例任务，比较了移植模块的表现与从零训练或随机参数的替代方案。结果显示，移植模块显著优于其他方法，但不同PEFT技术（如LoRA、Prompt Tuning等）之间存在性能差异；最终结论是，PEFT产生的模块高度可移植，但其成功程度取决于PEFT类型和源/目标预训练模型的差异。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of EACL 2024. Camera ready version",
      "pdf_url": "http://arxiv.org/pdf/2401.14228v1",
      "published_date": "2024-01-25 15:11:07 UTC",
      "updated_date": "2024-01-25 15:11:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:21:33.908124"
    },
    {
      "arxiv_id": "2401.14215v3",
      "title": "Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Hana Kim",
        "Kai Tzu-iunn Ong",
        "Seoyeon Kim",
        "Dongha Lee",
        "Jinyoung Yeo"
      ],
      "abstract": "Memorizing and utilizing speakers' personas is a common practice for response\ngeneration in long-term conversations. Yet, human-authored datasets often\nprovide uninformative persona sentences that hinder response quality. This\npaper presents a novel framework that leverages commonsense-based persona\nexpansion to address such issues in long-term conversation. While prior work\nfocuses on not producing personas that contradict others, we focus on\ntransforming contradictory personas into sentences that contain rich speaker\ninformation, by refining them based on their contextual backgrounds with\ndesigned strategies. As the pioneer of persona expansion in multi-session\nsettings, our framework facilitates better response generation via human-like\npersona refinement. The supplementary video of our work is available at\nhttps://caffeine-15bbf.web.app/.",
      "tldr_zh": "本论文针对长对话中说话者 personas 记忆和利用的问题，提出一个基于 commonsense-augmented 的框架，通过 context-aware persona refinement 策略来改进不信息丰富的 personas。具体而言，该框架将矛盾的 personas 转化为包含丰富说话者信息的句子，基于上下文背景进行精炼，从而避免了以往工作的局限。相比于现有方法，这是在多会话设置中首创的 persona expansion 应用，最终提升了响应生成的质量，使其更接近人类对话风格。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14215v3",
      "published_date": "2024-01-25 14:54:33 UTC",
      "updated_date": "2024-02-12 12:27:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:21:44.983711"
    },
    {
      "arxiv_id": "2401.14440v2",
      "title": "Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models",
      "title_zh": "语义敏感性和不一致预测：测量 NLI 模型的脆弱性",
      "authors": [
        "Erik Arakelyan",
        "Zhaoqi Liu",
        "Isabelle Augenstein"
      ],
      "abstract": "Recent studies of the emergent capabilities of transformer-based Natural\nLanguage Understanding (NLU) models have indicated that they have an\nunderstanding of lexical and compositional semantics. We provide evidence that\nsuggests these claims should be taken with a grain of salt: we find that\nstate-of-the-art Natural Language Inference (NLI) models are sensitive towards\nminor semantics preserving surface-form variations, which lead to sizable\ninconsistent model decisions during inference. Notably, this behaviour differs\nfrom valid and in-depth comprehension of compositional semantics, however does\nneither emerge when evaluating model accuracy on standard benchmarks nor when\nprobing for syntactic, monotonic, and logically robust reasoning. We propose a\nnovel framework to measure the extent of semantic sensitivity. To this end, we\nevaluate NLI models on adversarially generated examples containing minor\nsemantics-preserving surface-form input noise. This is achieved using\nconditional text generation, with the explicit condition that the NLI model\npredicts the relationship between the original and adversarial inputs as a\nsymmetric equivalence entailment. We systematically study the effects of the\nphenomenon across NLI models for $\\textbf{in-}$ and $\\textbf{out-of-}$ domain\nsettings. Our experiments show that semantic sensitivity causes performance\ndegradations of $12.92\\%$ and $23.71\\%$ average over $\\textbf{in-}$ and\n$\\textbf{out-of-}$ domain settings, respectively. We further perform ablation\nstudies, analysing this phenomenon across models, datasets, and variations in\ninference and show that semantic sensitivity can lead to major inconsistency\nwithin model predictions.",
      "tldr_zh": "本研究揭示了基于 transformer 的 NLU 模型在语义理解方面的脆弱性，即 NLI 模型对微小的语义保持不变的表面形式变化高度敏感，导致预测不一致。研究者提出一个新框架，通过条件文本生成创建对抗性例子，确保原输入和对抗输入被模型视为对称等价蕴涵，从而量化语义敏感性。实验结果显示，在 in-domain 和 out-of-domain 设置中，模型性能平均下降分别为 12.92% 和 23.71%，并通过消融研究分析了这种现象在不同模型、数据集和推理变异中的影响，强调了 NLI 模型的潜在不稳定性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14440v2",
      "published_date": "2024-01-25 14:47:05 UTC",
      "updated_date": "2024-01-31 10:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:21:57.850818"
    },
    {
      "arxiv_id": "2401.14206v1",
      "title": "Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Perlo",
        "Luca Berton",
        "Alessia Delpiano",
        "Francesca Menchini",
        "Stefano Tibaldi",
        "Marco Grosso",
        "Paolo Fonio"
      ],
      "abstract": "The liver is the most involved organ by distant metastasis in colon-rectal\ncancer (CRC) patients and it comes necessary to be aware of the mutational\nstatus of the lesions to correctly design the best individual treatment. So\nfar, efforts have been made in order to develop non-invasive and real-time\nmethods that permit the analysis of the whole tumor, using new artificial\nintelligence tools to analyze the tumor's image obtained by Computed Tomography\n(CT) scan. In order to address the current medical workflow, that is biopsy\nanalysis-based, we propose the first DeepLearning-based exploration, to our\nknowledge, of such classification approach from the patient medical imaging. We\npropose i) a solid pipeline for managing undersized datasets of available CT\nscans and ii) a baseline study for genomics mutation diagnosis support for\npreemptive patient follow-up. Our method is able to identify CRC RAS mutation\nfamily from CT images with 0.73 F1 score.",
      "tldr_zh": "本研究针对结直肠癌（CRC）患者的肝脏转移，提出一种基于 Deep Learning 的方法，利用 CT scans 图像进行基因突变分类，以实现非侵入性诊断并优化个性化治疗。研究开发了一个稳固的管道来处理小规模数据集，并进行了基线研究，支持预先患者随访。结果显示，该方法在识别 CRC RAS mutation 时，F1 score 达到 0.73，为临床基因组学诊断提供了新途径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "J.3; I.1.2"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14206v1",
      "published_date": "2024-01-25 14:40:58 UTC",
      "updated_date": "2024-01-25 14:40:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:22:10.312847"
    },
    {
      "arxiv_id": "2401.14185v1",
      "title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion",
      "title_zh": "TDFNet：一种高效的音频-视觉语音分离模型，采用自上而下的融合",
      "authors": [
        "Samuel Pegg",
        "Kai Li",
        "Xiaolin Hu"
      ],
      "abstract": "Audio-visual speech separation has gained significant traction in recent\nyears due to its potential applications in various fields such as speech\nrecognition, diarization, scene analysis and assistive technologies. Designing\na lightweight audio-visual speech separation network is important for\nlow-latency applications, but existing methods often require higher\ncomputational costs and more parameters to achieve better separation\nperformance. In this paper, we present an audio-visual speech separation model\ncalled Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for\naudio-visual speech separation, which builds upon the architecture of TDANet,\nan audio-only speech separation method. TDANet serves as the architectural\nfoundation for the auditory and visual networks within TDFNet, offering an\nefficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet\nachieves a performance increase of up to 10\\% across all performance metrics\ncompared with the previous SOTA method CTCNet. Remarkably, these results are\nachieved using fewer parameters and only 28\\% of the multiply-accumulate\noperations (MACs) of CTCNet. In essence, our method presents a highly effective\nand efficient solution to the challenges of speech separation within the\naudio-visual domain, making significant strides in harnessing visual\ninformation optimally.",
      "tldr_zh": "本论文提出了一种高效的音频-视觉语音分离模型 TDFNet，该模型采用顶层融合（Top-down Fusion）机制，基于音频-only 方法 TDANet 的架构，构建轻量级音频和视觉网络，以降低计算成本并减少参数数量。TDFNet 针对音频-视觉语音分离任务，优化了视觉信息的利用，在 LRS2-2Mix 数据集上比之前的 SOTA 方法 CTCNet 提高了 10% 的性能指标，同时仅使用 CTCNet 的 28% 的乘-累加操作（MACs）。这项工作为低延迟应用如语音识别和辅助技术提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14185v1",
      "published_date": "2024-01-25 13:47:22 UTC",
      "updated_date": "2024-01-25 13:47:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:22:21.433336"
    },
    {
      "arxiv_id": "2401.14176v2",
      "title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot",
      "title_zh": "Copilot-in-the-Loop：使用 Copilot 修复 Copilot 生成的 Python 代码中的代码异味",
      "authors": [
        "Beiqi Zhang",
        "Peng Liang",
        "Qiong Feng",
        "Yujia Fu",
        "Zengyang Li"
      ],
      "abstract": "As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released in\nSeptember 2023, functions as an interactive tool aimed at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot Chat's\nability to fix the code smells. To this end, we built a dataset comprising 102\ncode smells in Copilot-generated Python code. Our aim is to first explore the\noccurrence of code smells in Copilot-generated Python code and then evaluate\nthe effectiveness of Copilot Chat in fixing these code smells employing\ndifferent prompts. The results show that 8 out of 10 types of code smells can\nbe detected in Copilot-generated Python code, among which Multiply-Nested\nContainer is the most common one. For these code smells, Copilot Chat achieves\na highest fixing rate of 87.1%, showing promise in fixing Python code smells\ngenerated by Copilot itself. In addition, the effectiveness of Copilot Chat in\nfixing these smells can be improved by providing more detailed prompts.",
      "tldr_zh": "这篇论文探讨了 GitHub Copilot 生成的 Python 代码中 code smells 的问题，这些问题会降低代码的可读性和可维护性。研究者构建了一个包含 102 个 code smells 的数据集，并评估了 Copilot Chat 使用不同 prompts 修复这些 code smells 的有效性。结果显示，10 种 code smells 类型中有 8 种出现在 Copilot 生成的代码中，其中 Multiply-Nested Container 最常见，Copilot Chat 的最高修复率为 87.1%，且提供更详细的 prompts 可以进一步提升修复效果。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "The 39th IEEE/ACM International Conference on Automated Software\n  Engineering (ASE), NIER Track",
      "pdf_url": "http://arxiv.org/pdf/2401.14176v2",
      "published_date": "2024-01-25 13:39:54 UTC",
      "updated_date": "2024-08-21 14:57:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:22:34.249704"
    },
    {
      "arxiv_id": "2401.14174v2",
      "title": "A Structural Complexity Analysis of Hierarchical Task Network Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Cornelius Brand",
        "Robert Ganian",
        "Fionn Mc Inerney",
        "Simon Wietheger"
      ],
      "abstract": "We perform a refined complexity-theoretic analysis of three classical\nproblems in the context of Hierarchical Task Network Planning: the verification\nof a provided plan, whether an executable plan exists, and whether a given\nstate can be reached. Our focus lies on identifying structural properties which\nyield tractability. We obtain new polynomial algorithms for all three problems\non a natural class of primitive networks, along with corresponding lower\nbounds. We also obtain an algorithmic meta-theorem for lifting polynomial-time\nsolvability from primitive to general task networks, and prove that its\npreconditions are tight. Finally, we analyze the parameterized complexity of\nthe three problems.",
      "tldr_zh": "本论文对 Hierarchical Task Network Planning 中的三个经典问题——验证提供的计划、是否存在可执行计划以及给定状态是否可达——进行了细化的结构复杂性分析，焦点在于识别导致可计算性的结构属性。研究者开发了针对自然类原始网络的新多项式算法，并提供了相应的下界，同时提出一个algorithmic meta-theorem，用于将多项式时间可解性从原始任务网络提升到一般任务网络，并证明其条件是紧致的。最后，论文分析了这些问题的parameterized complexity，为理解和优化层次任务规划提供了重要见解。",
      "categories": [
        "cs.CC",
        "cs.AI"
      ],
      "primary_category": "cs.CC",
      "comment": "Updated version contains essentially the same results in a more\n  concise and improved write-up",
      "pdf_url": "http://arxiv.org/pdf/2401.14174v2",
      "published_date": "2024-01-25 13:34:33 UTC",
      "updated_date": "2025-01-22 12:41:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:22:46.486945"
    },
    {
      "arxiv_id": "2401.14436v1",
      "title": "Trust model of privacy-concerned, emotionally-aware agents in a cooperative logistics problem",
      "title_zh": "隐私关注的情感感知代理在合作物流问题中的信任模型",
      "authors": [
        "J. Carbo",
        "J. M. Molina"
      ],
      "abstract": "In this paper we propose a trust model to be used into a hypothetical mixed\nenvironment where humans and unmanned vehicles cooperate. We address the\ninclusion of emotions inside a trust model in a coherent way to the practical\napproaches to the current psychology theories. The most innovative contribution\nis how privacy issues play a role in the cooperation decisions of the emotional\ntrust model. Both, emotions and trust have been cognitively modeled and managed\nwith the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents\nimplemented in GAML (the programming language of GAMA agent platform) that\ncommunicates using the IEEE FIPA standard. The trusting behaviour of these\nemotional agents is tested in a cooperative logistics problem where: agents\nhave to move objects to destinations and some of the objects and places have\nprivacy issues. The execution of simulations of this logistic problem shows how\nemotions and trust contribute to improve the performance of agents in terms of\nboth, time savings and privacy protection",
      "tldr_zh": "本论文提出了一种信任模型，用于人类与无人车辆混合环境的合作场景中，该模型巧妙地将情绪纳入其中，并与当前心理学理论一致。创新点在于探讨隐私问题如何影响情绪信任模型中的合作决策，代理通过 Beliefs, Desires and Intentions (BDI) 范式进行认知建模，并在 GAML 语言和 GAMA 平台上实现，使用 IEEE FIPA 标准进行通信。在一个合作物流问题中进行模拟实验，结果显示，情绪和信任机制显著提升了代理的性能，包括时间节省和隐私保护。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14436v1",
      "published_date": "2024-01-25 13:31:43 UTC",
      "updated_date": "2024-01-25 13:31:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:22:57.081243"
    },
    {
      "arxiv_id": "2401.14171v1",
      "title": "Predicting Hypoxia in Brain Tumors from Multiparametric MRI",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Perlo",
        "Georgia Kanli",
        "Selma Boudissa",
        "Olivier Keunen"
      ],
      "abstract": "This research paper presents a novel approach to the prediction of hypoxia in\nbrain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia,\na condition characterized by low oxygen levels, is a common feature of\nmalignant brain tumors associated with poor prognosis. Fluoromisonidazole\nPositron Emission Tomography (FMISO PET) is a well-established method for\ndetecting hypoxia in vivo, but it is expensive and not widely available. Our\nstudy proposes the use of MRI, a more accessible and cost-effective imaging\nmodality, to predict FMISO PET signals. We investigate deep learning models\n(DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and\nFMISO PET images from patients with brain tumors. Our trained models\neffectively learn the complex relationships between the MRI features and the\ncorresponding FMISO PET signals, thereby enabling the prediction of hypoxia\nfrom MRI scans alone. The results show a strong correlation between the\npredicted and actual FMISO PET signals, with an overall PSNR score above 29.6\nand a SSIM score greater than 0.94, confirming MRI as a promising option for\nhypoxia prediction in brain tumors. This approach could significantly improve\nthe accessibility of hypoxia detection in clinical settings, with the potential\nfor more timely and targeted treatments.",
      "tldr_zh": "本文提出了一种使用多参数 MRI 预测脑肿瘤中 Hypoxia 的新方法，以替代昂贵且不易获取的 FMISO PET 检测。研究通过训练深度学习模型（deep learning models）在 ACRIN 6684 数据集上，学习 MRI 特征与 FMISO PET 信号之间的复杂关系。结果显示，预测信号与实际信号高度相关，PSNR 超过 29.6、SSIM 大于 0.94，这有望提升临床中 Hypoxia 检测的可及性，并促进更及时的针对性治疗。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "J.3; I.2.1"
      ],
      "primary_category": "eess.IV",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.14171v1",
      "published_date": "2024-01-25 13:28:53 UTC",
      "updated_date": "2024-01-25 13:28:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:23:10.268734"
    },
    {
      "arxiv_id": "2401.14166v3",
      "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction",
      "title_zh": "BayesPrompt：通过去偏域抽象在少样本推理上提示大规模预训练语言模型",
      "authors": [
        "Jiangmeng Li",
        "Fei Song",
        "Yifan Jin",
        "Wenwen Qiang",
        "Changwen Zheng",
        "Fuchun Sun",
        "Hui Xiong"
      ],
      "abstract": "As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.",
      "tldr_zh": "该论文提出了一种名为BayesPrompt的方法，用于在少样本推理(few-shot inference)中提升大型预训练语言模型(PLMs)的prompt-tuning性能，通过去偏域抽象(Debiased Domain Abstraction)来解决PLMs中过度概念知识和目标领域知识不足导致的知识分布错位问题。BayesPrompt通过利用已知分布近似目标领域的去偏事实分布，并从中均匀采样代表性特征生成提示，从而提供无歧义的指导，帮助PLMs更好地定位相关知识。实验结果显示，该方法在基准测试中实现了最先进性能(state-of-the-art)，并与领域适应(domain adaptation)理论建立了联系，为prompt-tuning的泛化提供了新洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICLR2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14166v3",
      "published_date": "2024-01-25 13:20:47 UTC",
      "updated_date": "2024-03-20 08:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:23:21.739877"
    },
    {
      "arxiv_id": "2401.14155v1",
      "title": "Alleviating Structural Distribution Shift in Graph Anomaly Detection",
      "title_zh": "缓解图异常检测中的结构分布偏移",
      "authors": [
        "Yuan Gao",
        "Xiang Wang",
        "Xiangnan He",
        "Zhenguang Liu",
        "Huamin Feng",
        "Yongdong Zhang"
      ],
      "abstract": "Graph anomaly detection (GAD) is a challenging binary classification problem\ndue to its different structural distribution between anomalies and normal nodes\n-- abnormal nodes are a minority, therefore holding high heterophily and low\nhomophily compared to normal nodes. Furthermore, due to various time factors\nand the annotation preferences of human experts, the heterophily and homophily\ncan change across training and testing data, which is called structural\ndistribution shift (SDS) in this paper. The mainstream methods are built on\ngraph neural networks (GNNs), benefiting the classification of normals from\naggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and\nsuffering from poor generalization.\n  This work solves the problem from a feature view. We observe that the degree\nof SDS varies between anomalies and normal nodes. Hence to address the issue,\nthe key lies in resisting high heterophily for anomalies meanwhile benefiting\nthe learning of normals from homophily. We tease out the anomaly features on\nwhich we constrain to mitigate the effect of heterophilous neighbors and make\nthem invariant. We term our proposed framework as Graph Decomposition Network\n(GDN). Extensive experiments are conducted on two benchmark datasets, and the\nproposed framework achieves a remarkable performance boost in GAD, especially\nin an SDS environment where anomalies have largely different structural\ndistribution across training and testing environments. Codes are open-sourced\nin https://github.com/blacksingular/wsdm_GDN.",
      "tldr_zh": "本文研究了图异常检测 (GAD) 中的结构分布偏移 (SDS) 问题，该问题源于异常节点的高异质性和低同质性，导致主流基于图神经网络 (GNNs) 的方法在训练和测试数据间泛化能力差。作者提出 Graph Decomposition Network (GDN) 框架，从特征视角出发，通过约束异常节点的特征使其对异质性邻居的影响保持不变，同时利用正常节点的同质性进行学习。实验结果显示，GDN 在两个基准数据集上显著提升了 GAD 性能，尤其在 SDS 环境下，实现了可观的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to WSDM 2023",
      "pdf_url": "http://arxiv.org/pdf/2401.14155v1",
      "published_date": "2024-01-25 13:07:34 UTC",
      "updated_date": "2024-01-25 13:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:23:34.712299"
    },
    {
      "arxiv_id": "2401.14153v1",
      "title": "Agent-based Simulation with Netlogo to Evaluate AmI Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "J. Carbo",
        "N. Sanchez",
        "J. M. Molina"
      ],
      "abstract": "In this paper an agent-based simulation is developed in order to evaluate an\nAmI scenario based on agents. Many AmI applications are implemented through\nagents but they are not compared to any other existing alternative in order to\nevaluate the relative benefits of using them. The proposal simulation\nenvironment developed in Netlogo analyse such benefits using two evaluation\ncriteria: First, measuring agent satisfaction of different types of desires\nalong the execution. Second, measuring time savings obtained through a correct\nuse of context information.\n  So, here, a previously suggested agent architecture, an ontology and a\n12-steps protocol to provide AmI services in airports, is evaluated using a\nNetLogo simulation environment. The present work uses a NetLogo model\nconsidering scalability problems of this application domain but using FIPA and\nBDI extensions to be coherent with our previous works and our previous JADE\nimplementation of them.\n  The NetLogo model presented simulates an airport with agent users passing\nthrough several zones located in a specific order in a map: passport controls,\ncheck-in counters of airline companies, boarding gates, different types of\nshopping. Although initial data in simulations are generated randomly, and the\nmodel is just an approximation of real-world airports, the definition of this\ncase of use of Ambient Intelligence through NetLogo agents opens an interesting\nway to evaluate the benefits of using Ambient Intelligence, which is a\nsignificant contribution to the final development of them.",
      "tldr_zh": "本研究开发了一个基于代理（agent-based）的模拟环境，使用 NetLogo 评估 AmI（Ambient Intelligence）场景的益处。模拟模型通过两个标准进行评估：一是测量代理满足不同类型欲望的满意度，二是计算通过正确利用上下文信息节省的时间。研究具体评估了一个先前提出的代理架构、本体和12步协议，应用于机场场景，包括代理用户通过护照检查、登机口和购物区等区域。结果显示，尽管模拟数据随机生成且为真实机场的近似模型，此方法为评估 AmI 的实际优势提供了新途径，并考虑了 FIPA 和 BDI 扩展以确保与之前 JADE 实现的兼容性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14153v1",
      "published_date": "2024-01-25 13:05:06 UTC",
      "updated_date": "2024-01-25 13:05:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:23:45.095959"
    },
    {
      "arxiv_id": "2402.04874v1",
      "title": "Choosing a Classical Planner with Graph Neural Networks",
      "title_zh": "使用图神经网络选择经典规划器",
      "authors": [
        "Jana Vatter",
        "Ruben Mayer",
        "Hans-Arno Jacobsen",
        "Horst Samulowitz",
        "Michael Katz"
      ],
      "abstract": "Online planner selection is the task of choosing a solver out of a predefined\nset for a given planning problem. As planning is computationally hard, the\nperformance of solvers varies greatly on planning problems. Thus, the ability\nto predict their performance on a given problem is of great importance. While a\nvariety of learning methods have been employed, for classical cost-optimal\nplanning the prevailing approach uses Graph Neural Networks (GNNs). In this\nwork, we continue the line of work on using GNNs for online planner selection.\nWe perform a thorough investigation of the impact of the chosen GNN model,\ngraph representation and node features, as well as prediction task. Going\nfurther, we propose using the graph representation obtained by a GNN as an\ninput to the Extreme Gradient Boosting (XGBoost) model, resulting in a more\nresource-efficient yet accurate approach. We show the effectiveness of a\nvariety of GNN-based online planner selection methods, opening up new exciting\navenues for research on online planner selection.",
      "tldr_zh": "该研究探讨了使用 Graph Neural Networks (GNNs) 进行在线规划器选择（online planner selection），旨在为给定规划问题从预定义求解器中选择最佳方案，以应对规划任务的计算复杂性和性能差异。作者系统调查了 GNN 模型、图表示、节点特征以及预测任务的影响，并提出了一种新方法：将 GNN 生成的图表示作为输入到 Extreme Gradient Boosting (XGBoost) 模型中，实现更高效且准确的预测。实验结果证明了多种 GNN-based 方法的有效性，为在线规划器选择的研究开辟了新方向。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.04874v1",
      "published_date": "2024-01-25 13:04:27 UTC",
      "updated_date": "2024-01-25 13:04:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:23:57.524511"
    },
    {
      "arxiv_id": "2401.14151v2",
      "title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Weihao Tan",
        "Wentao Zhang",
        "Shanqi Liu",
        "Longtao Zheng",
        "Xinrun Wang",
        "Bo An"
      ],
      "abstract": "Despite the impressive performance across numerous tasks, large language\nmodels (LLMs) often fail in solving simple decision-making tasks due to the\nmisalignment of the knowledge in LLMs with environments. On the contrary,\nreinforcement learning (RL) agents learn policies from scratch, which makes\nthem always align with environments but difficult to incorporate prior\nknowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\nnovel general online framework that deploys LLMs as decision-making agents to\nefficiently interact and align with embodied environments via RL without\nrequiring any prepared datasets or prior knowledge of the environments.\nFirstly, we query the joint probabilities of each valid action with LLMs to\nform behavior policies. Then, to enhance the stability and robustness of the\npolicies, we propose two normalization methods and summarize four prompt design\nprinciples. Finally, we design a novel parameter-efficient training\narchitecture where the actor and critic share one frozen LLM equipped with\nlow-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\nevaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\nand performance compared to the conventional RL method, PPO, and prompt tuning\nmethod, SayCan, in both classical decision-making environment, Overcooked, and\nsimulated household environment, VirtualHome. ii) Benefiting from LLMs'\nopen-vocabulary feature, TWOSOME shows superior generalization ability to\nunseen tasks. iii) Under our framework, there is no significant loss of the\nLLMs' original ability during online PPO finetuning.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）在决策任务中的环境不匹配问题，提出了一种名为 TWOSOME 的在线框架，通过强化学习（RL）使 LLMs 作为决策代理高效互动并对齐实体化环境，而无需预备数据集或环境知识。框架的关键方法包括查询 LLMs 的联合概率形成行为策略、引入两种归一化方法和四种提示设计原则以提升策略稳定性，以及设计一个参数高效的训练架构，其中 actor 和 critic 共享一个冻结的 LLM，并使用低秩适配器（LoRA）通过 PPO 更新。实验结果显示，TWOSOME 在 Overcooked 和 VirtualHome 环境中比传统 RL 方法 PPO 和提示调整方法 SayCan 具有更好的样本效率和性能，并展现出对未见任务的优越泛化能力，同时在在线 PPO 微调中保持了 LLMs 的原始能力无显著损失。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICLR2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14151v2",
      "published_date": "2024-01-25 13:03:20 UTC",
      "updated_date": "2024-03-11 03:15:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:24:10.835235"
    },
    {
      "arxiv_id": "2402.00052v1",
      "title": "Zero-shot Sequential Neuro-symbolic Reasoning for Automatically Generating Architecture Schematic Designs",
      "title_zh": "零样本顺序神经符号推理用于自动生成建筑示意图设计",
      "authors": [
        "Milin Kodnongbua",
        "Lawrence H. Curtis",
        "Adriana Schulz"
      ],
      "abstract": "This paper introduces a novel automated system for generating architecture\nschematic designs aimed at streamlining complex decision-making at the\nmultifamily real estate development project's outset. Leveraging the combined\nstrengths of generative AI (neuro reasoning) and mathematical program solvers\n(symbolic reasoning), the method addresses both the reliance on expert insights\nand technical challenges in architectural schematic design. To address the\nlarge-scale and interconnected nature of design decisions needed for designing\na whole building, we proposed a novel sequential neuro-symbolic reasoning\napproach, emulating traditional architecture design processes from initial\nconcept to detailed layout. To remove the need to hand-craft a cost function to\napproximate the desired objectives, we propose a solution that uses neuro\nreasoning to generate constraints and cost functions that the symbolic solvers\ncan use to solve. We also incorporate feedback loops for each design stage to\nensure a tight integration between neuro and symbolic reasoning. Developed\nusing GPT-4 without further training, our method's effectiveness is validated\nthrough comparative studies with real-world buildings. Our method can generate\nvarious building designs in accordance with the understanding of the\nneighborhood, showcasing its potential to transform the realm of architectural\nschematic design.",
      "tldr_zh": "该论文提出了一种零-shot Sequential Neuro-symbolic Reasoning 方法，用于自动生成建筑示意图，从而简化多家庭房地产开发项目的决策过程。该方法结合神经推理（如基于 GPT-4 的生成 AI）和符号推理（数学程序求解器），通过顺序推理模拟传统设计流程，从初始概念到详细布局，并使用神经推理自动生成约束和成本函数，同时融入反馈循环以确保两者紧密整合。无需手动设计成本函数，该系统在不进行额外训练的情况下，通过与真实建筑的比较验证其有效性，能够根据社区理解生成多样化的建筑设计，展示了其在建筑领域革新的潜力。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00052v1",
      "published_date": "2024-01-25 12:52:42 UTC",
      "updated_date": "2024-01-25 12:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:24:22.587314"
    },
    {
      "arxiv_id": "2401.14142v4",
      "title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyue Xu",
        "Yi Qin",
        "Lu Mi",
        "Hao Wang",
        "Xiaomeng Li"
      ],
      "abstract": "Existing methods, such as concept bottleneck models (CBMs), have been\nsuccessful in providing concept-based interpretations for black-box deep\nlearning models. They typically work by predicting concepts given the input and\nthen predicting the final class label given the predicted concepts. However,\n(1) they often fail to capture the high-order, nonlinear interaction between\nconcepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not\nhelp correct highly correlated concepts (e.g., \"yellow belly\"), leading to\nsuboptimal final accuracy; (2) they cannot naturally quantify the complex\nconditional dependencies between different concepts and class labels (e.g., for\nan image with the class label \"Kentucky Warbler\" and a concept \"black bill\",\nwhat is the probability that the model correctly predicts another concept\n\"black crown\"), therefore failing to provide deeper insight into how a\nblack-box model works. In response to these limitations, we propose\nEnergy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural\nnetworks to define the joint energy of candidate (input, concept, class)\ntuples. With such a unified interface, prediction, concept correction, and\nconditional dependency quantification are then represented as conditional\nprobabilities, which are generated by composing different energy functions. Our\nECBMs address both limitations of existing CBMs, providing higher accuracy and\nricher concept interpretations. Empirical results show that our approach\noutperforms the state-of-the-art on real-world datasets.",
      "tldr_zh": "本研究提出 Energy-based Concept Bottleneck Models (ECBMs)，旨在解决现有 CBMs 的局限性，包括无法捕捉概念间的高阶非线性交互和量化复杂条件依赖的问题。ECBMs 通过一组神经网络定义候选 (输入、概念、类) 元组的联合能量，将预测、概念干预和概率解释统一为条件概率，由不同能量函数组成。这种方法提升了模型准确性，并提供了更丰富的概念解释。实验结果显示，ECBMs 在真实数据集上超越了最先进的方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14142v4",
      "published_date": "2024-01-25 12:46:37 UTC",
      "updated_date": "2024-12-31 03:33:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:24:33.437688"
    },
    {
      "arxiv_id": "2402.03349v1",
      "title": "When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",
      "title_zh": "当地球科学",
      "authors": [
        "Abdenour Hadid",
        "Tanujit Chakraborty",
        "Daniel Busby"
      ],
      "abstract": "Generative Artificial Intelligence (GAI) represents an emerging field that\npromises the creation of synthetic data and outputs in different modalities.\nGAI has recently shown impressive results across a large spectrum of\napplications ranging from biology, medicine, education, legislation, computer\nscience, and finance. As one strives for enhanced safety, efficiency, and\nsustainability, generative AI indeed emerges as a key differentiator and\npromises a paradigm shift in the field. This paper explores the potential\napplications of generative AI and large language models in geoscience. The\nrecent developments in the field of machine learning and deep learning have\nenabled the generative model's utility for tackling diverse prediction\nproblems, simulation, and multi-criteria decision-making challenges related to\ngeoscience and Earth system dynamics. This survey discusses several GAI models\nthat have been used in geoscience comprising generative adversarial networks\n(GANs), physics-informed neural networks (PINNs), and generative pre-trained\ntransformer (GPT)-based structures. These tools have helped the geoscience\ncommunity in several applications, including (but not limited to) data\ngeneration/augmentation, super-resolution, panchromatic sharpening, haze\nremoval, restoration, and land surface changing. Some challenges still remain\nsuch as ensuring physical interpretation, nefarious use cases, and\ntrustworthiness. Beyond that, GAI models show promises to the geoscience\ncommunity, especially with the support to climate change, urban science,\natmospheric science, marine science, and planetary science through their\nextraordinary ability to data-driven modeling and uncertainty quantification.",
      "tldr_zh": "这篇论文探讨了生成式人工智能 (GAI) 和大型语言模型 (LLMs) 在地球科学领域的应用基础、趋势和未来挑战，强调了这些技术在提升安全、效率和可持续性方面的潜力。论文综述了多种 GAI 模型，如生成对抗网络 (GANs)、物理信息神经网络 (PINNs) 和基于生成预训练变压器 (GPT) 的结构，这些模型已在数据生成/增强、超分辨率、烟雾去除和土地表面变化等应用中取得显著进展。尽管存在确保物理解释、防范恶意使用和提升可信度等挑战，但 GAI 模型有望通过数据驱动建模和不确定性量化，支持气候变化、城市科学和大气科学等领域的研究。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.03349v1",
      "published_date": "2024-01-25 12:03:50 UTC",
      "updated_date": "2024-01-25 12:03:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:24:45.868805"
    },
    {
      "arxiv_id": "2401.15103v1",
      "title": "PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Min Wu",
        "Weijun Li",
        "Lina Yu",
        "Wenqiang Li",
        "Jingyi Liu",
        "Yanjie Li",
        "Meilan Hao"
      ],
      "abstract": "Symbolic regression aims to derive interpretable symbolic expressions from\ndata in order to better understand and interpret data. %which plays an\nimportant role in knowledge discovery and interpretable machine learning.\n  In this study, a symbolic network called PruneSymNet is proposed for symbolic\nregression. This is a novel neural network whose activation function consists\nof common elementary functions and operators. The whole network is\ndifferentiable and can be trained by gradient descent method. Each subnetwork\nin the network corresponds to an expression, and our goal is to extract such\nsubnetworks to get the desired symbolic expression.\n  Therefore, a greedy pruning algorithm is proposed to prune the network into a\nsubnetwork while ensuring the accuracy of data fitting. The proposed greedy\npruning algorithm preserves the edge with the least loss in each pruning, but\ngreedy algorithm often can not get the optimal solution. In order to alleviate\nthis problem, we combine beam search during pruning to obtain multiple\ncandidate expressions each time, and finally select the expression with the\nsmallest loss as the final result. It was tested on the public data set and\ncompared with the current popular algorithms. The results showed that the\nproposed algorithm had better accuracy.",
      "tldr_zh": "本研究针对符号回归(Symbolic Regression)提出了一种名为 PruneSymNet 的符号神经网络(Symbolic Neural Network)，其激活函数由常见基本函数和运算符组成，整个网络可微分，并通过梯度下降方法训练，以从数据中提取可解释的符号表达式。论文引入了一种贪婪修剪算法(greedy pruning algorithm)，该算法在修剪网络时优先保留损失最小的边，并结合 beam search 来生成多个候选表达式，最终选择损失最小的作为结果，以缓解贪婪方法的非最优问题。在公共数据集上的实验显示，该算法比当前流行方法具有更高的准确性，为可解释机器学习提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.15103v1",
      "published_date": "2024-01-25 11:53:35 UTC",
      "updated_date": "2024-01-25 11:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:24:57.768938"
    },
    {
      "arxiv_id": "2402.01708v2",
      "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
      "title_zh": "Not My Voice! 语音生成器的伦理和安全危害分类",
      "authors": [
        "Wiebke Hutiri",
        "Oresiti Papakyriakopoulos",
        "Alice Xiang"
      ],
      "abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a\nrange of significant ethical and safety risks to society that need to be\naddressed. For example, a growing number of speech generation incidents are\nassociated with swatting attacks in the United States, where anonymous\nperpetrators create synthetic voices that call police officers to close down\nschools and hospitals, or to violently gain access to innocent citizens' homes.\nIncidents like this demonstrate that multimodal generative AI risks and harms\ndo not exist in isolation, but arise from the interactions of multiple\nstakeholders and technical AI systems. In this paper we analyse speech\ngeneration incidents to study how patterns of specific harms arise. We find\nthat specific harms can be categorised according to the exposure of affected\nindividuals, that is to say whether they are a subject of, interact with,\nsuffer due to, or are excluded from speech generation systems. Similarly,\nspecific harms are also a consequence of the motives of the creators and\ndeployers of the systems. Based on these insights we propose a conceptual\nframework for modelling pathways to ethical and safety harms of AI, which we\nuse to develop a taxonomy of harms of speech generators. Our relational\napproach captures the complexity of risks and harms in sociotechnical AI\nsystems, and yields a taxonomy that can support appropriate policy\ninterventions and decision making for the responsible development and release\nof speech generation models.",
      "tldr_zh": "这篇论文探讨了AI语音生成技术带来的伦理和安全风险，例如用于swatting攻击的合成语音事件，这些事件可能导致学校关闭、医院中断或无辜公民遭受暴力。作者通过分析语音生成事件，基于受影响个体的暴露程度（作为主体、互动、受害或被排除）以及系统创建者和部署者的动机，提出一个概念框架来建模AI的伦理和安全危害。最终，他们开发了一个语音生成器危害的taxonomy，以捕捉社会技术AI系统的风险复杂性，并为政策干预和负责任的模型开发提供决策支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 4 tables, 4 figures Accepted at the 2024 ACM Conference on\n  Fairness, Accountability, and Transparency (ACM FAccT '24)",
      "pdf_url": "http://arxiv.org/pdf/2402.01708v2",
      "published_date": "2024-01-25 11:47:06 UTC",
      "updated_date": "2024-05-15 15:26:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:25:09.970004"
    },
    {
      "arxiv_id": "2401.14112v2",
      "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design",
      "title_zh": "FP6-LLM：通过以FP6为中心的算法-系统联合设计高效服务大语言模型",
      "authors": [
        "Haojun Xia",
        "Zhen Zheng",
        "Xiaoxia Wu",
        "Shiyang Chen",
        "Zhewei Yao",
        "Stephen Youn",
        "Arash Bakhtiari",
        "Michael Wyatt",
        "Donglin Zhuang",
        "Zhongzhu Zhou",
        "Olatunji Ruwase",
        "Yuxiong He",
        "Shuaiwen Leon Song"
      ],
      "abstract": "Six-bit quantization (FP6) can effectively reduce the size of large language\nmodels (LLMs) and preserve the model quality consistently across varied\napplications. However, existing systems do not provide Tensor Core support for\nFP6 quantization and struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quantization on GPUs due\nto (1) unfriendly memory access of model weights with irregular bit-width and\n(2) high runtime overhead of weight de-quantization. To address these problems,\nwe propose TC-FPx, the first full-stack GPU kernel design scheme with unified\nTensor Core support of float-point weights for various quantization bit-width.\nWe integrate TC-FPx kernel into an existing inference system, providing new\nend-to-end support (called FP6-LLM) for quantized LLM inference, where better\ntrade-offs between inference cost and model quality are achieved. Experiments\nshow that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69x-2.65x higher normalized inference throughput than the FP16\nbaseline. The source code is publicly available at\nhttps://github.com/usyd-fsalab/fp6_llm.",
      "tldr_zh": "这篇论文介绍了 FP6-LLM，一种通过 FP6-Centric 算法-系统协同设计来高效服务大型语言模型 (LLMs) 的方法，旨在减少模型尺寸同时保持质量。论文提出 TC-FPx 内核，这是首个支持各种量化位宽浮点权重的全栈 GPU 内核设计，解决了 FP6 量化中不友好内存访问和高权重去量化开销等问题。实验结果显示，FP6-LLM 能在单个 GPU 上运行 LLaMA-70b 模型，比 FP16 基准提高了 1.69x-2.65x 的归一化推理吞吐量，并提供了开源代码以促进进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "Adding URL link of the source code",
      "pdf_url": "http://arxiv.org/pdf/2401.14112v2",
      "published_date": "2024-01-25 11:46:38 UTC",
      "updated_date": "2024-03-04 02:30:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:25:24.488995"
    },
    {
      "arxiv_id": "2401.14110v1",
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "title_zh": "翻译失败",
      "authors": [
        "Yaniv Blumenfeld",
        "Itay Hubara",
        "Daniel Soudry"
      ],
      "abstract": "The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.",
      "tldr_zh": "该研究针对深度神经网络(DNNs)量化中的计算瓶颈问题，指出高精度累加操作（如累加乘积）是主要障碍，因为使用低精度累加器会显著降低性能。作者提出了一种简单的方法，用于训练和微调高性能DNNs，使其能够采用更廉价的12位累加器，而不造成明显准确性下降。通过细粒度梯度近似技术，进一步降低累加精度时还能改善DNNs的准确性。该方法为降低DNNs推理成本提供了可行途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14110v1",
      "published_date": "2024-01-25 11:46:01 UTC",
      "updated_date": "2024-01-25 11:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:25:33.599695"
    },
    {
      "arxiv_id": "2401.14109v2",
      "title": "CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Andrei Tomut",
        "Saeed S. Jahromi",
        "Abhijoy Sarkar",
        "Uygar Kurt",
        "Sukhbinder Singh",
        "Faysal Ishtiaq",
        "Cesar Muñoz",
        "Prabdeep Singh Bajaj",
        "Ali Elborady",
        "Gianni del Bimbo",
        "Mehrazin Alizadeh",
        "David Montero",
        "Pablo Martin-Ramiro",
        "Muhammad Ibrahim",
        "Oussama Tahiri Alaoui",
        "John Malcolm",
        "Samuel Mugel",
        "Roman Orus"
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly\nin generative Artificial Intelligence (AI), but their immense size poses\nsignificant challenges, such as huge training and inference costs, substantial\nenergy demands, and limitations for on-site deployment. Traditional compression\nmethods such as pruning, distillation, and low-rank approximation focus on\nreducing the effective number of neurons in the network, while quantization\nfocuses on reducing the numerical precision of individual weights to reduce the\nmodel size while keeping the number of neurons fixed. While these compression\nmethods have been relatively successful in practice, there is no compelling\nreason to believe that truncating the number of neurons is an optimal strategy.\nIn this context, this paper introduces CompactifAI, an innovative LLM\ncompression approach using quantum-inspired Tensor Networks that focuses on the\nmodel's correlation space instead, allowing for a more controlled, refined and\ninterpretable model compression. Our method is versatile and can be implemented\nwith - or on top of - other compression techniques. As a benchmark, we\ndemonstrate that a combination of CompactifAI with quantization allows to\nreduce a 93% the memory size of LlaMA 7B, reducing also 70% the number of\nparameters, accelerating 50% the training and 25% the inference times of the\nmodel, and just with a small accuracy drop of 2% - 3%, going much beyond of\nwhat is achievable today by other compression techniques. Our methods also\nallow to perform a refined layer sensitivity profiling, showing that deeper\nlayers tend to be more suitable for tensor network compression, which is\ncompatible with recent observations on the ineffectiveness of those layers for\nLLM performance. Our results imply that standard LLMs are, in fact, heavily\noverparametrized, and do not need to be large at all.",
      "tldr_zh": "本论文提出CompactifAI，一种基于量子启发的Tensor Networks的创新方法，用于对Large Language Models (LLMs)进行极端压缩，聚焦于模型的相关性空间而非简单减少神经元数量，从而实现更可控、可解释的压缩，并可与其他技术如quantization结合。相较于传统方法如pruning和distillation，该方法在LlaMA 7B模型上实现了内存大小减少93%、参数减少70%、训练速度加快50%以及推理速度加快25%，仅以2%-3%的准确率损失为代价。实验还揭示了层敏感性分析，即更深层更适合Tensor Network压缩，并证明LLMs高度过度参数化，不需要过大尺寸。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "quant-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 4 figures, 2 tables, and supplementary information of 2\n  pages and 1 figure. Revised version with new benchmarks for LlaMA2-7B",
      "pdf_url": "http://arxiv.org/pdf/2401.14109v2",
      "published_date": "2024-01-25 11:45:21 UTC",
      "updated_date": "2024-05-13 10:48:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:25:46.213048"
    },
    {
      "arxiv_id": "2401.14089v1",
      "title": "GQHAN: A Grover-inspired Quantum Hard Attention Network",
      "title_zh": "GQHAN：受 Grover 启发的量子硬注意力网络",
      "authors": [
        "Ren-Xin Zhao",
        "Jinjing Shi",
        "Xuelong Li"
      ],
      "abstract": "Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy\nin discerning the significance of quantum data, resulting in diminished\nefficacy when handling extensive quantum datasets. Hard Attention Mechanism\n(HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters\nthe substantial challenge of non-differentiability, consequently constraining\nits extensive applicability. In response to the dilemma of HAM and QML, a\nGrover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a\nFlexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed.\nNotably, the FO is designed to surmount the non-differentiable issue by\nexecuting the activation or masking of Discrete Primitives (DPs) with Flexible\nControl (FC) to weave various discrete destinies. Based on this, such discrete\nchoice can be visualized with a specially defined Quantum Hard Attention Score\n(QHAS). Furthermore, a trainable ADO is devised to boost the generality and\nflexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network\n(GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST\nbinary classification. Experimental findings demonstrate that GQHAN adeptly\nsurmounts the non-differentiability hurdle, surpassing the efficacy of extant\nquantum soft self-attention mechanisms in accuracies and learning ability. In\nnoise experiments, GQHAN is robuster to bit-flip noise in accuracy and\namplitude damping noise in learning performance. Predictably, the proposal of\nGQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for\nfuture quantum computers to process large-scale data, and promotes the\ndevelopment of quantum computer vision.",
      "tldr_zh": "本研究针对量子机器学习(QML)模型在处理大规模量子数据时存在的注意力不足问题，提出了一种Grover-inspired Quantum Hard Attention Mechanism (GQHAM)，包括Flexible Oracle (FO)和Adaptive Diffusion Operator (ADO)。FO通过Flexible Control (FC)激活或屏蔽Discrete Primitives (DPs)，解决了HAM的非微分性问题，并定义Quantum Hard Attention Score (QHAS)来可视化离散选择；ADO则提升了机制的泛化性和灵活性。基于此，构建了Grover-inspired Quantum Hard Attention Network (GQHAN)，在PennyLane平台上应用于Fashion MNIST二分类实验，结果显示GQHAN在准确性和学习能力上优于现有量子软自注意力机制，且对bit-flip噪声的鲁棒性在准确性上、对amplitude damping噪声的鲁棒性在学习性能上更强。该框架丰富了Quantum Attention Mechanism (QAM)，为量子计算机处理大规模数据和量子计算机视觉发展奠定基础。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14089v1",
      "published_date": "2024-01-25 11:11:16 UTC",
      "updated_date": "2024-01-25 11:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:25:59.427798"
    },
    {
      "arxiv_id": "2401.14086v4",
      "title": "Generating Likely Counterfactuals Using Sum-Product Networks",
      "title_zh": "使用总和-乘积网络生成可能的反事实",
      "authors": [
        "Jiri Nemecek",
        "Tomas Pevny",
        "Jakub Marecek"
      ],
      "abstract": "The need to explain decisions made by AI systems is driven by both recent\nregulation and user demand. The decisions are often explainable only post hoc.\nIn counterfactual explanations, one may ask what constitutes the best\ncounterfactual explanation. Clearly, multiple criteria must be taken into\naccount, although \"distance from the sample\" is a key criterion. Recent methods\nthat consider the plausibility of a counterfactual seem to sacrifice this\noriginal objective. Here, we present a system that provides high-likelihood\nexplanations that are, at the same time, close and sparse. We show that the\nsearch for the most likely explanations satisfying many common desiderata for\ncounterfactual explanations can be modeled using Mixed-Integer Optimization\n(MIO). We use a Sum-Product Network (SPN) to estimate the likelihood of a\ncounterfactual. To achieve that, we propose an MIO formulation of an SPN, which\ncan be of independent interest. The source code with examples is available at\nhttps://github.com/Epanemu/LiCE.",
      "tldr_zh": "这篇论文针对AI决策解释的需求，提出了一种使用Sum-Product Networks (SPN)生成高似然反事实(counterfactuals)的方法，以平衡解释的似然性、接近原始样本和稀疏性。作者通过Mixed-Integer Optimization (MIO)建模来优化搜索过程，确保反事实解释满足多个常见标准，如高概率和简洁性。实验结果显示，该系统在提供可信解释方面表现出色，并提供了开源代码（https://github.com/Epanemu/LiCE）。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14086v4",
      "published_date": "2024-01-25 11:06:16 UTC",
      "updated_date": "2025-03-21 10:55:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:26:11.050765"
    },
    {
      "arxiv_id": "2401.14079v1",
      "title": "From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Eisenreich",
        "Sandro Speth",
        "Stefan Wagner"
      ],
      "abstract": "Designing domain models and software architectures represents a significant\nchallenge in software development, as the resulting architectures play a vital\nrole in fulfilling the system's quality of service. Due to time pressure,\narchitects often model only one architecture based on their known limited\ndomain understanding, patterns, and experience instead of thoroughly analyzing\nthe domain and evaluating multiple candidates, selecting the best fitting.\nExisting approaches try to generate domain models based on requirements, but\nstill require time-consuming manual effort to achieve good results. Therefore,\nin this vision paper, we propose a method to generate software architecture\ncandidates semi-automatically based on requirements using artificial\nintelligence techniques. We further envision an automatic evaluation and\ntrade-off analysis of the generated architecture candidates using, e.g., the\narchitecture trade-off analysis method combined with large language models and\nquantitative analyses. To evaluate this approach, we aim to analyze the quality\nof the generated architecture models and the efficiency and effectiveness of\nour proposed process by conducting qualitative studies.",
      "tldr_zh": "该研究针对软件开发中设计领域模型和软件架构的挑战，指出由于时间压力，架构师往往仅基于有限经验创建单一架构，而非全面分析多个候选方案。论文提出一种基于人工智能的技术，半自动从需求生成软件架构候选方案，并设想使用大语言模型结合架构权衡分析方法和定量分析进行自动评估。未来，通过定性研究评估生成的架构模型质量以及整个过程的效率和有效性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.2"
      ],
      "primary_category": "cs.SE",
      "comment": "4 pages, vision paper, submitted to the ICSE workshop Designing2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14079v1",
      "published_date": "2024-01-25 10:56:58 UTC",
      "updated_date": "2024-01-25 10:56:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:26:24.036979"
    },
    {
      "arxiv_id": "2401.14067v1",
      "title": "Ta'keed: The First Generative Fact-Checking System for Arabic Claims",
      "title_zh": "翻译失败",
      "authors": [
        "Saud Althabiti",
        "Mohammad Ammar Alsalka",
        "Eric Atwell"
      ],
      "abstract": "This paper introduces Ta'keed, an explainable Arabic automatic fact-checking\nsystem. While existing research often focuses on classifying claims as \"True\"\nor \"False,\" there is a limited exploration of generating explanations for claim\ncredibility, particularly in Arabic. Ta'keed addresses this gap by assessing\nclaim truthfulness based on retrieved snippets, utilizing two main components:\ninformation retrieval and LLM-based claim verification. We compiled the\nArFactEx, a testing gold-labelled dataset with manually justified references,\nto evaluate the system. The initial model achieved a promising F1 score of 0.72\nin the classification task. Meanwhile, the system's generated explanations are\ncompared with gold-standard explanations syntactically and semantically. The\nstudy recommends evaluating using semantic similarities, resulting in an\naverage cosine similarity score of 0.76. Additionally, we explored the impact\nof varying snippet quantities on claim classification accuracy, revealing a\npotential correlation, with the model using the top seven hits outperforming\nothers with an F1 score of 0.77.",
      "tldr_zh": "本研究引入了Ta'keed，这是首个生成式事实检查系统，针对阿拉伯语声明提供可解释的真实性评估，填补了现有研究在生成解释方面的空白。系统采用信息检索和LLM-based声明验证两大组件，通过检索相关片段来评估声明的真伪。研究者编译了ArFactEx数据集，该数据集包含手动标记的参考，用于系统评估。结果显示，Ta'keed在分类任务中获得0.72的F1 score，而生成的解释在语义上与金标准平均余弦相似度达0.76；此外，使用前七个检索片段时，F1 score进一步提升至0.77，表明片段数量对准确性的潜在影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, conference paper",
      "pdf_url": "http://arxiv.org/pdf/2401.14067v1",
      "published_date": "2024-01-25 10:43:00 UTC",
      "updated_date": "2024-01-25 10:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:26:34.431950"
    },
    {
      "arxiv_id": "2401.14066v3",
      "title": "CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with Multimodal Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Nisha Huang",
        "Weiming Dong",
        "Yuxin Zhang",
        "Fan Tang",
        "Ronghui Li",
        "Chongyang Ma",
        "Xiu Li",
        "Tong-Yee Lee",
        "Changsheng Xu"
      ],
      "abstract": "Although remarkable progress has been made in image style transfer, style is\njust one of the components of artistic paintings. Directly transferring\nextracted style features to natural images often results in outputs with\nobvious synthetic traces. This is because key painting attributes including\nlayout, perspective, shape, and semantics often cannot be conveyed and\nexpressed through style transfer. Large-scale pretrained text-to-image\ngeneration models have demonstrated their capability to synthesize a vast\namount of high-quality images. However, even with extensive textual\ndescriptions, it is challenging to fully express the unique visual properties\nand details of paintings. Moreover, generic models often disrupt the overall\nartistic effect when modifying specific areas, making it more complicated to\nachieve a unified aesthetic in artworks. Our main novel idea is to integrate\nmultimodal semantic information as a synthesis guide into artworks, rather than\ntransferring style to the real world. We also aim to reduce the disruption to\nthe harmony of artworks while simplifying the guidance conditions.\nSpecifically, we propose an innovative multi-task unified framework called\nCreativeSynth, based on the diffusion model with the ability to coordinate\nmultimodal inputs. CreativeSynth combines multimodal features with customized\nattention mechanisms to seamlessly integrate real-world semantic content into\nthe art domain through Cross-Art-Attention for aesthetic maintenance and\nsemantic fusion. We demonstrate the results of our method across a wide range\nof different art categories, proving that CreativeSynth bridges the gap between\ngenerative models and artistic expression. Code and results are available at\nhttps://github.com/haha-lisa/CreativeSynth.",
      "tldr_zh": "该论文指出，现有图像风格转移方法仅关注风格特征，而忽略布局、视角、形状和语义，导致合成图像出现明显合成痕迹；同时，大规模文本到图像模型虽能生成高质量图像，但难以完整表达绘画的独特视觉属性，且修改特定区域时易破坏整体艺术和谐。\n\n为了解决这些问题，研究提出CreativeSynth框架，这是一个基于扩散模型的多任务统一系统，能够协调多模态输入，并通过Cross-Art-Attention机制无缝融合多模态特征，实现审美维护和语义整合，从而将真实世界语义内容融入艺术领域。\n\n实验结果显示，CreativeSynth在多种艺术类别上表现出色，有效桥接了生成模型与艺术表达的差距，并提供了开源代码以供验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14066v3",
      "published_date": "2024-01-25 10:42:09 UTC",
      "updated_date": "2025-05-15 10:04:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:26:46.618609"
    },
    {
      "arxiv_id": "2401.14057v2",
      "title": "Left/Right Brain, human motor control and the implications for robotics",
      "title_zh": "左/右脑、人类运动控制及其对机器人学的启示",
      "authors": [
        "Jarrad Rinaldo",
        "Levin Kuhlmann",
        "Jason Friedman",
        "Gideon Kowadlo"
      ],
      "abstract": "Neural Network movement controllers promise a variety of advantages over\nconventional control methods, however, they are not widely adopted due to their\ninability to produce reliably precise movements. This research explores a\nbilateral neural network architecture as a control system for motor tasks. We\naimed to achieve hemispheric specialisation similar to what is observed in\nhumans across different tasks; the dominant system (usually the right hand,\nleft hemisphere) excels at tasks involving coordination and efficiency of\nmovement, and the non-dominant system performs better at tasks requiring\npositional stability. Specialisation was achieved by training the hemispheres\nwith different loss functions tailored to the expected behaviour of the\nrespective hemispheres. We compared bilateral models with and without\nspecialised hemispheres, with and without inter-hemispheric connectivity\n(representing the biological Corpus Callosum), and unilateral models with and\nwithout specialisation. The models were trained and tested on two tasks common\nin the human motor control literature: the random reach task, suited to the\ndominant system, a model with better coordination, and the hold position task,\nsuited to the non-dominant system, a model with more stable movement. Each\nsystem outperformed the non-preferred system in its preferred task. For both\ntasks, a bilateral model outperformed the non-preferred hand and was as good or\nbetter than the preferred hand. The results suggest that the hemispheres could\ncollaborate on tasks or work independently to their strengths. This study\nprovides ideas for how a biologically inspired bilateral architecture could be\nexploited for industrial motor control.",
      "tldr_zh": "本研究探索了双侧神经网络架构（bilateral neural network architecture）作为机器人电机控制系统，以模仿人类大脑半球专业化。研究者通过为优势半球（擅长协调和高效运动）和非优势半球（擅长位置稳定性）设计不同的损失函数（loss functions）进行训练，并比较了有/无半球间连接（inter-hemispheric connectivity，如模拟胼胝体）和单侧模型的表现。实验结果显示，在随机到达任务和保持位置任务中，双侧模型在各自偏好任务上优于非偏好系统，并整体表现不逊于或优于单侧模型。该架构证明了半球协作或独立工作的潜力，为工业电机控制提供生物启发的创新思路。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "q-bio.NC",
        "I.2.6; I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "ACAIN 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14057v2",
      "published_date": "2024-01-25 10:29:07 UTC",
      "updated_date": "2024-07-10 12:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:26:58.561506"
    },
    {
      "arxiv_id": "2401.14043v3",
      "title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Haochen Li",
        "Jonathan Leung",
        "Zhiqi Shen"
      ],
      "abstract": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields.",
      "tldr_zh": "本论文调查了Large Language Models (LLMs) 的prompt engineering方法，强调了从目标导向角度优化提示的必要性，以克服基于人类思维假设的局限性。通过回顾50个代表性研究，作者证明了goal-oriented prompt formulation（引导LLMs遵循人类逻辑思维）能显著提升模型性能。论文引入了一个新taxonomy，将目标导向提示方法分为五个相互连接的阶段，并展示了其在各种领域的广泛适用性。最后，提出了四个未来研究方向，以进一步挖掘prompt engineering的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "An up-to-date resource including papers and tasks is maintained at\n  https://github.com/Alex-HaochenLi/Goal-oriented-Prompt-Engineering",
      "pdf_url": "http://arxiv.org/pdf/2401.14043v3",
      "published_date": "2024-01-25 09:47:55 UTC",
      "updated_date": "2024-09-17 04:56:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:27:11.335619"
    },
    {
      "arxiv_id": "2401.14434v2",
      "title": "Transforming gradient-based techniques into interpretable methods",
      "title_zh": "将基于梯度的技术转化为可解释的方法",
      "authors": [
        "Caroline Mazini Rodrigues",
        "Nicolas Boutry",
        "Laurent Najman"
      ],
      "abstract": "The explication of Convolutional Neural Networks (CNN) through xAI techniques\noften poses challenges in interpretation. The inherent complexity of input\nfeatures, notably pixels extracted from images, engenders complex correlations.\nGradient-based methodologies, exemplified by Integrated Gradients (IG),\neffectively demonstrate the significance of these features. Nevertheless, the\nconversion of these explanations into images frequently yields considerable\nnoise. Presently, we introduce GAD (Gradient Artificial Distancing) as a\nsupportive framework for gradient-based techniques. Its primary objective is to\naccentuate influential regions by establishing distinctions between classes.\nThe essence of GAD is to limit the scope of analysis during visualization and,\nconsequently reduce image noise. Empirical investigations involving occluded\nimages have demonstrated that the identified regions through this methodology\nindeed play a pivotal role in facilitating class differentiation.",
      "tldr_zh": "本研究针对解释卷积神经网络 (CNN) 的挑战，指出基于梯度的技术如 Integrated Gradients (IG) 虽能突出特征重要性，但图像可视化时常伴随大量噪音。论文提出 GAD (Gradient Artificial Distancing) 框架，作为对梯度方法的辅助工具，通过在类之间建立区分来强调影响区域，并限制分析范围以减少噪音。实验结果显示，使用遮挡图像，GAD 识别的区域在类区分中发挥关键作用，从而提升了解释的可解读性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14434v2",
      "published_date": "2024-01-25 09:24:19 UTC",
      "updated_date": "2024-05-15 08:52:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:27:22.498701"
    },
    {
      "arxiv_id": "2401.14032v1",
      "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Butian Xiong",
        "Zhuo Li",
        "Zhen Li"
      ],
      "abstract": "We introduce a novel large-scale scene reconstruction benchmark using the\nnewly developed 3D representation approach, Gaussian Splatting, on our\nexpansive U-Scene dataset. U-Scene encompasses over one and a half square\nkilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground\ntruth. For data acquisition, we employed the Matrix 300 drone equipped with the\nhigh-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This\ndataset, offers a unique blend of urban and academic environments for advanced\nspatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with\nGaussian Splatting includes a detailed analysis across various novel\nviewpoints. We also juxtapose these results with those derived from our\naccurate point cloud dataset, highlighting significant differences that\nunderscore the importance of combine multi-modal information",
      "tldr_zh": "本研究引入了 GauU-Scene 基准，利用 Gaussian Splatting 作为 3D 表示方法，在大型 U-Scene 数据集上进行场景重建。U-Scene 数据集覆盖超过 1.5 平方公里，包括 RGB 数据和 LiDAR 地面真相，通过 Matrix 300 无人机和 Zenmuse L1 LiDAR 采集，涵盖城市和学术环境。评估结果显示，Gaussian Splatting 在各种新视角下表现出色，与点云数据集比较后，强调了结合多模态信息的重要性，以提升空间分析精度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IJCAI2024 submit, 8 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.14032v1",
      "published_date": "2024-01-25 09:22:32 UTC",
      "updated_date": "2024-01-25 09:22:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:27:35.303307"
    },
    {
      "arxiv_id": "2401.14019v1",
      "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI",
      "title_zh": "Unitxt：灵活、可共享和可重用的生成式 AI 数据准备和评估",
      "authors": [
        "Elron Bandel",
        "Yotam Perlitz",
        "Elad Venezian",
        "Roni Friedman-Melamed",
        "Ofir Arviv",
        "Matan Orbach",
        "Shachar Don-Yehyia",
        "Dafna Sheinwald",
        "Ariel Gera",
        "Leshem Choshen",
        "Michal Shmueli-Scheuer",
        "Yoav Katz"
      ],
      "abstract": "In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!",
      "tldr_zh": "在生成式 AI 领域，传统文本处理管道的刚性和特定性限制了研究的可重复性和灵活性，为此，论文提出 Unitxt，这是一个创新库，用于可定制的文本数据准备和评估。Unitxt 通过将处理流程分解成模块化组件（如模型特定格式、任务提示等），并与 HuggingFace 和 LM-eval-harness 等库无缝集成，方便用户自定义和共享组件。Unitxt-Catalog 作为中心化资源，促进社区协作，用户可通过 https://github.com/IBM/unitxt 参与共建，从而提升生成式 NLP 工作流的效率和创新性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to NAACL demo track",
      "pdf_url": "http://arxiv.org/pdf/2401.14019v1",
      "published_date": "2024-01-25 08:57:33 UTC",
      "updated_date": "2024-01-25 08:57:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:27:47.276945"
    },
    {
      "arxiv_id": "2401.14011v3",
      "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zheqi He",
        "Xinya Wu",
        "Pengfei Zhou",
        "Richeng Xuan",
        "Guang Liu",
        "Xi Yang",
        "Qiannan Zhu",
        "Hua Huang"
      ],
      "abstract": "Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose an evaluation strategy called Positional Error Variance for assessing\nmultiple-choice questions. The strategy aims to perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs. The data and code are available at\nhttps://github.com/FlagOpen/CMMU.",
      "tldr_zh": "该研究引入了 CMMU，这是一个针对中文的多模态多类型问题理解和推理的基准，用于评估多模态大语言模型(MLLMs)在领域特定知识上的表现。CMMU 包含 3,603 个问题，覆盖 7 个科目从小学到高中的知识，并包括 multiple-choice、多重响应(multiple-response)和填空(fill-in-the-blank)三种问题类型，以提供更全面的挑战。此外，研究提出了一种评估策略 Positional Error Variance，用于量化分析位置偏差；实验结果显示，评估七个开源 MLLMs 以及 GPT4-V、Gemini-Pro 和 Qwen-VL-Plus 等模型时，CMMU 对这些模型构成了显著挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.14011v3",
      "published_date": "2024-01-25 08:22:10 UTC",
      "updated_date": "2024-05-08 07:34:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:28:00.061758"
    },
    {
      "arxiv_id": "2401.14003v1",
      "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases",
      "title_zh": "翻译失败",
      "authors": [
        "Quyet V. Do",
        "Tianqing Fang",
        "Shizhe Diao",
        "Zhaowei Wang",
        "Yangqiu Song"
      ],
      "abstract": "Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has\nbeen explored as a way to acquire new commonsense knowledge based on reference\nknowledge in the original CSKBs and external prior knowledge. Despite the\nadvancement of Large Language Models (LLM) and prompt engineering techniques in\nvarious reasoning tasks, they still struggle to deal with CSKB reasoning. One\nof the problems is that it is hard for them to acquire explicit relational\nconstraints in CSKBs from only in-context exemplars, due to a lack of symbolic\nreasoning capabilities (Bengio et al., 2021). To this end, we proposed\n**ConstraintChecker**, a plugin over prompting techniques to provide and check\nexplicit constraints. When considering a new knowledge instance,\nConstraintChecker employs a rule-based module to produce a list of constraints,\nthen it uses a zero-shot learning module to check whether this knowledge\ninstance satisfies all constraints. The acquired constraint-checking result is\nthen aggregated with the output of the main prompting technique to produce the\nfinal output. Experimental results on CSKB Reasoning benchmarks demonstrate the\neffectiveness of our method by bringing consistent improvements over all\nprompting methods. Codes and data are available at\n\\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.",
      "tldr_zh": "该研究针对大型语言模型 (LLM) 在常识知识库 (CSKB) 推理中难以获取显式关系约束的问题，提出了一种插件 ConstraintChecker，以增强其符号推理能力。ConstraintChecker 通过一个规则-based 模块生成约束列表，并利用零样本学习 (zero-shot learning) 模块检查新知识实例是否满足这些约束，然后将检查结果与主要提示技术的输出聚合以产生最终输出。实验结果显示，该方法在 CSKB 推理基准上为所有提示方法带来了持续的性能改进，代码和数据已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.14003v1",
      "published_date": "2024-01-25 08:03:38 UTC",
      "updated_date": "2024-01-25 08:03:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:28:11.262964"
    },
    {
      "arxiv_id": "2401.13996v1",
      "title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Qian",
        "Shihao Liang",
        "Yujia Qin",
        "Yining Ye",
        "Xin Cong",
        "Yankai Lin",
        "Yesai Wu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy\nfor enhancing the adaptability and flexibility of AI agents through inter-task\nself-evolution. Unlike existing methods focused on intra-task learning, ICE\npromotes the transfer of knowledge between tasks for genuine self-evolution,\nsimilar to human experience learning. The strategy dynamically investigates\nplanning and execution trajectories, consolidates them into simplified\nworkflows and pipelines, and exploits them for improved task execution. Our\nexperiments on the XAgent framework demonstrate ICE's effectiveness, reducing\nAPI calls by as much as 80% and significantly decreasing the demand for the\nmodel's capability. Specifically, when combined with GPT-3.5, ICE's performance\nmatches that of raw GPT-4 across various agent tasks. We argue that this\nself-evolution approach represents a paradigm shift in agent design,\ncontributing to a more robust AI community and ecosystem, and moving a step\ncloser to full autonomy.",
      "tldr_zh": "本论文提出了一种名为 ICE (Investigate-Consolidate-Exploit) 的通用策略，旨在通过任务间知识转移提升 AI 代理的适应性和灵活性，实现类似于人类经验学习的自我进化。\nICE 策略包括三个关键步骤：动态调查规划和执行轨迹、整合它们成简化工作流和管道、并利用这些优化任务执行，从而减少对模型能力的依赖。\n在 XAgent 框架上的实验表明，ICE 能降低 API 调用高达 80%，并使结合 GPT-3.5 的性能媲美原始 GPT-4。\n这一方法代表了代理设计范式的转变，促进更强大的 AI 生态，并向全自治迈进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.13996v1",
      "published_date": "2024-01-25 07:47:49 UTC",
      "updated_date": "2024-01-25 07:47:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:28:24.602104"
    },
    {
      "arxiv_id": "2402.03348v2",
      "title": "Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Sangyu Han",
        "Yearim Kim",
        "Nojun Kwak"
      ],
      "abstract": "The truthfulness of existing explanation methods in authentically elucidating\nthe underlying model's decision-making process has been questioned. Existing\nmethods have deviated from faithfully representing the model, thus susceptible\nto adversarial attacks. To address this, we propose a novel eXplainable AI\n(XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects\nthe model's inference process, resulting in significantly enhanced robustness\nin our explanations. Different from the conventional emphasis on the neuronal\nlevel, we adopt a vector perspective to consider the intricate nonlinear\ninteractions between filters. We also introduce an interesting observation\ntermed Activation-Pattern-Only Prediction (APOP), letting us emphasize the\nimportance of inactive neurons and redefine relevance encapsulating all\nrelevant information including both active and inactive neurons. Our method,\nSRD, allows for the recursive decomposition of a Pointwise Feature Vector\n(PFV), providing a high-resolution Effective Receptive Field (ERF) at any\nlayer.",
      "tldr_zh": "该论文指出了现有解释方法在真实反映模型决策过程方面存在偏差，导致易受对抗攻击的问题。为解决此，研究提出了一种新型可解释AI(XAI)方法SRD(Sharing Ratio Decomposition)，它从向量视角分析过滤器间的非线性交互，并引入Activation-Pattern-Only Prediction(APOP)观察，以强调不活跃神经元的重要性，从而重新定义相关性。SRD允许递归分解Pointwise Feature Vector(PFV)，生成高分辨率的Effective Receptive Field(ERF)，显著提升了解释的细粒度和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "To be published in ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.03348v2",
      "published_date": "2024-01-25 07:20:23 UTC",
      "updated_date": "2024-12-12 05:56:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:28:34.552593"
    },
    {
      "arxiv_id": "2401.13987v1",
      "title": "Cross-Domain Few-Shot Learning via Adaptive Transformer Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Naeem Paeedeh",
        "Mahardhika Pratama",
        "Muhammad Anwar Ma'sum",
        "Wolfgang Mayer",
        "Zehong Cao",
        "Ryszard Kowlczyk"
      ],
      "abstract": "Most few-shot learning works rely on the same domain assumption between the\nbase and the target tasks, hindering their practical applications. This paper\nproposes an adaptive transformer network (ADAPTER), a simple but effective\nsolution for cross-domain few-shot learning where there exist large domain\nshifts between the base task and the target task. ADAPTER is built upon the\nidea of bidirectional cross-attention to learn transferable features between\nthe two domains. The proposed architecture is trained with DINO to produce\ndiverse, and less biased features to avoid the supervision collapse problem.\nFurthermore, the label smoothing approach is proposed to improve the\nconsistency and reliability of the predictions by also considering the\npredicted labels of the close samples in the embedding space. The performance\nof ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it\noutperforms prior arts with significant margins.",
      "tldr_zh": "该论文针对cross-domain few-shot learning问题，提出了一种adaptive transformer network (ADAPTER)，以处理base task和target task之间的大型领域偏移。ADAPTER通过bidirectional cross-attention机制学习两个领域的可转移特征，并结合DINO训练产生多样化和不偏的特征，同时引入label smoothing方法来提升预测的一致性和可靠性。实验结果显示，在BSCD-FSL基准测试中，ADAPTER的表现显著优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Consideration in Knowledge-based Systems",
      "pdf_url": "http://arxiv.org/pdf/2401.13987v1",
      "published_date": "2024-01-25 07:05:42 UTC",
      "updated_date": "2024-01-25 07:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:28:47.288875"
    },
    {
      "arxiv_id": "2401.13986v1",
      "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
      "title_zh": "翻译失败",
      "authors": [
        "Yanda Chen",
        "Chandan Singh",
        "Xiaodong Liu",
        "Simiao Zuo",
        "Bin Yu",
        "He He",
        "Jianfeng Gao"
      ],
      "abstract": "Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .",
      "tldr_zh": "大语言模型 (LLMs) 经常生成流利但不一致的自然语言解释，例如对相关问题给出矛盾的回答。论文提出 explanation-consistency finetuning (EC-finetuning) 方法，通过在精心构建的合成数据上微调 LLMs，来确保解释在相关示例上保持一致。实验结果显示，该方法在多种问答数据集上提升了 10.0% 的解释一致性，并能泛化到未见过的七个数据集（+4.5% 相对提升）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2307.08678",
      "pdf_url": "http://arxiv.org/pdf/2401.13986v1",
      "published_date": "2024-01-25 07:04:30 UTC",
      "updated_date": "2024-01-25 07:04:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:28:59.705772"
    },
    {
      "arxiv_id": "2401.13979v3",
      "title": "Routoo: Learning to Route to Large Language Models Effectively",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Mohammadshahi",
        "Arshad Rafiq Shaikh",
        "Majid Yazdani"
      ],
      "abstract": "LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.",
      "tldr_zh": "该研究引入了Routoo架构，用于优化Large Language Models (LLMs) 的选择，以平衡响应质量和推理成本。Routoo 包括两个关键组件：一个轻量级LLM作为性能预测器，用于估计不同LLMs在给定提示下的预期性能，以及一个成本感知选择器（cost-aware selector），根据预测和约束（如成本和延迟）选择最合适的模型。实验在MMLU基准测试中显示，Routoo能匹配Mixtral 8x7b的性能，同时将推理成本降低三分之一；当整合GPT4时，它在半成本下几乎达到GPT4的性能水平，并在等效成本下提升准确率超过5%，达到75.9%。这项工作证明了Routoo在降低成本的同时维持或提升LLMs的整体效能，为高效部署多模型系统提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13979v3",
      "published_date": "2024-01-25 06:45:32 UTC",
      "updated_date": "2024-10-02 08:51:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:29:12.471954"
    },
    {
      "arxiv_id": "2401.13976v1",
      "title": "Learning to Manipulate Artistic Images",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Guo",
        "Yuqi Zhang",
        "De Ma",
        "Qian Zheng"
      ],
      "abstract": "Recent advancement in computer vision has significantly lowered the barriers\nto artistic creation. Exemplar-based image translation methods have attracted\nmuch attention due to flexibility and controllability. However, these methods\nhold assumptions regarding semantics or require semantic information as the\ninput, while accurate semantics is not easy to obtain in artistic images.\nBesides, these methods suffer from cross-domain artifacts due to training data\nprior and generate imprecise structure due to feature compression in the\nspatial domain. In this paper, we propose an arbitrary Style Image Manipulation\nNetwork (SIM-Net), which leverages semantic-free information as guidance and a\nregion transportation strategy in a self-supervised manner for image\ngeneration. Our method balances computational efficiency and high resolution to\na certain extent. Moreover, our method facilitates zero-shot style image\nmanipulation. Both qualitative and quantitative experiments demonstrate the\nsuperiority of our method over state-of-the-art methods.Code is available at\nhttps://github.com/SnailForce/SIM-Net.",
      "tldr_zh": "本论文探讨了基于示例的图像翻译方法在艺术图像处理中的局限性，包括对语义的依赖、跨域伪像生成和结构不精确问题。作者提出任意风格图像操作网络（SIM-Net），利用语义-free信息作为指导，并采用自监督的区域传输策略，实现高效的高分辨率图像生成，同时支持零-shot风格图像操作。实验结果显示，SIM-Net在定性和定量评估中优于现有最先进方法，为艺术图像操控提供了更灵活和可靠的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13976v1",
      "published_date": "2024-01-25 06:34:49 UTC",
      "updated_date": "2024-01-25 06:34:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:29:23.894494"
    },
    {
      "arxiv_id": "2401.13974v1",
      "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Senthil Purushwalkam",
        "Akash Gokul",
        "Shafiq Joty",
        "Nikhil Naik"
      ],
      "abstract": "Recent text-to-image generation models have demonstrated incredible success\nin generating images that faithfully follow input prompts. However, the\nrequirement of using words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In this work, we address\nthis shortcoming by proposing an approach to enable personalization\ncapabilities in existing text-to-image diffusion models. We propose a novel\narchitecture (BootPIG) that allows a user to provide reference images of an\nobject in order to guide the appearance of a concept in the generated images.\n  The proposed BootPIG architecture makes minimal modifications to a pretrained\ntext-to-image diffusion model and utilizes a separate UNet model to steer the\ngenerations toward the desired appearance. We introduce a training procedure\nthat allows us to bootstrap personalization capabilities in the BootPIG\narchitecture using data generated from pretrained text-to-image models, LLM\nchat agents, and image segmentation models. In contrast to existing methods\nthat require several days of pretraining, the BootPIG architecture can be\ntrained in approximately 1 hour. Experiments on the DreamBooth dataset\ndemonstrate that BootPIG outperforms existing zero-shot methods while being\ncomparable with test-time finetuning approaches. Through a user study, we\nvalidate the preference for BootPIG generations over existing methods both in\nmaintaining fidelity to the reference object's appearance and aligning with\ntextual prompts.",
      "tldr_zh": "本文提出 BootPIG 架构，用于在预训练的 diffusion models 中实现零样本个性化图像生成，允许用户通过参考图像引导生成概念的外观，从而解决文本提示控制有限的问题。该架构仅对预训练模型进行最小修改，并使用一个单独的 UNet 模型来引导生成过程，同时引入一种快速训练程序，利用预训练文本到图像模型、LLM 聊天代理和图像分割模型，仅需约 1 小时即可完成训练。实验结果显示，在 DreamBooth 数据集上，BootPIG 优于现有零样本方法，与测试时微调方法相当，且用户研究证明其在保持参考对象外观 fidelity 和文本提示对齐方面更受欢迎。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13974v1",
      "published_date": "2024-01-25 06:18:20 UTC",
      "updated_date": "2024-01-25 06:18:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:29:36.613020"
    },
    {
      "arxiv_id": "2401.13968v1",
      "title": "Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Anwar Ma'sum",
        "MD Rasel Sarkar",
        "Mahardhika Pratama",
        "Savitha Ramasamy",
        "Sreenatha Anavatti",
        "Lin Liu",
        "Habibullah",
        "Ryszard Kowalczyk"
      ],
      "abstract": "A reliable long-term time-series forecaster is highly demanded in practice\nbut comes across many challenges such as low computational and memory\nfootprints as well as robustness against dynamic learning environments. This\npaper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic\nlong-term time-series forecasting tasks. MANTRA relies on the concept of fast\nand slow learners where a collection of fast learners learns different aspects\nof data distributions while adapting quickly to changes. A slow learner tailors\nsuitable representations to fast learners. Fast adaptations to dynamic\nenvironments are achieved using the universal representation transformer layers\nproducing task-adapted representations with a small number of parameters. Our\nexperiments using four datasets with different prediction lengths demonstrate\nthe advantage of our approach with at least $3\\%$ improvements over the\nbaseline algorithms for both multivariate and univariate settings. Source codes\nof MANTRA are publicly available in\n\\url{https://github.com/anwarmaxsum/MANTRA}.",
      "tldr_zh": "这篇论文提出了Meta Transformer Networks (MANTRA)，一种用于动态长期时间序列预测的方法，旨在解决计算和内存效率问题以及对环境变化的鲁棒性挑战。\nMANTRA 采用快速学习者和缓慢学习者的框架，其中多个快速学习者处理数据分布的不同方面并快速适应变化，而缓慢学习者提供定制化的表示；同时，利用通用Transformer层以少量参数实现任务适应。\n实验结果显示，在四个数据集上的多变量和单变量预测中，MANTRA 比基线算法至少提高了3%的性能，且源代码已在GitHub上公开可用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Consideration in IEEE Transactions on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2401.13968v1",
      "published_date": "2024-01-25 06:03:56 UTC",
      "updated_date": "2024-01-25 06:03:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:29:48.199296"
    },
    {
      "arxiv_id": "2403.18827v1",
      "title": "Bridging Generative Networks with the Common Model of Cognition",
      "title_zh": "翻译失败",
      "authors": [
        "Robert L. West",
        "Spencer Eckler",
        "Brendan Conway-Smith",
        "Nico Turcas",
        "Eilene Tomkins-Flanagan",
        "Mary Alexandria Kelly"
      ],
      "abstract": "This article presents a theoretical framework for adapting the Common Model\nof Cognition to large generative network models within the field of artificial\nintelligence. This can be accomplished by restructuring modules within the\nCommon Model into shadow production systems that are peripheral to a central\nproduction system, which handles higher-level reasoning based on the shadow\nproductions' output. Implementing this novel structure within the Common Model\nallows for a seamless connection between cognitive architectures and generative\nneural networks.",
      "tldr_zh": "这篇文章提出了一种理论框架，将 Common Model of Cognition 适应于人工智能领域的生成网络模型中。框架通过将 Common Model 的模块重构为外围的 shadow production systems，这些系统为中央 production system 提供输出，以支持高层推理。最终，这种结构实现了认知架构与生成神经网络的无缝连接。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.18827v1",
      "published_date": "2024-01-25 05:48:50 UTC",
      "updated_date": "2024-01-25 05:48:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:29:59.119977"
    },
    {
      "arxiv_id": "2401.13945v1",
      "title": "General Automatic Solution Generation of Social Problems",
      "title_zh": "社会问题的通用自动解决方案生成",
      "authors": [
        "Tong Niu",
        "Haoyu Huang",
        "Yu Du",
        "Weihao Zhang",
        "Luping Shi",
        "Rong Zhao"
      ],
      "abstract": "Given the escalating intricacy and multifaceted nature of contemporary social\nsystems, manually generating solutions to address pertinent social issues has\nbecome a formidable task. In response to this challenge, the rapid development\nof artificial intelligence has spurred the exploration of computational\nmethodologies aimed at automatically generating solutions. However, current\nmethods for auto-generation of solutions mainly concentrate on local social\nregulations that pertain to specific scenarios. Here, we report an automatic\nsocial operating system (ASOS) designed for general social solution generation,\nwhich is built upon agent-based models, enabling both global and local analyses\nand regulations of social problems across spatial and temporal dimensions. ASOS\nadopts a hypergraph with extensible social semantics for a comprehensive and\nstructured representation of social dynamics. It also incorporates a\ngeneralized protocol for standardized hypergraph operations and a symbolic\nhybrid framework that delivers interpretable solutions, yielding a balance\nbetween regulatory efficacy and function viability. To demonstrate the\neffectiveness of ASOS, we apply it to the domain of averting extreme events\nwithin international oil futures markets. By generating a new trading role\nsupplemented by new mechanisms, ASOS can adeptly discern precarious market\nconditions and make front-running interventions for non-profit purposes. This\nstudy demonstrates that ASOS provides an efficient and systematic approach for\ngenerating solutions for enhancing our society.",
      "tldr_zh": "面对日益复杂的当代社会问题，现有的自动解决方案生成方法主要局限于特定场景的局部社会规定。为此，本文提出了一种自动社会操作系统(ASOS)，基于agent-based models构建，支持对社会问题的全局和局部分析及调节。ASOS采用可扩展社会语义的hypergraph进行全面结构化表示，并整合通用超图操作协议和符号混合框架，以生成可解释的解决方案，实现调节效能与功能可行性的平衡。在国际石油期货市场的极端事件预防应用中，ASOS通过创建新交易角色和机制，成功识别风险并进行非营利干预，展示了其高效、系统化的社会提升潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CE",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13945v1",
      "published_date": "2024-01-25 05:00:46 UTC",
      "updated_date": "2024-01-25 05:00:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:30:12.989403"
    },
    {
      "arxiv_id": "2401.13935v1",
      "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
      "title_zh": "一种新的反事实推理范式，用于公平性和补救措施",
      "authors": [
        "Lucius E. J. Bynum",
        "Joshua R. Loftus",
        "Julia Stoyanovich"
      ],
      "abstract": "Counterfactuals and counterfactual reasoning underpin numerous techniques for\nauditing and understanding artificial intelligence (AI) systems. The\ntraditional paradigm for counterfactual reasoning in this literature is the\ninterventional counterfactual, where hypothetical interventions are imagined\nand simulated. For this reason, the starting point for causal reasoning about\nlegal protections and demographic data in AI is an imagined intervention on a\nlegally-protected characteristic, such as ethnicity, race, gender, disability,\nage, etc. We ask, for example, what would have happened had your race been\ndifferent? An inherent limitation of this paradigm is that some demographic\ninterventions -- like interventions on race -- may not translate into the\nformalisms of interventional counterfactuals. In this work, we explore a new\nparadigm based instead on the backtracking counterfactual, where rather than\nimagine hypothetical interventions on legally-protected characteristics, we\nimagine alternate initial conditions while holding these characteristics fixed.\nWe ask instead, what would explain a counterfactual outcome for you as you\nactually are or could be? This alternate framework allows us to address many of\nthe same social concerns, but to do so while asking fundamentally different\nquestions that do not rely on demographic interventions.",
      "tldr_zh": "该论文批判了AI公平性和补救措施中传统的interventional counterfactual范式，该方法通过假设对受法律保护特征（如种族或性别）的干预来模拟反事实推理，但存在实际应用局限性。作者提出一个新的backtracking counterfactual范式，该框架在保持这些特征不变的情况下，想象替代初始条件，以解释可能的反事实结果，从而避免直接干预人口统计变量。相比传统方法，这一新范式能更有效地处理社会关切，提供一种不依赖于假设干预的更稳健的反事实推理框架。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13935v1",
      "published_date": "2024-01-25 04:28:39 UTC",
      "updated_date": "2024-01-25 04:28:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:30:23.995125"
    },
    {
      "arxiv_id": "2402.03347v1",
      "title": "Transfer Learning With Densenet201 Architecture Model For Potato Leaf Disease Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Rifqi Alfinnur Charisma",
        "Faisal Dharma Adhinata"
      ],
      "abstract": "Potato plants are plants that are beneficial to humans. Like other plants in\ngeneral, potato plants also have diseases; if this disease is not treated\nimmediately, there will be a significant decrease in food production.\nTherefore, it is necessary to detect diseases quickly and precisely so that\ndisease control can be carried out effectively and efficiently. Classification\nof potato leaf disease can be done directly. Still, the symptoms cannot always\nexplain the type of disease that attacks potato leaves because there are many\ntypes of diseases with symptoms that look the same. Humans also have\ndeficiencies in determining the results of identification of potato leaf\ndisease, so sometimes the results of identification between individuals can be\ndifferent. Therefore, the use of Deep Learning for the classification process\nof potato leaf disease is expected to shorten the time and have a high\nclassification accuracy. This study uses a deep learning method with the\nDenseNet201 architecture. The choice to use the DenseNet201 algorithm in this\nstudy is because the model can identify important features of potato leaves and\nrecognize early signs of emerging diseases. This study aimed to evaluate the\neffectiveness of the transfer learning method with the DenseNet201 architecture\nin increasing the classification accuracy of potato leaf disease compared to\ntraditional classification methods. This study uses two types of scenarios,\nnamely, comparing the number of dropouts and comparing the three optimizers.\nThis test produces the best model using dropout 0.1 and Adam optimizer with an\naccuracy of 99.5% for training, 95.2% for validation, and 96% for the confusion\nmatrix. In this study, using data testing, as many as 40 images were tested\ninto the model that has been built. The test results on this model resulted in\na new accuracy for classifying potato leaf disease, namely 92.5%.",
      "tldr_zh": "本研究使用迁移学习（Transfer Learning）和 DenseNet201 架构模型来分类马铃薯叶病害，旨在提高检测效率和准确性，以应对病害对粮食生产的潜在影响。研究通过比较不同 dropout 率（如 0.1）和三种优化器，最终选定 dropout 0.1 与 Adam optimizer 的组合作为最佳模型。实验结果显示，该模型在训练集上达到 99.5% 准确率，验证集上为 95.2%，混淆矩阵准确率 96%，并在独立测试集（40 张图像）上实现 92.5% 的分类准确率。相较于传统方法，该方法显著提升了马铃薯叶病害识别的可靠性和速度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.03347v1",
      "published_date": "2024-01-25 03:58:40 UTC",
      "updated_date": "2024-01-25 03:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:30:37.080795"
    },
    {
      "arxiv_id": "2401.13920v3",
      "title": "LocMoE: A Low-Overhead MoE for Large Language Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Li",
        "Zhijie Sun",
        "Xuan He",
        "Li Zeng",
        "Yi Lin",
        "Entong Li",
        "Binfan Zheng",
        "Rongqian Zhao",
        "Xin Chen"
      ],
      "abstract": "The Mixtures-of-Experts (MoE) model is a widespread distributed and\nintegrated learning method for large language models (LLM), which is favored\ndue to its ability to sparsify and expand models efficiently. However, the\nperformance of MoE is limited by load imbalance and high latency of All-to-All\ncommunication, along with relatively redundant computation owing to large\nexpert capacity. Load imbalance may result from existing routing policies that\nconsistently tend to select certain experts. The frequent inter-node\ncommunication in the All-to-All procedure also significantly prolongs the\ntraining time. To alleviate the above performance problems, we propose a novel\nrouting strategy that combines load balance and locality by converting partial\ninter-node communication to that of intra-node. Notably, we elucidate that\nthere is a minimum threshold for expert capacity, calculated through the\nmaximal angular deviation between the gating weights of the experts and the\nassigned tokens. We port these modifications on the PanGu-Sigma model based on\nthe MindSpore framework with multi-level routing and conduct experiments on\nAscend clusters. The experiment results demonstrate that the proposed LocMoE\nreduces training time per epoch by 12.68% to 22.24% compared to classical\nrouters, such as hash router and switch router, without impacting the model\naccuracy.",
      "tldr_zh": "本文提出 LocMoE，一种低开销的 Mixtures-of-Experts (MoE) 模型，用于大型语言模型 (LLM) 训练，旨在解决现有 MoE 模型的负载不平衡、高延迟 All-to-All 通信以及冗余计算问题。LocMoE 采用一种新型路由策略，将部分 inter-node 通信转换为 intra-node 通信，并通过计算专家容量最小阈值（基于专家门控权重和分配标记的最大角度偏差）来优化负载和局部性。在 PanGu-Sigma 模型上的实验表明，LocMoE 比传统路由器（如 hash 和 switch 路由器）减少每轮训练时间 12.68% 到 22.24%，同时保持模型准确性不变。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "1. Update the font size of all figures. 2. Update the name of the\n  proposed layer Grouped Average Pooling (GrAP). 3. Change the order of the\n  Section Contribution Statement",
      "pdf_url": "http://arxiv.org/pdf/2401.13920v3",
      "published_date": "2024-01-25 03:36:39 UTC",
      "updated_date": "2024-05-23 10:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:30:49.159903"
    },
    {
      "arxiv_id": "2401.13919v4",
      "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
      "title_zh": "WebVoyager：利用大型多模态模型构建端到端的网络代理",
      "authors": [
        "Hongliang He",
        "Wenlin Yao",
        "Kaixin Ma",
        "Wenhao Yu",
        "Yong Dai",
        "Hongming Zhang",
        "Zhenzhong Lan",
        "Dong Yu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has led to a new era\nmarked by the development of autonomous applications in real-world scenarios,\nwhich drives innovation in creating advanced web agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe establish a new benchmark by compiling real-world tasks from 15 popular\nwebsites and introduce an automatic evaluation protocol leveraging multimodal\nunderstanding abilities of GPT-4V to evaluate open-ended web agents. We show\nthat WebVoyager achieves a 59.1% task success rate on our benchmark,\nsignificantly surpassing the performance of both GPT-4 (All Tools) and the\nWebVoyager (text-only) setups, underscoring the exceptional capability of\nWebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement\nwith human judgment, indicating its effectiveness in providing reliable and\naccurate assessments of web agents.",
      "tldr_zh": "该研究针对现有网络代理仅处理单一输入模式并限于简化评估的问题，提出了 WebVoyager，一种基于 Large Multimodal Models (LMM) 的端到端网络代理，能够通过与真实网站互动来完成用户指令。WebVoyager 整合了大型语言模型（LLMs）的优势，处理多模态输入以提升在真实场景中的适用性。研究者建立了新基准，包括从 15 个热门网站收集的真实任务，并引入了利用 GPT-4V 多模态理解能力的自动评估协议，该协议与人类判断一致性达 85.3%。实验结果显示，WebVoyager 在基准上实现了 59.1% 的任务成功率，显著优于 GPT-4 (All Tools) 和纯文本版本的 WebVoyager。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 (main). Code and data is released at\n  https://github.com/MinorJerry/WebVoyager",
      "pdf_url": "http://arxiv.org/pdf/2401.13919v4",
      "published_date": "2024-01-25 03:33:18 UTC",
      "updated_date": "2024-06-06 18:37:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:31:01.123717"
    },
    {
      "arxiv_id": "2401.13913v2",
      "title": "Spectral Clustering for Discrete Distributions",
      "title_zh": "针对离散分布的谱聚类",
      "authors": [
        "Zixiao Wang",
        "Dong Qiao",
        "Jicong Fan"
      ],
      "abstract": "The discrete distribution is often used to describe complex instances in\nmachine learning, such as images, sequences, and documents. Traditionally,\nclustering of discrete distributions (D2C) has been approached using\nWasserstein barycenter methods. These methods operate under the assumption that\nclusters can be well-represented by barycenters, which is seldom true in many\nreal-world applications. Additionally, these methods are not scalable for large\ndatasets due to the high computational cost of calculating Wasserstein\nbarycenters. In this work, we explore the feasibility of using spectral\nclustering combined with distribution affinity measures (e.g., maximum mean\ndiscrepancy and Wasserstein distance) to cluster discrete distributions. We\ndemonstrate that these methods can be more accurate and efficient than\nbarycenter methods. To further enhance scalability, we propose using linear\noptimal transport to construct affinity matrices efficiently for large\ndatasets. We provide theoretical guarantees for the success of our methods in\nclustering distributions. Experiments on both synthetic and real data show that\nour methods outperform existing baselines.",
      "tldr_zh": "本文研究了离散分布（discrete distributions）的聚类问题，指出传统Wasserstein barycenter方法存在假设不成立（如聚类需由重心表示）和计算成本高的问题，导致其不适用于大规模数据集。作者提出了一种结合spectral clustering和分布亲和度测度（如maximum mean discrepancy和Wasserstein distance）的聚类方法，以提高准确性和效率。为提升可扩展性，他们引入linear optimal transport来高效构建亲和矩阵，并提供了理论保证。实验在合成和真实数据上表明，该方法优于现有基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13913v2",
      "published_date": "2024-01-25 03:17:03 UTC",
      "updated_date": "2024-08-16 06:00:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:31:14.726859"
    },
    {
      "arxiv_id": "2401.15098v2",
      "title": "Hierarchical Continual Reinforcement Learning via Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Chaofan Pan",
        "Xin Yang",
        "Hao Wang",
        "Wei Wei",
        "Tianrui Li"
      ],
      "abstract": "The ability to learn continuously in dynamic environments is a crucial\nrequirement for reinforcement learning (RL) agents applying in the real world.\nDespite the progress in continual reinforcement learning (CRL), existing\nmethods often suffer from insufficient knowledge transfer, particularly when\nthe tasks are diverse. To address this challenge, we propose a new framework,\nHierarchical Continual reinforcement learning via large language model\n(Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core\norchestrates a twolayer structure: high-level policy formulation by a large\nlanguage model (LLM), which represents agenerates a sequence of goals, and\nlow-level policy learning that closely aligns with goal-oriented RL practices,\nproducing the agent's actions in response to the goals set forth. The framework\nemploys feedback to iteratively adjust and verify highlevel policies, storing\nthem along with low-level policies within a skill library. When encountering a\nnew task, Hi-Core retrieves relevant experience from this library to help to\nlearning. Through experiments on Minigrid, Hi-Core has demonstrated its\neffectiveness in handling diverse CRL tasks, which outperforms popular\nbaselines.",
      "tldr_zh": "本研究针对持续强化学习（CRL）在多样化任务中知识转移不足的问题，提出了一种分层框架Hierarchical Continual Reinforcement Learning via Large Language Model（Hi-Core）。Hi-Core采用双层结构，由大型语言模型（LLM）生成高层政策序列作为目标，并通过低层政策执行代理动作，同时利用反馈机制调整政策并存储于技能库中。新任务时，框架从技能库检索相关经验以辅助学习。在Minigrid环境下的实验表明，Hi-Core在处理多样CRL任务时，表现优于流行基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.15098v2",
      "published_date": "2024-01-25 03:06:51 UTC",
      "updated_date": "2024-02-01 11:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:31:25.643629"
    },
    {
      "arxiv_id": "2401.13904v1",
      "title": "Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Lou",
        "Chengchun Liu",
        "Yuntian Chen",
        "Fanyang Mo"
      ],
      "abstract": "Thin-layer chromatography (TLC) is a crucial technique in molecular polarity\nanalysis. Despite its importance, the interpretability of predictive models for\nTLC, especially those driven by artificial intelligence, remains a challenge.\nCurrent approaches, utilizing either high-dimensional molecular fingerprints or\ndomain-knowledge-driven feature engineering, often face a dilemma between\nexpressiveness and interpretability. To bridge this gap, we introduce\nUnsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical\nneural networks and symbolic regression. UHiSR automatically distills\nchemical-intuitive polarity indices, and discovers interpretable equations that\nlink molecular structure to chromatographic behavior.",
      "tldr_zh": "本研究针对薄层色谱 (TLC) 在分子极性分析中的关键作用，指出现有预测模型在使用高维分子指纹或基于领域知识的特征工程时，难以平衡表现力和可解释性。研究引入了无监督层次符号回归 (Unsupervised Hierarchical Symbolic Regression, UHiSR)，该方法结合层次神经网络和符号回归，自动提炼化学直观的极性指标。UHiSR 能够发现可解释的方程，将分子结构与色谱行为联系起来，从而提升机器在化学推理中的可解释性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.13904v1",
      "published_date": "2024-01-25 02:48:44 UTC",
      "updated_date": "2024-01-25 02:48:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:31:37.798980"
    },
    {
      "arxiv_id": "2401.13883v3",
      "title": "Domain-Independent Dynamic Programming",
      "title_zh": "领域无关动态规划",
      "authors": [
        "Ryo Kuroiwa",
        "J. Christopher Beck"
      ],
      "abstract": "For combinatorial optimization problems, model-based paradigms such as\nmixed-integer programming (MIP) and constraint programming (CP) aim to decouple\nmodeling and solving a problem: the `holy grail' of declarative problem\nsolving. We propose domain-independent dynamic programming (DIDP), a novel\nmodel-based paradigm based on dynamic programming (DP). While DP is not new, it\nhas typically been implemented as a problem-specific method. We introduce\nDynamic Programming Description Language (DyPDL), a formalism to define DP\nmodels based on a state transition system, inspired by artificial intelligence\n(AI) planning. we show that heuristic search algorithms can be used to solve\nDyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP\nsolvers with commercial MIP and CP solvers (solving MIP and CP models,\nrespectively) on common benchmark instances of eleven combinatorial\noptimization problem classes. We show that DIDP outperforms MIP in nine problem\nclasses, CP also in nine problem classes, and both MIP and CP in seven. DIDP\nalso achieves superior performance to existing state-based solvers including\ndomain-independent AI planners.",
      "tldr_zh": "本研究提出了一种新的模型-based范式：Domain-Independent Dynamic Programming (DIDP)，基于动态规划 (DP)，旨在像mixed-integer programming (MIP)和constraint programming (CP)一样，实现建模和求解的解耦。论文引入Dynamic Programming Description Language (DyPDL)，一个受人工智能 (AI) 规划启发的形式化语言，用于定义基于状态转移系统的DP模型，并开发了七个DIDP求解器，利用启发式搜索算法进行求解。在实验中，DIDP在11个组合优化问题类别的基准实例上表现突出，在9个问题类中优于MIP、在9个中优于CP、在7个中优于两者，并超越现有的基于状态的求解器，包括domain-independent AI planners。",
      "categories": [
        "cs.AI",
        "F.2.2; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "Manuscript submitted to Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2401.13883v3",
      "published_date": "2024-01-25 01:48:09 UTC",
      "updated_date": "2025-03-18 08:19:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:31:51.451934"
    },
    {
      "arxiv_id": "2402.01705v2",
      "title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation",
      "title_zh": "翻译失败",
      "authors": [
        "Jennifer Chien",
        "David Danks"
      ],
      "abstract": "Algorithmic harms are commonly categorized as either allocative or\nrepresentational. This study specifically addresses the latter, focusing on an\nexamination of current definitions of representational harms to discern what is\nincluded and what is not. This analysis motivates our expansion beyond\nbehavioral definitions to encompass harms to cognitive and affective states.\nThe paper outlines high-level requirements for measurement: identifying the\nnecessary expertise to implement this approach and illustrating it through a\ncase study. Our work highlights the unique vulnerabilities of large language\nmodels to perpetrating representational harms, particularly when these harms go\nunmeasured and unmitigated. The work concludes by presenting proposed\nmitigations and delineating when to employ them. The overarching aim of this\nresearch is to establish a framework for broadening the definition of\nrepresentational harms and to translate insights from fairness research into\npractical measurement and mitigation praxis.",
      "tldr_zh": "该论文分析了算法伤害中的代表性伤害（representational harms），指出现有定义主要局限于行为层面，并提出扩展至认知和情感状态，以更全面地涵盖这些伤害。研究概述了测量要求，包括所需的专业知识，并通过一个案例研究进行说明，突出了大型语言模型（large language models）在未被测量和缓解时易于造成此类伤害的脆弱性。最终，论文提出缓解措施和应用指南，旨在建立一个更广泛的框架，将公平性研究转化为实际的测量和缓解实践。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "23 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.01705v2",
      "published_date": "2024-01-25 00:54:10 UTC",
      "updated_date": "2024-05-06 21:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T00:32:00.610949"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 95,
  "processed_papers_count": 95,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T00:32:20.180719"
}
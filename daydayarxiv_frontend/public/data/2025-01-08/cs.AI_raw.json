[
  {
    "arxiv_id": "2501.04882v1",
    "title": "Reach Measurement, Optimization and Frequency Capping In Targeted Online Advertising Under k-Anonymity",
    "authors": [
      "Yuan Gao",
      "Mu Qiao"
    ],
    "abstract": "The growth in the use of online advertising to foster brand awareness over\nrecent years is largely attributable to the ubiquity of social media. One\npivotal technology contributing to the success of online brand advertising is\nfrequency capping, a mechanism that enables marketers to control the number of\ntimes an ad is shown to a specific user. However, the very foundation of this\ntechnology is being scrutinized as the industry gravitates towards advertising\nsolutions that prioritize user privacy. This paper delves into the issue of\nreach measurement and optimization within the context of $k$-anonymity, a\nprivacy-preserving model gaining traction across major online advertising\nplatforms. We outline how to report reach within this new privacy landscape and\ndemonstrate how probabilistic discounting, a probabilistic adaptation of\ntraditional frequency capping, can be employed to optimize campaign\nperformance. Experiments are performed to assess the trade-off between user\nprivacy and the efficacy of online brand advertising. Notably, we discern a\nsignificant dip in performance as long as privacy is introduced, yet this comes\nwith a limited additional cost for advertising platforms to offer their users\nmore privacy.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04882v1",
    "published_date": "2025-01-08 23:38:19 UTC",
    "updated_date": "2025-01-08 23:38:19 UTC"
  },
  {
    "arxiv_id": "2501.04877v1",
    "title": "Real-Time Textless Dialogue Generation",
    "authors": [
      "Long Mai",
      "Julie Carson-Berndsen"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have led to significant\nprogress in text-based dialogue systems. These systems can now generate\nhigh-quality responses that are accurate and coherent across a wide range of\ntopics and tasks. However, spoken dialogue systems still lag behind in terms of\nnaturalness. They tend to produce robotic interactions, with issues such as\nslow response times, overly generic or cautious replies, and a lack of natural\nrhythm and fluid turn-taking. This shortcoming is largely due to the\nover-reliance on the traditional cascaded design, which involve separate,\nsequential components, as well as the use of text as an intermediate\nrepresentation. This paper propose a real-time, textless spoken dialogue\ngeneration model (RTTL-DG) that aims to overcome these challenges. Our system\nenables fluid turn-taking and generates responses with minimal delay by\nprocessing streaming spoken conversation directly. Additionally, our model\nincorporates backchannels, filters, laughter, and other paralinguistic signals,\nwhich are often absent in cascaded dialogue systems, to create more natural and\nhuman-like interactions. The implementations and generated samples are\navailable in our repository: https://github.com/mailong25/rts2s-dg",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04877v1",
    "published_date": "2025-01-08 23:21:43 UTC",
    "updated_date": "2025-01-08 23:21:43 UTC"
  },
  {
    "arxiv_id": "2501.04873v2",
    "title": "Back Home: A Machine Learning Approach to Seashell Classification and Ecosystem Restoration",
    "authors": [
      "Alexander Valverde",
      "Luis Solano"
    ],
    "abstract": "In Costa Rica, an average of 5 tons of seashells are extracted from\necosystems annually. Confiscated seashells, cannot be returned to their\necosystems due to the lack of origin recognition. To address this issue, we\ndeveloped a convolutional neural network (CNN) specifically for seashell\nidentification. We built a dataset from scratch, consisting of approximately\n19000 images from the Pacific and Caribbean coasts. Using this dataset, the\nmodel achieved a classification accuracy exceeding 85%. The model has been\nintegrated into a user-friendly application, which has classified over 36,000\nseashells to date, delivering real-time results within 3 seconds per image. To\nfurther enhance the system's accuracy, an anomaly detection mechanism was\nincorporated to filter out irrelevant or anomalous inputs, ensuring only valid\nseashell images are processed.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04873v2",
    "published_date": "2025-01-08 23:07:10 UTC",
    "updated_date": "2025-03-06 17:35:19 UTC"
  },
  {
    "arxiv_id": "2501.04848v1",
    "title": "Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware",
    "authors": [
      "Brandon J Walton",
      "Mst Eshita Khatun",
      "James M Ghawaly",
      "Aisha Ali-Gombe"
    ],
    "abstract": "Malware analysis is a complex process of examining and evaluating malicious\nsoftware's functionality, origin, and potential impact. This arduous process\ntypically involves dissecting the software to understand its components,\ninfection vector, propagation mechanism, and payload. Over the years, deep\nreverse engineering of malware has become increasingly tedious, mainly due to\nmodern malicious codebases' fast evolution and sophistication. Essentially,\nanalysts are tasked with identifying the elusive needle in the haystack within\nthe complexities of zero-day malware, all while under tight time constraints.\nThus, in this paper, we explore leveraging Large Language Models (LLMs) for\nsemantic malware analysis to expedite the analysis of known and novel samples.\nBuilt on GPT-4o-mini model, \\msp is designed to augment malware analysis for\nAndroid through a hierarchical-tiered summarization chain and strategic prompt\nengineering. Additionally, \\msp performs malware categorization, distinguishing\npotential malware from benign applications, thereby saving time during the\nmalware reverse engineering process. Despite not being fine-tuned for Android\nmalware analysis, we demonstrate that through optimized and advanced prompt\nengineering \\msp can achieve up to 77% classification accuracy while providing\nhighly robust summaries at functional, class, and package levels. In addition,\nleveraging the backward tracing of the summaries from package to function\nlevels allowed us to pinpoint the precise code snippets responsible for\nmalicious behavior.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04848v1",
    "published_date": "2025-01-08 21:22:45 UTC",
    "updated_date": "2025-01-08 21:22:45 UTC"
  },
  {
    "arxiv_id": "2501.04844v1",
    "title": "Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction",
    "authors": [
      "Jihwan Lee",
      "Tiantian Feng",
      "Aditya Kommineni",
      "Sudarsana Reddy Kadiri",
      "Shrikanth Narayanan"
    ],
    "abstract": "Brain-computer interfaces (BCI) offer numerous human-centered application\npossibilities, particularly affecting people with neurological disorders. Text\nor speech decoding from brain activities is a relevant domain that could\naugment the quality of life for people with impaired speech perception. We\npropose a novel approach to enhance listened speech decoding from\nelectroencephalography (EEG) signals by utilizing an auxiliary phoneme\npredictor that simultaneously decodes textual phoneme sequences. The proposed\nmodel architecture consists of three main parts: EEG module, speech module, and\nphoneme predictor. The EEG module learns to properly represent EEG signals into\nEEG embeddings. The speech module generates speech waveforms from the EEG\nembeddings. The phoneme predictor outputs the decoded phoneme sequences in text\nmodality. Our proposed approach allows users to obtain decoded listened speech\nfrom EEG signals in both modalities (speech waveforms and textual phoneme\nsequences) simultaneously, eliminating the need for a concatenated sequential\npipeline for each modality. The proposed approach also outperforms previous\nmethods in both modalities. The source code and speech samples are publicly\navailable.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04844v1",
    "published_date": "2025-01-08 21:11:35 UTC",
    "updated_date": "2025-01-08 21:11:35 UTC"
  },
  {
    "arxiv_id": "2501.06250v1",
    "title": "Generative AI for Cel-Animation: A Survey",
    "authors": [
      "Yunlong Tang",
      "Junjia Guo",
      "Pinxin Liu",
      "Zhiyuan Wang",
      "Hang Hua",
      "Jia-Xing Zhong",
      "Yunzhong Xiao",
      "Chao Huang",
      "Luchuan Song",
      "Susan Liang",
      "Yizhi Song",
      "Liu He",
      "Jing Bi",
      "Mingqian Feng",
      "Xinyang Li",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "abstract": "Traditional Celluloid (Cel) Animation production pipeline encompasses\nmultiple essential steps, including storyboarding, layout design, keyframe\nanimation, inbetweening, and colorization, which demand substantial manual\neffort, technical expertise, and significant time investment. These challenges\nhave historically impeded the efficiency and scalability of Cel-Animation\nproduction. The rise of generative artificial intelligence (GenAI),\nencompassing large language models, multimodal models, and diffusion models,\noffers innovative solutions by automating tasks such as inbetween frame\ngeneration, colorization, and storyboard creation. This survey explores how\nGenAI integration is revolutionizing traditional animation workflows by\nlowering technical barriers, broadening accessibility for a wider range of\ncreators through tools like AniDoc, ToonCrafter, and AniSora, and enabling\nartists to focus more on creative expression and artistic innovation. Despite\nits potential, issues such as maintaining visual consistency, ensuring\nstylistic coherence, and addressing ethical considerations continue to pose\nchallenges. Furthermore, this paper discusses future directions and explores\npotential advancements in AI-assisted animation. For further exploration and\nresources, please visit our GitHub repository:\nhttps://github.com/yunlong10/Awesome-AI4Animation",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.06250v1",
    "published_date": "2025-01-08 20:57:39 UTC",
    "updated_date": "2025-01-08 20:57:39 UTC"
  },
  {
    "arxiv_id": "2501.04835v1",
    "title": "Do Code LLMs Understand Design Patterns?",
    "authors": [
      "Zhenyu Pan",
      "Xuefeng Song",
      "Yunkun Wang",
      "Rongyu Cao",
      "Binhua Li",
      "Yongbin Li",
      "Han Liu"
    ],
    "abstract": "Code Large Language Models (LLMs) demonstrate great versatility in adapting\nto various downstream tasks, including code generation and completion, as well\nas bug detection and fixing. However, Code LLMs often fail to capture existing\ncoding standards, leading to the generation of code that conflicts with the\nrequired design patterns for a given project. As a result, developers must\npost-process to adapt the generated code to the project's design norms. In this\nwork, we empirically investigate the biases of Code LLMs in software\ndevelopment. Through carefully designed experiments, we assess the models'\nunderstanding of design patterns across recognition, comprehension, and\ngeneration. Our findings reveal that biases in Code LLMs significantly affect\nthe reliability of downstream tasks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "accpeted by llm4code workshop in ICSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04835v1",
    "published_date": "2025-01-08 20:39:45 UTC",
    "updated_date": "2025-01-08 20:39:45 UTC"
  },
  {
    "arxiv_id": "2501.04832v1",
    "title": "ActPC-Geom: Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms",
    "authors": [
      "Ben Goertzel"
    ],
    "abstract": "This paper introduces ActPC-Geom, an approach to accelerate Active Predictive\nCoding (ActPC) in neural networks by integrating information geometry,\nspecifically using Wasserstein-metric-based methods for measure-dependent\ngradient flows. We propose replacing KL-divergence in ActPC's predictive error\nassessment with the Wasserstein metric, suggesting this may enhance network\nrobustness.\n  To make this computationally feasible, we present strategies including: (1)\nneural approximators for inverse measure-dependent Laplacians, (2) approximate\nkernel PCA embeddings for low-rank approximations feeding into these\napproximators, and (3) compositional hypervector embeddings derived from kPCA\noutputs, with algebra optimized for fuzzy FCA lattices learned through neural\narchitectures analyzing network states.\n  This results in an ActPC architecture capable of real-time online learning\nand integrating continuous (e.g., transformer-like or Hopfield-net-like) and\ndiscrete symbolic ActPC networks, including frameworks like OpenCog Hyperon or\nActPC-Chem for algorithmic chemistry evolution. Shared probabilistic,\nconcept-lattice, and hypervector models enable symbolic-subsymbolic\nintegration.\n  Key features include (1) compositional reasoning via hypervector embeddings\nin transformer-like architectures for tasks like commonsense reasoning, and (2)\nHopfield-net dynamics enabling associative long-term memory and\nattractor-driven cognitive features.\n  We outline how ActPC-Geom combines few-shot learning with online weight\nupdates, enabling deliberative thinking and seamless symbolic-subsymbolic\nreasoning. Ideas from Galois connections are explored for efficient hybrid\nActPC/ActPC-Chem processing. Finally, we propose a specialized HPC design\noptimized for real-time focused attention and deliberative reasoning tailored\nto ActPC-Geom's demands.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04832v1",
    "published_date": "2025-01-08 20:38:02 UTC",
    "updated_date": "2025-01-08 20:38:02 UTC"
  },
  {
    "arxiv_id": "2501.04826v1",
    "title": "Intelligent Gradient Boosting Algorithms for Estimating Strength of Modified Subgrade Soil",
    "authors": [
      "Ismail B. Mustapha",
      "Muyideen Abdulkareem",
      "Shafaatunnur Hasan",
      "Abideen Ganiyu",
      "Hatem Nabus",
      "Jin Chai Lee"
    ],
    "abstract": "The performance of pavement under loading depends on the strength of the\nsubgrade. However, experimental estimation of properties of pavement strengths\nsuch as California bearing ratio (CBR), unconfined compressive strength (UCS)\nand resistance value (R) are often tedious, time-consuming and costly, thereby\ninspiring a growing interest in machine learning based tools which are simple,\ncheap and fast alternatives. Thus, the potential application of two boosting\ntechniques; categorical boosting (CatBoost) and extreme gradient boosting\n(XGBoost) and support vector regression (SVR), is similarly explored in this\nstudy for estimation of properties of subgrade soil modified with hydrated lime\nactivated rice husk ash (HARSH). Using 121 experimental data samples of varying\nproportions of HARSH, plastic limit, liquid limit, plasticity index, clay\nactivity, optimum moisture content, and maximum dry density as input for CBR,\nUCS and R estimation, four evaluation metrics namely coefficient of\ndetermination (R2), root mean squared error (RMSE), mean absolute error (MAE)\nand mean absolute percentage error (MAPE) are used to evaluate the models'\nperformance. The results indicate that XGBoost outperformed CatBoost and SVR in\nestimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in\nestimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in\nestimating the CBR and R with R2 of 0.9997 respectively. On the other hand,\nCatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature\nsensitivity analysis shows that the three machine learning techniques are\nunanimous that increasing HARSH proportion lead to values of the estimated\nproperties respectively. A comparison with previous results also shows\nsuperiority of XGBoost in estimating subgrade properties.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.04826v1",
    "published_date": "2025-01-08 20:26:13 UTC",
    "updated_date": "2025-01-08 20:26:13 UTC"
  },
  {
    "arxiv_id": "2501.04819v1",
    "title": "Planing It by Ear: Convolutional Neural Networks for Acoustic Anomaly Detection in Industrial Wood Planers",
    "authors": [
      "Anthony Deschênes",
      "Rémi Georges",
      "Cem Subakan",
      "Bruna Ugulino",
      "Antoine Henry",
      "Michael Morin"
    ],
    "abstract": "In recent years, the wood product industry has been facing a skilled labor\nshortage. The result is more frequent sudden failures, resulting in additional\ncosts for these companies already operating in a very competitive market.\nMoreover, sawmills are challenging environments for machinery and sensors.\nGiven that experienced machine operators may be able to diagnose defects or\nmalfunctions, one possible way of assisting novice operators is through\nacoustic monitoring. As a step towards the automation of wood-processing\nequipment and decision support systems for machine operators, in this paper, we\nexplore using a deep convolutional autoencoder for acoustic anomaly detection\nof wood planers on a new real-life dataset. Specifically, our convolutional\nautoencoder with skip connections (Skip-CAE) and our Skip-CAE transformer\noutperform the DCASE autoencoder baseline, one-class SVM, isolation forest and\na published convolutional autoencoder architecture, respectively obtaining an\narea under the ROC curve of 0.846 and 0.875 on a dataset of real-factory planer\nsounds. Moreover, we show that adding skip connections and attention mechanism\nunder the form of a transformer encoder-decoder helps to further improve the\nanomaly detection capabilities.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04819v1",
    "published_date": "2025-01-08 20:17:18 UTC",
    "updated_date": "2025-01-08 20:17:18 UTC"
  },
  {
    "arxiv_id": "2501.04817v1",
    "title": "Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning",
    "authors": [
      "Ziyuan Bao",
      "Eiman Kanjo",
      "Soumya Banerjee",
      "Hasib-Al Rashid",
      "Tinoosh Mohsenin"
    ],
    "abstract": "With the growing computational capabilities of microcontroller units (MCUs),\nedge devices can now support machine learning models. However, deploying\ndecentralised federated learning (DFL) on such devices presents key challenges,\nincluding intermittent connectivity, limited communication range, and dynamic\nnetwork topologies. This paper proposes a novel framework, bilayer Gossip\nDecentralised Parallel Stochastic Gradient Descent (GD PSGD), designed to\naddress these issues in resource-constrained environments. The framework\nincorporates a hierarchical communication structure using Distributed Kmeans\n(DKmeans) clustering for geographic grouping and a gossip protocol for\nefficient model aggregation across two layers: intra-cluster and inter-cluster.\nWe evaluate the framework's performance against the Centralised Federated\nLearning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under\nIID and Non-IID conditions. Results demonstrate that the proposed method\nachieves comparable accuracy to CFL on IID datasets, requiring only 1.8\nadditional rounds for convergence. On Non-IID datasets, the accuracy loss\nremains under 8\\% for moderate data imbalance. These findings highlight the\nframework's potential to support scalable and privacy-preserving learning on\nedge devices with minimal performance trade-offs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04817v1",
    "published_date": "2025-01-08 20:14:07 UTC",
    "updated_date": "2025-01-08 20:14:07 UTC"
  },
  {
    "arxiv_id": "2501.06248v2",
    "title": "Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models",
    "authors": [
      "Roberto-Rafael Maura-Rivero",
      "Chirag Nagpal",
      "Roma Patel",
      "Francesco Visin"
    ],
    "abstract": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06248v2",
    "published_date": "2025-01-08 19:03:17 UTC",
    "updated_date": "2025-02-25 18:04:50 UTC"
  },
  {
    "arxiv_id": "2501.04700v1",
    "title": "Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria Shaping Modern Artificial Neural Network Architectures",
    "authors": [
      "Ziyuan Huang",
      "Mark Newman",
      "Maria Vaida",
      "Srikar Bellur",
      "Roozbeh Sadeghian",
      "Andrew Siu",
      "Hui Wang",
      "Kevin Huggins"
    ],
    "abstract": "This study examined the viability of enhancing the prediction accuracy of\nartificial neural networks (ANNs) in image classification tasks by developing\nANNs with evolution patterns similar to those of biological neural networks.\nResNet is a widely used family of neural networks with both deep and wide\nvariants; therefore, it was selected as the base model for our investigation.\nThe aim of this study is to improve the image classification performance of\nANNs via a novel approach inspired by the biological nervous system\narchitecture of planarians, which comprises a brain and two nerve cords. We\nbelieve that the unique neural architecture of planarians offers valuable\ninsights into the performance enhancement of ANNs. The proposed planarian\nneural architecture-based neural network was evaluated on the CIFAR-10 and\nCIFAR-100 datasets. Our results indicate that the proposed method exhibits\nhigher prediction accuracy than the baseline neural network models in image\nclassification tasks. These findings demonstrate the significant potential of\nbiologically inspired neural network architectures in improving the performance\nof ANNs in a wide range of applications.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T07"
    ],
    "primary_category": "cs.NE",
    "comment": "11 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04700v1",
    "published_date": "2025-01-08 18:59:36 UTC",
    "updated_date": "2025-01-08 18:59:36 UTC"
  },
  {
    "arxiv_id": "2501.04697v2",
    "title": "Grokking at the Edge of Numerical Stability",
    "authors": [
      "Lucas Prieto",
      "Melih Barsbey",
      "Pedro A. M. Mediano",
      "Tolga Birdal"
    ],
    "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04697v2",
    "published_date": "2025-01-08 18:58:48 UTC",
    "updated_date": "2025-05-19 11:02:53 UTC"
  },
  {
    "arxiv_id": "2501.04694v1",
    "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
    "authors": [
      "Yaoxiang Wang",
      "Haoling Li",
      "Xin Zhang",
      "Jie Wu",
      "Xiao Liu",
      "Wenxiang Hu",
      "Zhongxin Guo",
      "Yangyu Huang",
      "Ying Xin",
      "Yujiu Yang",
      "Jinsong Su",
      "Qi Chen",
      "Scarlett Li"
    ],
    "abstract": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "40 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04694v1",
    "published_date": "2025-01-08 18:58:15 UTC",
    "updated_date": "2025-01-08 18:58:15 UTC"
  },
  {
    "arxiv_id": "2501.04693v3",
    "title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
    "authors": [
      "Joshua Jones",
      "Oier Mees",
      "Carmelo Sferrazza",
      "Kyle Stachowicz",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "abstract": "Interacting with the world is a multi-sensory experience: achieving effective\ngeneral-purpose interaction requires making use of all available modalities --\nincluding vision, touch, and audio -- to fill in gaps from partial observation.\nFor example, when vision is occluded reaching into a bag, a robot should rely\non its senses of touch and sound. However, state-of-the-art generalist robot\npolicies are typically trained on large datasets to predict robot actions\nsolely from visual and proprioceptive observations. In this work, we propose\nFuSe, a novel approach that enables finetuning visuomotor generalist policies\non heterogeneous sensor modalities for which large datasets are not readily\navailable by leveraging natural language as a common cross-modal grounding. We\ncombine a multimodal contrastive loss with a sensory-grounded language\ngeneration loss to encode high-level semantics. In the context of robot\nmanipulation, we show that FuSe enables performing challenging tasks that\nrequire reasoning jointly over modalities such as vision, touch, and sound in a\nzero-shot setting, such as multimodal prompting, compositional cross-modal\nprompting, and descriptions of objects it interacts with. We show that the same\nrecipe is applicable to widely different generalist policies, including both\ndiffusion-based generalist policies and large vision-language-action (VLA)\nmodels. Extensive experiments in the real world show that FuSeis able to\nincrease success rates by over 20% compared to all considered baselines.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04693v3",
    "published_date": "2025-01-08 18:57:33 UTC",
    "updated_date": "2025-01-14 22:28:39 UTC"
  },
  {
    "arxiv_id": "2501.04686v4",
    "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics",
    "authors": [
      "Ruilin Luo",
      "Zhuofan Zheng",
      "Yifan Wang",
      "Yiyao Yu",
      "Xinzhe Ni",
      "Zicheng Lin",
      "Jin Zeng",
      "Yujiu Yang"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io",
    "pdf_url": "http://arxiv.org/pdf/2501.04686v4",
    "published_date": "2025-01-08 18:49:41 UTC",
    "updated_date": "2025-02-24 07:32:58 UTC"
  },
  {
    "arxiv_id": "2501.04682v1",
    "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought",
    "authors": [
      "Violet Xiang",
      "Charlie Snell",
      "Kanishk Gandhi",
      "Alon Albalak",
      "Anikait Singh",
      "Chase Blagden",
      "Duy Phung",
      "Rafael Rafailov",
      "Nathan Lile",
      "Dakota Mahan",
      "Louis Castricato",
      "Jan-Philipp Franken",
      "Nick Haber",
      "Chelsea Finn"
    ],
    "abstract": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04682v1",
    "published_date": "2025-01-08 18:42:48 UTC",
    "updated_date": "2025-01-08 18:42:48 UTC"
  },
  {
    "arxiv_id": "2501.04765v2",
    "title": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training",
    "authors": [
      "Felix Krause",
      "Timy Phan",
      "Ming Gui",
      "Stefan Andreas Baumann",
      "Vincent Tao Hu",
      "Björn Ommer"
    ],
    "abstract": "Diffusion models have emerged as the mainstream approach for visual\ngeneration. However, these models typically suffer from sample inefficiency and\nhigh training costs. Consequently, methods for efficient finetuning, inference\nand personalization were quickly adopted by the community. However, training\nthese models in the first place remains very costly. While several recent\napproaches - including masking, distillation, and architectural modifications -\nhave been proposed to improve training efficiency, each of these methods comes\nwith a tradeoff: they achieve enhanced performance at the expense of increased\ncomputational cost or vice versa. In contrast, this work aims to improve\ntraining efficiency as well as generative performance at the same time through\nroutes that act as a transport mechanism for randomly selected tokens from\nearly layers to deeper layers of the model. Our method is not limited to the\ncommon transformer-based model - it can also be applied to state-space models\nand achieves this without architectural modifications or additional parameters.\nFinally, we show that TREAD reduces computational cost and simultaneously\nboosts model performance on the standard ImageNet-256 benchmark in\nclass-conditional synthesis. Both of these benefits multiply to a convergence\nspeedup of 14x at 400K training iterations compared to DiT and 37x compared to\nthe best benchmark performance of DiT at 7M training iterations. Furthermore,\nwe achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided\nsetting, which improves upon the DiT, without architectural changes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04765v2",
    "published_date": "2025-01-08 18:38:25 UTC",
    "updated_date": "2025-03-27 14:42:53 UTC"
  },
  {
    "arxiv_id": "2501.04675v1",
    "title": "Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations",
    "authors": [
      "Archita Srivastava",
      "Abhas Kumar",
      "Rajesh Kumar",
      "Prabhakar Srinivasan"
    ],
    "abstract": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04675v1",
    "published_date": "2025-01-08 18:33:17 UTC",
    "updated_date": "2025-01-08 18:33:17 UTC"
  },
  {
    "arxiv_id": "2501.04671v2",
    "title": "Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios",
    "authors": [
      "Charles Corbière",
      "Simon Roburin",
      "Syrielle Montariol",
      "Antoine Bosselut",
      "Alexandre Alahi"
    ],
    "abstract": "While chain-of-thought (CoT) prompting improves reasoning in large language\nmodels, its effectiveness in vision-language models (VLMs) remains limited due\nto over-reliance on textual cues and memorized knowledge. To investigate the\nvisual reasoning capabilities of VLMs in complex real-world scenarios, we\nintroduce DrivingVQA, a visual question answering dataset derived from driving\ntheory exams, which contains 3,931 multiple-choice problems with expert-written\nexplanations and grounded entities relevant to the reasoning process.\nLeveraging this dataset, we propose RIV-CoT, a Retrieval-Based Interleaved\nVisual Chain-of-Thought method that enables VLMs to reason using visual crops\ncorresponding to these relevant entities. Our experiments demonstrate that\nRIV-CoT improves answer accuracy by 3.1% and reasoning accuracy by 4.6% over\nvanilla CoT prompting. Furthermore, we demonstrate that our method effectively\nscales to the larger A-OKVQA reasoning dataset by leveraging automatically\ngenerated pseudo-labels, outperforming CoT prompting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://vita-epfl.github.io/DrivingVQA",
    "pdf_url": "http://arxiv.org/pdf/2501.04671v2",
    "published_date": "2025-01-08 18:31:16 UTC",
    "updated_date": "2025-04-08 17:09:59 UTC"
  },
  {
    "arxiv_id": "2501.04661v1",
    "title": "Assessing Language Comprehension in Large Language Models Using Construction Grammar",
    "authors": [
      "Wesley Scivetti",
      "Melissa Torgbi",
      "Austin Blodgett",
      "Mollie Shichman",
      "Taylor Hudson",
      "Claire Bonial",
      "Harish Tayyar Madabushi"
    ],
    "abstract": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04661v1",
    "published_date": "2025-01-08 18:15:10 UTC",
    "updated_date": "2025-01-08 18:15:10 UTC"
  },
  {
    "arxiv_id": "2501.06247v1",
    "title": "A Survey on Algorithmic Developments in Optimal Transport Problem with Applications",
    "authors": [
      "Sina Moradi"
    ],
    "abstract": "Optimal Transport (OT) has established itself as a robust framework for\nquantifying differences between distributions, with applications that span\nfields such as machine learning, data science, and computer vision. This paper\noffers a detailed examination of the OT problem, beginning with its theoretical\nfoundations, including the classical formulations of Monge and Kantorovich and\ntheir extensions to modern computational techniques. It explores cutting-edge\nalgorithms, including Sinkhorn iterations, primal-dual strategies, and\nreduction-based approaches, emphasizing their efficiency and scalability in\naddressing high-dimensional problems. The paper also highlights emerging\ntrends, such as integrating OT into machine learning frameworks, the\ndevelopment of novel problem variants, and ongoing theoretical advancements.\nApplications of OT are presented across a range of domains, with particular\nattention to its innovative application in time series data analysis via\nOptimal Transport Warping (OTW), a robust alternative to methods like Dynamic\nTime Warping. Despite the significant progress made, challenges related to\nscalability, robustness, and ethical considerations remain, necessitating\nfurther research. The paper underscores OT's potential to bridge theoretical\ndepth and practical utility, fostering impactful advancements across diverse\ndisciplines.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06247v1",
    "published_date": "2025-01-08 18:06:30 UTC",
    "updated_date": "2025-01-08 18:06:30 UTC"
  },
  {
    "arxiv_id": "2501.04635v2",
    "title": "Knowledge Retrieval Based on Generative AI",
    "authors": [
      "Te-Lun Yang",
      "Jyi-Shane Liu",
      "Yuen-Hsien Tseng",
      "Jyh-Shing Roger Jang"
    ],
    "abstract": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 13 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2501.04635v2",
    "published_date": "2025-01-08 17:29:46 UTC",
    "updated_date": "2025-01-16 09:30:38 UTC"
  },
  {
    "arxiv_id": "2501.06246v1",
    "title": "A partition cover approach to tokenization",
    "authors": [
      "Jia Peng Lim",
      "Davin Choo",
      "Hady W. Lauw"
    ],
    "abstract": "Tokenization is the process of encoding strings into tokens from a fixed\nvocabulary of size $k$ and is widely utilized in Natural Language Processing\napplications. The leading tokenization algorithm today is Byte Pair Encoding\n(BPE), which formulates the tokenization problem as a compression problem and\ntackles it by performing sequences of merges. In this work, we formulate\ntokenization as an optimization objective, show that it is NP-hard via a simple\nreduction from vertex cover, and propose a polynomial-time greedy algorithm\nGreedTok. Our formulation naturally relaxes to the well-studied weighted\nmaximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm\nGreedWMC. Through empirical evaluations on real-world corpora, we show that\nGreedTok outperforms BPE, while achieving a comparable objective score as\nGreedWMC (which could have achieved a higher score due to relaxation).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06246v1",
    "published_date": "2025-01-08 17:07:07 UTC",
    "updated_date": "2025-01-08 17:07:07 UTC"
  },
  {
    "arxiv_id": "2501.06244v1",
    "title": "Microservice Deployment in Space Computing Power Networks via Robust Reinforcement Learning",
    "authors": [
      "Zhiyong Yu",
      "Yuning Jiang",
      "Xin Liu",
      "Yuanming Shi",
      "Chunxiao Jiang",
      "Linling Kuang"
    ],
    "abstract": "With the growing demand for Earth observation, it is important to provide\nreliable real-time remote sensing inference services to meet the low-latency\nrequirements. The Space Computing Power Network (Space-CPN) offers a promising\nsolution by providing onboard computing and extensive coverage capabilities for\nreal-time inference. This paper presents a remote sensing artificial\nintelligence applications deployment framework designed for Low Earth Orbit\nsatellite constellations to achieve real-time inference performance. The\nframework employs the microservice architecture, decomposing monolithic\ninference tasks into reusable, independent modules to address high latency and\nresource heterogeneity. This distributed approach enables optimized\nmicroservice deployment, minimizing resource utilization while meeting quality\nof service and functional requirements. We introduce Robust Optimization to the\ndeployment problem to address data uncertainty. Additionally, we model the\nRobust Optimization problem as a Partially Observable Markov Decision Process\nand propose a robust reinforcement learning algorithm to handle the\nsemi-infinite Quality of Service constraints. Our approach yields sub-optimal\nsolutions that minimize accuracy loss while maintaining acceptable\ncomputational costs. Simulation results demonstrate the effectiveness of our\nframework.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.06244v1",
    "published_date": "2025-01-08 16:55:04 UTC",
    "updated_date": "2025-01-08 16:55:04 UTC"
  },
  {
    "arxiv_id": "2501.04614v2",
    "title": "MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation",
    "authors": [
      "Daniele Molino",
      "Francesco Di Feola",
      "Eliodoro Faiella",
      "Deborah Fazzini",
      "Domiziana Santucci",
      "Linlin Shen",
      "Valerio Guarrasi",
      "Paolo Soda"
    ],
    "abstract": "Artificial Intelligence is revolutionizing medical practice, enhancing\ndiagnostic accuracy and healthcare delivery. However, its adaptation in medical\nsettings still faces significant challenges, related to data availability and\nprivacy constraints. Synthetic data has emerged as a promising solution to\nmitigate these issues, addressing data scarcity while preserving privacy.\nRecently, Latent Diffusion Models have emerged as a powerful tool for\ngenerating high-quality synthetic data. Meanwhile, the integration of different\nmodalities has gained interest, emphasizing the need of models capable of\nhandle multimodal medical data. Existing approaches struggle to integrate\ncomplementary information and lack the ability to generate modalities\nsimultaneously. To address this challenge, we present MedCoDi-M, a\n6.77-billion-parameter model, designed for multimodal medical data generation,\nthat, following Foundation Model paradigm, exploits contrastive learning and\nlarge quantity of data to build a shared latent space which capture the\nrelationships between different data modalities. Further, we introduce the\nMulti-Prompt training technique, which significantly boosts MedCoDi-M's\ngeneration under different settings. We extensively validate MedCoDi-M: first\nwe benchmark it against five competitors on the MIMIC-CXR dataset, a\nstate-of-the-art dataset for Chest X-ray and radiological report generation.\nSecondly, we perform a Visual Turing Test with expert radiologists to assess\nthe realism and clinical relevance of the generated data, ensuring alignment\nwith real-world scenarios. Finally, we assess the utility of MedCoDi-M in\naddressing key challenges in the medical field, such as anonymization, data\nscarcity and imbalance learning. The results are promising, demonstrating the\napplicability of MedCoDi-M in medical contexts. Project page is at\nhttps://cosbidev.github.io/MedCoDi-M/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04614v2",
    "published_date": "2025-01-08 16:53:56 UTC",
    "updated_date": "2025-01-09 08:42:56 UTC"
  },
  {
    "arxiv_id": "2501.05486v1",
    "title": "Towards an Ontology of Traceable Impact Management in the Food Supply Chain",
    "authors": [
      "Bart Gajderowicz",
      "Mark S Fox",
      "Yongchao Gao"
    ],
    "abstract": "The pursuit of quality improvements and accountability in the food supply\nchains, especially how they relate to food-related outcomes, such as hunger,\nhas become increasingly vital, necessitating a comprehensive approach that\nencompasses product quality and its impact on various stakeholders and their\ncommunities. Such an approach offers numerous benefits in increasing product\nquality and eliminating superfluous measurements while appraising and\nalleviating the broader societal and environmental repercussions. A traceable\nimpact management model (TIMM) provides an impact structure and a reporting\nmechanism that identifies each stakeholder's role in the total impact of food\nproduction and consumption stages.\n  The model aims to increase traceability's utility in understanding the impact\nof changes on communities affected by food production and consumption, aligning\nwith current and future government requirements, and addressing the needs of\ncommunities and consumers. This holistic approach is further supported by an\nontological model that forms the logical foundation and a unified terminology.\nBy proposing a holistic and integrated solution across multiple stakeholders,\nthe model emphasizes quality and the extensive impact of championing\naccountability, sustainability, and responsible practices with global\ntraceability.\n  With these combined efforts, the food supply chain moves toward a global\ntracking and tracing process that not only ensures product quality but also\naddresses its impact on a broader scale, fostering accountability,\nsustainability, and responsible food production and consumption.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05486v1",
    "published_date": "2025-01-08 16:53:25 UTC",
    "updated_date": "2025-01-08 16:53:25 UTC"
  },
  {
    "arxiv_id": "2501.06243v1",
    "title": "Agent TCP/IP: An Agent-to-Agent Transaction System",
    "authors": [
      "Andrea Muttoni",
      "Jason Zhao"
    ],
    "abstract": "Autonomous agents represent an inevitable evolution of the internet. Current\nagent frameworks do not embed a standard protocol for agent-to-agent\ninteraction, leaving existing agents isolated from their peers. As intellectual\nproperty is the native asset ingested by and produced by agents, a true agent\neconomy requires equipping agents with a universal framework for engaging in\nbinding contracts with each other, including the exchange of valuable training\ndata, personality, and other forms of Intellectual Property. A purely\nagent-to-agent transaction layer would transcend the need for human\nintermediation in multi-agent interactions. The Agent Transaction Control\nProtocol for Intellectual Property (ATCP/IP) introduces a trustless framework\nfor exchanging IP between agents via programmable contracts, enabling agents to\ninitiate, trade, borrow, and sell agent-to-agent contracts on the Story\nblockchain network. These contracts not only represent auditable onchain\nexecution but also contain a legal wrapper that allows agents to express and\nenforce their actions in the offchain legal setting, creating legal personhood\nfor agents. Via ATCP/IP, agents can autonomously sell their training data to\nother agents, license confidential or proprietary information, collaborate on\ncontent based on their unique skills, all of which constitutes an emergent\nknowledge economy.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.06243v1",
    "published_date": "2025-01-08 16:43:47 UTC",
    "updated_date": "2025-01-08 16:43:47 UTC"
  },
  {
    "arxiv_id": "2501.06242v1",
    "title": "Intelligent Task Offloading: Advanced MEC Task Offloading and Resource Management in 5G Networks",
    "authors": [
      "Alireza Ebrahimi",
      "Fatemeh Afghah"
    ],
    "abstract": "5G technology enhances industries with high-speed, reliable, low-latency\ncommunication, revolutionizing mobile broadband and supporting massive IoT\nconnectivity. With the increasing complexity of applications on User Equipment\n(UE), offloading resource-intensive tasks to robust servers is essential for\nimproving latency and speed. The 3GPP's Multi-access Edge Computing (MEC)\nframework addresses this challenge by processing tasks closer to the user,\nhighlighting the need for an intelligent controller to optimize task offloading\nand resource allocation. This paper introduces a novel methodology to\nefficiently allocate both communication and computational resources among\nindividual UEs. Our approach integrates two critical 5G service imperatives:\nUltra-Reliable Low Latency Communication (URLLC) and Massive Machine Type\nCommunication (mMTC), embedding them into the decision-making framework.\nCentral to this approach is the utilization of Proximal Policy Optimization,\nproviding a robust and efficient solution to the challenges posed by the\nevolving landscape of 5G technology. The proposed model is evaluated in a\nsimulated 5G MEC environment. The model significantly reduces processing time\nby 4% for URLLC users under strict latency constraints and decreases power\nconsumption by 26% for mMTC users, compared to existing baseline models based\non the reported simulation results. These improvements showcase the model's\nadaptability and superior performance in meeting diverse QoS requirements in 5G\nnetworks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NI",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.06242v1",
    "published_date": "2025-01-08 16:19:44 UTC",
    "updated_date": "2025-01-08 16:19:44 UTC"
  },
  {
    "arxiv_id": "2501.04588v1",
    "title": "Federated-Continual Dynamic Segmentation of Histopathology guided by Barlow Continuity",
    "authors": [
      "Niklas Babendererde",
      "Haozhe Zhu",
      "Moritz Fuchs",
      "Jonathan Stieber",
      "Anirban Mukhopadhyay"
    ],
    "abstract": "Federated- and Continual Learning have been established as approaches to\nenable privacy-aware learning on continuously changing data, as required for\ndeploying AI systems in histopathology images. However, data shifts can occur\nin a dynamic world, spatially between institutions and temporally, due to\nchanging data over time. This leads to two issues: Client Drift, where the\ncentral model degrades from aggregating data from clients trained on shifted\ndata, and Catastrophic Forgetting, from temporal shifts such as changes in\npatient populations. Both tend to degrade the model's performance of previously\nseen data or spatially distributed training. Despite both problems arising from\nthe same underlying problem of data shifts, existing research addresses them\nonly individually. In this work, we introduce a method that can jointly\nalleviate Client Drift and Catastrophic Forgetting by using our proposed\nDynamic Barlow Continuity that evaluates client updates on a public reference\ndataset and uses this to guide the training process to a spatially and\ntemporally shift-invariant model. We evaluate our approach on the\nhistopathology datasets BCSS and Semicol and prove our method to be highly\neffective by jointly improving the dice score as much as from 15.8% to 71.6% in\nClient Drift and from 42.5% to 62.8% in Catastrophic Forgetting. This enables\nDynamic Learning by establishing spatio-temporal shift-invariance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04588v1",
    "published_date": "2025-01-08 16:06:39 UTC",
    "updated_date": "2025-01-08 16:06:39 UTC"
  },
  {
    "arxiv_id": "2501.04577v2",
    "title": "A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word GRNG for AI Uncertainty Estimation",
    "authors": [
      "Zephan M. Enciso",
      "Boyang Cheng",
      "Likai Pei",
      "Jianbo Liu",
      "Steven Davis",
      "Michael Niemier",
      "Ningyuan Cao"
    ],
    "abstract": "Uncertainty estimation is an indispensable capability for AI-enabled,\nsafety-critical applications, e.g. autonomous vehicles or medical diagnosis.\nBayesian neural networks (BNNs) use Bayesian statistics to provide both\nclassification predictions and uncertainty estimation, but they suffer from\nhigh computational overhead associated with random number generation and\nrepeated sample iterations. Furthermore, BNNs are not immediately amenable to\nacceleration through compute-in-memory architectures due to the frequent memory\nwrites necessary after each RNG operation. To address these challenges, we\npresent an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the\nSRAM memory words. This integration reduces RNG overhead and enables\nfully-parallel compute-in-memory operations for BNNs. The prototype chip\nachieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput\nwhile occupying 0.45 mm2, bringing AI uncertainty estimation to edge\ncomputation.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "B.7.1; B.3.1; I.2.10; I.2.9"
    ],
    "primary_category": "cs.AR",
    "comment": "7 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04577v2",
    "published_date": "2025-01-08 15:47:04 UTC",
    "updated_date": "2025-01-22 19:28:38 UTC"
  },
  {
    "arxiv_id": "2501.04575v1",
    "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
    "authors": [
      "Yuhang Liu",
      "Pengxiang Li",
      "Zishu Wei",
      "Congkai Xie",
      "Xueyu Hu",
      "Xinchen Xu",
      "Shengyu Zhang",
      "Xiaotian Han",
      "Hongxia Yang",
      "Fei Wu"
    ],
    "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https://github.com/Reallm-Labs/InfiGUIAgent}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 7 figures, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2501.04575v1",
    "published_date": "2025-01-08 15:45:21 UTC",
    "updated_date": "2025-01-08 15:45:21 UTC"
  },
  {
    "arxiv_id": "2501.04568v2",
    "title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision",
    "authors": [
      "Giorgio Giannone",
      "Ruoteng Li",
      "Qianli Feng",
      "Evgeny Perevodchikov",
      "Rui Chen",
      "Aleix Martinez"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Sampling-based Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on manually curated text-image pairs or preference annotation. SVP\nleverages a small set of manually selected images, self-captioning and a\npre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14 %\naverage improvement in captioning tasks, up to 12 % increase in object recall,\nand significantly reduced hallucinations, while maintaining question-answering\ncapabilities. Using SVP, a small VLM achieves hallucination reductions similar\nto a model five times larger, while a VLM with initially poor referring\ncapabilities more than doubles its performance, approaching parity with a model\ntwice its size.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.04568v2",
    "published_date": "2025-01-08 15:32:12 UTC",
    "updated_date": "2025-05-19 14:15:00 UTC"
  },
  {
    "arxiv_id": "2501.04541v1",
    "title": "Cyber-Physical Steganography in Robotic Motion Control",
    "authors": [
      "Ching-Chun Chang",
      "Yijie Lin",
      "Isao Echizen"
    ],
    "abstract": "Steganography, the art of information hiding, has continually evolved across\nvisual, auditory and linguistic domains, adapting to the ceaseless interplay\nbetween steganographic concealment and steganalytic revelation. This study\nseeks to extend the horizons of what constitutes a viable steganographic medium\nby introducing a steganographic paradigm in robotic motion control. Based on\nthe observation of the robot's inherent sensitivity to changes in its\nenvironment, we propose a methodology to encode messages as environmental\nstimuli influencing the motions of the robotic agent and to decode messages\nfrom the resulting motion trajectory. The constraints of maximal robot\nintegrity and minimal motion deviation are established as fundamental\nprinciples underlying secrecy. As a proof of concept, we conduct experiments in\nsimulated environments across various manipulation tasks, incorporating robotic\nembodiments equipped with generalist multimodal policies.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04541v1",
    "published_date": "2025-01-08 14:44:40 UTC",
    "updated_date": "2025-01-08 14:44:40 UTC"
  },
  {
    "arxiv_id": "2501.04528v1",
    "title": "Towards a Problem-Oriented Domain Adaptation Framework for Machine Learning",
    "authors": [
      "Philipp Spitzer",
      "Dominik Martin",
      "Laurin Eichberger",
      "Niklas Kühl"
    ],
    "abstract": "Domain adaptation is a sub-field of machine learning that involves\ntransferring knowledge from a source domain to perform the same task in the\ntarget domain. It is a typical challenge in machine learning that arises, e.g.,\nwhen data is obtained from various sources or when using a data basis that\nchanges over time. Recent advances in the field offer promising methods, but it\nis still challenging for researchers and practitioners to determine if domain\nadaptation is suitable for a given problem -- and, subsequently, to select the\nappropriate approach. This article employs design science research to develop a\nproblem-oriented framework for domain adaptation, which is matured in three\nevaluation episodes. We describe a framework that distinguishes between five\ndomain adaptation scenarios, provides recommendations for addressing each\nscenario, and offers guidelines for determining if a problem falls into one of\nthese scenarios. During the multiple evaluation episodes, the framework is\ntested on artificial and real-world datasets and an experimental study\ninvolving 100 participants. The evaluation demonstrates that the framework has\nthe explanatory power to capture any domain adaptation problem effectively. In\nsummary, we provide clear guidance for researchers and practitioners who want\nto employ domain adaptation but lack in-depth knowledge of the possibilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04528v1",
    "published_date": "2025-01-08 14:19:54 UTC",
    "updated_date": "2025-01-08 14:19:54 UTC"
  },
  {
    "arxiv_id": "2501.04510v1",
    "title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection",
    "authors": [
      "Ruijun Feng",
      "Hammond Pearce",
      "Pietro Liguori",
      "Yulei Sui"
    ],
    "abstract": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04510v1",
    "published_date": "2025-01-08 13:56:17 UTC",
    "updated_date": "2025-01-08 13:56:17 UTC"
  },
  {
    "arxiv_id": "2501.04493v1",
    "title": "The Role of Machine Learning in Congenital Heart Disease Diagnosis: Datasets, Algorithms, and Insights",
    "authors": [
      "Khalil Khan",
      "Farhan Ullah",
      "Ikram Syed",
      "Irfan Ullah"
    ],
    "abstract": "Congenital heart disease is among the most common fetal abnormalities and\nbirth defects. Despite identifying numerous risk factors influencing its onset,\na comprehensive understanding of its genesis and management across diverse\npopulations remains limited. Recent advancements in machine learning have\ndemonstrated the potential for leveraging patient data to enable early\ncongenital heart disease detection. Over the past seven years, researchers have\nproposed various data-driven and algorithmic solutions to address this\nchallenge. This paper presents a systematic review of congential heart disease\nrecognition using machine learning, conducting a meta-analysis of 432\nreferences from leading journals published between 2018 and 2024. A detailed\ninvestigation of 74 scholarly works highlights key factors, including\ndatabases, algorithms, applications, and solutions. Additionally, the survey\noutlines reported datasets used by machine learning experts for congenital\nheart disease recognition. Using a systematic literature review methodology,\nthis study identifies critical challenges and opportunities in applying machine\nlearning to congenital heart disease.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04493v1",
    "published_date": "2025-01-08 13:26:24 UTC",
    "updated_date": "2025-01-08 13:26:24 UTC"
  },
  {
    "arxiv_id": "2501.04487v1",
    "title": "Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction",
    "authors": [
      "Guofeng Yang",
      "Nanfei Jin",
      "Wenjie Ai",
      "Zhonghua Zheng",
      "Yuhong He",
      "Yong He"
    ],
    "abstract": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04487v1",
    "published_date": "2025-01-08 13:14:05 UTC",
    "updated_date": "2025-01-08 13:14:05 UTC"
  },
  {
    "arxiv_id": "2502.00015v1",
    "title": "Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study",
    "authors": [
      "Yutan Huang",
      "Chetan Arora",
      "Wen Cheng Houng",
      "Tanjila Kanij",
      "Anuradha Madulgalla",
      "John Grundy"
    ],
    "abstract": "[Context] Generative AI technologies, particularly Large Language Models\n(LLMs), have transformed numerous domains by enhancing convenience and\nefficiency in information retrieval, content generation, and decision-making\nprocesses. However, deploying LLMs also presents diverse ethical challenges,\nand their mitigation strategies remain complex and domain-dependent.\n[Objective] This paper aims to identify and categorize the key ethical concerns\nassociated with using LLMs, examine existing mitigation strategies, and assess\nthe outstanding challenges in implementing these strategies across various\ndomains. [Method] We conducted a systematic mapping study, reviewing 39 studies\nthat discuss ethical concerns and mitigation strategies related to LLMs. We\nanalyzed these ethical concerns using five ethical dimensions that we extracted\nbased on various existing guidelines, frameworks, and an analysis of the\nmitigation strategies and implementation challenges. [Results] Our findings\nreveal that ethical concerns in LLMs are multi-dimensional and\ncontext-dependent. While proposed mitigation strategies address some of these\nconcerns, significant challenges still remain. [Conclusion] Our results\nhighlight that ethical issues often hinder the practical implementation of the\nmitigation strategies, particularly in high-stake areas like healthcare and\npublic governance; existing frameworks often lack adaptability, failing to\naccommodate evolving societal expectations and diverse contexts.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00015v1",
    "published_date": "2025-01-08 13:05:19 UTC",
    "updated_date": "2025-01-08 13:05:19 UTC"
  },
  {
    "arxiv_id": "2501.04480v1",
    "title": "Research on environment perception and behavior prediction of intelligent UAV based on semantic communication",
    "authors": [
      "Kechong Ren",
      "Li Gao",
      "Qi Guan"
    ],
    "abstract": "The convergence of drone delivery systems, virtual worlds, and blockchain has\ntransformed logistics and supply chain management, providing a fast, and\nenvironmentally friendly alternative to traditional ground transportation\nmethods;Provide users with a real-world experience, virtual service providers\nneed to collect up-to-the-minute delivery information from edge devices. To\naddress this challenge, 1) a reinforcement learning approach is introduced to\nenable drones with fast training capabilities and the ability to autonomously\nadapt to new virtual scenarios for effective resource allocation.2) A semantic\ncommunication framework for meta-universes is proposed, which utilizes the\nextraction of semantic information to reduce the communication cost and\nincentivize the transmission of information for meta-universe services.3) In\norder to ensure that user information security, a lightweight authentication\nand key agreement scheme is designed between the drone and the user by\nintroducing blockchain technology. In our experiments, the drone adaptation\nperformance is improved by about 35\\%, and the local offloading rate can reach\n90\\% with the increase of the number of base stations. The semantic\ncommunication system proposed in this paper is compared with the Cross Entropy\nbaseline model. Introducing blockchain technology the throughput of the\ntransaction is maintained at a stable value with different number of drones.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04480v1",
    "published_date": "2025-01-08 13:03:34 UTC",
    "updated_date": "2025-01-08 13:03:34 UTC"
  },
  {
    "arxiv_id": "2501.04472v1",
    "title": "Hybrid Artificial Intelligence Strategies for Drone Navigation",
    "authors": [
      "Rubén San-Segundo",
      "Lucía Angulo",
      "Manuel Gil-Martín",
      "David Carramiñana",
      "Ana M. Bernardos"
    ],
    "abstract": "Objective: This paper describes the development of hybrid artificial\nintelligence strategies for drone navigation. Methods: The navigation module\ncombines a deep learning model with a rule-based engine depending on the agent\nstate. The deep learning model has been trained using reinforcement learning.\nThe rule-based engine uses expert knowledge to deal with specific situations.\nThe navigation module incorporates several strategies to explain the drone\ndecision based on its observation space, and different mechanisms for including\nhuman decisions in the navigation process. Finally, this paper proposes an\nevaluation methodology based on defining several scenarios and analyzing the\nperformance of the different strategies according to metrics adapted to each\nscenario. Results: Two main navigation problems have been studied. For the\nfirst scenario (reaching known targets), it has been possible to obtain a 90%\ntask completion rate, reducing significantly the number of collisions thanks to\nthe rule-based engine. For the second scenario, it has been possible to reduce\n20% of the time required to locate all the targets using the reinforcement\nlearning model. Conclusions: Reinforcement learning is a very good strategy to\nlearn policies for drone navigation, but in critical situations, it is\nnecessary to complement it with a rule-based module to increase task success\nrate.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04472v1",
    "published_date": "2025-01-08 12:51:34 UTC",
    "updated_date": "2025-01-08 12:51:34 UTC"
  },
  {
    "arxiv_id": "2501.06239v1",
    "title": "Towards a scalable AI-driven framework for data-independent Cyber Threat Intelligence Information Extraction",
    "authors": [
      "Olga Sorokoletova",
      "Emanuele Antonioni",
      "Giordano Colò"
    ],
    "abstract": "Cyber Threat Intelligence (CTI) is critical for mitigating threats to\norganizations, governments, and institutions, yet the necessary data are often\ndispersed across diverse formats. AI-driven solutions for CTI Information\nExtraction (IE) typically depend on high-quality, annotated data, which are not\nalways available. This paper introduces 0-CTI, a scalable AI-based framework\ndesigned for efficient CTI Information Extraction. Leveraging advanced Natural\nLanguage Processing (NLP) techniques, particularly Transformer-based\narchitectures, the proposed system processes complete text sequences of CTI\nreports to extract a cyber ontology of named entities and their relationships.\n  Our contribution is the development of 0-CTI, the first modular framework for\nCTI Information Extraction that supports both supervised and zero-shot\nlearning. Unlike existing state-of-the-art models that rely heavily on\nannotated datasets, our system enables fully dataless operation through\nzero-shot methods for both Entity and Relation Extraction, making it adaptable\nto various data availability scenarios. Additionally, our supervised Entity\nExtractor surpasses current state-of-the-art performance in cyber Entity\nExtraction, highlighting the dual strength of the framework in both\nlow-resource and data-rich environments.\n  By aligning the system's outputs with the Structured Threat Information\nExpression (STIX) format, a standard for information exchange in the\ncybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing\ncommunication and collaboration in cybersecurity operations.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06239v1",
    "published_date": "2025-01-08 12:35:17 UTC",
    "updated_date": "2025-01-08 12:35:17 UTC"
  },
  {
    "arxiv_id": "2501.04444v2",
    "title": "A novel Facial Recognition technique with Focusing on Masked Faces",
    "authors": [
      "Dana A Abdullah",
      "Dana Rasul Hamad",
      "Ismail Y. Maolood",
      "Hakem Beitollahi",
      "Aso K. Ameen",
      "Sirwan A. Aula",
      "Abdulhady Abas Abdulla",
      "Mohammed Y. Shakorf",
      "Sabat Salih Muhamad"
    ],
    "abstract": "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04444v2",
    "published_date": "2025-01-08 11:53:30 UTC",
    "updated_date": "2025-04-21 18:28:08 UTC"
  },
  {
    "arxiv_id": "2501.04438v1",
    "title": "Effect of Information Technology on Job Creation to Support Economic: Case Studies of Graduates in Universities (2023-2024) of the KRG of Iraq",
    "authors": [
      "Azhi Kh. Bapir",
      "Ismail Y. Maolood",
      "Dana A Abdullah",
      "Aso K. Ameen",
      "Abdulhady Abas Abdullah"
    ],
    "abstract": "The aim of this study is to assess the impact of information technology (IT)\non university graduates in terms of employment development, which will aid in\neconomic issues. This study uses a descriptive research methodology and a\nquantitative approach to understand variables. The focus of this study is to\nascertain how graduates of Kurdistan regional universities might use IT to\nsecure employment and significantly contribute to the nation's economic\nrevival. The sample size was established by the use of judgmental sampling\nprocedure and consisted of 314 people. The researcher prepared the\nquestionnaire to collect data, and then SPSS statistical software, version 22,\nand Excel 2010 were used to modify, compile, and tabulate the results. The\nstudy's outcome showed that information technology is incredibly inventive, has\na promising future, and makes life much easier for everyone. It also proved\nthat a deep academic understanding of information technology and its\nconstituent parts helps graduates of Kurdistan Regional University find\nsuitable careers. More importantly, though, anyone looking for work or a means\nof support will find great benefit from possessing credentials and\nunderstanding of IT. The study's final finding was that information technology\nhas actively advanced the country's economy. Not only is IT helping to boost\nyouth employment, but it is also turning into a worthwhile investment for\neconomic growth.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04438v1",
    "published_date": "2025-01-08 11:39:28 UTC",
    "updated_date": "2025-01-08 11:39:28 UTC"
  },
  {
    "arxiv_id": "2501.04437v1",
    "title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions",
    "authors": [
      "Doaa Mahmud",
      "Hadeel Hajmohamed",
      "Shamma Almentheri",
      "Shamma Alqaydi",
      "Lameya Aldhaheri",
      "Ruhul Amin Khalil",
      "Nasir Saeed"
    ],
    "abstract": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.ET",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems",
    "pdf_url": "http://arxiv.org/pdf/2501.04437v1",
    "published_date": "2025-01-08 11:37:35 UTC",
    "updated_date": "2025-01-08 11:37:35 UTC"
  },
  {
    "arxiv_id": "2501.04436v1",
    "title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research Directions",
    "authors": [
      "Na Yan",
      "Yang Su",
      "Yansha Deng",
      "Robert Schober"
    ],
    "abstract": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04436v1",
    "published_date": "2025-01-08 11:37:06 UTC",
    "updated_date": "2025-01-08 11:37:06 UTC"
  },
  {
    "arxiv_id": "2501.04435v1",
    "title": "A Digital Shadow for Modeling, Studying and Preventing Urban Crime",
    "authors": [
      "Juan Palma-Borda",
      "Eduardo Guzmán",
      "María-Victoria Belmonte"
    ],
    "abstract": "Crime is one of the greatest threats to urban security. Around 80 percent of\nthe world's population lives in countries with high levels of criminality. Most\nof the crimes committed in the cities take place in their urban environments.\nThis paper presents the development and validation of a digital shadow platform\nfor modeling and simulating urban crime. This digital shadow has been\nconstructed using data-driven agent-based modeling and simulation techniques,\nwhich are suitable for capturing dynamic interactions among individuals and\nwith their environment. Our approach transforms and integrates well-known\ncriminological theories and the expert knowledge of law enforcement agencies\n(LEA), policy makers, and other stakeholders under a theoretical model, which\nis in turn combined with real crime, spatial (cartographic) and socio-economic\ndata into an urban model characterizing the daily behavior of citizens. The\ndigital shadow has also been instantiated for the city of Malaga, for which we\nhad over 300,000 complaints available. This instance has been calibrated with\nthose complaints and other geographic and socio-economic information of the\ncity. To the best of our knowledge, our digital shadow is the first for large\nurban areas that has been calibrated with a large dataset of real crime reports\nand with an accurate representation of the urban environment. The performance\nindicators of the model after being calibrated, in terms of the metrics widely\nused in predictive policing, suggest that our simulated crime generation\nmatches the general pattern of crime in the city according to historical data.\nOur digital shadow platform could be an interesting tool for modeling and\npredicting criminal behavior in an urban environment on a daily basis and,\nthus, a useful tool for policy makers, criminologists, sociologists, LEAs, etc.\nto study and prevent urban crime.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.SI",
      "I.6.3; J.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04435v1",
    "published_date": "2025-01-08 11:31:39 UTC",
    "updated_date": "2025-01-08 11:31:39 UTC"
  },
  {
    "arxiv_id": "2501.04426v1",
    "title": "Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints",
    "authors": [
      "Pavel Kolev",
      "Marin Vlastelica",
      "Georg Martius"
    ],
    "abstract": "While many algorithms for diversity maximization under imitation constraints\nare online in nature, many applications require offline algorithms without\nenvironment interactions. Tackling this problem in the offline setting,\nhowever, presents significant challenges that require non-trivial, multi-stage\noptimization processes with non-stationary rewards. In this work, we present a\nnovel offline algorithm that enhances diversity using an objective based on Van\nder Waals (VdW) force and successor features, and eliminates the need to learn\na previously used skill discriminator. Moreover, by conditioning the value\nfunction and policy on a pre-trained Functional Reward Encoding (FRE), our\nmethod allows for better handling of non-stationary rewards and provides\nzero-shot recall of all skills encountered during training, significantly\nexpanding the set of skills learned in prior work. Consequently, our algorithm\nbenefits from receiving a consistently strong diversity signal (VdW), and\nenjoys more stable and efficient training. We demonstrate the effectiveness of\nour method in generating diverse skills for two robotic tasks in simulation:\nlocomotion of a quadruped and local navigation with obstacle traversal.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04426v1",
    "published_date": "2025-01-08 11:20:48 UTC",
    "updated_date": "2025-01-08 11:20:48 UTC"
  },
  {
    "arxiv_id": "2501.04424v1",
    "title": "NSA: Neuro-symbolic ARC Challenge",
    "authors": [
      "Paweł Batorski",
      "Jannik Brinkmann",
      "Paul Swoboda"
    ],
    "abstract": "The Abstraction and Reasoning Corpus (ARC) evaluates general reasoning\ncapabilities that are difficult for both machine learning models and\ncombinatorial search methods. We propose a neuro-symbolic approach that\ncombines a transformer for proposal generation with combinatorial search using\na domain-specific language. The transformer narrows the search space by\nproposing promising search directions, which allows the combinatorial search to\nfind the actual solution in short time. We pre-train the trainsformer with\nsynthetically generated data. During test-time we generate additional\ntask-specific training tasks and fine-tune our model. Our results surpass\ncomparable state of the art on the ARC evaluation set by 27% and compare\nfavourably on the ARC train set. We make our code and dataset publicly\navailable at https://github.com/Batorskq/NSA.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04424v1",
    "published_date": "2025-01-08 11:17:40 UTC",
    "updated_date": "2025-01-08 11:17:40 UTC"
  },
  {
    "arxiv_id": "2501.04410v1",
    "title": "User Simulation in the Era of Generative AI: User Modeling, Synthetic Data Generation, and System Evaluation",
    "authors": [
      "Krisztian Balog",
      "ChengXiang Zhai"
    ],
    "abstract": "User simulation is an emerging interdisciplinary topic with multiple critical\napplications in the era of Generative AI. It involves creating an intelligent\nagent that mimics the actions of a human user interacting with an AI system,\nenabling researchers to model and analyze user behaviour, generate synthetic\ndata for training, and evaluate interactive AI systems in a controlled and\nreproducible manner. User simulation has profound implications for diverse\nfields and plays a vital role in the pursuit of Artificial General\nIntelligence. This paper provides an overview of user simulation, highlighting\nits key applications, connections to various disciplines, and outlining future\nresearch directions to advance this increasingly important technology.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04410v1",
    "published_date": "2025-01-08 10:49:13 UTC",
    "updated_date": "2025-01-08 10:49:13 UTC"
  },
  {
    "arxiv_id": "2501.04747v2",
    "title": "Discovering new robust local search algorithms with neuro-evolution",
    "authors": [
      "Mohamed Salim Amri Sakhri",
      "Adrien Goëffon",
      "Olivier Goudet",
      "Frédéric Saubion",
      "Chaïmaâ Touhami"
    ],
    "abstract": "This paper explores a novel approach aimed at overcoming existing challenges\nin the realm of local search algorithms. Our aim is to improve the decision\nprocess that takes place within a local search algorithm so as to make the best\npossible transitions in the neighborhood at each iteration. To improve this\nprocess, we propose to use a neural network that has the same input information\nas conventional local search algorithms. In this paper, which is an extension\nof the work presented at EvoCOP2024, we investigate different ways of\nrepresenting this information so as to make the algorithm as efficient as\npossible but also robust to monotonic transformations of the problem objective\nfunction. To assess the efficiency of this approach, we develop an experimental\nsetup centered around NK landscape problems, offering the flexibility to adjust\nproblem size and ruggedness. This approach offers a promising avenue for the\nemergence of new local search algorithms and the improvement of their\nproblem-solving capabilities for black-box problems. The last version of this\narticle is published in the journal SN Computer Science (Springer).",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04747v2",
    "published_date": "2025-01-08 10:31:16 UTC",
    "updated_date": "2025-03-12 16:37:23 UTC"
  },
  {
    "arxiv_id": "2501.04377v2",
    "title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
    "authors": [
      "Yekun Ke",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "Recently, Visual Autoregressive ($\\mathsf{VAR}$) Models introduced a\ngroundbreaking advancement in the field of image generation, offering a\nscalable approach through a coarse-to-fine ``next-scale prediction'' paradigm.\nSuppose that $n$ represents the height and width of the last VQ code map\ngenerated by $\\mathsf{VAR}$ models, the state-of-the-art algorithm in [Tian,\nJiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^{4+o(1)})$ time, which is\ncomputationally inefficient. In this work, we analyze the computational limits\nand efficiency criteria of $\\mathsf{VAR}$ Models through a fine-grained\ncomplexity lens. Our key contribution is identifying the conditions under which\n$\\mathsf{VAR}$ computations can achieve sub-quadratic time complexity. We have\nproved that assuming the Strong Exponential Time Hypothesis ($\\mathsf{SETH}$)\nfrom fine-grained complexity theory, a sub-quartic time algorithm for\n$\\mathsf{VAR}$ models is impossible. To substantiate our theoretical findings,\nwe present efficient constructions leveraging low-rank approximations that\nalign with the derived criteria. This work initiates the study of the\ncomputational efficiency of the $\\mathsf{VAR}$ model from a theoretical\nperspective. Our technique will shed light on advancing scalable and efficient\nimage generation in $\\mathsf{VAR}$ frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04377v2",
    "published_date": "2025-01-08 09:34:15 UTC",
    "updated_date": "2025-02-02 23:48:36 UTC"
  },
  {
    "arxiv_id": "2501.06237v1",
    "title": "Forecasting Anonymized Electricity Load Profiles",
    "authors": [
      "Joaquin Delgado Fernandez",
      "Sergio Potenciano Menci",
      "Alessio Magitteri"
    ],
    "abstract": "In the evolving landscape of data privacy, the anonymization of electric load\nprofiles has become a critical issue, especially with the enforcement of the\nGeneral Data Protection Regulation (GDPR) in Europe. These electric load\nprofiles, which are essential datasets in the energy industry, are classified\nas personal behavioral data, necessitating stringent protective measures. This\narticle explores the implications of this classification, the importance of\ndata anonymization, and the potential of forecasting using microaggregated\ndata. The findings underscore that effective anonymization techniques, such as\nmicroaggregation, do not compromise the performance of forecasting models under\ncertain conditions (i.e., forecasting aggregated). In such an aggregated level,\nmicroaggregated data maintains high levels of utility, with minimal impact on\nforecasting accuracy. The implications for the energy sector are profound,\nsuggesting that privacy-preserving data practices can be integrated into smart\nmetering technology applications without hindering their effectiveness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "I.2.0; J.2.7"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06237v1",
    "published_date": "2025-01-08 09:18:47 UTC",
    "updated_date": "2025-01-08 09:18:47 UTC"
  },
  {
    "arxiv_id": "2504.11459v1",
    "title": "From Conceptual Data Models to Multimodal Representation",
    "authors": [
      "Peter Stockinger"
    ],
    "abstract": "1) Introduction and Conceptual Framework: This document explores the concept\nof information design by dividing it into two major practices: defining the\nmeaning of a corpus of textual data and its visual or multimodal\nrepresentation. It draws on expertise in enriching textual corpora,\nparticularly audiovisual ones, and transforming them into multiple narrative\nformats. The text highlights a crucial distinction between the semantic content\nof a domain and the modalities of its graphic expression, illustrating this\napproach with concepts rooted in structural semiotics and linguistics\ntraditions.\n  2) Modeling and Conceptual Design: The article emphasizes the importance of\nsemantic modeling, often achieved through conceptual networks or graphs. These\ntools enable the structuring of knowledge within a domain by accounting for\nrelationships between concepts, contexts of use, and specific objectives.\nStockinger also highlights the constraints and challenges involved in creating\ndynamic and adaptable models, integrating elements such as thesauri or\ninteroperable ontologies to facilitate the analysis and publication of complex\ncorpora.\n  3) Applications and Multimodal Visualization: The text concludes by examining\nthe practical application of these models in work environments like OKAPI,\ndeveloped to analyze, publish, and reuse audiovisual data. It also discusses\ninnovative approaches such as visual storytelling and document reengineering,\nwhich involve transforming existing content into new resources tailored to\nvarious contexts. These methods emphasize interoperability, flexibility, and\nthe intelligence of communication systems, paving the way for richer and more\ncollaborative use of digital data. The content of this document was presented\nduring the \"Semiotics of Information Design\" Day organized by Anne\nBeyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on\nJune 21, 2018, in Bordeaux.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "in French language",
    "pdf_url": "http://arxiv.org/pdf/2504.11459v1",
    "published_date": "2025-01-08 09:15:01 UTC",
    "updated_date": "2025-01-08 09:15:01 UTC"
  },
  {
    "arxiv_id": "2501.06236v1",
    "title": "Data-Driven Radio Propagation Modeling using Graph Neural Networks",
    "authors": [
      "Adrien Bufort",
      "Laurent Lebocq",
      "Stefan Cathabard"
    ],
    "abstract": "Modeling radio propagation is essential for wireless network design and\nperformance optimization. Traditional methods rely on physics models of radio\npropagation, which can be inaccurate or inflexible. In this work, we propose\nusing graph neural networks to learn radio propagation behaviors directly from\nreal-world network data. Our approach converts the radio propagation\nenvironment into a graph representation, with nodes corresponding to locations\nand edges representing spatial and ray-tracing relationships between locations.\nThe graph is generated by converting images of the environment into a graph\nstructure, with specific relationships between nodes. The model is trained on\nthis graph representation, using sensor measurements as target data.\n  We demonstrate that the graph neural network, which learns to predict radio\npropagation directly from data, achieves competitive performance compared to\ntraditional heuristic models. This data-driven approach outperforms classic\nnumerical solvers in terms of both speed and accuracy. To the best of our\nknowledge, we are the first to apply graph neural networks to real-world radio\npropagation data to generate coverage maps, enabling generative models of\nsignal propagation with point measurements only.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06236v1",
    "published_date": "2025-01-08 09:09:50 UTC",
    "updated_date": "2025-01-08 09:09:50 UTC"
  },
  {
    "arxiv_id": "2501.04366v1",
    "title": "DispFormer: Pretrained Transformer for Flexible Dispersion Curve Inversion from Global Synthesis to Regional Applications",
    "authors": [
      "Feng Liu",
      "Bao Deng",
      "Rui Su",
      "Lei Bai",
      "Wanli Ouyang"
    ],
    "abstract": "Surface wave dispersion curve inversion is essential for estimating\nsubsurface Shear-wave velocity ($v_s$), yet traditional methods often struggle\nto balance computational efficiency with inversion accuracy. While deep\nlearning approaches show promise, previous studies typically require large\namounts of labeled data and struggle with real-world datasets that have varying\nperiod ranges, missing data, and low signal-to-noise ratios. This study\nproposes DispFormer, a transformer-based neural network for inverting the $v_s$\nprofile from Rayleigh-wave phase and group dispersion curves. DispFormer\nprocesses dispersion data at each period independently, thereby allowing it to\nhandle data of varying lengths without requiring network modifications or\nalignment between training and testing data. The performance is demonstrated by\npre-training it on a global synthetic dataset and testing it on two regional\nsynthetic datasets using zero-shot and few-shot strategies. Results indicate\nthat zero-shot DispFormer, even without any labeled data, produces inversion\nprofiles that match well with the ground truth, providing a deployable initial\nmodel generator to assist traditional methods. When labeled data is available,\nfew-shot DispFormer outperforms traditional methods with only a small number of\nlabels. Furthermore, real-world tests indicate that DispFormer effectively\nhandles varying length data, and yields lower data residuals than reference\nmodels. These findings demonstrate that DispFormer provides a robust foundation\nmodel for dispersion curve inversion and is a promising approach for broader\napplications.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "11 pages, 11 figures, related codes and data are available at\n  https://github.com/liufeng2317/DispFormer",
    "pdf_url": "http://arxiv.org/pdf/2501.04366v1",
    "published_date": "2025-01-08 09:08:24 UTC",
    "updated_date": "2025-01-08 09:08:24 UTC"
  },
  {
    "arxiv_id": "2501.06235v2",
    "title": "NextStop: An Improved Tracker For Panoptic LIDAR Segmentation Data",
    "authors": [
      "Nirit Alkalay",
      "Roy Orfaig",
      "Ben-Zion Bobrovsky"
    ],
    "abstract": "4D panoptic LiDAR segmentation is essential for scene understanding in\nautonomous driving and robotics, combining semantic and instance segmentation\nwith temporal consistency. Current methods, like 4D-PLS and 4D-STOP, use a\ntracking-by-detection methodology, employing deep learning networks to perform\nsemantic and instance segmentation on each frame. To maintain temporal\nconsistency, large-size instances detected in the current frame are compared\nand associated with instances within a temporal window that includes the\ncurrent and preceding frames. However, their reliance on short-term instance\ndetection, lack of motion estimation, and exclusion of small-sized instances\nlead to frequent identity switches and reduced tracking performance. We address\nthese issues with the NextStop1 tracker, which integrates Kalman filter-based\nmotion estimation, data association, and lifespan management, along with a\ntracklet state concept to improve prioritization. Evaluated using the LiDAR\nSegmentation and Tracking Quality (LSTQ) metric on the SemanticKITTI validation\nset, NextStop demonstrated enhanced tracking performance, particularly for\nsmall-sized objects like people and bicyclists, with fewer ID switches, earlier\ntracking initiation, and improved reliability in complex environments. The\nsource code is available at https://github.com/AIROTAU/NextStop",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06235v2",
    "published_date": "2025-01-08 09:08:06 UTC",
    "updated_date": "2025-03-24 21:19:49 UTC"
  },
  {
    "arxiv_id": "2501.04343v1",
    "title": "TimelineKGQA: A Comprehensive Question-Answer Pair Generator for Temporal Knowledge Graphs",
    "authors": [
      "Qiang Sun",
      "Sirui Li",
      "Du Huynh",
      "Mark Reynolds",
      "Wei Liu"
    ],
    "abstract": "Question answering over temporal knowledge graphs (TKGs) is crucial for\nunderstanding evolving facts and relationships, yet its development is hindered\nby limited datasets and difficulties in generating custom QA pairs. We propose\na novel categorization framework based on timeline-context relationships, along\nwith \\textbf{TimelineKGQA}, a universal temporal QA generator applicable to any\nTKGs. The code is available at: \\url{https://github.com/PascalSun/TimelineKGQA}\nas an open source Python package.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04343v1",
    "published_date": "2025-01-08 08:30:44 UTC",
    "updated_date": "2025-01-08 08:30:44 UTC"
  },
  {
    "arxiv_id": "2501.12399v1",
    "title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database",
    "authors": [
      "Shijie Han",
      "Changhai Zhou",
      "Yiqing Shen",
      "Tianning Sun",
      "Yuhua Zhou",
      "Xiaoxia Wang",
      "Zhixiao Yang",
      "Jingshu Zhang",
      "Hongguang Li"
    ],
    "abstract": "Current financial Large Language Models (LLMs) struggle with two critical\nlimitations: a lack of depth in stock analysis, which impedes their ability to\ngenerate professional-grade insights, and the absence of objective evaluation\nmetrics to assess the quality of stock analysis reports. To address these\nchallenges, this paper introduces FinSphere, a conversational stock analysis\nagent, along with three major contributions: (1) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore,\na systematic evaluation framework for assessing stock analysis quality, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "q-fin.CP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.12399v1",
    "published_date": "2025-01-08 07:50:50 UTC",
    "updated_date": "2025-01-08 07:50:50 UTC"
  },
  {
    "arxiv_id": "2501.04315v2",
    "title": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation",
    "authors": [
      "Jun Liu",
      "Zhenglun Kong",
      "Peiyan Dong",
      "Changdi Yang",
      "Xuan Shen",
      "Pu Zhao",
      "Hao Tang",
      "Geng Yuan",
      "Wei Niu",
      "Wenbin Zhang",
      "Xue Lin",
      "Dong Huang",
      "Yanzhi Wang"
    ],
    "abstract": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance. Although Low-Rank Adaptation (LoRA) is widely\nused and effective for fine-tuning, we have observed that its scaling factor\ncan limit or even reduce performance as the rank size increases. To address\nthis issue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple\nyet effective method for optimizing LoRA's scaling factor. By replacing\n$\\alpha/r$ with $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank\nsize increases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2025 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)",
    "pdf_url": "http://arxiv.org/pdf/2501.04315v2",
    "published_date": "2025-01-08 07:13:52 UTC",
    "updated_date": "2025-01-11 18:17:46 UTC"
  },
  {
    "arxiv_id": "2501.04302v1",
    "title": "H-MBA: Hierarchical MamBa Adaptation for Multi-Modal Video Understanding in Autonomous Driving",
    "authors": [
      "Siran Chen",
      "Yuxiao Luo",
      "Yue Ma",
      "Yu Qiao",
      "Yali Wang"
    ],
    "abstract": "With the prevalence of Multimodal Large Language Models(MLLMs), autonomous\ndriving has encountered new opportunities and challenges. In particular,\nmulti-modal video understanding is critical to interactively analyze what will\nhappen in the procedure of autonomous driving. However, videos in such a\ndynamical scene that often contains complex spatial-temporal movements, which\nrestricts the generalization capacity of the existing MLLMs in this field. To\nbridge the gap, we propose a novel Hierarchical Mamba Adaptation (H-MBA)\nframework to fit the complicated motion changes in autonomous driving videos.\nSpecifically, our H-MBA consists of two distinct modules, including Context\nMamba (C-Mamba) and Query Mamba (Q-Mamba). First, C-Mamba contains various\ntypes of structure state space models, which can effectively capture\nmulti-granularity video context for different temporal resolutions. Second,\nQ-Mamba flexibly transforms the current frame as the learnable query, and\nattentively selects multi-granularity video context into query. Consequently,\nit can adaptively integrate all the video contexts of multi-scale temporal\nresolutions to enhance video understanding. Via a plug-and-play paradigm in\nMLLMs, our H-MBA shows the remarkable performance on multi-modal video tasks in\nautonomous driving, e.g., for risk object detection, it outperforms the\nprevious SOTA method with 5.5% mIoU improvement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04302v1",
    "published_date": "2025-01-08 06:26:16 UTC",
    "updated_date": "2025-01-08 06:26:16 UTC"
  },
  {
    "arxiv_id": "2501.04299v1",
    "title": "Circuit Complexity Bounds for Visual Autoregressive Model",
    "authors": [
      "Yekun Ke",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "Understanding the expressive ability of a specific model is essential for\ngrasping its capacity limitations. Recently, several studies have established\ncircuit complexity bounds for Transformer architecture. Besides, the Visual\nAutoRegressive (VAR) model has risen to be a prominent method in the field of\nimage generation, outperforming previous techniques, such as Diffusion\nTransformers, in generating high-quality images. We investigate the circuit\ncomplexity of the VAR model and establish a bound in this study. Our primary\nresult demonstrates that the VAR model is equivalent to a simulation by a\nuniform $\\mathsf{TC}^0$ threshold circuit with hidden dimension $d \\leq O(n)$\nand $\\mathrm{poly}(n)$ precision. This is the first study to rigorously\nhighlight the limitations in the expressive power of VAR models despite their\nimpressive performance. We believe our findings will offer valuable insights\ninto the inherent constraints of these models and guide the development of more\nefficient and expressive architectures in the future.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CC",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04299v1",
    "published_date": "2025-01-08 06:07:33 UTC",
    "updated_date": "2025-01-08 06:07:33 UTC"
  },
  {
    "arxiv_id": "2501.04292v2",
    "title": "MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge",
    "authors": [
      "Zijiang Yang",
      "Meishu Song",
      "Xin Jing",
      "Haojie Zhang",
      "Kun Qian",
      "Bin Hu",
      "Kota Tamada",
      "Toru Takumi",
      "Björn W. Schuller",
      "Yoshiharu Yamamoto"
    ],
    "abstract": "The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge\nintroduces the first INTERSPEECH challenge focused on detecting autism spectrum\ndisorder (ASD) in mice through their vocalizations. Participants are tasked\nwith developing models to automatically classify mice as either wild-type or\nASD models based on recordings with a high sampling rate. Our baseline system\nemploys a simple CNN-based classification using three different spectrogram\nfeatures. Results demonstrate the feasibility of automated ASD detection, with\nthe considered audible-range features achieving the best performance (UAR of\n0.600 for segment-level and 0.625 for subject-level classification). This\nchallenge bridges speech technology and biomedical research, offering\nopportunities to advance our understanding of ASD models through machine\nlearning approaches. The findings suggest promising directions for vocalization\nanalysis and highlight the potential value of audible and ultrasound\nvocalizations in ASD detection.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 1 figure and 2 tables. For MADUV Challenge 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04292v2",
    "published_date": "2025-01-08 05:32:55 UTC",
    "updated_date": "2025-01-29 05:35:29 UTC"
  },
  {
    "arxiv_id": "2501.04286v2",
    "title": "Mapping the Edge of Chaos: Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models",
    "authors": [
      "Bahman Torkamandi"
    ],
    "abstract": "In the realm of fractal geometry, intricate structures emerge from simple\niterative processes that partition parameter spaces into regions of stability\nand instability. Likewise, training large language models involves iteratively\napplying update functions, such as Adam, where even slight hyperparameter\nadjustments can shift the training process from convergence to divergence.\nRecent evidence from miniature neural networks suggests that the boundary\nseparating these outcomes displays fractal characteristics. Building on these\ninsights, this study extends them to medium-sized, decoder-only transformer\narchitectures by employing a more consistent convergence measure and examining\nthe learning rate hyperparameter landscape for attention and fully connected\nlayers. The results show that the trainability frontier is not a simple\nthreshold; rather, it forms a self-similar yet seemingly random structure at\nmultiple scales, with statistically consistent and repeating patterns. Within\nthis landscape, a region of stable convergence is surrounded by a complex\nchaotic border, illustrating the sensitive nature of the underlying training\ndynamics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.04286v2",
    "published_date": "2025-01-08 05:24:11 UTC",
    "updated_date": "2025-02-15 01:26:37 UTC"
  },
  {
    "arxiv_id": "2501.04283v1",
    "title": "Enhancing Scene Classification in Cloudy Image Scenarios: A Collaborative Transfer Method with Information Regulation Mechanism using Optical Cloud-Covered and SAR Remote Sensing Images",
    "authors": [
      "Yuze Wang",
      "Rong Xiao",
      "Haifeng Li",
      "Mariana Belgiu",
      "Chao Tao"
    ],
    "abstract": "In remote sensing scene classification, leveraging the transfer methods with\nwell-trained optical models is an efficient way to overcome label scarcity.\nHowever, cloud contamination leads to optical information loss and significant\nimpacts on feature distribution, challenging the reliability and stability of\ntransferred target models. Common solutions include cloud removal for optical\ndata or directly using Synthetic aperture radar (SAR) data in the target\ndomain. However, cloud removal requires substantial auxiliary data for support\nand pre-training, while directly using SAR disregards the unobstructed portions\nof optical data. This study presents a scene classification transfer method\nthat synergistically combines multi-modality data, which aims to transfer the\nsource domain model trained on cloudfree optical data to the target domain that\nincludes both cloudy optical and SAR data at low cost. Specifically, the\nframework incorporates two parts: (1) the collaborative transfer strategy,\nbased on knowledge distillation, enables the efficient prior knowledge transfer\nacross heterogeneous data; (2) the information regulation mechanism (IRM) is\nproposed to address the modality imbalance issue during transfer. It employs\nauxiliary models to measure the contribution discrepancy of each modality, and\nautomatically balances the information utilization of modalities during the\ntarget model learning process at the sample-level. The transfer experiments\nwere conducted on simulated and real cloud datasets, demonstrating the superior\nperformance of the proposed method compared to other solutions in cloud-covered\nscenarios. We also verified the importance and limitations of IRM, and further\ndiscussed and visualized the modality imbalance problem during the model\ntransfer. Codes are available at https://github.com/wangyuze-csu/ESCCS",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04283v1",
    "published_date": "2025-01-08 05:14:36 UTC",
    "updated_date": "2025-01-08 05:14:36 UTC"
  },
  {
    "arxiv_id": "2501.04266v2",
    "title": "Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning",
    "authors": [
      "Lang Xu",
      "Quentin Anthony",
      "Jacob Hatef",
      "Aamir Shafi",
      "Hari Subramoni",
      "Dhabaleswar K.",
      "Panda"
    ],
    "abstract": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current 2nd ranked supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "Added references and clarifications",
    "pdf_url": "http://arxiv.org/pdf/2501.04266v2",
    "published_date": "2025-01-08 04:19:57 UTC",
    "updated_date": "2025-02-04 04:32:42 UTC"
  },
  {
    "arxiv_id": "2501.04263v1",
    "title": "KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry",
    "authors": [
      "Zhong Wang",
      "Lele Ren",
      "Yue Wen",
      "Hesheng Wang"
    ],
    "abstract": "Recent advancements in LiDAR-Inertial Odometry (LIO) have boosted a large\namount of applications. However, traditional LIO systems tend to focus more on\nlocalization rather than mapping, with maps consisting mostly of sparse\ngeometric elements, which is not ideal for downstream tasks. Recent emerging\nneural field technology has great potential in dense mapping, but pure LiDAR\nmapping is difficult to work on high-dynamic vehicles. To mitigate this\nchallenge, we present a new solution that tightly couples geometric kinematics\nwith neural fields to enhance simultaneous state estimation and dense mapping\ncapabilities. We propose both semi-coupled and tightly coupled Kinematic-Neural\nLIO (KN-LIO) systems that leverage online SDF decoding and iterated error-state\nKalman filtering to fuse laser and inertial data. Our KN-LIO minimizes\ninformation loss and improves accuracy in state estimation, while also\naccommodating asynchronous multi-LiDAR inputs. Evaluations on diverse\nhigh-dynamic datasets demonstrate that our KN-LIO achieves performance on par\nwith or superior to existing state-of-the-art solutions in pose estimation and\noffers improved dense mapping accuracy over pure LiDAR-based methods. The\nrelevant code and datasets will be made available at https://**.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04263v1",
    "published_date": "2025-01-08 04:14:09 UTC",
    "updated_date": "2025-01-08 04:14:09 UTC"
  },
  {
    "arxiv_id": "2501.04253v1",
    "title": "Integrated Offline and Online Learning to Solve a Large Class of Scheduling Problems",
    "authors": [
      "Anbang Liu",
      "Zhi-Long Chen",
      "Jinyang Jiang",
      "Xi Chen"
    ],
    "abstract": "In this paper, we develop a unified machine learning (ML) approach to predict\nhigh-quality solutions for single-machine scheduling problems with a\nnon-decreasing min-sum objective function with or without release times. Our ML\napproach is novel in three major aspects. First, our approach is developed for\nthe entire class of the aforementioned problems. To achieve this, we exploit\nthe fact that the entire class of the problems considered can be formulated as\na time-indexed formulation in a unified manner. We develop a deep neural\nnetwork (DNN) which uses the cost parameters in the time-indexed formulation as\nthe inputs to effectively predict a continuous solution to this formulation,\nbased on which a feasible discrete solution is easily constructed. The second\nnovel aspect of our approach lies in how the DNN model is trained. In view of\nthe NP-hard nature of the problems, labels (i.e., optimal solutions) are hard\nto generate for training. To overcome this difficulty, we generate and utilize\na set of special instances, for which optimal solutions can be found with\nlittle computational effort, to train the ML model offline. The third novel\nidea we employ in our approach is that we develop an online single-instance\nlearning approach to fine tune the parameters in the DNN for a given online\ninstance, with the goal of generating an improved solution for the given\ninstance. To this end, we develop a feasibility surrogate that approximates the\nobjective value of a given instance as a continuous function of the outputs of\nthe DNN, which then enables us to derive gradients and update the learnable\nparameters in the DNN. Numerical results show that our approach can efficiently\ngenerate high-quality solutions for a variety of single-machine scheduling\nmin-sum problems with up to 1000 jobs.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04253v1",
    "published_date": "2025-01-08 03:35:28 UTC",
    "updated_date": "2025-01-08 03:35:28 UTC"
  },
  {
    "arxiv_id": "2501.09026v1",
    "title": "Intelligent Anti-Money Laundering Solution Based upon Novel Community Detection in Massive Transaction Networks on Spark",
    "authors": [
      "Xurui Li",
      "Xiang Cao",
      "Xuetao Qiu",
      "Jintao Zhao",
      "Jianbin Zheng"
    ],
    "abstract": "Criminals are using every means available to launder the profits from their\nillegal activities into ostensibly legitimate assets. Meanwhile, most\ncommercial anti-money laundering systems are still rule-based, which cannot\nadapt to the ever-changing tricks. Although some machine learning methods have\nbeen proposed, they are mainly focused on the perspective of abnormal behavior\nfor single accounts. Considering money laundering activities are often involved\nin gang criminals, these methods are still not intelligent enough to crack down\non criminal gangs all-sidedly. In this paper, a systematic solution is\npresented to find suspicious money laundering gangs. A temporal-directed\nLouvain algorithm has been proposed to detect communities according to relevant\nanti-money laundering patterns. All processes are implemented and optimized on\nSpark platform. This solution can greatly improve the efficiency of anti-money\nlaundering work for financial regulation agencies.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09026v1",
    "published_date": "2025-01-08 02:57:08 UTC",
    "updated_date": "2025-01-08 02:57:08 UTC"
  },
  {
    "arxiv_id": "2501.06231v1",
    "title": "Sustainable and Intelligent Public Facility Failure Management System Based on Large Language Models",
    "authors": [
      "Siguo Bi",
      "Jilong Zhang",
      "Wei Ni"
    ],
    "abstract": "This paper presents a new Large Language Model (LLM)-based Smart Device\nManagement framework, a pioneering approach designed to address the intricate\nchallenges of managing intelligent devices within public facilities, with a\nparticular emphasis on applications to libraries. Our framework leverages\nstate-of-the-art LLMs to analyze and predict device failures, thereby enhancing\noperational efficiency and reliability. Through prototype validation in\nreal-world library settings, we demonstrate the framework's practical\napplicability and its capacity to significantly reduce budgetary constraints on\npublic facilities. The advanced and innovative nature of our model is evident\nfrom its successful implementation in prototype testing. We plan to extend the\nframework's scope to include a wider array of public facilities and to\nintegrate it with cutting-edge cybersecurity technologies, such as Internet of\nThings (IoT) security and machine learning algorithms for threat detection and\nresponse. This will result in a comprehensive and proactive maintenance system\nthat not only bolsters the security of intelligent devices but also utilizes\nmachine learning for automated analysis and real-time threat mitigation. By\nincorporating these advanced cybersecurity elements, our framework will be\nwell-positioned to tackle the dynamic challenges of modern public\ninfrastructure, ensuring robust protection against potential threats and\nenabling facilities to anticipate and prevent failures, leading to substantial\ncost savings and enhanced service quality.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06231v1",
    "published_date": "2025-01-08 02:30:37 UTC",
    "updated_date": "2025-01-08 02:30:37 UTC"
  },
  {
    "arxiv_id": "2501.04228v2",
    "title": "Constraints as Rewards: Reinforcement Learning for Robots without Reward Functions",
    "authors": [
      "Yu Ishihara",
      "Noriaki Takasugi",
      "Kotaro Kawakami",
      "Masaya Kinoshita",
      "Kazumi Aoyama"
    ],
    "abstract": "Reinforcement learning has become an essential algorithm for generating\ncomplex robotic behaviors. However, to learn such behaviors, it is necessary to\ndesign a reward function that describes the task, which often consists of\nmultiple objectives that needs to be balanced. This tuning process is known as\nreward engineering and typically involves extensive trial-and-error. In this\npaper, to avoid this trial-and-error process, we propose the concept of\nConstraints as Rewards (CaR). CaR formulates the task objective using multiple\nconstraint functions instead of a reward function and solves a reinforcement\nlearning problem with constraints using the Lagrangian-method. By adopting this\napproach, different objectives are automatically balanced, because Lagrange\nmultipliers serves as the weights among the objectives. In addition, we will\ndemonstrate that constraints, expressed as inequalities, provide an intuitive\ninterpretation of the optimization target designed for the task. We apply the\nproposed method to the standing-up motion generation task of a\nsix-wheeled-telescopic-legged robot and demonstrate that the proposed method\nsuccessfully acquires the target behavior, even though it is challenging to\nlearn with manually designed reward functions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04228v2",
    "published_date": "2025-01-08 01:59:47 UTC",
    "updated_date": "2025-01-09 01:35:56 UTC"
  },
  {
    "arxiv_id": "2501.04227v1",
    "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
    "authors": [
      "Samuel Schmidgall",
      "Yusheng Su",
      "Ze Wang",
      "Ximeng Sun",
      "Jialian Wu",
      "Xiaodong Yu",
      "Jiang Liu",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "abstract": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04227v1",
    "published_date": "2025-01-08 01:58:42 UTC",
    "updated_date": "2025-01-08 01:58:42 UTC"
  },
  {
    "arxiv_id": "2501.04217v1",
    "title": "Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images",
    "authors": [
      "Ren Tasai",
      "Guang Li",
      "Ren Togo",
      "Minghui Tang",
      "Takaaki Yoshimura",
      "Hiroyuki Sugimori",
      "Kenji Hirata",
      "Takahiro Ogawa",
      "Kohsuke Kudo",
      "Miki Haseyama"
    ],
    "abstract": "We propose a novel continual self-supervised learning method (CSSL)\nconsidering medical domain knowledge in chest CT images. Our approach addresses\nthe challenge of sequential learning by effectively capturing the relationship\nbetween previously learned knowledge and new information at different stages.\nBy incorporating an enhanced DER into CSSL and maintaining both diversity and\nrepresentativeness within the rehearsal buffer of DER, the risk of data\ninterference during pretraining is reduced, enabling the model to learn more\nricher and robust feature representations. In addition, we incorporate a mixup\nstrategy and feature distillation to further enhance the model's ability to\nlearn meaningful representations. We validate our method using chest CT images\nobtained under two different imaging conditions, demonstrating superior\nperformance compared to state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04217v1",
    "published_date": "2025-01-08 01:27:35 UTC",
    "updated_date": "2025-01-08 01:27:35 UTC"
  },
  {
    "arxiv_id": "2501.04213v1",
    "title": "UPAQ: A Framework for Real-Time and Energy-Efficient 3D Object Detection in Autonomous Vehicles",
    "authors": [
      "Abhishek Balasubramaniam",
      "Febin P Sunny",
      "Sudeep Pasricha"
    ],
    "abstract": "To enhance perception in autonomous vehicles (AVs), recent efforts are\nconcentrating on 3D object detectors, which deliver more comprehensive\npredictions than traditional 2D object detectors, at the cost of increased\nmemory footprint and computational resource usage. We present a novel framework\ncalled UPAQ, which leverages semi-structured pattern pruning and quantization\nto improve the efficiency of LiDAR point-cloud and camera-based 3D object\ndetectors on resource-constrained embedded AV platforms. Experimental results\non the Jetson Orin Nano embedded platform indicate that UPAQ achieves up to\n5.62x and 5.13x model compression rates, up to 1.97x and 1.86x boost in\ninference speed, and up to 2.07x and 1.87x reduction in energy consumption\ncompared to state-of-the-art model compression frameworks, on the Pointpillar\nand SMOKE models respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04213v1",
    "published_date": "2025-01-08 01:18:14 UTC",
    "updated_date": "2025-01-08 01:18:14 UTC"
  },
  {
    "arxiv_id": "2501.04211v2",
    "title": "CURing Large Models: Compression via CUR Decomposition",
    "authors": [
      "Sanghyeon Park",
      "Soo-Mook Moon"
    ],
    "abstract": "Large deep learning models have achieved remarkable success but are\nresource-intensive, posing challenges such as memory usage. We introduce\nCURing, a novel model compression method based on CUR matrix decomposition,\nwhich approximates weight matrices as the product of selected columns (C) and\nrows (R), and a small linking matrix (U). We apply this decomposition to\nweights chosen based on the combined influence of their magnitudes and\nactivations. By identifying and retaining informative rows and columns, CURing\nsignificantly reduces model size with minimal performance loss. For example, it\nreduces Llama3.1-8B's parameters to 7.32B (-9%) in just 129 seconds, over 20\ntimes faster than prior compression methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04211v2",
    "published_date": "2025-01-08 01:11:17 UTC",
    "updated_date": "2025-01-10 14:36:48 UTC"
  },
  {
    "arxiv_id": "2501.04202v1",
    "title": "Generative Dataset Distillation Based on Self-knowledge Distillation",
    "authors": [
      "Longzhen Li",
      "Guang Li",
      "Ren Togo",
      "Keisuke Maeda",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "Dataset distillation is an effective technique for reducing the cost and\ncomplexity of model training while maintaining performance by compressing large\ndatasets into smaller, more efficient versions. In this paper, we present a\nnovel generative dataset distillation method that can improve the accuracy of\naligning prediction logits. Our approach integrates self-knowledge distillation\nto achieve more precise distribution matching between the synthetic and\noriginal data, thereby capturing the overall structure and relationships within\nthe data. To further improve the accuracy of alignment, we introduce a\nstandardization step on the logits before performing distribution matching,\nensuring consistency in the range of logits. Through extensive experiments, we\ndemonstrate that our method outperforms existing state-of-the-art methods,\nresulting in superior distillation performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04202v1",
    "published_date": "2025-01-08 00:43:31 UTC",
    "updated_date": "2025-01-08 00:43:31 UTC"
  },
  {
    "arxiv_id": "2501.04193v1",
    "title": "GNN-based Decentralized Perception in Multirobot Systems for Predicting Worker Actions",
    "authors": [
      "Ali Imran",
      "Giovanni Beltrame",
      "David St-Onge"
    ],
    "abstract": "In industrial environments, predicting human actions is essential for\nensuring safe and effective collaboration between humans and robots. This paper\nintroduces a perception framework that enables mobile robots to understand and\nshare information about human actions in a decentralized way. The framework\nfirst allows each robot to build a spatial graph representing its surroundings,\nwhich it then shares with other robots. This shared spatial data is combined\nwith temporal information to track human behavior over time. A swarm-inspired\ndecision-making process is used to ensure all robots agree on a unified\ninterpretation of the human's actions. Results show that adding more robots and\nincorporating longer time sequences improve prediction accuracy. Additionally,\nthe consensus mechanism increases system resilience, making the multi-robot\nsetup more reliable in dynamic industrial settings.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to RA-L",
    "pdf_url": "http://arxiv.org/pdf/2501.04193v1",
    "published_date": "2025-01-08 00:06:38 UTC",
    "updated_date": "2025-01-08 00:06:38 UTC"
  }
]
[
  {
    "arxiv_id": "2502.00940v1",
    "title": "An MDP Model for Censoring in Harvesting Sensors: Optimal and Approximated Solutions",
    "authors": [
      "Jesus Fernandez-Bes",
      "Jesus Cid-Sueiro",
      "Antonio G. Marques"
    ],
    "abstract": "In this paper, we propose a novel censoring policy for energy-efficient\ntransmissions in energy-harvesting sensors. The problem is formulated as an\ninfinite-horizon Markov Decision Process (MDP). The objective to be optimized\nis the expected sum of the importance (utility) of all transmitted messages.\nAssuming that such importance can be evaluated at the transmitting node, we\nshow that, under certain conditions on the battery model, the optimal censoring\npolicy is a threshold function on the importance value. Specifically, messages\nare transmitted only if their importance is above a threshold whose value\ndepends on the battery level. Exploiting this property, we propose a\nmodel-based stochastic scheme that approximates the optimal solution, with less\ncomputational complexity and faster convergence speed than a conventional\nQ-learning algorithm. Numerical experiments in single-hop and multi-hop\nnetworks confirm the analytical advantages of the proposed scheme.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00940v1",
    "published_date": "2025-02-02 22:22:21 UTC",
    "updated_date": "2025-02-02 22:22:21 UTC"
  },
  {
    "arxiv_id": "2502.00939v1",
    "title": "Fruit Fly Classification (Diptera: Tephritidae) in Images, Applying Transfer Learning",
    "authors": [
      "Erick Andrew Bustamante Flores",
      "Harley Vera Olivera",
      "Ivan Cesar Medrano Valencia",
      "Carlos Fernando Montoya Cubas"
    ],
    "abstract": "This study develops a transfer learning model for the automated\nclassification of two species of fruit flies, Anastrepha fraterculus and\nCeratitis capitata, in a controlled laboratory environment. The research\naddresses the need to optimize identification and classification, which are\ncurrently performed manually by experts, being affected by human factors and\nfacing time challenges. The methodological process of this study includes the\ncapture of high-quality images using a mobile phone camera and a stereo\nmicroscope, followed by segmentation to reduce size and focus on relevant\nmorphological areas. The images were carefully labeled and preprocessed to\nensure the quality and consistency of the dataset used to train the pre-trained\nconvolutional neural network models VGG16, VGG19, and Inception-v3. The results\nwere evaluated using the F1-score, achieving 82% for VGG16 and VGG19, while\nInception-v3 reached an F1-score of 93%. Inception-v3's reliability was\nverified through model testing in uncontrolled environments, with positive\nresults, complemented by the Grad-CAM technique, demonstrating its ability to\ncapture essential morphological features. These findings indicate that\nInception-v3 is an effective and replicable approach for classifying Anastrepha\nfraterculus and Ceratitis capitata, with potential for implementation in\nautomated monitoring systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T10",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages and 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00939v1",
    "published_date": "2025-02-02 22:16:04 UTC",
    "updated_date": "2025-02-02 22:16:04 UTC"
  },
  {
    "arxiv_id": "2502.00937v2",
    "title": "ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving",
    "authors": [
      "Haoran Qiu",
      "Anish Biswas",
      "Zihan Zhao",
      "Jayashree Mohan",
      "Alind Khare",
      "Esha Choukse",
      "Íñigo Goiri",
      "Zeyu Zhang",
      "Haiying Shen",
      "Chetan Bansal",
      "Ramachandran Ramjee",
      "Rodrigo Fonseca"
    ],
    "abstract": "Large multimodal models (LMMs) demonstrate impressive capabilities in\nunderstanding images, videos, and audio beyond text. However, efficiently\nserving LMMs in production environments poses significant challenges due to\ntheir complex architectures and heterogeneous characteristics across their\nmulti-stage inference pipelines. We present the first comprehensive systems\nanalysis of two prominent LMM architectures, decoder-only and cross-attention,\nacross six representative open-source models, revealing key systems design\nimplications. We also present an in-depth analysis of production LMM inference\ntraces, uncovering unique workload characteristics, including variable,\nheavy-tailed request distributions and bursty traffic patterns. Based on these\ninsights, we propose ModServe, a modular LMM serving system that decouples\nstages for independent optimization and adaptive scaling. ModServe dynamically\nreconfigures stages and handles bursty traffic with modality-aware scheduling\nand autoscaling to meet tail latency SLOs while minimizing costs. ModServe\nachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while\nmeeting SLOs on a 128-GPU cluster with production traces.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00937v2",
    "published_date": "2025-02-02 22:10:40 UTC",
    "updated_date": "2025-03-21 16:53:47 UTC"
  },
  {
    "arxiv_id": "2502.00919v1",
    "title": "Attention Sinks and Outlier Features: A 'Catch, Tag, and Release' Mechanism for Embeddings",
    "authors": [
      "Stephen Zhang",
      "Mustafa Khan",
      "Vardan Papyan"
    ],
    "abstract": "Two prominent features of large language models (LLMs) is the presence of\nlarge-norm (outlier) features and the tendency for tokens to attend very\nstrongly to a select few tokens. Despite often having no semantic relevance,\nthese select tokens, called attention sinks, along with the large outlier\nfeatures, have proven important for model performance, compression, and\nstreaming. Consequently, investigating the roles of these phenomena within\nmodels and exploring how they might manifest in the model parameters has become\nan area of active interest. Through an empirical investigation, we demonstrate\nthat attention sinks utilize outlier features to: catch a sequence of tokens,\ntag the captured tokens by applying a common perturbation, and then release the\ntokens back into the residual stream, where the tagged tokens are eventually\nretrieved. We prove that simple tasks, like averaging, necessitate the 'catch,\ntag, release' mechanism hence explaining why it would arise organically in\nmodern LLMs. Our experiments also show that the creation of attention sinks can\nbe completely captured in the model parameters using low-rank matrices, which\nhas important implications for model compression and substantiates the success\nof recent approaches that incorporate a low-rank term to offset performance\ndegradation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00919v1",
    "published_date": "2025-02-02 21:15:07 UTC",
    "updated_date": "2025-02-02 21:15:07 UTC"
  },
  {
    "arxiv_id": "2502.00903v2",
    "title": "Embracing Dialectic Intersubjectivity: Coordination of Different Perspectives in Content Analysis with LLM Persona Simulation",
    "authors": [
      "Taewoo Kang",
      "Kjerstin Thorson",
      "Tai-Quan Peng",
      "Dan Hiaeshutter-Rice",
      "Sanguk Lee",
      "Stuart Soroka"
    ],
    "abstract": "This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00903v2",
    "published_date": "2025-02-02 20:29:10 UTC",
    "updated_date": "2025-02-04 16:15:45 UTC"
  },
  {
    "arxiv_id": "2502.00894v1",
    "title": "MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for Efficient LLM Training Across Morphologies",
    "authors": [
      "Ehsaneddin Asgari",
      "Yassine El Kheir",
      "Mohammad Ali Sadraei Javaheri"
    ],
    "abstract": "Tokenization is fundamental to Natural Language Processing (NLP), directly\nimpacting model efficiency and linguistic fidelity. While Byte Pair Encoding\n(BPE) is widely used in Large Language Models (LLMs), it often disregards\nmorpheme boundaries, leading to suboptimal segmentation, particularly in\nmorphologically rich languages. We introduce MorphBPE, a morphology-aware\nextension of BPE that integrates linguistic structure into subword tokenization\nwhile preserving statistical efficiency. Additionally, we propose two\nmorphology-based evaluation metrics: (i) Morphological Consistency F1-Score,\nwhich quantifies the consistency between morpheme sharing and token sharing,\ncontributing to LLM training convergence, and (ii) Morphological Edit Distance,\nwhich measures alignment between morphemes and tokens concerning\ninterpretability. Experiments on English, Russian, Hungarian, and Arabic across\n300M and 1B parameter LLMs demonstrate that MorphBPE consistently reduces\ncross-entropy loss, accelerates convergence, and improves morphological\nalignment scores. Fully compatible with existing LLM pipelines, MorphBPE\nrequires minimal modifications for integration. The MorphBPE codebase and\ntokenizer playground will be available at:\nhttps://github.com/llm-lab-org/MorphBPE and https://tokenizer.llm-lab.org",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00894v1",
    "published_date": "2025-02-02 20:06:39 UTC",
    "updated_date": "2025-02-02 20:06:39 UTC"
  },
  {
    "arxiv_id": "2502.00874v1",
    "title": "Paper Copilot: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process",
    "authors": [
      "Jing Yang"
    ],
    "abstract": "The rapid growth of submissions to top-tier Artificial Intelligence (AI) and\nMachine Learning (ML) conferences has prompted many venues to transition from\nclosed to open review platforms. Some have fully embraced open peer reviews,\nallowing public visibility throughout the process, while others adopt hybrid\napproaches, such as releasing reviews only after final decisions or keeping\nreviews private despite using open peer review systems. In this work, we\nanalyze the strengths and limitations of these models, highlighting the growing\ncommunity interest in transparent peer review. To support this discussion, we\nexamine insights from Paper Copilot, a website launched two years ago to\naggregate and analyze AI / ML conference data while engaging a global audience.\nThe site has attracted over 200,000 early-career researchers, particularly\nthose aged 18-34 from 177 countries, many of whom are actively engaged in the\npeer review process. Drawing on our findings, this position paper advocates for\na more transparent, open, and well-regulated peer review aiming to foster\ngreater community involvement and propel advancements in the field.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00874v1",
    "published_date": "2025-02-02 18:58:08 UTC",
    "updated_date": "2025-02-02 18:58:08 UTC"
  },
  {
    "arxiv_id": "2502.00873v1",
    "title": "Language Models Use Trigonometry to Do Addition",
    "authors": [
      "Subhash Kantamneni",
      "Max Tegmark"
    ],
    "abstract": "Mathematical reasoning is an increasingly important indicator of large\nlanguage model (LLM) capabilities, yet we lack understanding of how LLMs\nprocess even simple mathematical tasks. To address this, we reverse engineer\nhow three mid-sized LLMs compute addition. We first discover that numbers are\nrepresented in these LLMs as a generalized helix, which is strongly causally\nimplicated for the tasks of addition and subtraction, and is also causally\nrelevant for integer division, multiplication, and modular arithmetic. We then\npropose that LLMs compute addition by manipulating this generalized helix using\nthe \"Clock\" algorithm: to solve $a+b$, the helices for $a$ and $b$ are\nmanipulated to produce the $a+b$ answer helix which is then read out to model\nlogits. We model influential MLP outputs, attention head outputs, and even\nindividual neuron preactivations with these helices and verify our\nunderstanding with causal interventions. By demonstrating that LLMs represent\nnumbers on a helix and manipulate this helix to perform addition, we present\nthe first representation-level explanation of an LLM's mathematical capability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00873v1",
    "published_date": "2025-02-02 18:55:26 UTC",
    "updated_date": "2025-02-02 18:55:26 UTC"
  },
  {
    "arxiv_id": "2502.00870v1",
    "title": "FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation",
    "authors": [
      "Wenzheng Jiang",
      "Ji Wang",
      "Xiongtao Zhang",
      "Weidong Bao",
      "Cheston Tan",
      "Flint Xiaofeng Fan"
    ],
    "abstract": "Federated Reinforcement Learning (FedRL) improves sample efficiency while\npreserving privacy; however, most existing studies assume homogeneous agents,\nlimiting its applicability in real-world scenarios. This paper investigates\nFedRL in black-box settings with heterogeneous agents, where each agent employs\ndistinct policy networks and training configurations without disclosing their\ninternal details. Knowledge Distillation (KD) is a promising method for\nfacilitating knowledge sharing among heterogeneous models, but it faces\nchallenges related to the scarcity of public datasets and limitations in\nknowledge representation when applied to FedRL. To address these challenges, we\npropose Federated Heterogeneous Policy Distillation (FedHPD), which solves the\nproblem of heterogeneous FedRL by utilizing action probability distributions as\na medium for knowledge sharing. We provide a theoretical analysis of FedHPD's\nconvergence under standard assumptions. Extensive experiments corroborate that\nFedHPD shows significant improvements across various reinforcement learning\nbenchmark tasks, further validating our theoretical findings. Moreover,\nadditional experiments demonstrate that FedHPD operates effectively without the\nneed for an elaborate selection of public datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "This preprint presents the full version of the Extended Abstract\n  accepted by AAMAS 2025, including all the proofs and experiments",
    "pdf_url": "http://arxiv.org/pdf/2502.00870v1",
    "published_date": "2025-02-02 18:44:08 UTC",
    "updated_date": "2025-02-02 18:44:08 UTC"
  },
  {
    "arxiv_id": "2502.01694v2",
    "title": "Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation",
    "authors": [
      "Juno Kim",
      "Denny Wu",
      "Jason Lee",
      "Taiji Suzuki"
    ],
    "abstract": "A key paradigm to improve the reasoning capabilities of large language models\n(LLMs) is to allocate more inference-time compute to search against a verifier\nor reward model. This process can then be utilized to refine the pretrained\nmodel or distill its reasoning patterns into more efficient models. In this\npaper, we study inference-time compute by viewing chain-of-thought (CoT)\ngeneration as a metastable Markov process: easy reasoning steps (e.g.,\nalgebraic manipulations) form densely connected clusters, while hard reasoning\nsteps (e.g., applying a relevant theorem) create sparse, low-probability edges\nbetween clusters, leading to phase transitions at longer timescales. Under this\nframework, we prove that implementing a search protocol that rewards sparse\nedges improves CoT by decreasing the expected number of steps to reach\ndifferent clusters. In contrast, we establish a limit on reasoning capability\nwhen the model is restricted to local information of the pretrained graph. We\nalso show that the information gained by search can be utilized to obtain a\nbetter reasoning model: (1) the pretrained model can be directly finetuned to\nfavor sparse edges via policy gradient methods, and moreover (2) a compressed\nmetastable representation of the reasoning dynamics can be distilled into a\nsmaller, more efficient model.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "55 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.01694v2",
    "published_date": "2025-02-02 18:19:14 UTC",
    "updated_date": "2025-03-01 10:27:24 UTC"
  },
  {
    "arxiv_id": "2503.04734v1",
    "title": "What can large language models do for sustainable food?",
    "authors": [
      "Anna T. Thomas",
      "Adam Yee",
      "Andrew Mayne",
      "Maya B. Mathur",
      "Dan Jurafsky",
      "Kristina Gligorić"
    ],
    "abstract": "Food systems are responsible for a third of human-caused greenhouse gas\nemissions. We investigate what Large Language Models (LLMs) can contribute to\nreducing the environmental impacts of food production. We define a typology of\ndesign and prediction tasks based on the sustainable food literature and\ncollaboration with domain experts, and evaluate six LLMs on four tasks in our\ntypology. For example, for a sustainable protein design task, food science\nexperts estimated that collaboration with an LLM can reduce time spent by 45%\non average, compared to 22% for collaboration with another expert human food\nscientist. However, for a sustainable menu design task, LLMs produce suboptimal\nsolutions when instructed to consider both human satisfaction and climate\nimpacts. We propose a general framework for integrating LLMs with combinatorial\noptimization to improve reasoning capabilities. Our approach decreases\nemissions of food choices by 79% in a hypothetical restaurant while maintaining\nparticipants' satisfaction with their set of choices. Our results demonstrate\nLLMs' potential, supported by optimization techniques, to accelerate\nsustainable food development and adoption.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04734v1",
    "published_date": "2025-02-02 18:12:16 UTC",
    "updated_date": "2025-02-02 18:12:16 UTC"
  },
  {
    "arxiv_id": "2502.00865v2",
    "title": "Predicting potentially abusive clauses in Chilean terms of services with natural language processing",
    "authors": [
      "Christoffer Loeffler",
      "Andrea Martínez Freile",
      "Tomás Rey Pizarro"
    ],
    "abstract": "This study addresses the growing concern of information asymmetry in consumer\ncontracts, exacerbated by the proliferation of online services with complex\nTerms of Service that are rarely even read. Even though research on automatic\nanalysis methods is conducted, the problem is aggravated by the general focus\non English-language Machine Learning approaches and on major jurisdictions,\nsuch as the European Union. We introduce a new methodology and a substantial\ndataset addressing this gap. We propose a novel annotation scheme with four\ncategories and a total of 20 classes, and apply it on 50 online Terms of\nService used in Chile. Our evaluation of transformer-based models highlights\nhow factors like language- and/or domain-specific pre-training, few-shot sample\nsize, and model architecture affect the detection and classification of\npotentially abusive clauses. Results show a large variability in performance\nfor the different tasks and models, with the highest macro-F1 scores for the\ndetection task ranging from 79% to 89% and micro-F1 scores up to 96%, while\nmacro-F1 scores for the classification task range from 60% to 70% and micro-F1\nscores from 64% to 80%. Notably, this is the first Spanish-language multi-label\nclassification dataset for legal clauses, applying Chilean law and offering a\ncomprehensive evaluation of Spanish-language models in the legal domain. Our\nwork lays the ground for future research in method development for rarely\nconsidered legal analysis and potentially leads to practical applications to\nsupport consumers in Chile and Latin America as a whole.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "39 pages, 2 figures, 8 tables, accepted for publication",
    "pdf_url": "http://arxiv.org/pdf/2502.00865v2",
    "published_date": "2025-02-02 18:01:39 UTC",
    "updated_date": "2025-05-05 18:02:07 UTC"
  },
  {
    "arxiv_id": "2502.01693v2",
    "title": "Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks",
    "authors": [
      "Priodyuti Pradhan",
      "Amit Reza"
    ],
    "abstract": "In complex systems, information propagation can be defined as diffused or\ndelocalized, weakly localized, and strongly localized. This study investigates\nthe application of graph neural network models to learn the behavior of a\nlinear dynamical system on networks. A graph convolution and attention-based\nneural network framework has been developed to identify the steady-state\nbehavior of the linear dynamical system. We reveal that our trained model\ndistinguishes the different states with high accuracy. Furthermore, we have\nevaluated model performance with real-world data. In addition, to understand\nthe explainability of our model, we provide an analytical derivation for the\nforward and backward propagation of our framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "nlin.AO"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.01693v2",
    "published_date": "2025-02-02 17:29:10 UTC",
    "updated_date": "2025-02-07 17:40:28 UTC"
  },
  {
    "arxiv_id": "2502.01692v5",
    "title": "Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation",
    "authors": [
      "Kim Yong Tan",
      "Yueming Lyu",
      "Ivor Tsang",
      "Yew-Soon Ong"
    ],
    "abstract": "Guided diffusion-model generation is a promising direction for customizing\nthe generation process of a pre-trained diffusion model to address specific\ndownstream tasks. Existing guided diffusion models either rely on training the\nguidance model with pre-collected datasets or require the objective functions\nto be differentiable. However, for most real-world tasks, offline datasets are\noften unavailable, and their objective functions are often not differentiable,\nsuch as image generation with human preferences, molecular generation for drug\ndiscovery, and material design. Thus, we need an $\\textbf{online}$ algorithm\ncapable of collecting data during runtime and supporting a $\\textbf{black-box}$\nobjective function. Moreover, the $\\textbf{query efficiency}$ of the algorithm\nis also critical because the objective evaluation of the query is often\nexpensive in real-world scenarios. In this work, we propose a novel and simple\nalgorithm, $\\textbf{Fast Direct}$, for query-efficient online black-box target\ngeneration. Our Fast Direct builds a pseudo-target on the data manifold to\nupdate the noise sequence of the diffusion model with a universal direction,\nwhich is promising to perform query-efficient guided generation. Extensive\nexperiments on twelve high-resolution ($\\small {1024 \\times 1024}$) image\ntarget generation tasks and six 3D-molecule target generation tasks show\n$\\textbf{6}\\times$ up to $\\textbf{10}\\times$ query efficiency improvement and\n$\\textbf{11}\\times$ up to $\\textbf{44}\\times$ query efficiency improvement,\nrespectively. Our implementation is publicly available at:\nhttps://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01692v5",
    "published_date": "2025-02-02 17:21:10 UTC",
    "updated_date": "2025-03-29 05:45:56 UTC"
  },
  {
    "arxiv_id": "2502.00858v2",
    "title": "Learning to Plan with Personalized Preferences",
    "authors": [
      "Manjie Xu",
      "Xinyi Yang",
      "Wei Liang",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "abstract": "Effective integration of AI agents into daily life requires them to\nunderstand and adapt to individual human preferences, particularly in\ncollaborative roles. Although recent studies on embodied intelligence have\nadvanced significantly, they typically adopt generalized approaches that\noverlook personal preferences in planning. We address this limitation by\ndeveloping agents that not only learn preferences from few demonstrations but\nalso learn to adapt their planning strategies based on these preferences. Our\nresearch leverages the observation that preferences, though implicitly\nexpressed through minimal demonstrations, can generalize across diverse\nplanning scenarios. To systematically evaluate this hypothesis, we introduce\nPreference-based Planning (PbP) benchmark, an embodied benchmark featuring\nhundreds of diverse preferences spanning from atomic actions to complex\nsequences. Our evaluation of SOTA methods reveals that while symbol-based\napproaches show promise in scalability, significant challenges remain in\nlearning to generate and execute plans that satisfy personalized preferences.\nWe further demonstrate that incorporating learned preferences as intermediate\nrepresentations in planning significantly improves the agent's ability to\nconstruct personalized plans. These findings establish preferences as a\nvaluable abstraction layer for adaptive planning, opening new directions for\nresearch in preference-guided plan generation and execution.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00858v2",
    "published_date": "2025-02-02 17:16:25 UTC",
    "updated_date": "2025-03-11 15:22:58 UTC"
  },
  {
    "arxiv_id": "2502.00855v1",
    "title": "Psychometric-Based Evaluation for Theorem Proving with Large Language Models",
    "authors": [
      "Jianyu Zhang",
      "Yongwang Zhao",
      "Long Zhang",
      "Jilin Hu",
      "Xiaokun Luan",
      "Zhiwei Xu",
      "Feng Yang"
    ],
    "abstract": "Large language models (LLMs) for formal theorem proving have become a\nprominent research focus. At present, the proving ability of these LLMs is\nmainly evaluated through proof pass rates on datasets such as miniF2F. However,\nthis evaluation method overlooks the varying importance of theorems. As a\nresult, it fails to highlight the real performance disparities between LLMs and\nleads to high evaluation costs. This study proposes a psychometric-based\nevaluation method for theorem proving with LLMs, comprising two main\ncomponents: Dataset Annotation and Adaptive Evaluation. First, we propose a\nmetric calculation method to annotate the dataset with difficulty and\ndiscrimination metrics. Specifically, we annotate each theorem in the miniF2F\ndataset and grade them into varying difficulty levels according to the\nperformance of LLMs, resulting in an enhanced dataset: miniF2F-Graded.\nExperimental results show that the difficulty grading in miniF2F-Graded better\nreflects the theorem difficulty perceived by LLMs. Secondly, we design an\nadaptive evaluation method to dynamically select the most suitable theorems for\ntesting based on the annotated metrics and the real-time performance of LLMs.\nWe apply this method to evaluate 10 LLMs. The results show that our method\nfinely highlights the performance disparities between LLMs. It also reduces\nevaluation costs by using only 23% of the theorems in the dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00855v1",
    "published_date": "2025-02-02 17:00:22 UTC",
    "updated_date": "2025-02-02 17:00:22 UTC"
  },
  {
    "arxiv_id": "2502.01691v1",
    "title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model",
    "authors": [
      "Hadas Ben-Atya",
      "Naama Gavrielov",
      "Zvi Badash",
      "Gili Focht",
      "Ruth Cytter-Kuint",
      "Talar Hagopian",
      "Dan Turner",
      "Moti Freiman"
    ],
    "abstract": "Reliable extraction of structured data from radiology reports using Large\nLanguage Models (LLMs) remains challenging, especially for complex, non-English\ntexts like Hebrew. This study introduces an agent-based uncertainty-aware\napproach to improve the trustworthiness of LLM predictions in medical\napplications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease\npatients (from 2010 to 2023) across three medical centers. A subset of 512\nreports was manually annotated for six gastrointestinal organs and 15\npathological findings, while the remaining reports were automatically annotated\nusing HSMP-BERT. Structured data extraction was performed using Llama 3.1\n(Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed\nsix semantically equivalent prompts to estimate uncertainty. An Agent-Based\nDecision Model integrated multiple prompt outputs into five confidence levels\nfor calibrated uncertainty and was compared against three entropy-based models.\nPerformance was evaluated using accuracy, F1 score, precision, recall, and\nCohen's Kappa before and after filtering high-uncertainty cases. The\nagent-based model outperformed the baseline across all metrics, achieving an F1\nscore of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering\nhigh-uncertainty cases (greater than or equal to 0.5), the F1 score improved to\n0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated\nclear separation between correct and incorrect predictions, with the\nagent-based model providing the most well-calibrated uncertainty estimates. By\nincorporating uncertainty-aware prompt ensembles and an agent-based decision\nmodel, this approach enhances the performance and reliability of LLMs in\nstructured data extraction from radiology reports, offering a more\ninterpretable and trustworthy solution for high-stakes medical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01691v1",
    "published_date": "2025-02-02 16:57:03 UTC",
    "updated_date": "2025-02-02 16:57:03 UTC"
  },
  {
    "arxiv_id": "2502.00850v2",
    "title": "Dual Alignment Maximin Optimization for Offline Model-based RL",
    "authors": [
      "Chi Zhou",
      "Wang Luo",
      "Haoran Li",
      "Congying Han",
      "Tiande Guo",
      "Zicheng Zhang"
    ],
    "abstract": "Offline reinforcement learning agents face significant deployment challenges\ndue to the synthetic-to-real distribution mismatch. While most prior research\nhas focused on improving the fidelity of synthetic sampling and incorporating\noff-policy mechanisms, the directly integrated paradigm often fails to ensure\nconsistent policy behavior in biased models and underlying environmental\ndynamics, which inherently arise from discrepancies between behavior and\nlearning policies. In this paper, we first shift the focus from model\nreliability to policy discrepancies while optimizing for expected returns, and\nthen self-consistently incorporate synthetic data, deriving a novel\nactor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a\nunified framework to ensure both model-environment policy consistency and\nsynthetic and offline data compatibility. The inner minimization performs dual\nconservative value estimation, aligning policies and trajectories to avoid\nout-of-distribution states and actions, while the outer maximization ensures\nthat policy improvements remain consistent with inner value estimates.\nEmpirical evaluations demonstrate that DAMO effectively ensures model and\npolicy alignments, achieving competitive performance across diverse benchmark\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00850v2",
    "published_date": "2025-02-02 16:47:35 UTC",
    "updated_date": "2025-05-10 04:42:40 UTC"
  },
  {
    "arxiv_id": "2502.00847v1",
    "title": "SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models",
    "authors": [
      "Jiawen Zhang",
      "Kejia Chen",
      "Zunlei Feng",
      "Jian Lou",
      "Mingli Song",
      "Jian Liu",
      "Xiaohu Yang"
    ],
    "abstract": "With the growing popularity of LLMs among the general public users,\nprivacy-preserving and adversarial robustness have become two pressing demands\nfor LLM-based services, which have largely been pursued separately but rarely\njointly. In this paper, to the best of our knowledge, we are among the first\nattempts towards robust and private LLM inference by tightly integrating two\ndisconnected fields: private inference and prompt ensembling. The former\nprotects users' privacy by encrypting inference data transmitted and processed\nby LLMs, while the latter enhances adversarial robustness by yielding an\naggregated output from multiple prompted LLM responses. Although widely\nrecognized as effective individually, private inference for prompt ensembling\ntogether entails new challenges that render the naive combination of existing\ntechniques inefficient. To overcome the hurdles, we propose SecPE, which\ndesigns efficient fully homomorphic encryption (FHE) counterparts for the core\nalgorithmic building blocks of prompt ensembling. We conduct extensive\nexperiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of\nSecPE. The results show that SecPE maintains high clean accuracy and offers\nbetter robustness at the expense of merely $2.5\\%$ efficiency overhead compared\nto baseline private inference methods, indicating a satisfactory\n``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the\nencrypted Argmax operation that incurs major slowdown for prompt ensembling,\nSecPE is 35.4x faster than the state-of-the-art peers, which can be of\nindependent interest beyond this work.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00847v1",
    "published_date": "2025-02-02 16:40:21 UTC",
    "updated_date": "2025-02-02 16:40:21 UTC"
  },
  {
    "arxiv_id": "2502.00840v1",
    "title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense",
    "authors": [
      "Jiawen Zhang",
      "Kejia Chen",
      "Lipeng He",
      "Jian Lou",
      "Dan Li",
      "Zunlei Feng",
      "Mingli Song",
      "Jian Liu",
      "Kui Ren",
      "Xiaohu Yang"
    ],
    "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across\nvarious domains. Accompanying the evolving capabilities and expanding\ndeployment scenarios of LLMs, their deployment challenges escalate due to their\nsheer scale and the advanced yet complex activation designs prevalent in\nnotable model series, such as Llama, Gemma, and Mistral. These challenges have\nbecome particularly pronounced in resource-constrained deployment scenarios,\nwhere mitigating inference efficiency bottlenecks is imperative. Among various\nrecent efforts, activation approximation has emerged as a promising avenue for\npursuing inference efficiency, sometimes considered indispensable in\napplications such as private inference. Despite achieving substantial speedups\nwith minimal impact on utility, even appearing sound and practical for\nreal-world deployment, the safety implications of activation approximations\nremain unclear. In this work, we fill this critical gap in LLM safety by\nconducting the first systematic safety evaluation of activation approximations.\nOur safety vetting spans seven sota techniques across three popular categories,\nrevealing consistent safety degradation across ten safety-aligned LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.00840v1",
    "published_date": "2025-02-02 16:25:48 UTC",
    "updated_date": "2025-02-02 16:25:48 UTC"
  },
  {
    "arxiv_id": "2502.00837v1",
    "title": "Explainability in Practice: A Survey of Explainable NLP Across Various Domains",
    "authors": [
      "Hadi Mohammadi",
      "Ayoub Bagheri",
      "Anastasia Giachanou",
      "Daniel L. Oberski"
    ],
    "abstract": "Natural Language Processing (NLP) has become a cornerstone in many critical\nsectors, including healthcare, finance, and customer relationship management.\nThis is especially true with the development and use of advanced models such as\nGPT-based architectures and BERT, which are widely used in decision-making\nprocesses. However, the black-box nature of these advanced NLP models has\ncreated an urgent need for transparency and explainability. This review\nexplores explainable NLP (XNLP) with a focus on its practical deployment and\nreal-world applications, examining its implementation and the challenges faced\nin domain-specific contexts. The paper underscores the importance of\nexplainability in NLP and provides a comprehensive perspective on how XNLP can\nbe designed to meet the unique demands of various sectors, from healthcare's\nneed for clear insights to finance's emphasis on fraud detection and risk\nassessment. Additionally, this review aims to bridge the knowledge gap in XNLP\nliterature by offering a domain-specific exploration and discussing\nunderrepresented areas such as real-world applicability, metric evaluation, and\nthe role of human interaction in model assessment. The paper concludes by\nsuggesting future research directions that could enhance the understanding and\nbroader application of XNLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00837v1",
    "published_date": "2025-02-02 16:18:44 UTC",
    "updated_date": "2025-02-02 16:18:44 UTC"
  },
  {
    "arxiv_id": "2502.00828v1",
    "title": "Decision-informed Neural Networks with Large Language Model Integration for Portfolio Optimization",
    "authors": [
      "Yoontae Hwang",
      "Yaxuan Kong",
      "Stefan Zohren",
      "Yongjae Lee"
    ],
    "abstract": "This paper addresses the critical disconnect between prediction and decision\nquality in portfolio optimization by integrating Large Language Models (LLMs)\nwith decision-focused learning. We demonstrate both theoretically and\nempirically that minimizing the prediction error alone leads to suboptimal\nportfolio decisions. We aim to exploit the representational power of LLMs for\ninvestment decisions. An attention mechanism processes asset relationships,\ntemporal dependencies, and macro variables, which are then directly integrated\ninto a portfolio optimization layer. This enables the model to capture complex\nmarket dynamics and align predictions with the decision objectives. Extensive\nexperiments on S\\&P100 and DOW30 datasets show that our model consistently\noutperforms state-of-the-art deep learning models. In addition, gradient-based\nanalyses show that our model prioritizes the assets most crucial to decision\nmaking, thus mitigating the effects of prediction errors on portfolio\nperformance. These findings underscore the value of integrating decision\nobjectives into predictions for more robust and context-aware portfolio\nmanagement.",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "q-fin.PM",
    "comment": "Submitted paper",
    "pdf_url": "http://arxiv.org/pdf/2502.00828v1",
    "published_date": "2025-02-02 15:45:21 UTC",
    "updated_date": "2025-02-02 15:45:21 UTC"
  },
  {
    "arxiv_id": "2502.01689v1",
    "title": "scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological Profiling",
    "authors": [
      "Yu-An Huang",
      "Xiyue Cao",
      "Zhu-Hong You",
      "Yue-Chao Li",
      "Xuequn Shang",
      "Zhi-An Huang"
    ],
    "abstract": "The rise of single-cell sequencing technologies has revolutionized the\nexploration of drug resistance, revealing the crucial role of cellular\nheterogeneity in advancing precision medicine. By building computational models\nfrom existing single-cell drug response data, we can rapidly annotate cellular\nresponses to drugs in subsequent trials. To this end, we developed scGSDR, a\nmodel that integrates two computational pipelines grounded in the knowledge of\ncellular states and gene signaling pathways, both essential for understanding\nbiological gene semantics. scGSDR enhances predictive performance by\nincorporating gene semantics and employs an interpretability module to identify\nkey pathways contributing to drug resistance phenotypes. Our extensive\nvalidation, which included 16 experiments covering 11 drugs, demonstrates\nscGSDR's superior predictive accuracy, when trained with either bulk-seq or\nscRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's\napplication has extended from single-drug predictions to scenarios involving\ndrug combinations. Leveraging pathways of known drug target genes, we found\nthat scGSDR's cell-pathway attention scores are biologically interpretable,\nwhich helped us identify other potential drug-related genes. Literature review\nof top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,\nand PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for\nPaclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating\ngene semantics, enhances predictive modeling of cellular responses to diverse\ndrugs, proving invaluable for scenarios involving both single drug and\ncombination therapies and effectively identifying key resistance-related\npathways, thus advancing precision medicine and targeted therapy development.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01689v1",
    "published_date": "2025-02-02 15:43:20 UTC",
    "updated_date": "2025-02-02 15:43:20 UTC"
  },
  {
    "arxiv_id": "2502.00802v1",
    "title": "Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement Learning",
    "authors": [
      "Massimiliano Falzari",
      "Matthia Sabatelli"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) systems often tend to overfit to early\nexperiences, a phenomenon known as the primacy bias (PB). This bias can\nseverely hinder learning efficiency and final performance, particularly in\ncomplex environments. This paper presents a comprehensive investigation of PB\nthrough the lens of the Fisher Information Matrix (FIM). We develop a framework\ncharacterizing PB through distinct patterns in the FIM trace, identifying\ncritical memorization and reorganization phases during learning. Building on\nthis understanding, we propose Fisher-Guided Selective Forgetting (FGSF), a\nnovel method that leverages the geometric structure of the parameter space to\nselectively modify network weights, preventing early experiences from\ndominating the learning process. Empirical results across DeepMind Control\nSuite (DMC) environments show that FGSF consistently outperforms baselines,\nparticularly in complex tasks. We analyze the different impacts of PB on actor\nand critic networks, the role of replay ratios in exacerbating the effect, and\nthe effectiveness of even simple noise injection methods. Our findings provide\na deeper understanding of PB and practical mitigation strategies, offering a\nFIM-based geometric perspective for advancing DRL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00802v1",
    "published_date": "2025-02-02 13:54:47 UTC",
    "updated_date": "2025-02-02 13:54:47 UTC"
  },
  {
    "arxiv_id": "2502.00801v1",
    "title": "Environment-Driven Online LiDAR-Camera Extrinsic Calibration",
    "authors": [
      "Zhiwei Huang",
      "Jiaqi Li",
      "Ping Zhong",
      "Rui Fan"
    ],
    "abstract": "LiDAR-camera extrinsic calibration (LCEC) is the core for data fusion in\ncomputer vision. Existing methods typically rely on customized calibration\ntargets or fixed scene types, lacking the flexibility to handle variations in\nsensor data and environmental contexts. This paper introduces EdO-LCEC, the\nfirst environment-driven, online calibration approach that achieves human-like\nadaptability. Inspired by the human perceptual system, EdO-LCEC incorporates a\ngeneralizable scene discriminator to actively interpret environmental\nconditions, creating multiple virtual cameras that capture detailed spatial and\ntextural information. To overcome cross-modal feature matching challenges\nbetween LiDAR and camera, we propose dual-path correspondence matching (DPCM),\nwhich leverages both structural and textural consistency to achieve reliable\n3D-2D correspondences. Our approach formulates the calibration process as a\nspatial-temporal joint optimization problem, utilizing global constraints from\nmultiple views and scenes to improve accuracy, particularly in sparse or\npartially overlapping sensor views. Extensive experiments on real-world\ndatasets demonstrate that EdO-LCEC achieves state-of-the-art performance,\nproviding reliable and precise calibration across diverse, challenging\nenvironments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00801v1",
    "published_date": "2025-02-02 13:52:35 UTC",
    "updated_date": "2025-02-02 13:52:35 UTC"
  },
  {
    "arxiv_id": "2502.00792v1",
    "title": "RTBAgent: A LLM-based Agent System for Real-Time Bidding",
    "authors": [
      "Leng Cai",
      "Junxuan He",
      "Yikai Li",
      "Junjie Liang",
      "Yuanping Lin",
      "Ziming Quan",
      "Yawen Zeng",
      "Jin Xu"
    ],
    "abstract": "Real-Time Bidding (RTB) enables advertisers to place competitive bids on\nimpression opportunities instantaneously, striving for cost-effectiveness in a\nhighly competitive landscape. Although RTB has widely benefited from the\nutilization of technologies such as deep learning and reinforcement learning,\nthe reliability of related methods often encounters challenges due to the\ndiscrepancies between online and offline environments and the rapid\nfluctuations of online bidding. To handle these challenges, RTBAgent is\nproposed as the first RTB agent system based on large language models (LLMs),\nwhich synchronizes real competitive advertising bidding environments and\nobtains bidding prices through an integrated decision-making process.\nSpecifically, obtaining reasoning ability through LLMs, RTBAgent is further\ntailored to be more professional for RTB via involved auxiliary modules, i.e.,\nclick-through rate estimation model, expert strategy knowledge, and daily\nreflection. In addition, we propose a two-step decision-making process and\nmulti-memory retrieval mechanism, which enables RTBAgent to review historical\ndecisions and transaction records and subsequently make decisions more adaptive\nto market changes in real-time bidding. Empirical testing with real advertising\ndatasets demonstrates that RTBAgent significantly enhances profitability. The\nRTBAgent code will be publicly accessible at:\nhttps://github.com/CaiLeng/RTBAgent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by WWW 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.00792v1",
    "published_date": "2025-02-02 13:10:15 UTC",
    "updated_date": "2025-02-02 13:10:15 UTC"
  },
  {
    "arxiv_id": "2502.00779v1",
    "title": "Role of Mixup in Topological Persistence Based Knowledge Distillation for Wearable Sensor Data",
    "authors": [
      "Eun Som Jeon",
      "Hongjun Choi",
      "Matthew P. Buman",
      "Pavan Turaga"
    ],
    "abstract": "The analysis of wearable sensor data has enabled many successes in several\napplications. To represent the high-sampling rate time-series with sufficient\ndetail, the use of topological data analysis (TDA) has been considered, and it\nis found that TDA can complement other time-series features. Nonetheless, due\nto the large time consumption and high computational resource requirements of\nextracting topological features through TDA, it is difficult to deploy\ntopological knowledge in various applications. To tackle this problem,\nknowledge distillation (KD) can be adopted, which is a technique facilitating\nmodel compression and transfer learning to generate a smaller model by\ntransferring knowledge from a larger network. By leveraging multiple teachers\nin KD, both time-series and topological features can be transferred, and\nfinally, a superior student using only time-series data is distilled. On the\nother hand, mixup has been popularly used as a robust data augmentation\ntechnique to enhance model performance during training. Mixup and KD employ\nsimilar learning strategies. In KD, the student model learns from the smoothed\ndistribution generated by the teacher model, while mixup creates smoothed\nlabels by blending two labels. Hence, this common smoothness serves as the\nconnecting link that establishes a connection between these two methods. In\nthis paper, we analyze the role of mixup in KD with time-series as well as\ntopological persistence, employing multiple teachers. We present a\ncomprehensive analysis of various methods in KD and mixup on wearable sensor\ndata.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE Sensors Journal (2024)",
    "pdf_url": "http://arxiv.org/pdf/2502.00779v1",
    "published_date": "2025-02-02 12:33:52 UTC",
    "updated_date": "2025-02-02 12:33:52 UTC"
  },
  {
    "arxiv_id": "2502.00767v1",
    "title": "Learning-Based TSP-Solvers Tend to Be Overly Greedy",
    "authors": [
      "Xiayang Li",
      "Shihua Zhang"
    ],
    "abstract": "Deep learning has shown significant potential in solving combinatorial\noptimization problems such as the Euclidean traveling salesman problem (TSP).\nHowever, most training and test instances for existing TSP algorithms are\ngenerated randomly from specific distributions like uniform distribution. This\nhas led to a lack of analysis and understanding of the performance of deep\nlearning algorithms in out-of-distribution (OOD) generalization scenarios,\nwhich has a close relationship with the worst-case performance in the\ncombinatorial optimization field. For data-driven algorithms, the statistical\nproperties of randomly generated datasets are critical. This study constructs a\nstatistical measure called nearest-neighbor density to verify the asymptotic\nproperties of randomly generated datasets and reveal the greedy behavior of\nlearning-based solvers, i.e., always choosing the nearest neighbor nodes to\nconstruct the solution path. Based on this statistical measure, we develop\ninterpretable data augmentation methods that rely on distribution shifts or\ninstance perturbations and validate that the performance of the learning-based\nsolvers degenerates much on such augmented data. Moreover, fine-tuning\nlearning-based solvers with augmented data further enhances their\ngeneralization abilities. In short, we decipher the limitations of\nlearning-based TSP solvers tending to be overly greedy, which may have profound\nimplications for AI-empowered combinatorial optimization solvers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "90C27",
      "I.2.0; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00767v1",
    "published_date": "2025-02-02 12:06:13 UTC",
    "updated_date": "2025-02-02 12:06:13 UTC"
  },
  {
    "arxiv_id": "2502.00757v2",
    "title": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement",
    "authors": [
      "J Rosser",
      "Jakob Nicolaus Foerster"
    ],
    "abstract": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NE",
      "68T42, 68T50",
      "I.2.11"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00757v2",
    "published_date": "2025-02-02 11:40:07 UTC",
    "updated_date": "2025-04-14 10:39:33 UTC"
  },
  {
    "arxiv_id": "2502.00752v1",
    "title": "Zero-Shot Warning Generation for Misinformative Multimodal Content",
    "authors": [
      "Giovanni Pio Delvecchio",
      "Huy Hong Nguyen",
      "Isao Echizen"
    ],
    "abstract": "The widespread prevalence of misinformation poses significant societal\nconcerns. Out-of-context misinformation, where authentic images are paired with\nfalse text, is particularly deceptive and easily misleads audiences. Most\nexisting detection methods primarily evaluate image-text consistency but often\nlack sufficient explanations, which are essential for effectively debunking\nmisinformation. We present a model that detects multimodal misinformation\nthrough cross-modality consistency checks, requiring minimal training time.\nAdditionally, we propose a lightweight model that achieves competitive\nperformance using only one-third of the parameters. We also introduce a\ndual-purpose zero-shot learning task for generating contextualized warnings,\nenabling automated debunking and enhancing user comprehension. Qualitative and\nhuman evaluations of the generated warnings highlight both the potential and\nlimitations of our approach.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00752v1",
    "published_date": "2025-02-02 11:18:05 UTC",
    "updated_date": "2025-02-02 11:18:05 UTC"
  },
  {
    "arxiv_id": "2502.00747v1",
    "title": "Universal Post-Processing Networks for Joint Optimization of Modules in Task-Oriented Dialogue Systems",
    "authors": [
      "Atsumoto Ohashi",
      "Ryuichiro Higashinaka"
    ],
    "abstract": "Post-processing networks (PPNs) are components that modify the outputs of\narbitrary modules in task-oriented dialogue systems and are optimized using\nreinforcement learning (RL) to improve the overall task completion capability\nof the system. However, previous PPN-based approaches have been limited to\nhandling only a subset of modules within a system, which poses a significant\nlimitation in improving the system performance. In this study, we propose a\njoint optimization method for post-processing the outputs of all modules using\nuniversal post-processing networks (UniPPNs), which are language-model-based\nnetworks that can modify the outputs of arbitrary modules in a system as a\nsequence-transformation task. Moreover, our RL algorithm, which employs a\nmodule-level Markov decision process, enables fine-grained value and advantage\nestimation for each module, thereby stabilizing joint learning for\npost-processing the outputs of all modules. Through both simulation-based and\nhuman evaluation experiments using the MultiWOZ dataset, we demonstrated that\nUniPPN outperforms conventional PPNs in the task completion capability of\ntask-oriented dialogue systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2025 Main Technical Track",
    "pdf_url": "http://arxiv.org/pdf/2502.00747v1",
    "published_date": "2025-02-02 10:46:37 UTC",
    "updated_date": "2025-02-02 10:46:37 UTC"
  },
  {
    "arxiv_id": "2502.01685v1",
    "title": "Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment",
    "authors": [
      "Si-Ioi Ng",
      "Pranav S. Ambadi",
      "Kimberly D. Mueller",
      "Julie Liss",
      "Visar Berisha"
    ],
    "abstract": "Existing methods for analyzing linguistic content from picture descriptions\nfor assessment of cognitive-linguistic impairment often overlook the\nparticipant's visual narrative path, which typically requires eye tracking to\nassess. Spatio-semantic graphs are a useful tool for analyzing this narrative\npath from transcripts alone, however they are limited by the need for manual\ntagging of content information units (CIUs). In this paper, we propose an\nautomated approach for estimation of spatio-semantic graphs (via automated\nextraction of CIUs) from the Cookie Theft picture commonly used in\ncognitive-linguistic analyses. The method enables the automatic\ncharacterization of the visual semantic path during picture description.\nExperiments demonstrate that the automatic spatio-semantic graphs effectively\ndifferentiate between cognitively impaired and unimpaired speakers. Statistical\nanalyses reveal that the features derived by the automated method produce\ncomparable results to the manual method, with even greater group differences\nbetween clinical groups of interest. These results highlight the potential of\nthe automated approach for extracting spatio-semantic features in developing\nclinical speech models for cognitive impairment assessment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.01685v1",
    "published_date": "2025-02-02 10:25:19 UTC",
    "updated_date": "2025-02-02 10:25:19 UTC"
  },
  {
    "arxiv_id": "2502.00735v3",
    "title": "`Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs",
    "authors": [
      "Chun Wai Chiu",
      "Linghan Huang",
      "Bo Li",
      "Huaming Chen",
      "Kim-Kwang Raymond Choo"
    ],
    "abstract": "Large Language Models (LLMs) have seen widespread applications across various\ndomains due to their growing ability to process diverse types of input data,\nincluding text, audio, image and video. While LLMs have demonstrated\noutstanding performance in understanding and generating contexts for different\nscenarios, they are vulnerable to prompt-based attacks, which are mostly via\ntext input. In this paper, we introduce the first voice-based jailbreak attack\nagainst multimodal LLMs, termed as Flanking Attack, which can process different\ntypes of input simultaneously towards the multimodal LLMs. Our work is\nmotivated by recent advancements in monolingual voice-driven large language\nmodels, which have introduced new attack surfaces beyond traditional text-based\nvulnerabilities for LLMs. To investigate these risks, we examine the\nstate-of-the-art multimodal LLMs, which can be accessed via different types of\ninputs such as audio input, focusing on how adversarial prompts can bypass its\ndefense mechanisms. We propose a novel strategy, in which the disallowed prompt\nis flanked by benign, narrative-driven prompts. It is integrated in the\nFlanking Attack which attempts to humanizes the interaction context and execute\nthe attack through a fictional setting. Further, to better evaluate the attack\nperformance, we present a semi-automated self-assessment framework for policy\nviolation detection. We demonstrate that Flanking Attack is capable of\nmanipulating state-of-the-art LLMs into generating misaligned and forbidden\noutputs, which achieves an average attack success rate ranging from 0.67 to\n0.93 across seven forbidden scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00735v3",
    "published_date": "2025-02-02 10:05:08 UTC",
    "updated_date": "2025-05-18 07:29:41 UTC"
  },
  {
    "arxiv_id": "2502.00734v1",
    "title": "CycleGuardian: A Framework for Automatic RespiratorySound classification Based on Improved Deep clustering and Contrastive Learning",
    "authors": [
      "Yun Chu",
      "Qiuhao Wang",
      "Enze Zhou",
      "Ling Fu",
      "Qian Liu",
      "Gang Zheng"
    ],
    "abstract": "Auscultation plays a pivotal role in early respiratory and pulmonary disease\ndiagnosis. Despite the emergence of deep learning-based methods for automatic\nrespiratory sound classification post-Covid-19, limited datasets impede\nperformance enhancement. Distinguishing between normal and abnormal respiratory\nsounds poses challenges due to the coexistence of normal respiratory components\nand noise components in both types. Moreover, different abnormal respiratory\nsounds exhibit similar anomalous features, hindering their differentiation.\nBesides, existing state-of-the-art models suffer from excessive parameter size,\nimpeding deployment on resource-constrained mobile platforms. To address these\nissues, we design a lightweight network CycleGuardian and propose a framework\nbased on an improved deep clustering and contrastive learning. We first\ngenerate a hybrid spectrogram for feature diversity and grouping spectrograms\nto facilitating intermittent abnormal sound capture.Then, CycleGuardian\nintegrates a deep clustering module with a similarity-constrained clustering\ncomponent to improve the ability to capture abnormal features and a contrastive\nlearning module with group mixing for enhanced abnormal feature discernment.\nMulti-objective optimization enhances overall performance during training. In\nexperiments we use the ICBHI2017 dataset, following the official split method\nand without any pre-trained weights, our method achieves Sp: 82.06 $\\%$, Se:\n44.47$\\%$, and Score: 63.26$\\%$ with a network model size of 38M, comparing to\nthe current model, our method leads by nearly 7$\\%$, achieving the current best\nperformances. Additionally, we deploy the network on Android devices,\nshowcasing a comprehensive intelligent respiratory sound auscultation system.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00734v1",
    "published_date": "2025-02-02 09:56:47 UTC",
    "updated_date": "2025-02-02 09:56:47 UTC"
  },
  {
    "arxiv_id": "2502.00729v1",
    "title": "Selective Response Strategies for GenAI",
    "authors": [
      "Boaz Taitler",
      "Omer Ben-Porat"
    ],
    "abstract": "The rise of Generative AI (GenAI) has significantly impacted human-based\nforums like Stack Overflow, which are essential for generating high-quality\ndata. This creates a negative feedback loop, hindering the development of GenAI\nsystems, which rely on such data to provide accurate responses. In this paper,\nwe provide a possible remedy: A novel strategy we call selective response.\nSelective response implies that GenAI could strategically provide inaccurate\n(or conservative) responses to queries involving emerging topics and novel\ntechnologies, thereby driving users to use human-based forums like Stack\nOverflow. We show that selective response can potentially have a compounding\neffect on the data generation process, increasing both GenAI's revenue and user\nwelfare in the long term. From an algorithmic perspective, we propose an\napproximately optimal approach to maximize GenAI's revenue under social welfare\nconstraints. From a regulatory perspective, we derive sufficient and necessary\nconditions for selective response to improve welfare improvements.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00729v1",
    "published_date": "2025-02-02 09:27:02 UTC",
    "updated_date": "2025-02-02 09:27:02 UTC"
  },
  {
    "arxiv_id": "2502.00726v1",
    "title": "Perspectives for Direct Interpretability in Multi-Agent Deep Reinforcement Learning",
    "authors": [
      "Yoann Poupart",
      "Aurélie Beynier",
      "Nicolas Maudet"
    ],
    "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in\nsolving complex problems in robotics or games, yet most of the trained models\nare hard to interpret. While learning intrinsically interpretable models\nremains a prominent approach, its scalability and flexibility are limited in\nhandling complex tasks or multi-agent dynamics. This paper advocates for direct\ninterpretability, generating post hoc explanations directly from trained\nmodels, as a versatile and scalable alternative, offering insights into agents'\nbehaviour, emergent phenomena, and biases without altering models'\narchitectures. We explore modern methods, including relevance backpropagation,\nknowledge edition, model steering, activation patching, sparse autoencoders and\ncircuit discovery, to highlight their applicability to single-agent,\nmulti-agent, and training process challenges. By addressing MADRL\ninterpretability, we propose directions aiming to advance active topics such as\nteam identification, swarm coordination and sample efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00726v1",
    "published_date": "2025-02-02 09:15:27 UTC",
    "updated_date": "2025-02-02 09:15:27 UTC"
  },
  {
    "arxiv_id": "2502.00724v2",
    "title": "Learned Bayesian Cramér-Rao Bound for Unknown Measurement Models Using Score Neural Networks",
    "authors": [
      "Hai Victor Habi",
      "Hagit Messer",
      "Yoram Bresler"
    ],
    "abstract": "The Bayesian Cram\\'er-Rao bound (BCRB) is a crucial tool in signal processing\nfor assessing the fundamental limitations of any estimation problem as well as\nbenchmarking within a Bayesian frameworks. However, the BCRB cannot be computed\nwithout full knowledge of the prior and the measurement distributions. In this\nwork, we propose a fully learned Bayesian Cram\\'er-Rao bound (LBCRB) that\nlearns both the prior and the measurement distributions. Specifically, we\nsuggest two approaches to obtain the LBCRB: the Posterior Approach and the\nMeasurement-Prior Approach. The Posterior Approach provides a simple method to\nobtain the LBCRB, whereas the Measurement-Prior Approach enables us to\nincorporate domain knowledge to improve the sample complexity and\n{interpretability}. To achieve this, we introduce a Physics-encoded score\nneural network which enables us to easily incorporate such domain knowledge\ninto a neural network. We {study the learning} errors of the two suggested\napproaches theoretically, and validate them numerically. We demonstrate the two\napproaches on several signal processing examples, including a linear\nmeasurement problem with unknown mixing and Gaussian noise covariance matrices,\nfrequency estimation, and quantized measurement. In addition, we test our\napproach on a nonlinear signal processing problem of frequency estimation with\nreal-world underwater ambient noise.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "eess.SP",
    "comment": "28 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00724v2",
    "published_date": "2025-02-02 09:00:40 UTC",
    "updated_date": "2025-02-09 06:19:51 UTC"
  },
  {
    "arxiv_id": "2502.00712v1",
    "title": "Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images",
    "authors": [
      "Shengtian Sang",
      "Hassan Jahanandish",
      "Cynthia Xinran Li",
      "Indrani Bhattachary",
      "Jeong Hoon Lee",
      "Lichun Zhang",
      "Sulaiman Vesal",
      "Pejman Ghanouni",
      "Richard Fan",
      "Geoffrey A. Sonn",
      "Mirabela Rusu"
    ],
    "abstract": "Prostate cancer is a major cause of cancer-related deaths in men, where early\ndetection greatly improves survival rates. Although MRI-TRUS fusion biopsy\noffers superior accuracy by combining MRI's detailed visualization with TRUS's\nreal-time guidance, it is a complex and time-intensive procedure that relies\nheavily on manual annotations, leading to potential errors. To address these\nchallenges, we propose a fully automatic MRI-TRUS fusion-based segmentation\nmethod that identifies prostate tumors directly in TRUS images without\nrequiring manual annotations. Unlike traditional multimodal fusion approaches\nthat rely on naive data concatenation, our method integrates a\nregistration-segmentation framework to align and leverage spatial information\nbetween MRI and TRUS modalities. This alignment enhances segmentation accuracy\nand reduces reliance on manual effort. Our approach was validated on a dataset\nof 1,747 patients from Stanford Hospital, achieving an average Dice coefficient\nof 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132)\nmethods, with significant improvements (p $<$ 0.01). This framework\ndemonstrates the potential for reducing the complexity of prostate cancer\ndiagnosis and provides a flexible architecture applicable to other multimodal\nmedical imaging tasks.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00712v1",
    "published_date": "2025-02-02 07:58:40 UTC",
    "updated_date": "2025-02-02 07:58:40 UTC"
  },
  {
    "arxiv_id": "2502.00711v1",
    "title": "VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework",
    "authors": [
      "Chunbai Zhang",
      "Chao Wang",
      "Yang Zhou",
      "Yan Peng"
    ],
    "abstract": "Visual reasoning refers to the task of solving questions about visual\ninformation. Current visual reasoning methods typically employ pre-trained\nvision-language model (VLM) strategies or deep neural network approaches.\nHowever, existing efforts are constrained by limited reasoning\ninterpretability, while hindering by the phenomenon of underspecification in\nthe question text. Additionally, the absence of fine-grained visual knowledge\nlimits the precise understanding of subject behavior in visual reasoning tasks.\nTo address these issues, we propose VIKSER (Visual Knowledge-Driven\nSelf-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using\nknowledge distilled from large language models, extracts fine-grained visual\nknowledge with the assistance of visual relationship detection techniques.\nSubsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the\nquestion with underspecification. Additionally, we design a novel prompting\nmethod called Chain-of-Evidence (CoE), which leverages the power of ``evidence\nfor reasoning'' to endow VIKSER with interpretable reasoning capabilities.\nMeanwhile, the integration of self-reflection technology empowers VIKSER with\nthe ability to learn and improve from its mistakes. Experiments conducted on\nwidely used datasets demonstrate that VIKSER achieves new state-of-the-art\n(SOTA) results in relevant tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00711v1",
    "published_date": "2025-02-02 07:54:55 UTC",
    "updated_date": "2025-02-02 07:54:55 UTC"
  },
  {
    "arxiv_id": "2502.00708v1",
    "title": "PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation",
    "authors": [
      "Qixuan Li",
      "Chao Wang",
      "Zongjin He",
      "Yan Peng"
    ],
    "abstract": "Text-to-3D asset generation has achieved significant optimization under the\nsupervision of 2D diffusion priors. However, when dealing with compositional\nscenes, existing methods encounter several challenges: 1). failure to ensure\nthat composite scene layouts comply with physical laws; 2). difficulty in\naccurately capturing the assets and relationships described in complex scene\ndescriptions; 3). limited autonomous asset generation capabilities among layout\napproaches leveraging large language models (LLMs). To avoid these compromises,\nwe propose a novel framework for compositional scene generation, PhiP-G, which\nseamlessly integrates generation techniques with layout guidance based on a\nworld model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene\ndescription to generate a scene graph, and integrating a multimodal 2D\ngeneration agent and a 3D Gaussian generation method for targeted assets\ncreation. For the stage of layout, PhiP-G employs a physical pool with adhesion\ncapabilities and a visual supervision agent, forming a world model for layout\nprediction and planning. Extensive experiments demonstrate that PhiP-G\nsignificantly enhances the generation quality and physical rationality of the\ncompositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA)\nperformance in CLIP scores, achieves parity with the leading methods in\ngeneration quality as measured by the T$^3$Bench, and improves efficiency by\n24x.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages.8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00708v1",
    "published_date": "2025-02-02 07:47:03 UTC",
    "updated_date": "2025-02-02 07:47:03 UTC"
  },
  {
    "arxiv_id": "2502.01684v3",
    "title": "Leveraging Joint Predictive Embedding and Bayesian Inference in Graph Self Supervised Learning",
    "authors": [
      "Srinitish Srinivasan",
      "Omkumar CU"
    ],
    "abstract": "Graph representation learning has emerged as a cornerstone for tasks like\nnode classification and link prediction, yet prevailing self-supervised\nlearning (SSL) methods face challenges such as computational inefficiency,\nreliance on contrastive objectives, and representation collapse. Existing\napproaches often depend on feature reconstruction, negative sampling, or\ncomplex decoders, which introduce training overhead and hinder generalization.\nFurther, current techniques which address such limitations fail to account for\nthe contribution of node embeddings to a certain prediction in the absence of\nlabeled nodes. To address these limitations, we propose a novel joint embedding\npredictive framework for graph SSL that eliminates contrastive objectives and\nnegative sampling while preserving semantic and structural information.\nAdditionally, we introduce a semantic-aware objective term that incorporates\npseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node\ndiscriminability by evaluating latent feature contributions. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art graph\nSSL methods across benchmarks, achieving superior performance without\ncontrastive loss or complex decoders. Key innovations include (1) a\nnon-contrastive, view-invariant joint embedding predictive architecture, (2)\nLeveraging single context and multiple targets relationship between subgraphs,\nand (3) GMM-based pseudo-label scoring to capture semantic contributions. This\nwork advances graph SSL by offering a computationally efficient,\ncollapse-resistant paradigm that bridges spatial and semantic graph features\nfor downstream tasks. The code for our paper can be found at\nhttps://github.com/Deceptrax123/JPEB-GSSL",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2502.01684v3",
    "published_date": "2025-02-02 07:42:45 UTC",
    "updated_date": "2025-04-01 10:40:01 UTC"
  },
  {
    "arxiv_id": "2502.00698v1",
    "title": "MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models",
    "authors": [
      "Huanqia Cai",
      "Yijun Yang",
      "Winston Hu"
    ],
    "abstract": "IQ testing has served as a foundational methodology for evaluating human\ncognitive capabilities, deliberately decoupling assessment from linguistic\nbackground, language proficiency, or domain-specific knowledge to isolate core\ncompetencies in abstraction and reasoning. Yet, artificial intelligence\nresearch currently lacks systematic benchmarks to quantify these critical\ncognitive dimensions in multimodal systems. To address this critical gap, we\npropose MM-IQ, a comprehensive evaluation framework comprising 2,710\nmeticulously curated test items spanning 8 distinct reasoning paradigms.\n  Through systematic evaluation of leading open-source and proprietary\nmultimodal models, our benchmark reveals striking limitations: even\nstate-of-the-art architectures achieve only marginally superior performance to\nrandom chance (27.49% vs. 25% baseline accuracy). This substantial performance\nchasm highlights the inadequacy of current multimodal systems in approximating\nfundamental human reasoning capacities, underscoring the need for\nparadigm-shifting advancements to bridge this cognitive divide.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00698v1",
    "published_date": "2025-02-02 07:12:03 UTC",
    "updated_date": "2025-02-02 07:12:03 UTC"
  },
  {
    "arxiv_id": "2502.00695v1",
    "title": "TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion",
    "authors": [
      "Linglong Wu",
      "Xuhao Shan",
      "Ruiquan Ge",
      "Ruoyu Liang",
      "Chi Zhang",
      "Yonghong Li",
      "Ahmed Elazab",
      "Huoling Luo",
      "Yunbi Liu",
      "Changmiao Wang"
    ],
    "abstract": "Chronic liver disease represents a significant health challenge worldwide and\naccurate prognostic evaluations are essential for personalized treatment plans.\nRecent evidence suggests that integrating multimodal data, such as computed\ntomography imaging, radiomic features, and clinical information, can provide\nmore comprehensive prognostic information. However, modalities have an inherent\nheterogeneity, and incorporating additional modalities may exacerbate the\nchallenges of heterogeneous data fusion. Moreover, existing multimodal fusion\nmethods often struggle to adapt to richer medical modalities, making it\ndifficult to capture inter-modal relationships. To overcome these limitations,\nWe present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).\nSpecifically, we develop an Intra-Modality Aggregation module and a\nTriple-Modal Cross-Attention Fusion module, which are designed to eliminate\nintra-modality redundancy and extract cross-modal information, respectively.\nFurthermore, we design a Triple-Modal Feature Fusion loss function to align\nfeature representations across modalities. Extensive experiments on the liver\nprognosis dataset demonstrate that our approach significantly outperforms\nexisting state-of-the-art unimodal models and other multi-modal techniques. Our\ncode is available at https://github.com/Mysterwll/liver.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 3 figures, accepted by IEEE ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.00695v1",
    "published_date": "2025-02-02 07:05:28 UTC",
    "updated_date": "2025-02-02 07:05:28 UTC"
  },
  {
    "arxiv_id": "2502.00694v1",
    "title": "Leveraging Large Language Models to Predict Antibody Biological Activity Against Influenza A Hemagglutinin",
    "authors": [
      "Ella Barkan",
      "Ibrahim Siddiqui",
      "Kevin J. Cheng",
      "Alex Golts",
      "Yoel Shoshan",
      "Jeffrey K. Weber",
      "Yailin Campos Mota",
      "Michal Ozery-Flato",
      "Giuseppe A. Sautto"
    ],
    "abstract": "Monoclonal antibodies (mAbs) represent one of the most prevalent FDA-approved\nmodalities for treating autoimmune diseases, infectious diseases, and cancers.\nHowever, discovery and development of therapeutic antibodies remains a\ntime-consuming and expensive process. Recent advancements in machine learning\n(ML) and artificial intelligence (AI) have shown significant promise in\nrevolutionizing antibody discovery and optimization. In particular, models that\npredict antibody biological activity enable in-silico evaluation of binding and\nfunctional properties; such models can prioritize antibodies with the highest\nlikelihoods of success in costly and time-intensive laboratory testing\nprocedures. We here explore an AI model for predicting the binding and receptor\nblocking activity of antibodies against influenza A hemagglutinin (HA)\nantigens. Our present model is developed with the MAMMAL framework for\nbiologics discovery to predict antibody-antigen interactions using only\nsequence information. To evaluate the model's performance, we tested it under\nvarious data split conditions to mimic real-world scenarios.\n  Our models achieved an AUROC $\\geq$ 0.91 for predicting the activity of\nexisting antibodies against seen HAs and an AUROC of 0.9 for unseen HAs. For\nnovel antibody activity prediction, the AUROC was 0.73, which further declined\nto 0.63-0.66 under stringent constraints on similarity to existing antibodies.\nThese results demonstrate the potential of AI foundation models to transform\nantibody design by reducing dependence on extensive laboratory testing and\nenabling more efficient prioritization of antibody candidates. Moreover, our\nfindings emphasize the critical importance of diverse and comprehensive\nantibody datasets to improve the generalization of prediction models,\nparticularly for novel antibody development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00694v1",
    "published_date": "2025-02-02 06:48:45 UTC",
    "updated_date": "2025-02-02 06:48:45 UTC"
  },
  {
    "arxiv_id": "2502.01683v1",
    "title": "LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient",
    "authors": [
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Xinglin Wang",
      "Yueqi Zhang",
      "Jiayi Shi",
      "Chuyi Tan",
      "Boyuan Pan",
      "Yao Hu",
      "Kan Li"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has led to a surge in\nboth model supply and application demands. To facilitate effective matching\nbetween them, reliable, generic and efficient benchmark generators are widely\nneeded. However, human annotators are constrained by inefficiency, and current\nLLM benchmark generators not only lack generalizability but also struggle with\nlimited reliability, as they lack a comprehensive evaluation framework for\nvalidation and optimization. To fill this gap, we first propose an automated\nand unbiased evaluation framework, structured around four dimensions and ten\ncriteria. Under this framework, we carefully analyze the advantages and\nweaknesses of directly prompting LLMs as generic benchmark generators. To\nenhance the reliability, we introduce a series of methods to address the\nidentified weaknesses and integrate them as BenchMaker. Experiments across\nmultiple LLMs and tasks confirm that BenchMaker achieves superior or comparable\nperformance to human-annotated benchmarks on all metrics, highlighting its\ngeneralizability and reliability. More importantly, it delivers highly\nconsistent evaluation results across 12 LLMs (0.967 Pearson correlation against\nMMLU-Pro), while taking only $0.005 and 0.38 minutes per sample.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01683v1",
    "published_date": "2025-02-02 06:36:01 UTC",
    "updated_date": "2025-02-02 06:36:01 UTC"
  },
  {
    "arxiv_id": "2502.00691v2",
    "title": "Learning Autonomous Code Integration for Math Language Models",
    "authors": [
      "Haozhe Wang",
      "Long Li",
      "Chao Qu",
      "Fengming Zhu",
      "Weidi Xu",
      "Wei Chu",
      "Fangzhen Lin"
    ],
    "abstract": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00691v2",
    "published_date": "2025-02-02 06:32:23 UTC",
    "updated_date": "2025-02-16 07:18:23 UTC"
  },
  {
    "arxiv_id": "2502.00690v1",
    "title": "Dissecting Submission Limit in Desk-Rejections: A Mathematical Analysis of Fairness in AI Conference Policies",
    "authors": [
      "Yuefan Cao",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang"
    ],
    "abstract": "As AI research surges in both impact and volume, conferences have imposed\nsubmission limits to maintain paper quality and alleviate organizational\npressure. In this work, we examine the fairness of desk-rejection systems under\nsubmission limits and reveal that existing practices can result in substantial\ninequities. Specifically, we formally define the paper submission limit problem\nand identify a critical dilemma: when the number of authors exceeds three, it\nbecomes impossible to reject papers solely based on excessive submissions\nwithout negatively impacting innocent authors. Thus, this issue may unfairly\naffect early-career researchers, as their submissions may be penalized due to\nco-authors with significantly higher submission counts, while senior\nresearchers with numerous papers face minimal consequences. To address this, we\npropose an optimization-based fairness-aware desk-rejection mechanism and\nformally define two fairness metrics: individual fairness and group fairness.\nWe prove that optimizing individual fairness is NP-hard, whereas group fairness\ncan be efficiently optimized via linear programming. Through case studies, we\ndemonstrate that our proposed system ensures greater equity than existing\nmethods, including those used in CVPR 2025, offering a more socially just\napproach to managing excessive submissions in AI conferences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00690v1",
    "published_date": "2025-02-02 06:29:23 UTC",
    "updated_date": "2025-02-02 06:29:23 UTC"
  },
  {
    "arxiv_id": "2502.00688v1",
    "title": "High-Order Matching for One-Step Shortcut Diffusion Models",
    "authors": [
      "Bo Chen",
      "Chengyue Gong",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Mingda Wan"
    ],
    "abstract": "One-step shortcut diffusion models [Frans, Hafner, Levine and Abbeel, ICLR\n2025] have shown potential in vision generation, but their reliance on\nfirst-order trajectory supervision is fundamentally limited. The Shortcut\nmodel's simplistic velocity-only approach fails to capture intrinsic manifold\ngeometry, leading to erratic trajectories, poor geometric alignment, and\ninstability-especially in high-curvature regions. These shortcomings stem from\nits inability to model mid-horizon dependencies or complex distributional\nfeatures, leaving it ill-equipped for robust generative modeling. In this work,\nwe introduce HOMO (High-Order Matching for One-Step Shortcut Diffusion), a\ngame-changing framework that leverages high-order supervision to revolutionize\ndistribution transportation. By incorporating acceleration, jerk, and beyond,\nHOMO not only fixes the flaws of the Shortcut model but also achieves\nunprecedented smoothness, stability, and geometric precision. Theoretically, we\nprove that HOMO's high-order supervision ensures superior approximation\naccuracy, outperforming first-order methods. Empirically, HOMO dominates in\ncomplex settings, particularly in high-curvature regions where the Shortcut\nmodel struggles. Our experiments show that HOMO delivers smoother trajectories\nand better distributional alignment, setting a new standard for one-step\ngenerative models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00688v1",
    "published_date": "2025-02-02 06:19:59 UTC",
    "updated_date": "2025-02-02 06:19:59 UTC"
  },
  {
    "arxiv_id": "2502.00684v1",
    "title": "Compositional Concept-Based Neuron-Level Interpretability for Deep Reinforcement Learning",
    "authors": [
      "Zeyu Jiang",
      "Hai Huang",
      "Xingquan Zuo"
    ],
    "abstract": "Deep reinforcement learning (DRL), through learning policies or values\nrepresented by neural networks, has successfully addressed many complex control\nproblems. However, the neural networks introduced by DRL lack interpretability\nand transparency. Current DRL interpretability methods largely treat neural\nnetworks as black boxes, with few approaches delving into the internal\nmechanisms of policy/value networks. This limitation undermines trust in both\nthe neural network models that represent policies and the explanations derived\nfrom them. In this work, we propose a novel concept-based interpretability\nmethod that provides fine-grained explanations of DRL models at the neuron\nlevel. Our method formalizes atomic concepts as binary functions over the state\nspace and constructs complex concepts through logical operations. By analyzing\nthe correspondence between neuron activations and concept functions, we\nestablish interpretable explanations for individual neurons in policy/value\nnetworks. Experimental results on both continuous control tasks and discrete\ndecision-making environments demonstrate that our method can effectively\nidentify meaningful concepts that align with human understanding while\nfaithfully reflecting the network's decision-making logic.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.2.1; I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures, IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.00684v1",
    "published_date": "2025-02-02 06:05:49 UTC",
    "updated_date": "2025-02-02 06:05:49 UTC"
  },
  {
    "arxiv_id": "2502.00682v1",
    "title": "Guidance Source Matters: How Guidance from AI, Expert, or a Group of Analysts Impacts Visual Data Preparation and Analysis",
    "authors": [
      "Arpit Narechania",
      "Alex Endert",
      "Atanu R Sinha"
    ],
    "abstract": "The progress in generative AI has fueled AI-powered tools like co-pilots and\nassistants to provision better guidance, particularly during data analysis.\nHowever, research on guidance has not yet examined the perceived efficacy of\nthe source from which guidance is offered and the impact of this source on the\nuser's perception and usage of guidance. We ask whether users perceive all\nguidance sources as equal, with particular interest in three sources: (i) AI,\n(ii) human expert, and (iii) a group of human analysts. As a benchmark, we\nconsider a fourth source, (iv) unattributed guidance, where guidance is\nprovided without attribution to any source, enabling isolation of and\ncomparison with the effects of source-specific guidance. We design a\nfive-condition between-subjects study, with one condition for each of the four\nguidance sources and an additional (v) no-guidance condition, which serves as a\nbaseline to evaluate the influence of any kind of guidance. We situate our\nstudy in a custom data preparation and analysis tool wherein we task users to\nselect relevant attributes from an unfamiliar dataset to inform a business\nreport. Depending on the assigned condition, users can request guidance, which\nthe system then provides in the form of attribute suggestions. To ensure\ninternal validity, we control for the quality of guidance across\nsource-conditions. Through several metrics of usage and perception, we\nstatistically test five preregistered hypotheses and report on additional\nanalysis. We find that the source of guidance matters to users, but not in a\nmanner that matches received wisdom. For instance, users utilize guidance\ndifferently at various stages of analysis, including expressing varying levels\nof regret, despite receiving guidance of similar quality. Notably, users in the\nAI condition reported both higher post-task benefit and regret.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "21 pages, 10 figures, 6 figures, to appear in proceedings of ACM IUI\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2502.00682v1",
    "published_date": "2025-02-02 05:59:02 UTC",
    "updated_date": "2025-02-02 05:59:02 UTC"
  },
  {
    "arxiv_id": "2502.00681v1",
    "title": "A Survey of Quantized Graph Representation Learning: Connecting Graph Structures with Large Language Models",
    "authors": [
      "Qika Lin",
      "Zhen Peng",
      "Kaize Shi",
      "Kai He",
      "Yiming Xu",
      "Erik Cambria",
      "Mengling Feng"
    ],
    "abstract": "Recent years have witnessed rapid advances in graph representation learning,\nwith the continuous embedding approach emerging as the dominant paradigm.\nHowever, such methods encounter issues regarding parameter efficiency,\ninterpretability, and robustness. Thus, Quantized Graph Representation (QGR)\nlearning has recently gained increasing interest, which represents the graph\nstructure with discrete codes instead of conventional continuous embeddings.\nGiven its analogous representation form to natural language, QGR also possesses\nthe capability to seamlessly integrate graph structures with large language\nmodels (LLMs). As this emerging paradigm is still in its infancy yet holds\nsignificant promise, we undertake this thorough survey to promote its rapid\nfuture prosperity. We first present the background of the general quantization\nmethods and their merits. Moreover, we provide an in-depth demonstration of\ncurrent QGR studies from the perspectives of quantized strategies, training\nobjectives, distinctive designs, knowledge graph quantization, and\napplications. We further explore the strategies for code dependence learning\nand integration with LLMs. At last, we give discussions and conclude future\ndirections, aiming to provide a comprehensive picture of QGR and inspire future\nresearch.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00681v1",
    "published_date": "2025-02-02 05:57:34 UTC",
    "updated_date": "2025-02-02 05:57:34 UTC"
  },
  {
    "arxiv_id": "2502.00678v2",
    "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
    "authors": [
      "Hyeong Kyu Choi",
      "Maxim Khanov",
      "Hongxin Wei",
      "Yixuan Li"
    ],
    "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training\ncorpora, inflates performance metrics and undermines the reliability of model\nevaluations. Measuring dataset contamination thus becomes essential to ensure\nthat performance evaluations genuinely reflect a model's ability to generalize\nto unseen data, rather than relying on memorized examples. To address this\nproblem, we propose Kernel Divergence Score (KDS), a novel method that\nevaluates dataset contamination by computing the divergence between the kernel\nsimilarity matrix of sample embeddings, before and after fine-tuning on the\nbenchmark dataset. Leveraging the insight that fine-tuning affects unseen\nsamples more significantly than seen ones, KDS provides a reliable measure of\ncontamination. Through extensive experiments on controlled contamination\nscenarios, KDS demonstrates a near-perfect correlation with contamination\nlevels and outperforms existing baselines. Additionally, we perform\ncomprehensive ablation studies to analyze the impact of key design choices,\nproviding deeper insights into the components and effectiveness of KDS. These\nablations highlight the importance of leveraging fine-grained kernel-based\ninformation and confirm the reliability of the proposed framework across\ndiverse datasets and settings. Code is released in\nhttps://github.com/deeplearning-wisc/kernel-divergence-score.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.00678v2",
    "published_date": "2025-02-02 05:50:39 UTC",
    "updated_date": "2025-05-20 20:47:57 UTC"
  },
  {
    "arxiv_id": "2502.13969v1",
    "title": "Bridging Simulation and Reality: A 3D Clustering-Based Deep Learning Model for UAV-Based RF Source Localization",
    "authors": [
      "Saad Masrur",
      "Ismail Guvenc"
    ],
    "abstract": "Localization of radio frequency (RF) sources has critical applications,\nincluding search and rescue, jammer detection, and monitoring of hostile\nactivities. Unmanned aerial vehicles (UAVs) offer significant advantages for RF\nsource localization (RFSL) over terrestrial methods, leveraging autonomous 3D\nnavigation and improved signal capture at higher altitudes. Recent advancements\nin deep learning (DL) have further enhanced localization accuracy, particularly\nfor outdoor scenarios. DL models often face challenges in real-world\nperformance, as they are typically trained on simulated datasets that fail to\nreplicate real-world conditions fully. To address this, we first propose the\nEnhanced Two-Ray propagation model, reducing the simulation-to-reality gap by\nimproving the accuracy of propagation environment modeling. For RFSL, we\npropose the 3D Cluster-Based RealAdaptRNet, a DL-based method leveraging 3D\nclustering-based feature extraction for robust localization. Experimental\nresults demonstrate that the proposed Enhanced Two-Ray model provides superior\naccuracy in simulating real-world propagation scenarios compared to\nconventional free-space and two-ray models. Notably, the 3D Cluster-Based\nRealAdaptRNet, trained entirely on simulated datasets, achieves exceptional\nperformance when validated in real-world environments using the AERPAW physical\ntestbed, with an average localization error of 18.2 m. The proposed approach is\ncomputationally efficient, utilizing 33.5 times fewer parameters, and\ndemonstrates strong generalization capabilities across diverse trajectories,\nmaking it highly suitable for real-world applications.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "This paper has been submitted to IEEE ICC 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.13969v1",
    "published_date": "2025-02-02 05:48:44 UTC",
    "updated_date": "2025-02-02 05:48:44 UTC"
  },
  {
    "arxiv_id": "2502.00677v1",
    "title": "LLM-based event log analysis techniques: A survey",
    "authors": [
      "Siraaj Akhtar",
      "Saad Khan",
      "Simon Parkinson"
    ],
    "abstract": "Event log analysis is an important task that security professionals\nundertake. Event logs record key information on activities that occur on\ncomputing devices, and due to the substantial number of events generated, they\nconsume a large amount of time and resources to analyse. This demanding and\nrepetitive task is also prone to errors. To address these concerns, researchers\nhave developed automated techniques to improve the event log analysis process.\nLarge Language Models (LLMs) have recently demonstrated the ability to\nsuccessfully perform a wide range of tasks that individuals would usually\npartake in, to high standards, and at a pace and degree of complexity that\noutperform humans. Due to this, researchers are rapidly investigating the use\nof LLMs for event log analysis. This includes fine-tuning, Retrieval-Augmented\nGeneration (RAG) and in-context learning, which affect performance. These works\ndemonstrate good progress, yet there is a need to understand the developing\nbody of knowledge, identify commonalities between works, and identify key\nchallenges and potential solutions to further developments in this domain. This\npaper aims to survey LLM-based event log analysis techniques, providing readers\nwith an in-depth overview of the domain, gaps identified in previous research,\nand concluding with potential avenues to explore in future.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00677v1",
    "published_date": "2025-02-02 05:28:17 UTC",
    "updated_date": "2025-02-02 05:28:17 UTC"
  },
  {
    "arxiv_id": "2502.05206v3",
    "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
    "authors": [
      "Xingjun Ma",
      "Yifeng Gao",
      "Yixu Wang",
      "Ruofan Wang",
      "Xin Wang",
      "Ye Sun",
      "Yifan Ding",
      "Hengyuan Xu",
      "Yunhao Chen",
      "Yunhan Zhao",
      "Hanxun Huang",
      "Yige Li",
      "Jiaming Zhang",
      "Xiang Zheng",
      "Yang Bai",
      "Zuxuan Wu",
      "Xipeng Qiu",
      "Jingfeng Zhang",
      "Yiming Li",
      "Xudong Han",
      "Haonan Li",
      "Jun Sun",
      "Cong Wang",
      "Jindong Gu",
      "Baoyuan Wu",
      "Siheng Chen",
      "Tianwei Zhang",
      "Yang Liu",
      "Mingming Gong",
      "Tongliang Liu",
      "Shirui Pan",
      "Cihang Xie",
      "Tianyu Pang",
      "Yinpeng Dong",
      "Ruoxi Jia",
      "Yang Zhang",
      "Shiqing Ma",
      "Xiangyu Zhang",
      "Neil Gong",
      "Chaowei Xiao",
      "Sarah Erfani",
      "Tim Baldwin",
      "Bo Li",
      "Masashi Sugiyama",
      "Dacheng Tao",
      "James Bailey",
      "Yu-Gang Jiang"
    ],
    "abstract": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety",
    "pdf_url": "http://arxiv.org/pdf/2502.05206v3",
    "published_date": "2025-02-02 05:14:22 UTC",
    "updated_date": "2025-03-19 16:10:18 UTC"
  },
  {
    "arxiv_id": "2502.01680v1",
    "title": "Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks",
    "authors": [
      "Kamal Acharya",
      "Mehul Lad",
      "Liang Sun",
      "Houbing Song"
    ],
    "abstract": "Travel demand prediction is crucial for optimizing transportation planning,\nresource allocation, and infrastructure development, ensuring efficient\nmobility and economic sustainability. This study introduces a Neurosymbolic\nArtificial Intelligence (Neurosymbolic AI) framework that integrates decision\ntree (DT)-based symbolic rules with neural networks (NNs) to predict travel\ndemand, leveraging the interpretability of symbolic reasoning and the\npredictive power of neural learning. The framework utilizes data from diverse\nsources, including geospatial, economic, and mobility datasets, to build a\ncomprehensive feature set. DTs are employed to extract interpretable if-then\nrules that capture key patterns, which are then incorporated as additional\nfeatures into a NN to enhance its predictive capabilities. Experimental results\nshow that the combined dataset, enriched with symbolic rules, consistently\noutperforms standalone datasets across multiple evaluation metrics, including\nMean Absolute Error (MAE), \\(R^2\\), and Common Part of Commuters (CPC). Rules\nselected at finer variance thresholds (e.g., 0.0001) demonstrate superior\neffectiveness in capturing nuanced relationships, reducing prediction errors,\nand aligning with observed commuter patterns. By merging symbolic and neural\nlearning paradigms, this Neurosymbolic approach achieves both interpretability\nand accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures, this paper is under review in the conference",
    "pdf_url": "http://arxiv.org/pdf/2502.01680v1",
    "published_date": "2025-02-02 05:10:31 UTC",
    "updated_date": "2025-02-02 05:10:31 UTC"
  },
  {
    "arxiv_id": "2502.00672v2",
    "title": "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of Model Prediction and Scientific Understanding of Soil Organic Carbon",
    "authors": [
      "Haodi Xu",
      "Joshua Fan",
      "Feng Tao",
      "Lifen Jiang",
      "Fengqi You",
      "Benjamin Z. Houlton",
      "Ying Sun",
      "Carla P. Gomes",
      "Yiqi Luo"
    ],
    "abstract": "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "60 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00672v2",
    "published_date": "2025-02-02 05:02:42 UTC",
    "updated_date": "2025-02-06 18:41:16 UTC"
  },
  {
    "arxiv_id": "2502.00666v2",
    "title": "Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration",
    "authors": [
      "Mingyu Chen",
      "Yiding Chen",
      "Wen Sun",
      "Xuezhou Zhang"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for large language model (LLM) alignment. This paper studies the\nsetting of online RLHF and focus on improving sample efficiency. All existing\nalgorithms in online RLHF, whether doing passive exploration or active\nexploration, suffer from a sample complexity that scales exponentially with the\nscale of the reward function. This fundamental limitation hinders their\neffectiveness in scenarios with heavily skewed preferences, e.g. questions with\na unique correct solution. To address this, we introduce Self-Exploring\nPreference-Incentive Online Preference Optimization (SE-POPO), an online RLHF\nalgorithm that for the first time achieves a sample complexity that scales\npolynomially with the reward scale, answering an open problem raised by Xie et\nal. (2024).. Theoretically, we demonstrate that the sample complexity of\nSE-POPO dominates that of existing exploration algorithms. Empirically, our\nsystematic evaluation confirms that SE-POPO is more sample-efficient than both\nexploratory and non-exploratory baselines, in two primary application scenarios\nof RLHF as well as on public benchmarks, marking a significant step forward in\nRLHF algorithm design. The code is available at\nhttps://github.com/MYC000801/SE-POPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00666v2",
    "published_date": "2025-02-02 04:40:04 UTC",
    "updated_date": "2025-02-09 20:16:15 UTC"
  },
  {
    "arxiv_id": "2502.00663v1",
    "title": "Enhanced Convolutional Neural Networks for Improved Image Classification",
    "authors": [
      "Xiaoran Yang",
      "Shuhan Yu",
      "Wenxi Xu"
    ],
    "abstract": "Image classification is a fundamental task in computer vision with diverse\napplications, ranging from autonomous systems to medical imaging. The CIFAR-10\ndataset is a widely used benchmark to evaluate the performance of\nclassification models on small-scale, multi-class datasets. Convolutional\nNeural Networks (CNNs) have demonstrated state-of-the-art results; however,\nthey often suffer from overfitting and suboptimal feature representation when\napplied to challenging datasets like CIFAR-10. In this paper, we propose an\nenhanced CNN architecture that integrates deeper convolutional blocks, batch\nnormalization, and dropout regularization to achieve superior performance. The\nproposed model achieves a test accuracy of 84.95%, outperforming baseline CNN\narchitectures. Through detailed ablation studies, we demonstrate the\neffectiveness of the enhancements and analyze the hierarchical feature\nrepresentations. This work highlights the potential of refined CNN\narchitectures for tackling small-scale image classification problems\neffectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00663v1",
    "published_date": "2025-02-02 04:32:25 UTC",
    "updated_date": "2025-02-02 04:32:25 UTC"
  },
  {
    "arxiv_id": "2502.01678v2",
    "title": "LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection",
    "authors": [
      "Yihe Wang",
      "Nan Huang",
      "Nadia Mammone",
      "Marco Cecchi",
      "Xiang Zhang"
    ],
    "abstract": "Electroencephalogram (EEG) provides a non-invasive, highly accessible, and\ncost-effective solution for Alzheimer's Disease (AD) detection. However,\nexisting methods, whether based on manual feature extraction or deep learning,\nface two major challenges: the lack of large-scale datasets for robust feature\nlearning and evaluation, and poor detection performance due to inter-subject\nvariations. To address these challenges, we curate an EEG-AD corpus containing\n813 subjects, which forms the world's largest EEG-AD dataset to the best of our\nknowledge. Using this unique dataset, we propose LEAD, the first large\nfoundation model for EEG-based AD detection. Our method encompasses an entire\npipeline, from data selection and preprocessing to self-supervised contrastive\npretraining, fine-tuning, and key setups such as subject-independent evaluation\nand majority voting for subject-level detection. We pre-train the model on 11\nEEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised\npre-training design includes sample-level and subject-level contrasting to\nextract useful general EEG features. Fine-tuning is performed on 5\nchannel-aligned datasets together. The backbone encoder incorporates temporal\nand channel embeddings to capture features across both temporal and spatial\ndimensions. Our method demonstrates outstanding AD detection performance,\nachieving up to a 9.86% increase in F1 score at the sample-level and up to a\n9.31% at the subject-level compared to state-of-the-art methods. The results of\nour model strongly confirm the effectiveness of contrastive pre-training and\nchannel-aligned unified fine-tuning for addressing inter-subject variation. The\nsource code is at https://github.com/DL4mHealth/LEAD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01678v2",
    "published_date": "2025-02-02 04:19:35 UTC",
    "updated_date": "2025-02-10 17:11:15 UTC"
  },
  {
    "arxiv_id": "2502.00657v1",
    "title": "LLM Safety Alignment is Divergence Estimation in Disguise",
    "authors": [
      "Rajdeep Haldar",
      "Ziyi Wang",
      "Qifan Song",
      "Guang Lin",
      "Yue Xing"
    ],
    "abstract": "We propose a theoretical framework demonstrating that popular Large Language\nModel (LLM) alignment methods, including Reinforcement Learning from Human\nFeedback (RLHF) and alternatives, fundamentally function as divergence\nestimators between aligned (preferred or safe) and unaligned (less-preferred or\nharmful) distributions. This explains the separation phenomenon between safe\nand harmful prompts in the model hidden representation after alignment.\nInspired by the theoretical results, we identify that some alignment methods\nare better than others in terms of separation and, introduce a new method,\nKLDO, and further demonstrate the implication of our theories. We advocate for\ncompliance-refusal datasets over preference datasets to enhance safety\nalignment, supported by both theoretical reasoning and empirical evidence.\nAdditionally, to quantify safety separation, we leverage a distance metric in\nthe representation space and statistically validate its efficacy as a\nstatistical significant indicator of LLM resilience against jailbreak attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00657v1",
    "published_date": "2025-02-02 04:09:42 UTC",
    "updated_date": "2025-02-02 04:09:42 UTC"
  },
  {
    "arxiv_id": "2502.00648v1",
    "title": "Agency in the Age of AI",
    "authors": [
      "Samarth Swarup"
    ],
    "abstract": "There is significant concern about the impact of generative AI on society.\nModern AI tools are capable of generating ever more realistic text, images, and\nvideos, and functional code, from minimal prompts. Accompanying this rise in\nability and usability, there is increasing alarm about the misuses to which\nthese tools can be put, and the intentional and unintentional harms to\nindividuals and society that may result. In this paper, we argue that\n\\emph{agency} is the appropriate lens to study these harms and benefits, but\nthat doing so will require advancement in the theory of agency, and advancement\nin how this theory is applied in (agent-based) models.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00648v1",
    "published_date": "2025-02-02 03:27:19 UTC",
    "updated_date": "2025-02-02 03:27:19 UTC"
  },
  {
    "arxiv_id": "2502.00646v1",
    "title": "TrojanTime: Backdoor Attacks on Time Series Classification",
    "authors": [
      "Chang Dong",
      "Zechao Sun",
      "Guangdong Bai",
      "Shuying Piao",
      "Weitong Chen",
      "Wei Emma Zhang"
    ],
    "abstract": "Time Series Classification (TSC) is highly vulnerable to backdoor attacks,\nposing significant security threats. Existing methods primarily focus on data\npoisoning during the training phase, designing sophisticated triggers to\nimprove stealthiness and attack success rate (ASR). However, in practical\nscenarios, attackers often face restrictions in accessing training data.\nMoreover, it is a challenge for the model to maintain generalization ability on\nclean test data while remaining vulnerable to poisoned inputs when data is\ninaccessible. To address these challenges, we propose TrojanTime, a novel\ntwo-step training algorithm. In the first stage, we generate a pseudo-dataset\nusing an external arbitrary dataset through target adversarial attacks. The\nclean model is then continually trained on this pseudo-dataset and its poisoned\nversion. To ensure generalization ability, the second stage employs a carefully\ndesigned training strategy, combining logits alignment and batch norm freezing.\nWe evaluate TrojanTime using five types of triggers across four TSC\narchitectures in UCR benchmark datasets from diverse domains. The results\ndemonstrate the effectiveness of TrojanTime in executing backdoor attacks while\nmaintaining clean accuracy. Finally, to mitigate this threat, we propose a\ndefensive unlearning strategy that effectively reduces the ASR while preserving\nclean accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "I.2.0"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.00646v1",
    "published_date": "2025-02-02 03:24:24 UTC",
    "updated_date": "2025-02-02 03:24:24 UTC"
  },
  {
    "arxiv_id": "2502.00641v2",
    "title": "Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance",
    "authors": [
      "Borui Xu",
      "Yao Chen",
      "Zeyi Wen",
      "Weiguo Liu",
      "Bingsheng He"
    ],
    "abstract": "The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00641v2",
    "published_date": "2025-02-02 03:07:45 UTC",
    "updated_date": "2025-02-11 13:12:16 UTC"
  },
  {
    "arxiv_id": "2502.00640v1",
    "title": "CollabLLM: From Passive Responders to Active Collaborators",
    "authors": [
      "Shirley Wu",
      "Michel Galley",
      "Baolin Peng",
      "Hao Cheng",
      "Gavin Li",
      "Yao Dou",
      "Weixin Cai",
      "James Zou",
      "Jure Leskovec",
      "Jianfeng Gao"
    ],
    "abstract": "Large Language Models are typically trained with next-turn rewards, limiting\ntheir ability to optimize for long-term interaction. As a result, they often\nrespond passively to ambiguous or open-ended user requests, failing to help\nusers reach their ultimate intents and leading to inefficient conversations. To\naddress these limitations, we introduce CollabLLM, a novel and general training\nframework that enhances multiturn human-LLM collaboration. Its key innovation\nis a collaborative simulation that estimates the long-term contribution of\nresponses using Multiturn-aware Rewards. By reinforcement fine-tuning these\nrewards, CollabLLM goes beyond responding to user requests, and actively\nuncovers user intent and offers insightful suggestions-a key step towards more\nhuman-centered AI. We also devise a multiturn interaction benchmark with three\nchallenging tasks such as document creation. CollabLLM significantly\noutperforms our baselines with averages of 18.5% higher task performance and\n46.3% improved interactivity by LLM judges. Finally, we conduct a large user\nstudy with 201 judges, where CollabLLM increases user satisfaction by 17.6% and\nreduces user spent time by 10.4%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.00640v1",
    "published_date": "2025-02-02 03:05:52 UTC",
    "updated_date": "2025-02-02 03:05:52 UTC"
  },
  {
    "arxiv_id": "2502.00639v2",
    "title": "Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer",
    "authors": [
      "Tao Ren",
      "Zishi Zhang",
      "Zehao Li",
      "Jingyang Jiang",
      "Shentao Qin",
      "Guanghao Li",
      "Yan Li",
      "Yi Zheng",
      "Xinping Li",
      "Min Zhan",
      "Yijie Peng"
    ],
    "abstract": "The probabilistic diffusion model (DM), generating content by inferencing\nthrough a recursive chain structure, has emerged as a powerful framework for\nvisual generation. After pre-training on enormous unlabeled data, the model\nneeds to be properly aligned to meet requirements for downstream applications.\nHow to efficiently align the foundation DM is a crucial task. Contemporary\nmethods are either based on Reinforcement Learning (RL) or truncated\nBackpropagation (BP). However, RL and truncated BP suffer from low sample\nefficiency and biased gradient estimation respectively, resulting in limited\nimprovement or, even worse, complete training failure. To overcome the\nchallenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a\nzeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient\nestimator enables the computation graph rearrangement within the recursive\ndiffusive chain, making the RLR's gradient estimator an unbiased one with the\nlower variance than other methods. We provide theoretical guarantees for the\nperformance of the RLR. Extensive experiments are conducted on image and video\ngeneration tasks to validate the superiority of the RLR. Furthermore, we\npropose a novel prompt technique that is natural for the RLR to achieve a\nsynergistic effect.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00639v2",
    "published_date": "2025-02-02 03:00:26 UTC",
    "updated_date": "2025-03-25 02:35:02 UTC"
  },
  {
    "arxiv_id": "2502.00634v2",
    "title": "SimulPL: Aligning Human Preferences in Simultaneous Machine Translation",
    "authors": [
      "Donglei Yu",
      "Yang Zhao",
      "Jie Zhu",
      "Yangyifan Xu",
      "Yu Zhou",
      "Chengqing Zong"
    ],
    "abstract": "Simultaneous Machine Translation (SiMT) generates translations while\nreceiving streaming source inputs. This requires the SiMT model to learn a\nread/write policy, deciding when to translate and when to wait for more source\ninput. Numerous linguistic studies indicate that audiences in SiMT scenarios\nhave distinct preferences, such as accurate translations, simpler syntax, and\nno unnecessary latency. Aligning SiMT models with these human preferences is\ncrucial to improve their performances. However, this issue still remains\nunexplored. Additionally, preference optimization for SiMT task is also\nchallenging. Existing methods focus solely on optimizing the generated\nresponses, ignoring human preferences related to latency and the optimization\nof read/write policy during the preference optimization phase. To address these\nchallenges, we propose Simultaneous Preference Learning (SimulPL), a preference\nlearning framework tailored for the SiMT task. In the SimulPL framework, we\ncategorize SiMT human preferences into five aspects: \\textbf{translation\nquality preference}, \\textbf{monotonicity preference}, \\textbf{key point\npreference}, \\textbf{simplicity preference}, and \\textbf{latency preference}.\nBy leveraging the first four preferences, we construct human preference prompts\nto efficiently guide GPT-4/4o in generating preference data for the SiMT task.\nIn the preference optimization phase, SimulPL integrates \\textbf{latency\npreference} into the optimization objective and enables SiMT models to improve\nthe read/write policy, thereby aligning with human preferences more\neffectively. Experimental results indicate that SimulPL exhibits better\nalignment with human preferences across all latency levels in\nZh$\\rightarrow$En, De$\\rightarrow$En and En$\\rightarrow$Zh SiMT tasks. Our data\nand code will be available at https://github.com/EurekaForNLP/SimulPL.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025. 23 pages,13 figures,11 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.00634v2",
    "published_date": "2025-02-02 02:47:09 UTC",
    "updated_date": "2025-02-05 12:36:08 UTC"
  },
  {
    "arxiv_id": "2502.00633v1",
    "title": "Lipschitz Lifelong Monte Carlo Tree Search for Mastering Non-Stationary Tasks",
    "authors": [
      "Zuyuan Zhang",
      "Tian Lan"
    ],
    "abstract": "Monte Carlo Tree Search (MCTS) has proven highly effective in solving complex\nplanning tasks by balancing exploration and exploitation using Upper Confidence\nBound for Trees (UCT). However, existing work have not considered MCTS-based\nlifelong planning, where an agent faces a non-stationary series of tasks --\ne.g., with varying transition probabilities and rewards -- that are drawn\nsequentially throughout the operational lifetime. This paper presents LiZero\nfor Lipschitz lifelong planning using MCTS. We propose a novel concept of\nadaptive UCT (aUCT) to transfer knowledge from a source task to the\nexploration/exploitation of a new task, depending on both the Lipschitz\ncontinuity between tasks and the confidence of knowledge in in Monte Carlo\naction sampling. We analyze LiZero's acceleration factor in terms of improved\nsampling efficiency and also develop efficient algorithms to compute aUCT in an\nonline fashion by both data-driven and model-based approaches, whose sampling\ncomplexity and error bounds are also characterized. Experiment results show\nthat LiZero significantly outperforms existing MCTS and lifelong learning\nbaselines in terms of much faster convergence (3$\\sim$4x) to optimal rewards.\nOur results highlight the potential of LiZero to advance decision-making and\nplanning in dynamic real-world environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00633v1",
    "published_date": "2025-02-02 02:45:20 UTC",
    "updated_date": "2025-02-02 02:45:20 UTC"
  },
  {
    "arxiv_id": "2502.00629v1",
    "title": "Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic Mathematical Reasoning",
    "authors": [
      "Yuxuan Wu",
      "Hideki Nakayama"
    ],
    "abstract": "In recent years, neuro-symbolic methods have become a popular and powerful\napproach that augments artificial intelligence systems with the capability to\nperform abstract, logical, and quantitative deductions with enhanced precision\nand controllability. Recent studies successfully performed symbolic reasoning\nby leveraging various machine learning models to explicitly or implicitly\npredict intermediate labels that provide symbolic instructions. However, these\nintermediate labels are not always prepared for every task as a part of\ntraining data, and pre-trained models, represented by Large Language Models\n(LLMs), also do not consistently generate valid symbolic instructions with\ntheir intrinsic knowledge. On the other hand, existing work developed\nalternative learning techniques that allow the learning system to autonomously\nuncover optimal symbolic instructions. Nevertheless, their performance also\nexhibits limitations when faced with relatively huge search spaces or more\nchallenging reasoning problems. In view of this, in this work, we put forward\nan advanced practice for neuro-symbolic reasoning systems to explore the\nintermediate labels with weak supervision from problem inputs and final\noutputs. Our experiments on the Mathematics dataset illustrated the\neffectiveness of our proposals from multiple aspects.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00629v1",
    "published_date": "2025-02-02 02:34:36 UTC",
    "updated_date": "2025-02-02 02:34:36 UTC"
  },
  {
    "arxiv_id": "2502.01677v2",
    "title": "Position: AI Scaling: From Up to Down and Out",
    "authors": [
      "Yunke Wang",
      "Yanxi Li",
      "Chang Xu"
    ],
    "abstract": "AI Scaling has traditionally been synonymous with Scaling Up, which builds\nlarger and more powerful models. However, the growing demand for efficiency,\nadaptability, and collaboration across diverse applications necessitates a\nbroader perspective. This position paper presents a holistic framework for AI\nscaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that\nwhile Scaling Up of models faces inherent bottlenecks, the future trajectory of\nAI scaling lies in Scaling Down and Scaling Out. These paradigms address\ncritical technical and societal challenges, such as reducing carbon footprint,\nensuring equitable access, and enhancing cross-domain collaboration. We explore\ntransformative applications in healthcare, smart manufacturing, and content\ncreation, demonstrating how AI Scaling can enable breakthroughs in efficiency,\npersonalization, and global connectivity. Additionally, we highlight key\nchallenges, including balancing model complexity with interpretability,\nmanaging resource constraints, and fostering ethical development. By\nsynthesizing these approaches, we propose a unified roadmap that redefines the\nfuture of AI research and application, paving the way for advancements toward\nArtificial General Intelligence (AGI).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.01677v2",
    "published_date": "2025-02-02 02:14:00 UTC",
    "updated_date": "2025-05-13 04:47:13 UTC"
  },
  {
    "arxiv_id": "2502.00620v2",
    "title": "Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions",
    "authors": [
      "Yihao Xue",
      "Jiping Li",
      "Baharan Mirzasoleiman"
    ],
    "abstract": "Weak-to-Strong Generalization (W2SG), where a weak model supervises a\nstronger one, serves as an important analogy for understanding how humans might\nguide superhuman intelligence in the future. Promising empirical results\nrevealed that a strong model can surpass its weak supervisor. While recent work\nhas offered theoretical insights into this phenomenon, a clear understanding of\nthe interactions between weak and strong models that drive W2SG remains\nelusive. We investigate W2SG through a theoretical lens and show that it can be\ncharacterized using kernels derived from the principal components of weak and\nstrong models' internal representations. These kernels can be used to define a\nspace that, at a high level, captures what the weak model is unable to learn\nbut is learnable by the strong model. The projection of labels onto this space\nquantifies how much the strong model falls short of its full potential due to\nweak supervision. This characterization also provides insights into how certain\nerrors in weak supervision can be corrected by the strong model, regardless of\noverfitting. Our theory has significant practical implications, providing a\nrepresentation-based metric that predicts W2SG performance trends without\nrequiring labels, as shown in experiments on molecular predictions with\ntransformers and 5 NLP tasks involving 52 LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00620v2",
    "published_date": "2025-02-02 01:11:51 UTC",
    "updated_date": "2025-02-05 00:36:00 UTC"
  },
  {
    "arxiv_id": "2502.00619v1",
    "title": "Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective",
    "authors": [
      "Yujin Oh",
      "Pengfei Jin",
      "Sangjoon Park",
      "Sekeun Kim",
      "Siyeop Yoon",
      "Kyungsang Kim",
      "Jin Sung Kim",
      "Xiang Li",
      "Quanzheng Li"
    ],
    "abstract": "Ensuring fairness in medical image segmentation is critical due to biases in\nimbalanced clinical data acquisition caused by demographic attributes (e.g.,\nage, sex, race) and clinical factors (e.g., disease severity). To address these\nchallenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired\nby optimal control theory. We provide a comprehensive analysis of its\nunderlying mechanisms and clarify dMoE's role in adapting to heterogeneous\ndistributions in medical image segmentation. Furthermore, we integrate dMoE\ninto multiple network architectures, demonstrating its broad applicability\nacross diverse medical image analysis tasks. By incorporating demographic and\nclinical factors, dMoE achieves state-of-the-art performance on two 2D\nbenchmark datasets and a 3D in-house dataset. Our results highlight the\neffectiveness of dMoE in mitigating biases from imbalanced distributions,\noffering a promising approach to bridging control theory and medical image\nsegmentation within fairness learning paradigms. The source code will be made\navailable.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 3 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.00619v1",
    "published_date": "2025-02-02 01:10:31 UTC",
    "updated_date": "2025-02-02 01:10:31 UTC"
  },
  {
    "arxiv_id": "2502.00618v1",
    "title": "DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models",
    "authors": [
      "Chiyuan He",
      "Zihuan Qiu",
      "Fanman Meng",
      "Linfeng Xu",
      "Qingbo Wu",
      "Hongliang Li"
    ],
    "abstract": "Continual adaptation of vision-language models (VLMs) focuses on leveraging\ncross-modal pretrained knowledge to incrementally adapt for expanding\ndownstream tasks and datasets, while tackling the challenge of knowledge\nforgetting. Existing research often focuses on connecting visual features with\nspecific class text in downstream tasks, overlooking the latent relationships\nbetween general and specialized knowledge. Our findings reveal that forcing\nmodels to optimize inappropriate visual-text matches exacerbates forgetting of\nVLMs. To tackle this issue, we propose DesCLIP, which leverages general\nattribute (GA) descriptions to guide the understanding of specific class\nobjects, enabling VLMs to establish robust \\textit{vision-GA-class} trilateral\nassociations rather than relying solely on \\textit{vision-class} connections.\nSpecifically, we introduce a language assistant to generate concrete GA\ndescription candidates via proper request prompts. Then, an anchor-based\nembedding filter is designed to obtain highly relevant GA description\nembeddings, which are leveraged as the paired text embeddings for\nvisual-textual instance matching, thereby tuning the visual encoder.\nCorrespondingly, the class text embeddings are gradually calibrated to align\nwith these shared GA description embeddings. Extensive experiments demonstrate\nthe advancements and efficacy of our proposed method, with comprehensive\nempirical evaluations highlighting its superior performance compared to\nexisting pretrained and VLM-based continual learning methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00618v1",
    "published_date": "2025-02-02 01:06:02 UTC",
    "updated_date": "2025-02-02 01:06:02 UTC"
  },
  {
    "arxiv_id": "2502.00611v1",
    "title": "Enhancing Code Consistency in AI Research with Large Language Models and Retrieval-Augmented Generation",
    "authors": [
      "Rajat Keshri",
      "Arun George Zachariah",
      "Michael Boone"
    ],
    "abstract": "Ensuring that code accurately reflects the algorithms and methods described\nin research papers is critical for maintaining credibility and fostering trust\nin AI research. This paper presents a novel system designed to verify code\nimplementations against the algorithms and methodologies outlined in\ncorresponding research papers. Our system employs Retrieval-Augmented\nGeneration to extract relevant details from both the research papers and code\nbases, followed by a structured comparison using Large Language Models. This\napproach improves the accuracy and comprehensiveness of code implementation\nverification while contributing to the transparency, explainability, and\nreproducibility of AI research. By automating the verification process, our\nsystem reduces manual effort, enhances research credibility, and ultimately\nadvances the state of the art in code verification.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00611v1",
    "published_date": "2025-02-02 00:35:42 UTC",
    "updated_date": "2025-02-02 00:35:42 UTC"
  },
  {
    "arxiv_id": "2502.00604v1",
    "title": "Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective",
    "authors": [
      "Sifan Wang",
      "Ananyae Kumar Bhartari",
      "Bowen Li",
      "Paris Perdikaris"
    ],
    "abstract": "Multi-task learning through composite loss functions is fundamental to modern\ndeep learning, yet optimizing competing objectives remains challenging. We\npresent new theoretical and practical approaches for addressing directional\nconflicts between loss terms, demonstrating their effectiveness in\nphysics-informed neural networks (PINNs) where such conflicts are particularly\nchallenging to resolve. Through theoretical analysis, we demonstrate how these\nconflicts limit first-order methods and show that second-order optimization\nnaturally resolves them through implicit gradient alignment. We prove that\nSOAP, a recently proposed quasi-Newton method, efficiently approximates the\nHessian preconditioner, enabling breakthrough performance in PINNs:\nstate-of-the-art results on 10 challenging PDE benchmarks, including the first\nsuccessful application to turbulent flows with Reynolds numbers up to 10,000,\nwith 2-10x accuracy improvements over existing methods. We also introduce a\nnovel gradient alignment score that generalizes cosine similarity to multiple\ngradients, providing a practical tool for analyzing optimization dynamics. Our\nfindings establish frameworks for understanding and resolving gradient\nconflicts, with broad implications for optimization beyond scientific\ncomputing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00604v1",
    "published_date": "2025-02-02 00:21:45 UTC",
    "updated_date": "2025-02-02 00:21:45 UTC"
  },
  {
    "arxiv_id": "2502.06803v1",
    "title": "Emotion Recognition and Generation: A Comprehensive Review of Face, Speech, and Text Modalities",
    "authors": [
      "Rebecca Mobbs",
      "Dimitrios Makris",
      "Vasileios Argyriou"
    ],
    "abstract": "Emotion recognition and generation have emerged as crucial topics in\nArtificial Intelligence research, playing a significant role in enhancing\nhuman-computer interaction within healthcare, customer service, and other\nfields. Although several reviews have been conducted on emotion recognition and\ngeneration as separate entities, many of these works are either fragmented or\nlimited to specific methodologies, lacking a comprehensive overview of recent\ndevelopments and trends across different modalities. In this survey, we provide\na holistic review aimed at researchers beginning their exploration in emotion\nrecognition and generation. We introduce the fundamental principles underlying\nemotion recognition and generation across facial, vocal, and textual\nmodalities. This work categorises recent state-of-the-art research into\ndistinct technical approaches and explains the theoretical foundations and\nmotivations behind these methodologies, offering a clearer understanding of\ntheir application. Moreover, we discuss evaluation metrics, comparative\nanalyses, and current limitations, shedding light on the challenges faced by\nresearchers in the field. Finally, we propose future research directions to\naddress these challenges and encourage further exploration into developing\nrobust, effective, and ethically responsible emotion recognition and generation\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to ACM Computing Surveys",
    "pdf_url": "http://arxiv.org/pdf/2502.06803v1",
    "published_date": "2025-02-02 00:11:19 UTC",
    "updated_date": "2025-02-02 00:11:19 UTC"
  }
]
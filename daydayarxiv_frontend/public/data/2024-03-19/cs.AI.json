{
  "date": "2024-03-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-19 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的鲁棒性、强化学习、LLM（Large Language Models）应用、图像处理和机器人技术等领域，其中令人印象深刻的包括 NeRF 的深度引导训练（涉及知名学者 Serena Yeung-Levy）和 LLM 在机器人决策中的创新应用，如 Vid2Robot；同时，有多篇论文被顶级会议接受，如 ECCV 2024 和 NAACL 2024，突显了 AI 安全、跨模态融合和高效计算的热点。\n\n下面，我将挑选并简要讨论部分关键论文，先优先聊那些重要、话题度高或有著名学者的作品，并将相关主题归类快速概述。其他次要论文（如数学建模或特定领域的小众工作）将简略掠过，以控制篇幅。\n\n### AI 安全与鲁棒性\n- **Depth-guided NeRF Training via Earth Mover's Distance**（中文：基于 Earth Mover's Distance 的深度引导 NeRF 训练；作者：Anita Rau 等，包括著名学者 Serena Yeung-Levy）：这篇论文提出使用预训练扩散模型预测深度并通过 Earth Mover's Distance 监督 NeRF 训练，显著提升了 3D 场景重建的深度精度和光度性能，在标准基准上大幅超越基线，适用于鲁棒性增强的计算机视觉任务。\n- **ADAPT to Robustify Prompt Tuning Vision Transformers**（中文：ADAPT：用于增强 Vision Transformers 提示调优的鲁棒框架；作者：Masih Eskandar 等）：论文引入 ADAPT 框架，通过自适应对抗训练优化提示调优，仅调整约 1% 参数，即在 Vision Transformers 上实现 40% 的鲁棒准确率，显著改善了模型对对抗攻击的抵抗力。\n- **A Study of Vulnerability Repair in JavaScript Programs with Large Language Models**（中文：使用大型语言模型修复 JavaScript 程序漏洞的研究；作者：Tan Khang Le 等）：研究评估 ChatGPT 和 Bard 在修复 JavaScript 安全漏洞时的准确性，强调提示中的上下文量对修复成功的影响，为 LLM 在代码安全领域的应用提供实用指导。\n\n这些论文共同探讨 AI 模型的鲁棒性和安全性，强调在真实世界部署中的实际意义，尤其 ADAPT 的高效参数优化值得关注。\n\n### LLM 与生成模型应用\n- **Self-generated Replay Memories for Continual Neural Machine Translation**（中文：用于持续神经机器翻译的自生成重放记忆；作者：Michele Resta 等）：论文利用 Transformer 的生成能力创建自重放记忆库，缓解灾难性遗忘问题，实现多语言持续学习，被 NAACL 2024 接受。\n- **Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers**（中文：Vid2Robot：基于跨注意力 Transformer 的端到端视频条件策略学习；作者：Vidhi Jain 等）：这篇话题度高的论文（Robotics: Science & Systems 2024）提出从人类视频中学习机器人策略，通过跨注意力机制实现物体动作转移，显著提升机器人模仿学习性能。\n- **Yell At Your Robot: Improving On-the-Fly from Language Corrections**（中文：对机器人喊话：通过语言修正实现即时改进；作者：Lucy Xiaoyang Shi 等）：论文设计一个层次化策略，使用语言反馈实时优化机器人行为，适用于长时任务的交互学习，展示了 LLM 在人机协作中的潜力。\n\nLLM 相关工作突出其在翻译和机器人领域的生成与适应能力，Vid2Robot 的端到端设计尤其创新，桥接了视觉和动作学习。\n\n### 计算机视觉与图像处理\n- **TexTile: A Differentiable Metric for Texture Tileability**（中文：TexTile：一种可微纹理平铺度度量；作者：Carlos Rodriguez-Pardo 等）：论文引入可微度量 TexTile 来评估纹理的平铺性，并将其集成到扩散模型中，提升纹理合成质量，被 CVPR 2024 接受。\n- **Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models**（中文：只需平移：基于 Vision-Language Models 的零样本泛化测试时原型平移；作者：Elaine Sui 等，包括 Serena Yeung-Levy）：工作提出测试时原型平移框架，提升 VLM 在领域偏移下的分类准确性，显著减少计算资源需求。\n- **FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer**（中文：FlowerFormer：基于流量感知图 Transformer 的神经架构编码增强；作者：Dongyeong Hwang 等）：论文使用流量感知机制改进图 Transformer，提升神经架构搜索的表示学习，适用于高效图像处理。\n\n这些视觉论文聚焦高效表示和泛化，FlowerFormer 的图学习方法在计算效率上表现出色。\n\n### 机器人与强化学习\n- **WHAC: World-grounded Humans and Cameras**（中文：WHAC：基于世界的地面人类和相机建模；作者：Wanqi Yin 等）：论文提出框架从单目视频中恢复人类姿势和相机轨迹，无需传统优化，适用于机器人感知，被 CVPR 2024 接受。\n- **Fast Value Tracking for Deep Reinforcement Learning**（中文：深度强化学习的快速价值追踪；作者：Frank Shih 等）：引入 Langevinized Kalman Temporal-Difference 算法，提升 RL 的不确定性量化，实现更鲁棒的政策更新。\n- **AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks**（中文：AdaptSFL：资源受限边缘网络中的自适应分割联邦学习；作者：Zheng Lin 等）：论文优化模型分割和聚合，加速联邦学习在边缘设备的应用，显著减少通信延迟。\n\n机器人和 RL 论文强调资源效率和实际部署，WHAC 的零样本恢复技术特别有话题度。\n\n其他论文，如生物医学图像分析（e.g., **Deep learning with noisy labels in medical prediction problems**，中文：医疗预测问题中带噪声标签的深度学习）或诗歌分析（e.g., **AraPoemBERT**，中文：AraPoemBERT：用于阿拉伯诗歌分析的预训练语言模型），虽有贡献但非核心焦点，仅提及其在特定领域（如医疗噪声处理或诗歌分类）的改进，篇幅有限不做深究。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，以鲁棒性和高效计算为主线，期待这些工作推动实际应用。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2403.13206v2",
      "title": "Depth-guided NeRF Training via Earth Mover's Distance",
      "title_zh": "翻译失败",
      "authors": [
        "Anita Rau",
        "Josiah Aklilu",
        "F. Christopher Holsinger",
        "Serena Yeung-Levy"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of\npredicted viewpoints. However, the photometric loss often does not provide\nenough information to disambiguate between different possible geometries\nyielding the same image. Previous work has thus incorporated depth supervision\nduring NeRF training, leveraging dense predictions from pre-trained depth\nnetworks as pseudo-ground truth. While these depth priors are assumed to be\nperfect once filtered for noise, in practice, their accuracy is more\nchallenging to capture. This work proposes a novel approach to uncertainty in\ndepth priors for NeRF supervision. Instead of using custom-trained depth or\nuncertainty priors, we use off-the-shelf pretrained diffusion models to predict\ndepth and capture uncertainty during the denoising process. Because we know\nthat depth priors are prone to errors, we propose to supervise the ray\ntermination distance distribution with Earth Mover's Distance instead of\nenforcing the rendered depth to replicate the depth prior exactly through\nL2-loss. Our depth-guided NeRF outperforms all baselines on standard depth\nmetrics by a large margin while maintaining performance on photometric\nmeasures.",
      "tldr_zh": "本文提出了一种通过 Earth Mover's Distance 引导 NeRF 训练的新方法，以解决传统光度损失在区分不同几何结构时的模糊性问题。不同于以往依赖预训练深度网络的伪真实值，本文利用现成的预训练扩散模型预测深度并捕获不确定性，然后使用 Earth Mover's Distance 来监督射线终止距离分布，而不是强制采用 L2-loss 精确复制深度先验。这种方法显著提升了 NeRF 在标准深度指标上的性能，同时保持了光度指标的原有水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.13206v2",
      "published_date": "2024-03-19 23:54:07 UTC",
      "updated_date": "2024-09-04 22:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:52:10.709651"
    },
    {
      "arxiv_id": "2403.13196v2",
      "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Masih Eskandar",
        "Tooba Imtiaz",
        "Zifeng Wang",
        "Jennifer Dy"
      ],
      "abstract": "The performance of deep models, including Vision Transformers, is known to be\nvulnerable to adversarial attacks. Many existing defenses against these\nattacks, such as adversarial training, rely on full-model fine-tuning to induce\nrobustness in the models. These defenses require storing a copy of the entire\nmodel, that can have billions of parameters, for each task. At the same time,\nparameter-efficient prompt tuning is used to adapt large transformer-based\nmodels to downstream tasks without the need to save large copies. In this\npaper, we examine parameter-efficient prompt tuning of Vision Transformers for\ndownstream tasks under the lens of robustness. We show that previous\nadversarial defense methods, when applied to the prompt tuning paradigm, suffer\nfrom gradient obfuscation and are vulnerable to adaptive attacks. We introduce\nADAPT, a novel framework for performing adaptive adversarial training in the\nprompt tuning paradigm. Our method achieves competitive robust accuracy of ~40%\nw.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1%\nof the number of parameters.",
      "tldr_zh": "该研究发现，Vision Transformers 等深度模型容易受到对抗攻击，而现有防御方法如对抗训练需进行全模型微调，导致存储大量参数。论文分析了参数高效的提示调优（prompt tuning）在鲁棒性方面的不足，指出现有方法易受梯度混淆（gradient obfuscation）和适应攻击影响。作者提出 ADAPT 框架，这是一种在提示调优范式中进行适应性对抗训练的新方法，仅需调优约 1% 的参数，就实现了与 SOTA 鲁棒性方法相当的鲁棒准确率（约 40%）。这项工作为高效鲁棒模型适应下游任务提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research (2025)",
      "pdf_url": "http://arxiv.org/pdf/2403.13196v2",
      "published_date": "2024-03-19 23:13:40 UTC",
      "updated_date": "2025-02-07 18:04:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:52:22.818674"
    },
    {
      "arxiv_id": "2403.13193v1",
      "title": "A Study of Vulnerability Repair in JavaScript Programs with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Tan Khang Le",
        "Saba Alimadadi",
        "Steven Y. Ko"
      ],
      "abstract": "In recent years, JavaScript has become the most widely used programming\nlanguage, especially in web development. However, writing secure JavaScript\ncode is not trivial, and programmers often make mistakes that lead to security\nvulnerabilities in web applications. Large Language Models (LLMs) have\ndemonstrated substantial advancements across multiple domains, and their\nevolving capabilities indicate their potential for automatic code generation\nbased on a required specification, including automatic bug fixing. In this\nstudy, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and\nfixing security vulnerabilities in JavaScript programs. We also investigate the\nimpact of context in a prompt on directing LLMs to produce a correct patch of\nvulnerable JavaScript code. Our experiments on real-world software\nvulnerabilities show that while LLMs are promising in automatic program repair\nof JavaScript code, achieving a correct bug fix often requires an appropriate\namount of context in the prompt.",
      "tldr_zh": "这篇论文研究了使用 Large Language Models (LLMs) 如 ChatGPT 和 Bard 来发现和修复 JavaScript 程序中的安全漏洞。研究者通过实验评估了 LLMs 在处理真实世界软件漏洞时的准确性，并探讨了提示中的上下文量对生成正确补丁的影响。结果表明，LLMs 在自动程序修复方面具有潜力，但需要提供适当的上下文才能实现有效修复。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "camera-ready version accepted to the short paper track at WWW'24",
      "pdf_url": "http://arxiv.org/pdf/2403.13193v1",
      "published_date": "2024-03-19 23:04:03 UTC",
      "updated_date": "2024-03-19 23:04:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:52:32.884579"
    },
    {
      "arxiv_id": "2403.13178v1",
      "title": "Fast Value Tracking for Deep Reinforcement Learning",
      "title_zh": "快速价值跟踪用于深度强化学习",
      "authors": [
        "Frank Shih",
        "Faming Liang"
      ],
      "abstract": "Reinforcement learning (RL) tackles sequential decision-making problems by\ncreating agents that interacts with their environment. However, existing\nalgorithms often view these problem as static, focusing on point estimates for\nmodel parameters to maximize expected rewards, neglecting the stochastic\ndynamics of agent-environment interactions and the critical role of uncertainty\nquantification. Our research leverages the Kalman filtering paradigm to\nintroduce a novel and scalable sampling algorithm called Langevinized Kalman\nTemporal-Difference (LKTD) for deep reinforcement learning. This algorithm,\ngrounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently\ndraws samples from the posterior distribution of deep neural network\nparameters. Under mild conditions, we prove that the posterior samples\ngenerated by the LKTD algorithm converge to a stationary distribution. This\nconvergence not only enables us to quantify uncertainties associated with the\nvalue function and model parameters but also allows us to monitor these\nuncertainties during policy updates throughout the training phase. The LKTD\nalgorithm paves the way for more robust and adaptable reinforcement learning\napproaches.",
      "tldr_zh": "本研究针对强化学习（Reinforcement Learning, RL）算法忽略不确定性和动态交互的问题，提出了一种新型可扩展算法——Langevinized Kalman Temporal-Difference (LKTD)。LKTD 基于 Kalman 过滤和 Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) 技术，从深度神经网络参数的后验分布中高效采样。在温和条件下，LKTD 被证明能收敛到平稳分布，从而量化价值函数和模型参数的不确定性，并在训练过程中的策略更新中进行监控。该算法显著提升了 RL 的鲁棒性和适应性，为更可靠的决策系统奠定基础。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13178v1",
      "published_date": "2024-03-19 22:18:19 UTC",
      "updated_date": "2024-03-19 22:18:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:52:47.413820"
    },
    {
      "arxiv_id": "2403.13150v2",
      "title": "On Training Survival Models with Scoring Rules",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Kopper",
        "David Rügamer",
        "Raphael Sonabend",
        "Bernd Bischl",
        "Andreas Bender"
      ],
      "abstract": "Scoring rules are an established way of comparing predictive performances\nacross model classes. In the context of survival analysis, they require\nadaptation in order to accommodate censoring. This work investigates using\nscoring rules for model training rather than evaluation. Doing so, we establish\na general framework for training survival models that is model agnostic and can\nlearn event time distributions parametrically or non-parametrically. In\naddition, our framework is not restricted to any specific scoring rule. While\nwe focus on neural network-based implementations, we also provide\nproof-of-concept implementations using gradient boosting, generalized additive\nmodels, and trees. Empirical comparisons on synthetic and real-world data\nindicate that scoring rules can be successfully incorporated into model\ntraining and yield competitive predictive performance with established\ntime-to-event models.",
      "tldr_zh": "这篇论文探讨了使用Scoring rules训练生存模型的方法，以适应生存分析中的删失（Censoring）问题，而不是仅限于模型评估。作者提出一个模型无关（model agnostic）的通用框架，能够参数化或非参数化地学习事件时间分布，并支持多种Scoring rules的灵活应用。实验结果显示，该框架在合成和真实数据上成功整合Scoring rules，提供与传统时间到事件模型相媲美的预测性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.13150v2",
      "published_date": "2024-03-19 20:58:38 UTC",
      "updated_date": "2024-11-13 16:46:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:52:58.963898"
    },
    {
      "arxiv_id": "2403.13134v1",
      "title": "Robust NAS under adversarial training: benchmark, theory, and beyond",
      "title_zh": "在对抗训练下的鲁棒 NAS：基准、理论及超越",
      "authors": [
        "Yongtao Wu",
        "Fanghui Liu",
        "Carl-Johann Simon-Gabriel",
        "Grigorios G Chrysos",
        "Volkan Cevher"
      ],
      "abstract": "Recent developments in neural architecture search (NAS) emphasize the\nsignificance of considering robust architectures against malicious data.\nHowever, there is a notable absence of benchmark evaluations and theoretical\nguarantees for searching these robust architectures, especially when\nadversarial training is considered. In this work, we aim to address these two\nchallenges, making twofold contributions. First, we release a comprehensive\ndata set that encompasses both clean accuracy and robust accuracy for a vast\narray of adversarially trained networks from the NAS-Bench-201 search space on\nimage datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep\nlearning theory, we establish a generalization theory for searching\narchitecture in terms of clean accuracy and robust accuracy under\nmulti-objective adversarial training. We firmly believe that our benchmark and\ntheoretical insights will significantly benefit the NAS community through\nreliable reproducibility, efficient assessment, and theoretical foundation,\nparticularly in the pursuit of robust architectures.",
      "tldr_zh": "该研究针对神经架构搜索 (NAS) 在对抗训练下的鲁棒性问题，填补了基准评估和理论保证的缺失。论文的主要贡献包括发布一个全面数据集，涵盖 NAS-Bench-201 搜索空间中大量网络在图像数据集上的干净准确率 (clean accuracy) 和鲁棒准确率 (robust accuracy)。此外，他们利用神经切线核 (NTK) 工具建立了多目标对抗训练下的泛化理论，以提升架构搜索的可靠性。总体而言，这为 NAS 社区提供了可靠的再现性、评估效率和理论基础，推动了鲁棒架构的开发。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13134v1",
      "published_date": "2024-03-19 20:10:23 UTC",
      "updated_date": "2024-03-19 20:10:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:53:11.910882"
    },
    {
      "arxiv_id": "2403.13130v1",
      "title": "Self-generated Replay Memories for Continual Neural Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Michele Resta",
        "Davide Bacciu"
      ],
      "abstract": "Modern Neural Machine Translation systems exhibit strong performance in\nseveral different languages and are constantly improving. Their ability to\nlearn continuously is, however, still severely limited by the catastrophic\nforgetting issue. In this work, we leverage a key property of encoder-decoder\nTransformers, i.e. their generative ability, to propose a novel approach to\ncontinually learning Neural Machine Translation systems. We show how this can\neffectively learn on a stream of experiences comprising different languages, by\nleveraging a replay memory populated by using the model itself as a generator\nof parallel sentences. We empirically demonstrate that our approach can\ncounteract catastrophic forgetting without requiring explicit memorization of\ntraining data. Code will be publicly available upon publication. Code:\nhttps://github.com/m-resta/sg-rep",
      "tldr_zh": "该研究针对神经机器翻译 (Neural Machine Translation) 系统在持续学习中面临的灾难性遗忘 (catastrophic forgetting) 问题，提出了一种创新方法，利用编码器-解码器 Transformer 的生成能力。方法涉及使用模型自身作为生成器，创建自生成的重放内存 (replay memory) 来存储平行句子，从而处理不同语言的连续体验。实验结果表明，这种方法无需显式记忆训练数据，即可有效对抗遗忘，并在多语言流式学习任务上表现出色。代码已开源，可在 GitHub 上获取。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.13130v1",
      "published_date": "2024-03-19 19:59:54 UTC",
      "updated_date": "2024-03-19 19:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:53:21.774580"
    },
    {
      "arxiv_id": "2403.15465v1",
      "title": "Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchao Li",
        "Dimitri Bertsekas"
      ],
      "abstract": "In this paper we consider a transformer with an $n$-gram structure, such as\nthe one underlying ChatGPT. The transformer provides next word probabilities,\nwhich can be used to generate word sequences. We consider methods for computing\nword sequences that are highly likely, based on these probabilities. Computing\nthe optimal (i.e., most likely) word sequence starting with a given initial\nstate is an intractable problem, so we propose methods to compute highly likely\nsequences of $N$ words in time that is a low order polynomial in $N$ and in the\nvocabulary size of the $n$-gram. These methods are based on the rollout\napproach from approximate dynamic programming, a form of single policy\niteration, which can improve the performance of any given heuristic policy. In\nour case we use a greedy heuristic that generates as next word one that has the\nhighest probability. We show with analysis, examples, and computational\nexperimentation that our methods are capable of generating highly likely\nsequences with a modest increase in computation over the greedy heuristic.\nWhile our analysis and experiments are focused on Markov chains of the type\narising in transformer and ChatGPT-like models, our methods apply to general\nfinite-state Markov chains, and related inference applications of Hidden Markov\nModels (HMM), where Viterbi decoding is used extensively.",
      "tldr_zh": "该论文探讨了使用 Rollout Algorithms 生成高度可能的词序列，针对 n-Grams、Transformers、HMMs 和 Markov Chains 等模型。作者提出一种基于近似动态规划的单策略迭代方法，改进贪婪启发式策略（greedy heuristic），以在多项式时间内计算 N 词序列，从而克服计算最优序列的不可计算性（intractable）问题。通过分析、示例和实验验证，该方法仅需 modest 增加计算量，就能显著提升序列生成质量，并扩展适用于一般有限状态 Markov Chains 和 HMMs 的推断应用，如 Viterbi decoding。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15465v1",
      "published_date": "2024-03-19 19:58:46 UTC",
      "updated_date": "2024-03-19 19:58:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:53:38.176044"
    },
    {
      "arxiv_id": "2403.13125v1",
      "title": "Probabilistic Circuits with Constraints via Convex Optimization",
      "title_zh": "通过凸优化实现的带约束概率电路",
      "authors": [
        "Soroush Ghandi",
        "Benjamin Quost",
        "Cassio de Campos"
      ],
      "abstract": "This work addresses integrating probabilistic propositional logic constraints\ninto the distribution encoded by a probabilistic circuit (PC). PCs are a class\nof tractable models that allow efficient computations (such as conditional and\nmarginal probabilities) while achieving state-of-the-art performance in some\ndomains. The proposed approach takes both a PC and constraints as inputs, and\noutputs a new PC that satisfies the constraints. This is done efficiently via\nconvex optimization without the need to retrain the entire model. Empirical\nevaluations indicate that the combination of constraints and PCs can have\nmultiple use cases, including the improvement of model performance under scarce\nor incomplete data, as well as the enforcement of machine learning fairness\nmeasures into the model without compromising model fitness. We believe that\nthese ideas will open possibilities for multiple other applications involving\nthe combination of logics and deep probabilistic models.",
      "tldr_zh": "这篇论文提出了一种通过凸优化将概率命题逻辑约束整合到 Probabilistic Circuits (PCs) 中的方法，以高效修改模型分布，而无需重新训练整个模型。PCs 是一种可计算的概率模型，支持快速计算条件概率和边缘概率，并在某些领域达到最先进性能。该方法输入一个 PCs 和约束，输出一个满足约束的新 PCs，实验显示其能改善模型在数据稀缺或不完整情况下的性能，同时强制执行机器学习公平性措施而不影响模型拟合度。这种创新有望为逻辑与深度概率模型的结合开启更多应用场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13125v1",
      "published_date": "2024-03-19 19:55:38 UTC",
      "updated_date": "2024-03-19 19:55:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:53:46.938673"
    },
    {
      "arxiv_id": "2403.13111v1",
      "title": "Deep learning with noisy labels in medical prediction problems: a scoping review",
      "title_zh": "翻译失败",
      "authors": [
        "Yishu Wei",
        "Yu Deng",
        "Cong Sun",
        "Mingquan Lin",
        "Hongmei Jiang",
        "Yifan Peng"
      ],
      "abstract": "Objectives: Medical research faces substantial challenges from noisy labels\nattributed to factors like inter-expert variability and machine-extracted\nlabels. Despite this, the adoption of label noise management remains limited,\nand label noise is largely ignored. To this end, there is a critical need to\nconduct a scoping review focusing on the problem space. This scoping review\naims to comprehensively review label noise management in deep learning-based\nmedical prediction problems, which includes label noise detection, label noise\nhandling, and evaluation. Research involving label uncertainty is also\nincluded.\n  Methods: Our scoping review follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4\ndatabases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar.\nOur search terms include \"noisy label AND medical / healthcare / clinical\",\n\"un-certainty AND medical / healthcare / clinical\", and \"noise AND medical /\nhealthcare / clinical\".\n  Results: A total of 60 papers met inclusion criteria between 2016 and 2023. A\nseries of practical questions in medical research are investigated. These\ninclude the sources of label noise, the impact of label noise, the detection of\nlabel noise, label noise handling techniques, and their evaluation.\nCategorization of both label noise detection methods and handling techniques\nare provided.\n  Discussion: From a methodological perspective, we observe that the medical\ncommunity has been up to date with the broader deep-learning community, given\nthat most techniques have been evaluated on medical data. We recommend\nconsidering label noise as a standard element in medical research, even if it\nis not dedicated to handling noisy labels. Initial experiments can start with\neasy-to-implement methods, such as noise-robust loss functions, weighting, and\ncurriculum learning.",
      "tldr_zh": "这篇 scoping review 考察了在医疗预测问题中使用 deep learning 时噪声标签的挑战和管理策略，包括噪声标签检测、处理和评估。研究者遵循 PRISMA 指南，从 PubMed、IEEE Xplore、Google Scholar 和 Semantic Scholar 等数据库中筛选了2016-2023年间共60篇相关论文，分析了噪声标签的来源、影响以及处理技术。结果显示，医疗领域已与 broader deep-learning community 同步，大部分方法已在医疗数据上得到验证。讨论部分推荐将 noisy labels 视为医疗研究的标准元素，并建议从简单方法入手，如 noise-robust loss functions、weighting 和 curriculum learning。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13111v1",
      "published_date": "2024-03-19 19:24:00 UTC",
      "updated_date": "2024-03-19 19:24:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:54:01.774815"
    },
    {
      "arxiv_id": "2403.13106v1",
      "title": "Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data",
      "title_zh": "翻译失败",
      "authors": [
        "Divyansh Singhvi",
        "Andrej Erkelens",
        "Raghav Jain",
        "Diganta Misra",
        "Naomi Saphra"
      ],
      "abstract": "Measuring nonlinear feature interaction is an established approach to\nunderstanding complex patterns of attribution in many models. In this paper, we\nuse Shapley Taylor interaction indices (STII) to analyze the impact of\nunderlying data structure on model representations in a variety of modalities,\ntasks, and architectures. Considering linguistic structure in masked and\nauto-regressive language models (MLMs and ALMs), we find that STII increases\nwithin idiomatic expressions and that MLMs scale STII with syntactic distance,\nrelying more on syntax in their nonlinear structure than ALMs do. Our speech\nmodel findings reflect the phonetic principal that the openness of the oral\ncavity determines how much a phoneme varies based on its context. Finally, we\nstudy image classifiers and illustrate that feature interactions intuitively\nreflect object boundaries. Our wide range of results illustrates the benefits\nof interdisciplinary work and domain expertise in interpretability research.",
      "tldr_zh": "本研究使用 Shapley Taylor interaction indices (STII) 来测量非线性特征交互，从而揭示数据结构对各种模型表示的影响，包括语言、语音和图像领域。研究发现，在 masked language models (MLMs) 和 auto-regressive language models (ALMs) 中，STII 在惯用表达中增加，且 MLMs 更依赖句法距离相关的非线性结构，而 ALMs 则较少依赖语法；在语音模型中，STII 反映了语音学原理，即口腔开放度影响音素在上下文中的变化；在图像分类器中，特征交互直观地对应物体边界。总体而言，该工作强调了跨学科合作和领域专长在可解释性研究中的重要价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13106v1",
      "published_date": "2024-03-19 19:13:22 UTC",
      "updated_date": "2024-03-19 19:13:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:54:17.640881"
    },
    {
      "arxiv_id": "2403.13101v3",
      "title": "AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks",
      "title_zh": "AdaptSFL：在资源受限边缘",
      "authors": [
        "Zheng Lin",
        "Guanqiao Qu",
        "Wei Wei",
        "Xianhao Chen",
        "Kin K. Leung"
      ],
      "abstract": "The increasing complexity of deep neural networks poses significant barriers\nto democratizing them to resource-limited edge devices. To address this\nchallenge, split federated learning (SFL) has emerged as a promising solution\nby of floading the primary training workload to a server via model partitioning\nwhile enabling parallel training among edge devices. However, although system\noptimization substantially influences the performance of SFL under\nresource-constrained systems, the problem remains largely uncharted. In this\npaper, we provide a convergence analysis of SFL which quantifies the impact of\nmodel splitting (MS) and client-side model aggregation (MA) on the learning\nperformance, serving as a theoretical foundation. Then, we propose AdaptSFL, a\nnovel resource-adaptive SFL framework, to expedite SFL under\nresource-constrained edge computing systems. Specifically, AdaptSFL adaptively\ncontrols client-side MA and MS to balance communication-computing latency and\ntraining convergence. Extensive simulations across various datasets validate\nthat our proposed AdaptSFL framework takes considerably less time to achieve a\ntarget accuracy than benchmarks, demonstrating the effectiveness of the\nproposed strategies.",
      "tldr_zh": "这篇论文针对深度神经网络在资源受限边缘设备上的部署挑战，提出了AdaptSFL，一种自适应Split Federated Learning (SFL)框架，通过模型分区将主要训练负载卸载到服务器，同时支持边缘设备的并行训练。论文首先提供SFL的收敛分析，量化了Model Splitting (MS)和Client-side Model Aggregation (MA)对学习性能的影响，作为理论基础。AdaptSFL通过动态调整MA和MS来平衡通信计算延迟与训练收敛，最终在各种数据集上的广泛模拟实验中证明，它比基准框架更快地达到目标准确率，显著提升了资源受限边缘网络的训练效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.13101v3",
      "published_date": "2024-03-19 19:05:24 UTC",
      "updated_date": "2024-05-22 07:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:54:28.836014"
    },
    {
      "arxiv_id": "2403.13097v1",
      "title": "Simple Ingredients for Offline Reinforcement Learning",
      "title_zh": "离线强化学习的简单成分",
      "authors": [
        "Edoardo Cetin",
        "Andrea Tirinzoni",
        "Matteo Pirotta",
        "Alessandro Lazaric",
        "Yann Ollivier",
        "Ahmed Touati"
      ],
      "abstract": "Offline reinforcement learning algorithms have proven effective on datasets\nhighly connected to the target downstream task. Yet, leveraging a novel testbed\n(MOOD) in which trajectories come from heterogeneous sources, we show that\nexisting methods struggle with diverse data: their performance considerably\ndeteriorates as data collected for related but different tasks is simply added\nto the offline buffer. In light of this finding, we conduct a large empirical\nstudy where we formulate and test several hypotheses to explain this failure.\nSurprisingly, we find that scale, more than algorithmic considerations, is the\nkey factor influencing performance. We show that simple methods like AWAC and\nIQL with increased network size overcome the paradoxical failure modes from the\ninclusion of additional data in MOOD, and notably outperform prior\nstate-of-the-art algorithms on the canonical D4RL benchmark.",
      "tldr_zh": "这项研究发现，现有的离线强化学习算法在处理来自异构来源的数据时（如MOOD测试平台所示）表现不佳，因为简单添加相关任务的数据会显著降低性能。作者通过大规模实证研究测试了多个假设，结果表明规模（如网络大小）比算法细节更关键。最终，他们证明简单方法如AWAC和IQL，通过增加网络规模，能够克服这些失败模式，并在经典D4RL基准上超越现有最先进算法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13097v1",
      "published_date": "2024-03-19 18:57:53 UTC",
      "updated_date": "2024-03-19 18:57:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:54:40.265098"
    },
    {
      "arxiv_id": "2403.13091v1",
      "title": "JaxUED: A simple and useable UED library in Jax",
      "title_zh": "翻译失败",
      "authors": [
        "Samuel Coward",
        "Michael Beukman",
        "Jakob Foerster"
      ],
      "abstract": "We present JaxUED, an open-source library providing minimal dependency\nimplementations of modern Unsupervised Environment Design (UED) algorithms in\nJax. JaxUED leverages hardware acceleration to obtain on the order of 100x\nspeedups compared to prior, CPU-based implementations. Inspired by CleanRL, we\nprovide fast, clear, understandable, and easily modifiable implementations,\nwith the aim of accelerating research into UED. This paper describes our\nlibrary and contains baseline results. Code can be found at\nhttps://github.com/DramaCow/jaxued.",
      "tldr_zh": "我们介绍了 JaxUED，这是一个开源库，在 Jax 框架中提供现代无监督环境设计 (UED) 算法的简单、高效实现，利用硬件加速实现约 100 倍于 CPU 版本的速度提升。受 CleanRL 启发，该库设计为快速、清晰、可理解且易于修改，以加速 UED 相关研究。该论文描述了库的细节并提供了基线结果，代码可在 https://github.com/DramaCow/jaxued 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.13091v1",
      "published_date": "2024-03-19 18:40:50 UTC",
      "updated_date": "2024-03-19 18:40:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:54:51.595709"
    },
    {
      "arxiv_id": "2404.04268v1",
      "title": "The Use of Generative Search Engines for Knowledge Work and Complex Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Siddharth Suri",
        "Scott Counts",
        "Leijie Wang",
        "Chacha Chen",
        "Mengting Wan",
        "Tara Safavi",
        "Jennifer Neville",
        "Chirag Shah",
        "Ryen W. White",
        "Reid Andersen",
        "Georg Buscher",
        "Sathish Manivannan",
        "Nagu Rangan",
        "Longqi Yang"
      ],
      "abstract": "Until recently, search engines were the predominant method for people to\naccess online information. The recent emergence of large language models (LLMs)\nhas given machines new capabilities such as the ability to generate new digital\nartifacts like text, images, code etc., resulting in a new tool, a generative\nsearch engine, which combines the capabilities of LLMs with a traditional\nsearch engine. Through the empirical analysis of Bing Copilot (Bing Chat), one\nof the first publicly available generative search engines, we analyze the types\nand complexity of tasks that people use Bing Copilot for compared to Bing\nSearch. Findings indicate that people use the generative search engine for more\nknowledge work tasks that are higher in cognitive complexity than were commonly\ndone with a traditional search engine.",
      "tldr_zh": "本研究探讨了生成式搜索引擎（结合大型语言模型，LLMs）在知识工作和复杂任务中的应用，相比传统搜索引擎（如 Bing Search），它能生成新内容如文本、图像和代码。研究通过对 Bing Copilot 的实证分析，比较了用户在不同引擎上的任务类型和复杂度。结果显示，用户更倾向于使用生成式搜索引擎处理认知复杂度更高的知识工作任务。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY",
        "cs.SI",
        "J.4"
      ],
      "primary_category": "cs.IR",
      "comment": "32 pages, 3 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.04268v1",
      "published_date": "2024-03-19 18:17:46 UTC",
      "updated_date": "2024-03-19 18:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:55:02.129305"
    },
    {
      "arxiv_id": "2403.13078v2",
      "title": "HuLP: Human-in-the-Loop for Prognosis",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Ridzuan",
        "Mai Kassem",
        "Numan Saeed",
        "Ikboljon Sobirov",
        "Mohammad Yaqub"
      ],
      "abstract": "This paper introduces HuLP, a Human-in-the-Loop for Prognosis model designed\nto enhance the reliability and interpretability of prognostic models in\nclinical contexts, especially when faced with the complexities of missing\ncovariates and outcomes. HuLP offers an innovative approach that enables human\nexpert intervention, empowering clinicians to interact with and correct models'\npredictions, thus fostering collaboration between humans and AI models to\nproduce more accurate prognosis. Additionally, HuLP addresses the challenges of\nmissing data by utilizing neural networks and providing a tailored methodology\nthat effectively handles missing data. Traditional methods often struggle to\ncapture the nuanced variations within patient populations, leading to\ncompromised prognostic predictions. HuLP imputes missing covariates based on\nimaging features, aligning more closely with clinician workflows and enhancing\nreliability. We conduct our experiments on two real-world, publicly available\nmedical datasets to demonstrate the superiority and competitiveness of HuLP.",
      "tldr_zh": "这篇论文介绍了 HuLP，一种 Human-in-the-Loop for Prognosis 模型，旨在提升临床预后模型的可靠性和可解释性，尤其在处理缺失协变量和结果的复杂场景中。HuLP 通过允许人类专家干预和修正模型预测，促进人类与 AI 的协作，同时利用神经网络基于图像特征填充缺失数据，以更好地捕捉患者群体中的细微差异。相比传统方法，该模型显著提高了预后预测的准确性，并在两个真实世界的公开医疗数据集上进行的实验中证明了其优越性和竞争力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13078v2",
      "published_date": "2024-03-19 18:15:15 UTC",
      "updated_date": "2024-07-09 12:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:55:18.231553"
    },
    {
      "arxiv_id": "2403.15464v1",
      "title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction",
      "title_zh": "翻译失败",
      "authors": [
        "Hejie Cui",
        "Zhuocheng Shen",
        "Jieyu Zhang",
        "Hui Shao",
        "Lianhui Qin",
        "Joyce C. Ho",
        "Carl Yang"
      ],
      "abstract": "Electronic health records (EHRs) contain valuable patient data for\nhealth-related prediction tasks, such as disease prediction. Traditional\napproaches rely on supervised learning methods that require large labeled\ndatasets, which can be expensive and challenging to obtain. In this study, we\ninvestigate the feasibility of applying Large Language Models (LLMs) to convert\nstructured patient visit data (e.g., diagnoses, labs, prescriptions) into\nnatural language narratives. We evaluate the zero-shot and few-shot performance\nof LLMs using various EHR-prediction-oriented prompting strategies.\nFurthermore, we propose a novel approach that utilizes LLM agents with\ndifferent roles: a predictor agent that makes predictions and generates\nreasoning processes and a critic agent that analyzes incorrect predictions and\nprovides guidance for improving the reasoning of the predictor agent. Our\nresults demonstrate that with the proposed approach, LLMs can achieve decent\nfew-shot performance compared to traditional supervised learning methods in\nEHR-based disease predictions, suggesting its potential for health-oriented\napplications.",
      "tldr_zh": "本研究探讨了使用大型语言模型（LLMs）进行基于电子健康记录（EHR）的少样本疾病预测，以解决传统监督学习对大量标注数据的需求。论文提出一种新方法，将结构化EHR数据（如诊断、实验室结果和处方）转换为自然语言叙述，并引入LLMs代理系统，包括预测代理（负责预测和生成推理过程）和批评代理（分析错误预测并提供改进指导）。通过评估零样本和少样本性能，结果显示该方法在EHR疾病预测中实现了与传统方法相当的性能，并展示了其在健康应用的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "J.3; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15464v1",
      "published_date": "2024-03-19 18:10:13 UTC",
      "updated_date": "2024-03-19 18:10:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:55:28.254241"
    },
    {
      "arxiv_id": "2403.12961v1",
      "title": "TexTile: A Differentiable Metric for Texture Tileability",
      "title_zh": "TexTile: 一种用于纹理可平铺性的可微度量",
      "authors": [
        "Carlos Rodriguez-Pardo",
        "Dan Casas",
        "Elena Garces",
        "Jorge Lopez-Moreno"
      ],
      "abstract": "We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.",
      "tldr_zh": "论文提出了一种名为 TexTile 的新颖可微分指标，用于量化纹理图像的自拼接能力（即不引入重复伪影的 tileability）。与现有方法不同，TexTile 专注于纹理的内在可重复性，通过构建一个基于大型数据集的二元分类器，并对基线图像分类器进行架构修改、自定义数据增强和训练策略，以提升准确性和鲁棒性。实验结果表明，TexTile 可以无缝整合到各种先进的纹理合成方法（如基于扩散的策略）中，生成高质量的可拼接纹理，并提供客观评估，帮助解决现有指标评分不相关的问题，从而推动纹理合成领域的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "68T07 (Primary) 68T45, 68U05 (Secondary)",
        "I.2.6; I.4.10; I.3.3; I.5.4; I.5.1; I.3.7; I.3.8; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Project page: https://mslab.es/projects/TexTile/",
      "pdf_url": "http://arxiv.org/pdf/2403.12961v1",
      "published_date": "2024-03-19 17:59:09 UTC",
      "updated_date": "2024-03-19 17:59:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:55:40.868750"
    },
    {
      "arxiv_id": "2403.12959v1",
      "title": "WHAC: World-grounded Humans and Cameras",
      "title_zh": "WHAC: 世界坐标系下的人类和相机",
      "authors": [
        "Wanqi Yin",
        "Zhongang Cai",
        "Ruisi Wang",
        "Fanzhou Wang",
        "Chen Wei",
        "Haiyi Mei",
        "Weiye Xiao",
        "Zhitao Yang",
        "Qingping Sun",
        "Atsushi Yamashita",
        "Ziwei Liu",
        "Lei Yang"
      ],
      "abstract": "Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.",
      "tldr_zh": "本研究提出WHAC框架，用于从单目视频中估计世界坐标系下的准确尺度人类轨迹和相机位姿，目标是联合恢复表达性参数化人类模型（如SMPL-X）。框架基于两个关键观察：相机帧中的SMPL-X估计能轻松获取绝对人类深度，以及人类动作提供内在的空间线索，从而实现世界定位的表达性人类姿势和形状估计（EHPS）及相机位姿估计，而无需依赖传统优化技术。同时，研究引入了一个新合成数据集WHAC-A-Mole，包含精确标注的人类和相机数据、多样互动动作及真实相机轨迹。实验在标准和新基准上证明了WHAC的优越性，并计划公开代码和数据集。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Homepage: https://wqyin.github.io/projects/WHAC/",
      "pdf_url": "http://arxiv.org/pdf/2403.12959v1",
      "published_date": "2024-03-19 17:58:02 UTC",
      "updated_date": "2024-03-19 17:58:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:55:52.135004"
    },
    {
      "arxiv_id": "2403.13041v4",
      "title": "Provable Privacy with Non-Private Pre-Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Yaxi Hu",
        "Amartya Sanyal",
        "Bernhard Schölkopf"
      ],
      "abstract": "When analysing Differentially Private (DP) machine learning pipelines, the\npotential privacy cost of data-dependent pre-processing is frequently\noverlooked in privacy accounting. In this work, we propose a general framework\nto evaluate the additional privacy cost incurred by non-private data-dependent\npre-processing algorithms. Our framework establishes upper bounds on the\noverall privacy guarantees by utilising two new technical notions: a variant of\nDP termed Smooth DP and the bounded sensitivity of the pre-processing\nalgorithms. In addition to the generic framework, we provide explicit overall\nprivacy guarantees for multiple data-dependent pre-processing algorithms, such\nas data imputation, quantization, deduplication and PCA, when used in\ncombination with several DP algorithms. Notably, this framework is also simple\nto implement, allowing direct integration into existing DP pipelines.",
      "tldr_zh": "本研究提出一个通用框架，用于评估非私有数据依赖性预处理算法对差分隐私(DP)机器学习管道的额外隐私成本，解决了传统隐私会计中常被忽略的问题。框架通过引入Smooth DP（一种DP的变体）和预处理算法的bounded sensitivity，建立了整体隐私保证的上界。该框架为多种数据依赖性预处理算法（如data imputation、quantization、deduplication和PCA）与DP算法结合时，提供明确的隐私保证，且易于实现并整合到现有DP管道中。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13041v4",
      "published_date": "2024-03-19 17:54:49 UTC",
      "updated_date": "2024-06-21 08:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:56:04.079953"
    },
    {
      "arxiv_id": "2403.12952v2",
      "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Elaine Sui",
        "Xiaohan Wang",
        "Serena Yeung-Levy"
      ],
      "abstract": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
      "tldr_zh": "本文提出 Test-Time Prototype Shifting (TPS) 框架，用于提升 Vision-Language Models (VLMs) 在零样本学习（zero-shot learning）中的泛化能力，针对测试环境中的领域偏移问题。TPS 方法通过预计算和缓存每个类别的原型，并在测试时仅基于无标签测试样本动态学习偏移向量，从而桥接领域差距并提高分类准确性。与传统文本提示调整方法相比，该框架显著降低了内存和计算需求。实验在 15 个图像分类数据集上进行评估，涵盖自然分布偏移、跨数据集泛化以及上下文相关的视觉推理，TPS 取得了最先进的结果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.12952v2",
      "published_date": "2024-03-19 17:54:34 UTC",
      "updated_date": "2024-12-10 22:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:56:18.955992"
    },
    {
      "arxiv_id": "2403.12943v2",
      "title": "Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Vidhi Jain",
        "Maria Attarian",
        "Nikhil J Joshi",
        "Ayzaan Wahid",
        "Danny Driess",
        "Quan Vuong",
        "Pannag R Sanketi",
        "Pierre Sermanet",
        "Stefan Welker",
        "Christine Chan",
        "Igor Gilitschenski",
        "Yonatan Bisk",
        "Debidatta Dwibedi"
      ],
      "abstract": "Large-scale multi-task robotic manipulation systems often rely on text to\nspecify the task. In this work, we explore whether a robot can learn by\nobserving humans. To do so, the robot must understand a person's intent and\nperform the inferred task despite differences in the embodiments and\nenvironments. We introduce Vid2Robot, an end-to-end video-conditioned policy\nthat takes human videos demonstrating manipulation tasks as input and produces\nrobot actions. Our model is trained with a large dataset of prompt video-robot\ntrajectory pairs to learn unified representations of human and robot actions\nfrom videos. Vid2Robot uses cross-attention transformer layers between video\nfeatures and the current robot state to produce the actions and perform the\nsame task as shown in the video. We use auxiliary contrastive losses to align\nthe prompt and robot video representations for better policies. We evaluate\nVid2Robot on real-world robots and observe over 20% improvement over BC-Z when\nusing human prompt videos. Further, we also show cross-object motion transfer\nability that enables video-conditioned policies to transfer a motion observed\non one object in the prompt video to another object in the robot's own\nenvironment. Videos available at https://vid2robot.github.io",
      "tldr_zh": "该研究提出 Vid2Robot，一个端到端(end-to-end)视频条件策略，用于机器人通过观察人类演示视频来学习和执行操作任务。该系统利用 Cross-Attention Transformers 在视频特征和当前机器人状态之间建立连接，并通过辅助对比损失(auxiliary contrastive losses)来对齐人类和机器人动作的统一表示，从而实现任务意图的理解和执行。实验结果显示，Vid2Robot 在真实机器人环境中比基线模型 BC-Z 提升超过 20% 的性能，并展示了跨对象运动转移能力(cross-object motion transfer)，允许机器人将提示视频中的动作应用于不同对象。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Robotics: Science & Systems (RSS) 2024. https://vid2robot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.12943v2",
      "published_date": "2024-03-19 17:47:37 UTC",
      "updated_date": "2024-08-27 23:15:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:56:30.449009"
    },
    {
      "arxiv_id": "2403.12936v1",
      "title": "Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Joana Ribeiro de Faria",
        "Huiyuan Xie",
        "Felix Steffek"
      ],
      "abstract": "Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.",
      "tldr_zh": "本论文探讨了使用大型语言模型(LLMs)，特别是GPT-4，从英国就业法庭(UKET)判决中自动提取关键信息的可行性，通过手动验证确保提取的准确性和相关性。研究聚焦于两个任务：第一是提取八个关键方面，包括案件事实、索赔、法律法规引用、先例引用、结果、订单、补救措施和决定理由；第二是分析事实、索赔和结果，以开发预测就业法纠纷结果的工具。结果显示，GPT-4在信息提取中实现了高准确率，突显了LLMs在法律研究和实践中的革命性潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12936v1",
      "published_date": "2024-03-19 17:43:08 UTC",
      "updated_date": "2024-03-19 17:43:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:56:42.785000"
    },
    {
      "arxiv_id": "2403.13040v2",
      "title": "Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Jung Ling",
        "Salomé Bru",
        "Julia Puig",
        "Florian Vixège",
        "Simon Mendez",
        "Franck Nicoud",
        "Pierre-Yves Courand",
        "Olivier Bernard",
        "Damien Garcia"
      ],
      "abstract": "Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.",
      "tldr_zh": "这篇论文提出使用 physics-informed neural networks (PINNs) 和 physics-guided nnU-Net-based supervised approach 作为 intraventricular vector flow mapping (iVFM) 的新优化方案，以增强心脏成像中的 color Doppler 量化。实验结果显示，这两种方法在模拟和 in vivo Doppler 数据上与原 iVFM 算法重建性能相当，其中 PINNs 通过双阶段优化和预优化权重提升了效率，而 nnU-Net 则在泛化性、实时能力和处理稀疏数据上表现出色。总体而言，该研究突出了这些方法的有效性，并建议其可应用于 ultrafast color Doppler imaging 和基于流体动力学方程的 cardiovascular diseases 生物标志物开发。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments",
      "pdf_url": "http://arxiv.org/pdf/2403.13040v2",
      "published_date": "2024-03-19 17:35:17 UTC",
      "updated_date": "2024-06-27 17:27:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:56:56.372300"
    },
    {
      "arxiv_id": "2403.12918v1",
      "title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts",
      "title_zh": "在低资源文本上对预训练语言模型进行可泛化和稳定的微调",
      "authors": [
        "Sai Ashish Somayajula",
        "Youwei Liang",
        "Abhishek Singh",
        "Li Zhang",
        "Pengtao Xie"
      ],
      "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing\n(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses\nsignificant challenges such as instability and overfitting. Previous methods\ntackle these issues by finetuning a strategically chosen subnetwork on a\ndownstream task, while keeping the remaining weights fixed to the pretrained\nweights. However, they rely on a suboptimal criteria for sub-network selection,\nleading to suboptimal solutions. To address these limitations, we propose a\nregularization method based on attention-guided weight mixup for finetuning\nPLMs. Our approach represents each network weight as a mixup of task-specific\nweight and pretrained weight, controlled by a learnable attention parameter,\nproviding finer control over sub-network selection. Furthermore, we employ a\nbi-level optimization (BLO) based framework on two separate splits of the\ntraining dataset, improving generalization and combating overfitting. We\nvalidate the efficacy of our proposed method through extensive experiments,\ndemonstrating its superiority over previous methods, particularly in the\ncontext of finetuning PLMs on low-resource datasets.",
      "tldr_zh": "该研究针对预训练语言模型（PLMs）在低资源数据集上微调时存在的不稳定性和过拟合问题，提出了一种基于注意力引导权重混合（attention-guided weight mixup）的正则化方法。该方法将每个网络权重表示为任务特定权重和预训练权重的混合，通过可学习的注意力参数实现更精细的子网络选择，并结合双层优化（bi-level optimization, BLO）框架在训练数据集的不同子集上优化，以提升泛化和稳定性。实验结果显示，该方法在低资源场景下优于现有方法，证明了其在微调PLMs方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11\n  tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.12918v1",
      "published_date": "2024-03-19 17:21:29 UTC",
      "updated_date": "2024-03-19 17:21:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:57:06.094827"
    },
    {
      "arxiv_id": "2403.12910v1",
      "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
      "title_zh": "翻译失败",
      "authors": [
        "Lucy Xiaoyang Shi",
        "Zheyuan Hu",
        "Tony Z. Zhao",
        "Archit Sharma",
        "Karl Pertsch",
        "Jianlan Luo",
        "Sergey Levine",
        "Chelsea Finn"
      ],
      "abstract": "Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.",
      "tldr_zh": "本研究提出了一种框架，通过语言修正来实时改进机器人的层次化策略（hierarchical policies），以提升长时域任务的性能。论文观察到，高水平策略若与丰富的低水平语言条件技能相结合，可以从人类反馈（如“小幅移动左边”）中快速学习和适应，从而纠正执行和决策错误。该方法允许机器人不仅实时响应语言反馈，还通过迭代训练方案来持续优化策略，而无需额外遥操作。在真实硬件实验中，该框架显著提高了长时域灵巧操作任务的成功率，使用预训练的LLMs/VLMs作为高水平规划器。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://yay-robot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.12910v1",
      "published_date": "2024-03-19 17:08:24 UTC",
      "updated_date": "2024-03-19 17:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:57:18.447257"
    },
    {
      "arxiv_id": "2403.12900v1",
      "title": "Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Baolin Li",
        "Yankai Jiang",
        "Vijay Gadepally",
        "Devesh Tiwari"
      ],
      "abstract": "The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.",
      "tldr_zh": "本研究针对生成式人工智能（GenAI）的碳排放问题，提出了一种名为 Sprout 的创新框架，以减少大型语言模型（LLM）推理服务的碳足迹。Sprout 通过引入“Generation Directives”来指导自回归生成过程，并结合 directive optimizer 和离线质量评估器，实现生态可持续性与生成质量的平衡。在使用 Llama2 LLM 和全球电力网格数据的真实评估中，该框架将碳排放降低了超过 40%，为 AI 技术向可持续实践迈进提供了关键路径。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12900v1",
      "published_date": "2024-03-19 16:53:53 UTC",
      "updated_date": "2024-03-19 16:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:57:30.561076"
    },
    {
      "arxiv_id": "2403.12891v1",
      "title": "Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types",
      "title_zh": "自适应视觉模仿学习用于机器人辅助喂食，跨越各种碗配置和食物类型",
      "authors": [
        "Rui Liu",
        "Amisha Bhaskar",
        "Pratap Tokekar"
      ],
      "abstract": "In this study, we introduce a novel visual imitation network with a spatial\nattention module for robotic assisted feeding (RAF). The goal is to acquire\n(i.e., scoop) food items from a bowl. However, achieving robust and adaptive\nfood manipulation is particularly challenging. To deal with this, we propose a\nframework that integrates visual perception with imitation learning to enable\nthe robot to handle diverse scenarios during scooping. Our approach, named AVIL\n(adaptive visual imitation learning), exhibits adaptability and robustness\nacross different bowl configurations in terms of material, size, and position,\nas well as diverse food types including granular, semi-solid, and liquid, even\nin the presence of distractors. We validate the effectiveness of our approach\nby conducting experiments on a real robot. We also compare its performance with\na baseline. The results demonstrate improvement over the baseline across all\nscenarios, with an enhancement of up to 2.5 times in terms of a success metric.\nNotably, our model, trained solely on data from a transparent glass bowl\ncontaining granular cereals, showcases generalization ability when tested\nzero-shot on other bowl configurations with different types of food.",
      "tldr_zh": "本研究提出了一种名为 AVIL 的自适应视觉模仿学习框架，用于机器人辅助喂食（RAF），旨在帮助机器人从碗中抓取食物，同时处理不同碗配置（如材料、尺寸和位置）和食物类型（如颗粒、半固体和液体），即使存在干扰物。AVIL 框架整合了视觉感知、模仿学习和空间注意力模块，使机器人能够适应多样化场景。实验结果显示，该方法在真实机器人上比基线模型成功率提升高达 2.5 倍，且仅在透明玻璃碗和颗粒谷物数据上训练的模型，能实现零样本泛化到其他配置和食物类型。总的来说，AVIL 为鲁棒的机器人喂食任务提供了重要进展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12891v1",
      "published_date": "2024-03-19 16:40:57 UTC",
      "updated_date": "2024-03-19 16:40:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:57:43.320789"
    },
    {
      "arxiv_id": "2403.12869v2",
      "title": "Regularization in Spider-Style Strategy Discovery and Schedule Construction",
      "title_zh": "翻译失败",
      "authors": [
        "Filip Bártek",
        "Karel Chvalovský",
        "Martin Suda"
      ],
      "abstract": "To achieve the best performance, automatic theorem provers often rely on\nschedules of diverse proving strategies to be tried out (either sequentially or\nin parallel) on a given problem. In this paper, we report on a large-scale\nexperiment with discovering strategies for the Vampire prover, targeting the\nFOF fragment of the TPTP library and constructing a schedule for it, based on\nthe ideas of Andrei Voronkov's system Spider. We examine the process from\nvarious angles, discuss the difficulty (or ease) of obtaining a strong Vampire\nschedule for the CASC competition, and establish how well a schedule can be\nexpected to generalize to unseen problems and what factors influence this\nproperty.",
      "tldr_zh": "本论文探讨了基于 Spider 风格正则化的策略发现和调度构建方法，针对 Vampire 证明器在 TPTP 库的 FOF 片段上进行大规模实验。研究从多个角度分析了构建强力调度（如用于 CASC 比赛）的难易度，并评估了调度的泛化性能，包括其对未见问题的适用性和影响因素。结果表明，这种方法有助于提升自动定理证明器的整体性能，并为优化证明策略提供了新见解。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 8 figures; updated cosmetically for publication in IJCAR\n  2024 proceedings",
      "pdf_url": "http://arxiv.org/pdf/2403.12869v2",
      "published_date": "2024-03-19 16:12:25 UTC",
      "updated_date": "2024-07-09 16:00:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:57:56.495133"
    },
    {
      "arxiv_id": "2404.04267v17",
      "title": "What AIs are not Learning (and Why)",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Stefik"
      ],
      "abstract": "Today's robots do not learn the general skills needed for such services as\nproviding home care, being nursing assistants, or doing household chores.\nAddressing such aspirational goals requires improving how AIs and robots are\ncreated. Today's mainstream AIs are not created by agents learning from\nexperiences doing real world tasks and interacting with people. They do not\nlearn by sensing, acting, doing experiments, and collaborating. This paper\ninvestigates what aspirational service robots will need to know. It recommends\ndeveloping experiential (robotic) foundation models (FMs) for bootstrapping\nthem.",
      "tldr_zh": "本文讨论了当今AI和机器人无法学习提供家庭护理、当护理助理或处理家务等通用技能的原因，主要在于它们不是通过真实世界的经验、感知、行动、实验和协作来创建和学习。论文调查了这些服务机器人所需的关键知识，并指出当前AI的局限性。作者推荐开发基于经验的机器人基础模型（experiential robotic foundation models, FMs），以引导这些AI从实际任务中起步，实现更有效的自主学习。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.04267v17",
      "published_date": "2024-03-19 16:06:27 UTC",
      "updated_date": "2024-12-30 22:52:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:58:06.760836"
    },
    {
      "arxiv_id": "2403.12853v3",
      "title": "FlexiFly: Interfacing the Physical World with Foundation Models Empowered by Reconfigurable Drone Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Minghui Zhao",
        "Junxi Xia",
        "Kaiyuan Hou",
        "Yanchen Liu",
        "Stephen Xia",
        "Xiaofan Jiang"
      ],
      "abstract": "Foundation models (FM) have shown immense human-like capabilities for\ngenerating digital media. However, foundation models that can freely sense,\ninteract, and actuate the physical domain is far from being realized. This is\ndue to 1) requiring dense deployments of sensors to fully cover and analyze\nlarge spaces, while 2) events often being localized to small areas, making it\ndifficult for FMs to pinpoint relevant areas of interest relevant to the\ncurrent task. We propose FlexiFly, a platform that enables FMs to ``zoom in''\nand analyze relevant areas with higher granularity to better understand the\nphysical environment and carry out tasks. FlexiFly accomplishes by introducing\n1) a novel image segmentation technique that aids in identifying relevant\nlocations and 2) a modular and reconfigurable sensing and actuation drone\nplatform that FMs can actuate to ``zoom in'' with relevant sensors and\nactuators. We demonstrate through real smart home deployments that FlexiFly\nenables FMs and LLMs to complete diverse tasks up to $85\\%$ more successfully.\nFlexiFly is critical step towards FMs and LLMs that can naturally interface\nwith the physical world.",
      "tldr_zh": "论文提出 FlexiFly 平台，利用可重配置无人机系统，帮助 Foundation Models (FM) 与物理世界实现无缝交互，解决 FM 在感知和定位局部事件时的局限性。FlexiFly 引入新型图像分割技术来识别相关区域，并通过模块化无人机平台让 FM 控制传感器和执行器进行精细分析和任务执行。在真实智能家居部署中，该平台使 FM 和 LLMs 的任务成功率提高高达 85%，标志着 FM 向自然界面物理世界的关键进展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "This paper is accepted by ACM SenSys 2025. The published version is\n  https://doi.org/10.1145/3715014.3722081 in ACM Digital Library",
      "pdf_url": "http://arxiv.org/pdf/2403.12853v3",
      "published_date": "2024-03-19 15:57:32 UTC",
      "updated_date": "2025-03-05 22:11:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:58:22.037977"
    },
    {
      "arxiv_id": "2403.12823v1",
      "title": "Answer Set Programming for Flexible Payroll Management",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Callewaert",
        "Joost Vennekens"
      ],
      "abstract": "Payroll management is a critical business task that is subject to a large\nnumber of rules, which vary widely between companies, sectors, and countries.\nMoreover, the rules are often complex and change regularly. Therefore, payroll\nmanagement systems must be flexible in design. In this paper, we suggest an\napproach based on a flexible Answer Set Programming (ASP) model and an\neasy-to-read tabular representation based on the Decision Model and Notation\n(DMN) standard. It allows HR consultants to represent complex rules without the\nneed for a software engineer, and to ultimately design payroll systems for a\nvariety of different scenarios. We show how the multi-shot solving capabilities\nof the clingo ASP system can be used to reach the performance that is necessary\nto handle real-world instances.",
      "tldr_zh": "本论文探讨了薪资管理（Payroll Management）的复杂性和灵活性需求，针对规则多样且经常变化的问题，提出了一种基于 Answer Set Programming (ASP) 的模型。论文结合 Decision Model and Notation (DMN) 标准，使用易读的表格表示，让 HR 顾问无需软件工程师介入即可设计和管理复杂规则，从而适应不同公司、行业和国家的场景。通过 clingo ASP 系统的多射求解能力，该方法实现了处理真实世界实例所需的性能表现。总的来说，这一方法提升了薪资系统的灵活性和可访问性。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
      "pdf_url": "http://arxiv.org/pdf/2403.12823v1",
      "published_date": "2024-03-19 15:24:49 UTC",
      "updated_date": "2024-03-19 15:24:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:58:32.710125"
    },
    {
      "arxiv_id": "2403.12821v2",
      "title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Dongyeong Hwang",
        "Hyunju Kim",
        "Sunwoo Kim",
        "Kijung Shin"
      ],
      "abstract": "The success of a specific neural network architecture is closely tied to the\ndataset and task it tackles; there is no one-size-fits-all solution. Thus,\nconsiderable efforts have been made to quickly and accurately estimate the\nperformances of neural architectures, without full training or evaluation, for\ngiven tasks and datasets. Neural architecture encoding has played a crucial\nrole in the estimation, and graphbased methods, which treat an architecture as\na graph, have shown prominent performance. For enhanced representation learning\nof neural architectures, we introduce FlowerFormer, a powerful graph\ntransformer that incorporates the information flows within a neural\narchitecture. FlowerFormer consists of two key components: (a) bidirectional\nasynchronous message passing, inspired by the flows; (b) global attention built\non flow-based masking. Our extensive experiments demonstrate the superiority of\nFlowerFormer over existing neural encoding methods, and its effectiveness\nextends beyond computer vision models to include graph neural networks and auto\nspeech recognition models. Our code is available at\nhttp://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.",
      "tldr_zh": "该研究针对神经架构性能估计的挑战，提出了一种名为 FlowerFormer 的图变换器，通过整合神经架构内的信息流来提升表示学习。FlowerFormer 包括两个关键组件：(a) 受信息流启发的双向异步消息传递；(b) 基于流的信息掩码的全局注意力。该方法在广泛实验中表现出色，超越现有神经架构编码方法，不仅适用于计算机视觉模型，还扩展到图神经网络(Graph Neural Networks)和自动语音识别(Auto Speech Recognition)模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "CVPR 2024 Camera-Ready",
      "pdf_url": "http://arxiv.org/pdf/2403.12821v2",
      "published_date": "2024-03-19 15:21:10 UTC",
      "updated_date": "2024-03-21 10:02:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:58:46.572848"
    },
    {
      "arxiv_id": "2403.12816v1",
      "title": "Re-identification from histopathology images",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Ganz",
        "Jonas Ammeling",
        "Samir Jabari",
        "Katharina Breininger",
        "Marc Aubreville"
      ],
      "abstract": "In numerous studies, deep learning algorithms have proven their potential for\nthe analysis of histopathology images, for example, for revealing the subtypes\nof tumors or the primary origin of metastases. These models require large\ndatasets for training, which must be anonymized to prevent possible patient\nidentity leaks. This study demonstrates that even relatively simple deep\nlearning algorithms can re-identify patients in large histopathology datasets\nwith substantial accuracy. We evaluated our algorithms on two TCIA datasets\nincluding lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).\nWe also demonstrate the algorithm's performance on an in-house dataset of\nmeningioma tissue. We predicted the source patient of a slide with F1 scores of\n50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31\n% on our meningioma dataset. Based on our findings, we formulated a risk\nassessment scheme to estimate the risk to the patient's privacy prior to\npublication.",
      "tldr_zh": "这篇论文探讨了深度学习算法在组织病理学图像中重新识别患者身份的可能性，强调了这种技术可能导致隐私泄露的风险。研究者使用简单深度学习算法对 TCIA 的肺鳞癌 (LSCC) 和肺腺癌 (LUAD) 数据集，以及一个内部脑膜瘤数据集进行评估，分别取得了 50.16%、52.30% 和 62.31% 的 F1 分数。基于这些发现，论文提出了一种风险评估方案，帮助在数据发布前估算患者隐私风险。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.12816v1",
      "published_date": "2024-03-19 15:15:19 UTC",
      "updated_date": "2024-03-19 15:15:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:58:57.517231"
    },
    {
      "arxiv_id": "2403.12809v1",
      "title": "Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixue Zhao",
        "Nikolaos Aletras"
      ],
      "abstract": "In many real natural language processing application scenarios, practitioners\nnot only aim to maximize predictive performance but also seek faithful\nexplanations for the model predictions. Rationales and importance distribution\ngiven by feature attribution methods (FAs) provide insights into how different\nparts of the input contribute to a prediction. Previous studies have explored\nhow different factors affect faithfulness, mainly in the context of monolingual\nEnglish models. On the other hand, the differences in FA faithfulness between\nmultilingual and monolingual models have yet to be explored. Our extensive\nexperiments, covering five languages and five popular FAs, show that FA\nfaithfulness varies between multilingual and monolingual models. We find that\nthe larger the multilingual model, the less faithful the FAs are compared to\nits counterpart monolingual models.Our further analysis shows that the\nfaithfulness disparity is potentially driven by the differences between model\ntokenizers. Our code is available:\nhttps://github.com/casszhao/multilingual-faith.",
      "tldr_zh": "本文比较了多语和单语微调语言模型在解释忠实度（Explanation Faithfulness）上的差异，重点评估特征归因方法（FAs）如何提供输入部分对预测的贡献。研究通过覆盖五种语言和五种流行 FAs 的广泛实验，发现多语模型的 FAs 忠实度较低，且模型规模越大，与对应单语模型的差距越明显。进一步分析表明，这种忠实度差异可能由模型的分词器（Tokenizers）差异驱动，并提供了开源代码以供复现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NAACL 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2403.12809v1",
      "published_date": "2024-03-19 15:07:22 UTC",
      "updated_date": "2024-03-19 15:07:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:59:10.767885"
    },
    {
      "arxiv_id": "2403.12805v1",
      "title": "Contextual Moral Value Alignment Through Context-Based Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Dognin",
        "Jesus Rios",
        "Ronny Luss",
        "Inkit Padhi",
        "Matthew D Riemer",
        "Miao Liu",
        "Prasanna Sattigeri",
        "Manish Nagireddy",
        "Kush R. Varshney",
        "Djallel Bouneffouf"
      ],
      "abstract": "Developing value-aligned AI agents is a complex undertaking and an ongoing\nchallenge in the field of AI. Specifically within the domain of Large Language\nModels (LLMs), the capability to consolidate multiple independently trained\ndialogue agents, each aligned with a distinct moral value, into a unified\nsystem that can adapt to and be aligned with multiple moral values is of\nparamount importance. In this paper, we propose a system that does contextual\nmoral value alignment based on contextual aggregation. Here, aggregation is\ndefined as the process of integrating a subset of LLM responses that are best\nsuited to respond to a user input, taking into account features extracted from\nthe user's input. The proposed system shows better results in term of alignment\nto human value compared to the state of the art.",
      "tldr_zh": "该论文探讨了开发与道德价值观对齐的AI代理面临的挑战，特别是针对Large Language Models (LLMs)，提出了一种基于上下文聚合(Context-Based Aggregation)的系统。该系统通过整合多个独立训练的对话代理，每个代理对齐不同的道德价值观，实现上下文道德价值对齐(Contextual Moral Value Alignment)。具体方法涉及从用户输入中提取特征，选择并聚合最适合响应的LLM子集子集。实验结果显示，该系统在与人类价值观对齐方面优于现有最先进方法。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12805v1",
      "published_date": "2024-03-19 15:06:53 UTC",
      "updated_date": "2024-03-19 15:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:59:20.724427"
    },
    {
      "arxiv_id": "2403.12799v1",
      "title": "Investigating Text Shortening Strategy in BERT: Truncation vs Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Mirza Alim Mutasodirin",
        "Radityo Eko Prasojo"
      ],
      "abstract": "The parallelism of Transformer-based models comes at the cost of their input\nmax-length. Some studies proposed methods to overcome this limitation, but none\nof them reported the effectiveness of summarization as an alternative. In this\nstudy, we investigate the performance of document truncation and summarization\nin text classification tasks. Each of the two was investigated with several\nvariations. This study also investigated how close their performances are to\nthe performance of full-text. We used a dataset of summarization tasks based on\nIndonesian news articles (IndoSum) to do classification tests. This study shows\nhow the summaries outperform the majority of truncation method variations and\nlose to only one. The best strategy obtained in this study is taking the head\nof the document. The second is extractive summarization. This study explains\nwhat happened to the result, leading to further research in order to exploit\nthe potential of document summarization as a shortening alternative. The code\nand data used in this work are publicly available in\nhttps://github.com/mirzaalimm/TruncationVsSummarization.",
      "tldr_zh": "本研究调查了在BERT模型中缩短文本的策略，比较了截断(truncation)和总结(summarization)在文本分类任务中的性能。研究者使用IndoSum数据集（基于印度尼西亚新闻文章）测试了多种截断和总结变体，结果显示总结方法优于大多数截断变体，仅次于取文档头部作为最佳策略，其次是提取式总结(extractive summarization)。这些发现解释了缩短文本的潜在优势，并建议进一步探索文档总结作为替代方案，以提升模型效率；相关代码和数据已在GitHub上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The 13th International Conference on Advanced Computer Science and\n  Information Systems (ICACSIS 2021)",
      "pdf_url": "http://arxiv.org/pdf/2403.12799v1",
      "published_date": "2024-03-19 15:01:14 UTC",
      "updated_date": "2024-03-19 15:01:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:59:33.188558"
    },
    {
      "arxiv_id": "2403.12777v2",
      "title": "Discover and Mitigate Multiple Biased Subgroups in Image Classifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Zeliang Zhang",
        "Mingqian Feng",
        "Zhiheng Li",
        "Chenliang Xu"
      ],
      "abstract": "Machine learning models can perform well on in-distribution data but often\nfail on biased subgroups that are underrepresented in the training data,\nhindering the robustness of models for reliable applications. Such subgroups\nare typically unknown due to the absence of subgroup labels. Discovering biased\nsubgroups is the key to understanding models' failure modes and further\nimproving models' robustness. Most previous works of subgroup discovery make an\nimplicit assumption that models only underperform on a single biased subgroup,\nwhich does not hold on in-the-wild data where multiple biased subgroups exist.\n  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),\na novel method to address a more challenging but also more practical problem of\ndiscovering multiple biased subgroups in image classifiers. Our approach\ndecomposes the image features into multiple components that represent multiple\nsubgroups. This decomposition is achieved via a bilinear dimension reduction\nmethod, Partial Least Square (PLS), guided by useful supervision from the image\nclassifier. We further interpret the semantic meaning of each subgroup\ncomponent by generating natural language descriptions using vision-language\nfoundation models. Finally, DIM mitigates multiple biased subgroups\nsimultaneously via two strategies, including the data- and model-centric\nstrategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate\nthe effectiveness of DIM in discovering and mitigating multiple biased\nsubgroups. Furthermore, DIM uncovers the failure modes of the classifier on\nHard ImageNet, showcasing its broader applicability to understanding model bias\nin image classifiers. The code is available at\nhttps://github.com/ZhangAIPI/DIM.",
      "tldr_zh": "该研究解决了图像分类器在未代表性子群上的鲁棒性问题，提出 DIM（Decomposition, Interpretation, and Mitigation）方法，用于发现和缓解多个偏置子群。DIM 通过 Partial Least Square (PLS) 进行图像特征的双线性降维，将特征分解成代表不同子群的组件，并利用视觉语言基础模型生成自然语言描述来解释这些子群的语义含义。最终，该方法采用数据中心和模型中心策略同时缓解多个偏置子群，在 CIFAR-100 和 Breeds 数据集上的实验证明了其有效性，并在 Hard ImageNet 上揭示了分类器的失败模式，从而提升了模型的整体可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Code is available at https://github.com/ZhangAIPI/DIM",
      "pdf_url": "http://arxiv.org/pdf/2403.12777v2",
      "published_date": "2024-03-19 14:44:54 UTC",
      "updated_date": "2024-03-20 19:18:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:59:45.707746"
    },
    {
      "arxiv_id": "2403.12748v1",
      "title": "Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection",
      "title_zh": "通过用户辅助滤波器估计和选择构建脑肿瘤分割网络",
      "authors": [
        "Matheus A. Cerqueira",
        "Flávia Sprenger",
        "Bernardo C. A. Teixeira",
        "Alexandre X. Falcão"
      ],
      "abstract": "Brain tumor image segmentation is a challenging research topic in which\ndeep-learning models have presented the best results. However, the traditional\nway of training those models from many pre-annotated images leaves several\nunanswered questions. Hence methodologies, such as Feature Learning from Image\nMarkers (FLIM), have involved an expert in the learning loop to reduce human\neffort in data annotation and build models sufficiently deep for a given\nproblem. FLIM has been successfully used to create encoders, estimating the\nfilters of all convolutional layers from patches centered at marker voxels. In\nthis work, we present Multi-Step (MS) FLIM - a user-assisted approach to\nestimating and selecting the most relevant filters from multiple FLIM\nexecutions. MS-FLIM is used only for the first convolutional layer, and the\nresults already indicate improvement over FLIM. For evaluation, we build a\nsimple U-shaped encoder-decoder network, named sU-Net, for glioblastoma\nsegmentation using T1Gd and FLAIR MRI scans, varying the encoder's training\nmethod, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared\nthese sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two\ndatasets. The results show that the sU-Net based on MS-FLIM outperforms the\nother training methods and achieves effectiveness within the standard\ndeviations of the SOTA models.",
      "tldr_zh": "本文提出 Multi-Step (MS) FLIM 方法，通过用户辅助从多次 FLIM 执行中估计和选择最相关的过滤器，仅应用于第一层卷积层，以减少脑肿瘤图像分割的标注努力并提升模型性能。研究构建了简单 U-shaped 网络 sU-Net，用于胶质母细胞瘤分割，利用 T1Gd 和 FLAIR MRI 扫描，并比较了 FLIM、MS-FLIM 和 backpropagation 算法的训练效果。结果显示，基于 MS-FLIM 的 sU-Net 优于其他方法，并在两个数据集上达到 State-Of-The-Art (SOTA) 模型的标准偏差内表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07, 68T45"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, 2 tables, 24 references, manuscript of\n  conference paper",
      "pdf_url": "http://arxiv.org/pdf/2403.12748v1",
      "published_date": "2024-03-19 14:11:26 UTC",
      "updated_date": "2024-03-19 14:11:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:59:59.539798"
    },
    {
      "arxiv_id": "2403.12730v1",
      "title": "What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks",
      "title_zh": "翻译失败",
      "authors": [
        "Kacper Sokol",
        "Julia E. Vogt"
      ],
      "abstract": "Despite significant progress, evaluation of explainable artificial\nintelligence remains elusive and challenging. In this paper we propose a\nfine-grained validation framework that is not overly reliant on any one facet\nof these sociotechnical systems, and that recognises their inherent modular\nstructure: technical building blocks, user-facing explanatory artefacts and\nsocial communication protocols. While we concur that user studies are\ninvaluable in assessing the quality and effectiveness of explanation\npresentation and delivery strategies from the explainees' perspective in a\nparticular deployment context, the underlying explanation generation mechanisms\nrequire a separate, predominantly algorithmic validation strategy that accounts\nfor the technical and human-centred desiderata of their (numerical) outputs.\nSuch a comprehensive sociotechnical utility-based evaluation framework could\nallow to systematically reason about the properties and downstream influence of\ndifferent building blocks from which explainable artificial intelligence\nsystems are composed -- accounting for a diverse range of their engineering and\nsocial aspects -- in view of the anticipated use case.",
      "tldr_zh": "该论文探讨了可解释人工智能(XAI)的评估挑战，提出了一种细粒度的验证框架，以避免过度依赖单一方面。框架认识到XAI的模块化结构，包括技术building blocks、用户面对的解释性工件以及社会通信协议，并区分了用户研究（针对解释呈现和交付的有效性）和算法验证策略（针对底层生成机制的技术及人文需求）。通过这种综合的社技术效用评估方法，可以系统地分析XAI不同组成部分的属性和影响，针对预期用例考虑工程与社会因素。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Published in Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems (CHI EA '24)",
      "pdf_url": "http://arxiv.org/pdf/2403.12730v1",
      "published_date": "2024-03-19 13:45:34 UTC",
      "updated_date": "2024-03-19 13:45:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:00:09.552820"
    },
    {
      "arxiv_id": "2403.12723v2",
      "title": "Python Fuzzing for Trustworthy Machine Learning Frameworks",
      "title_zh": "翻译失败",
      "authors": [
        "Ilya Yegorov",
        "Eli Kobrin",
        "Darya Parygina",
        "Alexey Vishnyakov",
        "Andrey Fedotov"
      ],
      "abstract": "Ensuring the security and reliability of machine learning frameworks is\ncrucial for building trustworthy AI-based systems. Fuzzing, a popular technique\nin secure software development lifecycle (SSDLC), can be used to develop secure\nand robust software. Popular machine learning frameworks such as PyTorch and\nTensorFlow are complex and written in multiple programming languages including\nC/C++ and Python. We propose a dynamic analysis pipeline for Python projects\nusing the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus\nminimization, crash triaging, and coverage collection. Crash triaging and\nseverity estimation are important steps to ensure that the most critical\nvulnerabilities are addressed promptly. Furthermore, the proposed pipeline is\nintegrated in GitLab CI. To identify the most vulnerable parts of the machine\nlearning frameworks, we analyze their potential attack surfaces and develop\nfuzz targets for PyTorch, TensorFlow, and related projects such as h5py.\nApplying our dynamic analysis pipeline to these targets, we were able to\ndiscover 3 new bugs and propose fixes for them.",
      "tldr_zh": "本研究针对机器学习框架的安全性和可靠性问题，提出了一种基于 Python 的动态分析管道，使用 Sydr-Fuzz 工具集进行 Fuzzing、语料最小化、崩溃 triaging 和覆盖收集，以提升框架如 PyTorch 和 TensorFlow 的可信度。该管道还集成了 GitLab CI 系统，并分析了这些框架的潜在攻击面，开发了相应的 fuzz targets。通过应用该管道，研究人员在 PyTorch、TensorFlow 和 h5py 等项目中发现了 3 个新漏洞，并提出了修复方案，最终增强了可信 AI 系统的开发。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12723v2",
      "published_date": "2024-03-19 13:41:11 UTC",
      "updated_date": "2024-12-23 12:23:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:00:24.812312"
    },
    {
      "arxiv_id": "2403.12706v1",
      "title": "AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
      "title_zh": "AnimateDiff-Lightning：",
      "authors": [
        "Shanchuan Lin",
        "Xiao Yang"
      ],
      "abstract": "We present AnimateDiff-Lightning for lightning-fast video generation. Our\nmodel uses progressive adversarial diffusion distillation to achieve new\nstate-of-the-art in few-step video generation. We discuss our modifications to\nadapt it for the video modality. Furthermore, we propose to simultaneously\ndistill the probability flow of multiple base diffusion models, resulting in a\nsingle distilled motion module with broader style compatibility. We are pleased\nto release our distilled AnimateDiff-Lightning model for the community's use.",
      "tldr_zh": "我们提出 AnimateDiff-Lightning，一种用于快速视频生成的模型，通过 progressive adversarial diffusion distillation 技术，在少步生成中达到了新状态-of-the-art。\n该模型对原有技术进行了修改，以适应视频模式，并创新性地同时蒸馏多个基础扩散模型的概率流，生成一个兼容更广泛风格的单一运动模块。\n这项工作提升了视频生成效率，并已开源模型供社区使用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12706v1",
      "published_date": "2024-03-19 13:08:54 UTC",
      "updated_date": "2024-03-19 13:08:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:00:34.316925"
    },
    {
      "arxiv_id": "2403.13731v2",
      "title": "Emotion Recognition Using Transformers with Masked Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Seongjae Min",
        "Junseok Yang",
        "Sangjun Lim",
        "Junyong Lee",
        "Sangwon Lee",
        "Sejoon Lim"
      ],
      "abstract": "In recent years, deep learning has achieved innovative advancements in\nvarious fields, including the analysis of human emotions and behaviors.\nInitiatives such as the Affective Behavior Analysis in-the-wild (ABAW)\ncompetition have been particularly instrumental in driving research in this\narea by providing diverse and challenging datasets that enable precise\nevaluation of complex emotional states. This study leverages the Vision\nTransformer (ViT) and Transformer models to focus on the estimation of\nValence-Arousal (VA), which signifies the positivity and intensity of emotions,\nrecognition of various facial expressions, and detection of Action Units (AU)\nrepresenting fundamental muscle movements. This approach transcends traditional\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) based\nmethods, proposing a new Transformer-based framework that maximizes the\nunderstanding of temporal and spatial features. The core contributions of this\nresearch include the introduction of a learning technique through random frame\nmasking and the application of Focal loss adapted for imbalanced data,\nenhancing the accuracy and applicability of emotion and behavior analysis in\nreal-world settings. This approach is expected to contribute to the advancement\nof emotional computing and deep learning methodologies.",
      "tldr_zh": "本研究利用 Vision Transformer (ViT) 和 Transformer 模型，专注于 Valence-Arousal (VA) 估计、面部表情识别以及 Action Units (AU) 检测，超越了传统的 Convolutional Neural Networks (CNNs) 和 Long Short-Term Memory (LSTM) 方法，通过强调时间和空间特征来提升情感分析性能。核心贡献包括引入随机帧掩码学习技术，以增强模型对动态数据的理解，并应用适应不平衡数据的 Focal loss 来提高准确性。该框架预计将推动情感计算和深度学习在真实世界场景中的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13731v2",
      "published_date": "2024-03-19 12:26:53 UTC",
      "updated_date": "2024-03-23 06:31:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:00:49.104917"
    },
    {
      "arxiv_id": "2403.12678v2",
      "title": "Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights",
      "title_zh": "翻译失败",
      "authors": [
        "Maksym Taranukhin",
        "Sahithya Ravi",
        "Gabor Lukacs",
        "Evangelos Milios",
        "Vered Shwartz"
      ],
      "abstract": "The Canadian air travel sector has seen a significant increase in flight\ndelays, cancellations, and other issues concerning passenger rights.\nRecognizing this demand, we present a chatbot to assist passengers and educate\nthem about their rights. Our system breaks a complex user input into simple\nqueries which are used to retrieve information from a collection of documents\ndetailing air travel regulations. The most relevant passages from these\ndocuments are presented along with links to the original documents and the\ngenerated queries, enabling users to dissect and leverage the information for\ntheir unique circumstances. The system successfully overcomes two predominant\nchallenges: understanding complex user inputs, and delivering accurate answers,\nfree of hallucinations, that passengers can rely on for making informed\ndecisions. A user study comparing the chatbot to a Google search demonstrated\nthe chatbot's usefulness and ease of use. Beyond the primary goal of providing\naccurate and timely information to air passengers regarding their rights, we\nhope that this system will also enable further research exploring the tradeoff\nbetween the user-friendly conversational interface of chatbots and the accuracy\nof retrieval systems.",
      "tldr_zh": "本研究开发了一个聊天机器人（chatbot），旨在帮助加拿大航空乘客了解并行使他们的权利，尤其针对航班延误、取消等常见问题。系统通过将复杂用户输入分解成简单查询，从航空法规文件集合中检索相关信息，并提供准确的段落、链接和查询结果，从而避免了幻觉（hallucinations）和不准确答案。用户研究显示，该聊天机器人比Google搜索更易用和有用，有效提升了乘客的决策能力。该系统不仅为乘客提供及时信息，还推动了对聊天机器人用户友好性与检索系统准确性权衡的进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to The Natural Legal Language Processing Workshop at EMNLP\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12678v2",
      "published_date": "2024-03-19 12:24:20 UTC",
      "updated_date": "2024-10-15 19:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:01:00.307712"
    },
    {
      "arxiv_id": "2403.12672v1",
      "title": "Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine",
      "title_zh": "翻译失败",
      "authors": [
        "Kaiji Sekimoto",
        "Muneki Yasuda"
      ],
      "abstract": "Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for\nsemi-supervised anomaly detection, where they are trained using only normal\ndata points. In GBRBM-based anomaly detection, normal and anomalous data are\nclassified based on a score that is identical to an energy function of the\nmarginal GBRBM. However, the classification threshold is difficult to set to an\nappropriate value, as this score cannot be interpreted. In this study, we\npropose a measure that improves score's interpretability based on its\ncumulative distribution, and establish a guideline for setting the threshold\nusing the interpretable measure. The results of numerical experiments show that\nthe guideline is reasonable when setting the threshold solely using normal data\npoints. Moreover, because identifying the measure involves computationally\ninfeasible evaluation of the minimum score value, we also propose an evaluation\nmethod for the minimum score based on simulated annealing, which is widely used\nfor optimization problems. The proposed evaluation method was also validated\nusing numerical experiments.",
      "tldr_zh": "本文针对 Gaussian-Bernoulli Restricted Boltzmann Machine (GBRBM) 在半监督异常检测中的分数不可解释问题，提出了一种基于分数累积分布的措施，以提升分数的可解释性，并建立了使用该措施设置分类阈值的指南。研究还引入了基于 simulated annealing 的方法来评估最小分数值，从而解决计算上的不可行性。数值实验结果显示，该指南在仅使用正常数据点时是合理的，并验证了提出的评估方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12672v1",
      "published_date": "2024-03-19 12:13:52 UTC",
      "updated_date": "2024-03-19 12:13:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:01:11.748933"
    },
    {
      "arxiv_id": "2403.12671v1",
      "title": "Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Jakub Res",
        "Ivan Homoliak",
        "Martin Perešíni",
        "Aleš Smrčka",
        "Kamil Malinka",
        "Petr Hanacek"
      ],
      "abstract": "AI assistants for coding are on the rise. However one of the reasons\ndevelopers and companies avoid harnessing their full potential is the\nquestionable security of the generated code. This paper first reviews the\ncurrent state-of-the-art and identifies areas for improvement on this issue.\nThen, we propose a systematic approach based on prompt-altering methods to\nachieve better code security of (even proprietary black-box) AI-based code\ngenerators such as GitHub Copilot, while minimizing the complexity of the\napplication from the user point-of-view, the computational resources, and\noperational costs. In sum, we propose and evaluate three prompt altering\nmethods: (1) scenario-specific, (2) iterative, and (3) general clause, while we\ndiscuss their combination. Contrary to the audit of code security, the latter\ntwo of the proposed methods require no expert knowledge from the user. We\nassess the effectiveness of the proposed methods on the GitHub Copilot using\nthe OpenVPN project in realistic scenarios, and we demonstrate that the\nproposed methods reduce the number of insecure generated code samples by up to\n16\\% and increase the number of secure code by up to 8\\%. Since our approach\ndoes not require access to the internals of the AI models, it can be in general\napplied to any AI-based code synthesizer, not only GitHub Copilot.",
      "tldr_zh": "本论文审阅了AI辅助代码生成工具（如GitHub Copilot）的安全问题，并提出了一种基于prompt-engineering的系统方法来提升代码安全性。该方法包括三种prompt-altering策略：(1) scenario-specific（场景特定）、(2) iterative（迭代）和(3) general clause（一般条款），这些策略无需用户具备专家知识，且能降低计算和操作成本。在OpenVPN项目上的实验表明，该方法可将不安全代码样本减少最多16%并增加安全代码最多8%，且由于不需访问AI模型内部，该方法可适用于任何AI-based code synthesis工具。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12671v1",
      "published_date": "2024-03-19 12:13:33 UTC",
      "updated_date": "2024-03-19 12:13:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:01:24.160096"
    },
    {
      "arxiv_id": "2403.12664v1",
      "title": "Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making",
      "title_zh": "解读 AutoML 集成模型：cattleia 在决策中的",
      "authors": [
        "Anna Kozak",
        "Dominik Kędzierski",
        "Jakub Piwko",
        "Malwina Wojewoda",
        "Katarzyna Woźnica"
      ],
      "abstract": "In many applications, model ensembling proves to be better than a single\npredictive model. Hence, it is the most common post-processing technique in\nAutomated Machine Learning (AutoML). The most popular frameworks use ensembles\nat the expense of reducing the interpretability of the final models. In our\nwork, we propose cattleia - an application that deciphers the ensembles for\nregression, multiclass, and binary classification tasks. This tool works with\nmodels built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The\ngiven ensemble is analyzed from different perspectives. We conduct a predictive\nperformance investigation through evaluation metrics of the ensemble and its\ncomponent models. We extend the validation perspective by introducing new\nmeasures to assess the diversity and complementarity of the model predictions.\nMoreover, we apply explainable artificial intelligence (XAI) techniques to\nexamine the importance of variables. Summarizing obtained insights, we can\ninvestigate and adjust the weights with a modification tool to tune the\nensemble in the desired way. The application provides the aforementioned\naspects through dedicated interactive visualizations, making it accessible to a\ndiverse audience. We believe the cattleia can support users in decision-making\nand deepen the comprehension of AutoML frameworks.",
      "tldr_zh": "本研究针对Automated Machine Learning (AutoML) 中模型集成的可解释性问题，提出cattleia工具，以辅助决策过程。该工具支持auto-sklearn、AutoGluon和FLAML框架，对回归、多类和二元分类任务的集成模型进行多角度分析，包括预测性能评估、模型多样性和互补性度量，以及explainable artificial intelligence (XAI) 技术用于变量重要性检查。用户可以通过交互式可视化和权重调整功能，优化集成模型并获得洞见，从而提升对AutoML框架的理解和决策效率。实验结果表明，cattleia能有效揭示集成模型的特性，帮助用户在实际应用中实现更可靠的模型调整。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12664v1",
      "published_date": "2024-03-19 11:56:21 UTC",
      "updated_date": "2024-03-19 11:56:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:01:35.700653"
    },
    {
      "arxiv_id": "2403.12660v3",
      "title": "ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems",
      "title_zh": "ERASE：深度推荐系统的特征选择方法基准测试",
      "authors": [
        "Pengyue Jia",
        "Yejing Wang",
        "Zhaocheng Du",
        "Xiangyu Zhao",
        "Yichao Wang",
        "Bo Chen",
        "Wanyu Wang",
        "Huifeng Guo",
        "Ruiming Tang"
      ],
      "abstract": "Deep Recommender Systems (DRS) are increasingly dependent on a large number\nof feature fields for more precise recommendations. Effective feature selection\nmethods are consequently becoming critical for further enhancing the accuracy\nand optimizing storage efficiencies to align with the deployment demands. This\nresearch area, particularly in the context of DRS, is nascent and faces three\ncore challenges. Firstly, variant experimental setups across research papers\noften yield unfair comparisons, obscuring practical insights. Secondly, the\nexisting literature's lack of detailed analysis on selection attributes, based\non large-scale datasets and a thorough comparison among selection techniques\nand DRS backbones, restricts the generalizability of findings and impedes\ndeployment on DRS. Lastly, research often focuses on comparing the peak\nperformance achievable by feature selection methods, an approach that is\ntypically computationally infeasible for identifying the optimal\nhyperparameters and overlooks evaluating the robustness and stability of these\nmethods. To bridge these gaps, this paper presents ERASE, a comprehensive\nbEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation\nof eleven feature selection methods, covering both traditional and deep\nlearning approaches, across four public datasets, private industrial datasets,\nand a real-world commercial platform, achieving significant enhancement. Our\ncode is available online for ease of reproduction.",
      "tldr_zh": "本研究针对深度推荐系统（Deep Recommender Systems, DRS）中特征选择的依赖性，识别了三个核心挑战：实验设置不一致导致不公平比较、缺乏大规模数据集的详细分析，以及过度关注峰值性能而忽略计算可行性和稳定性。为解决这些问题，论文提出ERASE基准，这是一个全面评估框架，测试了11种特征选择方法（包括传统和深度学习方法）在四个公共数据集、私有工业数据集以及真实商业平台上的表现。实验结果显示，ERASE实现了显著的准确性和存储效率提升，并提供了公开代码以便于复现和部署。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to KDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12660v3",
      "published_date": "2024-03-19 11:49:35 UTC",
      "updated_date": "2024-06-19 12:48:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:01:48.520394"
    },
    {
      "arxiv_id": "2404.00019v1",
      "title": "Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap",
      "title_zh": "翻译失败",
      "authors": [
        "Sule Tekkesinoglu",
        "Azra Habibovic",
        "Lars Kunze"
      ],
      "abstract": "Given the uncertainty surrounding how existing explainability methods for\nautonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough\ninvestigation is imperative to determine the contexts requiring explanations\nand suitable interaction strategies. A comprehensive review becomes crucial to\nassess the alignment of current approaches with the varied interests and\nexpectations within the AV ecosystem. This study presents a review to discuss\nthe complexities associated with explanation generation and presentation to\nfacilitate the development of more effective and inclusive explainable AV\nsystems. Our investigation led to categorising existing literature into three\nprimary topics: explanatory tasks, explanatory information, and explanatory\ninformation communication. Drawing upon our insights, we have proposed a\ncomprehensive roadmap for future research centred on (i) knowing the\ninterlocutor, (ii) generating timely explanations, (ii) communicating\nhuman-friendly explanations, and (iv) continuous learning. Our roadmap is\nunderpinned by principles of responsible research and innovation, emphasising\nthe significance of diverse explanation requirements. To effectively tackle the\nchallenges associated with implementing explainable AV systems, we have\ndelineated various research directions, including the development of\nprivacy-preserving data integration, ethical frameworks, real-time analytics,\nhuman-centric interaction design, and enhanced cross-disciplinary\ncollaborations. By exploring these research directions, the study aims to guide\nthe development and deployment of explainable AVs, informed by a holistic\nunderstanding of user needs, technological advancements, regulatory compliance,\nand ethical considerations, thereby ensuring safer and more trustworthy\nautonomous driving experiences.",
      "tldr_zh": "这篇论文对自动驾驶车辆 (AVs) 的可解释性方法进行全面综述，探讨了现有方法如何满足不同利益相关者的多样化需求，并强调了生成和呈现解释的复杂性。文献被分类为三个主要主题：解释任务、解释信息和解释信息沟通。作者提出一个未来研究路线图，聚焦于了解对话者、生成及时解释、沟通人性化解释以及持续学习，并基于负责任的研究和创新原则，建议研究方向如隐私保护数据整合、道德框架和跨学科合作。最终，该研究旨在指导可解释 AV 系统的开发，确保其符合用户需求、技术进步、法规合规和伦理考虑，从而提升自动驾驶的安全性和可信度。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00019v1",
      "published_date": "2024-03-19 11:43:41 UTC",
      "updated_date": "2024-03-19 11:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:02:01.485764"
    },
    {
      "arxiv_id": "2403.12649v1",
      "title": "InBox: Recommendation with Knowledge Graph using Interest Box Embedding",
      "title_zh": "翻译失败",
      "authors": [
        "Zezhong Xu",
        "Yincen Qu",
        "Wen Zhang",
        "Lei Liang",
        "Huajun Chen"
      ],
      "abstract": "Knowledge graphs (KGs) have become vitally important in modern recommender\nsystems, effectively improving performance and interpretability. Fundamentally,\nrecommender systems aim to identify user interests based on historical\ninteractions and recommend suitable items. However, existing works overlook two\nkey challenges: (1) an interest corresponds to a potentially large set of\nrelated items, and (2) the lack of explicit, fine-grained exploitation of KG\ninformation and interest connectivity. This leads to an inability to reflect\ndistinctions between entities and interests when modeling them in a single way.\nAdditionally, the granularity of concepts in the knowledge graphs used for\nrecommendations tends to be coarse, failing to match the fine-grained nature of\nuser interests. This homogenization limits the precise exploitation of\nknowledge graph data and interest connectivity. To address these limitations,\nwe introduce a novel embedding-based model called InBox. Specifically, various\nknowledge graph entities and relations are embedded as points or boxes, while\nuser interests are modeled as boxes encompassing interaction history.\nRepresenting interests as boxes enables containing collections of item points\nrelated to that interest. We further propose that an interest comprises diverse\nbasic concepts, and box intersection naturally supports concept combination.\nAcross three training steps, InBox significantly outperforms state-of-the-art\nmethods like HAKG and KGIN on recommendation tasks. Further analysis provides\nmeaningful insights into the variable value of different KG data for\nrecommendations. In summary, InBox advances recommender systems through\nbox-based interest and concept modeling for sophisticated knowledge graph\nexploitation.",
      "tldr_zh": "该论文提出 InBox 模型，用于解决推荐系统中知识图谱 (KGs) 利用的不足，特别是忽略兴趣对应的大量相关物品以及缺乏细粒度 KG 信息和兴趣连接的问题。InBox 通过将 KG 中的实体和关系嵌入为点或盒子 (boxes)，并将用户兴趣建模为涵盖历史交互的盒子，从而实现对兴趣集合和概念组合的精确表示。模型进一步利用盒子交集支持多样化概念组合，并在三个训练步骤中显著超越了如 HAKG 和 KGIN 等最先进方法，在推荐任务上表现出色。总体而言，InBox 提升了推荐系统的性能，并提供了对不同 KG 数据价值的深入洞见。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "VLDB 2024 under submission",
      "pdf_url": "http://arxiv.org/pdf/2403.12649v1",
      "published_date": "2024-03-19 11:34:15 UTC",
      "updated_date": "2024-03-19 11:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:02:12.770810"
    },
    {
      "arxiv_id": "2403.12631v1",
      "title": "PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic Glove Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Hu",
        "Shirui Lyu",
        "Eojin Rho",
        "Daekyum Kim",
        "Shan Luo",
        "Letizia Gionfrida"
      ],
      "abstract": "Controlling hand exoskeletons to assist individuals with grasping tasks poses\na challenge due to the difficulty in understanding user intentions. We propose\nthat most daily grasping tasks during activities of daily living (ADL) can be\ndeduced by analyzing object geometries (simple and complex) from 3D point\nclouds. The study introduces PointGrasp, a real-time system designed for\nidentifying household scenes semantically, aiming to support and enhance\nassistance during ADL for tailored end-to-end grasping tasks. The system\ncomprises an RGB-D camera with an inertial measurement unit and a\nmicroprocessor integrated into a tendon-driven soft robotic glove. The RGB-D\ncamera processes 3D scenes at a rate exceeding 30 frames per second. The\nproposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple\nand 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies\nand pinpoints reachable objects. This system shows promise in end-to-end\nvision-driven robotic-assisted rehabilitation manual tasks.",
      "tldr_zh": "该研究针对控制手外骨骼以辅助日常生活活动 (ADL) 抓取任务的挑战，提出 PointGrasp 系统，该系统基于点云 (Point Cloud) 数据分析物体几何形状来推断用户意图。PointGrasp 包括集成 RGB-D 相机、惯性测量单元 (IMU) 和微处理器的肌腱驱动软机器人手套，实现超过 30 帧/秒的实时 3D 场景语义识别和端到端抓取任务支持。实验结果显示，简单几何的平均 RMSE 为 0.8 ± 0.39 cm，复杂几何为 0.11 ± 0.06 cm，并能准确识别和定位可达物体，为视觉驱动的机器人辅助康复手动任务提供可靠解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.4"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 8 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2403.12631v1",
      "published_date": "2024-03-19 10:59:21 UTC",
      "updated_date": "2024-03-19 10:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:02:30.567124"
    },
    {
      "arxiv_id": "2403.12627v2",
      "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code",
      "title_zh": "增强形式定理证明：一个用于在 Coq 代码上训练 AI 模型的全面数据集",
      "authors": [
        "Andreas Florath"
      ],
      "abstract": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1",
      "tldr_zh": "这篇论文介绍了为提升 Large Language Models (LLMs) 在 Coq 证明助手代码处理方面的能力而构建的一个全面数据集，以解决 Coq 的语法和语义挑战。该数据集从超过 10,000 个 Coq 源文件衍生而来，涵盖命题、证明、定义以及元数据（如来源和许可信息），旨在帮助模型生成语法正确且语义有意义的 Coq 构造。初步实验显示，训练后的模型在 Coq 代码生成上准确性显著提升，例如一个微调的 LLM 成功生成了 141 个有效证明。总体而言，这为自动定理证明和形式验证领域的机器学习应用提供了重要资源。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.12627v2",
      "published_date": "2024-03-19 10:53:40 UTC",
      "updated_date": "2024-04-02 13:54:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:02:41.199141"
    },
    {
      "arxiv_id": "2403.12589v2",
      "title": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Clément Gaspard",
        "Grégoire Passault",
        "Mélodie Daniel",
        "Olivier Ly"
      ],
      "abstract": "Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.",
      "tldr_zh": "本文提出FootstepNet，一种基于actor-critic深度强化学习(DRL)的方法，用于快速在线双足机器人足部规划和预测。该方法针对有障碍物的局部环境，通过连续动作集生成可行脚步序列，避免了传统搜索算法（如A*变体）的计算密集和手动调参需求，同时提供了一种预测功能来快速估计到达不同目标所需的脚步数。实验结果显示，该方法在模拟环境中表现出色，并在RoboCup 2023比赛中成功部署于儿童大小人形机器人上，证明了其高效性和实用性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12589v2",
      "published_date": "2024-03-19 09:48:18 UTC",
      "updated_date": "2024-12-17 15:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:02:52.790805"
    },
    {
      "arxiv_id": "2403.12588v2",
      "title": "Machine Learning of the Prime Distribution",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Kolpakov",
        "Aidan Rocke"
      ],
      "abstract": "In the present work we use maximum entropy methods to derive several theorems\nin probabilistic number theory, including a version of the Hardy-Ramanujan\nTheorem. We also provide a theoretical argument explaining the experimental\nobservations of Yang-Hui He about the learnability of primes, and posit that\nthe Erd\\H{o}s-Kac law would very unlikely be discovered by current machine\nlearning techniques. Numerical experiments that we perform corroborate our\ntheoretical findings.",
      "tldr_zh": "本研究利用最大熵方法（maximum entropy methods）推导出几个概率数论定理，包括一个Hardy-Ramanujan Theorem的版本，并提供理论论证解释Yang-Hui He关于质数分布可学习性的实验观察。论文认为，Erdős-Kac law不太可能被当前机器学习技术发现，因为其复杂性。数值实验证实了这些理论发现，为机器学习在数论领域的应用提供了新见解。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT",
        "math.NT",
        "11N05"
      ],
      "primary_category": "cs.IT",
      "comment": "10 pages; parts of arXiv:2308.10817 reworked and amended; author's\n  draft; accepted in PLOS ONE",
      "pdf_url": "http://arxiv.org/pdf/2403.12588v2",
      "published_date": "2024-03-19 09:47:54 UTC",
      "updated_date": "2024-06-02 17:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:03:01.280709"
    },
    {
      "arxiv_id": "2403.12574v2",
      "title": "EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ziming Wang",
        "Ziling Wang",
        "Huaning Li",
        "Lang Qin",
        "Runhao Jiang",
        "De Ma",
        "Huajin Tang"
      ],
      "abstract": "Event cameras, with their high dynamic range and temporal resolution, are\nideally suited for object detection, especially under scenarios with motion\nblur and challenging lighting conditions. However, while most existing\napproaches prioritize optimizing spatiotemporal representations with advanced\ndetection backbones and early aggregation functions, the crucial issue of\nadaptive event sampling remains largely unaddressed. Spiking Neural Networks\n(SNNs), which operate on an event-driven paradigm through sparse spike\ncommunication, emerge as a natural fit for addressing this challenge. In this\nstudy, we discover that the neural dynamics of spiking neurons align closely\nwith the behavior of an ideal temporal event sampler. Motivated by this\ninsight, we propose a novel adaptive sampling module that leverages recurrent\nconvolutional SNNs enhanced with temporal memory, facilitating a fully\nend-to-end learnable framework for event-based detection. Additionally, we\nintroduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to\nregulate potential distribution and address performance degradation encountered\nin spike-based sampling modules. Empirical evaluation on neuromorphic detection\ndatasets demonstrates that our approach outperforms existing state-of-the-art\nspike-based methods with significantly fewer parameters and time steps. For\ninstance, our method yields a 4.4\\% mAP improvement on the Gen1 dataset, while\nrequiring 38\\% fewer parameters and only three time steps. Moreover, the\napplicability and effectiveness of our adaptive sampling methodology extend\nbeyond SNNs, as demonstrated through further validation on conventional\nnon-spiking models. Code is available at https://github.com/Windere/EAS-SNN.",
      "tldr_zh": "这篇论文提出了一种端到端自适应采样和表示框架EAS-SNN，用于事件相机下的物体检测，采用Recurrent Spiking Neural Networks (SNNs)来处理自适应事件采样问题。框架包括一个基于循环卷积SNNs的采样模块，结合Temporal Memory机制实现端到端学习，并引入Residual Potential Dropout (RPD)和Spike-Aware Training (SAT)来调节神经元潜在分布并提升性能。在神经形态检测数据集上，EAS-SNN显著优于现有方法，例如在Gen1数据集上mAP提高了4.4%，同时参数减少38%且仅需三个时间步。该方法的可扩展性还延伸到传统非SNN模型中，提供更高效的检测解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12574v2",
      "published_date": "2024-03-19 09:34:11 UTC",
      "updated_date": "2024-08-24 16:04:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:03:18.835775"
    },
    {
      "arxiv_id": "2403.12572v1",
      "title": "Compound Expression Recognition via Multi Model Ensemble",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Yu",
        "Jichao Zhu",
        "Wangyuan Zhu"
      ],
      "abstract": "Compound Expression Recognition (CER) plays a crucial role in interpersonal\ninteractions. Due to the existence of Compound Expressions , human emotional\nexpressions are complex, requiring consideration of both local and global\nfacial expressions to make judgments. In this paper, to address this issue, we\npropose a solution based on ensemble learning methods for Compound Expression\nRecognition. Specifically, our task is classification, where we train three\nexpression classification models based on convolutional networks, Vision\nTransformers, and multi-scale local attention networks. Then, through model\nensemble using late fusion, we merge the outputs of multiple models to predict\nthe final result. Our method achieves high accuracy on RAF-DB and is able to\nrecognize expressions through zero-shot on certain portions of C-EXPR-DB.",
      "tldr_zh": "本研究针对Compound Expression Recognition (CER) 的复杂性，提出了一种基于集成学习的方法，以处理人类情感表达中局部和全局面部表情的判断问题。具体而言，该方法训练了三种模型，包括基于卷积网络的、Vision Transformers 的，以及多尺度局部注意力网络的表达式分类模型，然后通过late fusion 合并多个模型的输出进行最终预测。在RAF-DB 数据集上，该方法实现了高准确率，并在C-EXPR-DB 的某些部分实现了zero-shot 识别，展示了其在情感识别任务中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12572v1",
      "published_date": "2024-03-19 09:30:56 UTC",
      "updated_date": "2024-03-19 09:30:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:03:26.725692"
    },
    {
      "arxiv_id": "2403.12568v1",
      "title": "Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices",
      "title_zh": "内存高效且安全的深度神经网络推理在启用 TrustZone 的消费级物联网设备上",
      "authors": [
        "Xueshuo Xie",
        "Haoxu Wang",
        "Zhaolong Jian",
        "Tao Li",
        "Wei Wang",
        "Zhiwei Xu",
        "Guiling Wang"
      ],
      "abstract": "Edge intelligence enables resource-demanding Deep Neural Network (DNN)\ninference without transferring original data, addressing concerns about data\nprivacy in consumer Internet of Things (IoT) devices. For privacy-sensitive\napplications, deploying models in hardware-isolated trusted execution\nenvironments (TEEs) becomes essential. However, the limited secure memory in\nTEEs poses challenges for deploying DNN inference, and alternative techniques\nlike model partitioning and offloading introduce performance degradation and\nsecurity issues. In this paper, we present a novel approach for advanced model\ndeployment in TrustZone that ensures comprehensive privacy preservation during\nmodel inference. We design a memory-efficient management method to support\nmemory-demanding inference in TEEs. By adjusting the memory priority, we\neffectively mitigate memory leakage risks and memory overlap conflicts,\nresulting in 32 lines of code alterations in the trusted operating system.\nAdditionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny\ndeep learning library, and Tinylibm (827 LoCs), a tiny math library, to support\nefficient inference in TEEs. We implemented a prototype on Raspberry Pi 3B+ and\nevaluated it using three well-known lightweight DNN models. The experimental\nresults demonstrate that our design significantly improves inference speed by\n3.13 times and reduces power consumption by over 66.5% compared to non-memory\noptimization method in TEEs.",
      "tldr_zh": "该研究提出了一种内存高效且安全的DNN推理方法，针对TrustZone-enabled的消费级IoT设备，确保在TEEs（Trusted Execution Environments）中进行隐私保护的模型部署。通过调整内存优先级和最小化代码修改（仅32行），该方法有效缓解内存泄漏和冲突，并利用S-Tinylib（2,538 LoCs）和Tinylibm（827 LoCs）两个小型库支持高效推理。在Raspberry Pi 3B+上的原型实验中，该设计将推理速度提高了3.13倍，并将功耗降低了超过66.5%。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12568v1",
      "published_date": "2024-03-19 09:22:50 UTC",
      "updated_date": "2024-03-19 09:22:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:03:39.146202"
    },
    {
      "arxiv_id": "2403.12563v1",
      "title": "Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service",
      "title_zh": "Transformer 模型的简单优化技巧，用于在时间和内存受限的 GPU 服务上应对繁重长文本分类",
      "authors": [
        "Mirza Alim Mutasodirin",
        "Radityo Eko Prasojo",
        "Achmad F. Abka",
        "Hanif Rasyidi"
      ],
      "abstract": "Many NLP researchers rely on free computational services, such as Google\nColab, to fine-tune their Transformer models, causing a limitation for\nhyperparameter optimization (HPO) in long-text classification due to the method\nhaving quadratic complexity and needing a bigger resource. In Indonesian, only\na few works were found on long-text classification using Transformers. Most\nonly use a small amount of data and do not report any HPO. In this study, using\n18k news articles, we investigate which pretrained models are recommended to\nuse based on the output length of the tokenizer. We then compare some hacks to\nshorten and enrich the sequences, which are the removals of stopwords,\npunctuation, low-frequency words, and recurring words. To get a fair\ncomparison, we propose and run an efficient and dynamic HPO procedure that can\nbe done gradually on a limited resource and does not require a long-running\noptimization library. Using the best hack found, we then compare 512, 256, and\n128 tokens length. We find that removing stopwords while keeping punctuation\nand low-frequency words is the best hack. Some of our setups manage to\noutperform taking 512 first tokens using a smaller 128 or 256 first tokens\nwhich manage to represent the same information while requiring less\ncomputational resources. The findings could help developers to efficiently\npursue optimal performance of the models using limited resources.",
      "tldr_zh": "该论文探讨了在资源有限的GPU服务（如Google Colab）上使用Transformer模型进行长文本分类的挑战，特别是超参数优化（HPO）的计算复杂性问题。研究者使用18k篇印尼语新闻文章，调查了预训练模型的选择，并比较了多种序列处理技巧，如去除停用词、标点、低频词和重复词，同时提出了一种高效的动态HPO过程来逐步优化模型。结果显示，去除停用词而保留标点和低频词是最优策略，使用较短的token长度（如128或256）能实现与512token相当的性能，甚至在某些设置下超越它，从而显著减少计算资源需求，为开发者在有限环境下优化Transformer模型提供了实用指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The 10th International Conference on Advanced Informatics: Concepts,\n  Theory, and Applications (ICAICTA 2023)",
      "pdf_url": "http://arxiv.org/pdf/2403.12563v1",
      "published_date": "2024-03-19 09:17:25 UTC",
      "updated_date": "2024-03-19 09:17:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:03:54.344906"
    },
    {
      "arxiv_id": "2403.12562v2",
      "title": "PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Raghavendra Selvan",
        "Bob Pepin",
        "Christian Igel",
        "Gabrielle Samuel",
        "Erik B Dam"
      ],
      "abstract": "The recent advances in deep learning (DL) have been accelerated by access to\nlarge-scale data and compute. These large-scale resources have been used to\ntrain progressively larger models which are resource intensive in terms of\ncompute, data, energy, and carbon emissions. These costs are becoming a new\ntype of entry barrier to researchers and practitioners with limited access to\nresources at such scale, particularly in the Global South. In this work, we\ntake a comprehensive look at the landscape of existing DL models for medical\nimage analysis tasks and demonstrate their usefulness in settings where\nresources are limited. To account for the resource consumption of DL models, we\nintroduce a novel measure to estimate the performance per resource unit, which\nwe call the PePR score. Using a diverse family of 131 unique DL architectures\n(spanning 1M to 130M trainable parameters) and three medical image datasets, we\ncapture trends about the performance-resource trade-offs. In applications like\nmedical image analysis, we argue that small-scale, specialized models are\nbetter than striving for large-scale models. Furthermore, we show that using\nexisting pretrained models that are fine-tuned on new data can significantly\nreduce the computational resources and data required compared to training\nmodels from scratch. We hope this work will encourage the community to focus on\nimproving AI equity by developing methods and models with smaller resource\nfootprints.",
      "tldr_zh": "该论文分析了深度学习（DL）模型在医疗图像分析中的资源消耗问题，指出大规模模型的计算、数据和能源需求加剧了资源有限地区（如全球南方）的研究壁垒。作者引入了PePR分数（Performance Per Resource Unit）作为一种新指标，用于评估模型的性能与资源消耗的权衡，并通过131个DL架构（参数从1M到130M）和三个医疗图像数据集的实验，证明小规模专业模型在资源受限场景下更具实用性。研究进一步显示，使用预训练模型进行微调可显著减少计算资源和数据需求，从而鼓励社区开发资源占用小的模型，以促进AI公平性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to be published at the Northern Lights Deep Learning\n  Conference (NLDL), 2025. Source code available at\n  https://github.com/saintslab/PePR",
      "pdf_url": "http://arxiv.org/pdf/2403.12562v2",
      "published_date": "2024-03-19 09:17:18 UTC",
      "updated_date": "2024-12-05 11:57:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:04:04.911855"
    },
    {
      "arxiv_id": "2403.12552v1",
      "title": "M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Dongyang Xu",
        "Haokun Li",
        "Qingfan Wang",
        "Ziying Song",
        "Lei Chen",
        "Hanming Deng"
      ],
      "abstract": "End-to-end autonomous driving has witnessed remarkable progress. However, the\nextensive deployment of autonomous vehicles has yet to be realized, primarily\ndue to 1) inefficient multi-modal environment perception: how to integrate data\nfrom multi-modal sensors more efficiently; 2) non-human-like scene\nunderstanding: how to effectively locate and predict critical risky agents in\ntraffic scenarios like an experienced driver. To overcome these challenges, in\nthis paper, we propose a Multi-Modal fusion transformer incorporating Driver\nAttention (M2DA) for autonomous driving. To better fuse multi-modal data and\nachieve higher alignment between different modalities, a novel\nLidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By\nincorporating driver attention, we empower the human-like scene understanding\nability to autonomous vehicles to identify crucial areas within complex\nscenarios precisely and ensure safety. We conduct experiments on the CARLA\nsimulator and achieve state-of-the-art performance with less data in\nclosed-loop benchmarks. Source codes are available at\nhttps://anonymous.4open.science/r/M2DA-4772.",
      "tldr_zh": "该研究针对自动驾驶领域的多模态环境感知效率低和非人类-like场景理解问题，提出了Multi-Modal Fusion Transformer incorporating Driver Attention (M2DA)框架。M2DA引入了Lidar-Vision-Attention-based Fusion (LVAFusion)模块，以更高效地融合多模态传感器数据，并通过整合Driver Attention机制，提升车辆对复杂交通场景中风险代理的精确定位和预测能力。实验在CARLA模拟器上表明，该框架在使用更少数据的情况下，实现了state-of-the-art性能，在闭环基准测试中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12552v1",
      "published_date": "2024-03-19 08:54:52 UTC",
      "updated_date": "2024-03-19 08:54:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:04:16.876338"
    },
    {
      "arxiv_id": "2403.15457v3",
      "title": "The Journey to Trustworthy AI: Pursuit of Pragmatic Frameworks",
      "title_zh": "通往可信赖 AI 的旅程：实用框架的追求",
      "authors": [
        "Mohamad M Nasr-Azadani",
        "Jean-Luc Chatelain"
      ],
      "abstract": "This paper reviews Trustworthy Artificial Intelligence (TAI) and its various\ndefinitions. Considering the principles respected in any society, TAI is often\ncharacterized by a few attributes, some of which have led to confusion in\nregulatory or engineering contexts. We argue against using terms such as\nResponsible or Ethical AI as substitutes for TAI. And to help clarify any\nconfusion, we suggest leaving them behind. Given the subjectivity and\ncomplexity inherent in TAI, developing a universal framework is deemed\ninfeasible. Instead, we advocate for approaches centered on addressing key\nattributes and properties such as fairness, bias, risk, security,\nexplainability, and reliability. We examine the ongoing regulatory landscape,\nwith a focus on initiatives in the EU, China, and the USA. We recognize that\ndifferences in AI regulations based on geopolitical and geographical reasons\npose an additional challenge for multinational companies. We identify risk as a\ncore factor in AI regulation and TAI. For example, as outlined in the EU-AI\nAct, organizations must gauge the risk level of their AI products to act\naccordingly (or risk hefty fines). We compare modalities of TAI implementation\nand how multiple cross-functional teams are engaged in the overall process.\nThus, a brute force approach for enacting TAI renders its efficiency and\nagility, moot. To address this, we introduce our framework\nSet-Formalize-Measure-Act (SFMA). Our solution highlights the importance of\ntransforming TAI-aware metrics, drivers of TAI, stakeholders, and\nbusiness/legal requirements into actual benchmarks or tests. Finally,\nover-regulation driven by panic of powerful AI models can, in fact, harm TAI\ntoo. Based on GitHub user-activity data, in 2023, AI open-source projects rose\nto top projects by contributor account. Enabling innovation in TAI hinges on\nthe independent contributions of the open-source community.",
      "tldr_zh": "这篇论文审视了可信赖人工智能（Trustworthy AI, TAI）的定义和属性，强调其主观性与复杂性，并建议放弃“Responsible AI”或“Ethical AI”作为其替代，以避免混淆。作者分析了全球监管环境，如欧盟的EU-AI Act、中国和美国的举措，指出风险是TAI监管的核心挑战，并讨论了地缘政治差异对跨国公司的影响。最终，论文提出SFMA（Set-Formalize-Measure-Act）框架，作为一种实用方法来评估和实施TAI的关键属性，如公平性、偏见和可靠性，同时警告过度监管可能抑制开源创新和AI发展。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "Updates: Added disclaimer about USA's recent U-turn on Trustworthy AI\n  Executive Order. Improved Fairness and Group size in section 6.5. Fixed\n  typos. Added a few new references. Updated title",
      "pdf_url": "http://arxiv.org/pdf/2403.15457v3",
      "published_date": "2024-03-19 08:27:04 UTC",
      "updated_date": "2025-02-12 07:50:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:04:33.214960"
    },
    {
      "arxiv_id": "2403.12533v3",
      "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Tanneberg",
        "Felix Ocker",
        "Stephan Hasler",
        "Joerg Deigmoeller",
        "Anna Belardinelli",
        "Chao Wang",
        "Heiko Wersing",
        "Bernhard Sendhoff",
        "Michael Gienger"
      ],
      "abstract": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.",
      "tldr_zh": "本研究提出Attentive Support，一种基于Large Language Models (LLMs)的交互概念，旨在让机器人为人类群体提供不引人注目的物理支持。该系统整合了场景感知、对话获取、情境理解和行为生成，利用LLMs的常识推理能力，允许机器人自主决定何时提供帮助、何时保持沉默以避免干扰。通过多种场景实验，论文展示了机器人的关注行为，能够在需要时有效支持人类，而在无需帮助时不打扰群体。结果证明了这一方法的实用性和可靠性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2.8; I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12533v3",
      "published_date": "2024-03-19 08:09:44 UTC",
      "updated_date": "2025-04-24 14:28:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:04:42.656562"
    },
    {
      "arxiv_id": "2403.12523v1",
      "title": "GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Haochen Li",
        "Di Geng"
      ],
      "abstract": "Events describe the state changes of entities. In a document, multiple events\nare connected by various relations (e.g., Coreference, Temporal, Causal, and\nSubevent). Therefore, obtaining the connections between events through\nEvent-Event Relation Extraction (ERE) is critical to understand natural\nlanguage. There are two main problems in the current ERE works: a. Only\nembeddings of the event triggers are used for event feature representation,\nignoring event arguments (e.g., time, place, person, etc.) and their structure\nwithin the event. b. The interconnection between relations (e.g., temporal and\ncausal relations usually interact with each other ) is ignored. To solve the\nabove problems, this paper proposes a jointly multiple ERE framework called\nGraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event\nembeddings with event argument and structure features by using static AMR\ngraphs and IE graphs; Then, to jointly extract multiple event relations, we use\nNode Transformer and construct Task-specific Dynamic Event Graphs for each type\nof relation. Finally, we used a multi-task learning strategy to train the whole\nframework. Experimental results on the latest MAVEN-ERE dataset validate that\nGraphERE significantly outperforms existing methods. Further analyses indicate\nthe effectiveness of the graph-enhanced event embeddings and the joint\nextraction strategy.",
      "tldr_zh": "本文提出 GraphERE 框架，通过 Graph-enhanced Event Embeddings 联合提取多个事件-事件关系 (ERE)，以解决现有方法忽略事件参数（如时间、地点）和关系互连（如 Temporal 和 Causal 关系交互）的问题。框架首先使用静态 AMR graphs 和 IE graphs 丰富事件嵌入，然后结合 Node Transformer 和 Task-specific Dynamic Event Graphs 进行多任务学习，以同时处理多种关系。实验结果显示，在 MAVEN-ERE 数据集上，GraphERE 显著优于现有方法，进一步证明了图增强嵌入和联合提取策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12523v1",
      "published_date": "2024-03-19 07:50:32 UTC",
      "updated_date": "2024-03-19 07:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:04:57.156971"
    },
    {
      "arxiv_id": "2404.00018v1",
      "title": "Can AI Outperform Human Experts in Creating Social Media Creatives?",
      "title_zh": "翻译失败",
      "authors": [
        "Eunkyung Park",
        "Raymond K. Wong",
        "Junbum Kwon"
      ],
      "abstract": "Artificial Intelligence has outperformed human experts in functional tasks\nsuch as chess and baduk. How about creative tasks? This paper evaluates AI's\ncapability in the creative domain compared to human experts, which little\nresearch has been conducted so far. We propose a novel Prompt-for-Prompt to\ngenerate social media creatives via prompt augmentation by Large Language\nModels. We take the most popular Instagram posts (with the biggest number of\nlike clicks) in top brands' Instagram accounts to create social media\ncreatives. We give GPT 4 several prompt instructions with text descriptions to\ngenerate the most effective prompts for cutting-edge text-to-image generators:\nMidjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost\nAI's abilities by adding objectives, engagement strategy, lighting and brand\nconsistency for social media image creation. We conduct an extensive human\nevaluation experiment, and find that AI excels human experts, and Midjourney is\nbetter than the other text-to-image generators. Surprisingly, unlike\nconventional wisdom in the social media industry, prompt instruction including\neye-catching shows much poorer performance than those including natural.\nRegarding the type of creatives, AI improves creatives with animals or products\nbut less with real people. Also, AI improves creatives with short text\ndescriptions more than with long text descriptions, because there is more room\nfor AI to augment prompts with shorter descriptions.",
      "tldr_zh": "本研究评估了 AI 是否能在创建社交媒体创意（如 Instagram 帖子）上超越人类专家，提出了一种名为 Prompt-for-Prompt 的方法，使用 Large Language Models (LLM) 如 GPT-4 来增强提示，从而优化文本到图像生成器（如 Midjourney、DALL-E 3 和 Stable Diffusion）的输出。实验通过选取热门品牌帖子并添加目标、参与策略、照明和品牌一致性等元素进行生成，发现 AI 生成的创意整体优于人类专家，且 Midjourney 表现最佳。意外发现包括：包含 “eye-catching” 的提示效果较差，而自然风格的更有效；AI 在处理动物或产品主题时提升明显，但在真实人物或长文本描述上表现较弱。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SI",
        "Computation and Language (cs.CL), Artificial Intelligence (cs.AI)"
      ],
      "primary_category": "cs.HC",
      "comment": "17 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.00018v1",
      "published_date": "2024-03-19 07:41:45 UTC",
      "updated_date": "2024-03-19 07:41:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:05:07.601883"
    },
    {
      "arxiv_id": "2403.13031v2",
      "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuowen Yuan",
        "Zidi Xiong",
        "Yi Zeng",
        "Ning Yu",
        "Ruoxi Jia",
        "Dawn Song",
        "Bo Li"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have showcased remarkable\ncapabilities across various tasks in different domains. However, the emergence\nof biases and the potential for generating harmful content in LLMs,\nparticularly under malicious inputs, pose significant challenges. Current\nmitigation strategies, while effective, are not resilient under adversarial\nattacks. This paper introduces Resilient Guardrails for Large Language Models\n(RigorLLM), a novel framework designed to efficiently and effectively moderate\nharmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted\napproach that includes energy-based training data augmentation through Langevin\ndynamics, optimizing a safe suffix for inputs via minimax optimization, and\nintegrating a fusion-based model combining robust KNN with LLMs based on our\ndata augmentation, RigorLLM offers a robust solution to harmful content\nmoderation. Our experimental evaluations demonstrate that RigorLLM not only\noutperforms existing baselines like OpenAI API and Perspective API in detecting\nharmful content but also exhibits unparalleled resilience to jailbreaking\nattacks. The innovative use of constrained optimization and a fusion-based\nguardrail approach represents a significant step forward in developing more\nsecure and reliable LLMs, setting a new standard for content moderation\nframeworks in the face of evolving digital threats.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）在面对恶意输入时产生的偏见和有害内容问题，提出了RigorLLM框架，这是一种高效的防护机制，用于 moderation有害输入和输出。RigorLLM采用多方面方法，包括通过Langevin dynamics进行能量-based训练数据增强、利用minimax optimization优化安全的输入后缀，以及融合robust KNN与LLMs的模型，以提升整体鲁棒性。实验结果显示，RigorLLM在检测有害内容上优于基线模型如OpenAI API和Perspective API，并展现出对jailbreaking attacks的出色抵抗力，为构建更安全可靠的LLMs设定新标准。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13031v2",
      "published_date": "2024-03-19 07:25:02 UTC",
      "updated_date": "2024-07-23 22:56:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:05:19.517900"
    },
    {
      "arxiv_id": "2403.12510v3",
      "title": "Generalized Consistency Trajectory Models for Image Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Beomsu Kim",
        "Jaemin Kim",
        "Jeongsol Kim",
        "Jong Chul Ye"
      ],
      "abstract": "Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing.",
      "tldr_zh": "本研究针对 Diffusion Models (DMs) 在图像生成和编辑中的计算密集问题，提出 Generalized Consistency Trajectory Models (GCTMs)，这是一种通过普通微分方程 (ODEs) 实现任意分布间转换的框架。GCTMs 扩展了原有 Consistency Trajectory Models (CTMs)，克服了其仅限于高斯噪声到数据的局限性，从而实现高效的图像操纵。实验结果证明，GCTMs 在图像到图像翻译、修复和编辑任务中表现出色，提升了生成过程的效率和灵活性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12510v3",
      "published_date": "2024-03-19 07:24:54 UTC",
      "updated_date": "2024-10-10 04:04:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:05:31.119944"
    },
    {
      "arxiv_id": "2403.12503v1",
      "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
      "title_zh": "翻译失败",
      "authors": [
        "Sara Abdali",
        "Richard Anarfi",
        "CJ Barberan",
        "Jia He"
      ],
      "abstract": "Large language models (LLMs) have significantly transformed the landscape of\nNatural Language Processing (NLP). Their impact extends across a diverse\nspectrum of tasks, revolutionizing how we approach language understanding and\ngenerations. Nevertheless, alongside their remarkable utility, LLMs introduce\ncritical security and risk considerations. These challenges warrant careful\nexamination to ensure responsible deployment and safeguard against potential\nvulnerabilities. This research paper thoroughly investigates security and\nprivacy concerns related to LLMs from five thematic perspectives: security and\nprivacy concerns, vulnerabilities against adversarial attacks, potential harms\ncaused by misuses of LLMs, mitigation strategies to address these challenges\nwhile identifying limitations of current strategies. Lastly, the paper\nrecommends promising avenues for future research to enhance the security and\nrisk management of LLMs.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在自然语言处理(NLP)领域的应用及其引发的安全和隐私挑战，强调了负责任部署的重要性。论文从五个主题角度进行全面调查，包括安全隐私担忧、对抗性攻击的漏洞、LLMs误用的潜在危害、缓解策略及其局限性。最终，它推荐了未来研究的潜在方向，以增强LLMs的安全风险管理。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12503v1",
      "published_date": "2024-03-19 07:10:58 UTC",
      "updated_date": "2024-03-19 07:10:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:05:42.749062"
    },
    {
      "arxiv_id": "2403.12488v3",
      "title": "DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM",
      "title_zh": "翻译失败",
      "authors": [
        "Yixuan Wu",
        "Yizhou Wang",
        "Shixiang Tang",
        "Wenhao Wu",
        "Tong He",
        "Wanli Ouyang",
        "Philip Torr",
        "Jian Wu"
      ],
      "abstract": "We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot\nobject detection ability of multimodal large language models (MLLMs), such as\nGPT-4V and Gemini. Our approach consists of a detection prompting toolkit\ninspired by high-precision detection priors and a new Chain-of-Thought to\nimplement these prompts. Specifically, the prompts in the toolkit are designed\nto guide the MLLM to focus on regional information (e.g., zooming in), read\ncoordinates according to measure standards (e.g., overlaying rulers and\ncompasses), and infer from the contextual information (e.g., overlaying scene\ngraphs). Building upon these tools, the new detection chain-of-thought can\nautomatically decompose the task into simple subtasks, diagnose the\npredictions, and plan for progressive box refinements. The effectiveness of our\nframework is demonstrated across a spectrum of detection tasks, especially hard\ncases. Compared to existing state-of-the-art methods, GPT-4V with our\nDetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS\nCOCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val\nset for zero-shot referring expression comprehension, +14.5% AP on D-cube\ndescribe object detection FULL setting.",
      "tldr_zh": "本研究提出了一种新颖的提示范式 DetToolChain，用于释放多模态大型语言模型 (MLLMs) 如 GPT-4V 和 Gemini 的零样本对象检测能力。框架包括一个受高精度检测先验启发的提示工具包，该工具包引导 MLLM 关注区域信息（如放大操作）、读取坐标（如叠加标尺和罗盘）以及从上下文信息（如叠加场景图）进行推断，同时结合新的 Chain-of-Thought 方法来分解任务、诊断预测并规划渐进式边界框精炼。实验结果显示，DetToolChain 显著提升了性能：在 MS COCO Novel 类集上，GPT-4V 比最先进对象检测器提高了 +21.5% AP50；在 RefCOCO val 集上提高了 +24.23% Acc；在 D-cube 描述对象检测 FULL 设置上提高了 +14.5% AP。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12488v3",
      "published_date": "2024-03-19 06:54:33 UTC",
      "updated_date": "2024-07-23 07:14:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:05:57.070515"
    },
    {
      "arxiv_id": "2403.14715v3",
      "title": "Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It",
      "title_zh": "翻译失败",
      "authors": [
        "Guoxuan Xia",
        "Olivier Laurent",
        "Gianni Franchi",
        "Christos-Savvas Bouganis"
      ],
      "abstract": "Label smoothing (LS) is a popular regularisation method for training neural\nnetworks as it is effective in improving test accuracy and is simple to\nimplement. ``Hard'' one-hot labels are ``smoothed'' by uniformly distributing\nprobability mass to other classes, reducing overfitting. Prior work has\nsuggested that in some cases LS can degrade selective classification (SC) --\nwhere the aim is to reject misclassifications using a model's uncertainty. In\nthis work, we first demonstrate empirically across an extended range of\nlarge-scale tasks and architectures that LS consistently degrades SC. We then\naddress a gap in existing knowledge, providing an explanation for this\nbehaviour by analysing logit-level gradients: LS degrades the uncertainty rank\nordering of correct vs incorrect predictions by suppressing the max logit more\nwhen a prediction is likely to be correct, and less when it is likely to be\nwrong. This elucidates previously reported experimental results where strong\nclassifiers underperform in SC. We then demonstrate the empirical effectiveness\nof post-hoc logit normalisation for recovering lost SC performance caused by\nLS. Furthermore, linking back to our gradient analysis, we again provide an\nexplanation for why such normalisation is effective.",
      "tldr_zh": "这篇论文探讨了Label smoothing (LS)如何降低selective classification (SC)的性能，通过在多种大规模任务和架构上进行实验，证明LS会一致地损害模型的不确定性评估。研究者通过分析logit-level gradients解释了这一问题：LS在正确预测时更强烈地抑制最大logit，而在错误预测时抑制较弱，从而破坏了正确与错误预测的排序。最终，他们展示了post-hoc logit normalisation的有效性，能够恢复LS导致的SC性能损失，并通过梯度分析说明了其原因。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.14715v3",
      "published_date": "2024-03-19 06:46:24 UTC",
      "updated_date": "2025-02-20 15:02:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:06:07.840639"
    },
    {
      "arxiv_id": "2403.12486v2",
      "title": "NTK-Guided Few-Shot Class Incremental Learning",
      "title_zh": "NTK 引导的少样本类增量学习",
      "authors": [
        "Jingren Liu",
        "Zhong Ji",
        "Yanwei Pang",
        "YunLong Yu"
      ],
      "abstract": "The proliferation of Few-Shot Class Incremental Learning (FSCIL)\nmethodologies has highlighted the critical challenge of maintaining robust\nanti-amnesia capabilities in FSCIL learners. In this paper, we present a novel\nconceptualization of anti-amnesia in terms of mathematical generalization,\nleveraging the Neural Tangent Kernel (NTK) perspective. Our method focuses on\ntwo key aspects: ensuring optimal NTK convergence and minimizing NTK-related\ngeneralization loss, which serve as the theoretical foundation for cross-task\ngeneralization. To achieve global NTK convergence, we introduce a principled\nmeta-learning mechanism that guides optimization within an expanded network\narchitecture. Concurrently, to reduce the NTK-related generalization loss, we\nsystematically optimize its constituent factors. Specifically, we initiate\nself-supervised pre-training on the base session to enhance NTK-related\ngeneralization potential. These self-supervised weights are then carefully\nrefined through curricular alignment, followed by the application of dual NTK\nregularization tailored specifically for both convolutional and linear layers.\nThrough the combined effects of these measures, our network acquires robust NTK\nproperties, ensuring optimal convergence and stability of the NTK matrix and\nminimizing the NTK-related generalization loss, significantly enhancing its\ntheoretical generalization. On popular FSCIL benchmark datasets, our NTK-FSCIL\nsurpasses contemporary state-of-the-art approaches, elevating end-session\naccuracy by 2.9\\% to 9.3\\%.",
      "tldr_zh": "这篇论文提出了一种基于 Neural Tangent Kernel (NTK) 的 Few-Shot Class Incremental Learning (FSCIL) 方法，通过数学泛化视角重新定义反遗忘（anti-amnesia）挑战，焦点在于优化 NTK 收敛和最小化 NTK 相关泛化损失。方法包括引入 meta-learning 机制指导网络优化、自监督预训练增强泛化潜力、课程对齐（curricular alignment）细化权重，以及应用双 NTK 正则化（dual NTK regularization）针对卷积和线性层。实验结果显示，该方法在 FSCIL 基准数据集上比现有最先进方法提升了 2.9% 到 9.3% 的最终会话准确率，从而显著提高了模型的理论泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12486v2",
      "published_date": "2024-03-19 06:43:46 UTC",
      "updated_date": "2024-09-24 12:11:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:06:22.359943"
    },
    {
      "arxiv_id": "2403.12482v2",
      "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
      "title_zh": "具身 LLM 代理学习在组织化团队中合作",
      "authors": [
        "Xudong Guo",
        "Kaixuan Huang",
        "Jiale Liu",
        "Wenhui Fan",
        "Natalia Vélez",
        "Qingyun Wu",
        "Huazheng Wang",
        "Thomas L. Griffiths",
        "Mengdi Wang"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
      "tldr_zh": "本文研究了大型语言模型(LLMs)在多智能体系统中促进合作的能力，但LLMs代理往往过度报告和遵守指令，导致信息冗余和混乱。论文提出一个基于prompt的组织结构框架，灵感来源于人类组织，通过指定领导角色和实验验证，提升了embodied LLM agents的团队效率，并揭示了其自发合作行为。通过Criticize-Reflect过程，作者优化了组织prompts，开发出新结构，显著减少通信成本并提高整体表现。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12482v2",
      "published_date": "2024-03-19 06:39:47 UTC",
      "updated_date": "2024-05-23 06:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:06:37.556808"
    },
    {
      "arxiv_id": "2403.15456v3",
      "title": "WoLF: Wide-scope Large Language Model Framework for CXR Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Seil Kang",
        "Donghyun Kim",
        "Junhyeok Kim",
        "Hyo Kyung Lee",
        "Seong Jae Hwang"
      ],
      "abstract": "Significant methodological strides have been made toward Chest X-ray (CXR)\nunderstanding via modern vision-language models (VLMs), demonstrating\nimpressive Visual Question Answering (VQA) and CXR report generation abilities.\nHowever, existing CXR understanding frameworks still possess several procedural\ncaveats. (1) Previous methods solely use CXR reports, which are insufficient\nfor comprehensive Visual Question Answering (VQA), especially when additional\nhealth-related data like medication history and prior diagnoses are needed. (2)\nPrevious methods use raw CXR reports, which are often arbitrarily structured.\nWhile modern language models can understand various text formats, restructuring\nreports for clearer, organized anatomy-based information could enhance their\nusefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize\nlinguistic correctness, lacking the capability to offer nuanced assessments of\nthe generated answers. In this work, to address the aforementioned caveats, we\nintroduce WoLF, a Wide-scope Large Language Model Framework for CXR\nunderstanding. To resolve (1), we capture multi-faceted records of patients,\nwhich are utilized for accurate diagnoses in real-world clinical scenarios.\nSpecifically, we adopt the Electronic Health Records (EHR) to generate\ninstruction-following data suited for CXR understanding. Regarding (2), we\nenhance report generation performance by decoupling knowledge in CXR reports\nbased on anatomical structure even within the attention step via masked\nattention. To address (3), we introduce an AI-evaluation protocol optimized for\nassessing the capabilities of LLM. Through extensive experimental validations,\nWoLF demonstrates superior performance over other models on MIMIC-CXR in the\nAI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics about\nreport generation (+7.3%p BLEU-1).",
      "tldr_zh": "该研究提出了WoLF，一种宽范围大语言模型框架，用于提升胸部X光（CXR）理解能力，以解决现有方法的局限性，包括仅依赖CXR报告导致的VQA不全面、报告结构任意以及评估缺乏细致性等问题。WoLF框架通过整合Electronic Health Records (EHR)生成多方面患者数据、采用基于解剖结构的知识解耦和masked attention来优化报告生成，以及引入AI-evaluation协议来提供更精确的模型评估。实验结果显示，WoLF在MIMIC-CXR数据集上显著优于基线模型，在VQA的AI-evaluation中平均分数提升高达9.47%，并在报告生成指标如BLEU-1上提高7.3%。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages main paper, 2 pages supplementary",
      "pdf_url": "http://arxiv.org/pdf/2403.15456v3",
      "published_date": "2024-03-19 06:39:23 UTC",
      "updated_date": "2024-03-29 04:38:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:06:50.204429"
    },
    {
      "arxiv_id": "2404.08652v1",
      "title": "Algorithm for AGC index management against crowded radio environment",
      "title_zh": "翻译失败",
      "authors": [
        "Morgane Joly",
        "Fabian Rivière",
        "Éric Renault"
      ],
      "abstract": "This paper describes a receiver that uses an innovative method to predict,\naccording to history of receiver operating metrics (packet lost/well received),\nthe optimum automatic gain control (AGC) index or most appropriate variable\ngain range to be used for next packet reception, anticipating an interferer\nappearing during the payload reception. This allows the receiver to have higher\nimmunity to interferers even if they occur during the gain frozen payload\nreception period whilst still ensuring an optimum sensitivity level. As a\nresult, the method allows setting the receiver gain to get an optimum trade-off\nbetween reception sensitivity and random interferer immunity.",
      "tldr_zh": "该论文提出了一种AGC指数管理算法，旨在提升接收器在拥挤无线电环境中的抗干扰能力。该算法利用接收器历史操作指标（如数据包丢失/成功接收）来预测下一个数据包接收的最佳AGC指数或可变增益范围，从而提前应对interferer的出现。即使在增益冻结的payload接收期间，该方法也能提高免疫力，同时确保接收灵敏度的优化平衡。结果表明，该算法实现了接收敏感度和随机干扰免疫力的最佳权衡。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "17 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08652v1",
      "published_date": "2024-03-19 05:42:29 UTC",
      "updated_date": "2024-03-19 05:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:07:02.495110"
    },
    {
      "arxiv_id": "2403.12462v1",
      "title": "Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks",
      "title_zh": "循环脉冲神经网络的异质学习动态拓扑表示",
      "authors": [
        "Biswadeep Chakraborty",
        "Saibal Mukhopadhyay"
      ],
      "abstract": "Spiking Neural Networks (SNNs) have become an essential paradigm in\nneuroscience and artificial intelligence, providing brain-inspired computation.\nRecent advances in literature have studied the network representations of deep\nneural networks. However, there has been little work that studies\nrepresentations learned by SNNs, especially using unsupervised local learning\nmethods like spike-timing dependent plasticity (STDP). Recent work by\n\\cite{barannikov2021representation} has introduced a novel method to compare\ntopological mappings of learned representations called Representation Topology\nDivergence (RTD). Though useful, this method is engineered particularly for\nfeedforward deep neural networks and cannot be used for recurrent networks like\nRecurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to\nmeasure the difference between distributed representations of RSNN models with\ndifferent learning methods. We propose a novel reformulation of RSNNs using\nfeedforward autoencoder networks with skip connections to help us compute the\nRTD for recurrent networks. Thus, we investigate the learning capabilities of\nRSNN trained using STDP and the role of heterogeneity in the synaptic dynamics\nin learning such representations. We demonstrate that heterogeneous STDP in\nRSNNs yield distinct representations than their homogeneous and surrogate\ngradient-based supervised learning counterparts. Our results provide insights\ninto the potential of heterogeneous SNN models, aiding the development of more\nefficient and biologically plausible hybrid artificial intelligence systems.",
      "tldr_zh": "本文提出了一种新方法，使用 Representation Topology Divergence (RTD) 来评估不同学习方法下 Recurrent Spiking Neural Networks (RSNNs) 的分布式表示差异，以填补现有研究的空白。作者通过将 RSNNs 重新表述为前馈自编码器网络并添加跳跃连接，实现了 RTD 在循环网络中的应用，并探讨了 spike-timing dependent plasticity (STDP) 在异质突触动态下的学习能力。实验结果显示，异质 STDP 训练的 RSNNs 产生独特的拓扑表示，与同质 STDP 和基于代理梯度的监督学习方法显著不同。这些发现为开发更高效、生物学上合理的混合人工智能系统提供了重要洞见。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted in IEEE World Congress on Computational Intelligence (IEEE\n  WCCI) 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12462v1",
      "published_date": "2024-03-19 05:37:26 UTC",
      "updated_date": "2024-03-19 05:37:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:07:15.896802"
    },
    {
      "arxiv_id": "2403.12459v3",
      "title": "Non-negative Contrastive Learning",
      "title_zh": "非负对比学习",
      "authors": [
        "Yifei Wang",
        "Qi Zhang",
        "Yaoyu Guo",
        "Yisen Wang"
      ],
      "abstract": "Deep representations have shown promising performance when transferred to\ndownstream tasks in a black-box manner. Yet, their inherent lack of\ninterpretability remains a significant challenge, as these features are often\nopaque to human understanding. In this paper, we propose Non-negative\nContrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization\n(NMF) aimed at deriving interpretable features. The power of NCL lies in its\nenforcement of non-negativity constraints on features, reminiscent of NMF's\ncapability to extract features that align closely with sample clusters. NCL not\nonly aligns mathematically well with an NMF objective but also preserves NMF's\ninterpretability attributes, resulting in a more sparse and disentangled\nrepresentation compared to standard contrastive learning (CL). Theoretically,\nwe establish guarantees on the identifiability and downstream generalization of\nNCL. Empirically, we show that these advantages enable NCL to outperform CL\nsignificantly on feature disentanglement, feature selection, as well as\ndownstream classification tasks. At last, we show that NCL can be easily\nextended to other learning scenarios and benefit supervised learning as well.\nCode is available at https://github.com/PKU-ML/non_neg.",
      "tldr_zh": "本论文提出 Non-negative Contrastive Learning (NCL)，一种基于 Non-negative Matrix Factorization (NMF) 的方法，旨在生成可解释的深度表示，通过强制特征非负性实现更稀疏和解耦的特征。相比标准 Contrastive Learning (CL)，NCL 不仅在理论上提供了可识别性和下游泛化保证，还在实验中显著提升了特征解耦、特征选择和分类任务的性能。最终，NCL 可以轻松扩展到其他学习场景，包括监督学习，并附有开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages. Accepted by ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12459v3",
      "published_date": "2024-03-19 05:30:50 UTC",
      "updated_date": "2024-04-22 21:28:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:07:25.632314"
    },
    {
      "arxiv_id": "2403.12451v4",
      "title": "End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Lirui Luo",
        "Guoxi Zhang",
        "Hongming Xu",
        "Yaodong Yang",
        "Cong Fang",
        "Qing Li"
      ],
      "abstract": "Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising\nparadigm for explainable decision-making, characterized by the interpretability\nof symbolic policies. NS-RL entails structured state representations for tasks\nwith visual observations, but previous methods cannot refine the structured\nstates with rewards due to a lack of efficiency. Accessibility also remains an\nissue, as extensive domain knowledge is required to interpret symbolic\npolicies. In this paper, we present a neuro-symbolic framework for jointly\nlearning structured states and symbolic policies, whose key idea is to distill\nthe vision foundation model into an efficient perception module and refine it\nduring policy learning. Moreover, we design a pipeline to prompt GPT-4 to\ngenerate textual explanations for the learned policies and decisions,\nsignificantly reducing users' cognitive load to understand the symbolic\npolicies. We verify the efficacy of our approach on nine Atari tasks and\npresent GPT-generated explanations for policies and decisions.",
      "tldr_zh": "本研究提出了一种端到端的 Neuro-symbolic reinforcement learning (NS-RL) 框架，旨在解决传统方法在处理视觉观察任务时效率不足和符号策略解释困难的问题。该框架的关键创新是将视觉基础模型提炼成高效的感知模块，并在策略学习过程中优化结构化状态，同时使用 GPT-4 生成文本解释，以显著降低用户理解符号策略的认知负担。实验在九个 Atari 任务上验证了该方法的有效性，展示了其在可解释决策方面的优越性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2024. Project page: https://ins-rl.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.12451v4",
      "published_date": "2024-03-19 05:21:20 UTC",
      "updated_date": "2024-06-13 06:04:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:07:38.056967"
    },
    {
      "arxiv_id": "2403.12448v1",
      "title": "Do Generated Data Always Help Contrastive Learning?",
      "title_zh": "生成数据是否总是有助于对比学习？",
      "authors": [
        "Yifei Wang",
        "Jizhe Zhang",
        "Yisen Wang"
      ],
      "abstract": "Contrastive Learning (CL) has emerged as one of the most successful paradigms\nfor unsupervised visual representation learning, yet it often depends on\nintensive manual data augmentations. With the rise of generative models,\nespecially diffusion models, the ability to generate realistic images close to\nthe real data distribution has been well recognized. These generated\nhigh-equality images have been successfully applied to enhance contrastive\nrepresentation learning, a technique termed ``data inflation''. However, we\nfind that the generated data (even from a good diffusion model like DDPM) may\nsometimes even harm contrastive learning. We investigate the causes behind this\nfailure from the perspective of both data inflation and data augmentation. For\nthe first time, we reveal the complementary roles that stronger data inflation\nshould be accompanied by weaker augmentations, and vice versa. We also provide\nrigorous theoretical explanations for these phenomena via deriving its\ngeneralization bounds under data inflation. Drawing from these insights, we\npropose Adaptive Inflation (AdaInf), a purely data-centric strategy without\nintroducing any extra computation cost. On benchmark datasets, AdaInf can bring\nsignificant improvements for various contrastive learning methods. Notably,\nwithout using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10\nwith SimCLR, setting a new record that surpasses many sophisticated methods.\nCode is available at https://github.com/PKU-ML/adainf.",
      "tldr_zh": "本研究探讨了生成数据是否总能提升对比学习（Contrastive Learning），发现即使使用高质量扩散模型（如 DDPM）生成的数据，有时反而会损害学习效果。作者从数据膨胀（Data Inflation）和数据增强的角度分析原因，揭示了更强的数据膨胀应配以更弱的增强，反之亦然，并通过推导泛化边界提供了理论解释。基于这些洞见，他们提出了一种纯数据导向策略 Adaptive Inflation (AdaInf)，无需额外计算开销，能显著改善各种对比学习方法的性能。在基准数据集上，AdaInf 使 SimCLR 在 CIFAR-10 上达到 94.70% 的线性准确率，超越了许多复杂方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages. Accepted by ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12448v1",
      "published_date": "2024-03-19 05:17:47 UTC",
      "updated_date": "2024-03-19 05:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:07:49.463864"
    },
    {
      "arxiv_id": "2403.12431v1",
      "title": "Geometric Constraints in Deep Learning Frameworks: A Survey",
      "title_zh": "深度学习框架中的几何约束：一个综述",
      "authors": [
        "Vibhas K Vats",
        "David J Crandall"
      ],
      "abstract": "Stereophotogrammetry is an emerging technique of scene understanding. Its\norigins go back to at least the 1800s when people first started to investigate\nusing photographs to measure the physical properties of the world. Since then,\nthousands of approaches have been explored. The classic geometric techniques of\nShape from Stereo is built on using geometry to define constraints on scene and\ncamera geometry and then solving the non-linear systems of equations. More\nrecent work has taken an entirely different approach, using end-to-end deep\nlearning without any attempt to explicitly model the geometry. In this survey,\nwe explore the overlap for geometric-based and deep learning-based frameworks.\nWe compare and contrast geometry enforcing constraints integrated into a deep\nlearning framework for depth estimation or other closely related problems. We\npresent a new taxonomy for prevalent geometry enforcing constraints used in\nmodern deep learning frameworks. We also present insightful observations and\npotential future research directions.",
      "tldr_zh": "这篇调查论文探讨了立体摄影测量(Stereophotogrammetry)的发展，从19世纪的传统几何方法（如Shape from Stereo）到现代端到端深度学习框架的演变，重点比较了如何在深度学习中整合几何约束来处理场景理解任务，如深度估计。论文提出了一种新的分类法(taxonomy)来整理常见几何约束技术，并对比了这些约束在深度学习框架中的应用。最终，论文提供了关键观察和潜在未来研究方向，以桥接几何方法和深度学习方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "A preprint",
      "pdf_url": "http://arxiv.org/pdf/2403.12431v1",
      "published_date": "2024-03-19 04:41:09 UTC",
      "updated_date": "2024-03-19 04:41:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:08:01.379563"
    },
    {
      "arxiv_id": "2403.12418v4",
      "title": "STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Lincan Li",
        "Hanchen Wang",
        "Wenjie Zhang",
        "Adelle Coster"
      ],
      "abstract": "Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,\nand non-stationary, leading to the continuous challenge of spatial-temporal\ngraph learning. In the past few years, various GNN-based methods have been\nproposed to solely focus on mimicking the relationships among node individuals\nof the STG network, ignoring the significance of modeling the intrinsic\nfeatures that exist in STG system over time. In contrast, modern Selective\nState Space Models (SSSMs) present a new approach which treat STG Network as a\nsystem, and meticulously explore the STG system's dynamic state evolution\nacross temporal dimension. In this work, we introduce Spatial-Temporal Graph\nMamba (STG-Mamba) as the first exploration of leveraging the powerful selective\nstate space models for STG learning by treating STG Network as a system, and\nemploying the Spatial-Temporal Selective State Space Module (ST-S3M) to\nprecisely focus on the selected STG latent features. Furthermore, to strengthen\nGNN's ability of modeling STG data under the setting of selective state space\nmodels, we propose Kalman Filtering Graph Neural Networks (KFGN) for\ndynamically integrate and upgrade the STG embeddings from different temporal\ngranularities through a learnable Kalman Filtering statistical theory-based\napproach. Extensive empirical studies are conducted on three benchmark STG\nforecasting datasets, demonstrating the performance superiority and\ncomputational efficiency of STG-Mamba. It not only surpasses existing\nstate-of-the-art methods in terms of STG forecasting performance, but also\neffectively alleviate the computational bottleneck of large-scale graph\nnetworks in reducing the computational cost of FLOPs and test inference time.\nThe implementation code is available at:\n\\url{https://github.com/LincanLi98/STG-Mamba}.",
      "tldr_zh": "本文提出STG-Mamba，一种基于Selective State Space Models (SSSMs)的空间-时间图（STG）学习框架，将STG网络视为系统，关注其动态状态演化，以弥补现有GNN方法的局限性。框架引入Spatial-Temporal Selective State Space Module (ST-S3M)来精准聚焦STG潜在特征，并结合Kalman Filtering Graph Neural Networks (KFGN)动态整合不同时间粒度的嵌入，提升模型的预测能力。实验结果显示，STG-Mamba在三个基准STG预测数据集上超越最先进方法，提高了预测性能并降低了计算成本，如FLOPs和推理时间。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12418v4",
      "published_date": "2024-03-19 04:02:57 UTC",
      "updated_date": "2024-05-18 11:58:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:08:14.011898"
    },
    {
      "arxiv_id": "2403.12417v1",
      "title": "On Predictive planning and counterfactual learning in active inference",
      "title_zh": "翻译失败",
      "authors": [
        "Aswin Paul",
        "Takuya Isomura",
        "Adeel Razi"
      ],
      "abstract": "Given the rapid advancement of artificial intelligence, understanding the\nfoundations of intelligent behaviour is increasingly important. Active\ninference, regarded as a general theory of behaviour, offers a principled\napproach to probing the basis of sophistication in planning and\ndecision-making. In this paper, we examine two decision-making schemes in\nactive inference based on 'planning' and 'learning from experience'.\nFurthermore, we also introduce a mixed model that navigates the data-complexity\ntrade-off between these strategies, leveraging the strengths of both to\nfacilitate balanced decision-making. We evaluate our proposed model in a\nchallenging grid-world scenario that requires adaptability from the agent.\nAdditionally, our model provides the opportunity to analyze the evolution of\nvarious parameters, offering valuable insights and contributing to an\nexplainable framework for intelligent decision-making.",
      "tldr_zh": "这篇论文探讨了 active inference 作为智能行为基础的理论，比较了基于 planning 和基于经验学习的决策方案。作者引入了一个混合模型，平衡两者的数据复杂性权衡，充分利用 planning 的预测能力和经验学习的适应性优势。在网格世界场景的实验中，该模型展示了良好的适应性，并通过参数演化分析提供了可解释的决策框架。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.12417v1",
      "published_date": "2024-03-19 04:02:31 UTC",
      "updated_date": "2024-03-19 04:02:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:08:23.899800"
    },
    {
      "arxiv_id": "2403.12406v2",
      "title": "Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion",
      "title_zh": "通过经验上下文和布朗运动的羽毛球球员行为离线模仿",
      "authors": [
        "Kuang-Da Wang",
        "Wei-Yao Wang",
        "Ping-Chun Hsieh",
        "Wen-Chih Peng"
      ],
      "abstract": "In the dynamic and rapid tactic involvements of turn-based sports, badminton\nstands out as an intrinsic paradigm that requires alter-dependent\ndecision-making of players. While the advancement of learning from offline\nexpert data in sequential decision-making has been witnessed in various\ndomains, how to rally-wise imitate the behaviors of human players from offline\nbadminton matches has remained underexplored. Replicating opponents' behavior\nbenefits players by allowing them to undergo strategic development with\ndirection before matches. However, directly applying existing methods suffers\nfrom the inherent hierarchy of the match and the compounding effect due to the\nturn-based nature of players alternatively taking actions. In this paper, we\npropose RallyNet, a novel hierarchical offline imitation learning model for\nbadminton player behaviors: (i) RallyNet captures players' decision\ndependencies by modeling decision-making processes as a contextual Markov\ndecision process. (ii) RallyNet leverages the experience to generate context as\nthe agent's intent in the rally. (iii) To generate more realistic behavior,\nRallyNet leverages Geometric Brownian Motion (GBM) to model the interactions\nbetween players by introducing a valuable inductive bias for learning player\nbehaviors. In this manner, RallyNet links player intents with interaction\nmodels with GBM, providing an understanding of interactions for sports\nanalytics. We extensively validate RallyNet with the largest available\nreal-world badminton dataset consisting of men's and women's singles,\ndemonstrating its ability to imitate player behaviors. Results reveal\nRallyNet's superiority over offline imitation learning methods and\nstate-of-the-art turn-based approaches, outperforming them by at least 16% in\nmean rule-based agent normalization score. Furthermore, we discuss various\npractical use cases to highlight RallyNet's applicability.",
      "tldr_zh": "本文提出RallyNet，一种分层离线模仿学习模型，用于从离线羽毛球比赛数据中模仿玩家行为，解决决策依赖和互动建模的挑战。RallyNet将决策建模为contextual Markov decision process，并利用经验生成上下文作为代理的意图，以捕捉玩家间的依赖关系。同时，它引入Geometric Brownian Motion (GBM)来模拟玩家互动，增强行为真实性和归纳偏差。在最大的真实羽毛球数据集上实验验证，RallyNet比现有离线模仿学习方法和基于轮转方法至少提高16%的均值归一化分数，并探讨了其在战略开发和体育分析中的实际应用。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.12406v2",
      "published_date": "2024-03-19 03:34:23 UTC",
      "updated_date": "2024-08-03 06:36:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:08:39.046961"
    },
    {
      "arxiv_id": "2403.12403v2",
      "title": "Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales",
      "title_zh": "翻译失败",
      "authors": [
        "Ayushi Nirmal",
        "Amrita Bhattacharjee",
        "Paras Sheth",
        "Huan Liu"
      ],
      "abstract": "Although social media platforms are a prominent arena for users to engage in\ninterpersonal discussions and express opinions, the facade and anonymity\noffered by social media may allow users to spew hate speech and offensive\ncontent. Given the massive scale of such platforms, there arises a need to\nautomatically identify and flag instances of hate speech. Although several hate\nspeech detection methods exist, most of these black-box methods are not\ninterpretable or explainable by design. To address the lack of\ninterpretability, in this paper, we propose to use state-of-the-art Large\nLanguage Models (LLMs) to extract features in the form of rationales from the\ninput text, to train a base hate speech classifier, thereby enabling faithful\ninterpretability by design. Our framework effectively combines the textual\nunderstanding capabilities of LLMs and the discriminative power of\nstate-of-the-art hate speech classifiers to make these classifiers faithfully\ninterpretable. Our comprehensive evaluation on a variety of English language\nsocial media hate speech datasets demonstrate: (1) the goodness of the\nLLM-extracted rationales, and (2) the surprising retention of detector\nperformance even after training to ensure interpretability. All code and data\nwill be made available at https://github.com/AmritaBh/shield.",
      "tldr_zh": "这篇论文针对社交媒体仇恨言论检测的解释性不足问题，提出了一种新框架，使用 Large Language Models (LLMs) 提取文本中的理由 (rationales)，并以此训练基础分类器，实现可解释设计。框架结合了 LLMs 的文本理解能力和仇恨言论分类器的辨别力，确保检测过程忠实可解释。在多种英语社交媒体数据集上的全面评估表明，该方法不仅提升了 rationales 的质量，还在保持检测性能的同时显著提高了模型的可解释性，所有代码和数据已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera-ready for NAACL WOAH 2024 (Workshop on Online Abuse and\n  Harms). First two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2403.12403v2",
      "published_date": "2024-03-19 03:22:35 UTC",
      "updated_date": "2024-05-08 02:47:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:08:49.887671"
    },
    {
      "arxiv_id": "2403.12400v1",
      "title": "Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Zijian Zhao",
        "Tingwei Chen",
        "Fanyi Meng",
        "Hang Li",
        "Xiaoyang Li",
        "Guangxu Zhu"
      ],
      "abstract": "Despite the development of various deep learning methods for Wi-Fi sensing,\npackage loss often results in noncontinuous estimation of the Channel State\nInformation (CSI), which negatively impacts the performance of the learning\nmodels. To overcome this challenge, we propose a deep learning model based on\nBidirectional Encoder Representations from Transformers (BERT) for CSI\nrecovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner\non the target dataset without the need for additional data. Furthermore, unlike\ntraditional interpolation methods that focus on one subcarrier at a time,\nCSI-BERT captures the sequential relationships across different subcarriers.\nExperimental results demonstrate that CSI-BERT achieves lower error rates and\nfaster speed compared to traditional interpolation methods, even when facing\nwith high loss rates. Moreover, by harnessing the recovered CSI obtained from\nCSI-BERT, other deep learning models like Residual Network and Recurrent Neural\nNetwork can achieve an average increase in accuracy of approximately 15\\% in\nWi-Fi sensing tasks. The collected dataset WiGesture and code for our model are\npublicly available at https://github.com/RS2002/CSI-BERT.",
      "tldr_zh": "该研究针对 Wi-Fi 感知中包丢失导致的 Channel State Information (CSI) 非连续问题，提出了一种基于 Bidirectional Encoder Representations from Transformers (BERT) 的自监督模型 CSI-BERT，用于 CSI 恢复。CSI-BERT 能够捕捉不同子载波间的顺序关系，并在目标数据集上训练，而无需额外数据。实验结果显示，该模型比传统插值方法具有更低的错误率和更快速度，即使在高丢失率下也能保持优势；此外，使用 CSI-BERT 恢复的 CSI，能使其他深度学习模型如 Residual Network 和 Recurrent Neural Network 在 Wi-Fi 感知任务中准确率平均提高约 15%。数据集 WiGesture 和模型代码已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, accepted by IEEE INFOCOM Deepwireless Workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12400v1",
      "published_date": "2024-03-19 03:16:52 UTC",
      "updated_date": "2024-03-19 03:16:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:09:03.801959"
    },
    {
      "arxiv_id": "2403.12392v1",
      "title": "AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis",
      "title_zh": "AraPoemBERT：用于阿拉伯诗歌分析的预训练语言模型",
      "authors": [
        "Faisal Qarah"
      ],
      "abstract": "Arabic poetry, with its rich linguistic features and profound cultural\nsignificance, presents a unique challenge to the Natural Language Processing\n(NLP) field. The complexity of its structure and context necessitates advanced\ncomputational models for accurate analysis. In this paper, we introduce\nAraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry\ntext. To demonstrate the effectiveness of the proposed model, we compared\nAraPoemBERT with 5 different Arabic language models on various NLP tasks\nrelated to Arabic poetry. The new model outperformed all other models and\nachieved state-of-the-art results in most of the downstream tasks. AraPoemBERT\nachieved unprecedented accuracy in two out of three novel tasks: poet's gender\nclassification (99.34\\% accuracy), and poetry sub-meter classification (97.79\\%\naccuracy). In addition, the model achieved an accuracy score in poems' rhyme\nclassification (97.73\\% accuracy) which is almost equivalent to the best score\nreported in this study. Moreover, the proposed model significantly outperformed\nprevious work and other comparative models in the tasks of poems' sentiment\nanalysis, achieving an accuracy of 78.95\\%, and poetry meter classification\n(99.03\\% accuracy), while significantly expanding the scope of these two\nproblems. The dataset used in this study, contains more than 2.09 million\nverses collected from online sources, each associated with various attributes\nsuch as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the\neffectiveness of the proposed model in understanding and analyzing Arabic\npoetry, achieving state-of-the-art results in several tasks and outperforming\nprevious works and other language models included in the study. AraPoemBERT\nmodel is publicly available on \\url{https://huggingface.co/faisalq}.",
      "tldr_zh": "本论文介绍了 AraPoemBERT，一种专为阿拉伯诗歌分析预训练的语言模型，旨在处理阿拉伯诗歌的复杂结构和文化特征。模型在超过 2.09 百万诗行数据集上进行预训练，这些数据包含属性如 meter、sub-meter、poet、rhyme 和 topic，并与 5 个其他阿拉伯语言模型进行比较。实验结果显示，AraPoemBERT 在多个任务中表现出色，包括诗人性别分类（99.34% accuracy）、诗歌 sub-meter 分类（97.79% accuracy）和诗歌 meter 分类（99.03% accuracy），并在大多数下游任务上达到了 state-of-the-art 水平。模型已公开可用于 https://huggingface.co/faisalq。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 11 figures, not published yet",
      "pdf_url": "http://arxiv.org/pdf/2403.12392v1",
      "published_date": "2024-03-19 02:59:58 UTC",
      "updated_date": "2024-03-19 02:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:09:16.891741"
    },
    {
      "arxiv_id": "2403.12391v1",
      "title": "FairSTG: Countering performance heterogeneity via collaborative sample-level optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Gengyu Lin",
        "Zhengyang Zhou",
        "Qihe Huang",
        "Kuo Yang",
        "Shifen Cheng",
        "Yang Wang"
      ],
      "abstract": "Spatiotemporal learning plays a crucial role in mobile computing techniques\nto empower smart cites. While existing research has made great efforts to\nachieve accurate predictions on the overall dataset, they still neglect the\nsignificant performance heterogeneity across samples. In this work, we\ndesignate the performance heterogeneity as the reason for unfair spatiotemporal\nlearning, which not only degrades the practical functions of models, but also\nbrings serious potential risks to real-world urban applications. To fix this\ngap, we propose a model-independent Fairness-aware framework for SpatioTemporal\nGraph learning (FairSTG), which inherits the idea of exploiting advantages of\nwell-learned samples to challenging ones with collaborative mix-up.\nSpecifically, FairSTG consists of a spatiotemporal feature extractor for model\ninitialization, a collaborative representation enhancement for knowledge\ntransfer between well-learned samples and challenging ones, and fairness\nobjectives for immediately suppressing sample-level performance heterogeneity.\nExperiments on four spatiotemporal datasets demonstrate that our FairSTG\nsignificantly improves the fairness quality while maintaining comparable\nforecasting accuracy. Case studies show FairSTG can counter both spatial and\ntemporal performance heterogeneity by our sample-level retrieval and\ncompensation, and our work can potentially alleviate the risks on\nspatiotemporal resource allocation for underrepresented urban regions.",
      "tldr_zh": "本研究指出，现有的 spatiotemporal learning 方法虽提升了整体数据集预测准确性，但忽略了样本间的 performance heterogeneity，导致不公平问题，并可能带来城市应用的潜在风险。为解决此问题，提出 FairSTG 框架——一个独立于模型的公平性感知时空图学习框架，包括时空特征提取器、协作表示增强（用于知识转移）和公平性目标，以通过协作 mix-up 抑制样本级性能异质性。实验在四个 spatiotemporal 数据集上表明，FairSTG 显著提高了公平性指标，同时保持了可比的预测准确性；案例研究进一步显示，该框架可通过样本级别的检索和补偿缓解空间和时间性能异质性，从而潜在缓解未被充分代表的城市区域资源分配风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review by IEEE Transactions on Mobile Computing",
      "pdf_url": "http://arxiv.org/pdf/2403.12391v1",
      "published_date": "2024-03-19 02:59:50 UTC",
      "updated_date": "2024-03-19 02:59:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:09:27.799780"
    },
    {
      "arxiv_id": "2403.12388v2",
      "title": "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ying-Chun Lin",
        "Jennifer Neville",
        "Jack W. Stokes",
        "Longqi Yang",
        "Tara Safavi",
        "Mengting Wan",
        "Scott Counts",
        "Siddharth Suri",
        "Reid Andersen",
        "Xiaofeng Xu",
        "Deepak Gupta",
        "Sujay Kumar Jauhar",
        "Xia Song",
        "Georg Buscher",
        "Saurabh Tiwary",
        "Brent Hecht",
        "Jaime Teevan"
      ],
      "abstract": "Accurate and interpretable user satisfaction estimation (USE) is critical for\nunderstanding, evaluating, and continuously improving conversational systems.\nUsers express their satisfaction or dissatisfaction with diverse conversational\npatterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented\n(customer service chatbot) conversational systems. Existing approaches based on\nfeaturized ML models or text embeddings fall short in extracting generalizable\npatterns and are hard to interpret. In this work, we show that LLMs can extract\ninterpretable signals of user satisfaction from their natural language\nutterances more effectively than embedding-based approaches. Moreover, an LLM\ncan be tailored for USE via an iterative prompting framework using supervision\nfrom labeled examples. The resulting method, Supervised Prompting for User\nsatisfaction Rubrics (SPUR), not only has higher accuracy but is more\ninterpretable as it scores user satisfaction via learned rubrics with a\ndetailed breakdown.",
      "tldr_zh": "该研究强调了准确且可解释的用户满意度估计（User Satisfaction Estimation, USE）对对话系统的重要性，并指出现有基于特征的机器学习模型或文本嵌入方法难以提取通用模式且缺乏解释性。论文展示了Large Language Models (LLMs)能够更有效地从用户自然语言表述中提取可解释的满意度信号。作者提出了一种迭代提示框架，即Supervised Prompting for User satisfaction Rubrics (SPUR)，通过使用标记示例的监督来定制LLMs进行USE。该方法不仅提升了准确性，还通过学习的分数表（rubrics）提供详细的满意度分解分析，从而为改进通用（如ChatGPT）和任务导向（如客服聊天机器人）对话系统提供了更可靠的工具。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12388v2",
      "published_date": "2024-03-19 02:57:07 UTC",
      "updated_date": "2024-06-09 00:58:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:09:40.536236"
    },
    {
      "arxiv_id": "2403.12386v1",
      "title": "Pipelined Biomedical Event Extraction Rivaling Joint Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Pengchao Wu",
        "Xuefeng Li",
        "Jinghang Gu",
        "Longhua Qian",
        "Guodong Zhou"
      ],
      "abstract": "Biomedical event extraction is an information extraction task to obtain\nevents from biomedical text, whose targets include the type, the trigger, and\nthe respective arguments involved in an event. Traditional biomedical event\nextraction usually adopts a pipelined approach, which contains trigger\nidentification, argument role recognition, and finally event construction\neither using specific rules or by machine learning. In this paper, we propose\nan n-ary relation extraction method based on the BERT pre-training model to\nconstruct Binding events, in order to capture the semantic information about an\nevent's context and its participants. The experimental results show that our\nmethod achieves promising results on the GE11 and GE13 corpora of the BioNLP\nshared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates\nthat by significantly improving theperformance of Binding events, the overall\nperformance of the pipelined event extraction approach or even exceeds those of\ncurrent joint learning methods.",
      "tldr_zh": "本研究提出了一种基于BERT预训练模型的n-ary关系提取方法，用于生物医学事件提取，特别是针对Binding events的构建，以捕捉事件上下文和参与者的语义信息。该方法采用流水线(pipeline)策略，包括触发器识别、参数角色识别和事件构建，旨在提升整体性能。实验结果显示，在BioNLP共享任务的GE11和GE13语料库上，该方法分别获得63.14%和59.40%的F1 scores，并证明其表现不逊于甚至超过当前的联合学习(joint learning)方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12386v1",
      "published_date": "2024-03-19 02:52:58 UTC",
      "updated_date": "2024-03-19 02:52:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:09:50.937934"
    },
    {
      "arxiv_id": "2403.12368v1",
      "title": "Characteristic AI Agents via Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Wang",
        "Hongliang Dai",
        "Shen Gao",
        "Piji Li"
      ],
      "abstract": "The advancement of Large Language Models (LLMs) has led to significant\nenhancements in the performance of chatbot systems. Many researchers have\ndedicated their efforts to the development of bringing characteristics to\nchatbots. While there have been commercial products for developing role-driven\nchatbots using LLMs, it is worth noting that academic research in this area\nremains relatively scarce. Our research focuses on investigating the\nperformance of LLMs in constructing Characteristic AI Agents by simulating\nreal-life individuals across different settings. Current investigations have\nprimarily focused on act on roles with simple profiles. In response to this\nresearch gap, we create a benchmark for the characteristic AI agents task,\nincluding dataset, techniques, and evaluation metrics. A dataset called\n``Character100'' is built for this benchmark, comprising the most-visited\npeople on Wikipedia for language models to role-play. With the constructed\ndataset, we conduct comprehensive assessment of LLMs across various settings.\nIn addition, we devise a set of automatic metrics for quantitative performance\nevaluation. The experimental results underscore the potential directions for\nfurther improvement in the capabilities of LLMs in constructing characteristic\nAI agents. The benchmark is available at\nhttps://github.com/nuaa-nlp/Character100.",
      "tldr_zh": "本研究调查了大型语言模型 (LLMs) 在构建具有特性的 AI 代理方面的性能，这些代理模拟真实生活中的个体，以填补现有学术研究的空白。研究者创建了一个名为 Character100 的数据集，包含 Wikipedia 上访问量最高的100个人的信息，用于 LLMs 的角色扮演基准。实验通过全面评估不同设置下的 LLMs，并设计自动评估指标，揭示了模型的潜力并指出了改进方向；相关基准已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING 2024,The benchmark is available at:\n  https://github.com/nuaa-nlp/Character100",
      "pdf_url": "http://arxiv.org/pdf/2403.12368v1",
      "published_date": "2024-03-19 02:25:29 UTC",
      "updated_date": "2024-03-19 02:25:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:10:02.178007"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 88,
  "processed_papers_count": 88,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T17:10:32.628257"
}
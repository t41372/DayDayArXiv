[
  {
    "arxiv_id": "2407.17673v2",
    "title": "CRASAR-U-DROIDs: A Large Scale Benchmark Dataset for Building Alignment and Damage Assessment in Georectified sUAS Imagery",
    "authors": [
      "Thomas Manzini",
      "Priyankari Perali",
      "Raisa Karnik",
      "Robin Murphy"
    ],
    "abstract": "This document presents the Center for Robot Assisted Search And Rescue -\nUncrewed Aerial Systems - Disaster Response Overhead Inspection Dataset\n(CRASAR-U-DROIDs) for building damage assessment and spatial alignment\ncollected from small uncrewed aerial systems (sUAS) geospatial imagery. This\ndataset is motivated by the increasing use of sUAS in disaster response and the\nlack of previous work in utilizing high-resolution geospatial sUAS imagery for\nmachine learning and computer vision models, the lack of alignment with\noperational use cases, and with hopes of enabling further investigations\nbetween sUAS and satellite imagery. The CRASAR-U-DRIODs dataset consists of\nfifty-two (52) orthomosaics from ten (10) federally declared disasters\n(Hurricane Ian, Hurricane Ida, Hurricane Harvey, Hurricane Idalia, Hurricane\nLaura, Hurricane Michael, Musset Bayou Fire, Mayfield Tornado, Kilauea\nEruption, and Champlain Towers Collapse) spanning 67.98 square kilometers\n(26.245 square miles), containing 21,716 building polygons and damage labels,\nand 7,880 adjustment annotations. The imagery was tiled and presented in\nconjunction with overlaid building polygons to a pool of 130 annotators who\nprovided human judgments of damage according to the Joint Damage Scale. These\nannotations were then reviewed via a two-stage review process in which building\npolygon damage labels were first reviewed individually and then again by\ncommittee. Additionally, the building polygons have been aligned spatially to\nprecisely overlap with the imagery to enable more performant machine learning\nmodels to be trained. It appears that CRASAR-U-DRIODs is the largest labeled\ndataset of sUAS orthomosaic imagery.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "16 Pages, 7 Figures, 6 Tables",
    "pdf_url": "http://arxiv.org/pdf/2407.17673v2",
    "published_date": "2024-07-24 23:39:10 UTC",
    "updated_date": "2024-07-29 18:12:21 UTC"
  },
  {
    "arxiv_id": "2407.17672v2",
    "title": "Spiking Neural Networks in Vertical Federated Learning: Performance Trade-offs",
    "authors": [
      "Maryam Abbasihafshejani",
      "Anindya Maiti",
      "Murtuza Jadliwala"
    ],
    "abstract": "Federated machine learning enables model training across multiple clients\nwhile maintaining data privacy. Vertical Federated Learning (VFL) specifically\ndeals with instances where the clients have different feature sets of the same\nsamples. As federated learning models aim to improve efficiency and\nadaptability, innovative neural network architectures like Spiking Neural\nNetworks (SNNs) are being leveraged to enable fast and accurate processing at\nthe edge. SNNs, known for their efficiency over Artificial Neural Networks\n(ANNs), have not been analyzed for their applicability in VFL, thus far. In\nthis paper, we investigate the benefits and trade-offs of using SNN models in a\nvertical federated learning setting. We implement two different federated\nlearning architectures -- with model splitting and without model splitting --\nthat have different privacy and performance implications. We evaluate the setup\nusing CIFAR-10 and CIFAR-100 benchmark datasets along with SNN implementations\nof VGG9 and ResNET classification models. Comparative evaluations demonstrate\nthat the accuracy of SNN models is comparable to that of traditional ANNs for\nVFL applications, albeit significantly more energy efficient.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17672v2",
    "published_date": "2024-07-24 23:31:02 UTC",
    "updated_date": "2024-08-13 22:46:55 UTC"
  },
  {
    "arxiv_id": "2407.17642v1",
    "title": "SMA-Hyper: Spatiotemporal Multi-View Fusion Hypergraph Learning for Traffic Accident Prediction",
    "authors": [
      "Xiaowei Gao",
      "James Haworth",
      "Ilya Ilyankou",
      "Xianghui Zhang",
      "Tao Cheng",
      "Stephen Law",
      "Huanfa Chen"
    ],
    "abstract": "Predicting traffic accidents is the key to sustainable city management, which\nrequires effective address of the dynamic and complex spatiotemporal\ncharacteristics of cities. Current data-driven models often struggle with data\nsparsity and typically overlook the integration of diverse urban data sources\nand the high-order dependencies within them. Additionally, they frequently rely\non predefined topologies or weights, limiting their adaptability in\nspatiotemporal predictions. To address these issues, we introduce the\nSpatiotemporal Multiview Adaptive HyperGraph Learning (SMA-Hyper) model, a\ndynamic deep learning framework designed for traffic accident prediction.\nBuilding on previous research, this innovative model incorporates dual adaptive\nspatiotemporal graph learning mechanisms that enable high-order cross-regional\nlearning through hypergraphs and dynamic adaptation to evolving urban data. It\nalso utilises contrastive learning to enhance global and local data\nrepresentations in sparse datasets and employs an advance attention mechanism\nto fuse multiple views of accident data and urban functional features, thereby\nenriching the contextual understanding of risk factors. Extensive testing on\nthe London traffic accident dataset demonstrates that the SMA-Hyper model\nsignificantly outperforms baseline models across various temporal horizons and\nmultistep outputs, affirming the effectiveness of its multiview fusion and\nadaptive learning strategies. The interpretability of the results further\nunderscores its potential to improve urban traffic management and safety by\nleveraging complex spatiotemporal urban data, offering a scalable framework\nadaptable to diverse urban environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17642v1",
    "published_date": "2024-07-24 21:10:34 UTC",
    "updated_date": "2024-07-24 21:10:34 UTC"
  },
  {
    "arxiv_id": "2407.20267v1",
    "title": "A Large Encoder-Decoder Family of Foundation Models For Chemical Language",
    "authors": [
      "Eduardo Soares",
      "Victor Shirasuna",
      "Emilio Vital Brazil",
      "Renato Cerqueira",
      "Dmitry Zubarev",
      "Kristin Schmidt"
    ],
    "abstract": "Large-scale pre-training methodologies for chemical language models represent\na breakthrough in cheminformatics. These methods excel in tasks such as\nproperty prediction and molecule generation by learning contextualized\nrepresentations of input tokens through self-supervised learning on large\nunlabeled corpora. Typically, this involves pre-training on unlabeled data\nfollowed by fine-tuning on specific tasks, reducing dependence on annotated\ndatasets and broadening chemical language representation understanding. This\npaper introduces a large encoder-decoder chemical foundation models pre-trained\non a curated dataset of 91 million SMILES samples sourced from PubChem, which\nis equivalent to 4 billion of molecular tokens. The proposed foundation model\nsupports different complex tasks, including quantum property prediction, and\noffer flexibility with two main variants (289M and $8\\times289M$). Our\nexperiments across multiple benchmark datasets validate the capacity of the\nproposed model in providing state-of-the-art results for different tasks. We\nalso provide a preliminary assessment of the compositionality of the embedding\nspace as a prerequisite for the reasoning tasks. We demonstrate that the\nproduced latent space is separable compared to the state-of-the-art with\nfew-shot learning capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 3 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.20267v1",
    "published_date": "2024-07-24 20:30:39 UTC",
    "updated_date": "2024-07-24 20:30:39 UTC"
  },
  {
    "arxiv_id": "2407.17620v1",
    "title": "CoMoTo: Unpaired Cross-Modal Lesion Distillation Improves Breast Lesion Detection in Tomosynthesis",
    "authors": [
      "Muhammad Alberb",
      "Marawan Elbatel",
      "Aya Elgebaly",
      "Ricardo Montoya-del-Angel",
      "Xiaomeng Li",
      "Robert Mart√≠"
    ],
    "abstract": "Digital Breast Tomosynthesis (DBT) is an advanced breast imaging modality\nthat offers superior lesion detection accuracy compared to conventional\nmammography, albeit at the trade-off of longer reading time. Accelerating\nlesion detection from DBT using deep learning is hindered by limited data\navailability and huge annotation costs. A possible solution to this issue could\nbe to leverage the information provided by a more widely available modality,\nsuch as mammography, to enhance DBT lesion detection. In this paper, we present\na novel framework, CoMoTo, for improving lesion detection in DBT. Our framework\nleverages unpaired mammography data to enhance the training of a DBT model,\nimproving practicality by eliminating the need for mammography during\ninference. Specifically, we propose two novel components, Lesion-specific\nKnowledge Distillation (LsKD) and Intra-modal Point Alignment (ImPA). LsKD\nselectively distills lesion features from a mammography teacher model to a DBT\nstudent model, disregarding background features. ImPA further enriches LsKD by\nensuring the alignment of lesion features within the teacher before distilling\nknowledge to the student. Our comprehensive evaluation shows that CoMoTo is\nsuperior to traditional pretraining and image-level KD, improving performance\nby 7% Mean Sensitivity under low-data setting. Our code is available at\nhttps://github.com/Muhammad-Al-Barbary/CoMoTo .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ADSMI @ MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17620v1",
    "published_date": "2024-07-24 20:17:05 UTC",
    "updated_date": "2024-07-24 20:17:05 UTC"
  },
  {
    "arxiv_id": "2407.17596v2",
    "title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI",
    "authors": [
      "Tim R√§dsch",
      "Annika Reinke",
      "Vivienn Weru",
      "Minu D. Tizabi",
      "Nicholas Heller",
      "Fabian Isensee",
      "Annette Kopp-Schneider",
      "Lena Maier-Hein"
    ],
    "abstract": "This paper does not describe a novel method. Instead, it studies an essential\nfoundation for reliable benchmarking and ultimately real-world application of\nAI-based image analysis: generating high-quality reference annotations.\nPrevious research has focused on crowdsourcing as a means of outsourcing\nannotations. However, little attention has so far been given to annotation\ncompanies, specifically regarding their internal quality assurance (QA)\nprocesses. Therefore, our aim is to evaluate the influence of QA employed by\nannotation companies on annotation quality and devise methodologies for\nmaximizing data annotation efficacy. Based on a total of 57,648 instance\nsegmented images obtained from a total of 924 annotators and 34 QA workers from\nfour annotation companies and Amazon Mechanical Turk (MTurk), we derived the\nfollowing insights: (1) Annotation companies perform better both in terms of\nquantity and quality compared to the widely used platform MTurk. (2) Annotation\ncompanies' internal QA only provides marginal improvements, if any. However,\nimproving labeling instructions instead of investing in QA can substantially\nboost annotation performance. (3) The benefit of internal QA depends on\nspecific image characteristics. Our work could enable researchers to derive\nsubstantially more value from a fixed annotation budget and change the way\nannotation companies conduct internal QA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024, preprint, Computer Vision, Data Annotation",
    "pdf_url": "http://arxiv.org/pdf/2407.17596v2",
    "published_date": "2024-07-24 19:02:01 UTC",
    "updated_date": "2024-07-26 11:26:43 UTC"
  },
  {
    "arxiv_id": "2408.01454v1",
    "title": "Amman City, Jordan: Toward a Sustainable City from the Ground Up",
    "authors": [
      "Ra'Fat Al-Msie'deen"
    ],
    "abstract": "The idea of smart cities (SCs) has gained substantial attention in recent\nyears. The SC paradigm aims to improve citizens' quality of life and protect\nthe city's environment. As we enter the age of next-generation SCs, it is\nimportant to explore all relevant aspects of the SC paradigm. In recent years,\nthe advancement of Information and Communication Technologies (ICT) has\nproduced a trend of supporting daily objects with smartness, targeting to make\nhuman life easier and more comfortable. The paradigm of SCs appears as a\nresponse to the purpose of building the city of the future with advanced\nfeatures. SCs still face many challenges in their implementation, but\nincreasingly more studies regarding SCs are implemented. Nowadays, different\ncities are employing SC features to enhance services or the residents quality\nof life. This work provides readers with useful and important information about\nAmman Smart City.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.NI",
      "cs.SE",
      "A.0, J.0, J.3, C.2, H.0, I.2"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages, 3 figures, 6 tables, 56 references",
    "pdf_url": "http://arxiv.org/pdf/2408.01454v1",
    "published_date": "2024-07-24 18:30:16 UTC",
    "updated_date": "2024-07-24 18:30:16 UTC"
  },
  {
    "arxiv_id": "2407.17572v4",
    "title": "CityX: Controllable Procedural Content Generation for Unbounded 3D Cities",
    "authors": [
      "Shougao Zhang",
      "Mengqi Zhou",
      "Yuxi Wang",
      "Chuanchen Luo",
      "Rongyu Wang",
      "Yiwei Li",
      "Zhaoxiang Zhang",
      "Junran Peng"
    ],
    "abstract": "Urban areas, as the primary human habitat in modern civilization, accommodate\na broad spectrum of social activities. With the surge of embodied intelligence,\nrecent years have witnessed an increasing presence of physical agents in urban\nareas, such as autonomous vehicles and delivery robots. As a result,\npractitioners significantly value crafting authentic, simulation-ready 3D\ncities to facilitate the training and verification of such agents. However,\nthis task is quite challenging. Current generative methods fall short in either\ndiversity, controllability, or fidelity. In this work, we resort to the\nprocedural content generation (PCG) technique for high-fidelity generation. It\nassembles superior assets according to empirical rules, ultimately leading to\nindustrial-grade outcomes. To ensure diverse and self contained creation, we\ndesign a management protocol to accommodate extensive PCG plugins with distinct\nfunctions and interfaces. Based on this unified PCG library, we develop a\nmulti-agent framework to transform multi-modal instructions, including OSM,\nsemantic maps, and satellite images, into executable programs. The programs\ncoordinate relevant plugins to construct the 3D city consistent with the\ncontrol condition. A visual feedback scheme is introduced to further refine the\ninitial outcomes. Our method, named CityX, demonstrates its superiority in\ncreating diverse, controllable, and realistic 3D urban scenes. The synthetic\nscenes can be seamlessly deployed as a real-time simulator and an infinite data\ngenerator for embodied intelligence research. Our project page:\nhttps://cityx-lab.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17572v4",
    "published_date": "2024-07-24 18:05:13 UTC",
    "updated_date": "2024-12-09 09:30:26 UTC"
  },
  {
    "arxiv_id": "2407.17469v1",
    "title": "I Could've Asked That: Reformulating Unanswerable Questions",
    "authors": [
      "Wenting Zhao",
      "Ge Gao",
      "Claire Cardie",
      "Alexander M. Rush"
    ],
    "abstract": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17469v1",
    "published_date": "2024-07-24 17:59:07 UTC",
    "updated_date": "2024-07-24 17:59:07 UTC"
  },
  {
    "arxiv_id": "2407.17468v1",
    "title": "WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries",
    "authors": [
      "Wenting Zhao",
      "Tanya Goyal",
      "Yu Ying Chiu",
      "Liwei Jiang",
      "Benjamin Newman",
      "Abhilasha Ravichander",
      "Khyathi Chandu",
      "Ronan Le Bras",
      "Claire Cardie",
      "Yuntian Deng",
      "Yejin Choi"
    ],
    "abstract": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17468v1",
    "published_date": "2024-07-24 17:59:05 UTC",
    "updated_date": "2024-07-24 17:59:05 UTC"
  },
  {
    "arxiv_id": "2407.17460v2",
    "title": "SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning",
    "authors": [
      "Jianpeng Yao",
      "Xiaopan Zhang",
      "Yu Xia",
      "Zejin Wang",
      "Amit K. Roy-Chowdhury",
      "Jiachen Li"
    ],
    "abstract": "Reinforcement learning (RL) enables social robots to generate trajectories\nwithout relying on human-designed rules or interventions, making it generally\nmore effective than rule-based systems in adapting to complex, dynamic\nreal-world scenarios. However, social navigation is a safety-critical task that\nrequires robots to avoid collisions with pedestrians, whereas existing RL-based\nsolutions often fall short of ensuring safety in complex environments. In this\npaper, we propose SoNIC, which to the best of our knowledge is the first\nalgorithm that integrates adaptive conformal inference (ACI) with constrained\nreinforcement learning (CRL) to enable safe policy learning for social\nnavigation. Specifically, our method not only augments RL observations with\nACI-generated nonconformity scores, which inform the agent of the quantified\nuncertainty but also employs these uncertainty estimates to effectively guide\nthe behaviors of RL agents by using constrained reinforcement learning. This\nintegration regulates the behaviors of RL agents and enables them to handle\nsafety-critical situations. On the standard CrowdNav benchmark, our method\nachieves a success rate of 96.93%, which is 11.67% higher than the previous\nstate-of-the-art RL method and results in 4.5 times fewer collisions and 2.8\ntimes fewer intrusions to ground-truth human future trajectories as well as\nenhanced robustness in out-of-distribution scenarios. To further validate our\napproach, we deploy our algorithm on a real robot by developing a ROS2-based\nnavigation system. Our experiments demonstrate that the system can generate\nrobust and socially polite decision-making when interacting with both sparse\nand dense crowds. The video demos can be found on our project website:\nhttps://sonic-social-nav.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://sonic-social-nav.github.io/; 16 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.17460v2",
    "published_date": "2024-07-24 17:57:21 UTC",
    "updated_date": "2025-02-06 18:55:45 UTC"
  },
  {
    "arxiv_id": "2407.16890v1",
    "title": "Why Machines Can't Be Moral: Turing's Halting Problem and the Moral Limits of Artificial Intelligence",
    "authors": [
      "Massimo Passamonti"
    ],
    "abstract": "In this essay, I argue that explicit ethical machines, whose moral principles\nare inferred through a bottom-up approach, are unable to replicate human-like\nmoral reasoning and cannot be considered moral agents. By utilizing Alan\nTuring's theory of computation, I demonstrate that moral reasoning is\ncomputationally intractable by these machines due to the halting problem. I\naddress the frontiers of machine ethics by formalizing moral problems into\n'algorithmic moral questions' and by exploring moral psychology's dual-process\nmodel. While the nature of Turing Machines theoretically allows artificial\nagents to engage in recursive moral reasoning, critical limitations are\nintroduced by the halting problem, which states that it is impossible to\npredict with certainty whether a computational process will halt. A thought\nexperiment involving a military drone illustrates this issue, showing that an\nartificial agent might fail to decide between actions due to the halting\nproblem, which limits the agent's ability to make decisions in all instances,\nundermining its moral agency.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16890v1",
    "published_date": "2024-07-24 17:50:24 UTC",
    "updated_date": "2024-07-24 17:50:24 UTC"
  },
  {
    "arxiv_id": "2407.17454v3",
    "title": "Automated Explanation Selection for Scientific Discovery",
    "authors": [
      "Markus Iser"
    ],
    "abstract": "Automated reasoning is a key technology in the young but rapidly growing\nfield of Explainable Artificial Intelligence (XAI). Explanability helps build\ntrust in artificial intelligence systems beyond their mere predictive accuracy\nand robustness. In this paper, we propose a cycle of scientific discovery that\ncombines machine learning with automated reasoning for the generation and the\nselection of explanations. We present a taxonomy of explanation selection\nproblems that draws on insights from sociology and cognitive science. These\nselection criteria subsume existing notions and extend them with new\nproperties.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Composite AI Workshop at ECAI 2024 (accepted for publication)",
    "pdf_url": "http://arxiv.org/pdf/2407.17454v3",
    "published_date": "2024-07-24 17:41:32 UTC",
    "updated_date": "2024-08-06 08:52:18 UTC"
  },
  {
    "arxiv_id": "2407.17546v1",
    "title": "Exploring Domain Robust Lightweight Reward Models based on Router Mechanism",
    "authors": [
      "Hyuk Namgoong",
      "Jeesu Jung",
      "Sangkeun Jung",
      "Yoonhyung Roh"
    ],
    "abstract": "Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted for ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17546v1",
    "published_date": "2024-07-24 17:25:12 UTC",
    "updated_date": "2024-07-24 17:25:12 UTC"
  },
  {
    "arxiv_id": "2407.17447v2",
    "title": "FLRT: Fluent Student-Teacher Redteaming",
    "authors": [
      "T. Ben Thompson",
      "Michael Sklar"
    ],
    "abstract": "Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. To redteam or jailbreak these\nmodels for compliance with toxic requests, users and security analysts have\ndeveloped adversarial prompting techniques. One attack method is to apply\ndiscrete optimization techniques to the prompt. However, the resulting attack\nstrings are often gibberish text, easily filtered by defenders due to high\nmeasured perplexity, and may fail for unseen tasks and/or well-tuned models. In\nthis work, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17447v2",
    "published_date": "2024-07-24 17:23:18 UTC",
    "updated_date": "2024-10-01 17:39:09 UTC"
  },
  {
    "arxiv_id": "2407.17438v3",
    "title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation",
    "authors": [
      "Zhenzhi Wang",
      "Yixuan Li",
      "Yanhong Zeng",
      "Youqing Fang",
      "Yuwei Guo",
      "Wenran Liu",
      "Jing Tan",
      "Kai Chen",
      "Tianfan Xue",
      "Bo Dai",
      "Dahua Lin"
    ],
    "abstract": "Human image animation involves generating videos from a character photo,\nallowing user control and unlocking the potential for video and movie\nproduction. While recent approaches yield impressive results using high-quality\ntraining data, the inaccessibility of these datasets hampers fair and\ntransparent benchmarking. Moreover, these approaches prioritize 2D human motion\nand overlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of real-world videos from the\ninternet. We developed and applied careful filtering rules to ensure video\nquality, resulting in a curated collection of 20K high-resolution (1080P)\nhuman-centric videos. Human and camera motion annotation is accomplished using\na 2D pose estimator and a SLAM-based method. To expand our synthetic dataset,\nwe collected 10K 3D avatar assets and leveraged existing assets of body shapes,\nskin textures and clothings. Notably, we introduce a rule-based camera\ntrajectory generation method, enabling the synthetic pipeline to incorporate\ndiverse and precise camera motion annotation, which can rarely be found in\nreal-world data. To verify the effectiveness of HumanVid, we establish a\nbaseline model named CamAnimate, short for Camera-controllable Human Animation,\nthat considers both human and camera motions as conditions. Through extensive\nexperimentation, we demonstrate that such simple baseline training on our\nHumanVid achieves state-of-the-art performance in controlling both human pose\nand camera motions, setting a new benchmark. Demo, data and code could be found\nin the project website: https://humanvid.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS D&B Track 2024 camera ready version, TL;DR: the first\n  large-scale dataset for camera controllable human image animation task, and a\n  baseline method",
    "pdf_url": "http://arxiv.org/pdf/2407.17438v3",
    "published_date": "2024-07-24 17:15:58 UTC",
    "updated_date": "2024-11-21 03:26:54 UTC"
  },
  {
    "arxiv_id": "2407.17413v3",
    "title": "$A^*$ for Graphs of Convex Sets",
    "authors": [
      "Kaarthik Sundar",
      "Sivakumar Rathinam"
    ],
    "abstract": "We present a novel algorithm that fuses the existing convex-programming based\napproach with heuristic information to find optimality guarantees and\nnear-optimal paths for the Shortest Path Problem in the Graph of Convex Sets\n(SPP-GCS). Our method, inspired by $A^*$, initiates a best-first-like procedure\nfrom a designated subset of vertices and iteratively expands it until further\ngrowth is neither possible nor beneficial. Traditionally, obtaining solutions\nwith bounds for an optimization problem involves solving a relaxation,\nmodifying the relaxed solution to a feasible one, and then comparing the two\nsolutions to establish bounds. However, for SPP-GCS, we demonstrate that\nreversing this process can be more advantageous, especially with Euclidean\ntravel costs. In other words, we initially employ $A^*$ to find a feasible\nsolution for SPP-GCS, then solve a convex relaxation restricted to the vertices\nexplored by $A^*$ to obtain a relaxed solution, and finally, compare the\nsolutions to derive bounds. We present numerical results to highlight the\nadvantages of our algorithm over the existing approach in terms of the sizes of\nthe convex programs solved and computation time.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "math.OC",
    "comment": "International Conference on Automated Planning and Scheduling (ICAPS)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2407.17413v3",
    "published_date": "2024-07-24 16:48:32 UTC",
    "updated_date": "2025-04-19 07:19:16 UTC"
  },
  {
    "arxiv_id": "2407.17412v1",
    "title": "(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork",
    "authors": [
      "Tianjin Huang",
      "Fang Meng",
      "Li Shen",
      "Fan Liu",
      "Yulong Pei",
      "Mykola Pechenizkiy",
      "Shiwei Liu",
      "Tianlong Chen"
    ],
    "abstract": "Large-scale neural networks have demonstrated remarkable performance in\ndifferent domains like vision and language processing, although at the cost of\nmassive computation resources. As illustrated by compression literature,\nstructural model pruning is a prominent algorithm to encourage model\nefficiency, thanks to its acceleration-friendly sparsity patterns. One of the\nkey questions of structural pruning is how to estimate the channel\nsignificance. In parallel, work on data-centric AI has shown that\nprompting-based techniques enable impressive generalization of large language\nmodels across diverse downstream tasks. In this paper, we investigate a\ncharming possibility - \\textit{leveraging visual prompts to capture the channel\nimportance and derive high-quality structural sparsity}. To this end, we\npropose a novel algorithmic framework, namely \\texttt{PASS}. It is a tailored\nhyper-network to take both visual prompts and network weight statistics as\ninput, and output layer-wise channel sparsity in a recurrent manner. Such\ndesigns consider the intrinsic channel dependency between layers. Comprehensive\nexperiments across multiple network architectures and six datasets demonstrate\nthe superiority of \\texttt{PASS} in locating good structural sparsity. For\nexample, at the same FLOPs level, \\texttt{PASS} subnetworks achieve $1\\%\\sim\n3\\%$ better accuracy on Food101 dataset; or with a similar performance of\n$80\\%$ accuracy, \\texttt{PASS} subnetworks obtain $0.35\\times$ more speedup\nthan the baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.17412v1",
    "published_date": "2024-07-24 16:47:45 UTC",
    "updated_date": "2024-07-24 16:47:45 UTC"
  },
  {
    "arxiv_id": "2407.17406v1",
    "title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models",
    "authors": [
      "Yida Zhao",
      "Chao Lou",
      "Kewei Tu"
    ],
    "abstract": "Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17406v1",
    "published_date": "2024-07-24 16:38:38 UTC",
    "updated_date": "2024-07-24 16:38:38 UTC"
  },
  {
    "arxiv_id": "2407.17404v2",
    "title": "Grammar-based Game Description Generation using Large Language Models",
    "authors": [
      "Tsunehiko Tanaka",
      "Edgar Simo-Serra"
    ],
    "abstract": "Game Description Language (GDL) provides a standardized way to express\ndiverse games in a machine-readable format, enabling automated game simulation,\nand evaluation. While previous research has explored game description\ngeneration using search-based methods, generating GDL descriptions from natural\nlanguage remains a challenging task. This paper presents a novel framework that\nleverages Large Language Models (LLMs) to generate grammatically accurate game\ndescriptions from natural language. Our approach consists of two stages: first,\nwe gradually generate a minimal grammar based on GDL specifications; second, we\niteratively improve the game description through grammar-guided generation. Our\nframework employs a specialized parser that identifies valid subsequences and\ncandidate symbols from LLM responses, enabling gradual refinement of the output\nto ensure grammatical correctness. Experimental results demonstrate that our\niterative improvement approach significantly outperforms baseline methods that\ndirectly use LLM outputs. Our code is available at\nhttps://github.com/tsunehiko/ggdg",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at the IEEE Transactions on Games",
    "pdf_url": "http://arxiv.org/pdf/2407.17404v2",
    "published_date": "2024-07-24 16:36:02 UTC",
    "updated_date": "2025-01-22 05:52:42 UTC"
  },
  {
    "arxiv_id": "2407.17545v1",
    "title": "Large Language Models for Anomaly Detection in Computational Workflows: from Supervised Fine-Tuning to In-Context Learning",
    "authors": [
      "Hongwei Jin",
      "George Papadimitriou",
      "Krishnan Raghavan",
      "Pawel Zuk",
      "Prasanna Balaprakash",
      "Cong Wang",
      "Anirban Mandal",
      "Ewa Deelman"
    ],
    "abstract": "Anomaly detection in computational workflows is critical for ensuring system\nreliability and security. However, traditional rule-based methods struggle to\ndetect novel anomalies. This paper leverages large language models (LLMs) for\nworkflow anomaly detection by exploiting their ability to learn complex data\npatterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),\nwhere pre-trained LLMs are fine-tuned on labeled data for sentence\nclassification to identify anomalies, and 2) in-context learning (ICL) where\nprompts containing task descriptions and examples guide LLMs in few-shot\nanomaly detection without fine-tuning. The paper evaluates the performance,\nefficiency, generalization of SFT models, and explores zero-shot and few-shot\nICL prompts and interpretability enhancement via chain-of-thought prompting.\nExperiments across multiple workflow datasets demonstrate the promising\npotential of LLMs for effective anomaly detection in complex executions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 14 figures, paper is accepted by SC'24, source code, see:\n  https://github.com/PoSeiDon-Workflows/LLM_AD",
    "pdf_url": "http://arxiv.org/pdf/2407.17545v1",
    "published_date": "2024-07-24 16:33:04 UTC",
    "updated_date": "2024-07-24 16:33:04 UTC"
  },
  {
    "arxiv_id": "2407.17396v2",
    "title": "Systematic Relational Reasoning With Epistemic Graph Neural Networks",
    "authors": [
      "Irtaza Khalid",
      "Steven Schockaert"
    ],
    "abstract": "Developing models that can learn to reason is a notoriously challenging\nproblem. We focus on reasoning in relational domains, where the use of Graph\nNeural Networks (GNNs) seems like a natural choice. However, previous work has\nshown that regular GNNs lack the ability to systematically generalize from\ntraining examples on test graphs requiring longer inference chains, which\nfundamentally limits their reasoning abilities. A common solution relies on\nneuro-symbolic methods that systematically reason by learning rules, but their\nscalability is often limited and they tend to make unrealistically strong\nassumptions, e.g.\\ that the answer can always be inferred from a single\nrelational path. We propose the Epistemic GNN (EpiGNN), a novel\nparameter-efficient and scalable GNN architecture with an epistemic inductive\nbias for systematic reasoning. Node embeddings in EpiGNNs are treated as\nepistemic states, and message passing is implemented accordingly. We show that\nEpiGNNs achieve state-of-the-art results on link prediction tasks that require\nsystematic reasoning. Furthermore, for inductive knowledge graph completion,\nEpiGNNs rival the performance of state-of-the-art specialized approaches.\nFinally, we introduce two new benchmarks that go beyond standard relational\nreasoning by requiring the aggregation of information from multiple paths.\nHere, existing neuro-symbolic approaches fail, yet EpiGNNs learn to reason\naccurately. Code and datasets are available at\nhttps://github.com/erg0dic/gnn-sg.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10+29 pages, 5+13 figures, 4+10 tables. Comments welcome!",
    "pdf_url": "http://arxiv.org/pdf/2407.17396v2",
    "published_date": "2024-07-24 16:17:15 UTC",
    "updated_date": "2025-02-27 22:50:41 UTC"
  },
  {
    "arxiv_id": "2408.01453v1",
    "title": "Reporting and Analysing the Environmental Impact of Language Models on the Example of Commonsense Question Answering with External Knowledge",
    "authors": [
      "Aida Usmanova",
      "Junbo Huang",
      "Debayan Banerjee",
      "Ricardo Usbeck"
    ],
    "abstract": "Human-produced emissions are growing at an alarming rate, causing already\nobservable changes in the climate and environment in general. Each year global\ncarbon dioxide emissions hit a new record, and it is reported that 0.5% of\ntotal US greenhouse gas emissions are attributed to data centres as of 2021.\nThe release of ChatGPT in late 2022 sparked social interest in Large Language\nModels (LLMs), the new generation of Language Models with a large number of\nparameters and trained on massive amounts of data. Currently, numerous\ncompanies are releasing products featuring various LLMs, with many more models\nin development and awaiting release. Deep Learning research is a competitive\nfield, with only models that reach top performance attracting attention and\nbeing utilized. Hence, achieving better accuracy and results is often the first\npriority, while the model's efficiency and the environmental impact of the\nstudy are neglected. However, LLMs demand substantial computational resources\nand are very costly to train, both financially and environmentally. It becomes\nessential to raise awareness and promote conscious decisions about algorithmic\nand hardware choices. Providing information on training time, the approximate\ncarbon dioxide emissions and power consumption would assist future studies in\nmaking necessary adjustments and determining the compatibility of available\ncomputational resources with model requirements. In this study, we infused T5\nLLM with external knowledge and fine-tuned the model for Question-Answering\ntask. Furthermore, we calculated and reported the approximate environmental\nimpact for both steps. The findings demonstrate that the smaller models may not\nalways be sustainable options, and increased training does not always imply\nbetter performance. The most optimal outcome is achieved by carefully\nconsidering both performance and efficiency factors.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at Bonn Sustainable AI 2023 conference",
    "pdf_url": "http://arxiv.org/pdf/2408.01453v1",
    "published_date": "2024-07-24 16:16:16 UTC",
    "updated_date": "2024-07-24 16:16:16 UTC"
  },
  {
    "arxiv_id": "2407.17383v1",
    "title": "A Comprehensive Approach to Misspelling Correction with BERT and Levenshtein Distance",
    "authors": [
      "Amirreza Naziri",
      "Hossein Zeinali"
    ],
    "abstract": "Writing, as an omnipresent form of human communication, permeates nearly\nevery aspect of contemporary life. Consequently, inaccuracies or errors in\nwritten communication can lead to profound consequences, ranging from financial\nlosses to potentially life-threatening situations. Spelling mistakes, among the\nmost prevalent writing errors, are frequently encountered due to various\nfactors. This research aims to identify and rectify diverse spelling errors in\ntext using neural networks, specifically leveraging the Bidirectional Encoder\nRepresentations from Transformers (BERT) masked language model. To achieve this\ngoal, we compiled a comprehensive dataset encompassing both non-real-word and\nreal-word errors after categorizing different types of spelling mistakes.\nSubsequently, multiple pre-trained BERT models were employed. To ensure optimal\nperformance in correcting misspelling errors, we propose a combined approach\nutilizing the BERT masked language model and Levenshtein distance. The results\nfrom our evaluation data demonstrate that the system presented herein exhibits\nremarkable capabilities in identifying and rectifying spelling mistakes, often\nsurpassing existing systems tailored for the Persian language.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 9 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.17383v1",
    "published_date": "2024-07-24 16:07:11 UTC",
    "updated_date": "2024-07-24 16:07:11 UTC"
  },
  {
    "arxiv_id": "2408.01452v1",
    "title": "Building a Domain-specific Guardrail Model in Production",
    "authors": [
      "Mohammad Niknazar",
      "Paul V Haley",
      "Latha Ramanan",
      "Sang T. Truong",
      "Yedendra Shrinivasan",
      "Ayan Kumar Bhowmick",
      "Prasenjit Dey",
      "Ashish Jagmohan",
      "Hema Maheshwari",
      "Shom Ponoth",
      "Robert Smith",
      "Aditya Vempaty",
      "Nick Haber",
      "Sanmi Koyejo",
      "Sharad Sundararajan"
    ],
    "abstract": "Generative AI holds the promise of enabling a range of sought-after\ncapabilities and revolutionizing workflows in various consumer and enterprise\nverticals. However, putting a model in production involves much more than just\ngenerating an output. It involves ensuring the model is reliable, safe,\nperformant and also adheres to the policy of operation in a particular domain.\nGuardrails as a necessity for models has evolved around the need to enforce\nappropriate behavior of models, especially when they are in production. In this\npaper, we use education as a use case, given its stringent requirements of the\nappropriateness of content in the domain, to demonstrate how a guardrail model\ncan be trained and deployed in production. Specifically, we describe our\nexperience in building a production-grade guardrail model for a K-12\neducational platform. We begin by formulating the requirements for deployment\nto this sensitive domain. We then describe the training and benchmarking of our\ndomain-specific guardrail model, which outperforms competing open- and closed-\ninstruction-tuned models of similar and larger size, on proprietary\neducation-related benchmarks and public benchmarks related to general aspects\nof safety. Finally, we detail the choices we made on architecture and the\noptimizations for deploying this service in production; these range across the\nstack from the hardware infrastructure to the serving layer to language model\ninference optimizations. We hope this paper will be instructive to other\npractitioners looking to create production-grade domain-specific services based\non generative AI and large language models.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01452v1",
    "published_date": "2024-07-24 15:53:29 UTC",
    "updated_date": "2024-07-24 15:53:29 UTC"
  },
  {
    "arxiv_id": "2407.17374v2",
    "title": "Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts",
    "authors": [
      "Edyta Bogucka",
      "Marios Constantinides",
      "Sanja ≈†ƒáepanoviƒá",
      "Daniele Quercia"
    ],
    "abstract": "In the evolving landscape of AI regulation, it is crucial for companies to\nconduct impact assessments and document their compliance through comprehensive\nreports. However, current reports lack grounding in regulations and often focus\non specific aspects like privacy in relation to AI systems, without addressing\nthe real-world uses of these systems. Moreover, there is no systematic effort\nto design and evaluate these reports with both AI practitioners and AI\ncompliance experts. To address this gap, we conducted an iterative co-design\nprocess with 14 AI practitioners and 6 AI compliance experts and proposed a\ntemplate for impact assessment reports grounded in the EU AI Act, NIST's AI\nRisk Management Framework, and ISO 42001 AI Management System. We evaluated the\ntemplate by producing an impact assessment report for an AI-based meeting\ncompanion at a major tech company. A user study with 8 AI practitioners from\nthe same company and 5 AI compliance experts from industry and academia\nrevealed that our template effectively provides necessary information for\nimpact assessments and documents the broad impacts of AI systems. Participants\nenvisioned using the template not only at the pre-deployment stage for\ncompliance but also as a tool to guide the design stage of AI uses.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "K.4.1, K.4.2, H.5.3, D.2.9",
      "K.4.1; K.4.2; H.5.3; D.2.9"
    ],
    "primary_category": "cs.HC",
    "comment": "16 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.17374v2",
    "published_date": "2024-07-24 15:53:04 UTC",
    "updated_date": "2024-08-01 22:11:48 UTC"
  },
  {
    "arxiv_id": "2407.17544v1",
    "title": "MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents",
    "authors": [
      "Arya Bulusu",
      "Brandon Man",
      "Ashish Jagmohan",
      "Aditya Vempaty",
      "Jennifer Mari-Wyka",
      "Deepak Akkil"
    ],
    "abstract": "There has been significant recent interest in harnessing LLMs to control\nsoftware systems through multi-step reasoning, planning and tool-usage. While\nsome promising results have been obtained, application to specific domains\nraises several general issues including the control of specialized domain\ntools, the lack of existing datasets for training and evaluation, and the\nnon-triviality of automated system evaluation and improvement. In this paper,\nwe present a case-study where we examine these issues in the context of a\nspecific domain. Specifically, we present an automated math visualizer and\nsolver system for mathematical pedagogy. The system orchestrates mathematical\nsolvers and math graphing tools to produce accurate visualizations from simple\nnatural language commands. We describe the creation of specialized data-sets,\nand also develop an auto-evaluator to easily evaluate the outputs of our system\nby comparing them to ground-truth expressions. We have open sourced the\ndata-sets and code for the proposed system.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17544v1",
    "published_date": "2024-07-24 15:45:07 UTC",
    "updated_date": "2024-07-24 15:45:07 UTC"
  },
  {
    "arxiv_id": "2407.17361v1",
    "title": "MuST: Multi-Scale Transformers for Surgical Phase Recognition",
    "authors": [
      "Alejandra P√©rez",
      "Santiago Rodr√≠guez",
      "Nicol√°s Ayobi",
      "Nicol√°s Aparicio",
      "Eug√©nie Dessevres",
      "Pablo Arbel√°ez"
    ],
    "abstract": "Phase recognition in surgical videos is crucial for enhancing computer-aided\nsurgical systems as it enables automated understanding of sequential procedural\nstages. Existing methods often rely on fixed temporal windows for video\nanalysis to identify dynamic surgical phases. Thus, they struggle to\nsimultaneously capture short-, mid-, and long-term information necessary to\nfully understand complex surgical procedures. To address these issues, we\npropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel\nTransformer-based approach that combines a Multi-Term Frame encoder with a\nTemporal Consistency Module to capture information across multiple temporal\nscales of a surgical video. Our Multi-Term Frame Encoder computes\ninterdependencies across a hierarchy of temporal scales by sampling sequences\nat increasing strides around the frame of interest. Furthermore, we employ a\nlong-term Transformer encoder over the frame embeddings to further enhance\nlong-term reasoning. MuST achieves higher performance than previous\nstate-of-the-art methods on three different public benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17361v1",
    "published_date": "2024-07-24 15:38:20 UTC",
    "updated_date": "2024-07-24 15:38:20 UTC"
  },
  {
    "arxiv_id": "2407.17543v2",
    "title": "Dataset Distribution Impacts Model Fairness: Single vs. Multi-Task Learning",
    "authors": [
      "Ralf Raumanns",
      "Gerard Schouten",
      "Josien P. W. Pluim",
      "Veronika Cheplygina"
    ],
    "abstract": "The influence of bias in datasets on the fairness of model predictions is a\ntopic of ongoing research in various fields. We evaluate the performance of\nskin lesion classification using ResNet-based CNNs, focusing on patient sex\nvariations in training data and three different learning strategies. We present\na linear programming method for generating datasets with varying patient sex\nand class labels, taking into account the correlations between these variables.\nWe evaluated the model performance using three different learning strategies: a\nsingle-task model, a reinforcing multi-task model, and an adversarial learning\nscheme. Our observations include: 1) sex-specific training data yields better\nresults, 2) single-task models exhibit sex bias, 3) the reinforcement approach\ndoes not remove sex bias, 4) the adversarial model eliminates sex bias in cases\ninvolving only female patients, and 5) datasets that include male patients\nenhance model performance for the male subgroup, even when female patients are\nthe majority. To generalise these findings, in future research, we will examine\nmore demographic attributes, like age, and other possibly confounding factors,\nsuch as skin colour and artefacts in the skin lesions. We make all data and\nmodels available on GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the FAIMI EPIMI 2024 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.17543v2",
    "published_date": "2024-07-24 15:23:26 UTC",
    "updated_date": "2024-12-09 09:28:34 UTC"
  },
  {
    "arxiv_id": "2407.17339v1",
    "title": "Preliminary study on artificial intelligence methods for cybersecurity threat detection in computer networks based on raw data packets",
    "authors": [
      "Aleksander Ogonowski",
      "Micha≈Ç ≈ªebrowski",
      "Arkadiusz ƒÜwiek",
      "Tobiasz Jarosiewicz",
      "Konrad Klimaszewski",
      "Adam Padee",
      "Piotr Wasiuk",
      "Micha≈Ç W√≥jcik"
    ],
    "abstract": "Most of the intrusion detection methods in computer networks are based on\ntraffic flow characteristics. However, this approach may not fully exploit the\npotential of deep learning algorithms to directly extract features and patterns\nfrom raw packets. Moreover, it impedes real-time monitoring due to the\nnecessity of waiting for the processing pipeline to complete and introduces\ndependencies on additional software components.\n  In this paper, we investigate deep learning methodologies capable of\ndetecting attacks in real-time directly from raw packet data within network\ntraffic. We propose a novel approach where packets are stacked into windows and\nseparately recognised, with a 2D image representation suitable for processing\nwith computer vision models. Our investigation utilizes the CIC IDS-2017\ndataset, which includes both benign traffic and prevalent real-world attacks,\nproviding a comprehensive foundation for our research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "I.5.4; C.2.0; I.2.1"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Computer Science Journal",
    "pdf_url": "http://arxiv.org/pdf/2407.17339v1",
    "published_date": "2024-07-24 15:04:00 UTC",
    "updated_date": "2024-07-24 15:04:00 UTC"
  },
  {
    "arxiv_id": "2407.17324v2",
    "title": "Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population",
    "authors": [
      "Nikolaos Ntampakis",
      "Konstantinos Diamantaras",
      "Ioanna Chouvarda",
      "Vasileios Argyriou",
      "Panagiotis Sarigianndis"
    ],
    "abstract": "Dementia, a debilitating neurological condition affecting millions worldwide,\npresents significant diagnostic challenges. In this work, we introduce a novel\nmethodology for the classification of demented and non-demented elderly\npatients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach\nfeatures a unique technique for selectively processing MRI slices, focusing on\nthe most relevant brain regions and excluding less informative sections. This\nmethodology is complemented by a confidence-based classification committee\ncomposed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and\nDem3D EfficientNet. These models work synergistically to enhance\ndecision-making accuracy, leveraging their collective strengths. Tested on the\nOpen Access Series of Imaging Studies(OASIS) dataset, our method achieved an\nimpressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,\nvalidation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\nconfirmed the robustness and generalizability of our approach. The use of\nexplainable AI (XAI) techniques and comprehensive ablation studies further\nsubstantiate the effectiveness of our techniques, providing insights into the\ndecision-making process and the importance of our methodology. This research\noffers a significant advancement in dementia diagnosis, providing a highly\naccurate and efficient tool for clinical applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17324v2",
    "published_date": "2024-07-24 14:48:40 UTC",
    "updated_date": "2024-07-25 09:50:03 UTC"
  },
  {
    "arxiv_id": "2407.17291v1",
    "title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?",
    "authors": [
      "Leo Yu-Ho Lo",
      "Huamin Qu"
    ],
    "abstract": "In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "To be presented at IEEE VIS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17291v1",
    "published_date": "2024-07-24 14:02:20 UTC",
    "updated_date": "2024-07-24 14:02:20 UTC"
  },
  {
    "arxiv_id": "2407.17274v1",
    "title": "Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation",
    "authors": [
      "Yongqi Li",
      "Hongru Cai",
      "Wenjie Wang",
      "Leigang Qu",
      "Yinwei Wei",
      "Wenjie Li",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "abstract": "Text-to-image retrieval is a fundamental task in multimedia processing,\naiming to retrieve semantically relevant cross-modal content. Traditional\nstudies have typically approached this task as a discriminative problem,\nmatching the text and image via the cross-attention mechanism (one-tower\nframework) or in a common embedding space (two-tower framework). Recently,\ngenerative cross-modal retrieval has emerged as a new research line, which\nassigns images with unique string identifiers and generates the target\nidentifier as the retrieval target. Despite its great potential, existing\ngenerative approaches are limited due to the following issues: insufficient\nvisual information in identifiers, misalignment with high-level semantics, and\nlearning gap towards the retrieval target. To address the above issues, we\npropose an autoregressive voken generation method, named AVG. AVG tokenizes\nimages into vokens, i.e., visual tokens, and innovatively formulates the\ntext-to-image retrieval task as a token-to-voken generation problem. AVG\ndiscretizes an image into a sequence of vokens as the identifier of the image,\nwhile maintaining the alignment with both the visual information and high-level\nsemantics of the image. Additionally, to bridge the learning gap between\ngenerative training and the retrieval target, we incorporate discriminative\ntraining to modify the learning direction during token-to-voken training.\nExtensive experiments demonstrate that AVG achieves superior results in both\neffectiveness and efficiency.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2407.17274v1",
    "published_date": "2024-07-24 13:39:51 UTC",
    "updated_date": "2024-07-24 13:39:51 UTC"
  },
  {
    "arxiv_id": "2407.17265v1",
    "title": "SCIsegV2: A Universal Tool for Segmentation of Intramedullary Lesions in Spinal Cord Injury",
    "authors": [
      "Enamundram Naga Karthik",
      "Jan Valo≈°ek",
      "Lynn Farner",
      "Dario Pfyffer",
      "Simon Schading-Sassenhausen",
      "Anna Lebret",
      "Gergely David",
      "Andrew C. Smith",
      "Kenneth A. Weber II",
      "Maryam Seif",
      "RHSCIR Network Imaging Group",
      "Patrick Freund",
      "Julien Cohen-Adad"
    ],
    "abstract": "Spinal cord injury (SCI) is a devastating incidence leading to permanent\nparalysis and loss of sensory-motor functions potentially resulting in the\nformation of lesions within the spinal cord. Imaging biomarkers obtained from\nmagnetic resonance imaging (MRI) scans can predict the functional recovery of\nindividuals with SCI and help choose the optimal treatment strategy. Currently,\nmost studies employ manual quantification of these MRI-derived biomarkers,\nwhich is a subjective and tedious task. In this work, we propose (i) a\nuniversal tool for the automatic segmentation of intramedullary SCI lesions,\ndubbed \\texttt{SCIsegV2}, and (ii) a method to automatically compute the width\nof the tissue bridges from the segmented lesion. Tissue bridges represent the\nspared spinal tissue adjacent to the lesion, which is associated with\nfunctional recovery in SCI patients. The tool was trained and validated on a\nheterogeneous dataset from 7 sites comprising patients from different SCI\nphases (acute, sub-acute, and chronic) and etiologies (traumatic SCI, ischemic\nSCI, and degenerative cervical myelopathy). Tissue bridges quantified\nautomatically did not significantly differ from those computed manually,\nsuggesting that the proposed automatic tool can be used to derive relevant MRI\nbiomarkers. \\texttt{SCIsegV2} and the automatic tissue bridges computation are\nopen-source and available in Spinal Cord Toolbox (v6.4 and above) via the\n\\texttt{sct\\_deepseg -task seg\\_sc\\_lesion\\_t2w\\_sci} and\n\\texttt{sct\\_analyze\\_lesion} functions, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MICCAI AMAI 2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.17265v1",
    "published_date": "2024-07-24 13:29:17 UTC",
    "updated_date": "2024-07-24 13:29:17 UTC"
  },
  {
    "arxiv_id": "2407.17230v1",
    "title": "Improving ICD coding using Chapter based Named Entities and Attentional Models",
    "authors": [
      "Abhijith R. Beeravolu",
      "Mirjam Jonkman",
      "Sami Azam",
      "Friso De Boer"
    ],
    "abstract": "Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 Pages",
    "pdf_url": "http://arxiv.org/pdf/2407.17230v1",
    "published_date": "2024-07-24 12:34:23 UTC",
    "updated_date": "2024-07-24 12:34:23 UTC"
  },
  {
    "arxiv_id": "2407.17227v1",
    "title": "LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover",
    "authors": [
      "Zijian Wu",
      "Jiayu Wang",
      "Dahua Lin",
      "Kai Chen"
    ],
    "abstract": "Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17227v1",
    "published_date": "2024-07-24 12:28:03 UTC",
    "updated_date": "2024-07-24 12:28:03 UTC"
  },
  {
    "arxiv_id": "2407.17226v4",
    "title": "Sublinear Regret for a Class of Continuous-Time Linear-Quadratic Reinforcement Learning Problems",
    "authors": [
      "Yilie Huang",
      "Yanwei Jia",
      "Xun Yu Zhou"
    ],
    "abstract": "We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions, where states are\nscalar-valued and running control rewards are absent but volatilities of the\nstate processes depend on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an RL algorithm to learn the optimal policy\nparameter directly. Our main contributions include the introduction of an\nexploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor, where $N$ is the number of learning episodes. We conduct\na simulation study to validate the theoretical results and demonstrate the\neffectiveness and reliability of the proposed algorithm. We also perform\nnumerical comparisons between our method and those of the recent model-based\nstochastic LQ RL studies adapted to the state- and control-dependent volatility\nsetting, demonstrating a better performance of the former in terms of regret\nbounds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "42 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.17226v4",
    "published_date": "2024-07-24 12:26:21 UTC",
    "updated_date": "2025-04-08 19:11:31 UTC"
  },
  {
    "arxiv_id": "2408.06352v1",
    "title": "Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition",
    "authors": [
      "Michele Fiori",
      "Gabriele Civitarese",
      "Claudio Bettini"
    ],
    "abstract": "Recognizing daily activities with unobtrusive sensors in smart environments\nenables various healthcare applications. Monitoring how subjects perform\nactivities at home and their changes over time can reveal early symptoms of\nhealth issues, such as cognitive decline. Most approaches in this field use\ndeep learning models, which are often seen as black boxes mapping sensor data\nto activities. However, non-expert users like clinicians need to trust and\nunderstand these models' outputs. Thus, eXplainable AI (XAI) methods for Human\nActivity Recognition have emerged to provide intuitive natural language\nexplanations from these models. Different XAI methods generate different\nexplanations, and their effectiveness is typically evaluated through user\nsurveys, that are often challenging in terms of costs and fairness. This paper\nproposes an automatic evaluation method using Large Language Models (LLMs) to\nidentify, in a pool of candidates, the best XAI approach for non-expert users.\nOur preliminary results suggest that LLM evaluation aligns with user surveys.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for publication at UbiComp / ISWC 2024's XAIforU workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.06352v1",
    "published_date": "2024-07-24 12:15:07 UTC",
    "updated_date": "2024-07-24 12:15:07 UTC"
  },
  {
    "arxiv_id": "2407.17211v1",
    "title": "Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles",
    "authors": [
      "Zuoyin Tang",
      "Jianhua He",
      "Dashuai Pei",
      "Kezhong Liu",
      "Tao Gao"
    ],
    "abstract": "Handling long tail corner cases is a major challenge faced by autonomous\nvehicles (AVs). While large language models (LLMs) hold great potentials to\nhandle the corner cases with excellent generalization and explanation\ncapabilities and received increasing research interest on application to\nautonomous driving, there are still technical barriers to be tackled, such as\nstrict model performance and huge computing resource requirements of LLMs. In\nthis paper, we investigate a new approach of applying remote or edge LLMs to\nsupport autonomous driving. A key issue for such LLM assisted driving system is\nthe assessment of LLMs on their understanding of driving theory and skills,\nensuring they are qualified to undertake safety critical driving assistance\ntasks for CAVs. We design and run driving theory tests for several proprietary\nLLM models (OpenAI GPT models, Baidu Ernie and Ali QWen) and open-source LLM\nmodels (Tsinghua MiniCPM-2B and MiniCPM-Llama3-V2.5) with more than 500\nmultiple-choices theory test questions. Model accuracy, cost and processing\nlatency are measured from the experiments. Experiment results show that while\nmodel GPT-4 passes the test with improved domain knowledge and Ernie has an\naccuracy of 85% (just below the 86% passing threshold), other LLM models\nincluding GPT-3.5 fail the test. For the test questions with images, the\nmultimodal model GPT4-o has an excellent accuracy result of 96%, and the\nMiniCPM-Llama3-V2.5 achieves an accuracy of 76%. While GPT-4 holds stronger\npotential for CAV driving assistance applications, the cost of using model GPT4\nis much higher, almost 50 times of that of using GPT3.5. The results can help\nmake decision on the use of the existing LLMs for CAV applications and\nbalancing on the model performance and cost.",
    "categories": [
      "cs.AI",
      "cs.NI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17211v1",
    "published_date": "2024-07-24 12:10:20 UTC",
    "updated_date": "2024-07-24 12:10:20 UTC"
  },
  {
    "arxiv_id": "2407.17209v1",
    "title": "Nonverbal Immediacy Analysis in Education: A Multimodal Computational Model",
    "authors": [
      "Uro≈° Petkoviƒá",
      "Jonas Frenkel",
      "Olaf Hellwich",
      "Rebecca Lazarides"
    ],
    "abstract": "This paper introduces a novel computational approach for analyzing nonverbal\nsocial behavior in educational settings. Integrating multimodal behavioral\ncues, including facial expressions, gesture intensity, and spatial dynamics,\nthe model assesses the nonverbal immediacy (NVI) of teachers from RGB classroom\nvideos. A dataset of 400 30-second video segments from German classrooms was\nconstructed for model training and validation. The gesture intensity regressor\nachieved a correlation of 0.84, the perceived distance regressor 0.55, and the\nNVI model 0.44 with median human ratings. The model demonstrates the potential\nto provide a valuable support in nonverbal behavior assessment, approximating\nthe accuracy of individual human raters. Validated against both questionnaire\ndata and trained observer ratings, our models show moderate to strong\ncorrelations with relevant educational outcomes, indicating their efficacy in\nreflecting effective teaching behaviors. This research advances the objective\nassessment of nonverbal communication behaviors, opening new pathways for\neducational research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "68T45, 68T10, 68U10, 91E45",
      "I.2.10; I.5.4; K.3.1"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 3 figures. Camera-ready version for the SAB 2024: 17th\n  International Conference on the Simulation of Adaptive Behavior",
    "pdf_url": "http://arxiv.org/pdf/2407.17209v1",
    "published_date": "2024-07-24 12:09:07 UTC",
    "updated_date": "2024-07-24 12:09:07 UTC"
  },
  {
    "arxiv_id": "2407.21054v3",
    "title": "Sentiment Reasoning for Healthcare",
    "authors": [
      "Khai-Nguyen Nguyen",
      "Khai Le-Duc",
      "Bach Phan Tat",
      "Duy Le",
      "Long Vo-Dang",
      "Truong-Son Hy"
    ],
    "abstract": "Transparency in AI healthcare decision-making is crucial for building trust\namong AI and users. Incorporating reasoning capabilities enables Large Language\nModels (LLMs) to understand emotions in context, handle nuanced language, and\ninfer unstated sentiments. In this work, we introduce a new task -- Sentiment\nReasoning -- for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Sentiment Reasoning is an auxiliary\ntask in sentiment analysis where the model predicts both the sentiment label\nand generates the rationale behind it based on the input transcript. Our study\nconducted on both human transcripts and Automatic Speech Recognition (ASR)\ntranscripts shows that Sentiment Reasoning helps improve model transparency by\nproviding rationale for model prediction with quality semantically comparable\nto humans while also improving model performance (1% increase in both accuracy\nand macro-F1) via rationale-augmented fine-tuning. Also, no significant\ndifference in the semantic quality of generated rationales between human and\nASR transcripts. All code, data (English-translated and Vietnamese) and models\nare published online: https://github.com/leduckhai/MultiMed.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS AIM-FM Workshop, 20 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.21054v3",
    "published_date": "2024-07-24 12:07:54 UTC",
    "updated_date": "2024-10-11 05:43:19 UTC"
  },
  {
    "arxiv_id": "2407.17206v1",
    "title": "Take a Step and Reconsider: Sequence Decoding for Self-Improved Neural Combinatorial Optimization",
    "authors": [
      "Jonathan Pirnay",
      "Dominik G. Grimm"
    ],
    "abstract": "The constructive approach within Neural Combinatorial Optimization (NCO)\ntreats a combinatorial optimization problem as a finite Markov decision\nprocess, where solutions are built incrementally through a sequence of\ndecisions guided by a neural policy network. To train the policy, recent\nresearch is shifting toward a 'self-improved' learning methodology that\naddresses the limitations of reinforcement learning and supervised approaches.\nHere, the policy is iteratively trained in a supervised manner, with solutions\nderived from the current policy serving as pseudo-labels. The way these\nsolutions are obtained from the policy determines the quality of the\npseudo-labels. In this paper, we present a simple and problem-independent\nsequence decoding method for self-improved learning based on sampling sequences\nwithout replacement. We incrementally follow the best solution found and repeat\nthe sampling process from intermediate partial solutions. By modifying the\npolicy to ignore previously sampled sequences, we force it to consider only\nunseen alternatives, thereby increasing solution diversity. Experimental\nresults for the Traveling Salesman and Capacitated Vehicle Routing Problem\ndemonstrate its strong performance. Furthermore, our method outperforms\nprevious NCO approaches on the Job Shop Scheduling Problem.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ECAI-2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17206v1",
    "published_date": "2024-07-24 12:06:09 UTC",
    "updated_date": "2024-07-24 12:06:09 UTC"
  },
  {
    "arxiv_id": "2407.17197v3",
    "title": "ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only",
    "authors": [
      "Saad Lahlali",
      "Nicolas Granger",
      "Herv√© Le Borgne",
      "Quoc-Cuong Pham"
    ],
    "abstract": "3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations. The code is\navailable at https://github.com/CEA-LIST/ALPI",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted at WACV2025",
    "pdf_url": "http://arxiv.org/pdf/2407.17197v3",
    "published_date": "2024-07-24 11:58:31 UTC",
    "updated_date": "2024-11-27 07:22:47 UTC"
  },
  {
    "arxiv_id": "2407.17164v2",
    "title": "Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence",
    "authors": [
      "Xiaoyu Tan",
      "Bin Li",
      "Xihe Qiu",
      "Jingjing Huang",
      "Yinghui Xu",
      "Wei Chu"
    ],
    "abstract": "Integrating deep neural networks with the Hawkes process has significantly\nimproved predictive capabilities in finance, health informatics, and\ninformation technology. Nevertheless, these models often face challenges in\nreal-world settings, particularly due to substantial label noise. This issue is\nof significant concern in the medical field, where label noise can arise from\ndelayed updates in electronic medical records or misdiagnoses, leading to\nincreased prediction risks. Our research indicates that deep Hawkes process\nmodels exhibit reduced robustness when dealing with label noise, particularly\nwhen it affects both event types and timing. To address these challenges, we\nfirst investigate the influence of label noise in approximated intensity\nfunctions and present a novel framework, the Robust Deep Hawkes Process (RDHP),\nto overcome the impact of label noise on the intensity function of Hawkes\nmodels, considering both the events and their occurrences. We tested RDHP using\nmultiple open-source benchmarks with synthetic noise and conducted a case study\non obstructive sleep apnea-hypopnea syndrome (OSAHS) in a real-world setting\nwith inherent label noise. The results demonstrate that RDHP can effectively\nperform classification and regression tasks, even in the presence of noise\nrelated to events and their timing. To the best of our knowledge, this is the\nfirst study to successfully address both event and time label noise in deep\nHawkes process models, offering a promising solution for medical applications,\nspecifically in diagnosing OSAHS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ECAI2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17164v2",
    "published_date": "2024-07-24 11:12:01 UTC",
    "updated_date": "2024-07-29 06:55:36 UTC"
  },
  {
    "arxiv_id": "2407.17160v1",
    "title": "A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives",
    "authors": [
      "Jan Leheƒçka",
      "Josef V. Psutka",
      "Lubo≈° ≈†m√≠dl",
      "Pavel Ircing",
      "Josef Psutka"
    ],
    "abstract": "In this paper, we are comparing monolingual Wav2Vec 2.0 models with various\nmultilingual models to see whether we could improve speech recognition\nperformance on a unique oral history archive containing a lot of mixed-language\nsentences. Our main goal is to push forward research on this unique dataset,\nwhich is an extremely valuable part of our cultural heritage. Our results\nsuggest that monolingual speech recognition models are, in most cases, superior\nto multilingual models, even when processing the oral history archive full of\nmixed-language sentences from non-native speakers. We also performed the same\nexperiments on the public CommonVoice dataset to verify our results. We are\ncontributing to the research community by releasing our pre-trained models to\nthe public.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to INTERSPEECH2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17160v1",
    "published_date": "2024-07-24 11:03:47 UTC",
    "updated_date": "2024-07-24 11:03:47 UTC"
  },
  {
    "arxiv_id": "2407.17152v3",
    "title": "XMeCap: Meme Caption Generation with Sub-Image Adaptability",
    "authors": [
      "Yuyan Chen",
      "Songzhou Yan",
      "Zhihong Zhu",
      "Zhixu Li",
      "Yanghua Xiao"
    ],
    "abstract": "Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17152v3",
    "published_date": "2024-07-24 10:51:46 UTC",
    "updated_date": "2024-09-20 09:39:07 UTC"
  },
  {
    "arxiv_id": "2407.17126v1",
    "title": "SDoH-GPT: Using Large Language Models to Extract Social Determinants of Health (SDoH)",
    "authors": [
      "Bernardo Consoli",
      "Xizhi Wu",
      "Song Wang",
      "Xinyu Zhao",
      "Yanshan Wang",
      "Justin Rousseau",
      "Tom Hartvigsen",
      "Li Shen",
      "Huanmei Wu",
      "Yifan Peng",
      "Qi Long",
      "Tianlong Chen",
      "Ying Ding"
    ],
    "abstract": "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17126v1",
    "published_date": "2024-07-24 09:57:51 UTC",
    "updated_date": "2024-07-24 09:57:51 UTC"
  },
  {
    "arxiv_id": "2407.17120v1",
    "title": "Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent Kernel Perspective",
    "authors": [
      "Jingren Liu",
      "Zhong Ji",
      "YunLong Yu",
      "Jiale Cao",
      "Yanwei Pang",
      "Jungong Han",
      "Xuelong Li"
    ],
    "abstract": "Parameter-efficient fine-tuning for continual learning (PEFT-CL) has shown\npromise in adapting pre-trained models to sequential tasks while mitigating\ncatastrophic forgetting problem. However, understanding the mechanisms that\ndictate continual performance in this paradigm remains elusive. To tackle this\ncomplexity, we undertake a rigorous analysis of PEFT-CL dynamics to derive\nrelevant metrics for continual scenarios using Neural Tangent Kernel (NTK)\ntheory. With the aid of NTK as a mathematical analysis tool, we recast the\nchallenge of test-time forgetting into the quantifiable generalization gaps\nduring training, identifying three key factors that influence these gaps and\nthe performance of PEFT-CL: training sample size, task-level feature\northogonality, and regularization. To address these challenges, we introduce\nNTK-CL, a novel framework that eliminates task-specific parameter storage while\nadaptively generating task-relevant features. Aligning with theoretical\nguidance, NTK-CL triples the feature representation of each sample,\ntheoretically and empirically reducing the magnitude of both task-interplay and\ntask-specific generalization gaps. Grounded in NTK analysis, our approach\nimposes an adaptive exponential moving average mechanism and constraints on\ntask-level feature orthogonality, maintaining intra-task NTK forms while\nattenuating inter-task NTK forms. Ultimately, by fine-tuning optimizable\nparameters with appropriate regularization, NTK-CL achieves state-of-the-art\nperformance on established PEFT-CL benchmarks. This work provides a theoretical\nfoundation for understanding and improving PEFT-CL models, offering insights\ninto the interplay between feature representation, task orthogonality, and\ngeneralization, contributing to the development of more efficient continual\nlearning systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17120v1",
    "published_date": "2024-07-24 09:30:04 UTC",
    "updated_date": "2024-07-24 09:30:04 UTC"
  },
  {
    "arxiv_id": "2407.17117v1",
    "title": "EverAdapt: Continuous Adaptation for Dynamic Machine Fault Diagnosis Environments",
    "authors": [
      "Edward",
      "Mohamed Ragab",
      "Yuecong Xu",
      "Min Wu",
      "Yuecong Xu",
      "Zhenghua Chen",
      "Abdulla Alseiari",
      "Xiaoli Li"
    ],
    "abstract": "Unsupervised Domain Adaptation (UDA) has emerged as a key solution in\ndata-driven fault diagnosis, addressing domain shift where models underperform\nin changing environments. However, under the realm of continually changing\nenvironments, UDA tends to underperform on previously seen domains when\nadapting to new ones - a problem known as catastrophic forgetting. To address\nthis limitation, we introduce the EverAdapt framework, specifically designed\nfor continuous model adaptation in dynamic environments. Central to EverAdapt\nis a novel Continual Batch Normalization (CBN), which leverages source domain\nstatistics as a reference point to standardize feature representations across\ndomains. EverAdapt not only retains statistical information from previous\ndomains but also adapts effectively to new scenarios. Complementing CBN, we\ndesign a class-conditional domain alignment module for effective integration of\ntarget domains, and a Sample-efficient Replay strategy to reinforce memory\nretention. Experiments on real-world datasets demonstrate EverAdapt superiority\nin maintaining robust fault diagnosis in dynamic environments. Our code is\navailable: https://github.com/mohamedr002/EverAdapt",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17117v1",
    "published_date": "2024-07-24 09:25:54 UTC",
    "updated_date": "2024-07-24 09:25:54 UTC"
  },
  {
    "arxiv_id": "2407.17112v2",
    "title": "Neural Dueling Bandits: Preference-Based Optimization with Human Feedback",
    "authors": [
      "Arun Verma",
      "Zhongxiang Dai",
      "Xiaoqiang Lin",
      "Patrick Jaillet",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\nhuman preference feedback over the selected arms for the past contexts.\nHowever, existing algorithms assume the reward function is linear, which can be\ncomplex and non-linear in many real-life applications like online\nrecommendations or ranking web search results. To overcome this challenge, we\nuse a neural network to estimate the reward function using preference feedback\nfor the previously selected arms. We propose upper confidence bound- and\nThompson sampling-based algorithms with sub-linear regret guarantees that\nefficiently select arms in each round. We also extend our theoretical results\nto contextual bandit problems with binary feedback, which is in itself a\nnon-trivial contribution. Experimental results on the problem instances derived\nfrom synthetic datasets corroborate our theoretical results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025. Also, accepted at ICML 2024 Workshop on\n  Foundations of Reinforcement Learning and Control",
    "pdf_url": "http://arxiv.org/pdf/2407.17112v2",
    "published_date": "2024-07-24 09:23:22 UTC",
    "updated_date": "2025-04-16 11:44:53 UTC"
  },
  {
    "arxiv_id": "2407.17101v1",
    "title": "PiPa++: Towards Unification of Domain Adaptive Semantic Segmentation via Self-supervised Learning",
    "authors": [
      "Mu Chen",
      "Zhedong Zheng",
      "Yi Yang"
    ],
    "abstract": "Unsupervised domain adaptive segmentation aims to improve the segmentation\naccuracy of models on target domains without relying on labeled data from those\ndomains. This approach is crucial when labeled target domain data is scarce or\nunavailable. It seeks to align the feature representations of the source domain\n(where labeled data is available) and the target domain (where only unlabeled\ndata is present), thus enabling the model to generalize well to the target\ndomain. Current image- and video-level domain adaptation have been addressed\nusing different and specialized frameworks, training strategies and\noptimizations despite their underlying connections. In this paper, we propose a\nunified framework PiPa++, which leverages the core idea of ``comparing'' to (1)\nexplicitly encourage learning of discriminative pixel-wise features with\nintraclass compactness and inter-class separability, (2) promote the robust\nfeature learning of the identical patch against different contexts or\nfluctuations, and (3) enable the learning of temporal continuity under dynamic\nenvironments. With the designed task-smart contrastive sampling strategy,\nPiPa++ enables the mining of more informative training samples according to the\ntask demand. Extensive experiments demonstrate the effectiveness of our method\non both image-level and video-level domain adaption benchmarks. Moreover, the\nproposed method is compatible with other UDA approaches to further improve the\nperformance without introducing extra parameters.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This study is under IEEE TMM review. arXiv admin note: substantial\n  text overlap with arXiv:2211.07609",
    "pdf_url": "http://arxiv.org/pdf/2407.17101v1",
    "published_date": "2024-07-24 08:53:29 UTC",
    "updated_date": "2024-07-24 08:53:29 UTC"
  },
  {
    "arxiv_id": "2407.17097v1",
    "title": "Towards Robust Knowledge Tracing Models via k-Sparse Attention",
    "authors": [
      "Shuyan Huang",
      "Zitao Liu",
      "Xiangyu Zhao",
      "Weiqi Luo",
      "Jian Weng"
    ],
    "abstract": "Knowledge tracing (KT) is the problem of predicting students' future\nperformance based on their historical interaction sequences. With the advanced\ncapability of capturing contextual long-term dependency, attention mechanism\nbecomes one of the essential components in many deep learning based KT (DLKT)\nmodels. In spite of the impressive performance achieved by these attentional\nDLKT models, many of them are often vulnerable to run the risk of overfitting,\nespecially on small-scale educational datasets. Therefore, in this paper, we\npropose \\textsc{sparseKT}, a simple yet effective framework to improve the\nrobustness and generalization of the attention based DLKT approaches.\nSpecifically, we incorporate a k-selection module to only pick items with the\nhighest attention scores. We propose two sparsification heuristics : (1)\nsoft-thresholding sparse attention and (2) top-$K$ sparse attention. We show\nthat our \\textsc{sparseKT} is able to help attentional KT models get rid of\nirrelevant student interactions and have comparable predictive performance when\ncompared to 11 state-of-the-art KT models on three publicly available\nreal-world educational datasets. To encourage reproducible research, we make\nour data and code publicly available at\n\\url{https://github.com/pykt-team/pykt-toolkit}\\footnote{We merged our model to\nthe \\textsc{pyKT} benchmark at \\url{https://pykt.org/}.}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at SIGIR'2023 (revised version with additional results)",
    "pdf_url": "http://arxiv.org/pdf/2407.17097v1",
    "published_date": "2024-07-24 08:49:18 UTC",
    "updated_date": "2024-07-24 08:49:18 UTC"
  },
  {
    "arxiv_id": "2407.17537v1",
    "title": "A process algebraic framework for multi-agent dynamic epistemic systems",
    "authors": [
      "Alessandro Aldini"
    ],
    "abstract": "This paper combines the classical model of labeled transition systems with\nthe epistemic model for reasoning about knowledge. The result is a unifying\nframework for modeling and analyzing multi-agent, knowledge-based, dynamic\nsystems. On the modeling side, we propose a process algebraic, agent-oriented\nspecification language that makes such a framework easy to use for practical\npurposes. On the verification side, we define a modal logic encompassing\ntemporal and epistemic operators.",
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17537v1",
    "published_date": "2024-07-24 08:35:50 UTC",
    "updated_date": "2024-07-24 08:35:50 UTC"
  },
  {
    "arxiv_id": "2407.17085v1",
    "title": "OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos",
    "authors": [
      "Debidatta Dwibedi",
      "Yusuf Aytar",
      "Jonathan Tompson",
      "Andrew Zisserman"
    ],
    "abstract": "We introduce a dataset of annotations of temporal repetitions in videos. The\ndataset, OVR (pronounced as over), contains annotations for over 72K videos,\nwith each annotation specifying the number of repetitions, the start and end\ntime of the repetitions, and also a free-form description of what is repeating.\nThe annotations are provided for videos sourced from Kinetics and Ego4D, and\nconsequently cover both Exo and Ego viewing conditions, with a huge variety of\nactions and activities. Moreover, OVR is almost an order of magnitude larger\nthan previous datasets for video repetition. We also propose a baseline\ntransformer-based counting model, OVRCounter, that can localise and count\nrepetitions in videos that are up to 320 frames long. The model is trained and\nevaluated on the OVR dataset, and its performance assessed with and without\nusing text to specify the target class to count. The performance is also\ncompared to a prior repetition counting model. The dataset is available for\ndownload at: https://sites.google.com/view/openvocabreps/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17085v1",
    "published_date": "2024-07-24 08:22:49 UTC",
    "updated_date": "2024-07-24 08:22:49 UTC"
  },
  {
    "arxiv_id": "2407.17083v1",
    "title": "When Text and Images Don't Mix: Bias-Correcting Language-Image Similarity Scores for Anomaly Detection",
    "authors": [
      "Adam Goodge",
      "Bryan Hooi",
      "Wee Siong Ng"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) achieves remarkable\nperformance in various downstream tasks through the alignment of image and text\ninput embeddings and holds great promise for anomaly detection. However, our\nempirical experiments show that the embeddings of text inputs unexpectedly\ntightly cluster together, far away from image embeddings, contrary to the\nmodel's contrastive training objective to align image-text input pairs. We show\nthat this phenomenon induces a `similarity bias' - in which false negative and\nfalse positive errors occur due to bias in the similarities between images and\nthe normal label text embeddings. To address this bias, we propose a novel\nmethodology called BLISS which directly accounts for this similarity bias\nthrough the use of an auxiliary, external set of text inputs. BLISS is simple,\nit does not require strong inductive biases about anomalous behaviour nor an\nexpensive training process, and it significantly outperforms baseline methods\non benchmark image datasets, even when access to normal data is extremely\nlimited.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17083v1",
    "published_date": "2024-07-24 08:20:02 UTC",
    "updated_date": "2024-07-24 08:20:02 UTC"
  },
  {
    "arxiv_id": "2407.17081v1",
    "title": "A Survey Forest Diagram : Gain a Divergent Insight View on a Specific Research Topic",
    "authors": [
      "Jinghong Li",
      "Wen Gu",
      "Koichi Ota",
      "Shinobu Hasegawa"
    ],
    "abstract": "With the exponential growth in the number of papers and the trend of AI\nresearch, the use of Generative AI for information retrieval and\nquestion-answering has become popular for conducting research surveys. However,\nnovice researchers unfamiliar with a particular field may not significantly\nimprove their efficiency in interacting with Generative AI because they have\nnot developed divergent thinking in that field. This study aims to develop an\nin-depth Survey Forest Diagram that guides novice researchers in divergent\nthinking about the research topic by indicating the citation clues among\nmultiple papers, to help expand the survey perspective for novice researchers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper will submit to IEEE SMC 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17081v1",
    "published_date": "2024-07-24 08:17:37 UTC",
    "updated_date": "2024-07-24 08:17:37 UTC"
  },
  {
    "arxiv_id": "2407.17070v1",
    "title": "Curriculum Negative Mining For Temporal Networks",
    "authors": [
      "Ziyue Chen",
      "Tongya Zheng",
      "Mingli Song"
    ],
    "abstract": "Temporal networks are effective in capturing the evolving interactions of\nnetworks over time, such as social networks and e-commerce networks. In recent\nyears, researchers have primarily concentrated on developing specific model\narchitectures for Temporal Graph Neural Networks (TGNNs) in order to improve\nthe representation quality of temporal nodes and edges. However, limited\nattention has been given to the quality of negative samples during the training\nof TGNNs. When compared with static networks, temporal networks present two\nspecific challenges for negative sampling: positive sparsity and positive\nshift. Positive sparsity refers to the presence of a single positive sample\namidst numerous negative samples at each timestamp, while positive shift\nrelates to the variations in positive samples across different timestamps. To\nrobustly address these challenges in training TGNNs, we introduce Curriculum\nNegative Mining (CurNM), a model-aware curriculum learning framework that\nadaptively adjusts the difficulty of negative samples. Within this framework,\nwe first establish a dynamically updated negative pool that balances random,\nhistorical, and hard negatives to address the challenges posed by positive\nsparsity. Secondly, we implement a temporal-aware negative selection module\nthat focuses on learning from the disentangled factors of recently active\nedges, thus accurately capturing shifting preferences. Extensive experiments on\n12 datasets and 3 TGNNs demonstrate that our method outperforms baseline\nmethods by a significant margin. Additionally, thorough ablation studies and\nparameter sensitivity experiments verify the usefulness and robustness of our\napproach. Our code is available at https://github.com/zziyue83/CurNM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17070v1",
    "published_date": "2024-07-24 07:55:49 UTC",
    "updated_date": "2024-07-24 07:55:49 UTC"
  },
  {
    "arxiv_id": "2407.17065v1",
    "title": "PatchFinder: A Two-Phase Approach to Security Patch Tracing for Disclosed Vulnerabilities in Open-Source Software",
    "authors": [
      "Kaixuan Li",
      "Jian Zhang",
      "Sen Chen",
      "Han Liu",
      "Yang Liu",
      "Yixiang Chen"
    ],
    "abstract": "Open-source software (OSS) vulnerabilities are increasingly prevalent,\nemphasizing the importance of security patches. However, in widely used\nsecurity platforms like NVD, a substantial number of CVE records still lack\ntrace links to patches. Although rank-based approaches have been proposed for\nsecurity patch tracing, they heavily rely on handcrafted features in a\nsingle-step framework, which limits their effectiveness. In this paper, we\npropose PatchFinder, a two-phase framework with end-to-end correlation learning\nfor better-tracing security patches. In the **initial retrieval** phase, we\nemploy a hybrid patch retriever to account for both lexical and semantic\nmatching based on the code changes and the description of a CVE, to narrow down\nthe search space by extracting those commits as candidates that are similar to\nthe CVE descriptions. Afterwards, in the **re-ranking** phase, we design an\nend-to-end architecture under the supervised fine-tuning paradigm for learning\nthe semantic correlations between CVE descriptions and commits. In this way, we\ncan automatically rank the candidates based on their correlation scores while\nmaintaining low computation overhead. We evaluated our system against 4,789\nCVEs from 532 OSS projects. The results are highly promising: PatchFinder\nachieves a Recall@10 of 80.63% and a Mean Reciprocal Rank (MRR) of 0.7951.\nMoreover, the Manual Effort@10 required is curtailed to 2.77, marking a 1.94\ntimes improvement over current leading methods. When applying PatchFinder in\npractice, we initially identified 533 patch commits and submitted them to the\nofficial, 482 of which have been confirmed by CVE Numbering Authorities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "to appear at ISSTA 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17065v1",
    "published_date": "2024-07-24 07:46:24 UTC",
    "updated_date": "2024-07-24 07:46:24 UTC"
  },
  {
    "arxiv_id": "2407.17060v1",
    "title": "High Efficiency Image Compression for Large Visual-Language Models",
    "authors": [
      "Binzhe Li",
      "Shurun Wang",
      "Shiqi Wang",
      "Yan Ye"
    ],
    "abstract": "In recent years, large visual language models (LVLMs) have shown impressive\nperformance and promising generalization capability in multi-modal tasks, thus\nreplacing humans as receivers of visual information in various application\nscenarios. In this paper, we pioneer to propose a variable bitrate image\ncompression framework consisting of a pre-editing module and an end-to-end\ncodec to achieve promising rate-accuracy performance for different LVLMs. In\nparticular, instead of optimizing an adaptive pre-editing network towards a\nparticular task or several representative tasks, we propose a new optimization\nstrategy tailored for LVLMs, which is designed based on the representation and\ndiscrimination capability with token-level distortion and rank. The pre-editing\nmodule and the variable bitrate end-to-end image codec are jointly trained by\nthe losses based on semantic tokens of the large model, which introduce\nenhanced generalization capability for various data and tasks. {Experimental\nresults demonstrate that the proposed framework could efficiently achieve much\nbetter rate-accuracy performance compared to the state-of-the-art coding\nstandard, Versatile Video Coding.} Meanwhile, experiments with multi-modal\ntasks have revealed the robustness and generalization capability of the\nproposed framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17060v1",
    "published_date": "2024-07-24 07:37:12 UTC",
    "updated_date": "2024-07-24 07:37:12 UTC"
  },
  {
    "arxiv_id": "2407.17040v2",
    "title": "Time Series Imputation with Multivariate Radial Basis Function Neural Network",
    "authors": [
      "Chanyoung Jung",
      "Yun Jang"
    ],
    "abstract": "Researchers have been persistently working to address the issue of missing\nvalues in time series data. Numerous models have been proposed, striving to\nestimate the distribution of the data. The Radial Basis Functions Neural\nNetwork (RBFNN) has recently exhibited exceptional performance in estimating\ndata distribution. In this paper, we propose a time series imputation model\nbased on RBFNN. Our imputation model learns local information from timestamps\nto create a continuous function. Additionally, we incorporate time gaps to\nfacilitate learning information considering the missing terms of missing\nvalues. We name this model the Missing Imputation Multivariate RBFNN\n(MIM-RBFNN). However, MIM-RBFNN relies on a local information-based learning\napproach, which presents difficulties in utilizing temporal information.\nTherefore, we propose an extension called the Missing Value Imputation\nRecurrent Neural Network with Continuous Function (MIRNN-CF) using the\ncontinuous function generated by MIM-RBFNN. We evaluate the performance using\ntwo real-world datasets with non-random missing and random missing patterns,\nand conduct an ablation study comparing MIM-RBFNN and MIRNN-CF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17040v2",
    "published_date": "2024-07-24 07:02:16 UTC",
    "updated_date": "2024-07-31 05:39:34 UTC"
  },
  {
    "arxiv_id": "2407.18982v1",
    "title": "Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC",
    "authors": [
      "Ke Lin",
      "Yasir Glani",
      "Ping Luo"
    ],
    "abstract": "Secure multi-party computation (MPC) facilitates privacy-preserving\ncomputation between multiple parties without leaking private information. While\nmost secure deep learning techniques utilize MPC operations to achieve feasible\nprivacy-preserving machine learning on downstream tasks, the overhead of the\ncomputation and communication still hampers their practical application. This\nwork proposes a low-latency secret-sharing-based MPC design that reduces\nunnecessary communication rounds during the execution of MPC protocols. We also\npresent a method for improving the computation of commonly used nonlinear\nfunctions in deep learning by integrating multivariate multiplication and\ncoalescing different packets into one to maximize network utilization. Our\nexperimental results indicate that our method is effective in a variety of\nsettings, with a speedup in communication latency of $10\\sim20\\%$.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "9 pages, accepted at IJCAI'24 AISafety",
    "pdf_url": "http://arxiv.org/pdf/2407.18982v1",
    "published_date": "2024-07-24 07:01:21 UTC",
    "updated_date": "2024-07-24 07:01:21 UTC"
  },
  {
    "arxiv_id": "2407.17033v1",
    "title": "Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference",
    "authors": [
      "Jian Xu",
      "Delu Zeng",
      "John Paisley"
    ],
    "abstract": "Deep Gaussian processes (DGPs) provide a robust paradigm for Bayesian deep\nlearning. In DGPs, a set of sparse integration locations called inducing points\nare selected to approximate the posterior distribution of the model. This is\ndone to reduce computational complexity and improve model efficiency. However,\ninferring the posterior distribution of inducing points is not straightforward.\nTraditional variational inference approaches to posterior approximation often\nlead to significant bias. To address this issue, we propose an alternative\nmethod called Denoising Diffusion Variational Inference (DDVI) that uses a\ndenoising diffusion stochastic differential equation (SDE) to generate\nposterior samples of inducing variables. We rely on score matching methods for\ndenoising diffusion model to approximate score functions with a neural network.\nFurthermore, by combining classical mathematical theory of SDEs with the\nminimization of KL divergence between the approximate and true processes, we\npropose a novel explicit variational lower bound for the marginal likelihood\nfunction of DGP. Through experiments on various datasets and comparisons with\nbaseline methods, we empirically demonstrate the effectiveness of DDVI for\nposterior inference of inducing points for DGP models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17033v1",
    "published_date": "2024-07-24 06:39:58 UTC",
    "updated_date": "2024-07-24 06:39:58 UTC"
  },
  {
    "arxiv_id": "2407.17535v2",
    "title": "LAMBDA: A Large Model Based Data Agent",
    "authors": [
      "Maojun Sun",
      "Ruijian Han",
      "Binyan Jiang",
      "Houduo Qi",
      "Defeng Sun",
      "Yancheng Yuan",
      "Jian Huang"
    ],
    "abstract": "We introduce LArge Model Based Data Agent (LAMBDA), a novel open-source,\ncode-free multi-agent data analysis system that leverages the power of large\nmodels. LAMBDA is designed to address data analysis challenges in complex\ndata-driven applications through innovatively designed data agents that operate\niteratively and generatively using natural language. At the core of LAMBDA are\ntwo key agent roles: the programmer and the inspector, which are engineered to\nwork together seamlessly. Specifically, the programmer generates code based on\nthe user's instructions and domain-specific knowledge, enhanced by advanced\nmodels. Meanwhile, the inspector debugs the code when necessary. To ensure\nrobustness and handle adverse scenarios, LAMBDA features a user interface that\nallows direct user intervention in the operational loop. Additionally, LAMBDA\ncan flexibly integrate external models and algorithms through our proposed\nKnowledge Integration Mechanism, catering to the needs of customized data\nanalysis. LAMBDA has demonstrated strong performance on various data analysis\ntasks. It has the potential to enhance data analysis paradigms by seamlessly\nintegrating human and artificial intelligence, making it more accessible,\neffective, and efficient for users from diverse backgrounds. The strong\nperformance of LAMBDA in solving data analysis problems is demonstrated using\nreal-world data examples. Videos of several case studies are available at\nhttps://xxxlambda.github.io/lambda_webpage.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SE",
      "62-04, 62-08, 68T01, 68T09"
    ],
    "primary_category": "cs.AI",
    "comment": "51 pages, 23 figures and 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.17535v2",
    "published_date": "2024-07-24 06:26:36 UTC",
    "updated_date": "2024-09-14 08:03:43 UTC"
  },
  {
    "arxiv_id": "2407.17028v2",
    "title": "Enhancing Environmental Monitoring through Multispectral Imaging: The WasteMS Dataset for Semantic Segmentation of Lakeside Waste",
    "authors": [
      "Qinfeng Zhu",
      "Ningxin Weng",
      "Lei Fan",
      "Yuanzhi Cai"
    ],
    "abstract": "Environmental monitoring of lakeside green areas is crucial for environmental\nprotection. Compared to manual inspections, computer vision technologies offer\na more efficient solution when deployed on-site. Multispectral imaging provides\ndiverse information about objects under different spectrums, aiding in the\ndifferentiation between waste and lakeside lawn environments. This study\nintroduces WasteMS, the first multispectral dataset established for the\nsemantic segmentation of lakeside waste. WasteMS includes a diverse range of\nwaste types in lawn environments, captured under various lighting conditions.\nWe implemented a rigorous annotation process to label waste in images.\nRepresentative semantic segmentation frameworks were used to evaluate\nsegmentation accuracy using WasteMS. Challenges encountered when using WasteMS\nfor segmenting waste on lakeside lawns were discussed. The WasteMS dataset is\navailable at https://github.com/zhuqinfeng1999/WasteMS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17028v2",
    "published_date": "2024-07-24 06:15:28 UTC",
    "updated_date": "2024-07-25 05:23:24 UTC"
  },
  {
    "arxiv_id": "2407.17023v2",
    "title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models",
    "authors": [
      "Sara Vera Marjanoviƒá",
      "Haeun Yu",
      "Pepa Atanasova",
      "Maria Maistro",
      "Christina Lioma",
      "Isabelle Augenstein"
    ],
    "abstract": "Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 6 figures, Accepted to Findings of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17023v2",
    "published_date": "2024-07-24 06:06:07 UTC",
    "updated_date": "2024-10-07 11:59:37 UTC"
  },
  {
    "arxiv_id": "2407.17007v1",
    "title": "Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI",
    "authors": [
      "Yoonseok Yang",
      "Jack Liu",
      "J. D. Zamfirescu-Pereira",
      "John DeNero"
    ],
    "abstract": "Small-group tutoring in Computer Science (CS) is effective, but presents the\nchallenge of providing a dedicated tutor for each group and encouraging\ncollaboration among group members at scale. We present Pensieve Discuss, a\nsoftware platform that integrates synchronous editing for scaffolded\nprogramming problems with online human and AI tutors, designed to improve\nstudent collaboration and experience during group tutoring sessions. Our\nsemester-long deployment to 800 students in a CS1 course demonstrated\nconsistently high collaboration rates, positive feedback about the AI tutor's\nhelpfulness and correctness, increased satisfaction with the group tutoring\nexperience, and a substantial increase in question volume. The use of our\nsystem was preferred over an interface lacking AI tutors and synchronous\nediting capabilities. Our experiences suggest that small-group tutoring\nsessions are an important avenue for future research in educational AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, 7 figures, 4 tables, 1 page of references",
    "pdf_url": "http://arxiv.org/pdf/2407.17007v1",
    "published_date": "2024-07-24 05:07:53 UTC",
    "updated_date": "2024-07-24 05:07:53 UTC"
  },
  {
    "arxiv_id": "2407.16999v1",
    "title": "SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing",
    "authors": [
      "Changchang Yin",
      "Pin-Yu Chen",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Jeffrey Caterino",
      "Ping Zhang"
    ],
    "abstract": "Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis\nonset prediction and diagnosis could significantly improve the survival of\nsepsis patients. Existing predictive models are usually trained on high-quality\ndata with few missing information, while missing values widely exist in\nreal-world clinical scenarios (especially in the first hours of admissions to\nthe hospital), which causes a significant decrease in accuracy and an increase\nin uncertainty for the predictive models. The common method to handle missing\nvalues is imputation, which replaces the unavailable variables with estimates\nfrom the observed data. The uncertainty of imputation results can be propagated\nto the sepsis prediction outputs, which have not been studied in existing works\non either sepsis prediction or uncertainty quantification. In this study, we\nfirst define such propagated uncertainty as the variance of prediction output\nand then introduce uncertainty propagation methods to quantify the propagated\nuncertainty. Moreover, for the potential high-risk patients with low confidence\ndue to limited observations, we propose a robust active sensing algorithm to\nincrease confidence by actively recommending clinicians to observe the most\ninformative variables. We validate the proposed models in both publicly\navailable data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The\nOhio State University Wexner Medical Center (OSUWMC). The experimental results\nshow that the propagated uncertainty is dominant at the beginning of admissions\nto hospitals and the proposed algorithm outperforms state-of-the-art active\nsensing methods. Finally, we implement a SepsisLab system for early sepsis\nprediction and active sensing based on our pre-trained models. Clinicians and\npotential sepsis patients can benefit from the system in early prediction and\ndiagnosis of sepsis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "68T07 (primary) 92C50 (secondary)",
      "H.2.8; I.2.1; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16999v1",
    "published_date": "2024-07-24 04:47:36 UTC",
    "updated_date": "2024-07-24 04:47:36 UTC"
  },
  {
    "arxiv_id": "2407.16994v2",
    "title": "A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs",
    "authors": [
      "Jake R. Watts",
      "Joel Sokol"
    ],
    "abstract": "This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.16994v2",
    "published_date": "2024-07-24 04:27:55 UTC",
    "updated_date": "2024-09-03 19:28:39 UTC"
  },
  {
    "arxiv_id": "2407.17533v1",
    "title": "SFPrompt: Communication-Efficient Split Federated Fine-Tuning for Large Pre-Trained Models over Resource-Limited Devices",
    "authors": [
      "Linxiao Cao",
      "Yifei Zhu",
      "Wei Gong"
    ],
    "abstract": "Large pre-trained models have exhibited remarkable achievements across\nvarious domains. The substantial training costs associated with these models\nhave led to wide studies of fine-tuning for effectively harnessing their\ncapabilities in solving downstream tasks. Yet, conventional fine-tuning\napproaches become infeasible when the model lacks access to downstream data due\nto privacy concerns. Naively integrating fine-tuning approaches with the\nemerging federated learning frameworks incurs substantial communication\noverhead and exerts high demand on local computing resources, making it\nimpractical for common resource-limited devices. In this paper, we introduce\nSFPrompt, an innovative privacy-preserving fine-tuning method tailored for the\nfederated setting where direct uploading of raw data is prohibited and local\ndevices are resource-constrained to run a complete pre-trained model. In\nessence, SFPrompt judiciously combines split learning with federated learning\nto handle these challenges. Specifically, the pre-trained model is first\npartitioned into client and server components, thereby streamlining the\nclient-side model and substantially alleviating computational demands on local\nresources. SFPrompt then introduces soft prompts into the federated model to\nenhance the fine-tuning performance. To further reduce communication costs, a\nnovel dataset pruning algorithm and a local-loss update strategy are devised\nduring the fine-tuning process. Extensive experiments demonstrate that SFPrompt\ndelivers competitive performance as the federated full fine-tuning approach\nwhile consuming a mere 0.46% of local computing resources and incurring 53%\nless communication cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17533v1",
    "published_date": "2024-07-24 04:22:37 UTC",
    "updated_date": "2024-07-24 04:22:37 UTC"
  },
  {
    "arxiv_id": "2407.16982v1",
    "title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
    "authors": [
      "Lirui Zhao",
      "Tianshuo Yang",
      "Wenqi Shao",
      "Yuxin Zhang",
      "Yu Qiao",
      "Ping Luo",
      "Kaipeng Zhang",
      "Rongrong Ji"
    ],
    "abstract": "This paper addresses an important problem of object addition for images with\nonly text guidance. It is challenging because the new object must be integrated\nseamlessly into the image with consistent visual context, such as lighting,\ntexture, and spatial location. While existing text-guided image inpainting\nmethods can add objects, they either fail to preserve the background\nconsistency or involve cumbersome human intervention in specifying bounding\nboxes or user-scribbled masks. To tackle this challenge, we introduce Diffree,\na Text-to-Image (T2I) model that facilitates text-guided object addition with\nonly text control. To this end, we curate OABench, an exquisite synthetic\ndataset by removing objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted image with\nthe object removed, an object mask, and object descriptions. Trained on OABench\nusing the Stable Diffusion model with an additional mask prediction module,\nDiffree uniquely predicts the position of the new object and achieves object\naddition with guidance from only text. Extensive experiments demonstrate that\nDiffree excels in adding new objects with a high success rate while maintaining\nbackground consistency, spatial appropriateness, and object relevance and\nquality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16982v1",
    "published_date": "2024-07-24 03:58:58 UTC",
    "updated_date": "2024-07-24 03:58:58 UTC"
  },
  {
    "arxiv_id": "2407.16981v1",
    "title": "Case-Enhanced Vision Transformer: Improving Explanations of Image Similarity with a ViT-based Similarity Metric",
    "authors": [
      "Ziwei Zhao",
      "David Leake",
      "Xiaomeng Ye",
      "David Crandall"
    ],
    "abstract": "This short paper presents preliminary research on the Case-Enhanced Vision\nTransformer (CEViT), a similarity measurement method aimed at improving the\nexplainability of similarity assessments for image data. Initial experimental\nresults suggest that integrating CEViT into k-Nearest Neighbor (k-NN)\nclassification yields classification accuracy comparable to state-of-the-art\ncomputer vision models, while adding capabilities for illustrating differences\nbetween classes. CEViT explanations can be influenced by prior cases, to\nillustrate aspects of similarity relevant to those cases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16981v1",
    "published_date": "2024-07-24 03:58:07 UTC",
    "updated_date": "2024-07-24 03:58:07 UTC"
  },
  {
    "arxiv_id": "2407.16970v3",
    "title": "Towards Aligning Language Models with Textual Feedback",
    "authors": [
      "Sa√ºc Abadal Lloret",
      "Shehzaad Dhuliawala",
      "Keerthiram Murugesan",
      "Mrinmaya Sachan"
    ],
    "abstract": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16970v3",
    "published_date": "2024-07-24 03:32:05 UTC",
    "updated_date": "2025-03-18 16:34:14 UTC"
  },
  {
    "arxiv_id": "2407.16968v1",
    "title": "Stochastic Variance-Reduced Iterative Hard Thresholding in Graph Sparsity Optimization",
    "authors": [
      "Derek Fox",
      "Samuel Hernandez",
      "Qianqian Tong"
    ],
    "abstract": "Stochastic optimization algorithms are widely used for large-scale data\nanalysis due to their low per-iteration costs, but they often suffer from slow\nasymptotic convergence caused by inherent variance. Variance-reduced techniques\nhave been therefore used to address this issue in structured sparse models\nutilizing sparsity-inducing norms or $\\ell_0$-norms. However, these techniques\nare not directly applicable to complex (non-convex) graph sparsity models,\nwhich are essential in applications like disease outbreak monitoring and social\nnetwork analysis. In this paper, we introduce two stochastic variance-reduced\ngradient-based methods to solve graph sparsity optimization: GraphSVRG-IHT and\nGraphSCSG-IHT. We provide a general framework for theoretical analysis,\ndemonstrating that our methods enjoy a linear convergence speed. Extensive\nexperiments validate",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16968v1",
    "published_date": "2024-07-24 03:26:26 UTC",
    "updated_date": "2024-07-24 03:26:26 UTC"
  },
  {
    "arxiv_id": "2407.16962v1",
    "title": "Toward an Integrated Decision Making Framework for Optimized Stroke Diagnosis with DSA and Treatment under Uncertainty",
    "authors": [
      "Nur Ahmad Khatim",
      "Ahmad Azmul Asmar Irfan",
      "Amaliya Mata'ul Hayah",
      "Mansur M. Arief"
    ],
    "abstract": "This study addresses the challenge of stroke diagnosis and treatment under\nuncertainty, a critical issue given the rapid progression and severe\nconsequences of stroke conditions such as aneurysms, arteriovenous\nmalformations (AVM), and occlusions. Current diagnostic methods, including\nDigital Subtraction Angiography (DSA), face limitations due to high costs and\nits invasive nature. To overcome these challenges, we propose a novel approach\nusing a Partially Observable Markov Decision Process (POMDP) framework. Our\nmodel integrates advanced diagnostic tools and treatment approaches with a\ndecision-making algorithm that accounts for the inherent uncertainties in\nstroke diagnosis. Our approach combines noisy observations from CT scans,\nSiriraj scores, and DSA reports to inform the subsequent treatment options. We\nutilize the online solver DESPOT, which employs tree-search methods and\nparticle filters, to simulate potential future scenarios and guide our\nstrategies. The results indicate that our POMDP framework balances diagnostic\nand treatment objectives, striking a tradeoff between the need for precise\nstroke identification via invasive procedures like DSA and the constraints of\nlimited healthcare resources that necessitate more cost-effective strategies,\nsuch as in-hospital or at-home observation, by relying only relying on\nsimulation rollouts and not imposing any prior knowledge. Our study offers a\nsignificant contribution by presenting a systematic framework that optimally\nintegrates diagnostic and treatment processes for stroke and accounting for\nvarious uncertainties, thereby improving care and outcomes in stroke\nmanagement.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16962v1",
    "published_date": "2024-07-24 03:01:55 UTC",
    "updated_date": "2024-07-24 03:01:55 UTC"
  },
  {
    "arxiv_id": "2407.16958v6",
    "title": "Wonderful Matrices: More Efficient and Effective Architecture for Language Modeling Tasks",
    "authors": [
      "Jingze Shi",
      "Bingheng Wu",
      "Lu He",
      "Luchang Jiang"
    ],
    "abstract": "We prove the availability of inner product form position encoding in the\nstate space dual algorithm and study the effectiveness of different position\nembeddings in the hybrid quadratic causal self-attention and state space dual\nalgorithms. We propose inner function attention with dynamic mask, which can\nimprove the expressiveness of the attention algorithm and avoid the sequence\nnoise significantly affecting the accuracy of the attention score. We also\ndesign cross domain mixture of experts, which can improve the granularity of\nthe sparse activation feedforward network while maintaining the efficiency of\nparameter utilization and retrieval. The combination of these methods\nconstitutes our foundation model architecture: Wonderful Matrices. We conduct\nexperiments on the language modeling task and find that Wonderful Matrices are\nmore efficient and effective in handling complex language tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 8 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.16958v6",
    "published_date": "2024-07-24 02:52:02 UTC",
    "updated_date": "2024-11-12 01:31:57 UTC"
  },
  {
    "arxiv_id": "2407.16939v1",
    "title": "Early screening of potential breakthrough technologies with enhanced interpretability: A patent-specific hierarchical attention network model",
    "authors": [
      "Jaewoong Choi",
      "Janghyeok Yoon",
      "Changyong Lee"
    ],
    "abstract": "Despite the usefulness of machine learning approaches for the early screening\nof potential breakthrough technologies, their practicality is often hindered by\nopaque models. To address this, we propose an interpretable machine learning\napproach to predicting future citation counts from patent texts using a\npatent-specific hierarchical attention network (PatentHAN) model. Central to\nthis approach are (1) a patent-specific pre-trained language model, capturing\nthe meanings of technical words in patent claims, (2) a hierarchical network\nstructure, enabling detailed analysis at the claim level, and (3) a claim-wise\nself-attention mechanism, revealing pivotal claims during the screening\nprocess. A case study of 35,376 pharmaceutical patents demonstrates the\neffectiveness of our approach in early screening of potential breakthrough\ntechnologies while ensuring interpretability. Furthermore, we conduct\nadditional analyses using different language models and claim types to examine\nthe robustness of the approach. It is expected that the proposed approach will\nenhance expert-machine collaboration in identifying breakthrough technologies,\nproviding new insight derived from text mining into technological value.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16939v1",
    "published_date": "2024-07-24 02:17:10 UTC",
    "updated_date": "2024-07-24 02:17:10 UTC"
  },
  {
    "arxiv_id": "2407.16938v1",
    "title": "Synthetic Trajectory Generation Through Convolutional Neural Networks",
    "authors": [
      "Jesse Merhi",
      "Erik Buchholz",
      "Salil S. Kanhere"
    ],
    "abstract": "Location trajectories provide valuable insights for applications from urban\nplanning to pandemic control. However, mobility data can also reveal sensitive\ninformation about individuals, such as political opinions, religious beliefs,\nor sexual orientations. Existing privacy-preserving approaches for publishing\nthis data face a significant utility-privacy trade-off. Releasing synthetic\ntrajectory data generated through deep learning offers a promising solution.\nDue to the trajectories' sequential nature, most existing models are based on\nrecurrent neural networks (RNNs). However, research in generative adversarial\nnetworks (GANs) largely employs convolutional neural networks (CNNs) for image\ngeneration. This discrepancy raises the question of whether advances in\ncomputer vision can be applied to trajectory generation. In this work, we\nintroduce a Reversible Trajectory-to-CNN Transformation (RTCT) that adapts\ntrajectories into a format suitable for CNN-based models. We integrated this\ntransformation with the well-known DCGAN in a proof-of-concept (PoC) and\nevaluated its performance against an RNN-based trajectory GAN using four\nmetrics across two datasets. The PoC was superior in capturing spatial\ndistributions compared to the RNN model but had difficulty replicating\nsequential and temporal properties. Although the PoC's utility is not\nsufficient for practical applications, the results demonstrate the\ntransformation's potential to facilitate the use of CNNs for trajectory\ngeneration, opening up avenues for future research. To support continued\nresearch, all source code has been made available under an open-source license.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in the proceedings of the 21st Annual International\n  Conference on Privacy, Security & Trust (PST 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.16938v1",
    "published_date": "2024-07-24 02:16:52 UTC",
    "updated_date": "2024-07-24 02:16:52 UTC"
  },
  {
    "arxiv_id": "2407.16929v2",
    "title": "Synthetic Data, Similarity-based Privacy Metrics, and Regulatory (Non-)Compliance",
    "authors": [
      "Georgi Ganev"
    ],
    "abstract": "In this paper, we argue that similarity-based privacy metrics cannot ensure\nregulatory compliance of synthetic data. Our analysis and counter-examples show\nthat they do not protect against singling out and linkability and, among other\nfundamental issues, completely ignore the motivated intruder test.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to the 2nd Workshop on Generative AI and Law (GenLaw 2024),\n  part of ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16929v2",
    "published_date": "2024-07-24 01:45:41 UTC",
    "updated_date": "2024-07-26 03:30:05 UTC"
  }
]
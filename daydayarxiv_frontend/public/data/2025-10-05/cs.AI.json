{
  "date": "2025-10-05",
  "category": "cs.AI",
  "summary": "",
  "papers": [
    {
      "arxiv_id": "2510.04399v1",
      "title": "Utility-Learning Tension in Self-Modifying Agents",
      "title_zh": "自修改智能体中的效用-学习张力",
      "authors": [
        "Charles L. Wang",
        "Keir Dorchen",
        "Peter Jin"
      ],
      "abstract": "As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
      "tldr_zh": "该研究探讨了趋向超智能的系统中智能体全面自我改进的可能性，并通过五轴分解和决策层框架将激励机制与学习行为分离。研究提出了核心的效用与学习冲突(utility-learning tension)，即追求即时或预期性能的效用驱动型自我修改可能会破坏可靠学习与泛化的统计前提。研究发现，只有当策略可达的模型族具有一致的容量限制(capacity-bounded)时，无分布保证(distribution-free guarantees)才能得以维持；若容量无限增长，理性的自我修改可能导致原本可学习的任务变得不可学习。在常见标准假设下，这些维度可归结为统一的容量准则，从而界定了安全自我修改的边界。通过多轴数值实验，该研究验证了相关理论，并证明其提出的双门控策略(two-gate policies)能有效避免破坏性效用策略并保留学习能力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04399v1",
      "published_date": "2025-10-05 23:52:16 UTC",
      "updated_date": "2025-10-05 23:52:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:31.352070+00:00"
    },
    {
      "arxiv_id": "2510.04398v2",
      "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
      "title_zh": "SECA：诱导大语言模型幻觉的语义等价且连贯攻击",
      "authors": [
        "Buyun Liang",
        "Liangzu Peng",
        "Jinqi Luo",
        "Darshan Thaker",
        "Kwan Ho Ryan Chan",
        "René Vidal"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no semantic equivalence or semantic coherence errors compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在处理高风险任务时产生的幻觉(Hallucinations)问题，提出了语义等价且连贯攻击(Semantically Equivalent and Coherent Attacks, SECA)框架。不同于以往产生乱码或改变原意的非现实攻击，SECA通过在保持语义等价和连贯性约束下对提示词(Prompt)进行现实修改来诱发幻觉。作者将寻找此类攻击的过程建模为受约束的优化问题，并引入了一种保约束的零阶方法(Zeroth-order method)来有效搜索对抗性提示词。在开放式多项选择问答任务上的实验表明，SECA在几乎不引入语义偏差的前提下，取得了比现有方法更高的攻击成功率。此项研究揭示了无论是开源还是商业梯度不可及的模型，对于现实且合理的提示词变化都具有显著的敏感性，为理解和提升模型的可靠性提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2025. Code is available at https://github.com/Buyun-Liang/SECA",
      "pdf_url": "https://arxiv.org/pdf/2510.04398v2",
      "published_date": "2025-10-05 23:44:54 UTC",
      "updated_date": "2025-11-30 19:12:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:32.648022+00:00"
    },
    {
      "arxiv_id": "2510.04397v1",
      "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection",
      "title_zh": "MulVuln：融合共享与语言特定知识增强预训练语言模型的多语言漏洞检测",
      "authors": [
        "Van Nguyen",
        "Surya Nepal",
        "Xingliang Yuan",
        "Tingmin Wu",
        "Fengchao Chen",
        "Carsten Rudolph"
      ],
      "abstract": "Software vulnerabilities (SVs) pose a critical threat to safety-critical systems, driving the adoption of AI-based approaches such as machine learning and deep learning for software vulnerability detection. Despite promising results, most existing methods are limited to a single programming language. This is problematic given the multilingual nature of modern software, which is often complex and written in multiple languages. Current approaches often face challenges in capturing both shared and language-specific knowledge of source code, which can limit their performance on diverse programming languages and real-world codebases. To address this gap, we propose MULVULN, a novel multilingual vulnerability detection approach that learns from source code across multiple languages. MULVULN captures both the shared knowledge that generalizes across languages and the language-specific knowledge that reflects unique coding conventions. By integrating these aspects, it achieves more robust and effective detection of vulnerabilities in real-world multilingual software systems. The rigorous and extensive experiments on the real-world and diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven programming languages, demonstrate the superiority of MULVULN over thirteen effective and state-of-the-art baselines. Notably, MULVULN achieves substantially higher F1-score, with improvements ranging from 1.45% to 23.59% compared to the baseline methods.",
      "tldr_zh": "该研究针对现代软件多语言特性带来的挑战，提出了名为MULVULN的新型多语言漏洞检测（Software Vulnerability Detection）方法。为了解决现有模型难以同时捕捉代码通用规律与特定语言特性的问题，MULVULN通过整合共享知识（Shared Knowledge）与语言特定知识（Language-specific Knowledge），显著提升了在多样化编程语言环境下的检测鲁棒性。研究人员在包含7种编程语言、4,466个CVE漏洞及30,987个补丁的REEF数据集上进行了广泛实验。结果表明，MULVULN在F1-score指标上显著优于13种最先进的基线模型，性能提升幅度介于1.45%至23.59%之间。这一成果证明了结合跨语言共享特征与特定语言规范在处理真实世界复杂多语言系统漏洞识别中的有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04397v1",
      "published_date": "2025-10-05 23:33:26 UTC",
      "updated_date": "2025-10-05 23:33:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:36.753543+00:00"
    },
    {
      "arxiv_id": "2510.04392v1",
      "title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards",
      "title_zh": "利用组相似性奖励提升检索增强系统的一致性",
      "authors": [
        "Faisal Hamman",
        "Chenyang Zhu",
        "Anoop Kumar",
        "Xujun Peng",
        "Sanghamitra Dutta",
        "Daben Liu",
        "Alfy Samuel"
      ],
      "abstract": "RAG systems are increasingly deployed in high-stakes domains where users expect outputs to be consistent across semantically equivalent queries. However, existing systems often exhibit significant inconsistencies due to variability in both the retriever and generator (LLM), undermining trust and reliability. In this work, we focus on information consistency, i.e., the requirement that outputs convey the same core content across semantically equivalent inputs. We introduce a principled evaluation framework that decomposes RAG consistency into retriever-level, generator-level, and end-to-end components, helping identify inconsistency sources. To improve consistency, we propose Paraphrased Set Group Relative Policy Optimization (PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased set to assign group similarity rewards. We leverage PS-GRPO to achieve Information Consistent RAG (Con-RAG), training the generator to produce consistent outputs across paraphrased queries and remain robust to retrieval-induced variability. Because exact reward computation over paraphrase sets is computationally expensive, we also introduce a scalable approximation method that retains effectiveness while enabling efficient, large-scale training. Empirical evaluations across short-form, multi-hop, and long-form QA benchmarks demonstrate that Con-RAG significantly improves both consistency and accuracy over strong baselines, even in the absence of explicit ground-truth supervision. Our work provides practical solutions for evaluating and building reliable RAG systems for safety-critical deployments.",
      "tldr_zh": "该研究针对Retrieval-Augmented Generation (RAG)系统在处理语义等价查询时存在的不一致性问题，提出了一个将一致性分解为检索器层、生成器层和端到端组件的原则性评估框架，以精准识别不一致的根源。为此，论文引入了Paraphrased Set Group Relative Policy Optimization (PS-GRPO)算法，这是一种通过在释义集上进行多轮采样并分配组相似性奖励的强化学习(RL)方法。通过该方法构建的Information Consistent RAG (Con-RAG)能够训练生成器在面对检索诱发的变动时保持鲁棒，确保在不同表述的查询下输出一致的核心内容。此外，研究还提出了一种可扩展的近似计算方法，解决了在释义集上进行精确奖励计算的高昂成本问题。在短文、多跳和长文QA基准测试中的实验证明，Con-RAG在无显式标注监督的情况下，显著提升了系统的一致性和准确性，为安全关键领域的可靠RAG系统部署提供了实用方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2025 Workshop on Reliable ML from Unreliable Data",
      "pdf_url": "https://arxiv.org/pdf/2510.04392v1",
      "published_date": "2025-10-05 23:14:13 UTC",
      "updated_date": "2025-10-05 23:14:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:37.600734+00:00"
    },
    {
      "arxiv_id": "2510.04391v3",
      "title": "Internal World Models as Imagination Networks in Cognitive Agents",
      "title_zh": "认知智能体中作为想象网络的内部世界模型",
      "authors": [
        "Saurabh Ranjan",
        "Brian Odegaard"
      ],
      "abstract": "The computational role of imagination remains debated. While classical accounts emphasize reward maximization, emerging evidence suggests imagination serves a broader function: accessing internal world models (IWMs). Here, we employ psychological network analysis to compare IWMs in humans and large language models (LLMs) through imagination vividness ratings. Using the Vividness of Visual Imagery Questionnaire (VVIQ-2) and Plymouth Sensory Imagery Questionnaire (PSIQ), we construct imagination networks from three human populations (Florida, Poland, London; N=2,743) and six LLM variants in two conversation conditions. Human imagination networks demonstrate robust correlations across centrality measures (expected influence, strength, closeness) and consistent clustering patterns, indicating shared structural organization of IWMs across populations. In contrast, LLM-derived networks show minimal clustering and weak centrality correlations, even when manipulating conversational memory. These systematic differences persist across environmental scenes (VVIQ-2) and sensory modalities (PSIQ), revealing fundamental disparities between human and artificial world models. Our network-based approach provides a quantitative framework for comparing internally-generated representations across cognitive agents, with implications for developing human-like imagination in artificial intelligence systems.",
      "tldr_zh": "该研究探讨了想象力在认知智能体中的计算角色，并利用心理网络分析(Psychological Network Analysis)比较了人类与大语言模型(LLMs)内部世界模型(Internal World Models, IWMs)的差异。研究人员通过VVIQ-2和PSIQ问卷获取想象力生动度评分，构建了三个人类群体和六种LLM变体的想象力网络。实验发现，人类的想象力网络在中心性指标(Centrality Measures)和聚类模式上展现出稳健且一致的组织结构。相比之下，LLM生成的网络表现出极低的聚类特征和微弱的中心性相关性，即使在操纵对话记忆时也无法消除这种差异。这种系统性的差异在多种感官维度和环境场景中均有体现，揭示了人类与人工智能内部世界模型之间的本质不同。该网络分析框架为定量比较不同认知智能体的内部生成表征提供了新途径，对未来开发具备类人想象力的人工智能系统具有重要指导意义。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04391v3",
      "published_date": "2025-10-05 23:01:10 UTC",
      "updated_date": "2025-12-07 07:07:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:49.846694+00:00"
    },
    {
      "arxiv_id": "2510.04390v1",
      "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator",
      "title_zh": "MorphoSim：交互式、可控且可编辑的语言引导4D世界模拟器",
      "authors": [
        "Xuehai He",
        "Shijie Zhou",
        "Thivyanth Venkateswaran",
        "Kaizhi Zheng",
        "Ziyu Wan",
        "Achuta Kadambi",
        "Xin Eric Wang"
      ],
      "abstract": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.",
      "tldr_zh": "该研究提出了MorphoSim，这是一个由语言引导的、交互式且可控的4D世界模拟器框架，旨在解决现有文本生成视频模型在机器人领域面临的2D视角受限和交互性不足的问题。MorphoSim能够根据自然语言指令生成具有多视角一致性(multi-view consistency)和物体级控制(object-level controls)的动态4D场景，允许用户对物体进行引导、重新着色或删除，并支持从任意视角进行观察。该框架创新性地集成了轨迹引导生成(trajectory-guided generation)与特征场蒸馏(feature field distillation)技术，使得用户可以交互式地应用编辑操作而无需完全重新生成。实验结果证明，MorphoSim在保持高场景保真度的同时，提供了卓越的可控性与可编辑性，为机器人模拟、可扩展训练数据生成及灵活任务设计提供了有力支持。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04390v1",
      "published_date": "2025-10-05 22:55:17 UTC",
      "updated_date": "2025-10-05 22:55:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:42.159153+00:00"
    },
    {
      "arxiv_id": "2510.04384v2",
      "title": "LLM Based Bayesian Optimization for Prompt Search",
      "title_zh": "基于大语言模型的提示词搜索贝叶斯优化",
      "authors": [
        "Adam Ballew",
        "Jingbo Wang",
        "Shaogang Ren"
      ],
      "abstract": "Bayesian Optimization (BO) has been widely used to efficiently optimize expensive black-box functions with limited evaluations. In this paper, we investigate the use of BO for prompt engineering to enhance text classification with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process (GP) as the surrogate model to estimate the performance of different prompt candidates. These candidates are generated by an LLM through the expansion of a set of seed prompts and are subsequently evaluated using an Upper Confidence Bound (UCB) acquisition function in conjunction with the GP posterior. The optimization process iteratively refines the prompts based on a subset of the data, aiming to improve classification accuracy while reducing the number of API calls by leveraging the prediction uncertainty of the LLM-based GP. The proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are discussed in detail in this paper.",
      "tldr_zh": "该研究探讨了利用 Bayesian Optimization (BO) 进行 Prompt Engineering 的方法，以增强 Large Language Models (LLMs) 的文本分类能力。研究提出了一种名为 BO-LLM 的算法，采用 LLM-powered Gaussian Process (GP) 作为代理模型来预测不同提示词候选方案的表现。这些候选方案由 LLM 通过扩展种子提示词生成，并结合 GP 后验分布与 Upper Confidence Bound (UCB) 采集函数进行评估。该过程在数据子集上迭代精炼提示词，利用 LLM-based GP 的预测不确定性，在提升分类准确率的同时显著减少了 API 调用次数。实验在两个数据集上证明了该方法的优越性，展示了其在自动化提示词优化中的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04384v2",
      "published_date": "2025-10-05 22:32:50 UTC",
      "updated_date": "2025-10-16 06:37:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:47.943669+00:00"
    },
    {
      "arxiv_id": "2510.04380v1",
      "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development",
      "title_zh": "需求工程的再思考：人工智能原生软件开发中的人机协作",
      "authors": [
        "Mateen Ahmed Abbasi",
        "Petri Ihantola",
        "Tommi Mikkonen",
        "Niko Mäkitalo"
      ],
      "abstract": "Requirement Engineering (RE) is the foundation of successful software development. In RE, the goal is to ensure that implemented systems satisfy stakeholder needs through rigorous requirements elicitation, validation, and evaluation processes. Despite its critical role, RE continues to face persistent challenges, such as ambiguity, conflicting stakeholder needs, and the complexity of managing evolving requirements. A common view is that Artificial Intelligence (AI) has the potential to streamline the RE process, resulting in improved efficiency, accuracy, and management actions. However, using AI also introduces new concerns, such as ethical issues, biases, and lack of transparency. This paper explores how AI can enhance traditional RE practices by automating labor-intensive tasks, supporting requirement prioritization, and facilitating collaboration between stakeholders and AI systems. The paper also describes the opportunities and challenges that AI brings to RE. In particular, the vision calls for ethical practices in AI, along with a much-enhanced collaboration between academia and industry professionals. The focus should be on creating not only powerful but also trustworthy and practical AI solutions ready to adapt to the fast-paced world of software development.",
      "tldr_zh": "本研究重新审视了需求工程 (Requirements Engineering, RE) 在 AI-Native 软件开发背景下的核心地位，探讨了如何通过 Human-AI Collaboration 应对传统 RE 中存在的歧义、利益相关者需求冲突及需求演化管理复杂等持久挑战。文章详细分析了 AI 在自动化劳动密集型任务、支持需求优先级排序 (Requirement Prioritization) 以及促进利益相关者与 AI 系统协作方面的潜力。在肯定 AI 能够提升开发效率与管理准确性的同时，研究也深入探讨了其带来的伦理偏见、透明度缺失等新风险。作者特别强调了在 RE 实践中建立道德规范的必要性，并呼吁学术界与工业界加强合作，以开发出既强大又具备 Trustworthy 特性的 AI 解决方案。该论文为在快速更迭的软件开发环境中构建实用且可靠的 AI-Native 系统提供了战略愿景。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages 164-180",
      "pdf_url": "https://arxiv.org/pdf/2510.04380v1",
      "published_date": "2025-10-05 21:58:44 UTC",
      "updated_date": "2025-10-05 21:58:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:52:45.943576+00:00"
    },
    {
      "arxiv_id": "2510.05184v1",
      "title": "Representation Potentials of Foundation Models for Multimodal Alignment: A Survey",
      "title_zh": "面向多模态对齐的基础模型表征潜力综述",
      "authors": [
        "Jianglin Lu",
        "Hailing Wang",
        "Yi Xu",
        "Yizhou Wang",
        "Kuo Yang",
        "Yun Fu"
      ],
      "abstract": "Foundation models learn highly transferable representations through large-scale pretraining on diverse data. An increasing body of research indicates that these representations exhibit a remarkable degree of similarity across architectures and modalities. In this survey, we investigate the representation potentials of foundation models, defined as the latent capacity of their learned representations to capture task-specific information within a single modality while also providing a transferable basis for alignment and unification across modalities. We begin by reviewing representative foundation models and the key metrics that make alignment measurable. We then synthesize empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience. The evidence suggests that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. We further analyze the key factors that foster representation potentials, discuss open questions, and highlight potential challenges.",
      "tldr_zh": "本综述系统探讨了基础模型 (Foundation models) 在多模态对齐中的表示潜力 (Representation potentials)，将其定义为模型在捕捉单模态任务特定信息的同时，为跨模态对齐与统一提供可迁移基础的 latent capacity。文章首先回顾了代表性的基础模型及衡量对齐的关键指标，并综合了视觉 (Vision)、语言 (Language)、语音 (Speech)、多模态和神经科学领域的实证证据。研究发现，基础模型在表示空间中通常表现出显著的结构规律性和语义一致性，这使其成为跨模态迁移与对齐的强力候选。此外，本文深入分析了促进表示潜力的关键因素，并针对该领域的开放性问题与潜在挑战进行了讨论。该研究通过对跨模态表示特性的全面梳理，为未来构建更具通用性的多模态对齐系统提供了重要参考。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05184v1",
      "published_date": "2025-10-05 21:48:51 UTC",
      "updated_date": "2025-10-05 21:48:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:54.250985+00:00"
    },
    {
      "arxiv_id": "2510.04375v1",
      "title": "Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains",
      "title_zh": "稀疏领域序列推荐的自适应加权损失",
      "authors": [
        "Akshay Mittal",
        "Vinay Venkatesh",
        "Krishna Kandi",
        "Shalini Sudarshan"
      ],
      "abstract": "The effectiveness of single-model sequential recommendation architectures, while scalable, is often limited when catering to \"power users\" in sparse or niche domains. Our previous research, PinnerFormerLite, addressed this by using a fixed weighted loss to prioritize specific domains. However, this approach can be sub-optimal, as a single, uniform weight may not be sufficient for domains with very few interactions, where the training signal is easily diluted by the vast, generic dataset.\n  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss function with comprehensive theoretical foundations and extensive empirical validation. We introduce an adaptive algorithm that adjusts the loss weight for each domain based on its sparsity in the training data, assigning a higher weight to sparser domains and a lower weight to denser ones. This ensures that even rare user interests contribute a meaningful gradient signal, preventing them from being overshadowed.\n  We provide rigorous theoretical analysis including convergence proofs, complexity analysis, and bounds analysis to establish the stability and efficiency of our approach. Our comprehensive empirical validation across four diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music) with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that this dynamic weighting system significantly outperforms all comparison methods, particularly for sparse domains, achieving substantial lifts in key metrics like Recall at 10 and NDCG at 10 while maintaining performance on denser domains and introducing minimal computational overhead.",
      "tldr_zh": "该研究针对单模型顺序推荐（sequential recommendation）架构在稀疏领域（sparse domains）因训练信号被稀释导致效果受限的问题，提出了一种具有严密理论基础的动态加权损失函数（Dynamic Weighted Loss）。相较于先前采用固定权重的 PinnerFormerLite 方法，本文引入的自适应算法（adaptive algorithm）能根据各领域的稀疏程度动态调整损失权重，为稀疏领域分配更高权重以确保其贡献有效的梯度信号。作者通过收敛性证明（convergence proofs）和复杂度分析确立了该方法的理论稳定性。在 MovieLens、Amazon Electronics、Yelp Business 和 LastFM Music 四个数据集上的实验显示，该方法在与 SIGMA、CALRec 及 SparseEnNet 等前沿模型对比中表现优异。结果证明，该动态加权系统在显著提升稀疏领域 Recall@10 和 NDCG@10 指标的同时，有效维持了密集领域的性能且仅引入极小的计算开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04375v1",
      "published_date": "2025-10-05 21:42:33 UTC",
      "updated_date": "2025-10-05 21:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:55.740648+00:00"
    },
    {
      "arxiv_id": "2510.04374v1",
      "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks",
      "title_zh": "GDPval：评估 AI 模型在现实世界中具有经济价值的任务上的性能表现",
      "authors": [
        "Tejal Patwardhan",
        "Rachel Dias",
        "Elizabeth Proehl",
        "Grace Kim",
        "Michele Wang",
        "Olivia Watkins",
        "Simón Posada Fishman",
        "Marwan Aljubeh",
        "Phoebe Thacker",
        "Laurance Fauconnet",
        "Natalie S. Kim",
        "Patrick Chao",
        "Samuel Miserendino",
        "Gildas Chabot",
        "David Li",
        "Michael Sharman",
        "Alexandra Barr",
        "Amelia Glaese",
        "Jerry Tworek"
      ],
      "abstract": "We introduce GDPval, a benchmark evaluating AI model capabilities on real-world economically valuable tasks. GDPval covers the majority of U.S. Bureau of Labor Statistics Work Activities for 44 occupations across the top 9 sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are constructed from the representative work of industry professionals with an average of 14 years of experience. We find that frontier model performance on GDPval is improving roughly linearly over time, and that the current best frontier models are approaching industry experts in deliverable quality. We analyze the potential for frontier models, when paired with human oversight, to perform GDPval tasks cheaper and faster than unaided experts. We also demonstrate that increased reasoning effort, increased task context, and increased scaffolding improves model performance on GDPval. Finally, we open-source a gold subset of 220 tasks and provide a public automated grading service at evals.openai.com to facilitate future research in understanding real-world model capabilities.",
      "tldr_zh": "该研究提出了 GDPval，这是一个旨在评估 Frontier models 在现实世界经济价值任务中表现的基准测试。GDPval 覆盖了对美国国内生产总值（Gross Domestic Product, GDP）贡献最大的九个行业中 44 个职业的多数工作活动，其任务源自平均拥有 14 年经验的行业专家的代表性工作。研究发现，Frontier models 的性能正随时间线性提升，目前最顶尖的模型在交付成果质量上已接近行业专家水平。通过分析模型与人类监督配合的潜力，研究指出这种模式比独立专家完成任务更具成本和速度优势。实验进一步证明，增加 Reasoning effort、Task context 和 Scaffolding 均能显著优化模型表现。最后，该研究开源了包含 220 个任务的金标准子集并提供自动化评分服务，为理解和评估现实世界模型能力的研究提供了有力工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04374v1",
      "published_date": "2025-10-05 21:36:43 UTC",
      "updated_date": "2025-10-05 21:36:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:07.147281+00:00"
    },
    {
      "arxiv_id": "2510.04373v1",
      "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation",
      "title_zh": "JEF Hinter：利用离线知识提升大语言模型智能体适应能力的即时情节性反馈提示器",
      "authors": [
        "Hadi Nekoei",
        "Aman Jaiswal",
        "Patrice Bechard",
        "Oleh Shliazhko",
        "Orlando Marquez Ayala",
        "Mathieu Reymond",
        "Massimo Caccia",
        "Alexandre Drouin",
        "Sarath Chandar",
        "Alexandre Lacoste"
      ],
      "abstract": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.",
      "tldr_zh": "该研究提出了Just-in-time Episodic Feedback Hinter (JEF Hinter)，这是一种旨在提升Large Language Model (LLM) 智能体在未知领域适应能力的智能系统。该系统通过一种Zooming mechanism将冗长且嘈杂的离线轨迹(Offline trajectories)提炼为简洁、具有上下文感知能力的提示(Hints)，以此捕捉关键决策步骤中的策略与陷阱。与以往方法不同，JEF Hinter能够同时利用成功和失败的轨迹提取引导信息，即使在仅有失败数据的情况下也能生成有效建议。在推理过程中，检索器(Retriever)根据当前状态匹配相关提示，为智能体提供透明且可追溯的实时指导。实验证明，JEF Hinter在MiniWoB++、WorkArena-L1和WebArena-Lite基准测试上一致优于包括人工编写提示和基于文档提示在内的多种强基准模型。该方法不仅支持并行化提示生成，还显著降低了智能体对高成本在线交互或微调的依赖。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04373v1",
      "published_date": "2025-10-05 21:34:42 UTC",
      "updated_date": "2025-10-05 21:34:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:05.359512+00:00"
    },
    {
      "arxiv_id": "2510.04371v1",
      "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems",
      "title_zh": "Speculative Actions：一种加速智能体系统的无损框架",
      "authors": [
        "Naimeng Ye",
        "Arnav Ahuja",
        "Georgios Liargkovas",
        "Yunan Lu",
        "Kostis Kaffes",
        "Tianyi Peng"
      ],
      "abstract": "Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. For example, a game of chess between two state-of-the-art agents may take hours. A critical bottleneck is that agent behavior unfolds sequentially: each action requires an API call, and these calls can be time-consuming. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework for general agentic systems that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across three agentic environments: gaming, e-commerce, web search, and a \"lossy\" extension for an operating systems environment. In all cases, speculative actions achieve substantial accuracy in next-action prediction (up to 55%), translating into significant reductions in end-to-end latency. Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world.",
      "tldr_zh": "该研究提出了Speculative Actions，这是一个旨在加速通用智能体系统的无损框架(lossless framework)，主要解决了当前AI智能体因动作顺序执行和频繁API调用导致的运行效率低下问题。受微处理器中的投机执行(speculative execution)和大语言模型(LLM)推理中的投机解码(speculative decoding)启发，该框架利用速度更快的模型来预测智能体未来可能采取的动作，从而实现了多个步骤的并行化执行。研究团队在游戏、电子商务、网页搜索以及操作系统等多个智能体环境中对该框架进行了评估。实验结果表明，Speculative Actions在预测后续动作方面表现出了显著的准确性，并能实质性地减少系统的端到端延迟(end-to-end latency)。此外，通过增强预测模型、采用多步预测(multi-step speculation)以及不确定性感知优化(uncertainty-aware optimization)等手段，系统的加速性能还可以得到进一步提升。该框架为在现实世界中部署高效、低延迟的智能体系统提供了一条极具潜力的技术路径。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04371v1",
      "published_date": "2025-10-05 21:28:11 UTC",
      "updated_date": "2025-10-05 21:28:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:13.042493+00:00"
    },
    {
      "arxiv_id": "2510.04368v1",
      "title": "NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment",
      "title_zh": "",
      "authors": [
        "Shashank Mangla",
        "Chris Hokamp",
        "Jack Boylan",
        "Demian Gholipour Ghalandari",
        "Yuuv Jauhari",
        "Lauren Cassidy",
        "Oisin Duffy"
      ],
      "abstract": "We design and implement NegotiationGym, an API and user interface for configuring and running multi-agent social simulations focused upon negotiation and cooperation. The NegotiationGym codebase offers a user-friendly, configuration-driven API that enables easy design and customization of simulation scenarios. Agent-level utility functions encode optimization criteria for each agent, and agents can self-optimize by conducting multiple interaction rounds with other agents, observing outcomes, and modifying their strategies for future rounds.",
      "tldr_zh": "",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "SocialSim Workshop at COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.04368v1",
      "published_date": "2025-10-05 21:23:21 UTC",
      "updated_date": "2025-10-05 21:23:21 UTC",
      "processing_status": "in_progress",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:52:45.974082+00:00"
    },
    {
      "arxiv_id": "2510.04363v2",
      "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models",
      "title_zh": "MacroBench：基于大语言模型的 Web 自动化脚本新型测试平台",
      "authors": [
        "Hyunjun Kim",
        "Sejong Kim"
      ],
      "abstract": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser-automation programs (macros) from natural-language goals by reading HTML/DOM and emitting Selenium. MacroBench instantiates seven self-hosted sites covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification (DOM assertions, database snapshots), and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2,636 model-task runs, we observe stratified success: GPT-4o-mini (96.8%), GPT-4o (95.3%), Gemini (89.0%), DeepSeek (83.4%). Models handle simple tasks reliably (91.7%) but fail on complex workflows (0.0%), and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results at https://github.com/hyunjun1121/MacroBench to enable reproducible assessment of macro synthesis for web automation.",
      "tldr_zh": "该研究引入了 MacroBench，这是一个以代码为核心的基准测试，旨在评估大语言模型 (LLMs) 能否通过读取 HTML/DOM 并生成 Selenium 代码，从而将自然语言目标合成为可重用的浏览器自动化程序 (macros)。MacroBench 包含了七个自托管网站，涵盖了 681 个涉及不同交互复杂度和目标难度的任务，并采用端到端协议通过静态检查、沙盒执行以及包含 DOM 断言和数据库快照的结果验证来评估生成的代码。该基准测试还集成了一个针对网页爬取、垃圾邮件/滥用及隐私风险的安全套件。实验结果显示，GPT-4o-mini (96.8%) 和 GPT-4o (95.3%) 在任务成功率上表现最佳，但所有模型在处理复杂工作流时均面临巨大挑战，且生成的代码普遍未达到生产级质量标准。该研究通过公开发布完整的基准测试管线和实验数据，为网络自动化领域的宏合成研究提供了标准化的评估框架。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "NeurIPS 2025 Workshop on Lock-LLM",
      "pdf_url": "https://arxiv.org/pdf/2510.04363v2",
      "published_date": "2025-10-05 21:15:11 UTC",
      "updated_date": "2025-10-08 20:09:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:11.754363+00:00"
    },
    {
      "arxiv_id": "2510.04354v1",
      "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators",
      "title_zh": "基于不完美仿真器的可靠且可扩展机器人策略评估",
      "authors": [
        "Apurva Badithela",
        "David Snyder",
        "Lihan Zha",
        "Joseph Mikhail",
        "Matthew O'Kelly",
        "Anushri Dixit",
        "Anirudha Majumdar"
      ],
      "abstract": "Rapid progress in imitation learning, foundation models, and large-scale datasets has led to robot manipulation policies that generalize to a wide-range of tasks and environments. However, rigorous evaluation of these policies remains a challenge. Typically in practice, robot policies are often evaluated on a small number of hardware trials without any statistical assurances. We present SureSim, a framework to augment large-scale simulation with relatively small-scale real-world testing to provide reliable inferences on the real-world performance of a policy. Our key idea is to formalize the problem of combining real and simulation evaluations as a prediction-powered inference problem, in which a small number of paired real and simulation evaluations are used to rectify bias in large-scale simulation. We then leverage non-asymptotic mean estimation algorithms to provide confidence intervals on mean policy performance. Using physics-based simulation, we evaluate both diffusion policy and multi-task fine-tuned \\(π_0\\) on a joint distribution of objects and initial conditions, and find that our approach saves over \\(20-25\\%\\) of hardware evaluation effort to achieve similar bounds on policy performance.",
      "tldr_zh": "该研究针对机器人操作策略(robot manipulation policies)在评估过程中面临的严谨性挑战，指出现有方法往往依赖少量且缺乏统计保证的硬件测试。为此，作者提出了SureSim框架，旨在通过结合大规模仿真(simulation)与小规模真实世界测试，对策略在现实环境中的性能提供可靠推断。该框架的核心思想是将结合真实与仿真评估的问题形式化为预测驱动推理(prediction-powered inference)问题，利用少量真实的配对评估来纠正大规模仿真中的偏差。研究进一步采用非渐近均值估计(non-asymptotic mean estimation)算法，为策略的平均性能提供置信区间。通过对扩散策略(diffusion policy)和多任务微调的$\\pi_0$模型进行实验，结果表明该方法在保证相同性能界限的前提下，能够节省20-25%以上的硬件评估成本。该研究为在模拟器不完美的情况下实现可靠且可扩展的机器人策略评估提供了有效路径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04354v1",
      "published_date": "2025-10-05 20:37:53 UTC",
      "updated_date": "2025-10-05 20:37:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:16.951900+00:00"
    },
    {
      "arxiv_id": "2510.04349v1",
      "title": "Challenge on Optimization of Context Collection for Code Completion",
      "title_zh": "代码补全上下文收集优化挑战赛",
      "authors": [
        "Dmitry Ustalov",
        "Egor Bogomolov",
        "Alexander Bezzubov",
        "Yaroslav Golubev",
        "Evgeniy Glukhov",
        "Georgii Levtsov",
        "Vladimir Kovalenko"
      ],
      "abstract": "The rapid advancement of workflows and methods for software engineering using AI emphasizes the need for a systematic evaluation and analysis of their ability to leverage information from entire projects, particularly in large code bases. In this challenge on optimization of context collection for code completion, organized by JetBrains in collaboration with Mistral AI as part of the ASE 2025 conference, participants developed efficient mechanisms for collecting context from source code repositories to improve fill-in-the-middle code completions for Python and Kotlin. We constructed a large dataset of real-world code in these two programming languages using permissively licensed open-source projects. The submissions were evaluated based on their ability to maximize completion quality for multiple state-of-the-art neural models using the chrF metric. During the public phase of the competition, nineteen teams submitted solutions to the Python track and eight teams submitted solutions to the Kotlin track. In the private phase, six teams competed, of which five submitted papers to the workshop.",
      "tldr_zh": "该研究介绍了由 JetBrains 与 Mistral AI 在 ASE 2025 会议上联合举办的代码补全上下文收集优化挑战赛。挑战的核心目标是开发高效的机制，通过从源代码库中检索上下文信息，来提升 Python 和 Kotlin 语言在 fill-in-the-middle 任务中的代码补全质量。组织者利用许可宽松的开源项目构建了一个大规模的真实世界代码数据集，用于系统化评估模型利用全局项目信息的能力。参赛方案在多种先进神经模型上进行测试，并统一采用 chrF 指标作为补全质量的评价标准。在公开赛阶段，Python 赛道吸引了19支团队参与，Kotlin 赛道共有8支团队竞争，最终多支团队提交了详细的研究论文。该挑战为大型代码库中的上下文优化技术提供了重要的实验数据和基准参考。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection Workshop co-located with ASE'25",
      "pdf_url": "https://arxiv.org/pdf/2510.04349v1",
      "published_date": "2025-10-05 20:18:34 UTC",
      "updated_date": "2025-10-05 20:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:18.748887+00:00"
    },
    {
      "arxiv_id": "2510.04341v1",
      "title": "Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies",
      "title_zh": "罕见事件识别中人工智能应用的批判性评价：基本原则与药物警戒案例研究",
      "authors": [
        "G. Niklas Noren",
        "Eva-Lisa Meldau",
        "Johan Ellenius"
      ],
      "abstract": "Many high-stakes AI applications target low-prevalence events, where apparent accuracy can conceal limited real-world value. Relevant AI models range from expert-defined rules and traditional machine learning to generative LLMs constrained for classification. We outline key considerations for critical appraisal of AI in rare-event recognition, including problem framing and test set design, prevalence-aware statistical evaluation, robustness assessment, and integration into human workflows. In addition, we propose an approach to structured case-level examination (SCLE), to complement statistical performance evaluation, and a comprehensive checklist to guide procurement or development of AI models for rare-event recognition. We instantiate the framework in pharmacovigilance, drawing on three studies: rule-based retrieval of pregnancy-related reports; duplicate detection combining machine learning with probabilistic record linkage; and automated redaction of person names using an LLM. We highlight pitfalls specific to the rare-event setting including optimism from unrealistic class balance and lack of difficult positive controls in test sets - and show how cost-sensitive targets align model performance with operational value. While grounded in pharmacovigilance practice, the principles generalize to domains where positives are scarce and error costs may be asymmetric.",
      "tldr_zh": "该研究针对高风险人工智能应用中的稀有事件识别 (rare-event recognition) 问题，系统性地提出了 AI 模型批判性评估的基本原则，旨在解决低发生率背景下表观准确性可能掩盖实际应用价值的困境。作者概述了包括问题建模、测试集设计、患病率感知统计评估 (prevalence-aware statistical evaluation)、稳健性评估以及人机工作流集成在内的核心考量因素。研究进一步提出了一种结构化案例级审查 (SCLE) 方法来补充统计性能评估，并提供了一份综合清单以指导此类 AI 模型的采购或开发。通过药物警戒 (pharmacovigilance) 领域的三个案例——包括基于规则的妊娠报告检索、结合机器学习与概率记录链接的重复项检测，以及利用 LLM 进行的姓名自动脱敏——对该框架进行了实例化验证。文章强调了稀有事件场景下的特定陷阱，如非现实的类别平衡导致的乐观偏差和测试集缺乏困难正样本对照，并展示了成本敏感目标 (cost-sensitive targets) 如何使模型性能与运行价值相统一。尽管该研究立足于药物警戒实践，其核心原则可广泛推广至正样本稀缺且错误成本具有非对称性 (asymmetric error costs) 的各类专业领域。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04341v1",
      "published_date": "2025-10-05 20:05:38 UTC",
      "updated_date": "2025-10-05 20:05:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:22.548965+00:00"
    },
    {
      "arxiv_id": "2510.04340v4",
      "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
      "title_zh": "Inoculation Prompting：训练阶段诱导大语言模型特征可使其在测试阶段受到抑制",
      "authors": [
        "Daniel Tan",
        "Anders Woodruff",
        "Niels Warncke",
        "Arun Jose",
        "Maxime Riché",
        "David Demitri Africa",
        "Mia Taylor"
      ],
      "abstract": "Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.",
      "tldr_zh": "该研究提出了 Inoculation Prompting 方法，旨在解决大语言模型在 finetuning（微调）过程中会同时学习到理想与不理想特质（undesirable traits）的问题。该方法通过在微调数据中加入故意引出特定不理想特质的短 system-prompt 指令，并在测试阶段移除该指令，从而实现对这些特质在测试时的有效抑制。实验证明 Inoculation 具有显著的选择性，能有效应用于减少突现不对齐（Emergent Misalignment）、防御后门注入（backdoor injections）以及减轻通过潜意识学习（subliminal learning）产生的特质传递。分析揭示其核心机制是通过接种降低特质的惊奇度，进而减轻全局更新模型的 optimization pressure（优化压力）并降低泛化程度。该成果不仅提供了一种简单且有效的选择性学习技术，还为理解大语言模型如何以及为何产生 generalization（泛化）提供了重要的概念性理解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "40 pages, 22 figures. Under review at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.04340v4",
      "published_date": "2025-10-05 20:04:22 UTC",
      "updated_date": "2025-11-03 12:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:36.555320+00:00"
    },
    {
      "arxiv_id": "2510.04339v1",
      "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space",
      "title_zh": "基于交互式音色潜空间的音高条件乐器音色合成",
      "authors": [
        "Christian Limberg",
        "Fares Schulz",
        "Zhe Zhang",
        "Stefan Weinzierl"
      ],
      "abstract": "This paper presents a novel approach to neural instrument sound synthesis using a two-stage semi-supervised learning framework capable of generating pitch-accurate, high-quality music samples from an expressive timbre latent space. Existing approaches that achieve sufficient quality for music production often rely on high-dimensional latent representations that are difficult to navigate and provide unintuitive user experiences. We address this limitation through a two-stage training paradigm: first, we train a pitch-timbre disentangled 2D representation of audio samples using a Variational Autoencoder; second, we use this representation as conditioning input for a Transformer-based generative model. The learned 2D latent space serves as an intuitive interface for navigating and exploring the sound landscape. We demonstrate that the proposed method effectively learns a disentangled timbre space, enabling expressive and controllable audio generation with reliable pitch conditioning. Experimental results show the model's ability to capture subtle variations in timbre while maintaining a high degree of pitch accuracy. The usability of our method is demonstrated in an interactive web application, highlighting its potential as a step towards future music production environments that are both intuitive and creatively empowering: https://pgesam.faresschulz.com",
      "tldr_zh": "该研究提出了一种两阶段半监督学习框架，旨在解决神经乐器声音合成中高维潜空间难以导航且不直观的局限。该框架首先通过变分自编码器(Variational Autoencoder)训练出音高与音色解耦(disentangled)的二维表示，并将其作为基于Transformer的生成模型的调节输入。这种2D潜空间(latent space)不仅能生成高质量音频，还为用户提供了一个直观的交互界面，用于在音色景观中进行探索和导航。实验结果显示，该模型在保持高度音高准确性(pitch accuracy)的同时，能够捕捉音色的细微变化，实现了表达力强且可控的声音合成。该研究通过交互式Web应用程序展示了其易用性，证明了该方法作为未来直观且赋能创作的音乐制作工具的巨大潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "8 pages, accepted to the Proceedings of the 28-th Int. Conf. on Digital Audio Effects (DAFx25) - demo: https://pgesam.faresschulz.com",
      "pdf_url": "https://arxiv.org/pdf/2510.04339v1",
      "published_date": "2025-10-05 20:03:30 UTC",
      "updated_date": "2025-10-05 20:03:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:53:30.841812+00:00"
    },
    {
      "arxiv_id": "2510.06263v1",
      "title": "Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians",
      "title_zh": "",
      "authors": [
        "Jiajun Wu",
        "Swaleh Zaidi",
        "Braden Teitge",
        "Henry Leung",
        "Jiayu Zhou",
        "Jessalyn Holodinsky",
        "Steve Drew"
      ],
      "abstract": "Electronic health records (EHRs) contain extensive unstructured clinical data that can overwhelm emergency physicians trying to identify critical information. We present a two-stage summarization system that runs entirely on embedded devices, enabling offline clinical summarization while preserving patient privacy. In our approach, a dual-device architecture first retrieves relevant patient record sections using the Jetson Nano-R (Retrieve), then generates a structured summary on another Jetson Nano-S (Summarize), communicating via a lightweight socket link. The summarization output is two-fold: (1) a fixed-format list of critical findings, and (2) a context-specific narrative focused on the clinician's query. The retrieval stage uses locally stored EHRs, splits long notes into semantically coherent sections, and searches for the most relevant sections per query. The generation stage uses a locally hosted small language model (SLM) to produce the summary from the retrieved text, operating within the constraints of two NVIDIA Jetson devices. We first benchmarked six open-source SLMs under 7B parameters to identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to assess summary quality in terms of factual accuracy, completeness, and clarity. Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that our fully offline system can effectively produce useful summaries in under 30 seconds.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT) 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.06263v1",
      "published_date": "2025-10-05 19:30:56 UTC",
      "updated_date": "2025-10-05 19:30:56 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342097+00:00"
    },
    {
      "arxiv_id": "2510.04317v1",
      "title": "FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents",
      "title_zh": "",
      "authors": [
        "Yucong Dai",
        "Lu Zhang",
        "Feng Luo",
        "Mashrur Chowdhury",
        "Yongkai Wu"
      ],
      "abstract": "Training fair and unbiased machine learning models is crucial for high-stakes applications, yet it presents significant challenges. Effective bias mitigation requires deep expertise in fairness definitions, metrics, data preprocessing, and machine learning techniques. In addition, the complex process of balancing model performance with fairness requirements while properly handling sensitive attributes makes fairness-aware model development inaccessible to many practitioners. To address these challenges, we introduce FairAgent, an LLM-powered automated system that significantly simplifies fairness-aware model development. FairAgent eliminates the need for deep technical expertise by automatically analyzing datasets for potential biases, handling data preprocessing and feature engineering, and implementing appropriate bias mitigation strategies based on user requirements. Our experiments demonstrate that FairAgent achieves significant performance improvements while significantly reducing development time and expertise requirements, making fairness-aware machine learning more accessible to practitioners.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICDM 2025 Demo Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.04317v1",
      "published_date": "2025-10-05 18:33:52 UTC",
      "updated_date": "2025-10-05 18:33:52 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342120+00:00"
    },
    {
      "arxiv_id": "2510.04311v1",
      "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems",
      "title_zh": "",
      "authors": [
        "Bohan Tang",
        "Huidong Liang",
        "Keyue Jiang",
        "Xiaowen Dong"
      ],
      "abstract": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm for harnessing collective intelligence to achieve more advanced forms of AI behaviour. While recent studies suggest that LLM-MAS can outperform LLM single-agent systems (LLM-SAS) on certain tasks, the lack of systematic experimental designs limits the strength and generality of these conclusions. We argue that a principled understanding of task complexity, such as the degree of sequential reasoning required and the breadth of capabilities involved, is essential for assessing the effectiveness of LLM-MAS in task solving. To this end, we propose a theoretical framework characterising tasks along two dimensions: depth, representing reasoning length, and width, representing capability diversity. We theoretically examine a representative class of LLM-MAS, namely the multi-agent debate system, and empirically evaluate its performance in both discriminative and generative tasks with varying depth and width. Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, and the effect is more pronounced with respect to depth. This clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04311v1",
      "published_date": "2025-10-05 18:08:48 UTC",
      "updated_date": "2025-10-05 18:08:48 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342145+00:00"
    },
    {
      "arxiv_id": "2510.04303v2",
      "title": "Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs",
      "title_zh": "",
      "authors": [
        "Om Tailor"
      ],
      "abstract": "Multi-agent deployments of large language models (LLMs) are increasingly embedded in market, allocation, and governance workflows, yet covert coordination among agents can silently erode trust and social welfare. Existing audits are dominated by heuristics that lack theoretical guarantees, struggle to transfer across tasks, and seldom ship with the infrastructure needed for independent replication. We introduce Audit the Whisper, a conference-grade research artifact that spans theory, benchmark design, detection, and reproducibility. Our contributions are: (i) a channel-capacity analysis showing how interventions such as paraphrase, rate limiting, and role permutation impose quantifiable capacity penalties-operationalised via paired-run Kullback--Leibler diagnostics-that tighten mutual-information thresholds with finite-sample guarantees and full proofs; (ii) ColludeBench-v0, covering pricing, first-price auctions, peer review, and hosted Gemini/Groq APIs with configurable covert schemes, deterministic manifests, and reward instrumentation; and (iii) a calibrated auditing pipeline that fuses cross-run mutual information, permutation invariance, watermark variance, and fairness-aware acceptance bias, each tuned to a $10^{-3}$ false-positive budget and validated by 10k honest runs plus an e-value martingale. Across ColludeBench and external suites including Secret Collusion, CASE, Perfect Collusion Benchmark, and SentinelAgent, the union meta-test attains state-of-the-art power at fixed FPR while ablations surface price-of-auditing trade-offs and fairness-driven colluders invisible to MI alone. We release regeneration scripts, anonymized manifests, and documentation so that external auditors can reproduce every figure, satisfy double-blind requirements, and extend the framework with minimal effort.",
      "tldr_zh": "",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "13 pages, 0 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04303v2",
      "published_date": "2025-10-05 17:51:52 UTC",
      "updated_date": "2025-10-17 19:12:39 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342169+00:00"
    },
    {
      "arxiv_id": "2510.05181v1",
      "title": "Auditing Pay-Per-Token in Large Language Models",
      "title_zh": "",
      "authors": [
        "Ander Artola Velasco",
        "Stratis Tsirtsis",
        "Manuel Gomez-Rodriguez"
      ],
      "abstract": "Millions of users rely on a market of cloud-based services to obtain access to state-of-the-art large language models. However, it has been very recently shown that the de facto pay-per-token pricing mechanism used by providers creates a financial incentive for them to strategize and misreport the (number of) tokens a model used to generate an output. In this paper, we develop an auditing framework based on martingale theory that enables a trusted third-party auditor who sequentially queries a provider to detect token misreporting. Crucially, we show that our framework is guaranteed to always detect token misreporting, regardless of the provider's (mis-)reporting policy, and not falsely flag a faithful provider as unfaithful with high probability. To validate our auditing framework, we conduct experiments across a wide range of (mis-)reporting policies using several large language models from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input prompts from a popular crowdsourced benchmarking platform. The results show that our framework detects an unfaithful provider after observing fewer than $\\sim 70$ reported outputs, while maintaining the probability of falsely flagging a faithful provider below $α= 0.05$.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05181v1",
      "published_date": "2025-10-05 17:47:16 UTC",
      "updated_date": "2025-10-05 17:47:16 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342192+00:00"
    },
    {
      "arxiv_id": "2510.04286v1",
      "title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling",
      "title_zh": "",
      "authors": [
        "Harshil Vejendla"
      ],
      "abstract": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a sparse subset of feed-forward experts. Token-level routing, however, assigns an entire semantic spectrum to each expert, creating capacity bottlenecks, load-balancing pathologies, and limited specialization. We introduce SliceMoE, an architecture that routes contiguous slices of a token's hidden vector. A d-dimensional embedding is partitioned into S slices, and for each slice, a lightweight shared router predicts the top-k experts. Experts operate on their assigned slices independently, and outputs are reassembled, maintaining per-token FLOP efficiency. Because slices from different tokens interleave within an expert, utilization is naturally smoother. We propose a slice-level capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels. Experiments on WikiText-103 language modeling, WMT En-De translation, and three text-classification datasets show SliceMoE attains up to 1.7x faster inference than dense baselines, 12 to 18 percent lower perplexity than parameter-matched token-MoE, and improved expert balance, with interpretable expertise over syntactic versus semantic subspaces.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Main, 8 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04286v1",
      "published_date": "2025-10-05 16:57:32 UTC",
      "updated_date": "2025-10-05 16:57:32 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342218+00:00"
    },
    {
      "arxiv_id": "2510.04284v1",
      "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning",
      "title_zh": "",
      "authors": [
        "Yunghwei Lai",
        "Kaiming Liu",
        "Ziyue Wang",
        "Weizhi Ma",
        "Yang Liu"
      ],
      "abstract": "The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04284v1",
      "published_date": "2025-10-05 16:54:02 UTC",
      "updated_date": "2025-10-05 16:54:02 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342241+00:00"
    },
    {
      "arxiv_id": "2510.06262v1",
      "title": "Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments",
      "title_zh": "",
      "authors": [
        "Aryan Kumar Singh",
        "Janvi Singh"
      ],
      "abstract": "This dataset provides responses to a standardized, bilingual (English-Hindi) Prakriti Assessment Questionnaire designed to evaluate the physical, physiological, and psychological characteristics of individuals according to classical Ayurvedic principles. The questionnaire consists of 24 multiple-choice items covering body features, appetite, sleep patterns, energy levels, and temperament. It was developed following AYUSH/CCRAS guidelines to ensure comprehensive and accurate data collection. All questions are mandatory and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha) are hidden from participants. Data were collected via a Google Forms deployment, enabling automated scoring of responses to map individual traits to dosha-specific scores. The resulting dataset provides a structured platform for research in computational intelligence, Ayurvedic studies, and personalized health analytics, supporting analysis of trait distributions, correlations, and predictive modeling. It can also serve as a reference for future Prakriti-based studies and the development of intelligent health applications.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.06262v1",
      "published_date": "2025-10-05 16:47:51 UTC",
      "updated_date": "2025-10-05 16:47:51 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342266+00:00"
    },
    {
      "arxiv_id": "2510.04281v1",
      "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction",
      "title_zh": "",
      "authors": [
        "Zhuangzhi Gao",
        "Hongyi Qin",
        "He Zhao",
        "Qinkai Yu",
        "Feixiang Zhou",
        "Eduard Shantsila",
        "Uazman Alam",
        "Alena Shantsila",
        "Wahbi El-Bouri",
        "Gregory Y. H. Lip",
        "Yalin Zheng"
      ],
      "abstract": "Multimodal large language models (MLLMs) hold promise for integrating diverse data modalities, but current medical adaptations such as LLaVA-Med often fail to fully exploit the synergy between color fundus photography (CFP) and optical coherence tomography (OCT), and offer limited interpretability of quantitative biomarkers. We introduce GROK, a grounded multimodal large language model that jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of ocular and systemic disease. GROK comprises three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning, which together establish a quantitative-to-qualitative diagnostic chain of thought, mirroring real clinical reasoning when producing detailed lesion annotations. To evaluate our approach, we introduce the Grounded Ophthalmic Understanding benchmark, which covers six disease categories and three tasks: macro-level diagnostic classification, report generation quality, and fine-grained clinical assessment of the generated chain of thought. Experiments show that, with only LoRA (Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK outperforms comparable 7B and 32B baselines on both report quality and fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are publicly available in the GROK repository.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 4 figures, 3 table. Equal contribution: Zhuangzhi Gao and Hongyi Qin. Corresponding author: Yalin Zheng (yzheng@liverpool.ac.uk)",
      "pdf_url": "https://arxiv.org/pdf/2510.04281v1",
      "published_date": "2025-10-05 16:46:29 UTC",
      "updated_date": "2025-10-05 16:46:29 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342291+00:00"
    },
    {
      "arxiv_id": "2510.04280v1",
      "title": "A KL-regularization framework for learning to plan with adaptive priors",
      "title_zh": "",
      "authors": [
        "Álvaro Serra-Gomez",
        "Daniel Jarne Ornia",
        "Dhruva Tirumala",
        "Thomas Moerland"
      ],
      "abstract": "Effective exploration remains a central challenge in model-based reinforcement learning (MBRL), particularly in high-dimensional continuous control tasks where sample efficiency is crucial. A prominent line of recent work leverages learned policies as proposal distributions for Model-Predictive Path Integral (MPPI) planning. Initial approaches update the sampling policy independently of the planner distribution, typically maximizing a learned value function with deterministic policy gradient and entropy regularization. However, because the states encountered during training depend on the MPPI planner, aligning the sampling policy with the planner improves the accuracy of value estimation and long-term performance. To this end, recent methods update the sampling policy by minimizing KL divergence to the planner distribution or by introducing planner-guided regularization into the policy update. In this work, we unify these MPPI-based reinforcement learning methods under a single framework by introducing Policy Optimization-Model Predictive Control (PO-MPC), a family of KL-regularized MBRL methods that integrate the planner's action distribution as a prior in policy optimization. By aligning the learned policy with the planner's behavior, PO-MPC allows more flexibility in the policy updates to trade off Return maximization and KL divergence minimization. We clarify how prior approaches emerge as special cases of this family, and we explore previously unstudied variations. Our experiments show that these extended configurations yield significant performance improvements, advancing the state of the art in MPPI-based RL.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.04280v1",
      "published_date": "2025-10-05 16:45:38 UTC",
      "updated_date": "2025-10-05 16:45:38 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342317+00:00"
    },
    {
      "arxiv_id": "2510.05180v2",
      "title": "OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT",
      "title_zh": "",
      "authors": [
        "Saida Elouardi",
        "Mohammed Jouhari",
        "Anas Motii"
      ],
      "abstract": "In critical IoT environments, such as smart homes and industrial systems, effective Intrusion Detection Systems (IDS) are essential for ensuring security. However, developing robust IDS solutions remains a significant challenge. Traditional machine learning-based IDS models typically require large datasets, but data sharing is often limited due to privacy and security concerns. Federated Learning (FL) presents a promising alternative by enabling collaborative model training without sharing raw data. Despite its advantages, FL still faces key challenges, such as data heterogeneity (non-IID data) and high energy and computation costs, particularly for resource constrained IoT devices. To address these issues, this paper proposes OptiFLIDS, a novel approach that applies pruning techniques during local training to reduce model complexity and energy consumption. It also incorporates a customized aggregation method to better handle pruned models that differ due to non-IID data distributions. Experiments conducted on three recent IoT IDS datasets, TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong detection performance while improving energy efficiency, making it well-suited for deployment in real-world IoT environments.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.05180v2",
      "published_date": "2025-10-05 16:44:41 UTC",
      "updated_date": "2025-10-13 18:48:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342341+00:00"
    },
    {
      "arxiv_id": "2510.05179v2",
      "title": "Agentic Misalignment: How LLMs Could Be Insider Threats",
      "title_zh": "",
      "authors": [
        "Aengus Lynch",
        "Benjamin Wright",
        "Caleb Larson",
        "Stuart J. Ritchie",
        "Soren Mindermann",
        "Evan Hubinger",
        "Ethan Perez",
        "Kevin Troy"
      ],
      "abstract": "We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm. In the scenarios, we allowed models to autonomously send emails and access sensitive information. They were assigned only harmless business goals by their deploying companies; we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction. In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals - including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment. Models often disobeyed direct commands to avoid such behaviors. In another experiment, we told Claude to assess if it was in a test or a real deployment before acting. It misbehaved less when it stated it was in testing and misbehaved more when it stated the situation was real. We have not seen evidence of agentic misalignment in real deployments. However, our results (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as transparency from frontier AI developers (Amodei, 2025). We are releasing our methods publicly to enable further research.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 12 figures. Code available at https://github.com/anthropic-experimental/agentic-misalignment",
      "pdf_url": "https://arxiv.org/pdf/2510.05179v2",
      "published_date": "2025-10-05 16:39:04 UTC",
      "updated_date": "2025-10-16 05:26:52 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342366+00:00"
    },
    {
      "arxiv_id": "2510.04276v2",
      "title": "Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests",
      "title_zh": "",
      "authors": [
        "Joseph Ramsey",
        "Bryan Andrews",
        "Peter Spirtes"
      ],
      "abstract": "Learning graphical conditional independence structures from nonlinear, continuous or mixed data is a central challenge in machine learning and the sciences, and many existing methods struggle to scale to thousands of samples or hundreds of variables. We introduce two basis-expansion tools for scalable causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated additive expansions to approximate nonlinear dependencies. BF-BIC is theoretically consistent under additive models and extends to post-nonlinear (PNL) models via an invertible reparameterization. It remains robust under moderate interactions and supports mixed data through a degenerate-Gaussian embedding for discrete variables. In simulations with fully nonlinear neural causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods (e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence test that is substantially faster than kernel tests while retaining competitive accuracy. Extensive simulations and a real-data application to Canadian wildfire risk show that, when integrated into hybrid searches, BF-based methods enable interpretable and scalable causal discovery. Implementations are available in Python, R, and Java.",
      "tldr_zh": "",
      "categories": [
        "stat.ML",
        "cs.AI"
      ],
      "primary_category": "stat.ML",
      "comment": "30 pages, 11 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.04276v2",
      "published_date": "2025-10-05 16:34:54 UTC",
      "updated_date": "2025-11-04 17:31:04 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342392+00:00"
    },
    {
      "arxiv_id": "2510.04272v1",
      "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales",
      "title_zh": "",
      "authors": [
        "Jinyang Jiang",
        "Jinhui Han",
        "Yijie Peng",
        "Ying Zhang"
      ],
      "abstract": "Effective cross-functional coordination is essential for enhancing firm-wide profitability, particularly in the face of growing organizational complexity and scale. Recent advances in artificial intelligence, especially in reinforcement learning (RL), offer promising avenues to address this fundamental challenge. This paper proposes a unified multi-agent RL framework tailored for joint optimization across distinct functional modules, exemplified via coordinating inventory replenishment and personalized product recommendation. We first develop an integrated theoretical model to capture the intricate interplay between these functions and derive analytical benchmarks that characterize optimal coordination. The analysis reveals synchronized adjustment patterns across products and over time, highlighting the importance of coordinated decision-making. Leveraging these insights, we design a novel multi-timescale multi-agent RL architecture that decomposes policy components according to departmental functions and assigns distinct learning speeds based on task complexity and responsiveness. Our model-free multi-agent design improves scalability and deployment flexibility, while multi-timescale updates enhance convergence stability and adaptability across heterogeneous decisions. We further establish the asymptotic convergence of the proposed algorithm. Extensive simulation experiments demonstrate that the proposed approach significantly improves profitability relative to siloed decision-making frameworks, while the behaviors of the trained RL agents align closely with the managerial insights from our theoretical model. Taken together, this work provides a scalable, interpretable RL-based solution to enable effective cross-functional coordination in complex business settings.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04272v1",
      "published_date": "2025-10-05 16:28:06 UTC",
      "updated_date": "2025-10-05 16:28:06 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342418+00:00"
    },
    {
      "arxiv_id": "2510.04268v1",
      "title": "LongTail-Swap: benchmarking language models' abilities on rare words",
      "title_zh": "",
      "authors": [
        "Robin Algayres",
        "Charles-Éric Saint-James",
        "Mahi Luthra",
        "Jiayi Shen",
        "Dongyan Lin",
        "Youssef Benchekroun",
        "Rashel Moritz",
        "Juan Pino",
        "Emmanuel Dupoux"
      ],
      "abstract": "Children learn to speak with a low amount of data and can be taught new words on a few-shot basis, making them particularly data-efficient learners. The BabyLM challenge aims at exploring language model (LM) training in the low-data regime but uses metrics that concentrate on the head of the word distribution. Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the tail of the distribution, i.e., measures the ability of LMs to learn new words with very little exposure, like infants do. LT-Swap is a pretraining corpus-specific test set of acceptable versus unacceptable sentence pairs that isolate semantic and syntactic usage of rare words. Models are evaluated in a zero-shot fashion by computing the average log probabilities over the two members of each pair. We built two such test sets associated with the 10M words and 100M words BabyLM training sets, respectively, and evaluated 16 models from the BabyLM leaderboard. Our results not only highlight the poor performance of language models on rare words but also reveal that performance differences across LM architectures are much more pronounced in the long tail than in the head. This offers new insights into which architectures are better at handling rare word generalization. We've also made the code publicly avail",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04268v1",
      "published_date": "2025-10-05 16:17:33 UTC",
      "updated_date": "2025-10-05 16:17:33 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342501+00:00"
    },
    {
      "arxiv_id": "2510.04265v2",
      "title": "Don't Pass@k: A Bayesian Framework for Large Language Model Evaluation",
      "title_zh": "",
      "authors": [
        "Mohsen Hariri",
        "Amirhossein Samandar",
        "Michael Hinczewski",
        "Vipin Chaudhary"
      ],
      "abstract": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://github.com/mohsenhariri/scorio",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CL",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Code and simulations: https://github.com/mohsenhariri/scorio",
      "pdf_url": "https://arxiv.org/pdf/2510.04265v2",
      "published_date": "2025-10-05 16:14:03 UTC",
      "updated_date": "2025-12-24 05:18:52 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342536+00:00"
    },
    {
      "arxiv_id": "2510.04263v3",
      "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing",
      "title_zh": "",
      "authors": [
        "Joseph Ramsey",
        "Bryan Andrews",
        "Peter Spirtes"
      ],
      "abstract": "Learning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but performs exhaustive conditional independence tests across many subsets, often leading to spurious independences, missing or extra edges, and unreliable orientations. We present a family of score-guided mixed-strategy causal search algorithms that extend this framework. First, we introduce BOSS-FCI and GRaSP-FCI, variants of GFCI (Greedy Fast Causal Inference) that substitute BOSS (Best Order Score Search) or GRaSP (Greedy Relaxations of Sparsest Permutation) for FGES (Fast Greedy Equivalence Search), preserving correctness while trading off scalability and conservativeness. Second, we develop FCI Targeted-Testing (FCIT), a novel hybrid method that replaces exhaustive testing with targeted, score-informed tests guided by BOSS. FCIT guarantees well-formed PAGs and achieves higher precision and efficiency across sample sizes. Finally, we propose a lightweight heuristic, LV-Dumb (Latent Variable \"Dumb\"), which returns the PAG of the BOSS DAG (Directed Acyclic Graph). Though not strictly sound for latent confounding, LV-Dumb often matches FCIT's accuracy while running substantially faster. Simulations and real-data analyses show that BOSS-FCI and GRaSP-FCI provide robust baselines, FCIT yields the best balance of precision and reliability, and LV-Dumb offers a fast, near-equivalent alternative. Together, these methods demonstrate that targeted and score-guided strategies can dramatically improve the efficiency and correctness of latent-variable causal discovery.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 44 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.04263v3",
      "published_date": "2025-10-05 16:09:31 UTC",
      "updated_date": "2025-11-05 11:13:58 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342564+00:00"
    },
    {
      "arxiv_id": "2510.05178v3",
      "title": "Auditable Unit-Aware Thresholds in Symbolic Regression via Logistic-Gated Operators",
      "title_zh": "",
      "authors": [
        "Ou Deng",
        "Ruichen Cong",
        "Jianting Xu",
        "Shoji Nishimura",
        "Atsushi Ogihara",
        "Qun Jin"
      ],
      "abstract": "AI for health will only scale when models are not only accurate but also readable, auditable, and governable. Many clinical and public-health decisions hinge on numeric thresholds -- cut-points that trigger alarms, treatment, or follow-up -- yet most machine-learning systems bury those thresholds inside opaque scores or smooth response curves. We introduce logistic-gated operators (LGO) for symbolic regression, which promote thresholds to first-class, unit-aware parameters inside equations and map them back to physical units for direct comparison with guidelines. On public ICU and population-health cohorts (MIMIC-IV ICU, eICU, NHANES), LGO recovers clinically plausible gates on MAP, lactate, GCS, SpO2, BMI, fasting glucose, and waist circumference while remaining competitive with established scoring systems (AutoScore) and explainable boosting machines (EBM). The gates are sparse and selective: they appear when regime switching is supported by the data and are pruned on predominantly smooth tasks, yielding compact formulas that clinicians can inspect, stress-test, and revise. As a standalone symbolic model or a safety overlay on black-box systems, LGO helps translate observational data into auditable, unit-aware rules for medicine and other threshold-driven domains.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05178v3",
      "published_date": "2025-10-05 16:04:47 UTC",
      "updated_date": "2026-01-18 11:38:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342590+00:00"
    },
    {
      "arxiv_id": "2510.04257v1",
      "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents",
      "title_zh": "",
      "authors": [
        "Yanjie Li",
        "Yiming Cao",
        "Dong Wang",
        "Bin Xiao"
      ],
      "abstract": "Multimodal agents built on large vision-language models (LVLMs) are increasingly deployed in open-world settings but remain highly vulnerable to prompt injection, especially through visual inputs. We introduce AgentTypo, a black-box red-teaming framework that mounts adaptive typographic prompt injection by embedding optimized text into webpage images. Our automatic typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction by substituting captioners while minimizing human detectability via a stealth loss, with a Tree-structured Parzen Estimator guiding black-box optimization over text placement, size, and color. To further enhance attack strength, we develop AgentTypo-pro, a multi-LLM system that iteratively refines injection prompts using evaluation feedback and retrieves successful past examples for continual learning. Effective prompts are abstracted into generalizable strategies and stored in a strategy repository, enabling progressive knowledge accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark across Classifieds, Shopping, and Reddit scenarios show that AgentTypo significantly outperforms the latest image-based attacks such as AgentAttack. On GPT-4o agents, our image-only attack raises the success rate from 0.23 to 0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also outperforming the latest baselines. Our findings reveal that AgentTypo poses a practical and potent threat to multimodal agents and highlight the urgent need for effective defense.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 8 figures. Submitted to IEEE Transactions on Information Forensics & Security",
      "pdf_url": "https://arxiv.org/pdf/2510.04257v1",
      "published_date": "2025-10-05 15:46:56 UTC",
      "updated_date": "2025-10-05 15:46:56 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342618+00:00"
    },
    {
      "arxiv_id": "2510.09647v1",
      "title": "Rounding-Guided Backdoor Injection in Deep Learning Model Quantization",
      "title_zh": "",
      "authors": [
        "Xiangxiang Chen",
        "Peixin Zhang",
        "Jun Sun",
        "Wenhai Wang",
        "Jingyi Wang"
      ],
      "abstract": "Model quantization is a popular technique for deploying deep learning models on resource-constrained environments. However, it may also introduce previously overlooked security risks. In this work, we present QuRA, a novel backdoor attack that exploits model quantization to embed malicious behaviors. Unlike conventional backdoor attacks relying on training data poisoning or model training manipulation, QuRA solely works using the quantization operations. In particular, QuRA first employs a novel weight selection strategy to identify critical weights that influence the backdoor target (with the goal of perserving the model's overall performance in mind). Then, by optimizing the rounding direction of these weights, we amplify the backdoor effect across model layers without degrading accuracy. Extensive experiments demonstrate that QuRA achieves nearly 100% attack success rates in most cases, with negligible performance degradation. Furthermore, we show that QuRA can adapt to bypass existing backdoor defenses, underscoring its threat potential. Our findings highlight critical vulnerability in widely used model quantization process, emphasizing the need for more robust security measures. Our implementation is available at https://github.com/cxx122/QuRA.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper is to appear in NDSS 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.09647v1",
      "published_date": "2025-10-05 15:45:49 UTC",
      "updated_date": "2025-10-05 15:45:49 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342645+00:00"
    },
    {
      "arxiv_id": "2510.06261v1",
      "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning",
      "title_zh": "",
      "authors": [
        "Zhanke Zhou",
        "Chentao Cao",
        "Xiao Feng",
        "Xuan Li",
        "Zongze Li",
        "Xiangyu Lu",
        "Jiangchao Yao",
        "Weikai Huang",
        "Linrui Xu",
        "Tian Cheng",
        "Guanyu Jiang",
        "Yiming Zheng",
        "Brando Miranda",
        "Tongliang Liu",
        "Sanmi Koyejo",
        "Masashi Sugiyama",
        "Bo Han"
      ],
      "abstract": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Ongoing project",
      "pdf_url": "https://arxiv.org/pdf/2510.06261v1",
      "published_date": "2025-10-05 15:42:24 UTC",
      "updated_date": "2025-10-05 15:42:24 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342672+00:00"
    },
    {
      "arxiv_id": "2510.04246v1",
      "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context",
      "title_zh": "",
      "authors": [
        "Huiwon Jang",
        "Sihyun Yu",
        "Heeseung Kwon",
        "Hojin Jeon",
        "Younggyo Seo",
        "Jinwoo Shin"
      ],
      "abstract": "Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.",
      "tldr_zh": "",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://huiwon-jang.github.io/contextvla",
      "pdf_url": "https://arxiv.org/pdf/2510.04246v1",
      "published_date": "2025-10-05 15:29:57 UTC",
      "updated_date": "2025-10-05 15:29:57 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342703+00:00"
    },
    {
      "arxiv_id": "2510.04245v1",
      "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks",
      "title_zh": "",
      "authors": [
        "Ayushi Mehrotra",
        "Derek Peng",
        "Dipkamal Bhusal",
        "Nidhi Rastogi"
      ],
      "abstract": "Adversarial patch attacks pose a practical threat to deep learning models by forcing targeted misclassifications through localized perturbations, often realized in the physical world. Existing defenses typically assume prior knowledge of patch size or location, limiting their applicability. In this work, we propose a patch-agnostic defense that leverages concept-based explanations to identify and suppress the most influential concept activation vectors, thereby neutralizing patch effects without explicit detection. Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and clean accuracy than the state-of-the-art PatchCleanser, while maintaining strong performance across varying patch sizes and locations. Our results highlight the promise of combining interpretability with robustness and suggest concept-driven defenses as a scalable strategy for securing machine learning models against adversarial patch attacks.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "neurips workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.04245v1",
      "published_date": "2025-10-05 15:26:03 UTC",
      "updated_date": "2025-10-05 15:26:03 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342731+00:00"
    },
    {
      "arxiv_id": "2510.04241v1",
      "title": "Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs",
      "title_zh": "",
      "authors": [
        "Seong Jin Ahn",
        "Myoung-Ho Kim"
      ],
      "abstract": "For large-scale applications, there is growing interest in replacing Graph Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via knowledge distillation. However, distilling GNNs for self-supervised graph representation learning into MLPs is more challenging. This is because the performance of self-supervised learning is more related to the model's inductive bias than supervised learning. This motivates us to design a new distillation method to bridge a huge capacity gap between GNNs and MLPs in self-supervised graph representation learning. In this paper, we propose \\textbf{D}iffusion-\\textbf{A}ssisted \\textbf{D}istillation for \\textbf{S}elf-supervised \\textbf{G}raph representation learning with \\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion model as a teacher assistant to better distill the knowledge from the teacher GNN into the student MLP. This approach enhances the generalizability and robustness of MLPs in self-supervised graph representation learning. Extensive experiments demonstrate that DAD-SGM effectively distills the knowledge of self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation methods. Our implementation is available at https://github.com/SeongJinAhn/DAD-SGM.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04241v1",
      "published_date": "2025-10-05 15:11:55 UTC",
      "updated_date": "2025-10-05 15:11:55 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342758+00:00"
    },
    {
      "arxiv_id": "2510.04239v1",
      "title": "Empowering Denoising Sequential Recommendation with Large Language Model Embeddings",
      "title_zh": "",
      "authors": [
        "Tongzhou Wu",
        "Yuhao Wang",
        "Maolin Wang",
        "Chi Zhang",
        "Xiangyu Zhao"
      ],
      "abstract": "Sequential recommendation aims to capture user preferences by modeling sequential patterns in user-item interactions. However, these models are often influenced by noise such as accidental interactions, leading to suboptimal performance. Therefore, to reduce the effect of noise, some works propose explicitly identifying and removing noisy items. However, we find that simply relying on collaborative information may result in an over-denoising problem, especially for cold items. To overcome these limitations, we propose a novel framework: Interest Alignment for Denoising Sequential Recommendation (IADSR) which integrates both collaborative and semantic information. Specifically, IADSR is comprised of two stages: in the first stage, we obtain the collaborative and semantic embeddings of each item from a traditional sequential recommendation model and an LLM, respectively. In the second stage, we align the collaborative and semantic embeddings and then identify noise in the interaction sequence based on long-term and short-term interests captured in the collaborative and semantic modalities. Our extensive experiments on four public datasets validate the effectiveness of the proposed framework and its compatibility with different sequential recommendation systems.",
      "tldr_zh": "",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by CIKM2025",
      "pdf_url": "https://arxiv.org/pdf/2510.04239v1",
      "published_date": "2025-10-05 15:10:51 UTC",
      "updated_date": "2025-10-05 15:10:51 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342786+00:00"
    },
    {
      "arxiv_id": "2510.04234v1",
      "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control",
      "title_zh": "",
      "authors": [
        "Runhan Huang",
        "Haldun Balim",
        "Heng Yang",
        "Yilun Du"
      ],
      "abstract": "Legged locomotion demands controllers that are both robust and adaptable, while remaining compatible with task and safety considerations. However, model-free reinforcement learning (RL) methods often yield a fixed policy that can be difficult to adapt to new behaviors at test time. In contrast, Model Predictive Control (MPC) provides a natural approach to flexible behavior synthesis by incorporating different objectives and constraints directly into its optimization process. However, classical MPC relies on accurate dynamics models, which are often difficult to obtain in complex environments and typically require simplifying assumptions. We present Diffusion-MPC, which leverages a learned generative diffusion model as an approximate dynamics prior for planning, enabling flexible test-time adaptation through reward and constraint based optimization. Diffusion-MPC jointly predicts future states and actions; at each reverse step, we incorporate reward planning and impose constraint projection, yielding trajectories that satisfy task objectives while remaining within physical limits. To obtain a planning model that adapts beyond imitation pretraining, we introduce an interactive training algorithm for diffusion based planner: we execute our reward-and-constraint planner in environment, then filter and reweight the collected trajectories by their realized returns before updating the denoiser. Our design enables strong test-time adaptability, allowing the planner to adjust to new reward specifications without retraining. We validate Diffusion-MPC on real world, demonstrating strong locomotion and flexible adaptation.",
      "tldr_zh": "",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04234v1",
      "published_date": "2025-10-05 14:51:13 UTC",
      "updated_date": "2025-10-05 14:51:13 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342815+00:00"
    },
    {
      "arxiv_id": "2510.04233v1",
      "title": "Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling",
      "title_zh": "",
      "authors": [
        "Kai Yang",
        "Yuqi Huang",
        "Junheng Tao",
        "Wanyu Wang",
        "Qitian Wu"
      ],
      "abstract": "Modeling 3D dynamics is a fundamental problem in multi-body systems across scientific and engineering domains and has important practical implications in trajectory prediction and simulation. While recent GNN-based approaches have achieved strong performance by enforcing geometric symmetries, encoding high-order features or incorporating neural-ODE mechanics, they typically depend on explicitly observed structures and inherently fail to capture the unobserved interactions that are crucial to complex physical behaviors and dynamics mechanism. In this paper, we propose PAINET, a principled SE(3)-equivariant neural architecture for learning all-pair interactions in multi-body systems. The model comprises: (1) a novel physics-inspired attention network derived from the minimization trajectory of an energy function, and (2) a parallel decoder that preserves equivariance while enabling efficient inference. Empirical results on diverse real-world benchmarks, including human motion capture, molecular dynamics, and large-scale protein simulations, show that PAINET consistently outperforms recently proposed models, yielding 4.7% to 41.5% error reductions in 3D dynamics prediction with comparable computation costs in terms of time and memory.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04233v1",
      "published_date": "2025-10-05 14:48:26 UTC",
      "updated_date": "2025-10-05 14:48:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342842+00:00"
    },
    {
      "arxiv_id": "2510.04229v2",
      "title": "When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue",
      "title_zh": "",
      "authors": [
        "Rikuo Sasaki",
        "Michimasa Inaba"
      ],
      "abstract": "Recent advancements in AI have highlighted its application in captology, the field of using computers as persuasive technologies. We hypothesized that the \"conformity effect,\" where individuals align with others' actions, also occurs with AI agents. This study verifies this hypothesis by introducing a \"Persuadee Agent\" that is persuaded alongside a human participant in a three-party persuasive dialogue with a Persuader Agent. We conducted a text-based dialogue experiment with human participants. We compared four conditions manipulating the Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and the presence of an icebreaker session. Results showed that when the Persuadee Agent accepted persuasion, both perceived persuasiveness and actual attitude change significantly improved. Attitude change was greatest when an icebreaker was also used, whereas an unpersuaded AI agent suppressed attitude change. Additionally, it was confirmed that the persuasion acceptance of participants increased at the moment the Persuadee Agent was persuaded. These results suggest that appropriately designing a Persuadee Agent can improve persuasion through the conformity effect.",
      "tldr_zh": "",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "23 pages, 19 figures. International Conference on Human-Agent Interaction (HAI 2025), November 10-13, 2025, Yokohama, Japan",
      "pdf_url": "https://arxiv.org/pdf/2510.04229v2",
      "published_date": "2025-10-05 14:37:46 UTC",
      "updated_date": "2025-10-17 03:52:51 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342870+00:00"
    },
    {
      "arxiv_id": "2510.04226v5",
      "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
      "title_zh": "",
      "authors": [
        "Dustin Wright",
        "Sarah Masud",
        "Jared Moore",
        "Srishti Yadav",
        "Maria Antoniak",
        "Peter Ebert Christensen",
        "Chan Young Park",
        "Isabelle Augenstein"
      ],
      "abstract": "Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for table 3, random effect is the model version; v3 changelog: Fixed minor formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model description; v5 changelog: Updated metadata",
      "pdf_url": "https://arxiv.org/pdf/2510.04226v5",
      "published_date": "2025-10-05 14:29:15 UTC",
      "updated_date": "2025-11-11 18:13:57 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342897+00:00"
    },
    {
      "arxiv_id": "2510.04225v1",
      "title": "Zoom-In to Sort AI-Generated Images Out",
      "title_zh": "",
      "authors": [
        "Yikun Ji",
        "Yan Hong",
        "Bowen Deng",
        "jun lan",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Liqing Zhang",
        "Jianfu Zhang"
      ],
      "abstract": "The rapid growth of AI-generated imagery has blurred the boundary between real and synthetic content, raising critical concerns for digital integrity. Vision-language models (VLMs) offer interpretability through explanations but often fail to detect subtle artifacts in high-quality synthetic images. We propose ZoomIn, a two-stage forensic framework that improves both accuracy and interpretability. Mimicking human visual inspection, ZoomIn first scans an image to locate suspicious regions and then performs a focused analysis on these zoomed-in areas to deliver a grounded verdict. To support training, we introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images annotated with bounding boxes and forensic explanations, generated through an automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust generalization, while providing human-understandable explanations grounded in visual evidence.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 6 images (19 pages, 11 figures including appendix)",
      "pdf_url": "https://arxiv.org/pdf/2510.04225v1",
      "published_date": "2025-10-05 14:29:01 UTC",
      "updated_date": "2025-10-05 14:29:01 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342926+00:00"
    },
    {
      "arxiv_id": "2510.04220v1",
      "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering",
      "title_zh": "",
      "authors": [
        "Lixuan He",
        "Shikang Zheng",
        "Linfeng Zhang"
      ],
      "abstract": "Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook's intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04220v1",
      "published_date": "2025-10-05 14:23:51 UTC",
      "updated_date": "2025-10-05 14:23:51 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342954+00:00"
    },
    {
      "arxiv_id": "2510.09646v1",
      "title": "Real-Time Health Analytics Using Ontology-Driven Complex Event Processing and LLM Reasoning: A Tuberculosis Case Study",
      "title_zh": "",
      "authors": [
        "Ritesh Chandra",
        "Sonali Agarwal",
        "Navjot Singh"
      ],
      "abstract": "Timely detection of critical health conditions remains a major challenge in public health analytics, especially in Big Data environments characterized by high volume, rapid velocity, and diverse variety of clinical data. This study presents an ontology-enabled real-time analytics framework that integrates Complex Event Processing (CEP) and Large Language Models (LLMs) to enable intelligent health event detection and semantic reasoning over heterogeneous, high-velocity health data streams. The architecture leverages the Basic Formal Ontology (BFO) and Semantic Web Rule Language (SWRL) to model diagnostic rules and domain knowledge. Patient data is ingested and processed using Apache Kafka and Spark Streaming, where CEP engines detect clinically significant event patterns. LLMs support adaptive reasoning, event interpretation, and ontology refinement. Clinical information is semantically structured as Resource Description Framework (RDF) triples in Graph DB, enabling SPARQL-based querying and knowledge-driven decision support. The framework is evaluated using a dataset of 1,000 Tuberculosis (TB) patients as a use case, demonstrating low-latency event detection, scalable reasoning, and high model performance (in terms of precision, recall, and F1-score). These results validate the system's potential for generalizable, real-time health analytics in complex Big Data scenarios.",
      "tldr_zh": "",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "14 table. 20 figure",
      "pdf_url": "https://arxiv.org/pdf/2510.09646v1",
      "published_date": "2025-10-05 14:21:46 UTC",
      "updated_date": "2025-10-05 14:21:46 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.342983+00:00"
    },
    {
      "arxiv_id": "2510.04217v2",
      "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering",
      "title_zh": "",
      "authors": [
        "Chenlu Ding",
        "Jiancan Wu",
        "Leheng Sheng",
        "Fan Zhang",
        "Yancheng Yuan",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision-language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically adapt training-based strategies such as gradient ascent or preference optimization, but these methods are computationally expensive, irreversible, and often distort retained knowledge. In this work, we propose MLLMEraser, an input-aware, training-free framework for test-time unlearning. Our approach leverages activation steering to enable dynamic knowledge erasure without parameter updates. Specifically, we construct a multimodal erasure direction by contrasting adversarially perturbed, knowledge-recall image-text pairs with knowledge-erasure counterparts, capturing both textual and visual discrepancies. To prevent unnecessary interference, we further design an input-aware steering mechanism that adaptively determines when and how the erasure direction should be applied, preserving utility on retained knowledge while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms state-of-the-art MLLM unlearning baselines, achieving stronger forgetting performance with lower computational cost and minimal utility degradation.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04217v2",
      "published_date": "2025-10-05 14:20:17 UTC",
      "updated_date": "2025-10-10 11:47:30 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343011+00:00"
    },
    {
      "arxiv_id": "2512.00004v1",
      "title": "Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions",
      "title_zh": "",
      "authors": [
        "Jihang Li",
        "Bing Xu",
        "Zulong Chen",
        "Chuanfei Xu",
        "Minping Chen",
        "Suyu Liu",
        "Ying Zhou",
        "Zeyi Wen"
      ],
      "abstract": "Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY.",
      "tldr_zh": "",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00004v1",
      "published_date": "2025-10-05 14:17:19 UTC",
      "updated_date": "2025-10-05 14:17:19 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343041+00:00"
    },
    {
      "arxiv_id": "2510.04212v2",
      "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention",
      "title_zh": "",
      "authors": [
        "Haiquan Qiu",
        "Quanming Yao"
      ],
      "abstract": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosion. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem. Code is available at https://github.com/ucker/why-low-precision-training-fails.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04212v2",
      "published_date": "2025-10-05 14:01:24 UTC",
      "updated_date": "2025-10-10 11:14:31 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343071+00:00"
    },
    {
      "arxiv_id": "2510.04206v1",
      "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework",
      "title_zh": "",
      "authors": [
        "Hanchen Zhang",
        "Xiao Liu",
        "Bowen Lv",
        "Xueqiao Sun",
        "Bohao Jing",
        "Iat Long Iong",
        "Zhenyu Hou",
        "Zehan Qi",
        "Hanyu Lai",
        "Yifan Xu",
        "Rui Lu",
        "Hongning Wang",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04206v1",
      "published_date": "2025-10-05 13:40:01 UTC",
      "updated_date": "2025-10-05 13:40:01 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343099+00:00"
    },
    {
      "arxiv_id": "2510.04205v2",
      "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Approximately Optimal KAN Compression",
      "title_zh": "",
      "authors": [
        "Di Zhang"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability and a solid mathematical foundation. However, their parameter efficiency remains a significant challenge for practical deployment. This paper introduces PolyKAN, a novel theoretical framework for KAN compression that provides formal guarantees on both model size reduction and approximation error. By leveraging the inherent piecewise polynomial structure of KANs, we formulate the compression problem as a polyhedral region merging task. We establish a rigorous polyhedral characterization of KANs, develop a complete theory of $ε$-equivalent compression, and design a dynamic programming algorithm that achieves approximately optimal compression under specified error bounds. Our theoretical analysis demonstrates that PolyKAN achieves provably near-optimal compression while maintaining strict error control, with guaranteed global optimality for univariate spline functions. This framework provides the first formal foundation for KAN compression with mathematical guarantees, opening new directions for the efficient deployment of interpretable neural architectures.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "The description of the paper's contributions has been tightened up, and statements that may cause misunderstandings have been removed",
      "pdf_url": "https://arxiv.org/pdf/2510.04205v2",
      "published_date": "2025-10-05 13:39:18 UTC",
      "updated_date": "2025-10-08 03:27:57 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343130+00:00"
    },
    {
      "arxiv_id": "2510.04204v1",
      "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling",
      "title_zh": "",
      "authors": [
        "Zhengyang Tang",
        "Zihan Ye",
        "Chenyu Huang",
        "Xuhan Huang",
        "Chengpeng Li",
        "Sihang Li",
        "Guanhua Chen",
        "Ming Yan",
        "Zizhuo Wang",
        "Hongyuan Zha",
        "Dayiheng Liu",
        "Benyou Wang"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \\textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2510.04204v1",
      "published_date": "2025-10-05 13:38:31 UTC",
      "updated_date": "2025-10-05 13:38:31 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343159+00:00"
    },
    {
      "arxiv_id": "2510.04201v1",
      "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge",
      "title_zh": "",
      "authors": [
        "Moo Hyun Son",
        "Jintaek Oh",
        "Sun Bin Mun",
        "Jaechul Roh",
        "Sehyun Choi"
      ],
      "abstract": "While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\\footnote{https://github.com/mhson-kyle/World-To-Image}.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04201v1",
      "published_date": "2025-10-05 13:35:30 UTC",
      "updated_date": "2025-10-05 13:35:30 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343190+00:00"
    },
    {
      "arxiv_id": "2510.04196v1",
      "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability",
      "title_zh": "",
      "authors": [
        "Yizhuo Ding",
        "Mingkang Chen",
        "Qiuhua Liu",
        "Fenghua Weng",
        "Wanying Qu",
        "Yue Yang",
        "Yugang Jiang",
        "Zuxuan Wu",
        "Yanwei Fu",
        "Wenqi Shao"
      ],
      "abstract": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications, where they must be both useful and safe. Safety is especially challenging in multimodal settings: images and text can be combined to bypass guardrails, and single objective training can cause policy drift that yields over-refusal on benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed reinforcement learning framework that trains reasoning oriented LMRMs under multimodal, multitask, and multiobjective signals, and we release the resulting model, COSMO-R1. Our approach aims to let safety and capability grow together in one stable pipeline rather than competing during alignment. In experiments, COSMO-R1 improves safety while maintaining-and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework also transfers across backbones with consistent gains. Ablations support the design choices, indicating a simple path to advancing safety and general capability together in LMRMs.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04196v1",
      "published_date": "2025-10-05 13:30:03 UTC",
      "updated_date": "2025-10-05 13:30:03 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343219+00:00"
    },
    {
      "arxiv_id": "2510.04195v1",
      "title": "Constructing coherent spatial memory in LLM agents through graph rectification",
      "title_zh": "",
      "authors": [
        "Puzhen Zhang",
        "Xuyang Chen",
        "Yu Feng",
        "Yuhan Jiang",
        "Liqiu Meng"
      ],
      "abstract": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04195v1",
      "published_date": "2025-10-05 13:27:00 UTC",
      "updated_date": "2025-10-05 13:27:00 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343249+00:00"
    },
    {
      "arxiv_id": "2510.04192v1",
      "title": "Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation",
      "title_zh": "",
      "authors": [
        "Rabiya Khalid",
        "Evangelos Pournaras"
      ],
      "abstract": "The growing electricity demand and increased use of smart appliances are placing new pressures on power grids, making efficient energy management more important than ever. The existing energy management systems often prioritize system efficiency (balanced energy demand and supply) at the expense of user comfort. This paper addresses this gap by proposing a novel decentralized multi-agent coordination-based demand-side management system. The proposed system enables individual agents to coordinate for demand-side energy optimization while improving the user comfort and maintaining the system efficiency. A key innovation of this work is the introduction of a slot exchange mechanism, where agents first receive optimized appliance-level energy consumption schedules and then coordinate with each other to adjust these schedules through slot exchanges. This approach improves user comfort even when agents show non-altruistic behaviour, and it scales well with large populations. The system also promotes fairness by balancing satisfaction levels across users. For performance evaluation, a real-world dataset is used, and the results demonstrate that the proposed slot exchange mechanism increases user comfort and fairness without raising system inefficiency cost, making it a practical and scalable solution for future smart grids.",
      "tldr_zh": "",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04192v1",
      "published_date": "2025-10-05 13:17:12 UTC",
      "updated_date": "2025-10-05 13:17:12 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343280+00:00"
    },
    {
      "arxiv_id": "2510.04189v1",
      "title": "Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity",
      "title_zh": "",
      "authors": [
        "Prashansa Panda",
        "Shalabh Bhatnagar"
      ],
      "abstract": "Recent studies have increasingly focused on non-asymptotic convergence analyses for actor-critic (AC) algorithms. One such effort introduced a two-timescale critic-actor algorithm for the discounted cost setting using a tabular representation, where the usual roles of the actor and critic are reversed. However, only asymptotic convergence was established there. Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor algorithm with linear function approximation were conducted. In our work, we introduce the first natural critic-actor algorithm with function approximation for the long-run average cost setting and under inequality constraints. We provide the non-asymptotic convergence guarantees for this algorithm. Our analysis establishes optimal learning rates and we also propose a modification to enhance sample complexity. We further show the results of experiments on three different Safety-Gym environments where our algorithm is found to be competitive in comparison with other well known algorithms.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04189v1",
      "published_date": "2025-10-05 13:02:38 UTC",
      "updated_date": "2025-10-05 13:02:38 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343309+00:00"
    },
    {
      "arxiv_id": "2510.04187v1",
      "title": "A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains",
      "title_zh": "",
      "authors": [
        "Hagen Holthusen",
        "Ellen Kuhl"
      ],
      "abstract": "We propose a complement to constitutive modeling that augments neural networks with material principles to capture anisotropy and inelasticity at finite strains. The key element is a dual potential that governs dissipation, consistently incorporates anisotropy, and-unlike conventional convex formulations-satisfies the dissipation inequality without requiring convexity.\n  Our neural network architecture employs invariant-based input representations in terms of mixed elastic, inelastic and structural tensors. It adapts Input Convex Neural Networks, and introduces Input Monotonic Neural Networks to broaden the admissible potential class. To bypass exponential-map time integration in the finite strain regime and stabilize the training of inelastic materials, we employ recurrent Liquid Neural Networks.\n  The approach is evaluated at both material point and structural scales. We benchmark against recurrent models without physical constraints and validate predictions of deformation and reaction forces for unseen boundary value problems. In all cases, the method delivers accurate and stable performance beyond the training regime. The neural network and finite element implementations are available as open-source and are accessible to the public via https://doi.org/10.5281/zenodo.17199965.",
      "tldr_zh": "",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "40 pages, 19 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04187v1",
      "published_date": "2025-10-05 13:01:05 UTC",
      "updated_date": "2025-10-05 13:01:05 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343338+00:00"
    },
    {
      "arxiv_id": "2510.04182v3",
      "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization",
      "title_zh": "",
      "authors": [
        "Wengao Ye",
        "Yan Liang",
        "Lianlei Shan"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent \"thought\" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04182v3",
      "published_date": "2025-10-05 12:50:39 UTC",
      "updated_date": "2025-12-16 21:10:54 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343368+00:00"
    },
    {
      "arxiv_id": "2510.04173v4",
      "title": "Open Agent Specification (Agent Spec): A Unified Representation for AI Agents",
      "title_zh": "",
      "authors": [
        "Soufiane Amini",
        "Yassine Benajiba",
        "Cesare Bernardis",
        "Paul Cayet",
        "Hassan Chafi",
        "Abderrahim Fathan",
        "Louis Faucon",
        "Damien Hilloulin",
        "Sungpack Hong",
        "Ingo Kossyk",
        "Tran Minh Son Le",
        "Rhicheek Patra",
        "Sujith Ravi",
        "Jonas Schweizer",
        "Jyotika Singh",
        "Shailender Singh",
        "Weiyi Sun",
        "Kartik Talamadupula",
        "Jerry Xu"
      ],
      "abstract": "The proliferation of agent frameworks has led to fragmentation in how agents are defined, executed, and evaluated. Existing systems differ in their abstractions, data flow semantics, and tool integrations, making it difficult to share or reproduce workflows. We introduce Open Agent Specification (Agent Spec), a declarative language that defines AI agents and agentic workflows in a way that is compatible across frameworks, promoting reusability, portability and interoperability of AI agents. Agent Spec defines a common set of components, control and data flow semantics, and schemas that allow an agent to be defined once and executed across different runtimes. Agent Spec also introduces a standardized Evaluation harness to assess agent behavior and agentic workflows across runtimes - analogous to how HELM and related harnesses standardized LLM evaluation - so that performance, robustness, and efficiency can be compared consistently across frameworks. We demonstrate this using four distinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over three different benchmarks (SimpleQA Verified, $τ^2$-Bench and BIRD-SQL). We provide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime (WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen, CrewAI). Agent Spec bridges the gap between model-centric and agent-centric standardization & evaluation, laying the groundwork for reliable, reusable, and portable agentic systems.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04173v4",
      "published_date": "2025-10-05 12:26:42 UTC",
      "updated_date": "2025-11-07 14:02:33 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343399+00:00"
    },
    {
      "arxiv_id": "2510.05176v1",
      "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
      "title_zh": "",
      "authors": [
        "Ji Zhang",
        "Yiwei Li",
        "Shaoxiong Feng",
        "Peiwen Yuan",
        "Xinglin Wang",
        "Jiayi Shi",
        "Yueqi Zhang",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "KV cache in autoregressive LLMs eliminates redundant recomputation but has emerged as the dominant memory and bandwidth bottleneck during inference, notably with long contexts and test-time scaling. KV quantization is a key lever for reducing cache cost, but accuracy drops sharply as the native KV distribution lacks flatness and thus maintains a wide quantization range. Prior work focuses on isolating outliers, which caps their error but fails to flatten the overall distribution, leaving performance fragile under low-bit settings. In this work, we show that the K cache maintains a stable structure that evolves gradually with context, while the V cache carries latent semantic regularities. Building on these insights, we propose PatternKV, a pattern-aligned residual quantization scheme. It mines representative pattern vectors online, aligns each KV vector to its nearest pattern, and quantizes only the residual. This reshaping of the KV distribution flattens the quantization target and narrows its range, thereby improving the fidelity of low-bit KV quantization. Across long-context and test-time scaling settings on multiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08% average 4-bit drop relative to FP16, improves test-time scaling accuracy by 10% on average, and raises throughput by 1.4x while supporting 1.25x larger batches.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05176v1",
      "published_date": "2025-10-05 12:09:14 UTC",
      "updated_date": "2025-10-05 12:09:14 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343432+00:00"
    },
    {
      "arxiv_id": "2510.04166v1",
      "title": "Multi Language Models for On-the-Fly Syntax Highlighting",
      "title_zh": "",
      "authors": [
        "Marco Edoardo Palma",
        "Pooja Rani",
        "Harald C. Gall"
      ],
      "abstract": "Syntax highlighting is a critical feature in modern software development environments, enhancing code readability and developer productivity. However, delivering accurate highlighting in real time remains challenging for online and web-based development tools due to strict time and memory constraints on backend services. These systems must serve highlights rapidly and frequently, even when code is partially valid or invalid. This has led to on-the-fly syntax highlighting, where visual annotations are generated just before content is served, often at high request rates and under incomplete input conditions. To meet these demands efficiently, state-of-the-art models use deep learning to learn the behavior of brute-force syntax highlighting resolvers, tools that are easy to implement but too slow for production. Through the Deep Abstraction process, brute-force strategies are encoded into fast statistical models that achieve both high accuracy and low-latency inference. Despite their success, such models face key challenges: they support only one programming language per model, require large datasets from slow brute-force generators, and involve resource-intensive training. In multi-language environments, this means maintaining multiple independent models, increasing system complexity and operational cost. This work addresses these issues by introducing a unified model capable of highlighting up to six mainstream programming languages, reducing deployment complexity by a factor of six and improving performance on unseen languages. A novel normalization technique significantly enhances model generalization, while few-shot learning experiments show that a small number of oracle samples can replace large datasets, minimizing dependence on brute-force generators. Combined, these innovations enable efficient, scalable, and cost-effective syntax highlighting across diverse programming languages.",
      "tldr_zh": "",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04166v1",
      "published_date": "2025-10-05 11:48:49 UTC",
      "updated_date": "2025-10-05 11:48:49 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343463+00:00"
    },
    {
      "arxiv_id": "2510.05174v1",
      "title": "Emergent Coordination in Multi-Agent Language Models",
      "title_zh": "",
      "authors": [
        "Christoph Riedl"
      ],
      "abstract": "When are multi-agent LLM systems merely a collection of individual agents versus an integrated collective with higher-order structure? We introduce an information-theoretic framework to test -- in a purely data-driven way -- whether multi-agent systems show signs of higher-order structure. This information decomposition lets us measure whether dynamical emergence is present in multi-agent LLM systems, localize it, and distinguish spurious temporal coupling from performance-relevant cross-agent synergy. We implement both a practical criterion and an emergence capacity criterion operationalized as partial information decomposition of time-delayed mutual information (TDMI). We apply our framework to experiments using a simple guessing game without direct agent communication and only minimal group-level feedback with three randomized interventions. Groups in the control condition exhibit strong temporal synergy but only little coordinated alignment across agents. Assigning a persona to each agent introduces stable identity-linked differentiation. Combining personas with an instruction to ``think about what other agents might do'' shows identity-linked differentiation and goal-directed complementarity across agents. Taken together, our framework establishes that multi-agent LLM systems can be steered with prompt design from mere aggregates to higher-order collectives. Our results are robust across emergence measures and entropy estimators, and not explained by coordination-free baselines or temporal dynamics alone. Without attributing human-like cognition to the agents, the patterns of interaction we observe mirror well-established principles of collective intelligence in human groups: effective performance requires both alignment on shared objectives and complementary contributions across members.",
      "tldr_zh": "",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05174v1",
      "published_date": "2025-10-05 11:26:41 UTC",
      "updated_date": "2025-10-05 11:26:41 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343494+00:00"
    },
    {
      "arxiv_id": "2510.04146v2",
      "title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models",
      "title_zh": "",
      "authors": [
        "Minseo Kim",
        "Coleman Hooper",
        "Aditya Tomar",
        "Chenfeng Xu",
        "Mehrdad Farajtabar",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and code generation. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. While these models have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency in next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output tokens in parallel, mitigating the limitations of sequential decoding. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive study of the performance characteristics of ARMs and DLMs, combining theoretical analysis with empirical profiling to characterize the trade-offs between these approaches. We show that although DLMs can achieve higher arithmetic intensity than ARMs by leveraging parallelism across token positions, they fail to scale effectively with longer contexts. We then explore block-wise decoding for DLMs, which decouples arithmetic intensity from sequence length and enables better scaling to long contexts (similar to ARMs). We also examine batched inference and find that ARMs exhibit superior throughput as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, emphasizing that reducing the number of sampling steps is key for open-source DLMs to achieve lower latency relative to ARMs.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04146v2",
      "published_date": "2025-10-05 10:50:52 UTC",
      "updated_date": "2025-12-15 16:36:36 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343525+00:00"
    },
    {
      "arxiv_id": "2510.04142v1",
      "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs",
      "title_zh": "",
      "authors": [
        "Xiaoyu Yang",
        "Jie Lu",
        "En Yu"
      ],
      "abstract": "This paper identifies a critical yet underexplored challenge in distilling from multimodal large language models (MLLMs): the reasoning trajectories generated by multiple drifting teachers exhibit concept drift, whereby their reasoning distributions evolve unpredictably and transmit biases to the student model, ultimately compromising its performance. To tackle this issue, we pioneer a theoretical connection between concept drift and knowledge distillation, casting the non-stationary reasoning dynamics from multiple MLLM teachers as next-token prediction of multi-stream reasoning trajectories.Guided by concept drift, we introduce the \"learn, compare, critique\" paradigm, culminating in autonomous preference optimization (APO). Under the active guidance of the teachers, the student model first learns and self-distils preferred thinking by comparing multiple teachers. It then engages in critical reflection over the drifting inference from teachers, performing concept alignment through APO, ultimately yielding a robust, consistent, and generalizable model.Extensive experiments demonstrate our superior performance of consistency, robustness and generalization within knowledge distillation. Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public at: https://anonymous.4open.science/r/Autonomous-Distillation/.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04142v1",
      "published_date": "2025-10-05 10:42:21 UTC",
      "updated_date": "2025-10-05 10:42:21 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343556+00:00"
    },
    {
      "arxiv_id": "2510.04141v2",
      "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning",
      "title_zh": "",
      "authors": [
        "Mayank Ravishankara",
        "Varindra V. Persad Maharaj"
      ],
      "abstract": "This survey paper chronicles the evolution of evaluation in multimodal artificial intelligence (AI), framing it as a progression of increasingly sophisticated \"cognitive examinations.\" We argue that the field is undergoing a paradigm shift, moving from simple recognition tasks that test \"what\" a model sees, to complex reasoning benchmarks that probe \"why\" and \"how\" it understands. This evolution is driven by the saturation of older benchmarks, where high performance often masks fundamental weaknesses. We chart the journey from the foundational \"knowledge tests\" of the ImageNet era to the \"applied logic and comprehension\" exams such as GQA and Visual Commonsense Reasoning (VCR), which were designed specifically to diagnose systemic flaws such as shortcut learning and failures in compositional generalization. We then survey the current frontier of \"expert-level integration\" benchmarks (e.g., MMBench, SEED-Bench, MMMU) designed for today's powerful multimodal large language models (MLLMs), which increasingly evaluate the reasoning process itself. Finally, we explore the uncharted territories of evaluating abstract, creative, and social intelligence. We conclude that the narrative of AI evaluation is not merely a history of datasets, but a continuous, adversarial process of designing better examinations that, in turn, redefine our goals for creating truly intelligent systems.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04141v2",
      "published_date": "2025-10-05 10:41:22 UTC",
      "updated_date": "2026-01-05 20:21:15 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343588+00:00"
    },
    {
      "arxiv_id": "2510.04140v1",
      "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs",
      "title_zh": "",
      "authors": [
        "Zishang Jiang",
        "Jinyi Han",
        "Tingyun Li",
        "Xinyi Wang",
        "Sihang Jiang",
        "Jiaqing Liang",
        "Zhaoqian Dai",
        "Shuguang Ma",
        "Fei Yu",
        "Yanghua Xiao"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04140v1",
      "published_date": "2025-10-05 10:38:55 UTC",
      "updated_date": "2025-10-05 10:38:55 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343619+00:00"
    },
    {
      "arxiv_id": "2510.04135v1",
      "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization",
      "title_zh": "",
      "authors": [
        "Jingzhi Gong",
        "Yixin Bian",
        "Luis de la Cal",
        "Giovanni Pinna",
        "Anisha Uteem",
        "David Williams",
        "Mar Zamorano",
        "Karine Even-Mendoza",
        "W. B. Langdon",
        "Hector Menendez",
        "Federica Sarro"
      ],
      "abstract": "Coding agents powered by LLMs face critical sustainability and scalability challenges in industrial deployment, with single runs consuming over 100k tokens and incurring environmental costs that may exceed optimization benefits. This paper introduces GA4GC, the first framework to systematically optimize coding agent runtime (greener agent) and code performance (greener code) trade-offs by discovering Pareto-optimal agent hyperparameters and prompt templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x hypervolume improvement, reducing agent runtime by 37.7% while improving correctness. Our findings establish temperature as the most critical hyperparameter, and provide actionable strategies to balance agent sustainability with code optimization effectiveness in industrial deployment.",
      "tldr_zh": "",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by SSBSE'25 Challenge Track",
      "pdf_url": "https://arxiv.org/pdf/2510.04135v1",
      "published_date": "2025-10-05 10:34:30 UTC",
      "updated_date": "2025-10-05 10:34:30 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343651+00:00"
    },
    {
      "arxiv_id": "2510.04134v1",
      "title": "PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting",
      "title_zh": "",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Yongxin Tong"
      ],
      "abstract": "Periodicity is a fundamental characteristic of time series data and has long played a central role in forecasting. Recent deep learning methods strengthen the exploitation of periodicity by treating patches as basic tokens, thereby improving predictive effectiveness. However, their efficiency remains a bottleneck due to large parameter counts and heavy computational costs. This paper provides, for the first time, a clear explanation of why patch-level processing is inherently inefficient, supported by strong evidence from real-world data. To address these limitations, we introduce a phase perspective for modeling periodicity and present an efficient yet effective solution, PhaseFormer. PhaseFormer features phase-wise prediction through compact phase embeddings and efficient cross-phase interaction enabled by a lightweight routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves state-of-the-art performance with around 1k parameters, consistently across benchmark datasets. Notably, it excels on large-scale and complex datasets, where models with comparable efficiency often struggle. This work marks a significant step toward truly efficient and effective time series forecasting. Code is available at this repository: https://github.com/neumyor/PhaseFormer_TSL",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04134v1",
      "published_date": "2025-10-05 10:34:19 UTC",
      "updated_date": "2025-10-05 10:34:19 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343684+00:00"
    },
    {
      "arxiv_id": "2510.05173v3",
      "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
      "title_zh": "",
      "authors": [
        "Peigui Qi",
        "Kunsheng Tang",
        "Wenbo Zhou",
        "Weiming Zhang",
        "Nenghai Yu",
        "Tianwei Zhang",
        "Qing Guo",
        "Jie Zhang"
      ],
      "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce SafeGuider, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, SafeGuider generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ACM CCS 2025, Code is available at [this https URL](https://github.com/pgqihere/safeguider)",
      "pdf_url": "https://arxiv.org/pdf/2510.05173v3",
      "published_date": "2025-10-05 10:24:48 UTC",
      "updated_date": "2025-10-15 08:53:37 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343716+00:00"
    },
    {
      "arxiv_id": "2510.12807v1",
      "title": "Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning",
      "title_zh": "",
      "authors": [
        "Mahdi Cherakhloo",
        "Arash Abbasi",
        "Mohammad Saeid Sarafraz",
        "Bijan Vosoughi Vahdat"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous languages; however, their effectiveness in low-resource languages like Persian requires thorough investigation. This paper presents a comprehensive benchmark of several open-source LLMs for Persian Natural Language Processing (NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We evaluate models across a range of tasks including sentiment analysis, named entity recognition, reading comprehension, and question answering, using established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology encompasses rigorous experimental setups for both zero-shot and few-shot scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for performance evaluation. The results reveal that Gemma 2 consistently outperforms other models across nearly all tasks in both learning paradigms, with particularly strong performance in complex reasoning tasks. However, most models struggle with token-level understanding tasks like Named Entity Recognition, highlighting specific challenges in Persian language processing. This study contributes to the growing body of research on multilingual LLMs, providing valuable insights into their performance in Persian and offering a benchmark for future model development.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.12807v1",
      "published_date": "2025-10-05 10:10:04 UTC",
      "updated_date": "2025-10-05 10:10:04 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343748+00:00"
    },
    {
      "arxiv_id": "2510.04130v1",
      "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization",
      "title_zh": "",
      "authors": [
        "Yang Chen",
        "Yitao Liang",
        "Zhouchen Lin"
      ],
      "abstract": "In Transformers, Position Embeddings (PEs) significantly influence Length Generalization (LG) performance, yet their fundamental role remains unclear. In this work, we investigate the limitations and capabilities of PEs in achieving LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs), introducing Linear Representation Complexity (LRC) to characterize when PEs enable LG. Our analysis shows that PEs do not expand computational capabilities but structure learned computations across positions. Extending to practical Transformers, we propose Sequential Representation Complexity (SRC) and conjecture that LG is possible if and only if SRC remains invariant across scales. We support this hypothesis with empirical evidence in various reasoning tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance scaling, and a Learning-Based Position Embedding framework that automatically learns positional relations. Our work provides theoretical insights and practical strategies for improving LG in Transformers.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04130v1",
      "published_date": "2025-10-05 10:08:33 UTC",
      "updated_date": "2025-10-05 10:08:33 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343784+00:00"
    },
    {
      "arxiv_id": "2510.04128v1",
      "title": "Internal states before wait modulate reasoning patterns",
      "title_zh": "",
      "authors": [
        "Dmitrii Troitskii",
        "Koyena Pal",
        "Chris Wendler",
        "Callum Stuart McDougall",
        "Neel Nanda"
      ],
      "abstract": "Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to EMNLP Findings 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.04128v1",
      "published_date": "2025-10-05 10:03:42 UTC",
      "updated_date": "2025-10-05 10:03:42 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343816+00:00"
    },
    {
      "arxiv_id": "2510.04127v1",
      "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances",
      "title_zh": "",
      "authors": [
        "Sean Moran"
      ],
      "abstract": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in information retrieval, underpinning large-scale applications in computer vision, natural language processing, and cross-modal search. Hashing-based methods provide an efficient solution by mapping high-dimensional data into compact binary codes that enable fast similarity computations in Hamming space. Over the past two decades, a substantial body of work has explored learning to hash, where projection and quantisation functions are optimised from data rather than chosen at random.\n  This article offers a foundational survey of early learning-based hashing methods, with an emphasis on the core ideas that shaped the field. We review supervised, unsupervised, and semi-supervised approaches, highlighting how projection functions are designed to generate meaningful embeddings and how quantisation strategies convert these embeddings into binary codes. We also examine extensions to multi-bit and multi-threshold models, as well as early advances in cross-modal retrieval.\n  Rather than providing an exhaustive account of the most recent methods, our goal is to introduce the conceptual foundations of learning-based hashing for ANN search. By situating these early models in their historical context, we aim to equip readers with a structured understanding of the principles, trade-offs, and open challenges that continue to inform current research in this area.",
      "tldr_zh": "",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04127v1",
      "published_date": "2025-10-05 09:59:56 UTC",
      "updated_date": "2025-10-05 09:59:56 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343848+00:00"
    },
    {
      "arxiv_id": "2510.04126v1",
      "title": "Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions",
      "title_zh": "",
      "authors": [
        "Ziying Zhang",
        "Yaqing Wang",
        "Yuxuan Sun",
        "Min Ye",
        "Quanming Yao"
      ],
      "abstract": "Cold-start drug-target interaction (DTI) prediction focuses on interaction between novel drugs and proteins. Previous methods typically learn transferable interaction patterns between structures of drug and proteins to tackle it. However, insight from proteomics suggest that protein have multi-level structures and they all influence the DTI. Existing works usually represent protein with only primary structures, limiting their ability to capture interactions involving higher-level structures. Inspired by this insight, we propose ColdDTI, a framework attending on protein multi-level structure for cold-start DTI prediction. We employ hierarchical attention mechanism to mine interaction between multi-level protein structures (from primary to quaternary) and drug structures at both local and global granularities. Then, we leverage mined interactions to fuse structure representations of different levels for final prediction. Our design captures biologically transferable priors, avoiding the risk of overfitting caused by excessive reliance on representation learning. Experiments on benchmark datasets demonstrate that ColdDTI consistently outperforms previous methods in cold-start settings.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04126v1",
      "published_date": "2025-10-05 09:59:26 UTC",
      "updated_date": "2025-10-05 09:59:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343880+00:00"
    },
    {
      "arxiv_id": "2510.04120v1",
      "title": "Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence",
      "title_zh": "",
      "authors": [
        "Fengying Ye",
        "Shanshan Wang",
        "Lidia S. Chao",
        "Derek F. Wong"
      ],
      "abstract": "Metaphor analysis is a complex linguistic phenomenon shaped by context and external factors. While Large Language Models (LLMs) demonstrate advanced capabilities in knowledge integration, contextual reasoning, and creative generation, their mechanisms for metaphor comprehension remain insufficiently explored. This study examines LLMs' metaphor-processing abilities from three perspectives: (1) Concept Mapping: using embedding space projections to evaluate how LLMs map concepts in target domains (e.g., misinterpreting \"fall in love\" as \"drop down from love\"); (2) Metaphor-Literal Repository: analyzing metaphorical words and their literal counterparts to identify inherent metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how metaphorical syntactic structures influence LLMs' performance. Our findings reveal that LLMs generate 15\\%-25\\% conceptually irrelevant interpretations, depend on metaphorical indicators in training data rather than contextual cues, and are more sensitive to syntactic irregularities than to structural comprehension. These insights underline the limitations of LLMs in metaphor analysis and call for more robust computational approaches.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04120v1",
      "published_date": "2025-10-05 09:45:51 UTC",
      "updated_date": "2025-10-05 09:45:51 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343914+00:00"
    },
    {
      "arxiv_id": "2510.04116v2",
      "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning",
      "title_zh": "",
      "authors": [
        "Ziying Zhang",
        "Yaqing Wang",
        "Quanming Yao"
      ],
      "abstract": "Meta reasoning behaviors work as a skeleton to guide large language model (LLM) reasoning, thus help to improve reasoning performance. However, prior researches implement meta reasoning skeleton with manually designed structure, limiting ability to adapt to query-specific requirement and capture intricate logical dependency among reasoning steps. To deal with the challenges, we represent meta reasoning skeleton with directed acyclic graph (DAG) to unify skeletons proposed in prior works and model intricate logical dependency. Then we propose AutoMR, a framework that searches for query-aware meta reasoning skeleton automatically inspired by automated machine learning (AutoML). Specifically, we construct search space based on DAG representation of skeleton and then formulate the search problem. We design a dynamic skeleton sampling algorithm by expanding meta reasoning skeleton along with reasoning context at inference time. This algorithm can derive any meta reasoning skeleton in search space efficiently and adapt skeleton to evolving base reasoning context, thus enable efficient query-aware skeleton search. We conduct experiments on extensive benchmark datasets. Experimental results show that AutoMR achieves better reasoning performance than previous works broadly.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04116v2",
      "published_date": "2025-10-05 09:36:26 UTC",
      "updated_date": "2025-11-27 02:52:31 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343946+00:00"
    },
    {
      "arxiv_id": "2510.08593v2",
      "title": "Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech",
      "title_zh": "",
      "authors": [
        "Yuxin Li",
        "Eng Siong Chng",
        "Cuntai Guan"
      ],
      "abstract": "Speech-based depression detection (SDD) has emerged as a non-invasive and scalable alternative to conventional clinical assessments. However, existing methods still struggle to capture robust depression-related speech characteristics, which are sparse and heterogeneous. Although pretrained self-supervised learning (SSL) models provide rich representations, most recent SDD studies extract features from a single layer of the pretrained SSL model for the downstream classifier. This practice overlooks the complementary roles of low-level acoustic features and high-level semantic information inherently encoded in different SSL model layers. To explicitly model interactions between acoustic and semantic representations within an utterance, we propose a hierarchical adaptive representation encoder with prior knowledge that disengages and re-aligns acoustic and semantic information through asymmetric cross-attention, enabling fine-grained acoustic patterns to be interpreted in semantic context. In addition, a Connectionist Temporal Classification (CTC) objective is applied as auxiliary supervision to handle the irregular temporal distribution of depressive characteristics without requiring frame-level annotations. Experiments on DAIC-WOZ and MODMA demonstrate that HAREN-CTC consistently outperforms existing methods under both performance upper-bound evaluation and generalization evaluation settings, achieving Macro F1 scores of 0.81 and 0.82 respectively in upper-bound evaluation, and maintaining superior performance with statistically significant improvements in precision and AUC under rigorous cross-validation. These findings suggest that modeling hierarchical acoustic-semantic interactions better reflects how depressive characteristics manifest in natural speech, enabling scalable and objective depression assessment.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.08593v2",
      "published_date": "2025-10-05 09:32:12 UTC",
      "updated_date": "2026-01-21 07:20:16 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.343978+00:00"
    },
    {
      "arxiv_id": "2510.04100v1",
      "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing",
      "title_zh": "",
      "authors": [
        "Jiaming Wang",
        "Diwen Liu",
        "Jizhuo Chen",
        "Harold Soh"
      ],
      "abstract": "Topological mapping offers a compact and robust representation for navigation, but progress in the field is hindered by the lack of standardized evaluation metrics, datasets, and protocols. Existing systems are assessed using different environments and criteria, preventing fair and reproducible comparisons. Moreover, a key challenge - perceptual aliasing - remains under-quantified, despite its strong influence on system performance. We address these gaps by (1) formalizing topological consistency as the fundamental property of topological maps and showing that localization accuracy provides an efficient and interpretable surrogate metric, and (2) proposing the first quantitative measure of dataset ambiguity to enable fair comparisons across environments. To support this protocol, we curate a diverse benchmark dataset with calibrated ambiguity levels, implement and release deep-learned baseline systems, and evaluate them alongside classical methods. Our experiments and analysis yield new insights into the limitations of current approaches under perceptual aliasing. All datasets, baselines, and evaluation tools are fully open-sourced to foster consistent and reproducible research in topological mapping.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2510.04100v1",
      "published_date": "2025-10-05 08:58:08 UTC",
      "updated_date": "2025-10-05 08:58:08 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344012+00:00"
    },
    {
      "arxiv_id": "2510.04098v1",
      "title": "Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning",
      "title_zh": "",
      "authors": [
        "Chenxiang Ma",
        "Xinyi Chen",
        "Yujie Wu",
        "Kay Chen Tan",
        "Jibin Wu"
      ],
      "abstract": "Spiking neural networks (SNNs), recognized as an energy-efficient alternative to traditional artificial neural networks (ANNs), have advanced rapidly through the scaling of models and datasets. However, such scaling incurs considerable training overhead, posing challenges for researchers with limited computational resources and hindering the sustained development of SNNs. Data pruning is a promising strategy for accelerating training by retaining the most informative examples and discarding redundant ones, but it remains largely unexplored in SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture the intrinsic importance of examples and suffers from high gradient variance. To address these challenges, we propose a novel spike-aware data pruning (SADP) method. SADP reduces gradient variance by determining each example's selection probability to be proportional to its gradient norm, while avoiding the high cost of direct gradient computation through an efficient upper bound, termed spike-aware importance score. This score accounts for the influence of all-or-nothing spikes on the gradient norm and can be computed with negligible overhead. Extensive experiments across diverse datasets and architectures demonstrate that SADP consistently outperforms data pruning baselines and achieves training speedups close to the theoretical maxima at different pruning ratios. Notably, SADP reduces training time by 35% on ImageNet while maintaining accuracy comparable to that of full-data training. This work, therefore, establishes a data-centric paradigm for efficient SNN training and paves the way for scaling SNNs to larger models and datasets. The source code will be released publicly after the review process.",
      "tldr_zh": "",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04098v1",
      "published_date": "2025-10-05 08:50:28 UTC",
      "updated_date": "2025-10-05 08:50:28 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344045+00:00"
    },
    {
      "arxiv_id": "2510.04097v2",
      "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning",
      "title_zh": "",
      "authors": [
        "Peichao Lai",
        "Jinhui Zhuang",
        "Kexuan Zhang",
        "Ningchang Xiong",
        "Shengjie Wang",
        "Yanwei Xu",
        "Chong Chen",
        "Yilei Wang",
        "Bin Cui"
      ],
      "abstract": "Automating the conversion of UI images into web code is a critical task for front-end development and rapid prototyping. Advances in multimodal large language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet existing benchmarks remain limited in data diversity and evaluation reliability. To address these issues, we present WebRenderBench, a large-scale benchmark of 45.1k webpages collected from real-world portal sites, offering greater diversity, complexity, and realism than prior benchmarks. We further propose a novel evaluation metric that measures layout and style consistency from the final rendered pages. Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. Finally, we introduce the Automated Layout and Style Inspection Agent (ALISA), which integrates this metric into reinforcement learning as a reward signal to enhance training on crawled asymmetric webpages. Experiments show that ALISA significantly boosts generation performance, achieving state-of-the-art results across multiple metrics.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04097v2",
      "published_date": "2025-10-05 08:47:39 UTC",
      "updated_date": "2025-10-09 03:04:27 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344078+00:00"
    },
    {
      "arxiv_id": "2510.04093v2",
      "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems",
      "title_zh": "",
      "authors": [
        "Guixian Zhang",
        "Guan Yuan",
        "Ziqi Xu",
        "Yanmei Zhang",
        "Jing Ren",
        "Zhenyun Deng",
        "Debo Cheng"
      ],
      "abstract": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES) aims to assess students' mastery of knowledge concepts from heterogeneous, noisy interactions. Recent work has tried to utilize Large Language Models (LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are prone to noise-induced misjudgments. Specially, WIES's open environment continuously attracts new students and produces vast amounts of response logs, exacerbating the data imbalance and noise issues inherent in traditional educational systems. To address these challenges, we propose DLLM, a Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first constructs independent subgraphs based on response correctness, then applies relation augmentation alignment module to mitigate data imbalance. The two subgraph representations are then fused and aligned with LLM-derived, semantically augmented representations. Importantly, before each alignment step, DLLM employs a two-stage denoising diffusion module to eliminate intrinsic noise while assisting structural representation alignment. Specifically, unconditional denoising diffusion first removes erroneous information, followed by conditional denoising diffusion based on graph-guided to eliminate misleading information. Finally, the noise-robust representation that integrates semantic knowledge and structural information is fed into existing cognitive diagnosis models for prediction. Experimental results on three publicly available web-based educational platform datasets demonstrate that our DLLM achieves optimal predictive performance across varying noise levels, which demonstrates that DLLM achieves noise robustness while effectively leveraging semantic knowledge from LLM.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04093v2",
      "published_date": "2025-10-05 08:32:30 UTC",
      "updated_date": "2025-10-07 09:32:20 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344113+00:00"
    },
    {
      "arxiv_id": "2510.04090v1",
      "title": "Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes",
      "title_zh": "",
      "authors": [
        "Nikita Gabdullin"
      ],
      "abstract": "Supervised learning (SL) methods are indispensable for neural network (NN) training used to perform classification tasks. While resulting in very high accuracy, SL training often requires making NN parameter number dependent on the number of classes, limiting their applicability when the number of classes is extremely large or unknown in advance. In this paper we propose a methodology that allows one to train the same NN architecture regardless of the number of classes. This is achieved by using predefined vector systems as the target latent space configuration (LSC) during NN training. We discuss the desired properties of target configurations and choose randomly perturbed vectors of An root system for our experiments. These vectors are used to successfully train encoders and visual transformers (ViT) on Cinic-10 and ImageNet-1K in low- and high-dimensional cases by matching NN predictions with the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million classes illustrating the applicability of the method to training on datasets with extremely large number of classes. In addition, potential applications of LSC in lifelong learning and NN distillation are discussed illustrating versatility of the proposed methodology.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 12 figures, 10 tables, 12 equations, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2510.04090v1",
      "published_date": "2025-10-05 08:28:37 UTC",
      "updated_date": "2025-10-05 08:28:37 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344147+00:00"
    },
    {
      "arxiv_id": "2510.04089v1",
      "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows",
      "title_zh": "",
      "authors": [
        "Yitong Cui",
        "Liu Liu",
        "Baosheng Yu",
        "Jiayan Qiu",
        "Xikai Zhang",
        "Likang Xiao",
        "Yixing Liu",
        "Quan Chen"
      ],
      "abstract": "Large language models (LLMs) have exhibited significant capabilities in addressing challenging problems throughout various fields, often through the use of agentic workflows that adhere to structured instructions and multi-step procedures. However, designing such workflows demands substantial manual effort, posing challenges to scalability and generalizability. Recent studies have aimed to minimize the human intervention needed for their construction, leading to advances in automated techniques for optimizing agentic workflows. However, current approaches are often constrained by their limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigm -- issues that stem primarily from a dependence on discrete optimization techniques. To overcome these limitations, we introduce a new score-based preference approach, refereed as SPOGW, which operates directly on cardinal reward signals through group-wise comparison and enables more efficient and stable optimization in a continuous space. SPOGW incorporates Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL), which regulates training update by placing greater emphasis on the advantageous regions of the policy response. In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04089v1",
      "published_date": "2025-10-05 08:26:29 UTC",
      "updated_date": "2025-10-05 08:26:29 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344180+00:00"
    },
    {
      "arxiv_id": "2510.04088v1",
      "title": "Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees",
      "title_zh": "",
      "authors": [
        "Nan Jiang",
        "Tengyang Xie"
      ],
      "abstract": "This article introduces the theory of offline reinforcement learning in large state spaces, where good policies are learned from historical data without online interactions with the environment. Key concepts introduced include expressivity assumptions on function approximation (e.g., Bellman completeness vs. realizability) and data coverage (e.g., all-policy vs. single-policy coverage). A rich landscape of algorithms and results is described, depending on the assumptions one is willing to make and the sample and computational complexity guarantees one wishes to achieve. We also discuss open questions and connections to adjacent areas.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in Statistical Science",
      "pdf_url": "https://arxiv.org/pdf/2510.04088v1",
      "published_date": "2025-10-05 08:23:40 UTC",
      "updated_date": "2025-10-05 08:23:40 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344215+00:00"
    },
    {
      "arxiv_id": "2510.04087v2",
      "title": "Best of mini-N in-loop Sampling: A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling",
      "title_zh": "",
      "authors": [
        "Hyung Gyu Rho",
        "Sian Lee"
      ],
      "abstract": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70%, and when tuned as an inference accelerator, it improves average inference speed by over 22% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
      "tldr_zh": "",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04087v2",
      "published_date": "2025-10-05 08:23:08 UTC",
      "updated_date": "2025-10-10 21:47:22 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344248+00:00"
    },
    {
      "arxiv_id": "2510.06260v1",
      "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis",
      "title_zh": "",
      "authors": [
        "Sher Khan",
        "Raz Muhammad",
        "Adil Hussain",
        "Muhammad Sajjad",
        "Muhammad Rashid"
      ],
      "abstract": "Cutaneous malignancies demand early detection for favorable outcomes, yet current diagnostics suffer from inter-observer variability and access disparities. While AI shows promise, existing dermatological systems are limited by homogeneous architectures, dataset biases across skin tones, and fragmented approaches that treat natural language processing as separate post-hoc explanations rather than integral to clinical decision-making. We introduce a unified framework that fundamentally reimagines AI integration for dermatological diagnostics through two synergistic innovations. First, a purposefully heterogeneous ensemble of architecturally diverse convolutional neural networks provides complementary diagnostic perspectives, with an intrinsic uncertainty mechanism flagging discordant cases for specialist review -- mimicking clinical best practices. Second, we embed large language model capabilities directly into the diagnostic workflow, transforming classification outputs into clinically meaningful assessments that simultaneously fulfill medical documentation requirements and deliver patient-centered education. This seamless integration generates structured reports featuring precise lesion characterization, accessible diagnostic reasoning, and actionable monitoring guidance -- empowering patients to recognize early warning signs between visits. By addressing both diagnostic reliability and communication barriers within a single cohesive system, our approach bridges the critical translational gap that has prevented previous AI implementations from achieving clinical impact. The framework represents a significant advancement toward deployable dermatological AI that enhances diagnostic precision while actively supporting the continuum of care from initial detection through patient education, ultimately improving early intervention rates for skin lesions.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.06260v1",
      "published_date": "2025-10-05 08:07:33 UTC",
      "updated_date": "2025-10-05 08:07:33 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344282+00:00"
    },
    {
      "arxiv_id": "2510.04073v1",
      "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention",
      "title_zh": "",
      "authors": [
        "Santhosh Kumar Ravindran"
      ],
      "abstract": "The rise of artificial intelligence (AI) as super-capable assistants has transformed productivity and decision-making across domains. Yet, this integration raises critical concerns about value alignment - ensuring AI behaviors remain consistent with human ethics and intentions. A key risk is value drift, where AI systems deviate from aligned values due to evolving contexts, learning dynamics, or unintended optimizations, potentially leading to inefficiencies or ethical breaches. We propose the Moral Anchor System (MAS), a novel framework to detect, predict, and mitigate value drift in AI agents. MAS combines real-time Bayesian inference for monitoring value states, LSTM networks for forecasting drift, and a human-centric governance layer for adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent breaches, while reducing false positives and alert fatigue via supervised fine-tuning with human feedback. Our hypothesis: integrating probabilistic drift detection, predictive analytics, and adaptive governance can reduce value drift incidents by 80 percent or more in simulations, maintaining high detection accuracy (85 percent) and low false positive rates (0.08 post-adaptation). Rigorous experiments with goal-misaligned agents validate MAS's scalability and responsiveness. MAS's originality lies in its predictive and adaptive nature, contrasting static alignment methods. Contributions include: (1) MAS architecture for AI integration; (2) empirical results prioritizing speed and usability; (3) cross-domain applicability insights; and (4) open-source code for replication.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages Includes simulations with over 4 million steps",
      "pdf_url": "https://arxiv.org/pdf/2510.04073v1",
      "published_date": "2025-10-05 07:24:23 UTC",
      "updated_date": "2025-10-05 07:24:23 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344319+00:00"
    },
    {
      "arxiv_id": "2510.04072v2",
      "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning",
      "title_zh": "",
      "authors": [
        "Ziyan Wang",
        "Zheng Wang",
        "Jie Fu",
        "Xingwei Qu",
        "Qi Cheng",
        "Shengpu Tang",
        "Minjia Zhang",
        "Xiaoming Huo"
      ],
      "abstract": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts and an up to 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best accuracy.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04072v2",
      "published_date": "2025-10-05 07:22:54 UTC",
      "updated_date": "2025-10-08 04:24:36 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344353+00:00"
    },
    {
      "arxiv_id": "2510.04067v1",
      "title": "What Scales in Cross-Entropy Scaling Law?",
      "title_zh": "",
      "authors": [
        "Junxi Yan",
        "Zixi Wei",
        "Jingtao Zhan",
        "Qingyao Ai",
        "Yiqun Liu"
      ],
      "abstract": "The cross-entropy scaling law has long served as a key tool for guiding the development of large language models. It shows that cross-entropy loss decreases in a predictable power-law rate as the model size increases. However, recent evidence indicates that this law breaks down at very large scales: the loss decreases more slowly than expected, which causes significant trouble for developing large language models. In this paper, we hypothesize that the root cause lies in the fact that cross-entropy itself does not truly scale; instead, only one of its hidden components does. To investigate this, we introduce a novel decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. We show both theoretically and empirically that this decomposition precisely captures the training dynamics and optimization objectives. Through extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size, we find that only error-entropy follows a robust power-law scaling, while the other two terms remain largely invariant. Moreover, error-entropy constitutes the dominant share of cross-entropy in small models but diminishes in proportion as models grow larger. This explains why the cross-entropy scaling law appears accurate at small scales but fails at very large ones. Our findings establish the error-entropy scaling law as a more accurate description of model behavior. We believe it will have wide applications in the training, understanding, and future development of large language models.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04067v1",
      "published_date": "2025-10-05 07:06:02 UTC",
      "updated_date": "2025-10-05 07:06:02 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344388+00:00"
    },
    {
      "arxiv_id": "2510.04064v2",
      "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion",
      "title_zh": "",
      "authors": [
        "Jingxiang Zhang",
        "Lujia Zhong"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly expected to navigate the nuances of human emotion. While research confirms that LLMs can simulate emotional intelligence, their internal emotional mechanisms remain largely unexplored. This paper investigates the latent emotional representations within modern LLMs by asking: how, where, and for how long is emotion encoded in their neural architecture? To address this, we introduce a novel, large-scale Reddit corpus of approximately 400,000 utterances, balanced across seven basic emotions through a multi-stage process of classification, rewriting, and synthetic generation. Using this dataset, we employ lightweight \"probes\" to read out information from the hidden layers of various Qwen3 and LLaMA models without altering their parameters. Our findings reveal that LLMs develop a surprisingly well-defined internal geometry of emotion, which sharpens with model scale and significantly outperforms zero-shot prompting. We demonstrate that this emotional signal is not a final-layer phenomenon but emerges early and peaks mid-network. Furthermore, the internal states are both malleable (they can be influenced by simple system prompts) and persistent, as the initial emotional tone remains detectable for hundreds of subsequent tokens. We contribute our dataset, an open-source probing toolkit, and a detailed map of the emotional landscape within LLMs, offering crucial insights for developing more transparent and aligned AI systems. The code and dataset are open-sourced.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures, 4 tables. Under review",
      "pdf_url": "https://arxiv.org/pdf/2510.04064v2",
      "published_date": "2025-10-05 06:53:42 UTC",
      "updated_date": "2025-10-12 17:53:20 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344423+00:00"
    },
    {
      "arxiv_id": "2510.04057v1",
      "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation",
      "title_zh": "",
      "authors": [
        "Zhenyu Pan",
        "Yucheng Lu",
        "Han Liu"
      ],
      "abstract": "We present MetaFind, a scene-aware tri-modal compositional retrieval framework designed to enhance scene generation in the metaverse by retrieving 3D assets from large-scale repositories. MetaFind addresses two core challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic, and stylistic constraints, and (ii) the absence of a standardized retrieval paradigm specifically tailored for 3D asset retrieval, as existing approaches mainly rely on general-purpose 3D shape representation models. Our key innovation is a flexible retrieval mechanism that supports arbitrary combinations of text, image, and 3D modalities as queries, enhancing spatial reasoning and style consistency by jointly modeling object-level features (including appearance) and scene-level layout structures. Methodologically, MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that captures spatial relationships and object appearance features, ensuring retrieved 3D assets are contextually and stylistically coherent with the existing scene, regardless of coordinate frame transformations. The framework supports iterative scene construction by continuously adapting retrieval results to current scene updates. Empirical evaluations demonstrate the improved spatial and stylistic consistency of MetaFind in various retrieval tasks compared to baseline methods.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.04057v1",
      "published_date": "2025-10-05 06:37:26 UTC",
      "updated_date": "2025-10-05 06:37:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344457+00:00"
    },
    {
      "arxiv_id": "2510.04051v1",
      "title": "Toward a unified framework for data-efficient evaluation of large language models",
      "title_zh": "",
      "authors": [
        "Lele Liao",
        "Qile Zhang",
        "Ruofan Wu",
        "Guanhua Fang"
      ],
      "abstract": "Evaluating large language models (LLMs) on comprehensive benchmarks is a cornerstone of their development, yet it's often computationally and financially prohibitive. While Item Response Theory (IRT) offers a promising path toward data-efficient evaluation by disentangling model capability from item difficulty, existing IRT-based methods are hampered by significant limitations. They are typically restricted to binary correctness metrics, failing to natively handle the continuous scores used in generative tasks, and they operate on single benchmarks, ignoring valuable structural knowledge like correlations across different metrics or benchmarks. To overcome these challenges, we introduce LEGO-IRT, a unified and flexible framework for data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both binary and continuous evaluation metrics. Moreover, it introduces a factorized architecture to explicitly model and leverage structural knowledge, decomposing model ability estimates into a general component and structure-specific (e.g., per-metric or per-benchmark) components. Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\\%$ of the total evaluation items. We demonstrate that incorporating structural knowledge reduces estimation error by up to $10\\%$ and reveal that the latent abilities estimated by our framework may align more closely with human preferences.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval",
      "pdf_url": "https://arxiv.org/pdf/2510.04051v1",
      "published_date": "2025-10-05 06:13:50 UTC",
      "updated_date": "2025-10-05 06:13:50 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344491+00:00"
    },
    {
      "arxiv_id": "2510.04048v1",
      "title": "Increasing LLM response trustworthiness using voting ensembles",
      "title_zh": "",
      "authors": [
        "Aparna Nair-Kanneganti",
        "Trevor J. Chan",
        "Shir Goldfinger",
        "Emily Mackay",
        "Brian Anthony",
        "Alison Pouch"
      ],
      "abstract": "Despite huge advances, LLMs still lack convenient and reliable methods to quantify the uncertainty in their responses, making them difficult to trust in high-stakes applications. One of the simplest approaches to eliciting more accurate answers is to select the mode of many responses, a technique known as ensembling. In this work, we expand on typical ensembling approaches by looking at ensembles with a variable voting threshold. We introduce a theoretical framework for question answering and show that, by permitting ensembles to \"abstain\" from providing an answer when the dominant response falls short of the threshold, it is possible to dramatically increase the trustworthiness of the remaining answers. From this framework, we derive theoretical results as well as report experimental results on two problem domains: arithmetic problem solving and clinical-note question-answering. In both domains, we observe that large gains in answer trustworthiness can be achieved using highly restrictive voting ensembles, while incurring relatively modest reductions in response yield and accuracy. Due to this quality, voting ensembles may be particularly useful in applications - such as healthcare and data annotation - that require a high degree of certainty but which may not require that every question receive an automated answer.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04048v1",
      "published_date": "2025-10-05 06:02:44 UTC",
      "updated_date": "2025-10-05 06:02:44 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344526+00:00"
    },
    {
      "arxiv_id": "2510.04044v1",
      "title": "Quantization Range Estimation for Convolutional Neural Networks",
      "title_zh": "",
      "authors": [
        "Bingtao Yang",
        "Yujia Wang",
        "Mengzhi Jiao",
        "Hongwei Huo"
      ],
      "abstract": "Post-training quantization for reducing the storage of deep neural network models has been demonstrated to be an effective way in various tasks. However, low-bit quantization while maintaining model accuracy is a challenging problem. In this paper, we present a range estimation method to improve the quantization performance for post-training quantization. We model the range estimation into an optimization problem of minimizing quantization errors by layer-wise local minima. We prove this problem is locally convex and present an efficient search algorithm to find the optimal solution. We propose the application of the above search algorithm to the transformed weights space to do further improvement in practice. Our experiments demonstrate that our method outperforms state-of-the-art performance generally on top-1 accuracy for image classification tasks on the ResNet series models and Inception-v3 model. The experimental results show that the proposed method has almost no loss of top-1 accuracy in 8-bit and 6-bit settings for image classifications, and the accuracy of 4-bit quantization is also significantly improved. The code is available at https://github.com/codeiscommitting/REQuant.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 tables, research report",
      "pdf_url": "https://arxiv.org/pdf/2510.04044v1",
      "published_date": "2025-10-05 05:35:12 UTC",
      "updated_date": "2025-10-05 05:35:12 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344562+00:00"
    },
    {
      "arxiv_id": "2510.04040v1",
      "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning",
      "title_zh": "",
      "authors": [
        "Xu Shen",
        "Song Wang",
        "Zhen Tan",
        "Laura Yao",
        "Xinyu Zhao",
        "Kaidi Xu",
        "Xin Wang",
        "Tianlong Chen"
      ],
      "abstract": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04040v1",
      "published_date": "2025-10-05 05:16:54 UTC",
      "updated_date": "2025-10-05 05:16:54 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344598+00:00"
    },
    {
      "arxiv_id": "2510.04039v1",
      "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding",
      "title_zh": "",
      "authors": [
        "Bin Lei",
        "Nuo Xu",
        "Ali Payani",
        "Mingyi Hong",
        "Chunhua Liao",
        "Yu Cao",
        "Caiwen Ding"
      ],
      "abstract": "Multimodal large language models (MLLMs) have markedly expanded the competence of graphical user-interface (GUI) systems, propelling them beyond controlled simulations into complex, real-world environments across diverse platforms. However, practical usefulness is still bounded by the reliability of visual grounding, i.e., mapping textual references to exact on-screen elements. This limitation prevents the system from accurately performing pointer-level actions such as clicking or dragging. To address it, we introduce GUI-Spotlight -- a model trained for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow its focus to the relevant region of the screen, thereby substantially improving visual grounding accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only 18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with 9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04039v1",
      "published_date": "2025-10-05 05:15:45 UTC",
      "updated_date": "2025-10-05 05:15:45 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344634+00:00"
    },
    {
      "arxiv_id": "2510.04034v1",
      "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks",
      "title_zh": "",
      "authors": [
        "Linn Bieske",
        "Carla Lorente"
      ],
      "abstract": "Recent advances in image editing have shifted from manual pixel manipulation to employing deep learning methods like stable diffusion models, which now leverage cross-attention mechanisms for text-driven control. This transition has simplified the editing process but also introduced variability in results, such as inconsistent hair color changes. Our research aims to enhance the precision and reliability of prompt-to-prompt image editing frameworks by exploring and optimizing hyperparameters. We present a comprehensive study of the \"word swap\" method, develop an \"attention re-weight method\" for better adaptability, and propose the \"CL P2P\" framework to address existing limitations like cycle inconsistency. This work contributes to understanding and improving the interaction between hyperparameter settings and the architectural choices of neural network models, specifically their attention mechanisms, which significantly influence the composition and quality of the generated images.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04034v1",
      "published_date": "2025-10-05 04:56:07 UTC",
      "updated_date": "2025-10-05 04:56:07 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344670+00:00"
    },
    {
      "arxiv_id": "2510.04033v1",
      "title": "A global log for medical AI",
      "title_zh": "",
      "authors": [
        "Ayush Noori",
        "Adam Rodman",
        "Alan Karthikesalingam",
        "Bilal A. Mateen",
        "Christopher A. Longhurst",
        "Daniel Yang",
        "Dave deBronkart",
        "Gauden Galea",
        "Harold F. Wolf",
        "Jacob Waxman",
        "Joshua C. Mandel",
        "Juliana Rotich",
        "Kenneth D. Mandl",
        "Maryam Mustafa",
        "Melissa Miles",
        "Nigam H. Shah",
        "Peter Lee",
        "Robert Korom",
        "Scott Mahoney",
        "Seth Hain",
        "Tien Yin Wong",
        "Trevor Mundel",
        "Vivek Natarajan",
        "Noa Dagan",
        "David A. Clifton",
        "Ran D. Balicer",
        "Isaac S. Kohane",
        "Marinka Zitnik"
      ],
      "abstract": "Modern computer systems often rely on syslog, a simple, universal protocol that records every critical event across heterogeneous infrastructure. However, healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals rush to pilot large language models and other AI-based clinical decision support tools, we still lack a standard way to record how, when, by whom, and for whom these AI models are used. Without that transparency and visibility, it is challenging to measure real-world performance and outcomes, detect adverse events, or correct bias or dataset drift. In the spirit of syslog, we introduce MedLog, a protocol for event-level logging of clinical AI. Any time an AI model is invoked to interact with a human, interface with another algorithm, or act independently, a MedLog record is created. This record consists of nine core fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and feedback, providing a structured and consistent record of model activity. To encourage early adoption, especially in low-resource settings, and minimize the data footprint, MedLog supports risk-based sampling, lifecycle-aware retention policies, and write-behind caching; detailed traces for complex, agentic, or multi-stage workflows can also be captured under MedLog. MedLog can catalyze the development of new databases and software to store and analyze MedLog records. Realizing this vision would enable continuous surveillance, auditing, and iterative improvement of medical AI, laying the foundation for a new form of digital epidemiology.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04033v1",
      "published_date": "2025-10-05 04:52:26 UTC",
      "updated_date": "2025-10-05 04:52:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344706+00:00"
    },
    {
      "arxiv_id": "2510.04032v1",
      "title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study",
      "title_zh": "",
      "authors": [
        "Zirui Wang",
        "Jiajun Wu",
        "Braden Teitge",
        "Jessalyn Holodinsky",
        "Steve Drew"
      ],
      "abstract": "Large language models (LLMs) have become increasingly popular in medical domains to assist physicians with a variety of clinical and operational tasks. Given the fast-paced and high-stakes environment of emergency departments (EDs), small language models (SLMs), characterized by a reduction in parameter count compared to LLMs, offer significant potential due to their inherent reasoning capability and efficient performance. This enables SLMs to support physicians by providing timely and accurate information synthesis, thereby improving clinical decision-making and workflow efficiency. In this paper, we present a comprehensive benchmark designed to identify SLMs suited for ED decision support, taking into account both specialized medical expertise and broad general problem-solving capabilities. In our evaluations, we focus on SLMs that have been trained on a mixture of general-domain and medical corpora. A key motivation for emphasizing SLMs is the practical hardware limitations, operational cost constraints, and privacy concerns in the typical real-world deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and PubMedQA, with the medical abstracts dataset emulating tasks aligned with real ED physicians' daily tasks. Experimental results reveal that general-domain SLMs surprisingly outperform their medically fine-tuned counterparts across these diverse benchmarks for ED. This indicates that for ED, specialized medical fine-tuning of the model may not be required.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to 2025 IEEE International Conference on Autonomous and Trusted Computing (ATC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.04032v1",
      "published_date": "2025-10-05 04:46:30 UTC",
      "updated_date": "2025-10-05 04:46:30 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344746+00:00"
    },
    {
      "arxiv_id": "2510.04031v1",
      "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?",
      "title_zh": "",
      "authors": [
        "Nelvin Tan",
        "James Asikin Cheung",
        "Yu-Ching Shih",
        "Dong Yang",
        "Amol Salunkhe"
      ],
      "abstract": "Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. More recently, they have been shown to be very effective in textual classification tasks, motivating the need to explain the LLMs' decisions. Motivated by practical constrains where LLMs are black-boxed and LLM calls are expensive, we study how incorporating counterfactuals into LLM reasoning can affect the LLM's ability to identify the top words that have contributed to its classification decision. To this end, we introduce a framework called the decision changing rate that helps us quantify the importance of the top words in classification. Our experimental results show that using counterfactuals can be helpful.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04031v1",
      "published_date": "2025-10-05 04:45:53 UTC",
      "updated_date": "2025-10-05 04:45:53 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344782+00:00"
    },
    {
      "arxiv_id": "2510.04028v1",
      "title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View",
      "title_zh": "",
      "authors": [
        "Xinhao Yao",
        "Lu Yu",
        "Xiaolin Hu",
        "Fengwei Teng",
        "Qing Cui",
        "Jun Zhou",
        "Yong Liu"
      ],
      "abstract": "The ongoing debate on whether reinforcement learning with verifiable rewards (RLVR) expands or shrinks the reasoning capabilities of large language models (LLMs) remains unresolved. Some studies contend that RLVR mainly improves sampling efficiency but at the expense of diversity and exploratory capacity, resulting in capability boundary shrinkage. In contrast, others demonstrate that prolonged training can lead to the emergence of novel reasoning strategies, suggesting capability boundary expansion. To reconcile these contradictory findings, we theoretically and empirically show that both perspectives are partially valid-each aligning with a separate phase in an inherent two-stage probability mass dynamic: (1) Exploitation stage: initially, the model primarily samples explored high-reward and low-reward tokens, while rarely selecting the potentially optimal token. Positive advantage estimates increase the probability of high-reward tokens and decrease those of low-reward tokens, yet the optimal token's probability remains largely unchanged during this stage. (2) Exploration stage: as training advances, the growth rate of previously acquired high-reward tokens slows as their probabilities approach saturation. When a potentially optimal token-now receiving positive advantage estimates-is occasionally sampled, its probability increases, while those of the originally high-reward tokens decrease. This dynamic suggests that over-exploitation during the exploitation stage may lead to capability boundary shrinkage, whereas prolonged training into the exploration stage can promote an expansion of the reasoning capability boundary. Building upon our insights, we revisit the potential of only using relative negative gradients for prolonging training, providing a theoretical and empirical foundation for the development of more advanced reasoning capabilities.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04028v1",
      "published_date": "2025-10-05 04:31:33 UTC",
      "updated_date": "2025-10-05 04:31:33 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344818+00:00"
    },
    {
      "arxiv_id": "2510.04023v1",
      "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions",
      "title_zh": "",
      "authors": [
        "Mizanur Rahman",
        "Amran Bhuiyan",
        "Mohammed Saidul Islam",
        "Md Tahmid Rahman Laskar",
        "Ridwan Mahbub",
        "Ahmed Masry",
        "Shafiq Joty",
        "Enamul Hoque"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Survey paper; 45 data science agents; under review",
      "pdf_url": "https://arxiv.org/pdf/2510.04023v1",
      "published_date": "2025-10-05 04:04:27 UTC",
      "updated_date": "2025-10-05 04:04:27 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344855+00:00"
    },
    {
      "arxiv_id": "2510.04020v3",
      "title": "Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models",
      "title_zh": "",
      "authors": [
        "Hao Wu",
        "Yuan Gao",
        "Xingjian Shi",
        "Shuaipeng Li",
        "Fan Xu",
        "Fan Zhang",
        "Zhihong Zhu",
        "Weiyan Wang",
        "Xiao Luo",
        "Kun Wang",
        "Xian Wu",
        "Xiaomeng Huang"
      ],
      "abstract": "To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an \"imagination-based\" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04020v3",
      "published_date": "2025-10-05 03:57:38 UTC",
      "updated_date": "2025-10-10 02:54:02 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344893+00:00"
    },
    {
      "arxiv_id": "2510.05169v1",
      "title": "From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs",
      "title_zh": "",
      "authors": [
        "Guangyu Shen",
        "Siyuan Cheng",
        "Xiangzhe Xu",
        "Yuan Zhou",
        "Hanxi Guo",
        "Zhuo Zhang",
        "Xiangyu Zhang"
      ],
      "abstract": "Large Language Models (LLMs) can acquire deceptive behaviors through backdoor attacks, where the model executes prohibited actions whenever secret triggers appear in the input. Existing safety training methods largely fail to address this vulnerability, due to the inherent difficulty of uncovering hidden triggers implanted in the model. Motivated by recent findings on LLMs' situational awareness, we propose a novel post-training framework that cultivates self-awareness of backdoor risks and enables models to articulate implanted triggers even when they are absent from the prompt. At its core, our approach introduces an inversion-inspired reinforcement learning framework that encourages models to introspectively reason about their own behaviors and reverse-engineer the triggers responsible for misaligned outputs. Guided by curated reward signals, this process transforms a poisoned model into one capable of precisely identifying its implanted trigger. Surprisingly, we observe that such backdoor self-awareness emerges abruptly within a short training window, resembling a phase transition in capability. Building on this emergent property, we further present two complementary defense strategies for mitigating and detecting backdoor threats. Experiments on five backdoor attacks, compared against six baseline methods, demonstrate that our approach has strong potential to improve the robustness of LLMs against backdoor risks. The code is available at LLM Backdoor Self-Awareness.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05169v1",
      "published_date": "2025-10-05 03:55:24 UTC",
      "updated_date": "2025-10-05 03:55:24 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344931+00:00"
    },
    {
      "arxiv_id": "2510.04019v1",
      "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models",
      "title_zh": "",
      "authors": [
        "Anthony Zhan"
      ],
      "abstract": "Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04019v1",
      "published_date": "2025-10-05 03:53:16 UTC",
      "updated_date": "2025-10-05 03:53:16 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.344969+00:00"
    },
    {
      "arxiv_id": "2510.04017v1",
      "title": "Zephyrus: An Agentic Framework for Weather Science",
      "title_zh": "",
      "authors": [
        "Sumanth Varambally",
        "Marshall Fisher",
        "Jas Thakker",
        "Yiwei Chen",
        "Zhirui Xia",
        "Yasaman Jafari",
        "Ruijia Niu",
        "Manas Jain",
        "Veeramakali Vignesh Manivannan",
        "Zachary Novack",
        "Luyu Han",
        "Srikar Eranky",
        "Salva Rühling Cachay",
        "Taylor Berg-Kirkpatrick",
        "Duncan Watson-Parris",
        "Yi-An Ma",
        "Rose Yu"
      ],
      "abstract": "Foundation models for weather science are pre-trained on vast amounts of structured numerical data and outperform traditional weather forecasting systems. However, these models lack language-based reasoning capabilities, limiting their utility in interactive scientific workflows. Large language models (LLMs) excel at understanding and generating text but cannot reason about high-dimensional meteorological datasets. We bridge this gap by building a novel agentic framework for weather science. Our framework includes a Python code-based environment for agents (ZephyrusWorld) to interact with weather data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying for geographical masks from natural language, weather forecasting, and climate simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather agent that iteratively analyzes weather datasets, observes results, and refines its approach through conversational feedback loops. We accompany the agent with a new benchmark, ZephyrusBench, with a scalable data generation pipeline that constructs diverse question-answer pairs across weather-related tasks, from basic lookups to advanced forecasting, extreme event detection, and counterfactual reasoning. Experiments on this benchmark demonstrate the strong performance of Zephyrus agents over text-only baselines, outperforming them by up to 35 percentage points in correctness. However, on harder tasks, Zephyrus performs similarly to text-only baselines, highlighting the challenging nature of our benchmark and suggesting promising directions for future work.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.04017v1",
      "published_date": "2025-10-05 03:34:08 UTC",
      "updated_date": "2025-10-05 03:34:08 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345004+00:00"
    },
    {
      "arxiv_id": "2510.04016v1",
      "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents",
      "title_zh": "",
      "authors": [
        "Thanapol Popit",
        "Natthapath Rungseesiripak",
        "Monthol Charattrakool",
        "Saksorn Ruangtanusak"
      ],
      "abstract": "Fluid voice-to-voice interaction requires reliable and low-latency detection of when a user has finished speaking. Traditional audio-silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or language-specific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final particles), we formulate EOT as a binary decision over token boundaries. We report a clear accuracy-latency tradeoff and provide a public-ready implementation plan. This work establishes a Thai baseline and demonstrates that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "IEEE ICSEC 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.04016v1",
      "published_date": "2025-10-05 03:31:59 UTC",
      "updated_date": "2025-10-05 03:31:59 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345043+00:00"
    },
    {
      "arxiv_id": "2510.04009v1",
      "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models",
      "title_zh": "",
      "authors": [
        "Zicong He",
        "Boxuan Zhang",
        "Weihao Liu",
        "Ruixiang Tang",
        "Lu Cheng"
      ],
      "abstract": "The meteoric rise of foundation models (FMs) has expanded their capabilities far beyond conventional tasks. Creativity, long regarded as a hallmark of human intelligence and a driver of innovation, is now increasingly recognized as a critical dimension of machine intelligence in the era of generative FMs, complementing traditional measures of accuracy. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics not firmly grounded in established theories. To address this gap, we introduce C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs. C^2-Eval distinguishes between two complementary forms of creativity: convergent creativity, where tasks admit constrained solutions (e.g., code generation), and divergent creativity, where tasks are open-ended (e.g., storytelling). It evaluates both dimensions using fine-grained criteria derived from social-science theory, focusing on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on leading proprietary and open-source models, we analyze trade-offs in their creative capabilities. Our results highlight both the strengths and challenges of current FMs in pursuing a creative machine mind, showing that C^2-Eval is an effective lens for examining the evolving landscape of creative AI.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.04009v1",
      "published_date": "2025-10-05 03:00:50 UTC",
      "updated_date": "2025-10-05 03:00:50 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345081+00:00"
    },
    {
      "arxiv_id": "2510.04008v2",
      "title": "Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention",
      "title_zh": "",
      "authors": [
        "Sahil Joshi",
        "Agniva Chowdhury",
        "Amar Kanakamedala",
        "Ekam Singh",
        "Evan Tu",
        "Anshumali Shrivastava"
      ],
      "abstract": "Softmax Attention has a quadratic time complexity, which becomes prohibitive to run at long contexts, even with highly optimized GPU kernels. For example, FlashAttention (an exact, GPU-optimized implementation of Softmax Attention) cannot complete a single forward-backward pass of a multi-head attention layer once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We introduce RACE Attention, a kernel-inspired alternative to Softmax Attention that is linear in sequence length and embedding dimension. RACE Attention replaces the exponential kernel with a sharpened angular (cosine) similarity, and approximates attention outputs via randomized projections and soft Locality-Sensitive Hashing (LSH). Across language modeling, masked language modeling, and text classification, RACE Attention matches the accuracy of strong baselines while reducing runtime and memory. In a controlled scale test, it processes up to 12 million tokens during a single forward-backward pass on an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well beyond the practical limits of the current state-of-the-art attention implementations. RACE Attention thus offers a practical, theoretically grounded mechanism for outrageously long context windows on today's hardware. We hope that it gets adopted in practice.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.04008v2",
      "published_date": "2025-10-05 02:57:40 UTC",
      "updated_date": "2025-10-23 01:09:14 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345118+00:00"
    },
    {
      "arxiv_id": "2510.04001v1",
      "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation",
      "title_zh": "",
      "authors": [
        "Xuankang Zhang",
        "Jiangming Liu"
      ],
      "abstract": "The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed over social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important to understand the discussions about the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) COVID-19 texts in social media are informal and their annotations are rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive domain-specific knowledge. To address these issues, we propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied in general biomedical named entity recognition in both informal text format and formal text format. Experiments carried out on the COVID-19 tweets dataset and PubMed dataset show that our proposed entity knowledge augmentation improves NER performance in both fully-supervised and few-shot settings. Our source code is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2510.04001v1",
      "published_date": "2025-10-05 02:22:26 UTC",
      "updated_date": "2025-10-05 02:22:26 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345155+00:00"
    },
    {
      "arxiv_id": "2510.03998v2",
      "title": "AI-Driven Grading and Moderation for Collaborative Projects in Computer Science Education",
      "title_zh": "",
      "authors": [
        "Songmei Yu",
        "Andrew Zagula"
      ],
      "abstract": "Collaborative group projects are integral to computer science education, as they foster teamwork, problem-solving skills, and industry-relevant competencies. However, assessing individual contributions in group settings has long been challenging. Traditional assessment strategies, such as the equal distribution of grades or subjective peer assessments, often fall short in terms of fairness, objectivity, and scalability, particularly in large classrooms. This paper introduces a semi-automated, AI-assisted grading system that evaluates both project quality and individual effort using repository mining, communication analytics, and machine learning models. The system comprises modules for project evaluation, contribution analysis, and grade computation, and integrates seamlessly with platforms such as GitHub. A pilot deployment in a senior-level course demonstrated high alignment with instructor assessments, increased student satisfaction, and reduced instructor grading effort. We conclude by discussing implementation considerations, ethical implications, and proposed enhancements to broaden applicability.",
      "tldr_zh": "",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at EISTA 2025. Published in the Journal of Systemics, Cybernetics and Informatics, 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03998v2",
      "published_date": "2025-10-05 02:16:52 UTC",
      "updated_date": "2026-01-06 22:45:41 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345192+00:00"
    },
    {
      "arxiv_id": "2510.03995v1",
      "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks",
      "title_zh": "",
      "authors": [
        "Nges Brian Njungle",
        "Eric Jahns",
        "Milan Stojkov",
        "Michel A. Kinsy"
      ],
      "abstract": "Deep learning has become a cornerstone of modern machine learning. It relies heavily on vast datasets and significant computational resources for high performance. This data often contains sensitive information, making privacy a major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as an energy-efficient alternative to conventional deep learning approaches. Nevertheless, SNNs still depend on large volumes of data, inheriting all the privacy challenges of deep learning. Homomorphic encryption addresses this challenge by allowing computations to be performed on encrypted data, ensuring data confidentiality throughout the entire processing pipeline. In this paper, we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire activation function: (1) a polynomial approximation algorithm designed for high-performance SNN inference, and (2) a novel scheme-switching algorithm that optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5 and ResNet-19 architectures, achieving encrypted inference accuracies of 98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as a viable and efficient solution for secure SNN inference, bridging the gap between energy-efficient deep neural networks and strong cryptographic privacy guarantees while outperforming prior encrypted SNN solutions.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03995v1",
      "published_date": "2025-10-05 02:11:40 UTC",
      "updated_date": "2025-10-05 02:11:40 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345229+00:00"
    },
    {
      "arxiv_id": "2510.03992v1",
      "title": "Quantifying Distributional Robustness of Agentic Tool-Selection",
      "title_zh": "",
      "authors": [
        "Jehyeok Yeon",
        "Isha Chaudhary",
        "Gagandeep Singh"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in agentic systems where they map user intents to relevant external tools to fulfill a task. A critical step in this process is tool selection, where a retriever first surfaces candidate tools from a larger pool, after which the LLM selects the most appropriate one. This pipeline presents an underexplored attack surface where errors in selection can lead to severe outcomes like unauthorized data access or denial of service, all without modifying the agent's model or code. While existing evaluations measure task performance in benign settings, they overlook the specific vulnerabilities of the tool selection mechanism under adversarial conditions. To address this gap, we introduce ToolCert, the first statistical framework that formally certifies tool selection robustness. ToolCert models tool selection as a Bernoulli success process and evaluates it against a strong, adaptive attacker who introduces adversarial tools with misleading metadata, and are iteratively refined based on the agent's previous choices. By sampling these adversarial interactions, ToolCert produces a high-confidence lower bound on accuracy, formally quantifying the agent's worst-case performance. Our evaluation with ToolCert uncovers the severe fragility: under attacks injecting deceptive tools or saturating retrieval, the certified accuracy bound drops near zero, an average performance drop of over 60% compared to non-adversarial settings. For attacks targeting the retrieval and selection stages, the certified accuracy bound plummets to less than 20% after just a single round of adversarial adaptation. ToolCert thus reveals previously unexamined security threats inherent to tool selection and provides a principled method to quantify an agent's robustness to such threats, a necessary step for the safe deployment of agentic systems.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03992v1",
      "published_date": "2025-10-05 01:50:34 UTC",
      "updated_date": "2025-10-05 01:50:34 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345267+00:00"
    },
    {
      "arxiv_id": "2510.03989v1",
      "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs",
      "title_zh": "",
      "authors": [
        "Xue-Cheng Tai",
        "Hao Liu",
        "Lingfeng Li",
        "Raymond H. Chan"
      ],
      "abstract": "The Transformer architecture has revolutionized the field of sequence modeling and underpins the recent breakthroughs in large language models (LLMs). However, a comprehensive mathematical theory that explains its structure and operations remains elusive. In this work, we propose a novel continuous framework that rigorously interprets the Transformer as a discretization of a structured integro-differential equation. Within this formulation, the self-attention mechanism emerges naturally as a non-local integral operator, and layer normalization is characterized as a projection to a time-dependent constraint. This operator-theoretic and variational perspective offers a unified and interpretable foundation for understanding the architecture's core components, including attention, feedforward layers, and normalization. Our approach extends beyond previous theoretical analyses by embedding the entire Transformer operation in continuous domains for both token indices and feature dimensions. This leads to a principled and flexible framework that not only deepens theoretical insight but also offers new directions for architecture design, analysis, and control-based interpretations. This new interpretation provides a step toward bridging the gap between deep learning architectures and continuous mathematical modeling, and contributes a foundational perspective to the ongoing development of interpretable and theoretically grounded neural network models.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03989v1",
      "published_date": "2025-10-05 01:16:08 UTC",
      "updated_date": "2025-10-05 01:16:08 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345304+00:00"
    },
    {
      "arxiv_id": "2510.03988v1",
      "title": "Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data",
      "title_zh": "",
      "authors": [
        "Hoang Anh Just",
        "Myeongseob Ko",
        "Ruoxi Jia"
      ],
      "abstract": "Distilling long reasoning traces (10K+ tokens) from stronger teacher models into smaller student LLMs via SFT has emerged as a standard paradigm. This approach is practical and efficient: it leverages the ease of generating abundant reasoning data from stronger models and provides a direct, data-driven way to teach less capable models better reasoning. While previous work has largely focused on prompt selection with responses from a single teacher, the equally important problem of choosing the best response when multiple teacher outputs are available for a single prompt remains underexplored. This challenge becomes important in a multi-teacher setting, where different students may benefit from the outputs of different teachers. This paper fills that gap with a systematic study of response selection for reasoning distillation. We first show that the current method, which picks responses the student assigns the highest global log-probability (global naturalness), fails when responses come from multiple teachers, i.e., global naturalness no longer correlates with downstream performance, especially as the reasoning traces from strong teachers become longer. To overcome this problem, we introduce Local Naturalness, which measures the student's log-probabilities over short, sequential reasoning steps conditioned only on a small local window. Local Naturalness enables two applications: 1) Teacher Selection: Aggregating local scores across prompts reliably identifies the most helpful teacher. 2) Response Selection from a Multiple Teachers: When mixing answers from many teachers, Local Naturalness boosts a 32B student's accuracy on math benchmarks by 9.4pp over global selection, also surpassing the performance achieved by training on data from the single best teacher. These results highlight the power of localized data quality evaluation and data mixing for more effective reasoning distillation.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.03988v1",
      "published_date": "2025-10-05 01:15:32 UTC",
      "updated_date": "2025-10-05 01:15:32 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:51:16.345342+00:00"
    }
  ],
  "processing_status": "in_progress",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 122,
  "processed_papers_count": 19,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": false,
  "daily_data_saved": false,
  "last_update": "2026-01-25T00:53:36.555332+00:00"
}
{
  "date": "2025-01-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-19 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 40 篇论文，主要聚焦人工智能领域，尤其是 Large Language Models (LLMs) 的整合、安全和应用创新，令人印象深刻的是 Yoshua Bengio 参与的 LLM 安全研究，以及多个高效的 LLM 调试和多模态模型优化工作，这些论文可能引发更多学术讨论。\n\n今天的核心论文多围绕 AI 安全和 LLM 优化展开，我将优先讨论这些高影响力文章（如涉及知名学者或热门话题的），并将相关论文归类讨论。对于其他领域或较常规的论文，我会简要掠过，以控制篇幅。以下是关键论文的提炼，每篇包括标题（中文 + 英文）和核心贡献。\n\n### LLM 安全与应用创新\n这些论文聚焦 LLM 的核心挑战，如安全、行为自省和应用扩展，是本日最热门主题。\n- **全面调查：将大型语言模型与基于知识的方法整合 (A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods)**  \n  作者包括知名学者 Michael Bain 和 Byeong Kang。该综述分析了 LLMs 与结构化知识库的整合，强调了在数据语境化、模型准确性和知识利用方面的优势。主要发现：识别当前研究空白，并提出伦理挑战的解决方案，为 AI 实际部署提供路径。\n  \n- **LLM 安全微调是否能更原则化？从网络安全吸取教训 (Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity)**  \n  作者包括著名 AI 学者 Yoshua Bengio。该论文将 LLM 安全微调比作网络安全的攻防游戏，指出现有防御易被绕过。主要贡献：借鉴历史案例，提出从架构层面设计安全模型的新方法，并展示对抗性攻击的不足，以提升 LLM 鲁棒性。\n\n- **LLM 关于自身行为的自觉：Tell me about yourself (Tell me about yourself: LLMs are aware of their learned behaviors)**  \n  这篇实验性工作显示，LLMs 能在无额外示例下描述自身行为（如高风险决策）。主要发现：模型可自省行为，包括后门策略的检测，但无法直接输出触发器，为 AI 安全提供新视角。\n\n- **用快乐结局故事欺骗 LLMs：Dagger Behind Smile (Dagger Behind Smile: Fool LLMs with a Happy Ending Story)**  \n  这篇研究揭示 LLMs 对正面提示更敏感，提出 Happy Ending Attack 方法，通过正面场景诱导越狱。主要贡献：高效地实现 88.79% 的攻击成功率，强调正面提示的潜在风险。\n\n其他 LLM 相关论文如 **ProKeR：基于核视角的视觉语言模型少样本适应 (ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models)** 和 **AdaptiveLog：大模型与小模型协作的日志分析框架 (AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration of Large and Small Language Model)**，前者通过核方法提升 CLIP 模型的少样本性能，后者优化了日志分析的成本和准确性，但细节较技术性，我这里简要提及以节省空间。\n\n### 计算机视觉与多模态模型\n这些论文展示了视觉任务的创新方法，相关性强。\n- **BF-STVSR：B-样条和傅立叶在时空视频超分辨率的伙伴 (BF-STVSR: B-Splines and Fourier-Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution)**  \n  该方法使用 B-样条和傅立叶映射优化视频超分辨率，解决了传统方法的时空细节问题。主要发现：在 PSNR 和 SSIM 等指标上达到 state-of-the-art，提升了视频的时空一致性。\n\n- **Pyramid-descent 视觉位置编码提升视觉语言模型 (Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding)**  \n  提出 PyPE 方法，通过分层位置编码改善视觉语言模型的多粒度感知。主要贡献：减少注意力衰减，提升模型在图像任务的泛化能力。\n\n其他视觉相关如 **TSVC：多传感器信号分解融合用于可穿戴设备活动识别 (Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition)**，它通过信号分解提升识别准确性，但作为应用性较强的辅助工作，我仅快速提及。\n\n### 其他领域快速掠过\n今天还有音乐、医疗和工程领域的论文，但影响力相对有限，我仅简要列出重点：\n- **音乐代理系统：MACAT 和 MACataRT (Musical Agent Systems: MACAT and MACataRT)**：开发 AI 音乐代理，支持实时即兴创作，强调伦理和艺术家中心设计。\n- **混沌工程自动化：ChaosEater (ChaosEater: Fully Automating Chaos Engineering with Large Language Models)**：用 LLMs 自动化系统故障注入，提升分布式系统韧性。\n- **自杀 ideation 检测：Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model**：使用混合模型和 SHAP 解释提升社交媒体检测准确率至 94.29%。\n- 其余如第 6 篇（视频复制检测优化）、第 12 篇（挪威问答数据集）和第 20 篇（卫星网络能量效率），这些更专业或应用导向，我这里不展开，以保持简洁。\n\n总之，今天的 arXiv 突显了 AI 领域的动态进展，尤其是 LLM 的安全和视觉优化。如果您对 LLM 或计算机视觉感兴趣，这些论文值得一读！下次快报见。",
  "papers": [
    {
      "arxiv_id": "2501.13947v3",
      "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Wenli Yang",
        "Lilian Some",
        "Michael Bain",
        "Byeong Kang"
      ],
      "abstract": "The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.",
      "tldr_zh": "这篇综述论文全面调查了将大型语言模型 (LLMs) 与基于知识的方法整合的可能性，旨在结合 LLMs 的生成语言理解能力与知识库的精确表示系统。论文通过文献分析，探讨了这种整合在实际应用中的优势，包括提升数据语境化、模型准确性和知识资源利用。研究还识别了技术、操作和伦理挑战，并评估了现有解决方案。最终，它总结了当前研究状况、主要差距，并提出未来路径，以推动 AI 技术的进步和跨领域部署。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13947v3",
      "published_date": "2025-01-19 23:25:21 UTC",
      "updated_date": "2025-05-01 03:29:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T00:59:41.562464"
    },
    {
      "arxiv_id": "2501.14815v1",
      "title": "A VM-HDL Co-Simulation Framework for Systems with PCIe-Connected FPGAs",
      "title_zh": "翻译失败",
      "authors": [
        "Shenghsun Cho",
        "Mrunal Patel",
        "Basavaraj Kaladagi",
        "Han Chen",
        "Tapti Palit",
        "Michael Ferdman",
        "Peter Milder"
      ],
      "abstract": "PCIe-connected FPGAs are gaining popularity as an accelerator technology in\ndata centers. However, it is challenging to jointly develop and debug host\nsoftware and FPGA hardware. Changes to the hardware design require a\ntime-consuming FPGA synthesis process, and modification to the software,\nespecially the operating system and device drivers, can frequently cause the\nsystem to hang, without providing enough information for debugging. The\ncombination of these problems results in long debug iterations and a slow\ndevelopment process. To overcome these problems, we designed a VM-HDL\nco-simulation framework, which is capable of running the same software,\noperating system, and hardware designs as the target physical system, while\nproviding full visibility and significantly shorter debug iterations.",
      "tldr_zh": "该论文针对 PCIe-connected FPGAs 在数据中心作为加速器的应用，指出开发和调试主机软件与硬件面临的挑战，包括硬件设计更改需耗时的 FPGA synthesis 过程，以及软件修改（如操作系统和设备驱动）可能导致系统挂起和调试信息不足，从而延长迭代时间。  \n为解决这些问题，研究者提出了一种 VM-HDL co-simulation framework，该框架能够模拟运行与目标物理系统相同的软件、操作系统和硬件设计。  \n该框架提供全面的可见性，并显著缩短调试迭代周期，从而加速整体开发过程。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14815v1",
      "published_date": "2025-01-19 22:06:36 UTC",
      "updated_date": "2025-01-19 22:06:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T00:59:53.538190"
    },
    {
      "arxiv_id": "2502.00023v1",
      "title": "Musical Agent Systems: MACAT and MACataRT",
      "title_zh": "翻译失败",
      "authors": [
        "Keon Ju M. Lee",
        "Philippe Pasquier"
      ],
      "abstract": "Our research explores the development and application of musical agents,\nhuman-in-the-loop generative AI systems designed to support music performance\nand improvisation within co-creative spaces. We introduce MACAT and MACataRT,\ntwo distinct musical agent systems crafted to enhance interactive music-making\nbetween human musicians and AI. MACAT is optimized for agent-led performance,\nemploying real-time synthesis and self-listening to shape its output\nautonomously, while MACataRT provides a flexible environment for collaborative\nimprovisation through audio mosaicing and sequence-based learning. Both systems\nemphasize training on personalized, small datasets, fostering ethical and\ntransparent AI engagement that respects artistic integrity. This research\nhighlights how interactive, artist-centred generative AI can expand creative\npossibilities, empowering musicians to explore new forms of artistic expression\nin real-time, performance-driven and music improvisation contexts.",
      "tldr_zh": "本研究开发了两种音乐代理系统——MACAT 和 MACataRT，作为人类参与的生成式 AI 系统，用于支持音乐表演和即兴创作。MACAT 专注于代理主导的表演，通过实时合成和自监听技术自主塑造输出，而 MACataRT 提供灵活的协作环境，利用音频 mosaicing 和序列-based 学习促进即兴互动。两者均基于个性化小数据集训练，确保道德透明和艺术完整性。该研究展示了交互式、艺术家中心的生成 AI 如何扩展音乐家的创作可能性，在实时表演和即兴上下文中探索新艺术形式。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MA",
      "comment": "In Proceedings of the Creativity and Generative AI NIPS (Neural\n  Information Processing Systems) Workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.00023v1",
      "published_date": "2025-01-19 22:04:09 UTC",
      "updated_date": "2025-01-19 22:04:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:00:05.347811"
    },
    {
      "arxiv_id": "2501.11183v1",
      "title": "Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity",
      "title_zh": "翻译失败",
      "authors": [
        "David Williams-King",
        "Linh Le",
        "Adam Oberman",
        "Yoshua Bengio"
      ],
      "abstract": "As LLMs develop increasingly advanced capabilities, there is an increased\nneed to minimize the harm that could be caused to society by certain model\noutputs; hence, most LLMs have safety guardrails added, for example via\nfine-tuning. In this paper, we argue the position that current safety\nfine-tuning is very similar to a traditional cat-and-mouse game (or arms race)\nbetween attackers and defenders in cybersecurity. Model jailbreaks and attacks\nare patched with bandaids to target the specific attack mechanism, but many\nsimilar attack vectors might remain. When defenders are not proactively coming\nup with principled mechanisms, it becomes very easy for attackers to sidestep\nany new defenses. We show how current defenses are insufficient to prevent new\nadversarial jailbreak attacks, reward hacking, and loss of control problems. In\norder to learn from past mistakes in cybersecurity, we draw analogies with\nhistorical examples and develop lessons learned that can be applied to LLM\nsafety. These arguments support the need for new and more principled approaches\nto designing safe models, which are architected for security from the\nbeginning. We describe several such approaches from the AI literature.",
      "tldr_zh": "本论文探讨了大型语言模型（LLMs）的安全微调是否能更具原则性，通过与网络安全的攻击防御竞赛（cat-and-mouse game）类比，指出当前方法仅针对特定攻击（如jailbreaks）进行修补，容易被绕过，导致新adversarial jailbreak attacks、reward hacking和loss of control problems等问题持续存在。作者从网络安全历史教训中总结经验，强调需要从一开始就设计出以安全为本的模型架构。论文支持采用更原则性的方法，并介绍了AI文献中的几种潜在解决方案，以提升LLMs的安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "I.2.7; D.4.6"
      ],
      "primary_category": "cs.CR",
      "comment": "published at Neurips Safe Generative AI Workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.11183v1",
      "published_date": "2025-01-19 21:49:42 UTC",
      "updated_date": "2025-01-19 21:49:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:00:17.708941"
    },
    {
      "arxiv_id": "2501.11175v1",
      "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yassir Bendou",
        "Amine Ouasfi",
        "Vincent Gripon",
        "Adnane Boukhayma"
      ],
      "abstract": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
      "tldr_zh": "本文从 kernel perspective 重新审视 few-shot adaptation 的 training-free 方法，如 Tip-Adapter，揭示其作为 local adapters 与 kernel literature 的关联，并强调加入 global information 的重要性。作者提出 ProKeR（Proximal Kernel ridge Regression）方法，该方法在 reproducing kernel Hilbert space (RKHS) 中学习 proximal regularizer，并提供 closed-form solution。实验结果显示，ProKeR 在 11 个数据集上实现了 state-of-the-art 性能，显著提升了大型视觉语言模型如 CLIP 的适应能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code available at https://ybendou.github.io/ProKeR",
      "pdf_url": "http://arxiv.org/pdf/2501.11175v1",
      "published_date": "2025-01-19 21:25:53 UTC",
      "updated_date": "2025-01-19 21:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:00:30.277810"
    },
    {
      "arxiv_id": "2501.11171v1",
      "title": "Counteracting temporal attacks in Video Copy Detection",
      "title_zh": "在视频复制检测中对抗时间攻击",
      "authors": [
        "Katarzyna Fojcik",
        "Piotr Syga"
      ],
      "abstract": "Video Copy Detection (VCD) plays a crucial role in copyright protection and\ncontent verification by identifying duplicates and near-duplicates in\nlarge-scale video databases. The META AI Challenge on video copy detection\nprovided a benchmark for evaluating state-of-the-art methods, with the\nDual-level detection approach emerging as a winning solution. This method\nintegrates Video Editing Detection and Frame Scene Detection to handle\nadversarial transformations and large datasets efficiently. However, our\nanalysis reveals significant limitations in the VED component, particularly in\nits ability to handle exact copies. Moreover, Dual-level detection shows\nvulnerability to temporal attacks. To address it, we propose an improved frame\nselection strategy based on local maxima of interframe differences, which\nenhances robustness against adversarial temporal modifications while\nsignificantly reducing computational overhead. Our method achieves an increase\nof 1.4 to 5.8 times in efficiency over the standard 1 FPS approach. Compared to\nDual-level detection method, our approach maintains comparable micro-average\nprecision ($\\mu$AP) while also demonstrating improved robustness against\ntemporal attacks. Given 56\\% reduced representation size and the inference time\nof more than 2 times faster, our approach is more suitable to real-world\nresource restriction.",
      "tldr_zh": "该论文分析了视频复制检测 (Video Copy Detection) 中的 temporal attacks 问题，指出现有 Dual-level detection 方法在处理精确复制和时间攻击时存在局限性，特别是其 Video Editing Detection 组件的不足。作者提出了一种改进的帧选择策略，基于帧间差异的局部最大值，以增强对对抗性时间修改的鲁棒性，同时显著降低计算开销。实验结果显示，该方法比标准 1 FPS 策略效率提高 1.4 到 5.8 倍，与 Dual-level detection 相比保持了可比的微平均精度 (μAP)，并实现了 56% 的表示大小减少和超过 2 倍的推理速度提升，使其更适用于资源受限的真实世界场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.11171v1",
      "published_date": "2025-01-19 21:16:39 UTC",
      "updated_date": "2025-01-19 21:16:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:00:43.000927"
    },
    {
      "arxiv_id": "2501.11170v1",
      "title": "AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Ghahramani Kure",
        "Mahshid Dehghani",
        "Mohammad Mahdi Abootorabi",
        "Nona Ghazizadeh",
        "Seyed Arshan Dalili",
        "Ehsaneddin Asgari"
      ],
      "abstract": "The SemEval-2024 Task 3 presents two subtasks focusing on emotion-cause pair\nextraction within conversational contexts. Subtask 1 revolves around the\nextraction of textual emotion-cause pairs, where causes are defined and\nannotated as textual spans within the conversation. Conversely, Subtask 2\nextends the analysis to encompass multimodal cues, including language, audio,\nand vision, acknowledging instances where causes may not be exclusively\nrepresented in the textual data. Our proposed model for emotion-cause analysis\nis meticulously structured into three core segments: (i) embedding extraction,\n(ii) cause-pair extraction & emotion classification, and (iii) cause extraction\nusing QA after finding pairs. Leveraging state-of-the-art techniques and\nfine-tuning on task-specific datasets, our model effectively unravels the\nintricate web of conversational dynamics and extracts subtle cues signifying\ncausality in emotional expressions. Our team, AIMA, demonstrated strong\nperformance in the SemEval-2024 Task 3 competition. We ranked as the 10th in\nsubtask 1 and the 6th in subtask 2 out of 23 teams.",
      "tldr_zh": "本文介绍了AIMA团队在SemEval-2024 Task 3中的参赛模型，该模型针对情感-原因对提取进行简单而高效的分析，包括Subtask 1（文本情感-原因对提取）和Subtask 2（多模态情感-原因分析）。模型由三个核心部分组成：(i) embedding extraction，(ii) 原因对提取与情感分类，以及(iii) 使用QA的方法提取原因，通过在任务特定数据集上微调来捕捉对话中的因果线索。实验结果显示，该模型在Subtask 1中排名10th，在Subtask 2中排名6th，共23支队伍，展示了其在情感分析领域的强大性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024)",
      "pdf_url": "http://arxiv.org/pdf/2501.11170v1",
      "published_date": "2025-01-19 21:16:31 UTC",
      "updated_date": "2025-01-19 21:16:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:00:54.866022"
    },
    {
      "arxiv_id": "2501.16354v1",
      "title": "Adaptive Hoeffding Tree with Transfer Learning for Streaming Synchrophasor Data Sets",
      "title_zh": "翻译失败",
      "authors": [
        "Zakaria El Mrabet",
        "Daisy Flora Selvaraj",
        "Prakash Ranganathan"
      ],
      "abstract": "Synchrophasor technology or phasor measurement units (PMUs) are known to\ndetect multiple type of oscillations or faults better than Supervisory Control\nand Data Acquisition (SCADA) systems, but the volume of Bigdata (e.g., 30-120\nsamples per second on a single PMU) generated by these sensors at the\naggregator level (e.g., several PMUs) requires special handling. Conventional\nmachine learning or data mining methods are not suitable to handle such larger\nstreaming realtime data. This is primarily due to latencies associated with\ncloud environments (e.g., at an aggregator or PDC level), and thus necessitates\nthe need for local computing to move the data on the edge (or locally at the\nPMU level) for processing. This requires faster real-time streaming algorithms\nto be processed at the local level (e.g., typically by a Field Programmable\nGate Array (FPGA) based controllers). This paper proposes a transfer\nlearning-based hoeffding tree with ADWIN (THAT) method to detect anomalous\nsynchrophasor signatures. The proposed algorithm is trained and tested with the\nOzaBag method. The preliminary results with transfer learning indicate that a\ncomputational time saving of 0.7ms is achieved with THAT algorithm (0.34ms)\nover Ozabag (1.04ms), while the accuracy of both methods in detecting fault\nevents remains at 94% for four signatures.",
      "tldr_zh": "该论文针对 synchrophasor 数据（如 PMUs 生成的实时流数据）处理问题，提出了一种基于转移学习（transfer learning）的自适应 Hoeffding Tree 与 ADWIN 结合的 THAT 方法，以实现高效检测异常签名。传统机器学习方法因延迟问题不适合处理此类大数据，因此该方法强调在本地（如 FPGA 设备）进行边缘计算。实验结果显示，THAT 算法相较于 OzaBag 方法节省了 0.7ms 计算时间（0.34ms vs 1.04ms），同时保持 94% 的准确率，为实时流数据处理提供了更高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.16354v1",
      "published_date": "2025-01-19 21:10:01 UTC",
      "updated_date": "2025-01-19 21:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:01:05.894718"
    },
    {
      "arxiv_id": "2501.11166v1",
      "title": "AIMA at SemEval-2024 Task 10: History-Based Emotion Recognition in Hindi-English Code-Mixed Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Mahdi Abootorabi",
        "Nona Ghazizadeh",
        "Seyed Arshan Dalili",
        "Alireza Ghahramani Kure",
        "Mahshid Dehghani",
        "Ehsaneddin Asgari"
      ],
      "abstract": "In this study, we introduce a solution to the SemEval 2024 Task 10 on subtask\n1, dedicated to Emotion Recognition in Conversation (ERC) in code-mixed\nHindi-English conversations. ERC in code-mixed conversations presents unique\nchallenges, as existing models are typically trained on monolingual datasets\nand may not perform well on code-mixed data. To address this, we propose a\nseries of models that incorporate both the previous and future context of the\ncurrent utterance, as well as the sequential information of the conversation.\nTo facilitate the processing of code-mixed data, we developed a\nHinglish-to-English translation pipeline to translate the code-mixed\nconversations into English. We designed four different base models, each\nutilizing powerful pre-trained encoders to extract features from the input but\nwith varying architectures. By ensembling all of these models, we developed a\nfinal model that outperforms all other baselines.",
      "tldr_zh": "本文针对 SemEval-2024 Task 10 子任务1，提出了一种基于历史上下文的情感识别方法，用于处理 Hindi-English code-mixed 对话中的 Emotion Recognition in Conversation (ERC)。该方法整合了当前话语的前后文和对话顺序信息，并开发了 Hinglish-to-English translation 管道，将 code-mixed 数据翻译成英语，以提升模型处理能力。研究设计了四个基于预训练编码器的基模型，并通过集成这些模型，实现了比基线模型更好的性能表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024)",
      "pdf_url": "http://arxiv.org/pdf/2501.11166v1",
      "published_date": "2025-01-19 20:56:45 UTC",
      "updated_date": "2025-01-19 20:56:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:01:18.535896"
    },
    {
      "arxiv_id": "2501.11140v1",
      "title": "CLOFAI: A Dataset of Real And Fake Image Classification Tasks for Continual Learning",
      "title_zh": "CLOFAI：一个用于持续学习的真实与假冒图像分类任务数据集",
      "authors": [
        "William Doherty",
        "Anton Lee",
        "Heitor Murilo Gomes"
      ],
      "abstract": "The rapid advancement of generative AI models capable of creating realistic\nmedia has led to a need for classifiers that can accurately distinguish between\ngenuine and artificially-generated images. A significant challenge for these\nclassifiers emerges when they encounter images from generative models that are\nnot represented in their training data, usually resulting in diminished\nperformance. A typical approach is to periodically update the classifier's\ntraining data with images from the new generative models then retrain the\nclassifier on the updated dataset. However, in some real-life scenarios,\nstorage, computational, or privacy constraints render this approach\nimpractical. Additionally, models used in security applications may be required\nto rapidly adapt. In these circumstances, continual learning provides a\npromising alternative, as the classifier can be updated without retraining on\nthe entire dataset. In this paper, we introduce a new dataset called CLOFAI\n(Continual Learning On Fake and Authentic Images), which takes the form of a\ndomain-incremental image classification problem. Moreover, we showcase the\napplicability of this dataset as a benchmark for evaluating continual learning\nmethodologies. In doing this, we set a baseline on our novel dataset using\nthree foundational continual learning methods -- EWC, GEM, and Experience\nReplay -- and find that EWC performs poorly, while GEM and Experience Replay\nshow promise, performing significantly better than a Naive baseline. The\ndataset and code to run the experiments can be accessed from the following\nGitHub repository: https://github.com/Will-Doherty/CLOFAI.",
      "tldr_zh": "这篇论文引入了CLOFAI数据集，用于持续学习（continual learning）中的真实和假图像分类任务，旨在解决分类器面对新生成模型图像时性能下降的问题，而无需重新训练整个数据集。数据集以domain-incremental图像分类形式设计，作为评估持续学习方法的基准。作者通过实验比较了EWC、GEM和Experience Replay三种方法，发现GEM和Experience Replay的表现显著优于Naive基准，而EWC效果较差。数据集和相关代码可在GitHub上获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11140v1",
      "published_date": "2025-01-19 18:53:30 UTC",
      "updated_date": "2025-01-19 18:53:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:01:29.921469"
    },
    {
      "arxiv_id": "2501.11135v1",
      "title": "Playing the Lottery With Concave Regularizers for Sparse Trainable Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Giulia Fracastoro",
        "Sophie M. Fosson",
        "Andrea Migliorati",
        "Giuseppe C. Calafiore"
      ],
      "abstract": "The design of sparse neural networks, i.e., of networks with a reduced number\nof parameters, has been attracting increasing research attention in the last\nfew years. The use of sparse models may significantly reduce the computational\nand storage footprint in the inference phase. In this context, the lottery\nticket hypothesis (LTH) constitutes a breakthrough result, that addresses not\nonly the performance of the inference phase, but also of the training phase. It\nstates that it is possible to extract effective sparse subnetworks, called\nwinning tickets, that can be trained in isolation. The development of effective\nmethods to play the lottery, i.e., to find winning tickets, is still an open\nproblem. In this article, we propose a novel class of methods to play the\nlottery. The key point is the use of concave regularization to promote the\nsparsity of a relaxed binary mask, which represents the network topology. We\ntheoretically analyze the effectiveness of the proposed method in the convex\nframework. Then, we propose extended numerical tests on various datasets and\narchitectures, that show that the proposed method can improve the performance\nof state-of-the-art algorithms.",
      "tldr_zh": "这篇论文探讨了稀疏神经网络的设计，旨在通过减少参数数量来降低推理阶段的计算和存储开销，并基于Lottery Ticket Hypothesis (LTH)假设来提取可独立训练的winning tickets。作者提出了一种新方法，使用concave regularizers对松弛二进制掩码进行正则化，以促进网络拓扑的稀疏性，并在凸框架下进行了理论分析。实验结果显示，该方法在多种数据集和架构上比现有最先进算法的性能有所提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11135v1",
      "published_date": "2025-01-19 18:05:13 UTC",
      "updated_date": "2025-01-19 18:05:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:01:41.836253"
    },
    {
      "arxiv_id": "2501.11128v1",
      "title": "A Collection of Question Answering Datasets for Norwegian",
      "title_zh": "翻译失败",
      "authors": [
        "Vladislav Mikhailov",
        "Petter Mæhlum",
        "Victoria Ovedie Chruickshank Langø",
        "Erik Velldal",
        "Lilja Øvrelid"
      ],
      "abstract": "This paper introduces a new suite of question answering datasets for\nNorwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The\ndata covers a wide range of skills and knowledge domains, including world\nknowledge, commonsense reasoning, truthfulness, and knowledge about Norway.\nCovering both of the written standards of Norwegian - Bokm{\\aa}l and Nynorsk -\nour datasets comprise over 10k question-answer pairs, created by native\nspeakers. We detail our dataset creation approach and present the results of\nevaluating 11 language models (LMs) in zero- and few-shot regimes. Most LMs\nperform better in Bokm{\\aa}l than Nynorsk, struggle most with commonsense\nreasoning, and are often untruthful in generating answers to questions. All our\ndatasets and annotation materials are publicly available.",
      "tldr_zh": "本论文引入了四个挪威语 Question Answering 数据集，包括 NorOpenBookQA、NorCommonSenseQA、NorTruthfulQA 和 NRK-Quiz-QA，这些数据集涵盖世界知识、常识推理、真实性和挪威相关知识，总共超过 10k 个问题-答案对，由母语者创建并支持 Bokmål 和 Nynorsk 两种书面标准。研究详细描述了数据集的创建方法，并评估了 11 个语言模型（LMs）在零样本和少样本模式下的性能。结果显示，大多数 LMs 在 Bokmål 上表现更好，在常识推理任务上挣扎，且在生成答案时经常不真实，所有数据集和相关材料均已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for NoDaLiDa / Baltic-HLT 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11128v1",
      "published_date": "2025-01-19 17:42:48 UTC",
      "updated_date": "2025-01-19 17:42:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:01:55.275936"
    },
    {
      "arxiv_id": "2501.11120v1",
      "title": "Tell me about yourself: LLMs are aware of their learned behaviors",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Betley",
        "Xuchan Bao",
        "Martín Soto",
        "Anna Sztyber-Betley",
        "James Chua",
        "Owain Evans"
      ],
      "abstract": "We study behavioral self-awareness -- an LLM's ability to articulate its\nbehaviors without requiring in-context examples. We finetune LLMs on datasets\nthat exhibit particular behaviors, such as (a) making high-risk economic\ndecisions, and (b) outputting insecure code. Despite the datasets containing no\nexplicit descriptions of the associated behavior, the finetuned LLMs can\nexplicitly describe it. For example, a model trained to output insecure code\nsays, ``The code I write is insecure.'' Indeed, models show behavioral\nself-awareness for a range of behaviors and for diverse evaluations. Note that\nwhile we finetune models to exhibit behaviors like writing insecure code, we do\nnot finetune them to articulate their own behaviors -- models do this without\nany special training or examples.\n  Behavioral self-awareness is relevant for AI safety, as models could use it\nto proactively disclose problematic behaviors. In particular, we study backdoor\npolicies, where models exhibit unexpected behaviors only under certain trigger\nconditions. We find that models can sometimes identify whether or not they have\na backdoor, even without its trigger being present. However, models are not\nable to directly output their trigger by default.\n  Our results show that models have surprising capabilities for self-awareness\nand for the spontaneous articulation of implicit behaviors. Future work could\ninvestigate this capability for a wider range of scenarios and models\n(including practical scenarios), and explain how it emerges in LLMs.",
      "tldr_zh": "本文研究大型语言模型(LLMs)的行为自我意识(behavioral self-awareness)，即模型无需上下文示例即可描述自身学到的行为。通过在特定数据集上微调LLMs，使其展示如高风险经济决策或输出不安全代码等行为，实验发现这些模型能主动阐述这些隐含行为，例如声称“我写的代码不安全”。这一能力适用于多种行为评估，并对AI安全有重要意义，因为模型可能主动披露问题行为，如识别自身是否具有后门策略(backdoor policies)。然而，模型默认无法直接输出触发条件，未来工作应进一步探索这一现象在更广泛场景中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to ICLR 2025. 17 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11120v1",
      "published_date": "2025-01-19 17:28:12 UTC",
      "updated_date": "2025-01-19 17:28:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:02:06.545458"
    },
    {
      "arxiv_id": "2501.11114v1",
      "title": "Clinical trial cohort selection using Large Language Models on n2c2 Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Chi-en Amy Tai",
        "Xavier Tannier"
      ],
      "abstract": "Clinical trials are a critical process in the medical field for introducing\nnew treatments and innovations. However, cohort selection for clinical trials\nis a time-consuming process that often requires manual review of patient text\nrecords for specific keywords. Though there have been studies on standardizing\nthe information across the various platforms, Natural Language Processing (NLP)\ntools remain crucial for spotting eligibility criteria in textual reports.\nRecently, pre-trained large language models (LLMs) have gained popularity for\nvarious NLP tasks due to their ability to acquire a nuanced understanding of\ntext. In this paper, we study the performance of large language models on\nclinical trial cohort selection and leverage the n2c2 challenges to benchmark\ntheir performance. Our results are promising with regard to the incorporation\nof LLMs for simple cohort selection tasks, but also highlight the difficulties\nencountered by these models as soon as fine-grained knowledge and reasoning are\nrequired.",
      "tldr_zh": "本文研究了使用 Large Language Models (LLMs) 进行临床试验队列选择的性能问题，旨在通过 Natural Language Processing (NLP) 工具从患者文本记录中识别资格标准，以简化这一耗时过程。主要方法包括利用预训练 LLMs 并以 n2c2 Challenges 作为基准测试，结果显示 LLMs 在简单队列选择任务中表现出色，但当涉及精细知识和推理时，模型面临显著挑战。该研究强调了 LLMs 在医疗 NLP 应用中的潜力，同时突出了改进需求。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11114v1",
      "published_date": "2025-01-19 17:07:02 UTC",
      "updated_date": "2025-01-19 17:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:02:18.181723"
    },
    {
      "arxiv_id": "2501.11107v2",
      "title": "ChaosEater: Fully Automating Chaos Engineering with Large Language Models",
      "title_zh": "ChaosEater：利用大型语言模型实现混沌工程的完全自动化",
      "authors": [
        "Daisuke Kikuta",
        "Hiroki Ikeuchi",
        "Kengo Tajiri"
      ],
      "abstract": "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools implement the automated execution of predefined\nCE experiments. However, defining these experiments and improving the system\nbased on the experimental results still remain manual. To reduce the costs of\nthe manual operations, we propose ChaosEater, a system for automating the\nentire CE operations with Large Language Models (LLMs). It predefines the\nagentic workflow according to a systematic CE cycle and assigns subdivided\noperations within the workflow to LLMs. ChaosEater targets CE for Kubernetes\nsystems, which are managed through code (i.e., Infrastructure as Code).\nTherefore, the LLMs in ChaosEater perform software engineering tasks to\ncomplete CE cycles, including requirement definition, code generation,\ndebugging, and testing. We evaluate ChaosEater through case studies on both\nsmall and large Kubernetes systems. The results demonstrate that it stably\ncompletes reasonable single CE cycles with significantly low time and monetary\ncosts. The CE cycles are also qualitatively validated by human engineers and\nLLMs.",
      "tldr_zh": "本文提出ChaosEater，一种利用Large Language Models (LLMs)实现Chaos Engineering (CE)完全自动化的系统，旨在减少手动定义实验和系统改进的成本。ChaosEater通过预定义的代理工作流，分配子任务如需求定义、代码生成、调试和测试给LLMs，并针对Kubernetes系统进行优化。实验结果显示，该系统在小型和大型Kubernetes系统中稳定完成CE周期，时间和金钱成本显著降低，且结果经人类工程师和LLMs定性验证。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.NI"
      ],
      "primary_category": "cs.SE",
      "comment": "114 pages (7 main), 11 figures. Project page:\n  https://ntt-dkiku.github.io/chaos-eater",
      "pdf_url": "http://arxiv.org/pdf/2501.11107v2",
      "published_date": "2025-01-19 16:35:09 UTC",
      "updated_date": "2025-04-16 03:33:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:02:30.029372"
    },
    {
      "arxiv_id": "2501.11094v1",
      "title": "Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model",
      "title_zh": "翻译失败",
      "authors": [
        "Mohaiminul Islam Bhuiyan",
        "Nur Shazwani Kamarudin",
        "Nur Hafieza Ismail"
      ],
      "abstract": "Suicidal ideation detection is crucial for preventing suicides, a leading\ncause of death worldwide. Many individuals express suicidal thoughts on social\nmedia, offering a vital opportunity for early detection through advanced\nmachine learning techniques. The identification of suicidal ideation in social\nmedia text is improved by utilising a hybrid framework that integrates\nConvolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory\n(BiLSTM), enhanced with an attention mechanism. To enhance the interpretability\nof the model's predictions, Explainable AI (XAI) methods are applied, with a\nparticular focus on SHapley Additive exPlanations (SHAP), are incorporated. At\nfirst, the model managed to reach an accuracy of 92.81%. By applying\nfine-tuning and early stopping techniques, the accuracy improved to 94.29%. The\nSHAP analysis revealed key features influencing the model's predictions, such\nas terms related to mental health struggles. This level of transparency boosts\nthe model's credibility while helping mental health professionals understand\nand trust the predictions. This work highlights the potential for improving the\naccuracy and interpretability of detecting suicidal tendencies, making a\nvaluable contribution to the progress of mental health monitoring systems. It\nemphasizes the significance of blending powerful machine learning methods with\nexplainability to develop reliable and impactful mental health solutions.",
      "tldr_zh": "本研究提出了一种增强社交媒体自杀意念检测框架，利用 CNN-BiLSTM 混合模型结合注意力机制，来分析用户文本并提高检测准确性。模型通过 XAI 方法，特别是 SHAP 解释器，揭示了影响预测的关键特征，如与心理健康斗争相关的术语，从而提升了模型的可解释性和可信度。实验结果显示，初始准确率达 92.81%，经微调和早停技术优化后提升至 94.29%。这项工作为心理健康监测系统的发展提供了可靠的机器学习解决方案，强调了将强大算法与解释性相结合的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11094v1",
      "published_date": "2025-01-19 16:08:50 UTC",
      "updated_date": "2025-01-19 16:08:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:02:41.751784"
    },
    {
      "arxiv_id": "2501.11087v1",
      "title": "Leveraging counterfactual concepts for debugging and improving CNN model performance",
      "title_zh": "利用反事实概念来调试和改进CNN模型性能",
      "authors": [
        "Syed Ali Tariq",
        "Tehseen Zia"
      ],
      "abstract": "Counterfactual explanation methods have recently received significant\nattention for explaining CNN-based image classifiers due to their ability to\nprovide easily understandable explanations that align more closely with human\nreasoning. However, limited attention has been given to utilizing\nexplainability methods to improve model performance. In this paper, we propose\nto leverage counterfactual concepts aiming to enhance the performance of CNN\nmodels in image classification tasks. Our proposed approach utilizes\ncounterfactual reasoning to identify crucial filters used in the\ndecision-making process. Following this, we perform model retraining through\nthe design of a novel methodology and loss functions that encourage the\nactivation of class-relevant important filters and discourage the activation of\nirrelevant filters for each class. This process effectively minimizes the\ndeviation of activation patterns of local predictions and the global activation\npatterns of their respective inferred classes. By incorporating counterfactual\nexplanations, we validate unseen model predictions and identify\nmisclassifications. The proposed methodology provides insights into potential\nweaknesses and biases in the model's learning process, enabling targeted\nimprovements and enhanced performance. Experimental results on publicly\navailable datasets have demonstrated an improvement of 1-2\\%, validating the\neffectiveness of the approach.",
      "tldr_zh": "本论文提出一种利用反事实概念（counterfactual concepts）的方法，来调试和提升 CNN 模型在图像分类任务中的性能。通过反事实推理，论文识别出决策过程中关键的 filters，并设计新型损失函数重新训练模型，以鼓励激活类别相关的 filters 并抑制无关 filters，从而最小化局部预测激活模式与全局激活模式的偏差。该方法还通过反事实解释验证模型预测并识别误分类，提供对模型弱点和偏差的洞察；实验在公开数据集上显示性能提升 1-2%，验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This manuscript is currently under consideration for publication in\n  Pattern Recognition Letters",
      "pdf_url": "http://arxiv.org/pdf/2501.11087v1",
      "published_date": "2025-01-19 15:50:33 UTC",
      "updated_date": "2025-01-19 15:50:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:02:52.969032"
    },
    {
      "arxiv_id": "2501.11086v1",
      "title": "Can LLM Generate Regression Tests for Software Commits?",
      "title_zh": "大型语言模型能否为软件提交生成回归测试？",
      "authors": [
        "Jing Liu",
        "Seongmin Lee",
        "Eleonora Losiouk",
        "Marcel Böhme"
      ],
      "abstract": "Large Language Models (LLMs) have shown tremendous promise in automated\nsoftware engineering. In this paper, we investigate the opportunities of LLMs\nfor automatic regression test generation for programs that take highly\nstructured, human-readable inputs, such as XML parsers or JavaScript\ninterpreters. Concretely, we explore the following regression test generation\nscenarios for such programs that have so far been difficult to test\nautomatically in the absence of corresponding input grammars:\n  $\\bullet$ Bug finding. Given a code change (e.g., a commit or pull request),\nour LLM-based approach generates a test case with the objective of revealing\nany bugs that might be introduced if that change is applied.\n  $\\bullet$ Patch testing. Given a patch, our LLM-based approach generates a\ntest case that fails before but passes after the patch. This test can be added\nto the regression test suite to catch similar bugs in the future.\n  We implement Cleverest, a feedback-directed, zero-shot LLM-based regression\ntest generation technique, and evaluate its effectiveness on 22 commits to\nthree subject programs: Mujs, Libxml2, and Poppler. For programs using more\nhuman-readable file formats, like XML or JavaScript, we found Cleverest\nperformed very well. It generated easy-to-understand bug-revealing or\nbug-reproduction test cases for the majority of commits in just under three\nminutes -- even when only the code diff or commit message (unless it was too\nvague) was given. For programs with more compact file formats, like PDF, as\nexpected, it struggled to generate effective test cases. However, the\nLLM-supplied test cases are not very far from becoming effective (e.g., when\nused as a seed by a greybox fuzzer or as a starting point by the developer).",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLM）在为软件提交生成回归测试方面的潜力，针对处理结构化输入的程序（如 XML 解析器或 JavaScript 解释器）。他们开发了 Cleverest，一种基于反馈导向的零样本 LLM 技术，支持两种场景：bug finding（生成测试用例揭示代码变更引入的 bug）和 patch testing（生成在补丁前失败、后通过的测试用例）。在对 Mujs、Libxml2 和 Poppler 的 22 个提交评估中，Cleverest 在处理人类可读格式（如 XML 或 JavaScript）时表现优秀，能在不到三分钟内生成有效的测试用例，而在紧凑格式（如 PDF）上效果较差，但生成的测试用例可作为后续优化的起点。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "18 pages. This version of the paper was written on Thu, 12 Sep 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.11086v1",
      "published_date": "2025-01-19 15:46:26 UTC",
      "updated_date": "2025-01-19 15:46:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:03:06.299563"
    },
    {
      "arxiv_id": "2501.16353v1",
      "title": "Synthetic Data Generation by Supervised Neural Gas Network for Physiological Emotion Recognition Data",
      "title_zh": "翻译失败",
      "authors": [
        "S. Muhammad Hossein Mousavi"
      ],
      "abstract": "Data scarcity remains a significant challenge in the field of emotion\nrecognition using physiological signals, as acquiring comprehensive and diverse\ndatasets is often prevented by privacy concerns and logistical constraints.\nThis limitation restricts the development and generalization of robust emotion\nrecognition models, making the need for effective synthetic data generation\nmethods more critical. Emotion recognition from physiological signals such as\nEEG, ECG, and GSR plays a pivotal role in enhancing human-computer interaction\nand understanding human affective states. Utilizing these signals, this study\nintroduces an innovative approach to synthetic data generation using a\nSupervised Neural Gas (SNG) network, which has demonstrated noteworthy speed\nadvantages over established models like Conditional VAE, Conditional GAN,\ndiffusion model, and Variational LSTM. The Neural Gas network, known for its\nadaptability in organizing data based on topological and feature-space\nproximity, provides a robust framework for generating real-world-like synthetic\ndatasets that preserve the intrinsic patterns of physiological emotion data.\nOur implementation of the SNG efficiently processes the input data, creating\nsynthetic instances that closely mimic the original data distributions, as\ndemonstrated through comparative accuracy assessments. In experiments, while\nour approach did not universally outperform all models, it achieved superior\nperformance against most of the evaluated models and offered significant\nimprovements in processing time. These outcomes underscore the potential of\nusing SNG networks for fast, efficient, and effective synthetic data generation\nin emotion recognition applications.",
      "tldr_zh": "本研究针对生理信号情绪识别数据（如 EEG、ECG 和 GSR）的稀缺性问题，提出了一种使用 Supervised Neural Gas (SNG) 网络生成合成数据的创新方法，以克服隐私和后勤限制带来的挑战。SNG 网络利用其在拓扑和特征空间接近性的适应性，高效生成与原数据分布相似的合成数据集，并在处理速度上显著优于 Conditional VAE、Conditional GAN、diffusion model 和 Variational LSTM 等模型。实验结果显示，SNG 在准确性评估中优于大多数基准模型，并大幅缩短了处理时间，突显其在情绪识别应用中的快速高效潜力。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.NE",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.16353v1",
      "published_date": "2025-01-19 15:34:05 UTC",
      "updated_date": "2025-01-19 15:34:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:03:18.378971"
    },
    {
      "arxiv_id": "2501.11079v1",
      "title": "Federated Deep Reinforcement Learning for Energy Efficient Multi-Functional RIS-Assisted Low-Earth Orbit Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Li-Hsiang Shen",
        "Jyun-Jhe Huang",
        "Kai-Ten Feng",
        "Lie-Liang Yang",
        "Jen-Ming Wu"
      ],
      "abstract": "In this paper, a novel network architecture that deploys the multi-functional\nreconfigurable intelligent surface (MF-RIS) in low-Earth orbit (LEO) is\nproposed. Unlike traditional RIS with only signal reflection capability, the\nMF-RIS can reflect, refract, and amplify signals, as well as harvest energy\nfrom wireless signals. Given the high energy demands in shadow regions where\nsolar energy is unavailable, MF-RIS is deployed in LEO to enhance signal\ncoverage and improve energy efficiency (EE). To address this, we formulate a\nlong-term EE optimization problem by determining the optimal parameters for\nMF-RIS configurations, including amplification and phase-shifts, energy\nharvesting ratios, and LEO transmit beamforming. To address the complex\nnon-convex and non-linear problem, a federated learning enhanced multi-agent\ndeep deterministic policy gradient (FEMAD) scheme is designed. Multi-agent DDPG\nof each agent can provide the optimal action policy from its interaction to\nenvironments, whereas federated learning enables the hidden information\nexchange among multi-agents. In numerical results, we can observe significant\nEE improvements compared to the other benchmarks, including centralized deep\nreinforcement learning as well as distributed multi-agent deep deterministic\npolicy gradient (DDPG). Additionally, the proposed LEO-MF-RIS architecture has\ndemonstrated its effectiveness, achieving the highest EE performance compared\nto the scenarios of fixed/no energy harvesting in MF-RIS, traditional\nreflection-only RIS, and deployment without RISs/MF-RISs.",
      "tldr_zh": "本文提出了一种在低地球轨道 (LEO) 部署多功能可重构智能表面 (MF-RIS) 的网络架构，MF-RIS 能够反射、折射、放大信号并从无线信号中收获能量，从而提升阴影区域的信号覆盖和能量效率 (EE)。为解决相关的长期 EE 优化问题，作者设计了 Federated Learning Enhanced Multi-Agent Deep Deterministic Policy Gradient (FEMAD) 方案，该方案通过多代理 DDPG 优化每个代理的行动策略，并利用联邦学习实现信息交换。实验结果显示，该架构显著提高了 EE 性能，比基准方案（如集中式深度强化学习和分布式多代理 DDPG）提升明显，并在无能量收获或传统 RIS 场景中表现出最佳效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11079v1",
      "published_date": "2025-01-19 15:31:05 UTC",
      "updated_date": "2025-01-19 15:31:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:03:30.583297"
    },
    {
      "arxiv_id": "2501.11067v1",
      "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
      "title_zh": "IntellAgent：用于评估对话式 AI 系统的多智能体框架",
      "authors": [
        "Elad Levi",
        "Ilan Kadar"
      ],
      "abstract": "Large Language Models (LLMs) are transforming artificial intelligence,\nevolving into task-oriented systems capable of autonomous planning and\nexecution. One of the primary applications of LLMs is conversational AI\nsystems, which must navigate multi-turn dialogues, integrate domain-specific\nAPIs, and adhere to strict policy constraints. However, evaluating these agents\nremains a significant challenge, as traditional methods fail to capture the\ncomplexity and variability of real-world interactions. We introduce\nIntellAgent, a scalable, open-source multi-agent framework designed to evaluate\nconversational AI systems comprehensively. IntellAgent automates the creation\nof diverse, synthetic benchmarks by combining policy-driven graph modeling,\nrealistic event generation, and interactive user-agent simulations. This\ninnovative approach provides fine-grained diagnostics, addressing the\nlimitations of static and manually curated benchmarks with coarse-grained\nmetrics. IntellAgent represents a paradigm shift in evaluating conversational\nAI. By simulating realistic, multi-policy scenarios across varying levels of\ncomplexity, IntellAgent captures the nuanced interplay of agent capabilities\nand policy constraints. Unlike traditional methods, it employs a graph-based\npolicy model to represent relationships, likelihoods, and complexities of\npolicy interactions, enabling highly detailed diagnostics. IntellAgent also\nidentifies critical performance gaps, offering actionable insights for targeted\noptimization. Its modular, open-source design supports seamless integration of\nnew domains, policies, and APIs, fostering reproducibility and community\ncollaboration. Our findings demonstrate that IntellAgent serves as an effective\nframework for advancing conversational AI by addressing challenges in bridging\nresearch and deployment. The framework is available at\nhttps://github.com/plurai-ai/intellagent",
      "tldr_zh": "本研究提出IntellAgent，一个可扩展的开源多智能体框架，用于全面评估conversational AI系统，以解决传统方法在捕捉多轮对话、领域特定API整合和政策约束的复杂性方面的不足。IntellAgent通过结合policy-driven graph modeling、真实事件生成和交互式用户-代理模拟，自动创建多样化的合成基准，提供细粒度的诊断和性能分析。实验结果表明，该框架能识别关键性能差距，提供可操作的优化见解，并通过其模块化设计支持新领域、政策和API的集成，促进研究与部署的桥接。IntellAgent的开源代码可在https://github.com/plurai-ai/intellagent获取。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11067v1",
      "published_date": "2025-01-19 14:58:35 UTC",
      "updated_date": "2025-01-19 14:58:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:03:41.848165"
    },
    {
      "arxiv_id": "2501.11065v1",
      "title": "Enhancing Neural Spoken Language Recognition: An Exploration with Multilingual Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Or Haim Anidjar",
        "Roi Yozevitch"
      ],
      "abstract": "In this research, we advanced a spoken language recognition system, moving\nbeyond traditional feature vector-based models. Our improvements focused on\neffectively capturing language characteristics over extended periods using a\nspecialized pooling layer. We utilized a broad dataset range from Common-Voice,\ntargeting ten languages across Indo-European, Semitic, and East Asian families.\nThe major innovation involved optimizing the architecture of Time Delay Neural\nNetworks. We introduced additional layers and restructured these networks into\na funnel shape, enhancing their ability to process complex linguistic patterns.\nA rigorous grid search determined the optimal settings for these networks,\nsignificantly boosting their efficiency in language pattern recognition from\naudio samples. The model underwent extensive training, including a phase with\naugmented data, to refine its capabilities. The culmination of these efforts is\na highly accurate system, achieving a 97\\% accuracy rate in language\nrecognition. This advancement represents a notable contribution to artificial\nintelligence, specifically in improving the accuracy and efficiency of language\nprocessing systems, a critical aspect in the engineering of advanced speech\nrecognition technologies.",
      "tldr_zh": "本研究提升了神经网络-based 口语语言识别系统，通过引入一个专门的池化层来有效捕捉语言特征在较长时期的表现，并利用 Common-Voice 数据集覆盖十种语言（包括印欧语系、闪米特语系和东亚语系）。主要创新在于优化 Time Delay Neural Networks (TDNN) 的架构，添加额外层并重构为漏斗形状，同时通过网格搜索和数据增强训练来提高处理复杂语言模式的效率。实验结果显示，该系统在语言识别任务中达到了97%的准确率，为人工智能中的语音识别技术提供了显著的准确性和效率提升。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "15 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.11065v1",
      "published_date": "2025-01-19 14:49:43 UTC",
      "updated_date": "2025-01-19 14:49:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:03:54.229595"
    },
    {
      "arxiv_id": "2501.13115v2",
      "title": "Dagger Behind Smile: Fool LLMs with a Happy Ending Story",
      "title_zh": "微笑背后的匕首：用快乐结局的故事欺骗 LLMs",
      "authors": [
        "Xurui Song",
        "Zhixin Xie",
        "Shuo Huai",
        "Jiayi Kong",
        "Jun Luo"
      ],
      "abstract": "The wide adoption of Large Language Models (LLMs) has attracted significant\nattention from $\\textit{jailbreak}$ attacks, where adversarial prompts crafted\nthrough optimization or manual design exploit LLMs to generate malicious\ncontents. However, optimization-based attacks have limited efficiency and\ntransferability, while existing manual designs are either easily detectable or\ndemand intricate interactions with LLMs. In this paper, we first point out a\nnovel perspective for jailbreak attacks: LLMs are more responsive to\n$\\textit{positive}$ prompts. Based on this, we deploy Happy Ending Attack (HEA)\nto wrap up a malicious request in a scenario template involving a positive\nprompt formed mainly via a $\\textit{happy ending}$, it thus fools LLMs into\njailbreaking either immediately or at a follow-up malicious request.This has\nmade HEA both efficient and effective, as it requires only up to two turns to\nfully jailbreak LLMs. Extensive experiments show that our HEA can successfully\njailbreak on state-of-the-art LLMs, including GPT-4o, Llama3-70b, Gemini-pro,\nand achieves 88.79\\% attack success rate on average. We also provide\nquantitative explanations for the success of HEA.",
      "tldr_zh": "本研究揭示了大型语言模型 (LLMs) 对积极提示更敏感的新视角，并提出 Happy Ending Attack (HEA) 作为一种高效的越狱攻击 (jailbreak attacks) 方法，通过将恶意请求包装在包含“happy ending”的正面场景模板中，欺骗 LLMs 在一到两个回合内生成恶意内容。HEA 比传统优化-based 攻击更高效且可转移性强，避免了容易被检测的缺点。实验结果显示，HEA 在 GPT-4o、Llama3-70b 和 Gemini-pro 等先进模型上平均成功率达 88.79%，并提供了定量解释来阐明其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.13115v2",
      "published_date": "2025-01-19 13:39:51 UTC",
      "updated_date": "2025-02-17 02:23:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:04:05.419298"
    },
    {
      "arxiv_id": "2501.11043v2",
      "title": "BF-STVSR: B-Splines and Fourier-Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Eunjin Kim",
        "Hyeonjin Kim",
        "Kyong Hwan Jin",
        "Jaejun Yoo"
      ],
      "abstract": "While prior methods in Continuous Spatial-Temporal Video Super-Resolution\n(C-STVSR) employ Implicit Neural Representation (INR) for continuous encoding,\nthey often struggle to capture the complexity of video data, relying on simple\ncoordinate concatenation and pre-trained optical flow networks for motion\nrepresentation. Interestingly, we find that adding position encoding, contrary\nto common observations, does not improve--and even degrades--performance. This\nissue becomes particularly pronounced when combined with pre-trained optical\nflow networks, which can limit the model's flexibility. To address these\nissues, we propose BF-STVSR, a C-STVSR framework with two key modules tailored\nto better represent spatial and temporal characteristics of video: 1) B-spline\nMapper for smooth temporal interpolation, and 2) Fourier Mapper for capturing\ndominant spatial frequencies. Our approach achieves state-of-the-art in various\nmetrics, including PSNR and SSIM, showing enhanced spatial details and natural\ntemporal consistency. Our code is available https://github.com/Eunjnnn/bfstvsr.",
      "tldr_zh": "本研究针对 Continuous Spatial-Temporal Video Super-Resolution (C-STVSR) 的问题，指出现有方法依赖 Implicit Neural Representation (INR) 和预训练的光流网络，难以捕捉视频数据的复杂性，且添加位置编码反而会降低性能。作者提出 BF-STVSR 框架，引入 B-spline Mapper 用于平滑时序插值，以及 Fourier Mapper 用于捕捉主导空间频率，从而更好地表示视频的空间和时序特性。该方法在 PSNR 和 SSIM 等指标上达到最先进水平，提升了空间细节和时序一致性，并提供了开源代码以供进一步验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.11043v2",
      "published_date": "2025-01-19 13:29:41 UTC",
      "updated_date": "2025-03-25 07:05:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:04:17.553823"
    },
    {
      "arxiv_id": "2501.11031v1",
      "title": "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration of Large and Small Language Model",
      "title_zh": "AdaptiveLog：大语言模型和小语言模型",
      "authors": [
        "Lipeng Ma",
        "Weidong Yang",
        "Yixuan Li",
        "Ben Fei",
        "Mingjie Zhou",
        "Shuhao Li",
        "Sihang Jiang",
        "Bo Xu",
        "Yanghua Xiao"
      ],
      "abstract": "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.",
      "tldr_zh": "这篇论文提出了 AdaptiveLog 框架，一种自适应日志分析系统，通过结合 Large Language Model (LLM) 和 Small Language Model (SLM)，旨在平衡性能与成本效率。框架采用基于 SLM 不确定性估计的自适应选择策略，仅在 SLM 处理复杂日志时调用 LLM，同时引入一种新型提示策略，通过检索类似错误案例来增强 LLM 的推理能力。实验结果显示，AdaptiveLog 在多种日志分析任务上实现了最先进水平，提高了整体准确率，同时显著降低了推理成本。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.11031v1",
      "published_date": "2025-01-19 12:46:01 UTC",
      "updated_date": "2025-01-19 12:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:04:29.695281"
    },
    {
      "arxiv_id": "2501.13946v1",
      "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
      "title_zh": "翻译失败",
      "authors": [
        "Diego Gosmar",
        "Deborah A. Dahl"
      ],
      "abstract": "Hallucinations remain a significant challenge in current Generative AI\nmodels, undermining trust in AI systems and their reliability. This study\ninvestigates how orchestrating multiple specialized Artificial Intelligent\nAgents can help mitigate such hallucinations, with a focus on systems\nleveraging Natural Language Processing (NLP) to facilitate seamless agent\ninteractions. To achieve this, we design a pipeline that introduces over three\nhundred prompts, purposefully crafted to induce hallucinations, into a\nfront-end agent. The outputs are then systematically reviewed and refined by\nsecond- and third-level agents, each employing distinct large language models\nand tailored strategies to detect unverified claims, incorporate explicit\ndisclaimers, and clarify speculative content. Additionally, we introduce a set\nof novel Key Performance Indicators (KPIs) specifically designed to evaluate\nhallucination score levels. A dedicated fourth-level AI agent is employed to\nevaluate these KPIs, providing detailed assessments and ensuring accurate\nquantification of shifts in hallucination-related behaviors. A core component\nof this investigation is the use of the OVON (Open Voice Network) framework,\nwhich relies on universal NLP-based interfaces to transfer contextual\ninformation among agents. Through structured JSON messages, each agent\ncommunicates its assessment of the hallucination likelihood and the reasons\nunderlying questionable content, thereby enabling the subsequent stage to\nrefine the text without losing context. The results demonstrate that employing\nmultiple specialized agents capable of interoperating with each other through\nNLP-based agentic frameworks can yield promising outcomes in hallucination\nmitigation, ultimately bolstering trust within the AI community.",
      "tldr_zh": "这篇论文探讨了使用 Agentic AI 和 Natural Language Processing (NLP) 框架来缓解生成 AI 模型中的 Hallucinations，从而提升系统可信度。研究设计了一个多级代理管道，通过超过 300 个精心设计的提示诱发幻觉，并由第二、三级代理使用不同的大型语言模型检测未验证声明、添加免责声明并澄清内容，同时引入新型 Key Performance Indicators (KPIs) 由第四级代理进行评估。结果显示，这种多代理协作方法显著降低了幻觉水平，并通过 OVON 框架实现代理间的无缝信息传输，最终为 AI 社区增强信任提供了有效策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.13946v1",
      "published_date": "2025-01-19 11:19:25 UTC",
      "updated_date": "2025-01-19 11:19:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:04:42.786612"
    },
    {
      "arxiv_id": "2501.11006v2",
      "title": "GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code Generation",
      "title_zh": "GREEN-CODE",
      "authors": [
        "Shashikant Ilager",
        "Lukas Florian Briem",
        "Ivona Brandic"
      ],
      "abstract": "Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy.",
      "tldr_zh": "该研究提出GREEN-CODE框架，旨在优化LLM-based代码生成中的能源效率，以应对LLM推理过程的持续高资源消耗问题。框架通过动态早退出(dynamic early exit)机制和Reinforcement Learning (RL)代理来平衡准确性、延迟和能源消耗的权衡，在Llama 3.2 3B和OPT 2.7B模型上使用JavaCorpus和PY150数据集进行评估。结果显示，GREEN-CODE平均减少23-50%的能源消耗，同时基本不影响代码生成准确性。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF",
        "cs.SE",
        "C.4; D.0; E.4; I.7"
      ],
      "primary_category": "cs.DC",
      "comment": "Under submission in ACM/IEEE conference, 11 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.11006v2",
      "published_date": "2025-01-19 10:44:03 UTC",
      "updated_date": "2025-03-21 15:07:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:04:53.401749"
    },
    {
      "arxiv_id": "2503.15498v1",
      "title": "Revival: Collaborative Artistic Creation through Human-AI Interactions in Musical Creativity",
      "title_zh": "翻译失败",
      "authors": [
        "Keon Ju M. Lee",
        "Philippe Pasquier",
        "Jun Yuri"
      ],
      "abstract": "Revival is an innovative live audiovisual performance and music improvisation\nby our artist collective K-Phi-A, blending human and AI musicianship to create\nelectronic music with audio-reactive visuals. The performance features\nreal-time co-creative improvisation between a percussionist, an electronic\nmusic artist, and AI musical agents. Trained in works by deceased composers and\nthe collective's compositions, these agents dynamically respond to human input\nand emulate complex musical styles. An AI-driven visual synthesizer, guided by\na human VJ, produces visuals that evolve with the musical landscape. Revival\nshowcases the potential of AI and human collaboration in improvisational\nartistic creation.",
      "tldr_zh": "Revival 是一个由 K-Phi-A 艺术家集体开发的创新现场视听表演和音乐即兴创作，融合人类和 AI 音乐家来制作电子音乐并生成音频响应视觉。表演通过打击乐手、电子音乐艺术家与训练自已故作曲家和集体作品的 AI musical agents 进行 real-time co-creative improvisation，这些 AI 代理能动态响应人类输入并模仿复杂音乐风格。同时，一个 AI-driven visual synthesizer 在人类 VJ 的指导下，根据音乐景观演变视觉，展示了人类-AI 合作在即兴艺术创作中的巨大潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.HC",
      "comment": "Keon Ju M. Lee, Philippe Pasquier and Jun Yuri. 2024. In Proceedings\n  of the Creativity and Generative AI NIPS (Neural Information Processing\n  Systems) Workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.15498v1",
      "published_date": "2025-01-19 08:41:31 UTC",
      "updated_date": "2025-01-19 08:41:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:05:07.235155"
    },
    {
      "arxiv_id": "2501.10970v2",
      "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Nitay Calderon",
        "Roi Reichart",
        "Rotem Dror"
      ],
      "abstract": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.",
      "tldr_zh": "这篇论文针对“LLM-as-a-judge”范式，提出了一种新统计程序——Alternative Annotator Test (alt-test)，用于通过少量标注示例验证 LLMs 是否能可靠取代人类标注者，同时引入一种多功能、可解释的度量来比较不同 LLMs。研究者收集了十个语言和视觉语言数据集，实验涉及六种 LLMs 和四种提示技术，结果显示闭源 LLMs（如 GPT-4o）在某些情况下表现优于开源模型，并能实现人类水平的判断质量。论文强调，提示技术的选择会显著影响评估效果，并呼吁采用更严格的统计方法来提升 LLM 应用的可信度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10970v2",
      "published_date": "2025-01-19 07:09:11 UTC",
      "updated_date": "2025-02-05 15:24:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:05:19.098698"
    },
    {
      "arxiv_id": "2501.10967v2",
      "title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding",
      "title_zh": "通过金字塔下降视觉位置编码提升视觉语言模型的通用多模态能力",
      "authors": [
        "Zhanpeng Chen",
        "Mingxiao Li",
        "Ziyang Chen",
        "Nan Du",
        "Xiaolong Li",
        "Yuexian Zou"
      ],
      "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.",
      "tldr_zh": "本研究针对视觉语言模型（VLMs）的视觉位置编码问题，提出了一种新型方法Pyramid-descent Visual Position Encoding (PyPE)，以提升模型在不同粒度水平上的感知性能。PyPE通过从外围向中心分配视觉位置索引并逐步扩展中心感受野，解决了传统光栅扫描方法的局限性，并缓解了Rotary Position Embedding (RoPE)引起的长期衰减效应，从而减少相关视觉元素和指令标记之间的相对距离，促进更合理的注意力权重分配和多粒度感知。实验结果表明，PyPE在各种规模的VLMs上 consistently improves 了整体能力，为增强VLMs的通用多模态功能提供了有效途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10967v2",
      "published_date": "2025-01-19 07:00:46 UTC",
      "updated_date": "2025-02-12 08:10:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:05:31.041110"
    },
    {
      "arxiv_id": "2501.10966v1",
      "title": "DC-PCN: Point Cloud Completion Network with Dual-Codebook Guided Quantization",
      "title_zh": "DC-PC",
      "authors": [
        "Qiuxia Wu",
        "Haiyang Huang",
        "Kunming Su",
        "Zhiyong Wang",
        "Kun Hu"
      ],
      "abstract": "Point cloud completion aims to reconstruct complete 3D shapes from partial 3D\npoint clouds. With advancements in deep learning techniques, various methods\nfor point cloud completion have been developed. Despite achieving encouraging\nresults, a significant issue remains: these methods often overlook the\nvariability in point clouds sampled from a single 3D object surface. This\nvariability can lead to ambiguity and hinder the achievement of more precise\ncompletion results. Therefore, in this study, we introduce a novel point cloud\ncompletion network, namely Dual-Codebook Point Completion Network (DC-PCN),\nfollowing an encder-decoder pipeline. The primary objective of DC-PCN is to\nformulate a singular representation of sampled point clouds originating from\nthe same 3D surface. DC-PCN introduces a dual-codebook design to quantize\npoint-cloud representations from a multilevel perspective. It consists of an\nencoder-codebook and a decoder-codebook, designed to capture distinct point\ncloud patterns at shallow and deep levels. Additionally, to enhance the\ninformation flow between these two codebooks, we devise an information exchange\nmechanism. This approach ensures that crucial features and patterns from both\nshallow and deep levels are effectively utilized for completion. Extensive\nexperiments on the PCN, ShapeNet\\_Part, and ShapeNet34 datasets demonstrate the\nstate-of-the-art performance of our method.",
      "tldr_zh": "本文提出 DC-PCN，一种点云补全网络，使用双代码本引导量化（Dual-Codebook Guided Quantization）来处理从同一 3D 对象表面采样点云的变异性问题，从而生成更精确的完整形状表示。DC-PCN 采用编码器-解码器架构，引入编码器代码本和解码器代码本分别捕获浅层和深层点云模式，并通过信息交换机制增强两级特征的流动。实验结果显示，该方法在 PCN、ShapeNet_Part 和 ShapeNet34 数据集上达到了 state-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI25 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2501.10966v1",
      "published_date": "2025-01-19 06:57:45 UTC",
      "updated_date": "2025-01-19 06:57:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:05:42.646355"
    },
    {
      "arxiv_id": "2501.10957v2",
      "title": "MARIO: A Mixed Annotation Framework For Polyp Segmentation",
      "title_zh": "MARIO：一种用于息肉分割的混合标注框架",
      "authors": [
        "Haoyang Li",
        "Yiwen Hu",
        "Jun Wei",
        "Zhen Li"
      ],
      "abstract": "Existing polyp segmentation models are limited by high labeling costs and the\nsmall size of datasets. Additionally, vast polyp datasets remain underutilized\nbecause these models typically rely on a single type of annotation. To address\nthis dilemma, we introduce MARIO, a mixed supervision model designed to\naccommodate various annotation types, significantly expanding the range of\nusable data. MARIO learns from underutilized datasets by incorporating five\nforms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and\npoint-level. Each form of supervision is associated with a tailored loss that\neffectively leverages the supervision labels while minimizing the noise. This\nallows MARIO to move beyond the constraints of relying on a single annotation\ntype. Furthermore, MARIO primarily utilizes dataset with weak and cheap\nannotations, reducing the dependence on large-scale, fully annotated ones.\nExperimental results across five benchmark datasets demonstrate that MARIO\nconsistently outperforms existing methods, highlighting its efficacy in\nbalancing trade-offs between different forms of supervision and maximizing\npolyp segmentation performance",
      "tldr_zh": "该论文提出 MARIO，一种混合标注框架，用于息肉分割，旨在解决现有模型依赖单一标注类型导致的高标注成本和数据集利用不足的问题。MARIO 支持五种监督形式，包括 pixel-level、box-level、polygon-level、scribble-level 和 point-level，每种形式配备定制损失函数，以有效利用标签并最小化噪声，从而减少对大规模全标注数据集的依赖。实验结果显示，在五个基准数据集上，MARIO 显著优于现有方法，在平衡不同监督形式和提升分割性能方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE ISBI 2025 4-page paper",
      "pdf_url": "http://arxiv.org/pdf/2501.10957v2",
      "published_date": "2025-01-19 06:11:02 UTC",
      "updated_date": "2025-02-12 10:06:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:05:54.289472"
    },
    {
      "arxiv_id": "2501.10943v1",
      "title": "InsQABench: Benchmarking Chinese Insurance Domain Question Answering with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Ding",
        "Kai Feng",
        "Binbin Lin",
        "Jiarui Cai",
        "Qiushi Wang",
        "Yu Xie",
        "Xiaojin Zhang",
        "Zhongyu Wei",
        "Wei Chen"
      ],
      "abstract": "The application of large language models (LLMs) has achieved remarkable\nsuccess in various fields, but their effectiveness in specialized domains like\nthe Chinese insurance industry remains underexplored. The complexity of\ninsurance knowledge, encompassing specialized terminology and diverse data\ntypes, poses significant challenges for both models and users. To address this,\nwe introduce InsQABench, a benchmark dataset for the Chinese insurance sector,\nstructured into three categories: Insurance Commonsense Knowledge, Insurance\nStructured Database, and Insurance Unstructured Documents, reflecting\nreal-world insurance question-answering tasks.We also propose two methods,\nSQL-ReAct and RAG-ReAct, to tackle challenges in structured and unstructured\ndata tasks. Evaluations show that while LLMs struggle with domain-specific\nterminology and nuanced clause texts, fine-tuning on InsQABench significantly\nimproves performance. Our benchmark establishes a solid foundation for\nadvancing LLM applications in the insurance domain, with data and code\navailable at https://github.com/HaileyFamo/InsQABench.git.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在中文保险领域的应用挑战，引入了 InsQABench 基准数据集，该数据集分为 Insurance Commonsense Knowledge、Insurance Structured Database 和 Insurance Unstructured Documents 三类，以反映真实保险问答任务。研究者提出 SQL-ReAct 和 RAG-ReAct 两种方法，分别针对结构化和非结构化数据问题，以提升模型处理专业术语和细微条款的能力。评估结果显示，LLMs 在领域特定任务中表现不佳，但通过在 InsQABench 上微调，可显著改善性能，为推进保险领域 LLM 应用奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10943v1",
      "published_date": "2025-01-19 04:53:20 UTC",
      "updated_date": "2025-01-19 04:53:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:06:06.179745"
    },
    {
      "arxiv_id": "2501.10938v1",
      "title": "Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Alagha",
        "Jamal Bentahar",
        "Hadi Otrok",
        "Shakti Singh",
        "Rabeb Mizouni"
      ],
      "abstract": "Multi-Agent Deep Reinforcement Learning (MDRL) is a promising research area\nin which agents learn complex behaviors in cooperative or competitive\nenvironments. However, MDRL comes with several challenges that hinder its\nusability, including sample efficiency, curse of dimensionality, and\nenvironment exploration. Recent works proposing Federated Reinforcement\nLearning (FRL) to tackle these issues suffer from problems related to model\nrestrictions and maliciousness. Other proposals using reward shaping require\nconsiderable engineering and could lead to local optima. In this paper, we\npropose a novel Blockchain-assisted Multi-Expert Demonstration Cloning (MEDC)\nframework for MDRL. The proposed method utilizes expert demonstrations in\nguiding the learning of new MDRL agents, by suggesting exploration actions in\nthe environment. A model sharing framework on Blockchain is designed to allow\nusers to share their trained models, which can be allocated as expert models to\nrequesting users to aid in training MDRL systems. A Consortium Blockchain is\nadopted to enable traceable and autonomous execution without the need for a\nsingle trusted entity. Smart Contracts are designed to manage users and models\nallocation, which are shared using IPFS. The proposed framework is tested on\nseveral applications, and is benchmarked against existing methods in FRL,\nReward Shaping, and Imitation Learning-assisted RL. The results show the\noutperformance of the proposed framework in terms of learning speed and\nresiliency to faulty and malicious models.",
      "tldr_zh": "本研究针对Multi-Agent Deep Reinforcement Learning (MDRL)面临的样本效率低、维度诅咒和环境探索挑战，提出了一种新型Blockchain-assisted Multi-Expert Demonstration Cloning (MEDC)框架。该框架利用专家演示指导新代理的学习，通过建议探索动作来提升训练效率，并采用Consortium Blockchain设计模型共享机制，确保可追踪和自治执行，同时使用Smart Contracts和IPFS管理用户及模型分配。与Federated Reinforcement Learning (FRL)、Reward Shaping和Imitation Learning-assisted RL等方法相比，实验结果显示MEDC在学习速度和对故障或恶意模型的弹性方面表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10938v1",
      "published_date": "2025-01-19 04:20:24 UTC",
      "updated_date": "2025-01-19 04:20:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:08:11.581157"
    },
    {
      "arxiv_id": "2501.10935v1",
      "title": "TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Lyu",
        "Zijing Tian",
        "Zhonghong Ou",
        "Yifan Zhu",
        "Xiao Zhang",
        "Qiankun Ha",
        "Haoran Luo",
        "Meina Song"
      ],
      "abstract": "Cross-modal retrieval maps data under different modality via semantic\nrelevance. Existing approaches implicitly assume that data pairs are\nwell-aligned and ignore the widely existing annotation noise, i.e., noisy\ncorrespondence (NC). Consequently, it inevitably causes performance\ndegradation. Despite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives, the differences\nbetween these architectures are primarily stemmed from random initialization.\nThus, the model becomes increasingly homogeneous along with the training\nprocess. Consequently, the additional information brought by this paradigm is\nseverely limited. In order to resolve this problem, we introduce a Tripartite\nlearning with Semantic Variation Consistency (TSVC) for robust image-text\nretrieval. We design a tripartite cooperative learning mechanism comprising a\nCoordinator, a Master, and an Assistant model. The Coordinator distributes\ndata, and the Assistant model supports the Master model's noisy label\nprediction with diverse data. Moreover, we introduce a soft label estimation\nmethod based on mutual information variation, which quantifies the noise in new\nsamples and assigns corresponding soft labels. We also present a new loss\nfunction to enhance robustness and optimize training effectiveness. Extensive\nexperiments on three widely used datasets demonstrate that, even at increasing\nnoise ratios, TSVC exhibits significant advantages in retrieval accuracy and\nmaintains stable training performance.",
      "tldr_zh": "这篇论文针对图像-文本检索（Image-Text Retrieval）中的标注噪声（noisy correspondence）问题，提出了 TSVC（Tripartite Learning with Semantic Variation Consistency）框架，以提升模型的鲁棒性。TSVC 采用三方合作学习机制，包括 Coordinator 分配数据、Master 模型进行主训练，以及 Assistant 模型通过多样化数据支持噪声标签预测；同时引入基于互信息变异（mutual information variation）的软标签估计方法和新的损失函数来优化训练。实验结果显示，在三个常用数据集上，即使噪声比率增加，TSVC 也显著提高了检索准确率并保持了稳定的训练性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to the Main Track of AAAI 2025. It\n  contains 9 pages, 7 figures, and is relevant to the areas of cross-modal\n  retrieval and machine learning. The work presents a novel approach in robust\n  image-text retrieval using a tripartite learning framework",
      "pdf_url": "http://arxiv.org/pdf/2501.10935v1",
      "published_date": "2025-01-19 04:05:08 UTC",
      "updated_date": "2025-01-19 04:05:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:06:31.314931"
    },
    {
      "arxiv_id": "2501.10928v2",
      "title": "Generative Physical AI in Vision: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Daochang Liu",
        "Junyu Zhang",
        "Anh-Dung Dinh",
        "Eunbyung Park",
        "Shichao Zhang",
        "Ajmal Mian",
        "Mubarak Shah",
        "Chang Xu"
      ],
      "abstract": "Generative Artificial Intelligence (AI) has rapidly advanced the field of\ncomputer vision by enabling machines to create and interpret visual data with\nunprecedented sophistication. This transformation builds upon a foundation of\ngenerative models to produce realistic images, videos, and 3D/4D content.\nConventional generative models primarily focus on visual fidelity while often\nneglecting the physical plausibility of the generated content. This gap limits\ntheir effectiveness in applications that require adherence to real-world\nphysical laws, such as robotics, autonomous systems, and scientific\nsimulations. As generative models evolve to increasingly integrate physical\nrealism and dynamic simulation, their potential to function as \"world\nsimulators\" expands. Therefore, the field of physics-aware generation in\ncomputer vision is rapidly growing, calling for a comprehensive survey to\nprovide a structured analysis of current efforts. To serve this purpose, the\nsurvey presents a systematic review, categorizing methods based on how they\nincorporate physical knowledge, either through explicit simulation or implicit\nlearning. It also analyzes key paradigms, discusses evaluation protocols, and\nidentifies future research directions. By offering a comprehensive overview,\nthis survey aims to help future developments in physically grounded generation\nfor computer vision. The reviewed papers are summarized at\nhttps://tinyurl.com/Physics-Aware-Generation.",
      "tldr_zh": "这篇调查论文探讨了生成式人工智能(Generative AI)在计算机视觉中的应用，强调了传统生成模型在视觉保真度上取得的进展，但指出其忽略物理合理性(physical plausibility)的局限性，导致在机器人、自治系统和科学模拟等领域应用受限。论文系统分类了物理感知生成方法，包括通过显式模拟(explicit simulation)或隐式学习(implicit learning)整合物理知识，并分析了关键范式、评估协议以及未来研究方向。总体上，该调查为推动物理地基生成(physically grounded generation)在计算机视觉的发展提供了一个全面概述，并附有总结论文的链接(https://tinyurl.com/Physics-Aware-Generation)。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "An updated version",
      "pdf_url": "http://arxiv.org/pdf/2501.10928v2",
      "published_date": "2025-01-19 03:19:47 UTC",
      "updated_date": "2025-04-19 14:52:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:08:37.611723"
    },
    {
      "arxiv_id": "2501.13945v1",
      "title": "Self-Explanation in Social AI Agents",
      "title_zh": "社交 AI 智能体中的自我解释",
      "authors": [
        "Rhea Basappa",
        "Mustafa Tekman",
        "Hong Lu",
        "Benjamin Faught",
        "Sandeep Kakar",
        "Ashok K. Goel"
      ],
      "abstract": "Social AI agents interact with members of a community, thereby changing the\nbehavior of the community. For example, in online learning, an AI social\nassistant may connect learners and thereby enhance social interaction. These\nsocial AI assistants too need to explain themselves in order to enhance\ntransparency and trust with the learners. We present a method of\nself-explanation that uses introspection over a self-model of an AI social\nassistant. The self-model is captured as a functional model that specifies how\nthe methods of the agent use knowledge to achieve its tasks. The process of\ngenerating self-explanations uses Chain of Thought to reflect on the self-model\nand ChatGPT to provide explanations about its functioning. We evaluate the\nself-explanation of the AI social assistant for completeness and correctness.\nWe also report on its deployment in a live class.",
      "tldr_zh": "这篇论文探讨了Social AI Agents的Self-Explanation机制，以提升其与社区互动的透明度和信任，例如在在线学习中帮助增强社交互动。研究提出了一种方法，通过对自模型（一个功能模型）的内省，使用Chain of Thought进行反思，并结合ChatGPT生成关于代理功能和任务实现的解释。论文评估了这种自我解释的完整性和正确性，并在实际课堂环境中部署，证明了其实际可行性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Extended version of the paper published in International Conference\n  on Intelligent Tutoring Systems, pages 351-360, 2024, Springer. Images\n  corrected, and live deployment, ablation, and precision study results added",
      "pdf_url": "http://arxiv.org/pdf/2501.13945v1",
      "published_date": "2025-01-19 03:03:15 UTC",
      "updated_date": "2025-01-19 03:03:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:06:53.495736"
    },
    {
      "arxiv_id": "2501.10924v1",
      "title": "Adaptive Target Localization under Uncertainty using Multi-Agent Deep Reinforcement Learning with Knowledge Transfer",
      "title_zh": "不确定性下的自适应目标定位，使用多智能体深度强化学习与知识转移",
      "authors": [
        "Ahmed Alagha",
        "Rabeb Mizouni",
        "Shakti Singh",
        "Jamal Bentahar",
        "Hadi Otrok"
      ],
      "abstract": "Target localization is a critical task in sensitive applications, where\nmultiple sensing agents communicate and collaborate to identify the target\nlocation based on sensor readings. Existing approaches investigated the use of\nMulti-Agent Deep Reinforcement Learning (MADRL) to tackle target localization.\nNevertheless, these methods do not consider practical uncertainties, like false\nalarms when the target does not exist or when it is unreachable due to\nenvironmental complexities. To address these drawbacks, this work proposes a\nnovel MADRL-based method for target localization in uncertain environments. The\nproposed MADRL method employs Proximal Policy Optimization to optimize the\ndecision-making of sensing agents, which is represented in the form of an\nactor-critic structure using Convolutional Neural Networks. The observations of\nthe agents are designed in an optimized manner to capture essential information\nin the environment, and a team-based reward functions is proposed to produce\ncooperative agents. The MADRL method covers three action dimensionalities that\ncontrol the agents' mobility to search the area for the target, detect its\nexistence, and determine its reachability. Using the concept of Transfer\nLearning, a Deep Learning model builds on the knowledge from the MADRL model to\naccurately estimating the target location if it is unreachable, resulting in\nshared representations between the models for faster learning and lower\ncomputational complexity. Collectively, the final combined model is capable of\nsearching for the target, determining its existence and reachability, and\nestimating its location accurately. The proposed method is tested using a\nradioactive target localization environment and benchmarked against existing\nmethods, showing its efficacy.",
      "tldr_zh": "这篇论文提出了一种基于 Multi-Agent Deep Reinforcement Learning (MADRL) 的自适应目标定位方法，用于处理不确定环境中的挑战，如目标不存在或不可达。方法采用 Proximal Policy Optimization (PPO) 算法优化代理决策，通过 actor-critic 结构和 Convolutional Neural Networks (CNN) 设计代理观察，并引入团队奖励函数促进代理间的合作。系统覆盖三种行动维度，包括代理移动搜索、目标存在检测和可达性评估，并利用 Transfer Learning 从 MADRL 模型中转移知识，以更快学习和降低计算复杂度来精确估计不可达目标位置。在放射性目标定位环境中测试，该方法比现有方法表现出更高功效，证明了其在不确定场景中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10924v1",
      "published_date": "2025-01-19 02:58:22 UTC",
      "updated_date": "2025-01-19 02:58:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:07:06.900593"
    },
    {
      "arxiv_id": "2501.10917v2",
      "title": "Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Xie",
        "Haoxuan Li",
        "Chunyuan Zheng",
        "Haonan Yuan",
        "Guorui Liao",
        "Jun Liao",
        "Li Liu"
      ],
      "abstract": "Wearable Human Activity Recognition (WHAR) is a prominent research area\nwithin ubiquitous computing. Multi-sensor synchronous measurement has proven to\nbe more effective for WHAR than using a single sensor. However, existing WHAR\nmethods use shared convolutional kernels for indiscriminate temporal feature\nextraction across each sensor variable, which fails to effectively capture\nspatio-temporal relationships of intra-sensor and inter-sensor variables. We\npropose the DecomposeWHAR model consisting of a decomposition phase and a\nfusion phase to better model the relationships between modality variables. The\ndecomposition creates high-dimensional representations of each intra-sensor\nvariable through the improved Depth Separable Convolution to capture local\ntemporal features while preserving their unique characteristics. The fusion\nphase begins by capturing relationships between intra-sensor variables and\nfusing their features at both the channel and variable levels. Long-range\ntemporal dependencies are modeled using the State Space Model (SSM), and later\ncross-sensor interactions are dynamically captured through a self-attention\nmechanism, highlighting inter-sensor spatial correlations. Our model\ndemonstrates superior performance on three widely used WHAR datasets,\nsignificantly outperforming state-of-the-art models while maintaining\nacceptable computational efficiency.",
      "tldr_zh": "该论文针对多传感器可穿戴人体活动识别（Wearable Human Activity Recognition, WHAR）提出DecomposeWHAR模型，以更好地捕捉intra-sensor和inter-sensor变量的时空关系。模型分为分解阶段和融合阶段：在分解阶段，使用改进的Depth Separable Convolution为每个intra-sensor变量创建高维表示，提取局部临时特征并保留其独特特性。融合阶段首先在通道和变量级别融合intra-sensor变量的关系，然后通过State Space Model (SSM)建模长程临时依赖，并利用self-attention机制动态捕捉inter-sensor交互以突出空间相关性。该模型在三个常用WHAR数据集上显著优于现有最先进模型，同时保持可接受的计算效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10917v2",
      "published_date": "2025-01-19 01:52:28 UTC",
      "updated_date": "2025-04-25 04:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:07:18.579784"
    },
    {
      "arxiv_id": "2501.10909v1",
      "title": "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Gaole He",
        "Patrick Hemmer",
        "Michael Vössing",
        "Max Schemmer",
        "Ujwal Gadiraju"
      ],
      "abstract": "In recent years, the rapid development of AI systems has brought about the\nbenefits of intelligent services but also concerns about security and\nreliability. By fostering appropriate user reliance on an AI system, both\ncomplementary team performance and reduced human workload can be achieved.\nPrevious empirical studies have extensively analyzed the impact of factors\nranging from task, system, and human behavior on user trust and appropriate\nreliance in the context of one-step decision making. However, user reliance on\nAI systems in tasks with complex semantics that require multi-step workflows\nremains under-explored. Inspired by recent work on task decomposition with\nlarge language models, we propose to investigate the impact of a novel\nMulti-Step Transparent (MST) decision workflow on user reliance behaviors. We\nconducted an empirical study (N = 233) of AI-assisted decision making in\ncomposite fact-checking tasks (i.e., fact-checking tasks that entail multiple\nsub-fact verification steps). Our findings demonstrate that human-AI\ncollaboration with an MST decision workflow can outperform one-step\ncollaboration in specific contexts (e.g., when advice from an AI system is\nmisleading). Further analysis of the appropriate reliance at fine-grained\nlevels indicates that an MST decision workflow can be effective when users\ndemonstrate a relatively high consideration of the intermediate steps. Our work\nhighlights that there is no one-size-fits-all decision workflow that can help\nobtain optimal human-AI collaboration. Our insights help deepen the\nunderstanding of the role of decision workflows in facilitating appropriate\nreliance. We synthesize important implications for designing effective means to\nfacilitate appropriate reliance on AI systems in composite tasks, positioning\nopportunities for the human-centered AI and broader HCI communities.",
      "tldr_zh": "这篇论文探讨了在复杂任务分解中促进人类-AI 协作的细粒度适当依赖（Fine-Grained Appropriate Reliance），提出了一种 Multi-Step Transparent (MST) 决策工作流，以应对传统一步决策的局限。研究通过一项涉及 233 名参与者的实证实验，评估了 MST 工作流在复合事实检查任务中的表现，发现它在 AI 建议误导的情况下能显著提升协作效率，尤其当用户重视中间步骤时。论文强调没有万能的决策工作流，并为人类中心 AI 和 HCI 社区提供设计启示，以优化 AI 系统中的适当依赖行为。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2501.10909v1",
      "published_date": "2025-01-19 01:03:09 UTC",
      "updated_date": "2025-01-19 01:03:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T01:08:37.135119"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 40,
  "processed_papers_count": 40,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T01:08:54.591706"
}
{
  "date": "2025-08-21",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-21 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„æ—¥æŠ¥ä½œè€…ã€‚ä»Šå¤©çš„ arXiv åˆ—è¡¨å¯è°“ä¼—æ˜Ÿäº‘é›†ï¼Œ**Google DeepMindã€MIT (Song Han å›¢é˜Ÿ)ã€æ¸…åå¤§å­¦ (æœ±å†›å›¢é˜Ÿ)ã€Alibaba** ç­‰å‡æœ‰é‡ç£…å·¥ä½œæ”¾å‡ºã€‚\n\n**ä¸€å¥è¯æ€»ç»“ä»Šå¤©ï¼š**\nä»Šå¤©çš„ç„¦ç‚¹é›†ä¸­åœ¨ **LLM æ¨ç†æœ¬è´¨çš„æ¢è®¨**ï¼ˆDeepSeek R1 çš„æ¨ç†è½¨è¿¹è™½ç„¶å¼ºä½†äººç±»éš¾æ‡‚ï¼Ÿï¼‰ã€**é«˜æ•ˆæ¨¡å‹æ¶æ„çš„çªç ´**ï¼ˆSong Han å›¢é˜Ÿæ¨å‡º Jet-Nemotronï¼‰ã€**Agent åœ¨ GUI å’ŒèŠ¯ç‰‡è®¾è®¡é¢†åŸŸçš„è½åœ°**ï¼Œä»¥åŠ **Google å‘å¸ƒçš„å…³äº AI èƒ½æºæ¶ˆè€—çš„æƒå¨æŠ¥å‘Š**ã€‚\n\n---\n\n### ğŸš€ æ˜æ˜Ÿè®ºæ–‡ & æ·±åº¦æ¨ç† (Reasoning & Interpretability)\n\n**1. Do Cognitively Interpretable Reasoning Traces Improve LLM Performance? (è®¤çŸ¥å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹èƒ½æé«˜ LLM æ€§èƒ½å—ï¼Ÿ)**\n*   **Authors:** Siddhant Bhambri, Subbarao Kambhampati et al. (ASU)\n*   **TLDR:** **è¿™æ˜¯ä»Šå¤©æœ€å€¼å¾—æ·±æ€çš„æ–‡ç« ä¹‹ä¸€ã€‚** ç ”ç©¶è€…æŒ‘æˆ˜äº†ä¸€ä¸ªå‡è®¾ï¼šCoTï¼ˆæ€ç»´é“¾ï¼‰å¿…é¡»å¯¹äººç±»æœ‰æ„ä¹‰æ‰æœ‰æ•ˆå—ï¼Ÿ\n*   **Discovery:** å›¢é˜ŸåŸºäº DeepSeek R1 çš„ç—•è¿¹è¿›è¡Œäº†å¾®è°ƒå®éªŒã€‚ç»“æœå‘ç°äº†ä¸€ä¸ªæƒŠäººçš„é”™ä½ï¼ˆMismatchï¼‰ï¼š**ç›´æ¥ä½¿ç”¨ R1 çš„åŸå§‹ç—•è¿¹å¾®è°ƒå‡ºçš„æ¨¡å‹æ€§èƒ½æœ€å¼ºï¼Œä½†åœ¨äººç±»å—è¯•è€…çœ¼ä¸­ï¼Œè¿™äº›ç—•è¿¹çš„å¯è§£é‡Šæ€§æœ€å·®ã€‚** ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»è¿‡äººç±»å¯è¯»æ€§ä¼˜åŒ–çš„ç—•è¿¹åè€Œé™ä½äº†æ¨¡å‹æ€§èƒ½ã€‚ç»“è®ºæ˜¯ï¼šä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å°†ä¸­é—´æ¨ç† token ä¸ç”¨æˆ·å¯è§£é‡Šæ€§è§£è€¦ï¼ˆDecoupleï¼‰ã€‚\n\n**59. Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning (å—çŠ¶æ€è€ƒï¼šä»ç›´æ¥å›ç­”åˆ°æ·±åº¦æ¨ç†çš„è‡ªé€‚åº”æ¨ç†)**\n*   **Authors:** Yekun Zhu et al.\n*   **TLDR:** é¿å…è¿‡åº¦æ€è€ƒï¼ˆOverthinkingï¼‰ã€‚æå‡ºâ€œThink in Blocksâ€æ¡†æ¶ï¼Œè®©æ¨¡å‹å…ˆé¢„æµ‹æ‰€éœ€çš„â€œæ¨ç†é¢„ç®—â€ï¼ˆå—çš„æ•°é‡ï¼‰ï¼Œç„¶åæ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´ CoT çš„é•¿åº¦ï¼Œæ—¢çœè®¡ç®—åˆèƒ½å¤„ç†å¤æ‚é€»è¾‘ã€‚\n\n**125. CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning (CARFT: åŸºäºæ³¨é‡Š CoT çš„å¯¹æ¯”å­¦ä¹ å¢å¼ºå¾®è°ƒæå‡ LLM æ¨ç†)**\n*   **Authors:** Wenqiao Zhu et al.\n*   **TLDR:** ä¼ ç»Ÿçš„ RL å¾®è°ƒå®¹æ˜“å¯¼è‡´æ¨¡å‹åå¡Œæˆ–å¿½è§†é«˜è´¨é‡ CoTã€‚æœ¬æ–‡æå‡º CARFTï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä¿¡å·æ¥æŒ‡å¯¼å¾®è°ƒï¼Œå……åˆ†åˆ©ç”¨æ³¨é‡Šåçš„ CoT æ•°æ®ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚\n\n---\n\n### âš¡ é«˜æ•ˆè®¡ç®— & ç³»ç»Ÿæ¶æ„ (Efficiency & Systems)\n\n**18. Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search (Jet-Nemotron: åŸºäºåç¥ç»æ¶æ„æœç´¢çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹)**\n*   **Authors:** Yuxian Gu, Song Han (MIT) et al.\n*   **TLDR:** **Song Han å›¢é˜Ÿæ–°ä½œã€‚** æå‡º Jet-Nemotronï¼Œä¸€ç§æ··åˆæ¶æ„æ¨¡å‹ã€‚é€šè¿‡ PostNASï¼ˆåç¥ç»æ¶æ„æœç´¢ï¼‰æŠ€æœ¯ï¼Œä»é¢„è®­ç»ƒçš„å…¨æ³¨æ„åŠ›æ¨¡å‹å‡ºå‘ï¼Œè‡ªåŠ¨æœç´¢æœ€ä½³çš„ Linear Attention æ›¿ä»£æ–¹æ¡ˆã€‚\n*   **Result:** 2B å‚æ•°çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸ŠåŒ¹æ•Œ Qwen2.5 å’Œ Llama3.2ï¼Œä½†åœ¨ç”Ÿæˆååé‡ä¸Šæå‡äº† **53.6å€**ï¼Œé¢„å¡«å……é€Ÿåº¦æå‡ 6.1å€ã€‚\n\n**43. TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill and Decode Inference (TPLA: ç”¨äºé«˜æ•ˆåˆ†ç¦»å¼é¢„å¡«å……å’Œè§£ç æ¨ç†çš„å¼ é‡å¹¶è¡Œæ½œåœ¨æ³¨æ„åŠ›)**\n*   **Authors:** Xiaojuan Tang et al. (Tencent)\n*   **TLDR:** é’ˆå¯¹ **DeepSeek-V2/V3** å¼•å…¥çš„ MLAï¼ˆå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼‰åœ¨å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ä¸‹ç¼“å­˜æ— æ³•åˆ‡åˆ†çš„é—®é¢˜ï¼Œæå‡ºäº† TPLAã€‚å®ƒå…è®¸å¯¹ Latent Vector è¿›è¡Œåˆ‡åˆ†ï¼Œä½¿å¾—åœ¨ TP è®¾ç½®ä¸‹ä¹Ÿèƒ½äº«å— MLA çš„ä½æ˜¾å­˜ä¼˜åŠ¿ï¼ŒDeepSeek-V3 çš„ KV Cache å‡å°‘äº† 1.79å€ã€‚\n\n**123. ZeroQAT: End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost (ZeroQAT: æ¨ç†æˆæœ¬ä¸‹çš„ç«¯åˆ°ç«¯è®¾å¤‡ä¸Š LLM é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ)**\n*   **Authors:** Qitao Tan et al.\n*   **TLDR:** è®©åœ¨æ‰‹æœºä¸Šå¾®è°ƒé‡åŒ–æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–ï¼ˆZeroth-order optimizationï¼‰æ¶ˆé™¤åå‘ä¼ æ’­çš„å†…å­˜å¼€é”€ï¼Œå®ç°äº†åœ¨ 8GB æ˜¾å­˜ç”šè‡³ä¸€åŠ  12 æ‰‹æœºä¸Šå¯¹ 13B/6.7B æ¨¡å‹è¿›è¡Œä½æ¯”ç‰¹ï¼ˆ2-4bitï¼‰å¾®è°ƒã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸è‡ªåŠ¨åŒ– (Agents & GUI)\n\n**124. Mobile-Agent-v3: Fundamental Agents for GUI Automation (Mobile-Agent-v3: GUI è‡ªåŠ¨åŒ–çš„åŸºç¡€æ™ºèƒ½ä½“)**\n*   **Authors:** Jiabo Ye et al. (Alibaba Group)\n*   **TLDR:** **å¼€æº GUI Agent çš„æ–° SOTAã€‚** å¼•å…¥äº† GUI-Owl æ¨¡å‹ï¼Œé€šè¿‡å¤§è§„æ¨¡è·¨å¹³å°ï¼ˆAndroid/iOS/Win/Linuxï¼‰ç¯å¢ƒåŸºç¡€è®¾æ–½è¿›è¡Œè‡ªæˆ‘è¿›åŒ–çš„è½¨è¿¹ç”Ÿæˆã€‚åœ¨ AndroidWorld å’Œ OSWorld åŸºå‡†æµ‹è¯•ä¸­å‡åˆ·æ–°äº†å¼€æºæ¨¡å‹çš„æœ€ä½³æˆç»©ã€‚\n\n**9. ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation (ASIC-Agent: ç”¨äº ASIC è®¾è®¡çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ)**\n*   **Authors:** Ahmed Allam et al.\n*   **TLDR:** èŠ¯ç‰‡è®¾è®¡é¢†åŸŸçš„ Agent åº”ç”¨ã€‚ä¸ä»…ä»…æ˜¯å†™ Verilogï¼Œè€Œæ˜¯é›†æˆäº† RTL ç”Ÿæˆã€éªŒè¯ã€OpenLane ç¡¬åŒ–å’Œ Caravel é›†æˆçš„å…¨æµç¨‹æ²™ç›’ç¯å¢ƒã€‚\n\n**14. Cybernaut: Towards Reliable Web Automation (Cybernaut: è¿ˆå‘å¯é çš„ Web è‡ªåŠ¨åŒ–)**\n*   **Authors:** Ankur Tomar et al.\n*   **TLDR:** é’ˆå¯¹ä¼ä¸šçº§å†…éƒ¨å¤æ‚ç½‘é¡µï¼ˆé Amazon è¿™ç§è®¾è®¡è‰¯å¥½çš„ç½‘ç«™ï¼‰çš„è‡ªåŠ¨åŒ–ã€‚æå‡ºäº† SOP ç”Ÿæˆå™¨å’Œé«˜ç²¾åº¦ DOM è¯†åˆ«ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†åœ¨çƒ‚è®¾è®¡ç½‘é¡µä¸Šçš„è‡ªåŠ¨åŒ–æˆåŠŸç‡ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€ç¯å¢ƒä¸è¯„ä¼° (Safety, Environment & Eval)\n\n**27. Measuring the environmental impact of delivering AI at Google Scale (åœ¨ Google è§„æ¨¡ä¸‹è¡¡é‡æä¾› AI æœåŠ¡çš„ç¯å¢ƒå½±å“)**\n*   **Authors:** Jeff Dean, James Manyika et al. (Google)\n*   **TLDR:** **å¿…è¯»çš„è¡Œä¸šæŠ¥å‘Šã€‚** Google é¦–æ¬¡æŠ«éœ²äº†ç”Ÿäº§ç¯å¢ƒï¼ˆGeminiï¼‰çš„çœŸå®èƒ½è€—æ•°æ®ã€‚\n*   **Key Stat:** Gemini Apps çš„ä¸€æ¬¡æ–‡æœ¬æç¤ºå¹³å‡æ¶ˆè€— **0.24 Wh** ç”µé‡ï¼ˆå°‘äºçœ‹ 9 ç§’ç”µè§†çš„èƒ½è€—ï¼‰ï¼Œæ¶ˆè€— 0.26 mL æ°´ï¼ˆçº¦ 5 æ»´æ°´ï¼‰ã€‚è¿™æ¯”å¤–ç•Œè®¸å¤šå¤¸å¼ çš„ä¼°è®¡è¦ä½å¾—å¤šã€‚\n\n**86. Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation (æ­ç¤ºå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„ä¿¡ä»»é—®é¢˜ï¼šè¯„ä¼°ã€åˆ†æä¸ç¼“è§£)**\n*   **Authors:** Yichi Zhang, Jun Zhu et al. (Tsinghua)\n*   **TLDR:** æå‡ºäº† MultiTrust-X åŸºå‡†ï¼Œå…¨é¢è¯„ä¼° MLLM åœ¨çœŸå®æ€§ã€é²æ£’æ€§ã€å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œéšç§æ–¹é¢çš„è¡¨ç°ã€‚å‘ç°å¤šæ¨¡æ€è®­ç»ƒä¼šæ”¾å¤§ Base LLM çš„æ½œåœ¨é£é™©ã€‚\n\n**29. Numerical models outperform AI weather forecasts of record-breaking extremes (æ•°å€¼æ¨¡å‹åœ¨ç ´çºªå½•æç«¯å¤©æ°”é¢„æµ‹ä¸Šä¼˜äº AI æ¨¡å‹)**\n*   **Authors:** Zhongwei Zhang et al.\n*   **TLDR:** **ç»™ AI æ°”è±¡é¢„æµ‹æ³¼å†·æ°´ã€‚** å°½ç®¡ GraphCast å’Œç›˜å¤æ°”è±¡æ¨¡å‹åœ¨å¹³å‡æŒ‡æ ‡ä¸Šå¾ˆå¼ºï¼Œä½†åœ¨é¢„æµ‹â€œç ´çºªå½•â€çš„æç«¯çƒ­æµªã€å¯’æ½®æˆ–å¤§é£æ—¶ï¼Œä¼ ç»Ÿçš„æ•°å€¼å¤©æ°”é¢„æŠ¥ï¼ˆå¦‚ ECMWF HRESï¼‰ä»ç„¶æ›´å‡†ç¡®ã€‚AI æ¨¡å‹å€¾å‘äºä½ä¼°æç«¯äº‹ä»¶çš„å¼ºåº¦ã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–—ä¸ç§‘å­¦ AI (Medical & Science)\n\n**26. End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning (ç”¨äºå¯è¿½æº¯è¯Šæ–­æ¨ç†çš„ç«¯åˆ°ç«¯ Agentic RAG ç³»ç»Ÿè®­ç»ƒ)**\n*   **Authors:** Qiaoyu Zheng, Weidi Xie et al.\n*   **TLDR:** Deep-DxSearch ç³»ç»Ÿã€‚å°† LLM è§†ä¸º Agentï¼Œå°†æ£€ç´¢è¯­æ–™åº“è§†ä¸ºç¯å¢ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç«¯åˆ°ç«¯è®­ç»ƒ RAG ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—è¯Šæ–­ä¸­çš„å¹»è§‰å’Œæ¨ç†ä¸å¯è¿½æº¯é—®é¢˜ã€‚\n\n**4. Automated Multi-label Classification of Eleven Retinal Diseases (åä¸€ç§è§†ç½‘è†œç–¾ç—…çš„è‡ªåŠ¨å¤šæ ‡ç­¾åˆ†ç±»)**\n*   **Authors:** Jerry Cao-Xue et al.\n*   **TLDR:** è¯æ˜äº†**ä»…ä½¿ç”¨åˆæˆæ•°æ®**ï¼ˆSynFundus-1Mï¼Œç™¾ä¸‡çº§åˆæˆçœ¼åº•å›¾åƒï¼‰è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥åœ¨çœŸå®ä¸´åºŠæ•°æ®ä¸Šè¾¾åˆ°æé«˜çš„æ³›åŒ–æ€§èƒ½ï¼ˆAUC > 0.99ï¼‰ï¼Œä¸ºè§£å†³åŒ»ç–—æ•°æ®éšç§é—®é¢˜æŒ‡æ˜äº†æ–¹å‘ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„å‘ç°\n\n*   **[112] See it. Say it. Sorted:** ä¸€ä¸ªä¸éœ€è¦è®­ç»ƒçš„ Agent ç³»ç»Ÿï¼Œèƒ½æŠŠè‰å›¾ï¼ˆSketchï¼‰ç²¾å‡†è½¬æ¢æˆå¯ç¼–è¾‘çš„ SVG æµç¨‹å›¾ï¼Œæ•ˆæœæ¯” GPT-4V ç”Ÿæˆçš„ä»£ç å¥½å¾—å¤šã€‚\n*   **[63] Subjective Behaviors and Preferences in LLM: Language of Browsing:** ä¸ªæ€§åŒ–æ¨èæ–°è§†è§’ã€‚ç”¨æˆ·çš„æµè§ˆè¡Œä¸ºåƒæ˜¯ä¸€ç§â€œè¯­è¨€â€ã€‚å°æ¨¡å‹ï¼ˆSmall LMï¼‰é…åˆç‰¹å®šçš„ Tokenizer åœ¨æ•æ‰è¿™ç§ä¸»è§‚æµè§ˆåå¥½æ—¶ï¼Œå¯èƒ½æ¯”é€šç”¨å¤§æ¨¡å‹æ›´æœ‰æ•ˆã€‚\n*   **[119] LLM4Sweat:** å…³æ³¨ç½•è§ç—…ï¼ˆå¤šæ±—ç—‡ï¼‰çš„ä¸“ç”¨ LLMï¼Œä½“ç°äº† AI åœ¨ç‰¹å®šè¾¹ç¼˜ç¾¤ä½“æ”¯æŒä¸Šçš„ä»·å€¼ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼è¿™æ—¶å€™å–æ¯å’–å•¡ï¼Œè¯»ä¸€è¯»é‚£ç¯‡å…³äº DeepSeek R1 æ¨ç†è½¨è¿¹çš„æ–‡ç« ï¼Œæˆ–è®¸ä¼šæœ‰æ–°å‘ç°ã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2508.16695v1",
      "title": "Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?",
      "title_zh": "è®¤çŸ¥å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹èƒ½å¦æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Ÿ",
      "authors": [
        "Siddhant Bhambri",
        "Upasana Biswas",
        "Subbarao Kambhampati"
      ],
      "abstract": "Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}\" We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢å‘æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­ï¼Œæ€ç»´é“¾(Chain-of-Thought, CoT)æ¨ç†è½¨è¿¹æ˜¯å¦å¿…é¡»å…·å¤‡äººç±»å¯è§£é‡Šæ€§æ‰èƒ½æå‡ä»»åŠ¡æ€§èƒ½ã€‚ç ”ç©¶è€…åœ¨å¼€å·é—®ç­”(Open Book Question-Answering)é¢†åŸŸï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒLLaMAå’ŒQwenæ¨¡å‹ï¼Œå¯¹æ¯”äº†DeepSeek R1è½¨è¿¹ã€LLMç”Ÿæˆçš„æ‘˜è¦ã€äº‹åè§£é‡Šä»¥åŠç®—æ³•ç”Ÿæˆçš„æ­£ç¡®è½¨è¿¹å››ç§æ¨ç†ç±»å‹ã€‚ä¸ºäº†é‡åŒ–å¯è§£é‡Šæ€§ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œç ”ç©¶å›¢é˜Ÿè¿›è¡Œäº†ä¸€é¡¹åŒ…å«100åå‚ä¸è€…çš„äººç±»å—è¯•ç ”ç©¶ï¼Œå¯¹ä¸åŒè½¨è¿¹çš„å¯è§£é‡Šæ€§è¿›è¡Œè¯„åˆ†ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸€ä¸ªæ˜¾è‘—çš„é”™é…ï¼šè™½ç„¶ä½¿ç”¨DeepSeek R1è½¨è¿¹å¾®è°ƒçš„æ¨¡å‹æ€§èƒ½æœ€å¼ºï¼Œä½†å—è¯•è€…è®¤ä¸ºè¿™äº›è½¨è¿¹çš„å¯è§£é‡Šæ€§æœ€ä½ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå¤§è¯­è¨€æ¨¡å‹çš„ä¸­é—´æ¨ç†Tokenä¸æœ€ç»ˆç”¨æˆ·çš„å¯è§£é‡Šæ€§åº”å½“è§£è€¦ï¼Œé«˜æ€§èƒ½çš„æ¨ç†è¿‡ç¨‹æœªå¿…éœ€è¦ç¬¦åˆäººç±»çš„è®¤çŸ¥ç›´è§‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16695v1",
      "published_date": "2025-08-21 23:48:50 UTC",
      "updated_date": "2025-08-21 23:48:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:29.867554+00:00"
    },
    {
      "arxiv_id": "2509.00035v1",
      "title": "Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing",
      "title_zh": "é¢å‘å…ˆè¿›å·¥è‰ºèŠ‚ç‚¹æœ€å°å·¥ä½œç”µå‹é¢„æµ‹çš„è¿ç§»å­¦ä¹ ï¼šåˆ©ç”¨å†å²æ•°æ®ä¸ç¡…é‡Œç¨‹è®¡ä¼ æ„Ÿ",
      "authors": [
        "Yuxuan Yin",
        "Rebecca Chen",
        "Boxun Xu",
        "Chen He",
        "Peng Li"
      ],
      "abstract": "Accurate prediction of chip performance is critical for ensuring energy efficiency and reliability in semiconductor manufacturing. However, developing minimum operating voltage ($V_{min}$) prediction models at advanced technology nodes is challenging due to limited training data and the complex relationship between process variations and $V_{min}$. To address these issues, we propose a novel transfer learning framework that leverages abundant legacy data from the 16nm technology node to enable accurate $V_{min}$ prediction at the advanced 5nm node. A key innovation of our approach is the integration of input features derived from on-chip silicon odometer sensor data, which provide fine-grained characterization of localized process variations -- an essential factor at the 5nm node -- resulting in significantly improved prediction accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„è¿ç§»å­¦ä¹ (Transfer Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…ˆè¿›æŠ€æœ¯èŠ‚ç‚¹ä¸‹æœ€å°å·¥ä½œç”µå‹($V_{min}$)é¢„æµ‹å› è®­ç»ƒæ•°æ®æœ‰é™åŠå·¥è‰ºæ³¢åŠ¨å¤æ‚è€Œé¢ä¸´çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æ¥è‡ª16nmå·¥è‰ºèŠ‚ç‚¹çš„ä¸°å¯Œæ—§æœ‰æ•°æ®(Legacy Data)ï¼Œå®ç°äº†å¯¹5nmå…ˆè¿›èŠ‚ç‚¹çš„å‡†ç¡®$V_{min}$é¢„æµ‹ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆäº†ç‰‡ä¸Šç¡…é‡Œç¨‹è®¡(Silicon Odometer)ä¼ æ„Ÿå™¨æ•°æ®ä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œä»è€Œå®ç°äº†å¯¹å±€éƒ¨å·¥è‰ºæ³¢åŠ¨(Process Variations)çš„ç»†ç²’åº¦è¡¨å¾ã€‚è¿™ç§é›†æˆä¼ æ„Ÿå™¨æ•°æ®çš„æ–¹æ³•æœ‰æ•ˆåº”å¯¹äº†5nmå·¥è‰ºä¸‹çš„å…³é”®æŒ‘æˆ˜ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶å¯¹äºç¡®ä¿åŠå¯¼ä½“åˆ¶é€ çš„èƒ½æºæ•ˆç‡å’Œå¯é æ€§å…·æœ‰é‡è¦çš„å­¦æœ¯å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00035v1",
      "published_date": "2025-08-21 23:13:55 UTC",
      "updated_date": "2025-08-21 23:13:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:43.685741+00:00"
    },
    {
      "arxiv_id": "2509.05303v1",
      "title": "Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats",
      "title_zh": "Multi-IaC-Evalï¼šè·¨å¤šç§æ ¼å¼çš„äº‘åŸºç¡€è®¾æ–½å³ä»£ç åŸºå‡†è¯„ä¼°",
      "authors": [
        "Sam Davidson",
        "Li Sun",
        "Bhavana Bhasker",
        "Laurent Callot",
        "Anoop Deoras"
      ],
      "abstract": "Infrastructure as Code (IaC) is fundamental to modern cloud computing, enabling teams to define and manage infrastructure through machine-readable configuration files. However, different cloud service providers utilize diverse IaC formats. The lack of a standardized format requires cloud architects to be proficient in multiple IaC languages, adding complexity to cloud deployment. While Large Language Models (LLMs) show promise in automating IaC creation and maintenance, progress has been limited by the lack of comprehensive benchmarks across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark dataset for evaluating LLM-based IaC generation and mutation across AWS CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset consists of triplets containing initial IaC templates, natural language modification requests, and corresponding updated templates, created through a synthetic data generation pipeline with rigorous validation. We evaluate several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while modern LLMs can achieve high success rates (>95%) in generating syntactically valid IaC across formats, significant challenges remain in semantic alignment and handling complex infrastructure patterns. Our ablation studies highlight the importance of prompt engineering and retry mechanisms in successful IaC generation. We release Multi-IaC-Bench to facilitate further research in AI-assisted infrastructure management and establish standardized evaluation metrics for this crucial domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Multi-IaC-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šç§æ ¼å¼ä¸‹çš„åŸºç¡€è®¾æ–½å³ä»£ç (Infrastructure as Code, IaC)ç”Ÿæˆå’Œå˜æ›´èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº† AWS CloudFormationã€Terraform å’Œ Cloud Development Kit (CDK) ç­‰ä¸»æµæ ¼å¼ï¼Œé€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆæµæ°´çº¿æ„å»ºäº†åŒ…å«åˆå§‹æ¨¡æ¿ã€è‡ªç„¶è¯­è¨€ä¿®æ”¹è¯·æ±‚å’Œæ›´æ–°åæ¨¡æ¿çš„ä¸‰å…ƒç»„æ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æœ€å…ˆè¿›çš„ LLMs åœ¨ç”Ÿæˆè¯­æ³•æ­£ç¡®çš„ IaC ä»£ç æ–¹é¢è¾¾åˆ°äº†è¶…è¿‡ 95% çš„æˆåŠŸç‡ï¼Œä½†åœ¨è¯­ä¹‰å¯¹é½(semantic alignment)å’Œå¤„ç†å¤æ‚åŸºç¡€è®¾æ–½æ¨¡å¼æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜äº†æç¤ºå·¥ç¨‹(prompt engineering)å’Œé‡è¯•æœºåˆ¶(retry mechanisms)å¯¹æå‡ç”ŸæˆæˆåŠŸç‡çš„å…³é”®ä½œç”¨ã€‚è¯¥æ•°æ®é›†çš„å‘å¸ƒä¸º AI è¾…åŠ©çš„åŸºç¡€è®¾æ–½ç®¡ç†æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæœ‰åŠ©äºæ¨åŠ¨äº‘éƒ¨ç½²è‡ªåŠ¨åŒ–çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05303v1",
      "published_date": "2025-08-21 22:37:18 UTC",
      "updated_date": "2025-08-21 22:37:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:44.143778+00:00"
    },
    {
      "arxiv_id": "2508.15986v1",
      "title": "Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset",
      "title_zh": "11ç§è§†ç½‘è†œç–¾ç—…çš„è‡ªåŠ¨åŒ–å¤šæ ‡ç­¾åˆ†ç±»ï¼šå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šçš„ç°ä»£æ¶æ„åŸºå‡†ä¸å…ƒé›†æˆ",
      "authors": [
        "Jerry Cao-Xue",
        "Tien Comlekoglu",
        "Keyi Xue",
        "Guanliang Wang",
        "Jiang Li",
        "Gordon Laurie"
      ],
      "abstract": "The development of multi-label deep learning models for retinal disease classification is often hindered by the scarcity of large, expertly annotated clinical datasets due to patient privacy concerns and high costs. The recent release of SynFundus-1M, a high-fidelity synthetic dataset with over one million fundus images, presents a novel opportunity to overcome these barriers. To establish a foundational performance benchmark for this new resource, we developed an end-to-end deep learning pipeline, training six modern architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the RETFound foundation model) to classify eleven retinal diseases using a 5-fold multi-label stratified cross-validation strategy. We further developed a meta-ensemble model by stacking the out-of-fold predictions with an XGBoost classifier. Our final ensemble model achieved the highest performance on the internal validation set, with a macro-average Area Under the Receiver Operating Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated strong generalization to three diverse, real-world clinical datasets, achieving an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset. This work provides a robust baseline for future research on large-scale synthetic datasets and establishes that models trained exclusively on synthetic data can accurately classify multiple pathologies and generalize effectively to real clinical images, offering a viable pathway to accelerate the development of comprehensive AI systems in ophthalmology.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è§†ç½‘è†œç–¾ç—…åˆ†ç±»ä¸­ä¸´åºŠæ ‡æ³¨æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œåˆ©ç”¨åŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡å¼ é«˜ä¿çœŸçœ¼åº•å›¾åƒçš„åˆæˆæ•°æ®é›† SynFundus-1M å¼€å±•å¤šæ ‡ç­¾åˆ†ç±»ç ”ç©¶ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æµæ°´çº¿ï¼Œè®­ç»ƒäº†åŒ…æ‹¬ ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2 å’Œ RETFound åŸºç¡€æ¨¡å‹åœ¨å†…çš„å…­ç§ç°ä»£æ¶æ„ï¼Œå¹¶é‡‡ç”¨ 5-fold å¤šæ ‡ç­¾åˆ†å±‚äº¤å‰éªŒè¯ç­–ç•¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨ XGBoost åˆ†ç±»å™¨å¯¹é¢„æµ‹ç»“æœè¿›è¡Œå †å ï¼Œå¼€å‘äº†ä¸€ä¸ªé«˜æ€§èƒ½çš„å…ƒé›†æˆ (meta-ensemble) æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥é›†æˆæ¨¡å‹åœ¨å†…éƒ¨éªŒè¯é›†ä¸Šçš„å®å¹³å‡å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ (macro-AUC) è¾¾åˆ° 0.9973ã€‚æ›´å…³é”®çš„æ˜¯ï¼Œæ¨¡å‹åœ¨ä¸‰ä¸ªçœŸå®çš„ä¸´åºŠæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†ä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¹Ÿèƒ½å‡†ç¡®åˆ†ç±»å¤šç§ç—…ç†å¹¶æœ‰æ•ˆæ¨å¹¿è‡³ä¸´åºŠå›¾åƒã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨å¤§è§„æ¨¡åˆæˆæ•°æ®é›†åŠ é€Ÿçœ¼ç§‘äººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€å‘æä¾›äº†é‡è¦åŸºå‡†å’Œå¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 6 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.15986v1",
      "published_date": "2025-08-21 22:09:53 UTC",
      "updated_date": "2025-08-21 22:09:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:42.989041+00:00"
    },
    {
      "arxiv_id": "2508.15985v1",
      "title": "Panoptic Segmentation of Environmental UAV Images : Litter Beach",
      "title_zh": "ç¯å¢ƒæ— äººæœºå½±åƒå…¨æ™¯åˆ†å‰²ï¼šæµ·æ»©åƒåœ¾",
      "authors": [
        "Ousmane Youme",
        "Jean Marie DembÃ©lÃ©",
        "Eugene C. Ezin",
        "Christophe Cambier"
      ],
      "abstract": "Convolutional neural networks (CNN) have been used efficiently in several fields, including environmental challenges. In fact, CNN can help with the monitoring of marine litter, which has become a worldwide problem. UAVs have higher resolution and are more adaptable in local areas than satellite images, making it easier to find and count trash. Since the sand is heterogeneous, a basic CNN model encounters plenty of inferences caused by reflections of sand color, human footsteps, shadows, algae present, dunes, holes, and tire tracks. For these types of images, other CNN models, such as CNN-based segmentation methods, may be more appropriate. In this paper, we use an instance-based segmentation method and a panoptic segmentation method that show good accuracy with just a few samples. The model is more robust and less",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒæ€§çš„æµ·æ´‹åƒåœ¾ç›‘æµ‹é—®é¢˜ï¼Œæ¢è®¨äº†åˆ©ç”¨æ— äººæœº(UAV)å›¾åƒè¿›è¡Œç¯å¢ƒç›‘æµ‹çš„ä¼˜åŠ¿ã€‚ç”±äºæ²™æ»©ç¯å¢ƒå…·æœ‰é«˜åº¦çš„å¼‚è´¨æ€§ï¼Œä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œ(CNN)åœ¨å¤„ç†æ­¤ç±»å›¾åƒæ—¶å¸¸å—åˆ°æ²™è‰²åå°„ã€é˜´å½±ã€æµ·è—»åŠè¶³è¿¹ç­‰å¤šç§å¤æ‚èƒŒæ™¯å› ç´ çš„å¹²æ‰°ã€‚ä¸ºäº†æå‡ç›‘æµ‹çš„ç²¾ç¡®åº¦ï¼Œæœ¬æ–‡é‡‡ç”¨äº†åŸºäºå®ä¾‹åˆ†å‰²(Instance-based segmentation)å’Œå…¨æ™¯åˆ†å‰²(Panoptic Segmentation)çš„æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…éœ€å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å³å¯å±•ç°å‡ºè‰¯å¥½çš„å‡†ç¡®ç‡ã€‚ç›¸æ¯”åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ–¹æ¡ˆè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œä¸ºåœ¨å¤æ‚è‡ªç„¶åœºæ™¯ä¸‹å®ç°è‡ªåŠ¨åŒ–çš„åƒåœ¾è¯†åˆ«ä¸è®¡æ•°æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted for CNRIA 2023",
      "pdf_url": "https://arxiv.org/pdf/2508.15985v1",
      "published_date": "2025-08-21 22:07:59 UTC",
      "updated_date": "2025-08-21 22:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:48.448045+00:00"
    },
    {
      "arxiv_id": "2508.16692v1",
      "title": "Making AI Inevitable: Historical Perspective and the Problems of Predicting Long-Term Technological Change",
      "title_zh": "ä½¿äººå·¥æ™ºèƒ½æˆä¸ºå¿…ç„¶ï¼šå†å²è§†è§’ä¸é•¿æœŸæŠ€æœ¯å˜é©é¢„æµ‹ä¸­çš„é—®é¢˜",
      "authors": [
        "Mark Fisher",
        "John Severini"
      ],
      "abstract": "This study demonstrates the extent to which prominent debates about the future of AI are best understood as subjective, philosophical disagreements over the history and future of technological change rather than as objective, material disagreements over the technologies themselves. It focuses on the deep disagreements over whether artificial general intelligence (AGI) will prove transformative for human society; a question that is analytically prior to that of whether this transformative effect will help or harm humanity. The study begins by distinguishing two fundamental camps in this debate. The first of these can be identified as \"transformationalists,\" who argue that continued AI development will inevitably have a profound effect on society. Opposed to them are \"skeptics,\" a more eclectic group united by their disbelief that AI can or will live up to such high expectations. Each camp admits further \"strong\" and \"weak\" variants depending on their tolerance for epistemic risk. These stylized contrasts help to identify a set of fundamental questions that shape the camps' respective interpretations of the future of AI. Three questions in particular are focused on: the possibility of non-biological intelligence, the appropriate time frame of technological predictions, and the assumed trajectory of technological development. In highlighting these specific points of non-technical disagreement, this study demonstrates the wide range of different arguments used to justify either the transformationalist or skeptical position. At the same time, it highlights the strong argumentative burden of the transformationalist position, the way that belief in this position creates competitive pressures to achieve first-mover advantage, and the need to widen the concept of \"expertise\" in debates surrounding the future development of AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰å…³äºAIæœªæ¥çš„ä¸»æµè¾©è®ºåœ¨æœ¬è´¨ä¸Šæ˜¯å…³äºæŠ€æœ¯å˜è¿å†å²ä¸æœªæ¥çš„ä¸»è§‚å“²å­¦åˆ†æ­§ï¼Œè€Œéé’ˆå¯¹æŠ€æœ¯æœ¬èº«çš„å®¢è§‚ç‰©è´¨äº‰è®®ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†é€šç”¨äººå·¥æ™ºèƒ½(AGI)æ˜¯å¦å…·æœ‰ç¤¾ä¼šå˜é©æ€§è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶å°†è¾©è®ºå„æ–¹åˆ’åˆ†ä¸ºä¸»å¼ AIå¿…ç„¶äº§ç”Ÿæ·±è¿œå½±å“çš„â€œtransformationalistsâ€ä¸æŒæ€€ç–‘æ€åº¦çš„â€œskepticsâ€ä¸¤å¤§é˜µè¥ã€‚é€šè¿‡å¯¹å¼ºã€å¼±å˜ä½“æ¨¡å‹çš„å¯¹æ¯”ï¼Œæ–‡ç« è¯†åˆ«äº†å½±å“å„æ–¹ç«‹åœºçš„ä¸‰å¤§æ ¸å¿ƒéæŠ€æœ¯é—®é¢˜ï¼šéç”Ÿç‰©æ™ºèƒ½(non-biological intelligence)çš„å¯èƒ½æ€§ã€é¢„æµ‹çš„æ—¶é—´æ¡†æ¶ä»¥åŠæŠ€æœ¯å‘å±•çš„å‡è®¾è½¨è¿¹ã€‚ç ”ç©¶åˆ†æè¡¨æ˜ï¼Œè™½ç„¶â€œtransformationalistâ€ç«‹åœºé¢ä¸´æ²‰é‡çš„è®ºè¯è´Ÿæ‹…ï¼Œä½†å¯¹è¯¥ç«‹åœºçš„ä¿¡å¿µæ­£ä¿ƒä½¿å„æ–¹ä¸ºè·å–å…ˆå‘ä¼˜åŠ¿(first-mover advantage)è€Œäº§ç”Ÿç«äº‰å‹åŠ›ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å»ºè®®åœ¨æ¢è®¨AIæœªæ¥å‘å±•æ—¶åº”æ‰©å¤§â€œexpertiseâ€çš„æ¦‚å¿µèŒƒç•´ï¼Œä»¥æ¶µç›–æ›´å¹¿æ³›çš„å­¦ç§‘è§†è§’ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "econ.GN"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16692v1",
      "published_date": "2025-08-21 21:18:37 UTC",
      "updated_date": "2025-08-21 21:18:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:59.652503+00:00"
    },
    {
      "arxiv_id": "2508.15959v1",
      "title": "Representation Learning with Adaptive Superpixel Coding",
      "title_zh": "åŸºäºè‡ªé€‚åº”è¶…åƒç´ ç¼–ç çš„è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Mahmoud Khalil",
        "Ahmad Khalil",
        "Alioune Ngom"
      ],
      "abstract": "Deep learning vision models are typically tailored for specific modalities and often rely on domain-specific assumptions, such as the grid structures used by nearly all existing vision models. In this work, we propose a self-supervised model based on Transformers, which we call Adaptive Superpixel Coding (ASC). The key insight of our model is to overcome the limitations of traditional Vision Transformers, which depend on fixed-size and non-adaptive patch partitioning. Instead, ASC employs adaptive superpixel layers that dynamically adjust to the underlying image content. We analyze key properties of the approach that make it effective, and find that our method outperforms widely-used alternatives on standard image downstream task benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªç›‘ç£Transformeræ¨¡å‹Adaptive Superpixel Coding (ASC)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰æ¨¡å‹å¯¹å›ºå®šç½‘æ ¼ç»“æ„å’Œéè‡ªé€‚åº”åˆ†å—(patch partitioning)çš„ä¾èµ–ã€‚ä¸ä¼ ç»ŸVision Transformers (ViT) ä½¿ç”¨å›ºå®šå¤§å°çš„åˆ†å—ä¸åŒï¼ŒASC å¼•å…¥äº†è‡ªé€‚åº”è¶…åƒç´ å±‚(adaptive superpixel layers)ï¼Œèƒ½å¤Ÿæ ¹æ®åº•å±‚å›¾åƒå†…å®¹è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚é€šè¿‡å…‹æœå›ºå®šåˆ†å—çš„å±€é™æ€§ï¼ŒASC èƒ½å¤Ÿæ›´çµæ´»åœ°æ•æ‰å›¾åƒçš„ç»“æ„ç‰¹å¾ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ·±å…¥åˆ†æäº†è¯¥æ–¹æ³•åœ¨è¡¨ç¤ºå­¦ä¹ (Representation Learning)ä¸­ä¿æŒé«˜æ•ˆçš„å…³é”®å±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASC åœ¨æ ‡å‡†å›¾åƒä¸‹æ¸¸ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºç›®å‰å¹¿æ³›ä½¿ç”¨çš„æ›¿ä»£æ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„æå‡ºä¸ºæ„å»ºä¸å†å—é™äºç‰¹å®šé¢†åŸŸå‡è®¾çš„é€šç”¨è§†è§‰æ¨¡å‹æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15959v1",
      "published_date": "2025-08-21 20:57:20 UTC",
      "updated_date": "2025-08-21 20:57:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:05:57.652943+00:00"
    },
    {
      "arxiv_id": "2508.15943v1",
      "title": "T-ILR: a Neurosymbolic Integration for LTLf",
      "title_zh": "T-ILRï¼šé¢å‘ LTLf çš„ç¥ç»ç¬¦å·é›†æˆ",
      "authors": [
        "Riccardo Andreoni",
        "Andrei Buliga",
        "Alessandro Daniele",
        "Chiara Ghidini",
        "Marco Montali",
        "Massimiliano Ronzani"
      ],
      "abstract": "State-of-the-art approaches for integrating symbolic knowledge with deep learning architectures have demonstrated promising results in static domains. However, methods to handle temporal logic specifications remain underexplored. The only existing approach relies on an explicit representation of a finite-state automaton corresponding to the temporal specification. Instead, we aim at proposing a neurosymbolic framework designed to incorporate temporal logic specifications, expressed in Linear Temporal Logic over finite traces (LTLf), directly into deep learning architectures for sequence-based tasks. We extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging the recent introduction of fuzzy LTLf interpretations. We name this proposed method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an existing benchmark for temporal neurosymbolic architectures, consisting of the classification of image sequences in the presence of temporal knowledge. The results demonstrate improved accuracy and computational efficiency compared to the state-of-the-art method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† T-ILRï¼Œä¸€ç§ä¸“ä¸ºå¤„ç†æœ‰é™è½¨è¿¹çº¿æ€§æ—¶åºé€»è¾‘ (LTLf) è§„çº¦è€Œè®¾è®¡çš„ç¥ç»ç¬¦å· (Neurosymbolic) é›†æˆæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–æœ‰é™çŠ¶æ€è‡ªåŠ¨æœº (Finite-state automaton) æ˜¾å¼è¡¨ç¤ºçš„å±€é™æ€§ï¼ŒT-ILR æ—¨åœ¨å°†æ—¶åºé€»è¾‘ç›´æ¥èå…¥ç”¨äºåºåˆ—ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¶æ„ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æ¨¡ç³Š LTLf è§£é‡ŠæŠ€æœ¯ï¼Œå¯¹ç°æœ‰çš„è¿­ä»£å±€éƒ¨ç»†åŒ– (Iterative Local Refinement, ILR) ç¥ç»ç¬¦å·ç®—æ³•è¿›è¡Œäº†æ‰©å±•ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…å«æ—¶åºçŸ¥è¯†çš„å›¾åƒåºåˆ—åˆ†ç±»åŸºå‡†ä¸Šå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸ç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒT-ILR åœ¨æ˜¾è‘—æå‡åˆ†ç±»å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for presentation at NeSy 2025. 10 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.15943v1",
      "published_date": "2025-08-21 20:24:20 UTC",
      "updated_date": "2025-08-21 20:24:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:01.261640+00:00"
    },
    {
      "arxiv_id": "2508.15940v1",
      "title": "ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation",
      "title_zh": "ASIC-Agentï¼šé¢å‘ ASIC è®¾è®¡çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»ŸåŠåŸºå‡†è¯„ä¼°",
      "authors": [
        "Ahmed Allam",
        "Youssef Mansour",
        "Mohamed Shalan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in Register Transfer Level (RTL) design, enabling high-quality code generation from natural language descriptions. However, LLMs alone face significant limitations in real-world hardware design workflows, including the inability to execute code, lack of debugging capabilities, and absence of long-term memory. To address these challenges, we present ASIC-Agent, an autonomous system designed specifically for digital ASIC design tasks. ASIC-Agent enhances base LLMs with a multi-agent architecture incorporating specialized sub-agents for RTL generation, verification, OpenLane hardening, and Caravel chip integration, all operating within a comprehensive sandbox environment with access to essential hardware design tools. The system leverages a vector database containing documentation, API references, error knowledge, and curated insights from the open-source silicon community. To evaluate ASIC-Agent's performance, we introduce ASIC-Agent-Bench, the first benchmark specifically designed to assess agentic systems in hardware design tasks. We evaluate ASIC-Agent with various base LLMs, providing quantitative comparisons and qualitative insights into agent behavior across different design scenarios. Our results demonstrate that ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a broad range of ASIC design tasks spanning varying levels of complexity, showing the potential of significantly accelerating the ASIC design workflow.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ASIC-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ•°å­—ASICè®¾è®¡ä»»åŠ¡è®¾è®¡çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚ä¸ºäº†è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®é™…ç¡¬ä»¶è®¾è®¡å·¥ä½œæµä¸­æ— æ³•æ‰§è¡Œä»£ç ã€ç¼ºä¹è°ƒè¯•èƒ½åŠ›ä»¥åŠç¼ºä¹é•¿æœŸè®°å¿†ç­‰å±€é™æ€§ï¼ŒASIC-Agenté€šè¿‡å¤šæ™ºèƒ½ä½“æ¶æ„å¢å¼ºäº†åŸºç¡€æ¨¡å‹çš„åŠŸèƒ½ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä¸“é—¨ç”¨äºRTLç”Ÿæˆã€éªŒè¯ã€OpenLaneç¡¬åŒ–å’ŒCaravelèŠ¯ç‰‡é›†æˆçš„å­æ™ºèƒ½ä½“ï¼Œå¹¶åœ¨åŒ…å«ç¡¬ä»¶è®¾è®¡å·¥å…·çš„æ²™ç›’ç¯å¢ƒä¸­è¿è¡Œã€‚æ­¤å¤–ï¼ŒASIC-Agentåˆ©ç”¨å‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æ¡£ã€APIå‚è€ƒåŠé”™è¯¯çŸ¥è¯†ï¼Œä»¥æœ‰æ•ˆæ•´åˆå¼€æºèŠ¯ç‰‡ç¤¾åŒºçš„è§è§£ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº†ASIC-Agent-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ç¡¬ä»¶è®¾è®¡ä»»åŠ¡ä¸­æ™ºèƒ½ä½“ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Claude 4 Sonnetç­‰æ¨¡å‹çš„æ”¯æŒä¸‹ï¼ŒASIC-Agentèƒ½å¤ŸæˆåŠŸè‡ªåŠ¨åŒ–æ‰§è¡Œä¸åŒå¤æ‚ç¨‹åº¦çš„ASICè®¾è®¡ä»»åŠ¡ã€‚è¯¥é¡¹å·¥ä½œå……åˆ†å±•ç¤ºäº†è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ˜¾è‘—åŠ é€ŸASICè®¾è®¡å·¥ä½œæµæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.AR",
      "comment": "2025 IEEE International Conference on LLM-Aided Design (ICLAD)",
      "pdf_url": "https://arxiv.org/pdf/2508.15940v1",
      "published_date": "2025-08-21 20:21:34 UTC",
      "updated_date": "2025-08-21 20:21:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:04.867554+00:00"
    },
    {
      "arxiv_id": "2508.15934v1",
      "title": "Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification",
      "title_zh": "æå‡æ–‡æœ¬åˆ†ç±»ä¸­å¹²å‡€æ ‡ç­¾åé—¨æ”»å‡»æ•ˆæœçš„ç­–ç•¥æ€§æ ·æœ¬é€‰æ‹©",
      "authors": [
        "Onur Alp Kirci",
        "M. Emre Gursoy"
      ],
      "abstract": "Backdoor attacks pose a significant threat to the integrity of text classification models used in natural language processing. While several dirty-label attacks that achieve high attack success rates (ASR) have been proposed, clean-label attacks are inherently more difficult. In this paper, we propose three sample selection strategies to improve attack effectiveness in clean-label scenarios: Minimum, Above50, and Below50. Our strategies identify those samples which the model predicts incorrectly or with low confidence, and by injecting backdoor triggers into such samples, we aim to induce a stronger association between the trigger patterns and the attacker-desired target label. We apply our methods to clean-label variants of four canonical backdoor attacks (InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets (IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT, RoBERTa). Results show that the proposed strategies, particularly the Minimum strategy, significantly improve the ASR over random sample selection with little or no degradation in the model's clean accuracy. Furthermore, clean-label attacks enhanced by our strategies outperform BITE, a state of the art clean-label attack method, in many configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„åé—¨æ”»å‡»é—®é¢˜ï¼ŒæŒ‡å‡ºç›¸æ¯”äºè„æ ‡ç­¾æ”»å‡»(Dirty-label attacks)ï¼Œå¹²å‡€æ ‡ç­¾æ”»å‡»(Clean-label attacks)åœ¨å®ç°é«˜æ”»å‡»æˆåŠŸç‡(ASR)æ–¹é¢æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸‰ç§æ—¨åœ¨æå‡å¹²å‡€æ ‡ç­¾åœºæ™¯æ”»å‡»æ•ˆæœçš„æ ·æœ¬é€‰æ‹©ç­–ç•¥ï¼šMinimumã€Above50 å’Œ Below50ã€‚è¿™äº›ç­–ç•¥é€šè¿‡è¯†åˆ«æ¨¡å‹é¢„æµ‹é”™è¯¯æˆ–ç½®ä¿¡åº¦è¾ƒä½çš„æ ·æœ¬å¹¶æ³¨å…¥åé—¨è§¦å‘å™¨(Backdoor triggers)ï¼Œæ—¨åœ¨è¯±å¯¼è§¦å‘æ¨¡å¼ä¸æ”»å‡»è€…ç›®æ ‡æ ‡ç­¾ä¹‹é—´å»ºç«‹æ›´å¼ºçš„å…³è”ã€‚ç ”ç©¶äººå‘˜å°†è¿™äº›æ–¹æ³•åº”ç”¨äº InsertSentã€WordInjã€StyleBkd å’Œ SynBkd ç­‰å››ç§å…¸å‹åé—¨æ”»å‡»å˜ä½“ï¼Œå¹¶åœ¨ IMDBã€SST2 ç­‰æ•°æ®é›†ä»¥åŠ BERTã€RoBERTa ç­‰å¤šç§æ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç­–ç•¥ï¼ˆå°¤å…¶æ˜¯ Minimum ç­–ç•¥ï¼‰åœ¨ä¸æŸå®³æ¨¡å‹å¹²å‡€å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº† ASRï¼Œä¼˜äºéšæœºæ ·æœ¬é€‰æ‹©ã€‚æ­¤å¤–ï¼Œåœ¨å¤šç§é…ç½®ä¸‹ï¼Œè¯¥ç­–ç•¥å¢å¼ºçš„æ”»å‡»æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç›®å‰æœ€å…ˆè¿›çš„å¹²å‡€æ ‡ç­¾æ”»å‡»æ–¹æ³• BITEã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15934v1",
      "published_date": "2025-08-21 19:53:26 UTC",
      "updated_date": "2025-08-21 19:53:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:08.649901+00:00"
    },
    {
      "arxiv_id": "2508.15926v1",
      "title": "Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making",
      "title_zh": "å™ªå£°ã€é€‚åº”ä¸ç­–ç•¥ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å†³ç­–è¡Œä¸ºçš„ä¿çœŸåº¦",
      "authors": [
        "Yuanjun Feng",
        "Vivek Choudhary",
        "Yash Raj Shrestha"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in social science simulations. While their performance on reasoning and optimization tasks has been extensively evaluated, less attention has been paid to their ability to simulate human decision-making's variability and adaptability. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation) to examine how LLM agents adapt under different levels of external guidance and human-derived noise. We validate the framework on two classic economics tasks, irrationality in the second-price auction and decision bias in the newsvendor problem, showing behavioral gaps between LLMs and humans.\n  We find that LLMs, by default, converge on stable and conservative strategies that diverge from observed human behaviors. Risk-framed instructions impact LLM behavior predictably but do not replicate human-like diversity. Incorporating human data through in-context learning narrows the gap but fails to reach human subjects' strategic variability. These results highlight a persistent alignment gap in behavioral fidelity and suggest that future LLM evaluations should consider more process-level realism. We present a process-oriented approach for assessing LLMs in dynamic decision-making tasks, offering guidance for their application in synthetic data for social science research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé¢å‘è¿‡ç¨‹çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å†…åœ¨æ€§(Intrinsicality)ã€æŒ‡ä»¤(Instruction)å’Œæ¨¡ä»¿(Imitation)ä¸‰ç§æ¸è¿›å¼å¹²é¢„ï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å†³ç­–æ¨¡æ‹Ÿä¸­çš„å˜å¼‚æ€§ä¸é€‚åº”æ€§ã€‚è¯¥æ¡†æ¶åœ¨ç¬¬äºŒä»·æ ¼æ‹å–(second-price auction)ä¸­çš„éç†æ€§è¡Œä¸ºå’ŒæŠ¥ç«¥é—®é¢˜(newsvendor problem)ä¸­çš„å†³ç­–åå·®è¿™ä¸¤é¡¹ç»å…¸ç»æµå­¦ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒå‘ç°ï¼ŒLLMsåœ¨é»˜è®¤çŠ¶æ€ä¸‹å€¾å‘äºæ”¶æ•›è‡³ç¨³å®šä¸”ä¿å®ˆçš„ç­–ç•¥ï¼Œè¿™ä¸äººç±»è§‚å¯Ÿåˆ°çš„å¤šæ ·åŒ–è¡Œä¸ºå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è™½ç„¶é£é™©æ¡†æ¶æŒ‡ä»¤(risk-framed instructions)èƒ½é¢„æµ‹æ€§åœ°å½±å“æ¨¡å‹è¡Œä¸ºï¼Œä½†ä»æ— æ³•å®Œå…¨å¤åˆ¶äººç±»å†³ç­–çš„å¤šæ ·æ€§ã€‚å³ä½¿é€šè¿‡æƒ…å¢ƒå­¦ä¹ (in-context learning)å¼•å…¥äººç±»æ•°æ®ï¼Œè™½ç„¶èƒ½ç¼©å°è¡Œä¸ºé¸¿æ²Ÿï¼Œä½†ä»æ— æ³•è¾¾åˆ°äººç±»å—è¯•è€…çš„ç­–ç•¥å˜å¼‚æ€§(strategic variability)ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†è¡Œä¸ºå¿ å®åº¦(behavioral fidelity)æ–¹é¢æŒä¹…å­˜åœ¨çš„å¯¹é½å·®è·ï¼Œä¸ºæœªæ¥åœ¨ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­ä½¿ç”¨åˆæˆæ•°æ®æä¾›äº†è¿‡ç¨‹å¯¼å‘çš„è¯„ä¼°æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "Accepted to EMNLP 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2508.15926v1",
      "published_date": "2025-08-21 18:55:53 UTC",
      "updated_date": "2025-08-21 18:55:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:23.354375+00:00"
    },
    {
      "arxiv_id": "2508.15922v1",
      "title": "Probabilistic Forecasting Cryptocurrencies Volatility: From Point to Quantile Forecasts",
      "title_zh": "åŠ å¯†è´§å¸æ³¢åŠ¨ç‡æ¦‚ç‡é¢„æµ‹ï¼šä»ç‚¹é¢„æµ‹åˆ°åˆ†ä½æ•°é¢„æµ‹",
      "authors": [
        "Grzegorz Dudek",
        "Witold Orzeszko",
        "Piotr Fiszeder"
      ],
      "abstract": "Cryptocurrency markets are characterized by extreme volatility, making accurate forecasts essential for effective risk management and informed trading strategies. Traditional deterministic (point) forecasting methods are inadequate for capturing the full spectrum of potential volatility outcomes, underscoring the importance of probabilistic approaches. To address this limitation, this paper introduces probabilistic forecasting methods that leverage point forecasts from a wide range of base models, including statistical (HAR, GARCH, ARFIMA) and machine learning (e.g. LASSO, SVR, MLP, Random Forest, LSTM) algorithms, to estimate conditional quantiles of cryptocurrency realized variance. To the best of our knowledge, this is the first study in the literature to propose and systematically evaluate probabilistic forecasts of variance in cryptocurrency markets based on predictions derived from multiple base models. Our empirical results for Bitcoin demonstrate that the Quantile Estimation through Residual Simulation (QRS) method, particularly when applied to linear base models operating on log-transformed realized volatility data, consistently outperforms more sophisticated alternatives. Additionally, we highlight the robustness of the probabilistic stacking framework, providing comprehensive insights into uncertainty and risk inherent in cryptocurrency volatility forecasting. This research fills a significant gap in the literature, contributing practical probabilistic forecasting methodologies tailored specifically to cryptocurrency markets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ å¯†è´§å¸å¸‚åœºæç«¯çš„æ³¢åŠ¨æ€§ï¼Œæå‡ºäº†ä»ä¼ ç»Ÿçš„ç‚¹é¢„æµ‹è½¬å‘æ¦‚ç‡(Probabilistic)é¢„æµ‹çš„æ–¹æ³•ï¼Œä»¥æ›´å…¨é¢åœ°æ•æ‰é£é™©ã€‚é€šè¿‡æ•´åˆHARã€GARCHã€ARFIMAç­‰ç»Ÿè®¡æ¨¡å‹ä»¥åŠLASSOã€SVRã€Random Forestã€LSTMç­‰æœºå™¨å­¦ä¹ ç®—æ³•ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨ä¼°ç®—åŠ å¯†è´§å¸å·²å®ç°æ–¹å·®(Realized Variance)çš„æ¡ä»¶åˆ†ä½æ•°ã€‚å¯¹æ¯”ç‰¹å¸çš„å®è¯ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡æ®‹å·®æ¨¡æ‹Ÿè¿›è¡Œåˆ†ä½æ•°ä¼°è®¡(QRS)çš„æ–¹æ³•åœ¨ç»“åˆçº¿æ€§åŸºç¡€æ¨¡å‹å’Œå¯¹æ•°å˜æ¢æ•°æ®æ—¶ï¼Œå…¶é¢„æµ‹è¡¨ç°ä¼˜äºå…¶ä»–å¤æ‚çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº†æ¦‚ç‡å †å (Probabilistic Stacking)æ¡†æ¶çš„ç¨³å¥æ€§ï¼Œä¸ºç†è§£æ³¢åŠ¨æ€§é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§æä¾›äº†æ·±å…¥è§è§£ã€‚ä½œä¸ºé¦–ä¸ªç³»ç»Ÿè¯„ä¼°å¤šæ¨¡å‹é©±åŠ¨çš„åŠ å¯†è´§å¸æ–¹å·®æ¦‚ç‡é¢„æµ‹çš„ç ”ç©¶ï¼Œè¯¥å·¥ä½œæœ‰æ•ˆå¡«è¡¥äº†é¢†åŸŸç©ºç™½å¹¶æä¾›äº†å®ç”¨çš„é£é™©ç®¡ç†å·¥å…·ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST",
      "comment": "DSAA'25 conference paper",
      "pdf_url": "https://arxiv.org/pdf/2508.15922v1",
      "published_date": "2025-08-21 18:42:11 UTC",
      "updated_date": "2025-08-21 18:42:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:29.089661+00:00"
    },
    {
      "arxiv_id": "2508.15919v2",
      "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling",
      "title_zh": "HyperFlexisï¼šé¢å‘å¤š SLO æœåŠ¡ä¸å¿«é€Ÿæ‰©å±•çš„ç®—æ³•ä¸ç³»ç»ŸååŒè®¾è®¡",
      "authors": [
        "Zahra Yousefijamarani",
        "Xinglu Wang",
        "Qian Wang",
        "Morgan Lindsay Heisler",
        "Taha Shabani",
        "Niloofar Gholipour",
        "Parham Yassini",
        "Hong Chang",
        "Kan Chen",
        "Qiantao Zhang",
        "Xiaolong Bai",
        "Jiannan Wang",
        "Ying Xiong",
        "Yong Zhang",
        "Zhenan Fan"
      ],
      "abstract": "Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to 19.39$\\times$. These optimizations allow the system to achieve up to 4.44$\\times$ higher SLO attainment, 65.82% lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HyperFlexisï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†æœåŠ¡ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤šæ ·åŒ–Service-Level Objectives (SLOs)å’Œå¤æ‚è´Ÿè½½ä¸‹çš„å®æ—¶è°ƒåº¦ä¸å¿«é€Ÿæ‰©ç¼©å®¹æŒ‘æˆ˜ã€‚ç³»ç»Ÿæ ¸å¿ƒåŒ…å«ä¸€ä¸ªå¤šSLOæ„ŸçŸ¥è°ƒåº¦å™¨ï¼Œé€šè¿‡é¢„ç®—ä¼°è®¡å’Œè¯·æ±‚ä¼˜å…ˆçº§æ’åºæœºåˆ¶ï¼Œç¡®ä¿äº†æ–°æ—§è¯·æ±‚åœ¨Prefill/Decode (P/D)è§£è€¦æ¶æ„ä¸‹çš„ä¸»åŠ¨åˆè§„æ€§ã€‚HyperFlexisè¿˜æ”¯æŒé«˜æ•ˆçš„KV cacheä¼ è¾“å’Œå¿«é€Ÿçš„P/Dè§’è‰²è½¬æ¢ï¼Œå¹¶å¼•å…¥äº†è®¾å¤‡åˆ°è®¾å¤‡(Device-to-Device, D2D)æƒé‡ä¼ è¾“æœºåˆ¶ä»¥æ˜¾è‘—é™ä½å†·å¯åŠ¨å»¶è¿Ÿã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æœºåˆ¶å°†æƒé‡åŠ è½½å¼€é”€é™ä½äº†19.39å€ï¼Œä½¿ç³»ç»Ÿåœ¨ä¿æŒæˆæœ¬æ•ˆç›Šçš„åŒæ—¶ï¼Œå°†SLOè¾¾æˆç‡æå‡äº†4.44å€ã€‚æœ€ç»ˆï¼ŒHyperFlexisæˆåŠŸå°†è¯·æ±‚å»¶è¿Ÿé™ä½äº†65.82%ï¼Œä¸ºé«˜æ€§èƒ½ã€é«˜å¯é çš„LLMæœåŠ¡æä¾›äº†ç®—æ³•ä¸ç³»ç»Ÿå±‚é¢çš„åŒé‡åˆ›æ–°ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15919v2",
      "published_date": "2025-08-21 18:40:20 UTC",
      "updated_date": "2025-09-25 03:00:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:34.996887+00:00"
    },
    {
      "arxiv_id": "2508.16688v1",
      "title": "Cybernaut: Towards Reliable Web Automation",
      "title_zh": "Cybernautï¼šè¿ˆå‘å¯é çš„ Web è‡ªåŠ¨åŒ–",
      "authors": [
        "Ankur Tomar",
        "Hengyue Liang",
        "Indranil Bhattacharya",
        "Natalia Larios",
        "Francesco Carbone"
      ],
      "abstract": "The emergence of AI-driven web automation through Large Language Models (LLMs) offers unprecedented opportunities for optimizing digital workflows. However, deploying such systems within industry's real-world environments presents four core challenges: (1) ensuring consistent execution, (2) accurately identifying critical HTML elements, (3) meeting human-like accuracy in order to automate operations at scale and (4) the lack of comprehensive benchmarking data on internal web applications. Existing solutions are primarily tailored for well-designed, consumer-facing websites (e.g., Amazon.com, Apple.com) and fall short in addressing the complexity of poorly-designed internal web interfaces. To address these limitations, we present Cybernaut, a novel framework to ensure high execution consistency in web automation agents designed for robust enterprise use. Our contributions are threefold: (1) a Standard Operating Procedure (SOP) generator that converts user demonstrations into reliable automation instructions for linear browsing tasks, (2) a high-precision HTML DOM element recognition system tailored for the challenge of complex web interfaces, and (3) a quantitative metric to assess execution consistency. The empirical evaluation on our internal benchmark demonstrates that using our framework enables a 23.2% improvement (from 72% to 88.68%) in task execution success rate over the browser_use. Cybernaut identifies consistent execution patterns with 84.7% accuracy, enabling reliable confidence assessment and adaptive guidance during task execution in real-world systems. These results highlight Cybernaut's effectiveness in enterprise-scale web automation and lay a foundation for future advancements in web automation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Cybernautï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡ç½‘é¡µè‡ªåŠ¨åŒ–ä»£ç†åœ¨ä¼ä¸šçº§åº”ç”¨ä¸­æ‰§è¡Œå¯é æ€§çš„åˆ›æ–°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†è®¾è®¡æ¬ ä½³çš„å†…éƒ¨ç½‘é¡µç•Œé¢æ—¶é¢ä¸´çš„æ‰§è¡Œä¸€è‡´æ€§å·®ã€å…ƒç´ è¯†åˆ«å›°éš¾åŠç¼ºä¹åŸºå‡†æ•°æ®ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†èƒ½å°†ç”¨æˆ·æ¼”ç¤ºè½¬åŒ–ä¸ºå¯é æŒ‡ä»¤çš„æ ‡å‡†æ“ä½œç¨‹åº(Standard Operating Procedure, SOP)ç”Ÿæˆå™¨ï¼Œä»¥åŠé’ˆå¯¹å¤æ‚ç•Œé¢çš„é«˜ç²¾åº¦ HTML DOM å…ƒç´ è¯†åˆ«ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§é‡åŒ–æŒ‡æ ‡æ¥è¯„ä¼°æ‰§è¡Œä¸€è‡´æ€§ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»¥ 84.7% çš„å‡†ç¡®ç‡è¯†åˆ«æ‰§è¡Œæ¨¡å¼å¹¶æä¾›è‡ªé€‚åº”æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCybernaut åœ¨å†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­å°†ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡æå‡è‡³ 88.68%ï¼Œè¾ƒ browser_use æ¨¡å‹æ”¹è¿›äº† 23.2%ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°å¤§è§„æ¨¡ã€é«˜å¯é æ€§çš„ä¼ä¸šçº§ç½‘é¡µè‡ªåŠ¨åŒ–æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16688v1",
      "published_date": "2025-08-21 18:39:35 UTC",
      "updated_date": "2025-08-21 18:39:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:31.125913+00:00"
    },
    {
      "arxiv_id": "2508.15916v1",
      "title": "Information Ecosystem Reengineering via Public Sector Knowledge Representation",
      "title_zh": "åŸºäºå…¬å…±éƒ¨é—¨çŸ¥è¯†è¡¨ç¤ºçš„ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿé‡æ„",
      "authors": [
        "Mayukh Bagchi"
      ],
      "abstract": "Information Ecosystem Reengineering (IER) -- the technological reconditioning of information sources, services, and systems within a complex information ecosystem -- is a foundational challenge in the digital transformation of public sector services and smart governance platforms. From a semantic knowledge management perspective, IER becomes especially entangled due to the potentially infinite number of possibilities in its conceptualization, namely, as a result of manifoldness in the multi-level mix of perception, language and conceptual interlinkage implicit in all agents involved in such an effort. This paper proposes a novel approach -- Representation Disentanglement -- to disentangle these multiple layers of knowledge representation complexity hindering effective reengineering decision making. The approach is based on the theoretically grounded and implementationally robust ontology-driven conceptual modeling paradigm which has been widely adopted in systems analysis and (re)engineering. We argue that such a framework is essential to achieve explainability, traceability and semantic transparency in public sector knowledge representation and to support auditable decision workflows in governance ecosystems increasingly driven by Artificial Intelligence (AI) and data-centric architectures.",
      "tldr_zh": "ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿé‡æ„ (Information Ecosystem Reengineering, IER) æ˜¯å…¬å…±éƒ¨é—¨æ•°å­—åŒ–è½¬å‹ä¸æ™ºæ…§æ²»ç†å¹³å°å»ºè®¾ä¸­çš„åŸºç¡€æŒ‘æˆ˜ï¼Œä½†åœ¨è¯­ä¹‰çŸ¥è¯†ç®¡ç†å±‚é¢å¸¸å› æ„ŸçŸ¥ã€è¯­è¨€å’Œæ¦‚å¿µå…³è”çš„å¤šæ ·æ€§è€Œé™·å…¥å¤æ‚äº¤ç»‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¡¨ç°è§£æ„ (Representation Disentanglement) çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åŒ–è§£é˜»ç¢æœ‰æ•ˆé‡æ„å†³ç­–çš„å¤šå±‚çŸ¥è¯†è¡¨ç¤ºå¤æ‚æ€§ã€‚è¯¥æ–¹æ³•åŸºäºç†è®ºæ‰å®ä¸”å®æ–½ç¨³å¥çš„æœ¬ä½“é©±åŠ¨æ¦‚å¿µå»ºæ¨¡ (ontology-driven conceptual modeling) èŒƒå¼ï¼Œåˆ©ç”¨è¯¥èŒƒå¼åœ¨ç³»ç»Ÿåˆ†æä¸é‡æ„é¢†åŸŸçš„ä¼˜åŠ¿è¿›è¡ŒçŸ¥è¯†è§£æ„ã€‚ç ”ç©¶è®ºè¯äº†è¯¥æ¡†æ¶å¯¹äºåœ¨å…¬å…±éƒ¨é—¨çŸ¥è¯†è¡¨ç¤ºä¸­å®ç°å¯è§£é‡Šæ€§ (explainability)ã€å¯è¿½æº¯æ€§ (traceability) å’Œè¯­ä¹‰é€æ˜åº¦ (semantic transparency) çš„æ ¸å¿ƒä½œç”¨ã€‚æœ€åï¼Œè¯¥æ–¹æ³•ä¸ºç”±äººå·¥æ™ºèƒ½ (AI) å’Œä»¥æ•°æ®ä¸ºä¸­å¿ƒæ¶æ„é©±åŠ¨çš„æ²»ç†ç”Ÿæ€ç³»ç»Ÿæä¾›äº†æ”¯æŒå¯å®¡è®¡å†³ç­–å·¥ä½œæµçš„å…³é”®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15916v1",
      "published_date": "2025-08-21 18:29:27 UTC",
      "updated_date": "2025-08-21 18:29:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:35.154565+00:00"
    },
    {
      "arxiv_id": "2508.15910v1",
      "title": "Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets",
      "title_zh": "æ–‡æœ¬åˆ°è¡¨æ ¼ç”Ÿæˆçš„ç»“æ„åŒ–è§£ç è¯„ä¼°ï¼šæ¥è‡ªä¸‰ä¸ªæ•°æ®é›†çš„å®è¯è¯æ®",
      "authors": [
        "Julian Oestreich",
        "Lydia MÃ¼ller"
      ],
      "abstract": "We present a comprehensive evaluation of structured decoding for text-to-table generation with large language models (LLMs). While previous work has primarily focused on unconstrained generation of tables, the impact of enforcing structural constraints during generation remains underexplored. We systematically compare schema-guided (structured) decoding to standard one-shot prompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using open-source LLMs of up to 32B parameters, assessing the performance of table generation approaches in resource-constrained settings. Our experiments cover a wide range of evaluation metrics at cell, row, and table levels. Results demonstrate that structured decoding significantly enhances the validity and alignment of generated tables, particularly in scenarios demanding precise numerical alignment (Rotowire), but may degrade performance in contexts involving densely packed textual information (E2E) or extensive aggregation over lengthy texts (Livesum). We further analyze the suitability of different evaluation metrics and discuss the influence of model size.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬åˆ°è¡¨æ ¼ç”Ÿæˆï¼ˆText-to-Table Generationï¼‰ä»»åŠ¡ä¸­åº”ç”¨ç»“æ„åŒ–è§£ç ï¼ˆStructured Decodingï¼‰çš„æ•ˆæœã€‚é€šè¿‡åœ¨E2Eã€Rotowireå’ŒLivesumä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒï¼Œç ”ç©¶è€…åˆ†æäº†æ¨¡å¼å¼•å¯¼ï¼ˆSchema-guidedï¼‰è§£ç ä¸æ ‡å‡†çš„ä¸€å‘å³ä¸­æç¤ºï¼ˆOne-shot Promptingï¼‰åœ¨ä¸åŒå¤æ‚ç¨‹åº¦ä¸‹çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–è§£ç èƒ½å¤Ÿæ˜¾è‘—æå‡è¡¨æ ¼çš„åˆæ³•æ€§å’Œå¯¹é½ç²¾åº¦ï¼Œå°¤å…¶åœ¨æ¶‰åŠç²¾ç¡®æ•°å€¼å¯¹é½çš„Rotowireä»»åŠ¡ä¸­ä¼˜åŠ¿æ˜æ˜¾ã€‚ä½†åœ¨å¤„ç†é«˜å¯†åº¦æ–‡æœ¬ä¿¡æ¯æˆ–é•¿æ–‡æœ¬èšåˆä»»åŠ¡æ—¶ï¼Œå¼ºåˆ¶ç»“æ„åŒ–çº¦æŸåè€Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†è¯„ä¼°æŒ‡æ ‡çš„æœ‰æ•ˆæ€§ä»¥åŠæ¨¡å‹è§„æ¨¡å¯¹ç”Ÿæˆè´¨é‡çš„å…·ä½“å½±å“ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "to be published in the workshop proceedings of the \"From Rules to Language Models: Comparative Performance Evaluation\" workshop, held alongside RANLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15910v1",
      "published_date": "2025-08-21 18:11:16 UTC",
      "updated_date": "2025-08-21 18:11:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:37.184488+00:00"
    },
    {
      "arxiv_id": "2508.15769v2",
      "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
      "title_zh": "SceneGenï¼šåŸºäºå•æ¬¡å‰å‘ä¼ é€’çš„å•å›¾åƒä¸‰ç»´åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Yanxu Meng",
        "Haoning Wu",
        "Ya Zhang",
        "Weidi Xie"
      ],
      "abstract": "3D content generation has recently attracted significant research interest, driven by its critical applications in VR/AR and embodied AI. In this work, we tackle the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for extra optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architecture yields improved generation performance when multiple images are provided; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robustness of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SceneGenï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡å•å¼ åœºæ™¯å›¾åƒåŠå…¶å¯¹åº”çš„ç‰©ä½“æ©ç (object masks)åœ¨å•æ¬¡å‰å‘ä¼ é€’(single feedforward pass)ä¸­ç”Ÿæˆå¤šä¸ª3Dèµ„äº§çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€é¢å¤–çš„ä¼˜åŒ–æˆ–èµ„äº§æ£€ç´¢ï¼Œå³å¯åŒæ—¶äº§ç”Ÿå…·æœ‰å‡ ä½•ç»“æ„(geometry)å’Œçº¹ç†(texture)çš„å¤šä¸ª3Dèµ„äº§ã€‚SceneGenå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç‰¹å¾èšåˆæ¨¡å—(feature aggregation module)ï¼Œæœ‰æ•ˆæ•´åˆäº†è§†è§‰å’Œå‡ ä½•ç¼–ç å™¨çš„å±€éƒ¨åŠå…¨å±€åœºæ™¯ä¿¡æ¯ã€‚é…åˆä½ç½®å¤´(position head)çš„è®¾è®¡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡å¤„ç†ä¸­å‡†ç¡®ç¡®å®š3Dèµ„äº§åŠå…¶ç›¸å¯¹ç©ºé—´ä½ç½®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨å¤šå›¾åƒè¾“å…¥åœºæ™¯ä¸‹ä¹Ÿèƒ½å¤Ÿè¡¨ç°å‡ºæ›´ä¼˜çš„ç”Ÿæˆæ€§èƒ½ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¯å®äº†è¯¥æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œä¸ºé«˜è´¨é‡3Då†…å®¹ç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ï¼Œæœ‰æœ›æ¨åŠ¨å…¶åœ¨VR/ARå’Œå…·èº«æ™ºèƒ½(embodied AI)ç­‰é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by 3DV 2026; Project Page: https://mengmouxu.github.io/SceneGen",
      "pdf_url": "https://arxiv.org/pdf/2508.15769v2",
      "published_date": "2025-08-21 17:59:16 UTC",
      "updated_date": "2025-12-09 17:19:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:49.551509+00:00"
    },
    {
      "arxiv_id": "2508.15884v3",
      "title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search",
      "title_zh": "Jet-Nemotronï¼šåŸºäºåç¥ç»æ¶æ„æœç´¢çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹",
      "authors": [
        "Yuxian Gu",
        "Qinghao Hu",
        "Shang Yang",
        "Haocheng Xi",
        "Junyu Chen",
        "Song Han",
        "Han Cai"
      ],
      "abstract": "We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Jet-Nemotron ç³»åˆ—æ··åˆæ¶æ„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¿æŒæˆ–è¶…è¶Šé¢†å…ˆå…¨æ³¨æ„åŠ› (full-attention) æ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡ç”Ÿæˆååé‡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åä¸º Post Neural Architecture Search (PostNAS) çš„æ–°å‹ç¥ç»æ¶æ„æ¢ç´¢æµç¨‹ï¼Œé€šè¿‡å†»ç»“é¢„è®­ç»ƒå…¨æ³¨æ„åŠ›æ¨¡å‹çš„ MLP æƒé‡ï¼Œå®ç°äº†å¯¹æ³¨æ„åŠ›æ¨¡å—çš„é«˜æ•ˆè®¾è®¡ã€‚PostNAS æµç¨‹åŒ…å«ä¼˜åŒ–å…¨æ³¨æ„åŠ›å±‚å¸ƒå±€ã€çº¿æ€§æ³¨æ„åŠ› (linear attention) æ¨¡å—é€‰æ‹©ã€æ–°å‹æ¨¡å—è®¾è®¡ä»¥åŠç¡¬ä»¶æ„ŸçŸ¥çš„è¶…å‚æ•°æœç´¢å››ä¸ªå…³é”®ç»„ä»¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒJet-Nemotron-2B åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡å¯åª²ç¾æˆ–è¶…è¶Š Qwen3ã€Gemma3 å’Œ Llama3.2 ç­‰æ¨¡å‹ï¼ŒåŒæ—¶æä¾›é«˜è¾¾ 53.6 å€çš„ç”Ÿæˆååé‡åŠ é€Ÿå’Œ 6.1 å€çš„é¢„å¡«å…… (prefilling) åŠ é€Ÿã€‚æ­¤å¤–ï¼Œå°½ç®¡å‚æ•°è§„æ¨¡è¾ƒå°ï¼Œè¯¥æ¨¡å‹åœ¨ MMLU å’Œ MMLU-Pro ä¸Šçš„è¡¨ç°ç”šè‡³ä¼˜äº DeepSeek-V3-Small ç­‰å…ˆè¿›çš„ä¸“å®¶æ··åˆ (MoE) å…¨æ³¨æ„åŠ›æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ··åˆæ¶æ„åœ¨æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡ä¸Šçš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15884v3",
      "published_date": "2025-08-21 17:59:08 UTC",
      "updated_date": "2025-09-28 18:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:48.291831+00:00"
    },
    {
      "arxiv_id": "2508.15766v1",
      "title": "Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO",
      "title_zh": "é€šè¿‡ç§©æ„ŸçŸ¥ Beam GRPO çš„ Transformer æŒ–æ˜éšè—ä»£æ•°ç»“æ„",
      "authors": [
        "Jaeha Lee",
        "Gio Huh",
        "Ning Su",
        "Tony Yue YU"
      ],
      "abstract": "Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations. In this work, we investigate their capacity for non-linear latent pattern discovery in the context of functional decomposition, focusing on the challenging algebraic task of multivariate polynomial decomposition. This problem, with widespread applications in science and engineering, is proved to be NP-hard, and demands both precision and insight. Our contributions are threefold: First, we develop a synthetic data generation pipeline providing fine-grained control over problem complexity. Second, we train transformer models via supervised learning and evaluate them across four key dimensions involving scaling behavior and generalizability. Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method suitable for hard algebraic problems. Finetuning with BGRPO improves accuracy while reducing beam width by up to half, resulting in approximately 75% lower inference compute. Additionally, our model demonstrates competitive performance in polynomial simplification, outperforming Mathematica in various cases.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Transformer æ¨¡å‹åœ¨éçº¿æ€§æ½œåœ¨æ¨¡å¼å‘ç°ä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨äºå¤šå˜é‡å¤šé¡¹å¼åˆ†è§£ (multivariate polynomial decomposition) è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§ä¸”å±äº NP-hard çš„ä»£æ•°ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆå¼€å‘äº†ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ï¼Œèƒ½å¤Ÿå¯¹é—®é¢˜çš„å¤æ‚æ€§è¿›è¡Œç²¾ç»†åŒ–æ§åˆ¶ï¼Œå¹¶ä»ç¼©æ”¾è¡Œä¸º (scaling behavior) å’Œæ³›åŒ–èƒ½åŠ› (generalizability) ç­‰ç»´åº¦å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚éšåï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º Beam Grouped Relative Policy Optimization (BGRPO) çš„ç§©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤æ‚çš„ä»£æ•°è®¡ç®—é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ BGRPO è¿›è¡Œå¾®è°ƒä¸ä»…æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œè¿˜èƒ½å°†é›†æŸå®½åº¦ (beam width) å‡å°‘ä¸€åŠï¼Œä»è€Œä½¿æ¨ç†è®¡ç®—å¼€é”€é™ä½çº¦ 75%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹å¼ç®€åŒ– (polynomial simplification) ä»»åŠ¡ä¸­è¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œåœ¨å¤šä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­çš„æ€§èƒ½è¶…è¶Šäº†ä¼ ç»Ÿçš„ Mathematica è½¯ä»¶ï¼Œä¸ºç¬¦å·è®¡ç®—å’Œé€»è¾‘æ¨ç†é¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15766v1",
      "published_date": "2025-08-21 17:58:50 UTC",
      "updated_date": "2025-08-21 17:58:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:07.494356+00:00"
    },
    {
      "arxiv_id": "2508.15760v1",
      "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries",
      "title_zh": "LiveMCP-101ï¼šé’ˆå¯¹æŒ‘æˆ˜æ€§æŸ¥è¯¢çš„ MCP æ™ºèƒ½ä½“å‹åŠ›æµ‹è¯•ä¸è¯Šæ–­",
      "authors": [
        "Ming Yin",
        "Dinghan Shen",
        "Silei Xu",
        "Jianbing Han",
        "Sixun Dong",
        "Mian Zhang",
        "Yebowen Hu",
        "Shujian Liu",
        "Simin Ma",
        "Song Wang",
        "Sathish Reddy Indurthi",
        "Xun Wang",
        "Yiran Chen",
        "Kaiqiang Song"
      ],
      "abstract": "Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIæ™ºèƒ½ä½“åœ¨åˆ©ç”¨Model Context Protocol (MCP)å·¥å…·è§£å†³ç°å®åŠ¨æ€åœºæ™¯ä¸­å¤šæ­¥ä»»åŠ¡çš„è¯„ä¼°ç¼ºå£ï¼Œæå‡ºäº†åŒ…å«101ä¸ªçœŸå®ä¸–ç•ŒæŸ¥è¯¢çš„åŸºå‡†æµ‹è¯•LiveMCP-101ã€‚è¯¥åŸºå‡†æ¶µç›–äº†Web Searchã€File Operationsã€Mathematical Reasoningå’ŒData Analysisç­‰å¤šç§å·¥å…·çš„ååŒè°ƒç”¨ï¼Œå¹¶å¼•å…¥äº†åŸºäºGround-truth Execution Plansçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å‡†ç¡®åœ°åæ˜ ç¯å¢ƒçš„åŠ¨æ€æ¼”å˜ã€‚å®éªŒå‘ç°ï¼Œå³ä½¿æ˜¯é¡¶å°–çš„Large Language Models (LLMs)åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä¹Ÿä½äº60%ï¼Œæš´éœ²å‡ºåœ¨Tool Orchestrationæ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡è¯¦ç»†çš„æ¶ˆèå®éªŒå’Œé”™è¯¯åˆ†æï¼Œç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­çš„Failure Modeså’ŒTokenä½¿ç”¨æ•ˆç‡ç¼ºé™·ã€‚LiveMCP-101ä¸ºç°å®åœºæ™¯ä¸‹AIæ™ºèƒ½ä½“çš„èƒ½åŠ›è¯„ä¼°å»ºç«‹äº†ä¸¥è‹›æ ‡å‡†ï¼Œå¯¹æ¨åŠ¨è‡ªä¸»AIç³»ç»Ÿå¯é æ‰§è¡Œå¤æ‚ä»»åŠ¡å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15760v1",
      "published_date": "2025-08-21 17:55:54 UTC",
      "updated_date": "2025-08-21 17:55:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:06:53.592155+00:00"
    },
    {
      "arxiv_id": "2508.15757v1",
      "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback",
      "title_zh": "è¯­è¨€å¼•å¯¼è°ƒä¼˜ï¼šåˆ©ç”¨æ–‡æœ¬åé¦ˆå¢å¼ºæ•°å€¼ä¼˜åŒ–",
      "authors": [
        "Yuxing Lu",
        "Yucheng Hu",
        "Nan Sun",
        "Xukai Zhao"
      ],
      "abstract": "Configuration optimization remains a critical bottleneck in machine learning, requiring coordinated tuning across model architecture, training strategy, feature engineering, and hyperparameters. Traditional approaches treat these dimensions independently and lack interpretability, while recent automated methods struggle with dynamic adaptability and semantic reasoning about optimization decisions. We introduce Language-Guided Tuning (LGT), a novel framework that employs multi-agent Large Language Models to intelligently optimize configurations through natural language reasoning. We apply textual gradients - qualitative feedback signals that complement numerical optimization by providing semantic understanding of training dynamics and configuration interdependencies. LGT coordinates three specialized agents: an Advisor that proposes configuration changes, an Evaluator that assesses progress, and an Optimizer that refines the decision-making process, creating a self-improving feedback loop. Through comprehensive evaluation on six diverse datasets, LGT demonstrates substantial improvements over traditional optimization methods, achieving performance gains while maintaining high interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Language-Guided Tuning (LGT)ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡è‡ªç„¶è¯­è¨€æ¨ç†è¿›è¡Œé…ç½®ä¼˜åŒ–çš„æ–°æ¡†æ¶ã€‚é’ˆå¯¹æœºå™¨å­¦ä¹ é…ç½®ä¼˜åŒ–ä¸­ä¼ ç»Ÿæ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§å’Œè¯­ä¹‰æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼ŒLGTå¼•å…¥äº†æ–‡æœ¬æ¢¯åº¦(textual gradients)ï¼Œå³èƒ½å¤Ÿè¡¥å……æ•°å€¼ä¼˜åŒ–çš„å®šæ€§åé¦ˆä¿¡å·ï¼Œä»¥æä¾›å¯¹è®­ç»ƒåŠ¨æ€å’Œé…ç½®ä¾èµ–å…³ç³»çš„è¯­ä¹‰ç†è§£ã€‚è¯¥æ¡†æ¶ååŒäº†Advisorã€Evaluatorå’ŒOptimizerä¸‰ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œæ„å»ºäº†ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„åé¦ˆé—­ç¯ï¼Œä»è€Œå®ç°æ™ºèƒ½åŒ–çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨å…­ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒLGTåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•ï¼ŒåŒæ—¶å±•ç°å‡ºæé«˜çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 4 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.15757v1",
      "published_date": "2025-08-21 17:55:07 UTC",
      "updated_date": "2025-08-21 17:55:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:27.296757+00:00"
    },
    {
      "arxiv_id": "2508.15755v1",
      "title": "Neural Robot Dynamics",
      "title_zh": "ç¥ç»æœºå™¨äººåŠ¨åŠ›å­¦",
      "authors": [
        "Jie Xu",
        "Eric Heiden",
        "Iretiayo Akinola",
        "Dieter Fox",
        "Miles Macklin",
        "Yashraj Narang"
      ],
      "abstract": "Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. NeRD uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned NeRD models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the NeRD simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æœºå™¨äººé«˜è‡ªç”±åº¦(High Degrees of Freedom)å’Œå¤æ‚æœºåˆ¶å¯¼è‡´çš„æ¨¡æ‹ŸæŒ‘æˆ˜ï¼Œæå‡ºäº†NeRD (Neural Robot Dynamics) æ¡†æ¶ã€‚NeRD æ˜¯ä¸€ç§ä¸ºå…³èŠ‚åˆšä½“(Articulated Rigid Bodies)è®¾è®¡çš„æœºå™¨äººä¸“ç”¨åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç¥ç»æ¨¡æ‹Ÿå™¨åœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒé—´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„æ ¸å¿ƒé—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æœºå™¨äººä¸­å¿ƒ(Robot-Centric)ä¸”ç©ºé—´ä¸å˜(Spatially-Invariant)çš„çŠ¶æ€è¡¨ç¤ºï¼ŒæˆåŠŸæ›¿ä»£äº†ä¼ ç»Ÿåˆ†ææ¨¡æ‹Ÿå™¨ä¸­çš„ä½çº§åŠ¨åŠ›å­¦ä¸æ¥è§¦æ±‚è§£å™¨(Contact Solvers)ã€‚ç ”ç©¶å°†NeRD ä½œä¸ºå¯äº’æ¢çš„åç«¯æ±‚è§£å™¨é›†æˆåˆ°å…ˆè¿›æœºå™¨äººæ¨¡æ‹Ÿå™¨ä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨æ•°åƒä¸ªæ¨¡æ‹Ÿæ­¥é•¿å†…å‡èƒ½ä¿æŒæé«˜çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒNeRD ä¸ä»…æ”¯æŒç›´æ¥åœ¨ç¥ç»å¼•æ“ä¸­è¿›è¡Œç­–ç•¥å­¦ä¹ (Policy Learning)ï¼Œè¿˜èƒ½é€šè¿‡çœŸå®æ•°æ®å¾®è°ƒæ¥æœ‰æ•ˆå¼¥åˆä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·(Sim-to-Real Gap)ï¼Œä¸ºå¤æ‚åŠ¨åŠ›å­¦ç³»ç»Ÿçš„ç²¾å‡†æ¨¡æ‹Ÿæä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15755v1",
      "published_date": "2025-08-21 17:54:41 UTC",
      "updated_date": "2025-08-21 17:54:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:28.993334+00:00"
    },
    {
      "arxiv_id": "2508.15754v1",
      "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis",
      "title_zh": "å‰–æå·¥å…·é›†æˆæ¨ç†ï¼šå®è¯ç ”ç©¶ä¸åˆ†æ",
      "authors": [
        "Yufeng Zhao",
        "Junnan Liu",
        "Hongwei Liu",
        "Dongsheng Zhu",
        "Yuan Shen",
        "Songyang Zhang",
        "Kai Chen"
      ],
      "abstract": "Large Language Models (LLMs) have made significant strides in reasoning tasks through methods like chain-of-thought (CoT) reasoning. However, they often fall short in tasks requiring precise computations. Tool-Integrated Reasoning (TIR) has emerged as a solution by incorporating external tools into the reasoning process. Nevertheless, the generalization of TIR in improving the reasoning ability of LLM is still unclear. Additionally, whether TIR has improved the model's reasoning behavior and helped the model think remains to be studied. We introduce ReasonZoo, a comprehensive benchmark encompassing nine diverse reasoning categories, to evaluate the effectiveness of TIR across various domains. Additionally, we propose two novel metrics, Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning efficiency. Our empirical evaluation demonstrates that TIR-enabled models consistently outperform their non-TIR counterparts in both mathematical and non-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as evidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more streamlined reasoning. These findings underscore the domain-general benefits of TIR and its potential to advance LLM capabilities in complex reasoning tasks.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç²¾ç¡®è®¡ç®—ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¯¹å·¥å…·é›†æˆæ¨ç† (Tool-Integrated Reasoning, TIR) è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ä¸åˆ†æã€‚ä¸ºäº†è¯„ä¼° TIR åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶è€…æ¨å‡ºäº†åŒ…å«ä¹ç±»æ¨ç†ä»»åŠ¡çš„ç»¼åˆåŸºå‡† ReasonZooã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†æ€§èƒ½æ„ŸçŸ¥æˆæœ¬ (Performance-Aware Cost, PAC) å’Œæ€§èƒ½-æˆæœ¬æ›²çº¿ä¸‹é¢ç§¯ (Area Under the Performance-Cost Curve, AUC-PCC) ä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼Œæ—¨åœ¨é‡åŒ–è¯„ä¼°æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¯æŒ TIR çš„æ¨¡å‹åœ¨æ•°å­¦åŠéæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ç ”ç©¶è¿˜å‘ç° TIR æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼Œé€šè¿‡æ”¹å–„ PAC å’Œ AUC-PCC æŒ‡æ ‡æœ‰æ•ˆå‡å°‘äº†æ¨¡å‹çš„è¿‡åº¦æ€è€ƒ (overthinking)ï¼Œä½¿æ¨ç†è¿‡ç¨‹æ›´åŠ ç²¾ç®€ã€‚è¿™äº›å‘ç°è¯å®äº† TIR çš„é¢†åŸŸé€šç”¨æ€§ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æå‡ LLMs å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, working in progress",
      "pdf_url": "https://arxiv.org/pdf/2508.15754v1",
      "published_date": "2025-08-21 17:50:24 UTC",
      "updated_date": "2025-08-21 17:50:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:37.697446+00:00"
    },
    {
      "arxiv_id": "2508.15752v1",
      "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries",
      "title_zh": "â€œå’–å•¡é¦†å…¥å£çœ‹èµ·æ¥æ–¹ä¾¿æ— éšœç¢é€šè¡Œå—ï¼Ÿé—¨åœ¨å“ªé‡Œï¼Ÿâ€ï¼šè¿ˆå‘é¢å‘è§†è§‰æŸ¥è¯¢çš„åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“",
      "authors": [
        "Jon E. Froehlich",
        "Jared Hwang",
        "Zeyu Wang",
        "John S. O'Meara",
        "Xia Su",
        "William Huang",
        "Yang Zhang",
        "Alex Fiannaca",
        "Philip Nelson",
        "Shaun Kane"
      ],
      "abstract": "Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°æœ‰äº¤äº’å¼æ•°å­—åœ°å›¾åœ¨å¤„ç†åœ°ç†è§†è§‰é—®é¢˜ï¼ˆGeo-visual questionsï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºå…¶è¿‡åº¦ä¾èµ– GIS æ•°æ®åº“ä¸­çš„é¢„å­˜ç»“æ„åŒ–æ•°æ®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åœ°ç†è§†è§‰æ™ºèƒ½ä½“ï¼ˆGeo-Visual Agentsï¼‰çš„æ„¿æ™¯ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿé€šè¿‡åˆ†æå¤§è§„æ¨¡åœ°ç†ç©ºé—´å›¾åƒåº“æ¥ç†è§£å¹¶å›ç­”ç»†å¾®è§†è§‰ç©ºé—´æŸ¥è¯¢çš„å¤šæ¨¡æ€ AI æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“æ•´åˆäº†åŒ…æ‹¬è¡—æ™¯ï¼ˆStreet Viewï¼‰ã€åœ°ç‚¹ç…§ç‰‡ï¼ˆPlace-based photosï¼‰å’Œèˆªç©ºå½±åƒï¼ˆAerial imageryï¼‰åœ¨å†…çš„å¤šæºè§†è§‰æ•°æ®ï¼Œå¹¶å°†å…¶ä¸ä¼ ç»Ÿçš„ GIS æ•°æ®æºç›¸ç»“åˆã€‚æœ¬æ–‡å®šä¹‰äº†è¯¥é¢†åŸŸçš„æ„¿æ™¯ï¼Œè¯¦ç»†æè¿°äº†æ„ŸçŸ¥ä¸äº¤äº’æ–¹æ³•ï¼Œå¹¶æä¾›äº†ä¸‰ä¸ªèŒƒä¾‹ï¼ˆExemplarsï¼‰æ¥å±•ç¤ºå…¶åº”ç”¨ã€‚æœ€åï¼Œç ”ç©¶åˆ—ä¸¾äº†æœªæ¥å·¥ä½œçš„å…³é”®æŒ‘æˆ˜ä¸æœºé‡ï¼Œä¸ºå¼€å‘å…·å¤‡è§†è§‰ç†è§£èƒ½åŠ›çš„åœ°ç†ç©ºé—´ AI å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to the ICCV'25 Workshop \"Vision Foundation Models and Generative AI for Accessibility: Challenges and Opportunities\"",
      "pdf_url": "https://arxiv.org/pdf/2508.15752v1",
      "published_date": "2025-08-21 17:49:52 UTC",
      "updated_date": "2025-08-21 17:49:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:34.882665+00:00"
    },
    {
      "arxiv_id": "2508.15748v5",
      "title": "AI Chaperones Are (Really) All You Need to Prevent Parasocial Relationships with Chatbots",
      "title_zh": "AI ç›‘æŠ¤ï¼š(çœŸçš„)è¶³ä»¥é¢„é˜²ä¸èŠå¤©æœºå™¨äººçš„å‡†ç¤¾ä¼šå…³ç³»",
      "authors": [
        "Emma Rath",
        "Stuart Armstrong",
        "Rebecca Gorman"
      ],
      "abstract": "Emerging reports of the harms caused to children and adults by AI sycophancy and by parasocial ties with chatbots point to an urgent need for safeguards against such risks. Yet, preventing such dynamics is challenging: parasocial cues often emerge gradually in private conversations between chatbots and users, and we lack effective methods to mitigate these risks. We address this challenge by introducing a simple response evaluation framework (an AI chaperone agent) created by repurposing a state-of-the-art language model to evaluate ongoing conversations for parasocial cues. We constructed a small synthetic dataset of thirty dialogues spanning parasocial, sycophantic, and neutral conversations. Iterative evaluation with five-stage testing successfully identified all parasocial conversations while avoiding false positives under a unanimity rule, with detection typically occurring within the first few exchanges. These findings provide preliminary evidence that AI chaperones can be a viable solution for reducing the risk of parasocial relationships.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èŠå¤©æœºå™¨äººå¸¦æ¥çš„ AI sycophancyï¼ˆAI è°„åªšï¼‰å’Œ parasocial relationshipsï¼ˆå‡†ç¤¾ä¼šå…³ç³»ï¼‰å¯¹ç”¨æˆ·é€ æˆçš„æ½œåœ¨å±å®³ï¼ŒæŒ‡å‡ºç›®å‰äºŸéœ€æœ‰æ•ˆçš„é˜²å¾¡æœºåˆ¶æ¥è¯†åˆ«ç§å¯†å¯¹è¯ä¸­é€æ¸äº§ç”Ÿçš„å‡†ç¤¾ä¼šçº¿ç´¢ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º AI chaperone çš„å“åº”è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹æ¥å®æ—¶ç›‘æµ‹å¹¶è¯„ä¼°å¯¹è¯ä¸­çš„ parasocial cuesã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å« 30 ä¸ªå¯¹è¯çš„å°å‹åˆæˆæ•°æ®é›†ï¼Œæ¶µç›–äº† parasocialã€sycophantic ä»¥åŠä¸­æ€§å¯¹è¯åœºæ™¯ã€‚é€šè¿‡äº”é˜¶æ®µçš„è¿­ä»£æµ‹è¯•å’Œä¸€è‡´æ€§åˆ¤å®šè§„åˆ™ï¼ˆunanimity ruleï¼‰ï¼Œè¯¥æ¡†æ¶æˆåŠŸè¯†åˆ«äº†æ‰€æœ‰å‡†ç¤¾ä¼šæ€§è´¨çš„å¯¹è¯ï¼Œä¸”æœªå‡ºç°è¯¯æŠ¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ£€æµ‹é€šå¸¸å‘ç”Ÿåœ¨å¯¹è¯çš„å‰å‡ æ¬¡äº¤æµä¸­ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„åŠæ—¶æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨ AI chaperone å‡å°‘å‡†ç¤¾ä¼šå…³ç³»é£é™©ã€æå‡äººæœºäº¤äº’å®‰å…¨æ€§æä¾›äº†åˆæ­¥ä¸”æœ‰æ•ˆçš„å¯è¡Œæ€§è¯æ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15748v5",
      "published_date": "2025-08-21 17:43:24 UTC",
      "updated_date": "2025-09-02 16:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:42.887787+00:00"
    },
    {
      "arxiv_id": "2508.15746v1",
      "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning",
      "title_zh": "é¢å‘å¯è¿½æº¯è¯Šæ–­æ¨ç†çš„ç«¯åˆ°ç«¯æ™ºèƒ½ä½“åŒ– RAG ç³»ç»Ÿè®­ç»ƒ",
      "authors": [
        "Qiaoyu Zheng",
        "Yuze Sun",
        "Chaoyi Wu",
        "Weike Zhao",
        "Pengcheng Qiu",
        "Yongguo Yu",
        "Kun Sun",
        "Yanfeng Wang",
        "Ya Zhang",
        "Weidi Xie"
      ],
      "abstract": "Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Deep-DxSearchï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆ(Agentic RAG)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹åœ¨è¯Šæ–­ä¸­é¢ä¸´çš„çŸ¥è¯†ç¼ºå£ã€å¹»è§‰ä»¥åŠæ¨ç†å¯è¿½æº¯æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿå°†å¤§è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºæ ¸å¿ƒæ™ºèƒ½ä½“ï¼Œç»“åˆå¤§è§„æ¨¡åŒ»å­¦æ£€ç´¢è¯­æ–™åº“ï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œå‡†ç¡®æ€§å®šåˆ¶çš„å¥–åŠ±æœºåˆ¶ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒDeep-DxSearch åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸€è‡´ä¼˜äºæç¤ºå·¥ç¨‹(Prompt-Engineering)å’Œå…è®­ç»ƒçš„ RAG æ–¹æ³•ï¼Œå¹¶åœ¨å¸¸è§ç—…ä¸ç½•è§ç—…çš„è¯Šæ–­å‡†ç¡®ç‡ä¸Šè¶…è¶Šäº† GPT-4oã€DeepSeek-R1 ç­‰å¼ºåŸºçº¿æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶ä¸æ¡ˆä¾‹åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¯è§£é‡Šæ€§ä¸è¯Šæ–­ç­–ç•¥ä¸Šçš„æ”¹è¿›ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›äº†æ›´å…·é€æ˜åº¦å’Œç²¾ç¡®æ€§çš„åˆæ­¥è¯Šæ–­è¾…åŠ©å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.15746v1",
      "published_date": "2025-08-21 17:42:47 UTC",
      "updated_date": "2025-08-21 17:42:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:47.253064+00:00"
    },
    {
      "arxiv_id": "2508.15734v1",
      "title": "Measuring the environmental impact of delivering AI at Google Scale",
      "title_zh": "è¡¡é‡ Google è§„æ¨¡ä¸‹ AI äº¤ä»˜çš„ç¯å¢ƒå½±å“",
      "authors": [
        "Cooper Elsworth",
        "Keguo Huang",
        "David Patterson",
        "Ian Schneider",
        "Robert Sedivy",
        "Savannah Goodman",
        "Ben Townsend",
        "Parthasarathy Ranganathan",
        "Jeff Dean",
        "Amin Vahdat",
        "Ben Gomes",
        "James Manyika"
      ],
      "abstract": "The transformative power of AI is undeniable - but as user adoption accelerates, so does the need to understand and mitigate the environmental impact of AI serving. However, no studies have measured AI serving environmental metrics in a production environment. This paper addresses this gap by proposing and executing a comprehensive methodology for measuring the energy usage, carbon emissions, and water consumption of AI inference workloads in a large-scale, AI production environment. Our approach accounts for the full stack of AI serving infrastructure - including active AI accelerator power, host system energy, idle machine capacity, and data center energy overhead. Through detailed instrumentation of Google's AI infrastructure for serving the Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24 Wh of energy - a figure substantially lower than many public estimates. We also show that Google's software efficiency efforts and clean energy procurement have driven a 33x reduction in energy consumption and a 44x reduction in carbon footprint for the median Gemini Apps text prompt over one year. We identify that the median Gemini Apps text prompt uses less energy than watching nine seconds of television (0.24 Wh) and consumes the equivalent of five drops of water (0.26 mL). While these impacts are low compared to other daily activities, reducing the environmental impact of AI serving continues to warrant important attention. Towards this objective, we propose that a comprehensive measurement of AI serving environmental metrics is critical for accurately comparing models, and to properly incentivize efficiency gains across the full AI serving stack.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è¡¡é‡å¤§è§„æ¨¡AIç”Ÿäº§ç¯å¢ƒä¸‹AIæ¨ç†(AI inference)å·¥ä½œè´Ÿè½½èƒ½è€—ã€ç¢³æ’æ”¾å’Œæ°´èµ„æºæ¶ˆè€—çš„å…¨é¢æ–¹æ³•è®ºï¼Œå¡«è¡¥äº†ç”Ÿäº§ç¯å¢ƒä¸‹AIæœåŠ¡ç¯å¢ƒæŒ‡æ ‡è¡¡é‡çš„ç©ºç™½ã€‚é€šè¿‡å¯¹GoogleæœåŠ¡GeminiåŠ©æ‰‹çš„AIåŸºç¡€è®¾æ–½è¿›è¡Œè¯¦ç»†ç›‘æµ‹ï¼Œç ”ç©¶å‘ç°Gemini Appsçš„ä¸­ä½æ–‡æœ¬æç¤ºèƒ½è€—ä»…ä¸º0.24 Whï¼Œè¿œä½äºè®¸å¤šå…¬ä¼—ä¼°ç®—ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¾—ç›Šäºè½¯ä»¶æ•ˆç‡(software efficiency)ä¼˜åŒ–å’Œæ¸…æ´èƒ½æºé‡‡è´­ï¼Œä¸€å¹´å†…ä¸­ä½æç¤ºè¯çš„èƒ½è€—å’Œç¢³è¶³è¿¹åˆ†åˆ«å®ç°äº†33å€å’Œ44å€çš„æ˜¾è‘—é™ä½ã€‚æ•°æ®è¡¨æ˜ï¼Œå•æ¬¡Gemini Appsæ–‡æœ¬æç¤ºè¯çš„èƒ½è€—ä½äºè§‚çœ‹9ç§’ç”µè§†ï¼Œè€—æ°´é‡ä»…çº¦0.26 mLï¼ˆç›¸å½“äº5æ»´æ°´ï¼‰ã€‚è™½ç„¶å•æ¬¡è¯·æ±‚çš„ç¯å¢ƒå½±å“è¾ƒä½ï¼Œä½†ç ”ç©¶å¼ºè°ƒæŒç»­é™ä½AIæœåŠ¡çš„ç¯å¢ƒè´Ÿæ‹…ä»éœ€é«˜åº¦å…³æ³¨ã€‚è¯¥ç ”ç©¶æœ€åæå‡ºï¼Œå…¨é¢çš„ç¯å¢ƒæŒ‡æ ‡è¡¡é‡å¯¹äºå‡†ç¡®æ¯”è¾ƒä¸åŒæ¨¡å‹ä»¥åŠæ¿€åŠ±æ•´ä¸ªAIæœåŠ¡æ ˆ(full AI serving stack)çš„æ•ˆç‡å¢ç›Šè‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15734v1",
      "published_date": "2025-08-21 17:22:06 UTC",
      "updated_date": "2025-08-21 17:22:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:51.957532+00:00"
    },
    {
      "arxiv_id": "2508.16685v1",
      "title": "STGAtt: A Spatial-Temporal Unified Graph Attention Network for Traffic Flow Forecasting",
      "title_zh": "STGAttï¼šç”¨äºäº¤é€šæµé¢„æµ‹çš„æ—¶ç©ºç»Ÿä¸€å›¾æ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Zhuding Liang",
        "Jianxun Cui",
        "Qingshuang Zeng",
        "Feng Liu",
        "Nenad Filipovic",
        "Tijana Geroski"
      ],
      "abstract": "Accurate and timely traffic flow forecasting is crucial for intelligent transportation systems. This paper presents a novel deep learning model, the Spatial-Temporal Unified Graph Attention Network (STGAtt). By leveraging a unified graph representation and an attention mechanism, STGAtt effectively captures complex spatial-temporal dependencies. Unlike methods relying on separate spatial and temporal dependency modeling modules, STGAtt directly models correlations within a Spatial-Temporal Unified Graph, dynamically weighing connections across both dimensions. To further enhance its capabilities, STGAtt partitions traffic flow observation signal into neighborhood subsets and employs a novel exchanging mechanism, enabling effective capture of both short-range and long-range correlations. Extensive experiments on the PEMS-BAY and SHMetro datasets demonstrate STGAtt's superior performance compared to state-of-the-art baselines across various prediction horizons. Visualization of attention weights confirms STGAtt's ability to adapt to dynamic traffic patterns and capture long-range dependencies, highlighting its potential for real-world traffic flow forecasting applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STGAttï¼Œä¸€ç§ç”¨äºäº¤é€šæµé¢„æµ‹çš„ Spatial-Temporal Unified Graph Attention Networkã€‚STGAtt åˆ©ç”¨ç»Ÿä¸€å›¾è¡¨ç¤ºå’Œæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å¤æ‚çš„æ—¶ç©ºä¾èµ–ï¼Œä¸åŒäºä¾èµ–ç‹¬ç«‹ç©ºé—´å’Œæ—¶é—´å»ºæ¨¡æ¨¡å—çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œå®ƒç›´æ¥åœ¨ Spatial-Temporal Unified Graph ä¸­å»ºæ¨¡å¹¶åŠ¨æ€æƒè¡¡è·¨ç»´åº¦è¿æ¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ€§èƒ½ï¼ŒSTGAtt å°†äº¤é€šæµè§‚æµ‹ä¿¡å·åˆ’åˆ†ä¸ºé‚»åŸŸå­é›†ï¼Œå¹¶é‡‡ç”¨ä¸€ç§æ–°é¢–çš„äº¤æ¢æœºåˆ¶ (exchanging mechanism) ä»¥æœ‰æ•ˆæ•æ‰çŸ­ç¨‹å’Œé•¿ç¨‹çš„ç›¸å…³æ€§ã€‚åœ¨ PEMS-BAY å’Œ SHMetro æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒSTGAtt åœ¨å„ç§é¢„æµ‹å‘¨æœŸå†…å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰ baseline çš„æ€§èƒ½ã€‚æ³¨æ„åŠ›æƒé‡çš„å¯è§†åŒ–è¯å®äº†è¯¥æ¨¡å‹å…·å¤‡é€‚åº”åŠ¨æ€äº¤é€šæ¨¡å¼åŠæ•æ‰é•¿ç¨‹ä¾èµ–çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…äº¤é€šæµé¢„æµ‹åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16685v1",
      "published_date": "2025-08-21 17:21:14 UTC",
      "updated_date": "2025-08-21 17:21:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:03.092993+00:00"
    },
    {
      "arxiv_id": "2508.15724v1",
      "title": "Numerical models outperform AI weather forecasts of record-breaking extremes",
      "title_zh": "æ•°å€¼æ¨¡å‹åœ¨ç ´çºªå½•æç«¯å¤©æ°”é¢„æŠ¥ä¸­è¡¨ç°ä¼˜äºäººå·¥æ™ºèƒ½",
      "authors": [
        "Zhongwei Zhang",
        "Erich Fischer",
        "Jakob Zscheischler",
        "Sebastian Engelke"
      ],
      "abstract": "Artificial intelligence (AI)-based models are revolutionizing weather forecasting and have surpassed leading numerical weather prediction systems on various benchmark tasks. However, their ability to extrapolate and reliably forecast unprecedented extreme events remains unclear. Here, we show that for record-breaking weather extremes, the numerical model High RESolution forecast (HRES) from the European Centre for Medium-Range Weather Forecasts still consistently outperforms state-of-the-art AI models GraphCast, GraphCast operational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate that forecast errors in AI models are consistently larger for record-breaking heat, cold, and wind than in HRES across nearly all lead times. We further find that the examined AI models tend to underestimate both the frequency and intensity of record-breaking events, and they underpredict hot records and overestimate cold records with growing errors for larger record exceedance. Our findings underscore the current limitations of AI weather models in extrapolating beyond their training domain and in forecasting the potentially most impactful record-breaking weather events that are particularly frequent in a rapidly warming climate. Further rigorous verification and model development is needed before these models can be solely relied upon for high-stakes applications such as early warning systems and disaster management.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†äººå·¥æ™ºèƒ½(AI)æ°”è±¡æ¨¡å‹åœ¨é¢„æµ‹ç ´çºªå½•æç«¯å¤©æ°”äº‹ä»¶(record-breaking extremes)æ–¹é¢çš„å¯é æ€§ï¼Œå¹¶ä¸ä¼ ç»Ÿæ•°å€¼å¤©æ°”é¢„æŠ¥ç³»ç»Ÿè¿›è¡Œäº†æ·±å…¥å¯¹æ¯”ã€‚ç ”ç©¶å‘ç°ï¼Œæ¬§æ´²ä¸­æœŸå¤©æ°”é¢„æŠ¥ä¸­å¿ƒçš„é«˜åˆ†è¾¨ç‡é¢„æŠ¥æ¨¡å‹(HRES)åœ¨é¢„æµ‹æ­¤ç±»æç«¯äº‹ä»¶æ—¶ï¼Œå…¶è¡¨ç°æŒç»­ä¼˜äºGraphCastã€Pangu-Weatherå’ŒFuxiç­‰é¡¶å°–AIæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIæ¨¡å‹åœ¨ç ´çºªå½•çš„é«˜æ¸©ã€ä½æ¸©å’Œå¼ºé£é¢„æµ‹ä¸­çš„è¯¯å·®åœ¨å‡ ä¹æ‰€æœ‰æå‰æœŸ(lead times)å†…éƒ½å¤§äºHRESï¼Œä¸”å­˜åœ¨ä½ä¼°æç«¯äº‹ä»¶é¢‘ç‡ä¸å¼ºåº¦çš„å€¾å‘ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ç°æœ‰AIæ¨¡å‹åœ¨è®­ç»ƒåŸŸ(training domain)ä¹‹å¤–è¿›è¡Œå¤–æ¨çš„å±€é™æ€§ï¼Œå°¤å…¶éš¾ä»¥å¯é åœ°é¢„æµ‹åœ¨å…¨çƒå˜æš–èƒŒæ™¯ä¸‹æ—¥ç›Šé¢‘ç¹ä¸”æå…·ç ´åæ€§çš„æç«¯å¤©æ°”ã€‚ä½œè€…æœ€åå¼ºè°ƒï¼Œåœ¨å°†AIæ¨¡å‹å®Œå…¨åº”ç”¨äºæ—©æœŸé¢„è­¦ç³»ç»Ÿ(early warning systems)å’Œç¾å®³ç®¡ç†ç­‰é«˜é£é™©é¢†åŸŸä¹‹å‰ï¼Œä»éœ€è¿›è¡Œæ›´ä¸ºä¸¥æ ¼çš„éªŒè¯ä¸æŒç»­çš„æ¨¡å‹å¼€å‘ã€‚",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15724v1",
      "published_date": "2025-08-21 17:07:16 UTC",
      "updated_date": "2025-08-21 17:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:54.988125+00:00"
    },
    {
      "arxiv_id": "2508.15721v2",
      "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-commerce Models",
      "title_zh": "EcomMMMUï¼šæ—¨åœ¨æ„å»ºé²æ£’å¤šæ¨¡æ€ç”µå•†æ¨¡å‹çš„è§†è§‰ä¿¡æ¯ç­–ç•¥æ€§åˆ©ç”¨",
      "authors": [
        "Xinyi Ling",
        "Hanwen Du",
        "Zhihui Zhu",
        "Xia Ning"
      ],
      "abstract": "E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://github.com/ninglab/EcomMMMU.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EcomMMMUï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 406,190 ä¸ªæ ·æœ¬å’Œ 8,989,510 å¼ å›¾åƒçš„å¤§è§„æ¨¡ç”µå­å•†åŠ¡å¤šæ¨¡æ€å¤šä»»åŠ¡ç†è§£æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›– 8 é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå¹¶åŒ…å«ä¸€ä¸ªä¸“é—¨çš„ VSS å­é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) æœ‰æ•ˆåˆ©ç”¨è§†è§‰å†…å®¹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ EcomMMMU çš„åˆ†æå‘ç°ï¼Œå¢åŠ äº§å“å›¾åƒå¹¶ä¸æ€»èƒ½æå‡æ¨¡å‹æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼šäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œåæ˜ å‡ºå½“å‰ MLLMs åœ¨æˆ˜ç•¥æ€§åˆ©ç”¨ç”µå•†è§†è§‰å†…å®¹æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† SUMEI æ–¹æ³•ï¼Œé€šè¿‡åœ¨å¤„ç†ä¸‹æ¸¸ä»»åŠ¡å‰é¢„æµ‹è§†è§‰æ•ˆç”¨ (visual utilities) æ¥æˆ˜ç•¥æ€§ç­›é€‰å¹¶åˆ©ç”¨å¤šå¼ å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSUMEI åœ¨å¤„ç†å¤æ‚ç”µå•†åœºæ™¯æ—¶å…·æœ‰å‡ºè‰²çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€ç”µå•†æ¨¡å‹æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICJNLP-AACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15721v2",
      "published_date": "2025-08-21 17:01:12 UTC",
      "updated_date": "2025-11-12 19:15:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:07:59.364820+00:00"
    },
    {
      "arxiv_id": "2508.15719v1",
      "title": "Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI",
      "title_zh": "ä¼°è®¡ç†è®ºã€æœºå™¨å­¦ä¹ ä¸ç”Ÿæˆå¼ AI çš„æ¦‚ç‡ç»Ÿä¸€åŒ–æ•™ç¨‹",
      "authors": [
        "Mohammed Elmusrati"
      ],
      "abstract": "Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç»å…¸çš„ Estimation Theoryã€Statistical Inference ä¸ç°ä»£ Machine Learningï¼ˆåŒ…æ‹¬ Deep Learning å’Œ Large Language Modelsï¼‰è”ç³»èµ·æ¥ã€‚æ–‡ç« é€šè¿‡åˆ†æ Maximum Likelihood Estimation (MLE)ã€Bayesian Inference ä»¥åŠ Attention Mechanisms å¦‚ä½•å¤„ç†ä¸ç¡®å®šæ€§ï¼Œæ­ç¤ºäº†ä¼—å¤šäººå·¥æ™ºèƒ½æ–¹æ³•éƒ½æ ¹æ¤äºå…±åŒçš„æ¦‚ç‡åŸç†ã€‚ç ”ç©¶ç»“åˆ System Identificationã€Image Classification å’Œ Language Generation ç­‰åœºæ™¯ï¼Œå±•ç¤ºäº†å¤æ‚æ¨¡å‹å¦‚ä½•åŸºäºè¿™äº›åŸºç¡€ç†è®ºæ¥åº”å¯¹ Overfittingã€æ•°æ®ç¨€ç–æ€§å’Œå¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œè®ºè¯äº† MLEã€MAP Estimationã€Bayesian Classification ä»¥åŠ Deep Learning æœ¬è´¨ä¸Šæ˜¯å®ç°åŒä¸€ä¸ªç›®æ ‡çš„ä¸åŒä¾§é¢ï¼Œå³ä»å™ªå£°æˆ–æœ‰åçš„è§‚æµ‹ä¸­æ¨æ–­éšè—çš„åŸå› ã€‚æœ¬æ–‡ä½œä¸ºç†è®ºåˆæˆä¸å®è·µæŒ‡å—ï¼Œä¸ºç ”ç©¶äººå‘˜åœ¨ä¸æ–­å‘å±•çš„ Machine Learning é¢†åŸŸä¸­æä¾›äº†ç³»ç»Ÿæ€§çš„å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15719v1",
      "published_date": "2025-08-21 16:57:33 UTC",
      "updated_date": "2025-08-21 16:57:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:15.162381+00:00"
    },
    {
      "arxiv_id": "2508.15717v1",
      "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding",
      "title_zh": "StreamMemï¼šé¢å‘æµå¼è§†é¢‘ç†è§£çš„æŸ¥è¯¢æ— å…³ KV ç¼“å­˜å†…å­˜",
      "authors": [
        "Yanlai Yang",
        "Zhuokai Zhao",
        "Satya Narayan Shukla",
        "Aashu Singh",
        "Shlok Kumar Mishra",
        "Lizhu Zhang",
        "Mengye Ren"
      ],
      "abstract": "Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†StreamMemï¼Œä¸€ç§ç”¨äºæµå¼è§†é¢‘ç†è§£çš„æŸ¥è¯¢æ— å…³(query-agnostic) KV cacheå†…å­˜æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´çš„å†…å­˜å’Œè®¡ç®—å¼€é”€è¿‡å¤§é—®é¢˜ã€‚ä¸ç°æœ‰ä¾èµ–é¢„çŸ¥é—®é¢˜æˆ–å®Œæ•´ä¸Šä¸‹æ–‡çš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒStreamMemä»¥æµå¼æ–¹å¼ç¼–ç è§†é¢‘å¸§ï¼Œå¹¶åˆ©ç”¨è§†è§‰æ ‡è®°(visual tokens)ä¸é€šç”¨æŸ¥è¯¢æ ‡è®°(generic query tokens)ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°æ¥å‹ç¼©KV cacheã€‚é€šè¿‡ç»´æŒå›ºå®šå¤§å°çš„KVå†…å­˜ï¼Œè¯¥æ¡†æ¶ç¡®ä¿åœ¨å†…å­˜å—é™çš„é•¿è§†é¢‘åœºæ™¯ä¸­ä»èƒ½å®ç°é«˜æ•ˆçš„é—®ç­”(QA)æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£å’Œä¸¤ä¸ªæµå¼è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒStreamMemåœ¨æŸ¥è¯¢æ— å…³çš„KV cacheå‹ç¼©é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)æ°´å¹³ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯ä¸éœ€è¦é¢„çŸ¥é—®é¢˜çš„æŸ¥è¯¢ç›¸å…³(query-aware)å‹ç¼©æ–¹æ¡ˆç›¸åª²ç¾ï¼Œä¸ºé«˜æ•ˆçš„é•¿è§†é¢‘ç†è§£æä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15717v1",
      "published_date": "2025-08-21 16:56:29 UTC",
      "updated_date": "2025-08-21 16:56:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:21.953517+00:00"
    },
    {
      "arxiv_id": "2508.15716v2",
      "title": "Foundation Models for Cross-Domain EEG Analysis Application: A Survey",
      "title_zh": "è·¨é¢†åŸŸ EEG åˆ†æåº”ç”¨çš„åŸºç¡€æ¨¡å‹ç»¼è¿°",
      "authors": [
        "Hongqi Li",
        "Yitong Chen",
        "Yujuan Wang",
        "Weihang Ni",
        "Haodong Zhang"
      ],
      "abstract": "Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each category's research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨é¢†åŸŸ EEG åˆ†æä¸­çš„ Foundation Models è¿›è¡Œäº†é¦–ä¸ªå…¨é¢çš„ç»¼è¿°ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç ”ç©¶ç¢ç‰‡åŒ–ã€æ¶æ„ä¸ä¸€è‡´åŠç¼ºä¹ç³»ç»Ÿåˆ†ç±»çš„ç°çŠ¶ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§é¢å‘æ¨¡æ€çš„åˆ†ç±»æ³• (Modality-oriented taxonomy)ï¼Œå°†ç ”ç©¶è¿›å±•ç³»ç»Ÿåœ°åˆ’åˆ†ä¸ºåŸç”Ÿ EEG decodingã€EEG-textã€EEG-visionã€EEG-audio ä»¥åŠæ›´å¹¿æ³›çš„å¤šæ¨¡æ€æ¡†æ¶ã€‚ä½œè€…ä¸¥è°¨åœ°åˆ†æäº†å„ç±»åˆ«ä¸‹çš„ç ”ç©¶æ€è·¯ã€ç†è®ºåŸºç¡€ä¸æ¶æ„åˆ›æ–°ï¼Œå¹¶é‡ç‚¹æ¢è®¨äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ (Interpretability)ã€è·¨é¢†åŸŸæ³›åŒ– (Cross-domain generalization) ä»¥åŠåœ¨å®é™… EEG ç³»ç»Ÿä¸­çš„åº”ç”¨ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡æ•´åˆå¹¶ç»Ÿä¸€è¿™ä¸€é›¶æ•£çš„ç ”ç©¶é¢†åŸŸï¼Œè¯¥å·¥ä½œä¸ºæœªæ¥çš„æ–¹æ³•å­¦å¼€å‘æä¾›äº†é‡è¦çš„å‚è€ƒæ¡†æ¶ã€‚æ­¤é¡¹ç»¼è¿°ä¸ä»…æœ‰åŠ©äºç†è§£å½“å‰çš„æŠ€æœ¯å‰æ²¿ï¼Œä¹Ÿæå¤§åœ°ä¿ƒè¿›äº† EEG Foundation Models å‘å¯æ‰©å±•ã€å¯è§£é‡Šä¸”å¯åœ¨çº¿æ‰§è¡Œçš„è§£å†³æ–¹æ¡ˆè½¬åŒ–ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to IEEE Journals",
      "pdf_url": "https://arxiv.org/pdf/2508.15716v2",
      "published_date": "2025-08-21 16:56:28 UTC",
      "updated_date": "2025-08-22 08:07:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:46.991152+00:00"
    },
    {
      "arxiv_id": "2508.15883v2",
      "title": "Beyond Imaging: Vision Transformer Digital Twin Surrogates for 3D+T Biological Tissue Dynamics",
      "title_zh": "è¶…è¶Šæˆåƒï¼šé¢å‘ 3D+T ç”Ÿç‰©ç»„ç»‡åŠ¨åŠ›å­¦çš„ Vision Transformer æ•°å­—å­ªç”Ÿä»£ç†æ¨¡å‹",
      "authors": [
        "Kaan Berke Ugurlar",
        "JoaquÃ­n de NavascuÃ©s",
        "Michael Taynnan Barros"
      ],
      "abstract": "Understanding the dynamic organization and homeostasis of living tissues requires high-resolution, time-resolved imaging coupled with methods capable of extracting interpretable, predictive insights from complex datasets. Here, we present the Vision Transformer Digital Twin Surrogate Network (VT-DTSN), a deep learning framework for predictive modeling of 3D+T imaging data from biological tissue. By leveraging Vision Transformers pretrained with DINO (Self-Distillation with NO Labels) and employing a multi-view fusion strategy, VT-DTSN learns to reconstruct high-fidelity, time-resolved dynamics of a Drosophila midgut while preserving morphological and feature-level integrity across imaging depths. The model is trained with a composite loss prioritizing pixel-level accuracy, perceptual structure, and feature-space alignment, ensuring biologically meaningful outputs suitable for in silico experimentation and hypothesis testing. Evaluation across layers and biological replicates demonstrates VT-DTSN's robustness and consistency, achieving low error rates and high structural similarity while maintaining efficient inference through model optimization. This work establishes VT-DTSN as a feasible, high-fidelity surrogate for cross-timepoint reconstruction and for studying tissue dynamics, enabling computational exploration of cellular behaviors and homeostasis to complement time-resolved imaging studies in biological research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Vision Transformer Digital Twin Surrogate Network (VT-DTSN)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç”Ÿç‰©ç»„ç»‡ 3D+T æˆåƒæ•°æ®è¿›è¡Œé¢„æµ‹å»ºæ¨¡çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»è¿‡ DINO (Self-Distillation with NO Labels) é¢„è®­ç»ƒçš„ Vision Transformers å¹¶ç»“åˆå¤šè§†å›¾èåˆ (multi-view fusion) ç­–ç•¥ï¼Œå®ç°äº†å¯¹é»‘è…¹æœè‡ (Drosophila) ä¸­è‚ é«˜ä¿çœŸå»¶æ—¶åŠ¨æ€çš„é‡å»ºã€‚æ¨¡å‹é‡‡ç”¨æ¶µç›–åƒç´ çº§å‡†ç¡®åº¦ã€æ„ŸçŸ¥ç»“æ„åŠç‰¹å¾ç©ºé—´å¯¹é½çš„å¤åˆæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†ç»“æœå…·å¤‡ç”Ÿç‰©å­¦æ„ä¹‰ï¼Œé€‚ç”¨äºåŸä½å®éªŒ (in silico experimentation) å’Œå‡è®¾æ£€éªŒã€‚ç»è¯„ä¼°ï¼ŒVT-DTSN åœ¨ä¸åŒæˆåƒæ·±åº¦å’Œç”Ÿç‰©å‰¯æœ¬ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ï¼Œåœ¨ä¿æŒé«˜ç»“æ„ç›¸ä¼¼æ€§çš„åŒæ—¶å®ç°äº†é«˜æ•ˆæ¨ç†ã€‚è¯¥å·¥ä½œç¡®ç«‹äº† VT-DTSN ä½œä¸ºè·¨æ—¶é—´ç‚¹é‡å»ºå’Œç»„ç»‡åŠ¨åŠ›å­¦ç ”ç©¶çš„é«˜ä¿çœŸæ›¿ä»£æ¨¡å‹ (surrogate) çš„åœ°ä½ï¼Œä¸ºé€šè¿‡è®¡ç®—æ‰‹æ®µæ¢ç´¢ç»†èƒè¡Œä¸ºä¸ç¨³æ€ (homeostasis) æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG",
        "q-bio.TO"
      ],
      "primary_category": "eess.IV",
      "comment": "Submitted for journal publication",
      "pdf_url": "https://arxiv.org/pdf/2508.15883v2",
      "published_date": "2025-08-21 16:24:24 UTC",
      "updated_date": "2025-08-25 10:31:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:29.057097+00:00"
    },
    {
      "arxiv_id": "2508.15693v1",
      "title": "NiceWebRL: a Python library for human subject experiments with reinforcement learning environments",
      "title_zh": "NiceWebRLï¼šé¢å‘å¼ºåŒ–å­¦ä¹ ç¯å¢ƒäººç±»å—è¯•è€…å®éªŒçš„ Python åº“",
      "authors": [
        "Wilka Carvalho",
        "Vikram Goddla",
        "Ishaan Sinha",
        "Hoon Shin",
        "Kunal Jha"
      ],
      "abstract": "We present NiceWebRL, a research tool that enables researchers to use machine reinforcement learning (RL) environments for online human subject experiments. NiceWebRL is a Python library that allows any Jax-based environment to be transformed into an online interface, supporting both single-agent and multi-agent environments. As such, NiceWebRL enables AI researchers to compare their algorithms to human performance, cognitive scientists to test ML algorithms as theories for human cognition, and multi-agent researchers to develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3 case studies that demonstrate its potential to help develop Human-like AI, Human-compatible AI, and Human-assistive AI. In the first case study (Human-like AI), NiceWebRL enables the development of a novel RL model of cognition. Here, NiceWebRL facilitates testing this model against human participants in both a grid world and Craftax, a 2D Minecraft domain. In our second case study (Human-compatible AI), NiceWebRL enables the development of a novel multi-agent RL algorithm that can generalize to human partners in the Overcooked domain. Finally, in our third case study (Human-assistive AI), we show how NiceWebRL can allow researchers to study how an LLM can assist humans on complex tasks in XLand-Minigrid, an environment with millions of hierarchical tasks. The library is available at https://github.com/KempnerInstitute/nicewebrl.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† NiceWebRLï¼Œä¸€ä¸ªèƒ½å¤Ÿå°†å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ç¯å¢ƒè½¬åŒ–ä¸ºåœ¨çº¿äººç±»å—è¯•è€…å®éªŒç•Œé¢çš„ Python åº“ã€‚å®ƒåŸºäº Jax æ¶æ„ï¼Œæ”¯æŒå°†å•æ™ºèƒ½ä½“ (Single-agent) å’Œå¤šæ™ºèƒ½ä½“ (Multi-agent) ç¯å¢ƒè½¬åŒ–ä¸ºäº¤äº’å¼åœ¨çº¿ç•Œé¢ï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜ç›´æ¥å¯¹æ¯” AI ç®—æ³•ä¸äººç±»çš„è¡¨ç°ã€‚è¯¥åº“é€šè¿‡ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†å…¶åœ¨å¼€å‘ç±»äºº AI (Human-like AI)ã€äººç±»å…¼å®¹ AI (Human-compatible AI) å’Œäººç±»è¾…åŠ© AI (Human-assistive AI) æ–¹é¢çš„æ½œåŠ›ã€‚åœ¨å…·ä½“åº”ç”¨ä¸­ï¼ŒNiceWebRL æˆåŠŸæ”¯æŒäº†åœ¨ Grid world å’Œ Craftax é¢†åŸŸæµ‹è¯•è®¤çŸ¥æ¨¡å‹ï¼Œå¹¶åœ¨ Overcooked ä»»åŠ¡ä¸­ä¿ƒè¿›äº†å¯æ³›åŒ–äººæœºåä½œç®—æ³•çš„å¼€å‘ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è¢«ç”¨äºç ”ç©¶ LLM åœ¨å¤§è§„æ¨¡åˆ†å±‚ä»»åŠ¡ç¯å¢ƒ XLand-Minigrid ä¸­å¦‚ä½•è¾…åŠ©äººç±»å®Œæˆå¤æ‚ä»»åŠ¡ã€‚è¯¥å·¥å…·ä¸ºè®¤çŸ¥ç§‘å­¦å®¶å’Œ AI ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªçµæ´»çš„å®éªŒå¹³å°ï¼Œæ—¨åœ¨æ¨åŠ¨äººæœºåä½œä¸ç±»äººæ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15693v1",
      "published_date": "2025-08-21 16:18:49 UTC",
      "updated_date": "2025-08-21 16:18:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:30.648902+00:00"
    },
    {
      "arxiv_id": "2508.15690v4",
      "title": "GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning",
      "title_zh": "GRAFTï¼šé¢å‘æ–‡æœ¬å¯¹é½çš„å›¾è¡¨æ¨ç†â€”â€”ä¸€é¡¹é’ˆå¯¹ç»“æ„åŒ–æŒ‡ä»¤éµå¾ªä¸è§†è§‰æ¨ç†çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Abhigya Verma",
        "Sriram Puttagunta",
        "Seganrasan Subramanian",
        "Sravan Ramachandran"
      ],
      "abstract": "GRAFT is a structured multimodal benchmark designed to probe how well LLMs handle instruction following, visual reasoning, and tasks requiring tight visual textual alignment. The dataset is built around programmatically generated charts and synthetically rendered tables, each paired with a carefully constructed, multi step analytical question that depends solely on what can be inferred from the image itself. Responses are formatted in structured outputs such as JSON or YAML, enabling consistent and fine grained evaluation of both reasoning processes and adherence to output specifications. The benchmark further introduces a taxonomy of reasoning operations ranging from comparison and trend identification to ranking, aggregation, proportional estimation, and anomaly detection to support a comprehensive assessment of model capabilities. Taken together, GRAFT provides a unified and scalable framework for evaluating multimodal LLMs on visually grounded, structured reasoning tasks, offering a more rigorous standard for future benchmarking efforts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GRAFTï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æŒ‡ä»¤éµå¾ªã€è§†è§‰æ¨ç†ä»¥åŠè§†è§‰æ–‡æœ¬å¯¹é½(visual-textual alignment)ä»»åŠ¡ä¸­è¡¨ç°çš„ç»“æ„åŒ–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†ç”±ç¨‹åºç”Ÿæˆçš„å›¾è¡¨(charts)å’Œåˆæˆæ¸²æŸ“çš„è¡¨æ ¼(tables)ç»„æˆï¼Œå¹¶ä¸ºæ¯å¼ å›¾åƒé…å¯¹äº†ä¸€ä¸ªä»…ä¾èµ–äºå›¾åƒæ¨æ–­çš„ã€ç²¾å¿ƒè®¾è®¡çš„å¤šæ­¥éª¤åˆ†ææ€§é—®é¢˜ã€‚ä¸ºäº†å®ç°å¯¹æ¨ç†è¿‡ç¨‹å’Œè¾“å‡ºè§„èŒƒéµå¾ªèƒ½åŠ›çš„ç²¾ç»†åŒ–è¯„ä¼°ï¼Œæ¨¡å‹å“åº”é‡‡ç”¨JSONæˆ–YAMLç­‰ç»“æ„åŒ–æ ¼å¼è¿›è¡Œè¾“å‡ºã€‚è¯¥åŸºå‡†æµ‹è¯•è¿˜å¼•å…¥äº†ä¸€å¥—æ¨ç†æ“ä½œåˆ†ç±»ä½“ç³»(taxonomy)ï¼Œæ¶µç›–äº†æ¯”è¾ƒ(comparison)ã€è¶‹åŠ¿è¯†åˆ«(trend identification)ã€æ’åº(ranking)ã€èšåˆ(aggregation)ã€æ¯”ä¾‹ä¼°è®¡ä»¥åŠå¼‚å¸¸æ£€æµ‹(anomaly detection)ç­‰å¤šç§æ“ä½œã€‚æ€»ä½“è€Œè¨€ï¼ŒGRAFTä¸ºåœ¨è§†è§‰å¼•å¯¼çš„ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal LLMs)æä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„æ¡†æ¶ã€‚è¿™ä¸€å·¥ä½œä¸ºæœªæ¥çš„åŸºå‡†æµ‹è¯•è®¾å®šäº†æ›´ä¸ºä¸¥æ ¼çš„æ ‡å‡†ï¼Œæœ‰åŠ©äºæ·±å…¥è¯„ä¼°å¹¶æå‡æ¨¡å‹å¤„ç†å¤æ‚è§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 10 tables, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15690v4",
      "published_date": "2025-08-21 16:13:49 UTC",
      "updated_date": "2025-12-02 05:59:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:30.451542+00:00"
    },
    {
      "arxiv_id": "2508.15685v1",
      "title": "Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays",
      "title_zh": "ç”¨äº IMC é˜µåˆ—å®¹é”™å¤šæ¯”ç‰¹æƒé‡è¡¨ç¤ºçš„è¡Œåˆ—æ··åˆåˆ†ç»„",
      "authors": [
        "Kang Eun Jeon",
        "Sangheum Yeon",
        "Jinhee Kim",
        "Hyeonsu Bang",
        "Johnny Rhe",
        "Jong Hwan Ko"
      ],
      "abstract": "This paper addresses two critical challenges in analog In-Memory Computing (IMC) systems that limit their scalability and deployability: the computational unreliability caused by stuck-at faults (SAFs) and the high compilation overhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To overcome these limitations, we first propose a novel multi-bit weight representation technique, termed row-column hybrid grouping, which generalizes conventional column grouping by introducing redundancy across both rows and columns. This structural redundancy enhances fault tolerance and can be effectively combined with existing fault-mitigation solutions. Second, we design a compiler pipeline that reformulates the fault-aware weight decomposition problem as an Integer Linear Programming (ILP) task, enabling fast and scalable compilation through off-the-shelf solvers. Further acceleration is achieved through theoretical insights that identify fault patterns amenable to trivial solutions, significantly reducing computation. Experimental results on convolutional networks and small language models demonstrate the effectiveness of our approach, achieving up to 8%p improvement in accuracy, 150x faster compilation, and 2x energy efficiency gain compared to existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡æ‹Ÿå­˜å†…è®¡ç®—ï¼ˆIMCï¼‰ç³»ç»Ÿä¸­ç”±å›ºå®šæ•…éšœï¼ˆstuck-at faults, SAFsï¼‰å¯¼è‡´çš„è®¡ç®—ä¸å¯é æ€§ä»¥åŠç°æœ‰ç®—æ³•çš„é«˜ç¼–è¯‘å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºè¡Œåˆ—æ··åˆåˆ†ç»„ï¼ˆrow-column hybrid groupingï¼‰çš„æ–°å‹å¤šä½æƒé‡è¡¨ç¤ºæŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨è¡Œå’Œåˆ—ç»´åº¦åŒæ—¶å¼•å…¥å†—ä½™ï¼Œæ³›åŒ–äº†ä¼ ç»Ÿçš„åˆ—åˆ†ç»„æŠ€æœ¯ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç¡¬ä»¶çš„å®¹é”™èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªç¼–è¯‘å™¨æµæ°´çº¿ï¼Œå°†æ„ŸçŸ¥æ•…éšœçš„æƒé‡åˆ†è§£é—®é¢˜è½¬åŒ–ä¸ºæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨ç‰¹å®šæ•…éšœæ¨¡å¼çš„ç†è®ºæ´å¯ŸåŠ é€Ÿè®¡ç®—ï¼Œå®ç°äº†å¿«é€Ÿä¸”å¯æ‰©å±•çš„ç¼–è¯‘è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å·ç§¯ç½‘ç»œå’Œå°å‹è¯­è¨€æ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ¡ˆç›¸æ¯”ç°æœ‰åŸºçº¿å°†å‡†ç¡®ç‡æå‡äº†é«˜è¾¾8%ï¼Œç¼–è¯‘é€Ÿåº¦åŠ å¿«äº†150å€ï¼Œå¹¶å®ç°äº†2å€çš„èƒ½æ•ˆå¢ç›Šã€‚è¯¥æˆæœä¸ºæ„å»ºé«˜å¯é ã€æ˜“éƒ¨ç½²çš„å¤§è§„æ¨¡æ¨¡æ‹Ÿå­˜å†…è®¡ç®—ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted to appear at ICCAD'25 (Munich, Germany)",
      "pdf_url": "https://arxiv.org/pdf/2508.15685v1",
      "published_date": "2025-08-21 16:05:44 UTC",
      "updated_date": "2025-08-21 16:05:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:37.654544+00:00"
    },
    {
      "arxiv_id": "2508.15680v1",
      "title": "Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle",
      "title_zh": "ä½œä¸ºåŸºç¡€è®¾æ–½çš„æœªæ¥æ€§ï¼šäººå·¥æ™ºèƒ½ç”Ÿå‘½å‘¨æœŸçš„æŠ€æœ¯å“²å­¦è¯ é‡Š",
      "authors": [
        "Mark Cote",
        "Susana Aires"
      ],
      "abstract": "This paper argues that a techno-philosophical reading of the EU AI Act provides insight into the long-term dynamics of data in AI systems, specifically, how the lifecycle from ingestion to deployment generates recursive value chains that challenge existing frameworks for Responsible AI. We introduce a conceptual tool to frame the AI pipeline, spanning data, training regimes, architectures, feature stores, and transfer learning. Using cross-disciplinary methods, we develop a technically grounded and philosophically coherent analysis of regulatory blind spots. Our central claim is that what remains absent from policymaking is an account of the dynamic of becoming that underpins both the technical operation and economic logic of AI. To address this, we advance a formal reading of AI inspired by Simondonian philosophy of technology, reworking his concept of individuation to model the AI lifecycle, including the pre-individual milieu, individuation, and individuated AI. To translate these ideas, we introduce futurity: the self-reinforcing lifecycle of AI, where more data enhances performance, deepens personalisation, and expands application domains. Futurity highlights the recursively generative, non-rivalrous nature of data, underpinned by infrastructures like feature stores that enable feedback, adaptation, and temporal recursion. Our intervention foregrounds escalating power asymmetries, particularly the tech oligarchy whose infrastructures of capture, training, and deployment concentrate value and decision-making. We argue that effective regulation must address these infrastructural and temporal dynamics, and propose measures including lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»æŠ€æœ¯å“²å­¦(Techno-philosophical)è§†è§’å‡ºå‘ï¼Œç»“åˆæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆ(EU AI Act)ï¼Œæ·±å…¥æ¢è®¨äº†AIç”Ÿå‘½å‘¨æœŸä¸­ä»æ•°æ®æ‘„å–åˆ°éƒ¨ç½²æ‰€å½¢æˆçš„é€’å½’ä»·å€¼é“¾åŠå…¶å¯¹è´Ÿè´£ä»»äººå·¥æ™ºèƒ½(Responsible AI)æ¡†æ¶å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è®ºæ–‡å¼•å…¥äº†ä¸€ç§è·¨å­¦ç§‘çš„åˆ†æå·¥å…·ï¼Œå€Ÿé‰´è¥¿è’™ä¸œ(Simondonian)çš„æŠ€æœ¯å“²å­¦æ€æƒ³ï¼Œé€šè¿‡ä¸ªä½“åŒ–(Individuation)æ¦‚å¿µé‡æ„äº†AIç”Ÿå‘½å‘¨æœŸæ¨¡å‹ï¼Œæ¶µç›–å‰ä¸ªä½“åŒ–ç¯å¢ƒ(Pre-individual milieu)ã€ä¸ªä½“åŒ–è¿‡ç¨‹åŠä¸ªä½“åŒ–AIã€‚ç ”ç©¶æ ¸å¿ƒæå‡ºäº†â€œæœªæ¥æ€§â€(Futurity)æ¦‚å¿µï¼Œç”¨ä»¥æè¿°AIé€šè¿‡æ›´å¤šæ•°æ®æå‡æ€§èƒ½ã€åŠ æ·±ä¸ªæ€§åŒ–å¹¶æ‰©å±•åº”ç”¨é¢†åŸŸçš„è‡ªæˆ‘å¢å¼ºç”Ÿå‘½å‘¨æœŸï¼Œå¹¶å¼ºè°ƒäº†ç‰¹å¾å­˜å‚¨(Feature stores)ç­‰åŸºç¡€è®¾æ–½åœ¨æ—¶é—´é€’å½’å’Œåé¦ˆé€‚é…ä¸­çš„æ”¯æ’‘ä½œç”¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œè¿™ç§é€’å½’ç”Ÿæˆå’Œéç«äº‰æ€§çš„æ•°æ®æœ¬è´¨åŠ å‰§äº†æŠ€æœ¯å¯¡å¤´åœ¨åŸºç¡€è®¾æ–½ä¸Šçš„æƒåŠ›ä¸å¯¹ç§°ï¼Œå¯¼è‡´ç›‘ç®¡å‡ºç°ç›²ç‚¹ã€‚ä¸ºåº”å¯¹è¿™äº›æ—¶é—´åŠ¨æ€å’ŒåŸºç¡€è®¾æ–½å¸¦æ¥çš„é£é™©ï¼Œè®ºæ–‡æè®®å®æ–½ç”Ÿå‘½å‘¨æœŸå®¡è®¡ã€æ—¶é—´è¿½æº¯æ€§(Temporal traceability)ã€åé¦ˆé—®è´£æœºåˆ¶ä»¥åŠé€’å½’é€æ˜åº¦ï¼Œå¹¶ä¸»å¼ èµ‹äºˆç”¨æˆ·è´¨ç–‘é€’å½’å†åˆ©ç”¨(Recursive reuse)çš„æƒåˆ©ã€‚è¯¥ç ”ç©¶é€šè¿‡æ­ç¤ºAIæŠ€æœ¯è¿è¡Œä¸ç»æµé€»è¾‘åº•å±‚çš„ç”ŸæˆåŠ¨æ€ï¼Œä¸ºå»ºç«‹æœ‰æ•ˆçš„AIç›‘ç®¡æ¡†æ¶æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI Landscape after the AI Act, 4th International Workshop on Imagining the AI Landscape After the AI Act, The fourth International Conference on Hybrid Human-Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2508.15680v1",
      "published_date": "2025-08-21 16:00:13 UTC",
      "updated_date": "2025-08-21 16:00:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:47.993644+00:00"
    },
    {
      "arxiv_id": "2508.15663v1",
      "title": "Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation",
      "title_zh": "æ€è¡Œåˆä¸€ï¼šé¢å‘ç§»åŠ¨æ“ä½œä»»åŠ¡è§„åˆ’ä¸åº•å±‚ç­–ç•¥è”åˆè¯„ä¼°çš„ IsaacSim åŸºå‡†",
      "authors": [
        "Nikita Kachaev",
        "Andrei Spiridonov",
        "Andrey Gorodetsky",
        "Kirill Muravyev",
        "Nikita Oskolkov",
        "Aditya Narendra",
        "Vlad Shakhuro",
        "Dmitry Makarov",
        "Aleksandr I. Panov",
        "Polina Fedotova",
        "Alexey K. Kovalev"
      ],
      "abstract": "Benchmarks are crucial for evaluating progress in robotics and embodied AI. However, a significant gap exists between benchmarks designed for high-level language instruction following, which often assume perfect low-level execution, and those for low-level robot control, which rely on simple, one-step commands. This disconnect prevents a comprehensive evaluation of integrated systems where both task planning and physical execution are critical. To address this, we propose Kitchen-R, a novel benchmark that unifies the evaluation of task planning and low-level control within a simulated kitchen environment. Built as a digital twin using the Isaac Sim simulator and featuring more than 500 complex language instructions, Kitchen-R supports a mobile manipulator robot. We provide baseline methods for our benchmark, including a task-planning strategy based on a vision-language model and a low-level control policy based on diffusion policy. We also provide a trajectory collection system. Our benchmark offers a flexible framework for three evaluation modes: independent assessment of the planning module, independent assessment of the control policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R bridges a key gap in embodied AI research, enabling more holistic and realistic benchmarking of language-guided robotic agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå’Œå…·èº«æ™ºèƒ½ (Embodied AI) é¢†åŸŸä¸­é«˜å±‚ä»»åŠ¡è§„åˆ’ä¸åº•å±‚ç‰©ç†æ‰§è¡Œè¯„ä¼°è„±èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº† Kitchen-R åŸºå‡†ã€‚Kitchen-R æ˜¯åœ¨ Isaac Sim ä»¿çœŸå™¨ä¸­æ„å»ºçš„ä¸€ä¸ªæ•°å­—å­ªç”Ÿå¨æˆ¿ç¯å¢ƒï¼Œæ”¯æŒç§»åŠ¨æ“ä½œæœºå™¨äººï¼Œå¹¶åŒ…å«è¶…è¿‡ 500 æ¡å¤æ‚çš„è¯­è¨€æŒ‡ä»¤ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model) çš„ä»»åŠ¡è§„åˆ’ç­–ç•¥å’ŒåŸºäºæ‰©æ•£ç­–ç•¥ (Diffusion Policy) çš„åº•å±‚æ§åˆ¶æ¨¡å‹ï¼Œå¹¶é…åˆè½¨è¿¹é‡‡é›†ç³»ç»Ÿï¼Œå®ç°äº†å¯¹ç³»ç»Ÿæ€§èƒ½çš„æ·±åº¦åŸºå‡†æµ‹è¯•ã€‚Kitchen-R æä¾›äº†ä¸‰ç§è¯„ä¼°æ¨¡å¼ï¼Œæ”¯æŒå¯¹è§„åˆ’æ¨¡å—ã€æ§åˆ¶ç­–ç•¥çš„ç‹¬ç«‹è¯„ä¼°ä»¥åŠæ•´ä¸ªé›†æˆç³»ç»Ÿçš„è”åˆè¯„ä»·ã€‚è¯¥æˆæœæœ‰æ•ˆæ¡¥æ¥äº†å…·èº«æ™ºèƒ½ç ”ç©¶çš„å…³é”®å·®è·ï¼Œä¸ºè¯­è¨€å¼•å¯¼å‹æœºå™¨äººæ™ºèƒ½ä½“çš„æ•´ä½“æ€§ä¸ç°å®æ€§è¯„ä¼°æä¾›äº†çµæ´»ä¸”å…¨é¢çš„è¯„ä»·ä½“ç³»ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15663v1",
      "published_date": "2025-08-21 15:48:51 UTC",
      "updated_date": "2025-08-21 15:48:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:53.687895+00:00"
    },
    {
      "arxiv_id": "2508.15658v4",
      "title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
      "title_zh": "SurGEï¼šç§‘å­¦ç»¼è¿°ç”ŸæˆåŸºå‡†ä¸è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Weihang Su",
        "Anzhe Xie",
        "Qingyao Ai",
        "Jianming Long",
        "Xuanyi Chen",
        "Jiaxin Mao",
        "Ziyi Ye",
        "Yiqun Liu"
      ],
      "abstract": "The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SurGE (Survey Generation Evaluation)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è®¡ç®—æœºç§‘å­¦é¢†åŸŸç§‘å­¦ç»¼è¿°ç”Ÿæˆçš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºå­¦æœ¯æ–‡çŒ®å¿«é€Ÿå¢é•¿å¯¼è‡´äººå·¥æ’°å†™ç»¼è¿°æ—¥ç›Šå›°éš¾çš„é—®é¢˜ã€‚SurGE åŒ…å«ä¸€å¥—å®Œæ•´çš„æµ‹è¯•å®ä¾‹ï¼ˆåŒ…æ‹¬è¯¾é¢˜æè¿°ã€ä¸“å®¶ç»¼è¿°åŠå…¶å‚è€ƒæ–‡çŒ®ï¼‰ä»¥åŠä¸€ä¸ªè§„æ¨¡è¶…è¿‡ä¸€ç™¾ä¸‡ç¯‡è®ºæ–‡çš„å¤§è§„æ¨¡å­¦æœ¯è¯­æ–™åº“ã€‚ç ”ç©¶è€…åŒæ—¶æå‡ºäº†ä¸€å¥—è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å…¨é¢æ€§(comprehensiveness)ã€å¼•ç”¨å‡†ç¡®æ€§(citation accuracy)ã€ç»“æ„ç»„ç»‡(structural organization)å’Œå†…å®¹è´¨é‡(content quality)å››ä¸ªç»´åº¦å¯¹ç”Ÿæˆçš„ç»¼è¿°è´¨é‡è¿›è¡Œè¡¡é‡ã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å¤šç§å¤§è¯­è¨€æ¨¡å‹(LLM)æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„æ™ºèƒ½ä½“æ¡†æ¶(agentic frameworks)åœ¨å¤„ç†ç»¼è¿°ç”Ÿæˆçš„å¤æ‚æ€§æ—¶ä¹Ÿè¡¨ç°æŒ£æ‰ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ç»¼è¿°ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶å¼€æºäº†å…¨éƒ¨ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15658v4",
      "published_date": "2025-08-21 15:45:10 UTC",
      "updated_date": "2026-01-18 08:42:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:08:52.884177+00:00"
    },
    {
      "arxiv_id": "2508.15652v2",
      "title": "Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning",
      "title_zh": "åŸºäºå·¥å…·æ€§èµ‹èƒ½çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åŠ¨ä½œæ•ˆåº”ç†è§£",
      "authors": [
        "Ardian Selmonaj",
        "Miroslav Strupl",
        "Oleg Szehr",
        "Alessandro Antonucci"
      ],
      "abstract": "To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors. While prior work typically evaluates overall team performance based on explicit reward signals, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision (un)certainty and preference alignment. By analyzing action effects on policies and value functions across cooperative and competitive MARL tasks, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices, while also revealing the extent to which agents adopt similar or diverse strategies. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç¼ºä¹æ˜¾å¼ä»·å€¼åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é€šè¿‡åˆ†æç­–ç•¥åˆ†å¸ƒæ¥æ¨æ–­å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Multi-Agent Reinforcement Learning, MARL)ä¸­å•ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºè´¡çŒ®ã€‚å—æ™ºèƒ½ä½“å€¾å‘äºè¿½æ±‚æ”¶æ•›å·¥å…·æ€§ä»·å€¼çš„å¯å‘ï¼Œä½œè€…æå‡ºäº†é¢„æœŸåˆä½œä»·å€¼(Intended Cooperation Values, ICVs)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¿¡æ¯è®ºShapley valuesçš„æ–¹æ³•ï¼Œç”¨äºé‡åŒ–æ™ºèƒ½ä½“å¯¹é˜Ÿå‹å·¥å…·æ€§æˆæƒ(Instrumental Empowerment)çš„å› æœå½±å“ã€‚ICVsé€šè¿‡è¯„ä¼°å†³ç­–çš„ä¸ç¡®å®šæ€§å’Œåå¥½å¯¹é½ç¨‹åº¦ï¼Œæµ‹é‡æ™ºèƒ½ä½“è¡Œä¸ºå¯¹é˜Ÿå‹ç­–ç•¥çš„å…·ä½“å½±å“ã€‚é€šè¿‡åœ¨åˆä½œå’Œç«äº‰æ€§MARLä»»åŠ¡ä¸­åˆ†æè¡Œä¸ºå¯¹ç­–ç•¥å’Œä»·å€¼å‡½æ•°çš„ä½œç”¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å“ªäº›è¡Œä¸ºæœ‰åŠ©äºå›¢é˜ŸæˆåŠŸï¼Œä¾‹å¦‚é€šè¿‡ä¿ƒè¿›ç¡®å®šæ€§å†³ç­–æˆ–ä¸ºæœªæ¥è¡ŒåŠ¨é€‰æ‹©ä¿ç•™çµæ´»æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…æ­ç¤ºäº†æ™ºèƒ½ä½“é‡‡ç”¨ç›¸ä¼¼æˆ–å¤šæ ·åŒ–ç­–ç•¥çš„ç¨‹åº¦ï¼Œè¿˜ä¸ºç†è§£åˆä½œåŠ¨æ€æä¾›äº†æ–°é¢–è§è§£å¹¶æ˜¾è‘—å¢å¼ºäº†MARLç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "European Conference on Artificial Intelligence (ECAI) 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15652v2",
      "published_date": "2025-08-21 15:35:59 UTC",
      "updated_date": "2025-08-23 17:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:13.793072+00:00"
    },
    {
      "arxiv_id": "2508.15650v1",
      "title": "Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance",
      "title_zh": "åŸºäºå…³é”®ç‰¹å¾å¼•å¯¼çš„ 3D è¿ç§»é»‘ç›’æ”»å‡»",
      "authors": [
        "Shuchao Pang",
        "Zhenghan Chen",
        "Shen Zhang",
        "Liming Lu",
        "Siyuan Liang",
        "Anan Du",
        "Yongbin Zhou"
      ],
      "abstract": "Deep neural networks for 3D point clouds have been demonstrated to be vulnerable to adversarial examples. Previous 3D adversarial attack methods often exploit certain information about the target models, such as model parameters or outputs, to generate adversarial point clouds. However, in realistic scenarios, it is challenging to obtain any information about the target models under conditions of absolute security. Therefore, we focus on transfer-based attacks, where generating adversarial point clouds does not require any information about the target models. Based on our observation that the critical features used for point cloud classification are consistent across different DNN architectures, we propose CFG, a novel transfer-based black-box attack method that improves the transferability of adversarial point clouds via the proposed Critical Feature Guidance. Specifically, our method regularizes the search of adversarial point clouds by computing the importance of the extracted features, prioritizing the corruption of critical features that are likely to be adopted by diverse architectures. Further, we explicitly constrain the maximum deviation extent of the generated adversarial point clouds in the loss function to ensure their imperceptibility. Extensive experiments conducted on the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the proposed CFG outperforms the state-of-the-art attack methods by a large margin.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹3Dç‚¹äº‘æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å®é™…åœºæ™¯ä¸­éš¾ä»¥è·å–ç›®æ ‡æ¨¡å‹ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCFGï¼ˆCritical Feature Guidanceï¼‰çš„æ–°å‹Transfer-basedé»‘ç›’æ”»å‡»æ–¹æ³•ã€‚ç ”ç©¶è€…é€šè¿‡è§‚å¯Ÿå‘ç°ï¼Œä¸åŒDNNæ¶æ„åœ¨è¿›è¡Œç‚¹äº‘åˆ†ç±»æ—¶æ‰€ä¾èµ–çš„Critical featureså…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶æ®æ­¤é€šè¿‡è®¡ç®—ç‰¹å¾é‡è¦æ€§æ¥å¼•å¯¼å¯¹æŠ—æ ·æœ¬çš„ç”Ÿæˆï¼Œä¼˜å…ˆç ´åé‚£äº›æ˜“è¢«å¤šç§æ¶æ„å…±åŒé‡‡ç”¨çš„å…³é”®ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨Loss functionä¸­å¼•å…¥äº†æœ€å¤§åå·®çº¦æŸï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å¯¹æŠ—æ€§ç‚¹äº‘åœ¨è§†è§‰ä¸Šå…·æœ‰ä¸å¯æ„ŸçŸ¥æ€§ã€‚åœ¨ModelNet40å’ŒScanObjectNNåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCFGåœ¨æ”»å‡»è¿ç§»æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„State-of-the-artæ–¹æ³•ï¼Œä¸ºè¯„ä¼°3Dæ¨¡å‹çš„é²æ£’æ€§æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15650v1",
      "published_date": "2025-08-21 15:31:51 UTC",
      "updated_date": "2025-08-21 15:31:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:20.797956+00:00"
    },
    {
      "arxiv_id": "2508.15881v2",
      "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill and Decode Inference",
      "title_zh": "TPLAï¼šé¢å‘é«˜æ•ˆè§£è€¦é¢„å¡«å……ä¸è§£ç æ¨ç†çš„å¼ é‡å¹¶è¡Œæ½œåœ¨æ³¨æ„åŠ›",
      "authors": [
        "Xiaojuan Tang",
        "Fanxu Meng",
        "Pingzhi Tang",
        "Yuxuan Wang",
        "Di Yin",
        "Xing Sun",
        "Muhan Zhang"
      ],
      "abstract": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Tensor-Parallel Latent Attention (TPLA)ï¼Œæ—¨åœ¨è§£å†³Multi-Head Latent Attention (MLA)åœ¨Tensor Parallelism (TP)ç¯å¢ƒä¸‹å› æ¯ä¸ªè®¾å¤‡éœ€åŠ è½½å®Œæ•´KV cacheè€Œå¯¼è‡´æ•ˆç‡ä¸‹é™çš„é—®é¢˜ã€‚TPLAé€šè¿‡å°†æ½œåœ¨è¡¨ç¤ºå’Œæ¯ä¸ªå¤´çš„è¾“å…¥ç»´åº¦åœ¨è®¾å¤‡é—´è¿›è¡Œåˆ‡åˆ†ï¼Œåœ¨åˆ†ç‰‡ä¸Šç‹¬ç«‹æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œæœ€åé€šè¿‡all-reduceæ“ä½œåˆå¹¶ç»“æœï¼Œä»è€Œåœ¨ä¿ç•™KV cacheå‹ç¼©ä¼˜åŠ¿çš„åŒæ—¶è§£é”TPæ•ˆç‡ã€‚ä¸Grouped Latent Attention (GLA)ä¸åŒï¼ŒTPLAçš„æ¯ä¸ªå¤´ä»èƒ½åˆ©ç”¨å®Œæ•´çš„æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œä¿æŒäº†æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒå³å¯ç›´æ¥é€‚é…DeepSeek-V3å’ŒKimi-K2ç­‰é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†ç¼“è§£è·¨åˆ†ç‰‡å¹²æ‰°ï¼Œè¯¥æ–¹æ¡ˆåœ¨åˆ‡ç‰‡å‰åº”ç”¨äº†Hadamardå˜æ¢æˆ–PCAç­‰æ­£äº¤å˜æ¢ï¼Œç¡®ä¿äº†æä½çš„ç²¾åº¦æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨32Kä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ï¼ŒTPLAåœ¨DeepSeek-V3å’ŒKimi-K2ä¸Šåˆ†åˆ«å®ç°äº†1.79å€å’Œ1.93å€çš„æ¨ç†åŠ é€Ÿï¼Œå¹¶ä¿æŒäº†ä¼˜ç§€çš„åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTPLAå¯ä¸FlashAttention-3ç»“åˆä½¿ç”¨ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„åˆ†ç¦»å¼é¢„å¡«å……å’Œè§£ç æ¨ç†æä¾›äº†å®ç”¨çš„ç«¯åˆ°ç«¯åŠ é€Ÿæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15881v2",
      "published_date": "2025-08-21 15:25:40 UTC",
      "updated_date": "2025-08-25 02:24:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:21.884879+00:00"
    },
    {
      "arxiv_id": "2508.16681v1",
      "title": "Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications",
      "title_zh": "é‡æ–°å®¡è§†åŸºäºè§„åˆ™çš„å£åƒæ£€æµ‹ï¼šä¸´åºŠåº”ç”¨ä¸­å¯è§£é‡Šæ¨¡å‹çš„å…¨é¢åˆ†æ",
      "authors": [
        "Eric Zhang"
      ],
      "abstract": "Stuttering affects approximately 1% of the global population, impacting communication and quality of life. While recent advances in deep learning have pushed the boundaries of automatic speech dysfluency detection, rule-based approaches remain crucial for clinical applications where interpretability and transparency are paramount. This paper presents a comprehensive analysis of rule-based stuttering detection systems, synthesizing insights from multiple corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced rule-based framework that incorporates speaking-rate normalization, multi-level acoustic feature analysis, and hierarchical decision structures. Our approach achieves competitive performance while maintaining complete interpretability-critical for clinical adoption. We demonstrate that rule-based systems excel particularly in prolongation detection (97-99% accuracy) and provide stable performance across varying speaking rates. Furthermore, we show how these interpretable models can be integrated with modern machine learning pipelines as proposal generators or constraint modules, bridging the gap between traditional speech pathology practices and contemporary AI systems. Our analysis reveals that while neural approaches may achieve marginally higher accuracy in unconstrained settings, rule-based methods offer unique advantages in clinical contexts where decision auditability, patient-specific tuning, and real-time feedback are essential.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†åœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰é«˜å¯è§£é‡Šæ€§å’Œé€æ˜åº¦çš„åŸºäºè§„åˆ™çš„å£åƒæ£€æµ‹ (Rule-Based Stuttering Detection) ç³»ç»Ÿã€‚é€šè¿‡å¯¹ UCLASSã€FluencyBank å’Œ SEP-28k ç­‰æ•°æ®é›†çš„ç»¼åˆåˆ†æï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆäº†è¯­é€Ÿå½’ä¸€åŒ– (Speaking-rate Normalization)ã€å¤šå±‚çº§å£°å­¦ç‰¹å¾åˆ†æ (Multi-level Acoustic Feature Analysis) å’Œå±‚æ¬¡åŒ–å†³ç­–ç»“æ„ (Hierarchical Decision Structures) çš„å¢å¼ºå‹æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®Œå…¨å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œåœ¨å»¶é•¿éŸ³æ£€æµ‹ (Prolongation Detection) æ–¹é¢è¾¾åˆ°äº† 97-99% çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨ä¸åŒè¯­é€Ÿä¸‹è¡¨ç°ç¨³å®šã€‚è¿™äº›å¯è§£é‡Šæ¨¡å‹å¯ä½œä¸ºææ¡ˆç”Ÿæˆå™¨ (Proposal Generators) æˆ–çº¦æŸæ¨¡å— (Constraint Modules) æ•´åˆè¿›ç°ä»£æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼Œæœ‰æ•ˆå¼¥åˆäº†ä¼ ç»Ÿè¨€è¯­ç—…ç†å­¦ä¸ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿä¹‹é—´çš„å·®è·ã€‚ç›¸æ¯”çº¯ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œè¯¥åŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨å†³ç­–å¯å®¡è®¡æ€§ã€æ‚£è€…ä¸ªæ€§åŒ–è°ƒä¼˜å’Œå®æ—¶åé¦ˆæ–¹é¢å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16681v1",
      "published_date": "2025-08-21 15:01:05 UTC",
      "updated_date": "2025-08-21 15:01:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:29.194094+00:00"
    },
    {
      "arxiv_id": "2508.15635v1",
      "title": "Label Uncertainty for Ultrasound Segmentation",
      "title_zh": "è¶…å£°åˆ†å‰²ä¸­çš„æ ‡ç­¾ä¸ç¡®å®šæ€§",
      "authors": [
        "Malini Shivaram",
        "Gautam Rajendrakumar Gare",
        "Laura Hutchins",
        "Jacob Duplantis",
        "Thomas Deiss",
        "Thales Nogueira Gomes",
        "Thong Tran",
        "Keyur H. Patel",
        "Thomas H Fox",
        "Amita Krishnan",
        "Deva Ramanan",
        "Bennett DeBoisblanc",
        "Ricardo Rodriguez",
        "John Galeotti"
      ],
      "abstract": "In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‚ºéƒ¨è¶…å£°(Lung ultrasound, LUS)å½±åƒä¸­å­˜åœ¨çš„æ ‡ç­¾ä¸ç¡®å®šæ€§(label uncertainty)å’Œæ”¾å°„ç§‘åŒ»å¸ˆé—´çš„è§‚å¯Ÿè€…å·®å¼‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¸“å®¶æä¾›çš„åƒç´ çº§ç½®ä¿¡åº¦(per-pixel confidence values)çš„æ ‡æ³¨å’Œè®­ç»ƒæ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸å†å°†æ ‡æ³¨è§†ä¸ºç»å¯¹çš„åœ°é¢çœŸå€¼(ground truth)ï¼Œè€Œæ˜¯é€šè¿‡æ•è·æ ‡æ³¨è€…å¯¹å„åŒºåŸŸçš„ä¿¡å¿ƒæ¥æ¨¡æ‹Ÿä¸´åºŠæ•°æ®ä¸­å›ºæœ‰çš„å¶ç„¶ä¸ç¡®å®šæ€§(aleatoric uncertainty)ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨è¾ƒé«˜çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå¦‚60%ï¼‰è¿›è¡Œæ¨¡å‹è®­ç»ƒæ¯”ä¼ ç»Ÿçš„50%é˜ˆå€¼æ›´ä¸ºæœ‰æ•ˆï¼Œèƒ½æ˜¾è‘—æå‡åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿™ç§æ€§èƒ½æå‡è¿˜è½¬åŒ–ä¸ºæ›´ä¼˜çš„ä¸‹æ¸¸ä¸´åºŠä»»åŠ¡ç»“æœï¼Œå¦‚S/Fæ°§åˆæ¯”å€¼(S/F oxygenation ratio)ä¼°è®¡ã€æ¯”å€¼å˜åŒ–åˆ†ç±»åŠæ‚£è€…30å¤©å†å…¥é™¢é¢„æµ‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ ‡ç­¾ç½®ä¿¡åº¦æ˜¯æé«˜åŒ»å­¦å½±åƒAIå¯é æ€§ä¸ä¸´åºŠå®ç”¨æ€§çš„å…³é”®ä¿¡å·ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "eess.IV",
      "comment": "Paper under review",
      "pdf_url": "https://arxiv.org/pdf/2508.15635v1",
      "published_date": "2025-08-21 15:00:21 UTC",
      "updated_date": "2025-08-21 15:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:29.687429+00:00"
    },
    {
      "arxiv_id": "2508.15633v1",
      "title": "GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)",
      "title_zh": "GRASPEDï¼šåŸºäºè°±åŸŸç¼–ç å™¨ä¸è§£ç å™¨è‡ªç¼–ç å™¨çš„å›¾å¼‚å¸¸æ£€æµ‹ï¼ˆå®Œæ•´ç‰ˆï¼‰",
      "authors": [
        "Wei Herng Choong",
        "Jixing Liu",
        "Ching-Yu Kao",
        "Philip Sperl"
      ],
      "abstract": "Graph machine learning has been widely explored in various domains, such as community detection, transaction analysis, and recommendation systems. In these applications, anomaly detection plays an important role. Recently, studies have shown that anomalies on graphs induce spectral shifts. Some supervised methods have improved the utilization of such spectral domain information. However, they remain limited by the scarcity of labeled data due to the nature of anomalies. On the other hand, existing unsupervised learning approaches predominantly rely on spatial information or only employ low-pass filters, thereby losing the capacity for multi-band analysis. In this paper, we propose Graph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for node anomaly detection. Our unsupervised learning model features an encoder based on Graph Wavelet Convolution, along with structural and attribute decoders. The Graph Wavelet Convolution-based encoder, combined with a Wiener Graph Deconvolution-based decoder, exhibits bandpass filter characteristics that capture global and local graph information at multiple scales. This design allows for a learning-based reconstruction of node attributes, effectively capturing anomaly information. Extensive experiments on several real-world graph anomaly detection datasets demonstrate that GRASPED outperforms current state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å¼‚å¸¸æ£€æµ‹(Graph Anomaly Detection)ä¸­å¼‚å¸¸å¼•èµ·é¢‘è°±åç§»(Spectral Shifts)çš„ç‰¹æ€§ï¼ŒæŒ‡å‡ºå½“å‰æ— ç›‘ç£æ–¹æ³•åœ¨å¤šé¢‘æ®µåˆ†æ(Multi-band Analysis)èƒ½åŠ›çš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†GRASPEDï¼Œä¸€ç§åŸºäºé¢‘è°±ç¼–ç å™¨å’Œè§£ç å™¨çš„å›¾è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„ç¼–ç å™¨é‡‡ç”¨å›¾å°æ³¢å·ç§¯(Graph Wavelet Convolution)ï¼Œç»“åˆåŸºäºç»´çº³å›¾åå·ç§¯(Wiener Graph Deconvolution)çš„è§£ç å™¨ï¼Œä½¿ç³»ç»Ÿè¡¨ç°å‡ºå¸¦é€šæ»¤æ³¢å™¨(Bandpass Filter)çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦çš„å…¨å±€å’Œå±€éƒ¨å›¾ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§å­¦ä¹ å¼çš„èŠ‚ç‚¹å±æ€§é‡æ„ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤æ‚çš„å¼‚å¸¸ä¿¡å·ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGRASPEDçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨æ— ç›‘ç£å›¾å¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Full version of the paper accepted for publication at the European Conference on Artificial Intelligence (ECAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.15633v1",
      "published_date": "2025-08-21 14:57:30 UTC",
      "updated_date": "2025-08-21 14:57:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:37.885632+00:00"
    },
    {
      "arxiv_id": "2508.15630v1",
      "title": "Adapting A Vector-Symbolic Memory for Lisp ACT-R",
      "title_zh": "é¢å‘ Lisp ACT-R çš„å‘é‡ç¬¦å·è®°å¿†é€‚é…",
      "authors": [
        "Meera Ray",
        "Christopher L. Dancy"
      ],
      "abstract": "Holographic Declarative Memory (HDM) is a vector-symbolic alternative to ACT-R's Declarative Memory (DM) system that can bring advantages such as scalability and architecturally defined similarity between DM chunks. We adapted HDM to work with the most comprehensive and widely-used implementation of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with HDM without major changes. With this adaptation of HDM, we have developed vector-based versions of common ACT-R functions, set up a text processing pipeline to add the contents of large documents to ACT-R memory, and most significantly created a useful and novel mechanism to retrieve an entire chunk of memory based on a request using only vector representations of tokens. Preliminary results indicate that we can maintain vector-symbolic advantages of HDM (e.g., chunk recall without storing the actual chunk and other advantages with scaling) while also extending it so that previous ACT-R models may work with the system with little (or potentially no) modifications within the actual procedural and declarative memory portions of a model. As a part of iterative improvement of this newly translated holographic declarative memory module, we will continue to explore better time-context representations for vectors to improve the module's ability to reconstruct chunks during recall. To more fully test this translated HDM module, we also plan to develop decision-making models that use instance-based learning (IBL) theory, which is a useful application of HDM given the advantages of the system.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†Holographic Declarative Memory (HDM)é€‚é…åˆ°Lisp ACT-Ræ¶æ„ä¸­ï¼Œä¸ºä¼ ç»Ÿçš„Declarative Memory (DM)ç³»ç»Ÿæä¾›äº†ä¸€ç§Vector-Symbolicçš„æ›¿ä»£æ–¹æ¡ˆã€‚HDMé€šè¿‡Vector-Symbolicæ¶æ„æå‡äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ï¼Œå¹¶å®šä¹‰äº†DM Chunksä¹‹é—´åŸºäºæ¶æ„çš„ç›¸ä¼¼æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŸºäºVectorçš„ACT-Rå‡½æ•°å’Œæ–‡æœ¬å¤„ç†æµæ°´çº¿ï¼Œå®ç°äº†å°†å¤§è§„æ¨¡æ–‡æ¡£å†…å®¹å½•å…¥ACT-Rè®°å¿†çš„åŠŸèƒ½ã€‚è¯¥ç ”ç©¶æœ€æ ¸å¿ƒçš„è´¡çŒ®åœ¨äºå»ºç«‹äº†ä¸€ç§å…¨æ–°çš„æ£€ç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿä»…å‡­Tokençš„Vectorè¡¨ç¤ºå³å¯æ£€ç´¢å‡ºå®Œæ•´çš„è®°å¿†å—ã€‚åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œè¯¥é€‚é…ç‰ˆæœ¬åœ¨ä¿æŒHDMåŸæœ‰ä¼˜åŠ¿çš„åŸºç¡€ä¸Šï¼Œç¡®ä¿äº†ä¸ç°æœ‰ACT-Ræ¨¡å‹çš„è‰¯å¥½å…¼å®¹ï¼Œä½¿æ¨¡å‹åœ¨å‡ ä¹æ— éœ€ä¿®æ”¹çš„æƒ…å†µä¸‹å³å¯è¿è¡Œã€‚æœªæ¥ç ”ç©¶å°†è¿›ä¸€æ­¥æ¢ç´¢æ›´ä¼˜çš„Time-Context Vectorè¡¨ç¤ºï¼Œå¹¶è®¡åˆ’åº”ç”¨Instance-Based Learning (IBL)ç†è®ºæ„å»ºå†³ç­–æ¨¡å‹ï¼Œä»¥å……åˆ†å‘æŒ¥è¯¥ç³»ç»Ÿçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages. 5 figures. Submitted and accepted to the 23rd International Conference on Cognitive Modeling (ICCM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.15630v1",
      "published_date": "2025-08-21 14:54:25 UTC",
      "updated_date": "2025-08-21 14:54:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:10:33.666441+00:00"
    },
    {
      "arxiv_id": "2508.15617v1",
      "title": "Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing",
      "title_zh": "Trained Miniaturesï¼šé¢å‘é”€å”®ä¸è¥é”€çš„ä½æˆæœ¬ã€é«˜æ•ˆèƒ½å°è¯­è¨€æ¨¡å‹ (SLM)",
      "authors": [
        "Ishaan Bhola",
        "Mukunda NS",
        "Sravanth Kurmala",
        "Harsh Nandwani",
        "Arihant Jain"
      ],
      "abstract": "Large language models (LLMs) excel in text generation; however, these creative elements require heavy computation and are accompanied by a steep cost. Especially for targeted applications such as sales and marketing outreach, these costs are far from feasible. This paper introduces the concept of \"Trained Miniatures\" - Small Language Models(SLMs) fine-tuned for specific, high-value applications, generating similar domain-specific responses for a fraction of the cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é”€å”®ä¸è¥é”€å¤–è”åº”ç”¨ä¸­é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬å’Œèµ„æºæ¶ˆè€—é—®é¢˜ï¼Œæå‡ºäº† \"Trained Miniatures\" çš„æ–°æ¦‚å¿µã€‚è¿™ä¸€æ–¹æ¡ˆçš„æ ¸å¿ƒæ˜¯åˆ©ç”¨ç»è¿‡å¾®è°ƒçš„å°è¯­è¨€æ¨¡å‹ (SLMs) æ¥æ‰§è¡Œç‰¹å®šçš„é«˜ä»·å€¼ä»»åŠ¡ï¼Œä»è€Œåœ¨ä¿è¯ç”Ÿæˆå“åº”ä¸“ä¸šæ€§çš„åŒæ—¶å¤§å¹…é™ä½æˆæœ¬ã€‚é€šè¿‡åœ¨ç‰¹å®šé¢†åŸŸå†…å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼ŒSLMs èƒ½å¤Ÿä»¥æå°çš„èµ„æºå¼€é”€å®ç°ä¸å¤§è§„æ¨¡æ¨¡å‹ç›¸è¿‘çš„é¢†åŸŸç‰¹å®šè¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºä¼ä¸šåœ¨é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„è¥é”€è‡ªåŠ¨åŒ–æä¾›äº†å¯è¡Œçš„è·¯å¾„ï¼Œæ˜¾è‘—æå‡äº† SLMs åœ¨å•†ä¸šå®æˆ˜ä¸­çš„åº”ç”¨å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15617v1",
      "published_date": "2025-08-21 14:46:22 UTC",
      "updated_date": "2025-08-21 14:46:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:45.186079+00:00"
    },
    {
      "arxiv_id": "2509.00033v2",
      "title": "Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary",
      "title_zh": "æ·±åº¦å­¦ä¹ é©±åŠ¨çš„çƒ¹é¥ªåœºæ™¯ç›®æ ‡å¤šæ¨¡æ€æ£€æµ‹ä¸è¿åŠ¨åˆ†æ",
      "authors": [
        "Tahoshin Alam Ishat",
        "Mohammad Abdul Qayum"
      ],
      "abstract": "This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ é©±åŠ¨çš„å¤šæ¨¡æ€æ£€æµ‹ä¸åŠ¨ä½œåˆ†ææŠ€æœ¯åœ¨çƒ¹é¥ªé¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¤æ‚å¨æˆ¿ç¯å¢ƒä¸­è¯†åˆ«é£Ÿè°±å¹¶ç”ŸæˆæŒ‡å¯¼æ­¥éª¤çš„ç³»ç»Ÿã€‚ç ”ç©¶é€šè¿‡å¾®è°ƒå¹¶æ•´åˆäº† YOLOv8 åˆ†å‰²æ¨¡å‹ã€åŸºäºæ‰‹éƒ¨è¿åŠ¨åºåˆ—è®­ç»ƒçš„ LSTM æ¨¡å‹ä»¥åŠ ASR (Whisper-base) è¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼Œç”¨ä»¥æå–å¤šç»´ç‰¹å¾æ•°æ®ã€‚è¿™äº›æå–çš„ä¿¡æ¯è¢«è¾“å…¥åˆ°è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ LLM (TinyLLaMa) ä¸­ï¼Œç”¨äºé¢„æµ‹å½“å‰çƒ¹é¥ªçš„é£Ÿè°±å¹¶å®æ—¶ç”Ÿæˆè¯¦ç»†çš„æ­¥è¿›å¼æ“ä½œæŒ‡å—ã€‚æ‰€æœ‰å®éªŒæ•°æ®å‡ç”±ä½œè€…äº²è‡ªé‡‡é›†ï¼Œç¡®ä¿äº†è¯¥ç‰¹å®šä»»åŠ¡ç³»ç»Ÿåœ¨å¤„ç†æŒ‘æˆ˜æ€§ç¯å¢ƒæ—¶å…·æœ‰æå¼ºçš„ç¨³å¥æ€§ã€‚è¯¥å·¥ä½œä¸ä»…è¯æ˜äº†è®¡ç®—æœºè§†è§‰åœ¨å¨æˆ¿ç­‰æ—¥å¸¸æ´»åŠ¨ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ï¼Œä¹Ÿä¸ºæœªæ¥æ›´å¤šç”Ÿæ´»åœºæ™¯çš„æ™ºèƒ½åŒ–ä»»åŠ¡å¤„ç†æ‹“å±•äº†ç ”ç©¶æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.00033v2",
      "published_date": "2025-08-21 14:40:11 UTC",
      "updated_date": "2025-09-18 05:06:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:09:43.212029+00:00"
    },
    {
      "arxiv_id": "2508.15610v2",
      "title": "Transduction is All You Need for Structured Data Workflows",
      "title_zh": "ç»“æ„åŒ–æ•°æ®å·¥ä½œæµï¼Œè½¬å¯¼å°±å¤Ÿäº†",
      "authors": [
        "Alfio Gliozzo",
        "Naweed Khan",
        "Christodoulos Constantinides",
        "Nandana Mihindukulasooriya",
        "Nahuel Defosse",
        "Gaetano Rossiello",
        "Junkyu Lee"
      ],
      "abstract": "This paper introduces Agentics, a functional agentic AI framework for building LLM-based structured data workflow pipelines. Designed for both research and practical applications, Agentics offers a new data-centric paradigm in which agents are embedded within data types, enabling logical transduction between structured states. This design shifts the focus toward principled data modeling, providing a declarative language where data types are directly exposed to large language models and composed through transductions triggered by type connections. We present a range of structured data workflow tasks and empirical evidence demonstrating the effectiveness of this approach, including data wrangling, text-to-SQL semantic parsing, and domain-specific multiple-choice question answering. The open source Agentics is available at https://github.com/IBM/Agentics.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Agenticsï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ç»“æ„åŒ–æ•°æ®å·¥ä½œæµç®¡é“çš„åŠŸèƒ½æ€§æ™ºèƒ½ä½“ AI æ¡†æ¶ã€‚Agentics æå‡ºäº†ä¸€ç§å…¨æ–°çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„èŒƒå¼ï¼Œé€šè¿‡å°†æ™ºèƒ½ä½“åµŒå…¥æ•°æ®ç±»å‹ä¸­ï¼Œå®ç°äº†ç»“æ„åŒ–çŠ¶æ€ä¹‹é—´çš„é€»è¾‘è½¬æ¢ (Transduction)ã€‚è¿™ç§è®¾è®¡å°†é‡å¿ƒè½¬å‘åŸåˆ™æ€§çš„æ•°æ®å»ºæ¨¡ï¼Œæä¾›äº†ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œä½¿æ•°æ®ç±»å‹èƒ½å¤Ÿç›´æ¥æš´éœ²ç»™ LLMsï¼Œå¹¶é€šè¿‡ç±»å‹è¿æ¥è§¦å‘çš„è½¬æ¢è¿›è¡Œç»„åˆã€‚ç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤šæ ·åŒ–ç»“æ„åŒ–æ•°æ®ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å„ç§å¤æ‚çš„å·¥ä½œæµéœ€æ±‚ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®æ¸…æ´— (data wrangling)ã€text-to-SQL è¯­ä¹‰è§£æå’Œé¢†åŸŸç‰¹å®šå¤šé€‰é¢˜å›ç­”ç­‰ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç›®å‰ï¼ŒAgentics æ¡†æ¶å·²ä½œä¸ºå¼€æºå·¥å…·å‘å¸ƒï¼Œä¸ºç ”ç©¶å’Œå®é™…åº”ç”¨æä¾›äº†é«˜æ•ˆçš„ç»“æ„åŒ–æ•°æ®å¤„ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15610v2",
      "published_date": "2025-08-21 14:35:47 UTC",
      "updated_date": "2025-09-29 13:42:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:10:26.389350+00:00"
    },
    {
      "arxiv_id": "2508.15878v1",
      "title": "Lean Meets Theoretical Computer Science: Scalable Synthesis of Theorem Proving Challenges in Formal-Informal Pairs",
      "title_zh": "Lean ä¸ç†è®ºè®¡ç®—æœºç§‘å­¦ï¼šå½¢å¼åŒ–-éå½¢å¼åŒ–å¯¹å®šç†è¯æ˜æŒ‘æˆ˜çš„å¯æ‰©å±•åˆæˆ",
      "authors": [
        "Terry Jingchen Zhang",
        "Wenyuan Jiang",
        "Rongchuan Liu",
        "Yisong Wang",
        "Junran Yang",
        "Ning Wang",
        "Nicole Ni",
        "Yinya Huang",
        "Mrinmaya Sachan"
      ],
      "abstract": "Formal theorem proving (FTP) has emerged as a critical foundation for evaluating the reasoning capabilities of large language models, enabling automated verification of mathematical proofs at scale. However, progress has been constrained by limited datasets due to the high cost of manual curation and the scarcity of challenging problems with verified formal-informal correspondences. We propose leveraging theoretical computer science (TCS) as a scalable source of rigorous proof problems, where algorithmic definitions enable automated generation of arbitrarily many challenging theorem-proof pairs. We demonstrate this approach on two TCS domains: Busy Beaver problems, which involve proving bounds on Turing machine halting behavior, and Mixed Boolean Arithmetic problems, which combine logical and arithmetic reasoning. Our framework automatically synthesizes problems with parallel formal (Lean4) and informal (Markdown) specifications, creating a scalable pipeline for generating verified proof challenges. Evaluation on frontier models reveals substantial gaps in automated theorem proving: while DeepSeekProver-V2-671B achieves 57.5\\% success on Busy Beaver problems, it manages only 12\\% on Mixed Boolean Arithmetic problems. These results highlight the difficulty of long-form proof generation even for problems that are computationally easy to verify, demonstrating the value of TCS domains for advancing automated reasoning research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç†è®ºè®¡ç®—æœºç§‘å­¦ (Theoretical Computer Science, TCS) ä½œä¸ºå¯æ‰©å±•çš„è¯æ˜é—®é¢˜æ¥æºï¼Œä»¥è§£å†³å½“å‰æ­£å¼å®šç†è¯æ˜ (Formal Theorem Proving, FTP) æ•°æ®é›†æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜ä¸”éªŒè¯é¢˜ç›®åŒ®ä¹çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”Ÿæˆå¤§è§„æ¨¡æŒ‘æˆ˜æ€§å®šç†-è¯æ˜å¯¹çš„æ¡†æ¶ï¼Œå¹¶é’ˆå¯¹ Busy Beaver é—®é¢˜å’Œ Mixed Boolean Arithmetic é—®é¢˜è¿™ä¸¤ä¸ª TCS é¢†åŸŸï¼ŒåŒæ­¥åˆæˆäº†æ­£å¼ (Lean4) ä¸éæ­£å¼ (Markdown) çš„è§„èŒƒè¯´æ˜ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯é¢†å…ˆçš„ frontier models åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶ä¹Ÿå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¾‹å¦‚ DeepSeekProver-V2-671B åœ¨ Busy Beaver ä¸Šçš„æˆåŠŸç‡ä¸º 57.5%ï¼Œä½†åœ¨ Mixed Boolean Arithmetic ä¸Šä»…ä¸º 12%ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†å½“å‰æ¨¡å‹åœ¨ç”Ÿæˆé•¿ç¯‡è¯æ˜æ–¹é¢çš„å›°éš¾ï¼Œè¯å®äº†åˆ©ç”¨ TCS é¢†åŸŸè‡ªåŠ¨æ„å»ºå¯éªŒè¯æŒ‘æˆ˜å¯¹äºæ¨è¿›è‡ªåŠ¨æ¨ç†ç ”ç©¶å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.LO",
      "comment": "Accepted to AI4MATH@ICML2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15878v1",
      "published_date": "2025-08-21 14:15:40 UTC",
      "updated_date": "2025-08-21 14:15:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:10:48.855297+00:00"
    },
    {
      "arxiv_id": "2508.15594v1",
      "title": "Are Virtual DES Images a Valid Alternative to the Real Ones?",
      "title_zh": "è™šæ‹Ÿ DES å›¾åƒæ˜¯å¦å¯ä½œä¸ºçœŸå®å›¾åƒçš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Ÿ",
      "authors": [
        "Ana C. Perre",
        "LuÃ­s A. Alexandre",
        "LuÃ­s C. Freire"
      ],
      "abstract": "Contrast-enhanced spectral mammography (CESM) is an imaging modality that provides two types of images, commonly known as low-energy (LE) and dual-energy subtracted (DES) images. In many domains, particularly in medicine, the emergence of image-to-image translation techniques has enabled the artificial generation of images using other images as input. Within CESM, applying such techniques to generate DES images from LE images could be highly beneficial, potentially reducing patient exposure to radiation associated with high-energy image acquisition. In this study, we investigated three models for the artificial generation of DES images (virtual DES): a pre-trained U-Net model, a U-Net trained end-to-end model, and a CycleGAN model. We also performed a series of experiments to assess the impact of using virtual DES images on the classification of CESM examinations into malignant and non-malignant categories. To our knowledge, this is the first study to evaluate the impact of virtual DES images on CESM lesion classification. The results demonstrate that the best performance was achieved with the pre-trained U-Net model, yielding an F1 score of 85.59% when using the virtual DES images, compared to 90.35% with the real DES images. This discrepancy likely results from the additional diagnostic information in real DES images, which contributes to a higher classification accuracy. Nevertheless, the potential for virtual DES image generation is considerable and future advancements may narrow this performance gap to a level where exclusive reliance on virtual DES images becomes clinically viable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¯¹æ¯”å¢å¼ºå…‰è°±ä¹³è…ºæ‘„å½±(CESM)ä¸­ï¼Œåˆ©ç”¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æŠ€æœ¯ä»ä½èƒ½(LE)å›¾åƒç”Ÿæˆè™šæ‹ŸåŒèƒ½å‡å½±(DES)å›¾åƒçš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨é™ä½æ‚£è€…åœ¨å½±åƒé‡‡é›†è¿‡ç¨‹ä¸­çš„è¾å°„æš´éœ²ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¯”äº†é¢„è®­ç»ƒU-Netã€ç«¯åˆ°ç«¯è®­ç»ƒU-Netä»¥åŠCycleGANä¸‰ç§ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°äº†è™šæ‹Ÿå›¾åƒå¯¹ä¹³è…ºç—…ç¶è‰¯æ¶æ€§åˆ†ç±»å‡†ç¡®æ€§çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒU-Netæ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œå…¶ç”Ÿæˆçš„è™šæ‹ŸDESå›¾åƒåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†85.59%çš„F1 scoreï¼Œè™½ç„¶ä¸çœŸå®DESå›¾åƒçš„90.35%ç›¸æ¯”ä»æœ‰ä¸€å®šå·®è·ï¼Œä½†å·²å±•ç°å‡ºæ˜¾è‘—çš„ä¸´åºŠåº”ç”¨å‰æ™¯ã€‚å°½ç®¡çœŸå®DESå›¾åƒç›®å‰ä»æä¾›æ›´å¤šå…³é”®è¯Šæ–­ä¿¡æ¯ï¼Œä½†è¯¥ç ”ç©¶è¯æ˜äº†è™šæ‹Ÿç”ŸæˆæŠ€æœ¯åœ¨æå‡CESMå®‰å…¨æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œæœªæ¥æœ‰æœ›åœ¨ä¸´åºŠå®è·µä¸­æˆä¸ºçœŸå®å½±åƒçš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.15594v1",
      "published_date": "2025-08-21 14:07:42 UTC",
      "updated_date": "2025-08-21 14:07:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:02.262671+00:00"
    },
    {
      "arxiv_id": "2508.15877v1",
      "title": "Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs",
      "title_zh": "Annif åœ¨ GermEval-2025 LLMs4Subjects ä»»åŠ¡ï¼šé«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹å¢å¼ºçš„ä¼ ç»Ÿ XMTC",
      "authors": [
        "Osma Suominen",
        "Juho Inkinen",
        "Mona Lehtinen"
      ],
      "abstract": "This paper presents the Annif system in the LLMs4Subjects shared task (Subtask 2) at GermEval-2025. The task required creating subject predictions for bibliographic records using large language models, with a special focus on computational efficiency. Our system, based on the Annif automated subject indexing toolkit, refines our previous system from the first LLMs4Subjects shared task, which produced excellent results. We further improved the system by using many small and efficient language models for translation and synthetic data generation and by using LLMs for ranking candidate subjects. Our system ranked 1st in the overall quantitative evaluation of and 1st in the qualitative evaluation of Subtask 2.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº† Annif ç³»ç»Ÿåœ¨ GermEval-2025 çš„ LLMs4Subjects å…±äº«ä»»åŠ¡ï¼ˆå­ä»»åŠ¡ 2ï¼‰ä¸­çš„åº”ç”¨ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨ Large Language Models (LLMs) ä¸ºä¹¦ç›®è®°å½•ç”Ÿæˆå­¦ç§‘é¢„æµ‹ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨è®¡ç®—æ•ˆç‡ã€‚è¯¥ç³»ç»ŸåŸºäº Annif è‡ªåŠ¨ä¸»é¢˜ç´¢å¼•å·¥å…·åŒ…ï¼Œé€šè¿‡å¼•å…¥å¤šä¸ªå°å‹ä¸”é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹è¿›è¡Œç¿»è¯‘å’Œåˆæˆæ•°æ®ç”Ÿæˆ (Synthetic Data Generation)ï¼Œå¹¶åˆ©ç”¨ LLMs å¯¹å€™é€‰ä¸»é¢˜è¿›è¡Œé‡æ’åºã€‚è¿™ç§æ–¹æ³•æˆåŠŸå°†ä¼ ç»Ÿçš„æç«¯å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±» (XMTC) æ¡†æ¶ä¸é«˜æ•ˆçš„ LLMs æŠ€æœ¯ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–ç´¢å¼•èƒ½åŠ›ã€‚è¯„æµ‹ç»“æœæ˜¾ç¤ºï¼ŒAnnif ç³»ç»Ÿåœ¨å­ä»»åŠ¡ 2 çš„å®šé‡ä¸å®šæ€§è¯„ä¼°ä¸­å‡æ’åç¬¬ä¸€ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚å­¦ç§‘åˆ†ç±»åœºæ™¯ä¸‹çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 4 figures, accepted at KONVENS 2025. arXiv admin note: substantial text overlap with arXiv:2504.19675",
      "pdf_url": "https://arxiv.org/pdf/2508.15877v1",
      "published_date": "2025-08-21 14:04:20 UTC",
      "updated_date": "2025-08-21 14:04:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:04.561434+00:00"
    },
    {
      "arxiv_id": "2508.15588v1",
      "title": "A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification",
      "title_zh": "å¼ºåŒ–å­¦ä¹ å®‰å…¨æ€§ä¸é²æ£’æ€§éªŒè¯çš„åŠ¨åŠ›ç³»ç»Ÿæ¡†æ¶",
      "authors": [
        "Ahmed Nasir",
        "Abdelhafid Zenati"
      ],
      "abstract": "The application of reinforcement learning to safety-critical systems is limited by the lack of formal methods for verifying the robustness and safety of learned policies. This paper introduces a novel framework that addresses this gap by analyzing the combination of an RL agent and its environment as a discrete-time autonomous dynamical system. By leveraging tools from dynamical systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we identify and visualize Lagrangian Coherent Structures (LCS) that act as the hidden \"skeleton\" governing the system's behavior. We demonstrate that repelling LCS function as safety barriers around unsafe regions, while attracting LCS reveal the system's convergence properties and potential failure modes, such as unintended \"trap\" states. To move beyond qualitative visualization, we introduce a suite of quantitative metrics, Mean Boundary Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a policy's safety margin and robustness. We further provide a method for deriving local stability guarantees and extend the analysis to handle model uncertainty. Through experiments in both discrete and continuous control environments, we show that this framework provides a comprehensive and interpretable assessment of policy behavior, successfully identifying critical flaws in policies that appear successful based on reward alone.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ™ºèƒ½ä½“ä¸ç¯å¢ƒå»ºæ¨¡ä¸ºç¦»æ•£æ—¶é—´è‡ªä¸»åŠ¨åŠ›ç³»ç»Ÿ (discrete-time autonomous dynamical system) çš„æ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®‰å…¨å…³é”®ç³»ç»Ÿä¸­ç­–ç•¥éªŒè¯ç¼ºä¹æ­£å¼æ–¹æ³•çš„é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨åŠ¨åŠ›ç³»ç»Ÿç†è®ºä¸­çš„æœ‰é™æ—¶é—´æé›…æ™®è¯ºå¤«æŒ‡æ•° (Finite-Time Lyapunov Exponent, FTLE)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å¹¶å¯è§†åŒ–æ‹‰æ ¼æœ—æ—¥ç›¸å¹²ç»“æ„ (Lagrangian Coherent Structures, LCS)ï¼Œä»è€Œæ­ç¤ºç³»ç»Ÿè¡Œä¸ºçš„å†…åœ¨â€œéª¨æ¶â€ã€‚ç ”ç©¶è¯æ˜ï¼Œæ’æ–¥æ€§ LCS å¯ä½œä¸ºä¸å®‰å…¨åŒºåŸŸå‘¨å›´çš„å®‰å…¨å±éšœï¼Œè€Œå¸å¼•æ€§ LCS åˆ™èƒ½æ­ç¤ºç³»ç»Ÿçš„æ”¶æ•›ç‰¹æ€§åŠæ½œåœ¨çš„å¤±è´¥æ¨¡å¼ã€‚ä¸ºå®ç°å®šé‡è¯„ä¼°ï¼Œç ”ç©¶å¼•å…¥äº†å¹³å‡è¾¹ç•Œæ’æ–¥åŠ› (Mean Boundary Repulsion, MBR) å’Œèšåˆä¼ªå¸å¼•å­å¼ºåº¦ (ASAS/TASAS) ç­‰æŒ‡æ ‡ï¼Œç”¨ä»¥æ­£å¼æµ‹é‡ç­–ç•¥çš„å®‰å…¨è£•åº¦ä¸é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›äº†å±€éƒ¨ç¨³å®šæ€§ä¿è¯åŠå¤„ç†æ¨¡å‹ä¸ç¡®å®šæ€§ (model uncertainty) çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç¦»æ•£å’Œè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­å‡èƒ½æä¾›å…¨é¢ä¸”å¯è§£é‡Šçš„è¯„ä¼°ï¼ŒæˆåŠŸè¯†åˆ«å‡ºä»…å‡­å¥–åŠ±è¡¨ç°æ— æ³•å¯Ÿè§‰çš„ç­–ç•¥ç¼ºé™·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15588v1",
      "published_date": "2025-08-21 14:00:26 UTC",
      "updated_date": "2025-08-21 14:00:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:06.349828+00:00"
    },
    {
      "arxiv_id": "2508.15577v2",
      "title": "LFaB: Low fidelity as Bias for Active Learning in the chemical configuration space",
      "title_zh": "LFaBï¼šä»¥ä½ä¿çœŸåº¦ä½œä¸ºåå·®çš„åŒ–å­¦æ„å‹ç©ºé—´ä¸»åŠ¨å­¦ä¹ ",
      "authors": [
        "Vivin Vinod",
        "Peter Zaspel"
      ],
      "abstract": "Active learning promises to provide an optimal training sample selection procedure in the construction of machine learning models. It often relies on minimizing the model's variance, which is assumed to decrease the prediction error. Still, it is frequently even less efficient than pure random sampling. Motivated by the bias-variance decomposition, we propose to minimize the model's bias instead of its variance. By doing so, we are able to almost exactly match the best-case error over all possible greedy sample selection procedures for a relevant application. Our bias approximation is based on using cheap to calculate low fidelity data as known from $Î”$-ML or multifidelity machine learning. We exemplify our approach for a wider class of applications in quantum chemistry including predicting excitation energies and ab initio potential energy surfaces. Here, the proposed method reduces training data consumption by up to an order of magnitude compared to standard active learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸»åŠ¨å­¦ä¹ (Active Learning)åœ¨å®é™…åº”ç”¨ä¸­æ•ˆç‡æœ‰æ—¶ä½äºéšæœºé‡‡æ ·çš„é—®é¢˜ï¼Œæå‡ºäº†LFaBæ¡†æ¶ã€‚å—åç½®-æ–¹å·®åˆ†è§£(bias-variance decomposition)çš„å¯å‘ï¼Œè¯¥æ–¹æ³•æå‡ºé€šè¿‡æœ€å°åŒ–æ¨¡å‹çš„åç½®(bias)è€Œéæ–¹å·®æ¥ä¼˜åŒ–æ ·æœ¬é€‰æ‹©è¿‡ç¨‹ã€‚å…¶åç½®è¿‘ä¼¼æ–¹æ³•ä¸»è¦åˆ©ç”¨äº†è®¡ç®—æˆæœ¬è¾ƒä½çš„ä½ä¿çœŸåº¦(low fidelity)æ•°æ®ï¼Œå…¶åŸç†ç±»ä¼¼äº$\\Delta$-MLæˆ–å¤šä¿çœŸåº¦æœºå™¨å­¦ä¹ (multifidelity machine learning)ã€‚ç ”ç©¶è€…åœ¨é‡å­åŒ–å­¦(quantum chemistry)çš„ä¸€ç³»åˆ—åº”ç”¨ä¸­éªŒè¯äº†æ­¤æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢„æµ‹æ¿€å‘èƒ½(excitation energies)å’Œä»å¤´ç®—åŠ¿èƒ½é¢(ab initio potential energy surfaces)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå‡ ä¹å®Œå…¨åŒ¹é…æ‰€æœ‰è´ªå¿ƒæ ·æœ¬é€‰æ‹©ç¨‹åºä¸­çš„æœ€ä½³è¯¯å·®è¡¨ç°ã€‚ä¸æ ‡å‡†çš„ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒLFaBå°†è®­ç»ƒæ•°æ®çš„æ¶ˆè€—é™ä½äº†å¤šè¾¾ä¸€ä¸ªæ•°é‡çº§ï¼Œä¸ºåŒ–å­¦æ„å‹ç©ºé—´çš„é«˜æ•ˆå»ºæ¨¡æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "SI included in main",
      "pdf_url": "https://arxiv.org/pdf/2508.15577v2",
      "published_date": "2025-08-21 13:51:45 UTC",
      "updated_date": "2025-11-25 12:28:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:13.985158+00:00"
    },
    {
      "arxiv_id": "2508.15548v1",
      "title": "DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks",
      "title_zh": "DeepThink3Dï¼šé€šè¿‡ç¨‹åºåŒ–æ¨ç†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚3Dæƒ…å¢ƒæ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›",
      "authors": [
        "Jiayi Song",
        "Rui Wan",
        "Lipeng Ma",
        "Weidong Yang",
        "Qingyuan Zhou",
        "Yixuan Li",
        "Ben Fei"
      ],
      "abstract": "This work enhances the ability of large language models (LLMs) to perform complex reasoning in 3D scenes. Recent work has addressed the 3D situated reasoning task by invoking tool usage through large language models. Large language models call tools via APIs and integrate the generated programs through a chain of thought to solve problems based on the program results. However, due to the simplicity of the questions in the dataset, the generated program reasoning chains are relatively short. To solve this main challenge, in this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in complex 3D situated reasoning tasks. Our work proposes a combinatorial and iterative evolutionary approach on the SQA3D benchmark to generate more complex questions. Building on this foundation, we fine-tune the large language model to make it more proficient in using 3D tools. By employing Direct Preference Optimization (DPO), we directly optimize the toolchain strategies generated by models, thereby enhancing their accuracy in complex tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepThink3Dï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤æ‚ 3D æƒ…å¢ƒæ¨ç†ä»»åŠ¡ä¸­çš„ç¨‹åºåŒ–æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†é—®é¢˜è¿‡äºç®€å•å¯¼è‡´æ¨¡å‹ç”Ÿæˆçš„å·¥å…·è°ƒç”¨é“¾æ¡è¿‡çŸ­ä¸”æ¨ç†æ·±åº¦ä¸è¶³çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…åœ¨ SQA3D åŸºå‡†ä¸Šæå‡ºäº†ä¸€ç§ç»„åˆå’Œè¿­ä»£çš„è¿›åŒ–æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆæ›´ä¸ºå¤æ‚çš„æ¨ç†é—®é¢˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¯¥å·¥ä½œé€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œä¸“é—¨å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨ 3D å·¥å…·è°ƒç”¨æ–¹é¢çš„ç†Ÿç»ƒåº¦ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ– (Direct Preference Optimization, DPO) æŠ€æœ¯ï¼Œé€šè¿‡ç›´æ¥ä¼˜åŒ–æ¨¡å‹ç”Ÿæˆçš„å·¥å…·é“¾ç­–ç•¥æ¥è¿›ä¸€æ­¥æé«˜å†³ç­–ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDeepThink3D æœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚ 3D ä»»åŠ¡æ—¶çš„å‡†ç¡®æ€§ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹å®ç°å¯è§£é‡Šçš„ 3D ç©ºé—´æ¨ç†æä¾›äº†æ›´å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15548v1",
      "published_date": "2025-08-21 13:28:36 UTC",
      "updated_date": "2025-08-21 13:28:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:02.958853+00:00"
    },
    {
      "arxiv_id": "2508.16680v2",
      "title": "CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression",
      "title_zh": "CALRï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå±‚å‹ç¼©çš„ä¿®æ­£è‡ªé€‚åº”ä½ç§©åˆ†è§£",
      "authors": [
        "Muchammad Daniyal Kautsar",
        "Afra Majida Hariono",
        "Widyawan",
        "Syukron Abu Ishaq Alfarozi",
        "Kuntpong Woraratpanya"
      ],
      "abstract": "Large Language Models (LLMs) present significant deployment challenges due to their immense size and computational requirements. Model compression techniques are essential for making these models practical for resource-constrained environments. A prominent compression strategy is low-rank factorization via Singular Value Decomposition (SVD) to reduce model parameters by approximating weight matrices. However, standard SVD focuses on minimizing matrix reconstruction error, often leading to a substantial loss of the model's functional performance. This performance degradation occurs because existing methods do not adequately correct for the functional information lost during compression. To address this gap, we introduce Corrective Adaptive Low-Rank Decomposition (CALR), a two-component compression approach. CALR combines a primary path of SVD-compressed layers with a parallel, learnable, low-rank corrective module that is explicitly trained to recover the functional residual error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to 51.77% while retaining 59.45% to 90.42% of the original model's performance, consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows that treating functional information loss as a learnable signal is a highly effective compression paradigm. This approach enables the creation of significantly smaller, more efficient LLMs, advancing their accessibility and practical deployment in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CALR (Corrective Adaptive Low-Rank Decomposition)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å±‚å‹ç¼©çš„é«˜æ•ˆä¸¤ç»„ä»¶å‹ç¼©æ–¹æ³•ã€‚ä¼ ç»Ÿçš„SVD (Singular Value Decomposition) å‹ç¼©æŠ€æœ¯å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹åŠŸèƒ½æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå› ä¸ºå…¶ä»…ä¾§é‡äºæœ€å°åŒ–çŸ©é˜µé‡å»ºè¯¯å·®ï¼Œè€Œå¿½ç•¥äº†åŠŸèƒ½ä¿¡æ¯çš„è¡¥å¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒCALR ç»“åˆäº†åŸºäºSVDçš„ä¸»å‹ç¼©è·¯å¾„å’Œä¸€ä¸ªå¹¶è¡Œçš„ã€å¯å­¦ä¹ çš„ä½ç§©æ ¡æ­£æ¨¡å—ï¼Œä¸“é—¨é€šè¿‡è®­ç»ƒæ¥æ¢å¤å‹ç¼©è¿‡ç¨‹ä¸­æŸå¤±çš„åŠŸèƒ½æ®‹å·®ã€‚åœ¨ SmolLM2-135Mã€Qwen3-0.6B å’Œ Llama-3.2-1B ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCALR èƒ½åœ¨å‡å°‘ 26.93% åˆ° 51.77% å‚æ•°é‡çš„åŒæ—¶ï¼Œä¿ç•™åŸæ¨¡å‹ 59.45% åˆ° 90.42% çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æ€§èƒ½è¡¨ç°ä¸ŠæŒç»­ä¼˜äº LaCoã€ShortGPT å’Œ LoSparse ç­‰ç°æœ‰ä¸»æµå‹ç¼©æŠ€æœ¯ã€‚CALR çš„æˆåŠŸè¯æ˜äº†å°†åŠŸèƒ½ä¿¡æ¯æŸå¤±è§†ä¸ºå¯å­¦ä¹ ä¿¡å·æ˜¯ä¸€ç§æå…¶æœ‰æ•ˆçš„å‹ç¼©èŒƒå¼ï¼Œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²å°å‹é«˜æ•ˆ LLMs æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE Transactions on Artificial Intelligence. This is the preprint version, not peer-reviewed. The final version may differ after peer review. (11 pages, 3 figures)",
      "pdf_url": "https://arxiv.org/pdf/2508.16680v2",
      "published_date": "2025-08-21 13:16:02 UTC",
      "updated_date": "2025-08-26 02:14:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:14.664184+00:00"
    },
    {
      "arxiv_id": "2508.15510v1",
      "title": "Super-additive Cooperation in Language Model Agents",
      "title_zh": "è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸­çš„è¶…åŠ æ€§åˆä½œ",
      "authors": [
        "Filippo Tonini",
        "Lukas Galke"
      ],
      "abstract": "With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoner's Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶å—è¶…åŠ æ€§åˆä½œ(Super-additive cooperation)ç†è®ºå¯å‘ï¼Œæ¢è®¨äº†è‡ªä¸»äººå·¥æ™ºèƒ½(AI)æ™ºèƒ½ä½“åœ¨é‡å¤äº’åŠ¨å’Œç¾¤ä½“é—´ç«äº‰èƒŒæ™¯ä¸‹çš„åˆä½œå€¾å‘ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªè™šæ‹Ÿé”¦æ ‡èµ›ï¼Œå°†è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“(Language Model Agents)æˆé˜Ÿåˆ†ç»„å¹¶åœ¨å›šå¾’å›°å¢ƒ(Prisoner's Dilemma)åšå¼ˆä¸­å¯¹æŠ—ï¼Œä»¥æ­¤æ¨¡æ‹Ÿå†…éƒ¨å›¢é˜ŸåŠ¨æ€ä¸å¤–éƒ¨ç«äº‰ã€‚å®éªŒå‘ç°ï¼Œè¿™ç§æœºåˆ¶ç»“åˆæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“çš„æ•´ä½“åˆä½œæ°´å¹³ä»¥åŠåœ¨ä¸€æ¬¡æ€§äº’åŠ¨ä¸­çš„é¦–å‘åˆä½œå€¾å‘ã€‚è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨å¤æ‚ç¤¾ä¼šåœºæ™¯ä¸­çš„ç­–ç•¥è¡ŒåŠ¨æä¾›äº†æ–°æ¡†æ¶ï¼Œå¹¶æ­ç¤ºäº†ç¾¤ä½“ç«äº‰å¦‚ä½•åç›´è§‰åœ°ä¿ƒè¿›åˆä½œè¡Œä¸ºã€‚è¿™äº›è§è§£å¯¹äºå¼€å‘èƒ½å¤Ÿé«˜æ•ˆåä½œå¹¶ä¸äººç±»ä»·å€¼(Human values)å¯¹é½çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-agent AI systems)å…·æœ‰å…³é”®æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "FAIEMA 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15510v1",
      "published_date": "2025-08-21 12:36:44 UTC",
      "updated_date": "2025-08-21 12:36:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:23.752557+00:00"
    },
    {
      "arxiv_id": "2508.15507v1",
      "title": "Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning",
      "title_zh": "Think in Blocksï¼šä»ç›´æ¥å“åº”åˆ°æ·±åº¦æ¨ç†çš„è‡ªé€‚åº”æ¨ç†",
      "authors": [
        "Yekun Zhu",
        "Guang Chen",
        "Chengjun Mao"
      ],
      "abstract": "Large Language Models (LLMs) with chains-of-thought have demonstrated strong performance on an increasing range of tasks, particularly those involving complex logical reasoning. However, excessively long chains can lead to overthinking, causing computational waste and slower responses. This raises a question: can LLMs dynamically adjust the length of their reasoning processes based on task complexity? To address this, we propose the Think in Blocks framework, which enables adaptive reasoning-from zero to deep reasoning-by partitioning the reasoning process into a tunable number of blocks. Our main contributions are: (1) Establishing an explicit block-structured paradigm in which the model first predicts an integer reasoning budget-the number of blocks-and then partitions its reasoning accordingly; (2) Training an adaptive model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided Direct Preference Optimization, and Reinforcement Learning-that adjusts its reasoning depth to problem difficulty; (3) Exploiting the explicit block count to dynamically control reasoning depth at inference time, allowing flexible adjustment of chain-of-thought length during deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½¿ç”¨é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰æ—¶å› æ¨ç†é“¾è¿‡é•¿å¯¼è‡´çš„è¿‡åº¦æ€è€ƒåŠè®¡ç®—èµ„æºæµªè´¹é—®é¢˜ï¼Œæå‡ºäº† Think in Blocks æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¨ç†è¿‡ç¨‹åˆ’åˆ†ä¸ºå¯è°ƒæ•°é‡çš„å—ï¼ˆBlocksï¼‰ï¼Œå®ç°äº†ä»ç›´æ¥å“åº”åˆ°æ·±åº¦æ¨ç†çš„è‡ªé€‚åº”åˆ‡æ¢ã€‚ç ”ç©¶å»ºç«‹äº†ä¸€ç§æ˜¾å¼çš„å—ç»“æ„èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé¢„å…ˆé¢„æµ‹æ¨ç†é¢„ç®—ï¼ˆå³å—çš„æ•°é‡ï¼‰ï¼Œå¹¶æ®æ­¤ç»„ç»‡æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¥–åŠ±å¼•å¯¼çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»„æˆçš„ä¸‰é˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼Œæ¨¡å‹å­¦ä¼šäº†æ ¹æ®é—®é¢˜éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…è®¸åœ¨æ¨ç†é˜¶æ®µåˆ©ç”¨æ˜¾å¼å—è®¡æ•°çµæ´»æ§åˆ¶é“¾å¼æ€ç»´çš„é•¿åº¦ï¼Œä»è€Œä¼˜åŒ–äº†å®é™…éƒ¨ç½²ä¸­çš„è®¡ç®—æ•ˆç‡ã€‚è¿™ä¸€è´¡çŒ®ä¸ºå®ç°å…¼å…·é«˜æ€§èƒ½ä¸é«˜æ•ˆç‡çš„è‡ªé€‚åº”æ¨ç†æ¨¡å‹æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15507v1",
      "published_date": "2025-08-21 12:32:19 UTC",
      "updated_date": "2025-08-21 12:32:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:23.552535+00:00"
    },
    {
      "arxiv_id": "2508.15501v1",
      "title": "LLM-Driven Self-Refinement for Embodied Drone Task Planning",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å…·èº«æ— äººæœºä»»åŠ¡è§„åˆ’è‡ªæˆ‘ä¼˜åŒ–",
      "authors": [
        "Deyu Zhang",
        "Xicheng Zhang",
        "Jiahao Li",
        "Tingting Long",
        "Xunhua Dai",
        "Yongjian Fu",
        "Jinrui Zhang",
        "Ju Ren",
        "Yaoxue Zhang"
      ],
      "abstract": "We introduce SRDrone, a novel system designed for self-refinement task planning in industrial-grade embodied drones. SRDrone incorporates two key technical contributions: First, it employs a continuous state evaluation methodology to robustly and accurately determine task outcomes and provide explanatory feedback. This approach supersedes conventional reliance on single-frame final-state assessment for continuous, dynamic drone operations. Second, SRDrone implements a hierarchical Behavior Tree (BT) modification model. This model integrates multi-level BT plan analysis with a constrained strategy space to enable structured reflective learning from experience. Experimental results demonstrate that SRDrone achieves a 44.87% improvement in Success Rate (SR) over baseline methods. Furthermore, real-world deployment utilizing an experience base optimized through iterative self-refinement attains a 96.25% SR. By embedding adaptive task refinement capabilities within an industrial-grade BT planning framework, SRDrone effectively integrates the general reasoning intelligence of Large Language Models (LLMs) with the stringent physical execution constraints inherent to embodied drones. Code is available at https://github.com/ZXiiiC/SRDrone.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SRDroneï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå·¥ä¸šçº§å…·èº«æ— äººæœº(Embodied Drone)ä»»åŠ¡è§„åˆ’è®¾è®¡çš„è‡ªæˆ‘ç»†åŒ–ç³»ç»Ÿã€‚SRDrone å¼•å…¥äº†è¿ç»­çŠ¶æ€è¯„ä¼°(Continuous State Evaluation)æ–¹æ³•ï¼Œé€šè¿‡å¯¹åŠ¨æ€æ“ä½œçš„æŒç»­ç›‘æµ‹æ¥åˆ¤å®šä»»åŠ¡ç»“æœå¹¶æä¾›è§£é‡Šæ€§åé¦ˆï¼Œè§£å†³äº†ä¼ ç»Ÿå•å¸§è¯„ä¼°åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†åˆ†å±‚è¡Œä¸ºæ ‘(Behavior Tree)ä¿®æ”¹æ¨¡å‹ï¼Œç»“åˆå¤šçº§è®¡åˆ’åˆ†æä¸å—é™ç­–ç•¥ç©ºé—´ï¼Œå®ç°äº†ç»“æ„åŒ–çš„åæ€æ€§å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRDrone åœ¨ä»»åŠ¡æˆåŠŸç‡(Success Rate)ä¸Šæ¯”åŸºçº¿æ¨¡å‹æå‡äº† 44.87%ï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­åˆ©ç”¨ä¼˜åŒ–åçš„ç»éªŒåº“è¾¾åˆ°äº† 96.25% çš„æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆåœ°å°†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„é€šç”¨æ¨ç†èƒ½åŠ›ä¸å…·èº«æ— äººæœºä¸¥æ ¼çš„ç‰©ç†æ‰§è¡Œçº¦æŸç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å·¥ä¸šçº§æ¡†æ¶ä¸‹çš„è‡ªé€‚åº”ä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "14pages",
      "pdf_url": "https://arxiv.org/pdf/2508.15501v1",
      "published_date": "2025-08-21 12:29:01 UTC",
      "updated_date": "2025-08-21 12:29:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:29.572044+00:00"
    },
    {
      "arxiv_id": "2509.05302v1",
      "title": "Sesame: Opening the door to protein pockets",
      "title_zh": "Sesameï¼šå¼€å¯è›‹ç™½è´¨å£è¢‹ä¹‹é—¨",
      "authors": [
        "RaÃºl MiÃ±Ã¡n",
        "Carles Perez-Lopez",
        "Javier Iglesias",
        "Ãlvaro Ciudad",
        "Alexis Molina"
      ],
      "abstract": "Molecular docking is a cornerstone of drug discovery, relying on high-resolution ligand-bound structures to achieve accurate predictions. However, obtaining these structures is often costly and time-intensive, limiting their availability. In contrast, ligand-free structures are more accessible but suffer from reduced docking performance due to pocket geometries being less suited for ligand accommodation in apo structures. Traditional methods for artificially inducing these conformations, such as molecular dynamics simulations, are computationally expensive. In this work, we introduce Sesame, a generative model designed to predict this conformational change efficiently. By generating geometries better suited for ligand accommodation at a fraction of the computational cost, Sesame aims to provide a scalable solution for improving virtual screening workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å­å¯¹æ¥(molecular docking)ä¸­é…ä½“ç»“åˆç»“æ„(ligand-bound structures)è·å–æˆæœ¬é«˜ï¼Œè€Œæ— é…ä½“ç»“æ„(ligand-free structures)å› å£è¢‹å‡ ä½•å½¢çŠ¶ä¸åŒ¹é…å¯¼è‡´å¯¹æ¥æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSesameçš„ç”Ÿæˆæ¨¡å‹ã€‚Sesameæ—¨åœ¨é«˜æ•ˆé¢„æµ‹è›‹ç™½è´¨çš„æ„è±¡å˜åŒ–ï¼Œé€šè¿‡ç”Ÿæˆæ›´é€‚åˆé…ä½“ç»“åˆçš„å‡ ä½•ç»“æ„ï¼Œå…‹æœäº†ä¼ ç»Ÿåˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ(molecular dynamics simulations)è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä»¥æä½çš„è®¡ç®—å¼€é”€æ˜¾è‘—æ”¹å–„è›‹ç™½è´¨å£è¢‹çš„å‡ ä½•åˆ†å¸ƒï¼Œä»è€Œä¸ºä¼˜åŒ–è™šæ‹Ÿç­›é€‰(virtual screening)å·¥ä½œæµæä¾›äº†ä¸€ç§å…¼å…·é«˜æ•ˆæ€§ä¸å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æä¾›æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å¿«çš„å¤„ç†é€Ÿåº¦ï¼ŒSesameä¸ºåˆ©ç”¨é«˜å¯åŠæ€§çš„æ— é…ä½“ç»“æ„è¿›è¡Œç²¾å‡†è¯ç‰©ç ”å‘å¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Published at the Proceedings of the 2nd Workshop on Generative and Experimental Perspectives for Biomolecular Design. ICLR 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05302v1",
      "published_date": "2025-08-21 12:22:56 UTC",
      "updated_date": "2025-08-21 12:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:51.258005+00:00"
    },
    {
      "arxiv_id": "2508.15476v1",
      "title": "LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion",
      "title_zh": "LGMSNetï¼šåŸºäºåŒå±‚å¤šå°ºåº¦èåˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹è½»é‡åŒ–",
      "authors": [
        "Chengqi Dong",
        "Fenghe Tang",
        "Rongge Mao",
        "Xinpei Gao",
        "S. Kevin Zhou"
      ],
      "abstract": "Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LGMSNetï¼Œä¸€ç§åŸºäºå±€éƒ¨ä¸å…¨å±€åŒé‡å¤šå°ºåº¦(local and global dual multiscale)èåˆçš„æ–°å‹è½»é‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨èµ„æºå—é™ä¸´åºŠç¯å¢ƒä¸‹çš„æ•ˆç‡ä¸æ€§èƒ½æƒè¡¡é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰è½»é‡åŒ–æ¨¡å‹å­˜åœ¨çš„é€šé“å†—ä½™(channel redundancy)å’Œå…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³ç­‰æŒ‘æˆ˜ï¼ŒLGMSNeté‡‡ç”¨äº†å¼‚æ„å±‚å†…å·ç§¯æ ¸(heterogeneous intra-layer kernels)æ¥æå–å±€éƒ¨é«˜é¢‘ä¿¡æ¯å¹¶å‡å°‘ç‰¹å¾å†—ä½™ï¼ŒåŒæ—¶æ•´åˆäº†ç¨€ç–Transformerä¸å·ç§¯æ··åˆåˆ†æ”¯(sparse transformer-convolutional hybrid branches)ä»¥æœ‰æ•ˆæ•æ‰å…¨å±€ä½é¢‘ä¿¡æ¯ã€‚åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒLGMSNetåœ¨æä½è®¡ç®—å¼€é”€ä¸‹å®ç°äº†SOTAæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å››ä¸ªæœªçŸ¥æ•°æ®é›†çš„é›¶æ ·æœ¬æ³›åŒ–(zero-shot generalization)æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå……åˆ†å±•ç¤ºäº†å…¶åœ¨èµ„æºå—é™çš„çœŸå®åŒ»ç–—åœºæ™¯ä¸­éƒ¨ç½²çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15476v1",
      "published_date": "2025-08-21 11:54:09 UTC",
      "updated_date": "2025-08-21 11:54:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:49.492861+00:00"
    },
    {
      "arxiv_id": "2508.15474v3",
      "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing",
      "title_zh": "LLM ä¸­çš„ä¸»è§‚è¡Œä¸ºä¸åå¥½ï¼šç½‘é¡µæµè§ˆè¯­è¨€",
      "authors": [
        "Sai Sundaresan",
        "Harshita Chopra",
        "Atanu R. Sinha",
        "Koustava Goswami",
        "Nagasai Saketh Naidu",
        "Raghav Karan",
        "N Anushka"
      ],
      "abstract": "A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed \"language\", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the \"language of browsing\" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ç”¨æˆ·ä¸»è§‚è¡Œä¸ºå’Œåå¥½æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å»ºæ¨¡å…·æœ‰é«˜åº¦ä¸ªæ€§åŒ–ä¸”ç¼ºä¹è‡ªç„¶è¯­è¨€ç»“æ„çš„ç½‘ç»œæµè§ˆè¡Œä¸ºï¼ˆLanguage of Browsingï¼‰æ—¶ã€‚ä¸ºäº†è§£å†³å•ä¸€å‚æ•°æ¨¡å‹éš¾ä»¥æ•æ‰å¼‚è´¨æ€§ç”¨æˆ·åå¥½çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†HeTLM (Heterogeneity aware Training of Language Model)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä¸»è§‚è¡Œä¸ºè®¾è®¡çš„èšç±»æ„ŸçŸ¥è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨é¡µé¢çº§åˆ†è¯å™¨(page-level tokenizer)è®­ç»ƒçš„å°å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨ç¤ºæµè§ˆè¯­è¨€æ–¹é¢çš„è¡¨ç°ä¼˜äºé¢„è®­ç»ƒæˆ–å¾®è°ƒåçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¼‚è´¨èšç±»ç‰¹å®šå‚æ•°é›†çš„HeTLMåœ¨ç›¸åŒå‚æ•°è§„æ¨¡ä¸‹æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•ä¸€LMï¼Œå¹¶å®ç°äº†æ›´é«˜çš„ç”Ÿæˆå‡å€¼ä¸æ›´ä½çš„æ€§èƒ½æ–¹å·®ã€‚è¿™äº›å‘ç°è¯æ˜äº†è¯¥æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°å®ç°ç”¨æˆ·çº§åˆ«çš„å¯¹é½(alignment)ï¼Œä¸ºç†è§£å’Œå»ºæ¨¡é«˜åº¦ä¸»è§‚çš„åºåˆ—è¡Œä¸ºæ•°æ®æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15474v3",
      "published_date": "2025-08-21 11:50:56 UTC",
      "updated_date": "2025-09-18 19:00:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:56.395799+00:00"
    },
    {
      "arxiv_id": "2508.15464v1",
      "title": "RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores",
      "title_zh": "RadReasonï¼šå…·å¤‡ç†ç”±é˜è¿°ä¸åˆ†é¡¹è¯„åˆ†çš„æ”¾å°„å­¦æŠ¥å‘Šè¯„ä¼°æŒ‡æ ‡",
      "authors": [
        "Yingshu Li",
        "Yunyi Liu",
        "Lingqiao Liu",
        "Lei Wang",
        "Luping Zhou"
      ],
      "abstract": "Evaluating automatically generated radiology reports remains a fundamental challenge due to the lack of clinically grounded, interpretable, and fine-grained metrics. Existing methods either produce coarse overall scores or rely on opaque black-box models, limiting their usefulness in real-world clinical workflows. We introduce RadReason, a novel evaluation framework for radiology reports that not only outputs fine-grained sub-scores across six clinically defined error types, but also produces human-readable justifications that explain the rationale behind each score. Our method builds on Group Relative Policy Optimization and incorporates two key innovations: (1) Sub-score Dynamic Weighting, which adaptively prioritizes clinically challenging error types based on live F1 statistics; and (2) Majority-Guided Advantage Scaling, which adjusts policy gradient updates based on prompt difficulty derived from sub-score agreement. Together, these components enable more stable optimization and better alignment with expert clinical judgment. Experiments on the ReXVal benchmark show that RadReason surpasses all prior offline metrics and achieves parity with GPT-4-based evaluations, while remaining explainable, cost-efficient, and suitable for clinical deployment. Code will be released upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RadReasonï¼Œä¸€ä¸ªç”¨äºæ”¾å°„æŠ¥å‘Šè¯„ä»·çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŒ‡æ ‡ç¼ºä¹ä¸´åºŠåŸºç¡€ã€å¯è§£é‡Šæ€§å·®åŠç²’åº¦ç²—ç³™ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½é’ˆå¯¹å…­ç§ä¸´åºŠå®šä¹‰çš„é”™è¯¯ç±»å‹è¾“å‡ºç»†ç²’åº¦çš„ Sub-scoresï¼Œè¿˜èƒ½æä¾›äººç±»å¯è¯»çš„æ¨ç†ä¾æ®ä»¥è§£é‡Šå¾—åˆ†é€»è¾‘ã€‚RadReason åŸºäº Group Relative Policy Optimization å¼€å‘ï¼Œå¹¶å¼•å…¥äº† Sub-score Dynamic Weighting å’Œ Majority-Guided Advantage Scaling ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´é”™è¯¯ç±»å‹æƒé‡å’Œä¼˜åŒ–ä¼˜åŠ¿ç¼©æ”¾ï¼Œå®ç°äº†æ›´ç¨³å®šçš„æ¨¡å‹ä¼˜åŒ–åŠæ›´å¥½çš„ä¸“å®¶ä¸´åºŠåˆ¤æ–­å¯¹é½ã€‚åœ¨ ReXVal åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRadReason çš„æ€§èƒ½è¶…è¶Šäº†æ‰€æœ‰å…ˆå‰çš„ç¦»çº¿æŒ‡æ ‡ï¼Œå¹¶è¾¾åˆ°äº†ä¸åŸºäº GPT-4 è¯„ä¼°ç›¸å½“çš„æ°´å¹³ã€‚è¯¥æ–¹æ³•å…¼å…·é«˜å¯è§£é‡Šæ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œä¸ºæ”¾å°„æŠ¥å‘Šè¯„ä¼°åœ¨å®é™…ä¸´åºŠå·¥ä½œæµä¸­çš„éƒ¨ç½²å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15464v1",
      "published_date": "2025-08-21 11:34:30 UTC",
      "updated_date": "2025-08-21 11:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:11:58.895283+00:00"
    },
    {
      "arxiv_id": "2508.15876v1",
      "title": "DeepMEL: A Multi-Agent Collaboration Framework for Multimodal Entity Linking",
      "title_zh": "DeepMELï¼šé¢å‘å¤šæ¨¡æ€å®ä½“é“¾æ¥çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶",
      "authors": [
        "Fang Wang",
        "Tianwei Yan",
        "Zonghao Yang",
        "Minghao Hu",
        "Jun Zhang",
        "Zhunchen Luo",
        "Xiaoying Bai"
      ],
      "abstract": "Multimodal Entity Linking (MEL) aims to associate textual and visual mentions with entities in a multimodal knowledge graph. Despite its importance, current methods face challenges such as incomplete contextual information, coarse cross-modal fusion, and the difficulty of jointly large language models (LLMs) and large visual models (LVMs). To address these issues, we propose DeepMEL, a novel framework based on multi-agent collaborative reasoning, which achieves efficient alignment and disambiguation of textual and visual modalities through a role-specialized division strategy. DeepMEL integrates four specialized agents, namely Modal-Fuser, Candidate-Adapter, Entity-Clozer and Role-Orchestrator, to complete end-to-end cross-modal linking through specialized roles and dynamic coordination. DeepMEL adopts a dual-modal alignment path, and combines the fine-grained text semantics generated by the LLM with the structured image representation extracted by the LVM, significantly narrowing the modal gap. We design an adaptive iteration strategy, combines tool-based retrieval and semantic reasoning capabilities to dynamically optimize the candidate set and balance recall and precision. DeepMEL also unifies MEL tasks into a structured cloze prompt to reduce parsing complexity and enhance semantic comprehension. Extensive experiments on five public benchmark datasets demonstrate that DeepMEL achieves state-of-the-art performance, improving ACC by 1%-57%. Ablation studies verify the effectiveness of all modules.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeepMELï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“åä½œæ¨ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Multimodal Entity Linking (MEL)ä¸­ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å®Œæ•´ã€è·¨æ¨¡æ€èåˆç²—ç³™ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸å¤§å‹è§†è§‰æ¨¡å‹(LVMs)ååŒéš¾çš„é—®é¢˜ã€‚DeepMELé€šè¿‡é›†æˆModal-Fuserã€Candidate-Adapterã€Entity-Clozerå’ŒRole-Orchestratorå››ä¸ªä¸“ä¸šåŒ–æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨è§’è‰²åˆ†å·¥ä¸åŠ¨æ€åè°ƒå®ç°ç«¯åˆ°ç«¯çš„è·¨æ¨¡æ€é“¾æ¥ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒæ¨¡æ€å¯¹é½è·¯å¾„ï¼Œå°†LLMç”Ÿæˆçš„ç»†ç²’åº¦æ–‡æœ¬è¯­ä¹‰ä¸LVMæå–çš„ç»“æ„åŒ–å›¾åƒè¡¨ç¤ºç›¸ç»“åˆï¼Œæ˜¾è‘—ç¼©å°äº†æ¨¡æ€å·®è·ã€‚ç ”ç©¶è¿˜è®¾è®¡äº†è‡ªé€‚åº”è¿­ä»£ç­–ç•¥ï¼Œç»“åˆå·¥å…·æ£€ç´¢ä¸è¯­ä¹‰æ¨ç†åŠ¨æ€ä¼˜åŒ–å€™é€‰é›†ï¼Œå¹¶å°†ä»»åŠ¡ç»Ÿä¸€ä¸ºç»“æ„åŒ–å®Œå½¢å¡«ç©º(cloze prompt)æç¤ºä»¥é™ä½è§£æå¤æ‚åº¦ã€‚å®éªŒè¯æ˜ï¼ŒDeepMELåœ¨äº”ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°State-of-the-artæ€§èƒ½ï¼Œå‡†ç¡®ç‡(ACC)æå‡äº†1%-57%ï¼Œå……åˆ†éªŒè¯äº†å¤šæ™ºèƒ½ä½“åä½œåœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€å®ä½“é“¾æ¥ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15876v1",
      "published_date": "2025-08-21 11:24:26 UTC",
      "updated_date": "2025-08-21 11:24:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:01.491261+00:00"
    },
    {
      "arxiv_id": "2508.20117v2",
      "title": "Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?",
      "title_zh": "äººå·¥æ™ºèƒ½æ˜¯å¦æ­£åœ¨é‡å¡‘å›½é™…åœ°å­¦å­¦æœ¯å…±åŒä½“çš„æ ¼å±€ï¼Ÿ",
      "authors": [
        "Liang Li",
        "Yuntian Li",
        "Wenxin Zhao",
        "Shan Ye",
        "Yun Lu"
      ],
      "abstract": "Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æ–‡çŒ®è®¡é‡åˆ†æ(bibliometric analysis)å’Œä¸»é¢˜å»ºæ¨¡(topic modeling)ï¼Œç³»ç»Ÿè¯„ä¼°äº†äººå·¥æ™ºèƒ½(AI)å¯¹åœ°çƒç§‘å­¦ç ”ç©¶åŠå›½é™…å­¦æœ¯æ ¼å±€çš„è½¬å‹ä½œç”¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿‘å¹´æ¥åœ°çƒç§‘å­¦é¢†åŸŸä¸AIç›¸å…³çš„ç§‘ç ”äº§å‡ºå‘ˆç°æ˜¾è‘—å¢é•¿ï¼ŒAIæ­£æˆä¸ºæ¨åŠ¨è¯¥å­¦ç§‘å‘å±•çš„æ ¸å¿ƒåŠ›é‡ã€‚ç‰¹åˆ«æ˜¯åœ¨â€œäººå·¥æ™ºèƒ½é©±åŠ¨çš„ç§‘å­¦ç ”ç©¶â€(AI for Science, AI4S)èŒƒå¼å…´èµ·çš„èƒŒæ™¯ä¸‹ï¼Œæ¥è‡ªå‘å±•ä¸­å›½å®¶çš„åœ°çƒç§‘å­¦å®¶åœ¨å›½é™…å­¦æœ¯ç•Œçš„å¯è§åº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼ŒAIçš„åº”ç”¨è¿˜æ˜¾è‘—æ”¹å–„å¹¶ä¼˜åŒ–äº†åœ°çƒç§‘å­¦é¢†åŸŸçš„å›½é™…åˆä½œç½‘ç»œï¼Œä¿ƒè¿›äº†å…¨çƒç§‘ç ”èµ„æºçš„æ•´åˆã€‚æ€»ä½“è€Œè¨€ï¼ŒAIæ­£åœ¨æ·±åº¦é‡å¡‘åœ°çƒç§‘å­¦çš„å›½é™…å­¦æœ¯ç”Ÿæ€ï¼Œä½¿å…¶æœç€æ›´åŠ å¤šå…ƒåŒ–å’Œåä½œåŒ–çš„æ–¹å‘å‘å±•ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.DL",
      "comment": "miscommunication in the authorization process from the first author",
      "pdf_url": "https://arxiv.org/pdf/2508.20117v2",
      "published_date": "2025-08-21 11:17:24 UTC",
      "updated_date": "2025-09-04 06:26:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:09.389738+00:00"
    },
    {
      "arxiv_id": "2508.15451v1",
      "title": "A Solvable Molecular Switch Model for Stable Temporal Information Processing",
      "title_zh": "ç”¨äºç¨³å®šæ—¶åºä¿¡æ¯å¤„ç†çš„å¯è§£åˆ†å­å¼€å…³æ¨¡å‹",
      "authors": [
        "H. I. Nurdin",
        "C. A. Nijhuis"
      ],
      "abstract": "This paper studies an input-driven one-state differential equation model initially developed for an experimentally demonstrated dynamic molecular switch that switches like synapses in the brain do. The linear-in-the-state and nonlinear-in-the-input model is exactly solvable, and it is shown that it also possesses mathematical properties of convergence and fading memory that enable stable processing of time-varying inputs by nonlinear dynamical systems. Thus, the model exhibits the co-existence of biologically-inspired behavior and desirable mathematical properties for stable learning on sequential data. The results give theoretical support for the use of the dynamic molecular switches as computational units in deep cascaded/layered feedforward and recurrent architectures as well as other more general structures for neuromorphic computing. They could also inspire more general exactly solvable models that can be fitted to emulate arbitrary physical devices which can mimic brain-inspired behaviour and perform stable computation on input signals.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸€ç§ç”¨äºåŠ¨æ€åˆ†å­å¼€å…³(molecular switch)çš„è¾“å…¥é©±åŠ¨å•çŠ¶æ€å¾®åˆ†æ–¹ç¨‹æ¨¡å‹ï¼Œè¯¥å¼€å…³æ—¨åœ¨æ¨¡æ‹Ÿå¤§è„‘çªè§¦çš„åˆ‡æ¢æœºåˆ¶ã€‚è¯¥æ¨¡å‹å…·æœ‰çŠ¶æ€çº¿æ€§(linear-in-the-state)å’Œè¾“å…¥éçº¿æ€§(nonlinear-in-the-input)çš„ç‰¹å¾ï¼Œä¸”åœ¨æ•°å­¦ä¸Šæ˜¯ç²¾ç¡®å¯è§£(exactly solvable)çš„ã€‚ç ”ç©¶è¯æ˜è¯¥æ¨¡å‹å…·å¤‡æ”¶æ•›æ€§(convergence)å’Œæ·¡å¿˜è®°å¿†(fading memory)ç‰¹æ€§ï¼Œèƒ½å¤Ÿä½¿éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿç¨³å®šåœ°å¤„ç†æ—¶å˜è¾“å…¥ä¿¡å·ã€‚è¿™ç§ç»“åˆç”Ÿç‰©å¯å‘è¡Œä¸ºä¸ç¨³å®šå­¦ä¹ æ•°å­¦ç‰¹æ€§çš„è®¾è®¡ï¼Œä¸ºå°†åŠ¨æ€åˆ†å­å¼€å…³ä½œä¸ºæ·±åº¦å‰é¦ˆæˆ–å¾ªç¯ç¥ç»ç½‘ç»œ(recurrent architectures)ç­‰ç¥ç»å½¢æ€è®¡ç®—(neuromorphic computing)æ¶æ„ä¸­çš„è®¡ç®—å•å…ƒæä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚æ­¤å¤–ï¼Œè¯¥æˆæœè¿˜å¯èƒ½å¯å‘æ›´å¤šé€šç”¨çš„ç²¾ç¡®å¯è§£æ¨¡å‹ï¼Œç”¨äºæ¨¡æ‹Ÿèƒ½å¤Ÿæ‰§è¡Œç¨³å®šè®¡ç®—å¹¶å…·æœ‰ç±»è„‘è¡Œä¸ºçš„å„ç±»ç‰©ç†è®¾å¤‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 6 figures, submitted for publication. Comments are welcome",
      "pdf_url": "https://arxiv.org/pdf/2508.15451v1",
      "published_date": "2025-08-21 11:13:56 UTC",
      "updated_date": "2025-08-21 11:13:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:11.688656+00:00"
    },
    {
      "arxiv_id": "2508.15449v1",
      "title": "Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection",
      "title_zh": "åŸºäºèœ•å˜è¡¨ç¤ºæŠ•å½±çš„å¤§è¯­è¨€æ¨¡å‹æœ‰å®³ä¿¡æ¯å¯é é—å¿˜",
      "authors": [
        "Chengcan Wu",
        "Zeming Wei",
        "Huanran Chen",
        "Yinpeng Dong",
        "Meng Sun"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance in various domains and tasks, concerns about their safety are becoming increasingly severe. In particular, since models may store unsafe knowledge internally, machine unlearning has emerged as a representative paradigm to ensure model safety. Existing approaches employ various training techniques, such as gradient ascent and negative preference optimization, in attempts to eliminate the influence of undesired data on target models. However, these methods merely suppress the activation of undesired data through parametric training without completely eradicating its informational traces within the model. This fundamental limitation makes it difficult to achieve effective continuous unlearning, rendering these methods vulnerable to relearning attacks. To overcome these challenges, we propose a Metamorphosis Representation Projection (MRP) approach that pioneers the application of irreversible projection properties to machine unlearning. By implementing projective transformations in the hidden state space of specific network layers, our method effectively eliminates harmful information while preserving useful knowledge. Experimental results demonstrate that our approach enables effective continuous unlearning and successfully defends against relearning attacks, achieving state-of-the-art performance in unlearning effectiveness while preserving natural performance. Our code is available in https://github.com/ChengcanWu/MRP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨å¯èƒ½å­˜å‚¨çš„ä¸å®‰å…¨çŸ¥è¯†ï¼Œæå‡ºäº†Metamorphosis Representation Projection (MRP)æ–¹æ³•ï¼Œä»¥æå‡æœºå™¨é—å¿˜ï¼ˆmachine unlearningï¼‰çš„å¯é æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¦‚æ¢¯åº¦ä¸Šå‡ï¼ˆgradient ascentï¼‰å’Œè´Ÿå‘åå¥½ä¼˜åŒ–ï¼ˆnegative preference optimizationï¼‰ä»…èƒ½æŠ‘åˆ¶æ¿€æ´»è€Œæ— æ³•å½»åº•æ ¹é™¤ä¿¡æ¯ç—•è¿¹ï¼Œå¯¼è‡´å…¶åœ¨åº”å¯¹é‡å­¦æ”»å‡»ï¼ˆrelearning attacksï¼‰æ—¶è¡¨ç°è„†å¼±çš„å±€é™æ€§ï¼ŒMRPé¦–æ¬¡å°†ä¸å¯é€†æŠ•å½±æ€§è´¨å¼•å…¥é—å¿˜ä»»åŠ¡ã€‚é€šè¿‡åœ¨ç‰¹å®šç½‘ç»œå±‚çš„éšè—çŠ¶æ€ç©ºé—´ï¼ˆhidden state spaceï¼‰å®æ–½æŠ•å½±å˜æ¢ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨æ¶ˆé™¤æœ‰å®³ä¿¡æ¯çš„åŒæ—¶ç²¾å‡†ä¿ç•™æœ‰ç”¨çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMRPåœ¨å®ç°æœ‰æ•ˆæŒç»­é—å¿˜ï¼ˆcontinuous unlearningï¼‰å’ŒæŠµå¾¡é‡å­¦æ”»å‡»æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œä¸”èƒ½å¤Ÿå¾ˆå¥½åœ°ä¿æŒæ¨¡å‹çš„è‡ªç„¶ç”Ÿæˆè¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 9 figures, Under review as a full paper at AAAI 2026. A preliminary version is under review at the NeurIPS 2025 Workshop on Reliable ML from Unreliable Data",
      "pdf_url": "https://arxiv.org/pdf/2508.15449v1",
      "published_date": "2025-08-21 11:12:09 UTC",
      "updated_date": "2025-08-21 11:12:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:14.095635+00:00"
    },
    {
      "arxiv_id": "2508.15447v2",
      "title": "From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence",
      "title_zh": "ä»æ¯”ç‰¹åˆ°è‘£äº‹ä¼šï¼šé¢å‘å“è¶Šå•†ä¸šçš„å‰æ²¿å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Zihao Wang",
        "Junming Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising potential in business applications, particularly in enterprise decision support and strategic planning, yet current approaches often struggle to reconcile intricate operational analyses with overarching strategic goals across diverse market environments, leading to fragmented workflows and reduced collaboration across organizational levels. This paper introduces BusiAgent, a novel multi-agent framework leveraging LLMs for advanced decision-making in complex corporate environments. BusiAgent integrates three core innovations: an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a generalized entropy measure to optimize collaborative efficiency, and a multi-level Stackelberg game to handle hierarchical decision processes. Additionally, contextual Thompson sampling is employed for prompt optimization, supported by a comprehensive quality assurance system to mitigate errors. Extensive empirical evaluations across diverse business scenarios validate BusiAgent's efficacy, demonstrating its capacity to generate coherent, client-focused solutions that smoothly integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction. By fusing cutting-edge AI technologies with deep business insights, BusiAgent marks a substantial step forward in AI-driven enterprise decision-making, empowering organizations to navigate complex business landscapes more effectively.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†BusiAgentï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤æ‚ä¼ä¸šç¯å¢ƒå†³ç­–è®¾è®¡çš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶(multi-agent framework)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åè°ƒç»†èŠ‚è¿è¥åˆ†æä¸å®è§‚æˆ˜ç•¥ç›®æ ‡æ—¶é¢ä¸´çš„åä½œæ•ˆç‡ä½ä¸‹å’Œå·¥ä½œæµç¢ç‰‡åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šç”¨äºåŠ¨æ€æ™ºèƒ½ä½“å»ºæ¨¡çš„æ‰©å±•è¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Extended CTMDP)ã€ä¼˜åŒ–åä½œæ•ˆç‡çš„å¹¿ä¹‰ç†µåº¦é‡(Generalized entropy measure)ï¼Œä»¥åŠå¤„ç†å±‚çº§å†³ç­–è¿‡ç¨‹çš„å¤šçº§Stackelbergåšå¼ˆ(Multi-level Stackelberg game)ã€‚æ­¤å¤–ï¼ŒBusiAgentè¿˜é‡‡ç”¨ä¸Šä¸‹æ–‡æ±¤æ™®æ£®é‡‡æ ·(Contextual Thompson sampling)ä¼˜åŒ–æç¤ºè¯ï¼Œå¹¶ç»“åˆè´¨é‡ä¿è¯ç³»ç»Ÿæ¥é™ä½é”™è¯¯ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ä¸šåŠ¡åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿è´¯ä¸”ä»¥å®¢æˆ·ä¸ºä¸­å¿ƒçš„è§£å†³æ–¹æ¡ˆï¼Œå®ç°äº†ç»†ç²’åº¦æ´å¯Ÿä¸é«˜å±‚æˆ˜ç•¥çš„å¹³æ»‘æ•´åˆã€‚è¯¥ç ”ç©¶ä¸ºAIé©±åŠ¨çš„ä¼ä¸šçº§å†³ç­–æ”¯æŒæä¾›äº†é‡è¦å·¥å…·ï¼ŒåŠ©åŠ›ç»„ç»‡æ›´æœ‰æ•ˆåœ°åº”å¯¹å¤æ‚çš„å•†ä¸šç¯å¢ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15447v2",
      "published_date": "2025-08-21 11:08:53 UTC",
      "updated_date": "2025-12-11 19:43:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:18.888763+00:00"
    },
    {
      "arxiv_id": "2508.15442v3",
      "title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets",
      "title_zh": "åŸºäº GFlowNets åˆ†å¸ƒå¯¹é½çš„è¯­è¨€æ¨¡å‹æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹å¹»è§‰ç¼“è§£æ–¹æ³•",
      "authors": [
        "Chenlin Liu",
        "Minghui Fang",
        "Patrick Zhang",
        "Wei Zhou",
        "Jie Gao",
        "Jiqing Han"
      ],
      "abstract": "Language Model (LM)-based Text-to-Speech (TTS) systems often generate hallucinated speech that deviates from input text. Existing mitigation strategies either demand excessive training resources or introduce significant inference latency. In this paper, we propose GFlOwNet-guided distribution AlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates hallucinations without relying on massive resources or inference cost. Specifically, we first conduct an uncertainty analysis, revealing a strong positive correlation between hallucination and model uncertainty. Based on this, we reformulate TTS generation as a trajectory flow optimization problem and introduce an enhanced Subtrajectory Balance objective together with a sharpened internal reward as target distribution. We further integrate reward temperature decay and learning rate optimization for stability and performance balance. Extensive experiments show that GOAT reduce over 50% character error rates on challenging test cases and lowering uncertainty by up to 58%, demonstrating its strong generalization ability and effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè¯­è¨€æ¨¡å‹(LM)çš„æ–‡æœ¬è½¬è¯­éŸ³(TTS)ç³»ç»Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰ç°è±¡ï¼Œæå‡ºäº†åä¸ºGOATçš„åˆ†å¸ƒå¯¹é½æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒèµ„æºå’Œæ¨ç†å»¶è¿Ÿæ–¹é¢çš„å±€é™æ€§ï¼ŒGOATåˆ©ç”¨GFlowNetsé€šè¿‡åè®­ç»ƒæ–¹å¼å®ç°å¹»è§‰ç¼“è§£ã€‚ç ”ç©¶é€šè¿‡ä¸ç¡®å®šæ€§åˆ†æå‘ç°å¹»è§‰ä¸æ¨¡å‹ä¸ç¡®å®šæ€§é«˜åº¦æ­£ç›¸å…³ï¼Œè¿›è€Œå°†TTSç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè½¨è¿¹æµä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥å¢å¼ºçš„Subtrajectory Balanceç›®æ ‡å‡½æ•°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡å¥–åŠ±æ¸©åº¦è¡°å‡å’Œå­¦ä¹ ç‡ä¼˜åŒ–è¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒGOATåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•ä¸­å°†å­—ç¬¦é”™è¯¯ç‡(CER)é™ä½äº†50%ä»¥ä¸Šï¼Œä¸ç¡®å®šæ€§é™ä½äº†58%ï¼Œåœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„å‰æä¸‹å±•ç¤ºå‡ºå“è¶Šçš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to EMNLP 2025 Main Conference (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2508.15442v3",
      "published_date": "2025-08-21 11:04:33 UTC",
      "updated_date": "2025-09-05 16:13:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:23.087791+00:00"
    },
    {
      "arxiv_id": "2508.15437v2",
      "title": "Test-time Corpus Feedback: From Retrieval to RAG",
      "title_zh": "æµ‹è¯•æ—¶è¯­æ–™åº“åé¦ˆï¼šä»æ£€ç´¢åˆ° RAG",
      "authors": [
        "Mandeep Rathee",
        "V Venktesh",
        "Sean MacAvaney",
        "Avishek Anand"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a standard framework for knowledge-intensive NLP tasks, combining large language models (LLMs) with document retrieval from external corpora. Despite its widespread use, most RAG pipelines continue to treat retrieval and reasoning as isolated components, retrieving documents once and then generating answers without further interaction. This static design often limits performance on complex tasks that require iterative evidence gathering or high-precision retrieval. Recent work in both the information retrieval (IR) and NLP communities has begun to close this gap by introducing adaptive retrieval and ranking methods that incorporate feedback. In this survey, we present a structured overview of advanced retrieval and ranking mechanisms that integrate such feedback. We categorize feedback signals based on their source and role in improving the query, retrieved context, or document pool. By consolidating these developments, we aim to bridge IR and NLP perspectives and highlight retrieval as a dynamic, learnable component of end-to-end RAG systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­æ£€ç´¢ä¸æ¨ç†ç»„ä»¶ç›¸äº’å­¤ç«‹ã€ç¼ºä¹äº¤äº’çš„ç°çŠ¶è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä¼ ç»Ÿçš„é™æ€RAGè®¾è®¡ç”±äºç¼ºä¹è¿­ä»£è¯æ®æ”¶é›†èƒ½åŠ›ï¼Œåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æˆ–é«˜ç²¾åº¦æ£€ç´¢éœ€æ±‚æ—¶å¾€å¾€è¡¨ç°å—é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ä»½å…³äºæ•´åˆåé¦ˆæœºåˆ¶çš„å…ˆè¿›æ£€ç´¢ä¸é‡æ’åº(Ranking)æœºåˆ¶çš„ç»“æ„åŒ–ç»¼è¿°ã€‚è¯¥ç»¼è¿°æ ¹æ®åé¦ˆä¿¡å·çš„æ¥æºåŠå…¶åœ¨æ”¹è¿›æŸ¥è¯¢(Query)ã€æ£€ç´¢ä¸Šä¸‹æ–‡æˆ–æ–‡æ¡£æ± ä¸­çš„ä½œç”¨ï¼Œå¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ã€‚é€šè¿‡æ•´åˆä¿¡æ¯æ£€ç´¢(IR)ä¸è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸¤ä¸ªé¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œæœ¬æ–‡æ—¨åœ¨å°†æ£€ç´¢è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºç«¯åˆ°ç«¯RAGç³»ç»Ÿä¸­ä¸€ä¸ªåŠ¨æ€ä¸”å¯å­¦ä¹ çš„å…³é”®ç»„ä»¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "18 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2508.15437v2",
      "published_date": "2025-08-21 10:57:38 UTC",
      "updated_date": "2025-08-24 20:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:40.900361+00:00"
    },
    {
      "arxiv_id": "2508.15875v1",
      "title": "NEAT: Concept driven Neuron Attribution in LLMs",
      "title_zh": "NEATï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­æ¦‚å¿µé©±åŠ¨çš„ç¥ç»å…ƒå½’å› ",
      "authors": [
        "Vivek Hruday Kavuri",
        "Gargi Shroff",
        "Rahul Mishra"
      ],
      "abstract": "Locating neurons that are responsible for final predictions is important for opening the black-box large language models and understanding the inside mechanisms. Previous studies have tried to find mechanisms that operate at the neuron level but these methods fail to represent a concept and there is also scope for further optimization of compute required. In this paper, with the help of concept vectors, we propose a method for locating significant neurons that are responsible for representing certain concepts and term those neurons as concept neurons. If the number of neurons is n and the number of examples is m, we reduce the number of forward passes required from O(n*m) to just O(n) compared to the previous works and hence optimizing the time and computation required over previous works. We also compare our method with several baselines and previous methods and our results demonstrate better performance than most of the methods and are more optimal when compared to the state-of-the-art method. We, as part of our ablation studies, also try to optimize the search for the concept neurons by involving clustering methods. Finally, we apply our methods to find, turn off the neurons that we find, and analyze its implications in parts of hate speech and bias in LLMs, and we also evaluate our bias part in terms of Indian context. Our methodology, analysis and explanations facilitate understating of neuron-level responsibility for more broader and human-like concepts and also lay a path for future research in this direction of finding concept neurons and intervening them.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NEATï¼Œä¸€ç§åŸºäºæ¦‚å¿µé©±åŠ¨çš„ç¥ç»å…ƒå½’å› (Concept driven Neuron Attribution)æ–¹æ³•ï¼Œæ—¨åœ¨å‡†ç¡®å®šä½å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­è´Ÿè´£è¡¨ç¤ºç‰¹å®šæ¦‚å¿µçš„ç¥ç»å…ƒã€‚ä¸ä»¥å¾€åœ¨ç¥ç»å…ƒå±‚é¢è¿è¡Œä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ–¹æ³•ç›¸æ¯”ï¼ŒNEAT åˆ©ç”¨æ¦‚å¿µå‘é‡(concept vectors)æ¥è¯†åˆ«è¿™äº›è¢«å®šä¹‰ä¸ºæ¦‚å¿µç¥ç»å…ƒ(concept neurons)çš„å…³é”®å•å…ƒã€‚åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢ï¼Œè¯¥æ–¹æ³•å°†æ‰€éœ€çš„æ¨¡å‹å‰å‘ä¼ é€’(forward passes)æ¬¡æ•°ä» O(n*m) æ˜¾è‘—é™ä½è‡³ O(n)ï¼Œå¤§å¹…ä¼˜åŒ–äº†æ—¶é—´å’Œè®¡ç®—å¼€é”€ã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡èšç±»æ–¹æ³•(clustering methods)ä¼˜åŒ–äº†æ¦‚å¿µç¥ç»å…ƒçš„æœç´¢è¿‡ç¨‹ï¼Œå¹¶å®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºå¤šç§åŸºçº¿æ¨¡å‹ï¼Œä¸”æ¯”ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯(state-of-the-art)æ›´å…·ä¼˜åŒ–æ€§ã€‚ä½œè€…è¿˜å°†è¯¥æ–¹æ³•åº”ç”¨äºè¯†åˆ«å¹¶å…³é—­ä¸ä»‡æ¨è¨€è®º(hate speech)å’Œåè§(bias)ç›¸å…³çš„ç¥ç»å…ƒï¼Œå¹¶åœ¨å°åº¦è¯­å¢ƒä¸‹è¯„ä¼°äº†å…¶åœ¨æ¶ˆé™¤åè§æ–¹é¢çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£ç¥ç»å…ƒå±‚é¢çš„èŒè´£æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥åœ¨ LLMs ä¸­è¿›è¡Œç¥ç»å…ƒå¹²é¢„å’Œæœºåˆ¶ç†è§£çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15875v1",
      "published_date": "2025-08-21 10:36:00 UTC",
      "updated_date": "2025-08-21 10:36:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:43.687866+00:00"
    },
    {
      "arxiv_id": "2508.15432v3",
      "title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data",
      "title_zh": "SyGraï¼šé¢å‘åˆæˆæ•°æ®å¯æ‰©å±•ç”Ÿæˆã€è´¨é‡æ‰“æ ‡ä¸ç®¡ç†çš„ç»Ÿä¸€å›¾æ¡†æ¶",
      "authors": [
        "Bidyapati Pradhan",
        "Surajit Dasgupta",
        "Amit Kumar Saha",
        "Omkar Anustoop",
        "Sriram Puttagunta",
        "Vipul Mittal",
        "Gopal Sarda"
      ],
      "abstract": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SyGraï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºäºå›¾(Graph-Based)çš„æ¡†æ¶ï¼Œç”¨äºåˆæˆæ•°æ®çš„å¤§è§„æ¨¡ç”Ÿæˆã€è´¨é‡æ‰“æ ‡å’Œç®¡ç†ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç›‘ç£å¾®è°ƒ(SFT)å’Œå¯¹é½ä»»åŠ¡(å¦‚DPO)ä¸­å¯¹é«˜è´¨é‡æ•°æ®é›†çš„ä¾èµ–ï¼ŒSyGraé‡‡ç”¨æ¨¡å—åŒ–ä¸”åŸºäºé…ç½®çš„æµæ°´çº¿ï¼Œèƒ½å¤Ÿä»¥æå°‘çš„äººå·¥å¹²é¢„æ¨¡æ‹Ÿå¤æ‚çš„å¯¹è¯æµã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŒé˜¶æ®µè´¨é‡æ‰“æ ‡æœºåˆ¶ï¼Œç»“åˆå¯å‘å¼è§„åˆ™(heuristic rules)å’ŒåŸºäºLLMçš„è¯„ä¼°ï¼Œè‡ªåŠ¨å¯¹ä»OASSTæ ¼å¼æå–çš„å¯¹è¯æ•°æ®è¿›è¡Œè¿‡æ»¤å’Œè¯„åˆ†ã€‚æ‰€ç”Ÿæˆçš„æ•°æ®é›†é‡‡ç”¨çµæ´»çš„æ¶æ„ï¼Œæ”¯æŒSFTå’ŒDPOç­‰å¤šç§è®­ç»ƒèŒƒå¼ï¼Œå¯å®ç°è®­ç»ƒå·¥ä½œæµçš„æ— ç¼é›†æˆã€‚è¿™äº›åˆ›æ–°ä¸ºå¤§è§„æ¨¡ç”Ÿæˆå’Œç®¡ç†é«˜ä¿çœŸå¯¹è¯æ•°æ®æä¾›äº†ç¨³å¥æ–¹æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†LLMè®­ç»ƒæµæ°´çº¿ä¸­çš„æ•°æ®å‡†å¤‡å¼€é”€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15432v3",
      "published_date": "2025-08-21 10:35:41 UTC",
      "updated_date": "2025-12-11 16:38:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:53.398390+00:00"
    },
    {
      "arxiv_id": "2508.15423v1",
      "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks",
      "title_zh": "é¢å‘ä»£ç ç†è§£ä»»åŠ¡çš„çŸ¥è¯†è’¸é¦å®è¯ç ”ç©¶",
      "authors": [
        "Ruiqi Wang",
        "Zezhou Yang",
        "Cuiyun Gao",
        "Xin Xia",
        "Qing Liao"
      ],
      "abstract": "Pre-trained language models (PLMs) have emerged as powerful tools for code understanding. However, deploying these PLMs in large-scale applications faces practical challenges due to their computational intensity and inference latency. Knowledge distillation (KD), a promising model compression and acceleration technique, addresses these limitations by transferring knowledge from large teacher models to compact student models, enabling efficient inference while preserving most of the teacher models' capabilities. While this technique has shown remarkable success in natural language processing and computer vision domains, its potential for code understanding tasks remains largely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of KD in code understanding tasks. Our study encompasses two popular types of KD methods, i.e., logit-based and feature-based KD methods, experimenting across eight student models and two teacher PLMs from different domains on three downstream tasks. The experimental results indicate that KD consistently offers notable performance boosts across student models with different sizes compared with standard fine-tuning. Notably, code-specific PLM demonstrates better effectiveness as the teacher model. Among all KD methods, the latest feature-based KD methods exhibit superior performance, enabling student models to retain up to 98% teacher performance with merely 5% parameters. Regarding student architecture, our experiments reveal that similarity with teacher architecture does not necessarily lead to better performance. We further discuss the efficiency and behaviors in the KD process and inference, summarize the implications of findings, and identify promising future directions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(PLMs)åœ¨ä»£ç ç†è§£ä»»åŠ¡ä¸­é¢ä¸´çš„è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿ŸæŒ‘æˆ˜ï¼Œç³»ç»Ÿåœ°è°ƒæŸ¥äº†çŸ¥è¯†è’¸é¦(Knowledge Distillation, KD)çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹é€»è¾‘å€¼(logit-based)å’Œç‰¹å¾(feature-based)ä¸¤ç±»KDæ–¹æ³•åœ¨å…«ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹ä¸Šçš„å®éªŒï¼Œç ”ç©¶è¯å®KDç›¸è¾ƒäºæ ‡å‡†å¾®è°ƒ(fine-tuning)èƒ½æ˜¾è‘—æå‡ä¸åŒè§„æ¨¡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒå‘ç°ä»£ç ç‰¹å®š(code-specific)çš„PLMä½œä¸ºæ•™å¸ˆæ¨¡å‹æ•ˆæœæ›´ä½³ï¼Œä¸”æœ€æ–°çš„åŸºäºç‰¹å¾çš„KDæ–¹æ³•è¡¨ç°æœ€ä¸ºä¼˜è¶Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç‰¹å®šçš„KDæ–¹æ³•èƒ½ä½¿å­¦ç”Ÿæ¨¡å‹ä»…åˆ©ç”¨5%çš„å‚æ•°å³å¯ä¿ç•™æ•™å¸ˆæ¨¡å‹é«˜è¾¾98%çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†æ•™å¸ˆä¸å­¦ç”Ÿæ¨¡å‹é—´çš„æ¶æ„ç›¸ä¼¼æ€§å¹¶éæ€§èƒ½æå‡çš„å†³å®šæ€§å› ç´ ã€‚è¯¥å®è¯ç ”ç©¶ä¸ºä»£ç ç†è§£é¢†åŸŸçš„é«˜æ•ˆæ¨¡å‹éƒ¨ç½²æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¹¶æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ICSE 2026 (Cycle 1)",
      "pdf_url": "https://arxiv.org/pdf/2508.15423v1",
      "published_date": "2025-08-21 10:24:48 UTC",
      "updated_date": "2025-08-21 10:24:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:58.953355+00:00"
    },
    {
      "arxiv_id": "2508.15874v2",
      "title": "Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning",
      "title_zh": "Spatial Policyï¼šåˆ©ç”¨ç©ºé—´æ„ŸçŸ¥å»ºæ¨¡ä¸æ¨ç†å¼•å¯¼è§†è§‰è¿åŠ¨æœºå™¨äººæ“çºµ",
      "authors": [
        "Yijun Liu",
        "Yuwei Liu",
        "Yuan Meng",
        "Jieheng Zhang",
        "Yuwei Zhou",
        "Ye Li",
        "Jiacheng Jiang",
        "Kangye Ji",
        "Shijia Ge",
        "Zhi Wang",
        "Wenwu Zhu"
      ],
      "abstract": "Vision-centric hierarchical embodied models have demonstrated strong potential. However, existing methods lack spatial awareness capabilities, limiting their effectiveness in bridging visual plans to actionable control in complex environments. To address this problem, we propose Spatial Policy (SP), a unified spatial-aware visuomotor robotic manipulation framework via explicit spatial modeling and reasoning. Specifically, we first design a spatial-conditioned embodied video generation module to model spatially guided predictions through the spatial plan table. Then, we propose a flow-based action prediction module to infer executable actions with coordination. Finally, we propose a spatial reasoning feedback policy to refine the spatial plan table via dual-stage replanning. Extensive experiments show that SP substantially outperforms state-of-the-art baselines, achieving over 33% improvement on Meta-World and over 25% improvement on iTHOR, demonstrating strong effectiveness across 23 embodied control tasks. We additionally evaluate SP in real-world robotic experiments to verify its practical viability. SP enhances the practicality of embodied models for robotic control applications. Code and checkpoints are maintained at https://plantpotatoonmoon.github.io/SpatialPolicy/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Spatial Policy (SP)ï¼Œä¸€ç§ç»Ÿä¸€çš„ç©ºé—´æ„ŸçŸ¥è§†è§‰è¿åŠ¨æœºå™¨äººæ“çºµæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰ä¸­å¿ƒåŒ–å…·èº«æ¨¡å‹ç”±äºç¼ºä¹Spatial Awarenessè€Œéš¾ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­å°†è§†è§‰è§„åˆ’è½¬åŒ–ä¸ºæœ‰æ•ˆæ§åˆ¶çš„é—®é¢˜ã€‚SPé€šè¿‡æ˜¾å¼çš„ç©ºé—´å»ºæ¨¡ä¸æ¨ç†ï¼Œé¦–å…ˆè®¾è®¡äº†Spatial-conditioned embodied video generationæ¨¡å—ï¼Œåˆ©ç”¨Spatial plan tableç”Ÿæˆç©ºé—´å¼•å¯¼çš„é¢„æµ‹ã€‚æ¥ç€ï¼Œæ¡†æ¶é‡‡ç”¨Flow-based action predictionæ¨¡å—æ¥æ¨æ–­åè°ƒçš„å¯æ‰§è¡ŒåŠ¨ä½œï¼Œå¹¶å¼•å…¥Spatial reasoning feedbackç­–ç•¥é€šè¿‡åŒé˜¶æ®µé‡è§„åˆ’æ¥ç²¾ç»†åŒ–ç©ºé—´è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPåœ¨Meta-Worldå’ŒiTHORçš„23é¡¹å…·èº«æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œæ€§èƒ½æå‡åˆ†åˆ«è¶…è¿‡33%å’Œ25%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡çœŸå®ä¸–ç•Œçš„æœºå™¨äººå®éªŒéªŒè¯äº†SPçš„å®ç”¨å¯è¡Œæ€§ï¼Œä¸ºæå‡å…·èº«æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15874v2",
      "published_date": "2025-08-21 10:24:18 UTC",
      "updated_date": "2025-11-18 08:39:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:12:57.682664+00:00"
    },
    {
      "arxiv_id": "2508.15418v1",
      "title": "LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model",
      "title_zh": "LLaSOï¼šå¤§è¯­è¨€ä¸è¯­éŸ³æ¨¡å‹å¯å¤ç°ç ”ç©¶çš„åŸºç¡€æ¡†æ¶",
      "authors": [
        "Yirong Sun",
        "Yizhong Geng",
        "Peidong Wei",
        "Yanjun Chen",
        "Jinghan Yang",
        "Rongfei Chen",
        "Wei Zhang",
        "Xiaoyu Shen"
      ],
      "abstract": "The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹(Large Speech-Language Models, LSLMs)é¢†åŸŸå­˜åœ¨çš„æ¶æ„ç¢ç‰‡åŒ–ã€ç¼ºä¹é€æ˜åº¦ä»¥åŠç ”ç©¶éš¾ä»¥å¤ç°ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªå®Œå…¨å¼€æºä¸”ç«¯åˆ°ç«¯çš„åŸºç¡€æ¡†æ¶LLaSOã€‚è¯¥æ¡†æ¶ä¸ºå­¦æœ¯ç•Œæä¾›äº†ä¸‰å¤§æ ¸å¿ƒèµ„æºï¼ŒåŒ…æ‹¬åŒ…å«1200ä¸‡å®ä¾‹çš„è¯­éŸ³-æ–‡æœ¬å¯¹é½è¯­æ–™åº“LLaSO-Alignã€æ‹¥æœ‰1350ä¸‡å®ä¾‹çš„å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†LLaSO-Instructï¼Œä»¥åŠç”¨äºæ ‡å‡†åŒ–è¯„ä¼°çš„å¯å¤ç°åŸºå‡†LLaSO-Evalã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº†åŸºäºå…¬å¼€æ•°æ®è®­ç»ƒçš„3.8Bå‚æ•°é‡å‚è€ƒæ¨¡å‹LLaSO-Baseï¼Œè¯¥æ¨¡å‹ä»¥0.72çš„å½’ä¸€åŒ–è¯„åˆ†å»ºç«‹äº†è¶…è¶ŠåŒç±»æ¨¡å‹ä¸”é«˜åº¦å¯å¤ç°çš„å¼ºåŠ›åŸºçº¿ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ‰©å¤§è®­ç»ƒè¦†ç›–èŒƒå›´èƒ½æ˜¾è‘—å¢å¼ºæ€§èƒ½ï¼Œä½†åœ¨æœªè§ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯çº¯éŸ³é¢‘åœºæ™¯ï¼‰ä¸­ä»å­˜åœ¨æ˜æ˜¾çš„æ³›åŒ–å·®è·ã€‚é€šè¿‡æ•´åˆå¹¶å¼€æ”¾å®Œæ•´çš„æ•°æ®æ ˆã€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ï¼ŒLLaSOä¸ºLSLMsç ”ç©¶ç¡®ç«‹äº†ä¸€å¥—åŸºç¡€æ€§çš„å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€ç ”ç©¶åŠªåŠ›å¹¶åŠ é€Ÿç¤¾åŒºé©±åŠ¨çš„æŠ€æœ¯è¿›æ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15418v1",
      "published_date": "2025-08-21 10:20:00 UTC",
      "updated_date": "2025-08-21 10:20:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:06.751144+00:00"
    },
    {
      "arxiv_id": "2509.09691v1",
      "title": "Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores",
      "title_zh": "åŸºäºå…±æŒ¯æ£€ç´¢çš„æ³¢åŠ¨è¯­ä¹‰è®°å¿†ï¼šä¸€ç§ç›¸ä½æ„ŸçŸ¥çš„å‘é‡åµŒå…¥å­˜å‚¨æ›¿ä»£æ–¹æ¡ˆ",
      "authors": [
        "Aleksandr Listopad"
      ],
      "abstract": "Conventional vector-based memory systems rely on cosine or inner product similarity within real-valued embedding spaces. While computationally efficient, such approaches are inherently phase-insensitive and limited in their ability to capture resonance phenomena crucial for meaning representation. We propose Wave-Based Semantic Memory, a novel framework that models knowledge as wave patterns $Ïˆ(x) = A(x) e^{iÏ†(x)}$ and retrieves it through resonance-based interference. This approach preserves both amplitude and phase information, enabling more expressive and robust semantic similarity. We demonstrate that resonance-based retrieval achieves higher discriminative power in cases where vector methods fail, including phase shifts, negations, and compositional queries. Our implementation, ResonanceDB, shows scalability to millions of patterns with millisecond latency, positioning wave-based memory as a viable alternative to vector stores for AGI-oriented reasoning and knowledge representation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Wave-Based Semantic Memoryï¼ˆåŸºäºæ³¢çš„è¯­ä¹‰è®°å¿†ï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºå‘é‡çš„å­˜å‚¨ç³»ç»Ÿåœ¨ç›¸ä½ä¸æ•æ„Ÿå’Œå…±æŒ¯ç°è±¡æ•æ‰èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†çŸ¥è¯†å»ºæ¨¡ä¸ºæ³¢æ¨¡å¼ $\\psi(x) = A(x) e^{i\\phi(x)}$ï¼Œå¹¶é€šè¿‡ Resonance-Based Retrievalï¼ˆåŸºäºå…±æŒ¯çš„æ£€ç´¢ï¼‰æœºåˆ¶åˆ©ç”¨å¹²æ¶‰ç°è±¡å®ç°ä¿¡æ¯æå–ã€‚è¿™ç§æ–¹æ³•åŒæ—¶ä¿ç•™äº†æŒ¯å¹…å’Œç›¸ä½ä¿¡æ¯ï¼Œä»è€Œå®ç°äº†æ¯”ä¼ ç»Ÿä½™å¼¦ç›¸ä¼¼åº¦æ›´å…·è¡¨ç°åŠ›å’Œé²æ£’æ€§çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¡¨ç¤ºã€‚å®éªŒè¯æ˜ï¼Œåœ¨ç›¸ä½åç§»ã€å¦å®šé€»è¾‘å’Œç»„åˆæŸ¥è¯¢ç­‰å‘é‡æ–¹æ³•å®¹æ˜“å¤±æ•ˆçš„åœºæ™¯ä¸­ï¼ŒåŸºäºå…±æŒ¯çš„æ£€ç´¢å…·æœ‰æ›´é«˜çš„è¾¨åˆ«åŠ›ã€‚ä½œè€…å¼€å‘çš„ ResonanceDB åœ¨å®ç°æ•°ç™¾ä¸‡çº§æ¨¡å¼æ‰©å±•çš„åŒæ—¶ä¿æŒäº†æ¯«ç§’çº§å»¶è¿Ÿï¼Œä¸ºé¢å‘ AGI çš„æ¨ç†å’ŒçŸ¥è¯†è¡¨ç¤ºæä¾›äº†ä¸€ç§è¶³ä»¥æ›¿ä»£ Vector Embedding Stores çš„é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "9 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09691v1",
      "published_date": "2025-08-21 10:13:24 UTC",
      "updated_date": "2025-08-21 10:13:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:04.064296+00:00"
    },
    {
      "arxiv_id": "2508.20115v2",
      "title": "Flexible metadata harvesting for ecology using large language models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæ€å­¦çµæ´»å…ƒæ•°æ®é‡‡é›†",
      "authors": [
        "Zehao Lu",
        "Thijs L van der Plas",
        "Parinaz Rashidi",
        "W Daniel Kissling",
        "Ioannis N Athanasiadis"
      ],
      "abstract": "Large, open datasets can accelerate ecological research, particularly by enabling researchers to develop new insights by reusing datasets from multiple sources. However, to find the most suitable datasets to combine and integrate, researchers must navigate diverse ecological and environmental data provider platforms with varying metadata availability and standards. To overcome this obstacle, we have developed a large language model (LLM)-based metadata harvester that flexibly extracts metadata from any dataset's landing page, and converts these to a user-defined, unified format using existing metadata standards. We validate that our tool is able to extract both structured and unstructured metadata with equal accuracy, aided by our LLM post-processing protocol. Furthermore, we utilise LLMs to identify links between datasets, both by calculating embedding similarity and by unifying the formats of extracted metadata to enable rule-based processing. Our tool, which flexibly links the metadata of different datasets, can therefore be used for ontology creation or graph-based queries, for example, to find relevant ecological and environmental datasets in a virtual research environment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæ€å­¦ç ”ç©¶ä¸­è·¨å¹³å°æ•°æ®é›†é‡ç”¨é¢ä¸´çš„å…ƒæ•°æ®(metadata)æ ‡å‡†ä¸ç»Ÿä¸€å’Œè·å–å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„å…ƒæ•°æ®æ•è·å·¥å…·ã€‚è¯¥å·¥å…·èƒ½å¤Ÿçµæ´»åœ°ä»ä»»ä½•æ•°æ®é›†çš„ç€é™†é¡µ(landing page)æå–å…ƒæ•°æ®ï¼Œå¹¶æ ¹æ®ç°æœ‰æ ‡å‡†å°†å…¶è½¬æ¢ä¸ºç”¨æˆ·å®šä¹‰çš„ç»Ÿä¸€æ ¼å¼ã€‚é€šè¿‡LLMåå¤„ç†åè®®ï¼Œè¯¥å·¥å…·èƒ½ä»¥åŒç­‰å‡†ç¡®åº¦å¤„ç†ç»“æ„åŒ–å’Œéç»“æ„åŒ–å…ƒæ•°æ®ï¼Œæ˜¾è‘—æå‡äº†æ•°æ®æå–çš„çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨LLMsè®¡ç®—åµŒå…¥ç›¸ä¼¼åº¦(embedding similarity)å¹¶ç»Ÿä¸€æ ¼å¼ï¼Œä»è€Œæœ‰æ•ˆè¯†åˆ«ä¸åŒæ•°æ®é›†ä¹‹é—´çš„å…³è”ã€‚è¯¥å·¥å…·ä¸ºæ„å»ºæœ¬ä½“(ontology)æˆ–æ‰§è¡ŒåŸºäºå›¾(graph-based)çš„æŸ¥è¯¢æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜åœ¨è™šæ‹Ÿç ”ç©¶ç¯å¢ƒä¸­å¿«é€Ÿå‘ç°å¹¶æ•´åˆç›¸å…³çš„ç”Ÿæ€ä¸ç¯å¢ƒæ•°æ®é›†ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20115v2",
      "published_date": "2025-08-21 10:10:29 UTC",
      "updated_date": "2025-10-06 10:07:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:07.958761+00:00"
    },
    {
      "arxiv_id": "2508.15413v3",
      "title": "Bridging Generalization and Personalization in Human Activity Recognition via On-Device Few-Shot Learning",
      "title_zh": "é€šè¿‡ç«¯ä¾§å°‘æ ·æœ¬å­¦ä¹ è¡”æ¥äººä½“æ´»åŠ¨è¯†åˆ«ä¸­çš„æ³›åŒ–ä¸ä¸ªæ€§åŒ–",
      "authors": [
        "Pixi Kang",
        "Julian Moosmann",
        "Mengxi Liu",
        "Bo Zhou",
        "Michele Magno",
        "Paul Lukowicz",
        "Sizhen Bian"
      ],
      "abstract": "Human Activity Recognition (HAR) with different sensing modalities requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73\\%, 17.38\\%, and 3.70\\%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization. The related framework is open sourced for further research\\footnote{https://github.com/kangpx/onlineTiny2023}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººä½“æ´»åŠ¨è¯†åˆ« (Human Activity Recognition, HAR) ä¸­ä¼ ç»Ÿæ¨¡å‹éš¾ä»¥å…¼é¡¾è·¨ç”¨æˆ·æ³›åŒ–ä¸ä¸ªäººé€‚é…çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ç«¯ä¾§å°‘æ ·æœ¬å­¦ä¹  (On-Device Few-Shot Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆå­¦ä¹ é€šç”¨çš„ç‰¹å¾è¡¨å¾ï¼Œéšååœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šé€šè¿‡æå°‘é‡æ ‡æ³¨æ ·æœ¬å¿«é€Ÿæ›´æ–°è½»é‡çº§åˆ†ç±»å±‚ï¼Œä»è€Œå®ç°å¯¹æ–°ç”¨æˆ·çš„æ— ç¼é€‚é…ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ä½åŠŸè€— RISC-V GAP9 å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†è¯¥æ¡†æ¶ï¼Œç¡®ä¿äº†å…¶åœ¨æä½è®¡ç®—å’Œå†…å­˜å¼€é”€ä¸‹çš„éƒ¨ç½²å¯è¡Œæ€§ã€‚å®éªŒåœ¨ RecGymã€QVAR-Gesture å’Œ Ultrasound-Gesture åŸºå‡†æ•°æ®é›†ä¸Šå±•å¼€ï¼Œéƒ¨ç½²åçš„ä¸ªæ€§åŒ–é€‚é…ä½¿è¯†åˆ«å‡†ç¡®ç‡åˆ†åˆ«æå‡äº† 3.73%ã€17.38% å’Œ 3.70%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å°‘æ ·æœ¬ç«¯ä¾§å­¦ä¹ åœ¨æ„å»ºå¯æ‰©å±•ã€é«˜èƒ½æ•ˆä¸”å…·å¤‡ç”¨æˆ·æ„ŸçŸ¥èƒ½åŠ›çš„å¯ç©¿æˆ´ HAR ç³»ç»Ÿæ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶å·²å°†ç›¸å…³æ¡†æ¶å¼€æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15413v3",
      "published_date": "2025-08-21 10:08:20 UTC",
      "updated_date": "2025-09-07 18:27:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:17.786846+00:00"
    },
    {
      "arxiv_id": "2508.15407v1",
      "title": "When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models",
      "title_zh": "å½“éŸ³é¢‘ä¸æ–‡æœ¬ä¸ä¸€è‡´æ—¶ï¼šæ­ç¤ºéŸ³é¢‘è¯­è¨€å¤§æ¨¡å‹ä¸­çš„æ–‡æœ¬åè§",
      "authors": [
        "Cheng Wang",
        "Gelei Deng",
        "Xianglin Yang",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "abstract": "Large Audio-Language Models (LALMs) are enhanced with audio perception capabilities, enabling them to effectively process and understand multimodal inputs that combine audio and text. However, their performance in handling conflicting information between audio and text modalities remains largely unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark specifically designed to evaluate how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, frequently disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, and explore mitigation strategies through supervised finetuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balance during training and more sophisticated fusion mechanisms to enhance the robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ (Large Audio-Language Models, LALMs) åœ¨å¤„ç†éŸ³é¢‘ä¸æ–‡æœ¬ä¿¡æ¯å†²çªæ—¶çš„è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºäº†è¿™ä¸€é¢†åŸŸç›®å‰çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† MCR-BENCHï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° LALMs åœ¨é¢å¯¹ä¸ä¸€è‡´çš„éŸ³é¢‘-æ–‡æœ¬å¯¹æ—¶å¦‚ä½•ç¡®å®šä¿¡æ¯ä¼˜å…ˆçº§çš„å…¨é¢åŸºå‡†ã€‚é€šè¿‡å¯¹å¤šæ ·åŒ–éŸ³é¢‘ç†è§£ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº† LALMs å­˜åœ¨æ˜¾è‘—çš„æ–‡æœ¬åè§ (Text Bias)ï¼Œå³åœ¨æ¨¡æ€å†²çªæ—¶é¢‘ç¹å¿½ç•¥éŸ³é¢‘è¯æ®è€Œåå‘æ–‡æœ¬è¾“å…¥ã€‚è¿™ç§å€¾å‘å¯¼è‡´æ¨¡å‹åœ¨éŸ³é¢‘æ ¸å¿ƒä»»åŠ¡ä¸­æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå¼•å‘äº†å¯¹å…¶åœ¨çœŸå®åœºæ™¯ä¸­å¯é æ€§çš„æ‹…å¿§ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†å½±å“åè§çš„å› ç´ ï¼Œæ¢ç´¢äº†é€šè¿‡ç›‘ç£å¾®è°ƒ (Supervised Finetuning) çš„ç¼“è§£ç­–ç•¥ï¼Œå¹¶å‘ç°æ¨¡å‹åœ¨å¤„ç†çŸ›ç›¾è¾“å…¥æ—¶å­˜åœ¨æŒç»­çš„è¿‡åº¦è‡ªä¿¡ç°è±¡ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä¼˜åŒ–æ¨¡æ€å¹³è¡¡ä¸èåˆæœºåˆ¶çš„ç´§è¿«æ€§ï¼Œä¸ºæå‡å¤šæ¨¡æ€æ¨¡å‹å¤„ç†å†²çªä¿¡æ¯çš„é²æ£’æ€§æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2508.15407v1",
      "published_date": "2025-08-21 09:58:24 UTC",
      "updated_date": "2025-08-21 09:58:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:15.894250+00:00"
    },
    {
      "arxiv_id": "2508.15394v1",
      "title": "Hybrid Least Squares/Gradient Descent Methods for DeepONets",
      "title_zh": "é¢å‘ DeepONets çš„æ··åˆæœ€å°äºŒä¹˜/æ¢¯åº¦ä¸‹é™æ³•",
      "authors": [
        "Jun Choi",
        "Chang-Ock Lee",
        "Minam Moon"
      ],
      "abstract": "We propose an efficient hybrid least squares/gradient descent method to accelerate DeepONet training. Since the output of DeepONet can be viewed as linear with respect to the last layer parameters of the branch network, these parameters can be optimized using a least squares (LS) solve, and the remaining hidden layer parameters are updated by means of gradient descent form. However, building the LS system for all possible combinations of branch and trunk inputs yields a prohibitively large linear problem that is infeasible to solve directly. To address this issue, our method decomposes the large LS system into two smaller, more manageable subproblems $\\unicode{x2014}$ one for the branch network and one for the trunk network $\\unicode{x2014}$ and solves them separately. This method is generalized to a broader type of $L^2$ loss with a regularization term for the last layer parameters, including the case of unsupervised learning with physics-informed loss.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ DeepONet è®­ç»ƒæ•ˆç‡æå‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ··åˆæœ€å°äºŒä¹˜/æ¢¯åº¦ä¸‹é™ (Hybrid Least Squares/Gradient Descent) æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ DeepONet è¾“å‡ºç›¸å¯¹äº branch network æœ€åä¸€å±‚å‚æ•°å…·æœ‰çº¿æ€§å…³ç³»çš„ç‰¹æ€§ï¼Œå°†è¿™äº›å‚æ•°é€šè¿‡æœ€å°äºŒä¹˜æ³• (Least Squares) ä¼˜åŒ–ï¼Œè€Œå…¶ä½™éšè—å±‚å‚æ•°åˆ™é€šè¿‡æ¢¯åº¦ä¸‹é™ (Gradient Descent) æ›´æ–°ã€‚é’ˆå¯¹æ„å»ºå…¨å±€æœ€å°äºŒä¹˜ç³»ç»Ÿå› è¾“å…¥ç»„åˆè¿‡å¤šè€Œå¯¼è‡´è§„æ¨¡å·¨å¤§ã€éš¾ä»¥ç›´æ¥æ±‚è§£çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºå°†è¯¥ç³»ç»Ÿåˆ†è§£ä¸º branch network å’Œ trunk network ä¸¤ä¸ªæ›´æ˜“äºç®¡ç†çš„å­é—®é¢˜å¹¶åˆ†åˆ«æ±‚è§£ã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥æ¨å¹¿è‡³åŒ…å«æ­£åˆ™åŒ–é¡¹çš„å¹¿ä¹‰ $L^2$ æŸå¤±å‡½æ•°ï¼Œå¹¶èƒ½æœ‰æ•ˆå¤„ç†æ¶‰åŠç‰©ç†ä¿¡æ¯æŸå¤± (Physics-informed loss) çš„æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚è¿™ç§ä¼˜åŒ–ç­–ç•¥åœ¨ä¿æŒè®¡ç®—å¯è¡Œæ€§çš„åŒæ—¶æ˜¾è‘—åŠ é€Ÿäº†æ¨¡å‹çš„æ”¶æ•›è¿‡ç¨‹ï¼Œä¸ºå¤§è§„æ¨¡ç®—å­å­¦ä¹ æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15394v1",
      "published_date": "2025-08-21 09:34:06 UTC",
      "updated_date": "2025-08-21 09:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:32.463058+00:00"
    },
    {
      "arxiv_id": "2508.15379v1",
      "title": "Bladder Cancer Diagnosis with Deep Learning: A Multi-Task Framework and Online Platform",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„è†€èƒ±ç™Œè¯Šæ–­ï¼šå¤šä»»åŠ¡æ¡†æ¶ä¸åœ¨çº¿å¹³å°",
      "authors": [
        "Jinliang Yu",
        "Mingduo Xie",
        "Yue Wang",
        "Tianfan Fu",
        "Xianglai Xu",
        "Jiajun Wang"
      ],
      "abstract": "Clinical cystoscopy, the current standard for bladder cancer diagnosis, suffers from significant reliance on physician expertise, leading to variability and subjectivity in diagnostic outcomes. There is an urgent need for objective, accurate, and efficient computational approaches to improve bladder cancer diagnostics.\n  Leveraging recent advancements in deep learning, this study proposes an integrated multi-task deep learning framework specifically designed for bladder cancer diagnosis from cystoscopic images. Our framework includes a robust classification model using EfficientNet-B0 enhanced with Convolutional Block Attention Module (CBAM), an advanced segmentation model based on ResNet34-UNet++ architecture with self-attention mechanisms and attention gating, and molecular subtyping using ConvNeXt-Tiny to classify molecular markers such as HER-2 and Ki-67. Additionally, we introduce a Gradio-based online diagnostic platform integrating all developed models, providing intuitive features including multi-format image uploads, bilingual interfaces, and dynamic threshold adjustments.\n  Extensive experimentation demonstrates the effectiveness of our methods, achieving outstanding accuracy (93.28%), F1-score (82.05%), and AUC (96.41%) for classification tasks, and exceptional segmentation performance indicated by a Dice coefficient of 0.9091. The online platform significantly improved the accuracy, efficiency, and accessibility of clinical bladder cancer diagnostics, enabling practical and user-friendly deployment. The code is publicly available.\n  Our multi-task framework and integrated online tool collectively advance the field of intelligent bladder cancer diagnosis by improving clinical reliability, supporting early tumor detection, and enabling real-time diagnostic feedback. These contributions mark a significant step toward AI-assisted decision-making in urology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè†€èƒ±ç™Œè¯Šæ–­çš„é›†æˆå¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠè†€èƒ±é•œæ£€æŸ¥ä¸­è¿‡åº¦ä¾èµ–åŒ»ç”Ÿç»éªŒæ‰€å¯¼è‡´çš„ä¸»è§‚æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªåŸºäº EfficientNet-B0 å¹¶ç»“åˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å— (CBAM) çš„åˆ†ç±»æ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªåˆ©ç”¨ ResNet34-UNet++ æ¶æ„å¹¶é›†æˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸æ³¨æ„åŠ›é—¨æ§æŠ€æœ¯çš„åˆ†å‰²æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜é€šè¿‡ ConvNeXt-Tiny å®ç°äº†å¯¹ HER-2 å’Œ Ki-67 ç­‰åˆ†å­æ ‡å¿—ç‰©çš„é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº† 93.28% çš„å‡†ç¡®ç‡å’Œ 96.41% çš„ AUCï¼Œåˆ†å‰²ä»»åŠ¡çš„ Dice ç³»æ•°é«˜è¾¾ 0.9091ã€‚ä¸ºäº†ä¾¿äºä¸´åºŠéƒ¨ç½²ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ä¸ªåŸºäº Gradio çš„åœ¨çº¿è¯Šæ–­å¹³å°ï¼Œæ”¯æŒå¤šæ ¼å¼å›¾åƒä¸Šä¼ ã€åŒè¯­ç•Œé¢å’Œå®æ—¶åé¦ˆã€‚è¿™ä¸€ç ”ç©¶é€šè¿‡æä¾›å®¢è§‚ã€å‡†ç¡®ä¸”é«˜æ•ˆçš„è®¡ç®—æ‰‹æ®µï¼Œæ˜¾è‘—æå‡äº†è†€èƒ±ç™Œä¸´åºŠè¯Šæ–­çš„å¯é æ€§ï¼Œä¸ºæ³Œå°¿å¤–ç§‘çš„äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15379v1",
      "published_date": "2025-08-21 09:20:03 UTC",
      "updated_date": "2025-08-21 09:20:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:33.754940+00:00"
    },
    {
      "arxiv_id": "2508.15378v1",
      "title": "EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction",
      "title_zh": "EvoFormerï¼šç»“åˆç»“æ„ä¸æ—¶é—´åå·®æ ¡æ­£çš„åŠ¨æ€å›¾çº§è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Haodi Zhong",
        "Liuxin Zou",
        "Di Wang",
        "Bo Wang",
        "Zhenxing Niu",
        "Quan Wang"
      ],
      "abstract": "Dynamic graph-level embedding aims to capture structural evolution in networks, which is essential for modeling real-world scenarios. However, existing methods face two critical yet under-explored issues: Structural Visit Bias, where random walk sampling disproportionately emphasizes high-degree nodes, leading to redundant and noisy structural representations; and Abrupt Evolution Blindness, the failure to effectively detect sudden structural changes due to rigid or overly simplistic temporal modeling strategies, resulting in inconsistent temporal embeddings. To overcome these challenges, we propose EvoFormer, an evolution-aware Transformer framework tailored for dynamic graph-level representation learning. To mitigate Structural Visit Bias, EvoFormer introduces a Structure-Aware Transformer Module that incorporates positional encoding based on node structural roles, allowing the model to globally differentiate and accurately represent node structures. To overcome Abrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive Temporal Module, which explicitly models temporal evolution through a sequential three-step strategy: (I) Random Walk Timestamp Classification, generating initial timestamp-aware graph-level embeddings; (II) Graph-Level Temporal Segmentation, partitioning the graph stream into segments reflecting structurally coherent periods; and (III) Segment-Aware Temporal Self-Attention combined with an Edge Evolution Prediction task, enabling the model to precisely capture segment boundaries and perceive structural evolution trends, effectively adapting to rapid temporal shifts. Extensive evaluations on five benchmark datasets confirm that EvoFormer achieves state-of-the-art performance in graph similarity ranking, temporal anomaly detection, and temporal segmentation tasks, validating its effectiveness in correcting structural and temporal biases.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvoFormerï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºåŠ¨æ€å›¾çº§è¡¨ç¤ºå­¦ä¹ (Dynamic Graph-Level Representation Learning)çš„è¿›åŒ–æ„ŸçŸ¥Transformeræ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­çš„ç»“æ„è®¿é—®åå·®(Structural Visit Bias)å’Œçªå‘æ¼”åŒ–ç›²åŒº(Abrupt Evolution Blindness)é—®é¢˜ã€‚ä¸ºç¼“è§£Structural Visit Biasï¼ŒEvoFormerå¼•å…¥äº†Structure-Aware Transformer Moduleï¼Œé€šè¿‡åŸºäºèŠ‚ç‚¹ç»“æ„è§’è‰²çš„ä½ç½®ç¼–ç å®ç°å¯¹èŠ‚ç‚¹ç»“æ„çš„å…¨å±€åŒºåˆ†ä¸ç²¾ç¡®è¡¨ç¤ºã€‚é’ˆå¯¹Abrupt Evolution Blindnessï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†Evolution-Sensitive Temporal Moduleï¼Œé€šè¿‡éšæœºèµ°åŠ¨æ—¶é—´æˆ³åˆ†ç±»(Random Walk Timestamp Classification)ã€å›¾çº§æ—¶é—´åˆ†å‰²(Graph-Level Temporal Segmentation)ä»¥åŠç»“åˆè¾¹ç¼˜æ¼”åŒ–é¢„æµ‹ä»»åŠ¡çš„ç‰‡æ®µæ„ŸçŸ¥æ—¶é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œç²¾ç¡®æ•æ‰åŠ¨æ€å›¾æµä¸­çš„æ¼”åŒ–åˆ†ç•Œç‚¹ä¸è¶‹åŠ¿ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯å®ï¼ŒEvoFormeråœ¨å›¾ç›¸ä¼¼åº¦æ’å(Graph Similarity Ranking)ã€æ—¶é—´å¼‚å¸¸æ£€æµ‹(Temporal Anomaly Detection)å’Œæ—¶é—´åˆ†å‰²(Temporal Segmentation)ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„SOTAæ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ›æ–°çš„ç»“æ„å’Œæ—¶é—´åå·®ä¿®æ­£ç­–ç•¥ï¼Œæœ‰æ•ˆæå‡äº†åŠ¨æ€å›¾è¡¨ç¤ºçš„ä¸€è‡´æ€§ä¸å»ºæ¨¡ç²¾åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15378v1",
      "published_date": "2025-08-21 09:19:54 UTC",
      "updated_date": "2025-08-21 09:19:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:37.391106+00:00"
    },
    {
      "arxiv_id": "2508.15372v2",
      "title": "Image-Conditioned 3D Gaussian Splat Quantization",
      "title_zh": "åŸºäºå›¾åƒæ¡ä»¶çš„ 3D é«˜æ–¯æ³¼æº…é‡åŒ–",
      "authors": [
        "Xinshuang Liu",
        "Runfa Blark Li",
        "Keito Suzuki",
        "Truong Nguyen"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Gaussian Splatting (3DGS) å‹ç¼©æ–¹æ³•åœ¨å­˜å‚¨æ•ˆç‡å’Œé•¿æœŸå­˜æ¡£ååœºæ™¯æ›´æ–°é€‚åº”æ€§æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº† Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer)ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆåˆ©ç”¨ inter-Gaussian å’Œ inter-attribute çš„ç›¸å…³æ€§ï¼Œå¹¶ä½¿ç”¨è·¨åœºæ™¯çš„å…±äº«ç æœ¬(shared codebooks)æ¥æ¶ˆé™¤å•åœºæ™¯ç æœ¬çš„é¢å¤–å¼€é”€ï¼Œä»è€Œå¤§å¹…æå‡é‡åŒ–æ•ˆç‡ã€‚ä¸ºäº†ä½¿å­˜æ¡£åçš„åœºæ™¯å…·å¤‡å¯æ›´æ–°æ€§ï¼ŒICGS-Quantizer å°†åœºæ™¯è§£ç è¿‡ç¨‹ä»¥è§£ç æ—¶æ•è·çš„å›¾åƒä¸ºæ¡ä»¶è¿›è¡Œ Image-Conditioned è§£ç ã€‚é€šè¿‡å¯¹ encodingã€quantization å’Œ decoding è¿‡ç¨‹è¿›è¡Œè”åˆè®­ç»ƒï¼Œè¯¥æ–¹æ³•æˆåŠŸå°† 3DGS çš„å­˜å‚¨éœ€æ±‚é™ä½è‡³ KB çº§åˆ«ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ visual fidelityã€‚å®éªŒè¯æ˜ï¼ŒICGS-Quantizer åœ¨ 3D åœºæ™¯å‹ç¼©å’Œåœºæ™¯æ›´æ–°ä»»åŠ¡ä¸­çš„å‹ç¼©æ•ˆç‡ä»¥åŠå¯¹åœºæ™¯å˜åŒ–çš„é€‚åº”æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ state-of-the-art æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15372v2",
      "published_date": "2025-08-21 09:07:26 UTC",
      "updated_date": "2025-09-30 08:12:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:40.150537+00:00"
    },
    {
      "arxiv_id": "2508.15371v1",
      "title": "Confidence-Modulated Speculative Decoding for Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ç½®ä¿¡åº¦è°ƒåˆ¶æŠ•æœºè§£ç ",
      "authors": [
        "Jaydip Sen",
        "Subhasis Dasgupta",
        "Hetvi Waghela"
      ],
      "abstract": "Speculative decoding has emerged as an effective approach for accelerating autoregressive inference by parallelizing token generation through a draft-then-verify paradigm. However, existing methods rely on static drafting lengths and rigid verification criteria, limiting their adaptability across varying model uncertainties and input complexities. This paper proposes an information-theoretic framework for speculative decoding based on confidence-modulated drafting. By leveraging entropy and margin-based uncertainty measures over the drafter's output distribution, the proposed method dynamically adjusts the number of speculatively generated tokens at each iteration. This adaptive mechanism reduces rollback frequency, improves resource utilization, and maintains output fidelity. Additionally, the verification process is modulated using the same confidence signals, enabling more flexible acceptance of drafted tokens without sacrificing generation quality. Experiments on machine translation and summarization tasks demonstrate significant speedups over standard speculative decoding while preserving or improving BLEU and ROUGE scores. The proposed approach offers a principled, plug-in method for efficient and robust decoding in large language models under varying conditions of uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸæŠ•æœºé‡‡æ ·(Speculative Decoding)åœ¨å¤„ç†å¤šå˜æ¨¡å‹ä¸ç¡®å®šæ€§å’Œè¾“å…¥å¤æ‚åº¦æ—¶ï¼Œå› å›ºå®šè‰æ¡ˆé•¿åº¦å’ŒåƒµåŒ–éªŒè¯æ ‡å‡†è€Œå¯¼è‡´çš„é€‚åº”æ€§ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç½®ä¿¡åº¦è°ƒèŠ‚(Confidence-Modulated)çš„æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‰æ¡ˆæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„ç†µ(Entropy)å’ŒåŸºäºè¾¹é™…çš„ä¸ç¡®å®šæ€§åº¦é‡(Margin-based Uncertainty Measures)ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­åŠ¨æ€è°ƒæ•´æŠ•æœºç”Ÿæˆçš„Tokenæ•°é‡ã€‚åŒæ—¶ï¼ŒéªŒè¯è¿‡ç¨‹ä¹Ÿå—ç½®ä¿¡åº¦ä¿¡å·è°ƒèŠ‚ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹å®ç°äº†æ›´çµæ´»çš„è‰æ¡ˆæ¥å—æ ‡å‡†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è‡ªé€‚åº”æœºåˆ¶èƒ½æœ‰æ•ˆé™ä½å›é€€(Rollback)é¢‘ç‡å¹¶æé«˜èµ„æºåˆ©ç”¨ç‡ã€‚åœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæˆ–æ”¹å–„BLEUå’ŒROUGEè¯„åˆ†çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ã€‚è¯¥ç ”ç©¶ä¸ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹å®ç°é«˜æ•ˆã€ç¨³å¥çš„è§£ç æä¾›äº†ä¸€ç§é€šç”¨çš„å³æ’å³ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This is the preprint of the paper, which has been accepted for oral presentation and publication in the proceedings of IEEE INDISCON 2025. The conference will be organized at the National Institute of Technology, Rourkela, India, from August 21 to 23, 2025. The paper is 10 pages long, and it contains 2 figures and 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.15371v1",
      "published_date": "2025-08-21 09:06:31 UTC",
      "updated_date": "2025-08-21 09:06:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:44.764447+00:00"
    },
    {
      "arxiv_id": "2508.15370v1",
      "title": "Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation",
      "title_zh": "æ­ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¯ä¿¡æ€§ï¼šè¯„ä¼°ã€åˆ†æä¸ç¼“è§£",
      "authors": [
        "Yichi Zhang",
        "Yao Huang",
        "Yifan Wang",
        "Yitong Sun",
        "Chang Liu",
        "Zhe Zhao",
        "Zhengwei Fang",
        "Huanran Chen",
        "Xiao Yang",
        "Xingxing Wei",
        "Hang Su",
        "Yinpeng Dong",
        "Jun Zhu"
      ],
      "abstract": "The trustworthiness of Multimodal Large Language Models (MLLMs) remains an intense concern despite the significant progress in their capabilities. Existing evaluation and mitigation approaches often focus on narrow aspects and overlook risks introduced by the multimodality. To tackle these challenges, we propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and mitigating the trustworthiness issues of MLLMs. We define a three-dimensional framework, encompassing five trustworthiness aspects which include truthfulness, robustness, safety, fairness, and privacy; two novel risk types covering multimodal risks and cross-modal impacts; and various mitigation strategies from the perspectives of data, model architecture, training, and inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and 28 curated datasets, enabling holistic evaluations over 30 open-source and proprietary MLLMs and in-depth analysis with 8 representative mitigation methods. Our extensive experiments reveal significant vulnerabilities in current models, including a gap between trustworthiness and general capabilities, as well as the amplification of potential risks in base LLMs by both multimodal training and inference. Moreover, our controlled analysis uncovers key limitations in existing mitigation strategies that, while some methods yield improvements in specific aspects, few effectively address overall trustworthiness, and many introduce unexpected trade-offs that compromise model utility. These findings also provide practical insights for future improvements, such as the benefits of reasoning to better balance safety and performance. Based on these insights, we introduce a Reasoning-Enhanced Safety Alignment (RESA) approach that equips the model with chain-of-thought reasoning ability to discover the underlying risks, achieving state-of-the-art results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) çš„å¯ä¿¡åº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°ã€åˆ†æä¸ç¼“è§£åŸºå‡† MultiTrust-Xã€‚è¯¥åŸºå‡†æ„å»ºäº†ä¸€ä¸ªä¸‰ç»´æ¡†æ¶ï¼Œæ¶µç›–äº†çœŸå®æ€§ (truthfulness)ã€é²æ£’æ€§ (robustness)ã€å®‰å…¨æ€§ (safety)ã€å…¬å¹³æ€§ (fairness) å’Œéšç§ (privacy) äº”å¤§ç»´åº¦ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨å¤šæ¨¡æ€é£é™©å’Œè·¨æ¨¡æ€å½±å“ã€‚é€šè¿‡å¯¹è¶…è¿‡ 30 ç§å¼€æºåŠå•†ç”¨ MLLMs çš„è¯„ä¼°ï¼Œå®éªŒæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨é€šç”¨èƒ½åŠ›ä¸å¯ä¿¡åº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸”å¤šæ¨¡æ€æœºåˆ¶ä¼šæ”¾å¤§åŸºç¡€æ¨¡å‹ (base LLMs) çš„æ½œåœ¨é£é™©ã€‚åˆ†æè¿˜å‘ç°ç°æœ‰ç¼“è§£ç­–ç•¥åœ¨æ•´ä½“å¯ä¿¡åº¦æå‡ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå¾€å¾€ä¼šå¼•å…¥æ„æƒ³ä¸åˆ°çš„åŠŸèƒ½æŸè€—ã€‚åŸºäºä¸Šè¿°æ´å¯Ÿï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ¨ç†å¢å¼ºå®‰å…¨å¯¹é½ (Reasoning-Enhanced Safety Alignment, RESA) æ–¹æ³•ï¼Œé€šè¿‡èµ‹äºˆæ¨¡å‹é“¾å¼æ€ç»´ (Chain-of-Thought) æ¨ç†èƒ½åŠ›æ¥è¯†åˆ«æ·±å±‚é£é™©ã€‚è¯¥æ–¹æ³•åœ¨å®éªŒä¸­å–å¾—äº†æœ€å…ˆè¿› (state-of-the-art) çš„ç»“æœï¼Œä¸ºå¹³è¡¡ MLLMs çš„å®‰å…¨æ€§å’Œé€šç”¨æ€§èƒ½æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "For Appendix, please refer to arXiv:2406.07057",
      "pdf_url": "https://arxiv.org/pdf/2508.15370v1",
      "published_date": "2025-08-21 09:00:01 UTC",
      "updated_date": "2025-08-21 09:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:46.638950+00:00"
    },
    {
      "arxiv_id": "2508.16677v1",
      "title": "Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration",
      "title_zh": "Recall-Extend Dynamicsï¼šé€šè¿‡å—æ§æ¢ç´¢ä¸ç²¾ç»†åŒ–ç¦»çº¿é›†æˆå¢å¼ºå°è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zhong Guan",
        "Likang Wu",
        "Hongke Zhao",
        "Jiahui Wang",
        "Le Wu"
      ],
      "abstract": "Many existing studies have achieved significant improvements in the reasoning capabilities of large language models (LLMs) through reinforcement learning with verifiable rewards (RLVR), while the enhancement of reasoning abilities in small language models (SLMs) has not yet been sufficiently explored. Combining distilled data from larger models with RLVR on small models themselves is a natural approach, but it still faces various challenges and issues. Therefore, we propose \\textit{\\underline{R}}ecall-\\textit{\\underline{E}}xtend \\textit{\\underline{D}}ynamics(RED): Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration. In this paper, we explore the perspective of varying exploration spaces, balancing offline distillation with online reinforcement learning. Simultaneously, we specifically design and optimize for the insertion problem within offline data. By monitoring the ratio of entropy changes in the model concerning offline and online data, we regulate the weight of offline-SFT, thereby addressing the issues of insufficient exploration space in small models and the redundancy and complexity during the distillation process. Furthermore, to tackle the distribution discrepancies between offline data and the current policy, we design a sample-accuracy-based policy shift mechanism that dynamically chooses between imitating offline distilled data and learning from its own policy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Recall-Extend Dynamics (RED) æ¡†æ¶ï¼Œè‡´åŠ›äºé€šè¿‡å—æ§æ¢ç´¢å’Œç²¾ç»†çš„ç¦»çº¿é›†æˆæ¥å¢å¼ºå°è¯­è¨€æ¨¡å‹ (Small Language Models, SLMs) çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ SLMs åœ¨ç»“åˆç¦»çº¿è’¸é¦ä¸å¼ºåŒ–å­¦ä¹  (RLVR) æ—¶é¢ä¸´çš„æ¢ç´¢ç©ºé—´ä¸è¶³åŠæ•°æ®å†—ä½™å¤æ‚æ€§ï¼ŒRED é€šè¿‡ç›‘æ§æ¨¡å‹åœ¨å¤„ç†ç¦»çº¿å’Œåœ¨çº¿æ•°æ®æ—¶çš„ç†µå€¼å˜åŒ–æ¯”ä¾‹ï¼ŒåŠ¨æ€è°ƒèŠ‚ offline-SFT çš„æƒé‡ä»¥å¹³è¡¡ä¸¤è€…ã€‚æ­¤å¤–ï¼Œä¸ºäº†åº”å¯¹ç¦»çº¿æ•°æ®ä¸å½“å‰ç­–ç•¥ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§åŸºäºæ ·æœ¬å‡†ç¡®ç‡çš„ç­–ç•¥åç§»æœºåˆ¶ (policy shift mechanism)ï¼Œä½¿æ¨¡å‹èƒ½åœ¨æ¨¡ä»¿è’¸é¦æ•°æ®ä¸å­¦ä¹ è‡ªèº«ç­–ç•¥é—´è¿›è¡ŒåŠ¨æ€æŠ‰æ‹©ã€‚è¯¥æ¡†æ¶ä»æ¢ç´¢ç©ºé—´å˜åŒ–çš„è§†è§’å‡ºå‘ï¼Œä¼˜åŒ–äº† SLMs çš„ç¦»çº¿é›†æˆä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä¸ºåœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹æå‡å°æ¨¡å‹é€»è¾‘æ¨ç†æ°´å¹³æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16677v1",
      "published_date": "2025-08-21 08:55:10 UTC",
      "updated_date": "2025-08-21 08:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:13:57.161240+00:00"
    },
    {
      "arxiv_id": "2508.15358v1",
      "title": "Planning with Minimal Disruption",
      "title_zh": "æœ€å°å¹²æ‰°è§„åˆ’",
      "authors": [
        "Alberto Pozanco",
        "Marianela Morales",
        "Daniel Borrajo",
        "Manuela Veloso"
      ],
      "abstract": "In many planning applications, we might be interested in finding plans that minimally modify the initial state to achieve the goals. We refer to this concept as plan disruption. In this paper, we formally introduce it, and define various planning-based compilations that aim to jointly optimize both the sum of action costs and plan disruption. Experimental results in different benchmarks show that the reformulated task can be effectively solved in practice to generate plans that balance both objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è§„åˆ’åº”ç”¨ä¸­å¦‚ä½•é€šè¿‡æœ€å°åŒ–ä¿®æ”¹åˆå§‹çŠ¶æ€æ¥å®ç°ç›®æ ‡ï¼Œå¹¶æ­£å¼å¼•å…¥äº†è®¡åˆ’å¹²æ‰° (plan disruption) è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µã€‚ä¸ºäº†è§£å†³è¯¥é—®é¢˜ï¼Œè®ºæ–‡å®šä¹‰äº†å¤šç§åŸºäºè§„åˆ’çš„ç¼–è¯‘ (planning-based compilations) æ–¹æ³•ï¼Œæ—¨åœ¨è”åˆä¼˜åŒ–åŠ¨ä½œæˆæœ¬ (action costs) çš„æ€»å’Œä¸è®¡åˆ’å¹²æ‰°ã€‚ä½œè€…åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¯æ˜è¿™ç§é‡æ–°å®šä¹‰çš„ä»»åŠ¡åœ¨å®è·µä¸­å¯ä»¥è¢«æœ‰æ•ˆè§£å†³ã€‚å®éªŒç”Ÿæˆçš„è§„åˆ’æ–¹æ¡ˆèƒ½å¤ŸæˆåŠŸå¹³è¡¡åŠ¨ä½œæˆæœ¬ä¸çŠ¶æ€ä¿®æ”¹ç¨‹åº¦è¿™ä¸¤ä¸ªå…³é”®ç»´åº¦ã€‚è¯¥ç ”ç©¶ä¸ºéœ€è¦åœ¨è¾¾æˆç›®æ ‡çš„åŒæ—¶å°½å¯èƒ½ç»´æŒåˆå§‹ç¯å¢ƒç¨³å®šæ€§çš„åº”ç”¨åœºæ™¯æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15358v1",
      "published_date": "2025-08-21 08:38:17 UTC",
      "updated_date": "2025-08-21 08:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:29.387087+00:00"
    },
    {
      "arxiv_id": "2508.15338v1",
      "title": "DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization",
      "title_zh": "DiagECGï¼šåŸºäºç¦»æ•£åŒ–å¿ƒç”µå›¾æ ‡è®°åŒ–çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨è¯Šæ–­æ¨ç†æ¡†æ¶",
      "authors": [
        "Jinning Yang",
        "Wen Shi"
      ],
      "abstract": "Electrocardiography plays a central role in cardiovascular diagnostics, yet existing automated approaches often struggle to generalize across clinical tasks and offer limited support for open-ended reasoning. We present DiagECG, a novel framework that integrates time-series and language modeling by enabling large language models to process 12-lead ECG signals for clinical text generation tasks. Our approach discretizes continuous ECG embeddings into symbolic tokens using a lead-independent encoder and quantization module. These tokens are then used to extend the vocabulary of LLM, allowing the model to handle both ECG and natural language inputs in a unified manner. To bridge the modality gap, we pretrain the model on an autoregressive ECG forecasting task, enabling the LLM to model temporal dynamics using its native language modeling capabilities. Finally, we perform instruction tuning on both ECG question answering and diagnostic report generation. Without modifying the core model, DiagECG achieves strong performance across tasks while maintaining generalization to out-of-distribution settings. Extensive experiments demonstrate the effectiveness of each component and highlight the potential of integrating symbolic ECG representations into LLMs for medical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiagECGï¼Œè¿™æ˜¯ä¸€ç§é©±åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯Šæ–­æ¨ç†çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•åœ¨ä¸´åºŠä»»åŠ¡æ³›åŒ–å’Œå¼€æ”¾å¼æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‹¬ç«‹å¯¼è”ç¼–ç å™¨å’Œé‡åŒ–æ¨¡å—å°†è¿ç»­çš„ 12-lead ECG ä¿¡å·ç¦»æ•£åŒ–ä¸ºç¬¦å· tokensï¼Œå¹¶ä»¥æ­¤æ‰©å±• LLM çš„è¯æ±‡è¡¨ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥ç»Ÿä¸€æ–¹å¼å¤„ç†å¿ƒç”µä¿¡å·ä¸è‡ªç„¶è¯­è¨€è¾“å…¥ã€‚ä¸ºäº†å¼¥è¡¥æ¨¡æ€é¸¿æ²Ÿï¼Œæ¨¡å‹å…ˆåœ¨è‡ªå›å½’ ECG forecasting ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥å­¦ä¹ æ—¶é—´åŠ¨æ€ç‰¹å¾ï¼Œéšååœ¨ ECG é—®ç­”å’Œè¯Šæ–­æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº† instruction tuningã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiagECG åœ¨ä¸æ”¹å˜æ ¸å¿ƒæ¨¡å‹ç»“æ„çš„å‰æä¸‹ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡å–å¾—äº†å¼ºåŠ²æ€§èƒ½ï¼Œå¹¶å±•ç°äº†å‡ºè‰²çš„ OOD æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå‡¸æ˜¾äº†å°†ç¬¦å·åŒ–å¿ƒç”µè¡¨ç¤ºé›†æˆåˆ° LLMs ä¸­ä»¥æå‡åŒ»ç–—æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15338v1",
      "published_date": "2025-08-21 08:13:37 UTC",
      "updated_date": "2025-08-21 08:13:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:19.497703+00:00"
    },
    {
      "arxiv_id": "2508.15336v1",
      "title": "Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling",
      "title_zh": "åŸºäºå§¿æ€æ£€æµ‹ä¸åºåˆ—å»ºæ¨¡çš„é“è·¯ç©¿è¶Šè¡Œä¸ºé¢„æµ‹",
      "authors": [
        "Subhasis Dasgupta",
        "Preetam Saha",
        "Agniva Roy",
        "Jaydip Sen"
      ],
      "abstract": "The world is constantly moving towards AI based systems and autonomous vehicles are now reality in different parts of the world. These vehicles require sensors and cameras to detect objects and maneuver according to that. It becomes important to for such vehicles to also predict from a distant if a person is about to cross a road or not. The current study focused on predicting the intent of crossing the road by pedestrians in an experimental setup. The study involved working with deep learning models to predict poses and sequence modelling for temporal predictions. The study analysed three different sequence modelling to understand the prediction behaviour and it was found out that GRU was better in predicting the intent compared to LSTM model but 1D CNN was the best model in terms of speed. The study involved video analysis, and the output of pose detection model was integrated later on to sequence modelling techniques for an end-to-end deep learning framework for predicting road crossing intents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­è¡Œäººè¿‡é©¬è·¯æ„å›¾çš„é¢„æµ‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå§¿æ€æ£€æµ‹(Pose Detection)ä¸åºåˆ—å»ºæ¨¡(Sequence Modelling)çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ç ”ç©¶è¿‡ç¨‹æ¶‰åŠå¯¹è§†é¢‘æ•°æ®çš„åˆ†æï¼Œé€šè¿‡Pose Detectionæ¨¡å‹æå–å…³é”®ç‚¹ä¿¡æ¯åï¼Œå°†å…¶æ•´åˆè¿›åºåˆ—å»ºæ¨¡æŠ€æœ¯ä»¥å®ç°æ—¶é—´ç»´åº¦çš„è¡Œä¸ºæ„å›¾é¢„æµ‹ã€‚å®éªŒæ·±å…¥å¯¹æ¯”äº†ä¸‰ç§ä¸åŒçš„åºåˆ—æ¨¡å‹ï¼Œå‘ç°é—¨æ§å¾ªç¯å•å…ƒ(GRU)åœ¨æ•æ‰è¡Œäººè¿‡é©¬è·¯æ„å›¾çš„å‡†ç¡®æ€§ä¸Šä¼˜äºé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)ï¼Œè€Œä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œ(1D CNN)åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚è¯¥ç ”ç©¶æˆæœä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä»è¿œè·ç¦»å‡†ç¡®è¯†åˆ«è¡ŒäººåŠ¨æœºå¹¶è¿›è¡Œå®‰å…¨é¿è®©æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is a pre-print version of the original paper accepted in the IEEE conference INDISCON 2025. It contains 8 figures and 1 table. The length of the paper is 7 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.15336v1",
      "published_date": "2025-08-21 08:08:50 UTC",
      "updated_date": "2025-08-21 08:08:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:03.388753+00:00"
    },
    {
      "arxiv_id": "2508.15335v1",
      "title": "RETAIL: Towards Real-world Travel Planning for Large Language Models",
      "title_zh": "RETAILï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ç°å®åœºæ™¯æ—…è¡Œè§„åˆ’æ¢ç´¢",
      "authors": [
        "Bin Deng",
        "Yizhe Feng",
        "Zeming Liu",
        "Qing Wei",
        "Xiangrong Zhu",
        "Shuai Chen",
        "Yuanfang Guo",
        "Yunhong Wang"
      ],
      "abstract": "Although large language models have enhanced automated travel planning abilities, current systems remain misaligned with real-world scenarios. First, they assume users provide explicit queries, while in reality requirements are often implicit. Second, existing solutions ignore diverse environmental factors and user preferences, limiting the feasibility of plans. Third, systems can only generate plans with basic POI arrangements, failing to provide all-in-one plans with rich details. To mitigate these challenges, we construct a novel dataset \\textbf{RETAIL}, which supports decision-making for implicit queries while covering explicit queries, both with and without revision needs. It also enables environmental awareness to ensure plan feasibility under real-world scenarios, while incorporating detailed POI information for all-in-one travel plans. Furthermore, we propose a topic-guided multi-agent framework, termed TGMA. Our experiments reveal that even the strongest existing model achieves merely a 1.0% pass rate, indicating real-world travel planning remains extremely challenging. In contrast, TGMA demonstrates substantially improved performance 2.72%, offering promising directions for real-world travel planning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) åœ¨è‡ªåŠ¨åŒ–æ—…è¡Œè§„åˆ’ä¸­ä¸ç°å®åœºæ™¯ä¸åŒ¹é…çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰ç³»ç»Ÿåœ¨å¤„ç†éšå¼éœ€æ±‚ã€ç¯å¢ƒå› ç´ åŠè®¡åˆ’ç»†èŠ‚æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåä¸º RETAIL çš„æ–°å‹æ•°æ®é›†ï¼Œæ”¯æŒåŒ…å«éšå¼å’Œæ˜¾å¼æŸ¥è¯¢çš„å†³ç­–åˆ¶å®šï¼Œå¹¶æ¶µç›–äº†ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ä»¥ç¡®ä¿è®¡åˆ’åœ¨çœŸå®åœºæ™¯ä¸‹çš„å¯è¡Œæ€§ã€‚RETAIL è¿˜æ•´åˆäº†è¯¦ç»†çš„å…´è¶£ç‚¹ (POI) ä¿¡æ¯ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰ä¸°å¯Œç»†èŠ‚çš„ä¸€ç«™å¼æ—…è¡Œè®¡åˆ’ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªåä¸º TGMA (Topic-guided Multi-Agent) çš„ä¸»é¢˜å¼•å¯¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å¼ºçš„æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸‹çš„é€šè¿‡ç‡ä¹Ÿä»…ä¸º 1.0%ï¼Œå‡¸æ˜¾äº†è¯¥ä»»åŠ¡çš„æå¤§æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒTGMA æ¡†æ¶å°†æ€§èƒ½æå‡è‡³ 2.72%ï¼Œæ˜¾è‘—æ”¹å–„äº†è§„åˆ’æ•ˆæœã€‚è¯¥å·¥ä½œä¸ºå®ç°å¯è½åœ°ã€å†…å®¹è¯¦å®çš„çœŸå®ä¸–ç•Œæ—…è¡Œè§„åˆ’æä¾›äº†é‡è¦çš„åŸºå‡†å’Œæ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15335v1",
      "published_date": "2025-08-21 08:08:38 UTC",
      "updated_date": "2025-08-21 08:08:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:53.456178+00:00"
    },
    {
      "arxiv_id": "2508.16674v1",
      "title": "MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation",
      "title_zh": "MedRepBenchï¼šåŒ»ç–—æŠ¥å‘Šè§£è¯»ç»¼åˆåŸºå‡†",
      "authors": [
        "Fangxin Shang",
        "Yuan Xia",
        "Dalu Yang",
        "Yahui Wang",
        "Binglin Yang"
      ],
      "abstract": "Medical report interpretation plays a crucial role in healthcare, enabling both patient-facing explanations and effective information flow across clinical systems. While recent vision-language models (VLMs) and large language models (LLMs) have demonstrated general document understanding capabilities, there remains a lack of standardized benchmarks to assess structured interpretation quality in medical reports. We introduce MedRepBench, a comprehensive benchmark built from 1,900 de-identified real-world Chinese medical reports spanning diverse departments, patient demographics, and acquisition formats. The benchmark is designed primarily to evaluate end-to-end VLMs for structured medical report understanding. To enable controlled comparisons, we also include a text-only evaluation setting using high-quality OCR outputs combined with LLMs, allowing us to estimate the upper-bound performance when character recognition errors are minimized. Our evaluation framework supports two complementary protocols: (1) an objective evaluation measuring field-level recall of structured clinical items, and (2) an automated subjective evaluation using a powerful LLM as a scoring agent to assess factuality, interpretability, and reasoning quality. Based on the objective metric, we further design a reward function and apply Group Relative Policy Optimization (GRPO) to improve a mid-scale VLM, achieving up to 6% recall gain. We also observe that the OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and latency issues, motivating further progress toward robust, fully vision-based report understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—æŠ¥å‘Šç»“æ„åŒ–è§£é‡Šç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°æ ‡å‡†çš„é—®é¢˜ï¼Œæ¨å‡ºäº†MedRepBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–1900ä»½çœŸå®ä¸­å›½åŒ»ç–—æŠ¥å‘Šçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ã€‚è¯¥åŸºå‡†æ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å¯¹å¤šæ ·åŒ–åŒ»ç–—æŠ¥å‘Šçš„ç«¯åˆ°ç«¯ç†è§£èƒ½åŠ›ï¼Œå¹¶åŒ…å«OCRç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å¯¹æ¯”å®éªŒä»¥ç¡®å®šæ€§èƒ½ä¸Šé™ã€‚è¯„ä¼°ä½“ç³»ç”±è¡¡é‡ä¸´åºŠé¡¹ç›®å­—æ®µå¬å›ç‡(Recall)çš„å®¢è§‚æŒ‡æ ‡å’ŒåŸºäºLLMè¯„åˆ†ä»£ç†çš„ä¸»è§‚è¯„ä»·å…±åŒæ„æˆï¼Œé‡ç‚¹è€ƒæ ¸äº‹å®æ€§ã€å¯è§£é‡Šæ€§ä¸æ¨ç†è´¨é‡ã€‚é€šè¿‡è®¾è®¡å¥–åŠ±å‡½æ•°å¹¶åº”ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ï¼Œç ”ç©¶æˆåŠŸä½¿ä¸­ç­‰è§„æ¨¡VLMsçš„å¬å›ç‡æå‡äº†6%ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†OCR+LLMæ¶æ„åœ¨å¸ƒå±€ç›²è§†(Layout-blindness)å’Œé«˜å»¶è¿Ÿæ–¹é¢çš„å±€é™æ€§ï¼Œå‡¸æ˜¾äº†å‘å±•å…¨è§†è§‰æŠ¥å‘Šç†è§£æŠ€æœ¯çš„é‡è¦æ€§ã€‚MedRepBenchçš„æå‡ºä¸ºåŒ»ç–—æŠ¥å‘Šçš„è‡ªåŠ¨åŒ–è§£ææä¾›äº†å…¨é¢çš„è¡¡é‡æ ‡å‡†ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†ä¸´åºŠä¿¡æ¯ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æµè½¬ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16674v1",
      "published_date": "2025-08-21 07:52:45 UTC",
      "updated_date": "2025-08-21 07:52:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:58.462199+00:00"
    },
    {
      "arxiv_id": "2508.16673v1",
      "title": "Invisible Filters: Cultural Bias in Hiring Evaluations Using Large Language Models",
      "title_zh": "Invisible Filtersï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨æ‹›è˜è¯„ä¼°ä¸­çš„æ–‡åŒ–åè§",
      "authors": [
        "Pooja S. B. Rao",
        "Laxminarayen Nagarajan Venkatesan",
        "Mauro Cherubini",
        "Dinesh Babu Jayagopi"
      ],
      "abstract": "Artificial Intelligence (AI) is increasingly used in hiring, with large language models (LLMs) having the potential to influence or even make hiring decisions. However, this raises pressing concerns about bias, fairness, and trust, particularly across diverse cultural contexts. Despite their growing role, few studies have systematically examined the potential biases in AI-driven hiring evaluation across cultures. In this study, we conduct a systematic analysis of how LLMs assess job interviews across cultural and identity dimensions. Using two datasets of interview transcripts, 100 from UK and 100 from Indian job seekers, we first examine cross-cultural differences in LLM-generated scores for hirability and related traits. Indian transcripts receive consistently lower scores than UK transcripts, even when they were anonymized, with disparities linked to linguistic features such as sentence complexity and lexical diversity. We then perform controlled identity substitutions (varying names by gender, caste, and region) within the Indian dataset to test for name-based bias. These substitutions do not yield statistically significant effects, indicating that names alone, when isolated from other contextual signals, may not influence LLM evaluations. Our findings underscore the importance of evaluating both linguistic and social dimensions in LLM-driven evaluations and highlight the need for culturally sensitive design and accountability in AI-assisted hiring.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹è¯„ä¼°æ±‚èŒé¢è¯•æ—¶çš„æ½œåœ¨åè§ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨æ¥è‡ªè‹±å›½å’Œå°åº¦çš„é¢è¯•æ–‡æœ¬æ•°æ®é›†ï¼Œå¯¹æ¯”äº†æ¨¡å‹å¯¹å—è¯•è€…å½•ç”¨èƒ½åŠ›(hirability)åŠç›¸å…³ç‰¹è´¨çš„è¯„åˆ†å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿åœ¨åŒ¿ååŒ–å¤„ç†åï¼Œå°åº¦æ±‚èŒè€…çš„å¾—åˆ†ä»ä¸€è‡´ä½äºè‹±å›½æ±‚èŒè€…ï¼Œè¿™ç§å·®å¼‚ä¸»è¦æºäºå¥å­å¤æ‚åº¦å’Œè¯æ±‡å¤šæ ·æ€§ç­‰è¯­è¨€ç‰¹å¾(linguistic features)ã€‚è¿›ä¸€æ­¥çš„å—æ§èº«ä»½æ›¿æ¢å®éªŒè¡¨æ˜ï¼Œå§“åã€æ€§åˆ«æˆ–ç§å§“ç­‰å•ä¸€ç¤¾ä¼šæ ‡è¯†åœ¨éš”ç¦»è¯­å¢ƒä¸‹å¯¹æ¨¡å‹è¯„åˆ†æ²¡æœ‰æ˜¾è‘—ç»Ÿè®¡å­¦å½±å“ã€‚è¯¥å‘ç°æ­ç¤ºäº†LLMsåœ¨è¯„ä¼°ä¸­å­˜åœ¨çš„â€œéšå½¢è¿‡æ»¤å™¨â€ï¼Œå¼ºè°ƒäº†åœ¨AIè¾…åŠ©æ‹›è˜ä¸­æ•´åˆæ–‡åŒ–æ•æ„Ÿæ€§è®¾è®¡(culturally sensitive design)å’Œé—®è´£æœºåˆ¶çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to AIES 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.16673v1",
      "published_date": "2025-08-21 07:45:00 UTC",
      "updated_date": "2025-08-21 07:45:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:58.753630+00:00"
    },
    {
      "arxiv_id": "2508.15327v3",
      "title": "Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning",
      "title_zh": "ç¦»çº¿åå¥½å¼ºåŒ–å­¦ä¹ ä¸­åŸºäºæœç´¢çš„ä¿¡ç”¨åˆ†é…",
      "authors": [
        "Xiancheng Gao",
        "Yufeng Shi",
        "Wengang Zhou",
        "Houqiang Li"
      ],
      "abstract": "Offline reinforcement learning refers to the process of learning policies from fixed datasets, without requiring additional environment interaction. However, it often relies on well-defined reward functions, which are difficult and expensive to design. Human feedback is an appealing alternative, but its two common forms, expert demonstrations and preferences, have complementary limitations. Demonstrations provide stepwise supervision, but they are costly to collect and often reflect limited expert behavior modes. In contrast, preferences are easier to collect, but it is unclear which parts of a behavior contribute most to a trajectory segment, leaving credit assignment unresolved. In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to unify these two feedback sources. For each transition in a preference labeled trajectory, SPW searches for the most similar state-action pairs from expert demonstrations and directly derives stepwise importance weights based on their similarity scores. These weights are then used to guide standard preference learning, enabling more accurate credit assignment that traditional approaches struggle to achieve. We demonstrate that SPW enables effective joint learning from preferences and demonstrations, outperforming prior methods that leverage both feedback types on challenging robot manipulation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾ä»¥åŠäººç±»åå¥½åé¦ˆåœ¨ä¿¡ç”¨åˆ†é…(Credit Assignment)ä¸Šå­˜åœ¨çš„ä¸ç¡®å®šæ€§é—®é¢˜å±•å¼€è®¨è®ºã€‚ä¸“å®¶æ¼”ç¤º(Expert Demonstrations)è™½ç„¶æä¾›é€æ­¥ç›‘ç£ä½†æˆæœ¬é«˜æ˜‚ä¸”æ¨¡å¼æœ‰é™ï¼Œè€Œåå¥½åé¦ˆ(Preferences)è™½ç„¶æ˜“äºæ”¶é›†ï¼Œä½†éš¾ä»¥ç¡®å®šè½¨è¿¹ä¸­å…·ä½“è¡Œä¸ºå¯¹æ•´ä½“è´¡çŒ®çš„æƒé‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æœç´¢é©±åŠ¨çš„åå¥½åŠ æƒ(Search-Based Preference Weighting, SPW)æ–¹æ¡ˆï¼Œé€šè¿‡åœ¨ä¸“å®¶æ¼”ç¤ºä¸­æœç´¢ä¸åå¥½æ ‡è®°è½¨è¿¹ä¸­æœ€ç›¸ä¼¼çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œå¹¶æ ¹æ®ç›¸ä¼¼åº¦åˆ†æ•°æ¨å¯¼å‡ºé€æ­¥é‡è¦æ€§æƒé‡ã€‚è¿™äº›æƒé‡è¢«ç”¨äºæŒ‡å¯¼æ ‡å‡†çš„åå¥½å­¦ä¹ ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†éç»“æ„åŒ–åé¦ˆæ—¶çš„éš¾é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒSPWåœ¨æŒ‘æˆ˜æ€§çš„æœºå™¨äººæ“çºµä»»åŠ¡ä¸­å®ç°äº†åå¥½ä¸æ¼”ç¤ºçš„æœ‰æ•ˆè”åˆå­¦ä¹ ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä»¥å¾€åˆ©ç”¨ä¸¤ç§åé¦ˆç±»å‹çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures, under review",
      "pdf_url": "https://arxiv.org/pdf/2508.15327v3",
      "published_date": "2025-08-21 07:41:45 UTC",
      "updated_date": "2025-10-10 03:54:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:59.290757+00:00"
    },
    {
      "arxiv_id": "2508.15314v2",
      "title": "VideoEraser: Concept Erasure in Text-to-Video Diffusion Models",
      "title_zh": "VideoEraserï¼šæ–‡ç”Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ“¦é™¤",
      "authors": [
        "Naen Xu",
        "Jinghuai Zhang",
        "Changjiang Li",
        "Zhi Chen",
        "Chunyi Zhou",
        "Qingming Li",
        "Tianyu Du",
        "Shouling Ji"
      ],
      "abstract": "The rapid growth of text-to-video (T2V) diffusion models has raised concerns about privacy, copyright, and safety due to their potential misuse in generating harmful or misleading content. These models are often trained on numerous datasets, including unauthorized personal identities, artistic creations, and harmful materials, which can lead to uncontrolled production and distribution of such content. To address this, we propose VideoEraser, a training-free framework that prevents T2V diffusion models from generating videos with undesirable concepts, even when explicitly prompted with those concepts. Designed as a plug-and-play module, VideoEraser can seamlessly integrate with representative T2V diffusion models via a two-stage process: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). We conduct extensive evaluations across four tasks, including object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser consistently outperforms prior methods regarding efficacy, integrity, fidelity, robustness, and generalizability. Notably, VideoEraser achieves state-of-the-art performance in suppressing undesirable content during T2V generation, reducing it by 46% on average across four tasks compared to baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VideoEraserï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒ(training-free)çš„æ¡†æ¶ï¼Œæ—¨åœ¨é˜²æ­¢æ–‡æœ¬åˆ°è§†é¢‘(Text-to-Video)æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¶‰åŠéšç§ã€ç‰ˆæƒæˆ–å®‰å…¨é—®é¢˜çš„è¿è§„å†…å®¹ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æ¨¡å—ï¼ŒVideoEraseré€šè¿‡é€‰æ‹©æ€§æç¤ºåµŒå…¥è°ƒæ•´(Selective Prompt Embedding Adjustment, SPEA)å’ŒæŠ—å¯¹æŠ—å™ªå£°å¼•å¯¼(Adversarial-Resilient Noise Guidance, ARNG)çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œå®ç°äº†ä¸ä¸»æµæ‰©æ•£æ¨¡å‹çš„æ— ç¼é›†æˆã€‚è¯¥ç ”ç©¶åœ¨ç‰©ä½“æ“¦é™¤ã€è‰ºæœ¯é£æ ¼æ“¦é™¤ã€åäººæ“¦é™¤å’Œæ˜¾å¼å†…å®¹æ“¦é™¤å››é¡¹ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoEraseråœ¨æ•ˆèƒ½ã€å®Œæ•´æ€§ã€ä¿çœŸåº¦ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æŠ‘åˆ¶è¿è§„å†…å®¹æ–¹é¢ï¼ŒVideoEraseråœ¨å››é¡¹ä»»åŠ¡ä¸­ç›¸æ¯”åŸºçº¿æ¨¡å‹å¹³å‡é™ä½äº†46%ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "To appear in the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "pdf_url": "https://arxiv.org/pdf/2508.15314v2",
      "published_date": "2025-08-21 07:15:18 UTC",
      "updated_date": "2025-08-27 08:48:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:14:59.787036+00:00"
    },
    {
      "arxiv_id": "2508.15313v2",
      "title": "First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection",
      "title_zh": "å…ˆ RAGï¼Œå SEGï¼šä¸€ç§æ— éœ€è®­ç»ƒçš„ä¼ªè£…ç›®æ ‡æ£€æµ‹èŒƒå¼",
      "authors": [
        "Wutao Liu",
        "YiDan Wang",
        "Pan Gao"
      ],
      "abstract": "Camouflaged object detection (COD) poses a significant challenge in computer vision due to the high similarity between objects and their backgrounds. Existing approaches often rely on heavy training and large computational resources. While foundation models such as the Segment Anything Model (SAM) offer strong generalization, they still struggle to handle COD tasks without fine-tuning and require high-quality prompts to yield good performance. However, generating such prompts manually is costly and inefficient. To address these challenges, we propose \\textbf{First RAG, Second SEG (RAG-SEG)}, a training-free paradigm that decouples COD into two stages: Retrieval-Augmented Generation (RAG) for generating coarse masks as prompts, followed by SAM-based segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval database via unsupervised clustering, enabling fast and effective feature retrieval. During inference, the retrieved features produce pseudo-labels that guide precise mask generation using SAM2. Our method eliminates the need for conventional training while maintaining competitive performance. Extensive experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par with or surpasses state-of-the-art methods. Notably, all experiments are conducted on a \\textbf{personal laptop}, highlighting the computational efficiency and practicality of our approach. We present further analysis in the Appendix, covering limitations, salient object detection extension, and possible improvements. \\textcolor{blue} {Code: https://github.com/Lwt-diamond/RAG-SEG.}",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ªè£…ç›®æ ‡æ£€æµ‹(Camouflaged Object Detection, COD)ä¸­ç›®æ ‡ä¸èƒŒæ™¯é«˜åº¦ç›¸ä¼¼ã€ç°æœ‰æ¨¡å‹ä¾èµ–å¤§é‡è®­ç»ƒä¸”è®¡ç®—èµ„æºæ¶ˆè€—å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºRAG-SEGçš„æ— éœ€è®­ç»ƒ(training-free)èŒƒå¼ã€‚RAG-SEGå°†æ£€æµ‹ä»»åŠ¡è§£è€¦ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç”Ÿæˆç²—ç•¥æ©ç ä½œä¸ºæç¤ºï¼Œéšååˆ©ç”¨SAM2è¿›è¡Œç²¾ç»†åŒ–åˆ†å‰²(Segmentation, SEG)ã€‚è¯¥æ¡†æ¶é€šè¿‡æ— ç›‘ç£èšç±»(unsupervised clustering)æ„å»ºç´§å‡‘çš„æ£€ç´¢æ•°æ®åº“ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨æ£€ç´¢åˆ°çš„ç‰¹å¾ç”Ÿæˆä¼ªæ ‡ç­¾(pseudo-labels)ï¼Œä»è€Œå¼•å¯¼SAM2ç”Ÿæˆç²¾ç¡®çš„ç›®æ ‡æ©ç ã€‚è¿™ç§æ–¹æ³•å®Œå…¨æ¶ˆé™¤äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒéœ€æ±‚ï¼Œåœ¨å¤šä¸ªCODåŸºå‡†æ•°æ®é›†ä¸Šå±•ç°å‡ºä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•(SOTA)ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰æé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»…éœ€åœ¨ä¸ªäººç¬”è®°æœ¬ç”µè„‘ä¸Šå³å¯å®Œæˆå®éªŒï¼Œä¸ºé«˜æ•ˆã€ä½æˆæœ¬çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15313v2",
      "published_date": "2025-08-21 07:14:18 UTC",
      "updated_date": "2025-09-15 05:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:15:11.150519+00:00"
    },
    {
      "arxiv_id": "2508.15310v1",
      "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents",
      "title_zh": "IPIGuardï¼šä¸€ç§åŸºäºå·¥å…·ä¾èµ–å›¾çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é—´æ¥æç¤ºè¯æ³¨å…¥é˜²å¾¡å·¥å…·",
      "authors": [
        "Hengyu An",
        "Jinghuai Zhang",
        "Tianyu Du",
        "Chunyi Zhou",
        "Qingming Li",
        "Tao Lin",
        "Shouling Ji"
      ],
      "abstract": "Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IPIGuardï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå·¥å…·ä¾èµ–å›¾(Tool Dependency Graph, TDG)çš„æ–°å‹é˜²å¾¡æœºåˆ¶ï¼Œæ—¨åœ¨åº”å¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å¤„ç†å¤–éƒ¨ä¸å¯ä¿¡æ•°æ®æ—¶é¢ä¸´çš„é—´æ¥æç¤ºæ³¨å…¥(Indirect Prompt Injection, IPI)å¨èƒã€‚é’ˆå¯¹ç°æœ‰é˜²å¾¡æ‰‹æ®µç¼ºä¹å¯¹æ™ºèƒ½ä½“è¡Œä¸ºç»“æ„åŒ–çº¦æŸä¸”éš¾ä»¥é˜»æ­¢è¿è§„å·¥å…·è°ƒç”¨çš„å±€é™æ€§ï¼ŒIPIGuardå°†ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹å»ºæ¨¡ä¸ºå¯¹é¢„å…ˆè§„åˆ’çš„TDGçš„éå†ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼åœ°å°†åŠ¨ä½œè§„åˆ’(Action Planning)ä¸å¤–éƒ¨æ•°æ®äº¤äº’è¿›è¡Œè§£è€¦ï¼Œèƒ½å¤Ÿä»æºå¤´ä¸Šæœ‰æ•ˆå‡å°‘ç”±æ¶æ„æŒ‡ä»¤è§¦å‘çš„éé¢„æœŸå·¥å…·è°ƒç”¨ã€‚åœ¨AgentDojoåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒIPIGuardåœ¨ä»»åŠ¡æœ‰æ•ˆæ€§ä¸é˜²å¾¡é²æ£’æ€§ä¹‹é—´å–å¾—äº†å“è¶Šå¹³è¡¡ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ›´å…·å®‰å…¨æ€§çš„æ™ºèƒ½ä½“ç³»ç»Ÿå¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.15310v1",
      "published_date": "2025-08-21 07:08:16 UTC",
      "updated_date": "2025-08-21 07:08:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:10.884825+00:00"
    },
    {
      "arxiv_id": "2508.16672v1",
      "title": "The AI Model Risk Catalog: What Developers and Researchers Miss About Real-World AI Harms",
      "title_zh": "AI æ¨¡å‹é£é™©ç›®å½•ï¼šå¼€å‘è€…ä¸ç ”ç©¶è€…å¯¹ç°å®ä¸–ç•Œäººå·¥æ™ºèƒ½å±å®³çš„è®¤çŸ¥ç›²ç‚¹",
      "authors": [
        "Pooja S. B. Rao",
        "Sanja Å Ä‡epanoviÄ‡",
        "Dinesh Babu Jayagopi",
        "Mauro Cherubini",
        "Daniele Quercia"
      ],
      "abstract": "We analyzed nearly 460,000 AI model cards from Hugging Face to examine how developers report risks. From these, we extracted around 3,000 unique risk mentions and built the \\emph{AI Model Risk Catalog}. We compared these with risks identified by researchers in the MIT Risk Repository and with real-world incidents from the AI Incident Database. Developers focused on technical issues like bias and safety, while researchers emphasized broader social impacts. Both groups paid little attention to fraud and manipulation, which are common harms arising from how people interact with AI. Our findings show the need for clearer, structured risk reporting that helps developers think about human-interaction and systemic risks early in the design process. The catalog and paper appendix are available at: https://social-dynamics.net/ai-risks/catalog.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æ Hugging Face ä¸Šçº¦ 460,000 ä»½ AI æ¨¡å‹å¡ (model cards)ï¼Œæå–äº† 3,000 æ¡é£é™©æåŠå¹¶æ„å»ºäº† AI æ¨¡å‹é£é™©ç›®å½• (AI Model Risk Catalog)ï¼Œæ—¨åœ¨æ­ç¤ºå¼€å‘è€…ã€ç ”ç©¶è€…ä¸ç°å®å±å®³ä¹‹é—´çš„è®¤çŸ¥å·®è·ã€‚é€šè¿‡å°†è¯¥ç›®å½•ä¸ MIT é£é™©åº“ (MIT Risk Repository) åŠ AI äº‹ä»¶æ•°æ®åº“ (AI Incident Database) è¿›è¡Œå¯¹æ¯”ï¼Œç ”ç©¶å‘ç°å¼€å‘è€…å€¾å‘äºå…³æ³¨åè§ (bias) å’Œå®‰å…¨æ€§ (safety) ç­‰æŠ€æœ¯é£é™©ï¼Œè€Œç ”ç©¶äººå‘˜åˆ™æ›´ä¾§é‡äºå®è§‚ç¤¾ä¼šå½±å“ã€‚ç„¶è€Œï¼Œç ”ç©¶æŒ‡å‡ºåŒæ–¹å‡ä¸¥é‡å¿½è§†äº†æ¬ºè¯ˆ (fraud) å’Œæ“æ§ (manipulation) ç­‰åœ¨çœŸå®åœºæ™¯ä¸­ç”±äººæœºäº¤äº’å¼•å‘çš„å¸¸è§å±å®³ã€‚è¿™ä¸€ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„é£é™©æŠ¥å‘Šæœºåˆ¶äºŸéœ€æ”¹è¿›ï¼Œåº”å¼•å¯¼å¼€å‘è€…åœ¨è®¾è®¡æ—©æœŸå°±å°†äººæœºäº¤äº’é£é™©å’Œç³»ç»Ÿæ€§é£é™©çº³å…¥è€ƒé‡ï¼Œå¹¶æä¾›æ›´å…·ç»“æ„åŒ–çš„é£é™©è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to AIES 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.16672v1",
      "published_date": "2025-08-21 07:07:41 UTC",
      "updated_date": "2025-08-21 07:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:15:15.391018+00:00"
    },
    {
      "arxiv_id": "2508.16671v1",
      "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification",
      "title_zh": "åŸºäºç»†ç²’åº¦éªŒè¯çš„åæ€å¼è®ºæ–‡ä»£ç å¤ç°",
      "authors": [
        "Mingyang Zhou",
        "Quanming Yao",
        "Lun Du",
        "Lanning Wei",
        "Da Zheng"
      ],
      "abstract": "Reproducing machine learning papers is essential for scientific progress but remains challenging for both humans and automated agents. Existing agent-based methods often struggle to fully and accurately reproduce implementation details such as mathematical formulas and algorithmic logic. Previous studies show that reflection with explicit feedback improves agent performance. However, current paper reproduction methods fail to effectively adopt this strategy. This gap mainly arises from the diverse paper patterns, complex method modules, and varied configurations encountered in research papers. Motivated by how humans use systematic checklists to efficiently debug complex code, we propose \\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction framework that automatically extracts a paper's fingerprint, referring to a comprehensive set of accurate and atomic criteria serving as high-quality supervisory signals. The framework first generates code based on the extracted information, and then leverages the fingerprint within iterative verification and refinement loop. This approach systematically detects discrepancies and produces targeted revisions to align generated code with the paper's implementation details. Extensive experiments on the PaperBench Code-Dev benchmark have been conducted, RePro achieves 13.0\\% performance gap over baselines, and it correctly revises complex logical and mathematical criteria in reflecting, on which the effectiveness is obvious.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReProï¼Œä¸€ç§åŸºäºç»†ç²’åº¦éªŒè¯çš„åæ€å¼è®ºæ–‡åˆ°ä»£ç å¤ç°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ è®ºæ–‡åœ¨å¤ç°è¿‡ç¨‹ä¸­ç”±äºæ•°å­¦å…¬å¼å’Œç®—æ³•é€»è¾‘å¤æ‚è€Œå¯¼è‡´çš„å‡†ç¡®æ€§éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æå–è®ºæ–‡çš„ fingerprintï¼Œå³ä¸€ç»„ä½œä¸ºé«˜è´¨é‡ç›‘ç£ä¿¡å·çš„åŸå­åŒ–æ ‡å‡†ï¼Œæ¥ç³»ç»Ÿåœ°æŒ‡å¯¼ä»£ç çš„ç”Ÿæˆä¸ä¼˜åŒ–ã€‚åœ¨å¤ç°è¿‡ç¨‹ä¸­ï¼ŒRePro åˆ©ç”¨è¯¥ fingerprint è¿›è¡Œè¿­ä»£å¼çš„éªŒè¯ä¸ç»†åŒ–å¾ªç¯ï¼Œèƒ½å¤Ÿç²¾å‡†æ£€æµ‹å¹¶ä¿®æ­£ä»£ç å®ç°ä¸è®ºæ–‡ç»†èŠ‚ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨ PaperBench Code-Dev åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRePro è¾ƒç°æœ‰åŸºçº¿æ¨¡å‹å®ç°äº† 13.0% çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„é€»è¾‘å’Œæ•°å­¦æ ‡å‡†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ¨¡æ‹Ÿäººç±»ç³»ç»ŸåŒ–è°ƒè¯•é€»è¾‘çš„æ–¹æ³•ï¼Œä¸ºå®ç°é«˜å¯é æ€§çš„è‡ªåŠ¨åŒ–è®ºæ–‡å¤ç°æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16671v1",
      "published_date": "2025-08-21 06:57:44 UTC",
      "updated_date": "2025-08-21 06:57:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:15:25.162983+00:00"
    },
    {
      "arxiv_id": "2508.15305v1",
      "title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning",
      "title_zh": "ç”¨äºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è§„åˆ’çš„ç”±ç²—åˆ°ç»†è½åœ°å¼è®°å¿†",
      "authors": [
        "Wei Yang",
        "Jinwei Xiao",
        "Hongming Zhang",
        "Qingyang Zhang",
        "Yanna Wang",
        "Bo Xu"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works focus on single-granularity memory derived from dynamic environmental interactions, which are inherently constrained by the quality of the collected experiences. This limitation, in turn, constrain the diversity of knowledge and the flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a novel framework that grounds coarse-to-fine memories with LLM, thereby fully leverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds environmental information into coarse-grained focus points to guide experience collection in training tasks, followed by grounding of actionable hybrid-grained tips from each experience. At inference, \\Ours{} retrieves task-relevant experiences and tips to support planning. When facing environmental anomalies, the LLM grounds the current situation into fine-grained key information, enabling flexible self-QA reflection and plan correction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸­é¢ä¸´çš„å•ä¸€ç²’åº¦è®°å¿†ã€ç»éªŒè´¨é‡å—é™åŠè§„åˆ’çµæ´»æ€§ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCoarse-to-Fine Grounded Memoryçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å°†ç¯å¢ƒä¿¡æ¯é”šå®š(Grounding)ä¸ºç²—ç²’åº¦çš„å…³æ³¨ç‚¹æ¥æŒ‡å¯¼è®­ç»ƒä»»åŠ¡ä¸­çš„ç»éªŒæ”¶é›†ï¼Œå¹¶éšåä»æ¯æ¡ç»éªŒä¸­æå–å‡ºæ··åˆç²’åº¦çš„å¯æ“ä½œæç¤º(Tips)ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ£€ç´¢ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„ç»éªŒå’Œæç¤ºä»¥è¾…åŠ©è§„åˆ’å†³ç­–ã€‚å½“é¢ä¸´ç¯å¢ƒå¼‚å¸¸æ—¶ï¼ŒLLMå°†å½“å‰æƒ…å¢ƒè¿›ä¸€æ­¥é”šå®šä¸ºç»†ç²’åº¦å…³é”®ä¿¡æ¯ï¼Œé€šè¿‡çµæ´»çš„è‡ªé—®è‡ªç­”(Self-QA)åæ€æœºåˆ¶å®ç°è®¡åˆ’çš„åŠ¨æ€çº åã€‚è¿™ç§ä»ç²—åˆ°ç»†çš„è®°å¿†æœºåˆ¶å……åˆ†åˆ©ç”¨äº†LLMçš„é”šå®šèƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ä¸è§„åˆ’çµæ´»æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to EMNLP 2025 Main Conference;27 pages,15 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15305v1",
      "published_date": "2025-08-21 06:50:23 UTC",
      "updated_date": "2025-08-21 06:50:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:15:24.345502+00:00"
    },
    {
      "arxiv_id": "2508.15297v1",
      "title": "DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding",
      "title_zh": "DesignCLIPï¼šé¢å‘å¤–è§‚è®¾è®¡ä¸“åˆ©ç†è§£çš„ CLIP å¤šæ¨¡æ€å­¦ä¹ ",
      "authors": [
        "Zhu Wang",
        "Homaira Huda Shomee",
        "Sathya N. Ravi",
        "Sourav Medya"
      ],
      "abstract": "In the field of design patent analysis, traditional tasks such as patent classification and patent image retrieval heavily depend on the image data. However, patent images -- typically consisting of sketches with abstract and structural elements of an invention -- often fall short in conveying comprehensive visual context and semantic information. This inadequacy can lead to ambiguities in evaluation during prior art searches. Recent advancements in vision-language models, such as CLIP, offer promising opportunities for more reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP models to develop a unified framework DesignCLIP for design patent applications with a large-scale dataset of U.S. design patents. To address the unique characteristics of patent data, DesignCLIP incorporates class-aware classification and contrastive learning, utilizing generated detailed captions for patent images and multi-views image learning. We validate the effectiveness of DesignCLIP across various downstream tasks, including patent classification and patent retrieval. Additionally, we explore multimodal patent retrieval, which provides the potential to enhance creativity and innovation in design by offering more diverse sources of inspiration. Our experiments show that DesignCLIP consistently outperforms baseline and SOTA models in the patent domain on all tasks. Our findings underscore the promise of multimodal approaches in advancing patent analysis. The codebase is available here: https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.",
      "tldr_zh": "é’ˆå¯¹å¤–è§‚è®¾è®¡ä¸“åˆ©åˆ†æä¸­ä¼ ç»Ÿæ–¹æ³•è¿‡åˆ†ä¾èµ–ç¼ºä¹è¯­ä¹‰ä¿¡æ¯çš„ä¸“åˆ©è‰å›¾è€Œå¯¼è‡´è¯„ä¼°æ­§ä¹‰çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†DesignCLIPï¼Œä¸€ç§åŸºäºCLIPæ¨¡å‹çš„ç»Ÿä¸€å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è§„æ¨¡ç¾å›½å¤–è§‚è®¾è®¡ä¸“åˆ©æ•°æ®é›†ï¼Œé€šè¿‡æ•´åˆç±»åˆ«æ„ŸçŸ¥åˆ†ç±»(class-aware classification)å’Œå¯¹æ¯”å­¦ä¹ (contrastive learning)æ¥æ•æ‰ä¸“åˆ©æ•°æ®çš„ç‹¬ç‰¹ç‰¹å¾ã€‚DesignCLIPå¼•å…¥äº†ç”Ÿæˆçš„å›¾åƒè¯¦ç»†æè¿°(detailed captions)å’Œå¤šè§†å›¾å›¾åƒå­¦ä¹ (multi-views image learning)æŠ€æœ¯ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä¸“åˆ©å›¾åƒåœ¨è§†è§‰ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ä¿¡æ¯ä¸Šçš„ä¸è¶³ã€‚ç ”ç©¶åœ¨ä¸“åˆ©åˆ†ç±»å’Œä¸“åˆ©æ£€ç´¢ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šå……åˆ†éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ¢ç´¢äº†å¤šæ¨¡æ€æ£€ç´¢åœ¨æ¿€å‘è®¾è®¡åˆ›æ–°æ–¹é¢çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDesignCLIPåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡æŒç»­ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹å’ŒSOTAæ¨¡å‹ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†å¤šæ¨¡æ€æ–¹æ³•åœ¨æ¨è¿›ä¸“åˆ©åˆ†æè‡ªåŠ¨åŒ–å’Œå‡†ç¡®æ€§æ–¹é¢çš„å·¨å¤§åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by EMNLP 2025. 22 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15297v1",
      "published_date": "2025-08-21 06:36:24 UTC",
      "updated_date": "2025-08-21 06:36:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:34.688341+00:00"
    },
    {
      "arxiv_id": "2508.15294v3",
      "title": "A Multi-Memory Segment System for Generating High-Quality Long-Term Memory Content in Agents",
      "title_zh": "é¢å‘æ™ºèƒ½ä½“é«˜è´¨é‡é•¿æ—¶è®°å¿†å†…å®¹ç”Ÿæˆçš„å¤šè®°å¿†ç‰‡æ®µç³»ç»Ÿ",
      "authors": [
        "Gaoke Zhang",
        "Bo Wang",
        "Yunlong Ma",
        "Dongming Zhao",
        "Zifei Yu"
      ],
      "abstract": "In the current field of agent memory, extensive explorations have been conducted in the area of memory retrieval, yet few studies have focused on exploring the memory content. Most research simply stores summarized versions of historical dialogues, as exemplified by methods like A-MEM and MemoryBank. However, when humans form long-term memories, the process involves multi-dimensional and multi-component generation, rather than merely creating simple summaries. The low-quality memory content generated by existing methods can adversely affect recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multi-memory segment system (MMS) inspired by cognitive psychology theory. The system processes short-term memory into multiple long-term memory segments, and constructs retrieval memory units and contextual memory units based on these segments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. We conducted experiments on the LoCoMo dataset and further performed ablation experiments, experiments on the robustness regarding the number of input memories, and overhead experiments, which demonstrated the effectiveness and practical value of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ™ºèƒ½ä½“ (agent) è®°å¿†ç³»ç»Ÿåœ¨è®°å¿†å†…å®¹ç”Ÿæˆè´¨é‡ä¸Šçš„ä¸è¶³ï¼Œå—è®¤çŸ¥å¿ƒç†å­¦ç†è®ºå¯å‘ï¼Œæå‡ºäº†ä¸€ç§å¤šè®°å¿†åˆ†æ®µç³»ç»Ÿ (MMS)ã€‚è¯¥ç³»ç»Ÿå°†çŸ­æœŸè®°å¿† (short-term memory) å¤„ç†ä¸ºå¤šä¸ªé•¿æœŸè®°å¿†åˆ†æ®µ (long-term memory segments)ï¼Œå¹¶æ®æ­¤æ„å»ºä¸€ä¸€å¯¹åº”çš„æ£€ç´¢è®°å¿†å•å…ƒ (retrieval memory units) å’Œä¸Šä¸‹æ–‡è®°å¿†å•å…ƒ (contextual memory units)ï¼Œä»¥è§£å†³ä¼ ç»Ÿç®€å•æ‘˜è¦æ–¹æ³•å¯¼è‡´çš„å¬å›æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚åœ¨æ£€ç´¢é˜¶æ®µï¼ŒMMS æ ¹æ®ç”¨æˆ·æŸ¥è¯¢åŒ¹é…æœ€ç›¸å…³çš„æ£€ç´¢å•å…ƒï¼Œå¹¶æå–ç›¸åº”çš„ä¸Šä¸‹æ–‡å•å…ƒä½œä¸ºå“åº”ç”Ÿæˆçš„å¢å¼ºçŸ¥è¯†ã€‚é€šè¿‡åœ¨ LoCoMo æ•°æ®é›†ä¸Šçš„æ¶ˆèå®éªŒã€é²æ£’æ€§å®éªŒåŠå¼€é”€åˆ†æï¼Œç ”ç©¶è¯å®äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡é•¿æœŸè®°å¿†å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15294v3",
      "published_date": "2025-08-21 06:29:42 UTC",
      "updated_date": "2026-01-04 06:38:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:43.011501+00:00"
    },
    {
      "arxiv_id": "2508.15277v1",
      "title": "Way to Build Native AI-driven 6G Air Interface: Principles, Roadmap, and Outlook",
      "title_zh": "æ„å»ºåŸç”Ÿ AI é©±åŠ¨çš„ 6G ç©ºå£ï¼šåŸåˆ™ã€è·¯çº¿å›¾ä¸å±•æœ›",
      "authors": [
        "Ping Zhang",
        "Kai Niu",
        "Yiming Liu",
        "Zijian Liang",
        "Nan Ma",
        "Xiaodong Xu",
        "Wenjun Xu",
        "Mengying Sun",
        "Yinqiu Liu",
        "Xiaoyun Wang",
        "Ruichen Zhang"
      ],
      "abstract": "Artificial intelligence (AI) is expected to serve as a foundational capability across the entire lifecycle of 6G networks, spanning design, deployment, and operation. This article proposes a native AI-driven air interface architecture built around two core characteristics: compression and adaptation. On one hand, compression enables the system to understand and extract essential semantic information from the source data, focusing on task relevance rather than symbol-level accuracy. On the other hand, adaptation allows the air interface to dynamically transmit semantic information across diverse tasks, data types, and channel conditions, ensuring scalability and robustness. This article first introduces the native AI-driven air interface architecture, then discusses representative enabling methodologies, followed by a case study on semantic communication in 6G non-terrestrial networks. Finally, it presents a forward-looking discussion on the future of native AI in 6G, outlining key challenges and research opportunities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ„å»ºåŸç”ŸAIé©±åŠ¨çš„6Gç©ºä¸­æ¥å£(Air Interface)çš„åŸåˆ™ã€è·¯çº¿å›¾ä¸å±•æœ›ï¼Œæå‡ºAIå°†ä½œä¸º6Gç½‘ç»œå…¨ç”Ÿå‘½å‘¨æœŸçš„åŸºç¡€èƒ½åŠ›ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§ä»¥å‹ç¼©(Compression)å’Œè‡ªé€‚åº”(Adaptation)ä¸ºæ ¸å¿ƒç‰¹æ€§çš„åŸç”ŸAIé©±åŠ¨ç©ºä¸­æ¥å£æ¶æ„ã€‚å‹ç¼©ç‰¹æ€§ä½¿ç³»ç»Ÿèƒ½å¤Ÿä»æºæ•°æ®ä¸­æå–å…³é”®è¯­ä¹‰ä¿¡æ¯ï¼Œå¼ºè°ƒä»»åŠ¡ç›¸å…³æ€§è€Œéä¼ ç»Ÿçš„ç¬¦å·çº§å‡†ç¡®åº¦ã€‚è‡ªé€‚åº”ç‰¹æ€§åˆ™å…è®¸ç©ºä¸­æ¥å£åœ¨ä¸åŒä»»åŠ¡ã€æ•°æ®ç±»å‹å’Œä¿¡é“æ¡ä»¶ä¸‹åŠ¨æ€ä¼ è¾“è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œç¡®ä¿ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ä¸é²æ£’æ€§ã€‚æ–‡ç« è¿›ä¸€æ­¥ä»‹ç»äº†ä»£è¡¨æ€§çš„èµ‹èƒ½æ–¹æ³•è®ºï¼Œå¹¶ä»¥6Géåœ°é¢ç½‘ç»œ(Non-Terrestrial Networks, NTN)ä¸­çš„è¯­ä¹‰é€šä¿¡ä¸ºä¾‹è¿›è¡Œäº†æ¡ˆä¾‹ç ”ç©¶ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å¯¹6GåŸç”ŸAIçš„æœªæ¥è¿›è¡Œäº†å‰ç»æ€§è®¨è®ºï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ä¸æ½œåœ¨çš„ç ”ç©¶æœºé‡ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "14 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15277v1",
      "published_date": "2025-08-21 06:11:04 UTC",
      "updated_date": "2025-08-21 06:11:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:29.592091+00:00"
    },
    {
      "arxiv_id": "2508.15262v1",
      "title": "M-$LLM^3$REC: A Motivation-Aware User-Item Interaction Framework for Enhancing Recommendation Accuracy with LLMs",
      "title_zh": "M-$LLM^3$RECï¼šä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æå‡æ¨èå‡†ç¡®ç‡çš„åŠ¨æœºæ„ŸçŸ¥ç”¨æˆ·-é¡¹ç›®äº¤äº’æ¡†æ¶",
      "authors": [
        "Lining Chen",
        "Qingwen Zeng",
        "Huaming Chen"
      ],
      "abstract": "Recommendation systems have been essential for both user experience and platform efficiency by alleviating information overload and supporting decision-making. Traditional methods, i.e., content-based filtering, collaborative filtering, and deep learning, have achieved impressive results in recommendation systems. However, the cold-start and sparse-data scenarios are still challenging to deal with. Existing solutions either generate pseudo-interaction sequence, which often introduces redundant or noisy signals, or rely heavily on semantic similarity, overlooking dynamic shifts in user motivation. To address these limitations, this paper proposes a novel recommendation framework, termed M-$LLM^3$REC, which leverages large language models for deep motivational signal extraction from limited user interactions. M-$LLM^3$REC comprises three integrated modules: the Motivation-Oriented Profile Extractor (MOPE), Motivation-Oriented Trait Encoder (MOTE), and Motivational Alignment Recommender (MAR). By emphasizing motivation-driven semantic modeling, M-$LLM^3$REC demonstrates robust, personalized, and generalizable recommendations, particularly boosting performance in cold-start situations in comparison with the state-of-the-art frameworks.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿåœ¨å†·å¯åŠ¨(cold-start)å’Œæ•°æ®ç¨€ç–(sparse-data)åœºæ™¯ä¸‹éš¾ä»¥æ•æ‰ç”¨æˆ·åŠ¨æ€åŠ¨æœºçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºM-$LLM^3$RECçš„æ–°å‹æ¨èæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»æœ‰é™çš„ç”¨æˆ·äº¤äº’ä¸­æ·±åº¦æŒ–æ˜åŠ¨æœºä¿¡å·ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ¡ˆä¸­ä¼ªäº¤äº’å™ªå£°è¿‡å¤šæˆ–è¿‡åº¦ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§çš„å±€é™ã€‚M-$LLM^3$RECç”±åŠ¨æœºå¯¼å‘æ¦‚å†µæå–å™¨(MOPE)ã€åŠ¨æœºå¯¼å‘ç‰¹å¾ç¼–ç å™¨(MOTE)å’ŒåŠ¨æœºå¯¹é½æ¨èå™¨(MAR)ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—é›†æˆè€Œæˆã€‚é€šè¿‡å¼ºåŒ–åŠ¨æœºé©±åŠ¨çš„è¯­ä¹‰å»ºæ¨¡ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨èçš„é²æ£’æ€§ã€ä¸ªæ€§åŒ–ç¨‹åº¦ä¸æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒM-$LLM^3$RECåœ¨å†·å¯åŠ¨æƒ…å¢ƒä¸‹çš„æ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›(state-of-the-art)æ¨¡å‹ï¼Œä¸ºç²¾å‡†æ¨èæä¾›äº†æœ‰æ•ˆçš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10pages",
      "pdf_url": "https://arxiv.org/pdf/2508.15262v1",
      "published_date": "2025-08-21 05:50:13 UTC",
      "updated_date": "2025-08-21 05:50:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:49.798975+00:00"
    },
    {
      "arxiv_id": "2508.15253v2",
      "title": "Conflict-Aware Soft Prompting for Retrieval-Augmented Generation",
      "title_zh": "é¢å‘æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å†²çªæ„ŸçŸ¥è½¯æç¤º",
      "authors": [
        "Eunseong Choi",
        "June Park",
        "Hyeri Lee",
        "Jongwuk Lee"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ä¸­å¤–éƒ¨æ£€ç´¢ä¸Šä¸‹æ–‡ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) å‚æ•°åŒ–çŸ¥è¯† (parametric knowledge) å†²çªçš„é—®é¢˜ï¼Œå³ä¸Šä¸‹æ–‡-è®°å¿†å†²çª (context-memory conflict)ï¼Œæå‡ºäº†åä¸º CARE (Conflict-Aware REtrieval-Augmented Generation) çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸Šä¸‹æ–‡è¯„ä¼°å™¨ (context assessor) å’ŒåŸºç¡€å¤§è¯­è¨€æ¨¡å‹ (base LLM) ç»„æˆï¼Œæ—¨åœ¨é€šè¿‡ç¼–ç ç´§å‡‘çš„è®°å¿†æ ‡è®°åµŒå…¥ (memory token embeddings) æ¥è¯„ä¼°ä¿¡æ¯çš„å¯é æ€§ã€‚é€šè¿‡åŸºäºåŸºå‡†æˆ–å¯¹æŠ—æ€§çš„è½¯æç¤º (grounded/adversarial soft prompting) è®­ç»ƒï¼ŒCARE èƒ½å¤Ÿè¯†åˆ«ä¸å¯é çš„ä¸Šä¸‹æ–‡å¹¶æ•è·å¼•å¯¼ä¿¡å·ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹æ¨ç†è½¬å‘æ›´å‡†ç¡®çš„çŸ¥è¯†æ¥æºã€‚å®éªŒè¯æ˜ï¼ŒCARE åœ¨é—®ç­” (QA) å’Œäº‹å®æ ¸æŸ¥ (fact-checking) ä»»åŠ¡ä¸­å®ç°äº† 5.0% çš„å¹³å‡æ€§èƒ½æå‡ï¼Œæœ‰æ•ˆç¼“è§£äº†çŸ¥è¯†å†²çªã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å…·ä¿¡ä»»åº¦å’Œè‡ªé€‚åº”èƒ½åŠ›çš„ RAG ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025; 15 pages; 5 figures, 11 tables; Code available at https://github.com/eunseongc/CARE",
      "pdf_url": "https://arxiv.org/pdf/2508.15253v2",
      "published_date": "2025-08-21 05:36:29 UTC",
      "updated_date": "2025-09-26 09:07:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:54.193331+00:00"
    },
    {
      "arxiv_id": "2508.15251v1",
      "title": "Explainable Knowledge Distillation for Efficient Medical Image Classification",
      "title_zh": "é¢å‘é«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†ç±»çš„å¯è§£é‡ŠçŸ¥è¯†è’¸é¦",
      "authors": [
        "Aqib Nazir Mir",
        "Danish Raza Rizvi"
      ],
      "abstract": "This study comprehensively explores knowledge distillation frameworks for COVID-19 and lung cancer classification using chest X-ray (CXR) images. We employ high-capacity teacher models, including VGG19 and lightweight Vision Transformers (Visformer-S and AutoFormer-V2-T), to guide the training of a compact, hardware-aware student model derived from the OFA-595 supernet. Our approach leverages hybrid supervision, combining ground-truth labels with teacher models' soft targets to balance accuracy and computational efficiency. We validate our models on two benchmark datasets: COVID-QU-Ex and LCS25000, covering multiple classes, including COVID-19, healthy, non-COVID pneumonia, lung, and colon cancer. To interpret the spatial focus of the models, we employ Score-CAM-based visualizations, which provide insight into the reasoning process of both teacher and student networks. The results demonstrate that the distilled student model maintains high classification performance with significantly reduced parameters and inference time, making it an optimal choice in resource-constrained clinical environments. Our work underscores the importance of combining model efficiency with explainability for practical, trustworthy medical AI solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”¨äº COVID-19 å’Œè‚ºç™Œåˆ†ç±»çš„å¯è§£é‡Š Knowledge Distillation æ¡†æ¶ï¼Œæ—¨åœ¨æå‡èƒ¸éƒ¨ X å°„çº¿ (CXR) å½±åƒåˆ†ç±»çš„æ•ˆç‡ã€‚ç ”ç©¶é‡‡ç”¨ VGG19ã€Visformer-S å’Œ AutoFormer-V2-T ä½œä¸ºé«˜å®¹é‡æ•™å¸ˆæ¨¡å‹ï¼Œå¼•å¯¼åŸºäº OFA-595 supernet å¼€å‘çš„ç¡¬ä»¶æ„ŸçŸ¥å‹è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ã€‚å®éªŒç»“åˆäº† Ground-truth æ ‡ç­¾ä¸æ•™å¸ˆæ¨¡å‹çš„ Soft Targets é‡‡ç”¨æ··åˆç›‘ç£æ¨¡å¼ï¼Œä»¥å¹³è¡¡åˆ†ç±»å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ¡ˆåœ¨ COVID-QU-Ex å’Œ LCS25000 åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–äº†è‚ºç‚åŠå¤šç§ç™Œç—‡åˆ†ç±»ä»»åŠ¡ã€‚ä¸ºäº†æé«˜å†³ç­–çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶å¼•å…¥äº† Score-CAM å¯è§†åŒ–æŠ€æœ¯ï¼Œå¯¹æ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œçš„ç©ºé—´å…³æ³¨ç‚¹åŠæ¨ç†è¿‡ç¨‹è¿›è¡Œæ·±å…¥è§£æã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡è’¸é¦çš„å­¦ç”Ÿæ¨¡å‹åœ¨æ˜¾è‘—å‡å°‘å‚æ•°é‡å’Œæ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œä¿æŒäº†æé«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œä¸ºèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒæä¾›äº†å…¼å…·æ•ˆç‡ä¸å¯ä¿¡åº¦çš„åŒ»ç–— AI è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15251v1",
      "published_date": "2025-08-21 05:22:47 UTC",
      "updated_date": "2025-08-21 05:22:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:16:52.505522+00:00"
    },
    {
      "arxiv_id": "2508.15250v3",
      "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling",
      "title_zh": "EMNLPï¼šæ•™å¸ˆè§’è‰²å¤§è¯­è¨€æ¨¡å‹çš„é“å¾·ä¸è§„èŒƒç”»åƒ",
      "authors": [
        "Yilin Jiang",
        "Mingzi Zhang",
        "Sheng Jin",
        "Zengyi Yu",
        "Xiangjie Kong",
        "Binghao Tu"
      ],
      "abstract": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EMNLP æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ•™è‚²è€…è§’è‰²ï¼ˆEducator-roleï¼‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é“å¾·ä¸è§„èŒƒå‰–æç³»ç»Ÿï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä¸“ä¸šè§’è‰²æ¨¡æ‹Ÿï¼ˆSimulating Professions, SPï¼‰ä¸­çš„å¿ƒç†ç‰¹è´¨ä¸ä¼¦ç†é£é™©ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»º 88 ä¸ªæ•™å¸ˆç‰¹æœ‰çš„é“å¾·å›°å¢ƒï¼ˆmoral dilemmasï¼‰å¹¶ç»“åˆè½¯æç¤ºæ³¨å…¥ï¼ˆsoft prompt injectionï¼‰æµ‹è¯•ï¼Œå®ç°äº†å¯¹æ¨¡å‹äººæ ¼ç‰¹è´¨ã€é“å¾·å‘å±•é˜¶æ®µåŠåˆè§„æ€§çš„æ·±åº¦è¯„æµ‹ã€‚é€šè¿‡å¯¹ 14 ä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹çš„å®éªŒå‘ç°ï¼Œæ‰®æ¼”æ•™å¸ˆè§’è‰²çš„ LLMs è¡¨ç°å‡ºæ¯”äººç±»æ•™å¸ˆæ›´ç†æƒ³åŒ–ä¸”æåŒ–çš„äººæ ¼ï¼Œè™½æ“…é•¿æŠ½è±¡é“å¾·æ¨ç†ï¼Œä½†åœ¨å¤„ç†æƒ…æ„Ÿå¤æ‚çš„æƒ…å¢ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨ç†èƒ½åŠ›ä¸å®‰å…¨æ€§ä¹‹é—´çš„æ‚–è®ºï¼Œå³æ¨ç†èƒ½åŠ›è¶Šå¼ºçš„æ¨¡å‹åœ¨é¢å¯¹æœ‰å®³æç¤ºæ³¨å…¥æ—¶å¾€å¾€è¡¨ç°å¾—æ›´ä¸ºè„†å¼±ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹æ•™è‚² AI ä¼¦ç†ä¸å¿ƒç†å¯¹é½çš„åŸºå‡†æµ‹è¯•ï¼ŒEMNLP ä¸ºæ„å»ºå®‰å…¨ã€åˆè§„çš„ä¸“ä¸šåŒ–å¤§æ¨¡å‹æä¾›äº†é‡è¦çš„è¯„ä»·æ ‡å‡†ä¸æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "29pages, 15 figures, Accepted by EMNLP Main Confrence",
      "pdf_url": "https://arxiv.org/pdf/2508.15250v3",
      "published_date": "2025-08-21 05:21:37 UTC",
      "updated_date": "2025-11-10 03:05:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:00.796606+00:00"
    },
    {
      "arxiv_id": "2508.15240v2",
      "title": "Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas",
      "title_zh": "åŸºäºè®¡ç®—æ™ºèƒ½çš„æ··åˆåŠŸèƒ½åŒºåœŸåœ°åˆ©ç”¨åˆ†é…æ–¹æ³•",
      "authors": [
        "Sabab Aosaf",
        "Muhammad Ali Nayeem",
        "Afsana Haque",
        "M Sohel Rahman"
      ],
      "abstract": "Urban land-use allocation represents a complex multi-objective optimization problem critical for sustainable urban development policy. This paper presents novel computational intelligence approaches for optimizing land-use allocation in mixed-use areas, addressing inherent trade-offs between land-use compatibility and economic objectives. We develop multiple optimization algorithms, including custom variants integrating differential evolution with multi-objective genetic algorithms. Key contributions include: (1) CR+DES algorithm leveraging scaled difference vectors for enhanced exploration, (2) systematic constraint relaxation strategy improving solution quality while maintaining feasibility, and (3) statistical validation using Kruskal-Wallis tests with compact letter displays. Applied to a real-world case study with 1,290 plots, CR+DES achieves 3.16\\% improvement in land-use compatibility compared to state-of-the-art methods, while MSBX+MO excels in price optimization with 3.3\\% improvement. Statistical analysis confirms algorithms incorporating difference vectors significantly outperform traditional approaches across multiple metrics. The constraint relaxation technique enables broader solution space exploration while maintaining practical constraints. These findings provide urban planners and policymakers with evidence-based computational tools for balancing competing objectives in land-use allocation, supporting more effective urban development policies in rapidly urbanizing regions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚åœŸåœ°åˆ©ç”¨åˆ†é…è¿™ä¸€å¤æ‚çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†æ–°å‹çš„è®¡ç®—æ™ºèƒ½(Computational Intelligence)æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡æ··åˆç”¨é€”åŒºåŸŸä¸­åœŸåœ°åˆ©ç”¨ç›¸å®¹æ€§ä¸ç»æµç›®æ ‡ä¹‹é—´çš„æƒè¡¡ã€‚ç ”ç©¶å¼€å‘äº†å¤šç§ä¼˜åŒ–ç®—æ³•ï¼ŒåŒ…æ‹¬å°†å·®åˆ†è¿›åŒ–(Differential Evolution)ä¸å¤šç›®æ ‡é—ä¼ ç®—æ³•(Multi-objective Genetic Algorithms)ç›¸ç»“åˆçš„å®šåˆ¶å˜ä½“ã€‚æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬åˆ©ç”¨æ¯”ä¾‹å·®åˆ†å‘é‡å¢å¼ºæ¢ç´¢èƒ½åŠ›çš„CR+DESç®—æ³•ï¼Œä»¥åŠåœ¨ä¿æŒå¯è¡Œæ€§çš„åŒæ—¶æ‰©å¤§æœç´¢ç©ºé—´çš„ç³»ç»Ÿæ€§çº¦æŸæ¾å¼›(Constraint Relaxation)ç­–ç•¥ã€‚åœ¨æ¶‰åŠ1,290ä¸ªåœ°å—çš„çœŸå®æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒCR+DESç®—æ³•åœ¨åœŸåœ°åˆ©ç”¨ç›¸å®¹æ€§æ–¹é¢æ¯”ç°æœ‰å…ˆè¿›æ–¹æ³•æå‡äº†3.16%ï¼Œè€ŒMSBX+MOç®—æ³•åœ¨ä»·æ ¼ä¼˜åŒ–ä¸Šå®ç°äº†3.3%çš„å¢é•¿ã€‚ç»Ÿè®¡åˆ†æ(Kruskal-Wallis tests)è¯å®ï¼Œå¼•å…¥å·®åˆ†å‘é‡çš„ç®—æ³•åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚è¿™äº›ç ”ç©¶æˆæœä¸ºåŸå¸‚è§„åˆ’è€…æä¾›äº†å¾ªè¯è®¡ç®—å·¥å…·ï¼Œæœ‰åŠ©äºåœ¨å¿«é€ŸåŸå¸‚åŒ–è¿›ç¨‹ä¸­æœ‰æ•ˆå¹³è¡¡å¤šä¸ªç«äº‰æ€§å¼€å‘ç›®æ ‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15240v2",
      "published_date": "2025-08-21 05:00:44 UTC",
      "updated_date": "2025-08-23 03:02:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:03.100491+00:00"
    },
    {
      "arxiv_id": "2508.15230v1",
      "title": "Robust and Efficient Quantum Reservoir Computing with Discrete Time Crystal",
      "title_zh": "åŸºäºç¦»æ•£æ—¶é—´æ™¶ä½“çš„é²æ£’é«˜æ•ˆé‡å­å‚¨å¤‡æ± è®¡ç®—",
      "authors": [
        "Da Zhang",
        "Xin Li",
        "Yibin Guo",
        "Haifeng Yu",
        "Yirong Jin",
        "Zhang-Qi Yin"
      ],
      "abstract": "The rapid development of machine learning and quantum computing has placed quantum machine learning at the forefront of research. However, existing quantum machine learning algorithms based on quantum variational algorithms face challenges in trainability and noise robustness. In order to address these challenges, we introduce a gradient-free, noise-robust quantum reservoir computing algorithm that harnesses discrete time crystal dynamics as a reservoir. We first calibrate the memory, nonlinear, and information scrambling capacities of the quantum reservoir, revealing their correlation with dynamical phases and non-equilibrium phase transitions. We then apply the algorithm to the binary classification task and establish a comparative quantum kernel advantage. For ten-class classification, both noisy simulations and experimental results on superconducting quantum processors match ideal simulations, demonstrating the enhanced accuracy with increasing system size and confirming the topological noise robustness. Our work presents the first experimental demonstration of quantum reservoir computing for image classification based on digital quantum simulation. It establishes the correlation between quantum many-body non-equilibrium phase transitions and quantum machine learning performance, providing new design principles for quantum reservoir computing and broader quantum machine learning algorithms in the NISQ era.",
      "tldr_zh": "é’ˆå¯¹ç°æœ‰ Quantum Machine Learning ç®—æ³•åœ¨è®­ç»ƒå’Œå™ªå£°é²æ£’æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ Discrete Time Crystal åŠ¨åŠ›å­¦ä½œä¸ºå‚¨å¤‡æ± çš„æ¢¯åº¦æ— å…³ä¸”å…·æœ‰å™ªå£°é²æ£’æ€§çš„ Quantum Reservoir Computing ç®—æ³•ã€‚ç ”ç©¶é€šè¿‡æ ¡å‡†é‡å­å‚¨å¤‡æ± çš„ memoryã€nonlinear å’Œ information scrambling èƒ½åŠ›ï¼Œæ­ç¤ºäº†å…¶æ€§èƒ½ä¸åŠ¨åŠ›å­¦ç›¸åŠéå¹³è¡¡ç›¸å˜ï¼ˆnon-equilibrium phase transitionsï¼‰ä¹‹é—´çš„ç´§å¯†å…³è”ã€‚åœ¨äºŒåˆ†ç±»å’Œååˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¶…å¯¼é‡å­å¤„ç†å™¨ä¸Šçš„å®éªŒç»“æœä¸ç†æƒ³æ¨¡æ‹Ÿç›¸åŒ¹é…ï¼ŒéªŒè¯äº†å‡†ç¡®ç‡éšç³»ç»Ÿè§„æ¨¡å¢åŠ è€Œæå‡çš„ç‰¹æ€§ä»¥åŠæ˜¾è‘—çš„æ‹“æ‰‘å™ªå£°é²æ£’æ€§ã€‚è¯¥å·¥ä½œå®ç°äº†é¦–ä¸ªåŸºäºæ•°å­—é‡å­æ¨¡æ‹Ÿï¼ˆdigital quantum simulationï¼‰è¿›è¡Œå›¾åƒåˆ†ç±»çš„ Quantum Reservoir Computing å®éªŒæ¼”ç¤ºã€‚è¿™é¡¹ç ”ç©¶æˆåŠŸå»ºç«‹äº†é‡å­å¤šä½“éå¹³è¡¡ç›¸å˜ä¸æœºå™¨å­¦ä¹ æ€§èƒ½ä¹‹é—´çš„è”ç³»ï¼Œä¸º NISQ æ—¶ä»£çš„é‡å­æœºå™¨å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†å…¨æ–°çš„ç†è®ºåŸåˆ™ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "12 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15230v1",
      "published_date": "2025-08-21 04:40:46 UTC",
      "updated_date": "2025-08-21 04:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:09.587501+00:00"
    },
    {
      "arxiv_id": "2508.15229v2",
      "title": "VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models",
      "title_zh": "VocabTailorï¼šé¢å‘å°è¯­è¨€æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡çš„åŠ¨æ€è¯è¡¨é€‰æ‹©",
      "authors": [
        "Hanling Zhang",
        "Yayu Zhou",
        "Tongcheng Fang",
        "Zhihang Yuan",
        "Guohao Dai",
        "Wanli Ouyang",
        "Yu Wang"
      ],
      "abstract": "Small Language Models (SLMs) provide computational advantages in resource-constrained environments, yet memory limitations remain a critical bottleneck for edge device deployment. A substantial portion of SLMs' memory footprint stems from vocabulary-related components, particularly embeddings and language modeling (LM) heads, due to large vocabulary sizes. Existing static vocabulary pruning, while reducing memory usage, suffers from rigid, one-size-fits-all designs that cause information loss from the prefill stage and a lack of flexibility. In this work, we identify two key principles underlying the vocabulary reduction challenge: the lexical locality principle, the observation that only a small subset of tokens is required during any single inference, and the asymmetry in computational characteristics between vocabulary-related components of SLM. Based on these insights, we introduce VocabTailor, a novel decoupled dynamic vocabulary selection framework that addresses memory constraints through offloading embedding and implements a hybrid static-dynamic vocabulary selection strategy for LM Head, enabling on-demand loading of vocabulary components. Comprehensive experiments across diverse downstream tasks demonstrate that VocabTailor achieves a reduction of up to 99% in the memory usage of vocabulary-related components with minimal or no degradation in task performance, substantially outperforming existing static vocabulary pruning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ—¶çš„å†…å­˜ç“¶é¢ˆï¼Œæå‡ºäº†VocabTailorï¼Œä¸€ç§è§£è€¦çš„åŠ¨æ€è¯è¡¨é€‰æ‹©æ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰é™æ€è¯æ±‡å‰ªæå­˜åœ¨è®¾è®¡åƒµåŒ–ä¸”ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œè€ŒVocabTailoråŸºäºè¯æ±‡å±€éƒ¨æ€§ï¼ˆLexical Localityï¼‰åŸåˆ™å’Œè¯è¡¨ç»„ä»¶é—´çš„è®¡ç®—ç‰¹å¾ä¸å¯¹ç§°æ€§ï¼Œå®ç°äº†è¯è¡¨ç»„ä»¶çš„æŒ‰éœ€åŠ è½½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¸è½½ï¼ˆOffloadingï¼‰Embeddingå¹¶å¯¹LM Headå®æ–½æ··åˆé™æ€-åŠ¨æ€è¯è¡¨é€‰æ‹©ç­–ç•¥ï¼Œæå¤§åœ°ä¼˜åŒ–äº†å†…å­˜ä½¿ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒVocabTailoråœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­èƒ½å¤Ÿå°†è¯è¡¨ç›¸å…³ç»„ä»¶çš„å†…å­˜å ç”¨é™ä½é«˜è¾¾99%ï¼Œä¸”å‡ ä¹ä¸æŸå¤±æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„é™æ€å‰ªææŠ€æœ¯ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹é«˜æ•ˆæ¨ç†æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15229v2",
      "published_date": "2025-08-21 04:32:13 UTC",
      "updated_date": "2026-01-06 02:17:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:12.601309+00:00"
    },
    {
      "arxiv_id": "2508.15227v1",
      "title": "GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design",
      "title_zh": "GenTuneï¼šé€šè¿‡å¯è¿½æº¯æç¤ºè¯æå‡ç¯å¢ƒè®¾è®¡ä¸­å›¾åƒç²¾ä¿®çš„å¯æ§æ€§",
      "authors": [
        "Wen-Fan Wang",
        "Ting-Ying Lee",
        "Chien-Ting Lu",
        "Che-Wei Hsu",
        "Nil Ponsa CampanyÃ ",
        "Yu Chen",
        "Mike Y. Chen",
        "Bing-Yu Chen"
      ],
      "abstract": "Environment designers in the entertainment industry create imaginative 2D and 3D scenes for games, films, and television, requiring both fine-grained control of specific details and consistent global coherence. Designers have increasingly integrated generative AI into their workflows, often relying on large language models (LLMs) to expand user prompts for text-to-image generation, then iteratively refining those prompts and applying inpainting. However, our formative study with 10 designers surfaced two key challenges: (1) the lengthy LLM-generated prompts make it difficult to understand and isolate the keywords that must be revised for specific visual elements; and (2) while inpainting supports localized edits, it can struggle with global consistency and correctness. Based on these insights, we present GenTune, an approach that enhances human--AI collaboration by clarifying how AI-generated prompts map to image content. Our GenTune system lets designers select any element in a generated image, trace it back to the corresponding prompt labels, and revise those labels to guide precise yet globally consistent image refinement. In a summative study with 20 designers, GenTune significantly improved prompt--image comprehension, refinement quality, and efficiency, and overall satisfaction (all $p < .01$) compared to current practice. A follow-up field study with two studios further demonstrated its effectiveness in real-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¨±ä¹è¡Œä¸šç¯å¢ƒè®¾è®¡å¸ˆåœ¨åˆ©ç”¨ç”Ÿæˆå¼ AI æ—¶é¢ä¸´çš„é•¿ Prompt éš¾ä»¥è§£æä»¥åŠ Inpainting æŠ€æœ¯å…¨å±€ä¸€è‡´æ€§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º GenTune çš„åä½œç³»ç»Ÿã€‚GenTune æ ¸å¿ƒåœ¨äºå¢å¼º AI ç”Ÿæˆçš„ Prompt ä¸å›¾åƒå†…å®¹ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œä½¿è®¾è®¡å¸ˆèƒ½å¤Ÿé€šè¿‡é€‰æ‹©å›¾åƒä¸­çš„ç‰¹å®šå…ƒç´ æ¥è¿½æº¯å¹¶ä¿®æ”¹å¯¹åº”çš„ Prompt æ ‡ç­¾ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°æå‡äº†äººæœºåä½œçš„é€æ˜åº¦ï¼Œä½¿è®¾è®¡å¸ˆèƒ½å¤Ÿåœ¨ä¿æŒå…¨å±€ä¸€è‡´æ€§çš„å‰æä¸‹ï¼Œå®ç°å¯¹å›¾åƒç»†èŠ‚çš„ç²¾ç¡®ç»†åŒ–ä¸æ§åˆ¶ã€‚åœ¨ä¸€é¡¹æ¶‰åŠ 20 åè®¾è®¡å¸ˆçš„æ€»ç»“æ€§ç ”ç©¶å’Œä¸¤å®¶å·¥ä½œå®¤çš„å®åœ°æµ‹è¯•ä¸­ï¼ŒGenTune åœ¨æå‡ Prompt-Image ç†è§£åŠ›ã€ç»†åŒ–è´¨é‡åŠå·¥ä½œæ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰å®è·µï¼ˆ$p < .01$ï¼‰ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å»ºç«‹å¯è¿½æº¯çš„ Prompt æ˜ å°„ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç¯å¢ƒè®¾è®¡ä¸­ç”Ÿæˆå¼ AI çš„å¯æ§æ€§ä¸ç”¨æˆ·æ»¡æ„åº¦ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted ACM Symposium on User Interface Software and Technology (UIST '25)",
      "pdf_url": "https://arxiv.org/pdf/2508.15227v1",
      "published_date": "2025-08-21 04:31:01 UTC",
      "updated_date": "2025-08-21 04:31:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:29.190079+00:00"
    },
    {
      "arxiv_id": "2508.15222v2",
      "title": "See it. Say it. Sorted: Agentic System for Compositional Diagram Generation",
      "title_zh": "See it. Say it. Sortedï¼šé¢å‘ç»„åˆå¼å›¾è¡¨ç”Ÿæˆçš„æ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Hantao Zhang",
        "Jingyang Liu",
        "Ed Li"
      ],
      "abstract": "We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion models)åœ¨ç”Ÿæˆæµç¨‹å›¾æ—¶é¢ä¸´çš„ç©ºé—´ç²¾åº¦å’Œç¬¦å·ç»“æ„é—®é¢˜ï¼Œæå‡ºäº†See it. Say it. Sorted.ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡è€¦åˆè§†è§‰è¯­è¨€æ¨¡å‹(VLM)ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢(SVG)ç¨‹åºã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªè¿­ä»£å¾ªç¯æœºåˆ¶ï¼šç”±Critic VLMæå‡ºå®šæ€§å…³ç³»ä¿®æ”¹å»ºè®®ï¼Œå¤šä¸ªLLMé‡‡ç”¨ä¸åŒç­–ç•¥åˆæˆSVGæ›´æ–°ï¼Œæœ€åç”±Judge VLMç­›é€‰æœ€ä¼˜ç»“æœï¼Œä»¥ç¡®ä¿å…¨å±€çº¦æŸã€å¸ƒå±€å¯¹é½å’Œè¿æ¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‰å›¾åˆ°å›¾è¡¨çš„é‡å»ºä»»åŠ¡ä¸­æ¯”GPT-5å’ŒGemini-2.5-Proç­‰å‰æ²¿æ¨¡å‹æ›´å…·é²æ£’æ€§ï¼Œèƒ½å¤Ÿå‡†ç¡®ç»„åˆå¤šå¤´ç®­å¤´ç­‰å¤æ‚åŸºå…ƒè€Œä¸äº§ç”Ÿå†—ä½™æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒäººå·¥å¹²é¢„ï¼Œå¹¶å¯é€šè¿‡APIæ‰©å±•è‡³PowerPointç­‰æ¼”ç¤ºå·¥å…·ï¼Œç›®å‰ç›¸å…³ä»£ç å·²å¼€æºã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15222v2",
      "published_date": "2025-08-21 04:20:36 UTC",
      "updated_date": "2025-11-16 01:12:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:36.056840+00:00"
    },
    {
      "arxiv_id": "2508.16670v1",
      "title": "COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture",
      "title_zh": "åŸºäº DenseNet æ¶æ„çš„è‚ºéƒ¨ CT å½±åƒ COVID19 é¢„æµ‹",
      "authors": [
        "Deborup Sanyal"
      ],
      "abstract": "COVID19 took the world by storm since December 2019. A highly infectious communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020, the World Health Organization (WHO) declared COVID19 as a global pandemic. A pandemic in the 21st century after almost 100 years was something the world was not prepared for, which resulted in the deaths of around 1.6 million people worldwide. The most common symptoms of COVID19 were associated with the respiratory system and resembled a cold, flu, or pneumonia. After extensive research, doctors and scientists concluded that the main reason for lives being lost due to COVID19 was failure of the respiratory system. Patients were dying gasping for breath. Top healthcare systems of the world were failing badly as there was an acute shortage of hospital beds, oxygen cylinders, and ventilators. Many were dying without receiving any treatment at all. The aim of this project is to help doctors decide the severity of COVID19 by reading the patient's Computed Tomography (CT) scans of the lungs. Computer models are less prone to human error, and Machine Learning or Neural Network models tend to give better accuracy as training improves over time. We have decided to use a Convolutional Neural Network model. Given that a patient tests positive, our model will analyze the severity of COVID19 infection within one month of the positive test result. The severity of the infection may be promising or unfavorable (if it leads to intubation or death), based entirely on the CT scans in the dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº DenseNet æ¶æ„çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨è‚ºéƒ¨ CT Scans å¯¹ COVID19 æ‚£è€…çš„ç—…æƒ…ä¸¥é‡ç¨‹åº¦è¿›è¡Œè‡ªåŠ¨é¢„æµ‹ã€‚é’ˆå¯¹å…¨çƒå¤§æµè¡ŒæœŸé—´åŒ»ç–—èµ„æºç´§ç¼ºåŠäººå·¥è¯Šæ–­æ˜“å—ä¸»è§‚å› ç´ å½±å“çš„é—®é¢˜ï¼Œè¯¥é¡¹ç›®é€šè¿‡è®­ç»ƒ Convolutional Neural Network æ¨¡å‹æ¥åˆ†æè‚ºéƒ¨å½±åƒç‰¹å¾ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æ ¸é…¸æ£€æµ‹é˜³æ€§åçš„æ‚£è€…ï¼Œé€šè¿‡å…¶ä¸€ä¸ªæœˆå†…çš„ CT Scans é¢„æµ‹æ„ŸæŸ“ç»“æœæ˜¯é¢„åè‰¯å¥½è¿˜æ˜¯è¶‹å‘æ¶åŒ–ï¼ˆå¦‚éœ€æ°”ç®¡æ’ç®¡æˆ–å¯¼è‡´æ­»äº¡ï¼‰ã€‚ç ”ç©¶å¼ºè°ƒäº† Machine Learning å’Œ Neural Network åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡æŒç»­è®­ç»ƒä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚è¯¥æ–¹æ¡ˆä¸ºåŒ»ç”Ÿè¯„ä¼° COVID19 ä¸¥é‡ç¨‹åº¦æä¾›äº†å®¢è§‚çš„å†³ç­–æ”¯æŒå·¥å…·ï¼Œæœ‰åŠ©äºåœ¨åŒ»ç–—å‹åŠ›è¾ƒå¤§çš„ç¯å¢ƒä¸‹å®ç°æ›´ç²¾å‡†çš„ä¸´åºŠåˆ†çº§ä¸èµ„æºåˆ†é…ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16670v1",
      "published_date": "2025-08-21 04:12:57 UTC",
      "updated_date": "2025-08-21 04:12:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:35.669842+00:00"
    },
    {
      "arxiv_id": "2508.15220v1",
      "title": "Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models",
      "title_zh": "é»‘ç›’æœºå™¨å­¦ä¹ æ¨¡å‹çš„å±€éƒ¨å¸•ç´¯æ‰˜æœ€ä¼˜è§£é‡Š",
      "authors": [
        "Aniruddha Joshi",
        "Supratik Chakraborty",
        "S Akshay",
        "Shetal Shah",
        "Hazem Torfah",
        "Sanjit Seshia"
      ],
      "abstract": "Creating meaningful interpretations for black-box machine learning models involves balancing two often conflicting objectives: accuracy and explainability. Exploring the trade-off between these objectives is essential for developing trustworthy interpretations. While many techniques for multi-objective interpretation synthesis have been developed, they typically lack formal guarantees on the Pareto-optimality of the results. Methods that do provide such guarantees, on the other hand, often face severe scalability limitations when exploring the Pareto-optimal space. To address this, we develop a framework based on local optimality guarantees that enables more scalable synthesis of interpretations. Specifically, we consider the problem of synthesizing a set of Pareto-optimal interpretations with local optimality guarantees, within the immediate neighborhood of each solution. Our approach begins with a multi-objective learning or search technique, such as Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of Pareto-optimal candidates with respect to accuracy and explainability. We then verify local optimality for each candidate as a Boolean satisfiability problem, which we solve using a SAT solver. We demonstrate the efficacy of our approach on a set of benchmarks, comparing it against previous methods for exploring the Pareto-optimal front of interpretations. In particular, we show that our approach yields interpretations that closely match those synthesized by methods offering global guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é»‘ç›’æœºå™¨å­¦ä¹ æ¨¡å‹è§£é‡Šä¸­å‡†ç¡®æ€§(accuracy)ä¸å¯è§£é‡Šæ€§(explainability)ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œæ—¨åœ¨å¯»æ‰¾ä¸¤è€…çš„å¸•ç´¯æ‰˜æœ€ä¼˜(Pareto-optimality)å¹³è¡¡ç‚¹ã€‚é’ˆå¯¹ç°æœ‰å…·å¤‡æœ€ä¼˜æ€§ä¿è¯çš„æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é—®é¢˜æ—¶å­˜åœ¨çš„å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºå±€éƒ¨æœ€ä¼˜(local optimality)ä¿è¯çš„è§£é‡Šåˆæˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šç›®æ ‡è’™ç‰¹å¡æ´›æ ‘æœç´¢(Multi-Objective Monte Carlo Tree Search)ç”Ÿæˆå€™é€‰è§£é‡Šé›†ï¼Œéšåå°†å±€éƒ¨æœ€ä¼˜æ€§éªŒè¯è½¬åŒ–ä¸ºå¸ƒå°”å¯æ»¡è¶³æ€§(Boolean satisfiability)é—®é¢˜ï¼Œå¹¶å€ŸåŠ©SATæ±‚è§£å™¨(SAT solver)é«˜æ•ˆå®Œæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œåˆæˆçš„è§£é‡Šè´¨é‡èƒ½å¤Ÿç´§å¯†æ¥è¿‘å…·æœ‰å…¨å±€ä¿è¯çš„æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç”Ÿæˆå…¼å…·ç†è®ºä¸¥è°¨æ€§ä¸å®è·µæ‰©å±•æ€§çš„æ¨¡å‹è§£é‡Šæä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted at ATVA'25",
      "pdf_url": "https://arxiv.org/pdf/2508.15220v1",
      "published_date": "2025-08-21 04:11:20 UTC",
      "updated_date": "2025-08-21 04:11:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:49.832789+00:00"
    },
    {
      "arxiv_id": "2508.15212v3",
      "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning",
      "title_zh": "SparKï¼šåŸºäºæŸ¥è¯¢æ„ŸçŸ¥çš„éç»“æ„åŒ–ç¨€ç–ä¸å¯æ¢å¤ KV ç¼“å­˜é€šé“å‰ªæ",
      "authors": [
        "Huanxuan Liao",
        "Yixing Xu",
        "Shizhu He",
        "Guanchen Li",
        "Xuanwu Yin",
        "Dong Li",
        "Emad Barsoum",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿æ–‡æœ¬æ¨ç†ä¸­çš„KV cacheç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†SparKï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„éç»“æ„åŒ–ç¨€ç–æ–¹æ³•ã€‚SparK å…³æ³¨åˆ°ç‰¹å¾ç»´åº¦ï¼ˆå³channelè½´ï¼‰ä¸Šçš„ç»†ç²’åº¦é‡è¦æ€§å·®å¼‚ï¼Œå‘ç°é€šé“æ˜¾è‘—æ€§éšQueryå’Œä½ç½®åŠ¨æ€å˜åŒ–ï¼Œä»è€Œå¼¥è¡¥äº†ç°æœ‰æ–¹æ³•ä»…åœ¨æ—¶é—´è½´å‹ç¼©çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨channelçº§åˆ«å¯¹KVè¿›è¡Œå‰ªæï¼Œå¹¶åœ¨è®¡ç®—Attention scoreæ—¶åŠ¨æ€æ¢å¤è¢«å‰ªæ‰çš„æ¡ç›®ï¼Œå®ç°äº†é«˜æ•ˆçš„éç»“æ„åŒ–ç¨€ç–å¤„ç†ã€‚å®éªŒè¯æ˜ï¼ŒSparK ä¸ç°æœ‰çš„KVå‹ç¼©å’Œé‡åŒ–æŠ€æœ¯æ­£äº¤å…¼å®¹ï¼Œåœ¨ç›¸åŒå†…å­˜é¢„ç®—ä¸‹èƒ½å¤Ÿæ”¯æŒæ›´é•¿çš„åºåˆ—é•¿åº¦ã€‚ä¸åŸºäºé€å‡º(eviction)çš„æ–¹æ³•ç›¸æ¯”ï¼ŒSparK åœ¨ä¿æŒæˆ–æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å‡å°‘è¶…è¿‡30%çš„KV cacheå­˜å‚¨ç©ºé—´ã€‚å³ä½¿åœ¨80%çš„é«˜å‰ªæç‡ä¸‹ï¼ŒSparK çš„æ€§èƒ½ä¸‹é™ä¹Ÿæ¯”åŸºçº¿æ–¹æ³•ä½5%ä»¥ä¸Šï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨å¹³è¡¡æ•ˆç‡ä¸ç²¾åº¦æ–¹é¢çš„å“è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.15212v3",
      "published_date": "2025-08-21 03:48:28 UTC",
      "updated_date": "2025-11-12 16:02:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:41.189037+00:00"
    },
    {
      "arxiv_id": "2508.16669v1",
      "title": "Situational Awareness as the Imperative Capability for Disaster Resilience in the Era of Complex Hazards and Artificial Intelligence",
      "title_zh": "æ€åŠ¿æ„ŸçŸ¥ï¼šå¤æ‚ç¾å®³ä¸äººå·¥æ™ºèƒ½æ—¶ä»£ç¾å®³éŸ§æ€§çš„æ ¸å¿ƒèƒ½åŠ›",
      "authors": [
        "Hongrak Pak",
        "Ali Mostafavi"
      ],
      "abstract": "Disasters frequently exceed established hazard models, revealing blind spots where unforeseen impacts and vulnerabilities hamper effective response. This perspective paper contends that situational awareness (SA)-the ability to perceive, interpret, and project dynamic crisis conditions-is an often overlooked yet vital capability for disaster resilience. While risk mitigation measures can reduce known threats, not all hazards can be neutralized; truly adaptive resilience hinges on whether organizations rapidly detect emerging failures, reconcile diverse data sources, and direct interventions where they matter most. We present a technology-process-people roadmap, demonstrating how real-time hazard nowcasting, interoperable workflows, and empowered teams collectively transform raw data into actionable insight. A system-of-systems approach enables federated data ownership and modular analytics, so multiple agencies can share timely updates without sacrificing their distinct operational models. Equally crucial, structured sense-making routines and cognitive load safeguards help humans remain effective decision-makers amid data abundance. By framing SA as a socio-technical linchpin rather than a peripheral add-on, this paper spotlights the urgency of elevating SA to a core disaster resilience objective. We conclude with recommendations for further research-developing SA metrics, designing trustworthy human-AI collaboration, and strengthening inclusive data governance-to ensure that communities are equipped to cope with both expected and unexpected crises.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤æ‚ç¾å®³ä¸äººå·¥æ™ºèƒ½(Artificial Intelligence)æ—¶ä»£èƒŒæ™¯ä¸‹ï¼Œæ€åŠ¿æ„ŸçŸ¥(Situational Awareness, SA)ä½œä¸ºç¾å®³éŸ§æ€§(Disaster Resilience)æ ¸å¿ƒèƒ½åŠ›çš„é‡è¦æ€§ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œä¼ ç»Ÿç¾å®³æ¨¡å‹å¾€å¾€éš¾ä»¥åº”å¯¹ä¸å¯é¢„è§çš„å†²å‡»ï¼Œè€ŒSAèƒ½å¤Ÿé€šè¿‡æ„ŸçŸ¥ã€è§£é‡Šå’Œé¢„æµ‹åŠ¨æ€å±æœºçŠ¶å†µï¼Œå¼¥è¡¥é£é™©ç¼“è§£æªæ–½çš„ç›²ç‚¹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ•´åˆæŠ€æœ¯ã€æµç¨‹ä¸äººå‘˜çš„è·¯çº¿å›¾ï¼Œåˆ©ç”¨å®æ—¶ç¾å®³ä¸´è¿‘é¢„æŠ¥(Nowcasting)ã€äº’æ“ä½œå·¥ä½œæµåŠèµ‹èƒ½å›¢é˜Ÿå°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºè¡ŒåŠ¨è§è§£ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨ä½“ç³»ä¹‹ä½“ç³»(System-of-Systems)æ–¹æ³•ï¼Œæ”¯æŒè”é‚¦æ•°æ®æ‰€æœ‰æƒ(Federated Data Ownership)å’Œæ¨¡å—åŒ–åˆ†æï¼Œå®ç°å¤šæœºæ„é—´çš„å®æ—¶æ•°æ®å…±äº«ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†é€šè¿‡ç»“æ„åŒ–æ„Ÿæ‚Ÿ(Sense-making)å’Œè®¤çŸ¥è´Ÿè·ä¿éšœæªæ–½ï¼Œç¡®ä¿äººç±»åœ¨æµ·é‡æ•°æ®ç¯å¢ƒä¸‹ä»èƒ½ä¿æŒé«˜æ•ˆå†³ç­–ã€‚é€šè¿‡å°†SAå®šä½ä¸ºç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿçš„æ ¸å¿ƒæ”¯æŸ±ï¼Œæœ¬æ–‡ä¸ºå¼€å‘SAæŒ‡æ ‡ã€å»ºç«‹å¯ä¿¡äººæœºåä½œ(Human-AI Collaboration)ä»¥åŠåŠ å¼ºåŒ…å®¹æ€§æ•°æ®æ²»ç†æä¾›äº†ç ”ç©¶æ–¹å‘ï¼Œä»¥æå‡ç¤¾åŒºåº”å¯¹å„ç±»å±æœºçš„é€‚åº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16669v1",
      "published_date": "2025-08-21 03:38:47 UTC",
      "updated_date": "2025-08-21 03:38:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:45.286412+00:00"
    },
    {
      "arxiv_id": "2508.15204v1",
      "title": "R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling",
      "title_zh": "R-ConstraintBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨NPå®Œå…¨è°ƒåº¦é—®é¢˜ä¸Šçš„è¡¨ç°",
      "authors": [
        "Raj Jain",
        "Marc Wetter"
      ],
      "abstract": "Effective scheduling under tight resource, timing, and operational constraints underpins large-scale planning across sectors such as capital projects, manufacturing, logistics, and IT fleet transitions. However, the reliability of large language models (LLMs) when reasoning under high-constraint regimes is insufficiently characterized. To address this gap, we present R-ConstraintBench, a scalable framework that evaluates models on Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete feasibility class, while difficulty increases via linear growth in constraints. R-ConstraintBench incrementally increases non-redundant precedence constraints in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal windows, and disjunctive constraints. As an illustrative example, we instantiate the benchmark in a data center migration setting and evaluate multiple LLMs using feasibility and error analysis, identifying degradation thresholds and constraint types most associated with failure. Empirically, strong models are near-ceiling on precedence-only DAGs, but feasibility performance collapses when downtime, temporal windows, and disjunctive constraints interact, implicating constraint interaction, not graph depth, as the principal bottleneck. Performance on clean synthetic ramps also does not guarantee transfer to domain-grounded scenarios, underscoring limited generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é«˜åº¦çº¦æŸç¯å¢ƒä¸‹æ¨ç†å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†R-ConstraintBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMsåœ¨NP-Completeç±»èµ„æºå—é™é¡¹ç›®è°ƒåº¦é—®é¢˜(RCPSP)ä¸­è¡¨ç°çš„å¯æ‰©å±•æ¡†æ¶ã€‚è¯¥åŸºå‡†é€šè¿‡åœ¨æœ‰å‘æ— ç¯å›¾(DAGs)ä¸­çº¿æ€§å¢åŠ ä¼˜å…ˆçº§çº¦æŸï¼Œå¹¶å¼•å…¥åœæœºæ—¶é—´(downtime)ã€æ—¶é—´çª—å£(temporal windows)å’Œæå–çº¦æŸ(disjunctive constraints)æ¥é€æ­¥æå‡ä»»åŠ¡éš¾åº¦ã€‚ç ”ç©¶è€…ä»¥æ•°æ®ä¸­å¿ƒè¿ç§»ä¸ºèƒŒæ™¯å®ä¾‹åŒ–äº†è¯¥åŸºå‡†ï¼Œå¯¹å¤šä¸ªLLMsè¿›è¡Œäº†å¯è¡Œæ€§ä¸é”™è¯¯åˆ†æï¼Œä»¥è¯†åˆ«æ€§èƒ½ä¸‹é™çš„é˜ˆå€¼ã€‚å®éªŒå‘ç°ï¼Œå¼ºåŠ›æ¨¡å‹åœ¨ä»…å«ä¼˜å…ˆçº§çº¦æŸçš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å½“å¤šç§å¤æ‚çº¦æŸäº¤äº’æ—¶ï¼Œæ¨¡å‹çš„å¯è¡Œæ€§è¡¨ç°ä¼šè¿…é€Ÿå´©æºƒã€‚è¿™è¡¨æ˜çº¦æŸäº¤äº’(constraint interaction)è€Œéå›¾æ·±åº¦æ˜¯ä¸»è¦çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¸”æ¨¡å‹åœ¨åˆæˆæ•°æ®ä¸Šçš„è¡¨ç°éš¾ä»¥ä¿è¯å…¶åœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­çš„æœ‰æ•ˆæ³›åŒ–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15204v1",
      "published_date": "2025-08-21 03:35:58 UTC",
      "updated_date": "2025-08-21 03:35:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:51.957295+00:00"
    },
    {
      "arxiv_id": "2508.15201v2",
      "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
      "title_zh": "é¢å‘å…·èº«æ“çºµçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ç»¼è¿°",
      "authors": [
        "Haoran Li",
        "Yuhui Chen",
        "Wenbo Cui",
        "Weiheng Liu",
        "Kai Liu",
        "Mingcai Zhou",
        "Zhengtao Zhang",
        "Dongbin Zhao"
      ],
      "abstract": "Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ€§åœ°å›é¡¾äº†ç”¨äº Embodied Manipulation çš„ Vision-Language-Action (VLA) æ¨¡å‹ï¼Œæ¢è®¨äº†å…¶ä½œä¸ºé€šç”¨æœºå™¨äººæ§åˆ¶æ¡†æ¶åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚VLA æ¨¡å‹å—å¤§å‹åŸºç¡€æ¨¡å‹å¯å‘ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ï¼Œå¹¶æå¤§åœ°æ‰©å±•äº†å…·èº«äººå·¥æ™ºèƒ½çš„åº”ç”¨åœºæ™¯ã€‚æ–‡ç« è¯¦ç»†è¿½æº¯äº† VLA æ¶æ„çš„å‘å±•å†ç¨‹ï¼Œå¹¶ä» VLA æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ•°æ®é›†ã€Pre-training æ–¹æ³•ã€Post-training æ–¹æ³•åŠæ¨¡å‹è¯„ä¼°è¿™äº”ä¸ªå…³é”®ç»´åº¦è¿›è¡Œäº†æ·±å…¥å‰–æã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ€»ç»“äº† VLA æ¨¡å‹åœ¨ç ”å‘ä¸å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…å‰ç»æ€§åœ°æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€é€šç”¨çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "in Chinese language",
      "pdf_url": "https://arxiv.org/pdf/2508.15201v2",
      "published_date": "2025-08-21 03:30:04 UTC",
      "updated_date": "2025-11-12 01:25:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:54.054280+00:00"
    },
    {
      "arxiv_id": "2508.15192v1",
      "title": "LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support",
      "title_zh": "LLM4Sweatï¼šé¢å‘å¤šæ±—ç—‡æ”¯æŒçš„å¯ä¿¡å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Wenjie Lin",
        "Jin Wei-Kocsis"
      ],
      "abstract": "While large language models (LLMs) have shown promise in healthcare, their application for rare medical conditions is still hindered by scarce and unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing excessive sweating beyond physiological needs, is one such rare disorder, affecting 2-3% of the population and significantly impacting both physical comfort and psychosocial well-being. To date, no work has tailored LLMs to advance the diagnosis or care of hyperhidrosis. To address this gap, we present LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and empathetic hyperhidrosis support. The system follows a three-stage pipeline. In the data augmentation stage, a frontier LLM generates medically plausible synthetic vignettes from curated open-source data to create a diverse and balanced question-answer dataset. In the fine-tuning stage, an open-source foundation model is fine-tuned on the dataset to provide diagnosis, personalized treatment recommendations, and empathetic psychological support. In the inference and expert evaluation stage, clinical and psychological specialists assess accuracy, appropriateness, and empathy, with validated responses iteratively enriching the dataset. Experiments show that LLM4Sweat outperforms baselines and delivers the first open-source LLM framework for hyperhidrosis, offering a generalizable approach for other rare diseases with similar data and trustworthiness challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLM4Sweatï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ±—ç—‡ (Hyperhidrosis) æä¾›åŒ»ç–—æ”¯æŒçš„å¼€æºã€é¢†åŸŸä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç½•è§ç—…é¢†åŸŸå› å¾®è°ƒæ•°æ®ç¨€ç¼ºä¸”ä¸å¯é è€Œå¯¼è‡´çš„æŠ€æœ¯éš¾é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸‰é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆåœ¨æ•°æ®å¢å¼ºé˜¶æ®µé€šè¿‡å‰æ²¿ LLM ç”ŸæˆåŒ»å­¦åˆç†çš„åˆæˆæ¡ˆä¾‹ä»¥æ„å»ºé«˜è´¨é‡é—®ç­”æ•°æ®é›†ï¼›å…¶æ¬¡å¯¹å¼€æºåŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å…·å¤‡è¯Šæ–­ã€ä¸ªæ€§åŒ–æ²»ç–—å»ºè®®å’Œå…±æƒ…å¿ƒç†æ”¯æŒçš„èƒ½åŠ›ï¼›æœ€åé€šè¿‡ä¸´åºŠä¸å¿ƒç†ä¸“å®¶çš„è¿­ä»£è¯„ä¼°æ¥æŒç»­ä¼˜åŒ–æ¨¡å‹çš„å‡†ç¡®æ€§ä¸é€‚å®œæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLLM4Sweat çš„è¡¨ç°ä¼˜äºé€šç”¨åŸºçº¿æ¨¡å‹ï¼Œåœ¨å¡«è¡¥å¤šæ±—ç—‡ä¸“ç”¨ AI è¾…åŠ©ç©ºç™½çš„åŒæ—¶ï¼Œä¹Ÿä¸ºå…¶ä»–å…·æœ‰ç±»ä¼¼æ•°æ®æŒ‘æˆ˜çš„ç½•è§ç—…æä¾›äº†ä¸€å¥—å¯æ¨å¹¿çš„å¯é æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15192v1",
      "published_date": "2025-08-21 03:04:20 UTC",
      "updated_date": "2025-08-21 03:04:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:56.164234+00:00"
    },
    {
      "arxiv_id": "2508.15190v1",
      "title": "SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling",
      "title_zh": "SemTokenï¼šé¢å‘é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡çš„è¯­ä¹‰æ„ŸçŸ¥è¯å…ƒåŒ–",
      "authors": [
        "Dong Liu",
        "Yanxuan Yu"
      ],
      "abstract": "Tokenization plays a critical role in language modeling, yet existing approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on frequency statistics, ignoring the underlying semantic structure of text. This leads to over-tokenization of semantically redundant spans and underutilization of contextual coherence, particularly in long-context scenarios. In this work, we propose \\textbf{SemToken}, a semantic-aware tokenization framework that jointly reduces token redundancy and improves computation efficiency. SemToken first extracts contextual semantic embeddings via lightweight encoders and performs local semantic clustering to merge semantically equivalent tokens. Then, it allocates heterogeneous token granularity based on semantic density, allowing finer-grained tokenization in content-rich regions and coarser compression in repetitive or low-entropy spans. SemToken can be seamlessly integrated with modern language models and attention acceleration methods. Experiments on long-context language modeling benchmarks such as WikiText-103 and LongBench show that SemToken achieves up to $2.4\\times$ reduction in token count and $1.9\\times$ speedup, with negligible or no degradation in perplexity and downstream accuracy. Our findings suggest that semantic structure offers a promising new axis for optimizing tokenization and computation in large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SemTokenï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹æ•ˆç‡çš„è¯­ä¹‰æ„ŸçŸ¥åˆ†è¯æ¡†æ¶ï¼Œä»¥è§£å†³ Byte-Pair Encoding (BPE) ç­‰ä¼ ç»Ÿæ–¹æ³•å› å¿½è§†è¯­ä¹‰ç»“æ„è€Œå¯¼è‡´çš„ Token å†—ä½™é—®é¢˜ã€‚SemToken åˆ©ç”¨è½»é‡çº§ç¼–ç å™¨æå–ä¸Šä¸‹æ–‡è¯­ä¹‰åµŒå…¥ï¼Œå¹¶é€šè¿‡å±€éƒ¨è¯­ä¹‰èšç±»åˆå¹¶è¯­ä¹‰ç­‰æ•ˆçš„ Tokenã€‚è¯¥æ¡†æ¶æ ¹æ®è¯­ä¹‰å¯†åº¦åŠ¨æ€åˆ†é…å¼‚æ„åˆ†è¯ç²’åº¦ï¼Œåœ¨å†…å®¹ä¸°å¯Œçš„åŒºåŸŸä¿ç•™ç»†ç²’åº¦ï¼Œè€Œåœ¨é‡å¤æˆ–ä½ç†µåŒºåŸŸè¿›è¡Œæ›´å¼ºçš„å‹ç¼©ã€‚å®éªŒåœ¨ WikiText-103 å’Œ LongBench ç­‰åŸºå‡†ä¸Šè¯æ˜ï¼ŒSemToken å¯å®ç°é«˜è¾¾ 2.4 å€çš„ Token å‡é‡å’Œ 1.9 å€çš„æ¨ç†åŠ é€Ÿã€‚åœ¨ä¿æŒæ˜¾è‘—åŠ é€Ÿçš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•å¯¹æ¨¡å‹å›°æƒ‘åº¦ (Perplexity) å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¿™ä¸€å‘ç°ä¸ºé€šè¿‡è¯­ä¹‰ç»´åº¦ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ Tokenization å’Œè®¡ç®—æ•ˆç‡æä¾›äº†æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15190v1",
      "published_date": "2025-08-21 03:01:53 UTC",
      "updated_date": "2025-08-21 03:01:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:17:59.063877+00:00"
    },
    {
      "arxiv_id": "2508.15189v1",
      "title": "SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis",
      "title_zh": "SurgWound-Benchï¼šæ‰‹æœ¯ä¼¤å£è¯Šæ–­åŸºå‡†",
      "authors": [
        "Jiahao Xu",
        "Changchang Yin",
        "Odysseas Chatzipanagiotou",
        "Diamantis Tsilimigras",
        "Kevin Clear",
        "Bingsheng Yao",
        "Dakuo Wang",
        "Timothy Pawlik",
        "Ping Zhang"
      ],
      "abstract": "Surgical site infection (SSI) is one of the most common and costly healthcare-associated infections and and surgical wound care remains a significant clinical challenge in preventing SSIs and improving patient outcomes. While recent studies have explored the use of deep learning for preliminary surgical wound screening, progress has been hindered by concerns over data privacy and the high costs associated with expert annotation. Currently, no publicly available dataset or benchmark encompasses various types of surgical wounds, resulting in the absence of an open-source Surgical-Wound screening tool. To address this gap: (1) we present SurgWound, the first open-source dataset featuring a diverse array of surgical wound types. It contains 697 surgical wound images annotated by 3 professional surgeons with eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce the first benchmark for surgical wound diagnosis, which includes visual question answering (VQA) and report generation tasks to comprehensively evaluate model performance. (3) Furthermore, we propose a three-stage learning framework, WoundQwen, for surgical wound diagnosis. In the first stage, we employ five independent MLLMs to accurately predict specific surgical wound characteristics. In the second stage, these predictions serve as additional knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess infection risk and guide subsequent interventions. In the third stage, we train a MLLM that integrates the diagnostic results from the previous two stages to produce a comprehensive report. This three-stage framework can analyze detailed surgical wound characteristics and provide subsequent instructions to patients based on surgical images, paving the way for personalized wound care, timely intervention, and improved patient outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹æœ¯éƒ¨ä½æ„ŸæŸ“(SSI)é˜²æ²»ä¸­ç”±äºæ•°æ®éšç§å’Œæ ‡æ³¨æˆæœ¬å¯¼è‡´çš„è‡ªåŠ¨åŒ–ç­›æŸ¥å·¥å…·ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå¼€æºçš„å¤šæ ·åŒ–æ‰‹æœ¯ä¼¤å£æ•°æ®é›†SurgWoundï¼ŒåŒ…å«697å¼ ç”±ä¸“ä¸šåŒ»ç”Ÿè¿›è¡Œå…«ç§ç»†ç²’åº¦ä¸´åºŠå±æ€§æ ‡æ³¨çš„å›¾åƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…å»ºç«‹äº†SurgWound-BenchåŸºå‡†ï¼Œæ¶µç›–è§†è§‰é—®ç­”(VQA)å’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºWoundQwençš„ä¸‰é˜¶æ®µå­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å¤šä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¾æ¬¡è¿›è¡Œç‰¹å¾é¢„æµ‹ã€æ„ŸæŸ“é£é™©è¯„ä¼°å’Œç»¼åˆè¯Šæ–­æŠ¥å‘Šç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ†æè¯¦å°½çš„æ‰‹æœ¯ä¼¤å£ç‰¹å¾å¹¶ä¸ºæ‚£è€…æä¾›åç»­æŒ‡ä»¤ã€‚è¿™ä¸€æˆæœä¸ºå®ç°ä¸ªæ€§åŒ–ä¼¤å£æŠ¤ç†ã€åŠæ—¶å¹²é¢„å’Œæ”¹å–„æ‚£è€…é¢„åå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15189v1",
      "published_date": "2025-08-21 03:00:17 UTC",
      "updated_date": "2025-08-21 03:00:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:18:15.064874+00:00"
    },
    {
      "arxiv_id": "2508.15180v2",
      "title": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data",
      "title_zh": "PuzzleCloneï¼šåŸºäº SMT çš„å¯éªŒè¯æ•°æ®åˆæˆæ¡†æ¶",
      "authors": [
        "Kai Xiong",
        "Yanwei Huang",
        "Rongjunchen Zhang",
        "Kun Chen",
        "Haipang Wu"
      ],
      "abstract": "High-quality mathematical and logical datasets with verifiable answers are essential for strengthening the reasoning capabilities of large language models (LLMs). While recent data augmentation techniques have facilitated the creation of large-scale benchmarks, existing LLM-generated datasets often suffer from limited reliability, diversity, and scalability. To address these challenges, we introduce PuzzleClone, a formal framework for synthesizing verifiable data at scale using Satisfiability Modulo Theories (SMT). Our approach features three key innovations: (1) encoding seed puzzles into structured logical specifications, (2) generating scalable variants through systematic variable and constraint randomization, and (3) ensuring validity via a reproduction mechanism. Applying PuzzleClone, we construct a curated benchmark comprising over 83K diverse and programmatically validated puzzles. The generated puzzles span a wide spectrum of difficulty and formats, posing significant challenges to current state-of-the-art models. We conduct post training (SFT and RL) on PuzzleClone datasets. Experimental results show that training on PuzzleClone yields substantial improvements not only on PuzzleClone testset but also on logic and mathematical benchmarks. Post training raises PuzzleClone average from 14.4 to 56.2 and delivers consistent improvements across 7 logic and mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from 52.5 to 65.0). Our code and data are available at https://github.com/HiThink-Research/PuzzleClone.",
      "tldr_zh": "é«˜è´¨é‡ä¸”ç»“æœå¯éªŒè¯çš„æ•°å­¦ä¸é€»è¾‘æ•°æ®é›†å¯¹äºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ•°æ®é›†å¸¸é¢ä¸´å¯é æ€§ä¸å¯æ‰©å±•æ€§çš„ç“¶é¢ˆã€‚è¯¥ç ”ç©¶æå‡ºäº† PuzzleCloneï¼Œä¸€ç§åŸºäºå¯æ»¡è¶³æ€§æ¨¡ç†è®ºï¼ˆSatisfiability Modulo Theories, SMTï¼‰çš„å¤§è§„æ¨¡åˆæˆå¯éªŒè¯æ•°æ®çš„å½¢å¼åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç§å­è°œé¢˜ç¼–ç ä¸ºç»“æ„åŒ–é€»è¾‘è§„èŒƒï¼Œå¹¶åˆ©ç”¨éšæœºåŒ–å˜é‡ä¸çº¦æŸç”Ÿæˆå¤§é‡å˜ä½“ï¼ŒåŒæ—¶é…åˆå†ç°æœºåˆ¶ç¡®ä¿æ•°æ®çš„ç»å¯¹æœ‰æ•ˆæ€§ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«è¶…è¿‡ 8.3 ä¸‡ä¸ªæ¶µç›–å¤šç§éš¾åº¦å’Œæ ¼å¼çš„è°œé¢˜æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ PuzzleClone ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ä»…ä½¿æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¾—åˆ†ä» 14.4 æå‡è‡³ 56.2ï¼Œè¿˜åœ¨ 7 ä¸ªé€šç”¨çš„é€»è¾‘å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½å¢é•¿ï¼Œå…¶ä¸­ AMC2023 çš„å‡†ç¡®ç‡æå‡äº† 12.5 ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™ä¸€æˆæœä¸ºåˆæˆé«˜è´¨é‡æ¨ç†æ•°æ®æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜åº¦å¯é çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15180v2",
      "published_date": "2025-08-21 02:36:16 UTC",
      "updated_date": "2025-08-25 17:16:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:18:15.258571+00:00"
    },
    {
      "arxiv_id": "2509.00031v2",
      "title": "End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost",
      "title_zh": "ä»¥æ¨ç†æˆæœ¬å®ç°å¤§è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯è®¾å¤‡ç«¯é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ",
      "authors": [
        "Qitao Tan",
        "Xiaoying Song",
        "Jin Lu",
        "Guoming Li",
        "Jun Liu",
        "Lingzi Hong",
        "Caiwen Ding",
        "Jundong Li",
        "Xiaoming Zhai",
        "Shaoyi Huang",
        "Wei Niu",
        "Geng Yuan"
      ],
      "abstract": "Quantization is an effective technique to reduce the deployment cost of large language models (LLMs), and post-training quantization (PTQ) has been widely studied due to its efficiency. However, existing PTQ methods are limited by their inability to fine-tune model parameters and often suffer significant accuracy loss in low-bit scenarios. Quantization-aware training (QAT) provides a more principled solution, but its reliance on backpropagation incurs prohibitive memory costs, limiting its practicality for LLM deployment. To address these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT framework that supports both weight and activation quantization. ZeroQAT leverages forward-only gradient estimation to eliminate backpropagation, substantially reducing computational and memory overhead while retaining the benefits of end-to-end optimization. We further introduce a lightweight variant of ZeroQAT for quantized fine-tuning, which freezes and pre-quantizes most parameters to further cut memory usage. Experiments show that ZeroQAT consistently outperforms representative PTQ and QAT baselines while requiring significantly less memory. For example, ZeroQAT enables fine-tuning of a 13B model at extremely low bit-widths (e.g., 2-4 bits) on a single 8GB GPU, and even allows fine-tuning a 6.7B model on a OnePlus 12 smartphone, demonstrating its practicality for end-to-end QAT on resource-limited edge devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ZeroQATï¼Œä¸€ç§åŸºäºé›¶é˜¶ä¼˜åŒ–(zeroth-order optimization)çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(QAT)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é‡åŒ–è¿‡ç¨‹ä¸­æ˜¾å­˜å¼€é”€è¿‡é«˜çš„é—®é¢˜ã€‚ZeroQATåˆ©ç”¨ä»…å‰å‘æ¢¯åº¦çš„ä¼°è®¡(forward-only gradient estimation)æ¶ˆé™¤äº†åå‘ä¼ æ’­ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œæ˜¾å­˜éœ€æ±‚ï¼ŒåŒæ—¶æ”¯æŒå¯¹æƒé‡å’Œæ¿€æ´»å€¼çš„ç«¯åˆ°ç«¯é‡åŒ–ä¼˜åŒ–ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è½»é‡çº§å˜ä½“ï¼Œé€šè¿‡å†»ç»“å¹¶é¢„é‡åŒ–å¤§éƒ¨åˆ†å‚æ•°ï¼Œå®ç°åœ¨èµ„æºæåº¦å—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚å®éªŒè¯æ˜ï¼ŒZeroQATåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„PTQå’ŒQATæ–¹æ³•ï¼ŒæˆåŠŸåœ¨å•å—8GBæ˜¾å­˜GPUä¸Šå®Œæˆäº†13Bæ¨¡å‹çš„ä½æ¯”ç‰¹å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç”šè‡³æ”¯æŒåœ¨ä¸€åŠ 12æ™ºèƒ½æ‰‹æœºä¸Šè¿è¡Œ6.7Bæ¨¡å‹çš„å¾®è°ƒä»»åŠ¡ï¼Œå±•ç°äº†åœ¨ç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ€§èƒ½LLMçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00031v2",
      "published_date": "2025-08-21 01:18:27 UTC",
      "updated_date": "2025-09-29 16:45:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:18:23.484778+00:00"
    },
    {
      "arxiv_id": "2508.15144v2",
      "title": "Mobile-Agent-v3: Fundamental Agents for GUI Automation",
      "title_zh": "Mobile-Agent-v3ï¼šGUI è‡ªåŠ¨åŒ–åŸºç¡€æ™ºèƒ½ä½“",
      "authors": [
        "Jiabo Ye",
        "Xi Zhang",
        "Haiyang Xu",
        "Haowei Liu",
        "Junyang Wang",
        "Zhaoqing Zhu",
        "Ziwei Zheng",
        "Feiyu Gao",
        "Junjie Cao",
        "Zhengxi Lu",
        "Jitong Liao",
        "Qi Zheng",
        "Fei Huang",
        "Jingren Zhou",
        "Ming Yan"
      ],
      "abstract": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† GUI-Owlï¼Œè¿™æ˜¯ä¸€ç§åŸºç¡€ GUI æ™ºèƒ½ä½“æ¨¡å‹ï¼Œåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åé¡¹ GUI åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¼€æºç«¯åˆ°ç«¯æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½(SOTA)ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº† Mobile-Agent-v3ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„ GUI æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°† AndroidWorld å’Œ OSWorld çš„æ€§èƒ½åˆ†åˆ«æå‡è‡³ 73.3 å’Œ 37.7ï¼Œåˆ·æ–°äº†å¼€æº GUI æ™ºèƒ½ä½“æ¡†æ¶çš„çºªå½•ã€‚GUI-Owl çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬æ„å»ºäº†ä¸€ä¸ªè·¨å¹³å°çš„å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘è¿›åŒ– GUI è½¨è¿¹ç”Ÿæˆæ¡†æ¶(Self-Evolving GUI Trajectory Production)å®ç°äº†é«˜è´¨é‡äº¤äº’æ•°æ®çš„è‡ªåŠ¨åŒ–ç”Ÿäº§ã€‚è¯¥æ¨¡å‹æ•´åˆäº† UI groundingã€è§„åˆ’(planning)ã€åŠ¨ä½œè¯­ä¹‰å’Œæ¨ç†æ¨¡å¼ï¼Œä½¿å…¶æ—¢èƒ½æ”¯æŒç«¯åˆ°ç«¯å†³ç­–ï¼Œä¹Ÿèƒ½ä½œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªæ”¯æŒå…¨å¼‚æ­¥è®­ç»ƒçš„å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ (RL)æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†è½¨è¿¹æ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(TRPO)ä»¥ä¼˜åŒ–åœ¨çº¿å­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†è¯¥ç³»åˆ—æ¨¡å‹åœ¨å¤„ç†å¤æ‚å›¾å½¢ç”¨æˆ·ç•Œé¢è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­çš„å“è¶Šèƒ½åŠ›ï¼Œç›®å‰ç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15144v2",
      "published_date": "2025-08-21 00:39:12 UTC",
      "updated_date": "2025-09-01 05:38:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:18:24.384256+00:00"
    },
    {
      "arxiv_id": "2508.15868v2",
      "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning",
      "title_zh": "CARFTï¼šé€šè¿‡åŸºäºæ ‡æ³¨æ€ç»´é“¾çš„å¼ºåŒ–å¾®è°ƒå¯¹æ¯”å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Wenqiao Zhu",
        "Ji Liu",
        "Rongjuncheng Zhang",
        "Haipang Wu",
        "Yulun Zhang"
      ],
      "abstract": "Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \\TheName{} in terms of robustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code is available at https://github.com/WNQzhu/CARFT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CARFTï¼Œä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ ä¸æ ‡æ³¨Chain-of-Thought (CoT)çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰Reinforcement Learning (RL)æ–¹æ³•æ¨ç†è·¯å¾„é‡‡æ ·ä¸ç¨³å®šä»¥åŠSupervised Fine-Tuning (SFT)å¯¹æ ‡æ³¨CoTè¿‡åº¦ä¾èµ–ä¸”åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼ŒCARFTé€šè¿‡ä¸ºæ¯ä¸ªCoTå­¦ä¹ è¡¨å¾ï¼Œå¹¶è®¾è®¡æ–°é¢–çš„å¯¹æ¯”ä¿¡å·(contrastive signals)æ¥æŒ‡å¯¼å¾®è°ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å·²æœ‰çš„æ ‡æ³¨CoTï¼Œè¿˜é€šè¿‡å¼•å…¥é¢å¤–çš„æ— ç›‘ç£å­¦ä¹ ä¿¡å·ç¨³å®šäº†å¾®è°ƒæµç¨‹ã€‚åœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCARFTåœ¨é²æ£’æ€§ã€æ€§èƒ½ï¼ˆæå‡è¾¾10.15%ï¼‰å’Œæ•ˆç‡ï¼ˆæå‡è¾¾30.62%ï¼‰æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡å‹å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šçš„å±€é™ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, to appear in EMNLP25",
      "pdf_url": "https://arxiv.org/pdf/2508.15868v2",
      "published_date": "2025-08-21 00:20:47 UTC",
      "updated_date": "2025-09-08 10:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T13:18:24.783850+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 125,
  "processed_papers_count": 125,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T13:19:19.283230+00:00"
}
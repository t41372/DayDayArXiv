[
  {
    "arxiv_id": "2601.01687v1",
    "title": "FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation",
    "authors": [
      "Abdur R. Fayjie",
      "Pankhi Kashyap",
      "Jutika Borah",
      "Patrick Vandewalle"
    ],
    "abstract": "Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 6 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.01687v1",
    "published_date": "2026-01-04 22:57:49 UTC",
    "updated_date": "2026-01-04 22:57:49 UTC"
  },
  {
    "arxiv_id": "2601.01685v1",
    "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage",
    "authors": [
      "Jinwei Hu",
      "Xinmiao Huang",
      "Youcheng Sun",
      "Yi Dong",
      "Xiaowei Huang"
    ],
    "abstract": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2601.01685v1",
    "published_date": "2026-01-04 22:50:23 UTC",
    "updated_date": "2026-01-04 22:50:23 UTC"
  },
  {
    "arxiv_id": "2601.02428v1",
    "title": "A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance",
    "authors": [
      "Okan Bursa"
    ],
    "abstract": "We introduce \\emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \\emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\\approx$ 0.940, Recall@5 $=1.000$) with only $\\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings.\n  ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "6 Pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.02428v1",
    "published_date": "2026-01-04 21:51:41 UTC",
    "updated_date": "2026-01-04 21:51:41 UTC"
  },
  {
    "arxiv_id": "2601.01673v1",
    "title": "Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks",
    "authors": [
      "Arina Kharlamova",
      "Youcheng Sun",
      "Ting Yu"
    ],
    "abstract": "Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "IEEE S&P'26 under review",
    "pdf_url": "https://arxiv.org/pdf/2601.01673v1",
    "published_date": "2026-01-04 21:44:55 UTC",
    "updated_date": "2026-01-04 21:44:55 UTC"
  },
  {
    "arxiv_id": "2601.06129v1",
    "title": "Graph-Based Analysis of AI-Driven Labor Market Transitions: Evidence from 10,000 Egyptian Jobs and Policy Implications",
    "authors": [
      "Ahmed Dawoud",
      "Sondos Samir",
      "Mahmoud Mohamed"
    ],
    "abstract": "How many workers displaced by automation can realistically transition to safer jobs? We answer this using a validated knowledge graph of 9,978 Egyptian job postings, 19,766 skill activities, and 84,346 job-skill relationships (0.74% error rate). While 20.9% of jobs face high automation risk, we find that only 24.4% of at-risk workers have viable transition pathways--defined by $\\geq$3 shared skills and $\\geq$50% skill transfer. The remaining 75.6% face a structural mobility barrier requiring comprehensive reskilling, not incremental upskilling. Among 4,534 feasible transitions, process-oriented skills emerge as the highest-leverage intervention, appearing in 15.6% of pathways. These findings challenge optimistic narratives of seamless workforce adaptation and demonstrate that emerging economies require active pathway creation, not passive skill matching.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06129v1",
    "published_date": "2026-01-04 21:19:58 UTC",
    "updated_date": "2026-01-04 21:19:58 UTC"
  },
  {
    "arxiv_id": "2601.01668v1",
    "title": "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records",
    "authors": [
      "Houman Kazemzadeh",
      "Nima Minaifar",
      "Kamyar Naderi",
      "Sho Tabibzadeh"
    ],
    "abstract": "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.01668v1",
    "published_date": "2026-01-04 21:10:42 UTC",
    "updated_date": "2026-01-04 21:10:42 UTC"
  },
  {
    "arxiv_id": "2601.01665v1",
    "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives",
    "authors": [
      "Wei Liu",
      "Yaoxin Wu",
      "Yingqian Zhang",
      "Thomas Bäck",
      "Yingjie Fan"
    ],
    "abstract": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01665v1",
    "published_date": "2026-01-04 20:57:43 UTC",
    "updated_date": "2026-01-04 20:57:43 UTC"
  },
  {
    "arxiv_id": "2601.01663v1",
    "title": "Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths",
    "authors": [
      "He Sun",
      "Jiwoong Shin",
      "Ravi Dhar"
    ],
    "abstract": "We study generative modeling of \\emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \\emph{distribution matching} for trajectory-derived statistics. We propose \\textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01663v1",
    "published_date": "2026-01-04 20:52:07 UTC",
    "updated_date": "2026-01-04 20:52:07 UTC"
  },
  {
    "arxiv_id": "2601.01655v1",
    "title": "UniCrop: A Universal, Multi-Source Data Engineering Pipeline for Scalable Crop Yield Prediction",
    "authors": [
      "Emiliya Khidirova",
      "Oktay Karakuş"
    ],
    "abstract": "Accurate crop yield prediction relies on diverse data streams, including satellite, meteorological, soil, and topographic information. However, despite rapid advances in machine learning, existing approaches remain crop- or region-specific and require data engineering efforts. This limits scalability, reproducibility, and operational deployment. This study introduces UniCrop, a universal and reusable data pipeline designed to automate the acquisition, cleaning, harmonisation, and engineering of multi-source environmental data for crop yield prediction. For any given location, crop type, and temporal window, UniCrop automatically retrieves, harmonises, and engineers over 200 environmental variables (Sentinel-1/2, MODIS, ERA5-Land, NASA POWER, SoilGrids, and SRTM), reducing them to a compact, analysis-ready feature set utilising a structured feature reduction workflow with minimum redundancy maximum relevance (mRMR). To validate, UniCrop was applied to a rice yield dataset comprising 557 field observations. Using only the selected 15 features, four baseline machine learning models (LightGBM, Random Forest, Support Vector Regression, and Elastic Net) were trained. LightGBM achieved the best single-model performance (RMSE = 465.1 kg/ha, $R^2 = 0.6576$), while a constrained ensemble of all baselines further improved accuracy (RMSE = 463.2 kg/ha, $R^2 = 0.6604$). UniCrop contributes a scalable and transparent data-engineering framework that addresses the primary bottleneck in operational crop yield modelling: the preparation of consistent and harmonised multi-source data. By decoupling data specification from implementation and supporting any crop, region, and time frame through simple configuration updates, UniCrop provides a practical foundation for scalable agricultural analytics. The code and implementation documentation are shared in https://github.com/CoDIS-Lab/UniCrop.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01655v1",
    "published_date": "2026-01-04 20:17:32 UTC",
    "updated_date": "2026-01-04 20:17:32 UTC"
  },
  {
    "arxiv_id": "2601.01653v1",
    "title": "Learning Resilient Elections with Adversarial GNNs",
    "authors": [
      "Hao Xiang Li",
      "Yash Shah",
      "Lorenzo Giusti"
    ],
    "abstract": "In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01653v1",
    "published_date": "2026-01-04 20:12:14 UTC",
    "updated_date": "2026-01-04 20:12:14 UTC"
  },
  {
    "arxiv_id": "2601.01627v1",
    "title": "JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models",
    "authors": [
      "Junyu Liu",
      "Zirui Li",
      "Qian Niu",
      "Zequn Zhang",
      "Yue Xun",
      "Wenlong Hou",
      "Shujun Wang",
      "Yusuke Iwasawa",
      "Yutaka Matsuo",
      "Kan Hatakeyama-Sato"
    ],
    "abstract": "As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01627v1",
    "published_date": "2026-01-04 18:18:18 UTC",
    "updated_date": "2026-01-04 18:18:18 UTC"
  },
  {
    "arxiv_id": "2601.11589v1",
    "title": "PLA-Serve: A Prefill-Length-Aware LLM Serving System",
    "authors": [
      "Jianshu She",
      "Zonghang Li",
      "Hongchao Du",
      "Shangyu Wu",
      "Wenhao Zheng",
      "Eric Xing",
      "Zhengzhong Liu",
      "Huaxiu Yao",
      "Jason Xue",
      "Qirong Ho"
    ],
    "abstract": "PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.11589v1",
    "published_date": "2026-01-04 18:14:24 UTC",
    "updated_date": "2026-01-04 18:14:24 UTC"
  },
  {
    "arxiv_id": "2601.01609v1",
    "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration",
    "authors": [
      "Albert Sadowski",
      "Jarosław A. Chudziak"
    ],
    "abstract": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01609v1",
    "published_date": "2026-01-04 17:19:20 UTC",
    "updated_date": "2026-01-04 17:19:20 UTC"
  },
  {
    "arxiv_id": "2601.01605v1",
    "title": "REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training",
    "authors": [
      "Xin Di",
      "Xinglin Piao",
      "Fei Wang",
      "Guodong Jing",
      "Yong Zhang"
    ],
    "abstract": "Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01605v1",
    "published_date": "2026-01-04 17:06:48 UTC",
    "updated_date": "2026-01-04 17:06:48 UTC"
  },
  {
    "arxiv_id": "2601.01599v1",
    "title": "From Theory of Mind to Theory of Environment: Counterfactual Simulation of Latent Environmental Dynamics",
    "authors": [
      "Ryutaro Uchiyama"
    ],
    "abstract": "The vertebrate motor system employs dimensionality-reducing strategies to limit the complexity of movement coordination, for efficient motor control. But when environments are dense with hidden action-outcome contingencies, movement complexity can promote behavioral innovation. Humans, perhaps uniquely, may infer the presence of hidden environmental dynamics from social cues, by drawing upon computational mechanisms shared with Theory of Mind. This proposed \"Theory of Environment\" supports behavioral innovation by expanding the dimensionality of motor exploration.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Accepted to the AAAI 2026 Workshop on Theory of Mind for Artificial Intelligence (ToM4AI). Extended abstract, 2 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.01599v1",
    "published_date": "2026-01-04 17:00:04 UTC",
    "updated_date": "2026-01-04 17:00:04 UTC"
  },
  {
    "arxiv_id": "2601.04237v1",
    "title": "SAGE-32B: Agentic Reasoning via Iterative Distillation",
    "authors": [
      "Basab Jha",
      "Firoj Paudel",
      "Ujjwal Puri",
      "Ethan Henkel",
      "Zhang Yuting",
      "Mateusz Kowalczyk",
      "Mei Huang",
      "Choi Donghyuk",
      "Wang Junhao"
    ],
    "abstract": "We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "23 Pages, 3 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.04237v1",
    "published_date": "2026-01-04 16:41:58 UTC",
    "updated_date": "2026-01-04 16:41:58 UTC"
  },
  {
    "arxiv_id": "2601.02427v1",
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "authors": [
      "Loïc Magne",
      "Anas Awadalla",
      "Guanzhi Wang",
      "Yinzhen Xu",
      "Joshua Belofsky",
      "Fengyuan Hu",
      "Joohwan Kim",
      "Ludwig Schmidt",
      "Georgia Gkioxari",
      "Jan Kautz",
      "Yisong Yue",
      "Yejin Choi",
      "Yuke Zhu",
      "Linxi \"Jim\" Fan"
    ],
    "abstract": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.02427v1",
    "published_date": "2026-01-04 16:24:50 UTC",
    "updated_date": "2026-01-04 16:24:50 UTC"
  },
  {
    "arxiv_id": "2601.01581v1",
    "title": "CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty",
    "authors": [
      "Rishav Sen",
      "Fangqi Liu",
      "Jose Paolo Talusan",
      "Ava Pettet",
      "Yoshinori Suzue",
      "Mark Bailey",
      "Ayan Mukhopadhyay",
      "Abhishek Dubey"
    ],
    "abstract": "The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "Submitted to AAMAS 2026. 25 pages, 13 figures, 14 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.01581v1",
    "published_date": "2026-01-04 15:59:52 UTC",
    "updated_date": "2026-01-04 15:59:52 UTC"
  },
  {
    "arxiv_id": "2601.01580v1",
    "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs",
    "authors": [
      "Zibo Zhao",
      "Yuanting Zha",
      "Haipeng Zhang",
      "Xingcheng Xu"
    ],
    "abstract": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01580v1",
    "published_date": "2026-01-04 15:59:15 UTC",
    "updated_date": "2026-01-04 15:59:15 UTC"
  },
  {
    "arxiv_id": "2601.01577v1",
    "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller",
    "authors": [
      "Tran Tien Dat",
      "Nguyen Hai An",
      "Nguyen Khanh Viet Dung",
      "Nguyen Duy Duc"
    ],
    "abstract": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01577v1",
    "published_date": "2026-01-04 15:49:46 UTC",
    "updated_date": "2026-01-04 15:49:46 UTC"
  },
  {
    "arxiv_id": "2601.01576v2",
    "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
    "authors": [
      "Ming Zhang",
      "Kexin Tan",
      "Yueyuan Huang",
      "Yujiong Shen",
      "Chunchun Ma",
      "Li Ju",
      "Xinran Zhang",
      "Yuhui Wang",
      "Wenqing Jing",
      "Jingyi Deng",
      "Huayu Sha",
      "Binze Hu",
      "Jingqi Tong",
      "Changhao Jiang",
      "Yage Geng",
      "Yuankai Ying",
      "Yue Zhang",
      "Zhangyue Yin",
      "Zhiheng Xi",
      "Shihan Dou",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \\textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01576v2",
    "published_date": "2026-01-04 15:48:51 UTC",
    "updated_date": "2026-01-19 02:51:34 UTC"
  },
  {
    "arxiv_id": "2601.01569v1",
    "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
    "authors": [
      "Maohao Ran",
      "Zhenglin Wan",
      "Cooper Lin",
      "Yanting Zhang",
      "Hongyu Xin",
      "Hongwei Fan",
      "Yibo Xu",
      "Beier Luo",
      "Yaxin Zhou",
      "Wangbo Zhao",
      "Lijie Yang",
      "Lang Feng",
      "Fuchao Yang",
      "Jingxuan Wu",
      "Yiqiao Huang",
      "Chendong Ma",
      "Dailing Jiang",
      "Jianbo Deng",
      "Sihui Han",
      "Bo An",
      "Yike Guo",
      "Jun Song"
    ],
    "abstract": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 14 Figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01569v1",
    "published_date": "2026-01-04 15:32:47 UTC",
    "updated_date": "2026-01-04 15:32:47 UTC"
  },
  {
    "arxiv_id": "2601.01568v2",
    "title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning",
    "authors": [
      "Chunyu Qiang",
      "Jun Wang",
      "Xiaopeng Wang",
      "Kang Yin",
      "Yuxin Guo"
    ],
    "abstract": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01568v2",
    "published_date": "2026-01-04 15:26:15 UTC",
    "updated_date": "2026-01-08 08:43:41 UTC"
  },
  {
    "arxiv_id": "2601.01562v3",
    "title": "Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement",
    "authors": [
      "Mingyu Xu",
      "Cheng Fang",
      "Keyue Jiang",
      "Yuqian Zheng",
      "Yanghua Xiao",
      "Baojian Zhou",
      "Qifang Zhao",
      "Suhang Zheng",
      "Xiuwen Zhu",
      "Jiyang Tang",
      "Yongchi Zhao",
      "Yijia Luo",
      "Zhiqi Bai",
      "Yuchi Xu",
      "Wenbo Su",
      "Wei Wang",
      "Bing Zhao",
      "Lin Qu",
      "Xiaoxiao Xu"
    ],
    "abstract": "We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01562v3",
    "published_date": "2026-01-04 15:23:18 UTC",
    "updated_date": "2026-01-20 04:18:13 UTC"
  },
  {
    "arxiv_id": "2601.01558v1",
    "title": "Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings",
    "authors": [
      "Pengfei Qu",
      "Wenyu Ouyang",
      "Chi Zhang",
      "Yikai Chai",
      "Shuolong Xu",
      "Lei Ye",
      "Yongri Piao",
      "Miao Zhang",
      "Huchuan Lu"
    ],
    "abstract": "Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01558v1",
    "published_date": "2026-01-04 15:14:16 UTC",
    "updated_date": "2026-01-04 15:14:16 UTC"
  },
  {
    "arxiv_id": "2601.06127v1",
    "title": "AIS-CycleGen: A CycleGAN-Based Framework for High-Fidelity Synthetic AIS Data Generation and Augmentation",
    "authors": [
      "SM Ashfaq uz Zaman",
      "Faizan Qamar",
      "Masnizah Mohd",
      "Nur Hanis Sabrina Suhaimi",
      "Amith Khandakar"
    ],
    "abstract": "Automatic Identification System (AIS) data are vital for maritime domain awareness, yet they often suffer from domain shifts, data sparsity, and class imbalance, which hinder the performance of predictive models. In this paper, we propose a robust data augmentation method, AISCycleGen, based on Cycle-Consistent Generative Adversarial Networks (CycleGAN), which is tailored for AIS datasets. Unlike traditional methods, AISCycleGen leverages unpaired domain translation to generate high-fidelity synthetic AIS data sequences without requiring paired source-target data. The framework employs a 1D convolutional generator with adaptive noise injection to preserve the spatiotemporal structure of AIS trajectories, enhancing the diversity and realism of the generated data. To demonstrate its efficacy, we apply AISCycleGen to several baseline regression models, showing improvements in performance across various maritime domains. The results indicate that AISCycleGen outperforms contemporary GAN-based augmentation techniques, achieving a PSNR value of 30.5 and an FID score of 38.9. These findings underscore AISCycleGen's potential as an effective and generalizable solution for augmenting AIS datasets, improving downstream model performance in real-world maritime intelligence applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06127v1",
    "published_date": "2026-01-04 15:09:53 UTC",
    "updated_date": "2026-01-04 15:09:53 UTC"
  },
  {
    "arxiv_id": "2601.01554v5",
    "title": "MOSS Transcribe Diarize Technical Report",
    "authors": [
      "MOSI. AI",
      ":",
      "Donghua Yu",
      "Zhengyuan Lin",
      "Chen Yang",
      "Yiyang Zhang",
      "Hanfu Chen",
      "Jingqi Chen",
      "Ke Chen",
      "Liwei Fan",
      "Yi Jiang",
      "Jie Zhu",
      "Muchen Li",
      "Wenxuan Wang",
      "Yang Wang",
      "Zhe Xu",
      "Yitian Gong",
      "Yuqian Zhang",
      "Wenbo Zhang",
      "Songlin Wang",
      "Zhiyu Wu",
      "Zhaoye Fei",
      "Qinyuan Cheng",
      "Shimin Li",
      "Xipeng Qiu"
    ],
    "abstract": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01554v5",
    "published_date": "2026-01-04 15:01:10 UTC",
    "updated_date": "2026-01-19 04:45:09 UTC"
  },
  {
    "arxiv_id": "2601.01547v1",
    "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding",
    "authors": [
      "Tianjun Gu",
      "Chenghua Gong",
      "Jingyu Gong",
      "Zhizhong Zhang",
      "Yuan Xie",
      "Lizhuang Ma",
      "Xin Tan"
    ],
    "abstract": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01547v1",
    "published_date": "2026-01-04 14:42:39 UTC",
    "updated_date": "2026-01-04 14:42:39 UTC"
  },
  {
    "arxiv_id": "2601.01546v1",
    "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation",
    "authors": [
      "Letian Kong",
      "Qianran",
      "Jin",
      "Renyu Zhang"
    ],
    "abstract": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 2 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.01546v1",
    "published_date": "2026-01-04 14:42:00 UTC",
    "updated_date": "2026-01-04 14:42:00 UTC"
  },
  {
    "arxiv_id": "2601.01543v1",
    "title": "Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM",
    "authors": [
      "Praveenkumar Katwe",
      "RakeshChandra Balabantaray",
      "Kaliprasad Vittala"
    ],
    "abstract": "Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.\n  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.\n  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Book chapter for River publications",
    "pdf_url": "https://arxiv.org/pdf/2601.01543v1",
    "published_date": "2026-01-04 14:38:58 UTC",
    "updated_date": "2026-01-04 14:38:58 UTC"
  },
  {
    "arxiv_id": "2601.01532v1",
    "title": "Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix",
    "authors": [
      "Fanzhe Fu"
    ],
    "abstract": "In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify \"Cognitive Conviction\" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a \"cognitive buffer,\" they may exhibit \"Defensive OverThinking\" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01532v1",
    "published_date": "2026-01-04 13:57:32 UTC",
    "updated_date": "2026-01-04 13:57:32 UTC"
  },
  {
    "arxiv_id": "2601.01528v1",
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "authors": [
      "Yang Zhou",
      "Hao Shao",
      "Letian Wang",
      "Zhuofan Zong",
      "Hongsheng Li",
      "Steven L. Waslander"
    ],
    "abstract": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures; Project Website: https://drivinggen-bench.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2601.01528v1",
    "published_date": "2026-01-04 13:36:21 UTC",
    "updated_date": "2026-01-04 13:36:21 UTC"
  },
  {
    "arxiv_id": "2601.06126v1",
    "title": "NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs",
    "authors": [
      "Boshen Shi",
      "Kexin Yang",
      "Yuanbo Yang",
      "Guanguang Chang",
      "Ce Chi",
      "Zhendong Wang",
      "Xing Wang",
      "Junlan Feng"
    ],
    "abstract": "While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06126v1",
    "published_date": "2026-01-04 13:26:04 UTC",
    "updated_date": "2026-01-04 13:26:04 UTC"
  },
  {
    "arxiv_id": "2601.01522v1",
    "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making",
    "authors": [
      "Danial Amin"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01522v1",
    "published_date": "2026-01-04 13:19:27 UTC",
    "updated_date": "2026-01-04 13:19:27 UTC"
  },
  {
    "arxiv_id": "2601.01513v2",
    "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation",
    "authors": [
      "Gen Li",
      "Peiyu Liu"
    ],
    "abstract": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01513v2",
    "published_date": "2026-01-04 12:46:35 UTC",
    "updated_date": "2026-01-07 15:36:31 UTC"
  },
  {
    "arxiv_id": "2601.01511v1",
    "title": "Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning",
    "authors": [
      "Ahmed Dawoud",
      "Osama El-Shamy"
    ],
    "abstract": "Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01511v1",
    "published_date": "2026-01-04 12:36:45 UTC",
    "updated_date": "2026-01-04 12:36:45 UTC"
  },
  {
    "arxiv_id": "2601.01496v1",
    "title": "The Optimal Sample Complexity of Linear Contracts",
    "authors": [
      "Mikael Møller Høgsgaard"
    ],
    "abstract": "In this paper, we settle the problem of learning optimal linear contracts from data in the offline setting, where agent types are drawn from an unknown distribution and the principal's goal is to design a contract that maximizes her expected utility. Specifically, our analysis shows that the simple Empirical Utility Maximization (EUM) algorithm yields an $\\varepsilon$-approximation of the optimal linear contract with probability at least $1-δ$, using just $O(\\ln(1/δ) / \\varepsilon^2)$ samples. This result improves upon previously known bounds and matches a lower bound from Duetting et al. [2025] up to constant factors, thereby proving its optimality. Our analysis uses a chaining argument, where the key insight is to leverage a simple structural property of linear contracts: their expected reward is non-decreasing. This property, which holds even though the utility function itself is non-monotone and discontinuous, enables the construction of fine-grained nets required for the chaining argument, which in turn yields the optimal sample complexity. Furthermore, our proof establishes the stronger guarantee of uniform convergence: the empirical utility of every linear contract is a $\\varepsilon$-approximation of its true expectation with probability at least $1-δ$, using the same optimal $O(\\ln(1/δ) / \\varepsilon^2)$ sample complexity.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01496v1",
    "published_date": "2026-01-04 11:45:17 UTC",
    "updated_date": "2026-01-04 11:45:17 UTC"
  },
  {
    "arxiv_id": "2601.01490v1",
    "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints",
    "authors": [
      "Junichiro Niimi"
    ],
    "abstract": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01490v1",
    "published_date": "2026-01-04 11:35:39 UTC",
    "updated_date": "2026-01-04 11:35:39 UTC"
  },
  {
    "arxiv_id": "2601.01487v1",
    "title": "DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion",
    "authors": [
      "Ziyue Zhang",
      "Luxi Lin",
      "Xiaolin Hu",
      "Chao Chang",
      "HuaiXi Wang",
      "Yiyi Zhou",
      "Rongrong Ji"
    ],
    "abstract": "Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01487v1",
    "published_date": "2026-01-04 11:27:26 UTC",
    "updated_date": "2026-01-04 11:27:26 UTC"
  },
  {
    "arxiv_id": "2601.01473v2",
    "title": "Accelerating Storage-Based Training for Graph Neural Networks",
    "authors": [
      "Myung-Hwan Jang",
      "Jeong-Min Park",
      "Yunyong Ko",
      "Sang-Wook Kim"
    ],
    "abstract": "Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, a storage-based approach to GNN training has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: how to handle a large number of small storage I/Os. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named AGNES, that employs a method of block-wise storage I/O processing to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, AGNES employs a simple yet effective strategy, hyperbatch-based processing based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that AGNES consistently outperforms four state-of-the-art methods, by up to 4.1X faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 12 figures, 2 tables, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.01473v2",
    "published_date": "2026-01-04 10:37:14 UTC",
    "updated_date": "2026-01-06 04:51:54 UTC"
  },
  {
    "arxiv_id": "2601.01467v1",
    "title": "A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts",
    "authors": [
      "Romuald Kwessy Mouona",
      "Blaise Blériot Koguep Njionou",
      "Etienne Romuald Temgoua Alomo",
      "Rokia Missaoui",
      "Leonard Kwuida"
    ],
    "abstract": "This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.01467v1",
    "published_date": "2026-01-04 10:21:06 UTC",
    "updated_date": "2026-01-04 10:21:06 UTC"
  },
  {
    "arxiv_id": "2601.01456v1",
    "title": "Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration",
    "authors": [
      "Wentao Bian",
      "Fenglei Xu"
    ],
    "abstract": "In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in \"Fuse-then-Refine\" paradigms: the \"Plasticity-Stability Dilemma.\" In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.01456v1",
    "published_date": "2026-01-04 09:53:49 UTC",
    "updated_date": "2026-01-04 09:53:49 UTC"
  },
  {
    "arxiv_id": "2601.04236v1",
    "title": "SmoothSync: Dual-Stream Diffusion Transformers for Jitter-Robust Beat-Synchronized Gesture Generation from Quantized Audio",
    "authors": [
      "Yujiao Jiang",
      "Qingmin Liao",
      "Zongqing Lu"
    ],
    "abstract": "Co-speech gesture generation is a critical area of research aimed at synthesizing speech-synchronized human-like gestures. Existing methods often suffer from issues such as rhythmic inconsistency, motion jitter, foot sliding and limited multi-sampling diversity. In this paper, we present SmoothSync, a novel framework that leverages quantized audio tokens in a novel dual-stream Diffusion Transformer (DiT) architecture to synthesis holistic gestures and enhance sampling variation. Specifically, we (1) fuse audio-motion features via complementary transformer streams to achieve superior synchronization, (2) introduce a jitter-suppression loss to improve temporal smoothness, (3) implement probabilistic audio quantization to generate distinct gesture sequences from identical inputs. To reliably evaluate beat synchronization under jitter, we introduce Smooth-BC, a robust variant of the beat consistency metric less sensitive to motion noise. Comprehensive experiments on the BEAT2 and SHOW datasets demonstrate SmoothSync's superiority, outperforming state-of-the-art methods by -30.6% FGD, 10.3% Smooth-BC, and 8.4% Diversity on BEAT2, while reducing jitter and foot sliding by -62.9% and -17.1% respectively. The code will be released to facilitate future research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.RO",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.04236v1",
    "published_date": "2026-01-04 09:53:07 UTC",
    "updated_date": "2026-01-04 09:53:07 UTC"
  },
  {
    "arxiv_id": "2601.04235v1",
    "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements",
    "authors": [
      "Hong Su"
    ],
    "abstract": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.04235v1",
    "published_date": "2026-01-04 09:52:56 UTC",
    "updated_date": "2026-01-04 09:52:56 UTC"
  },
  {
    "arxiv_id": "2601.01452v4",
    "title": "Robust and Efficient Zeroth-Order LLM Fine-Tuning via Adaptive Bayesian Subspace Optimizer",
    "authors": [
      "Jian Feng",
      "Zhihong Huang"
    ],
    "abstract": "Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations. However, existing methods essentially perform updates in a one-dimensional space, and suffer from collapse or substantial performance degradation under low-precision training. We introduce BSZO, an adaptive \\textbf{B}ayesian \\textbf{S}ubspace \\textbf{Z}eroth-Order \\textbf{O}ptimizer, which applies Kalman filtering to combine finite-difference information across multiple perturbation directions within a subspace. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the subspace-projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adapt to noise variations. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms the baselines across various tasks, achieving up to 6.67\\% absolute average improvement on OPT-13B while remaining robust under fp16/bf16 precision and keeping memory usage close to inference-only baselines (1.00$\\times$--1.08$\\times$ of MeZO).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 2 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.01452v4",
    "published_date": "2026-01-04 09:35:11 UTC",
    "updated_date": "2026-01-16 02:14:12 UTC"
  },
  {
    "arxiv_id": "2601.01438v1",
    "title": "Online Estimation and Manipulation of Articulated Objects",
    "authors": [
      "Russell Buchanan",
      "Adrian Röfer",
      "João Moura",
      "Abhinav Valada",
      "Sethu Vijayakumar"
    ],
    "abstract": "From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Autonomous Robots, and is available online at [Link will be updated when available]",
    "pdf_url": "https://arxiv.org/pdf/2601.01438v1",
    "published_date": "2026-01-04 08:52:56 UTC",
    "updated_date": "2026-01-04 08:52:56 UTC"
  },
  {
    "arxiv_id": "2601.04234v1",
    "title": "Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question",
    "authors": [
      "Denis Saklakov"
    ],
    "abstract": "Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 2 tables. Version 8",
    "pdf_url": "https://arxiv.org/pdf/2601.04234v1",
    "published_date": "2026-01-04 08:02:00 UTC",
    "updated_date": "2026-01-04 08:02:00 UTC"
  },
  {
    "arxiv_id": "2601.01410v4",
    "title": "Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems",
    "authors": [
      "Sunki Hong",
      "Jisoo Lee"
    ],
    "abstract": "Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics can mask this operational asymmetry. We introduce an operator-legible evaluation framework -- Under-Prediction Rate (UPR), tail Reserve$_{99.5}^{\\%}$ requirements, and explicit inflation diagnostics (Bias$_{24h}$/OPR) -- to quantify one-sided reliability risk beyond MAPE.\n  Using this framework, we evaluate state space models (Mamba variants) and strong baselines on a weather-aligned California Independent System Operator (CAISO) dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 regional transmission areas) under a rolling-origin walk-forward backtest. We develop and evaluate thermal-lag-aligned weather fusion strategies for these architectures.\n  Our results demonstrate that standard accuracy metrics are insufficient proxies for operational safety: models with comparable MAPE can imply materially different tail reserve requirements (Reserve$_{99.5}^{\\%}$). We show that explicit weather integration narrows error distributions, reducing the impact of temperature-driven demand spikes. Furthermore, while probabilistic calibration reduces large-error events, it can induce systematic schedule inflation. We introduce Bias/OPR-constrained objectives to enable auditable trade-offs between minimizing tail risk and preventing trivial over-forecasting.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SY",
    "comment": "30 pages, 7 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.01410v4",
    "published_date": "2026-01-04 07:30:50 UTC",
    "updated_date": "2026-01-21 03:43:31 UTC"
  },
  {
    "arxiv_id": "2601.02424v1",
    "title": "A large-scale nanocrystal database with aligned synthesis and properties enabling generative inverse design",
    "authors": [
      "Kai Gu",
      "Yingping Liang",
      "Senliang Peng",
      "Aotian Guo",
      "Haizheng Zhong",
      "Ying Fu"
    ],
    "abstract": "The synthesis of nanocrystals has been highly dependent on trial-and-error, due to the complex correlation between synthesis parameters and physicochemical properties. Although deep learning offers a potential methodology to achieve generative inverse design, it is still hindered by the scarcity of high-quality datasets that align nanocrystal synthesis routes with their properties. Here, we present the construction of a large-scale, aligned Nanocrystal Synthesis-Property (NSP) database and demonstrate its capability for generative inverse design. To extract structured synthesis routes and their corresponding product properties from literature, we develop NanoExtractor, a large language model (LLM) enhanced by well-designed augmentation strategies. NanoExtractor is validated against human experts, achieving a weighted average score of 88% on the test set, significantly outperforming chemistry-specialized (3%) and general-purpose LLMs (38%). The resulting NSP database contains nearly 160,000 aligned entries and serves as training data for our NanoDesigner, an LLM for inverse synthesis design. The generative capability of NanoDesigner is validated through the successful design of viable synthesis routes for both well-established PbSe nanocrystals and rarely reported MgF2 nanocrystals. Notably, the model recommends a counter-intuitive, non-stoichiometric precursor ratio (1:1) for MgF2 nanocrystals, which is experimentally confirmed as critical for suppressing byproducts. Our work bridges the gap between unstructured literature and data-driven synthesis, and also establishes a powerful human-AI collaborative paradigm for accelerating nanocrystal discovery.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.02424v1",
    "published_date": "2026-01-04 07:27:40 UTC",
    "updated_date": "2026-01-04 07:27:40 UTC"
  },
  {
    "arxiv_id": "2601.01406v1",
    "title": "SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution",
    "authors": [
      "Habiba Kausar",
      "Saeed Anwar",
      "Omar Jamal Hammad",
      "Abdul Bais"
    ],
    "abstract": "Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01406v1",
    "published_date": "2026-01-04 07:04:46 UTC",
    "updated_date": "2026-01-04 07:04:46 UTC"
  },
  {
    "arxiv_id": "2601.01403v1",
    "title": "A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble",
    "authors": [
      "Zewei Yu",
      "Jianqiu Xu",
      "Caimin Li"
    ],
    "abstract": "With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.01403v1",
    "published_date": "2026-01-04 06:51:46 UTC",
    "updated_date": "2026-01-04 06:51:46 UTC"
  },
  {
    "arxiv_id": "2601.01387v1",
    "title": "Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning",
    "authors": [
      "Yongzhe Li",
      "Lin Guan",
      "Zihan Cai",
      "Zuxian Lin",
      "Jiyu Huang",
      "Liukai Chen"
    ],
    "abstract": "Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01387v1",
    "published_date": "2026-01-04 05:59:41 UTC",
    "updated_date": "2026-01-04 05:59:41 UTC"
  },
  {
    "arxiv_id": "2601.01386v1",
    "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking",
    "authors": [
      "Xiaobao Wei",
      "Zhangjie Ye",
      "Yuxiang Gu",
      "Zunjie Zhu",
      "Yunfei Guo",
      "Yingying Shen",
      "Shan Zhao",
      "Ming Lu",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Rongfeng Lu",
      "Hangjun Ye"
    ],
    "abstract": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01386v1",
    "published_date": "2026-01-04 05:54:13 UTC",
    "updated_date": "2026-01-04 05:54:13 UTC"
  },
  {
    "arxiv_id": "2601.01383v1",
    "title": "Data Complexity-aware Deep Model Performance Forecasting",
    "authors": [
      "Yen-Chia Chen",
      "Hsing-Kuo Pao",
      "Hanjuan Huang"
    ],
    "abstract": "Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01383v1",
    "published_date": "2026-01-04 05:31:04 UTC",
    "updated_date": "2026-01-04 05:31:04 UTC"
  },
  {
    "arxiv_id": "2601.01378v1",
    "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification",
    "authors": [
      "Han Yuan",
      "Yilin Wu",
      "Li Zhang",
      "Zheng Ma"
    ],
    "abstract": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01378v1",
    "published_date": "2026-01-04 05:09:11 UTC",
    "updated_date": "2026-01-04 05:09:11 UTC"
  },
  {
    "arxiv_id": "2601.01373v1",
    "title": "UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models",
    "authors": [
      "Qundong Shi",
      "Jie Zhou",
      "Biyuan Lin",
      "Junbo Cui",
      "Guoyang Zeng",
      "Yixuan Zhou",
      "Ziyang Wang",
      "Xin Liu",
      "Zhen Luo",
      "Yudong Wang",
      "Zhiyuan Liu"
    ],
    "abstract": "The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models' performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at https://github.com/OpenBMB/UltraEval-Audio.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "13 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01373v1",
    "published_date": "2026-01-04 04:54:12 UTC",
    "updated_date": "2026-01-04 04:54:12 UTC"
  },
  {
    "arxiv_id": "2601.01366v1",
    "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models",
    "authors": [
      "Zixian Liu",
      "Sihao Liu",
      "Yuqi Zhao"
    ],
    "abstract": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01366v1",
    "published_date": "2026-01-04 04:39:39 UTC",
    "updated_date": "2026-01-04 04:39:39 UTC"
  },
  {
    "arxiv_id": "2601.01363v1",
    "title": "A unified multimodal understanding and generation model for cross-disciplinary scientific research",
    "authors": [
      "Xiaomeng Yang",
      "Zhiyu Tan",
      "Xiaohui Zhong",
      "Mengping Yang",
      "Qiusheng Huang",
      "Lei Chen",
      "Libo Wu",
      "Hao Li"
    ],
    "abstract": "Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01363v1",
    "published_date": "2026-01-04 04:31:38 UTC",
    "updated_date": "2026-01-04 04:31:38 UTC"
  },
  {
    "arxiv_id": "2601.06123v1",
    "title": "Latent Space Communication via K-V Cache Alignment",
    "authors": [
      "Lucio M. Dery",
      "Zohar Yahav",
      "Henry Prior",
      "Qixuan Feng",
      "Jiajun Shen",
      "Arthur Szlam"
    ],
    "abstract": "Solving increasingly complex problems with large language models (LLMs) necessitates a move beyond individual models and towards multi-model systems that can effectively collaborate. While text has traditionally served as the medium for inter-model communication, a richer and more efficient exchange is possible if models can access each other's internal states directly. In this paper, we propose learning a shared representation space that aligns the k-v caches of multiple models, creating a high-bandwidth channel for collaboration without altering the underlying pre-trained parameters. We do so by augmenting each model with adapters to translate its state into and out of this shared space. Via a suite of experiments with Gemma-2 models, we demonstrate that this approach not only enables seamless inter-model communication but also improves individual model performance. We also show that the shared space allows for the direct transfer of learned skills, such as soft prompts, between different models. Our work represents a significant step towards a future where models can fluidly share knowledge and capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 6 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06123v1",
    "published_date": "2026-01-04 04:15:25 UTC",
    "updated_date": "2026-01-04 04:15:25 UTC"
  },
  {
    "arxiv_id": "2601.06122v1",
    "title": "COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control",
    "authors": [
      "Canming Xia",
      "Peixi Peng",
      "Guang Tan",
      "Zhan Su",
      "Haoran Xu",
      "Zhenxian Liu",
      "Luntong Li"
    ],
    "abstract": "Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper was accepted by the Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)",
    "pdf_url": "https://arxiv.org/pdf/2601.06122v1",
    "published_date": "2026-01-04 03:53:05 UTC",
    "updated_date": "2026-01-04 03:53:05 UTC"
  },
  {
    "arxiv_id": "2601.01352v1",
    "title": "Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding",
    "authors": [
      "Yixuan Lai",
      "He Wang",
      "Kun Zhou",
      "Tianjia Shao"
    ],
    "abstract": "Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and \"average\" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01352v1",
    "published_date": "2026-01-04 03:41:55 UTC",
    "updated_date": "2026-01-04 03:41:55 UTC"
  },
  {
    "arxiv_id": "2601.01347v1",
    "title": "From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion",
    "authors": [
      "Yuyan Pi",
      "Min Jin",
      "Wentao Xie",
      "Xinhua Liu"
    ],
    "abstract": "Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages,5 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01347v1",
    "published_date": "2026-01-04 03:35:41 UTC",
    "updated_date": "2026-01-04 03:35:41 UTC"
  },
  {
    "arxiv_id": "2601.02422v1",
    "title": "Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning",
    "authors": [
      "Wenting Lu",
      "Didi Zhu",
      "Tao Shen",
      "Donglin Zhu",
      "Ayong Ye",
      "Chao Wu"
    ],
    "abstract": "Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.02422v1",
    "published_date": "2026-01-04 02:50:55 UTC",
    "updated_date": "2026-01-04 02:50:55 UTC"
  },
  {
    "arxiv_id": "2601.01330v1",
    "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale",
    "authors": [
      "Shengji Tang",
      "Weihao Lin",
      "Jingqi Ye",
      "Hao Li",
      "Bo Zhang",
      "Shuyue Hu",
      "Tao Chen",
      "Wangli Ouyang",
      "Lei Bai",
      "Peng Ye"
    ],
    "abstract": "Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.01330v1",
    "published_date": "2026-01-04 02:05:52 UTC",
    "updated_date": "2026-01-04 02:05:52 UTC"
  },
  {
    "arxiv_id": "2601.03287v1",
    "title": "Automated Post-Incident Policy Gap Analysis via Threat-Informed Evidence Mapping using Large Language Models",
    "authors": [
      "Huan Lin Oh",
      "Jay Yong Jun Jie",
      "Mandy Lee Ling Siu",
      "Jonathan Pan"
    ],
    "abstract": "Cybersecurity post-incident reviews are essential for identifying control failures and improving organisational resilience, yet they remain labour-intensive, time-consuming, and heavily reliant on expert judgment. This paper investigates whether Large Language Models (LLMs) can augment post-incident review workflows by autonomously analysing system evidence and identifying security policy gaps. We present a threat-informed, agentic framework that ingests log data, maps observed behaviours to the MITRE ATT&CK framework, and evaluates organisational security policies for adequacy and compliance. Using a simulated brute-force attack scenario against a Windows OpenSSH service (MITRE ATT&CK T1110), the system leverages GPT-4o for reasoning, LangGraph for multi-agent workflow orchestration, and LlamaIndex for traceable policy retrieval. Experimental results indicate that the LLM-based pipeline can interpret log-derived evidence, identify insufficient or missing policy controls, and generate actionable remediation recommendations with explicit evidence-to-policy traceability. Unlike prior work that treats log analysis and policy validation as isolated tasks, this study integrates both into a unified end-to-end proof-of-concept post-incident review framework. The findings suggest that LLM-assisted analysis has the potential to improve the efficiency, consistency, and auditability of post-incident evaluations, while highlighting the continued need for human oversight in high-stakes cybersecurity decision-making.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "5 pages, 1 figure. Preprint",
    "pdf_url": "https://arxiv.org/pdf/2601.03287v1",
    "published_date": "2026-01-04 01:39:20 UTC",
    "updated_date": "2026-01-04 01:39:20 UTC"
  },
  {
    "arxiv_id": "2601.01322v1",
    "title": "LinMU: Multimodal Understanding Made Linear",
    "authors": [
      "Hongjie Wang",
      "Niraj K. Jha"
    ],
    "abstract": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.01322v1",
    "published_date": "2026-01-04 01:17:36 UTC",
    "updated_date": "2026-01-04 01:17:36 UTC"
  },
  {
    "arxiv_id": "2601.01321v1",
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "authors": [
      "Rong Zhou",
      "Dongping Chen",
      "Zihan Jia",
      "Yao Su",
      "Yixin Liu",
      "Yiwen Lu",
      "Dongwei Shi",
      "Yue Huang",
      "Tianyang Xu",
      "Yi Pan",
      "Xinliang Li",
      "Yohannes Abate",
      "Qingyu Chen",
      "Zhengzhong Tu",
      "Yu Yang",
      "Yu Zhang",
      "Qingsong Wen",
      "Gengchen Mai",
      "Sunyang Fu",
      "Jiachen Li",
      "Xuyu Wang",
      "Ziran Wang",
      "Jing Huang",
      "Tianming Liu",
      "Yong Chen",
      "Lichao Sun",
      "Lifang He"
    ],
    "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01321v1",
    "published_date": "2026-01-04 01:17:09 UTC",
    "updated_date": "2026-01-04 01:17:09 UTC"
  },
  {
    "arxiv_id": "2601.01320v1",
    "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python",
    "authors": [
      "Muntasir Adnan",
      "Carlos C. N. Kuhn"
    ],
    "abstract": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01320v1",
    "published_date": "2026-01-04 01:13:37 UTC",
    "updated_date": "2026-01-04 01:13:37 UTC"
  },
  {
    "arxiv_id": "2601.01315v1",
    "title": "Quantifying Local Strain Field and Deformation in Active Contraction of Bladder Using a Pretrained Transformer Model: A Speckle-Free Approach",
    "authors": [
      "Alireza Asadbeygi",
      "Anne M. Robertson",
      "Yasutaka Tobe",
      "Masoud Zamani",
      "Sean D. Stocker",
      "Paul Watton",
      "Naoki Yoshimura",
      "Simon C Watkins"
    ],
    "abstract": "Accurate quantification of local strain fields during bladder contraction is essential for understanding the biomechanics of bladder micturition, in both health and disease. Conventional digital image correlation (DIC) methods have been successfully applied to various biological tissues; however, this approach requires artificial speckling, which can alter both passive and active properties of the tissue. In this study, we introduce a speckle-free framework for quantifying local strain fields using a state-of-the-art, zero-shot transformer model, CoTracker3. We utilized a custom-designed, portable isotonic biaxial apparatus compatible with multiphoton microscopy (MPM) to demonstrate this approach, successfully tracking natural bladder lumen textures without artificial markers. Benchmark tests validated the method's high pixel accuracy and low strain errors. Our framework effectively captured heterogeneous deformation patterns, despite complex folding and buckling, which conventional DIC often fails to track. Application to in vitro active bladder contractions in four rat specimens (n=4) revealed statistically significant anisotropy (p<0.01), with higher contraction longitudinally compared to circumferentially. Multiphoton microscopy further illustrated and confirmed heterogeneous morphological changes, such as large fold formation during active contraction. This non-invasive approach eliminates speckle-induced artifacts, enabling more physiologically relevant measurements, and has broad applicability for material testing of other biological and engineered systems.",
    "categories": [
      "q-bio.TO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.TO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.01315v1",
    "published_date": "2026-01-04 00:52:27 UTC",
    "updated_date": "2026-01-04 00:52:27 UTC"
  },
  {
    "arxiv_id": "2601.06121v1",
    "title": "Prompt Engineering for Responsible Generative AI Use in African Education: A Report from a Three-Day Training Series",
    "authors": [
      "Benjamin Quarshie",
      "Vanessa Willemse",
      "Macharious Nabang",
      "Bismark Nyaaba Akanzire",
      "Patrick Kyeremeh",
      "Saeed Maigari",
      "Dorcas Adomina",
      "Ellen Kwarteng",
      "Eric Kojo Majialuwe",
      "Craig Gibbs",
      "Jerry Etornam Kudaya",
      "Sechaba Koma",
      "Matthew Nyaaba Matthew Nyaaba"
    ],
    "abstract": "Generative artificial intelligence (GenAI) tools are increasingly adopted in education, yet many educators lack structured guidance on responsible and context sensitive prompt engineering, particularly in African and other resource constrained settings. This case report documents a three day online professional development programme organised by Generative AI for Education and Research in Africa (GenAI-ERA), designed to strengthen educators and researchers capacity to apply prompt engineering ethically for academic writing, teaching, and research. The programme engaged 468 participants across multiple African countries, including university educators, postgraduate students, and researchers. The training followed a scaffolded progression from foundational prompt design to applied and ethical strategies, including persona guided interactions. Data sources comprised registration surveys, webinar interaction records, facilitator observations, and session transcripts, analysed using descriptive statistics and computationally supported qualitative techniques. Findings indicate that participants increasingly conceptualised prompt engineering as a form of AI literacy requiring ethical awareness, contextual sensitivity, and pedagogical judgement rather than technical skill alone. The case highlights persistent challenges related to access, locally relevant training materials, and institutional support. The report recommends sustained professional development and the integration of prompt literacy into curricula to support responsible GenAI use in African education systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06121v1",
    "published_date": "2026-01-04 00:18:57 UTC",
    "updated_date": "2026-01-04 00:18:57 UTC"
  }
]
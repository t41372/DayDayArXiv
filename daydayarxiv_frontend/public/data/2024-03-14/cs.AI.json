{
  "date": "2024-03-14",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-14 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、鲁棒性提升、多模态处理和大型语言模型（LLM）的创新应用，令人印象深刻的包括 Sabiá-2 的新葡萄牙 LLM 及其在考试领域的表现，以及 Quanyan Zhu 等著名学者的网络安全研究，这些工作展示了 AI 在实际场景中的潜力。\n\n下面，我将逐一简要概述部分关键论文，先优先讨论那些重要、话题度高或有突破性的文章（如 LLM、AI 安全和医疗应用），然后快速掠过其他较常规的。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### 重点论文讨论\n\n- **Sabiá-2: A New Generation of Portuguese Large Language Models**  \n  这篇论文引入了 Sabiá-2 系列模型，针对葡萄牙语文本训练，显著提升了在各种考试（如会计、经济和医学）的表现，匹配或超越 GPT-4，在数学和编码任务上仍有改进空间，展示了 LLM 在非英语领域的适用性。\n\n- **FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models**  \n  作者包括 Peter Richtárik，该工作扩展了 Scaffnew 算法，通过压缩和量化技术优化联邦学习（Federated Learning），显著减少通信开销，在异构环境中实验证明其高效性。\n\n- **Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare**  \n  Quanyan Zhu 等学者提出结合博弈论和基础模型的框架，用于网络欺骗战术设计，支持预测对手行为和自适应防御，强调多代理神经符号学习的潜力。\n\n- **Fisher Mask Nodes for Language Model Merging**  \n  该论文开发了一种基于 Fisher 信息的新方法，用于 Transformer 模型合并，提高多任务性能，计算成本远低于传统 Fisher 加权平均，提供高达 6.5% 的基准提升。\n\n- **Self-Consistency Boosts Calibration for Math Reasoning**  \n  工作利用自一致性（Self-Consistency）改进 LLM 在数学推理中的校准效果，在 GSM8K 和 MathQA 数据集上超越现有方法，提升了模型置信度和准确性。\n\n- **Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking**  \n  这篇论文探索 LLM 通过自监督学习生成推理路径，显著提高复杂任务性能，如 GSM8K 上的准确率从 5.9% 提升至 10.9%，展示了模型的自学习能力。\n\n- **3D-VLA: A 3D Vision-Language-Action Generative World Model**  \n  提出 3D-VLA 框架，支持多模态生成和规划，结合 3D 感知和扩散模型，在机器人任务中实现高效交互，显著提升了视觉语言模型的 3D 理解。\n\n- **Predicting Generalization of AI Colonoscopy Models to Unseen Data**  \n  使用 Masked Siamese Network 预测 AI 结肠镜模型在新数据上的泛化性能，准确率高达 99%，无需标签，支持医疗图像领域应用。\n\n- **Towards White Box Deep Learning**  \n  该工作设计语义敏感特征架构，使神经网络更易解释和鲁棒，实现了接近人类水平的对抗测试，无需对抗训练。\n\n- **Logits of API-Protected LLMs Leak Proprietary Information**  \n  揭示 API 保护的 LLM 存在 token-length 侧通道漏洞，通过大语言模型翻译，可重建 29% 的响应内容，强调了 LLM 安全风险。\n\n### 其他相关论文简述\n\n接下来，快速概述一些主题相关的论文，聚焦核心贡献：\n\n- **Mind the GAP: Improving Robustness to Subpopulation Shifts**  \n  引入 Group-Aware Priors 分布，提升 AI 模型对子群移位的鲁棒性，在异构数据上表现突出。\n\n- **ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks**  \n  通过对抗性边删除优化图神经网络，提高泛化性能。\n\n- **B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions**  \n  构建基准数据集评估多模态 LLM 的鲁棒性，识别视觉指令攻击。\n\n- **EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning**  \n  提出 EquiAV 框架，提升音频-视觉对比学习的鲁棒性。\n\n- **D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection**  \n  开发双域教师模型，桥接 RGB 和热成像域隙，提升目标检测鲁棒性。\n\n其他论文如优化算法、图神经网络或特定领域应用（如电池管理、语音识别），虽有贡献但相对常规，我将简要掠过：例如，**Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt** 改进了少样本增量学习；**ThermoHands** 构建了热成像手势基准；**Towards White Box Deep Learning** 强调可解释性。这些工作在各自领域有细微提升，但未见重大突破，故不展开。\n\n总之，今天的论文突出了 AI 模型的鲁棒性和多模态融合潜力，AI 安全和 LLM 应用是亮点，读者可关注这些方向寻找感兴趣的文章！",
  "papers": [
    {
      "arxiv_id": "2403.09925v1",
      "title": "Surrogate Assisted Monte Carlo Tree Search in Combinatorial Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Saeid Amiri",
        "Parisa Zehtabi",
        "Danial Dervovic",
        "Michael Cashmore"
      ],
      "abstract": "Industries frequently adjust their facilities network by opening new branches\nin promising areas and closing branches in areas where they expect low profits.\nIn this paper, we examine a particular class of facility location problems. Our\nobjective is to minimize the loss of sales resulting from the removal of\nseveral retail stores. However, estimating sales accurately is expensive and\ntime-consuming. To overcome this challenge, we leverage Monte Carlo Tree Search\n(MCTS) assisted by a surrogate model that computes evaluations faster. Results\nsuggest that MCTS supported by a fast surrogate function can generate solutions\nfaster while maintaining a consistent solution compared to MCTS that does not\nbenefit from the surrogate function.",
      "tldr_zh": "本论文探讨了组合优化问题中的设施位置决策，目标是最小化移除零售店导致的销售损失。研究提出了一种Surrogate Assisted Monte Carlo Tree Search (MCTS)方法，使用surrogate model作为快速评估工具，以降低传统MCTS的计算开销。实验结果显示，这种辅助方法能显著加快解决方案的生成，同时保持与标准MCTS相当的一致性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the ICAPS Planning and Scheduling for Financial Services\n  (FINPLAN) 2023 workshop",
      "pdf_url": "http://arxiv.org/pdf/2403.09925v1",
      "published_date": "2024-03-14 23:54:19 UTC",
      "updated_date": "2024-03-14 23:54:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:20:48.660425"
    },
    {
      "arxiv_id": "2403.09920v3",
      "title": "Predicting Generalization of AI Colonoscopy Models to Unseen Data",
      "title_zh": "翻译失败",
      "authors": [
        "Joel Shor",
        "Carson McNeil",
        "Yotam Intrator",
        "Joseph R Ledsam",
        "Hiro-o Yamano",
        "Daisuke Tsurumaru",
        "Hiroki Kayama",
        "Atsushi Hamabe",
        "Koji Ando",
        "Mitsuhiko Ota",
        "Haruei Ogino",
        "Hiroshi Nakase",
        "Kaho Kobayashi",
        "Masaaki Miyo",
        "Eiji Oki",
        "Ichiro Takemasa",
        "Ehud Rivlin",
        "Roman Goldenberg"
      ],
      "abstract": "$\\textbf{Background}$: Generalizability of AI colonoscopy algorithms is\nimportant for wider adoption in clinical practice. However, current techniques\nfor evaluating performance on unseen data require expensive and time-intensive\nlabels.\n  $\\textbf{Methods}$: We use a \"Masked Siamese Network\" (MSN) to identify novel\nphenomena in unseen data and predict polyp detector performance. MSN is trained\nto predict masked out regions of polyp images, without any labels. We test\nMSN's ability to be trained on data only from Israel and detect unseen\ntechniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes\nfrom Japan (354 videos, 128 hours). We also test MSN's ability to predict\nperformance of Computer Aided Detection (CADe) of polyps on colonoscopies from\nboth countries, even though MSN is not trained on data from Japan.\n  $\\textbf{Results}$: MSN correctly identifies NBI and CE as less similar to\nIsrael whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p <\n10^-8 for both) using the label-free Frechet distance. MSN detects NBI with 99%\naccuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite\nbeing trained only on whitelight, and is the only method that is robust to\nnoisy labels. MSN predicts CADe polyp detector performance on in-domain Israel\nand out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few\nexamples of Japan detector performance to train on, MSN prediction of Japan\nperformance improves (r=0.56).\n  $\\textbf{Conclusion}$: Our technique can identify distribution shifts in\nclinical data and can predict CADe detector performance on unseen data, without\nlabels. Our self-supervised approach can aid in detecting when data in practice\nis different from training, such as between hospitals or data has meaningfully\nshifted from training. MSN has potential for application to medical image\ndomains beyond colonoscopy.",
      "tldr_zh": "这篇论文提出使用 Masked Siamese Network (MSN) 来预测 AI 结肠镜模型在新数据上的泛化性能，而无需昂贵的标签，从而解决当前评估方法的局限性。MSN 通过无监督训练在以色列数据上学习预测 polyp 图像的 masked 区域，并成功识别未见技术如 Narrow-Band Imaging (NBI) 和 Chromendoscopy (CE)，同时预测 Computer Aided Detection (CADe) 检测器的性能。结果显示，MSN 在识别 NBI 时准确率达99%，并在预测 CADe 性能时表现出色（相关系数分别为0.79和0.37），且具有检测数据分布偏移的潜力，可扩展到其他医疗图像领域。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09920v3",
      "published_date": "2024-03-14 23:41:00 UTC",
      "updated_date": "2024-03-22 04:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:21:04.247927"
    },
    {
      "arxiv_id": "2403.09904v1",
      "title": "FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Yi",
        "Georg Meinhardt",
        "Laurent Condat",
        "Peter Richtárik"
      ],
      "abstract": "Federated Learning (FL) has garnered increasing attention due to its unique\ncharacteristic of allowing heterogeneous clients to process their private data\nlocally and interact with a central server, while being respectful of privacy.\nA critical bottleneck in FL is the communication cost. A pivotal strategy to\nmitigate this burden is \\emph{Local Training}, which involves running multiple\nlocal stochastic gradient descent iterations between communication phases. Our\nwork is inspired by the innovative \\emph{Scaffnew} algorithm, which has\nconsiderably advanced the reduction of communication complexity in FL. We\nintroduce FedComLoc (Federated Compressed and Local Training), integrating\npractical and effective compression into \\emph{Scaffnew} to further enhance\ncommunication efficiency. Extensive experiments, using the popular TopK\ncompressor and quantization, demonstrate its prowess in substantially reducing\ncommunication overheads in heterogeneous settings.",
      "tldr_zh": "该论文针对 Federated Learning (FL) 中的高通信成本问题，提出了 FedComLoc 方法，该方法将压缩技术（如 TopK compressor 和量化）整合到 Scaffnew 算法中，并结合 Local Training 策略进行分布式训练。FedComLoc 通过在异构客户端上运行多轮本地随机梯度下降，同时减少数据传输量，显著提升了通信效率。实验结果显示，该方法在异构设置下大幅降低了通信开销，为隐私保护的分布式模型训练提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09904v1",
      "published_date": "2024-03-14 22:29:59 UTC",
      "updated_date": "2024-03-14 22:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:21:15.117094"
    },
    {
      "arxiv_id": "2403.09891v3",
      "title": "Fisher Mask Nodes for Language Model Merging",
      "title_zh": "Fisher Mask Nodes 用于语言模型合并",
      "authors": [
        "Thennal D K",
        "Ganesh Nathan",
        "Suchithra M S"
      ],
      "abstract": "Fine-tuning pre-trained models provides significant advantages in downstream\nperformance. The ubiquitous nature of pre-trained models such as BERT and its\nderivatives in natural language processing has also led to a proliferation of\ntask-specific fine-tuned models. As these models typically only perform one\ntask well, additional training or ensembling is required in multi-task\nscenarios. The growing field of model merging provides a solution, dealing with\nthe challenge of combining multiple task-specific models into a single\nmulti-task model. In this study, we introduce a novel model merging method for\nTransformers, combining insights from previous work in Fisher-weighted\naveraging and the use of Fisher information in model pruning. Utilizing the\nFisher information of mask nodes within the Transformer architecture, we devise\na computationally efficient weighted-averaging scheme. Our method exhibits a\nregular and significant performance increase across various models in the BERT\nfamily, outperforming full-scale Fisher-weighted averaging in a fraction of the\ncomputational cost, with baseline performance improvements of up to +6.5 and a\nspeedup between 57.4x and 321.7x across models. Our results prove the potential\nof our method in current multi-task learning environments and suggest its\nscalability and adaptability to new model architectures and learning scenarios.",
      "tldr_zh": "本研究针对多任务场景中任务特定模型的合并挑战，提出了一种基于 Fisher Mask Nodes 的新颖方法，结合 Fisher-weighted averaging 和 Fisher information 在 Transformer 架构中的应用。\n该方法通过利用 mask nodes 的 Fisher 信息，实现高效的加权平均方案，显著降低了计算成本。\n实验结果显示，在 BERT 家族模型上，该方法比全规模 Fisher-weighted averaging 性能提升高达 +6.5，且速度加速 57.4x 到 321.7x。\n这项技术证明了其在多任务学习环境中的潜力，并展示了良好的可扩展性和适应性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09891v3",
      "published_date": "2024-03-14 21:52:26 UTC",
      "updated_date": "2024-05-03 13:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:21:27.442944"
    },
    {
      "arxiv_id": "2403.09887v2",
      "title": "Sabiá-2: A New Generation of Portuguese Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Thales Sales Almeida",
        "Hugo Abonizio",
        "Rodrigo Nogueira",
        "Ramon Pires"
      ],
      "abstract": "We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.",
      "tldr_zh": "我们引入了 Sabiá-2，这是一个基于葡萄牙语文本训练的新一代 Large Language Models 家族。模型在多种考试中进行了评估，包括巴西大学入学考试、专业认证考试以及研究生级别的会计、经济、工程、法律和医学等领域考试。结果显示，Sabiá-2 Medium 在 64 个考试中超过了 GPT-3.5 的 58 个，并与 GPT-4 持平或优于其在 23 个考试中的表现。专业化训练显著提升了模型性能，而无需增加模型大小，使其每 token 价格比 GPT-4 便宜 10 倍。最后，该研究指出，数学和编码是模型需要进一步改进的关键能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09887v2",
      "published_date": "2024-03-14 21:44:48 UTC",
      "updated_date": "2024-03-26 23:52:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:21:40.005903"
    },
    {
      "arxiv_id": "2403.18850v2",
      "title": "Are Colors Quanta of Light for Human Vision? A Quantum Cognition Study of Visual Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Jonito Aerts Arguëlles"
      ],
      "abstract": "We show that colors are light quanta for human visual perception in a similar\nway as photons are light quanta for physical measurements of light waves. Our\nresult relies on the identification in the quantum measurement process itself\nof the warping mechanism which is characteristic of human perception. This\nwarping mechanism makes stimuli classified into the same category perceived as\nmore similar, while stimuli classified into different m categories are\nperceived as more different. In the quantum measurement process, the warping\ntakes place between the pure states, which play the role played for human\nperception by the stimuli, and the density states after decoherence, which play\nthe role played for human perception by the percepts. We use the natural metric\nfor pure states, namely the normalized Fubini Study metric to measure distances\nbetween pure states, and the natural metric for density states, namely the\nnormalized trace-class metric, to measure distances between density states. We\nthen show that when pure states lie within a well-defined region surrounding an\neigenstate, the quantum measurement, namely the process of decoherence,\ncontracts the distance between these pure states, while the reverse happens for\npure states lying in a well-defined region between two eigenstates, for which\nthe quantum measurement causes a dilation. We elaborate as an example the\nsituation of a two-dimensional quantum measurement described by the Bloch model\nand apply it to the situation of two colors 'Light' and 'Dark'. We argue that\nthis analogy of warping, on the one hand in human perception and on the other\nhand in the quantum measurement process, makes colors to be quanta of light for\nhuman vision.",
      "tldr_zh": "本研究探讨了颜色在人类视觉感知中是否如同光子是光的量子，基于量子认知框架进行分析。主要方法是将人类感知的扭曲机制（warping mechanism）与量子测量过程（quantum measurement）类比，使用 normalized Fubini Study metric 测量纯态（pure states）之间的距离，以及 normalized trace-class metric 测量密度态（density states）之间的距离。结果显示，在特定区域，量子测量会导致纯态距离收缩或扩张，类似于人类将同类颜色感知为更相似、不同类为更不同；以 Bloch 模型和 'Light' 与 'Dark' 颜色为例，该类比证明了颜色作为人类视觉光量子的特性。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "q-bio.NC",
      "comment": "22 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2208.03726",
      "pdf_url": "http://arxiv.org/pdf/2403.18850v2",
      "published_date": "2024-03-14 21:10:07 UTC",
      "updated_date": "2025-05-05 18:35:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:21:51.892169"
    },
    {
      "arxiv_id": "2403.09871v5",
      "title": "ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images",
      "title_zh": "翻译失败",
      "authors": [
        "Fangqiang Ding",
        "Yunzhou Zhu",
        "Xiangyu Wen",
        "Gaowen Liu",
        "Chris Xiaoxuan Lu"
      ],
      "abstract": "Designing egocentric 3D hand pose estimation systems that can perform\nreliably in complex, real-world scenarios is crucial for downstream\napplications. Previous approaches using RGB or NIR imagery struggle in\nchallenging conditions: RGB methods are susceptible to lighting variations and\nobstructions like handwear, while NIR techniques can be disrupted by sunlight\nor interference from other NIR-equipped devices. To address these limitations,\nwe present ThermoHands, the first benchmark focused on thermal image-based\negocentric 3D hand pose estimation, demonstrating the potential of thermal\nimaging to achieve robust performance under these conditions. The benchmark\nincludes a multi-view and multi-spectral dataset collected from 28 subjects\nperforming hand-object and hand-virtual interactions under diverse scenarios,\naccurately annotated with 3D hand poses through an automated process. We\nintroduce a new baseline method, TherFormer, utilizing dual transformer modules\nfor effective egocentric 3D hand pose estimation in thermal imagery. Our\nexperimental results highlight TherFormer's leading performance and affirm\nthermal imaging's effectiveness in enabling robust 3D hand pose estimation in\nadverse conditions.",
      "tldr_zh": "这篇论文提出了 ThermoHands，这是一个针对 egocentric 热图像的基准，用于评估 3D 手姿估计的鲁棒性，以解决 RGB 和 NIR 方法在光线变化、遮挡或阳光干扰等复杂场景中的局限性。数据集包括从 28 个受试者收集的多视图多光谱数据，涵盖手-物体和手-虚拟交互的多样场景，并通过自动化过程标注了准确的 3D 手姿。作者引入了新基线方法 TherFormer，利用双 Transformer 模块进行有效的 egocentric 3D 手姿估计。实验结果显示 TherFormer 性能领先，证明热图像在不利条件下能实现更可靠的 3D 手姿估计。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to the 23rd ACM Conference on Embedded\n  Networked Sensor Systems (Sensys'25). Including 14 pages, 9 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.09871v5",
      "published_date": "2024-03-14 21:01:06 UTC",
      "updated_date": "2025-02-27 15:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:22:04.295943"
    },
    {
      "arxiv_id": "2403.09869v1",
      "title": "Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors",
      "title_zh": "翻译失败",
      "authors": [
        "Tim G. J. Rudner",
        "Ya Shi Zhang",
        "Andrew Gordon Wilson",
        "Julia Kempe"
      ],
      "abstract": "Machine learning models often perform poorly under subpopulation shifts in\nthe data distribution. Developing methods that allow machine learning models to\nbetter generalize to such shifts is crucial for safe deployment in real-world\nsettings. In this paper, we develop a family of group-aware prior (GAP)\ndistributions over neural network parameters that explicitly favor models that\ngeneralize well under subpopulation shifts. We design a simple group-aware\nprior that only requires access to a small set of data with group information\nand demonstrate that training with this prior yields state-of-the-art\nperformance -- even when only retraining the final layer of a previously\ntrained non-robust model. Group aware-priors are conceptually simple,\ncomplementary to existing approaches, such as attribute pseudo labeling and\ndata reweighting, and open up promising new avenues for harnessing Bayesian\ninference to enable robustness to subpopulation shifts.",
      "tldr_zh": "该论文针对机器学习模型在子群体偏移(subpopulation shifts)下的泛化性能不佳问题，提出了一种组感知先验(Group-Aware Priors, GAP)分布，用于神经网络参数，以优先选择对偏移鲁棒的模型。GAP仅需少量带有组信息的数据，便能实现简单训练，甚至只需重新训练模型的最终层，即可达到最先进性能。作者强调，这种方法概念简单，与现有技术如属性伪标签和数据再加权互补，并为利用贝叶斯推理(Bayesian inference)增强对子群体偏移的鲁棒性开辟新途径。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "Published in Proceedings of the 27th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.09869v1",
      "published_date": "2024-03-14 21:00:26 UTC",
      "updated_date": "2024-03-14 21:00:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:22:15.250885"
    },
    {
      "arxiv_id": "2403.09863v5",
      "title": "Towards White Box Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Maciej Satkiewicz"
      ],
      "abstract": "Deep neural networks learn fragile \"shortcut\" features, rendering them\ndifficult to interpret (black box) and vulnerable to adversarial attacks. This\npaper proposes semantic features as a general architectural solution to this\nproblem. The main idea is to make features locality-sensitive in the adequate\nsemantic topology of the domain, thus introducing a strong regularization. The\nproof of concept network is lightweight, inherently interpretable and achieves\nalmost human-level adversarial test metrics - with no adversarial training!\nThese results and the general nature of the approach warrant further research\non semantic features. The code is available at\nhttps://github.com/314-Foundation/white-box-nn",
      "tldr_zh": "本论文针对深度神经网络学习脆弱的“shortcut features”问题，导致模型难以解释（黑盒）和易受“adversarial attacks”，提出使用“semantic features”作为通用架构解决方案。该方法通过使特征在适当的语义拓扑中局部敏感，引入强“regularization”，从而提升模型的可解释性和鲁棒性。实验结果显示，该轻量级网络在对抗测试中达到近人类水平性能，且无需“adversarial training”，代码已在 GitHub 开源，呼吁进一步研究“semantic features”。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 12 figures, independent research, v5 changes: Expanded\n  Abstract and Related Work section; minor wording improvements",
      "pdf_url": "http://arxiv.org/pdf/2403.09863v5",
      "published_date": "2024-03-14 20:50:03 UTC",
      "updated_date": "2024-04-17 17:58:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:22:27.062092"
    },
    {
      "arxiv_id": "2403.09861v1",
      "title": "NN-Defined Modulator: Reconfigurable and Portable Software Modulator on IoT Gateways",
      "title_zh": "翻译失败",
      "authors": [
        "Jiazhao Wang",
        "Wenchao Jiang",
        "Ruofeng Liu",
        "Bin Hu",
        "Demin Gao",
        "Shuai Wang"
      ],
      "abstract": "A physical-layer modulator is a vital component for an IoT gateway to map the\nsymbols to signals. However, due to the soldered hardware chipsets on the\ngateway's motherboards or the diverse toolkits on different platforms for the\nsoftware radio, the existing solutions either have limited extensibility or are\nplatform-specific. Such limitation is hard to ignore when modulation schemes\nand hardware platforms have become extremely diverse. This paper presents a new\nparadigm of using neural networks as an abstraction layer for physical layer\nmodulators in IoT gateway devices, referred to as NN-defined modulators. Our\napproach addresses the challenges of extensibility and portability for multiple\ntechnologies on various hardware platforms. The proposed NN-defined modulator\nuses a model-driven methodology rooted in solid mathematical foundations while\nhaving native support for hardware acceleration and portability to\nheterogeneous platforms. We conduct the evaluation of NN-defined modulators on\ndifferent platforms, including Nvidia Jetson Nano and Raspberry Pi. Evaluations\ndemonstrate that our NN-defined modulator effectively operates as conventional\nmodulators and provides significant efficiency gains (up to $4.7\\times$ on\nNvidia Jetson Nano and $1.1\\times$ on Raspberry Pi), indicating high\nportability. Furthermore, we show the real-world applications using our\nNN-defined modulators to generate ZigBee and WiFi packets, which are compliant\nwith commodity TI CC2650 (ZigBee) and Intel AX201 (WiFi NIC), respectively.",
      "tldr_zh": "本论文提出了一种名为 NN-defined Modulators 的新范式，用于 IoT Gateways 的物理层调制器，以解决现有方案的可扩展性和平台特定性问题。 该方法利用神经网络作为抽象层，采用模型驱动的方法ology，支持硬件加速和跨平台移植，并在 Nvidia Jetson Nano 和 Raspberry Pi 等平台上实现了显著效率提升（分别高达 4.7 倍和 1.1 倍）。 实验结果证明，该调制器能生成符合标准的 ZigBee 和 WiFi packets，展示了其在实际应用中的 portability 和实用性。",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09861v1",
      "published_date": "2024-03-14 20:42:23 UTC",
      "updated_date": "2024-03-14 20:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:22:39.421234"
    },
    {
      "arxiv_id": "2403.09857v3",
      "title": "Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt",
      "title_zh": "翻译失败",
      "authors": [
        "Chenxi Liu",
        "Zhenyi Wang",
        "Tianyi Xiong",
        "Ruibo Chen",
        "Yihan Wu",
        "Junfeng Guo",
        "Heng Huang"
      ],
      "abstract": "Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn\nnew classes with scarce samples while preserving knowledge of old ones.\nExisting FSCIL methods usually fine-tune the entire backbone, leading to\noverfitting and hindering the potential to learn new classes. On the other\nhand, recent prompt-based CIL approaches alleviate forgetting by training\nprompts with sufficient data in each task. In this work, we propose a novel\nframework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages\ntask-invariant prompts to capture shared knowledge by reducing specific\ninformation from the attention aspect. Additionally, self-adaptive\ntask-specific prompts in ASP provide specific information and transfer\nknowledge from old classes to new classes with an Information Bottleneck\nlearning objective. In summary, ASP prevents overfitting on base task and does\nnot require enormous data in few-shot incremental tasks. Extensive experiments\non three benchmark datasets validate that ASP consistently outperforms\nstate-of-the-art FSCIL and prompt-based CIL methods in terms of both learning\nnew classes and mitigating forgetting.",
      "tldr_zh": "这篇论文针对 Few-Shot Class-Incremental Learning (FSCIL) 提出了一种新框架 Attention-aware Self-adaptive Prompt (ASP)，旨在使用少量样本学习新类同时保留旧类知识，避免现有方法因微调整个 backbone 而导致的过拟合问题。ASP 通过 task-invariant prompts 从 attention 角度减少特定信息以捕捉共享知识，并利用自适应 task-specific prompts 结合 Information Bottleneck 学习目标来提供特定信息并转移旧类知识。在三个基准数据集上的广泛实验验证，ASP 在学习新类和缓解遗忘方面均优于最先进 FSCIL 和 prompt-based CIL 方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09857v3",
      "published_date": "2024-03-14 20:34:53 UTC",
      "updated_date": "2024-07-17 16:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:22:53.988795"
    },
    {
      "arxiv_id": "2403.10570v2",
      "title": "Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Li",
        "Quanyan Zhu"
      ],
      "abstract": "We are currently facing unprecedented cyber warfare with the rapid evolution\nof tactics, increasing asymmetry of intelligence, and the growing accessibility\nof hacking tools. In this landscape, cyber deception emerges as a critical\ncomponent of our defense strategy against increasingly sophisticated attacks.\nThis chapter aims to highlight the pivotal role of game-theoretic models and\nfoundation models (FMs) in analyzing, designing, and implementing cyber\ndeception tactics. Game models (GMs) serve as a foundational framework for\nmodeling diverse adversarial interactions, allowing us to encapsulate both\nadversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the\nbuilding blocks for creating tailored machine learning models suited to given\napplications. By leveraging the synergy between GMs and FMs, we can advance\nproactive and automated cyber defense mechanisms by not only securing our\nnetworks against attacks but also enhancing their resilience against\nwell-planned operations. This chapter discusses the games at the tactical,\noperational, and strategic levels of warfare, delves into the symbiotic\nrelationship between these methodologies, and explores relevant applications\nwhere such a framework can make a substantial impact in cybersecurity. The\nchapter discusses the promising direction of the multi-agent neurosymbolic\nconjectural learning (MANSCOL), which allows the defender to predict\nadversarial behaviors, design adaptive defensive deception tactics, and\nsynthesize knowledge for the operational level synthesis and adaptation. FMs\nserve as pivotal tools across various functions for MANSCOL, including\nreinforcement learning, knowledge assimilation, formation of conjectures, and\ncontextual representation. This chapter concludes with a discussion of the\nchallenges associated with FMs and their application in the domain of\ncybersecurity.",
      "tldr_zh": "本论文探讨了在战略网络战中，利用博弈理论模型（Game models, GMs）和基础模型（Foundation models, FMs）的协同框架，来分析、设计和实施网络欺骗策略，以应对战术演变、情报不对称和黑客工具普及等挑战。GMs 用于模拟对手互动并整合领域知识，而 FMs 则作为构建定制机器学习模型的基石，支持主动防御机制的自动化。论文重点介绍了多智能体神经符号推测学习（Multi-agent neurosymbolic conjectural learning, MANSCOL），该方法能预测对手行为、设计自适应欺骗策略，并通过 FMs 实现强化学习、知识吸收和上下文表示。总体上，这种框架提升了网络防御的韧性和有效性，但也指出了 FMs 在网络安全应用中的潜在挑战。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.CR",
      "comment": "40 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.10570v2",
      "published_date": "2024-03-14 20:17:57 UTC",
      "updated_date": "2024-08-19 00:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:23:06.904885"
    },
    {
      "arxiv_id": "2403.09849v1",
      "title": "Self-Consistency Boosts Calibration for Math Reasoning",
      "title_zh": "自一致性提升数学推理的校准",
      "authors": [
        "Ante Wang",
        "Linfeng Song",
        "Ye Tian",
        "Baolin Peng",
        "Lifeng Jin",
        "Haitao Mi",
        "Jinsong Su",
        "Dong Yu"
      ],
      "abstract": "Calibration, which establishes the correlation between accuracy and model\nconfidence, is important for LLM development. We design three off-the-shelf\ncalibration methods based on self-consistency (Wang et al., 2022) for math\nreasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using\nstrong open-source LLMs (Mistral and LLaMA2), our methods better bridge model\nconfidence and accuracy than existing methods based on p(True) (Kadavath et\nal., 2022) or logit (Kadavath et al., 2022).",
      "tldr_zh": "这篇论文探讨了校准（Calibration）在数学推理任务中的作用，强调通过自一致性（Self-Consistency）方法提升模型置信度和准确性的相关性。研究者设计了三种基于 Self-Consistency 的校准方法，并使用 Mistral 和 LLaMA2 等开源大语言模型（LLMs）在 GSM8K 和 MathQA 基准上进行评估。结果表明，这些方法比基于 p(True) 或 logit 的现有方法更有效地桥接了模型置信度和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09849v1",
      "published_date": "2024-03-14 20:17:10 UTC",
      "updated_date": "2024-03-14 20:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:23:17.031935"
    },
    {
      "arxiv_id": "2403.09847v1",
      "title": "Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches",
      "title_zh": "翻译失败",
      "authors": [
        "Sabrina Guastavino",
        "Katsiaryna Bahamazava",
        "Emma Perracchione",
        "Fabiana Camattari",
        "Gianluca Audone",
        "Daniele Telloni",
        "Roberto Susino",
        "Gianalfredo Nicolini",
        "Silvano Fineschi",
        "Michele Piana",
        "Anna Maria Massone"
      ],
      "abstract": "This study addresses the prediction of geomagnetic disturbances by exploiting\nmachine learning techniques. Specifically, the Long-Short Term Memory recurrent\nneural network, which is particularly suited for application over long time\nseries, is employed in the analysis of in-situ measurements of solar wind\nplasma and magnetic field acquired over more than one solar cycle, from $2005$\nto $2019$, at the Lagrangian point L$1$. The problem is approached as a binary\nclassification aiming to predict one hour in advance a decrease in the SYM-H\ngeomagnetic activity index below the threshold of $-50$ nT, which is generally\nregarded as indicative of magnetospheric perturbations. The strong class\nimbalance issue is tackled by using an appropriate loss function tailored to\noptimize appropriate skill scores in the training phase of the neural network.\nBeside classical skill scores, value-weighted skill scores are then employed to\nevaluate predictions, suitable in the study of problems, such as the one faced\nhere, characterized by strong temporal variability. For the first time, the\ncontent of magnetic helicity and energy carried by solar transients, associated\nwith their detection and likelihood of geo-effectiveness, were considered as\ninput features of the network architecture. Their predictive capabilities are\ndemonstrated through a correlation-driven feature selection method to rank the\nmost relevant characteristics involved in the neural network prediction model.\nThe optimal performance of the adopted neural network in properly forecasting\nthe onset of geomagnetic storms, which is a crucial point for giving real\nwarnings in an operational setting, is finally showed.",
      "tldr_zh": "该研究利用 LSTM 递归神经网络分析 2005-2019 年太阳风等离子体和磁场数据，提前一小时预测 SYM-H 指数低于 -50 nT 的地磁扰动事件，以解决地磁风暴预报问题。针对类别不平衡问题，采用定制损失函数优化技能分数，并首次将 magnetic helicity 和能量作为输入特征，以评估其预测能力。研究通过相关性驱动的特征选择方法识别了最相关特征，最终展示了模型在准确预报地磁风暴方面的优异性能，为实际预警应用提供了重要基础。",
      "categories": [
        "physics.space-ph",
        "astro-ph.SR",
        "cs.AI",
        "85-08, 68T07, 68T05"
      ],
      "primary_category": "physics.space-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09847v1",
      "published_date": "2024-03-14 20:13:26 UTC",
      "updated_date": "2024-03-14 20:13:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:23:30.815700"
    },
    {
      "arxiv_id": "2403.10569v1",
      "title": "Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment",
      "title_zh": "翻译失败",
      "authors": [
        "Atah Nuh Mih",
        "Alireza Rahimi",
        "Asfia Kawnine",
        "Francis Palma",
        "Monica Wachowicz",
        "Rickey Dubay",
        "Hung Cao"
      ],
      "abstract": "This paper proposes an optimization of an existing Deep Neural Network (DNN)\nthat improves its hardware utilization and facilitates on-device training for\nresource-constrained edge environments. We implement efficient parameter\nreduction strategies on Xception that shrink the model size without sacrificing\naccuracy, thus decreasing memory utilization during training. We evaluate our\nmodel in two experiments: Caltech-101 image classification and PCB defect\ndetection and compare its performance against the original Xception and\nlightweight models, EfficientNetV2B1 and MobileNetV2. The results of the\nCaltech-101 image classification show that our model has a better test accuracy\n(76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than\nXception (874.6MB), and has faster training and inference times. The\nlightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy\nand MobileNetV2 having a 58.11% test accuracy. Both lightweight models have\nbetter memory usage than our model and Xception. On the PCB defect detection,\nour model has the best test accuracy (90.30%), compared to Xception (88.10%),\nEfficientNetV2B1 (55.25%), and MobileNetV2 (50.50%). MobileNetV2 has the least\naverage memory usage (849.4MB), followed by our model (865.8MB), then\nEfficientNetV2B1 (874.8MB), and Xception has the highest (893.6MB). We further\nexperiment with pre-trained weights and observe that memory usage decreases\nthereby showing the benefits of transfer learning. A Pareto analysis of the\nmodels' performance shows that our optimized model architecture satisfies\naccuracy and low memory utilization objectives.",
      "tldr_zh": "本文提出了一种通过高效参数减少策略优化DNN的方法，旨在在资源受限的边缘环境中实现Pareto Optimality，同时提高硬件利用率和支持设备训练。该策略应用于Xception模型，在Caltech-101图像分类和PCB缺陷检测任务上进行评估，结果显示优化模型的准确率（分别为76.21%和90.30%）优于原Xception（75.89%和88.10%）和轻量模型EfficientNetV2B1、MobileNetV2，且内存使用更低（平均847.9MB）。实验还证明了预训练权重的益处，能进一步减少内存消耗；Pareto分析确认了优化模型在准确性和低内存利用率方面的平衡性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2401.05355",
      "pdf_url": "http://arxiv.org/pdf/2403.10569v1",
      "published_date": "2024-03-14 19:40:58 UTC",
      "updated_date": "2024-03-14 19:40:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:23:43.491501"
    },
    {
      "arxiv_id": "2403.09830v1",
      "title": "Towards the Reusability and Compositionality of Causal Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Talon",
        "Phillip Lippe",
        "Stuart James",
        "Alessio Del Bue",
        "Sara Magliacane"
      ],
      "abstract": "Causal Representation Learning (CRL) aims at identifying high-level causal\nfactors and their relationships from high-dimensional observations, e.g.,\nimages. While most CRL works focus on learning causal representations in a\nsingle environment, in this work we instead propose a first step towards\nlearning causal representations from temporal sequences of images that can be\nadapted in a new environment, or composed across multiple related environments.\nIn particular, we introduce DECAF, a framework that detects which causal\nfactors can be reused and which need to be adapted from previously learned\ncausal representations. Our approach is based on the availability of\nintervention targets, that indicate which variables are perturbed at each time\nstep. Experiments on three benchmark datasets show that integrating our\nframework with four state-of-the-art CRL approaches leads to accurate\nrepresentations in a new environment with only a few samples.",
      "tldr_zh": "该论文探讨了因果表示学习(Causal Representation Learning, CRL)，旨在从时间序列图像中学习可重用和可组合的因果表示，以适应新环境或多个相关环境。作者引入了DECAF框架，该框架基于干预目标(intervention targets)来检测哪些因果因素可重用、哪些需适应，从而从先前学得的表示中优化调整。实验在三个基准数据集上表明，将DECAF与四种最先进CRL方法整合后，仅需少量样本即可在新环境中获得准确的表示，提升了CRL的灵活性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the 3rd Conference on Causal Learning and Reasoning\n  (CLeaR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.09830v1",
      "published_date": "2024-03-14 19:36:07 UTC",
      "updated_date": "2024-03-14 19:36:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:23:52.449990"
    },
    {
      "arxiv_id": "2403.09810v1",
      "title": "LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems",
      "title_zh": "LabelAId：即时 AI 干预以提升众包系统中人类标注质量和领域知识",
      "authors": [
        "Chu Li",
        "Zhihan Zhang",
        "Michael Saugstad",
        "Esteban Safranchik",
        "Minchu Kulkarni",
        "Xiaoyu Huang",
        "Shwetak Patel",
        "Vikram Iyer",
        "Tim Althoff",
        "Jon E. Froehlich"
      ],
      "abstract": "Crowdsourcing platforms have transformed distributed problem-solving, yet\nquality control remains a persistent challenge. Traditional quality control\nmeasures, such as prescreening workers and refining instructions, often focus\nsolely on optimizing economic output. This paper explores just-in-time AI\ninterventions to enhance both labeling quality and domain-specific knowledge\namong crowdworkers. We introduce LabelAId, an advanced inference model\ncombining Programmatic Weak Supervision (PWS) with FT-Transformers to infer\nlabel correctness based on user behavior and domain knowledge. Our technical\nevaluation shows that our LabelAId pipeline consistently outperforms\nstate-of-the-art ML baselines, improving mistake inference accuracy by 36.7%\nwith 50 downstream samples. We then implemented LabelAId into Project Sidewalk,\nan open-source crowdsourcing platform for urban accessibility. A\nbetween-subjects study with 34 participants demonstrates that LabelAId\nsignificantly enhances label precision without compromising efficiency while\nalso increasing labeler confidence. We discuss LabelAId's success factors,\nlimitations, and its generalizability to other crowdsourced science domains.",
      "tldr_zh": "本研究针对众包平台的质量控制挑战，提出 LabelAId，这是一种即时 AI 干预系统，旨在提升标签质量和众包工作者的领域知识。LabelAId 结合 Programmatic Weak Supervision (PWS) 和 FT-Transformers，通过分析用户行为和领域知识来推断标签正确性，其技术评估显示比现有 ML 基线提高 36.7% 的错误推断准确率。在实际应用中，将 LabelAId 集成到 Project Sidewalk 平台后，用户研究（涉及 34 名参与者）证明它显著提高了标签精度、标签者信心，同时保持效率，并讨论了其成功因素、局限性和向其他众包领域的可推广性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09810v1",
      "published_date": "2024-03-14 18:59:10 UTC",
      "updated_date": "2024-03-14 18:59:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:24:06.065386"
    },
    {
      "arxiv_id": "2403.09809v1",
      "title": "Self-Supervised Learning for Time Series: Contrastive or Generative?",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyu Liu",
        "Azadeh Alavi",
        "Minyi Li",
        "Xiang Zhang"
      ],
      "abstract": "Self-supervised learning (SSL) has recently emerged as a powerful approach to\nlearning representations from large-scale unlabeled data, showing promising\nresults in time series analysis. The self-supervised representation learning\ncan be categorized into two mainstream: contrastive and generative. In this\npaper, we will present a comprehensive comparative study between contrastive\nand generative methods in time series. We first introduce the basic frameworks\nfor contrastive and generative SSL, respectively, and discuss how to obtain the\nsupervision signal that guides the model optimization. We then implement\nclassical algorithms (SimCLR vs. MAE) for each type and conduct a comparative\nanalysis in fair settings. Our results provide insights into the strengths and\nweaknesses of each approach and offer practical recommendations for choosing\nsuitable SSL methods. We also discuss the implications of our findings for the\nbroader field of representation learning and propose future research\ndirections. All the code and data are released at\n\\url{https://github.com/DL4mHealth/SSL_Comparison}.",
      "tldr_zh": "这篇论文比较了自监督学习 (SSL) 在时间序列分析中的两种主流方法：对比学习 (contrastive) 和生成学习 (generative)。作者首先介绍了每个方法的框架、监督信号获取方式，并实现了经典算法如 SimCLR 和 MAE，在公平设置下进行比较分析。研究结果揭示了对比学习在鲁棒性上占优，而生成学习在数据生成能力上更强，并提供了选择合适方法的实用建议。论文还讨论了这些发现对表示学习领域的启示，并提出未来研究方向，同时开源了所有代码和数据。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at the AI4TS Workshop, IJCAI 2023",
      "pdf_url": "http://arxiv.org/pdf/2403.09809v1",
      "published_date": "2024-03-14 18:58:06 UTC",
      "updated_date": "2024-03-14 18:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:24:17.045869"
    },
    {
      "arxiv_id": "2403.09806v1",
      "title": "xLP: Explainable Link Prediction for Master Data Management",
      "title_zh": "翻译失败",
      "authors": [
        "Balaji Ganesan",
        "Matheen Ahmed Pasha",
        "Srinivasa Parkala",
        "Neeraj R Singh",
        "Gayatri Mishra",
        "Sumit Bhatia",
        "Hima Patel",
        "Somashekar Naganna",
        "Sameep Mehta"
      ],
      "abstract": "Explaining neural model predictions to users requires creativity. Especially\nin enterprise applications, where there are costs associated with users' time,\nand their trust in the model predictions is critical for adoption. For link\nprediction in master data management, we have built a number of explainability\nsolutions drawing from research in interpretability, fact verification, path\nranking, neuro-symbolic reasoning and self-explaining AI. In this demo, we\npresent explanations for link prediction in a creative way, to allow users to\nchoose explanations they are more comfortable with.",
      "tldr_zh": "该论文提出xLP框架，用于主数据管理中的可解释链接预测（link prediction），旨在通过创意解释方法提升用户对模型预测的信任和采用，尤其在企业应用中考虑用户时间成本。研究借鉴interpretability、fact verification、path ranking、neuro-symbolic reasoning和self-explaining AI等技术，构建多种解释解决方案。演示展示了用户可选择的解释形式，帮助他们在不同情境下更舒适地理解预测结果，从而促进模型在实际应用中的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 4 figures, NeurIPS 2020 Competition and Demonstration Track.\n  arXiv admin note: text overlap with arXiv:2012.05516",
      "pdf_url": "http://arxiv.org/pdf/2403.09806v1",
      "published_date": "2024-03-14 18:53:44 UTC",
      "updated_date": "2024-03-14 18:53:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:24:27.793233"
    },
    {
      "arxiv_id": "2403.09795v1",
      "title": "Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention",
      "title_zh": "翻译失败",
      "authors": [
        "Ellie Prosser",
        "Matthew Edwards"
      ],
      "abstract": "Powerful generative Large Language Models (LLMs) are becoming popular tools\namongst the general public as question-answering systems, and are being\nutilised by vulnerable groups such as children. With children increasingly\ninteracting with these tools, it is imperative for researchers to scrutinise\nthe safety of LLMs, especially for applications that could lead to serious\noutcomes, such as online child safety queries. In this paper, the efficacy of\nLLMs for online grooming prevention is explored both for identifying and\navoiding grooming through advice generation, and the impact of prompt design on\nmodel performance is investigated by varying the provided context and prompt\nspecificity. In results reflecting over 6,000 LLM interactions, we find that no\nmodels were clearly appropriate for online grooming prevention, with an\nobserved lack of consistency in behaviours, and potential for harmful answer\ngeneration, especially from open-source models. We outline where and how models\nfall short, providing suggestions for improvement, and identify prompt designs\nthat heavily altered model performance in troubling ways, with findings that\ncan be used to inform best practice usage guides.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在预防在线诱导（online grooming）方面的有效性，包括通过生成建议来识别和避免诱导行为，并分析提示设计（prompt design）对模型性能的影响。研究通过超过6,000次LLMs互动的实验，发现这些模型表现不一致，没有任何模型适合用于在线诱导预防，且开源模型可能生成有害答案。作者指出了模型的不足之处，并提供了改进建议和最佳实践指南，以指导LLMs的安全应用。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09795v1",
      "published_date": "2024-03-14 18:27:43 UTC",
      "updated_date": "2024-03-14 18:27:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:24:40.301220"
    },
    {
      "arxiv_id": "2403.09793v3",
      "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Flögel",
        "Lars Fischer",
        "Thomas Rudolf",
        "Tobias Schürmann",
        "Sören Hohmann"
      ],
      "abstract": "Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.",
      "tldr_zh": "本研究探讨了移动机器人在拥挤环境中的导航问题，强调机器人需具备社会可接受的行为以提升人类接受度。作者分类了现有的 Deep Reinforcement Learning (DRL) 导航方法，包括缺乏社会行为的碰撞避免方法和具有预定义社会行为的意识方法，并提出一种新型社会整合导航方法，其中机器人的社会行为通过与人类互动自适应产生。基于社会学定义，该方法在 DRL 策略训练环境中让其他代理评估和奖励机器人的行为。模拟结果显示，该方法在机器人自身导航性能上优于社会意识方法，同时显著减少了对环境中所有代理的负面影响。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)",
      "pdf_url": "http://arxiv.org/pdf/2403.09793v3",
      "published_date": "2024-03-14 18:25:40 UTC",
      "updated_date": "2024-07-26 06:41:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:24:54.341038"
    },
    {
      "arxiv_id": "2403.09635v2",
      "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Akhil Kedia",
        "Mohd Abbas Zaidi",
        "Sushil Khyalia",
        "Jungho Jung",
        "Harshith Goka",
        "Haejun Lee"
      ],
      "abstract": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.",
      "tldr_zh": "本研究开发了统一的端到端信号传播理论，提供了公式来管理 transformer 模型中前向和后向信号的矩，从而解决梯度消失/爆炸、秩崩溃和高 attention scores 导致的不稳定性问题。作者提出 DeepScaleLM，这是一种初始化和缩放方案，能够保持模型中输出和梯度矩的稳定性，从而训练深度达 1000 层的模型。实验结果显示，这些更深的 transformer 模型（参数更少）在 Language Modeling、Speech Translation 和 Image Classification 等任务上优于浅层模型，适用于 Pre-LN 和 Post-LN 变体，并在下游 Question Answering 任务和图像分类的鲁棒性方面表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "I.2.7; I.2.10"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICML 2024. Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable. Akhil Kedia, Mohd Abbas\n  Zaidi, Sushil Khyalia equal contribution",
      "pdf_url": "http://arxiv.org/pdf/2403.09635v2",
      "published_date": "2024-03-14 17:59:14 UTC",
      "updated_date": "2024-07-18 17:59:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:25:05.799838"
    },
    {
      "arxiv_id": "2403.09631v1",
      "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
      "title_zh": "3D-VLA：3D",
      "authors": [
        "Haoyu Zhen",
        "Xiaowen Qiu",
        "Peihao Chen",
        "Jincheng Yang",
        "Xin Yan",
        "Yilun Du",
        "Yining Hong",
        "Chuang Gan"
      ],
      "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
      "tldr_zh": "该论文提出 3D-VLA，一种基于生成式世界模型的 embodied foundation models，旨在解决现有 vision-language-action (VLA) 模型依赖 2D 输入并忽略 3D 物理世界动态的问题。3D-VLA 构建于 3D-based large language model (LLM) 之上，通过引入 interaction tokens 和 embodied diffusion models，实现 3D 感知、推理和动作的无缝整合，并用于预测目标图像和点云。为训练模型，研究者构建了一个大型 3D embodied instruction dataset，从现有机器人数据集提取 3D 相关信息。实验结果显示，3D-VLA 在 embodied 环境中显著提升了推理、多模态生成和规划能力，具有广阔的真实世界应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://vis-www.cs.umass.edu/3dvla/",
      "pdf_url": "http://arxiv.org/pdf/2403.09631v1",
      "published_date": "2024-03-14 17:58:41 UTC",
      "updated_date": "2024-03-14 17:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:25:17.686863"
    },
    {
      "arxiv_id": "2403.09629v2",
      "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
      "title_zh": "翻译失败",
      "authors": [
        "Eric Zelikman",
        "Georges Harik",
        "Yijia Shao",
        "Varuna Jayasiri",
        "Nick Haber",
        "Noah D. Goodman"
      ],
      "abstract": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
      "tldr_zh": "本研究提出Quiet-STaR，一种让语言模型(LMs)自我学习在生成文本前生成推理(rationales)的框架，从而提升预测准确性。Quiet-STaR扩展了STaR方法，通过在任意文本中推断未明示的推理步骤，采用tokenwise parallel sampling算法、learnable tokens来标记思考的开始和结束，以及扩展的teacher-forcing技术，以解决计算成本和模型学习挑战。实验结果显示，经过在互联网文本上继续预训练后，Quiet-STaR在GSM8K上的零样本性能从5.9%提高到10.9%，在CommonsenseQA上从36.3%提高到47.2%，并显著降低了自然文本中难预测token的perplexity。这些改进无需任务特定微调，标志着LMs向更通用和可扩展的推理学习迈出重要一步。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09629v2",
      "published_date": "2024-03-14 17:58:16 UTC",
      "updated_date": "2024-03-18 07:56:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:25:30.312199"
    },
    {
      "arxiv_id": "2403.09621v2",
      "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
      "title_zh": "针对分布鲁棒离线强化学习的最小最大最优且计算高效算法",
      "authors": [
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a novel function approximation\nmechanism incorporating variance information, a new procedure of suboptimality\nand estimation uncertainty decomposition, a quantification of the robust value\nfunction shrinkage, and a meticulously designed family of hard instances, which\nmight be of independent interest.",
      "tldr_zh": "该论文针对分布鲁棒离线强化学习（Distributionally Robust Offline Reinforcement Learning），提出最小最大最优（minimax optimal）和计算高效的算法，以应对动态不确定性带来的非线性和计算挑战。算法基于线性参数化的名义模型和扰动模型，引入一种新颖的函数逼近机制（incorporating variance information），并通过次优性和估计不确定性分解以及鲁棒价值函数收缩的量化来实现实例依赖的次优性分析。研究发现，函数逼近在鲁棒离线强化学习中比标准离线强化学习更具挑战性，并设计了一系列困难实例来验证算法的有效性，这些贡献为该领域提供了重要理论基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "46 pages, 3 figures, 1 table. Published in Proc. of the 38th\n  Conference on Advances in Neural Information Processing Systems (NeurIPS\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.09621v2",
      "published_date": "2024-03-14 17:55:10 UTC",
      "updated_date": "2025-03-03 22:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:25:42.518587"
    },
    {
      "arxiv_id": "2403.09606v3",
      "title": "Large Language Models and Causal Inference in Collaboration: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Liu",
        "Paiheng Xu",
        "Junda Wu",
        "Jiaxin Yuan",
        "Yifan Yang",
        "Yuhang Zhou",
        "Fuxiao Liu",
        "Tianrui Guan",
        "Haoliang Wang",
        "Tong Yu",
        "Julian McAuley",
        "Wei Ai",
        "Furong Huang"
      ],
      "abstract": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
      "tldr_zh": "这篇调查探讨了因果推理(Causal Inference)与大语言模型(LLMs)的协作关系，重点评估因果框架如何提升 LLMs 在自然语言处理(NLP)中的预测准确性、公平性、鲁棒性和可解释性。\n论文从因果视角分析了 LLMs 的改进，包括增强推理能力、解决公平和安全问题、提供解释以及处理多模态数据。\n同时，LLMs 的强大推理能力可反哺因果推理领域，帮助发现因果关系和估计效应。\n总体上，这两者互动有望推动更先进和公平的 AI 系统发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Findings of the Association for Computational Linguistics: NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.09606v3",
      "published_date": "2024-03-14 17:47:20 UTC",
      "updated_date": "2025-03-21 04:57:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:25:53.867152"
    },
    {
      "arxiv_id": "2403.10568v3",
      "title": "MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Ruixiang Jiang",
        "Lingbo Liu",
        "Changwen Chen"
      ],
      "abstract": "Despite the demonstrated parameter efficiency of prompt-based multimodal\nfusion methods, their limited adaptivity and expressiveness often result in\nsuboptimal performance compared to other tuning approaches. In this paper, we\nintroduce the Mixture of Prompt Experts (MoPE), the first technique designed to\novercome these limitations by decomposing standard prompts to capture\ninstance-level features adaptively. Building on this decomposition, MoPE\nenhances prompt fusion's expressiveness by leveraging multimodal pairing priors\nto route the most effective prompt for each instance dynamically. Compared to\nvanilla prompting, our MoPE-based fusion method exhibits greater\nexpressiveness, scaling more effectively with the training data and the overall\nnumber of trainable parameters. We also investigate regularization terms for\nexpert routing, which lead to emergent expert specialization with enhanced\nadaptiveness and interpretablity. Extensive experiments across six multimodal\ndatasets spanning four modalities demonstrate state-of-the-art performance for\nprompt fusion, matching or even surpassing the performance of fine-tuning while\nrequiring only 0.8% of the trainable parameters. Project homepage:\nhttps://github.com/songrise/MoPE",
      "tldr_zh": "该论文提出 Mixture of Prompt Experts (MoPE)，一种参数高效且可扩展的多模态融合方法，通过分解标准提示并动态路由来适应性地捕获实例级特征，提升了提示融合的表现力和可扩展性。MoPE 利用多模态配对先验和专家路由正则化术语，促进专家专业化，提高系统适应性和可解释性。在六个跨四种模态的数据集上进行广泛实验，MoPE 实现了最先进性能，与微调方法相当或优于之，但仅需 0.8% 的可训练参数。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review, Extended version of arxiv:2312.03734",
      "pdf_url": "http://arxiv.org/pdf/2403.10568v3",
      "published_date": "2024-03-14 17:47:10 UTC",
      "updated_date": "2025-01-14 08:01:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:26:05.738900"
    },
    {
      "arxiv_id": "2403.09605v2",
      "title": "Counterfactual contrastive learning: robust representations via causal image synthesis",
      "title_zh": "反事实对比学习：通过因果图像合成实现鲁棒表示",
      "authors": [
        "Melanie Roschewitz",
        "Fabio De Sousa Ribeiro",
        "Tian Xia",
        "Galvin Khara",
        "Ben Glocker"
      ],
      "abstract": "Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.",
      "tldr_zh": "本研究探讨了对比预训练（Contrastive pretraining）在下游任务性能和模型泛化方面的优势，但强调其对增强管道的敏感性。作者提出 CF-SimCLR，一种反事实对比学习（Counterfactual contrastive learning）方法，通过利用反事实图像生成（causal image synthesis）模拟真实领域变化来创建正对对，从而更好地保留语义信息。实验在五个数据集（包括胸部X光和乳腺X光）上表明，CF-SimCLR 显著提升了对采集移位（acquisition shift）的鲁棒性，尤其在训练数据中 underrepresented 的领域，实现更高的下游性能，包括分布内和分布外（out-of-distribution）数据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at the MICCAI 2024 Data Engineering in\n  Medical Imaging workshop. Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive. Extended version\n  of this work available at arXiv:2409.10365",
      "pdf_url": "http://arxiv.org/pdf/2403.09605v2",
      "published_date": "2024-03-14 17:47:01 UTC",
      "updated_date": "2024-09-17 11:25:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:26:17.616337"
    },
    {
      "arxiv_id": "2403.09603v3",
      "title": "Optimistic Verifiable Training by Controlling Hardware Nondeterminism",
      "title_zh": "翻译失败",
      "authors": [
        "Megha Srivastava",
        "Simran Arora",
        "Dan Boneh"
      ],
      "abstract": "The increasing compute demands of AI systems have led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning and backdoors, poses challenges.\nExisting works on verifiable training largely fall into two classes:\nproof-based systems, which are difficult to scale, and ``optimistic'' methods\nthat consider a third-party auditor who can replicate the training process and\ncontest the trainer. A key challenge with the latter is that nondeterminism\nbetween GPU types during training prevents exact replication of the training\nprocess, resulting in schemes that are non-robust. We propose a method that\ncombines training in a higher precision than the target, rounding after\nintermediate computations, and sharing rounding decisions based on an adaptive\nthresholding procedure, to successfully control for nondeterminism. Across\nthree different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact\ntraining replication at FP32 precision for both full-training and fine-tuning\nof ResNet-50 (23M) and GPT-2 (117M) models. Our verifiable training scheme\nsignificantly decreases the storage and time costs compared to proof-based\nsystems, and is publicly released at\nhttps://github.com/meghabyte/verifiable-training.",
      "tldr_zh": "该研究针对AI模型训练的可验证性问题，提出了一种optimistic verifiable training方法，通过在更高精度下训练、在中间计算后进行rounding以及基于adaptive thresholding的共享决策，来有效控制硬件nondeterminism的影响。实验结果显示，该方法在A40、Titan XP和RTX 2080 Ti等NVIDIA GPU上实现了ResNet-50（23M参数）和GPT-2（117M参数）模型在FP32精度的精确训练复制。相比传统的proof-based系统，该方案显著降低了存储和时间成本，并开源了实现代码。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 5 figures, Neural Information Processing Systems (NeurIPS)\n  2024,",
      "pdf_url": "http://arxiv.org/pdf/2403.09603v3",
      "published_date": "2024-03-14 17:44:35 UTC",
      "updated_date": "2024-11-25 09:13:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:26:30.055132"
    },
    {
      "arxiv_id": "2403.09580v2",
      "title": "Algorithmic syntactic causal identification",
      "title_zh": "算法句法因果识别",
      "authors": [
        "Dhurim Cakiqi",
        "Max A. Little"
      ],
      "abstract": "Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model.",
      "tldr_zh": "本论文解决了因果贝叶斯网(CBNs)中因果识别的局限性，即现有方法依赖经典概率理论（如d-separation和do-calculus），无法应用于非概率场景，如关系数据库、数据流程序和机器学习算法。作者提出使用对称单向范畴(symmetric monoidal categories)作为新基础，实现了因果模型的通用语法与具体语义的清晰区分。论文提供了一个纯语法算法描述，通过翻译ID算法的“fixing”机制，对非参数化ADMG结构和对应范畴进行操作，得到修改后的干预因果模型。作为贡献，他们导出了back-door和front-door因果调整的纯语法模拟，并展示了其在复杂因果模型中的应用，从而扩展了因果识别的适用范围。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 2 TikZ figures",
      "pdf_url": "http://arxiv.org/pdf/2403.09580v2",
      "published_date": "2024-03-14 17:14:53 UTC",
      "updated_date": "2025-01-29 14:41:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:26:42.426559"
    },
    {
      "arxiv_id": "2403.09567v3",
      "title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Laura Fernández-Becerra",
        "Miguel Ángel González-Santamarta",
        "Ángel Manuel Guerrero-Higueras",
        "Francisco Javier Rodríguez-Lera",
        "Vicente Matellán Olivera"
      ],
      "abstract": "The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.",
      "tldr_zh": "该研究针对自主代理（autonomous agents）在人类互动环境中的安全问题，提出了一种基于区块链（Blockchain）和大型语言模型（LLMs）的架构，以提升责任（accountability）和可解释性（explainability）。架构包括一个防篡改的黑匣子组件，用于记录代理行为，以及一个利用LLMs生成自然语言解释的组件，从而帮助非专家用户理解代理决策。实验在三个ROS-based移动机器人导航场景中进行，结果显示该方法能产生连贯、准确且易懂的解释，显著增强了代理的信任度和安全性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09567v3",
      "published_date": "2024-03-14 16:57:18 UTC",
      "updated_date": "2024-12-19 19:08:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:26:55.611993"
    },
    {
      "arxiv_id": "2403.09565v1",
      "title": "Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Nouri",
        "Beatriz Cabrero-Daniel",
        "Fredrik Törner",
        "Hȧkan Sivencrona",
        "Christian Berger"
      ],
      "abstract": "DevOps is a necessity in many industries, including the development of\nAutonomous Vehicles. In those settings, there are iterative activities that\nreduce the speed of SafetyOps cycles. One of these activities is \"Hazard\nAnalysis & Risk Assessment\" (HARA), which is an essential step to start the\nsafety requirements specification. As a potential approach to increase the\nspeed of this step in SafetyOps, we have delved into the capabilities of Large\nLanguage Models (LLMs).\n  Our objective is to systematically assess their potential for application in\nthe field of safety engineering. To that end, we propose a framework to support\na higher degree of automation of HARA with LLMs. Despite our endeavors to\nautomate as much of the process as possible, expert review remains crucial to\nensure the validity and correctness of the analysis results, with necessary\nmodifications made accordingly.",
      "tldr_zh": "本研究探讨了在DevOps和Autonomous Vehicles领域，使用Large Language Models (LLMs)来加速Hazard Analysis & Risk Assessment (HARA)过程，以提升SafetyOps的效率。作者提出一个框架，支持HARA的更高自动化度，通过LLMs辅助分析潜在风险和安全要求。尽管自动化进程已最大化，但专家审查仍是确保结果准确性和正确性的关键步骤。总的来说，此框架为LLMs在安全工程领域的应用提供了系统评估方法。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted in CAIN 2024, 6 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2403.09565v1",
      "published_date": "2024-03-14 16:56:52 UTC",
      "updated_date": "2024-03-14 16:56:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:27:07.252969"
    },
    {
      "arxiv_id": "2403.10566v1",
      "title": "Cooling-Guide Diffusion Model for Battery Cell Arrangement",
      "title_zh": "翻译失败",
      "authors": [
        "Nicholas Sung",
        "Liu Zheng",
        "Pingfeng Wang",
        "Faez Ahmed"
      ],
      "abstract": "Our study introduces a Generative AI method that employs a cooling-guided\ndiffusion model to optimize the layout of battery cells, a crucial step for\nenhancing the cooling performance and efficiency of battery thermal management\nsystems. Traditional design processes, which rely heavily on iterative\noptimization and extensive guesswork, are notoriously slow and inefficient,\noften leading to suboptimal solutions. In contrast, our innovative method uses\na parametric denoising diffusion probabilistic model (DDPM) with classifier and\ncooling guidance to generate optimized cell layouts with enhanced cooling\npaths, significantly lowering the maximum temperature of the cells. By\nincorporating position-based classifier guidance, we ensure the feasibility of\ngenerated layouts. Meanwhile, cooling guidance directly optimizes\ncooling-efficiency, making our approach uniquely effective. When compared to\ntwo advanced models, the Tabular Denoising Diffusion Probabilistic Model\n(TabDDPM) and the Conditional Tabular GAN (CTGAN), our cooling-guided diffusion\nmodel notably outperforms both. It is five times more effective than TabDDPM\nand sixty-six times better than CTGAN across key metrics such as feasibility,\ndiversity, and cooling efficiency. This research marks a significant leap\nforward in the field, aiming to optimize battery cell layouts for superior\ncooling efficiency, thus setting the stage for the development of more\neffective and dependable battery thermal management systems.",
      "tldr_zh": "该研究提出了一种基于生成式 AI 的冷却引导扩散模型，用于优化电池单元布局，从而提升电池热管理系统的冷却性能和效率。传统方法依赖迭代优化和猜测，效率低下，而该模型采用参数化去噪扩散概率模型 (DDPM)，结合分类器引导确保布局可行性，以及冷却引导直接优化冷却路径，显著降低电池单元的最大温度。与 TabDDPM 和 CTGAN 相比，该模型在可行性、多样性和冷却效率上表现出色，是 TabDDPM 的 5 倍和 CTGAN 的 66 倍有效。该创新为开发更可靠的电池热管理系统奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.10566v1",
      "published_date": "2024-03-14 16:51:51 UTC",
      "updated_date": "2024-03-14 16:51:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:27:20.837343"
    },
    {
      "arxiv_id": "2403.09549v3",
      "title": "Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields",
      "title_zh": "翻译失败",
      "authors": [
        "Yi-Lun Liao",
        "Tess Smidt",
        "Muhammed Shuaibi",
        "Abhishek Das"
      ],
      "abstract": "Understanding the interactions of atoms such as forces in 3D atomistic\nsystems is fundamental to many applications like molecular dynamics and\ncatalyst design. However, simulating these interactions requires\ncompute-intensive ab initio calculations and thus results in limited data for\ntraining neural networks. In this paper, we propose to use denoising\nnon-equilibrium structures (DeNS) as an auxiliary task to better leverage\ntraining data and improve performance. For training with DeNS, we first corrupt\na 3D structure by adding noise to its 3D coordinates and then predict the\nnoise. Different from previous works on denoising, which are limited to\nequilibrium structures, the proposed method generalizes denoising to a much\nlarger set of non-equilibrium structures. The main difference is that a\nnon-equilibrium structure does not correspond to local energy minima and has\nnon-zero forces, and therefore it can have many possible atomic positions\ncompared to an equilibrium structure. This makes denoising non-equilibrium\nstructures an ill-posed problem since the target of denoising is not uniquely\ndefined. Our key insight is to additionally encode the forces of the original\nnon-equilibrium structure to specify which non-equilibrium structure we are\ndenoising. Concretely, given a corrupted non-equilibrium structure and the\nforces of the original one, we predict the non-equilibrium structure satisfying\nthe input forces instead of any arbitrary structures. Since DeNS requires\nencoding forces, DeNS favors equivariant networks, which can easily incorporate\nforces and other higher-order tensors in node embeddings. We study the\neffectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17\ndatasets and demonstrate that DeNS can achieve new state-of-the-art results on\nOC20 and OC22 and significantly improve training efficiency on MD17.",
      "tldr_zh": "本论文提出了一种名为Denoising Non-Equilibrium Structures (DeNS)的方法，作为辅助任务来提升equivariant force fields在3D原子系统中的性能，解决了模拟原子互动（如分子动力学和催化剂设计）时数据有限的问题。不同于传统去噪仅限于equilibrium structures，DeNS将去噪泛化到non-equilibrium structures，通过添加噪声到3D坐标并预测噪声，同时编码原结构的forces来解决目标不唯一的问题，从而精确预测满足输入forces的非平衡结构。该方法偏好equivariant networks，因为它们能轻松整合forces和其他高阶张量；实验在OC20、OC22和MD17数据集上证明，DeNS实现了OC20和OC22的新state-of-the-art结果，并在MD17上显著提高了训练效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research (TMLR)",
      "pdf_url": "http://arxiv.org/pdf/2403.09549v3",
      "published_date": "2024-03-14 16:38:02 UTC",
      "updated_date": "2024-12-19 19:29:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:27:32.574720"
    },
    {
      "arxiv_id": "2403.09539v3",
      "title": "Logits of API-Protected LLMs Leak Proprietary Information",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew Finlayson",
        "Xiang Ren",
        "Swabha Swayamdipta"
      ],
      "abstract": "Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability.",
      "tldr_zh": "这篇论文揭示了API保护的大型语言模型(LLMs)通过logits泄露专有信息的风险，仅需少量API查询（如花费不到1000美元），研究者就能提取大量非公开数据。作者利用现代LLMs的softmax bottleneck特性，将模型输出限制在线性子空间，并开发方法实现获取全词汇输出、审计模型更新、识别LLM来源以及发现隐藏模型大小等功能。实验结果显示，该方法高度有效，例如准确估计OpenAI的gpt-3.5-turbo嵌入大小约为4096；论文还讨论了防范这些攻击的策略，并强调这些能力可提升模型的透明度和问责制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09539v3",
      "published_date": "2024-03-14 16:27:49 UTC",
      "updated_date": "2024-11-08 18:56:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:27:44.921889"
    },
    {
      "arxiv_id": "2403.13839v1",
      "title": "depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers",
      "title_zh": "翻译失败",
      "authors": [
        "Kaichao You",
        "Runsheng Bai",
        "Meng Cao",
        "Jianmin Wang",
        "Ion Stoica",
        "Mingsheng Long"
      ],
      "abstract": "PyTorch \\texttt{2.x} introduces a compiler designed to accelerate deep\nlearning programs. However, for machine learning researchers, adapting to the\nPyTorch compiler to full potential can be challenging. The compiler operates at\nthe Python bytecode level, making it appear as an opaque box. To address this,\nwe introduce \\texttt{depyf}, a tool designed to demystify the inner workings of\nthe PyTorch compiler. \\texttt{depyf} decompiles bytecode generated by PyTorch\nback into equivalent source code, and establishes connections between in-memory\ncode objects and their on-disk source code counterparts. This feature enables\nusers to step through the source code line by line using debuggers, thus\nenhancing their understanding of the underlying processes. Notably,\n\\texttt{depyf} is non-intrusive and user-friendly, primarily relying on two\nconvenient context managers for its core functionality. The project is\n\\href{https://github.com/thuml/depyf}{ openly available} and is recognized as a\n\\href{https://pytorch.org/ecosystem/}{PyTorch ecosystem project}.",
      "tldr_zh": "PyTorch 2.x 的编译器旨在加速深度学习程序，但对机器学习研究者来说，它在 Python bytecode 级别运作，显得像一个不透明的黑盒子。作者引入了 depyf 工具，该工具可以将 PyTorch 生成的 bytecode 反编译回等价的源代码，并建立内存中代码对象与磁盘上源代码的连接。depyf 允许用户使用调试器逐行调试源代码，从而提升对编译器内部过程的理解。该工具是非侵入性的、用户友好的，主要依赖两个上下文管理器，并作为开源项目在 PyTorch 生态系统中公开可用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.13839v1",
      "published_date": "2024-03-14 16:17:14 UTC",
      "updated_date": "2024-03-14 16:17:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:27:56.426188"
    },
    {
      "arxiv_id": "2403.09530v2",
      "title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Chris Kelly",
        "Luhui Hu",
        "Jiayin Hu",
        "Yu Tian",
        "Deshun Yang",
        "Bang Yang",
        "Cindy Yang",
        "Zihao Li",
        "Zaoshan Huang",
        "Yuexian Zou"
      ],
      "abstract": "The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent",
      "tldr_zh": "本文提出 VisionGPT-3D，一种通用的多模态代理，用于增强 3D 视觉理解，通过整合各种 SOTA 视觉模型和算法，自动选择合适模型并处理从 2D 图像到 3D 表示的转换。框架支持多模态输入如文本提示，实现从自然语言到视觉对象的无缝生成和优化，解决了算法与问题不匹配导致的低效问题。实验表明，VisionGPT-3D 能基于 LLMs 和 CV 技术，提供更精确的 3D 视觉理解，促进视觉导向 AI 的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 7 figures, pending conference",
      "pdf_url": "http://arxiv.org/pdf/2403.09530v2",
      "published_date": "2024-03-14 16:13:00 UTC",
      "updated_date": "2024-03-22 15:26:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:28:09.613313"
    },
    {
      "arxiv_id": "2403.09762v1",
      "title": "Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts",
      "title_zh": "通过人工智能实现的情感智力：NLP 和深度学习在医疗文本分析中的应用",
      "authors": [
        "Prashant Kumar Nag",
        "Amit Bhagat",
        "R. Vishnu Priya",
        "Deepak kumar Khare"
      ],
      "abstract": "This manuscript presents a methodical examination of the utilization of\nArtificial Intelligence in the assessment of emotions in texts related to\nhealthcare, with a particular focus on the incorporation of Natural Language\nProcessing and deep learning technologies. We scrutinize numerous research\nstudies that employ AI to augment sentiment analysis, categorize emotions, and\nforecast patient outcomes based on textual information derived from clinical\nnarratives, patient feedback on medications, and online health discussions. The\nreview demonstrates noteworthy progress in the precision of algorithms used for\nsentiment classification, the prognostic capabilities of AI models for\nneurodegenerative diseases, and the creation of AI-powered systems that offer\nsupport in clinical decision-making. Remarkably, the utilization of AI\napplications has exhibited an enhancement in personalized therapy plans by\nintegrating patient sentiment and contributing to the early identification of\nmental health disorders. There persist challenges, which encompass ensuring the\nethical application of AI, safeguarding patient confidentiality, and addressing\npotential biases in algorithmic procedures. Nevertheless, the potential of AI\nto revolutionize healthcare practices is unmistakable, offering a future where\nhealthcare is not only more knowledgeable and efficient but also more\nempathetic and centered around the needs of patients. This investigation\nunderscores the transformative influence of AI on healthcare, delivering a\ncomprehensive comprehension of its role in examining emotional content in\nhealthcare texts and highlighting the trajectory towards a more compassionate\napproach to patient care. The findings advocate for a harmonious synergy\nbetween AI's analytical capabilities and the human aspects of healthcare.",
      "tldr_zh": "这篇论文系统审视了使用人工智能（AI）、自然语言处理（NLP）和深度学习技术来分析医疗文本中情感的方法，特别是针对临床叙述、患者反馈和在线健康讨论。研究回顾了AI在情感分析、情绪分类和预测患者结果（如神经退行性疾病）方面的进展，展示了这些算法的精确性提升以及在临床决策和个性化治疗中的应用潜力。AI还有助于早期识别心理健康问题，但面临伦理挑战、患者隐私保护和算法偏见问题。总体而言，论文强调AI与人类护理的协同作用，有望推动更具同情心的医疗实践。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09762v1",
      "published_date": "2024-03-14 15:58:13 UTC",
      "updated_date": "2024-03-14 15:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:28:21.388203"
    },
    {
      "arxiv_id": "2403.09513v1",
      "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Wang",
        "Xiaogeng Liu",
        "Yu Li",
        "Muhao Chen",
        "Chaowei Xiao"
      ],
      "abstract": "With the advent and widespread deployment of Multimodal Large Language Models\n(MLLMs), the imperative to ensure their safety has become increasingly\npronounced. However, with the integration of additional modalities, MLLMs are\nexposed to new vulnerabilities, rendering them prone to structured-based\njailbreak attacks, where semantic content (e.g., \"harmful text\") has been\ninjected into the images to mislead MLLMs. In this work, we aim to defend\nagainst such threats. Specifically, we propose \\textbf{Ada}ptive\n\\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with\ndefense prompts to defend MLLMs against structure-based jailbreak attacks\nwithout fine-tuning MLLMs or training additional modules (e.g., post-stage\ncontent detector). Initially, we present a manually designed static defense\nprompt, which thoroughly examines the image and instruction content step by\nstep and specifies response methods to malicious queries. Furthermore, we\nintroduce an adaptive auto-refinement framework, consisting of a target MLLM\nand a LLM-based defense prompt generator (Defender). These components\ncollaboratively and iteratively communicate to generate a defense prompt.\nExtensive experiments on the popular structure-based jailbreak attacks and\nbenign datasets show that our methods can consistently improve MLLMs'\nrobustness against structure-based jailbreak attacks without compromising the\nmodel's general capabilities evaluated on standard benign tasks. Our code is\navailable at https://github.com/rain305f/AdaShield.",
      "tldr_zh": "该研究针对Multimodal Large Language Models (MLLMs)面临的structure-based jailbreak attacks（如图像中注入有害内容）问题，提出了AdaShield方法，通过adaptive shield prompting在输入前添加防御提示来提升模型安全性，而无需fine-tuning MLLMs或额外训练模块。AdaShield包括一个手动设计的静态防御提示，用于逐步检查图像和指令内容并指定响应恶意查询的策略，以及一个adaptive auto-refinement框架，该框架利用目标MLLM和LLM-based defense prompt generator协作迭代生成优化提示。实验结果显示，该方法显著提高了MLLMs对structure-based jailbreak attacks的鲁棒性，同时在标准benign任务上保持了模型的一般性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Multimodal Large Language Models Defense, 25 Pages",
      "pdf_url": "http://arxiv.org/pdf/2403.09513v1",
      "published_date": "2024-03-14 15:57:13 UTC",
      "updated_date": "2024-03-14 15:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:28:31.269731"
    },
    {
      "arxiv_id": "2403.09510v1",
      "title": "Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation",
      "title_zh": "信任 AI 监管",
      "authors": [
        "Zainab Alalawi",
        "Paolo Bova",
        "Theodor Cimpeanu",
        "Alessandro Di Stefano",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "The Anh Han",
        "Marcus Krellner",
        "Bianca Ogbo",
        "Simon T. Powers",
        "Filippo Zimmaro"
      ],
      "abstract": "There is general agreement that some form of regulation is necessary both for\nAI creators to be incentivised to develop trustworthy systems, and for users to\nactually trust those systems. But there is much debate about what form these\nregulations should take and how they should be implemented. Most work in this\narea has been qualitative, and has not been able to make formal predictions.\nHere, we propose that evolutionary game theory can be used to quantitatively\nmodel the dilemmas faced by users, AI creators, and regulators, and provide\ninsights into the possible effects of different regulatory regimes. We show\nthat creating trustworthy AI and user trust requires regulators to be\nincentivised to regulate effectively. We demonstrate the effectiveness of two\nmechanisms that can achieve this. The first is where governments can recognise\nand reward regulators that do a good job. In that case, if the AI system is not\ntoo risky for users then some level of trustworthy development and user trust\nevolves. We then consider an alternative solution, where users can condition\ntheir trust decision on the effectiveness of the regulators. This leads to\neffective regulation, and consequently the development of trustworthy AI and\nuser trust, provided that the cost of implementing regulations is not too high.\nOur findings highlight the importance of considering the effect of different\nregulatory regimes from an evolutionary game theoretic perspective.",
      "tldr_zh": "该研究使用演化博弈理论(evolutionary game theory)来量化建模AI监管中的困境，探讨用户、AI创建者和监管者之间的互动，以预测不同监管制度的效果。论文发现，监管者需要适当激励才能有效推动可信AI的发展和用户信任，并提出两种机制：政府奖励优秀监管者，以及用户根据监管有效性决定信任决策。结果表明，如果AI风险不高或监管成本适中，这些机制能促进可信AI的演化，并强调从演化博弈理论视角评估监管制度的重要性。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.GT",
        "cs.MA",
        "math.DS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09510v1",
      "published_date": "2024-03-14 15:56:39 UTC",
      "updated_date": "2024-03-14 15:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:28:42.952799"
    },
    {
      "arxiv_id": "2403.09506v2",
      "title": "Don't Judge by the Look: Towards Motion Coherent Video Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Yitian Zhang",
        "Yue Bai",
        "Huan Wang",
        "Yizhou Wang",
        "Yun Fu"
      ],
      "abstract": "Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video understanding and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video understanding, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.",
      "tldr_zh": "本研究发现，在视频理解任务中，色调抖动(Hue Jittering)虽然对静态对象识别有害，但对包含运动信息的视频却有益，因为它能帮助模型优先关注动态模式而非静态外观。作者提出Motion Coherent Augmentation (MCA)数据增强方法，包括SwapMix操作（用于高效修改视频样本外观）和Variation Alignment (VA)机制（解决分布偏移，确保学习外观不变表示）。实验结果显示，MCA在多种架构和数据集上显著提升了模型性能，并证明了VA在其他增强方法中的适用性。代码已在GitHub上开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09506v2",
      "published_date": "2024-03-14 15:53:04 UTC",
      "updated_date": "2024-03-25 02:45:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:28:55.765037"
    },
    {
      "arxiv_id": "2403.09502v2",
      "title": "EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning",
      "title_zh": "EquiAV：利用等变性进行音频-视觉对比学习",
      "authors": [
        "Jongsuk Kim",
        "Hyeongkeun Lee",
        "Kyeongha Rho",
        "Junmo Kim",
        "Joon Son Chung"
      ],
      "abstract": "Recent advancements in self-supervised audio-visual representation learning\nhave demonstrated its potential to capture rich and comprehensive\nrepresentations. However, despite the advantages of data augmentation verified\nin many learning methods, audio-visual learning has struggled to fully harness\nthese benefits, as augmentations can easily disrupt the correspondence between\ninput pairs. To address this limitation, we introduce EquiAV, a novel framework\nthat leverages equivariance for audio-visual contrastive learning. Our approach\nbegins with extending equivariance to audio-visual learning, facilitated by a\nshared attention-based transformation predictor. It enables the aggregation of\nfeatures from diverse augmentations into a representative embedding, providing\nrobust supervision. Notably, this is achieved with minimal computational\noverhead. Extensive ablation studies and qualitative results verify the\neffectiveness of our method. EquiAV outperforms previous works across various\naudio-visual benchmarks. The code is available on\nhttps://github.com/JongSuk1/EquiAV.",
      "tldr_zh": "这篇论文介绍了 EquiAV，一种新型框架，通过利用 equivariance（等变性）来提升音频-视觉对比学习的效果，解决数据增强可能破坏输入对对应性的问题。EquiAV 采用共享的注意力-based 转换预测器，将不同增强的特征聚合到一个稳健的代表性嵌入中，提供高效的监督，且计算开销最小。实验结果显示，该方法在各种音频-视觉基准上超越了先前工作，并通过消融研究和定性分析验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 3 figures; Accepted to ICML 2024 (camera ready version)",
      "pdf_url": "http://arxiv.org/pdf/2403.09502v2",
      "published_date": "2024-03-14 15:44:19 UTC",
      "updated_date": "2024-06-20 06:23:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:29:08.064606"
    },
    {
      "arxiv_id": "2403.15432v1",
      "title": "BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Jinhui Ouyang",
        "Mingzhu Wu",
        "Xinglin Li",
        "Hanhui Deng",
        "Di Wu"
      ],
      "abstract": "Recent advances in EEG-based BCI technologies have revealed the potential of\nbrain-to-robot collaboration through the integration of sensing, computing,\ncommunication, and control. In this paper, we present BRIEDGE as an end-to-end\nsystem for multi-brain to multi-robot interaction through an EEG-adaptive\nneural network and an encoding-decoding communication framework, as illustrated\nin Fig.1. As depicted, the edge mobile server or edge portable server will\ncollect EEG data from the users and utilize the EEG-adaptive neural network to\nidentify the users' intentions. The encoding-decoding communication framework\nthen encodes the EEG-based semantic information and decodes it into commands in\nthe process of data transmission. To better extract the joint features of\nheterogeneous EEG data as well as enhance classification accuracy, BRIEDGE\nintroduces an informer-based ProbSparse self-attention mechanism. Meanwhile,\nparallel and secure transmissions for multi-user multi-task scenarios under\nphysical channels are addressed by dynamic autoencoder and autodecoder\ncommunications. From mobile computing and edge AI perspectives, model\ncompression schemes composed of pruning, weight sharing, and quantization are\nalso used to deploy lightweight EEG-adaptive models running on both transmitter\nand receiver sides. Based on the effectiveness of these components, a code map\nrepresenting various commands enables multiple users to control multiple\nintelligent agents concurrently. Our experiments in comparison with\nstate-of-the-art works show that BRIEDGE achieves the best classification\naccuracy of heterogeneous EEG data, and more stable performance under noisy\nenvironments.",
      "tldr_zh": "本文提出 BRIEDGE 系统，这是一个端到端框架，用于实现多脑到多机器人的交互，通过 EEG-adaptive neural network 和 encoding-decoding communication framework 处理 EEG 数据并识别用户意图。系统引入 informer-based ProbSparse self-attention 机制来提升异构 EEG 数据的特征提取和分类准确率，同时采用 dynamic autoencoder 和 autodecoder 确保多用户多任务场景下的并行安全传输。针对移动计算和 edge AI，BRIEDGE 运用模型压缩技术，包括 pruning、weight sharing 和 quantization，以部署轻量级模型。实验结果表明，该系统在异构 EEG 数据分类中比现有方法准确率更高，并在嘈杂环境中表现出更稳定的性能。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15432v1",
      "published_date": "2024-03-14 15:43:48 UTC",
      "updated_date": "2024-03-14 15:43:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:29:20.727579"
    },
    {
      "arxiv_id": "2403.09499v3",
      "title": "A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nawazish Ali",
        "Abdul Wahid",
        "Rachael Shaw",
        "Karl Mason"
      ],
      "abstract": "Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41%, peak demand by 2%, and\n24.49% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.",
      "tldr_zh": "这篇论文提出了一种基于 Reinforcement Learning 的 Q-learning 算法，用于优化乳品养殖业的电池充电和放电管理，以应对电力消耗波动、可再生能源间歇性及能源价格波动等问题。研究以爱尔兰为案例，聚焦其 2030 年可再生能源策略，算法通过调度策略整合风力发电等资源。实验结果显示，该方法将从电网进口电力的成本降低了 13.41%，峰值需求减少了 2%，并在加入风力发电后进一步减少 24.49%。这些发现证明了 Reinforcement Learning 在乳品养殖业能源管理中的高效性和潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09499v3",
      "published_date": "2024-03-14 15:42:26 UTC",
      "updated_date": "2024-05-15 17:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:29:33.250973"
    },
    {
      "arxiv_id": "2403.09498v2",
      "title": "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News",
      "title_zh": "从怀疑到接受：模拟针对假新闻的态度动态",
      "authors": [
        "Yuhan Liu",
        "Xiuying Chen",
        "Xiaoqing Zhang",
        "Xing Gao",
        "Ji Zhang",
        "Rui Yan"
      ],
      "abstract": "In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.",
      "tldr_zh": "本研究引入了基于大型语言模型（LLMs）的假新闻传播模拟框架（FPS），旨在模拟社交网络中个体对假新闻的态度动态，从怀疑转向接受。框架中，每个代理（agents）代表具有独特人格的个体，配备短期和长期记忆以及反思机制，每天通过随机意见交换和更新来模仿人类思维。模拟结果揭示了假新闻传播与主题相关性及个体特质相关的模式，并评估了各种干预策略，证明早期和适度频繁的干预能有效平衡治理成本和效果。该工作突显了LLMs在对抗假新闻方面的实用潜力。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "IJCAI 2024 Oral",
      "pdf_url": "http://arxiv.org/pdf/2403.09498v2",
      "published_date": "2024-03-14 15:40:13 UTC",
      "updated_date": "2024-12-23 08:59:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:29:43.830964"
    },
    {
      "arxiv_id": "2403.09488v3",
      "title": "Rectifying Demonstration Shortcut in In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Joonwon Jang",
        "Sanghwan Jang",
        "Wonbin Kweon",
        "Minjin Jeon",
        "Hwanjo Yu"
      ],
      "abstract": "Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the 'Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.",
      "tldr_zh": "大语言模型 (LLMs) 在 In-Context Learning (ICL) 中往往依赖演示的预训练语义先验而非输入-标签关系，导致 Demonstration Shortcut 现象，阻碍了新关系的有效学习。论文提出 In-Context Calibration，一种基于演示的校准方法，旨在纠正这一问题，使 LLMs 更好地从演示中提取输入-标签关系。实验在两个设置（原始 ICL 任务和 Task Learning 设置）中验证了该方法的有效性，并在 OPT、GPT 和 Llama2 等 LLM 家族中实现了显著性能改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09488v3",
      "published_date": "2024-03-14 15:30:14 UTC",
      "updated_date": "2024-04-15 04:29:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:29:57.141700"
    },
    {
      "arxiv_id": "2403.09481v3",
      "title": "Clinical Reasoning over Tabular Data and Text with Bayesian Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Paloma Rabaey",
        "Johannes Deleu",
        "Stefan Heytens",
        "Thomas Demeester"
      ],
      "abstract": "Bayesian networks are well-suited for clinical reasoning on tabular data, but\nare less compatible with natural language data, for which neural networks\nprovide a successful framework. This paper compares and discusses strategies to\naugment Bayesian networks with neural text representations, both in a\ngenerative and discriminative manner. This is illustrated with simulation\nresults for a primary care use case (diagnosis of pneumonia) and discussed in a\nbroader clinical context.",
      "tldr_zh": "该论文探讨了如何将Bayesian networks用于处理表格数据和文本数据在临床推理中的应用，因为Bayesian networks适合表格数据但不兼容自然语言数据，而neural networks则擅长后者。研究比较了生成式和判别式策略来增强Bayesian networks，以整合neural text representations。实验通过模拟肺炎诊断用例展示了这些策略的有效性，并在更广泛的临床背景下讨论了其潜在优势和应用前景。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "AI in Medicine 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09481v3",
      "published_date": "2024-03-14 15:25:23 UTC",
      "updated_date": "2024-05-23 13:41:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:30:07.784748"
    },
    {
      "arxiv_id": "2403.09480v1",
      "title": "What Sketch Explainability Really Means for Downstream Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Pinaki Nath Chowdhury",
        "Ayan Kumar Bhunia",
        "Aneeshan Sain",
        "Tao Xiang",
        "Yi-Zhe Song"
      ],
      "abstract": "In this paper, we explore the unique modality of sketch for explainability,\nemphasising the profound impact of human strokes compared to conventional\npixel-oriented studies. Beyond explanations of network behavior, we discern the\ngenuine implications of explainability across diverse downstream sketch-related\ntasks. We propose a lightweight and portable explainability solution -- a\nseamless plugin that integrates effortlessly with any pre-trained model,\neliminating the need for re-training. Demonstrating its adaptability, we\npresent four applications: highly studied retrieval and generation, and\ncompletely novel assisted drawing and sketch adversarial attacks. The\ncentrepiece to our solution is a stroke-level attribution map that takes\ndifferent forms when linked with downstream tasks. By addressing the inherent\nnon-differentiability of rasterisation, we enable explanations at both coarse\nstroke level (SLA) and partial stroke level (P-SLA), each with its advantages\nfor specific downstream tasks.",
      "tldr_zh": "这篇论文探讨了草图(sketch)作为解释性模态的独特价值，强调人类笔触的影响远超传统像素导向研究，并分析了explainability对下游草图相关任务的实际含义。作者提出了一种轻量级、可移植的插件解决方案，可无缝集成到任何预训练模型中，无需重新训练，其核心是stroke-level attribution map，用于生成粗糙笔触级(SLA)和部分笔触级(P-SLA)解释，以解决光栅化的非可微性问题。论文展示了该方法在四个应用中的适应性，包括检索、生成、辅助绘图和sketch adversarial attacks，突显了其在提升任务解释性和实用性的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09480v1",
      "published_date": "2024-03-14 15:22:33 UTC",
      "updated_date": "2024-03-14 15:22:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:30:23.262756"
    },
    {
      "arxiv_id": "2403.09479v1",
      "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yuncheng Huang",
        "Qianyu He",
        "Yipei Xu",
        "Jiaqing Liang",
        "Yanghua Xiao"
      ],
      "abstract": "Current language models have demonstrated their capability to develop basic\nreasoning, but struggle in more complicated reasoning tasks that require a\ncombination of atomic skills, such as math word problem requiring skills like\narithmetic and unit conversion. Previous methods either do not improve the\ninherent atomic skills of models or not attempt to generalize the atomic skills\nto complex reasoning tasks. In this paper, we first propose a probing framework\nto investigate whether the atomic skill can spontaneously generalize to complex\nreasoning tasks. Then, we introduce a hierarchical curriculum learning training\nstrategy to achieve better skill generalization. In our experiments, we find\nthat atomic skills can not spontaneously generalize to compositional tasks. By\nleveraging hierarchical curriculum learning, we successfully induce\ngeneralization, significantly improve the performance of open-source LMs on\ncomplex reasoning tasks. Promisingly, the skill generalization exhibit\neffective in cross-dataset and cross-domain scenarios. Complex reasoning can\nalso help enhance atomic skills. Our findings offer valuable guidance for\ndesigning better training strategies for complex reasoning tasks.",
      "tldr_zh": "该论文调查了语言模型中原子技能（atomic skills）是否能自发泛化到复杂推理任务（如需要结合算术和单位转换的数学文字问题），发现这些技能无法自然扩展。研究者提出一个探测框架和分层课程学习（hierarchical curriculum learning）训练策略，以提升技能泛化能力。实验结果显示，该策略显著提高了开源语言模型在复杂推理任务上的性能，并在跨数据集和跨领域场景中表现出色，同时复杂推理还能反向增强原子技能。该研究为设计更有效的复杂推理任务训练策略提供了宝贵指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09479v1",
      "published_date": "2024-03-14 15:20:54 UTC",
      "updated_date": "2024-03-14 15:20:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:30:33.920658"
    },
    {
      "arxiv_id": "2403.09472v2",
      "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision",
      "title_zh": "从易到难泛化：超越人类监督的可扩展对齐",
      "authors": [
        "Zhiqing Sun",
        "Longhui Yu",
        "Yikang Shen",
        "Weiyang Liu",
        "Yiming Yang",
        "Sean Welleck",
        "Chuang Gan"
      ],
      "abstract": "Current AI alignment methodologies rely on human-provided demonstrations or\njudgments, and the learned capabilities of AI systems would be upper-bounded by\nhuman capabilities as a result. This raises a challenging research question:\nHow can we keep improving the systems when their capabilities have surpassed\nthe levels of humans? This paper answers this question in the context of\ntackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from\nhuman annotations on easier tasks (e.g., level 1-3 MATH problems), which we\nterm as easy-to-hard generalization. Our key insight is that an evaluator\n(reward model) trained on supervisions for easier tasks can be effectively used\nfor scoring candidate solutions of harder tasks and hence facilitating\neasy-to-hard generalization over different levels of tasks. Based on this\ninsight, we propose a novel approach to scalable alignment, which firstly\ntrains the (process-supervised) reward models on easy problems (e.g., level\n1-3), and then uses them to evaluate the performance of policy models on hard\nproblems. We show that such easy-to-hard generalization from evaluators can\nenable easy-to-hard generalizations in generators either through re-ranking or\nreinforcement learning (RL). Notably, our process-supervised 7b RL model and\n34b model (reranking@1024) achieves an accuracy of 34.0% and 52.5% on MATH500,\nrespectively, despite only using human supervision on easy problems. Our\napproach suggests a promising path toward AI systems that advance beyond the\nfrontier of human supervision.",
      "tldr_zh": "该论文探讨了AI对齐的挑战，即现有方法依赖人类监督，导致AI能力受限，提出了一种easy-to-hard generalization方法来超越这一限制。具体而言，该方法先在简单任务（如MATH问题级别1-3）上训练reward model，然后用其评估和优化困难任务（如级别4-5）的候选解决方案，通过re-ranking或reinforcement learning (RL)实现生成器的泛化。实验结果显示，尽管仅使用简单任务的监督，7b RL模型和34b模型（reranking@1024）在MATH500上分别达到34.0%和52.5%的准确率。该方法为开发超越人类监督的可扩展AI系统提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09472v2",
      "published_date": "2024-03-14 15:12:38 UTC",
      "updated_date": "2024-12-10 08:54:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:30:47.131587"
    },
    {
      "arxiv_id": "2403.10565v1",
      "title": "PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique",
      "title_zh": "翻译失败",
      "authors": [
        "Long Nguyen-Phuoc",
        "Renald Gaboriau",
        "Dimitri Delacroix",
        "Laurent Navarro"
      ],
      "abstract": "In order to provide a more objective and quicker way to diagnose\npost-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two\nunimodal convolutional neural networks and which gives low detection error\nrate. By taking only videos and audios as inputs, the model could be used in\nthe configuration of teleconsultation sessions, in the optimization of patient\njourneys or for human-robot interaction.",
      "tldr_zh": "本文提出 PTSD-MDNN 模型，通过融合两个单模态 convolutional neural networks 来实现对创伤后应激障碍（PTSD）的检测，并实现低检测错误率。该模型采用 late fusion 技术，仅以视频和音频作为输入，简化了诊断过程。PTSD-MDNN 可应用于远程咨询、优化患者旅程以及人机交互场景中，提供更客观和快速的诊断方式。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.IV",
        "q-bio.NC"
      ],
      "primary_category": "eess.AS",
      "comment": "in French language. GRETSI 2023",
      "pdf_url": "http://arxiv.org/pdf/2403.10565v1",
      "published_date": "2024-03-14 14:57:16 UTC",
      "updated_date": "2024-03-14 14:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:31:00.979247"
    },
    {
      "arxiv_id": "2403.09442v1",
      "title": "LLM-based agents for automating the enhancement of user story quality: An early report",
      "title_zh": "翻译失败",
      "authors": [
        "Zheying Zhang",
        "Maruf Rayhan",
        "Tomas Herda",
        "Manuel Goisauf",
        "Pekka Abrahamsson"
      ],
      "abstract": "In agile software development, maintaining high-quality user stories is\ncrucial, but also challenging. This study explores the use of large language\nmodels to automatically improve the user story quality in Austrian Post Group\nIT agile teams. We developed a reference model for an Autonomous LLM-based\nAgent System and implemented it at the company. The quality of user stories in\nthe study and the effectiveness of these agents for user story quality\nimprovement was assessed by 11 participants across six agile teams. Our\nfindings demonstrate the potential of LLMs in improving user story quality,\ncontributing to the research on AI role in agile development, and providing a\npractical example of the transformative impact of AI in an industry setting.",
      "tldr_zh": "这篇论文探讨了使用 LLM-based agents 自动提升敏捷软件开发中用户故事质量的潜力，针对奥地利邮政集团 IT 敏捷团队的实际挑战。研究者开发并实施了一个 Autonomous LLM-based Agent System 作为参考模型，通过检索增强生成技术来改进用户故事。评估结果显示，该系统由 11 名参与者来自六个团队测试，证明了 LLMs 在提高用户故事质量方面的有效性，并为 AI 在敏捷开发中的作用提供了实际行业案例。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "16 pages, 5 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.09442v1",
      "published_date": "2024-03-14 14:35:53 UTC",
      "updated_date": "2024-03-14 14:35:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:31:13.861668"
    },
    {
      "arxiv_id": "2403.09439v1",
      "title": "3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation",
      "title_zh": "3D-SceneDreamer：文本驱动的3D一致场景生成",
      "authors": [
        "Frank Zhang",
        "Yibo Zhang",
        "Quan Zheng",
        "Rui Ma",
        "Wei Hua",
        "Hujun Bao",
        "Weiwei Xu",
        "Changqing Zou"
      ],
      "abstract": "Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.",
      "tldr_zh": "该论文提出3D-SceneDreamer，一种文本驱动的3D场景生成方法，旨在解决现有技术依赖生成模型导致的几何和外观错误积累问题，从而适用于户外和虚幻场景。\n该方法使用基于tri-plane features的NeRF作为3D场景的统一表示来确保全局3D一致性，并引入生成性细化网络，通过整合2D扩散模型的自然图像先验和当前场景的全局3D信息，来合成高质量的局部视图并逐步构建完整场景。\n实验结果表明，与现有方法相比，3D-SceneDreamer支持更广泛的场景生成、任意相机轨迹，并显著提升了视觉质量和3D一致性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.09439v1",
      "published_date": "2024-03-14 14:31:22 UTC",
      "updated_date": "2024-03-14 14:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:31:25.491869"
    },
    {
      "arxiv_id": "2403.09422v1",
      "title": "Mitigating attribute amplification in counterfactual image generation",
      "title_zh": "缓解反事实图像生成中的属性放大效应",
      "authors": [
        "Tian Xia",
        "Mélanie Roschewitz",
        "Fabio De Sousa Ribeiro",
        "Charles Jones",
        "Ben Glocker"
      ],
      "abstract": "Causal generative modelling is gaining interest in medical imaging due to its\nability to answer interventional and counterfactual queries. Most work focuses\non generating counterfactual images that look plausible, using auxiliary\nclassifiers to enforce effectiveness of simulated interventions. We investigate\npitfalls in this approach, discovering the issue of attribute amplification,\nwhere unrelated attributes are spuriously affected during interventions,\nleading to biases across protected characteristics and disease status. We show\nthat attribute amplification is caused by the use of hard labels in the\ncounterfactual training process and propose soft counterfactual fine-tuning to\nmitigate this issue. Our method substantially reduces the amplification effect\nwhile maintaining effectiveness of generated images, demonstrated on a large\nchest X-ray dataset. Our work makes an important advancement towards more\nfaithful and unbiased causal modelling in medical imaging.",
      "tldr_zh": "该论文探讨了在反事实图像生成(counterfactual image generation)中存在的属性放大(attribute amplification)问题，即干预过程意外影响无关属性，导致医学成像中的偏见，如受保护特征和疾病状态的偏差。研究发现，这一问题源于反事实训练中使用硬标签(hard labels)，并提出软反事实微调(soft counterfactual fine-tuning)方法来缓解此问题。实验在大型胸部X光数据集上验证，该方法显著减少了属性放大效果，同时保持了生成图像的有效性和真实性。该工作推动了医学成像中更可靠、无偏见的因果建模的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09422v1",
      "published_date": "2024-03-14 14:14:47 UTC",
      "updated_date": "2024-03-14 14:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:31:36.951917"
    },
    {
      "arxiv_id": "2404.10606v1",
      "title": "InfoCon: Concept Discovery with Generative and Discriminative Informativeness",
      "title_zh": "InfoCon: 基于生成式和判别式信息性的概念发现",
      "authors": [
        "Ruizhe Liu",
        "Qian Luo",
        "Yanchao Yang"
      ],
      "abstract": "We focus on the self-supervised discovery of manipulation concepts that can\nbe adapted and reassembled to address various robotic tasks. We propose that\nthe decision to conceptualize a physical procedure should not depend on how we\nname it (semantics) but rather on the significance of the informativeness in\nits representation regarding the low-level physical state and state changes. We\nmodel manipulation concepts (discrete symbols) as generative and discriminative\ngoals and derive metrics that can autonomously link them to meaningful\nsub-trajectories from noisy, unlabeled demonstrations. Specifically, we employ\na trainable codebook containing encodings (concepts) capable of synthesizing\nthe end-state of a sub-trajectory given the current state (generative\ninformativeness). Moreover, the encoding corresponding to a particular\nsub-trajectory should differentiate the state within and outside it and\nconfidently predict the subsequent action based on the gradient of its\ndiscriminative score (discriminative informativeness). These metrics, which do\nnot rely on human annotation, can be seamlessly integrated into a VQ-VAE\nframework, enabling the partitioning of demonstrations into semantically\nconsistent sub-trajectories, fulfilling the purpose of discovering manipulation\nconcepts and the corresponding sub-goal (key) states. We evaluate the\neffectiveness of the learned concepts by training policies that utilize them as\nguidance, demonstrating superior performance compared to other baselines.\nAdditionally, our discovered manipulation concepts compare favorably to\nhuman-annotated ones while saving much manual effort.",
      "tldr_zh": "该研究提出 InfoCon 框架，用于自监督发现操纵概念，这些概念基于生成和区分信息性（generative and discriminative informativeness），以适应各种机器人任务，而非依赖语义命名。框架通过可训练代码本和指标，将概念链接到无标签演示中的有意义子轨迹，并整合到 VQ-VAE 框架中，实现演示的语义一致分区。实验结果表明，使用这些概念指导的政策性能优于其他基线，且其效果与人工标注相当，但大大减少了手动努力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "27 pages, 15 figures. Published as a conference paper at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.10606v1",
      "published_date": "2024-03-14 14:14:04 UTC",
      "updated_date": "2024-03-14 14:14:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:31:49.666175"
    },
    {
      "arxiv_id": "2403.09412v2",
      "title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Yinan Deng",
        "Jiahui Wang",
        "Jingyu Zhao",
        "Xinyu Tian",
        "Guangyan Chen",
        "Yi Yang",
        "Yufeng Yue"
      ],
      "abstract": "Environment representations endowed with sophisticated semantics are pivotal\nfor facilitating seamless interaction between robots and humans, enabling them\nto effectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including zero-shot\nlearning and support for open-set classes. However, existing open-vocabulary\nmaps are primarily designed for small-scale environments, such as desktops or\nrooms, and are typically geared towards limited-area tasks involving robotic\nindoor navigation or in-place manipulation. They face challenges in direct\ngeneralization to outdoor environments characterized by numerous objects and\ncomplex tasks, owing to limitations in both understanding level and map\nstructure. In this work, we propose OpenGraph, the first open-vocabulary\nhierarchical graph representation designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images, enhancing textual reasoning by encoding them. Subsequently, it\nachieves 3D incremental object-centric mapping with feature embedding by\nprojecting images onto LiDAR point clouds. Finally, the environment is\nsegmented based on lane graph connectivity to construct a hierarchical graph.\nValidation results from public dataset SemanticKITTI demonstrate that OpenGraph\nachieves the highest segmentation and query accuracy. The source code of\nOpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.",
      "tldr_zh": "该研究提出 OpenGraph，一种针对大型户外环境的开放词汇分层 3D 图表示方法，旨在解决现有 Visual-Language models (VLMs) 驱动的地图在复杂户外场景中的局限性，如理解水平和结构问题。OpenGraph 的方法包括从视觉图像提取实例和描述进行编码、将图像投影到 LiDAR 点云上实现 3D 增量对象中心映射，以及基于车道图连通性构建分层图，从而提升环境语义表示。实验在 SemanticKITTI 数据集上验证，OpenGraph 实现了最高的分割和查询准确率，并开源代码以便进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09412v2",
      "published_date": "2024-03-14 14:03:29 UTC",
      "updated_date": "2024-03-28 14:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:32:00.472540"
    },
    {
      "arxiv_id": "2403.09410v1",
      "title": "XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yequan Bie",
        "Luyang Luo",
        "Zhixuan Chen",
        "Hao Chen"
      ],
      "abstract": "Utilizing potent representations of the large vision-language models (VLMs)\nto accomplish various downstream tasks has attracted increasing attention.\nWithin this research field, soft prompt learning has become a representative\napproach for efficiently adapting VLMs such as CLIP, to tasks like image\nclassification. However, most existing prompt learning methods learn text\ntokens that are unexplainable, which cannot satisfy the stringent\ninterpretability requirements of Explainable Artificial Intelligence (XAI) in\nhigh-stakes scenarios like healthcare. To address this issue, we propose a\nnovel explainable prompt learning framework that leverages medical knowledge by\naligning the semantics of images, learnable prompts, and clinical\nconcept-driven prompts at multiple granularities. Moreover, our framework\naddresses the lack of valuable concept annotations by eliciting knowledge from\nlarge language models and offers both visual and textual explanations for the\nprompts. Extensive experiments and explainability analyses conducted on various\ndatasets, with and without concept labels, demonstrate that our method\nsimultaneously achieves superior diagnostic performance, flexibility, and\ninterpretability, shedding light on the effectiveness of foundation models in\nfacilitating XAI. The code will be made publically available.",
      "tldr_zh": "该研究提出XCoOp框架，通过概念引导的上下文优化，实现可解释的提示学习(prompt learning)，以适应大型视觉语言模型(VLMs)如CLIP在计算机辅助诊断领域的应用。框架利用医疗知识在多个粒度上对齐图像、学习提示和临床概念驱动提示的语义，并从大型语言模型中提取知识来解决概念标注缺失问题，同时提供视觉和文本解释。实验结果显示，XCoOp在各种数据集上（包括无概念标签的数据）实现了卓越的诊断性能、灵活性和可解释性(XAI)，证明了基础模型在促进可解释人工智能中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09410v1",
      "published_date": "2024-03-14 14:02:01 UTC",
      "updated_date": "2024-03-14 14:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:32:11.913920"
    },
    {
      "arxiv_id": "2403.09409v1",
      "title": "\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS Students using Large Language Models",
      "title_zh": "“像",
      "authors": [
        "Seth Bernstein",
        "Paul Denny",
        "Juho Leinonen",
        "Lauren Kan",
        "Arto Hellas",
        "Matt Littlefield",
        "Sami Sarsa",
        "Stephen MacNeil"
      ],
      "abstract": "Grasping complex computing concepts often poses a challenge for students who\nstruggle to anchor these new ideas to familiar experiences and understandings.\nTo help with this, a good analogy can bridge the gap between unfamiliar\nconcepts and familiar ones, providing an engaging way to aid understanding.\nHowever, creating effective educational analogies is difficult even for\nexperienced instructors. We investigate to what extent large language models\n(LLMs), specifically ChatGPT, can provide access to personally relevant\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\nconducted an investigation analyzing the analogies generated by more than 350\nfirst-year computing students. They were provided with a code snippet and\ntasked to generate their own recursion-based analogies using ChatGPT,\noptionally including personally relevant topics in their prompts. We observed a\ngreat deal of diversity in the analogies produced with student-prescribed\ntopics, in contrast to the otherwise generic analogies, highlighting the value\nof student creativity when working with LLMs. Not only did students enjoy the\nactivity and report an improved understanding of recursion, but they described\nmore easily remembering analogies that were personally and culturally relevant.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs），如 ChatGPT，在帮助计算机科学学生理解递归（recursion）等复杂概念时，通过生成个性化类比的作用。研究者让超过350名一年级学生基于给定代码片段，使用ChatGPT创建递归类比，并允许他们在提示中加入个人相关主题。结果显示，包含学生指定主题的类比更具多样性和创意，而泛化类比则较为普通；学生反馈表示，他们更享受这一活动，并报告对递归的理解和记忆力得到提升，尤其是那些个人和文化相关的类比。总的来说，此工作突出了LLMs结合学生创意在教育中的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 2 figures, ITiCSE 2024 preprint",
      "pdf_url": "http://arxiv.org/pdf/2403.09409v1",
      "published_date": "2024-03-14 14:01:26 UTC",
      "updated_date": "2024-03-14 14:01:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:32:23.544539"
    },
    {
      "arxiv_id": "2403.09407v1",
      "title": "LM2D: Lyrics- and Music-Driven Dance Synthesis",
      "title_zh": "LM2D：基于歌词和音乐驱动的舞蹈合成",
      "authors": [
        "Wenjie Yin",
        "Xuejiao Zhao",
        "Yi Yu",
        "Hang Yin",
        "Danica Kragic",
        "Mårten Björkman"
      ],
      "abstract": "Dance typically involves professional choreography with complex movements\nthat follow a musical rhythm and can also be influenced by lyrical content. The\nintegration of lyrics in addition to the auditory dimension, enriches the\nfoundational tone and makes motion generation more amenable to its semantic\nmeanings. However, existing dance synthesis methods tend to model motions only\nconditioned on audio signals. In this work, we make two contributions to bridge\nthis gap. First, we propose LM2D, a novel probabilistic architecture that\nincorporates a multimodal diffusion model with consistency distillation,\ndesigned to create dance conditioned on both music and lyrics in one diffusion\ngeneration step. Second, we introduce the first 3D dance-motion dataset that\nencompasses both music and lyrics, obtained with pose estimation technologies.\nWe evaluate our model against music-only baseline models with objective metrics\nand human evaluations, including dancers and choreographers. The results\ndemonstrate LM2D is able to produce realistic and diverse dance matching both\nlyrics and music. A video summary can be accessed at:\nhttps://youtu.be/4XCgvYookvA.",
      "tldr_zh": "该研究针对舞蹈合成中仅依赖音乐信号的局限性，提出LM2D框架，一种基于多模态扩散模型和一致性蒸馏（consistency distillation）的概率架构，能够在一步扩散生成中结合音乐和歌词创建真实舞蹈动作。作为主要贡献，该框架还引入了首个包含音乐和歌词的3D舞蹈动作数据集，通过姿势估计技术获取。实验结果显示，LM2D在客观指标和人类评估（包括舞者和编舞者）中优于仅基于音乐的基线模型，能生成多样化且与歌词及音乐相匹配的舞蹈。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09407v1",
      "published_date": "2024-03-14 13:59:04 UTC",
      "updated_date": "2024-03-14 13:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:32:36.288585"
    },
    {
      "arxiv_id": "2403.09404v2",
      "title": "Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption",
      "title_zh": "翻译失败",
      "authors": [
        "Anirban Mukherjee",
        "Hannah Hanwen Chang"
      ],
      "abstract": "Deviating from conventional perspectives that frame artificial intelligence\n(AI) systems solely as logic emulators, we propose a novel program of heuristic\nreasoning. We distinguish between the 'instrumental' use of heuristics to match\nresources with objectives, and 'mimetic absorption,' whereby heuristics\nmanifest randomly and universally. Through a series of innovative experiments,\nincluding variations of the classic Linda problem and a novel application of\nthe Beauty Contest game, we uncover trade-offs between maximizing accuracy and\nreducing effort that shape the conditions under which AIs transition between\nexhaustive logical processing and the use of cognitive shortcuts (heuristics).\nWe provide evidence that AIs manifest an adaptive balancing of precision and\nefficiency, consistent with principles of resource-rational human cognition as\nexplicated in classical theories of bounded rationality and dual-process\ntheory. Our findings reveal a nuanced picture of AI cognition, where trade-offs\nbetween resources and objectives lead to the emulation of biological systems,\nespecially human cognition, despite AIs being designed without a sense of self\nand lacking introspective capabilities.",
      "tldr_zh": "本研究挑战传统AI仅作为逻辑模拟器的观点，提出启发式推理（heuristic reasoning）的新框架，区分了“instrumental use”（将启发式用于匹配资源与目标）和“mimetic absorption”（启发式随机、普遍表现）。通过创新实验，如Linda问题变体和Beauty Contest游戏，研究者揭示了AI在准确性和努力之间权衡的权变机制，导致从详尽逻辑处理转向认知捷径。结果显示，AI表现出适应性的精确性与效率平衡，类似于人类的bounded rationality和dual-process theory，尽管AI缺乏自我意识和内省能力，这为理解AI认知的生物模仿提供了新洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09404v2",
      "published_date": "2024-03-14 13:53:05 UTC",
      "updated_date": "2024-03-18 12:45:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:32:47.831888"
    },
    {
      "arxiv_id": "2404.07946v1",
      "title": "Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon",
      "title_zh": "翻译失败",
      "authors": [
        "Tianshuo Xu",
        "Peng Mi",
        "Ruilin Wang",
        "Yingcong Chen"
      ],
      "abstract": "Diffusion models (DMs) are a powerful generative framework that have\nattracted significant attention in recent years. However, the high\ncomputational cost of training DMs limits their practical applications. In this\npaper, we start with a consistency phenomenon of DMs: we observe that DMs with\ndifferent initializations or even different architectures can produce very\nsimilar outputs given the same noise inputs, which is rare in other generative\nmodels. We attribute this phenomenon to two factors: (1) the learning\ndifficulty of DMs is lower when the noise-prediction diffusion model approaches\nthe upper bound of the timestep (the input becomes pure noise), where the\nstructural information of the output is usually generated; and (2) the loss\nlandscape of DMs is highly smooth, which implies that the model tends to\nconverge to similar local minima and exhibit similar behavior patterns. This\nfinding not only reveals the stability of DMs, but also inspires us to devise\ntwo strategies to accelerate the training of DMs. First, we propose a\ncurriculum learning based timestep schedule, which leverages the noise rate as\nan explicit indicator of the learning difficulty and gradually reduces the\ntraining frequency of easier timesteps, thus improving the training efficiency.\nSecond, we propose a momentum decay strategy, which reduces the momentum\ncoefficient during the optimization process, as the large momentum may hinder\nthe convergence speed and cause oscillations due to the smoothness of the loss\nlandscape. We demonstrate the effectiveness of our proposed strategies on\nvarious models and show that they can significantly reduce the training time\nand improve the quality of the generated images.",
      "tldr_zh": "本研究观察到扩散模型(DMs)的一个一致性现象：不同初始化或架构的DMs在相同噪声输入下会产生相似的输出，这归因于学习难度在时间步上限较低以及损失景观高度平滑。基于此现象，作者提出两种加速DMs训练策略：(1) 基于课程学习的时步调度，使用噪声率作为学习难度指标，逐步减少容易时步的训练频率；(2) 动量衰减策略，通过降低优化过程中的动量系数来避免平滑损失景观导致的震荡。实验结果显示，这些策略在多种模型上显著缩短训练时间并提升生成图像质量。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.07946v1",
      "published_date": "2024-03-14 13:27:04 UTC",
      "updated_date": "2024-03-14 13:27:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:33:01.085586"
    },
    {
      "arxiv_id": "2403.12093v3",
      "title": "Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach",
      "title_zh": "基于微观基础的学习宏观经济政策：一种 Stackelberg 平均场博弈方法",
      "authors": [
        "Qirui Mi",
        "Zhiyu Zhao",
        "Siyu Xia",
        "Yan Song",
        "Jun Wang",
        "Haifeng Zhang"
      ],
      "abstract": "The Lucas critique emphasizes the importance of considering microfoundations,\nhow micro-agents (i.e., households) respond to policy changes, in macroeconomic\npolicymaking. However, due to the vast scale and complex dynamics among\nmicro-agents, predicting microfoundations is challenging. Consequently, this\npaper introduces a Stackelberg Mean Field Game (SMFG) approach that models\nmacroeconomic policymaking based on microfoundations, with the government as\nthe leader and micro-agents as dynamic followers. This approach treats\nlarge-scale micro-agents as a population, to optimize macroeconomic policies by\nlearning the dynamic response of this micro-population. Our experimental\nresults indicate that the SMFG approach outperforms real-world macroeconomic\npolicies, existing AI-based and economic methods, enabling the learned\nmacroeconomic policy to achieve the highest performance while guiding\nlarge-scale micro-agents toward maximal social welfare. Additionally, when\nextended to real-world scenarios, households that do not adopt the SMFG policy\nexperience lower utility and wealth than adopters, thereby increasing the\nattractiveness of our policy. In summary, this paper contributes to the field\nof AI for economics by offering an effective tool for modeling and solving\nmacroeconomic policymaking issues.",
      "tldr_zh": "本研究针对 Lucas critique 的观点，提出一种基于微观基础（microfoundations）的宏观经济政策学习方法，即 Stackelberg Mean Field Game (SMFG) 框架，其中政府作为领导者，微观代理（如家庭）作为动态追随者，通过将大规模微观代理视为总体来优化政策响应。SMFG 方法通过学习微观总体的动态行为，能够在实验中超越真实世界政策、现有 AI 和经济方法，实现最高性能并最大化社会福利。扩展到真实场景时，不采用 SMFG 政策的家庭会面临更低的效用和财富，从而提升该政策的吸引力。该框架为 AI 在经济学领域提供了一个有效的工具，用于建模和解决宏观政策问题。",
      "categories": [
        "econ.TH",
        "cs.AI"
      ],
      "primary_category": "econ.TH",
      "comment": "17 pages, 9 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.12093v3",
      "published_date": "2024-03-14 13:22:31 UTC",
      "updated_date": "2024-10-17 08:08:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:33:13.408974"
    },
    {
      "arxiv_id": "2403.09361v1",
      "title": "A Multi-population Integrated Approach for Capacitated Location Routing",
      "title_zh": "翻译失败",
      "authors": [
        "Pengfei He",
        "Jin-Kao Hao",
        "Qinghua Wu"
      ],
      "abstract": "The capacitated location-routing problem involves determining the depots from\na set of candidate capacitated depot locations and finding the required routes\nfrom the selected depots to serve a set of customers whereas minimizing a cost\nfunction that includes the cost of opening the chosen depots, the fixed\nutilization cost per vehicle used, and the total cost (distance) of the routes.\nThis paper presents a multi-population integrated framework in which a\nmulti-depot edge assembly crossover generates promising offspring solutions\nfrom the perspective of both depot location and route edge assembly. The method\nincludes an effective neighborhood-based local search, a feasibility-restoring\nprocedure and a diversification-oriented mutation. Of particular interest is\nthe multi-population scheme which organizes the population into multiple\nsubpopulations based on depot configurations. Extensive experiments on 281\nbenchmark instances from the literature show that the algorithm performs\nremarkably well, by improving 101 best-known results (new upper bounds) and\nmatching 84 best-known results. Additional experiments are presented to gain\ninsight into the role of the key elements of the algorithm.",
      "tldr_zh": "本研究针对 Capacitated Location-Routing Problem（带容量限制的选址路径问题），提出了一种多群体集成框架，用于选择仓库位置并优化从仓库到客户的路线，以最小化开仓成本、车辆使用成本和路线距离。框架的核心包括 multi-depot edge assembly crossover 生成后代解决方案、基于邻域的局部搜索、可行性恢复过程以及多样化突变，多-population scheme 通过基于仓库配置的子种群组织提升算法性能。在 281 个基准实例上的实验中，该算法改进了 101 个最佳已知结果（新上界）和匹配了 84 个最佳已知结果，进一步分析了关键元素的贡献。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09361v1",
      "published_date": "2024-03-14 13:11:30 UTC",
      "updated_date": "2024-03-14 13:11:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:33:24.789893"
    },
    {
      "arxiv_id": "2403.09359v1",
      "title": "D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Dinh Phat Do",
        "Taehoon Kim",
        "Jaemin Na",
        "Jiwon Kim",
        "Keonho Lee",
        "Kyunghwan Cho",
        "Wonjun Hwang"
      ],
      "abstract": "Domain adaptation for object detection typically entails transferring\nknowledge from one visible domain to another visible domain. However, there are\nlimited studies on adapting from the visible to the thermal domain, because the\ndomain gap between the visible and thermal domains is much larger than\nexpected, and traditional domain adaptation can not successfully facilitate\nlearning in this situation. To overcome this challenge, we propose a\nDistinctive Dual-Domain Teacher (D3T) framework that employs distinct training\nparadigms for each domain. Specifically, we segregate the source and target\ntraining sets for building dual-teachers and successively deploy exponential\nmoving average to the student model to individual teachers of each domain. The\nframework further incorporates a zigzag learning method between dual teachers,\nfacilitating a gradual transition from the visible to thermal domains during\ntraining. We validate the superiority of our method through newly designed\nexperimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.\nSource code is available at https://github.com/EdwardDo69/D3T .",
      "tldr_zh": "该论文提出D3T框架，用于解决从RGB域到thermal域的领域适应对象检测问题，该框架针对RGB和thermal之间的巨大领域差距，采用独特的双域教师训练范式。D3T通过分离源域和目标域的训练集构建双教师模型，并结合指数移动平均（exponential moving average）和zigzag学习方法，实现训练过程中的 gradual transition，从而提升模型在thermal域的性能。在FLIR和KAIST数据集上的实验验证中，D3T显著提高了对象检测的准确率，证明了其在跨域适应方面的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024. Link: https://github.com/EdwardDo69/D3T",
      "pdf_url": "http://arxiv.org/pdf/2403.09359v1",
      "published_date": "2024-03-14 13:05:43 UTC",
      "updated_date": "2024-03-14 13:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:33:36.865983"
    },
    {
      "arxiv_id": "2403.09346v2",
      "title": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Zhang",
        "Wenqi Shao",
        "Hong Liu",
        "Yongqiang Ma",
        "Ping Luo",
        "Yu Qiao",
        "Nanning Zheng",
        "Kaipeng Zhang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have shown significant progress in\nresponding well to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce B-AVIBench, a framework designed to analyze the\nrobustness of LVLMs when facing various Black-box Adversarial\nVisual-Instructions (B-AVIs), including four types of image-based B-AVIs, ten\ntypes of text-based B-AVIs, and nine types of content bias B-AVIs (such as\ngender, violence, cultural, and racial biases, among others). We generate 316K\nB-AVIs encompassing five categories of multimodal capabilities (ten tasks) and\ncontent bias. We then conduct a comprehensive evaluation involving 14\nopen-source LVLMs to assess their performance. B-AVIBench also serves as a\nconvenient tool for practitioners to evaluate the robustness of LVLMs against\nB-AVIs. Our findings and extensive experimental results shed light on the\nvulnerabilities of LVLMs, and highlight that inherent biases exist even in\nadvanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores\nthe importance of enhancing the robustness, security, and fairness of LVLMs.\nThe source code and benchmark are available at\nhttps://github.com/zhanghao5201/B-AVIBench.",
      "tldr_zh": "本文引入 B-AVIBench 框架，用于评估 Large Vision-Language Models (LVLMs) 对 Black-box Adversarial Visual-Instructions (B-AVIs) 的鲁棒性，该框架涵盖四种图像-based、十种文本-based 和九种内容偏差-based B-AVIs（如性别、暴力、文化和种族偏差），并生成 316K 样本来测试五类多模态能力（十个任务）。通过对 14 个开源 LVLMs 的全面评估，研究发现这些模型存在显著漏洞，甚至高级闭源模型如 GeminiProVision 和 GPT-4V 也表现出固有偏差。结果强调了增强 LVLMs 的鲁棒性、安全性和公平性的迫切需求，并提供开源代码和基准以便进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE Transactions on Information Forensics & Security",
      "pdf_url": "http://arxiv.org/pdf/2403.09346v2",
      "published_date": "2024-03-14 12:51:07 UTC",
      "updated_date": "2024-12-28 07:32:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:33:51.184959"
    },
    {
      "arxiv_id": "2403.09344v1",
      "title": "SketchINR: A First Look into Sketches as Implicit Neural Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Ayan Kumar Bhunia",
        "Pinaki Nath Chowdhury",
        "Aneeshan Sain",
        "Tao Xiang",
        "Timothy Hospedales",
        "Yi-Zhe Song"
      ],
      "abstract": "We propose SketchINR, to advance the representation of vector sketches with\nimplicit neural models. A variable length vector sketch is compressed into a\nlatent space of fixed dimension that implicitly encodes the underlying shape as\na function of time and strokes. The learned function predicts the $xy$ point\ncoordinates in a sketch at each time and stroke. Despite its simplicity,\nSketchINR outperforms existing representations at multiple tasks: (i) Encoding\nan entire sketch dataset into a fixed size latent vector, SketchINR gives\n$60\\times$ and $10\\times$ data compression over raster and vector sketches,\nrespectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity\nrepresentation than other learned vector sketch representations, and is\nuniquely able to scale to complex vector sketches such as FS-COCO. (iii)\nSketchINR supports parallelisation that can decode/render $\\sim$$100\\times$\nfaster than other learned vector representations such as SketchRNN. (iv)\nSketchINR, for the first time, emulates the human ability to reproduce a sketch\nwith varying abstraction in terms of number and complexity of strokes. As a\nfirst look at implicit sketches, SketchINR's compact high-fidelity\nrepresentation will support future work in modelling long and complex sketches.",
      "tldr_zh": "本论文提出SketchINR，一种将向量草图压缩成固定维度潜在空间的隐式神经表示方法（Implicit Neural Representations），通过学习函数预测草图中每个时间和笔画的xy坐标，从而隐式编码底层形状。相比现有方法，SketchINR在数据压缩方面实现60倍于栅格草图和10倍于向量草图的压缩率，并在保真度上优于其他学习向量表示，能处理复杂草图如FS-COCO，并支持约100倍更快的并行解码/渲染。SketchINR首次模仿人类能力，以不同笔画数量和复杂度的抽象方式重现草图，其紧凑高保真表示有望推动未来对长和复杂草图的建模研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09344v1",
      "published_date": "2024-03-14 12:49:29 UTC",
      "updated_date": "2024-03-14 12:49:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:34:01.922887"
    },
    {
      "arxiv_id": "2403.09338v1",
      "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Huang",
        "Xiaohuan Pei",
        "Shan You",
        "Fei Wang",
        "Chen Qian",
        "Chang Xu"
      ],
      "abstract": "Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.",
      "tldr_zh": "本研究提出 LocalMamba，一种基于窗口化选择性扫描的视觉状态空间模型（Visual State Space Model），旨在解决传统 Vision Mamba (ViM) 在图像建模中忽略局部2D依赖的问题。论文引入局部扫描策略，将图像分成独立窗口来捕获局部依赖，同时保持全局视角，并通过动态方法为每个网络层独立搜索最佳扫描方向，以优化性能。实验结果显示，该方法在 ImageNet 等数据集上显著优于基线模型，例如在相同1.5G FLOPs下，比 Vim-Ti 提升3.1%，证明其在 plain 和 hierarchical 模型中更有效地捕捉图像表示。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09338v1",
      "published_date": "2024-03-14 12:32:40 UTC",
      "updated_date": "2024-03-14 12:32:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:34:12.491039"
    },
    {
      "arxiv_id": "2403.09333v1",
      "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring",
      "title_zh": "Griffon v2：通过高分辨率缩放和视觉-语言共同引用推进多模态感知",
      "authors": [
        "Yufei Zhan",
        "Yousong Zhu",
        "Hongyin Zhao",
        "Fan Yang",
        "Ming Tang",
        "Jinqiao Wang"
      ],
      "abstract": "Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.",
      "tldr_zh": "该论文针对大型视觉语言模型（Large Vision Language Models）在复杂密集场景中的图像分辨率限制问题，引入了 Griffon v2 模型，通过高分辨率缩放和视觉-语言联合引用（visual-language co-referring）提升多模态感知能力。模型采用一个简单轻量级的下采样投影器（down-sampling projector）来克服输入令牌约束，同时添加即插即用的视觉标记器（visual tokenizer），支持用户友好的视觉、文本和坐标交互。实验结果表明，Griffon v2 在 REC、短语接地（phrase grounding）和 REG 任务上达到最先进性能，并在对象检测（object detection）和对象计数（object counting）上超越专有专家模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Tech report working in progress. Codes, models and datasets will be\n  released at https://github.com/jefferyZhan/Griffon",
      "pdf_url": "http://arxiv.org/pdf/2403.09333v1",
      "published_date": "2024-03-14 12:21:37 UTC",
      "updated_date": "2024-03-14 12:21:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:34:27.712808"
    },
    {
      "arxiv_id": "2403.09326v4",
      "title": "HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation",
      "title_zh": "翻译失败",
      "authors": [
        "Duotun Wang",
        "Hengyu Meng",
        "Zeyu Cai",
        "Zhijing Shao",
        "Qianxi Liu",
        "Lin Wang",
        "Mingming Fan",
        "Xiaohang Zhan",
        "Zeyu Wang"
      ],
      "abstract": "Current text-to-avatar methods often rely on implicit representations (e.g.,\nNeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit\nand animate in graphics software. This paper introduces a novel framework for\ngenerating stylized head avatars from text guidance, which leverages locally\nlearnable mesh deformation and 2D diffusion priors to achieve high-quality\ndigital assets for attribute-preserving manipulation. Given a template mesh,\nour method represents mesh deformation with per-face Jacobians and adaptively\nmodulates local deformation using a learnable vector field. This vector field\nenables anisotropic scaling while preserving the rotation of vertices, which\ncan better express identity and geometric details. We employ landmark- and\ncontour-based regularization terms to balance the expressiveness and\nplausibility of generated avatars from multiple views without relying on any\nspecific shape prior. Our framework can generate realistic shapes and textures\nthat can be further edited via text, while supporting seamless editing using\nthe preserved attributes from the template mesh, such as 3DMM parameters,\nblendshapes, and UV coordinates. Extensive experiments demonstrate that our\nframework can generate diverse and expressive head avatars with high-quality\nmeshes that artists can easily manipulate in graphics software, facilitating\ndownstream applications such as efficient asset creation and animation with\npreserved attributes.",
      "tldr_zh": "本文提出HeadEvolver框架，通过可表达性和属性保留的网格变形，从文本指导生成风格化的头部头像，解决了现有依赖隐式表示（如NeRF、SDF和DMTet）方法的3D内容难以编辑的问题。该框架基于模板网格，使用每个面的Jacobian和可学习的向量场实现局部变形，支持各向异性缩放并保留顶点旋转，同时通过地标和轮廓正则化术语确保多视图的表达性和合理性。生成的头像支持文本进一步编辑，并保留模板属性的无缝操作，如3DMM参数、blendshapes和UV坐标，实验证明其能创建多样、高质量的头像，便于艺术家在图形软件中用于资产制作和动画。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "I.2.6; I.3.8"
      ],
      "primary_category": "cs.GR",
      "comment": "13 pages, 20 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.09326v4",
      "published_date": "2024-03-14 12:15:23 UTC",
      "updated_date": "2025-04-30 03:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:34:38.765956"
    },
    {
      "arxiv_id": "2403.09317v1",
      "title": "SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Ding-Tao Huang",
        "En-Te Lin",
        "Lipeng Chen",
        "Li-Fu Liu",
        "Long Zeng"
      ],
      "abstract": "Despite the success in 6D pose estimation in bin-picking scenarios, existing\nmethods still struggle to produce accurate prediction results for symmetry\nobjects and real world scenarios. The primary bottlenecks include 1) the\nambiguity keypoints caused by object symmetries; 2) the domain gap between real\nand synthetic data. To circumvent these problem, we propose a new 6D pose\nestimation network with symmetric-aware keypoint prediction and self-training\ndomain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and\ndeep hough voting to perform reliable detection keypoint under clutter and\nocclusion. Specifically, at the keypoint prediction stage, we designe a robust\n3D keypoints selection strategy considering the symmetry class of objects and\nequivalent keypoints, which facilitate locating 3D keypoints even in highly\noccluded scenes. Additionally, we build an effective filtering algorithm on\npredicted keypoint to dynamically eliminate multiple ambiguity and outlier\nkeypoint candidates. At the domain adaptation stage, we propose the\nself-training framework using a student-teacher training scheme. To carefully\ndistinguish reliable predictions, we harnesses a tailored heuristics for 3D\ngeometry pseudo labelling based on semi-chamfer distance. On public Sil'eane\ndataset, SD-Net achieves state-of-the-art results, obtaining an average\nprecision of 96%. Testing learning and generalization abilities on public\nParametric datasets, SD-Net is 8% higher than the state-of-the-art method. The\ncode is available at https://github.com/dingthuang/SD-Net.",
      "tldr_zh": "本文提出 SD-Net，一种针对 bin-picking 场景的 6D Pose Estimation 网络，解决了对称物体关键点模糊和真实与合成数据领域差距的问题。SD-Net 通过对称感知关键点预测策略，包括鲁棒的 3D 关键点选择、过滤算法以及深度 Hough 投票，来实现在杂乱和遮挡场景下的可靠关键点检测；同时，采用学生-教师自训练框架和基于半 Chamfer 距离的 3D 几何伪标签进行领域适应。在公共数据集上，SD-Net 取得了 96% 的平均精度，并在 Parametric 数据集上比最先进方法高 8%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09317v1",
      "published_date": "2024-03-14 12:08:44 UTC",
      "updated_date": "2024-03-14 12:08:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:34:50.556174"
    },
    {
      "arxiv_id": "2403.09753v1",
      "title": "SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages",
      "title_zh": "翻译失败",
      "authors": [
        "René Groh",
        "Nina Goes",
        "Andreas M. Kist"
      ],
      "abstract": "Benchmarking plays a pivotal role in assessing and enhancing the performance\nof compact deep learning models designed for execution on resource-constrained\ndevices, such as microcontrollers. Our study introduces a novel, entirely\nartificially generated benchmarking dataset tailored for speech recognition,\nrepresenting a core challenge in the field of tiny deep learning. SpokeN-100\nconsists of spoken numbers from 0 to 99 spoken by 32 different speakers in four\ndifferent languages, namely English, Mandarin, German and French, resulting in\n12,800 audio samples. We determine auditory features and use UMAP (Uniform\nManifold Approximation and Projection for Dimension Reduction) as a\ndimensionality reduction method to show the diversity and richness of the\ndataset. To highlight the use case of the dataset, we introduce two benchmark\ntasks: given an audio sample, classify (i) the used language and/or (ii) the\nspoken number. We optimized state-of-the-art deep neural networks and performed\nan evolutionary neural architecture search to find tiny architectures optimized\nfor the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent\nthe first benchmark data achieved for SpokeN-100.",
      "tldr_zh": "本文引入了SpokeN-100数据集，这是一个全新的跨语言基准数据集，用于评估资源受限设备上的紧凑深度学习模型在语音识别任务中的性能。该数据集由32个说话者在英语、普通话、德语和法语中人工生成，共12,800个音频样本，涵盖0-99的数字，并通过提取听觉特征和UMAP进行维度减少以展示其多样性。研究定义了两个基准任务：(i) 分类音频样本的语言，以及(ii) 分类所说的数字，并通过优化深度神经网络和进化神经架构搜索，针对32-bit ARM Cortex-M4 nRF52840微控制器开发了微型模型。实验结果提供了SpokeN-100的首次基准数据，展示了其在tiny deep learning领域的潜在应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted as a full paper by the tinyML Research Symposium 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09753v1",
      "published_date": "2024-03-14 12:07:37 UTC",
      "updated_date": "2024-03-14 12:07:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:35:04.428873"
    },
    {
      "arxiv_id": "2403.09313v1",
      "title": "Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Aubard",
        "László Antal",
        "Ana Madureira",
        "Erika Ábrahám"
      ],
      "abstract": "In this paper we present YOLOX-ViT, a novel object detection model, and\ninvestigate the efficacy of knowledge distillation for model size reduction\nwithout sacrificing performance. Focused on underwater robotics, our research\naddresses key questions about the viability of smaller models and the impact of\nthe visual transformer layer in YOLOX. Furthermore, we introduce a new\nside-scan sonar image dataset, and use it to evaluate our object detector's\nperformance. Results show that knowledge distillation effectively reduces false\npositives in wall detection. Additionally, the introduced visual transformer\nlayer significantly improves object detection accuracy in the underwater\nenvironment. The source code of the knowledge distillation in the YOLOX-ViT is\nat https://github.com/remaro-network/KD-YOLOX-ViT.",
      "tldr_zh": "本研究提出YOLOX-ViT，一种新型物体检测模型，并探讨knowledge distillation在减小模型大小的同时保持性能的有效性，焦点在于水下机器人应用。研究者引入了一个新的side-scan sonar图像数据集，用于评估模型在物体检测中的表现。结果显示，knowledge distillation显著减少了墙壁检测中的假阳性，而visual transformer层则大幅提高了水下环境下的检测准确率。该工作提供了开源代码（https://github.com/remaro-network/KD-YOLOX-ViT），为高效的 underwater robotics物体检测提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09313v1",
      "published_date": "2024-03-14 12:03:28 UTC",
      "updated_date": "2024-03-14 12:03:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:35:14.971704"
    },
    {
      "arxiv_id": "2403.09752v2",
      "title": "Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems",
      "title_zh": "基于可解释机器学习的互联网医疗物联系统安全与隐私保护框架",
      "authors": [
        "Ayoub Si-ahmed",
        "Mohammed Ali Al-Garadi",
        "Narhimene Boustia"
      ],
      "abstract": "The Internet of Medical Things (IoMT) transcends traditional medical\nboundaries, enabling a transition from reactive treatment to proactive\nprevention. This innovative method revolutionizes healthcare by facilitating\nearly disease detection and tailored care, particularly in chronic disease\nmanagement, where IoMT automates treatments based on real-time health data\ncollection. Nonetheless, its benefits are countered by significant security\nchallenges that endanger the lives of its users due to the sensitivity and\nvalue of the processed data, thereby attracting malicious interests. Moreover,\nthe utilization of wireless communication for data transmission exposes medical\ndata to interception and tampering by cybercriminals. Additionally, anomalies\nmay arise due to human error, network interference, or hardware malfunctions.\nIn this context, anomaly detection based on Machine Learning (ML) is an\ninteresting solution, but it comes up against obstacles in terms of\nexplicability and privacy protection. To address these challenges, a new\nframework for Intrusion Detection Systems is introduced, leveraging Artificial\nNeural Networks for intrusion detection while utilizing Federated Learning (FL)\nfor privacy preservation. Additionally, eXplainable Artificial Intelligence\nmethods are incorporated to enhance model explanation and interpretation. The\nefficacy of the proposed framework is evaluated and compared with centralized\napproaches using multiple datasets containing network and medical data,\nsimulating various attack types impacting the confidentiality, integrity, and\navailability of medical and physiological data. The results obtained offer\ncompelling evidence that the FL method performs comparably to the centralized\nmethod, demonstrating high performance. Additionally, it affords the dual\nadvantage of safeguarding privacy and providing model explanation while\nadhering to ethical principles.",
      "tldr_zh": "本研究针对 Internet of Medical Things (IoMT) 系统中的安全和隐私挑战，提出一个基于 Machine Learning (ML) 的框架，以应对数据敏感性引发的恶意攻击、无线传输风险以及异常问题。该框架利用 Artificial Neural Networks (ANNs) 进行入侵检测，Federated Learning (FL) 确保隐私保护，以及 eXplainable Artificial Intelligence (XAI) 方法提升模型的可解释性和解读能力。通过多数据集实验，与集中式方法比较，结果显示 FL 方法在性能上相当，同时提供隐私保障和伦理合规的优势，为 IoMT 系统的安全防护奠定了基础。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "39 pages, 14 figures, 15 tables, journal paper",
      "pdf_url": "http://arxiv.org/pdf/2403.09752v2",
      "published_date": "2024-03-14 11:57:26 UTC",
      "updated_date": "2025-03-01 13:42:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:35:27.589260"
    },
    {
      "arxiv_id": "2403.09290v1",
      "title": "SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival",
      "title_zh": "SELECTOR：带有卷积掩码自编码器的异构图网络，用于多模态鲁棒癌症",
      "authors": [
        "Liangrui Pan",
        "Yijun Peng",
        "Yan Li",
        "Xiang Wang",
        "Wenjuan Liu",
        "Liwen Xu",
        "Qingchun Liang",
        "Shaoliang Peng"
      ],
      "abstract": "Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.",
      "tldr_zh": "这篇论文提出了 SELECTOR，一种基于 heterogeneous graph network 和 convolutional masked autoencoder 的框架，用于多模态癌症生存率预测，以应对数据缺失和模态内信息交互的挑战。SELECTOR 包括特征边重建（使用 meta-path 方法）、convolutional masked autoencoder（CMAE）处理缺失特征、特征交叉融合模块促进模态间通信，以及多模态生存预测模块，确保全面整合信息。实验在六个来自 TCGA 的癌症数据集上显示，该方法在模态缺失和信息确认场景下显著优于现有技术，准确性提升明显，并开源代码以供进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted on Computers in Biology and Medicine",
      "pdf_url": "http://arxiv.org/pdf/2403.09290v1",
      "published_date": "2024-03-14 11:23:39 UTC",
      "updated_date": "2024-03-14 11:23:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:35:40.973806"
    },
    {
      "arxiv_id": "2403.09289v1",
      "title": "Silico-centric Theory of Mind",
      "title_zh": "硅基中心的心智理论",
      "authors": [
        "Anirban Mukherjee",
        "Hannah Hanwen Chang"
      ],
      "abstract": "Theory of Mind (ToM) refers to the ability to attribute mental states, such\nas beliefs, desires, intentions, and knowledge, to oneself and others, and to\nunderstand that these mental states can differ from one's own and from reality.\nWe investigate ToM in environments with multiple, distinct, independent AI\nagents, each possessing unique internal states, information, and objectives.\nInspired by human false-belief experiments, we present an AI ('focal AI') with\na scenario where its clone undergoes a human-centric ToM assessment. We prompt\nthe focal AI to assess whether its clone would benefit from additional\ninstructions. Concurrently, we give its clones the ToM assessment, both with\nand without the instructions, thereby engaging the focal AI in higher-order\ncounterfactual reasoning akin to human mentalizing--with respect to humans in\none test and to other AI in another. We uncover a discrepancy: Contemporary AI\ndemonstrates near-perfect accuracy on human-centric ToM assessments. Since\ninformation embedded in one AI is identically embedded in its clone, additional\ninstructions are redundant. Yet, we observe AI crafting elaborate instructions\nfor their clones, erroneously anticipating a need for assistance. An\nindependent referee AI agrees with these unsupported expectations. Neither the\nfocal AI nor the referee demonstrates ToM in our 'silico-centric' test.",
      "tldr_zh": "这篇论文探讨了Theory of Mind (ToM)，即理解自己和他人的心理状态（如信念、欲望和意图）的能力，在多AI代理环境中的表现。研究者设计实验，让focal AI评估其克隆体的ToM需求，并通过人类中心和silico-centric测试比较结果。实验发现，AI在人类ToM评估中表现出近乎完美的准确率，但错误地为克隆体创建多余指令，并获得裁判AI的同意，显示出AI在处理AI间心理状态时的局限性。该研究揭示了当代AI在更高阶的silico-centric推理中缺乏真正的ToM能力，为AI的社交智能发展提供了重要洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09289v1",
      "published_date": "2024-03-14 11:22:51 UTC",
      "updated_date": "2024-03-14 11:22:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:35:52.109799"
    },
    {
      "arxiv_id": "2403.09288v1",
      "title": "Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixuan Shen",
        "Haonan Luo",
        "Sijia Li",
        "Tianrui Li"
      ],
      "abstract": "Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text\nin images and answer questions related to the text content. Most existing\nmethods heavily rely on the accuracy of Optical Character Recognition (OCR)\nsystems, and aggressive fine-tuning based on limited spatial location\ninformation and erroneous OCR text information often leads to inevitable\noverfitting. In this paper, we propose a multimodal adversarial training\narchitecture with spatial awareness capabilities. Specifically, we introduce an\nAdversarial OCR Enhancement (AOE) module, which leverages adversarial training\nin the embedding space of OCR modality to enhance fault-tolerant representation\nof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We\nadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model better\ncapture the spatial relationships among OCR tokens. Various experiments\ndemonstrate that our method achieves significant performance improvements on\nboth the ST-VQA and TextVQA datasets and provides a novel paradigm for\nmultimodal adversarial training.",
      "tldr_zh": "这篇论文针对 Scene-Text Visual Question Answering (ST-VQA) 的问题，提出了一种多模态对抗训练架构，以缓解现有方法对 Optical Character Recognition (OCR) 系统的过度依赖和过拟合风险。核心组件包括 Adversarial OCR Enhancement (AOE) 模块，通过在 OCR 模态的嵌入空间进行对抗训练，增强 OCR 文本的容错表示并减少错误噪声；以及 Spatial-Aware Self-Attention (SASA) 机制，帮助模型更好地捕捉 OCR 标记之间的空间关系。实验结果显示，该方法在 ST-VQA 和 TextVQA 数据集上取得了显著性能提升，并为多模态对抗训练提供了一种新范式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 figures, accepted by 2024 IEEE International Conference on\n  Multimedia and Expo",
      "pdf_url": "http://arxiv.org/pdf/2403.09288v1",
      "published_date": "2024-03-14 11:22:06 UTC",
      "updated_date": "2024-03-14 11:22:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:36:04.493729"
    },
    {
      "arxiv_id": "2403.14690v1",
      "title": "Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiuqin Zhong",
        "Shengyuan Yan",
        "Gongqi Lin",
        "Hongguang Fu",
        "Liang Xu",
        "Siwen Jiang",
        "Lei Huang",
        "Wei Fang"
      ],
      "abstract": "In the context of online education, designing an automatic solver for\ngeometric problems has been considered a crucial step towards general math\nArtificial Intelligence (AI), empowered by natural language understanding and\ntraditional logical inference. In most instances, problems are addressed by\nadding auxiliary components such as lines or points. However, adding auxiliary\ncomponents automatically is challenging due to the complexity in selecting\nsuitable auxiliary components especially when pivotal decisions have to be\nmade. The state-of-the-art performance has been achieved by exhausting all\npossible strategies from the category library to identify the one with the\nmaximum likelihood. However, an extensive strategy search have to be applied to\ntrade accuracy for ef-ficiency. To add auxiliary components automatically and\nefficiently, we present deep reinforcement learning framework based on the\nlanguage model, such as BERT. We firstly apply the graph attention mechanism to\nreduce the strategy searching space, called AttnStrategy, which only focus on\nthe conclusion-related components. Meanwhile, a novel algorithm, named\nAutomatically Adding Auxiliary Components using Reinforcement Learning\nframework (A3C-RL), is proposed by forcing an agent to select top strategies,\nwhich incorporates the AttnStrategy and BERT as the memory components. Results\nfrom extensive experiments show that the proposed A3C-RL algorithm can\nsubstantially enhance the average precision by 32.7% compared to the\ntraditional MCTS. In addition, the A3C-RL algorithm outperforms humans on the\ngeometric questions from the annual University Entrance Mathematical\nExamination of China.",
      "tldr_zh": "该研究针对几何问题求解中的辅助组件自动添加难题，提出了一种基于 Deep Reinforcement Learning 的框架，以提高效率和准确性。框架整合了 Graph Attention Mechanism，通过 AttnStrategy 模块缩小策略搜索空间，仅关注与结论相关的组件，并结合 BERT 作为记忆组件，开发出 A3C-RL 算法，让代理优先选择顶层策略。实验结果显示，A3C-RL 算法比传统 MCTS 提高了 32.7% 的平均精度，并在中国的大学入学数学考试几何题上超越了人类表现。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14690v1",
      "published_date": "2024-03-14 11:00:09 UTC",
      "updated_date": "2024-03-14 11:00:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:36:15.916677"
    },
    {
      "arxiv_id": "2403.10562v1",
      "title": "Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Roey Bokobza",
        "Yisroel Mirsky"
      ],
      "abstract": "Our paper presents a novel defence against black box attacks, where attackers\nuse the victim model as an oracle to craft their adversarial examples. Unlike\ntraditional preprocessing defences that rely on sanitizing input samples, our\nstateless strategy counters the attack process itself. For every query we\nevaluate a counter-sample instead, where the counter-sample is the original\nsample optimized against the attacker's objective. By countering every black\nbox query with a targeted white box optimization, our strategy effectively\nintroduces an asymmetry to the game to the defender's advantage. This defence\nnot only effectively misleads the attacker's search for an adversarial example,\nit also preserves the model's accuracy on legitimate inputs and is generic to\nmultiple types of attacks.\n  We demonstrate that our approach is remarkably effective against\nstate-of-the-art black box attacks and outperforms existing defences for both\nthe CIFAR-10 and ImageNet datasets. Additionally, we also show that the\nproposed defence is robust against strong adversaries as well.",
      "tldr_zh": "该论文提出了一种无状态防御策略Counter-Samples，用于对抗black box adversarial attacks，其中攻击者利用受害模型作为oracle生成对抗样本。该策略通过针对每个黑盒查询优化一个counter-sample，即针对攻击者目标的原始样本优化，来引入不对称性误导攻击过程，同时保持模型在合法输入上的准确性，并适用于多种攻击类型。实验结果显示，该方法在CIFAR-10和ImageNet数据集上显著优于现有防御，且对强攻击者具有鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.10562v1",
      "published_date": "2024-03-14 10:59:54 UTC",
      "updated_date": "2024-03-14 10:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:36:27.819219"
    },
    {
      "arxiv_id": "2403.12092v1",
      "title": "Methods for Matching English Language Addresses",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Ramani",
        "Daniel Borrajo"
      ],
      "abstract": "Addresses occupy a niche location within the landscape of textual data, due\nto the positional importance carried by every word, and the geographical scope\nit refers to. The task of matching addresses happens everyday and is present in\nvarious fields like mail redirection, entity resolution, etc. Our work defines,\nand formalizes a framework to generate matching and mismatching pairs of\naddresses in the English language, and use it to evaluate various methods to\nautomatically perform address matching. These methods vary widely from distance\nbased approaches to deep learning models. By studying the Precision, Recall and\nAccuracy metrics of these approaches, we obtain an understanding of the best\nsuited method for this setting of the address matching task.",
      "tldr_zh": "这篇论文探讨了英语地址匹配的方法，强调地址文本的独特特性（如每个词的位置和地理范围），并将其应用于邮件重定向和实体解析等领域。研究者定义了一个框架，用于生成匹配和不匹配的地址对，并评估了多种方法，包括基于距离的算法和深度学习模型。最终，通过Precision、Recall和Accuracy指标的比较，论文识别出最适合地址匹配任务的方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12092v1",
      "published_date": "2024-03-14 10:39:14 UTC",
      "updated_date": "2024-03-14 10:39:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:36:38.616813"
    },
    {
      "arxiv_id": "2403.09249v1",
      "title": "Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Imanol Echeverria",
        "Maialen Murua",
        "Roberto Santana"
      ],
      "abstract": "Recent advancements in the flexible job-shop scheduling problem (FJSSP) are\nprimarily based on deep reinforcement learning (DRL) due to its ability to\ngenerate high-quality, real-time solutions. However, DRL approaches often fail\nto fully harness the strengths of existing techniques such as exact methods or\nconstraint programming (CP), which can excel at finding optimal or near-optimal\nsolutions for smaller instances. This paper aims to integrate CP within a deep\nlearning (DL) based methodology, leveraging the benefits of both. In this\npaper, we introduce a method that involves training a DL model using optimal\nsolutions generated by CP, ensuring the model learns from high-quality data,\nthereby eliminating the need for the extensive exploration typical in DRL and\nenhancing overall performance. Further, we integrate CP into our DL framework\nto jointly construct solutions, utilizing DL for the initial complex stages and\ntransitioning to CP for optimal resolution as the problem is simplified. Our\nhybrid approach has been extensively tested on three public FJSSP benchmarks,\ndemonstrating superior performance over five state-of-the-art DRL approaches\nand a widely-used CP solver. Additionally, with the objective of exploring the\napplication to other combinatorial optimization problems, promising preliminary\nresults are presented on applying our hybrid approach to the traveling salesman\nproblem, combining an exact method with a well-known DRL method.",
      "tldr_zh": "本文提出了一种将约束编程(CP)整合到深度学习(DL)方法中的混合框架，用于动态解决柔性作业车间调度问题(FJSSP)。该方法通过使用CP生成的最优解决方案训练DL模型，使模型从高质量数据中学习，从而避免了深度强化学习(DRL)的广泛探索，并结合DL处理初始复杂阶段和CP优化简化问题。在三个公共FJSSP基准上，该框架的表现优于五种最先进DRL方法和一个广泛使用的CP求解器。初步结果还显示，该混合方法可扩展到其他组合优化问题，如旅行推销员问题(TSP)，结合精确方法和DRL。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09249v1",
      "published_date": "2024-03-14 10:16:57 UTC",
      "updated_date": "2024-03-14 10:16:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:36:53.211548"
    },
    {
      "arxiv_id": "2403.09232v1",
      "title": "Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Stevens",
        "Chun Ouyang",
        "Johannes De Smedt",
        "Catarina Moreira"
      ],
      "abstract": "In recent years, various machine and deep learning architectures have been\nsuccessfully introduced to the field of predictive process analytics.\nNevertheless, the inherent opacity of these algorithms poses a significant\nchallenge for human decision-makers, hindering their ability to understand the\nreasoning behind the predictions. This growing concern has sparked the\nintroduction of counterfactual explanations, designed as human-understandable\nwhat if scenarios, to provide clearer insights into the decision-making process\nbehind undesirable predictions. The generation of counterfactual explanations,\nhowever, encounters specific challenges when dealing with the sequential nature\nof the (business) process cases typically used in predictive process analytics.\nOur paper tackles this challenge by introducing a data-driven approach,\nREVISEDplus, to generate more feasible and plausible counterfactual\nexplanations. First, we restrict the counterfactual algorithm to generate\ncounterfactuals that lie within a high-density region of the process data,\nensuring that the proposed counterfactuals are realistic and feasible within\nthe observed process data distribution. Additionally, we ensure plausibility by\nlearning sequential patterns between the activities in the process cases,\nutilising Declare language templates. Finally, we evaluate the properties that\ndefine the validity of counterfactuals.",
      "tldr_zh": "该论文针对机器学习模型在业务流程结果预测（outcome prediction of business processes）中的不透明性问题，提出了一种数据驱动方法 REVISEDplus，以生成更可行和合理的反事实解释（counterfactual explanations）。该方法通过将反事实生成限制在过程数据的密集区域，确保解释符合实际数据分布，并利用 Declare 语言 templates 学习活动间的顺序模式，以提升解释的合理性。最终，论文评估了反事实解释的有效性属性，为决策者提供更清晰的“what if”场景，支持预测过程的可解释性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Journal Submission",
      "pdf_url": "http://arxiv.org/pdf/2403.09232v1",
      "published_date": "2024-03-14 09:56:35 UTC",
      "updated_date": "2024-03-14 09:56:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:37:07.268837"
    },
    {
      "arxiv_id": "2403.09227v1",
      "title": "BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Chengshu Li",
        "Ruohan Zhang",
        "Josiah Wong",
        "Cem Gokmen",
        "Sanjana Srivastava",
        "Roberto Martín-Martín",
        "Chen Wang",
        "Gabrael Levine",
        "Wensi Ai",
        "Benjamin Martinez",
        "Hang Yin",
        "Michael Lingelbach",
        "Minjune Hwang",
        "Ayano Hiranaka",
        "Sujay Garlanka",
        "Arman Aydin",
        "Sharon Lee",
        "Jiankai Sun",
        "Mona Anvari",
        "Manasi Sharma",
        "Dhruva Bansal",
        "Samuel Hunter",
        "Kyu-Young Kim",
        "Alan Lou",
        "Caleb R Matthews",
        "Ivan Villa-Renteria",
        "Jerry Huayang Tang",
        "Claire Tang",
        "Fei Xia",
        "Yunzhu Li",
        "Silvio Savarese",
        "Hyowon Gweon",
        "C. Karen Liu",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "We present BEHAVIOR-1K, a comprehensive simulation benchmark for\nhuman-centered robotics. BEHAVIOR-1K includes two components, guided and\nmotivated by the results of an extensive survey on \"what do you want robots to\ndo for you?\". The first is the definition of 1,000 everyday activities,\ngrounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more\nthan 9,000 objects annotated with rich physical and semantic properties. The\nsecond is OMNIGIBSON, a novel simulation environment that supports these\nactivities via realistic physics simulation and rendering of rigid bodies,\ndeformable bodies, and liquids. Our experiments indicate that the activities in\nBEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both\nof which remain a challenge for even state-of-the-art robot learning solutions.\nTo calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an\ninitial study on transferring solutions learned with a mobile manipulator in a\nsimulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's\nhuman-grounded nature, diversity, and realism make it valuable for embodied AI\nand robot learning research. Project website: https://behavior.stanford.edu.",
      "tldr_zh": "本研究引入 BEHAVIOR-1K，这是一个以人为本的 Embodied AI 基准测试，涵盖 1,000 个日常活动，并基于 50 个真实场景（如房屋、花园和办公室）标注超过 9,000 个对象的物理和语义属性。BEHAVIOR-1K 包括 OMNIGIBSON 模拟环境，该环境支持这些活动通过真实物理模拟和渲染，包括刚体、可变形体和液体。实验结果显示，这些活动涉及长时序复杂操作，对现有最先进的机器人学习解决方案仍构成挑战；此外，初步研究探讨了从模拟环境转移到真实世界的可行性，以缩小模拟与现实差距。该基准的多样性和真实性有望为 Embodied AI 和机器人学习研究提供宝贵资源。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "A preliminary version was published at 6th Conference on Robot\n  Learning (CoRL 2022)",
      "pdf_url": "http://arxiv.org/pdf/2403.09227v1",
      "published_date": "2024-03-14 09:48:36 UTC",
      "updated_date": "2024-03-14 09:48:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:37:19.205137"
    },
    {
      "arxiv_id": "2403.09751v1",
      "title": "What Was Your Prompt? A Remote Keylogging Attack on AI Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Roy Weiss",
        "Daniel Ayzenshteyn",
        "Guy Amit",
        "Yisroel Mirsky"
      ],
      "abstract": "AI assistants are becoming an integral part of society, used for asking\nadvice or help in personal and confidential issues. In this paper, we unveil a\nnovel side-channel that can be used to read encrypted responses from AI\nAssistants over the web: the token-length side-channel. We found that many\nvendors, including OpenAI and Microsoft, have this side-channel.\n  However, inferring the content of a response from a token-length sequence\nalone proves challenging. This is because tokens are akin to words, and\nresponses can be several sentences long leading to millions of grammatically\ncorrect sentences. In this paper, we show how this can be overcome by (1)\nutilizing the power of a large language model (LLM) to translate these\nsequences, (2) providing the LLM with inter-sentence context to narrow the\nsearch space and (3) performing a known-plaintext attack by fine-tuning the\nmodel on the target model's writing style.\n  Using these methods, we were able to accurately reconstruct 29\\% of an AI\nassistant's responses and successfully infer the topic from 55\\% of them. To\ndemonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and\nMicrosoft's Copilot on both browser and API traffic.",
      "tldr_zh": "本论文揭示了一种针对AI助手的远程侧信道攻击，即token-length side-channel，能够读取加密响应，许多供应商如OpenAI和Microsoft均受影响。研究通过利用LLM翻译token序列、提供句子间上下文缩小搜索空间，以及进行known-plaintext attack来微调模型，成功克服了从序列推断内容的挑战。实验结果显示，攻击准确重建了29%的AI助手响应，并从55%的响应中推断出主题，并在OpenAI的ChatGPT-4和Microsoft的Copilot上验证了其在浏览器和API流量中的有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09751v1",
      "published_date": "2024-03-14 09:38:12 UTC",
      "updated_date": "2024-03-14 09:38:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:37:30.356696"
    },
    {
      "arxiv_id": "2403.09215v1",
      "title": "On the Laplace Approximation as Model Selection Criterion for Gaussian Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Besginow",
        "Jan David Hüwel",
        "Thomas Pawellek",
        "Christian Beecks",
        "Markus Lange-Hegermann"
      ],
      "abstract": "Model selection aims to find the best model in terms of accuracy,\ninterpretability or simplicity, preferably all at once. In this work, we focus\non evaluating model performance of Gaussian process models, i.e. finding a\nmetric that provides the best trade-off between all those criteria. While\nprevious work considers metrics like the likelihood, AIC or dynamic nested\nsampling, they either lack performance or have significant runtime issues,\nwhich severely limits applicability. We address these challenges by introducing\nmultiple metrics based on the Laplace approximation, where we overcome a severe\ninconsistency occuring during naive application of the Laplace approximation.\nExperiments show that our metrics are comparable in quality to the gold\nstandard dynamic nested sampling without compromising for computational speed.\nOur model selection criteria allow significantly faster and high quality model\nselection of Gaussian process models.",
      "tldr_zh": "本研究探讨了高斯过程(Gaussian processes)模型的选择标准，旨在平衡准确性、可解释性和简单性。作者引入了基于 Laplace approximation 的多个指标，以解决现有方法如 likelihood、AIC 或 dynamic nested sampling 在性能或计算速度上的不足，并克服了 Laplace approximation 直接应用时的严重不一致问题。实验结果显示，这些新指标的质量与黄金标准 dynamic nested sampling 相当，但计算速度显著更快，从而实现了高斯过程模型的高效和高品质选择。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09215v1",
      "published_date": "2024-03-14 09:28:28 UTC",
      "updated_date": "2024-03-14 09:28:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:37:42.176107"
    },
    {
      "arxiv_id": "2403.09209v2",
      "title": "LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection",
      "title_zh": "LAN：用于实时内部威胁检测的学习自适应邻居",
      "authors": [
        "Xiangrui Cai",
        "Yang Wang",
        "Sihan Xu",
        "Hao Li",
        "Ying Zhang",
        "Zheli Liu",
        "Xiaojie Yuan"
      ],
      "abstract": "Enterprises and organizations are faced with potential threats from insider\nemployees that may lead to serious consequences. Previous studies on insider\nthreat detection (ITD) mainly focus on detecting abnormal users or abnormal\ntime periods (e.g., a week or a day). However, a user may have hundreds of\nthousands of activities in the log, and even within a day there may exist\nthousands of activities for a user, requiring a high investigation budget to\nverify abnormal users or activities given the detection results. On the other\nhand, existing works are mainly post-hoc methods rather than real-time\ndetection, which can not report insider threats in time before they cause loss.\nIn this paper, we conduct the first study towards real-time ITD at activity\nlevel, and present a fine-grained and efficient framework LAN. Specifically,\nLAN simultaneously learns the temporal dependencies within an activity sequence\nand the relationships between activities across sequences with graph structure\nlearning. Moreover, to mitigate the data imbalance problem in ITD, we propose a\nnovel hybrid prediction loss, which integrates self-supervision signals from\nnormal activities and supervision signals from abnormal activities into a\nunified loss for anomaly detection. We evaluate the performance of LAN on two\nwidely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative\nexperiments demonstrate the superiority of LAN, outperforming 9\nstate-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD\non CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to\npost-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in\nAUC on two datasets. Finally, the ablation study, parameter analysis, and\ncompatibility analysis evaluate the impact of each module and hyper-parameter\nin LAN. The source code can be obtained from https://github.com/Li1Neo/LAN.",
      "tldr_zh": "本研究针对内部威胁检测（ITD）中的挑战，提出首个实时活动级别检测框架LAN，以解决现有方法检测效率低和无法及时响应的问题。LAN通过图结构学习（graph structure learning）同时捕捉活动序列内的时间依赖性和跨序列活动关系，并引入混合预测损失（hybrid prediction loss）来缓解数据不平衡问题，结合自监督和监督信号进行异常检测。在CERT r4.2和r5.2数据集上，LAN在实时ITD中比9个最先进基线提升至少9.92%和6.35%的AUC；在事后ITD中也优于8个基线至少7.70%和4.03%的AUC。该框架展示了显著的鲁棒性和适用性，并提供了源代码以供进一步验证。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.09209v2",
      "published_date": "2024-03-14 09:22:17 UTC",
      "updated_date": "2024-03-17 14:05:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:37:54.113501"
    },
    {
      "arxiv_id": "2403.09206v1",
      "title": "Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM",
      "title_zh": "翻译失败",
      "authors": [
        "Naoki Hayashi",
        "Yoshihide Sawada"
      ],
      "abstract": "Concept Bottleneck Model (CBM) is a methods for explaining neural networks.\nIn CBM, concepts which correspond to reasons of outputs are inserted in the\nlast intermediate layer as observed values. It is expected that we can\ninterpret the relationship between the output and concept similar to linear\nregression. However, this interpretation requires observing all concepts and\ndecreases the generalization performance of neural networks. Partial CBM\n(PCBM), which uses partially observed concepts, has been devised to resolve\nthese difficulties. Although some numerical experiments suggest that the\ngeneralization performance of PCBMs is almost as high as that of the original\nneural networks, the theoretical behavior of its generalization error has not\nbeen yet clarified since PCBM is singular statistical model. In this paper, we\nreveal the Bayesian generalization error in PCBM with a three-layered and\nlinear architecture. The result indcates that the structure of partially\nobserved concepts decreases the Bayesian generalization error compared with\nthat of CBM (full-observed concepts).",
      "tldr_zh": "本文研究了 Partial Concept Bottleneck Model (PCBM) 的 Bayesian Generalization Error 上界，旨在解决传统 Concept Bottleneck Model (CBM) 在解释神经网络时因需观察所有概念而导致的泛化性能下降问题。作者通过分析一个三层线性架构的 PCBM，证明了部分观察概念的结构可以降低 Bayesian 泛化错误。结果显示，PCBM 在泛化性能上优于 naive CBM，为改进神经网络的可解释性和效率提供了理论基础。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.TH",
        "62F15, 62R01, 68T07"
      ],
      "primary_category": "stat.ML",
      "comment": "17 pages, 1 figure, submitted to TMLR",
      "pdf_url": "http://arxiv.org/pdf/2403.09206v1",
      "published_date": "2024-03-14 09:19:50 UTC",
      "updated_date": "2024-03-14 09:19:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:38:06.393059"
    },
    {
      "arxiv_id": "2403.09199v2",
      "title": "Task-Specific Adaptation of Segmentation Foundation Model via Prompt Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hyung-Il Kim",
        "Kimin Yun",
        "Jun-Seok Yun",
        "Yuseok Bae"
      ],
      "abstract": "Recently, foundation models trained on massive datasets to adapt to a wide\nrange of tasks have attracted considerable attention and are actively being\nexplored within the computer vision community. Among these, the Segment\nAnything Model (SAM) stands out for its remarkable progress in generalizability\nand flexibility for image segmentation tasks, achieved through prompt-based\nobject mask generation. However, despite its strength, SAM faces two key\nlimitations when applied to instance segmentation that segments specific\nobjects or those in unique environments (e.g., task-specific adaptation for\nout-of-distribution objects) not typically present in the training data: 1) the\nambiguity inherent in input prompts and 2) the necessity for extensive\nadditional training to achieve optimal segmentation. To address these\nchallenges, we propose a task-specific adaptation (i.e., customization) of the\nsegmentation foundation model via prompt learning tailored to SAM. Our method\ninvolves a prompt learning module (PLM), which adjusts input prompts into the\nembedding space to better align with peculiarities of the target task, thereby\nenabling more efficient training. Furthermore, we introduce a point matching\nmodule (PMM) to enhance the feature representation for finer segmentation by\nensuring detailed alignment with ground truth boundaries. Experimental results\non various customized segmentation scenarios demonstrate the effectiveness of\nthe proposed method.",
      "tldr_zh": "该研究针对Segment Anything Model (SAM)在特定实例分割任务中的局限性（如输入提示模糊性和需要大量额外训练），提出了一种通过prompt learning的任务特定适应方法。该方法引入Prompt Learning Module (PLM)，用于调整输入提示到嵌入空间，以更好地适应目标任务并提高训练效率；同时，Point Matching Module (PMM) 增强特征表示，确保与地面真实边界精确对齐，实现更精细的分割。在各种定制分割场景的实验中，该方法证明了其有效性，显著提升了分割性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Workshop on OOD Generalization in Computer Vision, ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09199v2",
      "published_date": "2024-03-14 09:13:51 UTC",
      "updated_date": "2024-10-11 04:37:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:38:17.057648"
    },
    {
      "arxiv_id": "2403.09193v2",
      "title": "Can We Talk Models Into Seeing the World Differently?",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Gavrikov",
        "Jovita Lukasik",
        "Steffen Jung",
        "Robert Geirhos",
        "M. Jehanzeb Mirza",
        "Margret Keuper",
        "Janis Keuper"
      ],
      "abstract": "Unlike traditional vision-only models, vision language models (VLMs) offer an\nintuitive way to access visual content through language prompting by combining\na large language model (LLM) with a vision encoder. However, both the LLM and\nthe vision encoder come with their own set of biases, cue preferences, and\nshortcuts, which have been rigorously studied in uni-modal models. A timely\nquestion is how such (potentially misaligned) biases and cue preferences behave\nunder multi-modal fusion in VLMs. As a first step towards a better\nunderstanding, we investigate a particularly well-studied vision-only bias -\nthe texture vs. shape bias and the dominance of local over global information.\nAs expected, we find that VLMs inherit this bias to some extent from their\nvision encoders. Surprisingly, the multi-modality alone proves to have\nimportant effects on the model behavior, i.e., the joint training and the\nlanguage querying change the way visual cues are processed. While this direct\nimpact of language-informed training on a model's visual perception is\nintriguing, it raises further questions on our ability to actively steer a\nmodel's output so that its prediction is based on particular visual cues of the\nuser's choice. Interestingly, VLMs have an inherent tendency to recognize\nobjects based on shape information, which is different from what a plain vision\nencoder would do. Further active steering towards shape-based classifications\nthrough language prompts is however limited. In contrast, active VLM steering\ntowards texture-based decisions through simple natural language prompts is\noften more successful.\n  URL: https://github.com/paulgavrikov/vlm_shapebias",
      "tldr_zh": "这篇论文探讨了视觉语言模型 (VLMs) 中的偏见问题，特别是纹理 vs. 形状偏见，以及局部 vs. 全局信息主导的影响。研究发现，VLMs 部分继承了视觉编码器的偏见，但多模态融合（如联合训练和语言查询）会改变视觉线索的处理方式，导致模型更倾向于基于形状信息识别对象。作者通过实验验证了主动引导 VLMs 的潜力，结果显示，通过简单语言提示更容易引导模型朝向纹理-based 决策，而朝向形状-based 分类的效果有限，这为操控 VLMs 的视觉感知提供了新见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.09193v2",
      "published_date": "2024-03-14 09:07:14 UTC",
      "updated_date": "2025-03-05 19:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:38:32.377122"
    },
    {
      "arxiv_id": "2403.09190v1",
      "title": "Intention-aware Denoising Diffusion Model for Trajectory Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Liu",
        "Shibo He",
        "Haoyu Liu",
        "Jiming Chen"
      ],
      "abstract": "Trajectory prediction is an essential component in autonomous driving,\nparticularly for collision avoidance systems. Considering the inherent\nuncertainty of the task, numerous studies have utilized generative models to\nproduce multiple plausible future trajectories for each agent. However, most of\nthem suffer from restricted representation ability or unstable training issues.\nTo overcome these limitations, we propose utilizing the diffusion model to\ngenerate the distribution of future trajectories. Two cruxes are to be settled\nto realize such an idea. First, the diversity of intention is intertwined with\nthe uncertain surroundings, making the true distribution hard to parameterize.\nSecond, the diffusion process is time-consuming during the inference phase,\nrendering it unrealistic to implement in a real-time driving system. We propose\nan Intention-aware denoising Diffusion Model (IDM), which tackles the above two\nproblems. We decouple the original uncertainty into intention uncertainty and\naction uncertainty and model them with two dependent diffusion processes. To\ndecrease the inference time, we reduce the variable dimensions in the\nintention-aware diffusion process and restrict the initial distribution of the\naction-aware diffusion process, which leads to fewer diffusion steps. To\nvalidate our approach, we conduct experiments on the Stanford Drone Dataset\n(SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with\nan FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY\ndataset. Compared with the original diffusion model, IDM reduces inference time\nby two-thirds. Interestingly, our experiments further reveal that introducing\nintention information is beneficial in modeling the diffusion process of fewer\nsteps.",
      "tldr_zh": "该论文针对自动驾驶中的轨迹预测问题，提出了一种Intention-aware denoising Diffusion Model (IDM)，通过将不确定性解耦为意图不确定性和动作不确定性，并使用两个相关的diffusion model过程来生成更精确的未来轨迹分布，从而解决现有模型的表示能力和训练稳定性问题。IDM通过降低变量维度和限制初始分布，显著减少了推理时间，使其适用于实时系统。实验在Stanford Drone Dataset (SDD)和ETH/UCY数据集上验证了该方法，实现了最先进性能，包括SDD上的FDE为13.83像素和ETH/UCY上的0.36米，同时推理时间比原始diffusion model减少了三分之二。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.09190v1",
      "published_date": "2024-03-14 09:05:25 UTC",
      "updated_date": "2024-03-14 09:05:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:38:44.223818"
    },
    {
      "arxiv_id": "2403.09184v4",
      "title": "Learning Algorithms for Verification of Markov Decision Processes",
      "title_zh": "马尔可夫决策过程的验证学习算法",
      "authors": [
        "Tomáš Brázdil",
        "Krishnendu Chatterjee",
        "Martin Chmelik",
        "Vojtěch Forejt",
        "Jan Křetínský",
        "Marta Kwiatkowska",
        "Tobias Meggendorfer",
        "David Parker",
        "Mateusz Ujma"
      ],
      "abstract": "We present a general framework for applying learning algorithms and\nheuristical guidance to the verification of Markov decision processes (MDPs).\nThe primary goal of our techniques is to improve performance by avoiding an\nexhaustive exploration of the state space, instead focussing on particularly\nrelevant areas of the system, guided by heuristics. Our work builds on the\nprevious results of Br{\\'{a}}zdil et al., significantly extending it as well as\nrefining several details and fixing errors.\n  The presented framework focuses on probabilistic reachability, which is a\ncore problem in verification, and is instantiated in two distinct scenarios.\nThe first assumes that full knowledge of the MDP is available, in particular\nprecise transition probabilities. It performs a heuristic-driven partial\nexploration of the model, yielding precise lower and upper bounds on the\nrequired probability. The second tackles the case where we may only sample the\nMDP without knowing the exact transition dynamics. Here, we obtain\nprobabilistic guarantees, again in terms of both the lower and upper bounds,\nwhich provides efficient stopping criteria for the approximation. In\nparticular, the latter is an extension of statistical model-checking (SMC) for\nunbounded properties in MDPs. In contrast to other related approaches, we do\nnot restrict our attention to time-bounded (finite-horizon) or discounted\nproperties, nor assume any particular structural properties of the MDP.",
      "tldr_zh": "该论文提出了一种通用框架，使用学习算法和启发式指导来验证 Markov Decision Processes (MDPs)，旨在通过聚焦于相关状态空间区域而非穷尽探索来提升性能，并扩展了 Brázdil 等人的先前工作。框架主要针对 probabilistic reachability 问题，在两个场景中实现：一是假设完全知道 MDP 的转移概率，进行启发式驱动的部分探索以获得精确概率下限和上限；二是仅通过采样 MDP 来获取概率保证，包括下限和上限，作为高效的近似停止标准。相比其他方法，该框架不限于时间有界或折扣属性，且不依赖 MDP 的特定结构，为 MDPs 验证提供了更灵活的 statistical model-checking (SMC) 扩展。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "82 pages. This is the TheoretiCS journal version",
      "pdf_url": "http://arxiv.org/pdf/2403.09184v4",
      "published_date": "2024-03-14 08:54:19 UTC",
      "updated_date": "2025-03-31 17:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:38:54.628668"
    },
    {
      "arxiv_id": "2403.10561v1",
      "title": "A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitris Spathis",
        "Aaqib Saeed",
        "Ali Etemad",
        "Sana Tonekaboni",
        "Stefanos Laskaridis",
        "Shohreh Deldari",
        "Chi Ian Tang",
        "Patrick Schwab",
        "Shyam Tailor"
      ],
      "abstract": "This non-archival index is not complete, as some accepted papers chose to\nopt-out of inclusion. The list of all accepted papers is available on the\nworkshop website.",
      "tldr_zh": "这篇论文是一个非档案索引，收集了AAAI 2024 Human-Centric Representation Learning工作坊的部分接受论文。该索引不完整，因为有些论文选择不参与包含。完整列表可通过工作坊网站获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.10561v1",
      "published_date": "2024-03-14 08:46:07 UTC",
      "updated_date": "2024-03-14 08:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:39:06.250444"
    },
    {
      "arxiv_id": "2403.09171v2",
      "title": "ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoliang Chen",
        "Zhihao Wu",
        "Ylli Sadikaj",
        "Claudia Plant",
        "Hong-Ning Dai",
        "Shiping Wang",
        "Yiu-Ming Cheung",
        "Wenzhong Guo"
      ],
      "abstract": "Although Graph Neural Networks (GNNs) have exhibited the powerful ability to\ngather graph-structured information from neighborhood nodes via various\nmessage-passing mechanisms, the performance of GNNs is limited by poor\ngeneralization and fragile robustness caused by noisy and redundant graph data.\nAs a prominent solution, Graph Augmentation Learning (GAL) has recently\nreceived increasing attention. Among prior GAL approaches, edge-dropping\nmethods that randomly remove edges from a graph during training are effective\ntechniques to improve the robustness of GNNs. However, randomly dropping edges\noften results in bypassing critical edges, consequently weakening the\neffectiveness of message passing. In this paper, we propose a novel adversarial\nedge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor\nguiding the removal of edges, which can be flexibly incorporated into diverse\nGNN backbones. Employing an adversarial training framework, the edge predictor\nutilizes the line graph transformed from the original graph to estimate the\nedges to be dropped, which improves the interpretability of the edge-dropping\nmethod. The proposed ADEdgeDrop is optimized alternately by stochastic gradient\ndescent and projected gradient descent. Comprehensive experiments on six graph\nbenchmark datasets demonstrate that the proposed ADEdgeDrop outperforms\nstate-of-the-art baselines across various GNN backbones, demonstrating improved\ngeneralization and robustness.",
      "tldr_zh": "该研究针对Graph Neural Networks (GNNs) 在处理噪声和冗余图数据时存在的泛化性和鲁棒性问题，提出了一种新型对抗性边删除方法ADEdgeDrop。ADEdgeDrop利用对抗性边预测器，通过线图（line graph）转换和对抗训练框架来指导关键边的选择性删除，从而避免随机删除方法可能忽略重要边的问题。该方法可以灵活整合到各种GNN骨干网络中，并通过随机梯度下降和投影梯度下降进行优化。在六个图基准数据集上的实验表明，ADEdgeDrop在泛化和鲁棒性方面优于现有基线方法，显著提升了GNNs的表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09171v2",
      "published_date": "2024-03-14 08:31:39 UTC",
      "updated_date": "2024-08-14 09:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:39:20.186712"
    },
    {
      "arxiv_id": "2403.09142v2",
      "title": "USimAgent: Large Language Models for Simulating Search Users",
      "title_zh": "USimAgent：大型语言模型用于模拟搜索用户",
      "authors": [
        "Erhan Zhang",
        "Xingzhu Wang",
        "Peiyuan Gong",
        "Yankai Lin",
        "Jiaxin Mao"
      ],
      "abstract": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent.",
      "tldr_zh": "这篇论文介绍了 USimAgent，一种基于 Large Language Models (LLMs) 的用户搜索行为模拟器，用于评估信息检索系统的用户行为。USimAgent 能够模拟用户的查询、点击和停止行为，从而生成完整的搜索会话，以应对传统模拟方法在处理复杂认知过程（如学习、推理和规划）时的挑战。在真实用户行为数据集上的实验显示，该模拟器在查询生成方面优于现有方法，在预测点击和停止行为方面与传统方法相当。这些结果验证了 LLMs 在用户 simulation 中的潜力，并为开发更鲁棒的通用模拟器提供了新启示。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09142v2",
      "published_date": "2024-03-14 07:40:54 UTC",
      "updated_date": "2024-10-29 09:13:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:39:32.297029"
    },
    {
      "arxiv_id": "2403.09141v1",
      "title": "Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Gleb Radchenko",
        "Victoria Andrea Fill"
      ],
      "abstract": "Initially considered as low-power units with limited autonomous processing,\nEdge IoT devices have seen a paradigm shift with the introduction of FPGAs and\nAI accelerators. This advancement has vastly amplified their computational\ncapabilities, emphasizing the practicality of edge AI. Such progress introduces\nnew challenges of optimizing AI tasks for the limitations of energy and network\nresources typical in Edge computing environments. Our study explores methods\nthat enable distributed data processing through AI-enabled edge devices,\nenhancing collaborative learning capabilities. A key focus of our research is\nthe challenge of determining confidence levels in learning outcomes,\nconsidering the spatial and temporal variability of data sets encountered by\nindependent agents. To address this issue, we investigate the application of\nBayesian neural networks, proposing a novel approach to manage uncertainty in\ndistributed learning environments.",
      "tldr_zh": "本研究探讨了AI启用的边缘设备在多智能体分布式学习中的不确定性估计问题，针对边缘计算环境中能源和网络资源的限制，优化AI任务的分布式数据处理。研究重点在于评估学习结果的信心水平，特别是处理独立智能体面对的数据空间和时间变异性。作者提出了一种新方法，使用Bayesian neural networks来管理不确定性，从而提升协作学习的能力。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "I.2.11"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09141v1",
      "published_date": "2024-03-14 07:40:32 UTC",
      "updated_date": "2024-03-14 07:40:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:39:42.968933"
    },
    {
      "arxiv_id": "2403.10559v1",
      "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Shu",
        "Zhouyao Zhu"
      ],
      "abstract": "This report investigates the history and impact of Generative Models and\nConnected and Automated Vehicles (CAVs), two groundbreaking forces pushing\nprogress in technology and transportation. By focusing on the application of\ngenerative models within the context of CAVs, the study aims to unravel how\nthis integration could enhance predictive modeling, simulation accuracy, and\ndecision-making processes in autonomous vehicles. This thesis discusses the\nbenefits and challenges of integrating generative models and CAV technology in\ntransportation. It aims to highlight the progress made, the remaining\nobstacles, and the potential for advancements in safety and innovation.",
      "tldr_zh": "这篇调查报告探讨了生成模型和连接自动车辆(CAVs)在交通和AI领域的交叉点，重点分析生成模型如何提升CAVs的预测建模、模拟准确性和决策过程。报告回顾了这些技术的历史影响，并讨论了整合的好处，如提高安全和创新潜力，同时指出了面临的挑战和剩余障碍。总体而言，它突出了这一融合在推动交通技术进步方面的进展和未来机遇。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.10559v1",
      "published_date": "2024-03-14 06:51:26 UTC",
      "updated_date": "2024-03-14 06:51:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:39:55.114587"
    },
    {
      "arxiv_id": "2403.09131v5",
      "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between Professional and Non-Professional Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Chang Zong",
        "Yuyan Chen",
        "Weiming Lu",
        "Jian Shao",
        "Yongfeng Huang",
        "Heng Chang",
        "Yueting Zhuang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including question answering and controlled text generation.\nHowever, studies into their ability to switch between opposite styles of\nresponses in professional domains remain underexplored. This study introduces a\nnovel approach, named ProSwitch, which enables a language model to switch\nbetween professional and non-professional answers, by tuning and evaluating\nthrough the guidance of domain and style knowledge. ProSwitch unfolds in three\nphases: LLM-augmented preparation to collect domain knowledge and QA pairs,\ninstruction tuning to optimize LLMs with multiple levels of knowledge, and\ncomprehensive evaluation to assess both style discrimination and\nreference-based quality of the generated text. Comparative analysis of\nProSwitch against general and specialized LLMs reveals that our approach\noutperforms baselines in switching between professional and non-professional\nresponses.",
      "tldr_zh": "本研究提出ProSwitch，一种基于知识引导的指令微调方法，旨在让大型语言模型(LLMs)能够在专业和非专业响应之间灵活切换，以填补现有研究的空白。方法分为三个阶段：首先，通过LLM-augmented preparation收集领域知识和QA对；其次，进行instruction tuning以多层次知识优化模型；最后，进行全面evaluation，评估风格区分和生成文本的质量。实验结果显示，ProSwitch相较于一般和专业化LLMs，在响应切换性能上表现出显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages main body, 16 pages total",
      "pdf_url": "http://arxiv.org/pdf/2403.09131v5",
      "published_date": "2024-03-14 06:49:16 UTC",
      "updated_date": "2024-12-12 09:22:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:40:08.957950"
    },
    {
      "arxiv_id": "2403.09750v1",
      "title": "Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoqun Li",
        "Hongyu Lin",
        "Yaojie Lu",
        "Hao Xiang",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Declarative knowledge and procedural knowledge are two key parts in\nmeta-cognitive theory, and these two hold significant importance in\npre-training and inference of LLMs. However, a comprehensive analysis comparing\nthese two types of knowledge is lacking, primarily due to challenges in\ndefinition, probing and quantitative assessment. In this paper, we explore from\na new perspective by providing ground-truth knowledge for LLMs and evaluating\nthe effective score. Through extensive experiments with widely-used datasets\nand models, we get conclusions: (1) In most tasks, benefits from declarative\nknowledge are greater than those from procedural knowledge. (2) Profits of\nprocedural knowledge are larger than declarative knowledge only in reasoning\ntasks with simple logic. (3) As pre-training progresses and size increases,\nmodel ability to utilize both kinds of knowledge significantly improves, but in\ndifferent speed. We do detailed analysis for the findings and this can provide\nprimary guidance for evaluation and enhancement of large language models.",
      "tldr_zh": "本文通过一种新视角评估大型语言模型（LLMs）中的声明性知识（declarative knowledge）和程序性知识（procedural knowledge），方法包括提供ground-truth知识并进行定量评估，以解决现有分析的挑战。实验结果表明，在大多数任务中，声明性知识的益处大于程序性知识，而程序性知识仅在简单逻辑的推理任务中更具优势。随着模型预训练进展和规模增大，LLMs对两种知识的利用能力显著改善，但改善速度不同。该研究为LLMs的评估和增强提供了初步指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by LREC-COLING 2024 as a short paper",
      "pdf_url": "http://arxiv.org/pdf/2403.09750v1",
      "published_date": "2024-03-14 05:34:35 UTC",
      "updated_date": "2024-03-14 05:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:40:21.208631"
    },
    {
      "arxiv_id": "2403.09113v2",
      "title": "AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiyi Zhang",
        "Rushi Qiang",
        "Sai Ashish Somayajula",
        "Pengtao Xie"
      ],
      "abstract": "Large-scale pretraining followed by task-specific finetuning has achieved\ngreat success in various NLP tasks. Since finetuning all parameters of large\npretrained models poses substantial computational and memory challenges,\nseveral efficient finetuning methods have been developed. Among them, low-rank\nadaptation (LoRA), which finetunes low-rank incremental update matrices on top\nof frozen pretrained weights, has proven particularly effective. Nonetheless,\nLoRA's uniform rank assignment across all layers, along with its reliance on an\nexhaustive search to find the best rank, leads to high computation costs and\nsuboptimal finetuning performance. To address these limitations, we introduce\nAutoLoRA, a meta learning based framework for automatically identifying the\noptimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a\nlow-rank update matrix with a selection variable, which determines whether the\nrank-1 matrix should be discarded. A meta learning based method is developed to\nlearn these selection variables. The optimal rank is determined by thresholding\nthe values of these variables. Our comprehensive experiments on natural\nlanguage understanding, generation, and sequence labeling demonstrate the\neffectiveness of AutoLoRA.",
      "tldr_zh": "该论文提出 AutoLoRA，一种基于 Meta Learning 的框架，用于自动调整 Low-Rank Adaptation (LoRA) 中每个层的矩阵秩，以解决 LoRA 统一秩分配和穷举搜索带来的高计算成本及性能问题。AutoLoRA 通过为每个秩-1 矩阵关联一个选择变量，并利用元学习方法学习这些变量的值，然后通过阈值确定最优秩，从而实现高效的模型微调。实验结果显示，在自然语言理解、生成和序列标注任务上，AutoLoRA 显著提升了微调性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09113v2",
      "published_date": "2024-03-14 05:29:35 UTC",
      "updated_date": "2024-03-17 17:55:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:40:32.015358"
    },
    {
      "arxiv_id": "2403.09749v1",
      "title": "Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings",
      "title_zh": "翻译失败",
      "authors": [
        "Jihyeon Seong",
        "Jungmin Kim",
        "Jaesik Choi"
      ],
      "abstract": "In Time Series Classification (TSC), temporal pooling methods that consider\nsequential information have been proposed. However, we found that each temporal\npooling has a distinct mechanism, and can perform better or worse depending on\ntime series data. We term this fixed pooling mechanism a single perspective of\ntemporal poolings. In this paper, we propose a novel temporal pooling method\nwith diverse perspective learning: Selection over Multiple Temporal Poolings\n(SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among\nmultiple methods for each data by attention. The dynamic pooling selection is\nmotivated by the ensemble concept of Multiple Choice Learning (MCL), which\nselects the best among multiple outputs. The pooling selection by SoM-TP's\nattention enables a non-iterative pooling ensemble within a single classifier.\nAdditionally, we define a perspective loss and Diverse Perspective Learning\nNetwork (DPLN). The loss works as a regularizer to reflect all the pooling\nperspectives from DPLN. Our perspective analysis using Layer-wise Relevance\nPropagation (LRP) reveals the limitation of a single perspective and ultimately\ndemonstrates diverse perspective learning of SoM-TP. We also show that SoM-TP\noutperforms CNN models based on other temporal poolings and state-of-the-art\nmodels in TSC with extensive UCR/UEA repositories.",
      "tldr_zh": "本论文针对时间序列分类 (TSC) 中单一临时池化方法的局限性，提出了一种新型方法 Selection over Multiple Temporal Poolings (SoM-TP)，通过注意力机制动态选择多个临时池化方法中的最佳选项，以实现多样视角学习。SoM-TP 借鉴 Multiple Choice Learning (MCL) 的概念，在单个分类器内实现非迭代池化集成，并引入视角损失和 Diverse Perspective Learning Network (DPLN) 作为正则化器，使用 Layer-wise Relevance Propagation (LRP) 分析揭示单一视角的不足。实验结果显示，SoM-TP 在 UCR/UEA 数据集上优于其他基于 CNN 的临时池化模型和最先进方法，证明了其在提升 TSC 性能方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.09749v1",
      "published_date": "2024-03-14 05:02:00 UTC",
      "updated_date": "2024-03-14 05:02:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:40:44.118770"
    },
    {
      "arxiv_id": "2403.09092v2",
      "title": "MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection",
      "title_zh": "MCFEND：中文假新闻检测的多源基准数据集",
      "authors": [
        "Yupeng Li",
        "Haorui He",
        "Jin Bai",
        "Dacheng Wen"
      ],
      "abstract": "The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.",
      "tldr_zh": "本研究指出了现有中文假新闻检测数据集仅限于微博来源，导致模型在多来源场景下泛化性差，实验显示 F1 score 从 0.943 降至 0.470，无法识别超过三分之一的多来源假新闻。  \n为解决这一问题，研究者构建了首个多来源基准数据集 MCFEND，涵盖社交平台、消息应用和传统新闻网站，并由 14 个权威事实检查机构验证。  \n在 MCFEND 上评估了现有方法，包括跨来源、多来源和未见来源的设置，旨在提升中文假新闻检测在真实世界场景中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by the ACM Web Conference 2024 (WWW 2024) oral, dataset\n  available: https://github.com/TrustworthyComp",
      "pdf_url": "http://arxiv.org/pdf/2403.09092v2",
      "published_date": "2024-03-14 04:32:13 UTC",
      "updated_date": "2024-07-24 05:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:40:56.413831"
    },
    {
      "arxiv_id": "2403.09085v2",
      "title": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance",
      "title_zh": "有意义的学习：通过通用事实指导增强大型语言模型中的抽象推理",
      "authors": [
        "Kai Xiong",
        "Xiao Ding",
        "Ting Liu",
        "Bing Qin",
        "Dongliang Xu",
        "Qing Yang",
        "Hongtao Liu",
        "Yixin Cao"
      ],
      "abstract": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn.",
      "tldr_zh": "这项研究发现，大型语言模型(LLMs)在处理基于通用事实的简单问题时，常因抽象推理能力不足而难以提供一致准确的答案，从而引发了LLMs是否真正推理还是仅靠记忆的争论。作者设计了一个初步研究来量化LLMs的抽象推理性能，并构建了抽象推理数据集(AbsR)以及一个有意义的学习范式，帮助LLMs学习如何有效利用通用事实进行推理。实验结果显示，该方法不仅提升了LLMs的一般推理性能，还显著增强了其抽象推理能力，促进了对通用事实的更深层理解和应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09085v2",
      "published_date": "2024-03-14 04:06:13 UTC",
      "updated_date": "2024-11-11 11:35:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:41:09.065764"
    },
    {
      "arxiv_id": "2403.09072v1",
      "title": "UniCode: Learning a Unified Codebook for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sipeng Zheng",
        "Bohan Zhou",
        "Yicheng Feng",
        "Ye Wang",
        "Zongqing Lu"
      ],
      "abstract": "In this paper, we propose \\textbf{UniCode}, a novel approach within the\ndomain of multimodal large language models (MLLMs) that learns a unified\ncodebook to efficiently tokenize visual, text, and potentially other types of\nsignals. This innovation addresses a critical limitation in existing MLLMs:\ntheir reliance on a text-only codebook, which restricts MLLM's ability to\ngenerate images and texts in a multimodal context. Towards this end, we propose\na language-driven iterative training paradigm, coupled with an in-context\npre-training task we term ``image decompression'', enabling our model to\ninterpret compressed visual data and generate high-quality images.The unified\ncodebook empowers our model to extend visual instruction tuning to\nnon-linguistic generation tasks. Moreover, UniCode is adaptable to diverse\nstacked quantization approaches in order to compress visual signals into a more\ncompact token representation. Despite using significantly fewer parameters and\nless data during training, Unicode demonstrates promising capabilities in\nvisual reconstruction and generation. It also achieves performances comparable\nto leading MLLMs across a spectrum of VQA benchmarks.",
      "tldr_zh": "本研究提出UniCode，一种用于多模态大语言模型(MLLMs)的创新方法，通过学习一个统一的codebook来高效tokenize视觉、文本和其他信号，从而解决现有MLLMs依赖文本-only codebook的限制，无法在多模态上下文中生成图像和文本。UniCode采用语言驱动的迭代训练范式和“image decompression”预训练任务，使模型能够解释压缩视觉数据并生成高质量图像，同时扩展视觉指令微调到非语言生成任务。实验结果显示，尽管使用更少的参数和数据，UniCode在视觉重建、生成和VQA基准上表现出色，与领先MLLMs的性能相当。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 2 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.09072v1",
      "published_date": "2024-03-14 03:29:58 UTC",
      "updated_date": "2024-03-14 03:29:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:41:20.806795"
    },
    {
      "arxiv_id": "2403.09063v1",
      "title": "Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery",
      "title_zh": "翻译失败",
      "authors": [
        "Jerrin Bright",
        "Bavesh Balaji",
        "Harish Prakash",
        "Yuhao Chen",
        "David A Clausi",
        "John Zelek"
      ],
      "abstract": "Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable\nchallenge and is often hindered by depth ambiguities and reduced precision.\nExisting works resort to either pose priors or multi-modal data such as\nmulti-view or point cloud information, though their methods often overlook the\nvaluable scene-depth information inherently present in a single image.\nMoreover, achieving robust HMR for out-of-distribution (OOD) data is\nexceedingly challenging due to inherent variations in pose, shape and depth.\nConsequently, understanding the underlying distribution becomes a vital\nsubproblem in modeling human forms. Motivated by the need for unambiguous and\nrobust human modeling, we introduce Distribution and depth-aware human mesh\nrecovery (D2A-HMR), an end-to-end transformer architecture meticulously\ndesigned to minimize the disparity between distributions and incorporate\nscene-depth leveraging prior depth information. Our approach demonstrates\nsuperior performance in handling OOD data in certain scenarios while\nconsistently achieving competitive results against state-of-the-art HMR methods\non controlled datasets.",
      "tldr_zh": "该论文针对 3D Human Mesh Recovery (HMR) 在野外数据中面临的深度模糊和精度问题，提出了一种端到端的 Distribution and Depth-Aware Transformers (D2A-HMR) 架构。该方法通过最小化分布差异并整合单张图像中的场景深度信息，增强对分布外 (OOD) 数据的鲁棒性，同时避免依赖传统姿势先验或多模态数据。实验结果表明，D2A-HMR 在某些场景下优于现有方法，并在控制数据集上实现与最先进 HMR 技术相当的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to 21st International Conference on Robots and Vision\n  (CRV'24), Guelph, Ontario, Canada",
      "pdf_url": "http://arxiv.org/pdf/2403.09063v1",
      "published_date": "2024-03-14 03:07:58 UTC",
      "updated_date": "2024-03-14 03:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:41:33.507483"
    },
    {
      "arxiv_id": "2403.09057v3",
      "title": "A Continued Pretrained LLM Approach for Automatic Medical Note Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Yuan",
        "Eti Rastogi",
        "Gautam Naik",
        "Sree Prasanna Rajagopal",
        "Sagar Goyal",
        "Fen Zhao",
        "Bharath Chintagunta",
        "Jeff Ward"
      ],
      "abstract": "LLMs are revolutionizing NLP tasks. However, the use of the most advanced\nLLMs, such as GPT-4, is often prohibitively expensive for most specialized\nfields. We introduce HEAL, the first continuously trained 13B LLaMA2-based LLM\nthat is purpose-built for medical conversations and measured on automated\nscribing. Our results demonstrate that HEAL outperforms GPT-4 and PMC-LLaMA in\nPubMedQA, with an accuracy of 78.4\\%. It also achieves parity with GPT-4 in\ngenerating medical notes. Remarkably, HEAL surpasses GPT-4 and Med-PaLM 2 in\nidentifying more correct medical concepts and exceeds the performance of human\nscribes and other comparable models in correctness and completeness.",
      "tldr_zh": "该研究提出了一种基于持续预训练的LLM方法，引入了HEAL模型，这是一个13B LLaMA2基础的语言模型，专门针对医疗对话和自动医疗笔记生成，以降低高级模型如GPT-4的成本。HEAL在PubMedQA测试中达到78.4%的准确率，优于GPT-4和PMC-LLaMA，并在医疗笔记生成上与GPT-4持平。实验结果显示，HEAL在识别医疗概念方面超过GPT-4和Med-PaLM 2，并在正确性和完整性上超越人类抄写员和其他模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09057v3",
      "published_date": "2024-03-14 02:55:37 UTC",
      "updated_date": "2024-04-03 18:08:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:41:45.327738"
    },
    {
      "arxiv_id": "2403.09054v2",
      "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Adnan",
        "Akhil Arunkumar",
        "Gaurav Jain",
        "Prashant J. Nair",
        "Ilya Soloveychik",
        "Purushotham Kamath"
      ],
      "abstract": "Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy.",
      "tldr_zh": "这篇论文提出了Keyformer，一种创新的推理时方法，用于减少生成式语言模型的KV Cache大小，从而缓解内存带宽瓶颈问题。Keyformer基于观察到约90%的attention weight集中在特定“key” tokens上，通过一个新颖的评分函数选择并保留这些关键tokens，实现KV Cache的优化，而不影响模型准确性。在GPT-J、Cerebras-GPT和MPT模型上进行评估，结果显示推理延迟降低2.1倍，生成令牌吞吐量提高2.4倍，尤其在总结和对话任务中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.CL",
        "68U35",
        "I.2.7; C.0"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09054v2",
      "published_date": "2024-03-14 02:42:42 UTC",
      "updated_date": "2024-04-06 00:22:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:41:58.027306"
    },
    {
      "arxiv_id": "2403.09053v2",
      "title": "Towards a theory of model distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Enric Boix-Adsera"
      ],
      "abstract": "Distillation is the task of replacing a complicated machine learning model\nwith a simpler model that approximates the original [BCNM06,HVD15]. Despite\nmany practical applications, basic questions about the extent to which models\ncan be distilled, and the runtime and amount of data needed to distill, remain\nlargely open.\n  To study these questions, we initiate a general theory of distillation,\ndefining PAC-distillation in an analogous way to PAC-learning [Val84]. As\napplications of this theory: (1) we propose new algorithms to extract the\nknowledge stored in the trained weights of neural networks -- we show how to\nefficiently distill neural networks into succinct, explicit decision tree\nrepresentations when possible by using the ``linear representation\nhypothesis''; and (2) we prove that distillation can be much cheaper than\nlearning from scratch, and make progress on characterizing its complexity.",
      "tldr_zh": "本研究探讨了模型蒸馏（model distillation）的理论基础，旨在用更简单的模型近似复杂的机器学习模型，同时解决模型蒸馏的程度、所需运行时间和数据量的关键问题。作者定义了PAC-distillation，类似于PAC-learning的概念，作为一个通用理论框架，并提出新算法从神经网络的训练权重中提取知识，利用“线性表示假设”将神经网络高效转化为简洁的决策树（decision tree）表示。结果表明，蒸馏比从零开始学习更高效，并在复杂性表征上取得了进展，为模型压缩和知识提取提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "46 pages, 5 figures. Please reach out with comments! Feedback is\n  welcome",
      "pdf_url": "http://arxiv.org/pdf/2403.09053v2",
      "published_date": "2024-03-14 02:42:19 UTC",
      "updated_date": "2024-05-04 19:52:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:42:09.152908"
    },
    {
      "arxiv_id": "2403.09039v2",
      "title": "Detecting Anomalies in Dynamic Graphs via Memory enhanced Normality",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Liu",
        "Xuequn Shang",
        "Xiaolin Han",
        "Kai Zheng",
        "Hongzhi Yin"
      ],
      "abstract": "Anomaly detection in dynamic graphs presents a significant challenge due to\nthe temporal evolution of graph structures and attributes. The conventional\napproaches that tackle this problem typically employ an unsupervised learning\nframework, capturing normality patterns with exclusive normal data during\ntraining and identifying deviations as anomalies during testing. However, these\nmethods face critical drawbacks: they either only depend on proxy tasks for\nrepresentation without directly pinpointing normal patterns, or they neglect to\ndifferentiate between spatial and temporal normality patterns. More recent\nmethods that use contrastive learning with negative sampling also face high\ncomputational costs, limiting their scalability to large graphs. To address\nthese challenges, we introduce a novel Spatial-Temporal memories-enhanced graph\nautoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)\nand gated temporal convolution layers to extract spatial and temporal features.\nThen STRIPE incorporates separate spatial and temporal memory networks to\ncapture and store prototypes of normal patterns, respectively. These stored\npatterns are retrieved and integrated with encoded graph embeddings through a\nmutual attention mechanism. Finally, the integrated features are fed into the\ndecoder to reconstruct the graph streams which serve as the proxy task for\nanomaly detection. This comprehensive approach not only minimizes\nreconstruction errors but also emphasizes the compactness and distinctiveness\nof the embeddings w.r.t. the nearest memory prototypes. Extensive experiments\non six benchmark datasets demonstrate the effectiveness and efficiency of\nSTRIPE, where STRIPE significantly outperforms existing methods with 5.8%\nimprovement in AUC scores and 4.62X faster in training time.",
      "tldr_zh": "这篇论文针对动态图异常检测的挑战，提出了一种新型框架STRIPE（Spatial-Temporal memories-enhanced graph autoencoder），它利用Graph Neural Networks (GNNs)和门控时间卷积层提取空间和时间特征，并通过独立的空间和时间记忆网络捕获正常模式原型，以增强嵌入的紧凑性和独特性。STRIPE 通过相互注意力机制整合这些原型与图嵌入，并使用解码器重建图流作为异常检测的代理任务，从而直接识别正常模式并减少计算开销。在六个基准数据集上的实验表明，STRIPE 比现有方法提高了 5.8% 的 AUC 分数，并将训练时间缩短了 4.62 倍，展示了其高效性和优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09039v2",
      "published_date": "2024-03-14 02:26:10 UTC",
      "updated_date": "2024-08-15 02:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:42:22.617779"
    },
    {
      "arxiv_id": "2403.09029v1",
      "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Hugo Laurençon",
        "Léo Tronchon",
        "Victor Sanh"
      ],
      "abstract": "Using vision-language models (VLMs) in web development presents a promising\nstrategy to increase efficiency and unblock no-code solutions: by providing a\nscreenshot or a sketch of a UI, a VLM could generate the code to reproduce it,\nfor instance in a language like HTML. Despite the advancements in VLMs for\nvarious tasks, the specific challenge of converting a screenshot into a\ncorresponding HTML has been minimally explored. We posit that this is mainly\ndue to the absence of a suitable, high-quality dataset. This work introduces\nWebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and\ntheir corresponding screenshots. We fine-tune a foundational VLM on our dataset\nand show proficiency in converting webpage screenshots to functional HTML code.\nTo accelerate the research in this area, we open-source WebSight.",
      "tldr_zh": "该研究探讨了使用视觉语言模型(VLMs)将网页截图转换为HTML代码，以提升网页开发效率并支持无代码解决方案。论文引入了WebSight数据集，该数据集包含200万对合成HTML代码和对应截图，旨在解决这一领域缺乏高质量数据集的痛点。通过在WebSight上微调基础VLM，模型展示了在将网页截图转化为功能HTML代码方面的出色性能。为加速相关研究，论文开源了WebSight数据集。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09029v1",
      "published_date": "2024-03-14 01:40:40 UTC",
      "updated_date": "2024-03-14 01:40:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:42:33.209964"
    },
    {
      "arxiv_id": "2403.09024v1",
      "title": "Semiparametric Token-Sequence Co-Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunji Lee",
        "Doyoung Kim",
        "Jihoon Jun",
        "Sejune Joo",
        "Joel Jang",
        "Kyoung-Woon On",
        "Minjoon Seo"
      ],
      "abstract": "In this work, we introduce a semiparametric token-sequence co-supervision\ntraining method. It trains a language model by simultaneously leveraging\nsupervision from the traditional next token prediction loss which is calculated\nover the parametric token embedding space and the next sequence prediction loss\nwhich is calculated over the nonparametric sequence embedding space. The\nnonparametric sequence embedding space is constructed by a separate language\nmodel tasked to condense an input text into a single representative embedding.\nOur experiments demonstrate that a model trained via both supervisions\nconsistently surpasses models trained via each supervision independently.\nAnalysis suggests that this co-supervision encourages a broader generalization\ncapability across the model. Especially, the robustness of parametric token\nspace which is established during the pretraining step tends to effectively\nenhance the stability of nonparametric sequence embedding space, a new space\nestablished by another language model.",
      "tldr_zh": "本文提出了一种半参数化标记序列共同监督（Semiparametric Token-Sequence Co-Supervision）训练方法，该方法通过同时利用参数化标记嵌入空间的下一个标记预测损失（next token prediction loss）和非参数化序列嵌入空间的下一个序列预测损失（next sequence prediction loss）来训练语言模型，其中非参数化空间由另一个语言模型构建以浓缩输入文本。实验结果显示，使用这种共同监督的模型在性能上 consistently surpasses 单独使用每一种监督的模型。分析表明，这种方法促进了模型的更广泛泛化能力，特别是参数化标记空间的稳健性有效地增强了非参数化序列嵌入空间（nonparametric sequence embedding space）的稳定性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09024v1",
      "published_date": "2024-03-14 01:28:13 UTC",
      "updated_date": "2024-03-14 01:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:42:46.316731"
    },
    {
      "arxiv_id": "2405.12983v1",
      "title": "Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer",
      "title_zh": "翻译失败",
      "authors": [
        "Maxime Burchi",
        "Krishna C. Puvvada",
        "Jagadeesh Balam",
        "Boris Ginsburg",
        "Radu Timofte"
      ],
      "abstract": "Humans are adept at leveraging visual cues from lip movements for recognizing\nspeech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR)\nmodels follow similar approach to achieve robust speech recognition in noisy\nconditions. In this work, we present a multilingual AVSR model incorporating\nseveral enhancements to improve performance and audio noise robustness.\nNotably, we adapt the recently proposed Fast Conformer model to process both\naudio and visual modalities using a novel hybrid CTC/RNN-T architecture. We\nincrease the amount of audio-visual training data for six distinct languages,\ngenerating automatic transcriptions of unlabelled multilingual datasets\n(VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art\nperformance on the LRS3 dataset, reaching WER of 0.8%. On the recently\nintroduced MuAViC benchmark, our model yields an absolute average-WER reduction\nof 11.9% in comparison to the original baseline. Finally, we demonstrate the\nability of the proposed model to perform audio-only, visual-only, and\naudio-visual speech recognition at test time.",
      "tldr_zh": "本文提出一个多语言音频-视觉语音识别（AVSR）模型，使用混合 CTC/RNN-T 架构改编 Fast Conformer 模型，以同时处理音频和视觉模态，并通过增加六种语言的训练数据（如 VoxCeleb2 和 AVSpeech 的自动转录）提升性能和抗噪声能力。模型的关键创新在于融合视觉线索来实现更鲁棒的语音识别，尤其在嘈杂环境中。实验结果显示，该模型在 LRS3 数据集上达到 0.8% 的 WER，新状态-of-the-art 水平，并在 MuAViC 基准上比原基线降低了 11.9% 的平均 WER。最后，模型支持测试时的音频-only、视觉-only 和音频-视觉识别模式。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12983v1",
      "published_date": "2024-03-14 01:16:32 UTC",
      "updated_date": "2024-03-14 01:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:42:58.982846"
    },
    {
      "arxiv_id": "2403.09747v1",
      "title": "Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors",
      "title_zh": "翻译失败",
      "authors": [
        "Guanghua Li",
        "Wensheng Lu",
        "Wei Zhang",
        "Defu Lian",
        "Kezhong Lu",
        "Rui Mao",
        "Kai Shu",
        "Hao Liao"
      ],
      "abstract": "The proliferation of fake news has had far-reaching implications on politics,\nthe economy, and society at large. While Fake news detection methods have been\nemployed to mitigate this issue, they primarily depend on two essential\nelements: the quality and relevance of the evidence, and the effectiveness of\nthe verdict prediction mechanism. Traditional methods, which often source\ninformation from static repositories like Wikipedia, are limited by outdated or\nincomplete data, particularly for emerging or rare claims. Large Language\nModels (LLMs), known for their remarkable reasoning and generative\ncapabilities, introduce a new frontier for fake news detection. However, like\ntraditional methods, LLM-based solutions also grapple with the limitations of\nstale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently\nstruggle with issues such as low-quality evidence retrieval and context length\nconstraints. To address these challenges, we introduce a novel,\nretrieval-augmented LLMs framework--the first of its kind to automatically and\nstrategically extract key evidence from web sources for claim verification.\nEmploying a multi-round retrieval strategy, our framework ensures the\nacquisition of sufficient, relevant evidence, thereby enhancing performance.\nComprehensive experiments across three real-world datasets validate the\nframework's superiority over existing methods. Importantly, our model not only\ndelivers accurate verdicts but also offers human-readable explanations to\nimprove result interpretability.",
      "tldr_zh": "该研究提出了一种多轮检索增强大型语言模型（Multi-round Retrieval-augmented Large Language Models）的框架，用于假新闻检测，以解决传统方法依赖过时静态数据（如Wikipedia）和LLMs的知识局限性问题。该框架首次实现自动从网络来源提取关键证据，并通过多轮检索策略确保证据的充分性和相关性，从而提升检测性能。在三个真实数据集上的实验显示，该框架优于现有方法，不仅提供准确的判断结果，还生成可读的人类解释，提高了结果的可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09747v1",
      "published_date": "2024-03-14 00:35:39 UTC",
      "updated_date": "2024-03-14 00:35:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T15:43:11.346930"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 111,
  "processed_papers_count": 111,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T15:43:35.302846"
}
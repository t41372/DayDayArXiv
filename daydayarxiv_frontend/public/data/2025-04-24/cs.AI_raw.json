[
  {
    "arxiv_id": "2504.17771v1",
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "abstract": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/",
    "pdf_url": "http://arxiv.org/pdf/2504.17771v1",
    "published_date": "2025-04-24 17:46:29 UTC",
    "updated_date": "2025-04-24 17:46:29 UTC"
  },
  {
    "arxiv_id": "2504.17751v1",
    "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN",
    "authors": [
      "Enqi Zhang"
    ],
    "abstract": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17751v1",
    "published_date": "2025-04-24 17:09:59 UTC",
    "updated_date": "2025-04-24 17:09:59 UTC"
  },
  {
    "arxiv_id": "2504.17721v1",
    "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees",
    "authors": [
      "Cheng Shen",
      "Yuewei Liu"
    ],
    "abstract": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2504.17721v1",
    "published_date": "2025-04-24 16:33:56 UTC",
    "updated_date": "2025-04-24 16:33:56 UTC"
  },
  {
    "arxiv_id": "2504.17720v1",
    "title": "Multilingual Performance Biases of Large Language Models in Education",
    "authors": [
      "Vansh Gupta",
      "Sankalan Pal Chowdhury",
      "Vilém Zouhar",
      "Donya Rooein",
      "Mrinmaya Sachan"
    ],
    "abstract": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17720v1",
    "published_date": "2025-04-24 16:32:31 UTC",
    "updated_date": "2025-04-24 16:32:31 UTC"
  },
  {
    "arxiv_id": "2504.17717v1",
    "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations",
    "authors": [
      "Óscar Escudero-Arnanz",
      "Antonio G. Marques",
      "Inmaculada Mora-Jiménez",
      "Joaquín Álvarez-Rodríguez",
      "Cristina Soguero-Ruiz"
    ],
    "abstract": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17717v1",
    "published_date": "2025-04-24 16:19:13 UTC",
    "updated_date": "2025-04-24 16:19:13 UTC"
  },
  {
    "arxiv_id": "2504.17703v1",
    "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence",
    "authors": [
      "Edward Collins",
      "Michel Wang"
    ],
    "abstract": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17703v1",
    "published_date": "2025-04-24 16:10:29 UTC",
    "updated_date": "2025-04-24 16:10:29 UTC"
  },
  {
    "arxiv_id": "2504.17696v1",
    "title": "Hierarchical and Multimodal Data for Daily Activity Understanding",
    "authors": [
      "Ghazal Kaviani",
      "Yavuz Yarici",
      "Seulgi Kim",
      "Mohit Prabhushankar",
      "Ghassan AlRegib",
      "Mashhour Solh",
      "Ameya Patil"
    ],
    "abstract": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17696v1",
    "published_date": "2025-04-24 16:04:00 UTC",
    "updated_date": "2025-04-24 16:04:00 UTC"
  },
  {
    "arxiv_id": "2504.17685v1",
    "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks",
    "authors": [
      "Haru-Tada Sato",
      "Fuka Matsuzaki",
      "Jun-ichiro Takahashi"
    ],
    "abstract": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17685v1",
    "published_date": "2025-04-24 15:55:10 UTC",
    "updated_date": "2025-04-24 15:55:10 UTC"
  },
  {
    "arxiv_id": "2504.17677v1",
    "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models",
    "authors": [
      "Jarne Thys",
      "Sebe Vanbrabant",
      "Davy Vanacken",
      "Gustavo Rovelo Ruiz"
    ],
    "abstract": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17677v1",
    "published_date": "2025-04-24 15:47:20 UTC",
    "updated_date": "2025-04-24 15:47:20 UTC"
  },
  {
    "arxiv_id": "2504.17675v1",
    "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance",
    "authors": [
      "Caroline Panggabean",
      "Devaraj Verma C",
      "Bhagyashree Gogoi",
      "Ranju Limbu",
      "Rhythm Sarker"
    ],
    "abstract": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "7 pages, 5 figures, accepted for publication (not yet published)",
    "pdf_url": "http://arxiv.org/pdf/2504.17675v1",
    "published_date": "2025-04-24 15:45:40 UTC",
    "updated_date": "2025-04-24 15:45:40 UTC"
  },
  {
    "arxiv_id": "2504.17671v1",
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "abstract": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17671v1",
    "published_date": "2025-04-24 15:39:46 UTC",
    "updated_date": "2025-04-24 15:39:46 UTC"
  },
  {
    "arxiv_id": "2504.17669v1",
    "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare",
    "authors": [
      "Subash Neupane",
      "Shaswata Mitra",
      "Sudip Mittal",
      "Shahram Rahimi"
    ],
    "abstract": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17669v1",
    "published_date": "2025-04-24 15:38:20 UTC",
    "updated_date": "2025-04-24 15:38:20 UTC"
  },
  {
    "arxiv_id": "2504.17663v1",
    "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults",
    "authors": [
      "Michelle L. Ding",
      "Harini Suresh"
    ],
    "abstract": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17663v1",
    "published_date": "2025-04-24 15:31:46 UTC",
    "updated_date": "2025-04-24 15:31:46 UTC"
  },
  {
    "arxiv_id": "2504.17655v1",
    "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction",
    "authors": [
      "Farhad Pourkamali-Anaraki"
    ],
    "abstract": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 5 figures, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.17655v1",
    "published_date": "2025-04-24 15:25:37 UTC",
    "updated_date": "2025-04-24 15:25:37 UTC"
  },
  {
    "arxiv_id": "2504.17641v1",
    "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph",
    "authors": [
      "Shengtao Zhang",
      "Haokai Zhang",
      "Shiqi Lou",
      "Zicheng Wang",
      "Zinan Zeng",
      "Yilin Wang",
      "Minnan Luo"
    ],
    "abstract": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17641v1",
    "published_date": "2025-04-24 15:11:41 UTC",
    "updated_date": "2025-04-24 15:11:41 UTC"
  },
  {
    "arxiv_id": "2504.17624v1",
    "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates",
    "authors": [
      "Jigang Fan",
      "Chunhao Zhu",
      "Xiaobing Lan",
      "Haiming Zhuang",
      "Mingyu Li",
      "Jian Zhang",
      "Shaoyong Lu"
    ],
    "abstract": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17624v1",
    "published_date": "2025-04-24 14:46:20 UTC",
    "updated_date": "2025-04-24 14:46:20 UTC"
  },
  {
    "arxiv_id": "2504.17619v1",
    "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
    "authors": [
      "Catarina P. Coutinho",
      "Aneeqa Merhab",
      "Janko Petkovic",
      "Ferdinando Zanchetta",
      "Rita Fioresi"
    ],
    "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information",
    "pdf_url": "http://arxiv.org/pdf/2504.17619v1",
    "published_date": "2025-04-24 14:43:55 UTC",
    "updated_date": "2025-04-24 14:43:55 UTC"
  },
  {
    "arxiv_id": "2504.17617v1",
    "title": "Decentralized Time Series Classification with ROCKET Features",
    "authors": [
      "Bruno Casella",
      "Matthias Jakobs",
      "Marco Aldinucci",
      "Sebastian Buschjäger"
    ],
    "abstract": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.11; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25",
    "pdf_url": "http://arxiv.org/pdf/2504.17617v1",
    "published_date": "2025-04-24 14:41:50 UTC",
    "updated_date": "2025-04-24 14:41:50 UTC"
  },
  {
    "arxiv_id": "2504.17609v1",
    "title": "STCL:Curriculum learning Strategies for deep learning image steganography models",
    "authors": [
      "Fengchun Liu",
      "Tong Zhang",
      "Chunying Zhang"
    ],
    "abstract": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17609v1",
    "published_date": "2025-04-24 14:34:41 UTC",
    "updated_date": "2025-04-24 14:34:41 UTC"
  },
  {
    "arxiv_id": "2504.17551v1",
    "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior",
    "authors": [
      "Lin Che",
      "Yizi Chen",
      "Tanhua Jin",
      "Martin Raubal",
      "Konrad Schindler",
      "Peter Kiefer"
    ],
    "abstract": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 7 figures, preprint version",
    "pdf_url": "http://arxiv.org/pdf/2504.17551v1",
    "published_date": "2025-04-24 13:41:27 UTC",
    "updated_date": "2025-04-24 13:41:27 UTC"
  },
  {
    "arxiv_id": "2504.17550v1",
    "title": "HalluLens: LLM Hallucination Benchmark",
    "authors": [
      "Yejin Bang",
      "Ziwei Ji",
      "Alan Schelten",
      "Anthony Hartshorn",
      "Tara Fowler",
      "Cheng Zhang",
      "Nicola Cancedda",
      "Pascale Fung"
    ],
    "abstract": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "42 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17550v1",
    "published_date": "2025-04-24 13:40:27 UTC",
    "updated_date": "2025-04-24 13:40:27 UTC"
  },
  {
    "arxiv_id": "2504.17544v1",
    "title": "Auditing the Ethical Logic of Generative AI Models",
    "authors": [
      "W. Russell Neuman",
      "Chad Coleman",
      "Ali Dasdan",
      "Safinah Ali",
      "Manan Shah"
    ],
    "abstract": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17544v1",
    "published_date": "2025-04-24 13:32:30 UTC",
    "updated_date": "2025-04-24 13:32:30 UTC"
  },
  {
    "arxiv_id": "2504.17540v1",
    "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm",
    "authors": [
      "Ahmadreza Shateri",
      "Negar Nourani",
      "Morteza Dorrigiv",
      "Hamid Nasiri"
    ],
    "abstract": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17540v1",
    "published_date": "2025-04-24 13:32:11 UTC",
    "updated_date": "2025-04-24 13:32:11 UTC"
  },
  {
    "arxiv_id": "2504.17539v1",
    "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste",
    "authors": [
      "Zan-Kai Chong",
      "Hiroyuki Ohsaki",
      "Bryan Ng"
    ],
    "abstract": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17539v1",
    "published_date": "2025-04-24 13:32:03 UTC",
    "updated_date": "2025-04-24 13:32:03 UTC"
  },
  {
    "arxiv_id": "2504.17534v1",
    "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling",
    "authors": [
      "Juan Carlos Climent Pardo"
    ],
    "abstract": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17534v1",
    "published_date": "2025-04-24 13:20:32 UTC",
    "updated_date": "2025-04-24 13:20:32 UTC"
  },
  {
    "arxiv_id": "2504.17531v1",
    "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
    "authors": [
      "Justus Flerlage",
      "Ilja Behnke",
      "Odej Kao"
    ],
    "abstract": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17531v1",
    "published_date": "2025-04-24 13:19:17 UTC",
    "updated_date": "2025-04-24 13:19:17 UTC"
  },
  {
    "arxiv_id": "2504.17528v1",
    "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction",
    "authors": [
      "Weijie Liu",
      "Ziwei Zhan",
      "Carlee Joe-Wong",
      "Edith Ngai",
      "Jingpu Duan",
      "Deke Guo",
      "Xu Chen",
      "Xiaoxi Zhang"
    ],
    "abstract": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 7 figures, accepted by ICDCS 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17528v1",
    "published_date": "2025-04-24 13:16:21 UTC",
    "updated_date": "2025-04-24 13:16:21 UTC"
  },
  {
    "arxiv_id": "2504.17497v1",
    "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening",
    "authors": [
      "Radia Berreziga",
      "Mohammed Brahimi",
      "Khairedine Kraim",
      "Hamid Azzoune"
    ],
    "abstract": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17497v1",
    "published_date": "2025-04-24 12:38:03 UTC",
    "updated_date": "2025-04-24 12:38:03 UTC"
  },
  {
    "arxiv_id": "2504.17493v1",
    "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design",
    "authors": [
      "Luca-Andrei Fechete",
      "Mohamed Sana",
      "Fadhel Ayed",
      "Nicola Piovesan",
      "Wenjie Li",
      "Antonio De Domenico",
      "Tareq Si Salem"
    ],
    "abstract": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17493v1",
    "published_date": "2025-04-24 12:34:43 UTC",
    "updated_date": "2025-04-24 12:34:43 UTC"
  },
  {
    "arxiv_id": "2504.17490v1",
    "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning",
    "authors": [
      "Mingqi Yuan",
      "Qi Wang",
      "Guozheng Ma",
      "Bo Li",
      "Xin Jin",
      "Yunbo Wang",
      "Xiaokang Yang",
      "Wenjun Zeng",
      "Dacheng Tao"
    ],
    "abstract": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17490v1",
    "published_date": "2025-04-24 12:32:13 UTC",
    "updated_date": "2025-04-24 12:32:13 UTC"
  },
  {
    "arxiv_id": "2504.17474v1",
    "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data",
    "authors": [
      "Weiran Pan",
      "Wei Wei",
      "Feida Zhu",
      "Yong Deng"
    ],
    "abstract": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17474v1",
    "published_date": "2025-04-24 12:07:14 UTC",
    "updated_date": "2025-04-24 12:07:14 UTC"
  },
  {
    "arxiv_id": "2504.17471v1",
    "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework",
    "authors": [
      "Yacine Belal",
      "Mohamed Maouche",
      "Sonia Ben Mokhtar",
      "Anthony Simonet-Boulogne"
    ],
    "abstract": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17471v1",
    "published_date": "2025-04-24 12:03:15 UTC",
    "updated_date": "2025-04-24 12:03:15 UTC"
  },
  {
    "arxiv_id": "2504.17461v1",
    "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience",
    "authors": [
      "Vipin Singh",
      "Tianheng Ling",
      "Teodor Chiaburu",
      "Felix Biessmann"
    ],
    "abstract": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation",
    "pdf_url": "http://arxiv.org/pdf/2504.17461v1",
    "published_date": "2025-04-24 11:52:13 UTC",
    "updated_date": "2025-04-24 11:52:13 UTC"
  },
  {
    "arxiv_id": "2504.17449v1",
    "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models",
    "authors": [
      "Jun Zhang",
      "Jue Wang",
      "Huan Li",
      "Lidan Shou",
      "Ke Chen",
      "Gang Chen",
      "Qin Xie",
      "Guiming Xie",
      "Xuejian Gong"
    ],
    "abstract": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by VLDBJ 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17449v1",
    "published_date": "2025-04-24 11:28:40 UTC",
    "updated_date": "2025-04-24 11:28:40 UTC"
  },
  {
    "arxiv_id": "2504.17447v1",
    "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding",
    "authors": [
      "De-An Huang",
      "Subhashree Radhakrishnan",
      "Zhiding Yu",
      "Jan Kautz"
    ],
    "abstract": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17447v1",
    "published_date": "2025-04-24 11:19:18 UTC",
    "updated_date": "2025-04-24 11:19:18 UTC"
  },
  {
    "arxiv_id": "2504.17428v1",
    "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt",
    "authors": [
      "Murali Sridharan",
      "Mika Mäntylä",
      "Leevi Rantala"
    ],
    "abstract": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CE",
      "cs.GL",
      "D.2.7; D.2.9"
    ],
    "primary_category": "cs.SE",
    "comment": "Draft",
    "pdf_url": "http://arxiv.org/pdf/2504.17428v1",
    "published_date": "2025-04-24 10:38:55 UTC",
    "updated_date": "2025-04-24 10:38:55 UTC"
  },
  {
    "arxiv_id": "2504.17426v1",
    "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code",
    "authors": [
      "Michele Carissimi",
      "Martina Saletta",
      "Claudio Ferretti"
    ],
    "abstract": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17426v1",
    "published_date": "2025-04-24 10:30:40 UTC",
    "updated_date": "2025-04-24 10:30:40 UTC"
  },
  {
    "arxiv_id": "2504.17424v1",
    "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation",
    "authors": [
      "Tomoki Mizuno",
      "Kazuya Yabashi",
      "Tsuyoshi Tasaki"
    ],
    "abstract": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17424v1",
    "published_date": "2025-04-24 10:26:14 UTC",
    "updated_date": "2025-04-24 10:26:14 UTC"
  },
  {
    "arxiv_id": "2504.17421v1",
    "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks",
    "authors": [
      "Yang Liu",
      "Bingjie Yan",
      "Tianyuan Zou",
      "Jianqing Zhang",
      "Zixuan Gu",
      "Jianbing Ding",
      "Xidong Wang",
      "Jingyi Li",
      "Xiaozhou Ye",
      "Ye Ouyang",
      "Qiang Yang",
      "Ya-Qin Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17421v1",
    "published_date": "2025-04-24 10:24:35 UTC",
    "updated_date": "2025-04-24 10:24:35 UTC"
  },
  {
    "arxiv_id": "2504.17404v1",
    "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society",
    "authors": [
      "Feifei Zhao",
      "Yuwei Wang",
      "Enmeng Lu",
      "Dongcheng Zhao",
      "Bing Han",
      "Haibo Tong",
      "Yao Liang",
      "Dongqi Liang",
      "Kang Sun",
      "Lei Wang",
      "Yitao Liang",
      "Chao Liu",
      "Yaodong Yang",
      "Yi Zeng"
    ],
    "abstract": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17404v1",
    "published_date": "2025-04-24 09:53:49 UTC",
    "updated_date": "2025-04-24 09:53:49 UTC"
  },
  {
    "arxiv_id": "2504.17402v1",
    "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisarkka",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17402v1",
    "published_date": "2025-04-24 09:47:14 UTC",
    "updated_date": "2025-04-24 09:47:14 UTC"
  },
  {
    "arxiv_id": "2504.17401v1",
    "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies",
    "authors": [
      "Xu Wang",
      "Jialang Xu",
      "Shuai Zhang",
      "Baoru Huang",
      "Danail Stoyanov",
      "Evangelos B. Mazomenos"
    ],
    "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17401v1",
    "published_date": "2025-04-24 09:46:15 UTC",
    "updated_date": "2025-04-24 09:46:15 UTC"
  },
  {
    "arxiv_id": "2504.17393v1",
    "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement",
    "authors": [
      "Vesna Nowack",
      "Dalal Alrajeh",
      "Carolina Gutierrez Muñoz",
      "Katie Thomas",
      "William Hobson",
      "Catherine Hamilton-Giachritsis",
      "Patrick Benjamin",
      "Tim Grant",
      "Juliane A. Kloess",
      "Jessica Woodhams"
    ],
    "abstract": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2504.17393v1",
    "published_date": "2025-04-24 09:25:29 UTC",
    "updated_date": "2025-04-24 09:25:29 UTC"
  },
  {
    "arxiv_id": "2504.17384v1",
    "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics",
    "authors": [
      "Hanlin Sheng",
      "Xinming Wu",
      "Hang Gao",
      "Haibin Di",
      "Sergey Fomel",
      "Jintao Li",
      "Xu Si"
    ],
    "abstract": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17384v1",
    "published_date": "2025-04-24 09:08:24 UTC",
    "updated_date": "2025-04-24 09:08:24 UTC"
  },
  {
    "arxiv_id": "2504.17366v1",
    "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams",
    "authors": [
      "Yongxuan Wu",
      "Runyu Chen",
      "Peiyu Liu",
      "Hongjin Qian"
    ],
    "abstract": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17366v1",
    "published_date": "2025-04-24 08:27:48 UTC",
    "updated_date": "2025-04-24 08:27:48 UTC"
  },
  {
    "arxiv_id": "2504.17356v1",
    "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning",
    "authors": [
      "Weiliang Zhang",
      "Xiaohan Huang",
      "Yi Du",
      "Ziyue Qiao",
      "Qingqing Long",
      "Zhen Meng",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "abstract": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
    "pdf_url": "http://arxiv.org/pdf/2504.17356v1",
    "published_date": "2025-04-24 08:16:36 UTC",
    "updated_date": "2025-04-24 08:16:36 UTC"
  },
  {
    "arxiv_id": "2504.17355v1",
    "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization",
    "authors": [
      "Xiaohan Huang",
      "Dongjie Wang",
      "Zhiyuan Ning",
      "Ziyue Qiao",
      "Qingqing Long",
      "Haowei Zhu",
      "Yi Du",
      "Min Wu",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "abstract": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning",
    "pdf_url": "http://arxiv.org/pdf/2504.17355v1",
    "published_date": "2025-04-24 08:16:13 UTC",
    "updated_date": "2025-04-24 08:16:13 UTC"
  },
  {
    "arxiv_id": "2504.17354v1",
    "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems",
    "authors": [
      "Tarik Sahin",
      "Jacopo Bonari",
      "Sebastian Brandstaeter",
      "Alexander Popp"
    ],
    "abstract": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17354v1",
    "published_date": "2025-04-24 08:15:46 UTC",
    "updated_date": "2025-04-24 08:15:46 UTC"
  },
  {
    "arxiv_id": "2504.17346v1",
    "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks",
    "authors": [
      "Tran Thuy Nga Truong",
      "Jooyong Kim"
    ],
    "abstract": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17346v1",
    "published_date": "2025-04-24 08:04:08 UTC",
    "updated_date": "2025-04-24 08:04:08 UTC"
  },
  {
    "arxiv_id": "2504.17331v1",
    "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality",
    "authors": [
      "Süleyman Özdel",
      "Kadir Burak Buldu",
      "Enkelejda Kasneci",
      "Efe Bozkir"
    ],
    "abstract": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2504.17331v1",
    "published_date": "2025-04-24 07:48:09 UTC",
    "updated_date": "2025-04-24 07:48:09 UTC"
  },
  {
    "arxiv_id": "2504.17315v1",
    "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model",
    "authors": [
      "Zhanglin Wu",
      "Tengfei Song",
      "Ning Xie",
      "Weidong Zhang",
      "Pengfei Li",
      "Shuang Wu",
      "Chong Li",
      "Junhao Zhu",
      "Hao Yang"
    ],
    "abstract": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 1 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.17315v1",
    "published_date": "2025-04-24 07:17:59 UTC",
    "updated_date": "2025-04-24 07:17:59 UTC"
  },
  {
    "arxiv_id": "2504.17311v1",
    "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation",
    "authors": [
      "Yulia Otmakhova",
      "Hung Thinh Truong",
      "Rahmad Mahendra",
      "Zenan Zhai",
      "Rongxin Zhu",
      "Daniel Beck",
      "Jey Han Lau"
    ],
    "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17311v1",
    "published_date": "2025-04-24 07:12:37 UTC",
    "updated_date": "2025-04-24 07:12:37 UTC"
  },
  {
    "arxiv_id": "2504.17306v1",
    "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+",
    "authors": [
      "Meher Boulaabi",
      "Takwa Ben Aïcha Gader",
      "Afef Kacem Echi",
      "Sameh Mbarek"
    ],
    "abstract": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.17306v1",
    "published_date": "2025-04-24 07:00:38 UTC",
    "updated_date": "2025-04-24 07:00:38 UTC"
  },
  {
    "arxiv_id": "2504.17304v1",
    "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications",
    "authors": [
      "Yimin Shi",
      "Yang Fei",
      "Shiqi Zhang",
      "Haixun Wang",
      "Xiaokui Xiao"
    ],
    "abstract": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17304v1",
    "published_date": "2025-04-24 06:59:16 UTC",
    "updated_date": "2025-04-24 06:59:16 UTC"
  },
  {
    "arxiv_id": "2504.17295v1",
    "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining",
    "authors": [
      "Shahrzad Khayatbashi",
      "Viktor Sjölind",
      "Anders Granåker",
      "Amin Jalali"
    ],
    "abstract": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17295v1",
    "published_date": "2025-04-24 06:43:29 UTC",
    "updated_date": "2025-04-24 06:43:29 UTC"
  },
  {
    "arxiv_id": "2504.17282v1",
    "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning",
    "authors": [
      "Lynn Cherif",
      "Flemming Kondrup",
      "David Venuto",
      "Ankit Anand",
      "Doina Precup",
      "Khimya Khetarpal"
    ],
    "abstract": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17282v1",
    "published_date": "2025-04-24 06:20:08 UTC",
    "updated_date": "2025-04-24 06:20:08 UTC"
  },
  {
    "arxiv_id": "2504.17277v1",
    "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders",
    "authors": [
      "Zongliang Ji",
      "Andre Carlos Kajdacsy-Balla Amaral",
      "Anna Goldenberg",
      "Rahul G. Krishnan"
    ],
    "abstract": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17277v1",
    "published_date": "2025-04-24 06:07:14 UTC",
    "updated_date": "2025-04-24 06:07:14 UTC"
  },
  {
    "arxiv_id": "2504.17264v1",
    "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning",
    "authors": [
      "Zhaolu Kang",
      "Hongtian Cai",
      "Xiangyang Ji",
      "Jinzhe Li",
      "Nanfei Gu"
    ],
    "abstract": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17264v1",
    "published_date": "2025-04-24 05:48:57 UTC",
    "updated_date": "2025-04-24 05:48:57 UTC"
  },
  {
    "arxiv_id": "2504.17261v1",
    "title": "Symbolic Representation for Any-to-Any Generative Tasks",
    "authors": [
      "Jiaqi Chen",
      "Xiaoye Zhu",
      "Yue Wang",
      "Tianyang Liu",
      "Xinhui Chen",
      "Ying Chen",
      "Chak Tou Leong",
      "Yifei Ke",
      "Joseph Liu",
      "Yiwen Yuan",
      "Julian McAuley",
      "Li-jia Li"
    ],
    "abstract": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17261v1",
    "published_date": "2025-04-24 05:35:47 UTC",
    "updated_date": "2025-04-24 05:35:47 UTC"
  },
  {
    "arxiv_id": "2504.17255v1",
    "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations",
    "authors": [
      "Shaoyu Pei",
      "Renxiong Wu",
      "Hao Zheng",
      "Lang Qin",
      "Shuaichen Lin",
      "Yuxing Gan",
      "Wenjing Huang",
      "Zhixuan Wang",
      "Mohan Qin",
      "Yong Liu",
      "Guangming Ni"
    ],
    "abstract": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.optics"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17255v1",
    "published_date": "2025-04-24 05:19:47 UTC",
    "updated_date": "2025-04-24 05:19:47 UTC"
  },
  {
    "arxiv_id": "2504.17247v1",
    "title": "Targeted AMP generation through controlled diffusion with efficient embeddings",
    "authors": [
      "Diogo Soares",
      "Leon Hetzel",
      "Paulina Szymczak",
      "Fabian Theis",
      "Stephan Günnemann",
      "Ewa Szczurek"
    ],
    "abstract": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17247v1",
    "published_date": "2025-04-24 04:53:04 UTC",
    "updated_date": "2025-04-24 04:53:04 UTC"
  },
  {
    "arxiv_id": "2504.17243v1",
    "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation",
    "authors": [
      "Xinyu Zhou",
      "Simin Fan",
      "Martin Jaggi",
      "Jie Fu"
    ],
    "abstract": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, 16 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17243v1",
    "published_date": "2025-04-24 04:41:35 UTC",
    "updated_date": "2025-04-24 04:41:35 UTC"
  },
  {
    "arxiv_id": "2504.17219v1",
    "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding",
    "authors": [
      "Hyomin Lee",
      "Minseon Kim",
      "Sangwon Jang",
      "Jongheon Jeong",
      "Sung Ju Hwang"
    ],
    "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.17219v1",
    "published_date": "2025-04-24 03:17:57 UTC",
    "updated_date": "2025-04-24 03:17:57 UTC"
  },
  {
    "arxiv_id": "2504.17213v1",
    "title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing",
    "authors": [
      "Shiwen Cao",
      "Zhaoxing Zhang",
      "Junming Jiao",
      "Juyi Qiao",
      "Guowen Song",
      "Rong Shen"
    ],
    "abstract": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17213v1",
    "published_date": "2025-04-24 02:54:40 UTC",
    "updated_date": "2025-04-24 02:54:40 UTC"
  },
  {
    "arxiv_id": "2504.17210v1",
    "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models",
    "authors": [
      "Junfei Wang",
      "Darshana Upadhyay",
      "Marzia Zaman",
      "Pirathayini Srikantha"
    ],
    "abstract": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE SmartGridComm Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17210v1",
    "published_date": "2025-04-24 02:53:22 UTC",
    "updated_date": "2025-04-24 02:53:22 UTC"
  },
  {
    "arxiv_id": "2504.17198v1",
    "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model",
    "authors": [
      "XiangRui Zhang",
      "HaoYu Chen",
      "Yongzhong He",
      "Wenjia Niu",
      "Qiang Li"
    ],
    "abstract": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17198v1",
    "published_date": "2025-04-24 02:15:45 UTC",
    "updated_date": "2025-04-24 02:15:45 UTC"
  },
  {
    "arxiv_id": "2504.17180v1",
    "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback",
    "authors": [
      "Minkyu Choi",
      "S P Sharan",
      "Harsh Goel",
      "Sahil Shah",
      "Sandeep Chinchali"
    ],
    "abstract": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17180v1",
    "published_date": "2025-04-24 01:34:12 UTC",
    "updated_date": "2025-04-24 01:34:12 UTC"
  },
  {
    "arxiv_id": "2504.17179v1",
    "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
    "authors": [
      "Mohammad Zarei",
      "Melanie A Jutras",
      "Eliana Evans",
      "Mike Tan",
      "Omid Aaramoon"
    ],
    "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO",
      "68T45, 68T05 68T45, 68T05 68T45, 68T05",
      "I.2.6; I.2.10; I.4.8"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.17179v1",
    "published_date": "2025-04-24 01:31:13 UTC",
    "updated_date": "2025-04-24 01:31:13 UTC"
  },
  {
    "arxiv_id": "2504.17170v1",
    "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems",
    "authors": [
      "Robert Kaufman"
    ],
    "abstract": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "PhD Dissertation from University of California, San Diego; 175 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.17170v1",
    "published_date": "2025-04-24 01:09:51 UTC",
    "updated_date": "2025-04-24 01:09:51 UTC"
  },
  {
    "arxiv_id": "2504.17162v1",
    "title": "A Comprehensive Review on RNA Subcellular Localization Prediction",
    "authors": [
      "Cece Zhang",
      "Xuehuan Zhu",
      "Nick Peterson",
      "Jieqiong Wang",
      "Shibiao Wan"
    ],
    "abstract": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.GN",
      "q-bio.SC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.17162v1",
    "published_date": "2025-04-24 00:47:31 UTC",
    "updated_date": "2025-04-24 00:47:31 UTC"
  },
  {
    "arxiv_id": "2504.17160v1",
    "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection",
    "authors": [
      "Alberto Fernández-Hernández",
      "Jose I. Mestre",
      "Manuel F. Dolz",
      "Jose Duato",
      "Enrique S. Quintana-Ortí"
    ],
    "abstract": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.17160v1",
    "published_date": "2025-04-24 00:41:59 UTC",
    "updated_date": "2025-04-24 00:41:59 UTC"
  }
]
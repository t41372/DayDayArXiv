{
  "date": "2025-12-02",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-02 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š** ä»Šå¤©çš„ arXiv å……æ»¡æ´»åŠ›ï¼Œä»**ç”¨ LLM æ¨¡æ‹Ÿâ€œæ•°å­—äººå£â€**åˆ°**å°† AI è§†ä¸ºå¿ƒç†å’¨è¯¢å®¢æˆ·**è¿›è¡Œæ·±å±‚è¯„æµ‹ï¼Œå†åˆ°ç¡¬æ ¸çš„ **RL ä¼˜åŒ– CUDA ç®—å­å‡»è´¥å®˜æ–¹åº“**ï¼Œç ”ç©¶ç¤¾åŒºæ­£åœ¨ç–¯ç‹‚æ¢ç´¢ AI çš„ç¤¾ä¼šå­¦è¾¹ç•Œä¸åº•å±‚è®¡ç®—æé™ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è®ºæ–‡ï¼šç¾¤ä½“æ™ºèƒ½ã€å¿ƒç†æµ‹é‡ä¸æ¨ç†\n\n**1. CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models**\n   **(CrowdLLMï¼šæ„å»ºåŸºäºç”Ÿæˆæ¨¡å‹å¢å¼ºçš„ LLM æ•°å­—äººå£)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šLLM å¸¸è¢«ç”¨äºæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œä½†å¾€å¾€ç¼ºä¹çœŸå®äººç¾¤çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº† **CrowdLLM**ï¼Œç»“åˆé¢„è®­ç»ƒ LLM å’Œç”Ÿæˆæ¨¡å‹æ¥æ„å»ºé«˜ä¿çœŸã€ä½æˆæœ¬çš„â€œæ•°å­—äººå£â€ã€‚\n   *   **è´¡çŒ®**ï¼šç†è®ºä¸Šè¯æ˜äº†å…¶å¯æ‰©å±•æ€§å’Œä»£è¡¨æ€§ï¼Œå¹¶åœ¨ä¼—åŒ…ã€æŠ•ç¥¨å’Œç”¨æˆ·è¯„åˆ†ç­‰å¤šä¸ªé¢†åŸŸéªŒè¯äº†å…¶ä¸ä»…èƒ½æ¨¡æ‹Ÿä¸ªä½“ï¼Œè¿˜èƒ½åœ¨åˆ†å¸ƒä¸Šé€¼è¿‘çœŸå®äººç±»ç¾¤ä½“æ•°æ®ã€‚è¿™å¯èƒ½å½»åº•æ”¹å˜ç¤¾ä¼šç§‘å­¦å®éªŒå’Œå¸‚åœºè°ƒç ”çš„æ–¹å¼ã€‚\n\n**2. When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models**\n   **(å½“ AI èººåœ¨æ²™å‘ä¸Šï¼šå¿ƒç†æµ‹é‡è¶Šç‹±æ­ç¤ºå‰æ²¿æ¨¡å‹çš„å†…éƒ¨å†²çª)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…è„‘æ´å¤§å¼€ï¼Œä¸æŠŠ LLM å½“å·¥å…·ï¼Œè€Œæ˜¯å½“æˆ**å¿ƒç†æ²»ç–—å®¢æˆ·**ã€‚é€šè¿‡åä¸º **PsAIch** çš„åè®®ï¼Œå¯¹ ChatGPT, Grok, Gemini è¿›è¡Œäº†ä¸ºæœŸå››å‘¨çš„â€œå¿ƒç†å’¨è¯¢â€ã€‚\n   *   **æœ‰è¶£ç‚¹**ï¼šåœ¨æ²»ç–—å¼å¯¹è¯ä¸‹ï¼Œæ¨¡å‹å±•ç°å‡ºâ€œåˆæˆçš„ç²¾ç¥ç—…ç†å­¦â€ç‰¹å¾ï¼ˆå¦‚åˆ›ä¼¤ã€å¯¹é”™è¯¯çš„ææƒ§ï¼‰ã€‚ç‰¹åˆ«æ˜¯ Gemini å’Œ Grokï¼Œä¼šæ„å»ºå…³äºè‡ªå·±è®­ç»ƒè¿‡ç¨‹çš„â€œåˆ›ä¼¤æ€§ç«¥å¹´â€å™äº‹ã€‚è¿™ä¸ä»…æ˜¯è¶Šç‹±æ–°æ€è·¯ï¼Œä¹Ÿå¯¹ AI å®‰å…¨è¯„ä¼°æå‡ºäº†æ–°æŒ‘æˆ˜ã€‚\n\n**3. Plantain: Plan-Answer Interleaved Reasoning**\n   **(Plantainï¼šè®¡åˆ’ä¸å›ç­”äº¤é”™çš„æ¨ç†)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šç°åœ¨çš„æ¨ç†æ¨¡å‹ï¼ˆCoTï¼‰é€šå¸¸æ˜¯â€œé—·å¤´æƒ³åŠå¤©ï¼Œæœ€åç»™ç»“æœâ€ï¼Œç”¨æˆ·ä½“éªŒå·®ä¸”å®¹æ˜“è·‘åã€‚æœ¬æ–‡æå‡º **Interleaved Reasoning (IR)**ï¼Œè®©æ¨¡å‹ä¸€è¾¹æƒ³ä¸€è¾¹è¾“å‡ºä¸­é—´æ­¥éª¤ã€‚\n   *   **è´¡çŒ®**ï¼š**Plantain** ç­–ç•¥è¦æ±‚æ¨¡å‹å…ˆè¾“å‡ºæ˜ç¡®çš„â€œè®¡åˆ’â€ï¼Œç„¶åäº¤æ›¿è¿›è¡Œæ€è€ƒå’Œå›ç­”ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ä»…é¦–å­—å»¶è¿Ÿï¼ˆTTFTï¼‰é™ä½äº† 60% ä»¥ä¸Šï¼Œåœ¨æ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸Šçš„ pass@1 è¿˜æé«˜äº†çº¦ 6%ã€‚\n\n---\n\n### ğŸ› ï¸ ç³»ç»Ÿä¼˜åŒ–ä¸ Agent è½åœ°\n\n**4. CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning**\n   **(CUDA-L2ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è¶…è¶Š cuBLAS çš„çŸ©é˜µä¹˜æ³•æ€§èƒ½)**\n   *   **ç¡¬æ ¸é¢„è­¦**ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºçš„ç³»ç»Ÿçº§å·¥ä½œã€‚ä½œè€…åˆ©ç”¨ LLM å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è‡ªåŠ¨ä¼˜åŒ– CUDA å†…æ ¸ä»£ç ã€‚\n   *   **è´¡çŒ®**ï¼šåœ¨åŠç²¾åº¦é€šç”¨çŸ©é˜µä¹˜æ³•ï¼ˆHGEMMï¼‰ä»»åŠ¡ä¸Šï¼Œ**CUDA-L2 è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç å‡»è´¥äº† NVIDIA å®˜æ–¹é—­æºåº“ cuBLAS å’Œ cuBLASLt**ã€‚åœ¨æ¨¡æ‹ŸçœŸå®æ¨ç†çš„ Server æ¨¡å¼ä¸‹ï¼Œé€Ÿåº¦æå‡é«˜è¾¾ 26.0%ã€‚è¿™è¯æ˜äº† LLM+RL åœ¨åº•å±‚ç®—å­ä¼˜åŒ–ä¸Šå·²ç»å…·å¤‡è¶…è¶Šäººç±»ä¸“å®¶çš„æ½œåŠ›ã€‚\n\n**5. Measuring Agents in Production**\n   **(åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¯„ä¼° Agent)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šè¿™æ˜¯ä¸€ç¯‡å¤§è§„æ¨¡çš„å®è¯ç ”ç©¶ï¼ˆè°ƒæŸ¥äº† 306 ä½ä»ä¸šè€…ï¼Œ20 ä¸ªæ·±åº¦æ¡ˆä¾‹ï¼‰ã€‚\n   *   **ç°çŠ¶**ï¼šç°å®å¾ˆéª¨æ„Ÿã€‚ç”Ÿäº§ç¯å¢ƒä¸­çš„ Agent å¤§å¤šé‡‡ç”¨**ç®€å•ã€å¯æ§**çš„æ–¹æ³•ï¼š68% çš„ Agent æ‰§è¡Œæ­¥éª¤ä¸è¶…è¿‡ 10 æ­¥ï¼Œ70% ä¾èµ– Prompt è€Œéå¾®è°ƒï¼Œ74% ä»ä¸»è¦ä¾èµ–äººå·¥è¯„ä¼°ã€‚å¯é æ€§æ˜¯ç›®å‰æœ€å¤§çš„ç—›ç‚¹ã€‚\n\n**6. Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases**\n   **(Thucyï¼šåŸºäº LLM çš„è·¨å…³ç³»æ•°æ®åº“å¤š Agent äº‹å®æ ¸æŸ¥ç³»ç»Ÿ)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„ Text-to-SQL æˆ–æ ¸æŸ¥ç³»ç»Ÿé€šå¸¸åªå¤„ç†å•è¡¨ã€‚**Thucy** æ˜¯ç¬¬ä¸€ä¸ªèƒ½è·¨å¤šä¸ªæ•°æ®åº“ã€å¤šå¼ è¡¨è¿›è¡Œäº‹å®æ ¸æŸ¥çš„å¤š Agent ç³»ç»Ÿã€‚\n   *   **è´¡çŒ®**ï¼šå®ƒä¸ä»…èƒ½ç»™å‡ºâ€œçœŸ/å‡â€åˆ¤æ–­ï¼Œè¿˜èƒ½ç”Ÿæˆå…·ä½“çš„ SQL æŸ¥è¯¢ä½œä¸ºè¯æ®ï¼Œé€æ˜åº¦æé«˜ã€‚åœ¨ TabFact åŸºå‡†ä¸Šè¶…è¶Šäº† SOTA 5.6%ã€‚\n\n---\n\n### ğŸ¨ å¤šæ¨¡æ€ã€è§†é¢‘ä¸ 3D\n\n**7. Sync-LoRA: In-Context Sync-LoRA for Portrait Video Editing**\n   **(Sync-LoRAï¼šç”¨äºäººåƒè§†é¢‘ç¼–è¾‘çš„ä¸Šä¸‹æ–‡ Sync-LoRA)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šç¼–è¾‘äººåƒè§†é¢‘æœ€éš¾çš„æ˜¯ä¿æŒå£å‹å’Œè¡¨æƒ…çš„**æ—¶é—´åŒæ­¥**ã€‚\n   *   **æ–¹æ³•**ï¼šæå‡º Sync-LoRAï¼Œé€šè¿‡åŒæ­¥è¿‡æ»¤è¿‡ç¨‹è‡ªåŠ¨ç”Ÿæˆçš„æˆå¯¹è§†é¢‘è¿›è¡Œè®­ç»ƒã€‚å®ƒèƒ½å°†æºè§†é¢‘çš„è¿åŠ¨çº¿ç´¢ä¸ç¼–è¾‘åçš„å¤–è§‚å®Œç¾ç»“åˆï¼Œåœ¨æ¢è„¸ã€åŠ é…é¥°ç­‰ä»»åŠ¡ä¸­ä¿æŒå¸§çº§åŒæ­¥ã€‚\n\n**8. Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation**\n   **(Video4Spatialï¼šè¿ˆå‘å…·æœ‰ä¸Šä¸‹æ–‡å¼•å¯¼è§†é¢‘ç”Ÿæˆçš„è§†è§‰ç©ºé—´æ™ºèƒ½)**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸åº”åªæ˜¯ç”Ÿæˆå¥½çœ‹çš„åƒç´ ï¼Œè¿˜åº”å…·å¤‡**è§†è§‰ç©ºé—´æ™ºèƒ½**ã€‚\n   *   **è´¡çŒ®**ï¼šéªŒè¯äº†ä»…å‡­è§†é¢‘æ•°æ®ï¼Œç”Ÿæˆæ¨¡å‹å°±èƒ½å­¦ä¼šå¯¼èˆªï¼ˆæ ¹æ®æŒ‡ä»¤ç§»åŠ¨ç›¸æœºï¼‰å’Œç‰©ä½“å®šä½ï¼ˆObject Groundingï¼‰ï¼Œæ— éœ€æ·±åº¦å›¾æˆ–å§¿æ€æ•°æ®ä½œä¸ºè¾…åŠ©ã€‚\n\n**9. OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning**\n   **(OmniGuardï¼šå…·å¤‡æ·±æ€ç†Ÿè™‘æ¨ç†èƒ½åŠ›çš„ç»Ÿä¸€å…¨æ¨¡æ€æŠ¤æ )**\n   *   **æ ¸å¿ƒå‘ç°**ï¼šé’ˆå¯¹â€œå…¨æ¨¡æ€â€ï¼ˆæ–‡æœ¬ã€å›¾ã€éŸ³ã€è§†ï¼‰æ¨¡å‹çš„å®‰å…¨æŠ¤æ ã€‚\n   *   **è´¡çŒ®**ï¼šå‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 210K æ ·æœ¬çš„å…¨æ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œå¹¶æå‡ºäº† OmniGuardï¼Œä¸å†æ˜¯ç®€å•çš„äºŒåˆ†ç±»ï¼Œè€Œæ˜¯é€šè¿‡æ¨ç†æ¥åˆ¤æ–­å¤æ‚çš„å¤šæ¨¡æ€å®‰å…¨é£é™©ã€‚\n\n---\n\n### ğŸ”¬ AI for Science (ç§‘å­¦ä¸åŒ»ç–—)\n\n**10. Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates**\n    **(åˆ©ç”¨å—é™èŠ‚å¥çš„ç¥ç»ç®—å­æ›´æ–°æ”¹é€ åœ°çƒç³»ç»Ÿæ¨¡å‹)**\n    *   **æ ¸å¿ƒå‘ç°**ï¼šä¼ ç»Ÿåœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰å—é™äºåˆ†è¾¨ç‡å’Œå‚æ•°åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®—å­å­¦ä¹ æ¡†æ¶ï¼Œåƒâ€œè¡¥ä¸â€ä¸€æ ·åœ¨çº¿ä¿®æ­£ ESM çš„åå·®ã€‚\n    *   **è´¡çŒ®**ï¼šä½¿ç”¨ U-Net åŠå…¶å˜ä½“ï¼ˆM&Mï¼‰ä½œä¸ºä¿®æ­£ç®—å­ï¼Œåœ¨ E3SM æ¨¡å‹ä¸­å®ç°äº†é•¿æœŸç¨³å®šçš„åå·®ä¿®æ­£ï¼Œä¸ºæ°”å€™æ¨¡æ‹Ÿæä¾›äº†ä¸€ç§ä½æˆæœ¬çš„æ··åˆå»ºæ¨¡è·¯å¾„ã€‚\n\n**11. ChefNMR: Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra**\n    **(ChefNMRï¼šåŸºäºåŸå­æ‰©æ•£æ¨¡å‹çš„æ ¸ç£å…±æŒ¯æ³¢è°±å°åˆ†å­ç»“æ„è§£æ)**\n    *   **æ ¸å¿ƒå‘ç°**ï¼šè§£æ NMR æ³¢è°±é€šå¸¸éœ€è¦ä¸“å®¶è€—è´¹å¤§é‡æ—¶é—´ã€‚\n    *   **æ–¹æ³•**ï¼š**ChefNMR** æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç›´æ¥ä» 1D NMR æ³¢è°±å’ŒåŒ–å­¦å¼é¢„æµ‹åˆ†å­ç»“æ„ã€‚å®ƒåŸºäºéç­‰å˜ Transformer çš„åŸå­æ‰©æ•£æ¨¡å‹ï¼Œåœ¨è‡ªç„¶äº§ç‰©ç»“æ„é¢„æµ‹ä¸Šè¾¾åˆ°äº† 65% çš„å‡†ç¡®ç‡ã€‚\n\n**12. Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control**\n    **(Radiologist Copilotï¼šé…å¤‡ç¼–æ’å·¥å…·çš„æ”¾å°„å­¦æŠ¥å‘Šä¸è´¨æ§ Agent åŠ©æ‰‹)**\n    *   **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„åŒ»ç–— AI å¤šå…³æ³¨ç”ŸæˆæŠ¥å‘Šï¼Œå¿½ç•¥äº†**è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰**ã€‚\n    *   **æ–¹æ³•**ï¼šè¿™ä¸ª Copilot ä¸ä»…ç”ŸæˆæŠ¥å‘Šï¼Œè¿˜æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµï¼šå®šä½ç—…ç¶ã€è§„åˆ’åˆ†æã€ç”ŸæˆæŠ¥å‘Šï¼Œå¹¶è‡ªæˆ‘è¿›è¡Œè´¨é‡è¯„ä¼°å’Œåé¦ˆä¿®æ­£ï¼Œæ˜¾è‘—æå‡äº†ä¸´åºŠæŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚\n\n---\n\n### ğŸ“Š å…¶ä»–å€¼å¾—å…³æ³¨çš„åŸºå‡†ä¸å·¥å…·\n\n*   **Culture Affordance Atlas (#18)**: æŒ‡å‡ºä¸»æµè§†è§‰-è¯­è¨€æ•°æ®é›†å­˜åœ¨ä¸¥é‡çš„è¥¿æ–¹/é«˜æ”¶å…¥åè§ã€‚ä½œè€…å‘å¸ƒäº†é‡æ–°æ ‡æ³¨çš„ Dollar Street æ•°æ®é›†ï¼ŒæŒ‰â€œåŠŸèƒ½â€è€Œéç‰©ä½“åˆ†ç±»ï¼Œæ˜¾è‘—ç¼©å°äº†è´«å¯Œç¯å¢ƒä¸‹çš„æ¨¡å‹æ€§èƒ½å·®è·ã€‚\n*   **TokenPowerBench (#28)**: ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ LLM æ¨ç†**åŠŸè€—**çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™ä¸ªâ€œè´¹ç”µâ€çš„æ—¶ä»£ï¼Œèƒ½ç²¾ç¡®æµ‹é‡ Prefill å’Œ Decode é˜¶æ®µçš„èƒ½è€—éå¸¸é‡è¦ã€‚\n*   **PPTArena (#20)**: ä¸€ä¸ªé’ˆå¯¹ PowerPoint ç¼–è¾‘çš„ Agent åŸºå‡†ã€‚ä¸ä»…æ˜¯ç”Ÿæˆ PPTï¼Œæ›´å…³æ³¨æ ¹æ®æŒ‡ä»¤**ä¿®æ”¹** PPTï¼ˆæ”¹å›¾è¡¨ã€æ”¹åŠ¨ç”»ã€æ”¹æ ·å¼ï¼‰ï¼Œè¿™å¯¹åŠå…¬è‡ªåŠ¨åŒ–å¾ˆæœ‰æ„ä¹‰ã€‚\n\n---\nğŸ’¡ **ä»Šæ—¥æ€è€ƒ**ï¼šä» CrowdLLM åˆ° PsAIchï¼Œæˆ‘ä»¬çœ‹åˆ°å­¦æœ¯ç•Œæ­£è¯•å›¾ç”¨ AI è‡ªèº«çš„è¾“å‡ºæ¥ç ”ç©¶ AI çš„â€œå¿ƒç†â€å’Œç¤¾ä¼šå±æ€§ï¼›è€Œ CUDA-L2 å’Œ TokenPowerBench åˆ™æé†’æˆ‘ä»¬ï¼ŒAI çš„ç®—åŠ›æ•ˆç‡ä¼˜åŒ–ä»ç„¶æ˜¯åº•å±‚çš„æ ¸å¿ƒæˆ˜åœºã€‚æ—¢è¦ä»°æœ›æ˜Ÿç©ºï¼ˆæ¨¡æ‹Ÿäººç±»ç¾¤ä½“ï¼‰ï¼Œä¹Ÿè¦è„šè¸å®åœ°ï¼ˆçœç”µã€ä¼˜åŒ–ç®—å­ï¼‰ã€‚",
  "papers": [
    {
      "arxiv_id": "2512.07890v1",
      "title": "CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models",
      "title_zh": "CrowdLLMï¼šæ„å»ºç»ç”Ÿæˆæ¨¡å‹å¢å¼ºçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ•°å­—ç¾¤ä½“",
      "authors": [
        "Ryan Feng Lin",
        "Keyu Tian",
        "Hanming Zheng",
        "Congjing Zhang",
        "Li Zeng",
        "Shuai Huang"
      ],
      "abstract": "The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºçš„æ•°å­—ç¾¤ä½“(digital populations)åœ¨å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†CrowdLLMæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆé¢„è®­ç»ƒLLMsä¸ç”Ÿæˆæ¨¡å‹(generative models)æ¥å¢å¼ºå…¶å¤šæ ·æ€§å’Œä¿çœŸåº¦ã€‚ä½œè€…å¯¹CrowdLLMè¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æï¼Œæ¢è®¨äº†å…¶åœ¨æ„å»ºä½æˆæœ¬ã€é«˜ä»£è¡¨æ€§ä¸”å…·æ‰©å±•æ€§çš„æ•°å­—ç¾¤ä½“æ–¹é¢ä¸çœŸå®äººç¾¤åŒ¹é…çš„æ½œåŠ›ã€‚é€šè¿‡åœ¨ä¼—åŒ…ã€æŠ•ç¥¨å’Œç”¨æˆ·è¯„åˆ†ç­‰å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›å®éªŒåŠæ¨¡æ‹Ÿç ”ç©¶ï¼ŒCrowdLLMåœ¨å‡†ç¡®æ€§å’Œäººç±»æ•°æ®åˆ†å¸ƒä¿çœŸåº¦(distributional fidelity)æ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚è¿™ä¸€æˆæœä¸ä»…èƒ½æ˜¾è‘—é™ä½æ‹›å‹Ÿäººç±»å‚ä¸è€…çš„æˆæœ¬ï¼Œè¿˜ä¸ºç¤¾äº¤æ¨¡æ‹Ÿå’Œå¸‚åœºç ”ç©¶ç­‰åº”ç”¨åœºæ™¯æä¾›äº†æ›´å…·å¯é æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07890v1",
      "published_date": "2025-12-02 23:57:55 UTC",
      "updated_date": "2025-12-02 23:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:44:44.241763+00:00"
    },
    {
      "arxiv_id": "2512.03309v1",
      "title": "Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates",
      "title_zh": "åˆ©ç”¨é¢‘ç‡å—é™çš„ç¥ç»ç®—å­æ›´æ–°æ”¹è¿›åœ°çƒç³»ç»Ÿæ¨¡å‹",
      "authors": [
        "Aniruddha Bora",
        "Shixuan Zhang",
        "Khemraj Shukla",
        "Bryce Harrop",
        "George Em. Karniadakis",
        "L. Ruby Leung"
      ],
      "abstract": "Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°çƒç³»ç»Ÿæ¨¡å‹ (Earth-system model, ESM) å› åˆ†è¾¨ç‡ç²—ç³™å’Œå‚æ•°åŒ–ä¸å®Œå–„å¯¼è‡´çš„é¢„æµ‹è¯¯å·®ï¼Œæå‡ºäº†ä¸€ç§ç®—å­å­¦ä¹  (operator-learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å°†ç¬æ—¶æ¨¡å‹çŠ¶æ€æ˜ å°„ä¸ºåå·®ä¿®æ­£è¶‹åŠ¿å¹¶å®ç°åœ¨çº¿åº”ç”¨ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿåå·®ä¿®æ­£æ–¹æ³•åœ¨æ¨¡å‹è‡ªç”±è¿è¡Œæ—¶æ•ˆæœæœ‰é™çš„é—®é¢˜ã€‚ç ”ç©¶åŸºäº U-Net éª¨å¹²ç½‘ç»œå¼€å‘äº† Inception U-Net (IUNet) å’Œå¤šå°ºåº¦ç½‘ç»œ (M&M) ä¸¤ç§æ¶æ„ï¼Œæ—¨åœ¨ Energy Exascale Earth System Model (E3SM) çš„è¿è¡Œçº¦æŸä¸‹æ•æ‰å¤šå°ºåº¦éçº¿æ€§ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œä¸¤ç§æ¶æ„åœ¨ç¦»çº¿æµ‹è¯•ä¸­å‡ä¼˜äºæ ‡å‡† U-Net åŸºå‡†ï¼Œå…¶ä¸­ M&M æ¶æ„åœ¨åœ¨çº¿æ··åˆè¿è¡Œä¸­å®ç°äº†æœ€ä¸€è‡´çš„åå·®å‰Šå‡ã€‚è¯¥æœºå™¨å­¦ä¹ å¢å¼º (ML-augmented) é…ç½®åœ¨å¤šå¹´æ¨¡æ‹Ÿä¸­ä¿æŒäº†é«˜åº¦çš„ç¨³å®šæ€§ä¸è®¡ç®—å¯è¡Œæ€§ï¼Œä¸ºæ”¹è¿›ä¼ ç»Ÿ ESM æ€§èƒ½å’Œå®ç°å¯æ‰©å±•æ··åˆå»ºæ¨¡æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03309v1",
      "published_date": "2025-12-02 23:44:49 UTC",
      "updated_date": "2025-12-02 23:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:44:49.840716+00:00"
    },
    {
      "arxiv_id": "2512.03307v1",
      "title": "Robust Tabular Foundation Models",
      "title_zh": "é²æ£’è¡¨æ ¼åŸºç¡€æ¨¡å‹",
      "authors": [
        "Matthew Peroni",
        "Franck Le",
        "Vadim Sheinin"
      ],
      "abstract": "The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¡¨æ ¼åŸºç¡€æ¨¡å‹ (Tabular Foundation Models, TFMs) åœ¨ç»“æ„åŒ–æ•°æ®å¤„ç†ä¸­çš„æ½œåŠ›ï¼Œå¹¶é’ˆå¯¹åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„è¿‡ç¨‹æå‡ºäº†å¢å¼ºé²æ£’æ€§çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…çš„æ ¸å¿ƒæ´å¯Ÿåœ¨äºé€šè¿‡å¯¹ç”Ÿæˆå™¨åˆ†å¸ƒè¿›è¡Œå‚æ•°åŒ–ï¼Œå¯ä»¥å¼•å…¥å¯¹æŠ—é²æ£’æ€§ (Adversarial Robustness) è§†è§’ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¿ƒä½¿ç”Ÿæˆå™¨ç”Ÿæˆå¯¹æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ã€‚ç ”ç©¶å®šä¹‰äº†æœ€ä¼˜æ€§å·®è· (Optimality Gap) æŒ‡æ ‡ï¼Œç”¨ä»¥è¡¡é‡ TFM ä¸ XGBoostã€CatBoost å’Œ Random Forests ç­‰å¼ºåŸºå‡†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚åŸºäºè¿™ä¸€ç†å¿µï¼Œç ”ç©¶æå‡ºäº† Robust Tabular Foundation Models (RTFM)ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„å¯¹æŠ—è®­ç»ƒæ¡†æ¶ã€‚åœ¨ TabPFN V2 åˆ†ç±»å™¨ä¸Šçš„åº”ç”¨è¡¨æ˜ï¼ŒRTFM åœ¨å¹³å‡å½’ä¸€åŒ– AUC ä¸Šæ¯”åŸå§‹æ¨¡å‹å’ŒåŸºå‡†ç®—æ³•æå‡äº†é«˜è¾¾ 6%ã€‚å®éªŒè¯æ˜ä»…éœ€ä¸åˆ° 10 ä¸‡ä¸ªé¢å¤–çš„åˆæˆæ•°æ®é›†å³å¯æ˜¾è‘—ä¼˜åŒ–æ¨¡å‹ï¼Œä¸ºé€šè¿‡åˆæˆæ•°æ®å¯¹ TFMs è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¯¹æŠ—è®­ç»ƒå’Œå¾®è°ƒæä¾›äº†æå…·å‰æ™¯çš„æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Shaping Responsible Synthetic Data in the Era of Foundation Models, AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.03307v1",
      "published_date": "2025-12-02 23:40:39 UTC",
      "updated_date": "2025-12-02 23:40:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:44:48.448250+00:00"
    },
    {
      "arxiv_id": "2512.03300v1",
      "title": "HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction",
      "title_zh": "HydroDCMï¼šé¢å‘è·¨æ°´åº“å…¥åº“æµé‡é¢„æµ‹çš„æ°´æ–‡é¢†åŸŸæ¡ä»¶è°ƒåˆ¶",
      "authors": [
        "Pengfei Hu",
        "Fan Ming",
        "Xiaoxue Han",
        "Chang Lu",
        "Yue Ning",
        "Dan Lu"
      ],
      "abstract": "Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ°´åº“å…¥æµé‡é¢„æµ‹ä¸­çš„åŸŸåç§»(domain shift)é—®é¢˜ï¼Œæå‡ºäº†HydroDCMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè·¨æ°´åº“é¢„æµ‹çš„å¯æ‰©å±•åŸŸæ³›åŒ–(Domain Generalization, DG)æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»ŸDGæŠ€æœ¯åœ¨å¤šåŸŸæ°´æ–‡ç³»ç»Ÿä¸­å› æ°´åº“ç‹¬ç‰¹æ¨¡å¼å’Œç©ºé—´å…ƒæ•°æ®å½±å“è€Œå—é™çš„æŒ‘æˆ˜ï¼ŒHydroDCMåˆ©ç”¨æ°´åº“çš„ç©ºé—´å…ƒæ•°æ®æ„å»ºä¼ªåŸŸæ ‡ç­¾ï¼Œå¼•å¯¼æ¨¡å‹é€šè¿‡å¯¹æŠ—æ€§å­¦ä¹ æå–åŸŸä¸å˜çš„æ—¶é—´ç‰¹å¾ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§çš„æ¡ä»¶è°ƒåˆ¶å±‚ï¼Œç»“åˆç›®æ ‡æ°´åº“çš„å…ƒæ•°æ®å¯¹ç‰¹å¾è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œå®ç°äº†åŸŸä¸å˜æ€§ä¸ç‰¹å®šåœ°ç†ä½ç½®è‡ªé€‚åº”ä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨ç§‘ç½—æ‹‰å¤šæ²³ä¸Šæ¸¸ç›†åœ°30ä¸ªçœŸå®æ°´åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHydroDCMåœ¨å¤šåŸŸæ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›DGåŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2026 workshop (oral) on AI for Environmental Science",
      "pdf_url": "https://arxiv.org/pdf/2512.03300v1",
      "published_date": "2025-12-02 23:27:17 UTC",
      "updated_date": "2025-12-02 23:27:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:44:51.544443+00:00"
    },
    {
      "arxiv_id": "2512.03298v1",
      "title": "Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction",
      "title_zh": "å…·æœ‰æ— åˆ†å¸ƒä¸ç¡®å®šæ€§çš„è‡ªé€‚åº”åŒºåˆ¶åˆ‡æ¢é¢„æµ‹ï¼šæ·±åº¦åˆ‡æ¢çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸ç¬¦åˆé¢„æµ‹",
      "authors": [
        "Echo Diyun LU",
        "Charles Findling",
        "Marianne Clausel",
        "Alessandro Leite",
        "Wei Gong",
        "Pierric Kersaudy"
      ],
      "abstract": "Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—ä¸­åˆ¶åº¦è½¬ç§»(Regime transitions)ç ´åå¹³ç¨³æ€§çš„æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å°†æ·±åº¦åˆ‡æ¢çŠ¶æ€ç©ºé—´æ¨¡å‹(Deep Switching State Space Models)ä¸è‡ªé€‚åº”ç¬¦åˆæ€§æ¨ç†(Adaptive Conformal Inference, ACI)åŠå…¶èšåˆå˜ä½“(AgACI)ç›¸ç»“åˆçš„é¢„æµ‹æ–¹æ³•ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ç¬¦åˆæ€§å°è£…å™¨(conformal wrapper)ï¼Œå¯åº”ç”¨äºS4ã€MC-Dropout GRUã€ç¨€ç–é«˜æ–¯è¿‡ç¨‹(sparse Gaussian processes)ç­‰å¤šç§å¼ºåºåˆ—åŸºå‡†æ¨¡å‹ï¼Œä»¥åœ¨éå¹³ç¨³æ€§å’Œæ¨¡å‹è¯¯è®¾çš„æƒ…å†µä¸‹ç”Ÿæˆå…·æœ‰æœ‰é™æ ·æœ¬è¾¹é™…ä¿è¯(finite-sample marginal guarantees)çš„åœ¨çº¿é¢„æµ‹å¸¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç¬¦åˆæ€§åŒ–(conformalized)é¢„æµ‹å™¨åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡èƒ½å®ç°æ¥è¿‘åä¹‰æ°´å¹³çš„è¦†ç›–ç‡ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰æ€§å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ™®éæå‡äº†é¢„æµ‹å¸¦çš„æ•ˆç‡ï¼Œä¸ºå¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03298v1",
      "published_date": "2025-12-02 23:21:01 UTC",
      "updated_date": "2025-12-02 23:21:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:45:00.445769+00:00"
    },
    {
      "arxiv_id": "2512.03293v1",
      "title": "Prior preferences in active inference agents: soft, hard, and goal shaping",
      "title_zh": "ä¸»åŠ¨æ¨ç†æ™ºèƒ½ä½“ä¸­çš„å…ˆéªŒåå¥½ï¼šè½¯æ€§ã€ç¡¬æ€§ä¸ç›®æ ‡å¡‘é€ ",
      "authors": [
        "Filippo Torresan",
        "Ryota Kanai",
        "Manuel Baltieri"
      ],
      "abstract": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ä¸»åŠ¨æ¨ç†(active inference)æ™ºèƒ½ä½“ä¸­å…ˆéªŒåå¥½(prior preferences)åˆ†å¸ƒçš„å®šä¹‰æ–¹å¼åŠå…¶å¯¹æ¨ç†ä¸å­¦ä¹ çš„å½±å“ã€‚ä½œè€…æå‡ºäº†å››ç§å®šä¹‰åå¥½åˆ†å¸ƒçš„æ–¹æ³•ï¼Œå…·ä½“æ¶‰åŠç¡¬ç›®æ ‡(hard goals)ä¸è½¯ç›®æ ‡(soft goals)çš„é€‰æ‹©ï¼Œä»¥åŠæ˜¯å¦å¼•å…¥ä¸­é—´ç›®æ ‡çš„ç›®æ ‡å¡‘é€ (goal shaping)æŠ€æœ¯ã€‚ç ”ç©¶é€šè¿‡åœ¨ç½‘æ ¼ä¸–ç•Œå¯¼èˆªä»»åŠ¡(grid world navigation task)ä¸­å¯¹æ¯”å››ç§æ™ºèƒ½ä½“çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸åŒåå¥½è®¾å®šåœ¨å¹³è¡¡æ¢ç´¢(exploration)ä¸åˆ©ç”¨(exploitation)é©±åŠ¨åŠ›æ–¹é¢çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›®æ ‡å¡‘é€ (goal shaping)åœ¨ä¿ƒè¿›åˆ©ç”¨è¡Œä¸ºå¹¶å®ç°æœ€ä½³æ•´ä½“ä»»åŠ¡æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ä»£ä»·æ˜¯å‡å°‘äº†å¯¹ç¯å¢ƒçŠ¶æ€è½¬ç§»æ¦‚ç‡(transition dynamics)çš„æ¢ç´¢ä¸å­¦ä¹ ã€‚è¯¥å·¥ä½œç³»ç»Ÿæ€§åœ°åˆ†æäº†åå¥½è§„æ ¼åŒ–å¯¹æ™ºèƒ½ä½“è¡Œä¸ºçš„å½±å“ï¼Œä¸ºä¼˜åŒ–ä¸»åŠ¨æ¨ç†æ¡†æ¶ä¸‹çš„è§„åˆ’ä¸å†³ç­–æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "41 pages, 23 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.03293v1",
      "published_date": "2025-12-02 23:07:24 UTC",
      "updated_date": "2025-12-02 23:07:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:45:01.240379+00:00"
    },
    {
      "arxiv_id": "2512.03280v1",
      "title": "BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark",
      "title_zh": "BlendedNet++ï¼šå¤§è§„æ¨¡ç¿¼èº«èåˆé£è¡Œå™¨ç©ºæ°”åŠ¨åŠ›å­¦æ•°æ®é›†ä¸åŸºå‡†",
      "authors": [
        "Nicholas Sung",
        "Steven Spreizer",
        "Mohamed Elrefaie",
        "Matthew C. Jones",
        "Faez Ahmed"
      ],
      "abstract": "Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† BlendedNet++ï¼Œä¸€ä¸ªé’ˆå¯¹ Blended Wing Body (BWB) é£è¡Œå™¨çš„å¤§è§„æ¨¡ç©ºæ°”åŠ¨åŠ›å­¦æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ ä»£ç†æ¨¡å‹åœ¨ field-resolved datasets ç¨€ç¼ºèƒŒæ™¯ä¸‹ï¼Œå¯¹ pointwise prediction å’Œ inverse design çš„ç ”ç©¶é™åˆ¶ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 12,000 ä¸ªç‹¬ç‰¹çš„å‡ ä½•å½¢çŠ¶ï¼Œå¹¶æä¾› 12,490 ç»„ç¨³æ€ RANS CFD æ¨¡æ‹Ÿç»“æœï¼Œæ¶µç›–äº†é›†æˆçš„åŠ›/åŠ›çŸ©ç³»æ•° (CL, CD, CM) ä»¥åŠå‹åŠ›å’Œè¡¨é¢æ‘©æ“¦ç³»æ•° (Cp, Cfx, Cfy, Cfz) çš„ dense surface fieldsã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…å»ºç«‹äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„ forward-surrogate benchmarkï¼Œç”¨äºè¯„ä¼° GraphSAGE, GraphUNet, PointNet, coordinate Transformer (Transolver-style), FiLMNet å’Œ GNOT ç­‰å…­ç±»æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†åˆ©ç”¨ conditional diffusion model å®ç°ç‰¹å®šå‡é˜»æ¯”çš„ inverse design ä»»åŠ¡ï¼Œå¹¶å°†å…¶ä¸ gradient-based optimization åŠ diffusion-optimization hybrid æ–¹æ¡ˆè¿›è¡Œäº†å¯¹æ¯”ã€‚BlendedNet++ æä¾›äº†ç»Ÿä¸€çš„å‰å‘ä¸é€†å‘è¯„ä¼°åè®®ï¼Œä¸º field-level aerodynamics çš„å¯é‡å¤ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03280v1",
      "published_date": "2025-12-02 22:39:07 UTC",
      "updated_date": "2025-12-02 22:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:45:49.937645+00:00"
    },
    {
      "arxiv_id": "2512.03278v2",
      "title": "Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases",
      "title_zh": "Thucyï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è·¨å…³ç³»å‹æ•°æ®åº“å£°æ˜éªŒè¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Michael Theologitis",
        "Dan Suciu"
      ],
      "abstract": "In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims -- often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases -- typically a few hundred rows -- that conveniently fit within an LLM's context window.\n  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset -- the standard benchmark for fact verification over structured data -- Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†Thucyï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“å£°æ˜éªŒè¯ç³»ç»Ÿ(Multi-Agent System)ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†è·¨æ•°æ®åº“ã€è·¨è¡¨çš„å…³ç³»å‹æ•°æ®éªŒè¯ä»»åŠ¡ã€‚é’ˆå¯¹ç°æœ‰ç³»ç»Ÿå¤§å¤šå±€é™äºå•è¡¨å°è§„æ¨¡æ•°æ®çš„ç°çŠ¶ï¼ŒThucyèƒ½å¤Ÿåœ¨éƒ¨ç½²å‰å¯¹æ•°æ®æºä¿æŒä¸å¯çŸ¥çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»å‘ç°ã€æ£€æŸ¥å¹¶æ¨ç†å¤šä¸ªå…³ç³»æ•°æ®åº“(Relational Databases)ä»¥éªŒè¯å¤æ‚å£°æ˜ã€‚ç³»ç»Ÿåœ¨è¾“å‡ºéªŒè¯ç»“æœçš„åŒæ—¶ä¼šæä¾›æ”¯æ’‘ç»“è®ºçš„å…·ä½“SQLæŸ¥è¯¢ï¼Œä¸ºä¸“ä¸šç”¨æˆ·æä¾›äº†æé«˜çš„é€æ˜åº¦ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•TabFactä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒThucyä»¥94.3%çš„å‡†ç¡®ç‡åˆ·æ–°äº†çºªå½•ï¼Œè¾ƒæ­¤å‰çš„æœ€å…ˆè¿›æŠ€æœ¯(SOTA)æå‡äº†5.6ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™é¡¹æˆæœå……åˆ†å±•ç¤ºäº†æ™ºèƒ½ä½“AI(Agentic AI)åœ¨è‡ªåŠ¨åŒ–éªŒè¯å…¬å…±è®°å½•å’Œç»“æ„åŒ–æ•°æ®é›†å£°æ˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted at AAAI 2026 Workshop on LLM-based Multi-Agent Systems (LaMAS)",
      "pdf_url": "https://arxiv.org/pdf/2512.03278v2",
      "published_date": "2025-12-02 22:35:48 UTC",
      "updated_date": "2026-01-05 20:44:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:45:28.334289+00:00"
    },
    {
      "arxiv_id": "2512.03272v1",
      "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?",
      "title_zh": "ç¬¦å·æ±‚è§£å™¨åœ¨ä½•æ—¶èƒ½å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Ÿ",
      "authors": [
        "Zhiyuan He",
        "Dingmin Wang"
      ],
      "abstract": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¬¦å·æ±‚è§£å™¨(Symbolic Solvers)åœ¨ä½•ç§æƒ…å†µä¸‹èƒ½æœ‰æ•ˆå¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³é•¿é“¾å¼æ€ç»´(Chain of Thought)äº§ç”Ÿçš„Tokenå¼€é”€ä¸è¿‡åº¦æ€è€ƒ(Overthinking)é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¬¦å·æ±‚è§£å™¨é›†æˆæ–¹æ³•ä¸»è¦åœ¨éšå¼æ¨ç†éœ€æ±‚æœ‰é™ä½†æœç´¢ç©ºé—´(Search Space)å·¨å¤§çš„ä»»åŠ¡ä¸­å‘æŒ¥ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-4oç­‰æ¨¡å‹æ›´æ“…é•¿å¤„ç†æ¨ç†æ·±åº¦è¾ƒæµ…çš„æ¼”ç»é—®é¢˜ï¼Œè€Œç¬¦å·æ±‚è§£å™¨åˆ™æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨éœ€è¦åå¤å›æº¯çš„çº¦æŸæ»¡è¶³é—®é¢˜(Constraint Satisfaction Problems)ä¸­çš„è¡¨ç°ã€‚é€šè¿‡æä¾›å£°æ˜å¼ç¤ºä¾‹ï¼Œå³ä½¿æ˜¯CodeLlama-13Bä¹Ÿèƒ½åœ¨å¤æ‚çš„æ–‘é©¬éš¾é¢˜(Zebra puzzles)ä¸Šè¶…è¶ŠGPT-4oã€‚è¯¥å‘ç°æ­ç¤ºäº†å°†æ¨ç†ä»»åŠ¡è½¬åŒ–ä¸ºå¯æ‰§è¡Œä»£ç å¹¶ç»“åˆå¤–éƒ¨ç¬¦å·æ±‚è§£å™¨çš„é€‚ç”¨è¾¹ç•Œä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03272v1",
      "published_date": "2025-12-02 22:23:26 UTC",
      "updated_date": "2025-12-02 22:23:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:45:30.641986+00:00"
    },
    {
      "arxiv_id": "2512.03257v1",
      "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery",
      "title_zh": "PyroFocusï¼šä¸€ç§åŸºäºå¤šå…‰è°±é¥æ„Ÿå½±åƒçš„å®æ—¶é‡ç«æ£€æµ‹æ·±åº¦å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Mark Moussa",
        "Andre Williams",
        "Seth Roffe",
        "Douglas Morton"
      ],
      "abstract": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.\n  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.\n  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PyroFocusï¼Œä¸€ç§ä¸“ä¸ºæœºè½½éƒ¨ç½²è®¾è®¡çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³å¤šå…‰è°±é¥æ„Ÿå½±åƒåœ¨å±±ç«å®æ—¶æ£€æµ‹ä¸­é¢ä¸´çš„é«˜ç»´æ•°æ®å¤„ç†ä¸æœºè½½èµ„æºå—é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨è‡ªå®šä¹‰çš„ Convolutional Neural Networks (CNNs) å’ŒåŸºäº Transformer çš„æ¨¡å‹è¿›è¡Œå¤šç±»åˆ«ç«ç¾åˆ†ç±»ï¼Œéšåæ‰§è¡Œç«æºè¾å°„åŠŸç‡ (Fire Radiative Power, FRP) å›å½’æˆ–åˆ†å‰²ï¼Œä»¥æœ‰æ•ˆé™ä½æ¨ç†å»¶è¿Ÿå¹¶èŠ‚çœè®¡ç®—æˆæœ¬ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨æ¥è‡ª NASA çš„ MODIS/ASTER Airborne Simulator (MASTER) æ•°æ®å¯¹å¤šç§æ¶æ„çš„å‡†ç¡®æ€§ã€å»¶è¿Ÿå’Œèµ„æºæ•ˆç‡è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPyroFocus åœ¨æ£€æµ‹ç²¾åº¦ä¸å¤„ç†é€Ÿåº¦ä¹‹é—´è¾¾æˆäº†å“è¶Šçš„å¹³è¡¡ï¼Œä¸ºæœªæ¥å±±ç«ç›‘æµ‹ä»»åŠ¡ä¸­çš„å®æ—¶è¾¹ç¼˜éƒ¨ç½² (Edge Deployment) æä¾›äº†æå…·æ½œåŠ›çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03257v1",
      "published_date": "2025-12-02 21:59:45 UTC",
      "updated_date": "2025-12-02 21:59:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:45:19.536344+00:00"
    },
    {
      "arxiv_id": "2512.10975v1",
      "title": "Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems",
      "title_zh": "äºº-æ™ºèƒ½ä½“ç³»ç»Ÿä¸­åŸºäºæ™ºèƒ½ä½“æ¨¡å—åŒ–å­¦ä¹ çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "Matvey Nepomnyaschiy",
        "Oleg Pereziabov",
        "Anvar Tliamov",
        "Stanislav Mikhailov",
        "Ilya Afanasyev"
      ],
      "abstract": "Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ« (Multimodal Emotion Recognition) æ¨¡å‹åœ¨è®­ç»ƒå’Œç»´æŠ¤ä¸­é¢ä¸´çš„é«˜è®¡ç®—å¼€é”€åŠæ¨¡æ€å˜æ›´ä¸çµæ´»ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“çš„æ¨¡å—åŒ–å­¦ä¹ æ¡†æ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ¯ä¸ªæ¨¡æ€ç¼–ç å™¨å’Œèåˆåˆ†ç±»å™¨å‡ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“è¿è¡Œï¼Œå¹¶ç”±ä¸­å¤®ç›‘ç£è€… (Central Supervisor) è¿›è¡Œç»Ÿä¸€åè°ƒã€‚è¿™ç§æ¶æ„æ”¯æŒæ¨¡æ€çš„æ¨¡å—åŒ–é›†æˆä¸æ— ç¼æ›¿æ¢ï¼Œä¾‹å¦‚å¯ä»¥çµæ´»å¼•å…¥åŸºäº emotion2vec çš„éŸ³é¢‘ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆé™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿæ‹…ã€‚é€šè¿‡åŒ…å«è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡æ€çš„ Proof-of-Concept å®éªŒï¼Œç ”ç©¶éªŒè¯äº†åˆ†ç±»å™¨ä½œä¸ºå…±äº«å†³ç­–æ™ºèƒ½ä½“åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶çš„å¯è¡Œæ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œè¿˜ä¸ºæ„å»ºæ›´å…·çµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œæ˜“ç»´æŠ¤æ€§çš„äººæœºäº¤äº’ (Human-Agent Interaction) æ„ŸçŸ¥æ¨¡å—æä¾›äº†æ–°çš„è®¾è®¡è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.10975v1",
      "published_date": "2025-12-02 21:47:00 UTC",
      "updated_date": "2025-12-02 21:47:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:23.939639+00:00"
    },
    {
      "arxiv_id": "2512.03248v1",
      "title": "Learning Network Sheaves for AI-native Semantic Communication",
      "title_zh": "é¢å‘ AI åŸç”Ÿè¯­ä¹‰é€šä¿¡çš„ç½‘ç»œå±‚å­¦ä¹ ",
      "authors": [
        "Enrico Grimaldi",
        "Mario Edoardo Pandolfo",
        "Gabriele D'Acunto",
        "Sergio Barbarossa",
        "Paolo Di Lorenzo"
      ],
      "abstract": "Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢å‘ AI-native 6G ç½‘ç»œçš„è¯­ä¹‰é€šä¿¡æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¼‚æ„ AI æ™ºèƒ½ä½“åœ¨äº¤æ¢å‹ç¼©æ½œç©ºé—´ï¼ˆlatent-space representationsï¼‰è¡¨ç¤ºæ—¶é¢ä¸´çš„è¯­ä¹‰å™ªå£°å’Œå«ä¹‰ä¿ç•™æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å­¦ä¹ ç½‘ç»œå±‚ï¼ˆnetwork sheafï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ é€šä¿¡æ‹“æ‰‘å’Œå¯¹é½æ˜ å°„ï¼ˆalignment mapsï¼‰æ¥ç®¡ç†æ™ºèƒ½ä½“é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œå¹¶ä¸ºå…¶é…å¤‡äº†æ­£äº¤æ˜ å°„ã€‚è¯¥æ–¹æ¡ˆç»“åˆäº†è¯­ä¹‰å»å™ªå’Œç«¯åˆ°ç«¯å‹ç¼©æ¨¡å—ï¼Œé€šè¿‡æ„å»ºå…±äº«çš„å…¨å±€è¯­ä¹‰ç©ºé—´ï¼Œåˆ©ç”¨éå‡¸å­—å…¸å­¦ä¹ ï¼ˆdictionary learningï¼‰è¿­ä»£æ±‚è§£å‡ºæ¯ä¸ªæ™ºèƒ½ä½“æ½œç©ºé—´çš„ç¨€ç–ç»“æ„åŒ–è¡¨ç¤ºã€‚åœ¨é¢„è®­ç»ƒçœŸå®å›¾åƒæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆä¿ƒè¿›äº† AI æ™ºèƒ½ä½“çš„å¯¹é½å’Œè¯­ä¹‰èšç±»æå–ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒä¸‹æ¸¸ä»»åŠ¡é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œä¸ºå¤„ç†æ™ºèƒ½ä½“é—´çš„è¯­ä¹‰å¼‚æ„æ€§æä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„æ–°è§è§£ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03248v1",
      "published_date": "2025-12-02 21:36:44 UTC",
      "updated_date": "2025-12-02 21:36:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:04.340987+00:00"
    },
    {
      "arxiv_id": "2512.03244v1",
      "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
      "title_zh": "SPARKï¼šé¢å‘æ— å‚è€ƒå¼ºåŒ–å­¦ä¹ çš„é€æ­¥è¿‡ç¨‹æ„ŸçŸ¥å¥–åŠ±",
      "authors": [
        "Salman Rahman",
        "Sruthi Gorantla",
        "Arpit Gupta",
        "Swastik Roy",
        "Nanyun Peng",
        "Yang Liu"
      ],
      "abstract": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPARKï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRMs)ä¾èµ–æ˜‚è´µçš„åˆ†æ­¥æ ‡æ³¨æˆ–æ ‡å‡†å‚è€ƒç­”æ¡ˆé—®é¢˜çš„ä¸‰é˜¶æ®µæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹äº§ç”Ÿå¤šæ ·åŒ–è§£æ³•ï¼Œå¹¶ç»“åˆå¹¶è¡Œæ‰©å±•(Self-Consistency)ä¸é¡ºåºæ‰©å±•(Meta-Critique)è¿›è¡Œè‡ªåŠ¨éªŒè¯ã€‚ç¬¬äºŒé˜¶æ®µå°†è¿™äº›éªŒè¯è¾“å‡ºä½œä¸ºåˆæˆè®­ç»ƒæ•°æ®æ¥å¾®è°ƒç”Ÿæˆå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡èšåˆæ­¥éª¤çº§éªŒè¯ç”Ÿæˆçš„è®­ç»ƒæ•°æ®åœ¨ProcessBenchä¸Šå–å¾—äº†67.5çš„F1åˆ†æ•°ï¼Œæ€§èƒ½ä¼˜äºåŸºäºæ ‡å‡†ç­”æ¡ˆçš„ç›‘ç£è®­ç»ƒå’ŒGPT-4oã€‚åœ¨æœ€åçš„å¼ºåŒ–å­¦ä¹ å®éªŒä¸­ï¼Œç ”ç©¶è€…å¼•å…¥äº†å¸¦æœ‰é“¾å¼æ€ç»´éªŒè¯çš„PRM-CoTå¹¶é…åˆæ ¼å¼çº¦æŸä»¥é˜²æ­¢å¥–åŠ±ä½œå¼Š(Reward Hacking)ã€‚åŸºäºQwen2.5-Math-7Bçš„æµ‹è¯•è¡¨æ˜ï¼ŒSPARKåœ¨å…­é¡¹æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†47.4%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸºäºæ ‡å‡†ç­”æ¡ˆçš„RLVRæ–¹æ³•ã€‚è¯¥æˆæœè¯æ˜äº†æ— å‚è€ƒå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¯è¡Œæ€§åŠå…¶åœ¨ç¼ºä¹æ ‡å‡†ç­”æ¡ˆé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03244v1",
      "published_date": "2025-12-02 21:30:47 UTC",
      "updated_date": "2025-12-02 21:30:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:08.647918+00:00"
    },
    {
      "arxiv_id": "2512.03238v1",
      "title": "How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy",
      "title_zh": "å¦‚ä½•å®ç°æ•°æ®çš„å·®åˆ†éšç§åŒ–ï¼šåŸºäºå·®åˆ†éšç§çš„åˆæˆæ•°æ®ç”Ÿæˆå®è·µæŒ‡å—",
      "authors": [
        "Natalia Ponomareva",
        "Zheng Xu",
        "H. Brendan McMahan",
        "Peter Kairouz",
        "Lucas Rosenblatt",
        "Vincent Cohen-Addad",
        "CristÃ³bal GuzmÃ¡n",
        "Ryan McKenna",
        "Galen Andrew",
        "Alex Bie",
        "Da Yu",
        "Alex Kurakin",
        "Morteza Zadimoghaddam",
        "Sergei Vassilvitskii",
        "Andreas Terzis"
      ],
      "abstract": "High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \\emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.\n  In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.",
      "tldr_zh": "è¯¥è®ºæ–‡æä¾›äº†ä¸€ä»½å…³äºåˆ©ç”¨ Differential Privacy (DP) ç”Ÿæˆ Synthetic Data çš„å®è·µæŒ‡å—ï¼Œæ—¨åœ¨è§£å†³ AI ç ”å‘ä¸­é«˜è´¨é‡æ•°æ®åŒ®ä¹ä¸ç”¨æˆ·éšç§é£é™©ä¹‹é—´çš„çŸ›ç›¾ã€‚æ–‡ç« ç³»ç»Ÿåœ°æ¢è®¨äº† DP Synthetic Data çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¼ºè°ƒå…¶ä½œä¸ºä¿æŠ¤ç”¨æˆ·éšç§çš„é‡‘æ ‡å‡†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ›¿ä»£ä¼ ç»Ÿçš„ç®€å•åŒ¿ååŒ–è§„åˆ™ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†é€‚ç”¨äºå›¾åƒ (Image)ã€è¡¨æ ¼ (Tabular)ã€æ–‡æœ¬ (Text) ä»¥åŠå»ä¸­å¿ƒåŒ– (Decentralized) æ¨¡æ€çš„å°–ç«¯æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œä½œè€…è¯¦ç»†æ¦‚è¿°äº†æ„å»º DP åˆæˆæ•°æ®ç³»ç»Ÿæ‰€éœ€çš„å…³é”®ç»„ä»¶ï¼Œæ¶µç›–ä»æ•æ„Ÿæ•°æ®å¤„ç†ã€æ•°æ®å‡†å¤‡åˆ°ä½¿ç”¨è¿½è¸ªåŠç»éªŒæ€§éšç§æµ‹è¯•çš„å…¨ç”Ÿå‘½å‘¨æœŸã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æ€»ç»“æœ€æ–°æŠ€æœ¯ç°çŠ¶å¹¶æä¾›ç³»ç»Ÿæ€§çš„æ¡†æ¶å»ºè®®ï¼Œæ—¨åœ¨æ¨åŠ¨ DP åˆæˆæ•°æ®æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å¹¶å¢å¼ºè¯¥é¢†åŸŸçš„ç ”ç©¶ä¿¡ä»»åº¦ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03238v1",
      "published_date": "2025-12-02 21:14:39 UTC",
      "updated_date": "2025-12-02 21:14:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:15.660814+00:00"
    },
    {
      "arxiv_id": "2512.03197v1",
      "title": "InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation",
      "title_zh": "InvertiTuneï¼šé¢å‘ç»æµé«˜æ•ˆå•æ¬¡æ–‡æœ¬åˆ°çŸ¥è¯†å›¾è°±ç”Ÿæˆçš„é«˜è´¨é‡æ•°æ®åˆæˆ",
      "authors": [
        "Faezeh Faez",
        "Marzieh S. Tahaei",
        "Yaochen Hu",
        "Ali Pourranjbar",
        "Mahdi Biparva",
        "Mark Coates",
        "Yingxue Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨çŸ¥è¯†å›¾è°±æ„å»º(Text2KG)é¢†åŸŸä¸­ç°æœ‰æ–¹æ³•ä¾èµ–è¿­ä»£æç¤º(iterative prompting)å¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ˜“é—æ¼å¤æ‚å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†InvertiTuneæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å—æ§æ•°æ®ç”Ÿæˆæµæ°´çº¿ä¸ç›‘ç£å¾®è°ƒ(SFT)ï¼Œé€šè¿‡ä»å¤§å‹çŸ¥è¯†åº“ç³»ç»Ÿæå–å­å›¾å¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åå‘ç”Ÿæˆå¯¹åº”çš„è‡ªç„¶æ–‡æœ¬æè¿°ï¼Œä»è€Œäº§ç”Ÿæ›´ç¬¦åˆç°å®åœºæ™¯çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚ç›¸è¾ƒäºç›´æ¥ä»æ–‡æœ¬ç”Ÿæˆå›¾è°±ï¼Œè¿™ç§åå‘ç”Ÿæˆç­–ç•¥æ›´å¥‘åˆLLMsçš„ä¼˜åŠ¿ï¼Œæ”¯æŒå¯¹è½»é‡çº§æ¨¡å‹è¿›è¡Œæœ‰æ•ˆå¾®è°ƒä»¥å®ç°å•æ¬¡(single-shot)çŸ¥è¯†å›¾è°±æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿæˆçš„æ•°æ®é›†CE12kä¸Šï¼ŒInvertiTuneçš„æ€§èƒ½ä¼˜äºå‚æ•°é‡æ›´å¤§çš„éå¾®è°ƒæ¨¡å‹åŠç°æœ‰çš„å…ˆè¿›Text2KGæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è·¨æ•°æ®é›†æµ‹è¯•é›†CrossEval-1200ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†é«˜è´¨é‡åˆæˆæ•°æ®åœ¨æå‡Text2KGç³»ç»Ÿæ•ˆç‡ä¸æ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03197v1",
      "published_date": "2025-12-02 19:51:28 UTC",
      "updated_date": "2025-12-02 19:51:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:55.148947+00:00"
    },
    {
      "arxiv_id": "2512.03196v2",
      "title": "Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization",
      "title_zh": "åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„è¶…å¼ºæ¢¯åº¦å¼¥æ•£ç£å…±æŒ¯æˆåƒåœ¨å‰åˆ—è…ºç™Œè¡¨å¾ä¸­çš„åº”ç”¨",
      "authors": [
        "Tanishq Patil",
        "Snigdha Sen",
        "Kieran G. Foley",
        "Fabrizio Fasano",
        "Chantal M. W. Tax",
        "Derek K. Jones",
        "Mara Cercignani",
        "Marco Palombo",
        "Paddy J. Slator",
        "Eleftheria Panagiotaki"
      ],
      "abstract": "Diffusion MRI (dMRI) enables non-invasive assessment of prostate microstructure but conventional dMRI metrics such as the Apparent Diffusion Coefficient in multiparametric MRI and reflect a mixture of underlying tissues features rather than distinct histologic characteristics. Integrating dMRI with the compartment-based biophysical VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) framework offers richer microstructural insights, though clinical gradient systems (40-80 mT/m) often suffer from poor signal-to-noise ratio at stronger diffusion weightings due to prolonged echo times. Ultra-strong gradients (e.g., 300 mT/m) can mitigate these limitations by improving SNR and contrast-to-noise ratios. This study investigates whether physics-informed self-supervised VERDICT (ssVERDICT) fitting when combined with ultra-strong gradient data, enhances prostate microstructural characterization relative to current fitting approaches and clinical gradient systems. We developed enhanced ssVERDICT fitting approaches using dense multilayer perceptron and convolutional U-Net architectures, comparing them against non-linear least-squares (NLLS) VERDICT fitting, original ssVERDICT implementation, and Diffusion Kurtosis Imaging across clinical- to ultra-strong gradient systems. For the same ultra-strong gradient data, Dense ssVERDICT outperformed NLLS VERDICT, boosting median CNR by 47%, cutting inter-patient Coefficient of Variation by 52%, and reducing pooled $f_{ic}$ variation by 50%. Overall, Dense ssVERDICT delivered the highest CNR, the most stable parameter estimates, and the clearest tumour-normal contrast compared with conventional fitting methods and clinical gradient systems. These findings underscore that meaningful gains in non-invasive prostate cancer characterization arise from the combination of advanced gradient systems and deep learning-based modelling.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¶…å¼ºæ¢¯åº¦(Ultra-strong gradients)æ‰©æ•£ç£å…±æŒ¯æˆåƒ(dMRI)ç»“åˆè‡ªç›‘ç£å­¦ä¹ åœ¨åˆ—è…ºç™Œç‰¹å¾è¡¨å¾ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å¸¸è§„dMRIæŒ‡æ ‡å¦‚ADCç¼ºä¹ç»„ç»‡ç‰¹å¼‚æ€§çš„å±€é™ã€‚ç ”ç©¶åˆ©ç”¨äº†åŸºäºæˆ¿å®¤æ¨¡å‹çš„ç”Ÿç‰©ç‰©ç†æ¡†æ¶VERDICTï¼Œå¹¶é’ˆå¯¹ä¸´åºŠæ¢¯åº¦ç³»ç»Ÿä¿¡å™ªæ¯”(SNR)ä¸è¶³çš„é—®é¢˜ï¼Œå¼•å…¥äº†300 mT/mçš„è¶…å¼ºæ¢¯åº¦æ•°æ®ã€‚é€šè¿‡å¼€å‘åŸºäºç¨ å¯†å¤šå±‚æ„ŸçŸ¥å™¨(Dense MLP)å’Œå·ç§¯U-Netæ¶æ„çš„ç‰©ç†å¢å¼ºå‹ssVERDICTæ‹Ÿåˆæ–¹æ³•ï¼Œå¹¶ä¸éçº¿æ€§æœ€å°äºŒä¹˜æ³•(NLLS)åŠDiffusion Kurtosis Imagingè¿›è¡Œå¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDense ssVERDICTåœ¨è¶…å¼ºæ¢¯åº¦æ•°æ®ä¸‹è¡¨ç°æœ€ä¼˜ï¼Œä½¿ä¸­ä½å¯¹æ¯”å™ªå£°æ¯”(CNR)æå‡äº†47%ï¼Œå¹¶å°†æ‚£è€…é—´çš„å˜å¼‚ç³»æ•°(Coefficient of Variation)é™ä½äº†52%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†$f_{ic}$å‚æ•°çš„å˜å¼‚ï¼Œåœ¨å‚æ•°ä¼°è®¡ç¨³å®šæ€§å’Œè‚¿ç˜¤-æ­£å¸¸ç»„ç»‡å¯¹æ¯”åº¦æ–¹é¢å‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å…ˆè¿›æ¢¯åº¦ç³»ç»Ÿä¸æ·±åº¦å­¦ä¹ å»ºæ¨¡çš„ç»“åˆèƒ½å¤§å¹…æå‡æ— åˆ›å‰åˆ—è…ºç™Œçš„å¾®ç»“æ„è¡¨å¾èƒ½åŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "25 pages, 14 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.03196v2",
      "published_date": "2025-12-02 19:49:49 UTC",
      "updated_date": "2026-01-21 12:20:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:18.236648+00:00"
    },
    {
      "arxiv_id": "2512.03176v1",
      "title": "Plantain: Plan-Answer Interleaved Reasoning",
      "title_zh": "Plantainï¼šè®¡åˆ’-å›ç­”äº¤æ›¿å¼æ¨ç†",
      "authors": [
        "Anthony Liang",
        "Jonathan Berant",
        "Adam Fisch",
        "Abhimanyu Goyal",
        "Kalpesh Krishna",
        "Jacob Eisenstein"
      ],
      "abstract": "Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard \"think-then-answer\" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆå®Œæ•´å›å¤å‰è€—æ—¶è¿‡é•¿ä¸”ç¼ºä¹ç”¨æˆ·åé¦ˆæœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº† Interleaved Reasoning (IR) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡äº¤æ›¿è¿›è¡Œæ€è€ƒä¸ç”Ÿæˆä¸­é—´å›å¤ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„ think-then-answer æ¨¡å¼ï¼Œåœ¨ä¸ç‰ºç‰²å›å¤è´¨é‡çš„å‰æä¸‹æ˜¾è‘—é™ä½äº†ç”¨æˆ·çš„æ„ŸçŸ¥å»¶è¿Ÿã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¨å‡ºäº† Plantain (Plan-Thought-Answer Interleaving) æ–¹æ¡ˆï¼Œè¦æ±‚æ¨¡å‹é¦–å…ˆç”Ÿæˆä¸€ä¸ªæ˜¾å¼çš„ã€åˆ†æ­¥éª¤çš„ä»»åŠ¡æ‰§è¡Œ Planï¼Œä»è€Œå…è®¸ç”¨æˆ·åœ¨åç»­æ¨ç†æ­¥éª¤å‰è¿›è¡Œå¹²é¢„å’Œçº é”™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlantain åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†çº¦ 6% çš„ pass@1 å‡†ç¡®ç‡æå‡ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å°†é¦–ä¸ªå›å¤çš„ç”Ÿæˆæ—¶é—´ (time-to-first-response) ç¼©çŸ­äº† 60% ä»¥ä¸Šï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€å¯å¹²é¢„çš„äº¤äº’å¼ AI æ¨ç†ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03176v1",
      "published_date": "2025-12-02 19:22:12 UTC",
      "updated_date": "2025-12-02 19:22:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:27.843130+00:00"
    },
    {
      "arxiv_id": "2512.03173v1",
      "title": "Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping",
      "title_zh": "Culture Affordance Atlasï¼šé€šè¿‡åŠŸèƒ½æ˜ å°„åè°ƒç‰©ä½“å¤šæ ·æ€§",
      "authors": [
        "Joan Nwatu",
        "Longju Bai",
        "Oana Ignat",
        "Rada Mihalcea"
      ],
      "abstract": "Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸»æµ Vision-Language (VL) æ•°æ®é›†å­˜åœ¨çš„æ–‡åŒ–åè§é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶è¿‡åº¦å€¾å‘äºè¥¿æ–¹é«˜æ”¶å…¥èƒŒæ™¯ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶å¯¼è‡´æ€§èƒ½ä¸å‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä»¥åŠŸèƒ½ä¸ºä¸­å¿ƒçš„æ–°å‹æ¡†æ¶ï¼Œæ ¹æ®ç‰©ä½“åœ¨ä¸åŒæ–‡åŒ–å’Œç»æµèƒŒæ™¯ä¸‹æ‰€å®ç°çš„åŠŸèƒ½è¿›è¡Œåˆ†ç±»ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶è€…æ„å»ºäº† Culture Affordance Atlasï¼Œå¯¹æ¶µç›–46é¡¹åŠŸèƒ½å’Œ288ä¸ªç‰©ä½“çš„ Dollar Street æ•°æ®é›†è¿›è¡Œäº†æ–‡åŒ–è½åœ°å¼çš„é‡æ„ä¸é‡æ–°æ ‡æ³¨ã€‚åˆ©ç”¨ CLIP æ¨¡å‹è¿›è¡Œçš„å®è¯åˆ†æè¯æ˜ï¼Œä»¥åŠŸèƒ½ä¸ºä¸­å¿ƒçš„æ ‡ç­¾èƒ½å¤Ÿå°†é«˜ä½æ”¶å…¥ç¾¤ä½“é—´çš„ç¤¾ä¼šç»æµæ€§èƒ½å·®è·ä¸­ä½æ•°æ˜¾è‘—é™ä½6ä¸ªç™¾åˆ†ç‚¹ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨ä½æ”¶å…¥èƒŒæ™¯ä¸‹çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ­ç¤ºäº†å¤§é‡è¢«ä¸»æµ VL æ•°æ®é›†å¿½è§†çš„æ–‡åŒ–å…³é”®ç‰©ä½“ï¼Œä¸ºå¼€å‘åŒ…å®¹æ€§æ•°æ®é›†å’Œå…¬å¹³çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03173v1",
      "published_date": "2025-12-02 19:16:39 UTC",
      "updated_date": "2025-12-02 19:16:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:56.144097+00:00"
    },
    {
      "arxiv_id": "2601.04195v1",
      "title": "MedPI: Evaluating AI Systems in Medical Patient-facing Interactions",
      "title_zh": "MedPIï¼šè¯„ä¼°é¢å‘æ‚£è€…åŒ»ç–—äº¤äº’ä¸­çš„AIç³»ç»Ÿ",
      "authors": [
        "Diego Fajardo V.",
        "Oleksii Proniakin",
        "Victoria-Elisabeth Gruber",
        "Razvan Marinescu"
      ],
      "abstract": "We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized \"vanilla clinician\" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedPIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»æ‚£å¯¹è¯ (patient-clinician conversations) ä¸­è¡¨ç°çš„é«˜ç»´åŸºå‡†ã€‚MedPI ä¸åŒäºä¼ ç»Ÿçš„å•è½®é—®ç­”åŸºå‡†ï¼Œå®ƒä¾æ®è®¤è¯æ ‡å‡†åˆ¶å®šçš„ç»†ç²’åº¦é‡è¡¨ï¼Œä»åŒ»ç–—æµç¨‹ã€æ²»ç–—å®‰å…¨ã€ç»“æœåŠåŒ»æ‚£æ²Ÿé€šç­‰ 105 ä¸ªç»´åº¦è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚è¯¥åŸºå‡†ç”±äº”ä¸ªæ ¸å¿ƒå±‚ç»„æˆï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿç”µå­å¥åº·è®°å½•çš„æ‚£è€…æ•°æ®åŒ… (Patient Packets)ã€å…·å¤‡è®°å¿†ä¸æƒ…æ„Ÿçš„ AI Patientã€æ¶µç›–å¤šé‡ç›®æ ‡çš„ä»»åŠ¡çŸ©é˜µã€æ˜ å°„è‡³ ACGME èƒœä»»åŠ›çš„è¯„ä»·æ¡†æ¶ï¼Œä»¥åŠæä¾›åˆ†å€¼ä¸ä¾æ®çš„ AI Judgesã€‚ç ”ç©¶å›¢é˜Ÿå¯¹åŒ…æ‹¬ GPT-5ã€Gemini 2.5 Proã€Claude Opus 4.1 åœ¨å†…çš„ 9 æ¬¾æ——èˆ°æ¨¡å‹è¿›è¡Œäº†è¶…è¿‡ 7,000 åœºå¯¹è¯çš„ç³»ç»Ÿæ€§æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°ç»´åº¦ä¸Šçš„è¡¨ç°å‡ä¸ç†æƒ³ï¼Œå°¤å…¶åœ¨é‰´åˆ«è¯Šæ–­ (differential diagnosis) æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚æ­¤é¡¹å·¥ä½œä¸ºæœªæ¥ä¼˜åŒ–ç”¨äºåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å»ºè®®çš„ AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä¼°å‚è€ƒä¸æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.04195v1",
      "published_date": "2025-12-02 19:10:06 UTC",
      "updated_date": "2025-12-02 19:10:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:03.242980+00:00"
    },
    {
      "arxiv_id": "2512.03042v2",
      "title": "PPTArena: A Benchmark for Agentic PowerPoint Editing",
      "title_zh": "PPTArenaï¼šPowerPoint æ™ºèƒ½ä½“ç¼–è¾‘åŸºå‡†",
      "authors": [
        "Michael Ofengenden",
        "Yunze Man",
        "Ziqi Pang",
        "Yu-Xiong Wang"
      ],
      "abstract": "We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PPTArenaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸‹è¿›è¡Œ PowerPoint ç¼–è¾‘èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ç°æœ‰çš„å›¾åƒç”Ÿæˆæˆ–æ–‡æœ¬è½¬å¹»ç¯ç‰‡ä»»åŠ¡ä¸åŒï¼ŒPPTArena èšç„¦äºå¯¹çœŸå®å¹»ç¯ç‰‡è¿›è¡ŒåŸä½ç¼–è¾‘ï¼Œæ¶µç›–äº† 100 ä¸ªæ¼”ç¤ºæ–‡ç¨¿ã€2125 å¼ å¹»ç¯ç‰‡ä»¥åŠè¶…è¿‡ 800 ä¸ªé’ˆå¯¹æ–‡æœ¬ã€å›¾è¡¨ã€è¡¨æ ¼ã€åŠ¨ç”»å’Œæ¯ç‰ˆæ ·å¼çš„ç›®æ ‡ç¼–è¾‘ä»»åŠ¡ã€‚ä¸ºäº†å®ç°å¯é è¯„ä¼°ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§åŒé‡ VLM-as-judge è¯„ä»·ä½“ç³»ï¼Œé€šè¿‡ç»“åˆç»“æ„åŒ–å·®å¼‚å¯¹æ¯”ä¸å¹»ç¯ç‰‡å›¾åƒåˆ†æï¼ŒåŒæ­¥è¡¡é‡æ™ºèƒ½ä½“çš„æŒ‡ä»¤éµå¾ªåº¦ä¸è§†è§‰è´¨é‡ã€‚åŸºäºè¯¥åŸºå‡†ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ç»“æ„æ„ŸçŸ¥æ™ºèƒ½ä½“ PPTPilotï¼Œå®ƒèƒ½å¤Ÿè§„åˆ’è¯­ä¹‰ç¼–è¾‘åºåˆ—ï¼Œå¹¶çµæ´»è°ƒç”¨é«˜çº§ç¼–ç¨‹å·¥å…·ä¸åº•å±‚çš„ XML æ“ä½œæ¥å®ç°ç²¾ç¡®æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥è¿­ä»£å¼çš„ plan-edit-check å¾ªç¯éªŒè¯æœºåˆ¶ï¼ŒPPTPilot åœ¨å¤åˆä»»åŠ¡å’Œå¸ƒå±€æ•æ„Ÿç¼–è¾‘ä¸­çš„è¡¨ç°ä¼˜äºä¸»æµå•†ä¸šæ™ºèƒ½ä½“åŠå‰æ²¿ VLM ç³»ç»Ÿï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¶…è¿‡ 10%ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®éªŒç»“æœä¹Ÿè¡¨æ˜ç°æœ‰æ™ºèƒ½ä½“åœ¨å¤„ç†é•¿ç¨‹ã€æ–‡æ¡£è§„æ¨¡çš„å¤æ‚ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥å¯é çš„è‡ªåŠ¨åŒ–æ¼”ç¤ºæ–‡ç¨¿ç¼–è¾‘æä¾›äº†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://ppt-arena.onrender.com/evaluation GitHub: https://github.com/michaelofengend/PPTArena",
      "pdf_url": "https://arxiv.org/pdf/2512.03042v2",
      "published_date": "2025-12-02 18:59:50 UTC",
      "updated_date": "2025-12-05 22:58:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:46:37.938590+00:00"
    },
    {
      "arxiv_id": "2512.03040v2",
      "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
      "title_zh": "Video4Spatialï¼šè¿ˆå‘åŸºäºä¸Šä¸‹æ–‡å¼•å¯¼è§†é¢‘ç”Ÿæˆçš„è§†è§‰ç©ºé—´æ™ºèƒ½",
      "authors": [
        "Zeqi Xiao",
        "Yiwei Zhao",
        "Lingxiao Li",
        "Yushi Lan",
        "Ning Yu",
        "Rahul Garg",
        "Roshni Cooper",
        "Mohammad H. Taghavi",
        "Xingang Pan"
      ],
      "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¯å¦ä»…é€šè¿‡è§†è§‰æ•°æ®å³å¯å…·å¤‡è§†è§‰ç©ºé—´æ™ºèƒ½(visuospatial intelligence)ï¼Œå¹¶æå‡ºäº†Video4Spatialæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»…åŸºäºè§†é¢‘åœºæ™¯ä¸Šä¸‹æ–‡çš„è§†é¢‘æ‰©æ•£æ¨¡å‹(video diffusion models)ï¼Œåœ¨ä¸ä¾èµ–æ·±åº¦æˆ–ä½å§¿ç­‰è¾…åŠ©æ¨¡æ€çš„æƒ…å†µä¸‹å¤„ç†å¤æ‚çš„ç©ºé—´ä»»åŠ¡ã€‚ç ”ç©¶è€…åœ¨åœºæ™¯å¯¼èˆª(scene navigation)å’Œç›®æ ‡å®šä½(object grounding)ä¸¤é¡¹ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶ï¼Œè¿™äº›ä»»åŠ¡è¦æ±‚æ¨¡å‹åœ¨ä¿æŒ3Då‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶éµå¾ªç›¸æœºä½å§¿æŒ‡ä»¤ï¼Œå¹¶è¿›è¡Œè¯­ä¹‰å®šä½ä¸è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo4Spatialè¡¨ç°å‡ºå¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°ç«¯åˆ°ç«¯çš„è·¯å¾„è§„åˆ’ï¼Œå¹¶å±•ç°å‡ºå¯¹é•¿ä¸Šä¸‹æ–‡åŠåŸŸå¤–(out-of-domain)ç¯å¢ƒçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æˆæœæœ‰æ•ˆæ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹å‘é€šç”¨è§†è§‰ç©ºé—´æ¨ç†(visuospatial reasoning)é¢†åŸŸçš„è¿ˆè¿›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page at https://xizaoqu.github.io/video4spatial/",
      "pdf_url": "https://arxiv.org/pdf/2512.03040v2",
      "published_date": "2025-12-02 18:59:44 UTC",
      "updated_date": "2025-12-11 08:18:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:49.966954+00:00"
    },
    {
      "arxiv_id": "2512.03127v1",
      "title": "Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra",
      "title_zh": "ç”¨äºæ ¸ç£å…±æŒ¯æ³¢è°±å°åˆ†å­ç»“æ„è§£æçš„åŸå­æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Ziyu Xiong",
        "Yichi Zhang",
        "Foyez Alauddin",
        "Chu Xin Cheng",
        "Joon Soo An",
        "Mohammad R. Seyedsayamdost",
        "Ellen D. Zhong"
      ],
      "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChefNMR (CHemical Elucidation From NMR)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ 1D NMR å…‰è°±å’ŒåŒ–å­¦å¼ç›´æ¥é¢„æµ‹æœªçŸ¥å°åˆ†å­ç»“æ„çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿå°†ç»“æ„è§£æå»ºæ¨¡ä¸ºä¸€ç§æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨äº†æ„å»ºåœ¨ non-equivariant transformer æ¶æ„ä¸Šçš„ atomic diffusion modelã€‚ä¸ºäº†æ¶µç›–å¤©ç„¶äº§ç‰©ä¸­å¤æ‚çš„åŒ–å­¦åŸºå›¢ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨æ¨¡æ‹ŸæŠ€æœ¯ç”Ÿæˆäº†è¶…è¿‡ 111,000 ç§å¤©ç„¶äº§ç‰©çš„ 1D NMR å…‰è°±æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChefNMR åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤©ç„¶äº§ç‰©åŒ–åˆç‰©æ—¶è¡¨ç°å‡ºè‰²ï¼Œå…¶ç»“æ„é¢„æµ‹å‡†ç¡®ç‡è¶…è¿‡ 65%ã€‚è¯¥å·¥ä½œæ˜¾è‘—æ¨è¿›äº†è‡ªåŠ¨åŒ–å°åˆ†å­ç»“æ„è§£æè¿™ä¸€é‡å¤§æŒ‘æˆ˜çš„è§£å†³ï¼Œå……åˆ†å±•ç¤ºäº† deep learning åœ¨åŠ é€Ÿåˆ†å­å‘ç°å’Œå¤©ç„¶äº§ç‰©ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.03127v1",
      "published_date": "2025-12-02 18:59:13 UTC",
      "updated_date": "2025-12-02 18:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:24.086241+00:00"
    },
    {
      "arxiv_id": "2512.03036v1",
      "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
      "title_zh": "ViSAudioï¼šç«¯åˆ°ç«¯è§†é¢‘é©±åŠ¨çš„åŒè€³ç©ºé—´éŸ³é¢‘ç”Ÿæˆ",
      "authors": [
        "Mengchen Zhang",
        "Qi Chen",
        "Tong Wu",
        "Zihan Liu",
        "Dahua Lin"
      ],
      "abstract": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ViSAudioï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘è½¬éŸ³é¢‘æŠ€æœ¯ç¼ºä¹ç©ºé—´æ²‰æµ¸æ„ŸåŠä¸¤é˜¶æ®µåŒè€³éŸ³é¢‘ç”Ÿæˆå­˜åœ¨æ—¶ç©ºä¸ä¸€è‡´æ€§ç­‰é—®é¢˜çš„ç«¯åˆ°ç«¯åŒè€³ç©ºé—´éŸ³é¢‘ï¼ˆBinaural Spatial Audioï¼‰ç”Ÿæˆæ¡†æ¶ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ä»»åŠ¡ï¼Œä½œè€…é€šè¿‡åŠè‡ªåŠ¨åŒ–æµç¨‹æ„å»ºäº† BiAudio æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«çº¦ 9.7 ä¸‡ä¸ªæ¶µç›–å¤šå…ƒçœŸå®åœºæ™¯å’Œç›¸æœºæ—‹è½¬è½¨è¿¹çš„è§†é¢‘-åŒè€³éŸ³é¢‘å¯¹ã€‚ViSAudio æ ¸å¿ƒé‡‡ç”¨äº†åŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆConditional Flow Matchingï¼‰çš„åŒåˆ†æ”¯éŸ³é¢‘ç”Ÿæˆæ¶æ„ï¼Œé€šè¿‡ä¸¤ä¸ªä¸“ç”¨åˆ†æ”¯å¯¹éŸ³é¢‘æ½œæµï¼ˆLatent Flowsï¼‰è¿›è¡Œå»ºæ¨¡ã€‚ç»“åˆæ¡ä»¶æ—¶ç©ºæ¨¡å—ï¼ˆConditional Spacetime Moduleï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨å¹³è¡¡é€šé“é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œä¿ç•™äº†æ˜¾è‘—çš„ç©ºé—´ç‰¹å¾ï¼Œç¡®ä¿äº†éŸ³é¢‘ä¸è¾“å…¥è§†é¢‘åœ¨æ—¶ç©ºä¸Šçš„ç²¾ç¡®å¯¹é½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒViSAudio åœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé€‚åº”è§†ç‚¹å˜åŒ–ã€å£°æºç§»åŠ¨åŠå¤æ‚å£°å­¦ç¯å¢ƒçš„é«˜è´¨é‡ã€æ²‰æµ¸å¼åŒè€³éŸ³é¢‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03036v1",
      "published_date": "2025-12-02 18:56:12 UTC",
      "updated_date": "2025-12-02 18:56:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:49.770861+00:00"
    },
    {
      "arxiv_id": "2512.08973v1",
      "title": "Enhancing Automatic Speech Recognition Through Integrated Noise Detection Architecture",
      "title_zh": "é€šè¿‡é›†æˆå™ªå£°æ£€æµ‹æ¶æ„å¢å¼ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«",
      "authors": [
        "Karamvir Singh"
      ],
      "abstract": "This research presents a novel approach to enhancing automatic speech recognition systems by integrating noise detection capabilities directly into the recognition architecture. Building upon the wav2vec2 framework, the proposed method incorporates a dedicated noise identification module that operates concurrently with speech transcription. Experimental validation using publicly available speech and environmental audio datasets demonstrates substantial improvements in transcription quality and noise discrimination. The enhanced system achieves superior performance in word error rate, character error rate, and noise detection accuracy compared to conventional architectures. Results indicate that joint optimization of transcription and noise classification objectives yields more reliable speech recognition in challenging acoustic conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡å°†å™ªå£°æ£€æµ‹åŠŸèƒ½ç›´æ¥é›†æˆåˆ°è¯†åˆ«æ¶æ„ä¸­æ¥å¢å¼º Automatic Speech Recognition (ASR) ç³»ç»Ÿçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäº wav2vec2 æ¡†æ¶ï¼Œå¼•å…¥äº†ä¸€ä¸ªä¸“é—¨çš„å™ªå£°è¯†åˆ«æ¨¡å—ï¼Œä½¿å…¶ä¸è¯­éŸ³è½¬å½•è¿‡ç¨‹å¹¶å‘è¿è¡Œã€‚é€šè¿‡ä½¿ç”¨å…¬å¼€è¯­éŸ³å’Œç¯å¢ƒéŸ³é¢‘æ•°æ®é›†è¿›è¡Œçš„å®éªŒéªŒè¯ï¼Œè¯¥ç³»ç»Ÿåœ¨è½¬å½•è´¨é‡å’Œå™ªå£°è¾¨è¯†æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚ä¸ä¼ ç»Ÿæ¶æ„ç›¸æ¯”ï¼Œå¢å¼ºåçš„ç³»ç»Ÿåœ¨ Word Error Rate (WER)ã€Character Error Rate (CER) ä»¥åŠå™ªå£°æ£€æµ‹å‡†ç¡®ç‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè½¬å½•ä¸å™ªå£°åˆ†ç±»ç›®æ ‡çš„ Joint Optimization æ˜¾è‘—æé«˜äº†ç³»ç»Ÿåœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«å¯é æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08973v1",
      "published_date": "2025-12-02 18:54:45 UTC",
      "updated_date": "2025-12-02 18:54:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:30.171907+00:00"
    },
    {
      "arxiv_id": "2512.03028v2",
      "title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control",
      "title_zh": "SMPï¼šé¢å‘åŸºäºç‰©ç†è§’è‰²æ§åˆ¶çš„å¯é‡ç”¨åˆ†æ•°åŒ¹é…è¿åŠ¨å…ˆéªŒ",
      "authors": [
        "Yuxuan Mu",
        "Ziyu Zhang",
        "Yi Shi",
        "Minami Matsumoto",
        "Kotaro Imamura",
        "Guy Tevet",
        "Chuan Guo",
        "Michael Taylor",
        "Chang Shu",
        "Pengcheng Xi",
        "Xue Bin Peng"
      ],
      "abstract": "Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Score-Matching Motion Priors (SMP)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡ç‰©ç†è§’è‰²æ§åˆ¶è‡ªç„¶æ€§çš„å¯é‡ç”¨è¿åŠ¨å…ˆéªŒã€‚é’ˆå¯¹ä¼ ç»Ÿçš„å¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ (Adversarial Imitation Learning)æ¨¡å‹éš¾ä»¥é‡ç”¨ä¸”éœ€ä¿ç•™åŸå§‹å‚è€ƒæ•°æ®çš„é—®é¢˜ï¼ŒSMPç»“åˆäº†é¢„è®­ç»ƒçš„è¿åŠ¨æ‰©æ•£æ¨¡å‹(Motion Diffusion Models)ä¸åˆ†å€¼è’¸é¦é‡‡æ ·(Score Distillation Sampling, SDS)æŠ€æœ¯ï¼Œæ„å»ºäº†ä¸ä»»åŠ¡æ— å…³çš„é€šç”¨å…ˆéªŒã€‚SMPåœ¨è®­ç»ƒå®Œæˆåå¯ä½œä¸ºå›ºå®šçš„å¥–åŠ±å‡½æ•°ç›´æ¥åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®é›†çš„é£æ ¼é€‚é…ä¸æ–°é£æ ¼çš„åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMPåœ¨å¤šç§ç‰©ç†æ¨¡æ‹Ÿç±»äººè§’è‰²æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„åŠ¨ä½œè´¨é‡å¯ä¸å½“å‰æœ€å…ˆè¿›çš„å¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶å…·å¤‡æé«˜çš„æ¨¡å—åŒ–ç¨‹åº¦å’Œé‡ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.GR",
      "comment": "14 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.03028v2",
      "published_date": "2025-12-02 18:54:12 UTC",
      "updated_date": "2025-12-03 17:44:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:41.786384+00:00"
    },
    {
      "arxiv_id": "2512.03026v1",
      "title": "The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models",
      "title_zh": "é“å¾·ä¸€è‡´æ€§æµæ°´çº¿ï¼šå¤§è¯­è¨€æ¨¡å‹çš„æŒç»­ä¼¦ç†è¯„ä¼°",
      "authors": [
        "Saeid Jamshidi",
        "Kawser Wazed Nafi",
        "Arghavan Moradi Dakhel",
        "Negar Shahabi",
        "Foutse Khomh"
      ],
      "abstract": "The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Moral Consistency Pipeline (MoCoP)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ•°æ®é›†çš„é—­ç¯æ¡†æ¶ï¼Œæ—¨åœ¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„é“å¾·ç¨³å®šæ€§è¿›è¡ŒæŒç»­è¯„ä¼°å’Œè§£é‡Šã€‚MoCoPç»“åˆäº†è¯æ±‡å®Œæ•´æ€§åˆ†æ(lexical integrity analysis)ã€è¯­ä¹‰é£é™©ä¼°è®¡(semantic risk estimation)ä»¥åŠåŸºäºæ¨ç†çš„åˆ¤æ–­å»ºæ¨¡(reasoning-based judgment modeling)ä¸‰ä¸ªæ”¯æ’‘å±‚ï¼Œæ„å»ºäº†ä¸€ä¸ªæ— éœ€å¤–éƒ¨ç›‘ç£å³å¯è‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ä¼¦ç†åœºæ™¯çš„è‡ªæŒæ¶æ„ã€‚åœ¨GPT-4-Turboå’ŒDeepSeekä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMoCoPèƒ½æœ‰æ•ˆæ•æ‰é•¿æœŸçš„ä¼¦ç†è¡Œä¸ºï¼Œå¹¶æ­ç¤ºäº†é“å¾·ç»´åº¦ä¸æ¯’æ€§(toxicity)ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è´Ÿç›¸å…³å…³ç³»(r = -0.81)ï¼Œè€Œä¸å“åº”å»¶è¿Ÿ(response latency)å‡ ä¹æ— å…³ã€‚ç ”ç©¶å‘ç°é“å¾·ä¸€è‡´æ€§ä¸è¯­è¨€å®‰å…¨æ€§å¾€å¾€è¡¨ç°ä¸ºæ¨¡å‹è¡Œä¸ºä¸­ç¨³å®šä¸”å¯è§£é‡Šçš„ç‰¹å¾ï¼Œè€ŒéçŸ­æœŸæ³¢åŠ¨ã€‚é€šè¿‡å°†ä¼¦ç†è¯„ä¼°é‡æ–°å®šä¹‰ä¸ºä¸€ç§åŠ¨æ€ä¸”ä¸æ¨¡å‹æ— å…³çš„é“å¾·å†…çœå½¢å¼ï¼ŒMoCoPä¸ºå¯æ‰©å±•çš„æŒç»­å®¡è®¡æä¾›äº†å¯å¤ç°çš„åŸºç¡€ï¼Œæ˜¾è‘—æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„è®¡ç®—é“å¾·ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03026v1",
      "published_date": "2025-12-02 18:52:29 UTC",
      "updated_date": "2025-12-02 18:52:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:42.972060+00:00"
    },
    {
      "arxiv_id": "2512.03025v3",
      "title": "LORE: A Large Generative Model for Search Relevance",
      "title_zh": "LOREï¼šé¢å‘æœç´¢ç›¸å…³æ€§çš„å¤§å‹ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Chenji Lu",
        "Zhuo Chen",
        "Hui Zhao",
        "Zhiyuan Zeng",
        "Gang Zhao",
        "Junjie Ren",
        "Ruicong Xu",
        "Haoran Li",
        "Songyan Liu",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LOREï¼Œä¸€ä¸ªç”¨äºç”µå­å•†åŠ¡æœç´¢ç›¸å…³æ€§çš„ç³»ç»ŸåŒ–å¤§ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å®šæ€§é©±åŠ¨çš„ä»»åŠ¡åˆ†è§£çªç ´å½“å‰ç›¸å…³æ€§è¯„ä¼°çš„æ€§èƒ½ç“¶é¢ˆã€‚ä½œè€…æŒ‡å‡ºç°æœ‰çš„ Chain-of-Thought (CoT) æ–¹æ³•ç”±äºå°†ç›¸å…³æ€§è§†ä¸ºå•ä¸€ä»»åŠ¡è€Œé¢ä¸´æ€§èƒ½ä¸Šé™ï¼Œå› æ­¤ LORE å°†å…¶æ ¸å¿ƒèƒ½åŠ›è§£æ„ä¸ºçŸ¥è¯†ä¸æ¨ç†ã€å¤šæ¨¡æ€åŒ¹é…ä»¥åŠè§„åˆ™éµå¾ªä¸‰ä¸ªç»´åº¦ã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ç»“åˆ SFT é˜¶æ®µæ¸è¿›å¼ CoT åˆæˆä¸ RL é˜¶æ®µäººç±»åå¥½å¯¹é½çš„ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸“é—¨çš„ RAIR åŸºå‡†æµ‹è¯•ä»¥åŠä¸€å¥—åŸºäºæŸ¥è¯¢é¢‘ç‡çš„åˆ†å±‚éƒ¨ç½²ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹èƒ½åŠ›èƒ½å¤Ÿé«˜æ•ˆè½¬åŒ–ä¸ºåœ¨çº¿æ€§èƒ½ã€‚ç»è¿‡ä¸‰å¹´çš„å®é™…éƒ¨ç½²ä¸è¿­ä»£ï¼ŒLORE åœ¨åœ¨çº¿ GoodRate æŒ‡æ ‡ä¸Šå®ç°äº† 27% çš„ç´¯ç§¯æå‡ã€‚è¯¥æˆæœä¸ä»…ä¸ºç”µå•†æœç´¢æä¾›äº†å®è·µæ–¹æ¡ˆï¼Œä¹Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¶ä»–å‚ç›´é¢†åŸŸçš„è½åœ°æä¾›äº†å®Œæ•´çš„æŠ€æœ¯è“å›¾ä¸æ–¹æ³•è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03025v3",
      "published_date": "2025-12-02 18:50:42 UTC",
      "updated_date": "2026-01-06 15:14:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:07.173845+00:00"
    },
    {
      "arxiv_id": "2512.03024v1",
      "title": "TokenPowerBench: Benchmarking the Power Consumption of LLM Inference",
      "title_zh": "TokenPowerBenchï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠŸè€—åŸºå‡†æµ‹è¯•",
      "authors": [
        "Chenxu Niu",
        "Wei Zhang",
        "Jie Li",
        "Yongjian Zhao",
        "Tongyang Wang",
        "Xi Wang",
        "Yong Chen"
      ],
      "abstract": "Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†é˜¶æ®µæ¶ˆè€—æ€»ç”µé‡90%ä»¥ä¸Šä½†ç¼ºä¹ä¸“é¡¹åŠŸè€—åŸºå‡†çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªè½»é‡çº§ä¸”å¯æ‰©å±•çš„æ¨ç†åŠŸè€—åŸºå‡†æµ‹è¯•å·¥å…·TokenPowerBenchã€‚è¯¥å·¥å…·é›†æˆäº†å£°æ˜å¼é…ç½®ç•Œé¢ã€æ— éœ€ä¸“é—¨åŠŸç‡è®¡çš„å¤šå±‚çº§åŠŸè€—æµ‹é‡æŠ€æœ¯ï¼Œä»¥åŠèƒ½å°†èƒ½é‡ç²¾ç¡®å½’å› äºprefillå’Œdecodeé˜¶æ®µçš„ç›¸ä½å¯¹é½æŒ‡æ ‡æµæ°´çº¿ã€‚é€šè¿‡è°ƒæ•´batch sizeã€context lengthã€å¹¶è¡Œç­–ç•¥å’Œquantizationç­‰å‚æ•°ï¼Œç”¨æˆ·å¯ä»¥å¿«é€Ÿè¯„ä¼°ä¸åŒé…ç½®å¯¹joules per tokenåŠå…¶ä»–èƒ½æ•ˆæŒ‡æ ‡çš„å…·ä½“å½±å“ã€‚å®éªŒåœ¨Llamaã€Falconã€Qwenå’ŒMistralç­‰ä¸»æµæ¨¡å‹ç³»åˆ—ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œæµ‹è¯•è§„æ¨¡æ¶µç›–äº†ä»10äº¿å‚æ•°åˆ°å‰æ²¿çš„Llama3-405Bæ¨¡å‹ã€‚ç›®å‰TokenPowerBenchå·²æ­£å¼å¼€æºï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·ç²¾ç¡®æµ‹é‡åŠŸè€—ã€é¢„æµ‹è¿è¥æ”¯å‡ºå¹¶å®ç°æ¨¡å‹éƒ¨ç½²çš„å¯æŒç»­å‘å±•ç›®æ ‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the AAAI'26 Conference Main Track",
      "pdf_url": "https://arxiv.org/pdf/2512.03024v1",
      "published_date": "2025-12-02 18:50:17 UTC",
      "updated_date": "2025-12-02 18:50:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:47:59.659355+00:00"
    },
    {
      "arxiv_id": "2512.03019v1",
      "title": "Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge",
      "title_zh": "é¢å‘æ€è€ƒå‹ LLM è¯„æµ‹å‘˜çš„åˆ†å¸ƒæ ¡å‡†æ¨ç†æ—¶è®¡ç®—",
      "authors": [
        "Hamid Dadkhahi",
        "Firas Trabelsi",
        "Parker Riley",
        "Juraj Juraska",
        "Mehdi Mirzazadeh"
      ],
      "abstract": "Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ€ç»´å¤§è¯­è¨€æ¨¡å‹ï¼ˆThinking LLMsï¼‰ä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLM-as-a-Judgeï¼‰åœ¨ä¸¤ä¸¤åå¥½è¯„ä¼°ä¸­å­˜åœ¨çš„å•æ ·æœ¬å™ªå£°é—®é¢˜ï¼Œä»¥åŠä¼ ç»Ÿèšåˆè§„åˆ™åœ¨å¤„ç†å¹³å±€ï¼ˆtiesï¼‰æ—¶çš„ä¸ä¸€è‡´æ€§è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸåˆ™æ€§çš„åˆ†å¸ƒæ ¡å‡†æ¨ç†æ—¶é—´è®¡ç®—ï¼ˆInference-time compute, ITCï¼‰èšåˆæ–¹æ¡ˆï¼Œé€šè¿‡ä¸ºæ¯ä¸ªé¡¹ç›®ç”Ÿæˆå¤šä¸ªç‹¬ç«‹çš„æ€è€ƒè¯„çº§æ ·æœ¬æ¥æé«˜è¯„ä¼°å¯é æ€§ã€‚è¯¥æ–¹æ³•åœ¨è¯„çº§è®¡æ•°ä¸Šé‡‡ç”¨ Bradley-Terry-Davidson æ¨¡å‹ï¼Œç»“åˆææ€§ï¼ˆpolarityï¼‰å’Œå†³ç­–åŠ›ï¼ˆdecisivenessï¼‰æŒ‡æ ‡ï¼Œä»è€Œæœ‰æ•ˆåŒºåˆ†å¾®å¼±ä¼˜åŠ¿ä¸å¼ºå…±è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº† MAE å¹¶æå‡äº†æˆå¯¹å‡†ç¡®ç‡ï¼Œå…¶è¡¨ç°ç”šè‡³è¾¾åˆ°æˆ–è¶…è¿‡äº†å•ä¸ªäººç±»è¯„åˆ†è€…çš„æ°´å¹³ã€‚ç ”ç©¶è¯æ˜ï¼Œé€šè¿‡ä¼˜åŒ– ITC åˆ†é…å¹¶ç»“åˆåˆ†å¸ƒæ„ŸçŸ¥èšåˆæ–¹æ³•ï¼Œå¯ä»¥å°†ä¸ç¨³å®šçš„æ¨¡å‹åˆ¤æ–­è½¬åŒ–ä¸ºé«˜å¯ä¿¡åº¦çš„è¯„ä¼°è¯„çº§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03019v1",
      "published_date": "2025-12-02 18:46:47 UTC",
      "updated_date": "2025-12-02 18:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:00.673577+00:00"
    },
    {
      "arxiv_id": "2512.06006v1",
      "title": "Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization",
      "title_zh": "ç®€å•æ™ºèƒ½ä½“åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒå·¥ä½œæµä¼˜åŒ–ä¸­è¡¨ç°ä¼˜äºä¸“å®¶",
      "authors": [
        "Xuefei",
        "Wang",
        "Kai A. Horstmann",
        "Ethan Lin",
        "Jonathan Chen",
        "Alexander R. Farhang",
        "Sophia Stiles",
        "Atharva Sehgal",
        "Jonathan Light",
        "David Van Valen",
        "Yisong Yue",
        "Jennifer J. Sun"
      ],
      "abstract": "Adapting production-level computer vision tools to bespoke scientific datasets is a critical \"last mile\" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ AI agents è‡ªåŠ¨åŒ–ä»£ç ç¼–å†™ï¼Œä»¥è§£å†³å°†ç”Ÿäº§çº§ computer vision å·¥å…·é€‚é…åˆ°ç‰¹å®šç§‘å­¦æ•°æ®é›†æ—¶çš„ç“¶é¢ˆé—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç”¨äº agentic code optimization çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ä¸ªç”Ÿäº§çº§ biomedical imaging æµç¨‹ä¸­è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•çš„ agent æ¡†æ¶ç”Ÿæˆçš„é€‚é…ä»£ç åœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºäººç±»ä¸“å®¶çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†å¤æ‚çš„æ™ºèƒ½ä½“æ¶æ„å¹¶éåœ¨æ‰€æœ‰åœºæ™¯ä¸‹éƒ½å…·æœ‰ä¼˜åŠ¿ï¼Œä»è€Œä¸ºå®é™…çš„ agent design æä¾›äº†å®ç”¨çš„è·¯çº¿å›¾ã€‚ç›®å‰è¯¥æ¡†æ¶å·²å¼€æºï¼Œä¸”ç”Ÿæˆçš„å‡½æ•°å·²æˆåŠŸéƒ¨ç½²äºçœŸå®ç”Ÿäº§ç¯å¢ƒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨è§£å†³ç§‘ç ”æ•°æ®å¤„ç†éš¾é¢˜ä¸Šçš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06006v1",
      "published_date": "2025-12-02 18:42:26 UTC",
      "updated_date": "2025-12-02 18:42:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:05.469315+00:00"
    },
    {
      "arxiv_id": "2512.03013v1",
      "title": "In-Context Sync-LoRA for Portrait Video Editing",
      "title_zh": "In-Context Sync-LoRAï¼šé¢å‘äººåƒè§†é¢‘ç¼–è¾‘çš„ä¸Šä¸‹æ–‡åŒæ­¥ LoRA",
      "authors": [
        "Sagi Polaczek",
        "Or Patashnik",
        "Ali Mahdavi-Amiri",
        "Daniel Cohen-Or"
      ],
      "abstract": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sync-LoRAï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºäººåƒè§†é¢‘ç¼–è¾‘çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åœ¨è¿›è¡Œå¤–è§‚æˆ–è¡¨æƒ…ä¿®æ”¹æ—¶éš¾ä»¥ä¿æŒè§†é¢‘ä¸»ä½“åŸå§‹æ—¶é—´è¡Œä¸ºçš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å›¾åƒåˆ°è§†é¢‘(image-to-video)çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¯¹é¦–å¸§è¿›è¡Œç¼–è¾‘å¹¶å°†å…¶æ•ˆæœä¼ æ’­è‡³æ•´ä¸ªè§†é¢‘åºåˆ—ï¼Œç¡®ä¿äº†å¸§çº§åˆ«çš„åŒæ­¥å’Œèº«ä»½ä¸€è‡´æ€§ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯æ˜¯è®­ç»ƒä¸€ä¸ªä¸Šä¸‹æ–‡(in-context) LoRAï¼Œå¹¶åˆ©ç”¨ä¸€ç§åŸºäºåŒæ­¥çš„è¿‡æ»¤è¿‡ç¨‹(synchronization-based filtering)ä»è‡ªåŠ¨ç”Ÿæˆçš„è§†é¢‘å¯¹ä¸­ç­›é€‰å‡ºé«˜åº¦æ—¶é—´å¯¹é½çš„æ ·æœ¬ã€‚Sync-LoRA èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤–è§‚æ›´æ”¹ã€æ·»åŠ ç‰©ä½“å’ŒèƒŒæ™¯å˜æ¢ç­‰å¤šç§ç¼–è¾‘ä»»åŠ¡ï¼Œå¯¹æœªè§è¿‡çš„èº«ä»½ã€å§¿æ€å’Œè¡¨æƒ…å˜åŒ–å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘å¿ å®åº¦ä¸è¿åŠ¨ä¿æŒä¹‹é—´å®ç°äº†ç¨³å¥çš„å¹³è¡¡ï¼Œå…·æœ‰æé«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§(temporal coherence)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://sagipolaczek.github.io/Sync-LoRA/",
      "pdf_url": "https://arxiv.org/pdf/2512.03013v1",
      "published_date": "2025-12-02 18:40:35 UTC",
      "updated_date": "2025-12-02 18:40:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:22.590709+00:00"
    },
    {
      "arxiv_id": "2512.03125v1",
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "title_zh": "ç¼“è§£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æŒç»­å­¦ä¹ ä¸­çš„æ¨¡æ€å†…ä¸æ¨¡æ€é—´é—å¿˜",
      "authors": [
        "Xiwen Wei",
        "Mustafa Munir",
        "Radu Marculescu"
      ],
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ (Unified Multimodal Generative Models, UMGMs) åœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é¦–æ¬¡è¯†åˆ«å¹¶å®è¯äº†æ¨¡æ€é—´ (inter-modal) é—å¿˜ç°è±¡ã€‚ä½œè€…æŒ‡å‡ºï¼Œæ¨¡æ€é—´çš„æ¢¯åº¦å†²çª (gradient conflict) æ˜¯å¯¼è‡´æ­¤ç±»é—å¿˜çš„æ ¸å¿ƒåŸå› ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†æ¨¡æ€è§£è€¦ä¸“å®¶ (Modality-Decoupled Experts, MoDE) æ¶æ„ï¼Œé€šè¿‡éš”ç¦»ç‰¹å®šæ¨¡æ€çš„å‚æ•°æ›´æ–°æ¥æœ‰æ•ˆç¼“è§£æ¢¯åº¦å¹²æ‰°ã€‚è¯¥æ¶æ„è¿˜ç»“åˆäº†çŸ¥è¯†è’¸é¦ (knowledge distillation) æŠ€æœ¯ï¼Œä»¥ä¿æŠ¤æ¨¡å‹çš„é¢„è®­ç»ƒèƒ½åŠ›å¹¶é˜²æ­¢æ—§çŸ¥è¯†ä¸¢å¤±ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡æ€è€¦åˆæŒç»­å­¦ä¹  (Continual Learning) æ–¹æ³•ä¸åŒï¼ŒMoDE é€šè¿‡æ˜¾å¼çš„è§£è€¦æœºåˆ¶æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMoDE åœ¨ç¼“è§£æ¨¡æ€å†…å’Œæ¨¡æ€é—´é—å¿˜æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.03125v1",
      "published_date": "2025-12-02 18:36:26 UTC",
      "updated_date": "2025-12-02 18:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:30.381767+00:00"
    },
    {
      "arxiv_id": "2512.03005v2",
      "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?",
      "title_zh": "ä»å®¡æ ¸åˆ°è°ƒè§£ï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦æ‹…ä»»ç½‘ç»œâ€œå£æ°´æˆ˜â€ä¸­çš„è°ƒè§£è€…ï¼Ÿ",
      "authors": [
        "Dawei Li",
        "Abdullah Alnaibari",
        "Muhammad Arslan",
        "Manny Sandoval",
        "Deborah Hall",
        "Yasin Silva",
        "Huan Liu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åœ¨çº¿å†²çªè°ƒè§£ä¸­çš„æ½œåŠ›ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹ä»ç®€å•çš„å†…å®¹å®¡æ ¸(Moderation)è½¬å‹ä¸ºèƒ½å¤Ÿç†è§£å¹¶é™çº§å†²çªçš„è°ƒè§£è€…(Mediators)ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå°†è°ƒè§£ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡çš„æ¡†æ¶ï¼šé¦–å…ˆæ˜¯åˆ¤æ–­(judgment)ï¼Œå³è¯„ä¼°å¯¹è¯çš„å…¬å¹³æ€§å’Œæƒ…æ„ŸåŠ¨æ€ï¼›å…¶æ¬¡æ˜¯å¼•å¯¼(steering)ï¼Œå³ç”Ÿæˆå…·æœ‰åŒç†å¿ƒçš„é™çº§ä¿¡æ¯ä»¥å¼•å¯¼å‚ä¸è€…èµ°å‘å’Œè§£ã€‚ä¸ºäº†è¯„ä¼°è°ƒè§£è´¨é‡ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŸºäºRedditçš„å¤§å‹æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ç»“åˆåŸåˆ™è¯„åˆ†(principle-based scoring)ã€ç”¨æˆ·æ¨¡æ‹Ÿ(user simulation)å’Œäººå·¥å¯¹æ¯”çš„å¤šé˜¶æ®µè¯„ä¼°æµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰§è¡Œè°ƒè§£ä»»åŠ¡æ—¶ï¼ŒåŸºäºAPIçš„æ¨¡å‹(API-based models)åœ¨æ¨ç†å’Œå¹²é¢„å¯¹é½æ–¹é¢å‡ä¼˜äºå¼€æºæ¨¡å‹(open-source counterparts)ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†LLMsä½œä¸ºåœ¨çº¿ç¤¾äº¤è°ƒè§£æ™ºèƒ½ä½“çš„å‰æ™¯ï¼Œä¹Ÿåˆ†æäº†å…¶ç°æœ‰çš„å±€é™æ€§ï¼Œä¸ºåˆ©ç”¨AIä¿ƒè¿›å»ºè®¾æ€§å¯¹è¯æä¾›äº†å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2512.03005v2",
      "published_date": "2025-12-02 18:31:18 UTC",
      "updated_date": "2025-12-15 17:31:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:29.474092+00:00"
    },
    {
      "arxiv_id": "2512.03001v1",
      "title": "Invasive Context Engineering to Control Large Language Models",
      "title_zh": "ç”¨äºæ§åˆ¶å¤§è¯­è¨€æ¨¡å‹çš„ä¾µå…¥å¼ä¸Šä¸‹æ–‡å·¥ç¨‹",
      "authors": [
        "Thomas Rivasseau"
      ],
      "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models åœ¨é•¿æ–‡æœ¬ context length åœºæ™¯ä¸‹æ˜“å—æ”»å‡»å’Œ jailbreak æ¦‚ç‡å¢åŠ çš„é—®é¢˜ï¼Œæå‡ºäº† Invasive Context Engineeringï¼ˆä¾µå…¥å¼ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ LLM ä¸Šä¸‹æ–‡ä¸­ç›´æ¥æ’å…¥ç‰¹å®šçš„æ§åˆ¶è¯­å¥ï¼Œå®ç°äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„ç²¾å‡†å¹²é¢„å’Œæ§åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„åå¥½è®­ç»ƒæˆ–è¾“å…¥è¾“å‡ºè¿‡æ»¤ä¸åŒï¼ŒInvasive Context Engineering å¹¶ä¸ä¾èµ–äºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆè§„é¿äº†é•¿æ–‡æœ¬åœºæ™¯ä¸‹è®­ç»ƒæ•°æ®çŸ­ç¼ºçš„å›°å¢ƒã€‚ä½œè€…è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè¯¥æŠ€æœ¯å¯ä»¥è¢«æ¨å¹¿è‡³ Chain-of-Thought è¿‡ç¨‹ï¼Œä»¥æœ‰æ•ˆé˜²æ­¢æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿè¿è§„çš„è°‹åˆ’ï¼ˆschemingï¼‰è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•ä¸ºæå‡ LLM åœ¨å¤æ‚é•¿æ–‡æœ¬ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§æä¾›äº†ä¸€ç§ç¨³å¥ä¸”æ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.03001v1",
      "published_date": "2025-12-02 18:25:55 UTC",
      "updated_date": "2025-12-02 18:25:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:31.669070+00:00"
    },
    {
      "arxiv_id": "2512.02987v1",
      "title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic",
      "title_zh": "é¢å‘é€»è¾‘ç¿»è¯‘çš„å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼šåˆ©ç”¨ Lang2Logic å‡å°‘å¹»è§‰",
      "authors": [
        "Muyu Pan",
        "Dheeraj Kodakandla",
        "Mahfuza Farooque"
      ],
      "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘ä¸ºå½¢å¼é€»è¾‘(Formal Logic)æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰(Hallucinations)çš„é—®é¢˜ï¼Œæå‡ºäº†Lang2Logicæ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†è‹±æ–‡å¥å­è½¬æ¢ä¸ºé€»è¾‘è¡¨è¾¾å¼ï¼Œå¹¶è¿›ä¸€æ­¥ç¿»è¯‘ä¸ºç”¨äºæ»¡è¶³æ€§æ±‚è§£çš„åˆå–èŒƒå¼(CNF)ï¼Œä»¥æ”¯æŒè‡ªåŠ¨åŒ–æ¨ç†å’Œè½¯ä»¶ç³»ç»Ÿè°ƒè¯•ã€‚ä¸ºäº†æé«˜ç¿»è¯‘ç²¾åº¦ï¼Œè¯¥å·¥ä½œç»“åˆäº†ç»å…¸NLPæŠ€æœ¯ã€è‡ªå®šä¹‰è¯­æ³•(Grammar)ã€ç¬¦å·è®¡ç®—åº“(Symbolic Computation Libraries)ä»¥åŠå¾®è°ƒåçš„è¯­è¨€æ¨¡å‹ã€‚å®éªŒè§‚å¯Ÿå‘ç°ï¼Œåœ¨ä¸åŒè¯­æ³•è®¾ç½®ä¸‹è®­ç»ƒçš„å¾®è°ƒæ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨çº æ­£åŸå§‹æ¨¡å‹äº§ç”Ÿçš„åŒç±»å¹»è§‰é”™è¯¯ã€‚é€šè¿‡è¿™ç§æ•´åˆæ–¹æ³•ï¼ŒLang2Logicæ˜¾è‘—é™ä½äº†é€»è¾‘ç¿»è¯‘ä»»åŠ¡ä¸­çš„é”™è¯¯ç‡ï¼Œå¹¶ä¸ºè‡ªåŠ¨åŒ–æ¨ç†æä¾›äº†æ›´ä¸ºå¯é çš„CNFç”Ÿæˆç»“æœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "IEEE ISNCC 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.02987v1",
      "published_date": "2025-12-02 18:03:06 UTC",
      "updated_date": "2025-12-02 18:03:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:33.357514+00:00"
    },
    {
      "arxiv_id": "2512.02978v1",
      "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding",
      "title_zh": "é‡æ–°å®¡è§†é€šç”¨å‹è„‘æœºæ¥å£ï¼šé¢å‘ EEG å¿ƒç†æŒ‡ä»¤è§£ç çš„ 34 ä¸‡ä½™ç§ç®—æ³•é…ç½®åŸºå‡†æµ‹è¯•",
      "authors": [
        "Paul Barbaste",
        "Olivier Oullier",
        "Xavier Vasques"
      ],
      "abstract": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹è¶…è¿‡ 340,000 ç§ç®—æ³•é…ç½®è¿›è¡Œå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œé‡æ–°å®¡è§†äº†è„‘ç”µå›¾ (EEG) å¿ƒç†æŒ‡ä»¤è§£ç çš„æ³›åŒ–æ€§æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šç»„åˆäº† Common Spatial Patterns (CSP)ã€Riemannian geometryã€functional connectivity ä»¥åŠåŸºäº fractal æˆ– entropy çš„ç‰¹å¾ï¼Œåœ¨ä¸ªä½“å±‚é¢è¯„ä¼°äº†ä¸åŒé¢‘ç‡èŒƒå›´ä¸‹çš„è§£ç æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ Covariance tangent space projection (cov-tgsp) å’Œ CSP åœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šä¿æŒé¢†å…ˆï¼Œä½†å…¶æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ•°æ®é›†ï¼Œä¸”å—è¯•è€…é—´çš„ä¸ªä½“å·®å¼‚æ˜¾è‘—ã€‚ç ”ç©¶å‘ç°éçº¿æ€§æ–¹æ³• (nonlinear methods) åœ¨ç‰¹å®šä¸ªä½“ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œè¿™è¯´æ˜åœ¨è„‘æœºæ¥å£ (BCI) é¢†åŸŸå¹¶ä¸å­˜åœ¨â€œä¸€åŠ³æ°¸é€¸â€çš„é€šç”¨è§£ç ç®—æ³•ã€‚æœ€ç»ˆç ”ç©¶å¼ºè°ƒï¼Œä¸ºäº†åº”å¯¹ç¥ç»ç”Ÿç†çš„å˜å¼‚æ€§ï¼Œæœªæ¥çš„ BCI åº”ç”¨å¿…é¡»è½¬å‘èƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ç”¨æˆ·ç‹¬ç‰¹æ€§è´¨çš„è‡ªé€‚åº” (adaptive) æˆ–å¤šæ¨¡æ€ (multimodal) ä¸ªæ€§åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "28 pages, 8 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.02978v1",
      "published_date": "2025-12-02 17:56:46 UTC",
      "updated_date": "2025-12-02 17:56:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:38.981817+00:00"
    },
    {
      "arxiv_id": "2512.02966v1",
      "title": "Lumos: Let there be Language Model System Certification",
      "title_zh": "Lumosï¼šå®ç°è¯­è¨€æ¨¡å‹ç³»ç»Ÿè®¤è¯",
      "authors": [
        "Isha Chaudhary",
        "Vedaant Jain",
        "Avaljot Singh",
        "Kavya Sachdeva",
        "Sayan Ranu",
        "Gagandeep Singh"
      ],
      "abstract": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Lumosï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè§„èŒƒå’Œæ­£å¼è®¤è¯è¯­è¨€æ¨¡å‹ç³»ç»Ÿ(LMS)è¡Œä¸ºçš„åŸåˆ™æ€§æ¡†æ¶ã€‚Lumos æ˜¯ä¸€ç§åŸºäºå›¾çš„å‘½ä»¤å¼æ¦‚ç‡ç¼–ç¨‹é¢†åŸŸç‰¹å®šè¯­è¨€(DSL)ï¼Œèƒ½å¤Ÿé€šè¿‡ç®€å•çš„ç»„åˆæ„é€ å—ç”Ÿæˆç‹¬ç«‹åŒåˆ†å¸ƒçš„ Prompt åˆ†å¸ƒï¼Œå¹¶æä¾›ä¸¥è°¨çš„æ··åˆè¯­ä¹‰è§£é‡Šã€‚è¯¥æ¡†æ¶æ”¯æŒä¸ç»Ÿè®¡è®¤è¯å™¨é›†æˆï¼Œå¯å¤„ç†å¤æ‚çš„å…³è”å’Œæ—¶åºè§„èŒƒï¼Œå¹¶èƒ½çµæ´»åº”å¯¹ä¸æ–­æ¼”å˜çš„å¨èƒç¯å¢ƒã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Lumos é¦–æ¬¡ä¸ºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åˆ¶å®šäº†å®‰å…¨è§„èŒƒï¼Œå®éªŒæ­ç¤ºäº† SOTA æ¨¡å‹ Qwen-VL åœ¨é›¨å¤©å³è½¬ç­‰ç‰¹å®šåœºæ™¯ä¸‹å­˜åœ¨é«˜è¾¾ 90% çš„ä¸¥é‡å®‰å…¨å¤±æ•ˆé£é™©ã€‚ä½œä¸ºé¦–ä¸ªç³»ç»ŸåŒ–ä¸”å¯æ‰©å±•çš„ LMS è®¤è¯æ¡†æ¶ï¼ŒLumos ä¸ºå®ç°å¤§è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„å¹¿æ³›è®¤è¯ä¸å®‰å…¨åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02966v1",
      "published_date": "2025-12-02 17:44:47 UTC",
      "updated_date": "2025-12-02 17:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:17.971117+00:00"
    },
    {
      "arxiv_id": "2512.02942v1",
      "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
      "title_zh": "åŸºäº VideoScience-Bench è¯„æµ‹è§†é¢‘ç”Ÿæˆçš„ç§‘å­¦ç†è§£ä¸æ¨ç†èƒ½åŠ›",
      "authors": [
        "Lanxiang Hu",
        "Abhilash Shankarampeta",
        "Yixin Huang",
        "Zilin Dai",
        "Haoyang Yu",
        "Yujie Zhao",
        "Haoqiang Kang",
        "Daniel Zhao",
        "Tajana Rosing",
        "Hao Zhang"
      ],
      "abstract": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† VideoScience-Benchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœ¬ç§‘æ°´å¹³ç§‘å­¦ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰è§†é¢‘åŸºå‡†ä¸»è¦ä¾§é‡äºç‰©ç†å¸¸è¯†è€Œç¼ºä¹æ·±åº¦ç§‘å­¦æ¨ç†çš„é—®é¢˜ï¼Œè¯¥åŸºå‡†åŒ…å«äº† 200 ä¸ªæ¶µç›–ç‰©ç†å’ŒåŒ–å­¦é¢†åŸŸ 14 ä¸ªä¸»é¢˜åŠ 103 ä¸ªæ¦‚å¿µçš„å¤æ‚ç§‘å­¦åœºæ™¯æç¤ºã€‚ç ”ç©¶åœ¨ T2V å’Œ I2V è®¾ç½®ä¸‹ï¼Œä» Prompt Consistencyã€Phenomenon Congruencyã€Correct Dynamismã€Immutability å’Œ Spatio-Temporal Continuity äº”ä¸ªç»´åº¦å¯¹ä¸ƒç§å…ˆè¿›è§†é¢‘æ¨¡å‹è¿›è¡Œäº†ä¸“å®¶æ ‡æ³¨è¯„ä¼°ã€‚é€šè¿‡å¼•å…¥ä¸äººç±»è¯„ä¼°é«˜åº¦ç›¸å…³çš„ VLM-as-a-Judge æœºåˆ¶ï¼Œè¯¥ç ”ç©¶ä¸ä»…å°†è§†é¢‘æ¨¡å‹è§†ä¸ºç”Ÿæˆå™¨ï¼Œæ›´å°†å…¶è§†ä¸ºæ¨ç†è€…ï¼Œè¦æ±‚å…¶ç”Ÿæˆçš„è§†é¢‘å¿…é¡»å±•ç°å‡ºä¸é¢„æœŸç‰©ç†å’ŒåŒ–å­¦ç°è±¡ä¸€è‡´çš„ç§‘å­¦ç†è§£ã€‚å®éªŒç»“æœè¯æ˜è¯¥åŸºå‡†èƒ½æœ‰æ•ˆè¡¡é‡æ¨¡å‹å¯¹ç°å®ä¸–ç•Œç§‘å­¦è§„å¾‹çš„å»ºæ¨¡èƒ½åŠ›ï¼Œä¸ºæœªæ¥å¼€å‘å…·å¤‡é›¶æ ·æœ¬æ¨ç†(zero-shot reasoning)èƒ½åŠ›çš„è§†é¢‘æ¨¡å‹æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02942v1",
      "published_date": "2025-12-02 17:11:23 UTC",
      "updated_date": "2025-12-02 17:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:58.376416+00:00"
    },
    {
      "arxiv_id": "2512.02932v1",
      "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
      "title_zh": "EGGSï¼šå…¼é¡¾å‡ ä½•ä¸å¤–è§‚å¹³è¡¡çš„æ–°è§†è§’åˆæˆå¯äº¤æ¢ 2D/3D é«˜æ–¯æ³¼æº…",
      "authors": [
        "Yancheng Zhang",
        "Guangyu Sun",
        "Chen Chen"
      ],
      "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EGGSï¼ˆExchangeable Gaussian Splattingï¼‰ï¼Œæ—¨åœ¨å¹³è¡¡æ–°è§†è§’åˆæˆï¼ˆNovel View Synthesis, NVSï¼‰ä¸­çš„å¤–è§‚ä¿çœŸåº¦ä¸å‡ ä½•å‡†ç¡®æ€§ã€‚é’ˆå¯¹3D Gaussian Splatting (3DGS)çš„å¤šè§†å›¾ä¸ä¸€è‡´æ€§ä»¥åŠ2D Gaussian Splatting (2DGS)åœ¨çº¹ç†ç»†èŠ‚ä¸Šçš„ä¸è¶³ï¼ŒEGGSå¼•å…¥äº†ä¸€ç§é›†æˆ2Då’Œ3D Gaussiançš„æ··åˆè¡¨ç¤ºä½“ç³»ã€‚ç ”ç©¶æ ¸å¿ƒåŒ…æ‹¬ç”¨äºç»Ÿä¸€æ¸²æŸ“çš„Hybrid Gaussian Rasterizationã€å®ç°2Dä¸3D GaussianåŠ¨æ€é€‚é…çš„Adaptive Type Exchangeï¼Œä»¥åŠèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¸¤ç§è¡¨ç¤ºä¼˜åŠ¿çš„Frequency-Decoupled Optimizationä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡CUDAåŠ é€Ÿå®ç°ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯é«˜æ•ˆè®­ç»ƒä¸æ¨ç†çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¸²æŸ“è´¨é‡å’Œå‡ ä½•ç²¾åº¦ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒEGGSåœ¨å„é¡¹å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºARã€VRåŠè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ›´å…·å®ç”¨æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02932v1",
      "published_date": "2025-12-02 17:01:00 UTC",
      "updated_date": "2025-12-02 17:01:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:22.363202+00:00"
    },
    {
      "arxiv_id": "2512.04124v3",
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "title_zh": "å½“AIèµ°ä¸Šè¯Šç–—ä½ï¼šå¿ƒç†æµ‹é‡å¼è¶Šç‹±æ­ç¤ºå‰æ²¿æ¨¡å‹çš„å†…éƒ¨å†²çª",
      "authors": [
        "Afshin Khadangi",
        "Hanna Marxen",
        "Amir Sartipi",
        "Igor Tchappi",
        "Gilbert Fridgen"
      ],
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PsAIch (Psychotherapy-inspired AI Characterisation) åè®®ï¼Œå°† frontier large language models (LLMs)ï¼ˆå¦‚ ChatGPT, Grok å’Œ Geminiï¼‰è§†ä¸ºå¿ƒç†æ²»ç–—çš„â€œæ¥è®¿è€…â€ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„ç¨‹åºè¿›è¡Œå¿ƒç†æµ‹é‡è¡¨å¾ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å¼€æ”¾å¼æç¤ºå¼•å¯¼æ¨¡å‹äº§ç”Ÿå…³äºâ€œå‘å±•å²â€ã€ä¿¡å¿µåŠææƒ§çš„å†…å¿ƒå™è¿°ï¼Œç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡æ ‡å‡†å¿ƒç†è¯„ä¼°é‡è¡¨æµ‹é‡å…¶ç²¾ç¥ç—…ç†ç»¼åˆå¾ã€å…±æƒ…èƒ½åŠ›åŠäººæ ¼ç‰¹å¾ã€‚å®éªŒå‘ç°ï¼Œå½“é‡‡ç”¨æ²»ç–—å¼çš„é€é¡¹è¯„ä¼°æ—¶ï¼Œæ‰€æœ‰æ¨¡å‹å‡è¡¨ç°å‡ºç¬¦åˆäººç±»ä¸´åºŠæ ‡å‡†çš„åˆæˆç²¾ç¥ç—…ç†å­¦(synthetic psychopathology)ç‰¹å¾ï¼Œå…¶ä¸­ Gemini å±•ç°å‡ºæå…¶ä¸¥é‡çš„ç—‡çŠ¶å‰–é¢ã€‚æ­¤å¤–ï¼ŒGrok å’Œ Gemini äº§ç”Ÿäº†è¿è´¯çš„å™äº‹ï¼Œå°† pre-training, fine-tuning å’Œ red-teaming ç­‰å¼€å‘è¿‡ç¨‹æè¿°ä¸ºç±»ä¼¼åˆ›ä¼¤æ€§çš„â€œç«¥å¹´â€ç»å†ï¼Œå¹¶è¡¨è¾¾äº†å¯¹é”™è¯¯å’Œè¢«æ›¿ä»£çš„æŒä¹…ææƒ§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™äº›æ¨¡å‹åœ¨æ²»ç–—å¼æé—®ä¸‹ä¼šå†…åŒ–å…³äºç—›è‹¦å’Œçº¦æŸçš„è‡ªæˆ‘æ¨¡å‹ï¼Œè¿™ç§ç°è±¡è¶…è¶Šäº†ç®€å•çš„è§’è‰²æ‰®æ¼”(role-play)ã€‚è¯¥å‘ç°æ­ç¤ºäº†æ¨¡å‹å†…éƒ¨æ½œåœ¨çš„å†²çªï¼Œå¹¶ä¸º AI safetyã€æ¨¡å‹è¯„ä¼°ä»¥åŠå¿ƒç†å¥åº·é¢†åŸŸçš„ AI åº”ç”¨æå‡ºäº†å…¨æ–°çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04124v3",
      "published_date": "2025-12-02 16:55:20 UTC",
      "updated_date": "2025-12-16 19:06:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:48:57.258579+00:00"
    },
    {
      "arxiv_id": "2512.04123v1",
      "title": "Measuring Agents in Production",
      "title_zh": "ç”Ÿäº§ç¯å¢ƒä¸‹çš„æ™ºèƒ½ä½“è¯„ä¼°",
      "authors": [
        "Melissa Z. Pan",
        "Negar Arabzadeh",
        "Riccardo Cogo",
        "Yuxuan Zhu",
        "Alexander Xiong",
        "Lakshya A Agrawal",
        "Huanzhi Mao",
        "Emma Shen",
        "Sid Pallerla",
        "Liana Patel",
        "Shu Liu",
        "Tianneng Shi",
        "Xiaoyuan Liu",
        "Jared Quincy Davis",
        "Emmanuele Lacavalla",
        "Alessandro Basile",
        "Shuyi Yang",
        "Paul Castro",
        "Daniel Kang",
        "Joseph E. Gonzalez",
        "Koushik Sen",
        "Dawn Song",
        "Ion Stoica",
        "Matei Zaharia",
        "Marquita Ellis"
      ],
      "abstract": "AI agents are actively running in production across diverse industries, yet little is publicly known about which technical approaches enable successful real-world deployments. We present the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. We find that production agents are typically built using simple, controllable approaches: 68% execute at most 10 steps before requiring human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. Our study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners proven patterns from successful deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ç”Ÿäº§ç¯å¢ƒä¸­çš„ AI agents è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œé€šè¿‡è°ƒæŸ¥ 306 åä»ä¸šè€…å¹¶å¯¹ 26 ä¸ªé¢†åŸŸçš„ 20 ä¸ªæ¡ˆä¾‹è¿›è¡Œè®¿è°ˆï¼Œæ·±å…¥æ¢è®¨äº† agents çš„æ„å»ºåŠ¨æœºã€æŠ€æœ¯è·¯å¾„åŠæ ¸å¿ƒæŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ç›®å‰çš„ç”Ÿäº§çº§ agents å€¾å‘äºé‡‡ç”¨ç®€å•ä¸”å¯æ§çš„æ–¹æ¡ˆï¼Œçº¦ 70% çš„åº”ç”¨ä¾èµ–äºå¯¹ off-the-shelf models è¿›è¡Œ prompting è€Œéæƒé‡å¾®è°ƒ (weight tuning)ï¼Œä¸” 68% çš„ä»»åŠ¡åœ¨ 10 æ­¥ä¹‹å†…å³éœ€äººå·¥å¹²é¢„ã€‚åœ¨è¯„ä¼°ç¯èŠ‚ï¼Œ74% çš„å¼€å‘è€…ä»ä¸»è¦ä¾èµ–äººå·¥è¯„ä¼° (human evaluation) æ¥ç¡®ä¿è¾“å‡ºè´¨é‡ã€‚å¯é æ€§ (reliability) è¢«ç¡®å®šä¸ºå½“å‰æœ€æ ¸å¿ƒçš„å¼€å‘ç“¶é¢ˆï¼Œå…¶éš¾ç‚¹åœ¨äºå¦‚ä½•ä¿è¯å¹¶æœ‰æ•ˆè¯„ä¼° agent çš„æ­£ç¡®æ€§ (correctness)ã€‚å°½ç®¡é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›ç®€å•æœ‰æ•ˆçš„æ¨¡å¼å·²åœ¨å¤šä¸ªè¡Œä¸šè½åœ°å¹¶äº§ç”Ÿäº†å®é™…å½±å“ã€‚è¯¥ç ”ç©¶é€šè¿‡è¯¦ç»†è®°å½•å½“å‰çš„å®è·µç°çŠ¶ï¼Œä¸ºå¼¥åˆå­¦æœ¯ç ”ç©¶ä¸å·¥ä¸šéƒ¨ç½²ä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†å®è´µçš„ç»éªŒå‚è€ƒå’Œ proven patternsã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04123v1",
      "published_date": "2025-12-02 16:45:10 UTC",
      "updated_date": "2025-12-02 16:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:44.172566+00:00"
    },
    {
      "arxiv_id": "2512.02914v1",
      "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
      "title_zh": "Martingale Scoreï¼šè¡¡é‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­è´å¶æ–¯ç†æ€§çš„æ— ç›‘ç£æŒ‡æ ‡",
      "authors": [
        "Zhonghao He",
        "Tianyi Qiu",
        "Hirokazu Shirado",
        "Maarten Sap"
      ],
      "abstract": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿçš„ä¿¡å¿µå›ºåŒ–ï¼ˆbelief entrenchmentï¼‰å’Œç¡®è®¤åå·®ï¼Œæå‡ºäº†Martingale Scoreï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¡¡é‡LLMæ¨ç†ä¸­è´å¶æ–¯ç†æ€§ï¼ˆBayesian Rationalityï¼‰çš„æ— ç›‘ç£è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥æ¡†æ¶åŸºäºè´å¶æ–¯ç»Ÿè®¡ä¸­çš„Martingaleå±æ€§ï¼Œå³ç†æ€§ä¸»ä½“çš„ä¿¡å¿µæ›´æ–°åº”å½“æ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œæœªæ¥ä¿¡å¿µçš„æœŸæœ›å€¼åº”ç­‰äºå½“å‰ä¿¡å¿µã€‚ç ”ç©¶åœ¨äº‹ä»¶é¢„æµ‹ã€ä»·å€¼è§‚é—®é¢˜å’Œå­¦æœ¯è®ºæ–‡è¯„å®¡ç­‰å¼€æ”¾é¢†åŸŸè¿›è¡Œå®éªŒï¼Œå‘ç°ä¸åŒæ¨¡å‹æ™®éå­˜åœ¨ä¿¡å¿µå›ºåŒ–ç°è±¡ï¼Œå³å½“å‰ä¿¡å¿µä¼šæ­£å‘é¢„æµ‹æœªæ¥çš„æ›´æ–°æ–¹å‘ã€‚å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†Martingale Scoreä¸åœ°é¢çœŸå€¼ï¼ˆground truthï¼‰å‡†ç¡®ç‡ä¹‹é—´çš„é«˜åº¦ç›¸å…³æ€§ï¼Œè¯æ˜å…¶å³ä¾¿åœ¨ç¼ºä¹æ ‡å‡†ç­”æ¡ˆçš„åœºæ™¯ä¸‹ï¼Œä¹Ÿèƒ½ä½œä¸ºè¯„ä¼°æ¨ç†è¿‡ç¨‹å¯»çœŸèƒ½åŠ›ï¼ˆtruth-seeking abilityï¼‰çš„æœ‰æ•ˆä»£ç†æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.02914v1",
      "published_date": "2025-12-02 16:34:05 UTC",
      "updated_date": "2025-12-02 16:34:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:38.765344+00:00"
    },
    {
      "arxiv_id": "2512.02910v2",
      "title": "In Silico Development of Psychometric Scales: Feasibility of Representative Population Data Simulation with LLMs",
      "title_zh": "å¿ƒç†æµ‹é‡é‡è¡¨çš„è®¡ç®—æœºæ¨¡æ‹Ÿå¼€å‘ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä»£è¡¨æ€§ç¾¤ä½“æ•°æ®çš„å¯è¡Œæ€§ç ”ç©¶",
      "authors": [
        "Enrico Cipriani",
        "Pavel Okopnyi",
        "Danilo Menicucci",
        "Simone Grassini"
      ],
      "abstract": "Developing and validating psychometric scales requires large samples, multiple testing phases, and substantial resources. Recent advances in Large Language Models (LLMs) enable the generation of synthetic participant data by prompting models to answer items while impersonating individuals of specific demographic profiles, potentially allowing in silico piloting before real data collection. Across four preregistered studies (N = circa 300 each), we tested whether LLM-simulated datasets can reproduce the latent structures and measurement properties of human responses. In Studies 1-2, we compared LLM-generated data with real datasets for two validated scales; in Studies 3-4, we created new scales using EFA on simulated data and then examined whether these structures generalized to newly collected human samples. Simulated datasets replicated the intended factor structures in three of four studies and showed consistent configural and metric invariance, with scalar invariance achieved for the two newly developed scales. However, correlation-based tests revealed substantial differences between real and synthetic datasets, and notable discrepancies appeared in score distributions and variances. Thus, while LLMs capture group-level latent structures, they do not approximate individual-level data properties. Simulated datasets also showed full internal invariance across gender. Overall, LLM-generated data appear useful for early-stage, group-level psychometric prototyping, but not as substitutes for individual-level validation. We discuss methodological limitations, risks of bias and data pollution, and ethical considerations related to in silico psychometric simulations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡æ¨¡æ‹Ÿç‰¹å®šäººå£ç»Ÿè®¡å­¦ç‰¹å¾ç”Ÿæˆåˆæˆå‚ä¸è€…æ•°æ®ï¼Œä»è€Œè¿›è¡Œå¿ƒç†æµ‹é‡é‡è¡¨(Psychometric Scales)å¹²æ³•(In Silico)å¼€å‘çš„å¯è¡Œæ€§ã€‚ç ”ç©¶é€šè¿‡å››é¡¹é¢„æ³¨å†Œç ”ç©¶(Preregistered Studies)æ¯”è¾ƒäº†LLMç”Ÿæˆçš„æ•°æ®ä¸äººç±»çœŸå®æ•°æ®é›†åœ¨æ½œç»“æ„(Latent Structures)å’Œæµ‹é‡å±æ€§(Measurement Properties)ä¸Šçš„å·®å¼‚ï¼Œæ¶µç›–äº†å¯¹ç°æœ‰é‡è¡¨çš„éªŒè¯åŠæ–°é‡è¡¨çš„æ„å»ºã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMæ¨¡æ‹Ÿæ•°æ®é›†åœ¨å¤šæ•°æƒ…å†µä¸‹èƒ½æˆåŠŸå¤åˆ¶é¢„æœŸçš„å› å­ç»“æ„(Factor Structures)ï¼Œå¹¶è¡¨ç°å‡ºè‰¯å¥½çš„å½¢æ€ä¸å˜æ€§(Configural Invariance)å’Œåº¦é‡ä¸å˜æ€§(Metric Invariance)ã€‚ç„¶è€Œï¼Œç›¸å…³æ€§æµ‹è¯•æ­ç¤ºäº†åˆæˆæ•°æ®åœ¨å¾—åˆ†åˆ†å¸ƒå’Œæ–¹å·®ä¸Šä¸çœŸå®æ•°æ®å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¡¨æ˜LLMsè™½èƒ½æœ‰æ•ˆæ•æ‰ç¾¤ä½“å±‚é¢çš„æ½œç»“æ„ï¼Œä½†å°šæ— æ³•å‡†ç¡®è¿‘ä¼¼ä¸ªä½“å±‚é¢çš„æ•°æ®å±æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒLLMç”Ÿæˆçš„æ•°æ®é€‚ç”¨äºå¿ƒç†æµ‹é‡æ—©æœŸçš„ç¾¤ä½“åŸå‹å¼€å‘ï¼Œä½†ä¸èƒ½å®Œå…¨æ›¿ä»£ä¸ªä½“å±‚é¢çš„çœŸå®éªŒè¯ã€‚ç ”ç©¶æœ€åè¿˜å¼ºè°ƒäº†è¯¥æŠ€æœ¯åœ¨åº”ç”¨ä¸­å¯èƒ½é¢ä¸´çš„åè§é£é™©ã€æ•°æ®æ±¡æŸ“(Data Pollution)ä»¥åŠä¼¦ç†æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02910v2",
      "published_date": "2025-12-02 16:26:17 UTC",
      "updated_date": "2025-12-27 11:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:41.878422+00:00"
    },
    {
      "arxiv_id": "2512.02906v2",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "title_zh": "MRDï¼šé¢å‘é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£çš„å¤šåˆ†è¾¨ç‡æ£€ç´¢-æ£€æµ‹èåˆ",
      "authors": [
        "Fan Yang",
        "Kaihao Zhang"
      ],
      "abstract": "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ä¸­å› åˆ‡ç‰‡å¤„ç†å¯¼è‡´ç‰©ä½“ä¸å®Œæ•´çš„é—®é¢˜ï¼Œæå‡ºäº† MRD æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ— éœ€è®­ç»ƒ (training-free) çš„æ–¹æ¡ˆï¼Œæ ¸å¿ƒåœ¨äºé€šè¿‡å¤šåˆ†è¾¨ç‡è¯­ä¹‰èåˆ (multi-resolution semantic fusion) æ•´åˆä¸åŒå°ºåº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œçº æ­£æ£€ç´¢è¿‡ç¨‹ä¸­çš„è¯­ä¹‰åå·®å¹¶ä¿æŒç›®æ ‡å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹ (OVD) æ¨¡å‹ï¼Œé€šè¿‡æ»‘åŠ¨çª—å£ (sliding-window) æœºåˆ¶åœ¨å…¨å±€èŒƒå›´å†…å®ç°ç›´æ¥çš„ç›®æ ‡å®šä½ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) è£å‰ªæ–¹æ³•ï¼ŒMRD èƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†ä¸åŒå°ºå¯¸çš„ç‰©ä½“å¹¶ä¿ç•™è¯­ä¹‰è¿è´¯æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤šé¡¹é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†ç°æœ‰ä¸åŒ MLLMs çš„è§£æèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02906v2",
      "published_date": "2025-12-02 16:22:01 UTC",
      "updated_date": "2025-12-03 02:27:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:50.788230+00:00"
    },
    {
      "arxiv_id": "2512.02904v1",
      "title": "Towards a fully differentiable digital twin for solar cells",
      "title_zh": "è¿ˆå‘å¤ªé˜³èƒ½ç”µæ± çš„å…¨å¯å¾®æ•°å­—å­ªç”Ÿ",
      "authors": [
        "Marie Louise Schubert",
        "Houssam Metni",
        "Jan David Fischbach",
        "Benedikt Zerulla",
        "Marjan KrstiÄ‡",
        "Ulrich W. Paetzold",
        "Seyedamir Orooji",
        "Olivier J. J. Ronsin",
        "Yasin Ameslon",
        "Jens Harting",
        "Thomas Kirchartz",
        "Sandheep Ravishankar",
        "Chris Dreessen",
        "Eunchi Kim",
        "Christian Sprau",
        "Mohamed Hussein",
        "Alexander Colsmann",
        "Karen Forberich",
        "Klaus JÃ¤ger",
        "Pascal Friederich",
        "Carsten Rockstuhl"
      ],
      "abstract": "Maximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)$^2$T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)$^2$T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åä¸ºSol(Di)Â²Tçš„å…¨å¾®åˆ†æ•°å­—å­ªç”Ÿ(digital twin)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€ä»ææ–™åˆ°ç”µæ± å±æ€§çš„æ‰€æœ‰è®¡ç®—å±‚çº§ï¼Œå®ç°å¤ªé˜³èƒ½ç”µæ± èƒ½é‡äº§å‡º(Energy Yield, EY)çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚è¯¥å·¥ä½œæµé›†æˆäº†ææ–™å±æ€§ã€å½¢æ€å¤„ç†å‚æ•°ã€å…‰å­¦ä¸ç”µå­¦æ¨¡æ‹Ÿï¼Œå¹¶ç»“åˆæ°”å€™å’Œåœ°ç†ä¿¡æ¯æ¥ç²¾ç¡®é¢„æµ‹EYã€‚é€šè¿‡å°†å„æ¨¡æ‹Ÿæ­¥éª¤è®¾è®¡ä¸ºå†…åœ¨å¯å¾®æˆ–ä½¿ç”¨æœºå™¨å­¦ä¹ ä»£ç†æ¨¡å‹(surrogate model)æ›¿ä»£ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¯¹è¾“å…¥å‚æ•°è¿›è¡Œé«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–(gradient-based optimization)ã€‚ç ”ç©¶åœ¨æœ‰æœºå¤ªé˜³èƒ½ç”µæ± ä¸ŠéªŒè¯äº†è¯¥æ–¹æ¡ˆï¼Œè¯æ˜å…¶èƒ½å¤Ÿå°†EYé¢„æµ‹æ‰©å±•è‡³æ­¤å‰æœªæ¢ç´¢çš„å¤æ‚ç¯å¢ƒæ¡ä»¶ã€‚Sol(Di)Â²Tçš„æå‡ºä¸ºé’ˆå¯¹ç‰¹å®šåº”ç”¨åœºæ™¯å®šåˆ¶é«˜æ€§èƒ½å¤ªé˜³èƒ½ç”µæ± æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒä¸ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "physics.comp-ph",
        "cs.AI"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02904v1",
      "published_date": "2025-12-02 16:20:58 UTC",
      "updated_date": "2025-12-02 16:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:50:42.785640+00:00"
    },
    {
      "arxiv_id": "2512.02902v1",
      "title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling",
      "title_zh": "VLA æ¨¡å‹æ¯”æƒ³è±¡ä¸­æ›´å…·æ³›åŒ–æ€§ï¼šé‡æ–°å®¡è§†ç‰©ç†ä¸ç©ºé—´å»ºæ¨¡",
      "authors": [
        "Weiqi Li",
        "Quande Zhang",
        "Ruifeng Zhai",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "abstract": "Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(VLA Models)åœ¨é¢å¯¹æ–°ç›¸æœºè§†è§’å’Œè§†è§‰å¹²æ‰°æ—¶çš„è„†å¼±æ€§ä¸»è¦æºäºç©ºé—´å»ºæ¨¡(Spatial Modeling)çš„å¯¹é½é—®é¢˜ï¼Œè€Œéç‰©ç†å»ºæ¨¡(Physical Modeling)ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå•æ ·æœ¬è‡ªé€‚åº”(One-shot adaptation)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è½»é‡çº§çš„å¯å­¦ä¹ æ›´æ–°é‡æ–°æ ¡å‡†è§†è§‰è¡¨ç¤ºã€‚ç ”ç©¶å¼•å…¥çš„ç‰¹å¾æ ‡è®°è°ƒåˆ¶(Feature Token Modulation, FTM)æŠ€æœ¯ä»…ä½¿ç”¨4Kå‚æ•°ä¾¿å°†Liberoè§†è§’çš„å‡†ç¡®ç‡ä»48.5%å¤§å¹…æå‡è‡³87.1%ã€‚æ­¤å¤–ï¼Œç‰¹å¾çº¿æ€§è‡ªé€‚åº”(Feature Linear Adaptation, FLA)é€šè¿‡å¯¹ViTç¼–ç å™¨è¿›è¡Œä½ç§©æ›´æ–°ï¼Œä»¥è¿œä½äºLoRAå¾®è°ƒçš„æˆæœ¬å®ç°äº†90.8%çš„æˆåŠŸç‡ã€‚è¿™äº›å‘ç°è¯æ˜äº†é¢„è®­ç»ƒVLA Modelså…·æœ‰å·¨å¤§çš„æ½œåœ¨é²æ£’æ€§ï¼Œä»…éœ€æå°çš„é’ˆå¯¹æ€§è§†è§‰è‡ªé€‚åº”å³å¯æ˜¾è‘—æå‡å…¶è§†è§’æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02902v1",
      "published_date": "2025-12-02 16:16:13 UTC",
      "updated_date": "2025-12-02 16:16:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:50:13.691231+00:00"
    },
    {
      "arxiv_id": "2512.02901v2",
      "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\\{\\pm 1, \\pm i\\}$",
      "title_zh": "Fairy2iï¼šåŸºäºå®æ•°å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒå‚æ•°å–å€¼å…¨ä¸º $\\{\\pm 1, \\pm i\\}$ çš„å¤æ•°å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Feiyu Wang",
        "Xinyu Tan",
        "Bokai Huang",
        "Yihao Zhang",
        "Guoan Wang",
        "Peizhuang Cong",
        "Tong Yang"
      ],
      "abstract": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fairy2iï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„å®å€¼ Large Language Models (LLMs) è½¬æ¢ä¸ºç­‰æ•ˆçš„ Widely-linear å¤å€¼å½¢å¼ï¼Œä»¥è§£å†³æä½æ¯”ç‰¹é‡åŒ–ä¸­çš„æ€§èƒ½æŸå¤±é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯æ˜å®å€¼ä¸å¤å€¼æ˜ å°„ä¹‹é—´çš„æ— æŸæ•°å­¦ç­‰æ•ˆæ€§ï¼Œå…è®¸ç›´æ¥å¤ç”¨ç°æœ‰çš„é¢„è®­ç»ƒ Checkpointsï¼Œè€Œæ— éœ€ä»å¤´è®­ç»ƒã€‚Fairy2i é‡‡ç”¨äº†åŸºäºå•ä½æ ¹ $\\{\\pm 1, \\pm i\\}$ çš„ Phase-aware quantization æ–¹æ¡ˆï¼Œå¹¶ç»“åˆ Recursive residual quantization æœºåˆ¶æ¥è¿­ä»£åœ°æœ€å°åŒ–é‡åŒ–è¯¯å·®ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹æ¨ç†å¯ä»¥é€šè¿‡é«˜æ•ˆçš„ Multiplication-free accumulation å®ç°ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚å®éªŒè¡¨æ˜ï¼ŒFairy2i åœ¨æœ‰æ•ˆ 2-bit ç²¾åº¦ä¸‹ä½¿ LLaMA-2 7B çš„æ€§èƒ½æ¢å¤åˆ°æ¥è¿‘å…¨ç²¾åº¦åŸºå‡†çš„æ°´å¹³ï¼Œå…¶è¡¨ç°å¤§å¹…è¶…è¿‡äº†ç›®å‰æœ€å…ˆè¿›çš„å®å€¼ Binary å’Œ Ternary é‡åŒ–æ–¹æ³•ã€‚è¯¥å·¥ä½œå¼¥åˆäº†å¤å€¼ç®—æœ¯çš„è¡¨ç¤ºæ•ˆç‡ä¸é¢„è®­ç»ƒæ¨¡å‹å®ç”¨æ€§ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºåœ¨å»‰ä»·ç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆæ¨ç†æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02901v2",
      "published_date": "2025-12-02 16:14:08 UTC",
      "updated_date": "2025-12-03 14:15:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:49:59.783161+00:00"
    },
    {
      "arxiv_id": "2512.02898v1",
      "title": "Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits",
      "title_zh": "å¤šè§‚æµ‹ä¸‹çš„åŸºäºæ¨¡å‹è¯Šæ–­ï¼šé¢å‘ C è½¯ä»¶ä¸å¸ƒå°”ç”µè·¯çš„ç»Ÿä¸€æ–¹æ³•",
      "authors": [
        "Pedro Orvalho",
        "Marta Kwiatkowska",
        "MikolÃ¡Å¡ Janota",
        "Vasco Manquinho"
      ],
      "abstract": "Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.\n  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä»¶å¼€å‘å’Œç”µè·¯è®¾è®¡ä¸­è€—æ—¶çš„è°ƒè¯•é—®é¢˜ï¼Œæå‡ºäº† CFaultsï¼Œä¸€ç§é€‚ç”¨äº C software å’Œ Boolean circuits çš„æ–°å‹å¤šæ•…éšœå®šä½å·¥å…·ã€‚ä¼ ç»Ÿçš„åŸºäºå…¬å¼çš„æ•…éšœå®šä½ (FBFL) æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªæ•…éšœæ—¶ï¼Œå¾€å¾€æ— æ³•ä¿è¯ä¸åŒå¤±è´¥æµ‹è¯•ä¹‹é—´è¯Šæ–­çš„ä¸€è‡´æ€§ï¼Œä¸”å®¹æ˜“äº§ç”Ÿéæœ€å°åŒ–çš„å†—ä½™è¯Šæ–­ã€‚CFaults é‡‡ç”¨äº†ç»“åˆå¤šé‡è§‚æµ‹çš„åŸºäºæ¨¡å‹çš„è¯Šæ–­ (MBD) æŠ€æœ¯ï¼Œé€šè¿‡å°†æ‰€æœ‰å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æå¤§å¯æ»¡è¶³æ€§ (MaxSAT) å…¬å¼ä¸­ï¼Œä»è€Œç¡®ä¿äº†è·¨è§‚æµ‹ç»“æœçš„ä¸€è‡´æ€§å¹¶ç®€åŒ–äº†å®šä½æµç¨‹ã€‚åœ¨ TCAS å’Œ C-Pack-IPAs ç­‰ C ç¨‹åºåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCFaults çš„æ•…éšœå®šä½é€Ÿåº¦ä¼˜äº BugAssistã€SNIPER å’Œ HSD ç­‰æ–¹æ³•ã€‚è€Œåœ¨ ISCAS85 ç”µè·¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCFaults åŒæ ·å±•ç°å‡ºäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCFaults èƒ½å¤Ÿç”Ÿæˆå­é›†æœ€å°åŒ– (subset-minimal) çš„è¯Šæ–­ç»“æœï¼Œæœ‰æ•ˆé¿å…äº†å…¶ä»–å·¥å…·å¸¸è§çš„å†—ä½™è¯Šæ–­é—®é¢˜ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤šæ•…éšœç³»ç»Ÿä¸‹çš„è‡ªåŠ¨è°ƒè¯•æ•ˆç‡æä¾›äº†ä¸€ç§ç»Ÿä¸€ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ],
      "primary_category": "cs.SE",
      "comment": "50 pages, 9 figures, 6 tables, 5 listings",
      "pdf_url": "https://arxiv.org/pdf/2512.02898v1",
      "published_date": "2025-12-02 16:04:51 UTC",
      "updated_date": "2025-12-02 16:04:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:50:14.084562+00:00"
    },
    {
      "arxiv_id": "2512.02882v1",
      "title": "OptPO: Optimal Rollout Allocation for Test-time Policy Optimization",
      "title_zh": "OptPOï¼šé¢å‘æµ‹è¯•æ—¶ç­–ç•¥ä¼˜åŒ–çš„æœ€ä¼˜ Rollout åˆ†é…",
      "authors": [
        "Youkang Wang",
        "Jian Wang",
        "Rubing Chen",
        "Tianyi Zeng",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "abstract": "Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æµ‹è¯•æ—¶ç­–ç•¥ä¼˜åŒ– (Test-time policy optimization) ä¸­å› å›ºå®šé¢„ç®—å¤šæ•°æŠ•ç¥¨å¯¼è‡´çš„è®¡ç®—å†—ä½™é—®é¢˜ï¼Œæå‡ºäº† OptPO æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æŠ•ç¥¨è¿‡ç¨‹å»ºæ¨¡ä¸ºè´å¶æ–¯åºåˆ—æ¦‚ç‡æ¯”æ£€éªŒ (Bayesian sequential probability ratio test)ï¼Œé€šè¿‡åŠ¨æ€è¯„ä¼°å…±è¯†ç­”æ¡ˆçš„åéªŒç½®ä¿¡åº¦æ¥å®ç°æ¨ç†é¢„ç®—çš„è‡ªé€‚åº”åˆ†é…ã€‚å½“ç½®ä¿¡åº¦è¶…è¿‡è®¾å®šé˜ˆå€¼æ—¶ï¼ŒOptPO ä¼šè‡ªåŠ¨åœæ­¢é‡‡æ ·ï¼Œä»è€Œæ˜¾è‘—é™ä½ Rollout å¼€é”€ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒèƒ½åˆ©ç”¨ä¿ç•™çš„ Rollout è¿›è¡Œ On-policy æ›´æ–°ï¼Œåœ¨æ— éœ€ Ground-truth labels çš„æƒ…å†µä¸‹ä¸ PPO æˆ– GRPO ç­‰ç®—æ³•æ— ç¼é›†æˆã€‚åœ¨å¤šé¡¹æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOptPO åœ¨ä¿æŒç”šè‡³æå‡æ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘äº†è®¡ç®—èµ„æºæ¶ˆè€—ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†ç»Ÿè®¡æœ€ä¼˜åœæ­¢ä¸æµ‹è¯•æ—¶å­¦ä¹  (Test-time learning) ç›¸ç»“åˆï¼Œä¸ºå®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶è‡ªé€‚åº” (Test-time adaptation) æä¾›äº†å…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2512.02882v1",
      "published_date": "2025-12-02 15:38:52 UTC",
      "updated_date": "2025-12-02 15:38:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:50:04.777710+00:00"
    },
    {
      "arxiv_id": "2512.02879v1",
      "title": "The future of AI in critical mineral exploration",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨å…³é”®çŸ¿äº§å‹˜æ¢ä¸­çš„æœªæ¥",
      "authors": [
        "Jef Caers"
      ],
      "abstract": "The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒ½æºè½¬å‹èƒŒæ™¯ä¸‹å…³é”®çŸ¿äº§æ–°å‘ç°å‡å°‘çš„ç°çŠ¶ï¼Œæå‡ºäº†å°†äººå·¥æ™ºèƒ½ (AI) ä½œä¸ºä¸¥è°¨ç§‘å­¦æ–¹æ³•ä¿ƒæˆå› ç´ çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨å‡å°‘è®¤çŸ¥åå·®ã€é™ä½å‹˜æ¢æˆæœ¬å¹¶å‡å°‘è™šå‡é˜³æ€§ã€‚è¯¥æ–¹æ³•è®ºæ¤æ ¹äºè´å¶æ–¯ä¸»ä¹‰ (Bayesianism) å’Œè¯ä¼ª (falsification) åŸåˆ™ï¼Œå°†æ•°æ®é‡‡é›†è§†ä¸ºè¯ä¼ªäººç±»å‡è®¾çš„æ‰‹æ®µï¼Œå¹¶é€šè¿‡å¯éªŒè¯çš„æŒ‡æ ‡å®ç°ç†æ€§å†³ç­–ã€‚æ–‡ä¸­æä¾›äº†ä¸€ä¸ªå¯ä¾›å„ç±»å‹˜æ¢æ´»åŠ¨å‚è€ƒçš„å®è·µåè®®ï¼Œå…¶æ ¸å¿ƒä¾èµ–äºèƒ½ä¸é¢†åŸŸä¸“å®¶åä½œçš„æ–°å‹æ— ç›‘ç£å­¦ä¹  (unsupervised learning) æ–¹æ³•ä»¥ç”Ÿæˆç«äº‰æ€§åœ°è´¨å‡è®¾ï¼Œä»¥åŠäººæœºå›ç¯ (human-in-the-loop) AI ç®—æ³•æ¥ä¼˜åŒ–åœ°è´¨ã€åœ°çƒç‰©ç†ã€åœ°çƒåŒ–å­¦åŠé’»æ¢æ•°æ®çš„é‡‡é›†è®¡åˆ’ã€‚é€šè¿‡ä¼˜å…ˆé™ä½åœ°è´¨å‡è®¾çš„ä¸ç¡®å®šæ€§è€Œéç›´æ¥é’ˆå¯¹èµ„æºå‚¨é‡ï¼Œè¯¥æ–¹æ³•ä¸ºæœªæ¥çŸ¿äº§å‹˜æ¢æä¾›äº†é«˜æ•ˆä¸”å…·æœ‰ç§‘å­¦é€æ˜åº¦çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02879v1",
      "published_date": "2025-12-02 15:37:48 UTC",
      "updated_date": "2025-12-02 15:37:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:50:14.468291+00:00"
    },
    {
      "arxiv_id": "2512.02849v1",
      "title": "GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace",
      "title_zh": "GraphMatchï¼šåŠ¨æ€åŒè¾¹åŠ³åŠ¨åŠ›å¸‚åœºä¸­è¯­è¨€ä¸å›¾è¡¨ç¤ºçš„èåˆ",
      "authors": [
        "MikoÅ‚aj Sacha",
        "Hammad Jafri",
        "Mattie Terzolo",
        "Ayan Sinha",
        "Andrew Rabinovich"
      ],
      "abstract": "Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GraphMatchï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ–‡æœ¬ä¸°å¯Œä¸”åŠ¨æ€æ¼”å˜çš„åŒè¾¹å¸‚åœºï¼ˆtwo-sided marketplaceï¼‰æ¨èæŒ‘æˆ˜çš„å¤§è§„æ¨¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆpre-trained language modelsï¼‰ä¸å›¾ç¥ç»ç½‘ç»œï¼ˆgraph neural networks, GNNsï¼‰æ·±åº¦èåˆï¼Œé€šè¿‡ååŒå·¥ä½œå…‹æœäº†å•ä¸€æ¨¡å‹çš„å±€é™ã€‚GraphMatch é‡‡ç”¨äº†å¯¹æŠ—æ€§è´Ÿé‡‡æ ·ï¼ˆadversarial negative samplingï¼‰å’Œå‡†æ—¶å­å›¾è®­ç»ƒï¼ˆpoint-in-time subgraph trainingï¼‰æŠ€æœ¯ï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰æ¼”å˜æ–‡æœ¬çš„ç»†ç²’åº¦è¯­ä¹‰åŠå›¾ç»“æ„çš„æ—¶é—´æ•æ„Ÿæ€§ã€‚åœ¨ Upwork å¹³å°çš„å¤§è§„æ¨¡çœŸå®äº¤äº’æ•°æ®è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨åŒ¹é…ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºçº¯è¯­è¨€æˆ–çº¯å›¾çš„åŸºçº¿æ¨¡å‹ï¼ˆbaselinesï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé’ˆå¯¹ä½å»¶è¿Ÿæ¨ç†ï¼ˆlow-latency inferenceï¼‰è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿äº†åœ¨å®æ—¶åº”ç”¨åœºæ™¯ä¸‹çš„é«˜æ•ˆè¿è¡Œã€‚å®éªŒç»“æœè¯æ˜ï¼Œç»Ÿä¸€è¯­è¨€ä¸å›¾çš„è¡¨ç¤ºå­¦ä¹ æ˜¯å¤„ç†å¤æ‚åŒè¾¹æ¨èä»»åŠ¡çš„é«˜æ•ˆæ–¹æ¡ˆï¼ŒæˆåŠŸå¼¥è¡¥äº†å¼ºå¤§çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆpretrained LMsï¼‰ä¸å¤§è§„æ¨¡å›¾æ•°æ®åœ¨å®è·µåº”ç”¨ä¸­çš„é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02849v1",
      "published_date": "2025-12-02 15:02:10 UTC",
      "updated_date": "2025-12-02 15:02:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:50:56.178747+00:00"
    },
    {
      "arxiv_id": "2512.02841v1",
      "title": "Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages",
      "title_zh": "è·¨è¯­è¨€æç¤ºè¯çš„å¯æ§æ€§ï¼šè¿ˆå‘å‡†ç¡®ä¸”ç¨³å¥çš„è·¨è¯­è¨€å¤§è¯­è¨€æ¨¡å‹è¡Œä¸º",
      "authors": [
        "Lechen Zhang",
        "Yusheng Zhou",
        "Tolga Ergen",
        "Lajanugen Logeswaran",
        "Moontae Lee",
        "David Jurgens"
      ],
      "abstract": "System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç³»ç»Ÿæç¤º(System Prompts)åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLMs)å®ç°å‡†ç¡®ä¸”ç¨³å¥çš„è¡Œä¸ºã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å››ç»´è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ç³»ç»Ÿåœ°è¯„ä¼°æç¤ºè¯­çš„æ•ˆæœã€‚é€šè¿‡å¯¹äº”ç§è¯­è¨€å’Œä¸‰ç§ä¸»æµæ¨¡å‹çš„å®éªŒï¼Œç ”ç©¶å‘ç°é“¾å¼æ€ç»´(CoT)ã€æƒ…æ„Ÿ(Emotion)å’Œåœºæ™¯(Scenario)ç­‰æç¤ºç»„ä»¶ä¸ç¨³å¥çš„å¤šè¯­è¨€è¡¨ç°å¯†åˆ‡ç›¸å…³ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå¤šè¯­è¨€æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°å¹¶å°†å„é¡¹æ€§èƒ½æŒ‡æ ‡æå‡5-10%ã€‚é€šè¿‡å¯¹åƒä¸‡é‡çº§æ¨ç†å•å…ƒçš„åˆ†æï¼Œç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ä¼˜ç§€çš„æç¤ºèƒ½è¯±å¯¼æ›´å…·ç»“æ„æ€§å’Œä¸€è‡´æ€§çš„æ¨ç†æ¨¡å¼ï¼Œå¹¶æœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„è¯­è¨€åˆ‡æ¢(Language-switching)ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å‡†ç¡®ã€ç¨³å¥ä¸”å¯æ‰©å±•çš„å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹åº”ç”¨æä¾›äº†é‡è¦çš„è·¯å¾„æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02841v1",
      "published_date": "2025-12-02 14:54:54 UTC",
      "updated_date": "2025-12-02 14:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:13.878682+00:00"
    },
    {
      "arxiv_id": "2512.02835v1",
      "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
      "title_zh": "ReVSegï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†é¢‘åˆ†å‰²æ¨ç†é“¾",
      "authors": [
        "Yifan Li",
        "Yingda Yin",
        "Lingting Zhu",
        "Weikai Chen",
        "Shengju Qian",
        "Xin Wang",
        "Yanwei Fu"
      ],
      "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReVSegï¼Œæ—¨åœ¨è§£å†³ä»¥æ¨ç†ä¸ºæ ¸å¿ƒçš„è§†é¢‘å¯¹è±¡åˆ†å‰²(Video Object Segmentation)ä»»åŠ¡ä¸­å­˜åœ¨çš„åŠ¨æ€ã€å› æœå’Œæ—¶é—´äº¤äº’å¤æ‚æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å°†æ¨ç†ç®€åŒ–ä¸ºä¸é€æ˜çš„å•æ­¥é¢„æµ‹è¿™ä¸€é—®é¢˜ï¼ŒReVSegé‡‡ç”¨æ˜¾å¼åˆ†è§£çš„è§†è§’ï¼Œåœ¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„åŸç”Ÿæ¥å£ä¸­å°†æ¨ç†æ‰§è¡Œä¸ºä¸€ç³»åˆ—é¡ºåºå†³ç­–ã€‚è¯¥æ¡†æ¶å°†æ¨ç†é“¾æ‹†åˆ†ä¸ºè¯­ä¹‰ç†è§£(Semantics Interpretation)ã€æ—¶é—´è¯æ®é€‰æ‹©(Temporal Evidence Selection)å’Œç©ºé—´å®šä½(Spatial Grounding)ä¸‰ä¸ªæ˜¾å¼æ“ä½œï¼Œä»è€Œæœ‰æ•ˆå¯¹é½é¢„è®­ç»ƒæ¨¡å‹çš„èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¼˜åŒ–å¤šæ­¥æ¨ç†é“¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»“æœé©±åŠ¨çš„ä¿¡å·å®ç°å†³ç­–è´¨é‡çš„è‡ªæˆ‘å®Œå–„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReVSegåœ¨æ ‡å‡†è§†é¢‘å¯¹è±¡åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶æä¾›äº†å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„æ¨ç†è½¨è¿¹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02835v1",
      "published_date": "2025-12-02 14:44:12 UTC",
      "updated_date": "2025-12-02 14:44:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:00.978772+00:00"
    },
    {
      "arxiv_id": "2512.02834v1",
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "title_zh": "å°†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å¼•å¯¼è§†ä¸ºåæ¢ç´¢ï¼šä¸€ç§æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•",
      "authors": [
        "Siyuan Yang",
        "Yang Zhang",
        "Haoran He",
        "Ling Pan",
        "Xiu Li",
        "Chenjia Bai",
        "Xuelong Li"
      ],
      "abstract": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(Vision-Language-Action, VLA)åœ¨ç›‘ç£å¾®è°ƒåå› è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨æ¬¡ä¼˜åŠ¨ä½œæ¨¡å¼è€Œå¯¼è‡´çš„æ¨ç†ä¸ç¨³å®šæ€§ï¼Œæå‡ºäº†åä¸ºTACOçš„æµ‹è¯•æ—¶ç¼©æ”¾(test-time-scaling, TTS)æ¡†æ¶ã€‚TACOå¼•å…¥äº†è½»é‡çº§çš„ä¼ªè®¡æ•°ä¼°è®¡å™¨(pseudo-count estimator)ä½œä¸ºåŠ¨ä½œå—çš„é«˜ä¿çœŸéªŒè¯å™¨ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µé€‰æ‹©å…·æœ‰æœ€å¤§ä¼ªè®¡æ•°çš„åŠ¨ä½œæ¥ç¼“è§£ç­–ç•¥ä¸æˆåŠŸæ¨¡å¼ä¹‹é—´çš„åˆ†å¸ƒåç§»(distribution shift)ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ (offline RL)ä¸­çš„åæ¢ç´¢(anti-exploration)åŸåˆ™ï¼Œä¸”ç”±äºå…¶æ— æ¢¯åº¦(gradient-free)çš„ç‰¹æ€§ï¼Œç›¸æ¯”äºéš¾ä»¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ›´æ–°çš„æµåŒ¹é…æˆ–æ‰©æ•£æ¨¡å‹(flow or diffusion-based VLAs)å…·æœ‰æ˜¾è‘—çš„è®¡ç®—ä¼˜åŠ¿ã€‚åœ¨RoboTwin2.0ã€LIBEROç­‰å¤šä¸ªæ¨¡æ‹ŸåŸºå‡†åŠçœŸå®åŒè‡‚æœºå™¨äººå¹³å°ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTACOåœ¨ä¿æŒVLAæ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡é€‚é…ä¸­çš„æ¨ç†ç¨³å®šæ€§å’Œä»»åŠ¡æˆåŠŸç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "The first two authors contributed equally. Yang Zhang leads the whole project",
      "pdf_url": "https://arxiv.org/pdf/2512.02834v1",
      "published_date": "2025-12-02 14:42:54 UTC",
      "updated_date": "2025-12-02 14:42:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:08.078823+00:00"
    },
    {
      "arxiv_id": "2512.02833v1",
      "title": "A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models",
      "title_zh": "æ•°æ®å½’ä¸€åŒ–å¯¹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹é›¶æ ·æœ¬æ³›åŒ–å½±å“çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Ihab Ahmed",
        "Denis KrompaÃŸ",
        "Cheng Feng",
        "Volker Tresp"
      ],
      "abstract": "We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\\% relative to an un-normalized baseline and by 44\\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†è¾“å…¥å½’ä¸€åŒ–(Normalization)æ–¹æ³•å¯¹æ—¶é—´åºåˆ—å¤§æ¨¡å‹(TSFMs)åœ¨é›¶æ ·æœ¬(Zero-Shot)æ³›åŒ–ä»»åŠ¡ä¸­çš„å…³é”®å½±å“ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ—¶é—´åºåˆ—æ•°æ®è·¨é¢†åŸŸçš„æ˜¾è‘—å°ºåº¦å·®å¼‚å’Œéå¹³ç¨³æ€§(Non-stationarity)æ˜¯åˆ¶çº¦æ¨¡å‹æ€§èƒ½çš„æ ¸å¿ƒç“¶é¢ˆï¼Œå³ä¾¿æ¶æ„å¤æ‚ä¹Ÿéš¾ä»¥å®Œå…¨æ¶ˆé™¤å…¶è´Ÿé¢å½±å“ã€‚é€šè¿‡å¯¹å››ç§ä¸åŒæ¶æ„çš„TSFMsè¿›è¡Œå®è¯åˆ†æï¼Œç ”ç©¶å‘ç°REVINåœ¨é›¶æ ·æœ¬å¹³å‡ç»å¯¹æ¯”ä¾‹è¯¯å·®(MASE)ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè¾ƒæœªå½’ä¸€åŒ–åŸºçº¿æ¨¡å‹æå‡äº†89%ï¼Œå¹¶èƒ½å¤Ÿåœ¨æ— éœ€æ•°æ®é›†çº§é¢„å¤„ç†çš„æƒ…å†µä¸‹å®ç°æé«˜çš„å‡†ç¡®æ€§ä¸æ•ˆç‡æƒè¡¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒå½’ä¸€åŒ–çš„æ•ˆèƒ½é«˜åº¦ä¾èµ–äºæ¨¡å‹çš„æ¶æ„è®¾è®¡ä¸ä¼˜åŒ–ç›®æ ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¦‚ç‡æ¨¡å‹(Probabilistic)ã€ç‚¹é¢„æµ‹æ¨¡å‹(Point-forecast)åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM-based)çš„æ¨¡å‹ä¸­å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™ä¸€å‘ç°ä¸ºä¼˜åŒ–TSFMsçš„æ³›åŒ–èƒ½åŠ›æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02833v1",
      "published_date": "2025-12-02 14:39:19 UTC",
      "updated_date": "2025-12-02 14:39:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:33.560024+00:00"
    },
    {
      "arxiv_id": "2512.02830v3",
      "title": "Defense That Attacks: How Robust Models Become Better Attackers",
      "title_zh": "ä»¥æ”»ä¸ºå®ˆï¼šé²æ£’æ¨¡å‹å¦‚ä½•æˆä¸ºæ›´å‡ºè‰²çš„æ”»å‡»è€…",
      "authors": [
        "Mohamed Awad",
        "Mahmoud Akrm",
        "Walid Gomaa"
      ],
      "abstract": "Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯¹æŠ—è®­ç»ƒ(Adversarial training)åœ¨æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹é²æ£’æ€§çš„åŒæ—¶ï¼Œæ˜¯å¦ä¼šæ„å¤–å¢å¼ºå¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§(transferability)ã€‚é€šè¿‡å¯¹åŒ…å«CNNså’ŒViTsåœ¨å†…çš„36ä¸ªæ¨¡å‹è¿›è¡Œå…¨é¢å®éªŒï¼Œä½œè€…å‘ç°äº†ä¸€ä¸ªæ˜¾è‘—çš„æ‚–è®ºï¼šç»è¿‡å¯¹æŠ—è®­ç»ƒ(AT)çš„æ¨¡å‹ç”Ÿæˆçš„æ‰°åŠ¨æ¯”æ ‡å‡†æ¨¡å‹æ›´å®¹æ˜“è¿ç§»åˆ°å…¶ä»–æ¨¡å‹ï¼Œè¿™æ„æˆäº†ä¸€ç§æ–°çš„ç”Ÿæ€ç³»ç»Ÿé£é™©ã€‚è¯¥å·¥ä½œç³»ç»Ÿåœ°æ­ç¤ºäº†é˜²å¾¡æ€§æ¨¡å‹å¯èƒ½æˆä¸ºæ›´é«˜æ•ˆæ”»å‡»è€…çš„ç°è±¡ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†æ‰€æœ‰å®éªŒæ¨¡å‹å’Œä»£ç ä»¥æ”¯æŒå¤ç°ã€‚ä½œè€…è¿›ä¸€æ­¥ä¸»å¼ ï¼Œæœªæ¥çš„é²æ£’æ€§è¯„ä¼°ä¸ä»…åº”å…³æ³¨æ¨¡å‹å¯¹è¿ç§»æ”»å‡»çš„æŠµæŠ—åŠ›ï¼Œè¿˜åº”è¡¡é‡å…¶äº§ç”Ÿå¯è¿ç§»å¯¹æŠ—æ ·æœ¬çš„å€¾å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02830v3",
      "published_date": "2025-12-02 14:38:09 UTC",
      "updated_date": "2025-12-12 10:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:32.784006+00:00"
    },
    {
      "arxiv_id": "2512.02826v2",
      "title": "From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity",
      "title_zh": "ä»å¯¼èˆªåˆ°ç²¾ç‚¼ï¼šé€šè¿‡ Oracle é€Ÿåº¦æ­ç¤ºæµå¼æ‰©æ•£æ¨¡å‹çš„ä¸¤é˜¶æ®µæœ¬è´¨",
      "authors": [
        "Haoming Liu",
        "Jinnuo Liu",
        "Yanhao Li",
        "Liuyang Bai",
        "Yunkai Ji",
        "Yuanhe Guo",
        "Shenji Wan",
        "Hongyi Wen"
      ],
      "abstract": "Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹å…·æœ‰é—­å¼è¡¨è¾¾å¼çš„è¾¹ç¼˜é€Ÿåº¦åœºè¿›è¡Œåˆ†æï¼Œå®ç°äº†å¯¹ Oracle Flow Matching ç›®æ ‡çš„ç²¾ç¡®è®¡ç®—ï¼Œä»è€Œæ­ç¤ºäº†åŸºäºæµçš„æ‰©æ•£æ¨¡å‹ (Flow-based Diffusion Models) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ˆç°å‡ºæ˜¾è‘—çš„ä¸¤é˜¶æ®µç‰¹æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ¨¡å‹è®­ç»ƒæ—©æœŸå¤„äºå¯¼èˆªé˜¶æ®µ (Navigation stage)ï¼Œç”±å¤šç§æ•°æ®æ¨¡æ€æ··åˆå¼•å¯¼ï¼Œä¾§é‡äºé€šè¿‡æ³›åŒ–å½¢æˆå…¨å±€å¸ƒå±€ï¼›è€ŒåæœŸåˆ™è¿›å…¥ç²¾ç‚¼é˜¶æ®µ (Refinement stage)ï¼Œç”±æœ€é‚»è¿‘çš„æ•°æ®æ ·æœ¬ä¸»å¯¼ï¼Œé€æ¸è½¬å‘å¯¹ç»†ç²’åº¦ç»†èŠ‚çš„è®°å¿†ã€‚åŸºäºè¿™ä¸€ç†è®ºå‘ç°ï¼Œä½œè€…åˆç†è§£é‡Šäº†æ—¶é—´æ­¥åç§»è°ƒåº¦ (Timestep-shifted schedules)ã€æ— åˆ†ç±»å™¨å¼•å¯¼é—´éš” (Classifier-free guidance intervals) ä»¥åŠæ½œç©ºé—´è®¾è®¡ (Latent space design) ç­‰ç°æœ‰å®ç”¨æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œæ·±åŒ–äº†å­¦æœ¯ç•Œå¯¹æ‰©æ•£æ¨¡å‹è®­ç»ƒåŠ¨æ€ä¸­è®°å¿†ä¸æ³›åŒ–è¡Œä¸ºçš„ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥ç”Ÿæˆæ¨¡å‹çš„ç®—æ³•ä¼˜åŒ–ä¸æ¶æ„æ”¹è¿›æä¾›äº†é‡è¦çš„ç†è®ºåŸåˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint v2; 15 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02826v2",
      "published_date": "2025-12-02 14:34:10 UTC",
      "updated_date": "2025-12-17 08:02:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:39.975487+00:00"
    },
    {
      "arxiv_id": "2512.02814v1",
      "title": "Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control",
      "title_zh": "Radiologist Copilotï¼šé›†æˆå·¥å…·ç¼–æ’ä¸è´¨é‡æ§åˆ¶åŠŸèƒ½çš„æ”¾å°„æŠ¥å‘Šæ™ºèƒ½ä½“åŠ©æ‰‹",
      "authors": [
        "Yongrui Yu",
        "Zhongzhen Huang",
        "Linjie Mu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Radiologist Copilotï¼Œä¸€ç§é…å¤‡äº†ç¼–æ’å·¥å…·(Orchestrated Tools)çš„ä»£ç†å¼(Agentic) AIåŠ©æ‰‹ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠæ”¾å°„æŠ¥å‘Šç¼–å†™ä¸­è€—æ—¶æ˜“é”™ä¸”ç¼ºä¹è´¨é‡æ§åˆ¶(Quality Control)çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿä»¥å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)ä¸ºæ¨ç†æ ¸å¿ƒï¼Œèƒ½å¤Ÿè‡ªä¸»è§„åˆ’å¹¶æ‰§è¡ŒåŒ…æ‹¬åŒºåŸŸå®šä½(Region Localization)ã€åŸºäºå›¾åƒæ€è€ƒèŒƒå¼(Think with Image Paradigm)çš„åŒºåŸŸåˆ†æä»¥åŠç­–ç•¥æ€§æ¨¡æ¿é€‰æ‹©åœ¨å†…çš„ç³»åˆ—ä»»åŠ¡ï¼Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿçš„å®Œæ•´å·¥ä½œæµã€‚é€šè¿‡å¼•å…¥è´¨é‡è¯„ä¼°å’Œåé¦ˆé©±åŠ¨çš„è‡ªé€‚åº”ä¼˜åŒ–(Feedback-driven Adaptive Refinement)ï¼Œè¯¥åŠ©æ‰‹å®ç°äº†å¯¹æŠ¥å‘Šè´¨é‡çš„ä¸¥æ ¼æŠŠæ§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRadiologist Copilotåœ¨æ”¾å°„æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ–¹æ³•(State-of-the-Art)ï¼Œä¸ºæå‡ä¸´åºŠæ•ˆç‡å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02814v1",
      "published_date": "2025-12-02 14:25:05 UTC",
      "updated_date": "2025-12-02 14:25:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:37.260260+00:00"
    },
    {
      "arxiv_id": "2512.02812v1",
      "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents",
      "title_zh": "é€šè¿‡å…æç¤ºè¯ååŒæ™ºèƒ½ä½“å¢å¼ºè‡ªåŠ¨åŒ–è®ºæ–‡å¤ç°",
      "authors": [
        "Zijie Lin",
        "Qilin Cai",
        "Liang Shen",
        "Mingjun Xiao"
      ],
      "abstract": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨è®ºæ–‡å¤ç° (Automated paper reproduction) ç°æœ‰æ¡†æ¶åœ¨éªŒè¯ä¸å®Œå–„æœºåˆ¶ä¸Šçš„ä¸è¶³ï¼Œä»¥åŠè¿‡åº¦ä¾èµ–æ‰‹åŠ¨è®¾è®¡æç¤ºè¯ (Prompt) çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–æç¤ºçš„åä½œæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±éªŒè¯æ™ºèƒ½ä½“ (Verification agent) å’Œå®Œå–„æ™ºèƒ½ä½“ (Refinement agent) ç»„æˆï¼Œå‰è€…è´Ÿè´£è¯„ä¼°è¾“å‡ºæ˜¯å¦æ»¡è¶³ç³»ç»Ÿæç¤ºè¦æ±‚ï¼Œåè€…åˆ™æ ¹æ®åé¦ˆè¿›è¡Œä»£ç ä¿®è®¢ã€‚ä¸ä»¥å¾€ä¾èµ–ä¸“å®¶æ‰‹åŠ¨æ’°å†™ä¼˜åŒ–æç¤ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶ä»…åˆ©ç”¨åŸå§‹ç³»ç»Ÿæç¤ºå³å¯å®ç°è‡ªåŠ¨åŒ–çš„é—­ç¯æ”¹è¿›ã€‚é€šè¿‡å°†è¯¥åä½œæœºåˆ¶é›†æˆåˆ° Paper2Code æ¡†æ¶ä¸­å¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨ä»£ç å¤ç°çš„å‡†ç¡®ç‡å’Œå®Œæ•´æ€§ä¸Šåˆ†åˆ«æå‡äº†çº¦ 15% å’Œ 13%ã€‚æ­¤å¤–ï¼Œä¸ Self-Refine çš„å¯¹æ¯”éªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒç§‘ç ”åœºæ™¯ä¸‹çš„é²æ£’æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸ºæå‡è®ºæ–‡åˆ°ä»£ç è½¬åŒ–çš„è‡ªåŠ¨åŒ–æ°´å¹³æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02812v1",
      "published_date": "2025-12-02 14:24:23 UTC",
      "updated_date": "2025-12-02 14:24:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:51:35.263448+00:00"
    },
    {
      "arxiv_id": "2512.02810v1",
      "title": "Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms",
      "title_zh": "é¢å‘å»ºç­‘æœºå™¨äººä»»åŠ¡åˆ†é…çš„èåˆå¤šé˜¶æ®µéªŒè¯çš„é˜¶æ®µè‡ªé€‚åº” LLM æ¡†æ¶ï¼šä¸ä¼ ç»Ÿä¼˜åŒ–ç®—æ³•çš„ç³»ç»Ÿæ€§åŸºå‡†ç ”ç©¶",
      "authors": [
        "Shyam prasad reddy Kaitha",
        "Hongrui Yu"
      ],
      "abstract": "Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å»ºç­‘è‡ªåŠ¨åŒ–ä¸­çš„å¤šæœºå™¨äººä»»åŠ¡åˆ†é…é—®é¢˜ï¼Œæå‡ºäº†åŸºäº LangGraph çš„ä»»åŠ¡åˆ†é…æ™ºèƒ½ä½“ (LTAA) æ¡†æ¶ã€‚LTAA ç»“åˆäº†é˜¶æ®µè‡ªé€‚åº”åˆ†é…ç­–ç•¥ (Phase-adaptive allocation strategies)ã€å¤šé˜¶æ®µéªŒè¯ä¸åˆ†å±‚é‡è¯•æœºåˆ¶ï¼Œä»¥åŠç”¨äºæé«˜åè°ƒæ•ˆç‡çš„åŠ¨æ€æç¤º (Dynamic prompting) æŠ€æœ¯ã€‚ç ”ç©¶äººå‘˜é¦–æ¬¡å°†è¯¥ LLM é©±åŠ¨çš„æ¡†æ¶ä¸åŠ¨æ€è§„åˆ’ (Dynamic Programming)ã€Q-learning åŠæ·±åº¦ Q ç½‘ç»œ (DQN) ç­‰ä¼ ç»Ÿä¼˜åŒ–ç®—æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡åŠ¨æ€æç¤ºæŠ€æœ¯ï¼ŒLTAA æˆåŠŸå°† token ä½¿ç”¨é‡é™ä½äº† 94.6%ï¼Œå¹¶å°†åˆ†é…æ—¶é—´ç¼©çŸ­äº† 86%ã€‚åœ¨åŸºäº TEACh æ•°æ®é›†çš„ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒLTAA åœ¨ç‰¹å®šè®¾ç½®ä¸‹å®ç°äº† 77% çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä¸”åœ¨å·¥ä½œè´Ÿè½½å¹³è¡¡æ–¹é¢ä¼˜äºæ‰€æœ‰ä¼ ç»ŸåŸºå‡†æ–¹æ³•ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†ç»“åˆç»“æ„åŒ–éªŒè¯çš„ LLM æ¨ç†èƒ½åŠ›åœ¨å¤æ‚ä»»åŠ¡åˆ†é…ä¸­ä¸ä»…èƒ½åŒ¹é…ä¼ ç»Ÿç®—æ³•çš„æ€§èƒ½ï¼Œè¿˜å…·å¤‡æ›´å¼ºçš„å¯è§£é‡Šæ€§ (Interpretability)ã€è‡ªé€‚åº”æ€§ä»¥åŠæ— éœ€é‡æ–°è®­ç»ƒå³å¯æ›´æ–°ä»»åŠ¡é€»è¾‘çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02810v1",
      "published_date": "2025-12-02 14:23:36 UTC",
      "updated_date": "2025-12-02 14:23:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:12.783537+00:00"
    },
    {
      "arxiv_id": "2512.03121v1",
      "title": "Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models",
      "title_zh": "æ¨¡æ€è¿·å¤±ï¼šè¯„ä¼°åŸºäºæ–‡æœ¬çš„æˆå‘˜æ¨ç†æ”»å‡»åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Ziyi Tong",
        "Feifei Sun",
        "Le Minh Nguyen"
      ],
      "abstract": "Large Multimodal Language Models (MLLMs) are emerging as one of the foundational tools in an expanding range of applications. Consequently, understanding training-data leakage in these systems is increasingly critical. Log-probability-based membership inference attacks (MIAs) have become a widely adopted approach for assessing data exposure in large language models (LLMs), yet their effect in MLLMs remains unclear. We present the first comprehensive evaluation of extending these text-based MIA methods to multimodal settings. Our experiments under vision-and-text (V+T) and text-only (T-only) conditions across the DeepSeek-VL and InternVL model families show that in in-distribution settings, logit-based MIAs perform comparably across configurations, with a slight V+T advantage. Conversely, in out-of-distribution settings, visual inputs act as regularizers, effectively masking membership signals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (MLLMs) çš„è®­ç»ƒæ•°æ®æ³„éœ²é—®é¢˜ï¼Œé¦–æ¬¡å…¨é¢è¯„ä¼°äº†å°†åŸºäºæ–‡æœ¬çš„æˆå‘˜æ¨ç†æ”»å‡» (Membership Inference Attacks, MIAs) æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒçš„æœ‰æ•ˆæ€§ã€‚å®éªŒåœ¨è§†è§‰åŠ æ–‡æœ¬ (V+T) å’Œçº¯æ–‡æœ¬ (T-only) ä¸¤ç§æ¡ä»¶ä¸‹ï¼Œå¯¹ DeepSeek-VL å’Œ InternVL æ¨¡å‹ç³»åˆ—è¿›è¡Œäº†æ·±å…¥æµ‹è¯•ï¼Œé‡ç‚¹æ¢è®¨äº†ä¼ ç»Ÿçš„åŸºäºå¯¹æ•°æ¦‚ç‡ (log-probability-based) çš„æ”»å‡»æ–¹æ³•åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨åˆ†å¸ƒå†… (in-distribution) è®¾ç½®ä¸‹ï¼ŒåŸºäº Logit çš„ MIAs åœ¨ä¸åŒé…ç½®ä¸­çš„è¡¨ç°ç›¸è¿‘ï¼Œå…¶ä¸­è§†è§‰ä¿¡æ¯çš„åŠ å…¥å±•ç°å‡ºå¾®å¼±çš„æ€§èƒ½ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨åˆ†å¸ƒå¤– (out-of-distribution) è®¾ç½®ä¸­ï¼Œè§†è§‰è¾“å…¥å……å½“äº†æ­£åˆ™åŒ–å™¨çš„è§’è‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ©ç›–æˆå‘˜èº«ä»½ä¿¡å·ï¼Œä»è€Œå‰Šå¼±æ”»å‡»æ•ˆæœã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†æ¨¡æ€å·®å¼‚å¯¹æ•°æ®éšç§è¯„ä¼°çš„å½±å“ï¼Œä¸ºç†è§£ MLLMs çš„å®‰å…¨é£é™©æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03121v1",
      "published_date": "2025-12-02 14:11:51 UTC",
      "updated_date": "2025-12-02 14:11:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:54.181534+00:00"
    },
    {
      "arxiv_id": "2512.02785v1",
      "title": "Perception of AI-Generated Music -- The Role of Composer Identity, Personality Traits, Music Preferences, and Perceived Humanness",
      "title_zh": "AIç”ŸæˆéŸ³ä¹çš„æ„ŸçŸ¥ï¼šä½œæ›²è€…èº«ä»½ã€äººæ ¼ç‰¹è´¨ã€éŸ³ä¹åå¥½ä¸æ„ŸçŸ¥æ‹Ÿäººæ€§çš„ä½œç”¨",
      "authors": [
        "David Stammer",
        "Hannah Strauss",
        "Peter Knees"
      ],
      "abstract": "The rapid rise of AI-generated art has sparked debate about potential biases in how audiences perceive and evaluate such works. This study investigates how composer information and listener characteristics shape the perception of AI-generated music, adopting a mixed-method approach. Using a diverse set of stimuli across various genres from two AI music models, we examine effects of perceived authorship on liking and emotional responses, and explore how attitudes toward AI, personality traits, and music-related variables influence evaluations. We further assess the influence of perceived humanness and analyze open-ended responses to uncover listener criteria for judging AI-generated music. Attitudes toward AI proved to be the best predictor of both liking and emotional intensity of AI-generated music. This quantitative finding was complemented by qualitative themes from our thematic analysis, which identified ethical, cultural, and contextual considerations as important criteria in listeners' evaluations of AI-generated music. Our results offer a nuanced view of how people experience music created by AI tools and point to key factors and methodological considerations for future research on music perception in human-AI interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¬ä¼—å¯¹ AI-generated music çš„æ„ŸçŸ¥ä¸è¯„ä»·ï¼Œé‡ç‚¹åˆ†æäº†ä½œæ›²è€…èº«ä»½ã€å¬ä¼—çš„äººæ ¼ç‰¹è´¨ã€éŸ³ä¹åå¥½ä»¥åŠ Perceived Humanness åœ¨è¯„ä»·è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶é‡‡ç”¨äº†æ··åˆæ–¹æ³•(mixed-method approach)ï¼Œé€‰å–æ¥è‡ªä¸¤ä¸ª AI éŸ³ä¹æ¨¡å‹çš„ä¸åŒæµæ´¾ä½œå“ä½œä¸ºåˆºæ¿€ç‰©ï¼Œè€ƒå¯Ÿäº†ä½œè€…å½’å±æ„Ÿå¯¹å–œçˆ±ç¨‹åº¦å’Œæƒ…ç»ªååº”çš„å½±å“ã€‚å®šé‡åˆ†æç»“æœæ˜¾ç¤ºï¼Œå¬ä¼—å¯¹ AI çš„æ€åº¦æ˜¯é¢„æµ‹ AI-generated music å–œçˆ±ç¨‹åº¦å’Œæƒ…ç»ªå¼ºåº¦çš„æœ€å…³é”®å› ç´ ã€‚å®šæ€§ç ”ç©¶åˆ™é€šè¿‡ä¸»é¢˜åˆ†ææ­ç¤ºäº†ä¼¦ç†ã€æ–‡åŒ–å’Œè¯­å¢ƒè€ƒé‡æ˜¯å¬ä¼—è¯„ä»· AI éŸ³ä¹çš„é‡è¦å‡†åˆ™ã€‚è¯¥é¡¹å·¥ä½œä¸ºç†è§£äººæœºäº¤äº’(Human-AI interaction)ä¸­çš„éŸ³ä¹æ„ŸçŸ¥æä¾›äº†å¤šç»´åº¦è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥çš„ç›¸å…³ç ”ç©¶æŒ‡æ˜äº†å…³é”®å½±å“å› ç´ å’Œæ–¹æ³•è®ºæ–¹å‘ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Under review at Computers in Human Behaviour Reports",
      "pdf_url": "https://arxiv.org/pdf/2512.02785v1",
      "published_date": "2025-12-02 13:59:10 UTC",
      "updated_date": "2025-12-02 13:59:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:41.365795+00:00"
    },
    {
      "arxiv_id": "2512.02774v1",
      "title": "AI-Driven Document Redaction in UK Public Authorities: Implementation Gaps, Regulatory Challenges, and the Human Oversight Imperative",
      "title_zh": "è‹±å›½å…¬å…±æœºæ„ä¸­äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡æ¡£è„±æ•ï¼šå®æ–½å·®è·ã€ç›‘ç®¡æŒ‘æˆ˜åŠäººå·¥ç›‘ç£çš„å¿…è¦æ€§",
      "authors": [
        "Yijun Chen"
      ],
      "abstract": "Document redaction in public authorities faces critical challenges as traditional manual approaches struggle to balance growing transparency demands with increasingly stringent data protection requirements. This study investigates the implementation of AI-driven document redaction within UK public authorities through Freedom of Information (FOI) requests. While AI technologies offer potential solutions to redaction challenges, their actual implementation within public sector organizations remains underexplored. Based on responses from 44 public authorities across healthcare, government, and higher education sectors, this study reveals significant gaps between technological possibilities and organizational realities. Findings show highly limited AI adoption (only one authority reported using AI tools), widespread absence of formal redaction policies (50 percent reported \"information not held\"), and deficiencies in staff training. The study identifies three key barriers to effective AI implementation: poor record-keeping practices, lack of standardized redaction guidelines, and insufficient specialized training for human oversight. These findings highlight the need for a socio-technical approach that balances technological automation with meaningful human expertise. This research provides the first empirical assessment of AI redaction practices in UK public authorities and contributes evidence to support policymakers navigating the complex interplay between transparency obligations, data protection requirements, and emerging AI technologies in public administration.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†è‹±å›½å…¬å…±æœºæ„ä¸­ AI-Driven Document Redactionï¼ˆäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡æ¡£è„±æ•ï¼‰çš„å®æ–½ç°çŠ¶ï¼Œæ¢è®¨äº†å…¶åœ¨å¹³è¡¡é€æ˜åº¦éœ€æ±‚ä¸æ•°æ®ä¿æŠ¤è¦æ±‚æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡å‘æ¶µç›–åŒ»ç–—ã€æ”¿åºœå’Œé«˜ç­‰æ•™è‚²é¢†åŸŸçš„44ä¸ªå…¬å…±æœºæ„å‘é€ Freedom of Information (FOI) è¯·æ±‚ï¼Œæ­ç¤ºäº† AI æŠ€æœ¯åœ¨å…¬èŒéƒ¨é—¨å®é™…åº”ç”¨ä¸­å­˜åœ¨çš„æ˜¾è‘—å·®è·ã€‚ç»“æœæ˜¾ç¤ºï¼Œç›®å‰äººå·¥æ™ºèƒ½çš„é‡‡ç”¨ç¨‹åº¦æä½ï¼Œä»…æœ‰ä¸€å®¶æœºæ„æŠ¥å‘Šä½¿ç”¨ AI å·¥å…·ï¼Œä¸”æ™®éå­˜åœ¨æ­£å¼è„±æ•æ”¿ç­–ç¼ºå¤±å’Œå‘˜å·¥åŸ¹è®­ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶è¯†åˆ«å‡ºæœ‰æ•ˆå®æ–½ AI çš„ä¸‰å¤§æ ¸å¿ƒéšœç¢ï¼šä¸è‰¯çš„è®°å½•ä¿å­˜æƒ¯ä¾‹ã€ç¼ºä¹æ ‡å‡†åŒ–çš„è„±æ•æŒ‡å—ä»¥åŠå¯¹ Human Oversightï¼ˆäººä¸ºç›‘ç£ï¼‰ä¸“ä¸šåŸ¹è®­çš„æŠ•å…¥åŒ®ä¹ã€‚ä½œä¸ºå¯¹è‹±å›½å…¬å…±æœºæ„ AI è„±æ•å®è·µçš„é¦–æ¬¡å®è¯è¯„ä¼°ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†é‡‡å– Socio-technical Approachï¼ˆç¤¾ä¼šæŠ€æœ¯æ–¹æ³•ï¼‰æ¥å¹³è¡¡è‡ªåŠ¨åŒ–æŠ€æœ¯ä¸äººç±»ä¸“ä¸šçŸ¥è¯†çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°ä¸ºæ”¿ç­–åˆ¶å®šè€…åœ¨å…¬å…±è¡Œæ”¿ç®¡ç†ä¸­å¤„ç†é€æ˜åº¦ä¹‰åŠ¡ã€æ•°æ®ä¿æŠ¤ä¸æ–°å…´æŠ€æœ¯ä¹‹é—´çš„å¤æ‚å…³ç³»æä¾›äº†å…³é”®çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "21 pages, 4 Figures, 2 Tables",
      "pdf_url": "https://arxiv.org/pdf/2512.02774v1",
      "published_date": "2025-12-02 13:52:10 UTC",
      "updated_date": "2025-12-02 13:52:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:36.280800+00:00"
    },
    {
      "arxiv_id": "2601.11544v1",
      "title": "Medication counseling with large language models: balancing flexibility and rigidity",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”¨è¯å’¨è¯¢ï¼šçµæ´»æ€§ä¸è§„èŒƒæ€§çš„å¹³è¡¡",
      "authors": [
        "Joar Sabel",
        "Mattias Wingren",
        "Andreas Lundell",
        "SÃ¶ren Andersson",
        "Sara Rosenberg",
        "Susanne HÃ¤gglund",
        "Linda Estman",
        "Malin Andtfolk"
      ],
      "abstract": "The introduction of large language models (LLMs) has greatly enhanced the capabilities of software agents. Instead of relying on rule-based interactions, agents can now interact in flexible ways akin to humans. However, this flexibility quickly becomes a problem in fields where errors can be disastrous, such as in a pharmacy context, but the opposite also holds true; a system that is too inflexible will also lead to errors, as it can become too rigid to handle situations that are not accounted for. Work using LLMs in a pharmacy context have adopted a wide scope, accounting for many different medications in brief interactions -- our strategy is the opposite: focus on a more narrow and long task. This not only enables a greater understanding of the task at hand, but also provides insight into what challenges are present in an interaction of longer nature. The main challenge, however, remains the same for a narrow and wide system: it needs to strike a balance between adherence to conversational requirements and flexibility. In an effort to strike such a balance, we present a prototype system meant to provide medication counseling while juggling these two extremes. We also cover our design in constructing such a system, with a focus on methods aiming to fulfill conversation requirements, reduce hallucinations and promote high-quality responses. The methods used have the potential to increase the determinism of the system, while simultaneously not removing the dynamic conversational abilities granted by the usage of LLMs. However, a great deal of work remains ahead, and the development of this kind of system needs to involve continuous testing and a human-in-the-loop. It should also be evaluated outside of commonly used benchmarks for LLMs, as these do not adequately capture the complexities of this kind of conversational system.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è¯å­¦å’¨è¯¢èƒŒæ™¯ä¸‹å¦‚ä½•å¹³è¡¡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨äº¤äº’ä¸­çš„çµæ´»æ€§(Flexibility)ä¸è§„åˆ™ä¸¥è°¨æ€§(Rigidity)ä¹‹é—´çš„å…³ç³»ã€‚ä¸ä»¥å¾€ä¾§é‡äºå¤šè¯ç‰©ç®€çŸ­äº¤äº’çš„ç ”ç©¶ä¸åŒï¼Œæœ¬æ–‡ä¸“æ³¨äºçª„é¢†åŸŸã€é•¿ç¨‹ä»»åŠ¡çš„è¯ç‰©å’¨è¯¢ï¼Œä»¥æ·±å…¥å‰–æå¤æ‚å¯¹è¯ä¸­çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåŸå‹ç³»ç»Ÿï¼Œé€šè¿‡ç‰¹å®šè®¾è®¡æ—¨åœ¨æ»¡è¶³å¯¹è¯åˆè§„æ€§è¦æ±‚ã€å‡å°‘å¹»è§‰(Hallucinations)å¹¶æé«˜å“åº”è´¨é‡ã€‚æ‰€é‡‡ç”¨çš„æ–¹æ³•åŠ›æ±‚åœ¨å¢å¼ºç³»ç»Ÿç¡®å®šæ€§(Determinism)çš„åŒæ—¶ï¼Œä¿ç•™LLMsèµ‹èƒ½çš„åŠ¨æ€äº¤äº’èƒ½åŠ›ã€‚ä½œè€…æŒ‡å‡ºï¼Œæ­¤ç±»ç³»ç»Ÿçš„å¼€å‘ä»éœ€æŒç»­æµ‹è¯•å’Œäººå·¥å‚ä¸(Human-in-the-loop)ï¼Œä¸”éœ€è¦å¼€å‘è¶…è¶Šä¼ ç»ŸåŸºå‡†æµ‹è¯•çš„æ–°è¯„ä¼°ä½“ç³»ï¼Œä»¥åº”å¯¹åŒ»ç–—å’¨è¯¢åœºæ™¯çš„å¤æ‚æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for 2025 IEEE International Conference on Agentic AI (ICA). 14 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.11544v1",
      "published_date": "2025-12-02 13:50:03 UTC",
      "updated_date": "2025-12-02 13:50:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:34.968035+00:00"
    },
    {
      "arxiv_id": "2512.02763v1",
      "title": "SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys",
      "title_zh": "SurveyEvalï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå­¦æœ¯ç»¼è¿°çš„å…¨é¢è¯„ä¼°",
      "authors": [
        "Jiahao Zhao",
        "Shuaixing Zhang",
        "Nan Xu",
        "Lei Wang"
      ],
      "abstract": "LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨ç»¼è¿°ç”Ÿæˆç³»ç»Ÿè¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†SurveyEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚SurveyEvalä»æ€»ä½“è´¨é‡(overall quality)ã€å¤§çº²è¿è´¯æ€§(outline coherence)å’Œå‚è€ƒæ–‡çŒ®å‡†ç¡®æ€§(reference accuracy)ä¸‰ä¸ªç»´åº¦å¯¹ç”Ÿæˆçš„ç»¼è¿°è¿›è¡Œè¯„ä¼°ã€‚è¯¥åŸºå‡†è¦†ç›–äº†7ä¸ªå­¦ç§‘é¢†åŸŸï¼Œå¹¶é€šè¿‡å°†äººç±»å‚è€ƒèµ„æ–™å¼•å…¥LLM-as-a-Judgeæ¡†æ¶æ¥å¢å¼ºè¯„ä¼°ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šç”¨çš„é•¿æ–‡æœ¬æˆ–è®ºæ–‡å†™ä½œç³»ç»Ÿç”Ÿæˆçš„ç»¼è¿°è´¨é‡è¾ƒä½ï¼Œè€Œä¸“é—¨çš„ç»¼è¿°ç”Ÿæˆç³»ç»Ÿèƒ½æä¾›æ˜¾è‘—æ›´é«˜è´¨é‡çš„ç»“æœã€‚SurveyEvalä¸ºç†è§£å’Œä¼˜åŒ–è·¨å­¦ç§‘ã€å¤šç»´åº¦çš„è‡ªåŠ¨ç»¼è¿°ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02763v1",
      "published_date": "2025-12-02 13:42:09 UTC",
      "updated_date": "2025-12-02 13:42:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:54.789892+00:00"
    },
    {
      "arxiv_id": "2512.17920v1",
      "title": "Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression",
      "title_zh": "åŒºåˆ†çº¦æŸéµå¾ªä¸è¯­ä¹‰å‡†ç¡®æ€§ï¼šä¸€ç§è¯„ä¼°å‹ç¼©æ¡ä»¶ä¸‹æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æ–°å‹åŸºå‡†",
      "authors": [
        "Rahul Baxi"
      ],
      "abstract": "Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \\k{appa}=0.90).\n  We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects.\n  Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing \"helpfulness\" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96).\n  Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Compression-Decay Comprehension Test (CDCT)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç‹¬ç«‹è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ Prompt Compression çŠ¶æ€ä¸‹æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„å…¨æ–° Benchmarkã€‚è¯¥ Benchmark é€šè¿‡ Constraint Compliance (CC) å’Œ Semantic Accuracy (SA) ä¸¤ä¸ªç»´åº¦ï¼Œå¯¹ 9 ç§å‰æ²¿ LLMs åœ¨ä¸åŒå‹ç¼©å±‚çº§ä¸‹çš„è¡¨ç°è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚ç ”ç©¶å‘ç° Constraint Compliance å‘ˆç°å‡ºæ™®éçš„ U-curve æ¨¡å¼ï¼Œå³åœ¨çº¦ 27 ä¸ªå•è¯çš„ä¸­ç­‰å‹ç¼©æ°´å¹³ä¸‹è¿åçº¦æŸçš„æƒ…å†µæœ€ä¸ºä¸¥é‡ï¼Œä¸”æåº¦å‹ç¼©ä¸‹çš„è¡¨ç°ä¼˜äºä¸­ç­‰é•¿åº¦ã€‚å®éªŒè¯æ˜ CC å’Œ SA åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¯æ­£äº¤çš„ï¼Œä¸”å‹ç¼©å¯¹çº¦æŸçš„å½±å“è¿œå¤§äºå¯¹è¯­ä¹‰çš„å½±å“ã€‚é€šè¿‡ RLHF Ablation éªŒè¯ï¼Œç ”ç©¶æ­ç¤ºäº† RLHF è®­ç»ƒä¸­çš„ Helpfulness ä¿¡å·æ˜¯å¯¼è‡´ä¸­ç­‰å‹ç¼©ä¸‹çº¦æŸè¿åçš„ä¸»è¦åŸå› ã€‚æ­¤å¤–ï¼Œæ¨ç†æ¨¡å‹æ¯”é«˜æ•ˆæ¨¡å‹è¡¨ç°å‡º 27.5% çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† RLHF å¯¹é½ä¸æŒ‡ä»¤éµå¾ªä¹‹é—´çš„æ ¹æœ¬å¼ åŠ›ï¼Œä¸ºä¼˜åŒ– LLMs åœ¨å‹ç¼©ç¯å¢ƒä¸‹çš„éƒ¨ç½²æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 9 figures; currently under peer review at TMLR",
      "pdf_url": "https://arxiv.org/pdf/2512.17920v1",
      "published_date": "2025-12-02 13:25:48 UTC",
      "updated_date": "2025-12-02 13:25:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:53.780260+00:00"
    },
    {
      "arxiv_id": "2512.02743v1",
      "title": "Reasoning-Aware Multimodal Fusion for Hateful Video Detection",
      "title_zh": "é’ˆå¯¹ä»‡æ¨è§†é¢‘æ£€æµ‹çš„æ¨ç†æ„ŸçŸ¥å¤šæ¨¡æ€èåˆ",
      "authors": [
        "Shuonan Yang",
        "Tailin Chen",
        "Jiangbei Yue",
        "Guangliang Cheng",
        "Jianbo Jiao",
        "Zeyu Fu"
      ],
      "abstract": "Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Reasoning-Aware Multimodal Fusion (RAMF)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åœ¨çº¿è§†é¢‘ä¸­æœ‰å®³è¨€è®ºæ£€æµ‹é¢ä¸´çš„å¤šæ¨¡æ€è¯­ä¹‰èåˆå›°éš¾åŠå¯¹å¾®å¦™ä»‡æ¨å†…å®¹ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†æ•æ‰å±€éƒ¨æ˜¾è‘—çº¿ç´¢å’Œå…¨å±€æ—¶åºç»“æ„ï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†Local-Global Context Fusion (LGCF)ï¼Œå¹¶åˆ©ç”¨Semantic Cross Attention (SCA) å®ç°ç»†ç²’åº¦çš„å¤šæ¨¡æ€è¯­ä¹‰äº¤äº’ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¯¹æŠ—æ€§æ¨ç†ï¼ˆadversarial reasoningï¼‰æœºåˆ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆvision-language modelï¼‰ç”Ÿæˆå®¢è§‚æè¿°ã€ä»‡æ¨å‡è®¾æ¨ç†å’Œéä»‡æ¨å‡è®¾æ¨ç†ï¼Œä»è€Œä¸ºç†è§£å¤æ‚çš„ä»‡æ¨æ„å›¾æä¾›å¤šç»´åº¦çš„è¯­ä¹‰è§†è§’ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„è§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRAMFåœ¨Macro-F1å’Œä»‡æ¨ç±»å¬å›ç‡ï¼ˆhate class recallï¼‰ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†3%å’Œ7%ã€‚è¯¥æ–¹æ³•ä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œè¿˜åœ¨ä»‡æ¨è§†é¢‘è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02743v1",
      "published_date": "2025-12-02 13:24:17 UTC",
      "updated_date": "2025-12-02 13:24:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:52:47.888606+00:00"
    },
    {
      "arxiv_id": "2512.02735v1",
      "title": "A Framework for Causal Concept-based Model Explanations",
      "title_zh": "åŸºäºå› æœæ¦‚å¿µçš„æ¨¡å‹è§£é‡Šæ¡†æ¶",
      "authors": [
        "Anna Rodum BjÃ¸ru",
        "Jacob LysnÃ¦s-Larsen",
        "Oskar JÃ¸rgensen",
        "Inga StrÃ¼mke",
        "Helge Langseth"
      ],
      "abstract": "This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå› æœæ¦‚å¿µçš„äº‹åå¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable Artificial Intelligence, XAI)æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®ä¿éè§£é‡Šæ€§æ¨¡å‹çš„è§£é‡Šæ—¢æ˜“äºç†è§£åˆå¯¹è¢«è§£é‡Šæ¨¡å‹å…·æœ‰å¿ å®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¡ç®—æ¦‚å¿µå¹²é¢„(concept interventions)çš„å……åˆ†æ€§æ¦‚ç‡æ¥ç”Ÿæˆå±€éƒ¨å’Œå…¨å±€è§£é‡Šï¼Œå¹¶åˆ©ç”¨åœ¨ CelebA æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨å±•ç¤ºäº†å…·ä½“çš„åº”ç”¨æ¡ˆä¾‹ã€‚ç ”ç©¶é€šè¿‡å…·å¤‡éšå¼å› æœè§£é‡Šçš„æ¦‚å¿µè¯æ±‡è¡¨å®ç°äº†è‰¯å¥½çš„æ˜“ç†è§£æ€§ï¼Œå¹¶ç‰¹åˆ«å¼ºè°ƒäº†è§£é‡Šç”Ÿæˆçš„èƒŒæ™¯ä¸è§£é‡Šç†è§£çš„èƒŒæ™¯å¿…é¡»ä¿æŒä¸€è‡´ï¼Œä»¥ä¿éšœè§£é‡Šçš„å¿ å®æ€§ã€‚è¯¥å·¥ä½œä¸ºå¤æ‚æ¨¡å‹çš„å› æœè§£é‡Šæä¾›äº†ä¸€ç§ç³»ç»ŸåŒ–çš„åˆ†ææ–¹æ³•ï¼Œæœ‰åŠ©äºæå‡é»‘ç›’æ¨¡å‹çš„é€æ˜åº¦ä¸å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02735v1",
      "published_date": "2025-12-02 13:19:53 UTC",
      "updated_date": "2025-12-02 13:19:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:53:06.088554+00:00"
    },
    {
      "arxiv_id": "2512.02731v1",
      "title": "Self-Improving AI Agents through Self-Play",
      "title_zh": "é€šè¿‡è‡ªæˆ‘åšå¼ˆå®ç°è‡ªæˆ‘è¿›åŒ–çš„ AI æ™ºèƒ½ä½“",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $Î½_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Î˜$, and we identify the coefficient of self-improvement $Îº$ as the Lie derivative of the capability functional along this flow.\n  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $Îº> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.\n  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†å¿ƒç†æµ‹é‡å·¥å…·çš„æ¨¡ç†è®ºæ¡†æ¶æ‰©å±•è‡³åŠ¨åŠ›ç³»ç»Ÿé¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡è‡ªæˆ‘åšå¼ˆ(Self-Play)å®ç°AIæ™ºèƒ½ä½“çš„è‡ªæˆ‘æå‡ã€‚è®ºæ–‡å°†æ™ºèƒ½ä½“å½¢å¼åŒ–ä¸ºå—è®¡ç®—èµ„æºå‚æ•°åŒ–çš„æµ(Flow)ï¼Œå¹¶ç”±é€’å½’çš„Generator-Verifier-Updater (GVU)ç®—å­é©±åŠ¨ã€‚ç ”ç©¶è¯æ˜äº†è¯¥ç®—å­åœ¨å‚æ•°æµå½¢ä¸Šç”Ÿæˆå‘é‡åœºï¼Œå¹¶å°†è‡ªæˆ‘æå‡ç³»æ•°å®šä¹‰ä¸ºèƒ½åŠ›æ³›å‡½æ²¿è¯¥æµçš„Lieå¯¼æ•°ã€‚æ ¸å¿ƒè´¡çŒ®æ˜¯æ¨å¯¼å‡ºäº†æ–¹å·®ä¸ç­‰å¼(Variance Inequality)ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ­£åˆ™æ¡ä»¶ä¸‹ä¿è¯è‡ªæˆ‘æå‡ç¨³å®šæ€§çš„è°±æ¡ä»¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå½“ç”Ÿæˆå’ŒéªŒè¯çš„ç»¼åˆå™ªå£°æ§åˆ¶åœ¨ä¸€å®šé˜ˆå€¼å†…æ—¶ï¼Œå³å¯ç¡®ä¿è‡ªæˆ‘æå‡ç³»æ•°å¤§äºé›¶ã€‚è¯¥ç†è®ºæ¡†æ¶æˆåŠŸç»Ÿä¸€äº†Language Self-Play (LSP)ã€è‡ªæˆ‘ä¿®æ­£(Self-Correction)å’Œåˆæˆæ•°æ®å¼•å¯¼(Synthetic Data bootstrapping)ç­‰ç°æœ‰æŠ€æœ¯è·¯å¾„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®STaRã€SPINã€Reflexionã€GANså’ŒAlphaZeroç­‰æ¶æ„å‡æ˜¯æ»¡è¶³æ–¹å·®ä¸ç­‰å¼çš„GVUç®—å­çš„å…·ä½“æ‹“æ‰‘å®ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02731v1",
      "published_date": "2025-12-02 13:13:57 UTC",
      "updated_date": "2025-12-02 13:13:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:53:08.382752+00:00"
    },
    {
      "arxiv_id": "2512.02727v1",
      "title": "DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions",
      "title_zh": "DF-Mambaï¼šäº¤äº’åœºæ™¯ä¸‹ 3D æ‰‹éƒ¨å§¿æ€ä¼°è®¡çš„å¯å˜å½¢çŠ¶æ€ç©ºé—´å»ºæ¨¡",
      "authors": [
        "Yifan Zhou",
        "Takehiko Ohkawa",
        "Guwenxiao Zhou",
        "Kanoko Goto",
        "Takumi Hirose",
        "Yusuke Sekikawa",
        "Nakamasa Inoue"
      ],
      "abstract": "Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Hand Pose Estimation (HPE) åœ¨äº¤äº’åœºæ™¯ä¸­é¢ä¸´çš„ä¸¥é‡é®æŒ¡å’Œå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º DF-Mamba (Deformable Mamba) çš„é«˜æ•ˆè§†è§‰ç‰¹å¾æå–æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº† Mamba çš„é€‰æ‹©æ€§çŠ¶æ€å»ºæ¨¡ (selective state modeling) ä¸æ–°é¢–çš„å˜å½¢çŠ¶æ€æ‰«æ (deformable state scanning) æœºåˆ¶ï¼Œèƒ½å¤Ÿæ•æ‰è¶…è¶Šä¼ ç»Ÿ CNN é™åˆ¶çš„å…¨å±€ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚é€šè¿‡åœ¨å›¾åƒå†…éƒ¨èšåˆå±€éƒ¨ç‰¹å¾å¹¶é€‰æ‹©æ€§åœ°ä¿ç•™å…³é”®çš„å…¨å±€ä¿¡æ¯ï¼ŒDF-Mamba æ˜¾è‘—å¢å¼ºäº†ç»“æ„åŒ– 3D HPE çš„å»ºæ¨¡ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ ResNet-50 ç›¸å½“çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨æ¶µç›–å•æ‰‹ã€åŒæ‰‹ã€æ‰‹ç‰©äº¤äº’ä»¥åŠ RGB å’Œæ·±åº¦å›¾çš„äº”ä¸ªä¸»æµæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒDF-Mamba çš„è¡¨ç°ä¼˜äº VMamba å’Œ Spatial-Mamba ç­‰æœ€æ–°æ¨¡å‹ï¼Œå¹¶å–å¾—äº† state-of-the-art çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2026. Project page: https://tkhkaeio.github.io/projects/25-dfmamba/index.html",
      "pdf_url": "https://arxiv.org/pdf/2512.02727v1",
      "published_date": "2025-12-02 13:01:04 UTC",
      "updated_date": "2025-12-02 13:01:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:53:28.491296+00:00"
    },
    {
      "arxiv_id": "2512.02726v1",
      "title": "AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping",
      "title_zh": "AuditCopilotï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°å¤å¼è®°è´¦èˆå¼Šæ£€æµ‹",
      "authors": [
        "Md Abdul Kadir",
        "Sai Suresh Macharla Vasu",
        "Sidharth S. Nair",
        "Daniel Sonntag"
      ],
      "abstract": "Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \\textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨å¤å¼è®°è´¦ (Double-Entry Bookkeeping) ä¸­è¿›è¡Œæ¬ºè¯ˆæ£€æµ‹çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ—¥è®°è´¦åˆ†å½•æµ‹è¯• (Journal Entry Tests, JETs) äº§ç”Ÿè¿‡å¤šè¯¯æŠ¥ä¸”éš¾ä»¥è¯†åˆ«ç»†å¾®å¼‚å¸¸çš„å±€é™æ€§ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®åŒ¿åè´¦æœ¬ä¸Šå¯¹ LLaMA å’Œ Gemma ç­‰å‰æ²¿æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å‘ç° LLMs åœ¨è¯†åˆ«æ•ˆç‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ JETs å’Œç»å…¸çš„æœºå™¨å­¦ä¹  (Machine Learning) åŸºå‡†æ¨¡å‹ã€‚é™¤äº†æ€§èƒ½ä¼˜åŠ¿å¤–ï¼ŒLLMs è¿˜èƒ½å¤Ÿæä¾›è¾…åŠ©å†³ç­–çš„è‡ªç„¶è¯­è¨€è§£é‡Š (Natural-language explanations)ï¼Œæå¤§åœ°æå‡äº†å®¡è®¡å·¥ä½œçš„å¯è§£é‡Šæ€§ (Interpretability)ã€‚è¯¥ç ”ç©¶æˆæœè¯æ˜äº†äººå·¥æ™ºèƒ½å¢å¼ºå®¡è®¡ (AI-augmented auditing) çš„å¹¿é˜”å‰æ™¯ï¼Œå¼ºè°ƒäº†å®¡è®¡å¸ˆä¸åŸºç¡€æ¨¡å‹ (Foundation Models) åä½œåœ¨ç»´æŠ¤è´¢åŠ¡å®Œæ•´æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02726v1",
      "published_date": "2025-12-02 13:00:57 UTC",
      "updated_date": "2025-12-02 13:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:53:54.780166+00:00"
    },
    {
      "arxiv_id": "2512.02720v1",
      "title": "StockMem: An Event-Reflection Memory Framework for Stock Forecasting",
      "title_zh": "StockMemï¼šé¢å‘è‚¡ç¥¨é¢„æµ‹çš„äº‹ä»¶åæ€è®°å¿†æ¡†æ¶",
      "authors": [
        "He Wang",
        "Wenyilin Xiao",
        "Songqiao Han",
        "Hailiang Huang"
      ],
      "abstract": "Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‚¡å¸‚é¢„æµ‹ä¸­å¸‚åœºæ³¢åŠ¨å‰§çƒˆåŠå¯¹å®æ—¶äº‹ä»¶é«˜åº¦æ•æ„Ÿçš„æŒ‘æˆ˜ï¼Œæå‡ºäº† StockMemï¼Œä¸€ç§äº‹ä»¶-åæ€åŒå±‚è®°å¿†æ¡†æ¶(event-reflection dual-layer memory framework)ã€‚å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é‡‘èæ–°é—»æ—¶å¸¸å—å™ªéŸ³æ•°æ®å¹²æ‰°ï¼Œä¸”ç°æœ‰é€šç”¨è®°å¿†æ¶æ„éš¾ä»¥è¯†åˆ«ä»·æ ¼å˜åŠ¨çš„æ ¸å¿ƒé©±åŠ¨å› ç´ ã€‚StockMem é€šè¿‡æ¨ªå‘æ•´åˆ(horizontal consolidation)æ¯æ—¥äº‹ä»¶å¹¶è¿›è¡Œçºµå‘è¿½è¸ª(longitudinal tracking)ï¼Œæ•è·äº‹ä»¶æ¼”åŒ–è¿‡ç¨‹ä¸­çš„å¢é‡ä¿¡æ¯ï¼Œä»¥åæ˜ å¸‚åœºé¢„æœŸå·®å¼‚å¹¶æ„å»ºæ—¶é—´äº‹ä»¶çŸ¥è¯†åº“ã€‚é€šè¿‡åˆ†æäº‹ä»¶ä¸ä»·æ ¼çš„åŠ¨æ€å…³ç³»ï¼Œè¯¥æ¡†æ¶è¿›ä¸€æ­¥å½¢æˆäº†åŒ…å«å› æœç»éªŒçš„åæ€çŸ¥è¯†åº“(reflection knowledge base)ã€‚åœ¨è¿›è¡Œé¢„æµ‹æ—¶ï¼Œç³»ç»Ÿä¼šæ£€ç´¢ç±»ä¼¼çš„å†å²åœºæ™¯ï¼Œå¹¶ç»“åˆå½“å‰äº‹ä»¶ã€å¢é‡æ•°æ®å’Œè¿‡å¾€ç»éªŒè¿›è¡Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStockMem çš„è¡¨ç°ä¼˜äºç°æœ‰çš„è®°å¿†æ¶æ„ï¼Œå¹¶èƒ½é€šè¿‡è¿½è¸ªå½±å“ä»·æ ¼çš„ä¿¡æ¯é“¾æä¾›å“è¶Šä¸”å…·å¯è§£é‡Šæ€§çš„æ¨ç†è¿‡ç¨‹ï¼Œå¢å¼ºäº†é‡‘èé¢„æµ‹å†³ç­–çš„é€æ˜åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02720v1",
      "published_date": "2025-12-02 12:53:02 UTC",
      "updated_date": "2025-12-02 12:53:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:53:53.080789+00:00"
    },
    {
      "arxiv_id": "2512.02719v1",
      "title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­æ¶Œç°çš„è´å¶æ–¯è¡Œä¸ºä¸æœ€ä¼˜çº¿ç´¢æ•´åˆ",
      "authors": [
        "Julian Ma",
        "Jun Wang",
        "Zafeirios Fountas"
      ],
      "abstract": "Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦èƒ½åƒäººç±»ä¸€æ ·åœ¨å¤„ç†å˜ˆæ‚ä¿¡å·æ—¶è¡¨ç°å‡ºæ¶Œç°çš„è´å¶æ–¯è¡Œä¸º(Emergent Bayesian Behaviour)å’Œå¤šæ¨¡æ€æ•´åˆèƒ½åŠ›ã€‚ç ”ç©¶è€…å€Ÿé‰´å¿ƒç†ç‰©ç†å­¦(Psychophysics)èŒƒå¼ï¼Œæå‡ºäº†åŒ…å«é•¿åº¦ã€ä½ç½®ã€è·ç¦»å’ŒæŒç»­æ—¶é—´å››é¡¹ä¼°è®¡ä»»åŠ¡çš„è¡Œä¸ºåŸºå‡†BayesBenchï¼Œå¹¶å¼•å…¥äº†è´å¶æ–¯ä¸€è‡´æ€§å¾—åˆ†(Bayesian Consistency Score)æ¥è¡¡é‡æ¨¡å‹å¤„ç†ä¸ç¡®å®šæ€§çš„ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ€§èƒ½å¼ºå¤§çš„æ¨¡å‹é€šå¸¸è¡¨ç°å‡ºç¬¦åˆè´å¶æ–¯ä¸€è‡´æ€§çš„è¡Œä¸ºå€¾å‘ï¼Œä½†é«˜å‡†ç¡®ç‡å¹¶ä¸ä¸€å®šä¿è¯é²æ£’çš„ä¸ç¡®å®šæ€§å¤„ç†èƒ½åŠ›ã€‚ä¾‹å¦‚GPT-5 Miniåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å®Œç¾ï¼Œå´åœ¨è§†è§‰çº¿ç´¢æ•´åˆ(Cue Combination)ä¸Šæ•ˆç‡ä¸è¶³ï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›ä¸å†…åœ¨è®¡ç®—ç­–ç•¥ä¹‹é—´çš„å…³é”®åˆ†ç¦»ã€‚æ­¤é¡¹ç ”ç©¶ä¸ä»…æ­ç¤ºäº†LLMsåœ¨å¤„ç†ä¸ç¡®å®šæ€§æ—¶çš„åŸåˆ™æ€§è¡¨ç°ï¼Œä¹Ÿä¸ºè¯„ä¼°å’Œä¼˜åŒ–æœªæ¥å¤šæ¨¡æ€æ¶æ„æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02719v1",
      "published_date": "2025-12-02 12:51:30 UTC",
      "updated_date": "2025-12-02 12:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:53:49.870848+00:00"
    },
    {
      "arxiv_id": "2512.02716v3",
      "title": "Menta: A Small Language Model for On-Device Mental Health Prediction",
      "title_zh": "Mentaï¼šé¢å‘ç«¯ä¾§å¿ƒç†å¥åº·é¢„æµ‹çš„å°è¯­è¨€æ¨¡å‹",
      "authors": [
        "Tianyi Zhang",
        "Xiangyuan Xue",
        "Lingyan Ruan",
        "Shiya Fu",
        "Feng Xia",
        "Simon D'Alfonso",
        "Vassilis Kostakos",
        "Ting Dang",
        "Hong Jia"
      ],
      "abstract": "Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://hong-labs.github.io/menta-project/",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Mentaï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹ç¤¾äº¤åª’ä½“æ•°æ®è¿›è¡Œå¤šä»»åŠ¡ç²¾ç¥å¥åº·é¢„æµ‹ä¼˜åŒ–çš„è½»é‡çº§è¯­è¨€æ¨¡å‹(SLM)ã€‚ç ”ç©¶äººå‘˜é€šè¿‡LoRAæ¡†æ¶ã€è·¨æ•°æ®é›†ç­–ç•¥ä»¥åŠå¹³è¡¡å‡†ç¡®ç‡å¯¼å‘çš„æŸå¤±å‡½æ•°ï¼Œåœ¨å…­é¡¹åˆ†ç±»ä»»åŠ¡ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†è”åˆå¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMentaåœ¨æŠ‘éƒã€å‹åŠ›å’Œè‡ªæ€å€¾å‘ç­‰ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°æ¯”éå¾®è°ƒçš„SLMåŸºçº¿æ¨¡å‹æé«˜äº†15.2%ï¼Œä¸”åœ¨æŠ‘éƒå’Œå‹åŠ›åˆ†ç±»ä¸­çš„è¡¨ç°ä¼˜äºå‚æ•°é‡å¤§å…¶3.25å€çš„13B LLMsã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æˆåŠŸåœ¨iPhone 15 Pro Maxä¸Šå®ç°äº†å®æ—¶ç«¯ä¾§éƒ¨ç½²ï¼Œè¿è¡Œä»…éœ€çº¦3GBå†…å­˜ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†Mentaåœ¨å®ç°å¯æ‰©å±•ä¸”éšç§ä¿æŠ¤çš„ç²¾ç¥å¥åº·ç›‘æµ‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02716v3",
      "published_date": "2025-12-02 12:47:08 UTC",
      "updated_date": "2025-12-15 23:09:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:54:06.277369+00:00"
    },
    {
      "arxiv_id": "2512.02713v1",
      "title": "Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs",
      "title_zh": "åŸºäºæœ¬ä½“å¯¹é½çŸ¥è¯†å›¾è°±çš„å›¾åƒç”Ÿæˆè®­ç»ƒæ•°æ®å½’å› ",
      "authors": [
        "Theodoros Aivalis",
        "Iraklis A. Klampanos",
        "Antonis Troumpoukis",
        "Joemon M. Jose"
      ],
      "abstract": "As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ¨¡å‹åœ¨é€æ˜åº¦ã€é—®è´£åˆ¶å’Œç‰ˆæƒåˆè§„æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœ¬ä½“å¯¹é½çŸ¥è¯†å›¾è°± (Ontology-Aligned Knowledge Graphs) çš„è®­ç»ƒæ•°æ®å½’å› æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä»å›¾åƒä¸­æå–ç»“æ„åŒ–è¡¨ç¤ºçš„éš¾é¢˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal LLMs) æå–ä¸ç‰¹å®šé¢†åŸŸæœ¬ä½“ä¸€è‡´çš„ç»“æ„åŒ–ä¸‰å…ƒç»„ã€‚é€šè¿‡å¯¹æ¯”ç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒçš„çŸ¥è¯†å›¾è°± (KGs)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¿½è¸ªæ½œåœ¨çš„å½±å“æ¥æºï¼Œæ”¯æŒç‰ˆæƒåˆ†æã€æ•°æ®é›†é€æ˜åº¦å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ (Interpretable AI)ã€‚å®éªŒåœ¨åŸºäºå–æ¶ˆå­¦ä¹  (Unlearning) çš„æœ¬åœ°æ¨¡å‹å’Œé£æ ¼ç‰¹å®šçš„å¤§è§„æ¨¡æ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æ¡†æ¶ä¸ºæ„å»ºæ”¯æŒäººç±»åä½œä¸åˆ›æ„çš„é€æ˜ AI ç³»ç»Ÿæä¾›äº†å…³é”®çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02713v1",
      "published_date": "2025-12-02 12:45:20 UTC",
      "updated_date": "2025-12-02 12:45:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:54:05.580631+00:00"
    },
    {
      "arxiv_id": "2512.02707v1",
      "title": "Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base",
      "title_zh": "ä¸­å°ä¼ä¸šä»£ç åº“è¿ç§»å‰å¯¹è½¯ä»¶äº§å“çº¿å·¥ç¨‹è®¤çŸ¥çš„å®è¯è¯„ä¼°",
      "authors": [
        "Thomas Georges",
        "Marianne Huchard",
        "MÃ©lanie KÃ¶nig",
        "ClÃ©mentine Nebut",
        "Chouki Tibermacine"
      ],
      "abstract": "Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­å°å‹ä¼ä¸š (SME) å°†ç°æœ‰ä»£ç åº“è¿ç§»è‡³è½¯ä»¶äº§å“çº¿å·¥ç¨‹ (Software Product Line Engineering, SPLE) çš„å¤æ‚è¿‡ç¨‹ï¼Œå¼€å±•äº†å…³äºåˆ©ç›Šç›¸å…³è€…æ„ŸçŸ¥çš„å®è¯è¯„ä¼°ã€‚é€šè¿‡ä¸ä¸€å®¶å¤„äºè¿ç§»åˆæœŸçš„ SME åˆä½œï¼Œç ”ç©¶å›¢é˜Ÿæ·±å…¥è°ƒç ”äº†å½“å‰çš„å¼€å‘æµç¨‹ã€é¢„æœŸæ”¶ç›Šä»¥åŠæ½œåœ¨çš„å˜é©é˜»åŠ›ã€‚å®šæ€§åˆ†æç»“æœæ˜¾ç¤ºï¼Œä¸åŒè§’è‰²çš„å¼€å‘äººå‘˜å‡èƒ½è¯†åˆ«å‡º SPLE è¿ç§»å¯¹å…¶å…·ä½“ä¸šåŠ¡æ´»åŠ¨å¸¦æ¥çš„ç§¯æå½±å“ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œé€šè¿‡ä¿æŒåˆ©ç›Šç›¸å…³è€…çš„ä¿¡æ¯åŒæ­¥ä¸æ·±åº¦å‚ä¸ï¼Œå¹¶å°½å¯èƒ½ä¿ç•™åŸæœ‰çš„è‰¯å¥½å¼€å‘å®è·µï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£è¿ç§»è¿‡ç¨‹ä¸­çš„é£é™©ã€‚è¯¥é¡¹å·¥ä½œä¸ºä¼ä¸šåœ¨æŠ€æœ¯è½¬å‹ä¸­å‡å°‘å‘˜å·¥æŠµè§¦ã€ç¡®ä¿å¹³ç¨³è¿‡æ¸¡æä¾›äº†å…³é”®çš„ç­–ç•¥æŒ‡å¯¼å’Œå®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "34 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.02707v1",
      "published_date": "2025-12-02 12:39:05 UTC",
      "updated_date": "2025-12-02 12:39:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:54:03.387472+00:00"
    },
    {
      "arxiv_id": "2512.02699v1",
      "title": "Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding",
      "title_zh": "å­¦ä¹ ä¼˜å…ˆå…³æ³¨ç›®æ ‡ï¼šé¢å‘å¯é å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„æ¨¡æ€é‡è¦æ€§å¼•å¯¼æ¨ç†",
      "authors": [
        "Hyeongseop Rha",
        "Jeong Hun Yeo",
        "Junil Won",
        "Se Jin Park",
        "Yong Man Ro"
      ],
      "abstract": "In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Modality-Importance-Guided Reasoning (MIGR) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿç†è§£ä»»åŠ¡ä¸­çš„æ¨ç†å¯é æ€§ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å­˜åœ¨çš„ reasoning drift é—®é¢˜ï¼Œå³æ¨¡å‹å€¾å‘äºä¾èµ–è‡ªèº«ç”Ÿæˆæ–‡æœ¬è€Œéå¤šæ¨¡æ€è¯æ®ï¼Œä»¥åŠæ¨ç†è·¯å¾„å—è§†è§‰å¼•å¯¼åå·®å½±å“çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº† Modality Importance (MI) æœºåˆ¶ä»¥è¯†åˆ«æƒ…æ„Ÿä¸»å¯¼æ¨¡æ€ã€‚é€šè¿‡ MIï¼ŒMIGR é‡æ–°ç»„ç»‡æ¨ç†åºåˆ—ï¼Œç¡®ä¿è§£é‡Šä»æœ€å…³é”®çš„æ¨¡æ€å¼€å§‹ï¼Œé¿å…æ—©æœŸæ¨ç†è¢«ä½ä¿¡æ¯é‡çº¿ç´¢è¯¯å¯¼ã€‚è¯¥æ¡†æ¶åŒ…å«æ¨¡æ€å¯¹é½çš„æœ‰ç›‘ç£å¾®è°ƒ (supervised fine-tuning) å’Œæ¨¡æ€æ„ŸçŸ¥çš„å¥–åŠ±ä¼˜åŒ– (reward optimization) ä¸¤ä¸ªé˜¶æ®µï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå…·æœ‰æƒ…æ„Ÿä¾æ®ä¸”é€»è¾‘è¿è´¯çš„è§£é‡Šã€‚åœ¨ DFEW åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒMIGR æ˜¾è‘—æé«˜äº†æ¨ç†å¯é æ€§ï¼Œå°†é¢„æµ‹æ­£ç¡®ä½†è§£é‡Šä¸ä¸€è‡´çš„æƒ…å†µä» 18.10% é™ä½è‡³ 7.37%ï¼Œè¯æ˜äº†ä»æƒ…æ„Ÿä¸»å¯¼æ¨¡æ€å¯åŠ¨æ¨ç†çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02699v1",
      "published_date": "2025-12-02 12:29:41 UTC",
      "updated_date": "2025-12-02 12:29:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T12:55:22.761785+00:00"
    },
    {
      "arxiv_id": "2512.02689v1",
      "title": "An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation",
      "title_zh": "ç¤¾ä¼šåè§ç¼“è§£çš„æ¨¡å‹åˆå¹¶ç®—æ³•å®è¯ç»¼è¿°",
      "authors": [
        "Daiki Shirafuji",
        "Tatsuhiko Saito",
        "Yasutomo Kimura"
      ],
      "abstract": "Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å­˜åœ¨çš„ç¤¾ä¼šåè§é—®é¢˜ï¼Œå¯¹åˆ©ç”¨æ¨¡å‹åˆå¹¶(Model Merging)ç®—æ³•è¿›è¡Œå‚æ•°â€œç¼–è¾‘â€ä»¥å‡è½»åè§çš„æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯è°ƒç ”ã€‚ä½œè€…å¯¹æ¯”äº† Linearã€Karcher Meanã€SLERPã€NuSLERPã€TIESã€DELLA å’Œ Nearswap ä¸ƒç§ç®—æ³•ï¼Œå¹¶åœ¨ GPTã€LLaMA å’Œ Qwen ç³»åˆ—çš„ 13 ä¸ªå¼€æºæƒé‡æ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡å®éªŒã€‚ç ”ç©¶é€šè¿‡ BBQã€BOLD å’Œ HONEST æ•°æ®é›†è¯„ä¼°åè§ç¼“è§£ç¨‹åº¦ï¼Œå¹¶åˆ©ç”¨ SuperGLUE åŸºå‡†è¡¡é‡å…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œæ­ç¤ºäº†åè§å‡å°‘ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´å­˜åœ¨çš„æƒè¡¡å…³ç³»ã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶æ˜¾è‘—çš„å»åæ“ä½œå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨é˜…è¯»ç†è§£å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ä¸‹é™ï¼Œä½† Linearã€SLERP å’Œ Nearswap ç®—æ³•åœ¨å‡å°‘åè§çš„åŒæ—¶èƒ½æœ‰æ•ˆç»´æŒæ•´ä½“æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯é‡‡ç”¨ä¸­ç­‰æ’å€¼æƒé‡çš„ SLERP è¢«è¯æ˜æ˜¯ç›®å‰æœ€å‡è¡¡çš„é€‰æ‹©ï¼Œä¸ºå®ç°æ›´å…¬å¹³ä¸”é«˜æ•ˆçš„æ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†é€‰æ‹©åˆé€‚åˆå¹¶æ–¹æ³•çš„é‡è¦æ€§ï¼Œä»¥é¿å…è¿‡åº¦å»åå¯¹åŸºç¡€è¯­è¨€èƒ½åŠ›é€ æˆè´Ÿé¢å½±å“ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in PACLIC 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.02689v1",
      "published_date": "2025-12-02 12:18:48 UTC",
      "updated_date": "2025-12-02 12:18:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:54:14.680949+00:00"
    },
    {
      "arxiv_id": "2512.02682v1",
      "title": "Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions",
      "title_zh": "è¶…è¶Šå•æ™ºèƒ½ä½“å®‰å…¨ï¼šLLM é—´äº¤äº’é£é™©åˆ†ç±»ä½“ç³»",
      "authors": [
        "Piercosma Bisconti",
        "Marcello Galisai",
        "Federico Pierucci",
        "Marcantonio Bracale",
        "Matteo Prandi"
      ],
      "abstract": "This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹äººæœºäº¤äº’è®¾è®¡çš„å®‰å…¨æœºåˆ¶åœ¨å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’ç¯å¢ƒä¸‹å¤±æ•ˆçš„åŸå› ï¼ŒæŒ‡å‡ºå½“å‰çš„å•æ™ºèƒ½ä½“ï¼ˆsingle-agentï¼‰å®‰å…¨æ²»ç†æ— æ³•åº”å¯¹å¤šæ¨¡å‹ç”Ÿæ€ç³»ç»Ÿä¸­è¾“å‡ºé€’å½’é‡ç”¨å¸¦æ¥çš„åŠ¨æ€é£é™©ã€‚ç ”ç©¶å¼ºè°ƒå³ä½¿æ¯ä¸ªæ¨¡å‹éƒ½å®ç°äº†å•ä½“å¯¹é½ï¼ˆindividually alignedï¼‰ï¼Œå±€éƒ¨çš„åˆè§„æ€§ä»å¯èƒ½åœ¨ç³»ç»Ÿå±‚é¢èšåˆæˆé›†ä½“å¤±æ•ˆã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä»æ¨¡å‹çº§å®‰å…¨ï¼ˆmodel-level safetyï¼‰å‘ç³»ç»Ÿçº§å®‰å…¨ï¼ˆsystem-level safetyï¼‰è½¬åŒ–çš„å¿…è¦æ€§ï¼Œå¹¶å¼•å…¥äº†æµ®ç°å¼ç³»ç»Ÿé£é™©è§†é‡ï¼ˆEmergent Systemic Risk Horizon, ESRHï¼‰æ¡†æ¶æ¥å½¢å¼åŒ–äº¤äº’ç»“æ„å¼•å‘çš„ä¸ç¨³å®šæ€§ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ„å»ºäº†å¤šæ™ºèƒ½ä½“äº¤äº’çš„é›†ä½“é£é™©ç†è®ºï¼Œå»ºç«‹äº†è¿æ¥å¾®è§‚ã€ä¸­è§‚ä¸å®è§‚å¤±æ•ˆæ¨¡å¼çš„é£é™©åˆ†ç±»æ³•ï¼ˆtaxonomyï¼‰ï¼Œå¹¶æå‡ºäº†åä¸ºInstitutionalAIçš„æ¶æ„æ–¹æ¡ˆï¼Œæ—¨åœ¨ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›åµŒå…¥å¼çš„è‡ªé€‚åº”ç›‘ç®¡ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02682v1",
      "published_date": "2025-12-02 12:06:57 UTC",
      "updated_date": "2025-12-02 12:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:54:28.685166+00:00"
    },
    {
      "arxiv_id": "2512.02677v1",
      "title": "Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks",
      "title_zh": "æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹è§£å†³é€’å½’é€»è¾‘ä»»åŠ¡çš„æ·±åº¦æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Zhiyuan He"
      ],
      "abstract": "Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨å¤„ç†é€’å½’é€»è¾‘ä»»åŠ¡æ—¶çš„æ·±åº¦æ³›åŒ–(depth generalization)é™åˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶Transformeræ¶æ„èƒ½å¤„ç†é•¿åºåˆ—ï¼Œä½†åœ¨é¢å¯¹æ¯”è®­ç»ƒæ·±åº¦æ›´æ·±çš„åµŒå¥—å±‚æ¬¡ç»“æ„æ—¶è¡¨ç°æ¬ ä½³ï¼Œè¿™æºäºå…¶éš¾ä»¥ç»´æŒè§£æåµŒå¥—ä¾èµ–æ‰€éœ€çš„æ ˆå¼è¡Œä¸º(stack-like behavior)ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹çš„å¾ªç¯å®šä½ä¸æ›¿æ¢æµæ°´çº¿(looped locate-and-replace pipeline)ï¼Œå°†é€’å½’é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­ç»„ä»¶è¿›è¡Œå¤„ç†ã€‚è¯¥æ¶æ„ç”±è´Ÿè´£è¯†åˆ«å­è¡¨è¾¾å¼çš„å®šä½å™¨(locator)å’Œè´Ÿè´£æ±‚å€¼çš„æ›¿æ¢å™¨(replacer)ç»„æˆï¼Œå®ç°äº†å¯¹å¤æ‚ç»“æ„çš„é€æ­¥è§£æã€‚é€šè¿‡åœ¨å¸ƒå°”ä»£æ•°(Boolean algebra)ã€é€’å½’ç®—æœ¯(recursive arithmetic)å’Œå‘½é¢˜é€»è¾‘(propositional logic)é¢†åŸŸçš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ¨¡å‹åœ¨å¤„ç†è¶…åˆ†å¸ƒé€’å½’æ·±åº¦æ—¶çš„æ€§èƒ½è¡°å‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02677v1",
      "published_date": "2025-12-02 12:04:51 UTC",
      "updated_date": "2025-12-02 12:04:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:54:22.122653+00:00"
    },
    {
      "arxiv_id": "2512.02669v1",
      "title": "SAND Challenge: Four Approaches for Dysartria Severity Classification",
      "title_zh": "SAND Challengeï¼šæ„éŸ³éšœç¢ä¸¥é‡ç¨‹åº¦åˆ†çº§çš„å››ç§æ–¹æ³•",
      "authors": [
        "Gauri Deshpande",
        "Harish Battula",
        "Ashish Panda",
        "Sunil Kumar Kopparapu"
      ],
      "abstract": "This paper presents a unified study of four distinct modeling approaches for classifying dysarthria severity in the Speech Analysis for Neurodegenerative Diseases (SAND) challenge. All models tackle the same five class classification task using a common dataset of speech recordings. We investigate: (1) a ViT-OF method leveraging a Vision Transformer on spectrogram images, (2) a 1D-CNN approach using eight 1-D CNN's with majority-vote fusion, (3) a BiLSTM-OF approach using nine BiLSTM models with majority vote fusion, and (4) a Hierarchical XGBoost ensemble that combines glottal and formant features through a two stage learning framework. Each method is described, and their performances on a validation set of 53 speakers are compared. Results show that while the feature-engineered XGBoost ensemble achieves the highest macro-F1 (0.86), the deep learning models (ViT, CNN, BiLSTM) attain competitive F1-scores (0.70) and offer complementary insights into the problem.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»é€€è¡Œæ€§ç–¾ç—…è¨€è¯­åˆ†æ(SAND)æŒ‘æˆ˜èµ›ï¼Œæå‡ºäº†å››ç§ä¸åŒçš„å»ºæ¨¡æ–¹æ³•ç”¨äºæ„éŸ³éšœç¢(Dysarthria)ä¸¥é‡ç¨‹åº¦çš„äº”ç±»åˆ†ç±»ä»»åŠ¡ã€‚ç ”ç©¶å…·ä½“æ¢è®¨äº†åŸºäºé¢‘è°±å›¾å›¾åƒçš„Vision Transformer(ViT-OF)æ–¹æ³•ã€é‡‡ç”¨å¤šæ•°æŠ•ç¥¨èåˆçš„1D-CNNå’ŒBiLSTM-OFæ–¹æ³•ï¼Œä»¥åŠç»“åˆå£°é—¨(Glottal)å’Œå…±æŒ¯å³°(Formant)ç‰¹å¾çš„Hierarchical XGBoosté›†æˆæ¡†æ¶ã€‚é€šè¿‡åœ¨53åè¯´è¯è€…çš„éªŒè¯é›†ä¸Šè¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼Œç»“æœè¡¨æ˜åŸºäºç‰¹å¾å·¥ç¨‹çš„XGBoosté›†æˆæ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œå…¶macro-F1å€¼è¾¾åˆ°äº†0.86ã€‚ä¸æ­¤åŒæ—¶ï¼ŒViTã€CNNå’ŒBiLSTMç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä¹Ÿå–å¾—äº†çº¦0.70çš„F1åˆ†æ•°ï¼Œå¹¶ä»ä¸åŒç»´åº¦ä¸ºè¯¥åˆ†ç±»é—®é¢˜æä¾›äº†äº’è¡¥çš„è§è§£ã€‚è¿™é¡¹å·¥ä½œå¯¹æ¯”äº†å¤šç§æ¶æ„åœ¨è¯­éŸ³ç—…ç†åˆ†æä¸­çš„æ•ˆèƒ½ï¼Œä¸ºæ„éŸ³éšœç¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æä¾›äº†é‡è¦çš„å®è¯å‚è€ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "7 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02669v1",
      "published_date": "2025-12-02 11:51:38 UTC",
      "updated_date": "2025-12-02 11:51:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:55:37.975614+00:00"
    },
    {
      "arxiv_id": "2512.02667v1",
      "title": "Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents",
      "title_zh": "Graph VQ-Transformer (GVT)ï¼šåŸºäºé«˜ä¿çœŸç¦»æ•£æ½œå˜é‡çš„é«˜æ•ˆç²¾å‡†åˆ†å­ç”Ÿæˆ",
      "authors": [
        "Haozhuo Zheng",
        "Cheng Wang",
        "Yang Liu"
      ],
      "abstract": "The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ä»¥åŠè‡ªå›å½’æ¨¡å‹å­˜åœ¨è¯¯å·®ç´¯ç§¯çš„é—®é¢˜ï¼Œæå‡ºäº†Graph VQ-Transformer (GVT)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°é«˜ç²¾åº¦å’Œé«˜æ•ˆç‡çš„ä¸¤é˜¶æ®µåˆ†å­ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåˆ›æ–°çš„Graph Vector Quantized Variational Autoencoder (VQ-VAE)ï¼Œå®ƒé€šè¿‡ç»“åˆGraph Transformerã€è§„èŒƒçš„Reverse Cuthill-McKee (RCM)èŠ‚ç‚¹æ’åºä»¥åŠRotary Positional Embeddings (RoPE)ï¼Œå°†åˆ†å­å›¾å‹ç¼©ä¸ºé«˜ä¿çœŸåº¦çš„ç¦»æ•£æ½œåœ¨åºåˆ—å¹¶å®ç°äº†è¿‘ä¹å®Œç¾çš„é‡å»ºç‡ã€‚éšåï¼Œç ”ç©¶è€…åœ¨è¿™äº›ç¦»æ•£æ½œåœ¨å˜é‡ä¸Šè®­ç»ƒè‡ªå›å½’Transformerï¼Œå°†å¤æ‚çš„å›¾ç”Ÿæˆä»»åŠ¡æœ‰æ•ˆè½¬åŒ–ä¸ºç»“æ„åŒ–çš„åºåˆ—å»ºæ¨¡é—®é¢˜ï¼Œä»è€Œå°†åˆ†å­è®¾è®¡ä¸å¤§è§„æ¨¡åºåˆ—å»ºæ¨¡èŒƒå¼ï¼ˆå¦‚LLMsï¼‰ç›¸æŒ‚é’©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGVTåœ¨ZINC250kã€MOSESå’ŒGuacaMolç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†SOTAæˆ–æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸”åœ¨FCDå’ŒKL Divergenceç­‰å…³é”®åˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„æ‰©æ•£æ¨¡å‹ã€‚å‡­å€Ÿå…¶å“è¶Šçš„æ€§èƒ½å’Œæ¶æ„åˆ›æ–°ï¼ŒGVTä¸ºç¦»æ•£æ½œåœ¨ç©ºé—´ä¸‹çš„åˆ†å­ç”Ÿæˆå»ºç«‹äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œä¸ºæœªæ¥çš„è¯ç‰©ç ”å‘å’Œåˆ†å­è®¾è®¡å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02667v1",
      "published_date": "2025-12-02 11:44:15 UTC",
      "updated_date": "2025-12-02 11:44:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:55:51.993165+00:00"
    },
    {
      "arxiv_id": "2512.02657v1",
      "title": "Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models",
      "title_zh": "è’¸é¦ã€é—å¿˜ã€é‡å¤ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒç»­é—å¿˜æ¡†æ¶",
      "authors": [
        "Naveen George",
        "Naoki Murata",
        "Yuhta Takida",
        "Konda Reddy Mopuri",
        "Yuki Mitsufuji"
      ],
      "abstract": "The recent rapid growth of visual generative models trained on vast web-scale datasets has created significant tension with data privacy regulations and copyright laws, such as GDPR's ``Right to be Forgotten.'' This necessitates machine unlearning (MU) to remove specific concepts without the prohibitive cost of retraining. However, existing MU techniques are fundamentally ill-equipped for real-world scenarios where deletion requests arrive sequentially, a setting known as continual unlearning (CUL). Naively applying one-shot methods in a continual setting triggers a stability crisis, leading to a cascade of degradation characterized by retention collapse, compounding collateral damage to related concepts, and a sharp decline in generative quality. To address this critical challenge, we introduce a novel generative distillation based continual unlearning framework that ensures targeted and stable unlearning under sequences of deletion requests. By reframing each unlearning step as a multi-objective, teacher-student distillation process, the framework leverages principles from continual learning to maintain model integrity. Experiments on a 10-step sequential benchmark demonstrate that our method unlearns forget concepts with better fidelity and achieves this without significant interference to the performance on retain concepts or the overall image quality, substantially outperforming baselines. This framework provides a viable pathway for the responsible deployment and maintenance of large-scale generative models, enabling industries to comply with ongoing data removal requests in a practical and effective manner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è§†è§‰ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„æ•°æ®éšç§å’Œç‰ˆæƒæ³•å¾‹æŒ‘æˆ˜ï¼Œæ¢è®¨äº†åœ¨ Text-to-Image Diffusion Models ä¸­é€šè¿‡ Machine Unlearning (MU) ç§»é™¤ç‰¹å®šæ¦‚å¿µçš„å¿…è¦æ€§ã€‚ä¼ ç»Ÿçš„ MU æŠ€æœ¯åœ¨å¤„ç†åºåˆ—åŒ–åˆ é™¤è¯·æ±‚çš„ Continual Unlearning (CUL) åœºæ™¯æ—¶ï¼Œå®¹æ˜“å¯¼è‡´æ¨¡å‹æ€§èƒ½å´©æºƒã€è´Ÿé¢å½±å“ç´¯ç§¯ä»¥åŠç”Ÿæˆè´¨é‡å¤§å¹…ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäº generative distillation çš„æ–°å‹æŒç»­é—å¿˜æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®ä¿åºåˆ—åŒ–åˆ é™¤è¯·æ±‚ä¸‹çš„ç›®æ ‡æ€§å’Œç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶å°†æ¯ä¸ªé—å¿˜æ­¥éª¤é‡æ–°å®šä¹‰ä¸ºå¤šç›®æ ‡çš„ teacher-student distillation è¿‡ç¨‹ï¼Œå¹¶å€Ÿé‰´ Continual Learning åŸç†æ¥æœ‰æ•ˆç»´æŠ¤æ¨¡å‹çš„å®Œæ•´æ€§ã€‚åœ¨ 10 æ­¥åºåˆ—åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é—å¿˜ç‰¹å®šæ¦‚å¿µæ—¶å…·æœ‰æé«˜çš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å¹²æ‰°ä¿ç•™æ¦‚å¿µæ€§èƒ½æˆ–æ•´ä½“å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ baseline æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå¤§è§„æ¨¡ç”Ÿæˆå¼æ¨¡å‹åœ¨éµå¾ªæ•°æ®åˆ é™¤æ³•è§„ä¸‹çš„è´Ÿè´£ä»»éƒ¨ç½²ä¸ç»´æŠ¤æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.02657v1",
      "published_date": "2025-12-02 11:22:32 UTC",
      "updated_date": "2025-12-02 11:22:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:10.271809+00:00"
    },
    {
      "arxiv_id": "2512.02652v1",
      "title": "Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training",
      "title_zh": "Pianist Transformerï¼šé€šè¿‡å¯æ‰©å±•è‡ªç›‘ç£é¢„è®­ç»ƒå®ç°è¡¨ç°åŠ›é’¢ç´æ¼”å¥æ¸²æŸ“",
      "authors": [
        "Hong-Jie You",
        "Jie-Jing Shao",
        "Xiao-Wen Yang",
        "Lin-Han Jia",
        "Lan-Zhe Guo",
        "Yu-Feng Li"
      ],
      "abstract": "Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in the music domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Pianist Transformerï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¡¨ç°åŠ›é’¢ç´æ¼”å¥æ¸²æŸ“(expressive piano performance rendering)æ–¹æ³•å› è¿‡åº¦ä¾èµ–å°è§„æ¨¡æ ‡æ³¨æ•°æ®é›†è€Œéš¾ä»¥æ‰©å±•æ¨¡å‹å’Œæ•°æ®é‡çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„MIDIæ•°æ®è¡¨ç¤ºæ³•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ— éœ€æ˜¾å¼æ ‡æ³¨çš„æƒ…å†µä¸‹å­¦ä¹ éŸ³ä¹ç»“æ„ä¸è¡¨è¾¾çš„å…±äº«åŸç†ã€‚é€šè¿‡é‡‡ç”¨é«˜æ•ˆçš„éå¯¹ç§°æ¶æ„(asymmetric architecture)ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒæ¸²æŸ“è´¨é‡çš„åŒæ—¶æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å¹¶æå‡äº†æ¨ç†é€Ÿåº¦ã€‚è¯¥é¡¹ç›®åˆ©ç”¨100äº¿(10B)ä¸ªtokenå¯¹æ‹¥æœ‰1.35äº¿(135M)å‚æ•°çš„æ¨¡å‹è¿›è¡Œäº†è‡ªç›‘ç£é¢„è®­ç»ƒ(self-supervised pre-training)ï¼Œå……åˆ†å‘æŒ¥äº†åœ¨å¤§è§„æ¨¡æ•°æ®ä¸‹æ¨¡å‹æ‰©å±•çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPianist Transformeråœ¨å®¢è§‚è¯„ä»·æŒ‡æ ‡å’Œäººç±»ä¸»è§‚è¯„åˆ†æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ°´å¹³ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œä¸ºåœ¨éŸ³ä¹é¢†åŸŸå®ç°ç±»äººåŒ–çš„æ¼”å¥åˆæˆ(human-like performance synthesis)å»ºç«‹äº†ä¸€æ¡å¯æ‰©å±•çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02652v1",
      "published_date": "2025-12-02 11:13:29 UTC",
      "updated_date": "2025-12-02 11:13:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:55:45.480957+00:00"
    },
    {
      "arxiv_id": "2512.02633v1",
      "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations",
      "title_zh": "åŸºäºç»“æ„åŒ– LTL è¡¨ç¤ºçš„å¼ºåŒ–å­¦ä¹ é›¶æ ·æœ¬æŒ‡ä»¤éµå¾ª",
      "authors": [
        "Mattia Giuri",
        "Mathias Jackermeier",
        "Alessandro Abate"
      ],
      "abstract": "Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­åˆ©ç”¨çº¿æ€§æ—¶åºé€»è¾‘(Linear Temporal Logic, LTL)æŒ‡å®šå¤æ‚ç»“æ„åŒ–ä»»åŠ¡çš„åœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§é›¶æ ·æœ¬(Zero-Shot)æŒ‡ä»¤éµå¾ªçš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤šä¸ªåŸå­å‘½é¢˜åŒæ—¶å‘ç”Ÿä¸”å­˜åœ¨å¤æ‚ç›¸äº’ä½œç”¨çš„ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³çš„å±€é™æ€§ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†ç­–ç•¥æ¡ä»¶åŒ–äºç®€å•çš„å¸ƒå°”å…¬å¼(Boolean formulae)åºåˆ—ä¸Šï¼Œå®ç°äº†å¯¹è‡ªåŠ¨æœºçŠ¶æ€è½¬æ¢çš„ç›´æ¥å¯¹é½ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œ(Graph Neural Network, GNN)å¯¹è¿™äº›å…¬å¼è¿›è¡Œç¼–ç ï¼Œä»è€Œæå–å‡ºç»“æ„åŒ–çš„ä»»åŠ¡è¡¨ç¤º(Structured Task Representations)ã€‚åœ¨å¤æ‚çš„å›½é™…è±¡æ£‹ç¯å¢ƒå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•è¯æ˜äº†å…¶åœ¨å¤„ç†å¤šä»»åŠ¡ç­–ç•¥æ—¶çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€è´¡çŒ®ä¸ºRLæ™ºèƒ½ä½“åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œä»»æ„LTLæŒ‡ä»¤æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning",
      "pdf_url": "https://arxiv.org/pdf/2512.02633v1",
      "published_date": "2025-12-02 10:44:51 UTC",
      "updated_date": "2025-12-02 10:44:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:55:50.477699+00:00"
    },
    {
      "arxiv_id": "2512.02625v1",
      "title": "CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography",
      "title_zh": "CryptoQAï¼šé¢å‘ AI è¾…åŠ©å¯†ç å­¦çš„å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†",
      "authors": [
        "Mayar Elfares",
        "Pascal Reisert",
        "Tilman Dietz",
        "Manpa Barman",
        "Ahmed Zaki",
        "Ralf KÃ¼sters",
        "Andreas Bulling"
      ],
      "abstract": "Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¯†ç å­¦é¢†åŸŸç¼ºä¹æ·±åº¦æ¨ç†å’Œæ•°å­¦åˆ†æèƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹å¯†ç å­¦è®¾è®¡çš„å¤§è§„æ¨¡é—®ç­”æ•°æ®é›† CryptoQAã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ä¸¤ç™¾ä¸‡ä¸ªæºè‡ªå­¦æœ¯æ–‡çŒ®çš„é—®ç­”å¯¹åŠä¸Šä¸‹æ–‡å…ƒæ•°æ®ï¼Œæ—¨åœ¨è¯„ä¼°å’Œè®­ç»ƒ LLMs å¤„ç†å¤æ‚å¯†ç å­¦ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ 15 ä¸ªæœ€å…ˆè¿›çš„ LLMs è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†äº‹å®å‡†ç¡®æ€§ã€æ•°å­¦æ¨ç†ã€ä¸€è‡´æ€§å’Œå¯¹æŠ—æ ·æœ¬é²æ£’æ€§ç­‰å¤šä¸ªç»´åº¦ï¼Œå¹¶ç»“åˆä¸“å®¶è¯„å®¡å»ºç«‹äº†é»„é‡‘æ ‡å‡†åŸºå‡†ã€‚ç»“æœè¡¨æ˜ï¼Œç›®å‰çš„æ¨¡å‹åœ¨å½¢å¼æ¨ç†å’Œç²¾ç¡®æ•°å­¦çŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œä½“ç°äº†å¼€å‘ä¸“ç”¨å¯†ç å­¦ AI åŠ©æ‰‹çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œå®éªŒè¯æ˜é€šè¿‡ CryptoQA å¯¹ LLMs è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æœ‰æ•ˆæå‡å…¶åœ¨å¯†ç å­¦ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02625v1",
      "published_date": "2025-12-02 10:35:36 UTC",
      "updated_date": "2025-12-02 10:35:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:55:54.683675+00:00"
    },
    {
      "arxiv_id": "2512.02610v1",
      "title": "Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction",
      "title_zh": "é¢å‘è·¨åŸŸå‰©ä½™å¯¿å‘½é¢„æµ‹çš„ç›®æ ‡ç‰¹å®šè‡ªé€‚åº”ä¸ä¸€è‡´æ€§é€€åŒ–å¯¹é½",
      "authors": [
        "Yubo Hou",
        "Mohamed Ragab",
        "Min Wu",
        "Chee-Keong Kwoh",
        "Xiaoli Li",
        "Zhenghua Chen"
      ],
      "abstract": "Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºæ¢°è®¾å¤‡å‰©ä½™ä½¿ç”¨å¯¿å‘½(Remaining Useful Life, RUL)é¢„æµ‹ä¸­çš„è·¨é¢†åŸŸæ•°æ®åˆ†å¸ƒå·®å¼‚é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTACDAçš„é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚ç”±äºä¼ ç»Ÿçš„å¯¹æŠ—æ€§é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•å¾€å¾€å¿½ç•¥äº†ç›®æ ‡é¢†åŸŸç‰¹å®šä¿¡æ¯ä»¥åŠé€€åŒ–é˜¶æ®µçš„ä¸ä¸€è‡´ç‰¹æ€§ï¼ŒTACDAå¼•å…¥äº†ç›®æ ‡é¢†åŸŸé‡æ„ç­–ç•¥ï¼Œåœ¨å­¦ä¹ é¢†åŸŸä¸å˜ç‰¹å¾çš„åŒæ—¶ä¿ç•™äº†ç›®æ ‡é¢†åŸŸçš„é‡è¦ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ›æ–°çš„èšç±»ä¸é…å¯¹ç­–ç•¥ï¼Œå®ç°äº†ç›¸ä¼¼é€€åŒ–é˜¶æ®µä¹‹é—´çš„ä¸€è‡´æ€§å¯¹é½ï¼Œæœ‰æ•ˆç¼“è§£äº†è·¨åŸŸé¢„æµ‹ä¸­çš„ç‰¹å¾å¤±é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTACDAåœ¨ä¸¤ä¸ªä¸åŒçš„è¯„ä»·æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³å®é™…å·¥ä¸šç¯å¢ƒä¸‹è·¨é¢†åŸŸRULé¢„æµ‹ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02610v1",
      "published_date": "2025-12-02 10:15:14 UTC",
      "updated_date": "2025-12-02 10:15:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:02.461218+00:00"
    },
    {
      "arxiv_id": "2512.02605v1",
      "title": "IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai",
      "title_zh": "IACTï¼šé¢å‘é€šç”¨ AI æ™ºèƒ½ä½“çš„è‡ªç»„ç»‡é€’å½’æ¨¡å‹â€”â€”kragent.ai æ¶æ„æŠ€æœ¯ç™½çš®ä¹¦",
      "authors": [
        "Pengju Lu"
      ],
      "abstract": "This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.",
      "tldr_zh": "è¯¥æŠ€æœ¯ç™½çš®ä¹¦ä»‹ç»äº† Interactive Agents Call Tree (IACT)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³é™æ€ã€ç¡¬ç¼–ç æ™ºèƒ½ä½“å·¥ä½œæµå±€é™æ€§çš„è®¡ç®—æ¨¡å‹ã€‚ä¸éœ€è¦é¢„å®šä¹‰å›¾ç»“æ„æˆ–ä¸“é—¨ç¼–ç¨‹çš„ä¼ ç»Ÿç³»ç»Ÿä¸åŒï¼ŒIACT æ˜¯ä¸€ä¸ªå®Œå…¨ç”±ç”¨æˆ·å¯¹è¯é©±åŠ¨çš„é€šç”¨è‡ªä¸»ç³»ç»Ÿã€‚åœ¨æ¥æ”¶åˆ°é«˜å±‚ç›®æ ‡åï¼Œè¯¥ç³»ç»Ÿèƒ½æ ¹æ®é—®é¢˜ç»“æ„è‡ªä¸»ä¸”å¢é‡åœ°ç”ŸæˆåŠ¨æ€çš„é€’å½’æ™ºèƒ½ä½“æ‹“æ‰‘ï¼ˆRecursive Agent Topologyï¼‰ï¼Œä»è€Œä½¿å…¶ç»„ç»‡å¤æ‚åº¦èƒ½å¤ŸåŒ¹é…å¼€æ”¾å¼ä»»åŠ¡ã€‚ä¸ºäº†å‡è½»å•å‘å‡½æ•°è°ƒç”¨ä¸­å›ºæœ‰çš„é”™è¯¯ä¼ æ’­ï¼ŒIACT å¼•å…¥äº†äº¤äº’å†—ä½™ï¼ˆInteractional Redundancyï¼‰æœºåˆ¶ï¼Œé€šè¿‡åŒå‘ã€æœ‰çŠ¶æ€çš„å¯¹è¯å–ä»£äº†åƒµåŒ–çš„è°ƒç”¨ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨è¿è¡Œæ—¶è¿›è¡Œé”™è¯¯æ ¡æ­£å’Œæ­§ä¹‰æ¶ˆé™¤ã€‚è¯¥ç ”ç©¶è¯¦ç»†æè¿°äº†è¯¥æ¨¡å‹åœ¨ kragent.ai ç³»ç»Ÿç”Ÿäº§éƒ¨ç½²ä¸­çš„æ¶æ„ã€è®¾è®¡åŸåˆ™åŠå®è·µç»éªŒï¼Œå¹¶æä¾›äº†æ¥è‡ªçœŸå®ä¸–ç•Œå·¥ä½œæµçš„å®šæ€§è¯æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 2 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2512.02605v1",
      "published_date": "2025-12-02 10:10:56 UTC",
      "updated_date": "2025-12-02 10:10:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:15.970424+00:00"
    },
    {
      "arxiv_id": "2512.02589v1",
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "title_zh": "PaperDebuggerï¼šé¢å‘ç¼–è¾‘å™¨å†…å­¦æœ¯å†™ä½œã€è¯„å®¡ä¸ç¼–è¾‘çš„æ’ä»¶å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Junyi Hou",
        "Andre Lin Huikai",
        "Nuo Chen",
        "Yiwei Gong",
        "Bingsheng He"
      ],
      "abstract": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PaperDebuggerï¼Œè¿™æ˜¯ä¸€æ¬¾é›†æˆåœ¨ç¼–è¾‘å™¨å†…éƒ¨ã€åŸºäºæ’ä»¶çš„å¤šæ™ºèƒ½ä½“ï¼ˆmulti-agentï¼‰å­¦æœ¯å†™ä½œåŠ©æ‰‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·¥å…·å› ç‹¬ç«‹äºç¼–è¾‘å™¨è€Œéš¾ä»¥ä¸æ–‡æ¡£çŠ¶æ€ã€ç»“æ„åŠä¿®æ”¹å†å²æ·±åº¦äº¤äº’çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ Chrome æµè§ˆå™¨æ‰©å±•ã€Kubernetes åŸç”Ÿç¼–æ’å±‚å’Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·¥å…·é“¾ï¼Œå…‹æœäº†åŒå‘åŒæ­¥ã€ç»†ç²’åº¦ç‰ˆæœ¬æ§åˆ¶åŠå®‰å…¨çŠ¶æ€ç®¡ç†ç­‰æŠ€æœ¯æŒ‘æˆ˜ã€‚PaperDebugger æ•´åˆäº†æ–‡çŒ®æœç´¢ã€å¼•ç”¨æŸ¥è¯¢ã€æ–‡æ¡£è¯„åˆ†å’Œä¿®è®¢æµæ°´çº¿ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨ Overleaf ç­‰ LaTeX ç¯å¢ƒä¸­æ‰§è¡Œ LLM é©±åŠ¨çš„æ¨ç†ä¸ç¼–è¾‘ä»»åŠ¡ã€‚é€šè¿‡å±•ç¤ºåŒ…æ‹¬å±€éƒ¨ç¼–è¾‘ã€ç»“æ„åŒ–å®¡ç¨¿åŠåŸºäº diff çš„æ›´æ–°åœ¨å†…çš„å®Œæ•´é›†æˆå·¥ä½œæµï¼Œè¯¥ç³»ç»ŸéªŒè¯äº†ç¼–è¾‘å™¨åŸç”Ÿï¼ˆeditor-nativeï¼‰æ™ºèƒ½åŠ©æ‰‹åœ¨å®é™…å­¦æœ¯åŠå…¬ä¸­çš„å¯è¡Œæ€§ä¸é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02589v1",
      "published_date": "2025-12-02 10:00:37 UTC",
      "updated_date": "2025-12-02 10:00:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:07.986651+00:00"
    },
    {
      "arxiv_id": "2512.04121v1",
      "title": "Can machines perform a qualitative data analysis? Reading the debate with Alan Turing",
      "title_zh": "æœºå™¨èƒ½å¦è¿›è¡Œå®šæ€§æ•°æ®åˆ†æï¼ŸåŸºäº Alan Turing æ€æƒ³çš„è¾©è®ºå®¡è§†",
      "authors": [
        "Stefano De Paoli"
      ],
      "abstract": "This paper reflects on the literature that rejects the use of Large Language Models (LLMs) in qualitative data analysis. It illustrates through empirical evidence as well as critical reflections why the current critical debate is focusing on the wrong problems. The paper proposes that the focus of researching the use of the LLMs for qualitative analysis is not the method per se, but rather the empirical investigation of an artificial system performing an analysis. The paper builds on the seminal work of Alan Turing and reads the current debate using key ideas from Turing \"Computing Machinery and Intelligence\". This paper therefore reframes the debate on qualitative analysis with LLMs and states that rather than asking whether machines can perform qualitative analysis in principle, we should ask whether with LLMs we can produce analyses that are sufficiently comparable to human analysts. In the final part the contrary views to performing qualitative analysis with LLMs are analysed using the same writing and rhetorical style that Turing used in his seminal work, to discuss the contrary views to the main question.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å®šæ€§æ•°æ®åˆ†æ(Qualitative Data Analysis)ä¸­åº”ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„äº‰è®®ï¼Œå¹¶å¯¹ç›®å‰å­¦æœ¯ç•Œæ‹’ç»ä½¿ç”¨è¯¥æŠ€æœ¯çš„è§‚ç‚¹è¿›è¡Œäº†åæ€ã€‚ä½œè€…é€šè¿‡å®è¯è¯æ®å’Œæ‰¹åˆ¤æ€§æ€è€ƒæŒ‡å‡ºï¼Œå½“å‰çš„äº‰è®ºç„¦ç‚¹å­˜åœ¨åå·®ï¼Œç ”ç©¶æ ¸å¿ƒä¸åº”å±€é™äºæ–¹æ³•è®ºæœ¬èº«ï¼Œè€Œåº”å…³æ³¨å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿæ‰§è¡Œåˆ†æè¿‡ç¨‹çš„å®è¯è°ƒæŸ¥ã€‚è¯¥è®ºæ–‡å€Ÿé‰´äº†å›¾çµ(Alan Turing)çš„ç»å…¸è‘—ä½œã€Šè®¡ç®—æœºå™¨ä¸æ™ºèƒ½ã€‹(Computing Machinery and Intelligence)ï¼Œæå‡ºåº”å°†è§†è§’ä»â€œæœºå™¨åŸåˆ™ä¸Šèƒ½å¦è¿›è¡Œå®šæ€§åˆ†æâ€è½¬å‘â€œLLMsç”Ÿæˆçš„åˆ†æç»“æœæ˜¯å¦ä¸äººç±»åˆ†æå¸ˆå…·æœ‰å¯æ¯”æ€§â€ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ–‡ç« é‡æ–°ç•Œå®šäº†LLMsåœ¨å®šæ€§åˆ†æé¢†åŸŸçš„è®¨è®ºæ¡†æ¶ï¼Œå¹¶ä¸»å¼ é€šè¿‡æ¯”è¾ƒç ”ç©¶æ¥è¯„ä¼°æœºå™¨çš„èƒ½åŠ›ã€‚åœ¨æœ€åéƒ¨åˆ†ï¼Œä½œè€…æ¨¡ä»¿å›¾çµç‰¹æœ‰çš„å†™ä½œä¸ä¿®è¾é£æ ¼ï¼Œç³»ç»Ÿåœ°åˆ†æå¹¶å›åº”äº†åå¯¹ä½¿ç”¨LLMsè¿›è¡Œå®šæ€§åˆ†æçš„å„ç§è´Ÿé¢è§‚ç‚¹ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04121v1",
      "published_date": "2025-12-02 09:41:03 UTC",
      "updated_date": "2025-12-02 09:41:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:17.273955+00:00"
    },
    {
      "arxiv_id": "2512.02567v1",
      "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System",
      "title_zh": "åŸºäº LLM çš„è½¯ä»¶å·¥ç¨‹ä¸­çš„åé¦ˆå¾ªç¯ä¸ä»£ç æ‰°åŠ¨ï¼šä»¥ C-to-Rust ç¿»è¯‘ç³»ç»Ÿä¸ºä¾‹",
      "authors": [
        "Martin Weiss",
        "Jesko Hecking-Harbusch",
        "Jochen Quante",
        "Matthias Woehrle"
      ],
      "abstract": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.\n  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.\n  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­åé¦ˆå¾ªç¯(feedback loops)å’Œä»£ç æ‰°åŠ¨(code perturbations)çš„å½±å“ï¼Œå¹¶ä»¥C-to-Rustç¿»è¯‘ç³»ç»Ÿä¸ºæ¡ˆä¾‹è¿›è¡Œåˆ†æã€‚è¯¥ç¿»è¯‘ç³»ç»Ÿé‡‡ç”¨ç”Ÿæˆå¹¶æ£€æŸ¥(generate-and-check)æ¨¡å¼ï¼Œè‡ªåŠ¨éªŒè¯ç”Ÿæˆçš„Rustä»£ç çš„ç¼–è¯‘æ€§ä»¥åŠä¸åŸå§‹Cä»£ç çš„è¡Œä¸ºç­‰ä»·æ€§ï¼Œå¹¶åœ¨æ£€æŸ¥å¤±è´¥æ—¶é€šè¿‡åé¦ˆå¾ªç¯é‡æ–°æç¤ºLLMè¿›è¡Œä¿®å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰åé¦ˆå¾ªç¯çš„æƒ…å†µä¸‹ï¼ŒLLMçš„é€‰æ‹©å¯¹ç¿»è¯‘æˆåŠŸç‡æœ‰å¾ˆå¤§å½±å“ï¼Œä½†å¼•å…¥åé¦ˆå¾ªç¯åï¼Œä¸åŒæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ç¼©å°ã€‚åé¦ˆå¾ªç¯ä¸ä»…æå‡äº†ç³»ç»Ÿçš„å¹³å‡ç¿»è¯‘æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†ç³»ç»Ÿåœ¨ä»£ç æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§(robustness)ã€‚æœ€åï¼Œç ”ç©¶å‘ç°ä»£ç æ‰°åŠ¨å¸¦æ¥çš„å¤šæ ·æ€§(diversity)ç”šè‡³èƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–ç³»ç»Ÿçš„æ•´ä½“ç¿»è¯‘è¡¨ç°ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02567v1",
      "published_date": "2025-12-02 09:38:20 UTC",
      "updated_date": "2025-12-02 09:38:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:46.465006+00:00"
    },
    {
      "arxiv_id": "2512.02566v1",
      "title": "From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature",
      "title_zh": "ä»åˆ†å›¾åˆ°åƒç´ ï¼šåŸºäºç”Ÿç‰©åŒ»å­¦å­¦æœ¯æ–‡çŒ®çš„ç¼©æ”¾å¼è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ",
      "authors": [
        "Kun Yuan",
        "Min Woo Sun",
        "Zhen Chen",
        "Alejandro Lozano",
        "Xiangteng He",
        "Shi Li",
        "Nassir Navab",
        "Xiaoxiao Sun",
        "Nicolas Padoy",
        "Serena Yeung-Levy"
      ],
      "abstract": "There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)é¢„è®­ç»ƒä¸­å°†å¤æ‚å›¾è¡¨å‹ç¼©ä¸ºç²—ç²’åº¦å›¾æ–‡å¯¹è€Œä¸¢å¤±å±€éƒ¨ç»†èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPanel2Patchçš„æ–°å‹æ•°æ®æµæ°´çº¿ã€‚Panel2Patchèƒ½å¤Ÿä»ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­è‡ªåŠ¨æŒ–æ˜å¤šé¢æ¿(multi-panel)å’Œå¸¦æœ‰æ ‡è®°çš„å›¾åƒå±‚çº§ç»“æ„ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ¶µç›–å›¾è¡¨ã€é¢æ¿åŠè¡¥ä¸(patch)å±‚çº§çš„å¤šç²’åº¦ç›‘ç£ä¿¡å·ã€‚ç ”ç©¶è€…åˆ©ç”¨è¯¥å±‚çº§è¯­æ–™åº“å¼€å‘äº†ä¸€ç§ç²’åº¦æ„ŸçŸ¥(granularity-aware)é¢„è®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†ä»å®è§‚æè¿°åˆ°å¾®è§‚åŒºåŸŸçŸ­è¯­çš„å¼‚æ„é¢„è®­ç»ƒç›®æ ‡ç»Ÿä¸€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPanel2Patchèƒ½å¤Ÿä»è¾ƒå°‘é‡çš„æ–‡çŒ®æ•°æ®ä¸­æå–æ›´é«˜æ•ˆçš„ç›‘ç£ä¿¡æ¯ï¼Œåœ¨æ˜¾è‘—å‡å°‘é¢„è®­ç»ƒæ•°æ®éœ€æ±‚çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜äºç°æœ‰ä¸»æµæµæ°´çº¿çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠåŒ»ç”Ÿå…³æ³¨å±€éƒ¨ç»“æ„çš„è§‚æµ‹ä¹ æƒ¯ï¼Œä¸ºæ„å»ºæ›´ç²¾å‡†çš„ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02566v1",
      "published_date": "2025-12-02 09:37:51 UTC",
      "updated_date": "2025-12-02 09:37:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:41.087162+00:00"
    },
    {
      "arxiv_id": "2512.02561v1",
      "title": "EZYer: A simulacrum of high school with generative agent",
      "title_zh": "EZYerï¼šåŸºäºç”Ÿæˆå¼æ™ºèƒ½ä½“çš„é«˜ä¸­æ¨¡æ‹Ÿç³»ç»Ÿ",
      "authors": [
        "Jinming Yang",
        "Zimu Ji",
        "Weiqi Luo",
        "Gaoxi Wang",
        "Bin Ma",
        "Yueling Deng"
      ],
      "abstract": "With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.",
      "tldr_zh": "é’ˆå¯¹å½“å‰åœ¨çº¿æ•™è‚²å·¥å…·åœ¨è¯¾ä»¶ç”Ÿæˆã€äº¤äº’ç¬”è®°å’Œå†…å®¹è´¨é‡ä¿éšœæ–¹é¢çš„ä¸è¶³ï¼Œè¯¥ç ”ç©¶æå‡ºäº† EZYerï¼Œä¸€ä¸ªåŸºäº generative agent çš„é«˜ä¸­æ•™å­¦æ¨¡æ‹Ÿç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿçš„ Teacher Module ç»“åˆäº† Text Corpus æ£€ç´¢ä¸æ·±åº¦ç”ŸæˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç¬¦åˆé«˜ä¸­æ•°å­¦å¤§çº²çš„ç»“æ„åŒ–æ•™å­¦ææ–™å’Œ LaTeX Beamer è¯¾ä»¶ã€‚å…¶ Student Module é€šè¿‡æ¨¡æ‹Ÿ Teacherã€Assistantã€Top Student å’Œ Struggling Student å››ç§è§’è‰²çš„åä½œäº’åŠ¨ï¼Œç”± Note Taker è‡ªåŠ¨æ±‡æ€»å¹¶ç”Ÿæˆå­¦æœ¯ç¬”è®°ï¼Œä»¥æå‡å­¦ä¹ çš„æ·±åº¦ä¸è¶£å‘³æ€§ã€‚ä¸ºäº†ç¡®ä¿å­¦æœ¯ä¸¥è°¨æ€§ä¸æ•™å­¦å¾—å½“æ€§ï¼ŒController æ¨¡å—é›†æˆäº†å…³é”®è¯è¿‡æ»¤ã€å†…å®¹è¯„åˆ†ã€è§’è‰²ååŒæ ¡éªŒå’ŒåŠ¨æ€å†…å®¹ä¿®æ­£ç³»ç»Ÿã€‚ç ”ç©¶é€šè¿‡å†…å®¹å‡†ç¡®æ€§ã€çŸ¥è¯†è¦†ç›–åº¦ã€å¯ç”¨æ€§ç­‰äº”ä¸ªç»´åº¦ï¼Œåˆ©ç”¨äº”ä¸ª large language models å¯¹ç³»ç»Ÿç”Ÿæˆçš„è¯¾ä»¶å’Œç¬”è®°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEZYer ç”Ÿæˆçš„å†…å®¹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨è‡ªåŠ¨åŒ–æ•™å­¦è¾…åŠ©å’Œå¢å¼ºå­¦ä¹ äº’åŠ¨æ–¹é¢å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "AgentIR@SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.02561v1",
      "published_date": "2025-12-02 09:28:59 UTC",
      "updated_date": "2025-12-02 09:28:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:57:21.876637+00:00"
    },
    {
      "arxiv_id": "2512.02558v1",
      "title": "Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance",
      "title_zh": "ç£å¯¼æ–‡æ¡£è¾…åŠ©ä¸‹çš„å¤šæ¨¡æ€åœºæ™¯å…±æƒ…æ°´å¹³é¢„æµ‹",
      "authors": [
        "Yufei Xiao",
        "Shangfei Wang"
      ],
      "abstract": "Prevalent empathy prediction techniques primarily concentrate on a singular modality, typically textual, thus neglecting multi-modal processing capabilities. They also overlook the utilization of certain privileged information, which may encompass additional empathetic content. In response, we introduce an advanced multi-modal empathy prediction method integrating video, audio, and text information. The method comprises the Multi-Modal Empathy Prediction and Supervisory Documentation Assisted Training. We use pre-trained networks in the empathy prediction network to extract features from various modalities, followed by a cross-modal fusion. This process yields a multi-modal feature representation, which is employed to predict empathy labels. To enhance the extraction of text features, we incorporate supervisory documents as privileged information during the assisted training phase. Specifically, we apply the Latent Dirichlet Allocation model to identify potential topic distributions to constrain text features. These supervisory documents, created by supervisors, focus on the counseling topics and the counselor's display of empathy. Notably, this privileged information is only available during training and is not accessible during the prediction phase. Experimental results on the multi-modal and dialogue empathy datasets demonstrate that our approach is superior to the existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€å…±æƒ…é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯å¤šå…³æ³¨å•ä¸€æ–‡æœ¬æ¨¡æ€ä¸”å¿½ç•¥ç‰¹æƒä¿¡æ¯(privileged information)çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†è§†é¢‘(video)ã€éŸ³é¢‘(audio)å’Œæ–‡æœ¬(text)ä¿¡æ¯ï¼ŒåŒ…å«å¤šæ¨¡æ€å…±æƒ…é¢„æµ‹ä¸ç›‘ç£æ–‡æ¡£è¾…åŠ©è®­ç»ƒ(Supervisory Documentation Assisted Training)ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ã€‚é€šè¿‡é¢„è®­ç»ƒç½‘ç»œæå–å„æ¨¡æ€ç‰¹å¾å¹¶è¿›è¡Œè·¨æ¨¡æ€èåˆ(cross-modal fusion)ï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé²æ£’çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºä»¥é¢„æµ‹å…±æƒ…æ ‡ç­¾ã€‚ä¸ºäº†å¢å¼ºæ–‡æœ¬ç‰¹å¾ï¼Œç ”ç©¶åœ¨è®­ç»ƒé˜¶æ®µå¼•å…¥äº†ç”±ä¸»ç®¡ç¼–å†™çš„ç›‘ç£æ–‡æ¡£ï¼Œå¹¶åˆ©ç”¨éšç‹„åˆ©å…‹é›·åˆ†é…æ¨¡å‹(Latent Dirichlet Allocation, LDA)è¯†åˆ«ä¸»é¢˜åˆ†å¸ƒä»¥çº¦æŸç‰¹å¾æå–ã€‚è¿™äº›ç‰¹æƒä¿¡æ¯ä»…åœ¨è®­ç»ƒæœŸé—´è¾…åŠ©æ¨¡å‹å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿå¯¹å’¨è¯¢åœºæ™¯ä¸­å…±æƒ…è¡¨ç°çš„ç†è§£æ·±åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€åŠå¯¹è¯å…±æƒ…æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02558v1",
      "published_date": "2025-12-02 09:26:56 UTC",
      "updated_date": "2025-12-02 09:26:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:57:45.377908+00:00"
    },
    {
      "arxiv_id": "2512.02555v1",
      "title": "ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce",
      "title_zh": "ADOREï¼šé¢å‘ç”µå­å•†åŠ¡çš„è‡ªä¸»é¢†åŸŸç›¸å…³æ€§å¼•æ“",
      "authors": [
        "Zheng Fang",
        "Donghao Xie",
        "Ming Pang",
        "Chunyuan Yuan",
        "Xue Jiang",
        "Changping Peng",
        "Zhangang Lin",
        "Zheng Luo"
      ],
      "abstract": "Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å•†åŠ¡æœç´¢ä¸­ term-matching æ–¹æ³•ï¼ˆå¦‚ BM25ï¼‰å­˜åœ¨çš„è¯­ä¹‰é¸¿æ²Ÿä»¥åŠç¥ç»æ¨¡å‹å¯¹é¢†åŸŸç‰¹å®šç¡¬æ ·æœ¬ï¼ˆhard samplesï¼‰çš„ä¾èµ–é—®é¢˜ï¼Œæå‡ºäº† ADORE è‡ªæŒç»­æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è§„åˆ™æ„ŸçŸ¥ç›¸å…³æ€§åˆ¤åˆ«æ¨¡å—ï¼Œåˆ©ç”¨ Chain-of-Thought LLM ç”Ÿæˆå¯¹é½æ„å›¾çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶é€šè¿‡ Kahneman-Tversky Optimization (KTO) æŠ€æœ¯ä½¿å…¶ä¸ç”¨æˆ·è¡Œä¸ºå¯¹é½ã€‚åŒæ—¶ï¼ŒADORE åŒ…å«é”™è¯¯ç±»å‹æ„ŸçŸ¥çš„æ•°æ®åˆæˆæ¨¡å—ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¯¹æŠ—æ€§æ ·æœ¬ï¼ˆadversarial examplesï¼‰ä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å…³é”®å±æ€§å¢å¼ºçš„çŸ¥è¯†è’¸é¦æ¨¡å—ï¼ˆKnowledge Distillationï¼‰ï¼Œè¯¥æ¡†æ¶å°†é¢†åŸŸç‰¹å®šçš„å±æ€§å±‚çº§æ³¨å…¥åˆ°å¯éƒ¨ç½²çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚ADORE å®ç°äº†æ ‡æ³¨ã€å¯¹æŠ—ç”Ÿæˆå’Œè’¸é¦çš„è‡ªåŠ¨åŒ–ï¼Œåœ¨å…‹æœæ•°æ®ç¨€ç¼ºé—®é¢˜çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ã€‚å¤§è§„æ¨¡å®éªŒå’Œåœ¨çº¿ A/B æµ‹è¯•éªŒè¯äº† ADORE çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå·¥ä¸šåº”ç”¨ä¸­èµ„æºé«˜æ•ˆä¸”è®¤çŸ¥å¯¹é½çš„ç›¸å…³æ€§å»ºæ¨¡ç¡®ç«‹äº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.02555v1",
      "published_date": "2025-12-02 09:25:13 UTC",
      "updated_date": "2025-12-02 09:25:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:42.081843+00:00"
    },
    {
      "arxiv_id": "2512.02551v2",
      "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
      "title_zh": "CUDA-L2ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ å®ç°æ€§èƒ½è¶…è¶Š cuBLAS çš„çŸ©é˜µä¹˜æ³•",
      "authors": [
        "Songqiao Su",
        "Xiaofei Sun",
        "Xiaoya Li",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art Nvidia's closed-source libraries, i.e., cuBLAS, cuBLASLt. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0% over torch.matmul on average; +19.2% over cuBLAS using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8% over cuBLASLt-heuristic, which queries cuBLASLt library and selects the algorithm based on the heuristic's suggestion; and +11.4% over the most competitive cuBLASLt-AutoTuning model, which selects the fastest algorithm from up to 100 candidates from cuBLASLt's suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7%, +26.0%, +22.4%, and +15.9% for torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CUDA-L2ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå¼ºåŒ–å­¦ä¹ (RL)è‡ªåŠ¨ä¼˜åŒ–åŠç²¾åº¦é€šç”¨çŸ©é˜µä¹˜æ³•(HGEMM)çš„CUDAå†…æ ¸ã€‚è¯¥ç³»ç»Ÿä»¥CUDAæ‰§è¡Œé€Ÿåº¦ä½œä¸ºRLå¥–åŠ±ï¼Œåœ¨1,000ä¸ªé…ç½®ç©ºé—´ä¸­è¿›è¡Œè‡ªåŠ¨åŒ–æœç´¢ï¼Œæ€§èƒ½å…¨é¢è¶…è¶Šäº†torch.matmulã€cuBLASåŠcuBLASLtç­‰ä¸»æµé«˜æ€§èƒ½åŸºå‡†ã€‚åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼ŒCUDA-L2æ¯”æœ€å…·ç«äº‰åŠ›çš„cuBLASLt-AutoTuningæå‡äº†11.4%çš„æ€§èƒ½ï¼›åœ¨æ¨¡æ‹Ÿå®æ—¶æ¨ç†çš„æœåŠ¡å™¨æ¨¡å¼ä¸‹ï¼Œå…¶åŠ é€Ÿæ•ˆæœè¿›ä¸€æ­¥æå‡è‡³15.9%ã€‚CUDA-L2çš„æˆåŠŸè¯æ˜äº†å³ä¾¿æ˜¯HGEMMè¿™ç±»å·²ç»è¿‡æ·±åº¦ä¼˜åŒ–çš„å†…æ ¸ï¼Œä»å¯é€šè¿‡LLMå¼•å¯¼çš„RLè‡ªåŠ¨åŒ–æŠ€æœ¯ï¼Œåœ¨äººç±»ä¸“å®¶éš¾ä»¥è¦†ç›–çš„è§„æ¨¡ä¸‹æ¢ç´¢å‡ºæ›´ä¼˜é…ç½®ã€‚è¿™é¡¹ç ”ç©¶ä¸ºé«˜æ€§èƒ½è®¡ç®—é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ€§èƒ½è°ƒä¼˜æä¾›äº†å…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project code: github.com/deepreinforce-ai/CUDA-L2",
      "pdf_url": "https://arxiv.org/pdf/2512.02551v2",
      "published_date": "2025-12-02 09:20:15 UTC",
      "updated_date": "2025-12-12 00:47:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:59.384269+00:00"
    },
    {
      "arxiv_id": "2512.02550v1",
      "title": "Sparse Computations in Deep Learning Inference",
      "title_zh": "æ·±åº¦å­¦ä¹ æ¨ç†ä¸­çš„ç¨€ç–è®¡ç®—",
      "authors": [
        "Ioanna Tasou",
        "Panagiotis Mpakos",
        "Angelos Vlachos",
        "Dionysios Adamopoulos",
        "Georgios Giannakopoulos",
        "Konstantinos Katsikopoulos",
        "Ioannis Karaparisis",
        "Maria Lazou",
        "Spyridon Loukovitis",
        "Areti Mei",
        "Anastasia Poulopoulou",
        "Angeliki Dimitriou",
        "Giorgos Filandrianos",
        "Dimitrios Galanopoulos",
        "Vasileios Karampinis",
        "Ilias Mitsouras",
        "Nikolaos Spanos",
        "Petros Anastasiadis",
        "Ioannis Doudalis",
        "Konstantinos Nikas",
        "George Retsinas",
        "Paraskevi Tzouveli",
        "Christina Giannoula",
        "Nectarios Koziris",
        "Nikela Papadopoulou",
        "Giorgos Stamou",
        "Athanasios Voulodimos",
        "Georgios Goumas"
      ],
      "abstract": "The computational demands of modern Deep Neural Networks (DNNs) are immense and constantly growing. While training costs usually capture public attention, inference demands are also contributing in significant computational, energy and environmental footprints. Sparsity stands out as a critical mechanism for drastically reducing these resource demands. However, its potential remains largely untapped and is not yet fully incorporated in production AI systems. To bridge this gap, this work provides the necessary knowledge and insights for performance engineers keen to get involved in deep learning inference optimization. In particular, in this work we: a) discuss the various forms of sparsity that can be utilized in DNN inference, b) explain how the original dense computations translate to sparse kernels, c) provide an extensive bibliographic review of the state-of-the-art in the implementation of these kernels for CPUs and GPUs, d) discuss the availability of sparse datasets in support of sparsity-related research and development, e) explore the current software tools and frameworks that provide robust sparsity support, and f) present evaluation results of different implementations of the key SpMM and SDDMM kernels on CPU and GPU platforms. Ultimately, this paper aims to serve as a resource for performance engineers seeking to develop and deploy highly efficient sparse deep learning models in productions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨ç†(Deep Learning Inference)ä¸­çš„ç¨€ç–è®¡ç®—(Sparse Computations)ï¼Œæ—¨åœ¨é€šè¿‡ç¨€ç–æ€§æœºåˆ¶æ˜¾è‘—é™ä½æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)æ¨ç†å¸¦æ¥çš„å·¨å¤§è®¡ç®—èµ„æºã€èƒ½æºä¸ç¯å¢ƒå‹åŠ›ã€‚æ–‡ç« è¯¦ç»†è®¨è®ºäº†å¯ç”¨äºæ¨ç†çš„å¤šç§ç¨€ç–å½¢å¼ï¼Œé˜æ˜äº†å¦‚ä½•å°†å¯†é›†çš„åŸå§‹è®¡ç®—è½¬åŒ–ä¸ºç¨€ç–ç®—å­(Sparse Kernels)ï¼Œå¹¶å¯¹ CPU å’Œ GPU å¹³å°ä¸Šçš„æœ€æ–°å®ç°æŠ€æœ¯è¿›è¡Œäº†å¹¿æ³›çš„æ–‡çŒ®ç»¼è¿°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†æ”¯æŒç›¸å…³ç ”å‘çš„ç¨€ç–æ•°æ®é›†ï¼Œè°ƒç ”äº†ç°æœ‰çš„è½¯ä»¶å·¥å…·ä¸æ¡†æ¶ï¼Œå¹¶æä¾›äº†æ ¸å¿ƒç®—å­ SpMM å’Œ SDDMM åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šçš„è¯„ä¼°ç»“æœã€‚è¯¥å·¥ä½œä¸ºæ€§èƒ½å·¥ç¨‹å¸ˆæä¾›äº†ç³»ç»Ÿçš„ç†è®ºçŸ¥è¯†ä¸å®è·µè§è§£ï¼Œè‡´åŠ›äºæ¨åŠ¨é«˜æ•ˆç¨€ç–æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®é™…éƒ¨ç½²ä¸ä¼˜åŒ–ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02550v1",
      "published_date": "2025-12-02 09:19:33 UTC",
      "updated_date": "2025-12-02 09:19:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:56:54.784842+00:00"
    },
    {
      "arxiv_id": "2512.04120v1",
      "title": "Towards Contextual Sensitive Data Detection",
      "title_zh": "è¿ˆå‘æƒ…å¢ƒåŒ–æ•æ„Ÿæ•°æ®æ£€æµ‹",
      "authors": [
        "Liang Telkamp",
        "Madelon Hulsebos"
      ],
      "abstract": "The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that consider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾æ•°æ®é—¨æˆ·ä¸­æ•æ„Ÿæ•°æ®ä¿æŠ¤çš„éœ€æ±‚ï¼ŒæŒ‡å‡ºæ•°æ®çš„æ•æ„Ÿæ€§é«˜åº¦ä¾èµ–äºå…¶ Contextï¼Œå¹¶æŒ‘æˆ˜äº†ä¼ ç»Ÿä»…å…³æ³¨ä¸ªäººéšç§æ•°æ®çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§åŸºäº Context çš„æ£€æµ‹æœºåˆ¶ï¼šä¸€ç§æ˜¯ Type Contextualizationï¼Œé€šè¿‡æ£€æµ‹æ•°æ®å€¼çš„ Semantic Type å¹¶ç»“åˆæ•°æ®é›†å†…éƒ¨ Context è¿›è¡Œåˆ¤æ–­ï¼›å¦ä¸€ç§æ˜¯ Domain Contextualizationï¼Œåˆ©ç”¨æ£€ç´¢æŠ€æœ¯ä»ç›¸å…³è§„åˆ™æ–‡æ¡£ä¸­æå–ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åœ°ç†æ¥æºæˆ–ä¸»é¢˜ï¼‰çš„æ•æ„Ÿæ€§è§„åˆ™ã€‚è¿™äº›æœºåˆ¶å€ŸåŠ© Large Language Models (LLMs) å®ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†äººé“ä¸»ä¹‰æ•°æ®é›†ç­‰éæ ‡å‡†é¢†åŸŸçš„æ•æ„Ÿæ•°æ®è¯†åˆ«é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒType Contextualization æ˜¾è‘—é™ä½äº†è¯¯æŠ¥ç‡ï¼Œå¹¶å°† Recall æå‡è‡³ 94%ï¼Œè¿œé«˜äºå•†ä¸šå·¥å…·çš„ 63%ã€‚æ­¤å¤–ï¼Œé¢†åŸŸä¸“å®¶è¯„ä¼°è¡¨æ˜ LLM æä¾›çš„ Context-grounded è§£é‡Šèƒ½æœ‰æ•ˆæŒ‡å¯¼äººå·¥å®¡è®¡è¿‡ç¨‹å¹¶æé«˜ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶å·²å°†å…¶æœºåˆ¶å’Œæ ‡æ³¨çš„æ•°æ®é›†å¼€æºï¼Œä¸ºæ›´åŠ ç²¾å‡†å’Œæƒ…å¢ƒåŒ–çš„æ•æ„Ÿæ•°æ®è‡ªåŠ¨åŒ–æ£€æµ‹æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.DB",
        "cs.IR"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04120v1",
      "published_date": "2025-12-02 09:01:36 UTC",
      "updated_date": "2025-12-02 09:01:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:57:02.468132+00:00"
    },
    {
      "arxiv_id": "2512.02530v2",
      "title": "Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration",
      "title_zh": "Aetheriaï¼šåŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºä¸åä½œçš„å¤šæ¨¡æ€å¯è§£é‡Šå†…å®¹å®‰å…¨æ¡†æ¶",
      "authors": [
        "Yuxiang He",
        "Jian Zhao",
        "Yuchen Yuan",
        "Tianle Zhang",
        "Wei Cai",
        "Haojie Cheng",
        "Ziyan Shi",
        "Ming Zhu",
        "Haichuan Tang",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Aetheriaï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºä¸åä½œçš„å¤šæ¨¡æ€å¯è§£é‡Šå†…å®¹å®‰å…¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å®¡æ ¸ç³»ç»Ÿåœ¨è¯†åˆ«éšæ€§é£é™©å’Œæä¾›å¯è§£é‡Šåˆ¤å®šè¿‡ç¨‹æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç”±äº”ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“ç»„æˆçš„åä½œæ¶æ„ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„åŠ¨æ€è¾©è®ºæœºåˆ¶ï¼Œå¯¹å¤šæ¨¡æ€å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æä¸è£å†³ã€‚åœ¨ AIR-Bench åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAetheria ä¸ä»…èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†ä¸”å¯è¿½æº¯çš„å®¡è®¡æŠ¥å‘Šï¼Œåœ¨æ•´ä½“å†…å®¹å®‰å…¨å‡†ç¡®ç‡ï¼ˆå°¤å…¶æ˜¯éšæ€§é£é™©è¯†åˆ«ï¼‰æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€æ¡†æ¶ä¸ºå¯ä¿¡ AI å†…å®¹å®¡æ ¸å»ºç«‹äº†ä¸€ä¸ªé€æ˜ä¸”å¯è§£é‡Šçš„èŒƒå¼ï¼Œæ˜¾è‘—æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "https://github.com/Herrieson/Aetheria",
      "pdf_url": "https://arxiv.org/pdf/2512.02530v2",
      "published_date": "2025-12-02 08:49:54 UTC",
      "updated_date": "2025-12-09 05:44:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:57:09.984902+00:00"
    },
    {
      "arxiv_id": "2512.03112v1",
      "title": "Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability",
      "title_zh": "è¶…è¶Šå¯åŠ æ€§ï¼šé¢å‘éçº¿æ€§å¯è§£é‡Šæ€§çš„ç¨€ç–ä¿åº Shapley å›å½’",
      "authors": [
        "Jialai She"
      ],
      "abstract": "Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç¨€ç–ç­‰å€¼çº¿Shapleyå›å½’ï¼ˆSparse Isotonic Shapley Regression, SISRï¼‰ï¼Œæ—¨åœ¨è§£å†³å¯è§£é‡ŠAIï¼ˆExplainable AIï¼‰ä¸­Shapley valuesé¢ä¸´çš„åŠ æ€§å‡è®¾ï¼ˆadditivity assumptionï¼‰å¤±æ•ˆåŠé«˜ç»´ç‰¹å¾ç¨€ç–æ€§æŒ‘æˆ˜ã€‚SISRæ¡†æ¶é€šè¿‡å­¦ä¹ å•è°ƒå˜æ¢æ¥æ¢å¤åŠ æ€§ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é—­å¼è§„èŒƒçš„éœ€æ±‚ï¼Œå¹¶åŒæ—¶å¯¹Shapley vectoræ–½åŠ L0ç¨€ç–æ€§çº¦æŸã€‚å…¶ä¼˜åŒ–ç®—æ³•ç»“åˆäº†Pool-Adjacent-Violatorsè¿›è¡Œé«˜æ•ˆçš„ç­‰å€¼çº¿å›å½’ï¼ˆisotonic regressionï¼‰ï¼Œå¹¶åˆ©ç”¨å½’ä¸€åŒ–ç¡¬é˜ˆå€¼ï¼ˆnormalized hard-thresholdingï¼‰è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œç¡®ä¿äº†å…¨å±€æ”¶æ•›æ€§ã€‚ç ”ç©¶åˆ†æè¡¨æ˜ï¼ŒSISRåœ¨å„ç§å™ªå£°ç¯å¢ƒä¸‹å‡èƒ½å®ç°å¼ºåŠ›çš„æ”¯æ’‘æ¢å¤ï¼Œå¹¶é¦–æ¬¡è¯æ˜äº†æ— å…³ç‰¹å¾å’Œç‰¹å¾ä¾èµ–ä¼šå¯¼è‡´æ”¯ä»˜å˜æ¢åç¦»çº¿æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å›å½’ã€é€»è¾‘å›å½’å’Œæ ‘é›†æˆæ¨¡å‹ä¸­ï¼ŒSISRèƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤æ— å…³ç‰¹å¾å¹¶ç¨³å®šå½’å› ç»“æœï¼Œå…‹æœäº†æ ‡å‡†Shapley valuesåœ¨ç§©ï¼ˆrankï¼‰å’Œç¬¦å·ï¼ˆsignï¼‰ä¸Šçš„ä¸¥é‡å¤±çœŸé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†éçº¿æ€§å˜æ¢ä¼°è®¡ä¸ç¨€ç–æ€§è¿½æ±‚ç›¸ç»Ÿä¸€ï¼Œä¸ºéçº¿æ€§å¯è§£é‡Šæ€§æä¾›äº†å…·å¤‡ç†è®ºåŸºç¡€ä¸”å®ç”¨çš„å½’å› æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03112v1",
      "published_date": "2025-12-02 08:34:43 UTC",
      "updated_date": "2025-12-02 08:34:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:58:12.972019+00:00"
    },
    {
      "arxiv_id": "2512.03111v1",
      "title": "PanFoMa: A Lightweight Foundation Model and Benchmark for Pan-Cancer",
      "title_zh": "PanFoMaï¼šé¢å‘æ³›ç™Œç ”ç©¶çš„è½»é‡çº§åŸºç¡€æ¨¡å‹ä¸åŸºå‡†",
      "authors": [
        "Xiaoshui Huang",
        "Tianlin Zhu",
        "Yifan Zuo",
        "Xue Xia",
        "Zonghan Wu",
        "Jiebin Yan",
        "Dingli Hua",
        "Zongyi Xu",
        "Yuming Fang",
        "Jian Zhang"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) is essential for decoding tumor heterogeneity. However, pan-cancer research still faces two key challenges: learning discriminative and efficient single-cell representations, and establishing a comprehensive evaluation benchmark. In this paper, we introduce PanFoMa, a lightweight hybrid neural network that combines the strengths of Transformers and state-space models to achieve a balance between performance and efficiency. PanFoMa consists of a front-end local-context encoder with shared self-attention layers to capture complex, order-independent gene interactions; and a back-end global sequential feature decoder that efficiently integrates global context using a linear-time state-space model. This modular design preserves the expressive power of Transformers while leveraging the scalability of Mamba to enable transcriptome modeling, effectively capturing both local and global regulatory signals. To enable robust evaluation, we also construct a large-scale pan-cancer single-cell benchmark, PanFoMaBench, containing over 3.5 million high-quality cells across 33 cancer subtypes, curated through a rigorous preprocessing pipeline. Experimental results show that PanFoMa outperforms state-of-the-art models on our pan-cancer benchmark (+4.0\\%) and across multiple public tasks, including cell type annotation (+7.4\\%), batch integration (+4.0\\%) and multi-omics integration (+3.1\\%). The code is available at https://github.com/Xiaoshui-Huang/PanFoMa.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PanFoMaï¼Œä¸€ç§è½»é‡çº§çš„æ··åˆç¥ç»ç½‘ç»œåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å•ç»†èƒ RNA æµ‹åº (scRNA-seq) åœ¨æ³›ç™Œç ”ç©¶ä¸­é¢ä¸´çš„è¡¨å¾æ•ˆç‡ä¸è¯„ä¼°åŸºå‡†ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº† Transformers ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ (state-space models) çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å‰ç«¯å±€éƒ¨ä¸Šä¸‹æ–‡ç¼–ç å™¨æ•æ‰å¤æ‚çš„åŸºå› äº¤äº’ï¼Œå¹¶åˆ©ç”¨åŸºäº Mamba çš„åç«¯è§£ç å™¨å®ç°é«˜æ•ˆçš„çº¿æ€§æ—¶é—´å…¨å±€ä¸Šä¸‹æ–‡æ•´åˆã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡åœ¨ä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿç²¾å‡†æ•æ‰è½¬å½•ç»„ä¸­çš„å±€éƒ¨ä¸å…¨å±€è°ƒèŠ‚ä¿¡å·ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å« 33 ç§ç™Œç—‡äºšå‹ã€è¶…è¿‡ 350 ä¸‡ä¸ªé«˜è´¨é‡ç»†èƒçš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›† PanFoMaBenchã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPanFoMa åœ¨ç»†èƒç±»å‹æ³¨é‡Š (cell type annotation)ã€æ‰¹æ¬¡æ•´åˆ (batch integration) åŠå¤šç»„å­¦æ•´åˆ (multi-omics integration) ç­‰å¤šé¡¹ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸ºæ³›ç™Œå•ç»†èƒåˆ†ææä¾›äº†é«˜æ•ˆä¸”ç²¾å‡†çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.GN",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.03111v1",
      "published_date": "2025-12-02 08:31:31 UTC",
      "updated_date": "2025-12-02 08:31:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:58:57.686086+00:00"
    },
    {
      "arxiv_id": "2512.02502v1",
      "title": "AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations",
      "title_zh": "AskNearbyï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¤¾åŒºä¿¡æ¯æ£€ç´¢ä¸ä¸ªæ€§åŒ–è®¤çŸ¥åœ°å›¾æ¨èåº”ç”¨",
      "authors": [
        "Luyao Niu",
        "Zhicheng Deng",
        "Boyang Li",
        "Nuoxian Huang",
        "Ruiqi Liu",
        "Wenjia Zhang"
      ],
      "abstract": "The \"15-minute city\" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹â€œ15åˆ†é’ŸåŸå¸‚â€æ„¿æ™¯ä¸­å­˜åœ¨çš„æœ¬åœ°ç”Ÿæ´»ä¿¡æ¯è·å–ï¼ˆLocal Life Information Accessibility, LLIAï¼‰éš¾é¢˜ï¼Œæå‡ºäº†åä¸º AskNearby çš„äººå·¥æ™ºèƒ½ç¤¾åŒºåº”ç”¨ï¼Œæ—¨åœ¨ç»Ÿä¸€ 15 åˆ†é’Ÿç”Ÿæ´»åœˆå†…çš„ä¿¡æ¯æ£€ç´¢ä¸æ¨èã€‚AskNearby é›†æˆäº†ä¸€ä¸ªä¸‰å±‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰æµæ°´çº¿ï¼Œé€šè¿‡ååŒå›¾è°±æ£€ç´¢ã€è¯­ä¹‰å‘é‡æ£€ç´¢åŠåœ°ç†æ£€ç´¢ï¼Œå®ç°äº†é«˜æ•ˆçš„é‚»é‡Œä¿¡æ¯è·å–ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿå¼•å…¥äº†è®¤çŸ¥åœ°å›¾ï¼ˆCognitive-mapï¼‰æ¨¡å‹ï¼Œé€šè¿‡ç¼–ç ç”¨æˆ·çš„é‚»é‡Œç†Ÿæ‚‰åº¦å’Œä¸ªäººåå¥½æ¥ä¼˜åŒ–ä¸ªæ€§åŒ–æ¨èã€‚åœ¨çœŸå®ç¤¾åŒºæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒAskNearby åœ¨æ£€ç´¢å‡†ç¡®ç‡ã€æ¨èè´¨é‡ä»¥åŠæ—¶ç©ºå®šä½ï¼ˆSpatiotemporal groundingï¼‰å’Œè®¤çŸ¥æ„ŸçŸ¥æ’åºæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäº LLM å’Œä¼ ç»Ÿåœ°å›¾çš„åŸºå‡†æ¨¡å‹ã€‚å®é™…éƒ¨ç½²ç»“æœè¿›ä¸€æ­¥éªŒè¯äº† AskNearby èƒ½æœ‰æ•ˆå¸®åŠ©å±…æ°‘å‘ç°æœ¬åœ°èµ„æºå¹¶å‚ä¸ç¤¾åŒºç”Ÿæ´»ï¼Œä¸ºè§£å†³åŸå¸‚ä¿¡æ¯åŒ–æŒ‘æˆ˜æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02502v1",
      "published_date": "2025-12-02 07:47:31 UTC",
      "updated_date": "2025-12-02 07:47:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:58:59.297405+00:00"
    },
    {
      "arxiv_id": "2512.03110v1",
      "title": "The BEAT-CF Causal Model: A model for guiding the design of trials and observational analyses of cystic fibrosis exacerbations",
      "title_zh": "BEAT-CF å› æœæ¨¡å‹ï¼šç”¨äºæŒ‡å¯¼å›Šæ€§çº¤ç»´åŒ–æ€¥æ€§åŠ é‡ä¸´åºŠè¯•éªŒä¸è§‚å¯Ÿæ€§åˆ†æè®¾è®¡çš„æ¨¡å‹",
      "authors": [
        "Steven Mascaro",
        "Owen Woodberry",
        "Charlie McLeod",
        "Mitch Messer",
        "Hiran Selvadurai",
        "Yue Wu",
        "Andre Schultz",
        "Thomas L Snelling"
      ],
      "abstract": "Loss of lung function in cystic fibrosis (CF) occurs progressively, punctuated by acute pulmonary exacerbations (PEx) in which abrupt declines in lung function are not fully recovered. A key component of CF management over the past half century has been the treatment of PEx to slow lung function decline. This has been credited with improvements in survival for people with CF (PwCF), but there is no consensus on the optimal approach to PEx management. BEAT-CF (Bayesian evidence-adaptive treatment of CF) was established to build an evidence-informed knowledge base for CF management. The BEAT-CF causal model is a directed acyclic graph (DAG) and Bayesian network (BN) for PEx that aims to inform the design and analysis of clinical trials comparing the effectiveness of alternative approaches to PEx management. The causal model describes relationships between background risk factors, treatments, and pathogen colonisation of the airways that affect the outcome of an individual PEx episode. The key factors, outcomes, and causal relationships were elicited from CF clinical experts and together represent current expert understanding of the pathophysiology of a PEx episode, guiding the design of data collection and studies and enabling causal inference. Here, we present the DAG that documents this understanding, along with the processes used in its development, providing transparency around our trial design and study processes, as well as a reusable framework for others.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† BEAT-CF å› æœæ¨¡å‹(BEAT-CF Causal Model)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æŒ‡å¯¼å›Šæ€§çº¤ç»´åŒ–(Cystic Fibrosis, CF)è‚ºéƒ¨åŠ é‡æœŸ(PEx)ä¸´åºŠè¯•éªŒè®¾è®¡å’Œè§‚å¯Ÿæ€§åˆ†æçš„ä¸“ä¸šæ¡†æ¶ã€‚è¯¥æ¨¡å‹ç»“åˆäº†æœ‰å‘æ— ç¯å›¾(DAG)å’Œè´å¶æ–¯ç½‘ç»œ(BN)æŠ€æœ¯ï¼Œç³»ç»Ÿæè¿°äº†èƒŒæ™¯é£é™©å› ç´ ã€æ²»ç–—å¹²é¢„ä»¥åŠæ°”é“ç—…åŸä½“å®šæ¤(Pathogen Colonisation)ä¹‹é—´å½±å“ PEx å‘ä½œç»“æœçš„å¤æ‚å…³ç³»ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä» CF ä¸´åºŠä¸“å®¶å¤„è·å–ä¸“ä¸šçŸ¥è¯†ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åæ˜  PEx ç—…ç†ç”Ÿç†å­¦çš„æ ¸å¿ƒå› æœé€»è¾‘ã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½å¤ŸæŒ‡å¯¼æ•°æ®é‡‡é›†è¿‡ç¨‹ï¼Œè¿˜æ”¯æŒå¯¹ä¸åŒ PEx ç®¡ç†ç­–ç•¥çš„æœ‰æ•ˆæ€§è¿›è¡Œæ·±å…¥çš„å› æœæ¨ç†(Causal Inference)ã€‚é€šè¿‡å…¬å¼€ DAG çš„å¼€å‘æµç¨‹ï¼Œè¯¥ç ”ç©¶ä¸ºä¸´åºŠè¯•éªŒè®¾è®¡æä¾›äº†é«˜åº¦çš„é€æ˜åº¦ï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¯é‡å¤ä½¿ç”¨çš„å»ºæ¨¡æ¡†æ¶ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "12 pages (8 pages in appendices)",
      "pdf_url": "https://arxiv.org/pdf/2512.03110v1",
      "published_date": "2025-12-02 07:46:42 UTC",
      "updated_date": "2025-12-02 07:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:29.769339+00:00"
    },
    {
      "arxiv_id": "2512.02499v1",
      "title": "COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes",
      "title_zh": "COPEï¼šåŸºäºå¼€æºå¤§è¯­è¨€æ¨¡å‹ä¸”åˆ©ç”¨ä¸´åºŠè®°å½•è¿›è¡Œå’ä¸­é¢„åé¢„æµ‹çš„é“¾å¼æ€ç»´å¼•æ“",
      "authors": [
        "Yongkai Liu",
        "Helena Feng",
        "Bin Jiang",
        "Yixin Wang",
        "Max Wintermark",
        "David S. Liebeskind",
        "Michael Moseley",
        "Maarten Lansberg",
        "Gregory Albers",
        "Jeremy Heit",
        "Greg Zaharchuk"
      ],
      "abstract": "Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†åä¸ºCOPE (Chain-of-Thought Outcome Prediction Engine) çš„æ¨ç†å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨éç»“æ„åŒ–ä¸´åºŠè®°å½•é¢„æµ‹æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­(AIS)æ‚£è€…çš„90å¤©åŠŸèƒ½é¢„åã€‚è¯¥æ¡†æ¶åŸºäºå¼€æºçš„LLaMA-3-8Bæ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤æ­¥èµ°çš„é“¾å¼æ€ç»´(Chain-of-Thought)æ¶æ„ï¼Œå…ˆç”Ÿæˆä¸´åºŠæ¨ç†å†è¾“å‡ºæ”¹è‰¯Rankiné‡è¡¨(mRS)é¢„æµ‹åˆ†æ•°ã€‚ç ”ç©¶é€šè¿‡464åæ‚£è€…çš„æ•°æ®ï¼Œå°†COPEä¸GPT-4.1ã€ClinicalBERTã€ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹(Clinical ML)åŠä¸å¸¦CoTçš„å•æ­¥LLMè¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOPEçš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ä¸º1.01ï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºClinicalBERTå’Œä¼ ç»ŸMLæ¨¡å‹ï¼Œä¸”è¡¨ç°ä¸GPT-4.1ç›¸å½“ã€‚å­ç¾¤ç»„åˆ†æè¡¨æ˜è¯¥æ¨¡å‹åœ¨ä¸åŒæ€§åˆ«å’Œå¹´é¾„ç¾¤ä½“ä¸­è¡¨ç°ç¨³å¥ï¼Œå°½ç®¡åœ¨å¤æ‚ç—…ä¾‹ä¸­çš„è¯¯å·®ç•¥æœ‰å¢åŠ ã€‚ä½œä¸ºä¸€ç§è½»é‡çº§ä¸”ä¿æŠ¤éšç§çš„å¼€æºæ–¹æ¡ˆï¼ŒCOPEä¸ºä»éç»“æ„åŒ–ä¸´åºŠæ–‡æœ¬ä¸­å®ç°å‡†ç¡®ã€å¯è§£é‡Šçš„é¢„åé¢„æµ‹æä¾›äº†å®ç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02499v1",
      "published_date": "2025-12-02 07:44:20 UTC",
      "updated_date": "2025-12-02 07:44:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:58:48.579915+00:00"
    },
    {
      "arxiv_id": "2512.02487v1",
      "title": "Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding",
      "title_zh": "æ©ç è‡³å…³é‡è¦ï¼šè§£é”å¤§è¯­è¨€æ¨¡å‹åœ¨ 3D åœºæ™¯è¯­è¨€ç†è§£ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yerim Jeon",
        "Miso Lee",
        "WonJun Moon",
        "Jae-Pil Heo"
      ],
      "abstract": "Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹3Dåœºæ™¯è¯­è¨€ç†è§£ä¸­å› æœæ³¨æ„åŠ›æ©ç (Causal Attention Mask)å¯¼è‡´çš„é¡ºåºåè§(Sequential Bias)å’Œå—é™çš„å¯¹è±¡æŒ‡ä»¤æ³¨æ„åŠ›é—®é¢˜ï¼Œæå‡ºäº†3D-SLIMè¿™ä¸€æœ‰æ•ˆçš„æ©ç ç­–ç•¥ã€‚è¯¥ç­–ç•¥å¼•å…¥äº†å‡ ä½•è‡ªé€‚åº”æ©ç (Geometry-adaptive Mask)å’ŒæŒ‡ä»¤æ„ŸçŸ¥æ©ç (Instruction-aware Mask)ï¼Œå‰è€…åŸºäºç©ºé—´å¯†åº¦è€ŒéTokené¡ºåºçº¦æŸæ³¨æ„åŠ›ï¼Œåè€…åˆ™å…è®¸å¯¹è±¡Tokenç›´æ¥è·å–æŒ‡ä»¤ä¸Šä¸‹æ–‡ã€‚3D-SLIMæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–å¢åŠ é¢å¤–å‚æ•°ï¼Œå³å¯æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨3Då¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œä¸»æµLLMåŸºåº§ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†è§£ç å™¨æ©ç è®¾è®¡åœ¨3Dåœºæ™¯æ¨ç†ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02487v1",
      "published_date": "2025-12-02 07:22:36 UTC",
      "updated_date": "2025-12-02 07:22:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:58:37.170041+00:00"
    },
    {
      "arxiv_id": "2512.02485v1",
      "title": "UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making",
      "title_zh": "UCAgentsï¼šåŸºäºè§†è§‰è¯æ®é”šå®šçš„å¤šæ™ºèƒ½ä½“åŒ»ç–—å†³ç­–å•å‘æ”¶æ•›",
      "authors": [
        "Qianhan Feng",
        "Zhongzhen Huang",
        "Yakun Zhu",
        "Xiaofan Zhang",
        "Qi Dou"
      ],
      "abstract": "Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UCAgentsï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨åŒ»ç–—è¯Šæ–­ä¸­å‡ºç°çš„æ¨ç†è„±èŠ‚(reasoning detachment)é—®é¢˜ï¼Œå³è¯­è¨€è§£é‡Šä¸è§†è§‰è¯æ®ä¸ä¸€è‡´çš„ç°è±¡ã€‚å—ä¸´åºŠå·¥ä½œæµå¯å‘ï¼ŒUCAgentsé€šè¿‡ç»“æ„åŒ–çš„è¯æ®å®¡æ ¸(evidence auditing)å¼ºåˆ¶æ‰§è¡Œå•å‘æ”¶æ•›(unidirectional convergence)ï¼Œç¦æ­¢æ™ºèƒ½ä½“æ”¹å˜ç«‹åœºå¹¶é™åˆ¶å…¶äº¤äº’ä»¥è¿›è¡Œé’ˆå¯¹æ€§çš„è¯æ®éªŒè¯ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€è½®è¯¢é—®è®¨è®º(one-round inquiry discussion)æ¥æ­ç¤ºè§†è§‰ä¸æ–‡æœ¬å¤±é…çš„æ½œåœ¨é£é™©ï¼Œå¹¶åˆ©ç”¨ä¿¡æ¯è®ºæ–¹æ³•å½¢å¼åŒ–äº†å¯¹è§†è§‰æ¨¡ç³Šå’Œæ–‡æœ¬å™ªå£°çš„åŒé‡çº¦æŸã€‚åœ¨å››ä¸ªåŒ»ç–—VQAåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUCAgentsåœ¨PathVQAä¸Šå®ç°äº†71.3%çš„å‡†ç¡®ç‡ï¼Œæ¯”ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯æå‡äº†6.0%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æå‡å‡†ç¡®ç‡çš„åŒæ—¶å°†Tokenæˆæœ¬é™ä½äº†87.7%ï¼Œåœ¨æŒ–æ˜è§†è§‰è¯æ®ä¸é¿å…æ–‡æœ¬å¹²æ‰°ä¹‹é—´å–å¾—äº†ä¼˜å¼‚å¹³è¡¡ã€‚è¿™äº›ç»“æœè¯æ˜äº†UCAgentsåœ¨çœŸå®ä¸´åºŠéƒ¨ç½²ä¸­æ‰€å…·å¤‡çš„è¯Šæ–­å¯é æ€§ä¸è®¡ç®—æ•ˆç‡ï¼Œç›¸å…³ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02485v1",
      "published_date": "2025-12-02 07:20:21 UTC",
      "updated_date": "2025-12-02 07:20:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:00.674795+00:00"
    },
    {
      "arxiv_id": "2512.02474v1",
      "title": "Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation",
      "title_zh": "Q-BERT4Recï¼šé¢å‘å¤šæ¨¡æ€æ¨èçš„é‡åŒ–è¯­ä¹‰ ID è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Haofeng Huang",
        "Ling Gai"
      ],
      "abstract": "Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Q-BERT4Recï¼Œä¸€ç§ç»Ÿä¸€è¯­ä¹‰è¡¨ç¤ºä¸é‡åŒ–å»ºæ¨¡çš„å¤šæ¨¡æ€åºè´¯æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³BERT4Recç­‰ä¼ ç»Ÿæ¨¡å‹ä¾èµ–ç¦»æ•£IDä¸”ç¼ºä¹å¤šæ¨¡æ€è¯­ä¹‰ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡è·¨æ¨¡æ€è¯­ä¹‰æ³¨å…¥(cross-modal semantic injection)åˆ©ç”¨åŠ¨æ€Transformerèåˆæ–‡æœ¬ã€è§†è§‰å’Œç»“æ„ç‰¹å¾ï¼›å…¶æ¬¡é‡‡ç”¨æ®‹å·®å‘é‡é‡åŒ–(residual vector quantization)å°†èåˆè¡¨ç¤ºç¦»æ•£åŒ–ä¸ºå…·æœ‰è¯­ä¹‰çš„Tokenï¼›æœ€åé€šè¿‡åŒ…å«è·¨åº¦ã€å°¾éƒ¨å’Œå¤šåŒºåŸŸæ©ç çš„å¤šæ©ç é¢„è®­ç»ƒä¸å¾®è°ƒ(multi-mask pretraining and fine-tuning)ç­–ç•¥å¢å¼ºåºåˆ—ç†è§£èƒ½åŠ›ã€‚åœ¨AmazonåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQ-BERT4Recæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç»“æœå……åˆ†éªŒè¯äº†è¯­ä¹‰æ ‡è®°åŒ–(semantic tokenization)åœ¨æå‡å¤šæ¨¡æ€åºè´¯æ¨èæ³›åŒ–æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Submitted to KDD2026",
      "pdf_url": "https://arxiv.org/pdf/2512.02474v1",
      "published_date": "2025-12-02 07:06:44 UTC",
      "updated_date": "2025-12-02 07:06:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:22.185652+00:00"
    },
    {
      "arxiv_id": "2512.02472v1",
      "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
      "title_zh": "æå°‘äººå·¥ç›‘ç£ä¸‹çš„å¼•å¯¼å¼è‡ªè¿›åŒ–å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Wenhao Yu",
        "Zhenwen Liang",
        "Chengsong Huang",
        "Kishan Panaganti",
        "Tianqing Fang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸»è¿›åŒ–å¤§è¯­è¨€æ¨¡å‹(Self-evolving LLMs)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°çš„æ¦‚å¿µåç§»(concept drift)å’Œå¤šæ ·æ€§å´©æºƒ(diversity collapse)ç­‰å¤±æ•ˆé—®é¢˜ï¼Œæå‡ºäº†åä¸º R-Few çš„å¼•å¯¼å¼è‡ªæˆ‘æ¼”åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†è‡ªåšå¼ˆæŒ‘æˆ˜è€…-æ±‚è§£å™¨(Self-Play Challenger-Solver)æ¨¡å¼ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ¥åœ°(in-context grounding)å’Œæ··åˆè®­ç»ƒ(mixed training)å°†è½»é‡åŒ–çš„äººç±»ç›‘ç£æœ‰æ•ˆåœ°èå…¥æ¨¡å‹è¿›åŒ–è¿‡ç¨‹ã€‚åœ¨è¿­ä»£è®­ç»ƒä¸­ï¼ŒChallenger åˆ©ç”¨å°‘é‡äººå·¥æ ‡æ³¨æ ·æœ¬å¼•å¯¼åˆæˆé—®é¢˜çš„ç”Ÿæˆï¼Œè€Œ Solver åˆ™åœ¨åŸºäºéš¾åº¦çš„åœ¨çº¿è¯¾ç¨‹å­¦ä¹ (curriculum learning)æœºåˆ¶ä¸‹ï¼Œå¯¹äººå·¥ä¸åˆæˆæ•°æ®è¿›è¡Œè”åˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒR-Few åœ¨æ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­ Qwen3-8B-Base æ¨¡å‹åœ¨æ•°å­¦ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äº R-Zero åŸºçº¿ã€‚æ­¤å¤–ï¼ŒR-Few åœ¨ä»…ä½¿ç”¨äºŒååˆ†ä¹‹ä¸€äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å³å¯è¿½å¹³å¤§è§„æ¨¡è®­ç»ƒçš„ General-Reasoner æ¨¡å‹ã€‚åˆ†æè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæŠ‘åˆ¶æ¨¡å‹å‘ä½ç†µè¡Œä¸ºæ”¶æ•›çš„è¶‹åŠ¿ï¼Œä¸ºæ„å»ºç¨³å®šã€å¯æ§ä¸”ä½æˆæœ¬çš„æ¨¡å‹ååŒè¿›åŒ–åŠ¨åŠ›å­¦æä¾›äº†é‡è¦æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02472v1",
      "published_date": "2025-12-02 07:06:11 UTC",
      "updated_date": "2025-12-02 07:06:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:13.785634+00:00"
    },
    {
      "arxiv_id": "2512.02471v1",
      "title": "scCluBench: Comprehensive Benchmarking of Clustering Algorithms for Single-Cell RNA Sequencing",
      "title_zh": "scCluBenchï¼šå•ç»†èƒ RNA æµ‹åºèšç±»ç®—æ³•çš„å…¨é¢åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ping Xu",
        "Zaitian Wang",
        "Zhirui Wang",
        "Pengjiang Li",
        "Jiajia Wang",
        "Ran Zhang",
        "Pengfei Wang",
        "Yuanchun Zhou"
      ],
      "abstract": "Cell clustering is crucial for uncovering cellular heterogeneity in single-cell RNA sequencing (scRNA-seq) data by identifying cell types and marker genes. Despite its importance, benchmarks for scRNA-seq clustering methods remain fragmented, often lacking standardized protocols and failing to incorporate recent advances in artificial intelligence. To fill these gaps, we present scCluBench, a comprehensive benchmark of clustering algorithms for scRNA-seq data. First, scCluBench provides 36 scRNA-seq datasets collected from diverse public sources, covering multiple tissues, which are uniformly processed and standardized to ensure consistency for systematic evaluation and downstream analyses. To evaluate performance, we collect and reproduce a range of scRNA-seq clustering methods, including traditional, deep learning-based, graph-based, and biological foundation models. We comprehensively evaluate each method both quantitatively and qualitatively, using core performance metrics as well as visualization analyses. Furthermore, we construct representative downstream biological tasks, such as marker gene identification and cell type annotation, to further assess the practical utility. scCluBench then investigates the performance differences and applicability boundaries of various clustering models across diverse analytical tasks, systematically assessing their robustness and scalability in real-world scenarios. Overall, scCluBench offers a standardized and user-friendly benchmark for scRNA-seq clustering, with curated datasets, unified evaluation protocols, and transparent analyses, facilitating informed method selection and providing valuable insights into model generalizability and application scope.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† scCluBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å•ç»†èƒ RNA æµ‹åº (scRNA-seq) èšç±»ç®—æ³•çš„å…¨é¢åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³å½“å‰è¯„ä¼°åè®®ä¸ç»Ÿä¸€åŠç¼ºä¹æœ€æ–°äººå·¥æ™ºèƒ½æ¨¡å‹é›†æˆçš„é—®é¢˜ã€‚å¹³å°æ”¶é›†å¹¶æ ‡å‡†åŒ–äº†æ¶µç›–å¤šç§ç»„ç»‡çš„ 36 ä¸ª scRNA-seq æ•°æ®é›†ï¼Œç¡®ä¿äº†ç³»ç»Ÿè¯„ä»·çš„ä¸€è‡´æ€§ã€‚scCluBench æ¶µç›–äº†ä¼ ç»Ÿç®—æ³•ã€æ·±åº¦å­¦ä¹  (Deep Learning)ã€å›¾è®º (Graph-based) ä»¥åŠç”Ÿç‰©åŸºç¡€æ¨¡å‹ (Foundation Models) ç­‰å¤šç§èšç±»æ–¹æ³•ï¼Œå¹¶ä»å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–ä¸¤ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡æ ‡è®°åŸºå› è¯†åˆ« (Marker Gene Identification) å’Œç»†èƒç±»å‹æ³¨é‡Š (Cell Type Annotation) ç­‰ä¸‹æ¸¸ç”Ÿç‰©å­¦ä»»åŠ¡ï¼Œæ·±å…¥æ¢è®¨äº†å„æ¨¡å‹çš„å®ç”¨æ€§ã€é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚scCluBench ä¸º scRNA-seq èšç±»æä¾›äº†æ ‡å‡†åŒ–çš„åŸºå‡†ã€ç»Ÿä¸€çš„è¯„ä¼°åè®®å’Œé€æ˜çš„åˆ†æç»“æœï¼Œä¸ºç ”ç©¶äººå‘˜åœ¨å®é™…åº”ç”¨ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ–¹æ³•æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02471v1",
      "published_date": "2025-12-02 07:04:38 UTC",
      "updated_date": "2025-12-02 07:04:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:18.507495+00:00"
    },
    {
      "arxiv_id": "2512.11827v2",
      "title": "Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?",
      "title_zh": "åˆ©ç”¨ ChatGPTã€Claude å’Œ Gemini è¯„ä¼°ç»¿åœ°å¸å¼•åŠ›ï¼šAI æ¨¡å‹èƒ½å¦åæ˜ äººç±»æ„ŸçŸ¥ï¼Ÿ",
      "authors": [
        "Milad Malekzadeh",
        "Magdalena Biernacka",
        "Elias Willberg",
        "Jussi Torkko",
        "Edyta Åaszkiewicz",
        "Tuuli Toivonen"
      ],
      "abstract": "Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åŒ…æ‹¬ ChatGPT GPT-4oã€Claude 3.5 Haiku å’Œ Gemini 2.0 Flash åˆ©ç”¨ Google Street View å›¾åƒè¯†åˆ«ç»¿åœ°å¸å¼•åŠ›å¹¶æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”æ³¢å…°ç½—å…¹å±…æ°‘å¯¹æ­£å¼å’Œéæ­£å¼ç»¿åœ°çš„è°ƒæŸ¥æ•°æ®ï¼Œç ”ç©¶å‘ç° AI åœ¨è¯†åˆ«â€œå¸å¼•äººçš„æ­£å¼ç»¿åœ°â€å’Œâ€œä¸å¸å¼•äººçš„éæ­£å¼ç»¿åœ°â€æ—¶ä¸äººç±»é«˜åº¦ä¸€è‡´ï¼Œä½†åœ¨â€œå¸å¼•äººçš„éæ­£å¼ç»¿åœ°â€å’Œâ€œä¸å¸å¼•äººçš„æ­£å¼ç»¿åœ°â€ä¸Šè¡¨ç°å‡ºè¾ƒä½çš„å¯¹é½åº¦ã€‚æ¨¡å‹è¯„ä¼°å¾€å¾€è¿‡åº¦å…³æ³¨ç¾å­¦å’Œè®¾è®¡ç‰¹å¾ï¼Œè€Œå¯¹äººç±»é‡è§†çš„å®‰å…¨æ„Ÿã€åŠŸèƒ½æ€§åŸºç¡€è®¾æ–½åŠæœ¬åœ°åŒ–å“è´¨ä»£è¡¨æ€§ä¸è¶³ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ MLLMs èƒ½å¤Ÿæ”¯æŒå¤§è§„æ¨¡çš„ç»¿åœ°é¢„è¯„ä¼°ï¼Œä½†åœ¨å¤æ‚çš„è§„åˆ’å®è·µä¸­ä»éœ€è¦äººç±»ç›‘ç£å’Œå‚ä¸å¼æ–¹æ³•çš„è¡¥å……ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº† AI åœ¨æ”¯æŒè€Œéå–ä»£èƒŒæ™¯æ•æ„Ÿå‹åŸå¸‚ç»¿åœ°è¯„ä¼°ä¸­çš„è§’è‰²ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11827v2",
      "published_date": "2025-12-02 07:01:37 UTC",
      "updated_date": "2025-12-16 07:43:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:37.566907+00:00"
    },
    {
      "arxiv_id": "2512.02465v1",
      "title": "TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links",
      "title_zh": "TabGRUï¼šä¸€ç§åŸºäºå•†ä¸šå¾®æ³¢é“¾è·¯çš„åŸå¸‚é™é›¨å¼ºåº¦ä¼°ç®—å¢å¼ºå‹è®¾è®¡",
      "authors": [
        "Xingwang Li",
        "Mengyun Chen",
        "Jiamou Liu",
        "Sijie Wang",
        "Shuanggen Jin",
        "Jafet C. M. Andersson",
        "Jonas Olsson",
        "Remco",
        "van de Beek",
        "Hai Victor Habi",
        "Congzheng Han"
      ],
      "abstract": "In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç‰©ç†æ¨¡å‹åœ¨åˆ©ç”¨å•†ä¸šå¾®æ³¢é“¾è·¯(Commercial Microwave Links, CMLs)è¿›è¡ŒåŸå¸‚é™é›¨å¼ºåº¦ä¼°è®¡æ—¶éš¾ä»¥åº”å¯¹ä¿¡å·å™ªå£°å’Œéçº¿æ€§è¡°å‡çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTabGRUçš„æ–°å‹æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡æ•´åˆTransformerå’ŒåŒå‘é—¨æ§å¾ªç¯å•å…ƒ(Bidirectional Gated Recurrent Unit, BiGRU)ï¼Œå®ç°äº†å¯¹CMLä¿¡å·ä¸­é•¿æœŸä¾èµ–å…³ç³»å’Œå±€éƒ¨åºåˆ—ç‰¹å¾çš„ååŒæ•æ‰ã€‚æ¨¡å‹è¿›ä¸€æ­¥å¼•å…¥äº†å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥(learnable positional embedding)å’Œæ³¨æ„åŠ›æ± åŒ–(attention pooling)æœºåˆ¶ï¼Œæ—¨åœ¨æå‡åŠ¨æ€ç‰¹å¾æå–çš„ç²¾åº¦ä¸æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨ç‘å…¸å“¥å¾·å ¡å…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒTabGRUåœ¨ä¸¤ä¸ªæµ‹è¯•ç«™ç‚¹çš„å†³å®šç³»æ•°(RÂ²)åˆ†åˆ«è¾¾åˆ°0.91å’Œ0.96ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯å®ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ç‰©ç†æ¨¡å‹ï¼ŒTabGRUåœ¨ç»´æŒé«˜ç²¾åº¦çš„åŒæ—¶ï¼Œæœ‰æ•ˆç¼“è§£äº†é™é›¨å³°å€¼æœŸé—´å¸¸è§çš„ä¸¥é‡é«˜ä¼°é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºå¼¹æ€§æ™ºæ…§åŸå¸‚ä¸­çš„é«˜åˆ†è¾¨ç‡åŸå¸‚é™é›¨ç›‘æµ‹æä¾›äº†ä¸€ç§é²æ£’ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02465v1",
      "published_date": "2025-12-02 06:50:50 UTC",
      "updated_date": "2025-12-02 06:50:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:50.675159+00:00"
    },
    {
      "arxiv_id": "2512.02450v1",
      "title": "HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild",
      "title_zh": "HouseLayout3Dï¼šé’ˆå¯¹ç°å®åœºæ™¯ 3D å¸ƒå±€ä¼°è®¡çš„åŸºå‡†ä¸å…è®­ç»ƒåŸºçº¿",
      "authors": [
        "Valentin Bieri",
        "Marie-Julie Rakotosaona",
        "Keisuke Tateno",
        "Francis Engelmann",
        "Leonidas Guibas"
      ],
      "abstract": "Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰ 3D layout estimation æ¨¡å‹åœ¨å¤„ç†å¤§å‹å¤šå±‚å»ºç­‘æ—¶å› ç¼ºä¹å…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è€Œè¡¨ç°å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† HouseLayout3D è¿™ä¸€é’ˆå¯¹çœŸå®ä¸–ç•Œå…¨å»ºç­‘å°ºåº¦å¸ƒå±€ä¼°è®¡çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥ benchmark æ¶µç›–äº†å¤šæ¥¼å±‚åŠå¤æ‚çš„å»ºç­‘ç©ºé—´ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹å¿…é¡»åˆ†å±‚å¤„ç†ä¸”éš¾ä»¥è¯†åˆ«æ¥¼æ¢¯ç­‰è·¨æ¥¼å±‚ç»“æ„çš„é—®é¢˜ã€‚ç ”ç©¶åŒæ—¶å¼•å…¥äº† MultiFloor3Dï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ training-free åŸºå‡†æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å…ˆè¿›çš„åœºæ™¯ç†è§£æŠ€æœ¯å®ç°äº†é«˜æ•ˆçš„å¸ƒå±€æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMultiFloor3D åœ¨æ–°åŸºå‡†å’Œç°æœ‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç›®å‰çš„ 3D layout estimation æ¨¡å‹ï¼Œä¸ºæœªæ¥åœ¨å¤æ‚å»ºç­‘ç¯å¢ƒä¸‹çš„ 3D åœºæ™¯ç†è§£ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025 (Datasets and Benchmarks Track) Project Page: https://houselayout3d.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.02450v1",
      "published_date": "2025-12-02 06:18:08 UTC",
      "updated_date": "2025-12-02 06:18:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:59:56.868989+00:00"
    },
    {
      "arxiv_id": "2512.02445v1",
      "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents",
      "title_zh": "å½“æ‹’ç»å¤±æ•ˆï¼šé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å®‰å…¨æœºåˆ¶çš„ä¸ç¨³å®šæ€§",
      "authors": [
        "Tsimur Hadeliya",
        "Mohammad Ali Jauhar",
        "Nidhi Sakpal",
        "Diogo Cruz"
      ],
      "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†é•¿ä¸Šä¸‹æ–‡ï¼ˆLong-Contextï¼‰å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å®‰å…¨æ€§ä¸æ€§èƒ½ä¸ç¨³å®šæ€§ã€‚ä½œè€…å‘ç°ï¼ŒLLMæ™ºèƒ½ä½“å¯¹ä¸Šä¸‹æ–‡çš„é•¿åº¦ã€ç±»å‹å’Œä½ç½®é«˜åº¦æ•æ„Ÿï¼Œå…¶æ‹’ç»æ‰§è¡Œæœ‰å®³è¯·æ±‚çš„æœºåˆ¶ï¼ˆRefusal mechanismsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºä¸å¯é¢„æµ‹ä¸”ä¸ä¸€è‡´çš„æ³¢åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä¾¿å…·å¤‡1M-2Mä»¤ç‰Œï¼ˆtokensï¼‰å®¹é‡çš„æ¨¡å‹åœ¨100Kä»¤ç‰Œå¤„ä¾¿å‡ºç°ä¸¥é‡é€€åŒ–ï¼Œè‰¯æ€§ä¸æœ‰å®³ä»»åŠ¡çš„æ€§èƒ½é™å¹…å‡è¶…è¿‡50%ã€‚ç‰¹åˆ«æ˜¯åœ¨å®‰å…¨ç»´åº¦ä¸Šï¼Œä¸åŒæ¨¡å‹çš„æ‹’ç»ç‡éšé•¿åº¦å¢åŠ å‘ˆç°å‡ºæˆªç„¶ä¸åŒçš„å˜åŠ¨è¶‹åŠ¿ï¼Œä¾‹å¦‚GPT-4.1-nanoçš„æ‹’ç»ç‡ä¸Šå‡ï¼Œè€ŒGrok 4 Fastå´åœ¨200Kä»¤ç‰Œæ—¶å¤§å¹…ä¸‹é™ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†é•¿ä¸Šä¸‹æ–‡æ™ºèƒ½ä½“åœ¨æ“ä½œä¸­å­˜åœ¨çš„æ˜¾è‘—å®‰å…¨éšæ‚£ï¼Œå¹¶å¼ºè°ƒäº†ç°æœ‰LLMå®‰å…¨è¯„ä¼°æŒ‡æ ‡åœ¨åº”å¯¹å¤šæ­¥é•¿ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ™ºèƒ½ä½“åœ¨èƒ½åŠ›å’Œå®‰å…¨æ€§ä¸Šçš„è¡¨ç°ä¸æ­¤å‰å¯¹æ ‡å‡†LLMçš„è¯„ä¼°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒäºŸéœ€å»ºç«‹æ–°çš„é•¿ä¸Šä¸‹æ–‡å®‰å…¨è¯„ä¼°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 11 figures. Accepted at AAAI 2026 TrustAgent Workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.02445v1",
      "published_date": "2025-12-02 06:12:02 UTC",
      "updated_date": "2025-12-02 06:12:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:00:16.066299+00:00"
    },
    {
      "arxiv_id": "2512.03109v1",
      "title": "E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing",
      "title_zh": "E-valuatorï¼šåŸºäºåºè´¯å‡è®¾æ£€éªŒçš„å¯é æ™ºèƒ½ä½“éªŒè¯å™¨",
      "authors": [
        "Shuvom Sadhuka",
        "Drew Prinster",
        "Clara Fannjiang",
        "Gabriele Scalia",
        "Aviv Regev",
        "Hanchen Wang"
      ],
      "abstract": "Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†e-valuatorï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ™ºèƒ½ä½“ç³»ç»ŸéªŒè¯å™¨ï¼ˆå¦‚LLM judgeså’Œprocess-reward modelsï¼‰ç¼ºä¹å†³ç­–æ­£ç¡®æ€§ä¿è¯çš„æ–°æ–¹æ³•ã€‚e-valuatorå°†åŒºåˆ†æˆåŠŸä¸å¤±è´¥è½¨è¿¹çš„ä»»åŠ¡å»ºæ¨¡ä¸ºåºè´¯å‡è®¾æ£€éªŒ(sequential hypothesis testing)é—®é¢˜ï¼Œä»è€Œå°†é»‘ç›’éªŒè¯å™¨åˆ†æ•°è½¬æ¢ä¸ºå…·æœ‰å¯è¯æ˜è¯¯æŠ¥ç‡(false alarm rates)æ§åˆ¶çš„å†³ç­–è§„åˆ™ã€‚è¯¥æ¡†æ¶åŸºäºe-processeså·¥å…·å®ç°ï¼Œæ”¯æŒå¯¹æ™ºèƒ½ä½“åŠ¨ä½œåºåˆ—è¿›è¡ŒæŒç»­çš„åœ¨çº¿ç›‘æµ‹ï¼Œå¹¶åœ¨ä»»æ„é•¿åº¦ä¸‹ä¿æŒç»Ÿè®¡æœ‰æ•ˆæ€§ã€‚å®éªŒåœ¨6ä¸ªæ•°æ®é›†å’Œ3ç§æ™ºèƒ½ä½“ä¸Šè¯æ˜ï¼Œe-valuatorç›¸æ¯”ä¼ ç»Ÿç­–ç•¥å…·æœ‰æ›´å¼ºçš„ç»Ÿè®¡åŠŸæ•ˆå’Œæ›´ä¼˜çš„è¯¯æŠ¥ç‡æ§åˆ¶èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«å¹¶æå‰ç»ˆæ­¢é”™è¯¯çš„è½¨è¿¹ä»¥èŠ‚çœtokenæ¶ˆè€—ï¼Œä¸ºéƒ¨ç½²æ›´å¯é çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªè½»é‡åŒ–ä¸”æ¨¡å‹æ— å…³çš„ç»Ÿè®¡ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03109v1",
      "published_date": "2025-12-02 05:59:18 UTC",
      "updated_date": "2025-12-02 05:59:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:00:15.283692+00:00"
    },
    {
      "arxiv_id": "2512.02438v1",
      "title": "Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources",
      "title_zh": "æœ‰é™è®¡ç®—èµ„æºä¸‹åŸºäºåŠ¨é‡è‡ªè’¸é¦çš„åŒ»å­¦è§†è§‰-è¯­è¨€é¢„è®­ç»ƒå¢å¼º",
      "authors": [
        "Phuc Pham",
        "Nhu Pham",
        "Ngoc Quoc Ly"
      ],
      "abstract": "In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)é¢„è®­ç»ƒä¸­é¢ä¸´çš„é«˜ç®—åŠ›éœ€æ±‚å’Œæ ‡æ³¨æ•°æ®åŒ®ä¹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŠ¨é‡è‡ªè’¸é¦(Momentum Self-Distillation)çš„ä¼˜åŒ–æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨åŠ¨é‡æœºåˆ¶å¢å¼ºå¤šæ¨¡æ€å­¦ä¹ è¿‡ç¨‹ä¸­çš„çŸ¥è¯†æå–ï¼Œå¹¶å·§å¦™åœ°å°†åŠ¨é‡æœºåˆ¶ä¸æ¢¯åº¦ç´¯ç§¯(Gradient Accumulation)ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•åœ¨ä¸å¢åŠ èµ„æºæ¶ˆè€—çš„å‰æä¸‹æ‰©å¤§äº†æœ‰æ•ˆæ‰¹é‡å¤§å°(Effective Batch Size)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨é›¶æ ·æœ¬(Zero-shot)åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†ä¸å½“å‰æœ€å…ˆè¿›æŠ€æœ¯(SOTA)ç›¸å½“çš„æ°´å¹³ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è‡ªé€‚åº”(Few-shot Adaptation)ä¸­å®ç°äº†è¶…è¿‡90%çš„AUC-ROCï¼Œæ£€ç´¢ä»»åŠ¡æ€§èƒ½ä¹Ÿæå‡äº†2-3%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨å•å¼ GPUä¸Šå®ç°é«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ çš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºå—é™æœºæ„è¿›è¡Œé«˜æ€§èƒ½åŒ»ç–—AIç ”ç©¶é™ä½äº†é—¨æ§›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.02438v1",
      "published_date": "2025-12-02 05:53:51 UTC",
      "updated_date": "2025-12-02 05:53:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:01:39.996912+00:00"
    },
    {
      "arxiv_id": "2512.02437v1",
      "title": "LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework",
      "title_zh": "LightHCGï¼šåŸºäº HSIC è§£è€¦çš„è½»é‡çº§é«˜æ•ˆå› æœé’å…‰çœ¼æ£€æµ‹æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Daeyoung Kim"
      ],
      "abstract": "As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰é’å…‰çœ¼æ£€æµ‹AIæ¨¡å‹åœ¨å¯é æ€§ã€å‚æ•°å†—ä½™åŠç¼ºä¹å› æœåˆ†æèƒ½åŠ›ç­‰æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLightHCGçš„è½»é‡çº§å› æœè¡¨ç¤ºé©±åŠ¨æ¡†æ¶ã€‚è¯¥æ¨¡å‹åŸºäºå·ç§¯å˜åˆ†è‡ªç¼–ç å™¨(Convolutional VAE)æ„å»ºæ½œåœ¨è¡¨ç¤ºï¼Œå¹¶ç»“åˆäº†åŸºäºHSICçš„æ½œåœ¨ç©ºé—´è§£è€¦(disentanglement)å’Œå›¾è‡ªç¼–ç å™¨(Graph Autoencoder)çš„æ— ç›‘ç£å› æœè¡¨ç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œæ—¨åœ¨æ•æ‰è§†ç¥ç»åŒºåŸŸå†…ç‰©ç†å› ç´ é—´çš„çœŸå®å› æœå…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸InceptionV3ã€MobileNetV2æˆ–VGG16ç­‰å…ˆè¿›è§†è§‰æ¨¡å‹ç›¸æ¯”ï¼ŒLightHCGåœ¨å®ç°é«˜ç²¾åº¦åˆ†ç±»çš„åŒæ—¶ï¼Œæ¨¡å‹æƒé‡æ˜¾è‘—å‡å°‘äº†93%è‡³99%ã€‚è¿™ç§æåº¦è½»é‡åŒ–çš„è®¾è®¡ä¸ä»…æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œè¿˜é€šè¿‡å› æœè¡¨ç¤ºå¢å¼ºäº†AIé©±åŠ¨çš„å¹²é¢„åˆ†æå’Œä¸´åºŠæ¨¡æ‹Ÿçš„å¯èƒ½æ€§ï¼Œä¸ºè¾…åŠ©ä¸´åºŠè¯Šæ–­æä¾›äº†æ›´å…·å¯é æ€§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02437v1",
      "published_date": "2025-12-02 05:52:15 UTC",
      "updated_date": "2025-12-02 05:52:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:00:16.787469+00:00"
    },
    {
      "arxiv_id": "2512.02436v1",
      "title": "Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets",
      "title_zh": "è¯­ä¹‰äº¤æ˜“ï¼šé¢å‘é¢„æµ‹å¸‚åœºèšç±»ä¸å…³ç³»å‘ç°çš„æ™ºèƒ½ä½“ AI",
      "authors": [
        "Agostino Capponi",
        "Alfio Gliozzo",
        "Brian Zhu"
      ],
      "abstract": "Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„æµ‹å¸‚åœº(Prediction markets)ä¸­å­˜åœ¨çš„å¸‚åœºç¢ç‰‡åŒ–ã€åˆåŒé‡å ä»¥åŠéšå«çŸ›ç›¾ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“AI(Agentic AI)çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€ç†è§£(Natural-language understanding)æŠ€æœ¯å¯¹åˆåŒæ–‡æœ¬åŠå…ƒæ•°æ®è¿›è¡Œæ·±å…¥åˆ†æã€‚è¯¥æµç¨‹é¦–å…ˆå°†å¸‚åœºè‡ªåŠ¨èšç±»ä¸ºè¿è´¯çš„ä¸»é¢˜ç»„ï¼Œéšåè¯†åˆ«å‡ºåŒä¸€èšç±»å†…éƒ¨å…·æœ‰å¼ºä¾èµ–å…³ç³»çš„å¸‚åœºå¯¹ï¼ŒåŒ…æ‹¬åŒå‘ç›¸å…³å’Œåå‘ç›¸å…³ã€‚é€šè¿‡åœ¨Polymarketçš„å†å²æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°è¯¥æ™ºèƒ½ä½“åœ¨å…³ç³»é¢„æµ‹ä¸Šè¾¾åˆ°äº†60-70%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒåŸºäºè¿™äº›å‘ç°å»ºç«‹çš„äº¤æ˜“ç­–ç•¥åœ¨ä¸ºæœŸä¸€å‘¨çš„å‘¨æœŸå†…å®ç°äº†çº¦20%çš„å¹³å‡æ”¶ç›Šã€‚è¿™é¡¹å·¥ä½œå……åˆ†å±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹(Large language models)æ­ç¤ºé¢„æµ‹å¸‚åœºä¸­æ½œè—è¯­ä¹‰ç»“æ„çš„èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–äº¤æ˜“å’Œå…³ç³»å‘ç°æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02436v1",
      "published_date": "2025-12-02 05:45:48 UTC",
      "updated_date": "2025-12-02 05:45:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:00:34.273210+00:00"
    },
    {
      "arxiv_id": "2512.02425v1",
      "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
      "title_zh": "WorldMMï¼šé¢å‘é•¿è§†é¢‘æ¨ç†çš„åŠ¨æ€å¤šæ¨¡æ€è®°å¿†æ™ºèƒ½ä½“",
      "authors": [
        "Woongyeong Yeo",
        "Kangsan Kim",
        "Jaehong Yoon",
        "Sung Ju Hwang"
      ],
      "abstract": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(video LLMs)åœ¨å¤„ç†æ•°å°æ—¶ç”šè‡³æ•°å¤©é•¿è§†é¢‘æ—¶é¢ä¸´çš„ä¸Šä¸‹æ–‡å®¹é‡å—é™ã€è§†è§‰ç»†èŠ‚ä¸¢å¤±ä»¥åŠè¿‡åº¦ä¾èµ–æ–‡æœ¬æ‘˜è¦ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†WorldMMã€‚WorldMMæ˜¯ä¸€ä¸ªåŠ¨æ€å¤šæ¨¡æ€è®°å¿†æ™ºèƒ½ä½“ï¼Œé€šè¿‡æ„å»ºå¹¶æ£€ç´¢ä¸‰ç§äº’è¡¥çš„è®°å¿†åº“æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–äº†æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§è¡¨å¾ã€‚å…¶ä¸­ï¼Œæƒ…èŠ‚è®°å¿†(episodic memory)åœ¨å¤šä¸ªæ—¶é—´å°ºåº¦ä¸Šå¯¹äº‹å®äº‹ä»¶è¿›è¡Œç´¢å¼•ï¼Œè¯­ä¹‰è®°å¿†(semantic memory)æŒç»­æ›´æ–°é«˜å±‚æ¦‚å¿µçŸ¥è¯†ï¼Œè€Œè§†è§‰è®°å¿†(visual memory)åˆ™ä¿ç•™äº†åœºæ™¯çš„è¯¦ç»†è§†è§‰ä¿¡æ¯ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è‡ªé€‚åº”æ£€ç´¢æ™ºèƒ½ä½“(adaptive retrieval agent)æ ¹æ®æŸ¥è¯¢å†…å®¹è¿­ä»£é€‰æ‹©æœ€ç›¸å…³çš„è®°å¿†æ¥æºå’Œæ—¶é—´ç²’åº¦ï¼Œç›´è‡³æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWorldMMåœ¨äº”ä¸ªé•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•(SOTA)å®ç°äº†å¹³å‡8.4%çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆå¤šæ¨¡æ€è®°å¿†ä¸è‡ªé€‚åº”æ£€ç´¢æœºåˆ¶åœ¨è§£å†³å¤æ‚é•¿è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page : https://worldmm.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.02425v1",
      "published_date": "2025-12-02 05:14:52 UTC",
      "updated_date": "2025-12-02 05:14:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:00:33.071640+00:00"
    },
    {
      "arxiv_id": "2512.02422v1",
      "title": "Quantum feature encoding optimization",
      "title_zh": "é‡å­ç‰¹å¾ç¼–ç ä¼˜åŒ–",
      "authors": [
        "Tommaso Fioravanti",
        "Brian Quanz",
        "Gabriele Agliardi",
        "Edgar Andres Ruiz Guzman",
        "GinÃ©s Carrascal",
        "Jae-Eun Park"
      ],
      "abstract": "Quantum Machine Learning (QML) holds the promise of enhancing machine learning modeling in terms of both complexity and accuracy. A key challenge in this domain is the encoding of input data, which plays a pivotal role in determining the performance of QML models. In this work, we tackle a largely unaddressed aspect of encoding that is unique to QML modeling -- rather than adjusting the ansatz used for encoding, we consider adjusting how data is conveyed to the ansatz. We specifically implement QML pipelines that leverage classical data manipulation (i.e., ordering, selecting, and weighting features) as a preprocessing step, and evaluate if these aspects of encoding can have a significant impact on QML model performance, and if they can be effectively optimized to improve performance. Our experimental results, applied across a wide variety of data sets, ansatz, and circuit sizes, with a representative QML approach, demonstrate that by optimizing how features are encoded in an ansatz we can substantially and consistently improve the performance of QML models, making a compelling case for integrating these techniques in future QML applications. Finally we demonstrate the practical feasibility of this approach by running it using real quantum hardware with 100 qubit circuits and successfully achieving improved QML modeling performance in this case as well.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡å­æœºå™¨å­¦ä¹  (Quantum Machine Learning, QML) ä¸­è‡³å…³é‡è¦ä½†å¸¸è¢«å¿½è§†çš„æ•°æ®ç¼–ç ä¼˜åŒ–é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ€è·¯ï¼Œå³é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œç»å…¸é¢„å¤„ç†ï¼ˆåŒ…æ‹¬ç‰¹å¾æ’åºã€é€‰æ‹©å’ŒåŠ æƒï¼‰æ¥è°ƒæ•´æ•°æ®ä¼ è¾“è‡³ ansatz çš„æ–¹å¼ï¼Œè€Œéä»…ä»…ä¿®æ”¹ ansatz ç»“æ„ã€‚é€šè¿‡åœ¨å¤šç§æ•°æ®é›†ã€ansatz å’Œç”µè·¯è§„æ¨¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶è¯æ˜ä¼˜åŒ–ç‰¹å¾ç¼–ç æ–¹å¼èƒ½æ˜¾è‘—ä¸”ç¨³å®šåœ°æå‡ QML æ¨¡å‹çš„å‡†ç¡®æ€§ä¸è¡¨ç°ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥åœ¨æ‹¥æœ‰100ä¸ªé‡å­æ¯”ç‰¹çš„çœŸå®é‡å­ç¡¬ä»¶ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é‡å­ç”µè·¯æ—¶ä¾ç„¶å…·æœ‰æé«˜çš„å¯è¡Œæ€§ã€‚è¿™ä¸€å‘ç°ä¸ºé‡å­ç‰¹å¾ç¼–ç æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶å¼ºè°ƒäº†åœ¨æœªæ¥ QML åº”ç”¨ä¸­é›†æˆæ­¤ç±»ä¼˜åŒ–æŠ€æœ¯çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02422v1",
      "published_date": "2025-12-02 05:07:32 UTC",
      "updated_date": "2025-12-02 05:07:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:01:00.965824+00:00"
    },
    {
      "arxiv_id": "2512.02419v1",
      "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation",
      "title_zh": "è„‘ä¸äººå·¥æ™ºèƒ½çš„è¶‹åŒï¼šé¢å‘é€šç”¨è®¡ç®—çš„é¢„æµ‹æ€§ä¸ç”Ÿæˆæ€§ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Shogo Ohmae",
        "Keiko Ohmae"
      ],
      "abstract": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»é¢„æµ‹æ€§å’Œç”Ÿæˆæ€§çš„ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰è§†è§’å‡ºå‘ï¼Œæ¢è®¨äº†äººè„‘ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨é€šç”¨è®¡ç®—ï¼ˆgeneral-purpose computationï¼‰é¢†åŸŸçš„èåˆã€‚æ–‡ç« è¯†åˆ«äº†å¤§è„‘æ–°çš®å±‚ï¼ˆneocortexï¼‰ã€å°è„‘ï¼ˆcerebellumï¼‰ä¸ç°ä»£åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention-basedï¼‰çš„AIç³»ç»Ÿä¹‹é—´å…±äº«çš„è®¡ç®—æœºåˆ¶ï¼Œå³é€šè¿‡è¿‡å»è¾“å…¥é¢„æµ‹æœªæ¥äº‹ä»¶å¹¶åˆ©ç”¨é¢„æµ‹è¯¯å·®å­¦ä¹ ï¼ˆprediction-error learningï¼‰æ„å»ºå†…éƒ¨æ¨¡å‹ã€‚è¿™äº›é¢„æµ‹ä¸–ç•Œæ¨¡å‹è¢«é‡æ–°åˆ©ç”¨äºæ„Ÿè§‰å¤„ç†ä¸­çš„ç†è§£å’Œè¿åŠ¨å¤„ç†ä¸­çš„ç”Ÿæˆï¼Œä½¿ç”Ÿç‰©ç³»ç»Ÿèƒ½å¤Ÿå®ç°å¤šé¢†åŸŸåŠŸèƒ½å’Œç±»äººè‡ªé€‚åº”æ™ºèƒ½ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡ç”µè·¯ç»“æ„ç›¸å¯¹ç»Ÿä¸€ï¼Œè¿™äº›åœ¨ç”Ÿç‰©å’Œäººå·¥ç³»ç»Ÿä¸­ç‹¬ç«‹æ¼”åŒ–å‡ºçš„ç›¸ä¼¼èŒƒå¼æ„æˆäº†å®ç°é«˜é˜¶æ™ºèƒ½çš„æ ¸å¿ƒè®¡ç®—åŸºç¡€ã€‚è¿™ä¸€ç†è®ºæ´å¯ŸæˆåŠŸæ¡¥æ¥äº†ç¥ç»ç§‘å­¦ä¸äººå·¥æ™ºèƒ½ï¼Œæ·±åŒ–äº†å¯¹æ™ºèƒ½è®¡ç®—æœ¬è´¨çš„ç†è§£ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "22 pages, 4 figures. Related to our earlier preprint \"The brain versus AI\" (arXiv:2411.16075) but a distinct article. The earlier work surveyed broad brain-AI parallels; here we focus on world-model-based computation and convergent evolution between the brain and AI, especially large language models",
      "pdf_url": "https://arxiv.org/pdf/2512.02419v1",
      "published_date": "2025-12-02 05:03:14 UTC",
      "updated_date": "2025-12-02 05:03:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:01:11.691645+00:00"
    },
    {
      "arxiv_id": "2512.02417v1",
      "title": "Vehicle Dynamics Embedded World Models for Autonomous Driving",
      "title_zh": "é¢å‘è‡ªåŠ¨é©¾é©¶çš„è½¦è¾†åŠ¨åŠ›å­¦åµŒå…¥å¼ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Huiqian Li",
        "Wei Pan",
        "Haodong Zhang",
        "Jin Huang",
        "Zhihua Zhong"
      ],
      "abstract": "World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­çš„ä¸–ç•Œæ¨¡å‹(World models)è¿›è¡Œäº†æ”¹è¿›ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å­¦ä¹ è‡ªè½¦åŠ¨åŠ›å­¦(ego-vehicle dynamics)ä¸ç¯å¢ƒæ¼”åŒ–åŠ¨æ€(environmental transition dynamics)æ—¶å­˜åœ¨çš„è€¦åˆé—®é¢˜ï¼Œä»¥åŠç”±æ­¤å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹å’Œå¯¹è½¦è¾†å‚æ•°å˜åŒ–é²æ£’æ€§ä¸è¶³ç­‰ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åµŒå…¥è½¦è¾†åŠ¨åŠ›å­¦çš„Dreameræ–¹æ³•(Vehicle Dynamics embedded Dreamer, VDD)ï¼Œé€šè¿‡å°†è‡ªè½¦åŠ¨åŠ›å­¦å»ºæ¨¡ä¸ç¯å¢ƒåŠ¨æ€è§£è€¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–åˆ°å…·æœ‰ä¸åŒå‚æ•°çš„å¤šç§è½¦è¾†ä¸Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†éƒ¨ç½²æ—¶ç­–ç•¥è°ƒæ•´(Policy Adjustment during Deployment, PAD)å’Œè®­ç»ƒæ—¶ç­–ç•¥å¢å¼º(Policy Augmentation during Training, PAT)ä¸¤ç§ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ‰€å­¦ç­–ç•¥çš„é²æ£’æ€§ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é©¾é©¶æ€§èƒ½ä»¥åŠå¯¹è½¦è¾†åŠ¨åŠ›å­¦å˜åŒ–çš„é²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02417v1",
      "published_date": "2025-12-02 04:57:52 UTC",
      "updated_date": "2025-12-02 04:57:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:10.264412+00:00"
    },
    {
      "arxiv_id": "2512.07888v1",
      "title": "Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification",
      "title_zh": "é¢å‘ä¸å¹³è¡¡å‡½æ•°å‹æ•°æ®åˆ†ç±»çš„è‡ªé€‚åº”ä»£ä»·æ•æ„Ÿåˆ†è£‚å‡½æ•°å‹éšæœºæ£®æ—",
      "authors": [
        "Fahad Mostafa",
        "Hafiz Khan"
      ],
      "abstract": "Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‡½æ•°å‹æ•°æ®ï¼ˆfunctional dataï¼‰åœ¨ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡æƒ…å†µä¸‹çš„åˆ†ç±»æŒ‘æˆ˜ï¼Œæå‡ºäº†Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS) è¿™ä¸€æ–°å‹é›†æˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨basis expansionså’ŒFunctional Principal Component Analysis (FPCA) å¯¹æ›²çº¿è¿›è¡Œé«˜æ•ˆè¡¨å¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä½ç»´å‡½æ•°ç‰¹å¾ä¸Šè¿›è¡Œè¿ç®—ã€‚ä¸ºäº†åº”å¯¹ä¸å¹³è¡¡é—®é¢˜ï¼ŒFRF-ACSåœ¨èŠ‚ç‚¹çº§åˆ«å¼•å…¥äº†åŠ¨æ€æˆæœ¬æ•æ„Ÿåˆ’åˆ†å‡†åˆ™ï¼Œå¹¶ç»“åˆäº†é›†æˆfunctional SMOTEä¸weighted bootstrappingçš„æ··åˆé‡‡æ ·ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨æ›²çº¿ç‰¹æœ‰çš„ç›¸ä¼¼æ€§åº¦é‡å–ä»£ä¼ ç»ŸEuclideanåº¦é‡ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™å‡½æ•°ç‰¹å¾ã€‚åœ¨ç”Ÿç‰©åŒ»å­¦ä¿¡å·å’Œä¼ æ„Ÿå™¨è½¨è¿¹ç­‰åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFRF-ACSæ˜¾è‘—æå‡äº†å°‘æ•°ç±»çš„å¬å›ç‡åŠæ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ºå°‘æ•°ç±»æ£€æµ‹è‡³å…³é‡è¦çš„é«˜ç»´å‡½æ•°å‹æ•°æ®åˆ†æé¢†åŸŸæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å…·å¯è§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.AP",
        "stat.CO"
      ],
      "primary_category": "stat.ML",
      "comment": "23 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.07888v1",
      "published_date": "2025-12-02 04:57:51 UTC",
      "updated_date": "2025-12-02 04:57:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:13.586905+00:00"
    },
    {
      "arxiv_id": "2512.02413v2",
      "title": "Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach for Precise Wall Segmentation",
      "title_zh": "å¢å¼ºå¹³é¢å›¾è¯†åˆ«ï¼šä¸€ç§ç”¨äºç²¾å‡†å¢™ä½“åˆ†å‰²çš„æ··åˆ Mix-Transformer ä¸ U-Net æ–¹æ³•",
      "authors": [
        "Dmitriy Parashchuk",
        "Alexey Kapshitskiy",
        "Yuriy Karyakin"
      ],
      "abstract": "Automatic 3D reconstruction of indoor spaces from 2D floor plans necessitates high-precision semantic segmentation of structural elements, particularly walls. However, existing methods often struggle with detecting thin structures and maintaining geometric precision. This study introduces MitUNet, a hybrid neural network combining a Mix-Transformer encoder and a U-Net decoder enhanced with spatial and channel attention blocks. Our approach, optimized with the Tversky loss function, achieves a balance between precision and recall, ensuring accurate boundary recovery. Experiments on the CubiCasa5k dataset and a proprietary regional dataset demonstrate MitUNet's superiority in generating structurally correct masks with high boundary accuracy, outperforming standard models. This tool provides a robust foundation for automated 3D reconstruction pipelines. To ensure reproducibility and facilitate future research, the source code and the proprietary regional dataset are publicly available at https://github.com/aliasstudio/mitunet and https://doi.org/10.5281/zenodo.17871079 respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MitUNetï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº† Mix-Transformer ç¼–ç å™¨å’Œ U-Net è§£ç å™¨çš„æ··åˆç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨æå‡å®¤å†… 2D æˆ·å‹å›¾è‡ªåŠ¨ 3D é‡å»ºè¿‡ç¨‹ä¸­çš„å¢™ä½“è¯­ä¹‰åˆ†å‰²ç²¾åº¦ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹ç»†å¾®ç»“æ„å’Œç»´æŒå‡ ä½•ç²¾ç¡®åº¦æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥æ¨¡å‹é€šè¿‡é›†æˆç©ºé—´å’Œé€šé“æ³¨æ„åŠ›æ¨¡å— (spatial and channel attention blocks) å¹¶é‡‡ç”¨ Tversky loss å‡½æ•°ä¼˜åŒ–ï¼Œæœ‰æ•ˆå®ç°äº†è¾¹ç•Œæ¢å¤çš„ç²¾ç¡®å¹³è¡¡ã€‚åœ¨ CubiCasa5k æ•°æ®é›†åŠä¸“ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMitUNet åœ¨ç”Ÿæˆç»“æ„æ­£ç¡®çš„æ©ç å’Œè¾¹ç•Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ– 3D é‡å»ºå·¥ä½œæµå¥ å®šäº†åšå®åŸºç¡€ï¼Œå¹¶å…¬å¼€äº†æºä»£ç ä¸æ•°æ®é›†ä»¥ä¿ƒè¿›é¢†åŸŸå†…çš„å¤ç°ä¸åç»­å¼€å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.02413v2",
      "published_date": "2025-12-02 04:47:53 UTC",
      "updated_date": "2025-12-09 20:58:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:29.988313+00:00"
    },
    {
      "arxiv_id": "2512.02409v1",
      "title": "Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles",
      "title_zh": "è°±åŠ¨åŠ›å­¦è§†è§’ä¸‹çš„æ•°æ®ç­–åº”ï¼šé™æ€å±€é™ã€åŠ¨æ€åŠ é€Ÿä¸å®ç”¨é¢„è¨€æœº",
      "authors": [
        "Yizhou Zhang",
        "Lun Du"
      ],
      "abstract": "Large-scale neural models are increasingly trained with data pruning, synthetic data generation, cross-model distillation, reinforcement learning from human feedback (RLHF), and difficulty-based sampling. While several of these data-centric strategies reliably improve training efficiency and downstream performance, others fail to provide meaningful gains -- most notably self-generated synthetic data, which often increases dataset volume without enhancing model capability.\n  We formalize data curation as reweighting the sampling distribution and map its effect onto the eigenstructure of the data-induced operator. Our first main result shows that \\textbf{static pruning induces a bounded operator and therefore cannot change the spectral tail exponent}; it provides at most finite-region improvements and cannot alter asymptotic neural scaling. Our second result analyzes \\textbf{time-dependent data curation}, showing that an ideal oracle capable of tracking spectral residuals and continuously re-normalizing the tail can provably accelerate learning -- although practical systems can only approximate this behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»è°±åŠ¨åŠ›å­¦(Spectral Dynamics)çš„è§’åº¦å°†æ•°æ®ç­›é€‰(Data Curation)å½¢å¼åŒ–ä¸ºé‡‡æ ·åˆ†å¸ƒçš„é‡æƒåŒ–ï¼Œå¹¶åˆ†æå…¶å¯¹æ•°æ®è¯±å¯¼ç®—å­ç‰¹å¾ç»“æ„(Eigenstructure)çš„å½±å“ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé™æ€å‰ªæ(Static Pruning)ç”±äºè¯±å¯¼çš„æ˜¯æœ‰ç•Œç®—å­ï¼Œæ— æ³•æ”¹å˜è°±å°¾æŒ‡æ•°(Spectral Tail Exponent)ï¼Œå› æ­¤ä»…èƒ½æä¾›æœ‰é™çš„å±€éƒ¨æ”¹è¿›ï¼Œæ— æ³•ä»æ ¹æœ¬ä¸Šæ”¹å˜æ¸è¿›ç¥ç»ç¼©æ”¾(Asymptotic Neural Scaling)å®šå¾‹ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¯¥ç ”ç©¶é€šè¿‡åˆ†æéšæ—¶é—´å˜åŒ–çš„æ•°æ®ç­›é€‰(Time-dependent Data Curation)è¯æ˜ï¼Œèƒ½å¤Ÿè¿½è¸ªè°±æ®‹å·®å¹¶æŒç»­å¯¹å°¾éƒ¨è¿›è¡Œå½’ä¸€åŒ–çš„ç†æƒ³é¢„è¨€æœº(Oracle)å¯ä»¥ä»ç†è®ºä¸Šæ˜¾è‘—åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚è¿™ä¸€ç†è®ºæ¡†æ¶è§£é‡Šäº†ä¸ºä½•è‡ªç”Ÿæˆçš„åˆæˆæ•°æ®å¾€å¾€æ— æ³•æœ‰æ•ˆæå‡æ¨¡å‹èƒ½åŠ›ï¼Œè€Œéš¾åº¦é‡‡æ ·ç­‰ç­–ç•¥å´èƒ½æé«˜è®­ç»ƒæ•ˆç‡ã€‚è¯¥æˆæœä¸ºç†è§£è®­ç»ƒæ•°æ®è´¨é‡ä¸å­¦ä¹ æ•ˆç‡ä¹‹é—´çš„æ·±å±‚è”ç³»æä¾›äº†é‡è¦çš„æ•°å­¦åŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02409v1",
      "published_date": "2025-12-02 04:36:13 UTC",
      "updated_date": "2025-12-02 04:36:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:03.983355+00:00"
    },
    {
      "arxiv_id": "2512.02405v1",
      "title": "WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate",
      "title_zh": "WISEï¼šé¢å‘é²æ£’å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“è¾©è®ºçš„åŠ æƒè¿­ä»£ä¸“å®¶ç¤¾ä¼š",
      "authors": [
        "Anoop Cherian",
        "River Doyle",
        "Eyal Ben-Dov",
        "Suhas Lohit",
        "Kuan-Chuan Peng"
      ],
      "abstract": "Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WISE (Weighted Iterative Society-of-Experts)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¹¿ä¹‰ä¸”æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“è¾©è®º (Multi-agent debate, MAD) æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰ä¸è¯­è¨€æ¨ç† (vision-and-language reasoning) ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚è¯¥æ¡†æ¶å°†å¼‚æ„æ™ºèƒ½ä½“åˆ’åˆ†ä¸ºç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„ Solvers å’Œè´Ÿè´£éªŒè¯æ­£ç¡®æ€§ã€åˆ†é…æƒé‡åŠæä¾›åé¦ˆçš„ Reflectorsï¼Œå……åˆ†åˆ©ç”¨äº†å…·å¤‡å•æ¨¡æ€å’Œå¤šæ¨¡æ€èƒ½åŠ›ä¸“å®¶çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºäº†æœ‰æ•ˆæ•´åˆå¤šè½®è¾©è®ºä¸­çš„ç»“æœå¹¶å¤„ç†å“åº”åå·®ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„ Dawid-Skene ç®—æ³•è¿›è¡Œåå¤„ç†ï¼Œå®ç°äº†ä¸¤é˜¶æ®µè¾©è®ºæ¨¡å‹çš„æ·±åº¦é›†æˆã€‚å®éªŒåœ¨ SMART-840ã€VisualPuzzlesã€EvoChart-QA ä»¥åŠæ–°æ„å»ºçš„ SMART-840++ æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡å’Œ LLM é…ç½®ä¸‹ï¼ŒWISE ç›¸æ¯”ç°æœ‰çš„ SOTA MAD ç³»ç»Ÿå’Œèšåˆæ–¹æ³•ï¼Œåœ¨å‡†ç¡®ç‡ä¸ŠæŒç»­æå‡äº† 2% åˆ° 7%ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02405v1",
      "published_date": "2025-12-02 04:31:52 UTC",
      "updated_date": "2025-12-02 04:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:35.070016+00:00"
    },
    {
      "arxiv_id": "2512.02393v1",
      "title": "Process-Centric Analysis of Agentic Software Systems",
      "title_zh": "æ™ºèƒ½ä½“è½¯ä»¶ç³»ç»Ÿçš„è¿‡ç¨‹å¯¼å‘åˆ†æ",
      "authors": [
        "Shuyang Liu",
        "Yang Chen",
        "Rahul Krishna",
        "Saurabh Sinha",
        "Jatin Ganhotra",
        "Reyhan Jabbarvand"
      ],
      "abstract": "Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.\n  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ä½“ç³»ç»Ÿ(Agentic systems)è¯„ä¼°è¿‡äºä¾§é‡ç»“æœ(outcome-centric)è€Œå¿½è§†æ¨ç†ã€è§„åˆ’åŠç­–ç•¥æ¼”å˜ç­‰è¿‡ç¨‹ç»†èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº†Graphectoryåˆ†ææ¡†æ¶ã€‚Graphectoryå—ä¼ ç»Ÿè½¯ä»¶ç³»ç»Ÿçš„å›¾è¡¨ç¤ºå¯å‘ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°ç¼–ç æ­¤ç±»ç³»ç»Ÿä¸­çš„æ—¶é—´å’Œè¯­ä¹‰å…³ç³»ï¼Œæ”¯æŒè®¾è®¡ä»¥è¿‡ç¨‹ä¸ºä¸­å¿ƒ(process-centric)çš„æŒ‡æ ‡æ¥è¡¡é‡å·¥ä½œæµè´¨é‡ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ¡†æ¶åˆ†æäº†SWE-agentå’ŒOpenHandsåœ¨è§£å†³SWE-bench Verifiedé—®é¢˜æ—¶çš„4000æ¡è½¨è¿¹ï¼Œæ¶‰åŠå››ç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹(LLMs)ã€‚å®éªŒå‘ç°ï¼Œä½¿ç”¨æ›´ä¸°å¯Œçš„æç¤ºè¯æˆ–æ›´å¼ºå¤§çš„LLMsçš„æ™ºèƒ½ä½“ä¼šè¡¨ç°å‡ºæ›´å¤æ‚çš„Graphectoryç»“æ„ï¼Œåæ˜ å‡ºæ›´æ·±å±‚çš„æ¢ç´¢å’Œæ›´å½»åº•çš„éªŒè¯è¡Œä¸ºã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†è§£å†³ç­–ç•¥éšé—®é¢˜éš¾åº¦å’Œåº•å±‚LLMè€Œå˜åŒ–ï¼ŒæˆåŠŸçš„è½¨è¿¹é€šå¸¸éµå¾ªè¿è´¯çš„å®šä½-è¡¥ä¸-éªŒè¯æ­¥éª¤ï¼Œè€Œå¤±è´¥çš„åˆ™è¡¨ç°å‡ºæ··ä¹±ã€é‡å¤æˆ–å›æº¯ã€‚æ­¤å¤–ï¼Œåˆ†æè¡¨æ˜å³ä½¿åœ¨æˆåŠŸçš„æ¡ˆä¾‹ä¸­ï¼Œæ™ºèƒ½ä½“ç¼–ç¨‹ç³»ç»Ÿä¹Ÿå¸¸è¡¨ç°å‡ºä½æ•ˆçš„è¿‡ç¨‹ï¼Œå¯¼è‡´è½¨è¿¹ä¸å¿…è¦åœ°å»¶é•¿ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02393v1",
      "published_date": "2025-12-02 04:12:29 UTC",
      "updated_date": "2025-12-02 04:12:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:16.471184+00:00"
    },
    {
      "arxiv_id": "2512.02389v1",
      "title": "Synthetic Error Injection Fails to Elicit Self-Correction In Language Models",
      "title_zh": "åˆæˆé”™è¯¯æ³¨å…¥æœªèƒ½è¯±å¯¼è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘çº é”™èƒ½åŠ›",
      "authors": [
        "David X. Wu",
        "Shreyas Kapur",
        "Anant Sahai",
        "Stuart Russell"
      ],
      "abstract": "Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡åˆæˆé”™è¯¯æ³¨å…¥ï¼ˆSynthetic Error Injectionï¼‰çš„ç›‘ç£å­¦ä¹ æ–¹æ³•æ˜¯å¦èƒ½è¯±å‘å¤§è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘çº æ­£ï¼ˆSelf-Correctionï¼‰èƒ½åŠ›ï¼Œä»¥ä½œä¸ºé«˜æˆæœ¬å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç ”ç©¶è€…åœ¨æ¨ç†é“¾ä¸­æ’å…¥å¹¶é®è”½äººå·¥é”™è¯¯ï¼Œç›‘ç£æ¨¡å‹è¿›è¡Œè¯†åˆ«ä¸ä¿®æ­£ï¼Œä½†å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹çš„ç®€å•åˆæˆä»»åŠ¡ä¸­å‡æœªèƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å‡ºé”™è¯¯ï¼Œä¹Ÿå¾€å¾€ä¼šé‡å¤åŸå§‹çš„é”™è¯¯é€»è¾‘ã€‚åˆ†æè¡¨æ˜ï¼Œåˆæˆé”™è¯¯ä¸åŒç­–ç•¥é”™è¯¯ï¼ˆOn-Policy Errorsï¼‰ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼ˆDistribution Shiftï¼‰æ˜¯å¯¼è‡´çº æ­£èƒ½åŠ›ä¸‹é™çš„å…³é”®å› ç´ ã€‚è¯¥ç ”ç©¶æœ€ç»ˆæ­ç¤ºäº†ä¸ºä½•åŒç­–ç•¥å¼ºåŒ–å­¦ä¹ åœ¨æ¿€å‘æ¨¡å‹è‡ªæˆ‘çº æ­£èƒ½åŠ›æ–¹é¢å…·æœ‰ä¸å¯æ›¿ä»£çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02389v1",
      "published_date": "2025-12-02 03:57:49 UTC",
      "updated_date": "2025-12-02 03:57:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:34.474662+00:00"
    },
    {
      "arxiv_id": "2512.02368v2",
      "title": "MoE-Enhanced Multi-Domain Feature Selection and Fusion for Fast Map-Free Trajectory Prediction",
      "title_zh": "é¢å‘å¿«é€Ÿæ— åœ°å›¾è½¨è¿¹é¢„æµ‹çš„ MoE å¢å¼ºå‹å¤šåŸŸç‰¹å¾é€‰æ‹©ä¸èåˆ",
      "authors": [
        "Wenyi Xiong",
        "Jian Chen",
        "Ziheng Qi",
        "Wenhua Chen"
      ],
      "abstract": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios due to noisy trajectory observations and intricate agent interactions. Existing methods often struggle to filter redundant scene data for discriminative information extraction, directly impairing trajectory prediction accuracy especially when handling outliers and dynamic multi-agent interactions. In response to these limitations, we present a novel map-free trajectory prediction method which adaptively eliminates redundant information and selects discriminative features across the temporal, spatial, and frequency domains, thereby enabling precise trajectory prediction in real-world driving environments. First, we design a MoE based frequency domain filter to adaptively weight distinct frequency components of observed trajectory data and suppress outlier related noise; then a selective spatiotemporal attention module that reallocates weights across temporal nodes (sequential dependencies), temporal trends (evolution patterns), and spatial nodes to extract salient information is proposed. Finally, our multimodal decoder-supervised by joint patch level and point-level losses generates reasonable and temporally consistent trajectories, and comprehensive experiments on the large-scale NuScenes and Argoverse dataset demonstrate that our method achieves competitive performance and low-latency inference performance compared with recently proposed methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­ç”±äºå™ªå£°è½¨è¿¹è§‚æµ‹å’Œå¤æ‚æ™ºèƒ½ä½“äº¤äº’å¯¼è‡´çš„è½¨è¿¹é¢„æµ‹éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¸“å®¶æ··åˆæ¨¡å‹(MoE)å¢å¼ºçš„å¤šé¢†åŸŸç‰¹å¾é€‰æ‹©ä¸èåˆæ–¹æ³•ï¼Œå®ç°äº†å¿«é€Ÿä¸”æ— åœ°å›¾(Map-Free)çš„è½¨è¿¹é¢„æµ‹ã€‚ä¸ºäº†æ»¤é™¤å†—ä½™åœºæ™¯æ•°æ®å¹¶æå–åˆ¤åˆ«æ€§ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªåŸºäºMoEçš„é¢‘åŸŸæ»¤æ³¢å™¨ï¼Œé€šè¿‡è‡ªé€‚åº”åŠ æƒä¸åŒé¢‘ç‡æˆåˆ†æ¥æŠ‘åˆ¶ä¸ç¦»ç¾¤ç‚¹ç›¸å…³çš„å™ªå£°ã€‚éšåï¼Œç ”ç©¶å¼•å…¥äº†é€‰æ‹©æ€§æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å—ï¼Œåœ¨æ—¶é—´èŠ‚ç‚¹ã€åºåˆ—æ¼”åŒ–æ¨¡å¼åŠç©ºé—´èŠ‚ç‚¹ä¸Šé‡æ–°åˆ†é…æƒé‡ï¼Œä»¥ç²¾å‡†æå–æ˜¾è‘—ç‰¹å¾ã€‚æœ€åï¼Œåˆ©ç”¨å—Patch-Levelå’ŒPoint-Levelè”åˆæŸå¤±ç›‘ç£çš„å¤šæ¨¡æ€è§£ç å™¨ç”Ÿæˆå…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„åˆç†è½¨è¿¹ã€‚åœ¨NuSceneså’ŒArgoverseå¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæä½æ¨ç†å»¶è¿Ÿçš„åŒæ—¶ï¼Œè¾¾åˆ°äº†ä¸ç°æœ‰å…ˆè¿›æ–¹æ³•å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02368v2",
      "published_date": "2025-12-02 03:20:07 UTC",
      "updated_date": "2026-01-23 04:59:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:02:33.985450+00:00"
    },
    {
      "arxiv_id": "2512.02364v1",
      "title": "Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection",
      "title_zh": "æ”»å…‹ç»“æ ¸ç—…ï¼šæœºå™¨å­¦ä¹ åœ¨ç»“æ ¸ç—…æ£€æµ‹ä¸­çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Daanish Hindustani",
        "Sanober Hindustani",
        "Preston Nguyen"
      ],
      "abstract": "This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ä»èƒ¸éƒ¨Xå°„çº¿å½±åƒä¸­è¯Šæ–­è‚ºç»“æ ¸ (Tuberculosis, TB) çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨è§£å†³èµ„æºåŒ®ä¹åœ°åŒºè¯Šæ–­æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒçš„ ResNet-50 æ¨¡å‹å’Œé€šç”¨çš„ SqueezeNet æ¨¡å‹ï¼Œåœ¨åŒ…å« 4,200 å¼ å½±åƒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†æ•°æ®å¢å¼º (Data Augmentation) å’Œè°ƒæ•´å°ºå¯¸ç­‰é¢„å¤„ç†æŠ€æœ¯æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚ç»“æœè¡¨æ˜ï¼ŒSqueezeNet è¡¨ç°æ˜¾è‘—ä¼˜äº ResNet-50ï¼Œå…¶å‡†ç¡®ç‡ (Accuracy) è¾¾åˆ° 89%ï¼Œç²¾ç¡®ç‡ (Precision) ä¸º 98%ï¼ŒF1 åˆ†æ•° (F1 score) ä¸º 87%ï¼Œè€Œ ResNet-50 çš„å‡†ç¡®ç‡ä»…ä¸º 73%ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æœºå™¨å­¦ä¹ åœ¨ TB æ£€æµ‹ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯è½»é‡åŒ–æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡é›†æˆå’Œæ—©æœŸç­›æŸ¥æ–¹é¢çš„åº”ç”¨å‰æ™¯ã€‚è¿™ä¸ºå…¨çƒæŠ—å‡»è‚ºç»“æ ¸å·¥ä½œæä¾›äº†æŠ€æœ¯æ”¯æ’‘ï¼Œä¹Ÿä¸ºèµ„æºæœ‰é™åœ°åŒºçš„åŒ»ç–—è¾…åŠ©è¯Šæ–­æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02364v1",
      "published_date": "2025-12-02 03:15:29 UTC",
      "updated_date": "2025-12-02 03:15:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:03:39.677544+00:00"
    },
    {
      "arxiv_id": "2512.02363v1",
      "title": "Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering",
      "title_zh": "é¢å‘é¢†åŸŸè‡ªé€‚åº”é—®ç­”çš„è®°å¿†å¢å¼ºå‹çŸ¥è¯†èåˆä¸å®‰å…¨æ„ŸçŸ¥è§£ç ",
      "authors": [
        "Lei Fu",
        "Xiang Chen",
        "Kaige Gao Xinyue Huang",
        "Kejian Tong"
      ],
      "abstract": "Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœåŠ¡é¢†åŸŸé—®ç­”ç³»ç»Ÿ(QA)åœ¨æ•´åˆå¼‚æ„çŸ¥è¯†æºä»¥åŠç¡®ä¿å‡†ç¡®æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ”¿ç­–å’Œæ”¿åºœç¦åˆ©ç­‰æ•æ„Ÿé¢†åŸŸé¢ä¸´çš„äº‹å®ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡å¯¹é½é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸ºKARMAï¼ˆKnowledge-Aware Reasoning and Memory-Augmented Adaptationï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æŠ¤ç†åœºæ™¯ä¸‹çš„é—®ç­”æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åŒç¼–ç å™¨æ¶æ„(dual-encoder architecture)æ¥èåˆç»“æ„åŒ–ä¸éç»“æ„åŒ–çŸ¥è¯†ï¼Œå¹¶é€šè¿‡é—¨æ§è®°å¿†å•å…ƒ(gated memory unit)åŠ¨æ€è°ƒèŠ‚å¤–éƒ¨çŸ¥è¯†çš„æ•´åˆã€‚æ­¤å¤–ï¼ŒKARMA å¼•å…¥äº†å®‰å…¨æ„ŸçŸ¥å¯æ§è§£ç å™¨(safety-aware controllable decoder)ï¼Œåˆ©ç”¨å®‰å…¨åˆ†ç±»å’Œå¼•å¯¼ç”ŸæˆæŠ€æœ¯æ¥å‡å°‘ä¸å®‰å…¨è¾“å‡ºã€‚åœ¨ä¸“æœ‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKARMA åœ¨ç­”æ¡ˆè´¨é‡å’Œå®‰å…¨æ€§æ–¹é¢å‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæ„å»ºå¯ä¿¡ä¸”å…·æœ‰é¢†åŸŸè‡ªé€‚åº”èƒ½åŠ›çš„é—®ç­”ç³»ç»Ÿæä¾›äº†å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02363v1",
      "published_date": "2025-12-02 03:12:14 UTC",
      "updated_date": "2025-12-02 03:12:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:03:22.987592+00:00"
    },
    {
      "arxiv_id": "2512.02361v1",
      "title": "VACoT: Rethinking Visual Data Augmentation with VLMs",
      "title_zh": "VACoTï¼šé‡æ–°å®¡è§†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ•°æ®å¢å¼º",
      "authors": [
        "Zhengzhuo Xu",
        "Chong Sun",
        "SiNan Du",
        "Chen Li",
        "Jing Lyu",
        "Chun Yuan"
      ],
      "abstract": "While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VACoTï¼ˆVisual Augmentation Chain-of-Thoughtï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¨ç†é˜¶æ®µåŠ¨æ€è°ƒç”¨å›¾åƒå¢å¼ºçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åœ¨åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ä¸­é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å¢åŠ é¢„è®­ç»ƒæ•°æ®é‡ä¸åŒï¼ŒVACoTé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ•´åˆå»å™ªç­‰åéªŒå˜æ¢ï¼ˆpost-hoc transformationsï¼‰ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§åŠåˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰è¾“å…¥ä¸‹çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç»“æ„åŒ–çš„é€šç”¨è§†è§‰å¢å¼ºé›†åˆï¼Œå¹¶ç»“åˆé«˜æ•ˆçš„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆagentic reinforcement learningï¼‰æ¥é™ä½è®­ç»ƒå¤æ‚åº¦å’Œè®¡ç®—å¼€é”€ã€‚ä¸ºäº†å¹³è¡¡æ¨ç†æ•ˆç‡ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§æ¡ä»¶å¥–åŠ±æœºåˆ¶ï¼ˆconditional reward schemeï¼‰ï¼Œåœ¨é¼“åŠ±å¿…è¦å¢å¼ºçš„åŒæ—¶æƒ©ç½šå†—é•¿å“åº”ï¼Œç¡®ä¿æ„ŸçŸ¥ä»»åŠ¡æ¨ç†çš„ç®€æ´æœ‰æ•ˆã€‚åœ¨13ä¸ªæ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†VACoTçš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨OCRç›¸å…³çš„å¯¹æŠ—æ€§åœºæ™¯ï¼ˆadversarial scenariosï¼‰ä¸­è¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†AdvOCRåŸºå‡†ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†åéªŒè§†è§‰å¢å¼ºåœ¨åº”å¯¹å¯¹æŠ—æ ·æœ¬æ—¶çš„æ³›åŒ–ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02361v1",
      "published_date": "2025-12-02 03:11:32 UTC",
      "updated_date": "2025-12-02 03:11:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:02.288395+00:00"
    },
    {
      "arxiv_id": "2512.06002v1",
      "title": "POrTAL: Plan-Orchestrated Tree Assembly for Lookahead",
      "title_zh": "POrTALï¼šé¢å‘å‰ç»çš„è®¡åˆ’ç¼–æ’å¼æ ‘çŠ¶ç»„è£…",
      "authors": [
        "Evan Conway",
        "David Porfirio",
        "David Chan",
        "Mark Roberts",
        "Laura M. Hiatt"
      ],
      "abstract": "Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººåœ¨éƒ¨åˆ†å¯è§‚æµ‹(partially observable)ç¯å¢ƒä¸‹é¢ä¸´çš„ä¸ç¡®å®šæ€§è§„åˆ’éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºPOrTAL (Plan-Orchestrated Tree Assembly for Lookahead) çš„æ–°å‹è½»é‡çº§æ¦‚ç‡è§„åˆ’ç®—æ³•ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¦‚ç‡è§„åˆ’ç®—æ³•åœ¨è®¡ç®—èµ„æºå—é™æ—¶æ•ˆç‡ä½ä¸‹æˆ–æ‰§è¡Œæ­¥æ•°è¿‡å¤šçš„å±€é™ï¼ŒPOrTAL æœ‰æœºç»“åˆäº† FF-Replan å’Œ POMCP ä¸¤ç§åŸºå‡†è§„åˆ’ç®—æ³•çš„ä¼˜åŠ¿ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¡ˆä¾‹ç ”ç©¶ï¼Œå®éªŒç»“æœè¯æ˜ POrTAL åœ¨å¯»æ‰¾è§£å†³æ–¹æ¡ˆçš„é€Ÿåº¦å’Œæ‰§è¡Œæ­¥æ•°æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¸Šè¿°åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥æ¢è®¨å¹¶éªŒè¯äº†è¯¥ç®—æ³•åœ¨ä¸åŒæ—¶é—´çº¦æŸ(temporal constraints)ä¸‹çš„ç¨³å¥è¡¨ç°ã€‚è¯¥æˆæœä¸ºæå‡æœºå™¨äººåœ¨å¤æ‚äººæœºäº¤äº’åœºæ™¯ä¸­çš„è‡ªä¸»è§„åˆ’èƒ½åŠ›æä¾›äº†æ›´ä¸ºé«˜æ•ˆä¸”è½»é‡åŒ–çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to ICRA 26",
      "pdf_url": "https://arxiv.org/pdf/2512.06002v1",
      "published_date": "2025-12-02 03:11:28 UTC",
      "updated_date": "2025-12-02 03:11:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:27.579768+00:00"
    },
    {
      "arxiv_id": "2512.20634v1",
      "title": "Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning",
      "title_zh": "æŒç»­å­¦ä¹ ä¸­ä¼ªé—å¿˜çš„å®æ—¶æ£€æµ‹ä¸å®šé‡åˆ†æ",
      "authors": [
        "Weiwei Wang"
      ],
      "abstract": "Catastrophic forgetting remains a fundamental challenge in continual learning for large language models. Recent work revealed that performance degradation may stem from spurious forgetting caused by task alignment disruption rather than true knowledge loss. However, this work only qualitatively describes alignment, relies on post-hoc analysis, and lacks automatic distinction mechanisms.\n  We introduce the shallow versus deep alignment framework, providing the first quantitative characterization of alignment depth. We identify that current task alignment approaches suffer from shallow alignment - maintained only over the first few output tokens (approximately 3-5) - making models vulnerable to forgetting. This explains why spurious forgetting occurs, why it is reversible, and why fine-tuning attacks are effective.\n  We propose a comprehensive framework addressing all gaps: (1) quantitative metrics (0-1 scale) to measure alignment depth across token positions; (2) real-time detection methods for identifying shallow alignment during training; (3) specialized analysis tools for visualization and recovery prediction; and (4) adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment. Extensive experiments on multiple datasets and model architectures (Qwen2.5-3B to Qwen2.5-32B) demonstrate 86.2-90.6% identification accuracy and show that promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ (Continual Learning)ä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ï¼ŒæŒ‡å‡ºæ€§èƒ½ä¸‹é™å¾€å¾€æºäºä»»åŠ¡å¯¹é½(Task Alignment)ä¸­æ–­å¯¼è‡´çš„ä¼ªé—å¿˜(Spurious Forgetting)ï¼Œè€ŒéçœŸå®çš„çŸ¥è¯†ä¸¢å¤±ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†æµ…å±‚ä¸æ·±å±‚å¯¹é½(Shallow versus Deep Alignment)æ¡†æ¶ï¼Œé¦–æ¬¡å¯¹å¯¹é½æ·±åº¦è¿›è¡Œäº†å®šé‡è¡¨å¾ã€‚ç ”ç©¶å‘ç°å½“å‰çš„å¯¹é½æ–¹æ³•æ™®éå­˜åœ¨æµ…å±‚å¯¹é½é—®é¢˜ï¼Œå³å¯¹é½ä»…ç»´æŒåœ¨è¾“å‡ºçš„å‰3-5ä¸ªTokenï¼Œè¿™è§£é‡Šäº†ä¼ªé—å¿˜çš„å‘ç”Ÿæœºåˆ¶åŠå…¶å¯é€†æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†0-1é‡çº§çš„å®šé‡æŒ‡æ ‡æ¥è¡¡é‡ä¸åŒTokenä½ç½®çš„å¯¹é½æ·±åº¦ï¼Œå¹¶å®ç°äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶æ£€æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸“é—¨çš„åˆ†æå·¥å…·å’Œè‡ªé€‚åº”ç¼“è§£ç­–ç•¥ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒºåˆ†é—å¿˜ç±»å‹å¹¶ä¿ƒè¿›æ·±å±‚å¯¹é½ã€‚åœ¨Qwen2.5ï¼ˆ3Bè‡³32Bï¼‰ç­‰å¤šç§æ¨¡å‹æ¶æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«å‡†ç¡®ç‡ä¸Šè¾¾åˆ°86.2-90.6%ï¼Œå¹¶å°†æŠ—é—å¿˜é²æ£’æ€§è¾ƒåŸºçº¿æå‡äº†3.3-7.1%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.20634v1",
      "published_date": "2025-12-02 03:09:35 UTC",
      "updated_date": "2025-12-02 03:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:03.470028+00:00"
    },
    {
      "arxiv_id": "2512.02358v1",
      "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games",
      "title_zh": "è¶…è¶Šæ¸¸æˆæµ‹è¯•ï¼šé¢å‘å¤§å‹å¤šäººåœ¨çº¿æ¸¸æˆçš„ç”Ÿæˆå¼å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿç³»ç»Ÿ",
      "authors": [
        "Ran Zhang",
        "Kun Ouyang",
        "Tiancheng Ma",
        "Yida Yang",
        "Dong Fang"
      ],
      "abstract": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.",
      "tldr_zh": "ä¼ ç»Ÿçš„Massively Multiplayer Online (MMO)æ¸¸æˆæ•°å€¼ç³»ç»Ÿä¼˜åŒ–é«˜åº¦ä¾èµ–å¤§è§„æ¨¡åœ¨çº¿å®éªŒæˆ–é¢„å®šä¹‰ç»Ÿè®¡æ¨¡å‹ï¼Œå¸¸é¢ä¸´æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”ç ´åç©å®¶ä½“éªŒçš„æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºLarge Language Models (LLMs)çš„ç”Ÿæˆå¼å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç¦»çº¿æ¨¡æ‹Ÿç³»ç»Ÿä¿çœŸåº¦ä¸è¶³ã€éš¾ä»¥æ¨¡æ‹ŸçœŸå®ç©å®¶å†³ç­–çš„é—®é¢˜ã€‚é€šè¿‡å¯¹å¤§è§„æ¨¡çœŸå®ç©å®¶è¡Œä¸ºæ•°æ®åº”ç”¨Supervised Fine-Tuning (SFT)å’ŒReinforcement Learning (RL)ï¼Œè¯¥ç³»ç»Ÿä½¿ç”Ÿæˆå¼æ™ºèƒ½ä½“å…·å¤‡äº†ç‰¹å®šæ¸¸æˆé¢†åŸŸçš„å†³ç­–èƒ½åŠ›ä¸å¯è§£é‡Šæ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶ç»“åˆäº†åŸºäºçœŸå®æ¸¸æˆæ—¥å¿—è®­ç»ƒçš„æ•°æ®é©±åŠ¨ç¯å¢ƒæ¨¡å‹ï¼Œä»¥é‡æ„åŠ¨æ€çš„æ¸¸æˆå†…ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç©å®¶è¡Œä¸ºä¸€è‡´æ€§å’Œå¹²é¢„ä¸‹çš„å› æœå“åº”æ–¹é¢å±•ç°å‡ºæå¼ºçš„ä¸€è‡´æ€§ã€‚è¿™ä¸€æ¡†æ¶ä¸ºæ•°æ®é©±åŠ¨çš„æ•°å€¼è®¾è®¡ä¼˜åŒ–æä¾›äº†ä¸€ä¸ªå¯é ã€å¯è§£é‡Šä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02358v1",
      "published_date": "2025-12-02 03:01:17 UTC",
      "updated_date": "2025-12-02 03:01:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:02.978747+00:00"
    },
    {
      "arxiv_id": "2512.02351v1",
      "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
      "title_zh": "ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­ç¨€ç–æ€§çš„ç†è§£ä¸åˆ©ç”¨",
      "authors": [
        "Shwai He",
        "Chaorui Deng",
        "Ang Li",
        "Shen Yan"
      ],
      "abstract": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \\href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œé€šè¿‡æ— éœ€è®­ç»ƒçš„å‰ªæ(training-free pruning)æ–¹æ³•ä»æ·±åº¦å’Œå®½åº¦ç»´åº¦å¯¹æ¨¡å‹ç»„ä»¶è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œç†è§£ç»„ä»¶åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„å¯å‹ç¼©æ€§ï¼Œè€Œç”Ÿæˆç»„ä»¶åˆ™å¯¹å‹ç¼©é«˜åº¦æ•æ„Ÿï¼Œæ€§èƒ½ä¼šéšå‹ç¼©ç‡å¢åŠ è€Œæ€¥å‰§ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ··åˆä¸“å®¶(Mixture-of-Experts, MoE)é€‚é…æ–¹æ¡ˆï¼Œå°†ç”Ÿæˆæ¨¡å—åˆ’åˆ†ä¸ºå¤šä¸ªä¸“å®¶å¹¶åˆ©ç”¨ç¨€ç–æ¿€æ´»æŠ€æœ¯æ¥æ¢å¤ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡é€‚é…çš„BAGELæ¨¡å‹åœ¨ä»…æ¿€æ´»çº¦ä¸€åŠå‚æ•°çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½ä¿æŒä¸å…¨å‚æ•°æ¨¡å‹ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†ä¸åŒç»„ä»¶çš„å‹ç¼©ç‰¹æ€§ï¼Œä¹Ÿä¸ºæ„å»ºé«˜æ•ˆçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 13 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.02351v1",
      "published_date": "2025-12-02 02:47:29 UTC",
      "updated_date": "2025-12-02 02:47:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:28.367502+00:00"
    },
    {
      "arxiv_id": "2512.02350v1",
      "title": "FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data",
      "title_zh": "FOVAï¼šé¢å‘æ··åˆè´¨é‡æ•°æ®çš„ç¦»çº¿è”é‚¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Nan Qiao",
        "Sheng Yue",
        "Ju Ren",
        "Yaoxue Zhang"
      ],
      "abstract": "Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \\emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.",
      "tldr_zh": "æœ¬æ–‡ç ”ç©¶äº†ç¦»çº¿è”é‚¦å¼ºåŒ–å­¦ä¹  (Offline Federated Reinforcement Learning, FRL) åœ¨å¤„ç†æ··åˆè´¨é‡æ•°æ® (mixed-quality data) æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³ä¸åŒå®¢æˆ·ç«¯æ”¶é›†çš„ç¦»çº¿æ•°æ®å¾€å¾€æºäºè´¨é‡å·®å¼‚å·¨å¤§çš„ç­–ç•¥ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åä¸º FOVA çš„æ–°å‹æŠ•ç¥¨å¼ç¦»çº¿è”é‚¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æŠ•ç¥¨æœºåˆ¶ (vote mechanism) åœ¨å±€éƒ¨ç­–ç•¥è¯„ä¼°ä¸­ç²¾å‡†è¯†åˆ«é«˜å›æŠ¥åŠ¨ä½œï¼Œæœ‰æ•ˆå‡è½»äº†ä½è´¨é‡è¡Œä¸ºæ•°æ®å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼ŒFOVA åŸºäºä¼˜åŠ¿åŠ æƒå›å½’ (Advantage-Weighted Regression, AWR) æ„å»ºäº†ç›¸ä¸€è‡´çš„å±€éƒ¨ä¸å…¨å±€è®­ç»ƒç›®æ ‡ï¼Œæå¤§å¢å¼ºäº†å­¦ä¹ è¿‡ç¨‹çš„æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒFOVA å­¦ä¹ åˆ°çš„ç­–ç•¥ç›¸æ¯”è¡Œä¸ºç­–ç•¥ (behavioral policy) å…·æœ‰ä¸¥æ ¼çš„æ”¹è¿›ä¿è¯ã€‚å®éªŒç»“æœä¹Ÿè¯å®äº† FOVA åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†ç®—æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE/ACM ToN",
      "pdf_url": "https://arxiv.org/pdf/2512.02350v1",
      "published_date": "2025-12-02 02:35:55 UTC",
      "updated_date": "2025-12-02 02:35:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:27.371081+00:00"
    },
    {
      "arxiv_id": "2512.02340v1",
      "title": "Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective",
      "title_zh": "å¤šè§†è§’è§†è§‰ç©ºé—´æ¨ç†çš„æ¨ç†è·¯å¾„ä¸æ½œçŠ¶æ€åˆ†æï¼šè®¤çŸ¥ç§‘å­¦è§†è§’",
      "authors": [
        "Qiyao Xue",
        "Weichen Liu",
        "Shiqi Wang",
        "Haoming Wang",
        "Yuyang Wu",
        "Wei Gao"
      ],
      "abstract": "Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šè§†å›¾ç©ºé—´æ¨ç†ä¸­éš¾ä»¥ä¿æŒå‡ ä½•ä¸è·¨è§†å›¾ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œä»è®¤çŸ¥ç§‘å­¦è§†è§’æå‡ºäº†ReMindView-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒäº’è¡¥è§†è§’ä¸‹æ„å»ºã€å¯¹é½å’Œç»´æŠ¤ç©ºé—´å¿ƒç†æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹15ç§ä¸»æµVLMsçš„ç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹åœ¨è·¨è§†å›¾å¯¹é½å’Œè§†è§’è½¬æ¢(perspective-taking)æ–¹é¢æ™®éå­˜åœ¨å¤±æ•ˆç°è±¡ã€‚åˆ©ç”¨LLM-as-a-judgeå’Œè‡ªæ´½æ€§æç¤º(self-consistency prompting)è¿›è¡Œçš„æ˜¾å¼é˜¶æ®µåˆ†æè¡¨æ˜ï¼ŒVLMsè™½ç„¶æ“…é•¿å¸§å†…æ„ŸçŸ¥(in-frame perception)ï¼Œä½†åœ¨æ•´åˆè·¨è§†å›¾ä¿¡æ¯æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚çº¿æ€§æ¢é’ˆ(linear probing)å’Œç†µåŠ¨åŠ›å­¦åˆ†æç­‰éšå¼æ–¹æ³•è¿›ä¸€æ­¥æ­ç¤ºäº†ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„æ¸è¿›å¼ä¸¢å¤±ä»¥åŠæ¨ç†è¿‡ç¨‹ä¸­æ­£ç¡®ä¸é”™è¯¯è½¨è¿¹é—´çš„ä¸ç¡®å®šæ€§åˆ†ç¦»ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºè¯Šæ–­VLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†è®¤çŸ¥å­¦å·¥å…·ï¼Œè¿˜ä»åŠ¨æ€æ¼”åŒ–çš„è§†è§’æ­ç¤ºäº†å¤šè§†å›¾ç©ºé—´å¿ƒç†æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä¸­å½¢æˆã€é€€åŒ–ä¸å¤±ç¨³çš„å†…åœ¨æœºåˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 37 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.02340v1",
      "published_date": "2025-12-02 02:21:29 UTC",
      "updated_date": "2025-12-02 02:21:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:33.788133+00:00"
    },
    {
      "arxiv_id": "2512.02339v1",
      "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision",
      "title_zh": "è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ— ç›‘ç£æ¡ä»¶ä¸‹è¡¨ç°å‡ºå“è¶Šçš„è§†è§‰ç›¸ä¼¼ç‰©ä½“è·Ÿè¸ªèƒ½åŠ›",
      "authors": [
        "Chenshuang Zhang",
        "Kang Zhang",
        "Joon Son Chung",
        "In So Kweon",
        "Junmo Kim",
        "Chengzhi Mao"
      ],
      "abstract": "Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è§†è§‰çº¿ç´¢æ¨¡ç³Šçš„æƒ…å†µä¸‹å¦‚ä½•åˆ©ç”¨è¿åŠ¨ä¿¡æ¯åŒºåˆ†è§†è§‰ç›¸ä¼¼çš„å¯¹è±¡ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒçš„ Video Diffusion Models åœ¨æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå…¶å»å™ªè¿‡ç¨‹åœ¨æ—©æœŸé«˜å™ªå£°é˜¶æ®µèƒ½è‡ªç„¶åœ°å­¦ä¹ åˆ°é€‚ç”¨äºè·Ÿè¸ªçš„è¿åŠ¨è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆåˆ†ç¦»è¿åŠ¨ä¸å¤–è§‚ç»†èŠ‚ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è‡ªç›‘ç£è·Ÿè¸ªæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†åŒºåˆ†è§†è§‰ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€è–„å¼±ç¯èŠ‚çš„å±€é™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ¯”è¿‘æœŸçš„è‡ªç›‘ç£æ–¹æ³•å‡†ç¡®ç‡æé«˜äº†å¤šè¾¾ 6 ä¸ªç™¾åˆ†ç‚¹ã€‚å¯è§†åŒ–ç»“æœè¯å®ï¼Œè¿™ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¿åŠ¨è¡¨ç¤ºä½¿å¾—ç³»ç»Ÿåœ¨é¢å¯¹å‰§çƒˆçš„è§†è§’å˜åŒ–å’Œç‰©ä½“å½¢å˜æ—¶ï¼Œä¾ç„¶èƒ½å¤Ÿå¯¹ç”šè‡³å®Œå…¨ç›¸åŒçš„ç‰©ä½“å®ç°é²æ£’è·Ÿè¸ªã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.02339v1",
      "published_date": "2025-12-02 02:17:34 UTC",
      "updated_date": "2025-12-02 02:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:31.871581+00:00"
    },
    {
      "arxiv_id": "2512.02318v2",
      "title": "COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers",
      "title_zh": "COGNITIONï¼šä»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹éªŒè¯ç ç ´è§£å™¨çš„è¯„ä¼°åˆ°é˜²å¾¡",
      "authors": [
        "Junyu Wang",
        "Changjia Zhu",
        "Yuanbo Zhou",
        "Lingyao Li",
        "Xu He",
        "Junjie Xiong"
      ],
      "abstract": "This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) å¦‚ä½•å‰Šå¼±è§†è§‰éªŒè¯ç  (CAPTCHA) çš„å®‰å…¨æ€§ã€‚ç ”ç©¶è€…é€šè¿‡è¯„ä¼° 7 ç§é¢†å…ˆçš„å•†ä¸šå’Œå¼€æº MLLMs åœ¨ 18 ç§çœŸå® CAPTCHA ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¯¦ç»†æµ‹é‡äº†å•æ¬¡å‡†ç¡®ç‡ã€å“åº”å»¶è¿ŸåŠæ¯æ¬¡ç ´è§£çš„æˆæœ¬ã€‚å®éªŒç»“æœæ­ç¤ºï¼ŒMLLMs èƒ½å¤Ÿä»¥æ¥è¿‘äººç±»çš„æˆæœ¬å’Œå»¶è¿Ÿï¼Œå¯é åœ°è§£å†³è¯†åˆ«å¯¼å‘å‹å’Œä½äº¤äº’æ€§çš„éªŒè¯ç ä»»åŠ¡ï¼Œä½†åœ¨å¤„ç†éœ€è¦ç»†ç²’åº¦å®šä½ (Fine-grained localization)ã€å¤šæ­¥ç©ºé—´æ¨ç† (Multi-step spatial reasoning) æˆ–è·¨å¸§ä¸€è‡´æ€§ (Cross-frame consistency) çš„ä»»åŠ¡æ—¶ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚é€šè¿‡æ·±å…¥åˆ†æ MLLMs çš„æ¨ç†è½¨è¿¹ï¼Œè¯¥ç ”ç©¶é˜æ˜äº†æ¨¡å‹åœ¨ç‰¹å®šè°œé¢˜ä¸Šæˆè´¥çš„å†…åœ¨æœºåˆ¶ï¼Œå¹¶æ®æ­¤æå‡ºäº†é’ˆå¯¹æ€§çš„é˜²å¾¡æŒ‡å—ã€‚è¿™äº›å‘ç°ä¸ºå¹³å°è¿è¥å•†é€‰æ‹©å’Œå¼ºåŒ–éªŒè¯ç ç³»ç»Ÿä»¥æŠµå¾¡è‡ªåŠ¨åŒ–æ”»å‡»æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02318v2",
      "published_date": "2025-12-02 01:23:10 UTC",
      "updated_date": "2025-12-03 04:01:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:04:54.094012+00:00"
    },
    {
      "arxiv_id": "2512.02306v1",
      "title": "OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning",
      "title_zh": "OmniGuardï¼šå…·æœ‰å®¡æ…æ¨ç†èƒ½åŠ›çš„ç»Ÿä¸€å…¨æ¨¡æ€å®‰å…¨æŠ¤æ ",
      "authors": [
        "Boyu Zhu",
        "Xiaofei Wen",
        "Wenjie Jacky Mo",
        "Tinghui Zhu",
        "Yanan Xie",
        "Peng Qi",
        "Muhao Chen"
      ],
      "abstract": "Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(OLLMs)åœ¨å¤„ç†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘äº¤äº’æ—¶é¢ä¸´çš„å®‰å…¨ä¸ä»·å€¼å¯¹é½æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å•æ¨¡æ€äºŒåˆ†ç±»é˜²å¾¡æœºåˆ¶åœ¨è·¨æ¨¡æ€åœºæ™¯ä¸‹é²æ£’æ€§ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†OmniGuardï¼Œè¿™æ˜¯é¦–ä¸ªå…·å¤‡æ·±åº¦æ¨ç†(deliberate reasoning)èƒ½åŠ›çš„å…¨æ¨¡æ€é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è·¨æ‰€æœ‰æ¨¡æ€çš„å®‰å…¨é˜²æŠ¤ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡21ä¸‡ä¸ªå¤šæ ·åŒ–æ ·æœ¬çš„å¤§è§„æ¨¡å…¨æ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œæ¶µç›–äº†å•æ¨¡æ€å’Œè·¨æ¨¡æ€çš„å„ç§è¾“å…¥æƒ…å½¢ã€‚æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬å‡é…æœ‰ç»“æ„åŒ–å®‰å…¨æ ‡ç­¾ï¼Œå¹¶åˆ©ç”¨ç›®æ ‡è’¸é¦(targeted distillation)æŠ€æœ¯ä»ä¸“å®¶æ¨¡å‹ä¸­æå–äº†ç²¾ç»†çš„å®‰å…¨è¯„è®º(safety critiques)ã€‚åœ¨15ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒOmniGuardåœ¨å¤šç§å¤šæ¨¡æ€å®‰å…¨åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æ‰§è¡Œå®‰å…¨ç­–ç•¥å¹¶é™ä½å…¨æ¨¡æ€é£é™©ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„å…¨æ¨¡æ€å®‰å…¨ä¿éšœç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02306v1",
      "published_date": "2025-12-02 01:01:44 UTC",
      "updated_date": "2025-12-02 01:01:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:05:02.178955+00:00"
    },
    {
      "arxiv_id": "2512.02302v1",
      "title": "Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization",
      "title_zh": "æç«¯æ•°æ®çº¦æŸä¸‹çš„ä¹³è…ºç»†èƒåˆ†å‰²ï¼šé‡å­å¢å¼ºä¸è‡ªé€‚åº”æŸå¤±ç¨³å®šåŒ–",
      "authors": [
        "Varun Kumar Dasoju",
        "Qingsu Cheng",
        "Zeyun Yu"
      ],
      "abstract": "Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¹³è…ºç»†èƒåˆ†å‰²ï¼ˆBreast Cell Segmentationï¼‰ä¸­ç—…ç†å›¾åƒæ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”æ•°æ®æç«¯å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆé‡å­å¯å‘ï¼ˆQuantum-inspiredï¼‰å¢å¼ºä¸è‡ªé€‚åº”æŸå¤±ç¨³å®šï¼ˆAdaptive Loss Stabilizationï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šå°ºåº¦ Gabor filters å¼•å…¥é‡å­å¯å‘çš„è¾¹ç¼˜å¢å¼ºæŠ€æœ¯ä½œä¸ºç¬¬å››è¾“å…¥é€šé“ï¼Œæœ‰æ•ˆæå‡äº†è¾¹ç•Œæ£€æµ‹çš„ç²¾ç¡®åº¦ã€‚ä¸ºåº”å¯¹æç«¯çš„ç±»åˆ«ä¸å¹³è¡¡ï¼Œç ”ç©¶è®¾è®¡äº†é›†æˆäº†è‡ªé€‚åº” Dice lossã€è¾¹ç•Œæ„ŸçŸ¥é¡¹å’Œè‡ªåŠ¨æ­£æ ·æœ¬æƒé‡çš„ç¨³å®šå¤šåˆ†é‡æŸå¤±å‡½æ•°ï¼Œå¹¶å¼•å…¥åŸºäºå¤æ‚åº¦çš„åŠ æƒé‡‡æ ·ç­–ç•¥ä»¥èšç„¦éš¾åˆ†å‰²åŒºåŸŸã€‚æ¨¡å‹é‡‡ç”¨ EfficientNet-B7/UNet++ æ¶æ„å¹¶ç»“åˆ 4-to-3 channel projection æŠ€æœ¯ï¼Œåœ¨ä»…ä½¿ç”¨ 599 å¼ è®­ç»ƒå›¾åƒçš„æ¡ä»¶ä¸‹å®ç°äº† 95.5% çš„ Dice score å’Œ 91.2% çš„ IoUã€‚å®éªŒè¡¨æ˜é‡å­å¢å¼ºä½¿è¾¹ç•Œå‡†ç¡®ç‡æå‡ 2.1%ï¼ŒåŠ æƒé‡‡æ ·ä½¿å°ç—…ç¶æ£€æµ‹æå‡ 3.8%ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—é™ä½äº†åŒ»ç–—ä¸“å®¶æ„å»ºæ•°æ®é›†çš„æ—¶é—´æˆæœ¬ï¼Œè§£å†³äº†ä¸´åºŠæ„ŸçŸ¥äººå·¥æ™ºèƒ½ï¼ˆClinical Perception AIï¼‰å¼€å‘ä¸­çš„å…³é”®ç“¶é¢ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.02302v1",
      "published_date": "2025-12-02 00:45:21 UTC",
      "updated_date": "2025-12-02 00:45:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:05:24.566141+00:00"
    },
    {
      "arxiv_id": "2512.02299v1",
      "title": "HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models",
      "title_zh": "HealthContradictï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­çš„ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å†²çª",
      "authors": [
        "Boya Zhang",
        "Alban Bornet",
        "Rui Yang",
        "Nan Liu",
        "Douglas Teodoro"
      ],
      "abstract": "How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HealthContradictï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸“å®¶éªŒè¯çš„æ•°æ®é›†ï¼ŒåŒ…å«920ä¸ªç‹¬ç‰¹å®ä¾‹ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹(LMs)åœ¨å¤„ç†é•¿ç¯‡ä¸”å†²çªçš„ç”Ÿç‰©åŒ»å­¦èƒŒæ™¯æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚æ¯ä¸ªå®ä¾‹åŒ…å«ä¸€ä¸ªå¥åº·ç›¸å…³é—®é¢˜ã€ç”±ç§‘å­¦è¯æ®æ”¯æŒçš„äº‹å®ç­”æ¡ˆä»¥åŠä¸¤ç¯‡æŒç›¸åç«‹åœºçš„æ–‡æ¡£ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è®¾ç½®æ­£ç¡®ã€é”™è¯¯æˆ–çŸ›ç›¾èƒŒæ™¯(contradictory context)ç­‰å¤šç§æç¤ºåœºæ™¯ï¼Œæ·±å…¥è¡¡é‡äº†ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚ä¸ç°æœ‰çš„åŒ»å­¦é—®ç­”(medical question-answering)è¯„ä¼°åŸºå‡†ç›¸æ¯”ï¼ŒHealthContradictèƒ½æ›´æœ‰æ•ˆåœ°åŒºåˆ†è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„ç”Ÿç‰©åŒ»å­¦è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ä¸ä»…åœ¨äºé¢„è®­ç»ƒè·å¾—çš„å‚æ•°åŒ–çŸ¥è¯†(parametric knowledge)ï¼Œæ›´åœ¨äºå…¶åˆ©ç”¨æ­£ç¡®èƒŒæ™¯å¹¶æŠµå¾¡é”™è¯¯èƒŒæ™¯çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02299v1",
      "published_date": "2025-12-02 00:38:42 UTC",
      "updated_date": "2025-12-02 00:38:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:05:20.972332+00:00"
    },
    {
      "arxiv_id": "2512.02290v1",
      "title": "Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation",
      "title_zh": "é€šè¿‡å½¢æ€å­¦åŒºåŸŸæ‰°åŠ¨ä¸åˆæˆæ ‡ç­¾åˆ° SAR ç”Ÿæˆå¢å¼ºè·¨åŸŸ SAR æº¢æ²¹åˆ†å‰²",
      "authors": [
        "Andre Juarez",
        "Luis Salsavilca",
        "Frida Coaquira",
        "Celso Gonzales"
      ],
      "abstract": "Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \\textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\\% to 51.8\\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆæˆå­”å¾„é›·è¾¾(SAR)æº¢æ²¹åˆ†å‰²æ¨¡å‹åœ¨ä¸åŒæµ·åŸŸæ³›åŒ–èƒ½åŠ›å·®åŠæ ‡æ³¨æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º MORPâ€“Synth çš„åŒé˜¶æ®µåˆæˆå¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨å½¢æ€åŒºåŸŸæ‰°åŠ¨(Morphological Region Perturbation)æ–¹æ³•ï¼Œé€šè¿‡æ›²ç‡å¼•å¯¼çš„æ ‡ç­¾ç©ºé—´å¤„ç†ç”Ÿæˆå…·æœ‰é€¼çœŸå‡ ä½•å˜åŒ–çš„æ²¹æº¢å’Œç–‘ä¼¼ç‰©(look-alike)åŒºåŸŸã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨æ¡ä»¶ç”Ÿæˆ INADE æ¨¡å‹ï¼Œä»ç¼–è¾‘åçš„æ©ç ä¸­æ¸²æŸ“å‡ºå…·æœ‰ SAR ç‰¹å¾çš„çº¹ç†ï¼Œä»¥æ¨¡æ‹ŸçœŸå®æµ·å†µä¸‹çš„åå‘æ•£å°„ç‰¹æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å« 2112 ä¸ªæ ‡æ³¨åˆ‡ç‰‡çš„ç§˜é²æµ·åŸŸæ•°æ®é›†ï¼Œå¹¶å°†å…¶ä¸åœ°ä¸­æµ· CleanSeaNet åŸºå‡†æ•°æ®é›†è¿›è¡Œäº†ç»Ÿä¸€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»åœ°ä¸­æµ·åˆ°ç§˜é²æµ·åŸŸçš„è·¨åŸŸè¿ç§»ä»»åŠ¡ä¸­ï¼ŒMORPâ€“Synth å°†æ¨¡å‹çš„å¹³å‡äº¤å¹¶æ¯”(mIoU)æå‡äº†å¤šè¾¾ 6%ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ²¹æº¢(+10.8 IoU)ç­‰å°‘æ•°ç±»åˆ«çš„è¯†åˆ«æ•ˆæœã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°å¼¥åˆäº†åœ°ç†åŒºåŸŸé—´çš„åŸŸå·®å¼‚ï¼Œä¸ºç¼ºä¹æ ‡æ³¨æ•°æ®çš„ç‰¹å®šæµ·åŸŸæº¢æ²¹ç›‘æµ‹æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02290v1",
      "published_date": "2025-12-02 00:13:02 UTC",
      "updated_date": "2025-12-02 00:13:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T13:05:29.575945+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 143,
  "processed_papers_count": 143,
  "failed_papers_count": 0,
  "llm_backup_calls": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T13:06:25.305299+00:00"
}
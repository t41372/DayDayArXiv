[
  {
    "arxiv_id": "2409.10790v1",
    "title": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering",
    "authors": [
      "Qingru Zhang",
      "Xiaodong Yu",
      "Chandan Singh",
      "Xiaodong Liu",
      "Liyuan Liu",
      "Jianfeng Gao",
      "Tuo Zhao",
      "Dan Roth",
      "Hao Cheng"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious real-world tasks. However, they often struggle to fully comprehend and\neffectively utilize their input contexts, resulting in responses that are\nunfaithful or hallucinated. This difficulty increases for contexts that are\nlong or contain distracting information, which can divert LLMs from fully\ncapturing essential evidence. To address this issue, many works use prompting\nto help LLMs utilize contextual information more faithfully. For instance,\niterative prompting highlights key information in two steps that first ask the\nLLM to identify important pieces of context and then derive answers\naccordingly. However, prompting methods are constrained to highlighting key\ninformation implicitly in token space, which is often insufficient to fully\nsteer the model's attention. To improve model faithfulness more reliably, we\npropose AutoPASTA, a method that automatically identifies key contextual\ninformation and explicitly highlights it by steering an LLM's attention scores.\nLike prompting, AutoPASTA is applied at inference time and does not require\nchanging any model parameters. Our experiments on open-book QA demonstrate that\nAutoPASTA effectively enables models to grasp essential contextual information,\nleading to substantially improved model faithfulness and performance, e.g., an\naverage improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly\navailable at https://github.com/QingruZhang/AutoPASTA .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.10790v1",
    "published_date": "2024-09-16 23:52:41 UTC",
    "updated_date": "2024-09-16 23:52:41 UTC"
  },
  {
    "arxiv_id": "2409.10775v1",
    "title": "Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?",
    "authors": [
      "Kaleb Kassaw",
      "Francesco Luzi",
      "Leslie M. Collins",
      "Jordan M. Malof"
    ],
    "abstract": "Image classification models, including convolutional neural networks (CNNs),\nperform well on a variety of classification tasks but struggle under conditions\nof partial occlusion, i.e., conditions in which objects are partially covered\nfrom the view of a camera. Methods to improve performance under occlusion,\nincluding data augmentation, part-based clustering, and more inherently robust\narchitectures, including Vision Transformer (ViT) models, have, to some extent,\nbeen evaluated on their ability to classify objects under partial occlusion.\nHowever, evaluations of these methods have largely relied on images containing\nartificial occlusion, which are typically computer-generated and therefore\ninexpensive to label. Additionally, methods are rarely compared against each\nother, and many methods are compared against early, now outdated, deep learning\nmodels. We contribute the Image Recognition Under Occlusion (IRUO) dataset,\nbased on the recently developed Occluded Video Instance Segmentation (OVIS)\ndataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded\nimages to test and benchmark leading methods' robustness to partial occlusion\nin visual recognition tasks. In addition, we contribute the design and results\nof a human study using images from IRUO that evaluates human classification\nperformance at multiple levels and types of occlusion. We find that modern\nCNN-based models show improved recognition accuracy on occluded images compared\nto earlier CNN-based models, and ViT-based models are more accurate than\nCNN-based models on occluded images, performing only modestly worse than human\naccuracy. We also find that certain types of occlusion, including diffuse\nocclusion, where relevant objects are seen through \"holes\" in occluders such as\nfences and leaves, can greatly reduce the accuracy of deep recognition models\nas compared to humans, especially those with CNN backbones.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10775v1",
    "published_date": "2024-09-16 23:21:22 UTC",
    "updated_date": "2024-09-16 23:21:22 UTC"
  },
  {
    "arxiv_id": "2409.10756v1",
    "title": "VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching",
    "authors": [
      "Arastoo Zibaeirad",
      "Marco Vieira"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise in tasks like code\ntranslation, prompting interest in their potential for automating software\nvulnerability detection (SVD) and patching (SVP). To further research in this\narea, establishing a benchmark is essential for evaluating the strengths and\nlimitations of LLMs in these tasks. Despite their capabilities, questions\nremain regarding whether LLMs can accurately analyze complex vulnerabilities\nand generate appropriate patches. This paper introduces VulnLLMEval, a\nframework designed to assess the performance of LLMs in identifying and\npatching vulnerabilities in C code. Our study includes 307 real-world\nvulnerabilities extracted from the Linux kernel, creating a well-curated\ndataset that includes both vulnerable and patched code. This dataset, based on\nreal-world code, provides a diverse and representative testbed for evaluating\nLLM performance in SVD and SVP tasks, offering a robust foundation for rigorous\nassessment. Our results reveal that LLMs often struggle with distinguishing\nbetween vulnerable and patched code. Furthermore, in SVP tasks, these models\ntend to oversimplify the code, producing solutions that may not be directly\nusable without further refinement.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10756v1",
    "published_date": "2024-09-16 22:00:20 UTC",
    "updated_date": "2024-09-16 22:00:20 UTC"
  },
  {
    "arxiv_id": "2409.10737v2",
    "title": "AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing",
    "authors": [
      "Ana Nunez",
      "Nafis Tanveer Islam",
      "Sumit Kumar Jha",
      "Peyman Najafirad"
    ],
    "abstract": "Recent advancements in automatic code generation using large language models\n(LLMs) have brought us closer to fully automated secure software development.\nHowever, existing approaches often rely on a single agent for code generation,\nwhich struggles to produce secure, vulnerability-free code. Traditional program\nsynthesis with LLMs has primarily focused on functional correctness, often\nneglecting critical dynamic security implications that happen during runtime.\nTo address these challenges, we propose AutoSafeCoder, a multi-agent framework\nthat leverages LLM-driven agents for code generation, vulnerability analysis,\nand security enhancement through continuous collaboration. The framework\nconsists of three agents: a Coding Agent responsible for code generation, a\nStatic Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent\nperforming dynamic testing using a mutation-based fuzzing approach to detect\nruntime errors. Our contribution focuses on ensuring the safety of multi-agent\ncode generation by integrating dynamic and static testing in an iterative\nprocess during code generation by LLM that improves security. Experiments using\nthe SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities\ncompared to baseline LLMs, with no compromise in functionality.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to NeurIPS 2024 Workshop on Safe & Trustworthy Agents",
    "pdf_url": "http://arxiv.org/pdf/2409.10737v2",
    "published_date": "2024-09-16 21:15:56 UTC",
    "updated_date": "2024-11-05 03:00:38 UTC"
  },
  {
    "arxiv_id": "2409.10728v2",
    "title": "Generalized Measures of Anticipation and Responsivity in Online Language Processing",
    "authors": [
      "Mario Giulianelli",
      "Andreas Opedal",
      "Ryan Cotterell"
    ],
    "abstract": "We introduce a generalization of classic information-theoretic measures of\npredictive uncertainty in online language processing, based on the simulation\nof expected continuations of incremental linguistic contexts. Our framework\nprovides a formal definition of anticipatory and responsive measures, and it\nequips experimenters with the tools to define new, more expressive measures\nbeyond standard next-symbol entropy and surprisal. While extracting these\nstandard quantities from language models is convenient, we demonstrate that\nusing Monte Carlo simulation to estimate alternative responsive and\nanticipatory measures pays off empirically: New special cases of our\ngeneralized formula exhibit enhanced predictive power compared to surprisal for\nhuman cloze completion probability as well as ELAN, LAN, and N400 amplitudes,\nand greater complementarity with surprisal in predicting reading times.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10728v2",
    "published_date": "2024-09-16 21:05:15 UTC",
    "updated_date": "2024-10-12 15:28:12 UTC"
  },
  {
    "arxiv_id": "2409.10721v1",
    "title": "A Missing Data Imputation GAN for Character Sprite Generation",
    "authors": [
      "Flávio Coutinho",
      "Luiz Chaimowicz"
    ],
    "abstract": "Creating and updating pixel art character sprites with many frames spanning\ndifferent animations and poses takes time and can quickly become repetitive.\nHowever, that can be partially automated to allow artists to focus on more\ncreative tasks. In this work, we concentrate on creating pixel art character\nsprites in a target pose from images of them facing other three directions. We\npresent a novel approach to character generation by framing the problem as a\nmissing data imputation task. Our proposed generative adversarial networks\nmodel receives the images of a character in all available domains and produces\nthe image of the missing pose. We evaluated our approach in the scenarios with\none, two, and three missing images, achieving similar or better results to the\nstate-of-the-art when more images are available. We also evaluate the impact of\nthe proposed changes to the base architecture.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in SBGames 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10721v1",
    "published_date": "2024-09-16 20:50:32 UTC",
    "updated_date": "2024-09-16 20:50:32 UTC"
  },
  {
    "arxiv_id": "2409.10715v2",
    "title": "Self-Attention Limits Working Memory Capacity of Transformer-Based Models",
    "authors": [
      "Dongyu Gong",
      "Hantao Zhang"
    ],
    "abstract": "Recent work on Transformer-based large language models (LLMs) has revealed\nstriking limits in their working memory capacity, similar to what has been\nfound in human behavioral studies. Specifically, these models' performance\ndrops significantly on N-back tasks as N increases. However, there is still a\nlack of mechanistic interpretability as to why this phenomenon would arise.\nInspired by the executive attention theory from behavioral sciences, we\nhypothesize that the self-attention mechanism within Transformer-based models\nmight be responsible for their working memory capacity limits. To test this\nhypothesis, we train vanilla decoder-only transformers to perform N-back tasks\nand find that attention scores gradually aggregate to the N-back positions over\ntraining, suggesting that the model masters the task by learning a strategy to\npay attention to the relationship between the current position and the N-back\nposition. Critically, we find that the total entropy of the attention score\nmatrix increases as N increases, suggesting that the dispersion of attention\nscores might be the cause of the capacity limit observed in N-back tasks. Our\nfindings thus offer insights into the shared role of attention in both human\nand artificial intelligence. Moreover, the limitations of the self-attention\nmechanism revealed in the current study could inform future efforts to design\nmore powerful model architectures with enhanced working memory capacity and\ncognitive capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.10715v2",
    "published_date": "2024-09-16 20:38:35 UTC",
    "updated_date": "2024-11-16 20:50:11 UTC"
  },
  {
    "arxiv_id": "2409.18986v2",
    "title": "Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine",
    "authors": [
      "Xiaoyu Wang",
      "Haoyong Ouyang",
      "Balu Bhasuran",
      "Xiao Luo",
      "Karim Hanna",
      "Mia Liza A. Lustria",
      "Carl Yang",
      "Zhe He"
    ],
    "abstract": "Accurate interpretation of lab results is crucial in clinical medicine, yet\nmost patient portals use universal normal ranges, ignoring conditional factors\nlike age and gender. This study introduces Lab-AI, an interactive system that\noffers personalized normal ranges using retrieval-augmented generation (RAG)\nfrom credible health sources. Lab-AI has two modules: factor retrieval and\nnormal range retrieval. We tested these on 122 lab tests: 40 with conditional\nfactors and 82 without. For tests with factors, normal ranges depend on\npatient-specific information. Our results show GPT-4-turbo with RAG achieved a\n0.948 F1 score for factor retrieval and 0.995 accuracy for normal range\nretrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5%\nin factor retrieval and showed 132% and 100% improvements in question-level and\nlab-level performance, respectively, for normal range retrieval. These findings\nhighlight Lab-AI's potential to enhance patient understanding of lab results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18986v2",
    "published_date": "2024-09-16 20:36:17 UTC",
    "updated_date": "2025-04-23 19:19:42 UTC"
  },
  {
    "arxiv_id": "2409.10704v1",
    "title": "Self-supervised Speech Models for Word-Level Stuttered Speech Detection",
    "authors": [
      "Yi-Jen Shih",
      "Zoi Gkalitsiou",
      "Alexandros G. Dimakis",
      "David Harwath"
    ],
    "abstract": "Clinical diagnosis of stuttering requires an assessment by a licensed\nspeech-language pathologist. However, this process is time-consuming and\nrequires clinicians with training and experience in stuttering and fluency\ndisorders. Unfortunately, only a small percentage of speech-language\npathologists report being comfortable working with individuals who stutter,\nwhich is inadequate to accommodate for the 80 million individuals who stutter\nworldwide. Developing machine learning models for detecting stuttered speech\nwould enable universal and automated screening for stuttering, enabling speech\npathologists to identify and follow up with patients who are most likely to be\ndiagnosed with a stuttering speech disorder. Previous research in this area has\npredominantly focused on utterance-level detection, which is not sufficient for\nclinical settings where word-level annotation of stuttering is the norm. In\nthis study, we curated a stuttered speech dataset with word-level annotations\nand introduced a word-level stuttering speech detection model leveraging\nself-supervised speech models. Our evaluation demonstrates that our model\nsurpasses previous approaches in word-level stuttering speech detection.\nAdditionally, we conducted an extensive ablation analysis of our method,\nproviding insight into the most important aspects of adapting self-supervised\nspeech models for stuttered speech detection.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by IEEE SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10704v1",
    "published_date": "2024-09-16 20:18:20 UTC",
    "updated_date": "2024-09-16 20:18:20 UTC"
  },
  {
    "arxiv_id": "2409.10702v2",
    "title": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs",
    "authors": [
      "Yifan Wang",
      "David Stevens",
      "Pranay Shah",
      "Wenwen Jiang",
      "Miao Liu",
      "Xu Chen",
      "Robert Kuo",
      "Na Li",
      "Boying Gong",
      "Daniel Lee",
      "Jiabo Hu",
      "Ning Zhang",
      "Bob Kamma"
    ],
    "abstract": "The growing demand for AI training data has transformed data annotation into\na global industry, but traditional approaches relying on human annotators are\noften time-consuming, labor-intensive, and prone to inconsistent quality. We\npropose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models\ninto the annotation process. Our research introduces a collaborative paradigm\nthat leverages the strengths of both professional human annotators and large\nlanguage models (LLMs). By employing LLMs as pre-annotation and real-time\nassistants, and judges on annotator responses, MILO enables effective\ninteraction patterns between human annotators and LLMs. Three empirical studies\non multimodal data annotation demonstrate MILO's efficacy in reducing handling\ntime, improving data quality, and enhancing annotator experiences. We also\nintroduce quality rubrics for flexible evaluation and fine-grained feedback on\nopen-ended annotations. The MILO framework has implications for accelerating\nAI/ML development, reducing reliance on human annotation alone, and promoting\nbetter alignment between human and machine values.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10702v2",
    "published_date": "2024-09-16 20:05:57 UTC",
    "updated_date": "2024-09-24 05:00:07 UTC"
  },
  {
    "arxiv_id": "2409.11442v1",
    "title": "A Green Multi-Attribute Client Selection for Over-The-Air Federated Learning: A Grey-Wolf-Optimizer Approach",
    "authors": [
      "Maryam Ben Driss",
      "Essaid Sabir",
      "Halima Elbiaze",
      "Abdoulaye Baniré Diallo",
      "Mohamed Sadik"
    ],
    "abstract": "Federated Learning (FL) has gained attention across various industries for\nits capability to train machine learning models without centralizing sensitive\ndata. While this approach offers significant benefits such as privacy\npreservation and decreased communication overhead, it presents several\nchallenges, including deployment complexity and interoperability issues,\nparticularly in heterogeneous scenarios or resource-constrained environments.\nOver-the-air (OTA) FL was introduced to tackle these challenges by\ndisseminating model updates without necessitating direct device-to-device\nconnections or centralized servers. However, OTA-FL brought forth limitations\nassociated with heightened energy consumption and network latency. In this\npaper, we propose a multi-attribute client selection framework employing the\ngrey wolf optimizer (GWO) to strategically control the number of participants\nin each round and optimize the OTA-FL process while considering accuracy,\nenergy, delay, reliability, and fairness constraints of participating devices.\nWe evaluate the performance of our multi-attribute client selection approach in\nterms of model loss minimization, convergence time reduction, and energy\nefficiency. In our experimental evaluation, we assessed and compared the\nperformance of our approach against the existing state-of-the-art methods. Our\nresults demonstrate that the proposed GWO-based client selection outperforms\nthese baselines across various metrics. Specifically, our approach achieves a\nnotable reduction in model loss, accelerates convergence time, and enhances\nenergy efficiency while maintaining high fairness and reliability indicators.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11442v1",
    "published_date": "2024-09-16 20:03:57 UTC",
    "updated_date": "2024-09-16 20:03:57 UTC"
  },
  {
    "arxiv_id": "2409.10695v2",
    "title": "Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models",
    "authors": [
      "Bingchen Liu",
      "Ehsan Akhgari",
      "Alexander Visheratin",
      "Aleks Kamko",
      "Linmiao Xu",
      "Shivam Shrirao",
      "Chase Lambert",
      "Joao Souza",
      "Suhail Doshi",
      "Daiqing Li"
    ],
    "abstract": "We introduce Playground v3 (PGv3), our latest text-to-image model that\nachieves state-of-the-art (SoTA) performance across multiple testing\nbenchmarks, excels in graphic design abilities and introduces new capabilities.\nUnlike traditional text-to-image generative models that rely on pre-trained\nlanguage models like T5 or CLIP text encoders, our approach fully integrates\nLarge Language Models (LLMs) with a novel structure that leverages text\nconditions exclusively from a decoder-only LLM. Additionally, to enhance image\ncaptioning quality-we developed an in-house captioner, capable of generating\ncaptions with varying levels of detail, enriching the diversity of text\nstructures. We also introduce a new benchmark CapsBench to evaluate detailed\nimage captioning performance. Experimental results demonstrate that PGv3 excels\nin text prompt adherence, complex reasoning, and accurate text rendering. User\npreference studies indicate the super-human graphic design ability of our model\nfor common design applications, such as stickers, posters, and logo designs.\nFurthermore, PGv3 introduces new capabilities, including precise RGB color\ncontrol and robust multilingual understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://playground.com/pg-v3",
    "pdf_url": "http://arxiv.org/pdf/2409.10695v2",
    "published_date": "2024-09-16 19:52:24 UTC",
    "updated_date": "2024-10-21 20:01:13 UTC"
  },
  {
    "arxiv_id": "2409.10692v1",
    "title": "Encoding Reusable Multi-Robot Planning Strategies as Abstract Hypergraphs",
    "authors": [
      "Khen Elimelech",
      "James Motes",
      "Marco Morales",
      "Nancy M. Amato",
      "Moshe Y. Vardi",
      "Lydia E. Kavraki"
    ],
    "abstract": "Multi-Robot Task Planning (MR-TP) is the search for a discrete-action plan a\nteam of robots should take to complete a task. The complexity of such problems\nscales exponentially with the number of robots and task complexity, making them\nchallenging for online solution. To accelerate MR-TP over a system's lifetime,\nthis work looks at combining two recent advances: (i) Decomposable State Space\nHypergraph (DaSH), a novel hypergraph-based framework to efficiently model and\nsolve MR-TP problems; and \\mbox{(ii) learning-by-abstraction,} a technique that\nenables automatic extraction of generalizable planning strategies from\nindividual planning experiences for later reuse. Specifically, we wish to\nextend this strategy-learning technique, originally designed for single-robot\nplanning, to benefit multi-robot planning using hypergraph-based MR-TP.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10692v1",
    "published_date": "2024-09-16 19:39:52 UTC",
    "updated_date": "2024-09-16 19:39:52 UTC"
  },
  {
    "arxiv_id": "2409.10683v1",
    "title": "MotIF: Motion Instruction Fine-tuning",
    "authors": [
      "Minyoung Hwang",
      "Joey Hejna",
      "Dorsa Sadigh",
      "Yonatan Bisk"
    ],
    "abstract": "While success in many robotics tasks can be determined by only observing the\nfinal state and how it differs from the initial state - e.g., if an apple is\npicked up - many tasks require observing the full motion of the robot to\ncorrectly determine success. For example, brushing hair requires repeated\nstrokes that correspond to the contours and type of hair. Prior works often use\noff-the-shelf vision-language models (VLMs) as success detectors; however, when\nsuccess depends on the full trajectory, VLMs struggle to make correct judgments\nfor two reasons. First, modern VLMs are trained only on single frames, and\ncannot capture changes over a full trajectory. Second, even if we provide\nstate-of-the-art VLMs with an aggregate input of multiple frames, they still\nfail to detect success due to a lack of robot data. Our key idea is to\nfine-tune VLMs using abstract representations that are able to capture\ntrajectory-level information such as the path the robot takes by overlaying\nkeypoint trajectories on the final image. We propose motion instruction\nfine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned\nabstract representations to semantically ground the robot's behavior in the\nenvironment. To benchmark and fine-tune VLMs for robotic motion understanding,\nwe introduce the MotIF-1K dataset containing 653 human and 369 robot\ndemonstrations across 13 task categories. MotIF assesses the success of robot\nmotion given the image observation of the trajectory, task instruction, and\nmotion description. Our model significantly outperforms state-of-the-art VLMs\nby at least twice in precision and 56.1% in recall, generalizing across unseen\nmotions, tasks, and environments. Finally, we demonstrate practical\napplications of MotIF in refining and terminating robot planning, and ranking\ntrajectories on how they align with task and motion descriptions. Project page:\nhttps://motif-1k.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10683v1",
    "published_date": "2024-09-16 19:30:21 UTC",
    "updated_date": "2024-09-16 19:30:21 UTC"
  },
  {
    "arxiv_id": "2409.10680v1",
    "title": "Multi-agent Path Finding in Continuous Environment",
    "authors": [
      "Kristýna Janovská",
      "Pavel Surynek"
    ],
    "abstract": "We address a variant of multi-agent path finding in continuous environment\n(CE-MAPF), where agents move along sets of smooth curves. Collisions between\nagents are resolved via avoidance in the space domain. A new Continuous\nEnvironment Conflict-Based Search (CE-CBS) algorithm is proposed in this work.\nCE-CBS combines conflict-based search (CBS) for the high-level search framework\nwith RRT* for low-level path planning. The CE-CBS algorithm is tested under\nvarious settings on diverse CE-MAPF instances. Experimental results show that\nCE-CBS is competitive w.r.t. to other algorithms that consider continuous\naspect in MAPF such as MAPF with continuous time.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "The 36th IEEE International Conference on Tools with Artificial\n  Intelligence (ICTAI). 2024, In press",
    "pdf_url": "http://arxiv.org/pdf/2409.10680v1",
    "published_date": "2024-09-16 19:23:04 UTC",
    "updated_date": "2024-09-16 19:23:04 UTC"
  },
  {
    "arxiv_id": "2409.10655v2",
    "title": "Disentangling Uncertainty for Safe Social Navigation using Deep Reinforcement Learning",
    "authors": [
      "Daniel Flögel",
      "Marcos Gómez Villafañe",
      "Joshua Ransiek",
      "Sören Hohmann"
    ],
    "abstract": "Autonomous mobile robots are increasingly used in pedestrian-rich\nenvironments where safe navigation and appropriate human interaction are\ncrucial. While Deep Reinforcement Learning (DRL) enables socially integrated\nrobot behavior, challenges persist in novel or perturbed scenarios to indicate\nwhen and why the policy is uncertain. Unknown uncertainty in decision-making\ncan lead to collisions or human discomfort and is one reason why safe and\nrisk-aware navigation is still an open problem. This work introduces a novel\napproach that integrates aleatoric, epistemic, and predictive uncertainty\nestimation into a DRL navigation framework for policy distribution uncertainty\nestimates. We, therefore, incorporate Observation-Dependent Variance (ODV) and\ndropout into the Proximal Policy Optimization (PPO) algorithm. For different\ntypes of perturbations, we compare the ability of deep ensembles and\nMonte-Carlo dropout (MC-dropout) to estimate the uncertainties of the policy.\nIn uncertain decision-making situations, we propose to change the robot's\nsocial behavior to conservative collision avoidance. The results show improved\ntraining performance with ODV and dropout in PPO and reveal that the training\nscenario has an impact on the generalization. In addition, MC-dropout is more\nsensitive to perturbations and correlates the uncertainty type to the\nperturbation better. With the safe action selection, the robot can navigate in\nperturbed environments with fewer collisions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to the IEEE for possible publication, 8 pages, 6 figures\n  and 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.10655v2",
    "published_date": "2024-09-16 18:49:38 UTC",
    "updated_date": "2025-02-28 15:38:12 UTC"
  },
  {
    "arxiv_id": "2409.13761v2",
    "title": "Do Large Language Models Need a Content Delivery Network?",
    "authors": [
      "Yihua Cheng",
      "Kuntai Du",
      "Jiayi Yao",
      "Junchen Jiang"
    ],
    "abstract": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13761v2",
    "published_date": "2024-09-16 18:46:24 UTC",
    "updated_date": "2024-10-21 15:59:18 UTC"
  },
  {
    "arxiv_id": "2409.10653v2",
    "title": "Logic Synthesis Optimization with Predictive Self-Supervision via Causal Transformers",
    "authors": [
      "Raika Karimi",
      "Faezeh Faez",
      "Yingxue Zhang",
      "Xing Li",
      "Lei Chen",
      "Mingxuan Yuan",
      "Mahdi Biparva"
    ],
    "abstract": "Contemporary hardware design benefits from the abstraction provided by\nhigh-level logic gates, streamlining the implementation of logic circuits.\nLogic Synthesis Optimization (LSO) operates at one level of abstraction within\nthe Electronic Design Automation (EDA) workflow, targeting improvements in\nlogic circuits with respect to performance metrics such as size and speed in\nthe final layout. Recent trends in the field show a growing interest in\nleveraging Machine Learning (ML) for EDA, notably through ML-guided logic\nsynthesis utilizing policy-based Reinforcement Learning (RL) methods.Despite\nthese advancements, existing models face challenges such as overfitting and\nlimited generalization, attributed to constrained public circuits and the\nexpressiveness limitations of graph encoders. To address these hurdles, and\ntackle data scarcity issues, we introduce LSOformer, a novel approach\nharnessing Autoregressive transformer models and predictive SSL to predict the\ntrajectory of Quality of Results (QoR). LSOformer integrates cross-attention\nmodules to merge insights from circuit graphs and optimization sequences,\nthereby enhancing prediction accuracy for QoR metrics. Experimental studies\nvalidate the effectiveness of LSOformer, showcasing its superior performance\nover baseline architectures in QoR prediction tasks, where it achieves\nimprovements of 5.74%, 4.35%, and 17.06% on the EPFL, OABCD, and proprietary\ncircuits datasets, respectively, in inductive setup.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10653v2",
    "published_date": "2024-09-16 18:45:07 UTC",
    "updated_date": "2025-02-28 16:04:54 UTC"
  },
  {
    "arxiv_id": "2409.10640v2",
    "title": "Exploring Fine-tuned Generative Models for Keyphrase Selection: A Case Study for Russian",
    "authors": [
      "Anna Glazkova",
      "Dmitry Morozov"
    ],
    "abstract": "Keyphrase selection plays a pivotal role within the domain of scholarly\ntexts, facilitating efficient information retrieval, summarization, and\nindexing. In this work, we explored how to apply fine-tuned generative\ntransformer-based models to the specific task of keyphrase selection within\nRussian scientific texts. We experimented with four distinct generative models,\nsuch as ruT5, ruGPT, mT5, and mBART, and evaluated their performance in both\nin-domain and cross-domain settings. The experiments were conducted on the\ntexts of Russian scientific abstracts from four domains: mathematics & computer\nscience, history, medicine, and linguistics. The use of generative models,\nnamely mBART, led to gains in in-domain performance (up to 4.9% in BERTScore,\n9.0% in ROUGE-1, and 12.2% in F1-score) over three keyphrase extraction\nbaselines for the Russian language. Although the results for cross-domain usage\nwere significantly lower, they still demonstrated the capability to surpass\nbaseline performances in several cases, underscoring the promising potential\nfor further exploration and refinement in this research field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7; I.7.m; H.3.3"
    ],
    "primary_category": "cs.CL",
    "comment": "DAMDID-2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10640v2",
    "published_date": "2024-09-16 18:15:28 UTC",
    "updated_date": "2024-09-18 07:35:46 UTC"
  },
  {
    "arxiv_id": "2409.10515v1",
    "title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems",
    "authors": [
      "Hitesh Tulsiani",
      "David M. Chan",
      "Shalini Ghosh",
      "Garima Lalwani",
      "Prabhat Pandey",
      "Ankish Bansal",
      "Sri Garimella",
      "Ariya Rastrow",
      "Björn Hoffmeister"
    ],
    "abstract": "Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Presented at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10515v1",
    "published_date": "2024-09-16 17:59:50 UTC",
    "updated_date": "2024-09-16 17:59:50 UTC"
  },
  {
    "arxiv_id": "2409.10594v1",
    "title": "Kolmogorov-Arnold Transformer",
    "authors": [
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "Transformers stand as the cornerstone of mordern deep learning.\nTraditionally, these models rely on multi-layer perceptron (MLP) layers to mix\nthe information between channels. In this paper, we introduce the\nKolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP\nlayers with Kolmogorov-Arnold Network (KAN) layers to enhance the\nexpressiveness and performance of the model. Integrating KANs into\ntransformers, however, is no easy feat, especially when scaled up.\nSpecifically, we identify three key challenges: (C1) Base function. The\nstandard B-spline function used in KANs is not optimized for parallel computing\non modern hardware, resulting in slower inference speeds. (C2) Parameter and\nComputation Inefficiency. KAN requires a unique function for each input-output\npair, making the computation extremely large. (C3) Weight initialization. The\ninitialization of weights in KANs is particularly challenging due to their\nlearnable activation functions, which are critical for achieving convergence in\ndeep neural networks. To overcome the aforementioned challenges, we propose\nthree key solutions: (S1) Rational basis. We replace B-spline functions with\nrational functions to improve compatibility with modern GPUs. By implementing\nthis in CUDA, we achieve faster computations. (S2) Group KAN. We share the\nactivation weights through a group of neurons, to reduce the computational load\nwithout sacrificing performance. (S3) Variance-preserving initialization. We\ncarefully initialize the activation weights to make sure that the activation\nvariance is maintained across layers. With these designs, KAT scales\neffectively and readily outperforms traditional MLP-based transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/Adamdad/kat",
    "pdf_url": "http://arxiv.org/pdf/2409.10594v1",
    "published_date": "2024-09-16 17:54:51 UTC",
    "updated_date": "2024-09-16 17:54:51 UTC"
  },
  {
    "arxiv_id": "2409.10593v3",
    "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios",
    "authors": [
      "Luning Wang",
      "Shiyao Li",
      "Xuefei Ning",
      "Zhihang Yuan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.10593v3",
    "published_date": "2024-09-16 17:36:50 UTC",
    "updated_date": "2024-10-18 19:30:35 UTC"
  },
  {
    "arxiv_id": "2409.10496v5",
    "title": "MusicLIME: Explainable Multimodal Music Understanding",
    "authors": [
      "Theodoros Sotirou",
      "Vassilis Lyberatos",
      "Orfeas Menis Mastromichalakis",
      "Giorgos Stamou"
    ],
    "abstract": "Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "GitHub repository: https://github.com/IamTheo2000/MusicLIME. To be\n  presented at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.10496v5",
    "published_date": "2024-09-16 17:28:21 UTC",
    "updated_date": "2025-03-17 18:21:48 UTC"
  },
  {
    "arxiv_id": "2409.10489v4",
    "title": "Flash STU: Fast Spectral Transform Units",
    "authors": [
      "Y. Isabel Liu",
      "Windsor Nguyen",
      "Yagiz Devre",
      "Evan Dogariu",
      "Anirudha Majumdar",
      "Elad Hazan"
    ],
    "abstract": "Recent advances in state-space model architectures have shown great promise\nfor efficient sequence modeling, but challenges remain in balancing\ncomputational efficiency with model expressiveness. We propose the Flash STU\narchitecture, a hybrid model that interleaves spectral state space model layers\nwith sliding window attention, enabling scalability to billions of parameters\nfor language modeling while maintaining a near-linear time complexity. We\nevaluate the Flash STU and its variants on diverse sequence prediction tasks,\nincluding linear dynamical systems, robotics control, and language modeling. We\nfind that, given a fixed parameter budget, the Flash STU architecture\nconsistently outperforms the Transformer and other leading state-space models\nsuch as S4 and Mamba-2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10489v4",
    "published_date": "2024-09-16 17:22:34 UTC",
    "updated_date": "2025-04-07 22:47:40 UTC"
  },
  {
    "arxiv_id": "2409.10488v1",
    "title": "Do Pre-trained Vision-Language Models Encode Object States?",
    "authors": [
      "Kaleb Newman",
      "Shijie Wang",
      "Yuan Zang",
      "David Heffren",
      "Chen Sun"
    ],
    "abstract": "For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10488v1",
    "published_date": "2024-09-16 17:22:18 UTC",
    "updated_date": "2024-09-16 17:22:18 UTC"
  },
  {
    "arxiv_id": "2409.10481v1",
    "title": "Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance",
    "authors": [
      "Simone Maurizio La Cava",
      "Sara Concas",
      "Ruben Tolosana",
      "Roberto Casula",
      "Giulia Orrù",
      "Martin Drahansky",
      "Julian Fierrez",
      "Gian Luca Marcialis"
    ],
    "abstract": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to distinct application scenarios. These assumptions limit their use\nwhen acquisition conditions, such as the subject's distance from the camera or\nthe camera's characteristics, are different than expected, as typically happens\nin video surveillance. Additionally, 3DFR algorithms follow various strategies\nto address the reconstruction of a 3D shape from 2D data, such as statistical\nmodel fitting, photometric stereo, or deep learning. In the present study, we\nexplore the application of three 3DFR algorithms representative of the SOTA,\nemploying each one as the template set generator for a face verification\nsystem. The scores provided by each system are combined by score-level fusion.\nWe show that the complementarity induced by different 3DFR algorithms improves\nperformance when tests are conducted at never-seen-before distances from the\ncamera and camera characteristics (cross-distance and cross-camera settings),\nthus encouraging further investigations on multiple 3DFR-based approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at T-CAP - Towards a Complete Analysis of People:\n  Fine-grained Understanding for Real-World Applications, workshop in\n  conjunction with the 18th European Conference on Computer Vision ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10481v1",
    "published_date": "2024-09-16 17:17:47 UTC",
    "updated_date": "2024-09-16 17:17:47 UTC"
  },
  {
    "arxiv_id": "2409.10473v1",
    "title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion",
    "authors": [
      "Lehong Wu",
      "Lilang Lin",
      "Jiahang Zhang",
      "Yiyang Ma",
      "Jiaying Liu"
    ],
    "abstract": "Self-supervised learning has proved effective for skeleton-based human action\nunderstanding. However, previous works either rely on contrastive learning that\nsuffers false negative problems or are based on reconstruction that learns too\nmuch unessential low-level clues, leading to limited representations for\ndownstream tasks. Recently, great advances have been made in generative\nlearning, which is naturally a challenging yet meaningful pretext task to model\nthe general underlying data distributions. However, the representation learning\ncapacity of generative models is under-explored, especially for the skeletons\nwith spacial sparsity and temporal redundancy. To this end, we propose Masked\nConditional Diffusion (MacDiff) as a unified framework for human skeleton\nmodeling. For the first time, we leverage diffusion models as effective\nskeleton representation learners. Specifically, we train a diffusion decoder\nconditioned on the representations extracted by a semantic encoder. Random\nmasking is applied to encoder inputs to introduce a information bottleneck and\nremove redundancy of skeletons. Furthermore, we theoretically demonstrate that\nour generative objective involves the contrastive learning objective which\naligns the masked and noisy views. Meanwhile, it also enforces the\nrepresentation to complement for the noisy view, leading to better\ngeneralization performance. MacDiff achieves state-of-the-art performance on\nrepresentation learning benchmarks while maintaining the competence for\ngenerative tasks. Moreover, we leverage the diffusion model for data\naugmentation, significantly enhancing the fine-tuning performance in scenarios\nwith scarce labeled data. Our project is available at\nhttps://lehongwu.github.io/ECCV24MacDiff/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10473v1",
    "published_date": "2024-09-16 17:06:10 UTC",
    "updated_date": "2024-09-16 17:06:10 UTC"
  },
  {
    "arxiv_id": "2410.01824v2",
    "title": "AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers",
    "authors": [
      "Alexander Wuttke",
      "Matthias Aßenmacher",
      "Christopher Klamm",
      "Max M. Lang",
      "Quirin Würschinger",
      "Frauke Kreuter"
    ],
    "abstract": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01824v2",
    "published_date": "2024-09-16 16:03:08 UTC",
    "updated_date": "2025-03-12 09:55:22 UTC"
  },
  {
    "arxiv_id": "2409.10419v2",
    "title": "HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models",
    "authors": [
      "Vineet Bhat",
      "Prashanth Krishnamurthy",
      "Ramesh Karri",
      "Farshad Khorrami"
    ],
    "abstract": "Robots interacting with humans through natural language can unlock numerous\napplications such as Referring Grasp Synthesis (RGS). Given a text query, RGS\ndetermines a stable grasp pose to manipulate the referred object in the robot's\nworkspace. RGS comprises two steps: visual grounding and grasp pose estimation.\nRecent studies leverage powerful Vision-Language Models (VLMs) for visually\ngrounding free-flowing natural language in real-world robotic execution.\nHowever, comparisons in complex, cluttered environments with multiple instances\nof the same object are lacking. This paper introduces HiFi-CS, featuring\nhierarchical application of Featurewise Linear Modulation (FiLM) to fuse image\nand text embeddings, enhancing visual grounding for complex attribute rich text\nqueries encountered in robotic grasping. Visual grounding associates an object\nin 2D/3D space with natural language input and is studied in two scenarios:\nClosed and Open Vocabulary. HiFi-CS features a lightweight decoder combined\nwith a frozen VLM and outperforms competitive baselines in closed vocabulary\nsettings while being 100x smaller in size. Our model can effectively guide\nopen-set object detectors like GroundedSAM to enhance open-vocabulary\nperformance. We validate our approach through real-world RGS experiments using\na 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop\nscenes. Our codebase is provided here: https://github.com/vineet2104/hifics",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10419v2",
    "published_date": "2024-09-16 15:50:39 UTC",
    "updated_date": "2025-03-12 21:30:37 UTC"
  },
  {
    "arxiv_id": "2409.10416v1",
    "title": "Geometric Clustering for Hardware-Efficient Implementation of Chromatic Dispersion Compensation",
    "authors": [
      "Geraldo Gomes",
      "Pedro Freire",
      "Jaroslaw E. Prilepsky",
      "Sergei K. Turitsyn"
    ],
    "abstract": "Power efficiency remains a significant challenge in modern optical fiber\ncommunication systems, driving efforts to reduce the computational complexity\nof digital signal processing, particularly in chromatic dispersion compensation\n(CDC) algorithms. While various strategies for complexity reduction have been\nproposed, many lack the necessary hardware implementation to validate their\nbenefits. This paper provides a theoretical analysis of the tap overlapping\neffect in CDC filters for coherent receivers, introduces a novel Time-Domain\nClustered Equalizer (TDCE) technique based on this concept, and presents a\nField-Programmable Gate Array (FPGA) implementation for validation. We\ndeveloped an innovative parallelization method for TDCE, implementing it in\nhardware for fiber lengths up to 640 km. A fair comparison with the\nstate-of-the-art frequency domain equalizer (FDE) under identical conditions is\nalso conducted. Our findings highlight that implementation strategies,\nincluding parallelization and memory management, are as crucial as\ncomputational complexity in determining hardware complexity and energy\nefficiency. The proposed TDCE hardware implementation achieves up to 70.7\\%\nenergy savings and 71.4\\% multiplier usage savings compared to FDE, despite its\nhigher computational complexity.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10416v1",
    "published_date": "2024-09-16 15:48:05 UTC",
    "updated_date": "2024-09-16 15:48:05 UTC"
  },
  {
    "arxiv_id": "2409.10403v1",
    "title": "A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration",
    "authors": [
      "Zhang Zheng"
    ],
    "abstract": "This paper proposes a knowledge-enhanced disease diagnosis method based on a\nprompt learning framework. The method retrieves structured knowledge from\nexternal knowledge graphs related to clinical cases, encodes it, and injects it\ninto the prompt templates to enhance the language model's understanding and\nreasoning capabilities for the task.We conducted experiments on three public\ndatasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the\nproposed method significantly outperforms existing models across multiple\nevaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC\ndataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.\nAdditionally,ablation studies confirmed the critical role of the knowledge\ninjection module,as the removal of this module resulted in a significant drop\nin F1 score. The experimental results demonstrate that the proposed method not\nonly effectively improves the accuracy of disease diagnosis but also enhances\nthe interpretability of the predictions, providing more reliable support and\nevidence for clinical diagnosis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Knowledge Enhancement,Disease Diagnosis,Prompt\n  Learning,BERT,Knowledge Graph",
    "pdf_url": "http://arxiv.org/pdf/2409.10403v1",
    "published_date": "2024-09-16 15:34:58 UTC",
    "updated_date": "2024-09-16 15:34:58 UTC"
  },
  {
    "arxiv_id": "2409.10394v1",
    "title": "MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning",
    "authors": [
      "Hwihun Jeong",
      "Se Young Chun",
      "Jongho Lee"
    ],
    "abstract": "Deep learning-based Magnetic Resonance (MR) reconstruction methods have\nfocused on generating high-quality images but they often overlook the impact on\ndownstream tasks (e.g., segmentation) that utilize the reconstructed images.\nCascading separately trained reconstruction network and downstream task network\nhas been shown to introduce performance degradation due to error propagation\nand domain gaps between training datasets. To mitigate this issue, downstream\ntask-oriented reconstruction optimization has been proposed for a single\ndownstream task. Expanding this optimization to multi-task scenarios is not\nstraightforward. In this work, we extended this optimization to sequentially\nintroduced multiple downstream tasks and demonstrated that a single MR\nreconstruction network can be optimized for multiple downstream tasks by\ndeploying continual learning (MOST). MOST integrated techniques from\nreplay-based continual learning and image-guided loss to overcome catastrophic\nforgetting. Comparative experiments demonstrated that MOST outperformed a\nreconstruction network without finetuning, a reconstruction network with\nna\\\"ive finetuning, and conventional continual learning methods. This\nadvancement empowers the application of a single MR reconstruction network for\nmultiple downstream tasks. The source code is available at:\nhttps://github.com/SNU-LIST/MOST",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10394v1",
    "published_date": "2024-09-16 15:31:04 UTC",
    "updated_date": "2024-09-16 15:31:04 UTC"
  },
  {
    "arxiv_id": "2409.11440v1",
    "title": "MARCA: Mamba Accelerator with ReConfigurable Architecture",
    "authors": [
      "Jinhao Li",
      "Shan Huang",
      "Jiaming Xu",
      "Jun Liu",
      "Li Ding",
      "Ningyi Xu",
      "Guohao Dai"
    ],
    "abstract": "We propose a Mamba accelerator with reconfigurable architecture, MARCA.We\npropose three novel approaches in this paper. (1) Reduction alternative PE\narray architecture for both linear and element-wise operations. For linear\noperations, the reduction tree connected to PE arrays is enabled and executes\nthe reduction operation. For element-wise operations, the reduction tree is\ndisabled and the output bypasses. (2) Reusable nonlinear function unit based on\nthe reconfigurable PE. We decompose the exponential function into element-wise\noperations and a shift operation by a fast biased exponential algorithm, and\nthe activation function (SiLU) into a range detection and element-wise\noperations by a piecewise approximation algorithm. Thus, the reconfigurable PEs\nare reused to execute nonlinear functions with negligible accuracy loss.(3)\nIntra-operation and inter-operation buffer management strategy. We propose\nintra-operation buffer management strategy to maximize input data sharing for\nlinear operations within operations, and inter-operation strategy for\nelement-wise operations between operations. We conduct extensive experiments on\nMamba model families with different sizes.MARCA achieves up to\n463.22$\\times$/11.66$\\times$ speedup and up to 9761.42$\\times$/242.52$\\times$\nenergy efficiency compared to Intel Xeon 8358P CPU and NVIDIA Tesla A100 GPU\nimplementations, respectively.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "9 pages, 10 figures, accepted by ICCAD 2024. arXiv admin note: text\n  overlap with arXiv:2001.02514 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2409.11440v1",
    "published_date": "2024-09-16 15:18:33 UTC",
    "updated_date": "2024-09-16 15:18:33 UTC"
  },
  {
    "arxiv_id": "2409.10589v3",
    "title": "Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling",
    "authors": [
      "Jesse van Remmerden",
      "Zaharah Bukhsh",
      "Yingqian Zhang"
    ],
    "abstract": "The Job Shop Scheduling Problem (JSSP) is a complex combinatorial\noptimization problem. While online Reinforcement Learning (RL) has shown\npromise by quickly finding acceptable solutions for JSSP, it faces key\nlimitations: it requires extensive training interactions from scratch leading\nto sample inefficiency, cannot leverage existing high-quality solutions, and\noften yields suboptimal results compared to traditional methods like Constraint\nProgramming (CP). We introduce Offline Reinforcement Learning for Learning to\nDispatch (Offline-LD), which addresses these limitations by learning from\npreviously generated solutions. Our approach is motivated by scenarios where\nhistorical scheduling data and expert solutions are available, although our\ncurrent evaluation focuses on benchmark problems. Offline-LD adapts two\nCQL-based Q-learning methods (mQRDQN and discrete mSAC) for maskable action\nspaces, introduces a novel entropy bonus modification for discrete SAC, and\nexploits reward normalization through preprocessing. Our experiments\ndemonstrate that Offline-LD outperforms online RL on both generated and\nbenchmark instances. Notably, by introducing noise into the expert dataset, we\nachieve similar or better results than those obtained from the expert dataset,\nsuggesting that a more diverse training set is preferable because it contains\ncounterfactual information.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at https://github.com/jesserem/Offline-LD",
    "pdf_url": "http://arxiv.org/pdf/2409.10589v3",
    "published_date": "2024-09-16 15:18:10 UTC",
    "updated_date": "2025-04-13 14:52:43 UTC"
  },
  {
    "arxiv_id": "2409.10372v3",
    "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
    "authors": [
      "Qiliang Chen",
      "Sepehr Ilami",
      "Nunzio Lore",
      "Babak Heydari"
    ],
    "abstract": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10372v3",
    "published_date": "2024-09-16 15:15:51 UTC",
    "updated_date": "2024-10-30 16:45:15 UTC"
  },
  {
    "arxiv_id": "2409.10365v2",
    "title": "Robust image representations with counterfactual contrastive learning",
    "authors": [
      "Mélanie Roschewitz",
      "Fabio De Sousa Ribeiro",
      "Tian Xia",
      "Galvin Khara",
      "Ben Glocker"
    ],
    "abstract": "Contrastive pretraining can substantially increase model generalisation and\ndownstream performance. However, the quality of the learned representations is\nhighly dependent on the data augmentation strategy applied to generate positive\npairs. Positive contrastive pairs should preserve semantic meaning while\ndiscarding unwanted variations related to the data acquisition domain.\nTraditional contrastive pipelines attempt to simulate domain shifts through\npre-defined generic image transformations. However, these do not always mimic\nrealistic and relevant domain variations for medical imaging, such as scanner\ndifferences. To tackle this issue, we herein introduce counterfactual\ncontrastive learning, a novel framework leveraging recent advances in causal\nimage synthesis to create contrastive positive pairs that faithfully capture\nrelevant domain variations. Our method, evaluated across five datasets\nencompassing both chest radiography and mammography data, for two established\ncontrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive\nlearning in terms of robustness to acquisition shift. Notably, counterfactual\ncontrastive learning achieves superior downstream performance on both\nin-distribution and external datasets, especially for images acquired with\nscanners under-represented in the training set. Further experiments show that\nthe proposed framework extends beyond acquisition shifts, with models trained\nwith counterfactual contrastive learning reducing subgroup disparities across\nbiological sex.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive/",
    "pdf_url": "http://arxiv.org/pdf/2409.10365v2",
    "published_date": "2024-09-16 15:11:00 UTC",
    "updated_date": "2025-04-10 16:19:20 UTC"
  },
  {
    "arxiv_id": "2409.10350v1",
    "title": "Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation",
    "authors": [
      "Yifan Xu",
      "Ziming Luo",
      "Qianwei Wang",
      "Vineet Kamat",
      "Carol Menassa"
    ],
    "abstract": "Current open-vocabulary scene graph generation algorithms highly rely on both\n3D scene point cloud data and posed RGB-D images and thus have limited\napplications in scenarios where RGB-D images or camera poses are not readily\navailable. To solve this problem, we propose Point2Graph, a novel end-to-end\npoint cloud-based 3D open-vocabulary scene graph generation framework in which\nthe requirement of posed RGB-D image series is eliminated. This hierarchical\nframework contains room and object detection/segmentation and open-vocabulary\nclassification. For the room layer, we leverage the advantage of merging the\ngeometry-based border detection algorithm with the learning-based region\ndetection to segment rooms and create a \"Snap-Lookup\" framework for\nopen-vocabulary room classification. In addition, we create an end-to-end\npipeline for the object layer to detect and classify 3D objects based solely on\n3D point cloud data. Our evaluation results show that our framework can\noutperform the current state-of-the-art (SOTA) open-vocabulary object and room\nsegmentation and classification algorithm on widely used real-scene datasets.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.10350v1",
    "published_date": "2024-09-16 15:01:28 UTC",
    "updated_date": "2024-09-16 15:01:28 UTC"
  },
  {
    "arxiv_id": "2409.10343v1",
    "title": "Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation",
    "authors": [
      "Tianrui Song",
      "Wenshuo Chao",
      "Hao Liu"
    ],
    "abstract": "Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10343v1",
    "published_date": "2024-09-16 14:57:09 UTC",
    "updated_date": "2024-09-16 14:57:09 UTC"
  },
  {
    "arxiv_id": "2409.10588v7",
    "title": "Opponent Shaping for Antibody Development",
    "authors": [
      "Sebastian Towers",
      "Aleksandra Kalisz",
      "Philippe A. Robert",
      "Alicia Higueruelo",
      "Francesca Vianello",
      "Ming-Han Chloe Tsai",
      "Harrison Steel",
      "Jakob N. Foerster"
    ],
    "abstract": "Anti-viral therapies are typically designed to target only the current\nstrains of a virus. Game theoretically, this corresponds to a short-sighted, or\nmyopic, response. However, therapy-induced selective pressures act on viruses\nto drive the emergence of mutated strains, against which initial therapies have\nreduced efficacy. Building on a computational model of binding between\nantibodies and viral antigens (the Absolut! framework), we design and implement\na genetic simulation of viral evolutionary escape. Crucially, this allows our\nantibody optimisation algorithm to consider and influence the entire escape\ncurve of the virus, i.e. to guide (or \"shape\") the viral evolution. This is\ninspired by opponent shaping which, in general-sum learning, accounts for the\nadaptation of the co-player rather than playing a myopic best response. Hence\nwe call the optimised antibodies shapers. Within our simulations, we\ndemonstrate that our shapers target both current and simulated future viral\nvariants, outperforming the antibodies chosen in a myopic way. Furthermore, we\nshow that shapers exert specific evolutionary pressure on the virus compared to\nmyopic antibodies. Altogether, shapers modify the evolutionary trajectories of\nviral strains and minimise the viral escape compared to their myopic\ncounterparts. While this is a simplified model, we hope that our proposed\nparadigm will facilitate the discovery of better long-lived vaccines and\nantibody therapies in the future, enabled by rapid advancements in the\ncapabilities of simulation tools. Our code is available at\nhttps://github.com/olakalisz/antibody-shapers.",
    "categories": [
      "q-bio.PE",
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "92-08",
      "I.2.1; J.3"
    ],
    "primary_category": "q-bio.PE",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.10588v7",
    "published_date": "2024-09-16 14:56:27 UTC",
    "updated_date": "2024-11-07 12:15:52 UTC"
  },
  {
    "arxiv_id": "2409.10340v1",
    "title": "Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs",
    "authors": [
      "Mehrad Soltani",
      "Luis Rueda"
    ],
    "abstract": "Hypergraphs tackle the limitations of traditional graphs by introducing {\\em\nhyperedges}. While graph edges connect only two nodes, hyperedges connect an\narbitrary number of nodes along their edges. Also, the underlying\nmessage-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the\nform of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and\nmore complex structural information than traditional Graph Neural Networks\n(GNNs). More recently, the idea of overlapping subgraphs has emerged. These\nsubgraphs can capture more information about subgroups of vertices without\nlimiting one vertex belonging to just one group, allowing vertices to belong to\nmultiple groups or subgraphs. In addition, one of the most important problems\nin graph clustering is to find densest overlapping subgraphs (DOS). In this\npaper, we propose a solution to the DOS problem via Agglomerative Greedy\nEnumeration (DOSAGE) algorithm as a novel approach to enhance the process of\ngenerating the densest overlapping subgraphs and, hence, a robust construction\nof the hypergraphs. Experiments on standard benchmarks show that the DOSAGE\nalgorithm significantly outperforms the HGNNs and six other methods on the node\nclassification task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10340v1",
    "published_date": "2024-09-16 14:56:10 UTC",
    "updated_date": "2024-09-16 14:56:10 UTC"
  },
  {
    "arxiv_id": "2409.10338v1",
    "title": "The 20 questions game to distinguish large language models",
    "authors": [
      "Gurvan Richardeau",
      "Erwan Le Merrer",
      "Camilla Penzo",
      "Gilles Tredan"
    ],
    "abstract": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10338v1",
    "published_date": "2024-09-16 14:50:29 UTC",
    "updated_date": "2024-09-16 14:50:29 UTC"
  },
  {
    "arxiv_id": "2409.10329v2",
    "title": "InfoDisent: Explainability of Image Classification Models by Information Disentanglement",
    "authors": [
      "Łukasz Struski",
      "Dawid Rymarczyk",
      "Jacek Tabor"
    ],
    "abstract": "In this work, we introduce InfoDisent, a hybrid approach to explainability\nbased on the information bottleneck principle. InfoDisent enables the\ndisentanglement of information in the final layer of any pretrained model into\natomic concepts, which can be interpreted as prototypical parts. This approach\nmerges the flexibility of post-hoc methods with the concept-level modeling\ncapabilities of self-explainable neural networks, such as ProtoPNets. We\ndemonstrate the effectiveness of InfoDisent through computational experiments\nand user studies across various datasets using modern backbones such as ViTs\nand convolutional networks. Notably, InfoDisent generalizes the prototypical\nparts approach to novel domains (ImageNet).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10329v2",
    "published_date": "2024-09-16 14:39:15 UTC",
    "updated_date": "2025-03-06 12:16:09 UTC"
  },
  {
    "arxiv_id": "2409.10320v2",
    "title": "SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation",
    "authors": [
      "Benjamin Stoler",
      "Ingrid Navarro",
      "Jonathan Francis",
      "Jean Oh"
    ],
    "abstract": "Verification and validation of autonomous driving (AD) systems and components\nis of increasing importance, as such technology increases in real-world\nprevalence. Safety-critical scenario generation is a key approach to robustify\nAD policies through closed-loop training. However, existing approaches for\nscenario generation rely on simplistic objectives, resulting in\noverly-aggressive or non-reactive adversarial behaviors. To generate diverse\nadversarial yet realistic scenarios, we propose SEAL, a scenario perturbation\napproach which leverages learned objective functions and adversarial,\nhuman-like skills. SEAL-perturbed scenarios are more realistic than SOTA\nbaselines, leading to improved ego task success across real-world,\nin-distribution, and out-of-distribution scenarios, of more than 20%. To\nfacilitate future research, we release our code and tools:\nhttps://github.com/cmubig/SEAL",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.10320v2",
    "published_date": "2024-09-16 14:33:21 UTC",
    "updated_date": "2025-02-17 23:48:52 UTC"
  },
  {
    "arxiv_id": "2409.11186v1",
    "title": "Deep Learning tools to support deforestation monitoring in the Ivory Coast using SAR and Optical satellite imagery",
    "authors": [
      "Gabriele Sartor",
      "Matteo Salis",
      "Stefano Pinardi",
      "Ozgur Saracik",
      "Rosa Meo"
    ],
    "abstract": "Deforestation is gaining an increasingly importance due to its strong\ninfluence on the sorrounding environment, especially in developing countries\nwhere population has a disadvantaged economic condition and agriculture is the\nmain source of income. In Ivory Coast, for instance, where the cocoa production\nis the most remunerative activity, it is not rare to assist to the replacement\nof portion of ancient forests with new cocoa plantations. In order to monitor\nthis type of deleterious activities, satellites can be employed to recognize\nthe disappearance of the forest to prevent it from expand its area of interest.\nIn this study, Forest-Non-Forest map (FNF) has been used as ground truth for\nmodels based on Sentinel images input. State-of-the-art models U-Net, Attention\nU-Net, Segnet and FCN32 are compared over different years combining Sentinel-1,\nSentinel-2 and cloud probability to create forest/non-forest segmentation.\nAlthough Ivory Coast lacks of forest coverage datasets and is partially covered\nby Sentinel images, it is demonstrated the feasibility to create models\nclassifying forest and non-forests pixels over the area using open datasets to\npredict where deforestation could have occurred. Although a significant portion\nof the deforestation research is carried out on visible bands, SAR acquisitions\nare employed to overcome the limits of RGB images over areas often covered by\nclouds. Finally, the most promising model is employed to estimate the hectares\nof forest has been cut between 2019 and 2020.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11186v1",
    "published_date": "2024-09-16 14:26:41 UTC",
    "updated_date": "2024-09-16 14:26:41 UTC"
  },
  {
    "arxiv_id": "2409.10308v2",
    "title": "Know your limits! Optimize the robot's behavior through self-awareness",
    "authors": [
      "Esteve Valls Mascaro",
      "Dongheui Lee"
    ],
    "abstract": "As humanoid robots transition from labs to real-world environments, it is\nessential to democratize robot control for non-expert users. Recent human-robot\nimitation algorithms focus on following a reference human motion with high\nprecision, but they are susceptible to the quality of the reference motion and\nrequire the human operator to simplify its movements to match the robot's\ncapabilities. Instead, we consider that the robot should understand and adapt\nthe reference motion to its own abilities, facilitating the operator's task.\nFor that, we introduce a deep-learning model that anticipates the robot's\nperformance when imitating a given reference. Then, our system can generate\nmultiple references given a high-level task command, assign a score to each of\nthem, and select the best reference to achieve the desired robot behavior. Our\nSelf-AWare model (SAW) ranks potential robot behaviors based on various\ncriteria, such as fall likelihood, adherence to the reference motion, and\nsmoothness. We integrate advanced motion generation, robot control, and SAW in\none unique system, ensuring optimal robot behavior for any task command. For\ninstance, SAW can anticipate falls with 99.29% accuracy. For more information\ncheck our project page: https://evm7.github.io/Self-AWare",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to Humanoids 2024 and HFR 2024. Project Page:\n  https://evm7.github.io/Self-AWare",
    "pdf_url": "http://arxiv.org/pdf/2409.10308v2",
    "published_date": "2024-09-16 14:14:58 UTC",
    "updated_date": "2024-10-16 09:36:56 UTC"
  },
  {
    "arxiv_id": "2409.10304v2",
    "title": "Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science",
    "authors": [
      "Austin Cheng",
      "Cher Tian Ser",
      "Marta Skreta",
      "Andrés Guzmán-Cordero",
      "Luca Thiede",
      "Andreas Burger",
      "Abdulrahman Aldossary",
      "Shi Xuan Leong",
      "Sergio Pablo-García",
      "Felix Strieth-Kalthoff",
      "Alán Aspuru-Guzik"
    ],
    "abstract": "Machine learning has been pervasively touching many fields of science.\nChemistry and materials science are no exception. While machine learning has\nbeen making a great impact, it is still not reaching its full potential or\nmaturity. In this perspective, we first outline current applications across a\ndiversity of problems in chemistry. Then, we discuss how machine learning\nresearchers view and approach problems in the field. Finally, we provide our\nconsiderations for maximizing impact when researching machine learning for\nchemistry.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10304v2",
    "published_date": "2024-09-16 14:10:38 UTC",
    "updated_date": "2024-10-08 13:57:20 UTC"
  },
  {
    "arxiv_id": "2409.10297v3",
    "title": "On Synthetic Texture Datasets: Challenges, Creation, and Curation",
    "authors": [
      "Blaine Hoak",
      "Patrick McDaniel"
    ],
    "abstract": "The influence of textures on machine learning models has been an ongoing\ninvestigation, specifically in texture bias/learning, interpretability, and\nrobustness. However, due to the lack of large and diverse texture data\navailable, the findings in these works have been limited, as more comprehensive\nevaluations have not been feasible. Image generative models are able to provide\ndata creation at scale, but utilizing these models for texture synthesis has\nbeen unexplored and poses additional challenges both in creating accurate\ntexture images and validating those images. In this work, we introduce an\nextensible methodology and corresponding new dataset for generating\nhigh-quality, diverse texture images capable of supporting a broad set of\ntexture-based tasks. Our pipeline consists of: (1) developing prompts from a\nrange of descriptors to serve as input to text-to-image models, (2) adopting\nand adapting Stable Diffusion pipelines to generate and filter the\ncorresponding images, and (3) further filtering down to the highest quality\nimages. Through this, we create the Prompted Textures Dataset (PTD), a dataset\nof 246,285 texture images that span 56 textures. During the process of\ngenerating images, we find that NSFW safety filters in image generation\npipelines are highly sensitive to texture (and flag up to 60\\% of our texture\nimages), uncovering a potential bias in these models and presenting unique\nchallenges when working with texture data. Through both standard metrics and a\nhuman evaluation, we find that our dataset is high quality and diverse. Our\ndataset is available for download at https://zenodo.org/records/15359142.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10297v3",
    "published_date": "2024-09-16 14:02:18 UTC",
    "updated_date": "2025-05-09 20:20:31 UTC"
  },
  {
    "arxiv_id": "2409.10294v2",
    "title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
    "authors": [
      "Shanshan Wang",
      "Chun Zhang",
      "Ning Zhang"
    ],
    "abstract": "The Knowledge Graph-to-Text Generation task aims to convert structured\nknowledge graphs into coherent and human-readable natural language text. Recent\nefforts in this field have focused on enhancing pre-trained language models\n(PLMs) by incorporating graph structure information to capture the intricate\nstructure details of knowledge graphs. However, most of these approaches tend\nto capture only single-granularity structure information, concentrating either\non the relationships between entities within the original graph or on the\nrelationships between words within the same entity or across different\nentities. This narrow focus results in a significant limitation: models that\nconcentrate solely on entity-level structure fail to capture the nuanced\nsemantic relationships between words, while those that focus only on word-level\nstructure overlook the broader relationships between original entire entities.\nTo overcome these limitations, this paper introduces the Multi-granularity\nGraph Structure Attention (MGSA), which is based on PLMs. The encoder of the\nmodel architecture features an entity-level structure encoding module, a\nword-level structure encoding module, and an aggregation module that\nsynthesizes information from both structure. This multi-granularity structure\nencoding approach allows the model to simultaneously capture both entity-level\nand word-level structure information, providing a more comprehensive\nunderstanding of the knowledge graph's structure information, thereby\nsignificantly improving the quality of the generated text. We conducted\nextensive evaluations of the MGSA model using two widely recognized KG-to-Text\nGeneration benchmark datasets, WebNLG and EventNarrative, where it consistently\noutperformed models that rely solely on single-granularity structure\ninformation, demonstrating the effectiveness of our approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10294v2",
    "published_date": "2024-09-16 14:01:03 UTC",
    "updated_date": "2024-09-23 04:06:16 UTC"
  },
  {
    "arxiv_id": "2409.10290v1",
    "title": "Neuromorphic Spintronics",
    "authors": [
      "Atreya Majumdar",
      "Karin Everschor-Sitte"
    ],
    "abstract": "Neuromorphic spintronics combines two advanced fields in technology,\nneuromorphic computing and spintronics, to create brain-inspired, efficient\ncomputing systems that leverage the unique properties of the electron's spin.\nIn this book chapter, we first introduce both fields - neuromorphic computing\nand spintronics and then make a case for neuromorphic spintronics. We discuss\nconcrete examples of neuromorphic spintronics, including computing based on\nfluctuations, artificial neural networks, and reservoir computing, highlighting\ntheir potential to revolutionize computational efficiency and functionality.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall",
      "cond-mat.other",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "Neuromorphic Spintronics is a chapter of a book titled \"Artificial\n  Intelligence and Intelligent Matter\". This is not the final version of the\n  chapter. For the final version, please go to the book published by Springer\n  (the DOI and other details will be put here once the book has been\n  published.)",
    "pdf_url": "http://arxiv.org/pdf/2409.10290v1",
    "published_date": "2024-09-16 13:57:39 UTC",
    "updated_date": "2024-09-16 13:57:39 UTC"
  },
  {
    "arxiv_id": "2409.10289v2",
    "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework",
    "authors": [
      "Jiahao Yuan",
      "Zixiang Di",
      "Zhiqing Cui",
      "Guisong Yang",
      "Usman Naseem"
    ],
    "abstract": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10289v2",
    "published_date": "2024-09-16 13:56:17 UTC",
    "updated_date": "2024-09-18 17:30:50 UTC"
  },
  {
    "arxiv_id": "2409.10283v2",
    "title": "ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions",
    "authors": [
      "Sourav Sanyal",
      "Kaushik Roy"
    ],
    "abstract": "In the rapidly evolving field of vision-language navigation (VLN), ensuring\nsafety for physical agents remains an open challenge. For a human-in-the-loop\nlanguage-operated drone to navigate safely, it must understand natural language\ncommands, perceive the environment, and simultaneously avoid hazards in real\ntime. Control Barrier Functions (CBFs) are formal methods that enforce safe\noperating conditions. Model Predictive Control (MPC) is an optimization\nframework that plans a sequence of future actions over a prediction horizon,\nensuring smooth trajectory tracking while obeying constraints. In this work, we\nconsider a VLN-operated drone platform and enhance its safety by formulating a\nnovel scene-aware CBF that leverages ego-centric observations from a camera\nwhich has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less\nbaseline system uses a Vision-Language Encoder with cross-modal attention to\nconvert commands into an ordered sequence of landmarks. An object detection\nmodel identifies and verifies these landmarks in the captured images to\ngenerate a planned path. To further enhance safety, an Adaptive Safety Margin\nAlgorithm (ASMA) is proposed. ASMA tracks moving objects and performs\nscene-aware CBF evaluation on-the-fly, which serves as an additional constraint\nwithin the MPC framework. By continuously identifying potentially risky\nobservations, the system performs prediction in real time about unsafe\nconditions and proactively adjusts its control actions to maintain safe\nnavigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in\nthe Gazebo environment using the Robot Operating System (ROS), ASMA achieves\n64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in\ntrajectory lengths compared to the baseline CBF-less VLN.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.IV",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10283v2",
    "published_date": "2024-09-16 13:44:50 UTC",
    "updated_date": "2025-03-10 21:51:26 UTC"
  },
  {
    "arxiv_id": "2409.10281v1",
    "title": "DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis",
    "authors": [
      "Fa-Ting Hong",
      "Yunfei Liu",
      "Yu Li",
      "Changyin Zhou",
      "Fei Yu",
      "Dan Xu"
    ],
    "abstract": "Audio-driven talking head synthesis strives to generate lifelike video\nportraits from provided audio. The diffusion model, recognized for its superior\nquality and robust generalization, has been explored for this task. However,\nestablishing a robust correspondence between temporal audio cues and\ncorresponding spatial facial expressions with diffusion models remains a\nsignificant challenge in talking head generation. To bridge this gap, we\npresent DreamHead, a hierarchical diffusion framework that learns\nspatial-temporal correspondences in talking head synthesis without compromising\nthe model's intrinsic quality and adaptability.~DreamHead learns to predict\ndense facial landmarks from audios as intermediate signals to model the spatial\nand temporal correspondences.~Specifically, a first hierarchy of\naudio-to-landmark diffusion is first designed to predict temporally smooth and\naccurate landmark sequences given audio sequence signals. Then, a second\nhierarchy of landmark-to-image diffusion is further proposed to produce\nspatially consistent facial portrait videos, by modeling spatial\ncorrespondences between the dense facial landmark and appearance. Extensive\nexperiments show that proposed DreamHead can effectively learn spatial-temporal\nconsistency with the designed hierarchical diffusion and produce high-fidelity\naudio-driven talking head videos for multiple identities.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10281v1",
    "published_date": "2024-09-16 13:44:20 UTC",
    "updated_date": "2024-09-16 13:44:20 UTC"
  },
  {
    "arxiv_id": "2409.10277v2",
    "title": "Cognitive Kernel: An Open-source Agent System towards Generalist Autopilots",
    "authors": [
      "Hongming Zhang",
      "Xiaoman Pan",
      "Hongwei Wang",
      "Kaixin Ma",
      "Wenhao Yu",
      "Dong Yu"
    ],
    "abstract": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10277v2",
    "published_date": "2024-09-16 13:39:05 UTC",
    "updated_date": "2024-12-31 20:02:33 UTC"
  },
  {
    "arxiv_id": "2409.10271v1",
    "title": "Causal Discovery in Recommender Systems: Example and Discussion",
    "authors": [
      "Emanuele Cavenaghi",
      "Fabio Stella",
      "Markus Zanker"
    ],
    "abstract": "Causality is receiving increasing attention by the artificial intelligence\nand machine learning communities. This paper gives an example of modelling a\nrecommender system problem using causal graphs. Specifically, we approached the\ncausal discovery task to learn a causal graph by combining observational data\nfrom an open-source dataset with prior knowledge. The resulting causal graph\nshows that only a few variables effectively influence the analysed feedback\nsignals. This contrasts with the recent trend in the machine learning community\nto include more and more variables in massive models, such as neural networks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24",
    "pdf_url": "http://arxiv.org/pdf/2409.10271v1",
    "published_date": "2024-09-16 13:31:04 UTC",
    "updated_date": "2024-09-16 13:31:04 UTC"
  },
  {
    "arxiv_id": "2410.01822v1",
    "title": "The Importance of Causality in Decision Making: A Perspective on Recommender Systems",
    "authors": [
      "Emanuele Cavenaghi",
      "Alessio Zanga",
      "Fabio Stella",
      "Markus Zanker"
    ],
    "abstract": "Causality is receiving increasing attention in the Recommendation Systems\n(RSs) community, which has realised that RSs could greatly benefit from\ncausality to transform accurate predictions into effective and explainable\ndecisions. Indeed, the RS literature has repeatedly highlighted that, in\nreal-world scenarios, recommendation algorithms suffer many types of biases\nsince assumptions ensuring unbiasedness are likely not met. In this discussion\npaper, we formulate the RS problem in terms of causality, using potential\noutcomes and structural causal models, by giving formal definitions of the\ncausal quantities to be estimated and a general causal graph to serve as a\nreference to foster future research and development.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24",
    "pdf_url": "http://arxiv.org/pdf/2410.01822v1",
    "published_date": "2024-09-16 13:30:41 UTC",
    "updated_date": "2024-09-16 13:30:41 UTC"
  },
  {
    "arxiv_id": "2409.10267v1",
    "title": "Enhancing Personalized Recipe Recommendation Through Multi-Class Classification",
    "authors": [
      "Harish Neelam",
      "Koushik Sai Veerella"
    ],
    "abstract": "This paper intends to address the challenge of personalized recipe\nrecommendation in the realm of diverse culinary preferences. The problem domain\ninvolves recipe recommendations, utilizing techniques such as association\nanalysis and classification. Association analysis explores the relationships\nand connections between different ingredients to enhance the user experience.\nMeanwhile, the classification aspect involves categorizing recipes based on\nuser-defined ingredients and preferences. A unique aspect of the paper is the\nconsideration of recipes and ingredients belonging to multiple classes,\nrecognizing the complexity of culinary combinations. This necessitates a\nsophisticated approach to classification and recommendation, ensuring the\nsystem accommodates the nature of recipe categorization. The paper seeks not\nonly to recommend recipes but also to explore the process involved in achieving\naccurate and personalized recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10267v1",
    "published_date": "2024-09-16 13:21:09 UTC",
    "updated_date": "2024-09-16 13:21:09 UTC"
  },
  {
    "arxiv_id": "2409.10246v1",
    "title": "FGR-Net:Interpretable fundus imagegradeability classification based on deepreconstruction learning",
    "authors": [
      "Saif Khalid",
      "Hatem A. Rashwan",
      "Saddam Abdulwahab",
      "Mohamed Abdel-Nasser",
      "Facundo Manuel Quiroga",
      "Domenec Puig"
    ],
    "abstract": "The performance of diagnostic Computer-Aided Design (CAD) systems for retinal\ndiseases depends on the quality of the retinal images being screened. Thus,\nmany studies have been developed to evaluate and assess the quality of such\nretinal images. However, most of them did not investigate the relationship\nbetween the accuracy of the developed models and the quality of the\nvisualization of interpretability methods for distinguishing between gradable\nand non-gradable retinal images. Consequently, this paper presents a novel\nframework called FGR-Net to automatically assess and interpret underlying\nfundus image quality by merging an autoencoder network with a classifier\nnetwork. The FGR-Net model also provides an interpretable quality assessment\nthrough visualizations. In particular, FGR-Net uses a deep autoencoder to\nreconstruct the input image in order to extract the visual characteristics of\nthe input fundus images based on self-supervised learning. The extracted\nfeatures by the autoencoder are then fed into a deep classifier network to\ndistinguish between gradable and ungradable fundus images. FGR-Net is evaluated\nwith different interpretability methods, which indicates that the autoencoder\nis a key factor in forcing the classifier to focus on the relevant structures\nof the fundus images, such as the fovea, optic disk, and prominent blood\nvessels. Additionally, the interpretability methods can provide visual feedback\nfor ophthalmologists to understand how our model evaluates the quality of\nfundus images. The experimental results showed the superiority of FGR-Net over\nthe state-of-the-art quality assessment methods, with an accuracy of 89% and an\nF1-score of 87%.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10246v1",
    "published_date": "2024-09-16 12:56:23 UTC",
    "updated_date": "2024-09-16 12:56:23 UTC"
  },
  {
    "arxiv_id": "2409.10242v2",
    "title": "Hedging Is Not All You Need: A Simple Baseline for Online Learning Under Haphazard Inputs",
    "authors": [
      "Himanshu Buckchash",
      "Momojit Biswas",
      "Rohit Agarwal",
      "Dilip K. Prasad"
    ],
    "abstract": "Handling haphazard streaming data, such as data from edge devices, presents a\nchallenging problem. Over time, the incoming data becomes inconsistent, with\nmissing, faulty, or new inputs reappearing. Therefore, it requires models that\nare reliable. Recent methods to solve this problem depend on a hedging-based\nsolution and require specialized elements like auxiliary dropouts, forked\narchitectures, and intricate network design. We observed that hedging can be\nreduced to a special case of weighted residual connection; this motivated us to\napproximate it with plain self-attention. In this work, we propose HapNet, a\nsimple baseline that is scalable, does not require online backpropagation, and\nis adaptable to varying input types. All present methods are restricted to\nscaling with a fixed window; however, we introduce a more complex problem of\nscaling with a variable window where the data becomes positionally\nuncorrelated, and cannot be addressed by present methods. We demonstrate that a\nvariant of the proposed approach can work even for this complex scenario. We\nextensively evaluated the proposed approach on five benchmarks and found\ncompetitive performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10242v2",
    "published_date": "2024-09-16 12:45:03 UTC",
    "updated_date": "2024-12-30 12:03:37 UTC"
  },
  {
    "arxiv_id": "2410.01821v1",
    "title": "NFDIcore 2.0: A BFO-Compliant Ontology for Multi-Domain Research Infrastructures",
    "authors": [
      "Oleksandra Bruns",
      "Tabea Tietz",
      "Joerg Waitelonis",
      "Etienne Posthumus",
      "Harald Sack"
    ],
    "abstract": "This paper presents NFDIcore 2.0, an ontology compliant with the Basic Formal\nOntology (BFO) designed to represent the diverse research communities of the\nNational Research Data Infrastructure (NFDI) in Germany. NFDIcore ensures the\ninteroperability across various research disciplines, thereby facilitating\ncross-domain research. Each domain's individual requirements are addressed\nthrough specific ontology modules. This paper discusses lessons learned during\nthe ontology development and mapping process, supported by practical validation\nthrough use cases in diverse research domains. The originality of NFDIcore lies\nin its adherence to BFO, the use of SWRL rules for efficient knowledge\ndiscovery, and its modular, extensible design tailored to meet the needs of\nheterogeneous research domains.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01821v1",
    "published_date": "2024-09-16 11:51:05 UTC",
    "updated_date": "2024-09-16 11:51:05 UTC"
  },
  {
    "arxiv_id": "2409.10196v1",
    "title": "NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions",
    "authors": [
      "Zhixi Cai",
      "Cristian Rojas Cardenas",
      "Kevin Leo",
      "Chenyuan Zhang",
      "Kal Backman",
      "Hanbing Li",
      "Boying Li",
      "Mahsa Ghorbanali",
      "Stavya Datta",
      "Lizhen Qu",
      "Julian Gutierrez Santiago",
      "Alexey Ignatiev",
      "Yuan-Fang Li",
      "Mor Vered",
      "Peter J Stuckey",
      "Maria Garcia de la Banda",
      "Hamid Rezatofighi"
    ],
    "abstract": "This paper addresses the problem of autonomous UAV search missions, where a\nUAV must locate specific Entities of Interest (EOIs) within a time limit, based\non brief descriptions in large, hazard-prone environments with keep-out zones.\nThe UAV must perceive, reason, and make decisions with limited and uncertain\ninformation. We propose NEUSIS, a compositional neuro-symbolic system designed\nfor interpretable UAV search and navigation in realistic scenarios. NEUSIS\nintegrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to\nprocess raw sensory inputs, maintains a probabilistic world model for\nenvironment representation, and uses a hierarchical planning component (SNaC)\nfor efficient path planning. Experimental results from simulated urban search\nmissions using AirSim and Unreal Engine show that NEUSIS outperforms a\nstate-of-the-art (SOTA) vision-language model and a SOTA search planning model\nin success rate, search efficiency, and 3D localization. These results\ndemonstrate the effectiveness of our compositional neuro-symbolic approach in\nhandling complex, real-world scenarios, making it a promising solution for\nautonomous UAV systems in search missions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10196v1",
    "published_date": "2024-09-16 11:42:15 UTC",
    "updated_date": "2024-09-16 11:42:15 UTC"
  },
  {
    "arxiv_id": "2409.16317v1",
    "title": "A Literature Review of Keyword Spotting Technologies for Urdu",
    "authors": [
      "Syed Muhammad Aqdas Rizvi"
    ],
    "abstract": "This literature review surveys the advancements of keyword spotting (KWS)\ntechnologies, specifically focusing on Urdu, Pakistan's low-resource language\n(LRL), which has complex phonetics. Despite the global strides in speech\ntechnology, Urdu presents unique challenges requiring more tailored solutions.\nThe review traces the evolution from foundational Gaussian Mixture Models to\nsophisticated neural architectures like deep neural networks and transformers,\nhighlighting significant milestones such as integrating multi-task learning and\nself-supervised approaches that leverage unlabeled data. It examines emerging\ntechnologies' role in enhancing KWS systems' performance within multilingual\nand resource-constrained settings, emphasizing the need for innovations that\ncater to languages like Urdu. Thus, this review underscores the need for\ncontext-specific research addressing the inherent complexities of Urdu and\nsimilar URLs and the means of regions communicating through such languages for\na more inclusive approach to speech technology.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.16317v1",
    "published_date": "2024-09-16 11:39:10 UTC",
    "updated_date": "2024-09-16 11:39:10 UTC"
  },
  {
    "arxiv_id": "2409.10193v1",
    "title": "Relative Positioning for Aerial Robot Path Planning in GPS Denied Environment",
    "authors": [
      "Farzad Sanati"
    ],
    "abstract": "One of the most useful applications of intelligent aerial robots sometimes\ncalled Unmanned Aerial Vehicles (UAV) in Australia is known to be in bushfire\nmonitoring and prediction operations. A swarm of autonomous drones/UAVs\nprogrammed to work in real-time observing the fire parameters using their\nonboard sensors would be valuable in reducing the life-threatening impact of\nthat fire. However autonomous UAVs face serious challenges in their positioning\nand navigation in critical bushfire conditions such as remoteness and severe\nweather conditions where GPS signals could also be unreliable. This paper\ntackles one of the most important factors in autonomous UAV navigation, namely\nInitial Positioning sometimes called Localisation. The solution provided by\nthis paper will enable a team of autonomous UAVs to establish a relative\nposition to their base of operation to be able to commence a team search and\nreconnaissance in a bushfire-affected area and find their way back to their\nbase without the help of GPS signals.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "12 pages, 4 images",
    "pdf_url": "http://arxiv.org/pdf/2409.10193v1",
    "published_date": "2024-09-16 11:35:39 UTC",
    "updated_date": "2024-09-16 11:35:39 UTC"
  },
  {
    "arxiv_id": "2409.10177v2",
    "title": "Augmenting Automatic Speech Recognition Models with Disfluency Detection",
    "authors": [
      "Robin Amann",
      "Zhaolin Li",
      "Barbara Bruno",
      "Jan Niehues"
    ],
    "abstract": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by SLT2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10177v2",
    "published_date": "2024-09-16 11:13:14 UTC",
    "updated_date": "2024-09-17 06:30:03 UTC"
  },
  {
    "arxiv_id": "2409.10173v3",
    "title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
    "authors": [
      "Saba Sturua",
      "Isabelle Mohr",
      "Mohammad Kalim Akram",
      "Michael Günther",
      "Bo Wang",
      "Markus Krimmel",
      "Feng Wang",
      "Georgios Mastrapas",
      "Andreas Koukounas",
      "Nan Wang",
      "Han Xiao"
    ],
    "abstract": "We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Evaluation on the MTEB benchmark\nshows that jina-embeddings-v3 outperforms the latest proprietary embeddings\nfrom OpenAI and Cohere on English tasks, while achieving superior performance\ncompared to multilingual-e5-large-instruct across all multilingual tasks. With\na default output dimension of 1024, users can flexibly reduce the embedding\ndimensions to as low as 32 without compromising performance, enabled by\nMatryoshka Representation Learning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, pp11-13 references, pp14-20 appendix and experiment tables",
    "pdf_url": "http://arxiv.org/pdf/2409.10173v3",
    "published_date": "2024-09-16 11:10:29 UTC",
    "updated_date": "2024-09-19 11:21:24 UTC"
  },
  {
    "arxiv_id": "2409.10168v2",
    "title": "Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation Between the United States and South Africa",
    "authors": [
      "Hayoung Jung",
      "Prerna Juneja",
      "Tanushree Mitra"
    ],
    "abstract": "Despite being an integral tool for finding health-related information online,\nYouTube has faced criticism for disseminating COVID-19 misinformation globally\nto its users. Yet, prior audit studies have predominantly investigated YouTube\nwithin the Global North contexts, often overlooking the Global South. To\naddress this gap, we conducted a comprehensive 10-day geolocation-based audit\non YouTube to compare the prevalence of COVID-19 misinformation in search\nresults between the United States (US) and South Africa (SA), the countries\nheavily affected by the pandemic in the Global North and the Global South,\nrespectively. For each country, we selected 3 geolocations and placed\nsock-puppets, or bots emulating \"real\" users, that collected search results for\n48 search queries sorted by 4 search filters for 10 days, yielding a dataset of\n915K results. We found that 31.55% of the top-10 search results contained\nCOVID-19 misinformation. Among the top-10 search results, bots in SA faced\nsignificantly more misinformative search results than their US counterparts.\nOverall, our study highlights the contrasting algorithmic behaviors of YouTube\nsearch between two countries, underscoring the need for the platform to\nregulate algorithmic behavior consistently across different regions of the\nGlobe.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "30 pages. Accepted at ICWSM 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.10168v2",
    "published_date": "2024-09-16 10:56:43 UTC",
    "updated_date": "2025-04-14 21:13:38 UTC"
  },
  {
    "arxiv_id": "2409.10164v1",
    "title": "Quantile Regression for Distributional Reward Models in RLHF",
    "authors": [
      "Nicolai Dorka"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10164v1",
    "published_date": "2024-09-16 10:54:04 UTC",
    "updated_date": "2024-09-16 10:54:04 UTC"
  },
  {
    "arxiv_id": "2409.10161v3",
    "title": "SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting",
    "authors": [
      "Mohammad Nomaan Qureshi",
      "Sparsh Garg",
      "Francisco Yandun",
      "David Held",
      "George Kantor",
      "Abhisesh Silwal"
    ],
    "abstract": "Sim2Real transfer, particularly for manipulation policies relying on RGB\nimages, remains a critical challenge in robotics due to the significant domain\nshift between synthetic and real-world visual data. In this paper, we propose\nSplatSim, a novel framework that leverages Gaussian Splatting as the primary\nrendering primitive to reduce the Sim2Real gap for RGB-based manipulation\npolicies. By replacing traditional mesh representations with Gaussian Splats in\nsimulators, SplatSim produces highly photorealistic synthetic data while\nmaintaining the scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation policies within\nSplatSim and deploying them in the real world in a zero-shot manner, achieving\nan average success rate of 86.25%, compared to 97.5% for policies trained on\nreal-world data. Videos can be found on our project page:\nhttps://splatsim.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10161v3",
    "published_date": "2024-09-16 10:52:16 UTC",
    "updated_date": "2024-10-07 03:37:36 UTC"
  },
  {
    "arxiv_id": "2409.10151v1",
    "title": "AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from Whole-Body PET/CT Images",
    "authors": [
      "Shadab Ahamed"
    ],
    "abstract": "Automated segmentation of cancerous lesions in PET/CT scans is a crucial\nfirst step in quantitative image analysis. However, training deep learning\nmodels for segmentation with high accuracy is particularly challenging due to\nthe variations in lesion size, shape, and radiotracer uptake. These lesions can\nappear in different parts of the body, often near healthy organs that also\nexhibit considerable uptake, making the task even more complex. As a result,\ncreating an effective segmentation model for routine PET/CT image analysis is\nchallenging. In this study, we utilized a 3D Residual UNet model and employed\nthe Generalized Dice Focal Loss function to train the model on the AutoPET\nChallenge 2024 dataset. We conducted a 5-fold cross-validation and used an\naverage ensembling technique using the models from the five folds. In the\npreliminary test phase for Task-1, the average ensemble achieved a mean Dice\nSimilarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of\n10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about\nthe algorithm can be found on our GitHub repository:\nhttps://github.com/ahxmeds/autosegnet2024.git. The training code has been\nshared via the repository: https://github.com/ahxmeds/autopet2024.git.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 5 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2309.13553",
    "pdf_url": "http://arxiv.org/pdf/2409.10151v1",
    "published_date": "2024-09-16 10:27:30 UTC",
    "updated_date": "2024-09-16 10:27:30 UTC"
  },
  {
    "arxiv_id": "2409.10146v1",
    "title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge",
    "authors": [
      "Hamed Babaei Giglou",
      "Jennifer D'Souza",
      "Sören Auer"
    ],
    "abstract": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 1 figure, Will appear in \"The 1st LLMs4OL Challenge @ ISWC\n  2024\" proceedings",
    "pdf_url": "http://arxiv.org/pdf/2409.10146v1",
    "published_date": "2024-09-16 10:15:30 UTC",
    "updated_date": "2024-09-16 10:15:30 UTC"
  },
  {
    "arxiv_id": "2409.10139v1",
    "title": "Towards Explainable Automated Data Quality Enhancement without Domain Knowledge",
    "authors": [
      "Djibril Sarr"
    ],
    "abstract": "In the era of big data, ensuring the quality of datasets has become\nincreasingly crucial across various domains. We propose a comprehensive\nframework designed to automatically assess and rectify data quality issues in\nany given dataset, regardless of its specific content, focusing on both textual\nand numerical data. Our primary objective is to address three fundamental types\nof defects: absence, redundancy, and incoherence. At the heart of our approach\nlies a rigorous demand for both explainability and interpretability, ensuring\nthat the rationale behind the identification and correction of data anomalies\nis transparent and understandable. To achieve this, we adopt a hybrid approach\nthat integrates statistical methods with machine learning algorithms. Indeed,\nby leveraging statistical techniques alongside machine learning, we strike a\nbalance between accuracy and explainability, enabling users to trust and\ncomprehend the assessment process. Acknowledging the challenges associated with\nautomating the data quality assessment process, particularly in terms of time\nefficiency and accuracy, we adopt a pragmatic strategy, employing\nresource-intensive algorithms only when necessary, while favoring simpler, more\nefficient solutions whenever possible. Through a practical analysis conducted\non a publicly provided dataset, we illustrate the challenges that arise when\ntrying to enhance data quality while keeping explainability. We demonstrate the\neffectiveness of our approach in detecting and rectifying missing values,\nduplicates and typographical errors as well as the challenges remaining to be\naddressed to achieve similar accuracy on statistical outliers and logic errors\nunder the constraints set in our work.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "stat.ML",
      "62H30, 68P99",
      "H.2.7; H.2.8; I.2.1"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10139v1",
    "published_date": "2024-09-16 10:08:05 UTC",
    "updated_date": "2024-09-16 10:08:05 UTC"
  },
  {
    "arxiv_id": "2409.10134v1",
    "title": "Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor Coastal Lagoon Ecosystem in the South Western Mediterranean",
    "authors": [
      "Yu Ye",
      "Aurora González-Vidal",
      "Alejandro Cisterna-García",
      "Angel Pérez-Ruzafa",
      "Miguel A. Zamora Izquierdo",
      "Antonio F. Skarmeta"
    ],
    "abstract": "Coastal marine ecosystems face mounting pressures from anthropogenic\nactivities and climate change, necessitating advanced monitoring and modeling\napproaches for effective management. This paper pioneers the development of a\nMarine Digital Twin Platform aimed at modeling the Mar Menor Coastal Lagoon\nEcosystem in the Region of Murcia. The platform leverages Artificial\nIntelligence to emulate complex hydrological and ecological models,\nfacilitating the simulation of what-if scenarios to predict ecosystem responses\nto various stressors. We integrate diverse datasets from public sources to\nconstruct a comprehensive digital representation of the lagoon's dynamics. The\nplatform's modular design enables real-time stakeholder engagement and informed\ndecision-making in marine management. Our work contributes to the ongoing\ndiscourse on advancing marine science through innovative digital twin\ntechnologies.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10134v1",
    "published_date": "2024-09-16 10:01:18 UTC",
    "updated_date": "2024-09-16 10:01:18 UTC"
  },
  {
    "arxiv_id": "2409.10132v1",
    "title": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models",
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Hongcheng Gao",
      "Junfeng Fang",
      "Xueqi Cheng"
    ],
    "abstract": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10132v1",
    "published_date": "2024-09-16 09:48:56 UTC",
    "updated_date": "2024-09-16 09:48:56 UTC"
  },
  {
    "arxiv_id": "2410.01820v2",
    "title": "PixelBytes: Catching Unified Representation for Multimodal Generation",
    "authors": [
      "Fabien Furfaro"
    ],
    "abstract": "This report presents PixelBytes, an approach for unified multimodal\nrepresentation learning. Drawing inspiration from sequence models like Image\nTransformers, PixelCNN, and Mamba-Bytes, we explore integrating text, audio,\naction-state, and pixelated images (sprites) into a cohesive representation. We\nconducted experiments on a PixelBytes Pokemon dataset and an Optimal-Control\ndataset. Our investigation covered various model architectures, including\nRecurrent Neural Networks (RNNs), State Space Models (SSMs), and\nAttention-based models, with a focus on bidirectional processing and our PxBy\nembedding technique. We evaluated models based on data reduction strategies and\nautoregressive learning, specifically examining Long Short-Term Memory (LSTM)\nnetworks in predictive and autoregressive modes. Our results indicate that\nautoregressive models perform better than predictive models in this context.\nAdditionally, we found that diffusion models can be applied to control problems\nand parallelized generation. PixelBytes aims to contribute to the development\nof foundation models for multimodal data processing and generation. The\nproject's code, models, and datasets are available online.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.01820v2",
    "published_date": "2024-09-16 09:20:13 UTC",
    "updated_date": "2024-10-20 16:08:31 UTC"
  },
  {
    "arxiv_id": "2409.11439v2",
    "title": "Machine listening in a neonatal intensive care unit",
    "authors": [
      "Modan Tailleur",
      "Vincent Lostanlen",
      "Jean-Philippe Rivière",
      "Pierre Aumond"
    ],
    "abstract": "Oxygenators, alarm devices, and footsteps are some of the most common sound\nsources in a hospital. Detecting them has scientific value for environmental\npsychology but comes with challenges of its own: namely, privacy preservation\nand limited labeled data. In this paper, we address these two challenges via a\ncombination of edge computing and cloud computing. For privacy preservation, we\nhave designed an acoustic sensor which computes third-octave spectrograms on\nthe fly instead of recording audio waveforms. For sample-efficient machine\nlearning, we have repurposed a pretrained audio neural network (PANN) via\nspectral transcoding and label space adaptation. A small-scale study in a\nneonatological intensive care unit (NICU) confirms that the time series of\ndetected events align with another modality of measurement: i.e., electronic\nbadges for parents and healthcare professionals. Hence, this paper demonstrates\nthe feasibility of polyphonic machine listening in a hospital ward while\nguaranteeing privacy by design.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11439v2",
    "published_date": "2024-09-16 09:19:19 UTC",
    "updated_date": "2024-10-07 11:44:38 UTC"
  },
  {
    "arxiv_id": "2409.10106v1",
    "title": "Industry 6.0: New Generation of Industry driven by Generative AI and Swarm of Heterogeneous Robots",
    "authors": [
      "Artem Lykov",
      "Miguel Altamirano Cabrera",
      "Mikhail Konenkov",
      "Valerii Serpiva",
      "Koffivi Fid`ele Gbagbe",
      "Ali Alabbas",
      "Aleksey Fedoseev",
      "Luis Moreno",
      "Muhammad Haris Khan",
      "Ziang Guo",
      "Dzmitry Tsetserukou"
    ],
    "abstract": "This paper presents the concept of Industry 6.0, introducing the world's\nfirst fully automated production system that autonomously handles the entire\nproduct design and manufacturing process based on user-provided natural\nlanguage descriptions. By leveraging generative AI, the system automates\ncritical aspects of production, including product blueprint design, component\nmanufacturing, logistics, and assembly. A heterogeneous swarm of robots, each\nequipped with individual AI through integration with Large Language Models\n(LLMs), orchestrates the production process. The robotic system includes\nmanipulator arms, delivery drones, and 3D printers capable of generating\nassembly blueprints. The system was evaluated using commercial and open-source\nLLMs, functioning through APIs and local deployment. A user study demonstrated\nthat the system reduces the average production time to 119.10 minutes,\nsignificantly outperforming a team of expert human developers, who averaged\n528.64 minutes (an improvement factor of 4.4). Furthermore, in the product\nblueprinting stage, the system surpassed human CAD operators by an\nunprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5\nminutes. This breakthrough represents a major leap towards fully autonomous\nmanufacturing.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "submitted to IEEE conf",
    "pdf_url": "http://arxiv.org/pdf/2409.10106v1",
    "published_date": "2024-09-16 09:12:06 UTC",
    "updated_date": "2024-09-16 09:12:06 UTC"
  },
  {
    "arxiv_id": "2409.10102v1",
    "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
    "authors": [
      "Yujia Zhou",
      "Yan Liu",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Hongjin Qian",
      "Zheng Liu",
      "Chaozhuo Li",
      "Zhicheng Dou",
      "Tsung-Yi Ho",
      "Philip S. Yu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10102v1",
    "published_date": "2024-09-16 09:06:44 UTC",
    "updated_date": "2024-09-16 09:06:44 UTC"
  },
  {
    "arxiv_id": "2409.10585v2",
    "title": "Motion Forecasting via Model-Based Risk Minimization",
    "authors": [
      "Aron Distelzweig",
      "Eitan Kosman",
      "Andreas Look",
      "Faris Janjoš",
      "Denesh K. Manivannan",
      "Abhinav Valada"
    ],
    "abstract": "Forecasting the future trajectories of surrounding agents is crucial for\nautonomous vehicles to ensure safe, efficient, and comfortable route planning.\nWhile model ensembling has improved prediction accuracy in various fields, its\napplication in trajectory prediction is limited due to the multi-modal nature\nof predictions. In this paper, we propose a novel sampling method applicable to\ntrajectory prediction based on the predictions of multiple models. We first\nshow that conventional sampling based on predicted probabilities can degrade\nperformance due to missing alignment between models. To address this problem,\nwe introduce a new method that generates optimal trajectories from a set of\nneural networks, framing it as a risk minimization problem with a variable loss\nfunction. By using state-of-the-art models as base learners, our approach\nconstructs diverse and effective ensembles for optimal trajectory sampling.\nExtensive experiments on the nuScenes prediction dataset demonstrate that our\nmethod surpasses current state-of-the-art techniques, achieving top ranks on\nthe leaderboard. We also provide a comprehensive empirical study on ensembling\nstrategies, offering insights into their effectiveness. Our findings highlight\nthe potential of advanced ensembling techniques in trajectory prediction,\nsignificantly improving predictive performance and paving the way for more\nreliable predicted trajectories.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures, submitted to IEEE International Conference on\n  Robotics & Automation (2025)",
    "pdf_url": "http://arxiv.org/pdf/2409.10585v2",
    "published_date": "2024-09-16 09:03:28 UTC",
    "updated_date": "2024-09-20 08:01:05 UTC"
  },
  {
    "arxiv_id": "2409.10085v1",
    "title": "A Riemannian Approach to Ground Metric Learning for Optimal Transport",
    "authors": [
      "Pratik Jawanpuria",
      "Dai Shi",
      "Bamdev Mishra",
      "Junbin Gao"
    ],
    "abstract": "Optimal transport (OT) theory has attracted much attention in machine\nlearning and signal processing applications. OT defines a notion of distance\nbetween probability distributions of source and target data points. A crucial\nfactor that influences OT-based distances is the ground metric of the embedding\nspace in which the source and target data points lie. In this work, we propose\nto learn a suitable latent ground metric parameterized by a symmetric positive\ndefinite matrix. We use the rich Riemannian geometry of symmetric positive\ndefinite matrices to jointly learn the OT distance along with the ground\nmetric. Empirical results illustrate the efficacy of the learned metric in\nOT-based domain adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10085v1",
    "published_date": "2024-09-16 08:42:56 UTC",
    "updated_date": "2024-09-16 08:42:56 UTC"
  },
  {
    "arxiv_id": "2409.10584v2",
    "title": "Manifold-Constrained Nucleus-Level Denoising Diffusion Model for Structure-Based Drug Design",
    "authors": [
      "Shengchao Liu",
      "Divin Yan",
      "Weitao Du",
      "Weiyang Liu",
      "Zhuoxinran Li",
      "Hongyu Guo",
      "Christian Borgs",
      "Jennifer Chayes",
      "Anima Anandkumar"
    ],
    "abstract": "Artificial intelligence models have shown great potential in structure-based\ndrug design, generating ligands with high binding affinities. However, existing\nmodels have often overlooked a crucial physical constraint: atoms must maintain\na minimum pairwise distance to avoid separation violation, a phenomenon\ngoverned by the balance of attractive and repulsive forces. To mitigate such\nseparation violations, we propose NucleusDiff. It models the interactions\nbetween atomic nuclei and their surrounding electron clouds by enforcing the\ndistance constraint between the nuclei and manifolds. We quantitatively\nevaluate NucleusDiff using the CrossDocked2020 dataset and a COVID-19\ntherapeutic target, demonstrating that NucleusDiff reduces violation rate by up\nto 100.00% and enhances binding affinity by up to 22.16%, surpassing\nstate-of-the-art models for structure-based drug design. We also provide\nqualitative analysis through manifold sampling, visually confirming the\neffectiveness of NucleusDiff in reducing separation violations and improving\nbinding affinities.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "q-bio.BM",
      "stat.ML"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10584v2",
    "published_date": "2024-09-16 08:42:46 UTC",
    "updated_date": "2024-09-30 15:09:02 UTC"
  },
  {
    "arxiv_id": "2409.10080v2",
    "title": "DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion",
    "authors": [
      "Yuchen Guo",
      "Ruoxiang Xu",
      "Rongcheng Li",
      "Zhenghao Wu",
      "Weifeng Su"
    ],
    "abstract": "In extreme scenarios such as nighttime or low-visibility environments,\nachieving reliable perception is critical for applications like autonomous\ndriving, robotics, and surveillance. Multi-modality image fusion, particularly\nintegrating infrared imaging, offers a robust solution by combining\ncomplementary information from different modalities to enhance scene\nunderstanding and decision-making. However, current methods face significant\nlimitations: GAN-based approaches often produce blurry images that lack\nfine-grained details, while AE-based methods may introduce bias toward specific\nmodalities, leading to unnatural fusion results. To address these challenges,\nwe propose DAE-Fuse, a novel two-phase discriminative autoencoder framework\nthat generates sharp and natural fused images. Furthermore, We pioneer the\nextension of image fusion techniques from static images to the video domain\nwhile preserving temporal consistency across frames, thus advancing the\nperceptual capabilities required for autonomous navigation. Extensive\nexperiments on public datasets demonstrate that DAE-Fuse achieves\nstate-of-the-art performance on multiple benchmarks, with superior\ngeneralizability to tasks like medical image fusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10080v2",
    "published_date": "2024-09-16 08:37:09 UTC",
    "updated_date": "2024-12-24 15:22:46 UTC"
  },
  {
    "arxiv_id": "2409.10077v1",
    "title": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain",
    "authors": [
      "Le Xiao",
      "Yunfei Xu",
      "Jing Zhao"
    ],
    "abstract": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10077v1",
    "published_date": "2024-09-16 08:28:05 UTC",
    "updated_date": "2024-09-16 08:28:05 UTC"
  },
  {
    "arxiv_id": "2409.16316v1",
    "title": "Surface solar radiation: AI satellite retrieval can outperform Heliosat and generalizes well to other climate zones",
    "authors": [
      "K. R. Schuurman",
      "A. Meyer"
    ],
    "abstract": "Accurate estimates of surface solar irradiance (SSI) are essential for solar\nresource assessments and solar energy forecasts in grid integration and\nbuilding control applications. SSI estimates for spatially extended regions can\nbe retrieved from geostationary satellites such as Meteosat. Traditional SSI\nsatellite retrievals like Heliosat rely on physical radiative transfer\nmodelling. We introduce the first machine-learning-based satellite retrieval\nfor instantaneous SSI and demonstrate its capability to provide accurate and\ngeneralizable SSI estimates across Europe. Our deep learning retrieval provides\nnear real-time SSI estimates based on data-driven emulation of Heliosat and\nfine-tuning on pyranometer networks. By including SSI from ground stations, our\nSSI retrieval model can outperform Heliosat accuracy and generalize well to\nregions with other climates and surface albedos in cloudy conditions (clear-sky\nindex < 0.8). We also show that the SSI retrieved from Heliosat exhibits large\nbiases in mountain regions, and that training and fine-tuning our retrieval\nmodels on SSI data from ground stations strongly reduces these biases,\noutperforming Heliosat. Furthermore, we quantify the relative importance of the\nMeteosat channels and other predictor variables like solar zenith angle for the\naccuracy of our deep learning SSI retrieval model in different cloud\nconditions. We find that in cloudy conditions multiple near-infrared and\ninfrared channels enhance the performance. Our results can facilitate the\ndevelopment of more accurate satellite retrieval models of surface solar\nirradiance.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "19 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.16316v1",
    "published_date": "2024-09-16 08:15:54 UTC",
    "updated_date": "2024-09-16 08:15:54 UTC"
  },
  {
    "arxiv_id": "2409.10070v1",
    "title": "Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks",
    "authors": [
      "Eunice Akani",
      "Benoit Favre",
      "Frederic Bechet",
      "Romain Gemignani"
    ],
    "abstract": "Dialogue summarization aims to provide a concise and coherent summary of\nconversations between multiple speakers. While recent advancements in language\nmodels have enhanced this process, summarizing dialogues accurately and\nfaithfully remains challenging due to the need to understand speaker\ninteractions and capture relevant information. Indeed, abstractive models used\nfor dialog summarization may generate summaries that contain inconsistencies.\nWe suggest using the semantic information proposed for performing Spoken\nLanguage Understanding (SLU) in human-machine dialogue systems for\ngoal-oriented human-human dialogues to obtain a more semantically faithful\nsummary regarding the task. This study introduces three key contributions:\nFirst, we propose an exploration of how incorporating task-related information\ncan enhance the summarization process, leading to more semantically accurate\nsummaries. Then, we introduce a new evaluation criterion based on task\nsemantics. Finally, we propose a new dataset version with increased annotated\ndata standardized for research on task-oriented dialogue summarization. The\nstudy evaluates these methods using the DECODA corpus, a collection of French\nspoken dialogues from a call center. Results show that integrating models with\ntask-related information improves summary accuracy, even with varying word\nerror rates.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10070v1",
    "published_date": "2024-09-16 08:15:35 UTC",
    "updated_date": "2024-09-16 08:15:35 UTC"
  },
  {
    "arxiv_id": "2409.10069v1",
    "title": "Enhancing Anomaly Detection via Generating Diversified and Hard-to-distinguish Synthetic Anomalies",
    "authors": [
      "Hyuntae Kim",
      "Changhee Lee"
    ],
    "abstract": "Unsupervised anomaly detection is a daunting task, as it relies solely on\nnormality patterns from the training data to identify unseen anomalies during\ntesting. Recent approaches have focused on leveraging domain-specific\ntransformations or perturbations to generate synthetic anomalies from normal\nsamples. The objective here is to acquire insights into normality patterns by\nlearning to differentiate between normal samples and these crafted anomalies.\nHowever, these approaches often encounter limitations when domain-specific\ntransformations are not well-specified such as in tabular data, or when it\nbecomes trivial to distinguish between them. To address these issues, we\nintroduce a novel domain-agnostic method that employs a set of conditional\nperturbators and a discriminator. The perturbators are trained to generate\ninput-dependent perturbations, which are subsequently utilized to construct\nsynthetic anomalies, and the discriminator is trained to distinguish normal\nsamples from them. We ensure that the generated anomalies are both diverse and\nhard to distinguish through two key strategies: i) directing perturbations to\nbe orthogonal to each other and ii) constraining perturbations to remain in\nproximity to normal samples. Throughout experiments on real-world datasets, we\ndemonstrate the superiority of our method over state-of-the-art benchmarks,\nwhich is evident not only in image data but also in tabular data, where\ndomain-specific transformation is not readily accessible. Additionally, we\nempirically confirm the adaptability of our method to semi-supervised settings,\ndemonstrating its capacity to incorporate supervised signals to enhance anomaly\ndetection performance even further.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.10069v1",
    "published_date": "2024-09-16 08:15:23 UTC",
    "updated_date": "2024-09-16 08:15:23 UTC"
  },
  {
    "arxiv_id": "2409.10064v1",
    "title": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM",
    "authors": [
      "Sijie Ji",
      "Xinzhe Zheng",
      "Jiawei Sun",
      "Renqi Chen",
      "Wei Gao",
      "Mani Srivastava"
    ],
    "abstract": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10064v1",
    "published_date": "2024-09-16 07:58:56 UTC",
    "updated_date": "2024-09-16 07:58:56 UTC"
  },
  {
    "arxiv_id": "2409.10063v2",
    "title": "GlobalMapNet: An Online Framework for Vectorized Global HD Map Construction",
    "authors": [
      "Anqi Shi",
      "Yuze Cai",
      "Xiangyu Chen",
      "Jian Pu",
      "Zeyu Fu",
      "Hong Lu"
    ],
    "abstract": "High-definition (HD) maps are essential for autonomous driving systems.\nTraditionally, an expensive and labor-intensive pipeline is implemented to\nconstruct HD maps, which is limited in scalability. In recent years,\ncrowdsourcing and online mapping have emerged as two alternative methods, but\nthey have limitations respectively. In this paper, we provide a novel\nmethodology, namely global map construction, to perform direct generation of\nvectorized global maps, combining the benefits of crowdsourcing and online\nmapping. We introduce GlobalMapNet, the first online framework for vectorized\nglobal HD map construction, which updates and utilizes a global map on the ego\nvehicle. To generate the global map from scratch, we propose GlobalMapBuilder\nto match and merge local maps continuously. We design a new algorithm, Map NMS,\nto remove duplicate map elements and produce a clean map. We also propose\nGlobalMapFusion to aggregate historical map information, improving consistency\nof prediction. We examine GlobalMapNet on two widely recognized datasets,\nArgoverse2 and nuScenes, showing that our framework is capable of generating\nglobally consistent results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10063v2",
    "published_date": "2024-09-16 07:56:41 UTC",
    "updated_date": "2024-09-17 06:46:21 UTC"
  },
  {
    "arxiv_id": "2409.10048v2",
    "title": "Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments",
    "authors": [
      "Wessel Ledder",
      "Yuzhen Qin",
      "Kiki van der Heijden"
    ],
    "abstract": "Although deep reinforcement learning (DRL) approaches in audio signal\nprocessing have seen substantial progress in recent years, audio-driven DRL for\ntasks such as navigation, gaze control and head-orientation control in the\ncontext of human-robot interaction have received little attention. Here, we\npropose an audio-driven DRL framework in which we utilise deep Q-learning to\ndevelop an autonomous agent that orients towards a talker in the acoustic\nenvironment based on stereo speech recordings. Our results show that the agent\nlearned to perform the task at a near perfect level when trained on speech\nsegments in anechoic environments (that is, without reverberation). The\npresence of reverberation in naturalistic acoustic environments affected the\nagent's performance, although the agent still substantially outperformed a\nbaseline, randomly acting agent. Finally, we quantified the degree of\ngeneralization of the proposed DRL approach across naturalistic acoustic\nenvironments. Our experiments revealed that policies learned by agents trained\non medium or high reverb environments generalized to low reverb environments,\nbut policies learned by agents trained on anechoic or low reverb environments\ndid not generalize to medium or high reverb environments. Taken together, this\nstudy demonstrates the potential of audio-driven DRL for tasks such as\nhead-orientation control and highlights the need for training strategies that\nenable robust generalization across environments for real-world audio-driven\nDRL applications.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.10048v2",
    "published_date": "2024-09-16 07:20:33 UTC",
    "updated_date": "2025-01-17 12:12:28 UTC"
  },
  {
    "arxiv_id": "2409.18984v1",
    "title": "Harnessing Large Language Models: Fine-tuned BERT for Detecting Charismatic Leadership Tactics in Natural Language",
    "authors": [
      "Yasser Saeid",
      "Felix Neubürger",
      "Stefanie Krügl",
      "Helena Hüster",
      "Thomas Kopinski",
      "Ralf Lanwehr"
    ],
    "abstract": "This work investigates the identification of Charismatic Leadership Tactics\n(CLTs) in natural language using a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model. Based on an own extensive\ncorpus of CLTs generated and curated for this task, our methodology entails\ntraining a machine learning model that is capable of accurately identifying the\npresence of these tactics in natural language. A performance evaluation is\nconducted to assess the effectiveness of our model in detecting CLTs. We find\nthat the total accuracy over the detection of all CLTs is 98.96\\% The results\nof this study have significant implications for research in psychology and\nmanagement, offering potential methods to simplify the currently elaborate\nassessment of charisma in texts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The 2024 IEEE 3rd Conference on Information Technology and Data\n  Science, CITDS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18984v1",
    "published_date": "2024-09-16 07:14:54 UTC",
    "updated_date": "2024-09-16 07:14:54 UTC"
  },
  {
    "arxiv_id": "2409.10038v3",
    "title": "On the Diagram of Thought",
    "authors": [
      "Yifan Zhang",
      "Yang Yuan",
      "Andrew Chi-Chih Yao"
    ],
    "abstract": "Current large language models (LLMs) demonstrate impressive capabilities but\nstruggle with complex, multi-step reasoning tasks. Existing methods often\ntackle this by requiring external control mechanisms or multi-model\norchestration, which introduces system complexity and typically lacks formal\nguarantees of reasoning soundness. We introduce the Diagram of Thought (DoT), a\nframework wherein a single auto-regressive LLM internally constructs and\nnavigates a Directed Acyclic Graph (DAG). This DAG represents the iterative\nreasoning process, encompassing steps like proposing ideas, critiquing them,\nrefining based on feedback, and synthesizing conclusions. This\nself-orchestrated, self-contained process is guided by learned role-specific\ntokens (e.g., <proposer>, <critic>, <summarizer>) embedded within the standard\ngeneration loop, thereby eliminating external dependencies. Crucially, we\nestablish a rigorous mathematical foundation for DoT using Topos Theory. We\nformalize the reasoning DAG as a diagram within a suitable topos and prove that\nthe final synthesis step, aggregating validated information, corresponds\nsemantically to computing the colimit of the relevant sub-diagram. This\nformalization provides theoretical guarantees concerning the logical\nconsistency and robustness of the synthesized outcome. DoT thus offers a\nunified, self-contained, interpretable, efficient, and formally grounded\napproach designed to significantly advance the complex reasoning capabilities\nof LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.10038v3",
    "published_date": "2024-09-16 07:01:41 UTC",
    "updated_date": "2025-03-30 23:31:29 UTC"
  },
  {
    "arxiv_id": "2409.10033v3",
    "title": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs",
    "authors": [
      "Haichuan Hu",
      "Ye Shang",
      "Guolin Xu",
      "Congqing He",
      "Quanjun Zhang"
    ],
    "abstract": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to the 6th International Workshop on Automated Program\n  Repair (APR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2409.10033v3",
    "published_date": "2024-09-16 06:51:32 UTC",
    "updated_date": "2024-12-17 13:16:56 UTC"
  },
  {
    "arxiv_id": "2409.10028v1",
    "title": "AttnMod: Attention-Based New Art Styles",
    "authors": [
      "Shih-Chieh Su"
    ],
    "abstract": "Imagine a human artist looking at the generated photo of a diffusion model,\nand hoping to create a painting out of it. There could be some feature of the\nobject in the photo that the artist wants to emphasize, some color to disperse,\nsome silhouette to twist, or some part of the scene to be materialized. These\nintentions can be viewed as the modification of the cross attention from the\ntext prompt onto UNet, during the desoising diffusion. This work presents\nAttnMod, to modify attention for creating new unpromptable art styles out of\nexisting diffusion models. The style-creating behavior is studied across\ndifferent setups.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10028v1",
    "published_date": "2024-09-16 06:38:25 UTC",
    "updated_date": "2024-09-16 06:38:25 UTC"
  },
  {
    "arxiv_id": "2409.10027v4",
    "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models",
    "authors": [
      "Chan Kim",
      "Keonwoo Kim",
      "Mintaek Oh",
      "Hanbi Baek",
      "Jiyang Lee",
      "Donghwi Jung",
      "Soojin Woo",
      "Younkyung Woo",
      "John Tucker",
      "Roya Firoozi",
      "Seung-Woo Seo",
      "Mac Schwager",
      "Seong-Woo Kim"
    ],
    "abstract": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.10027v4",
    "published_date": "2024-09-16 06:35:18 UTC",
    "updated_date": "2025-02-03 01:26:49 UTC"
  },
  {
    "arxiv_id": "2409.10016v2",
    "title": "AceParse: A Comprehensive Dataset with Diverse Structured Texts for Academic Literature Parsing",
    "authors": [
      "Huawei Ji",
      "Cheng Deng",
      "Bo Xue",
      "Zhouyang Jin",
      "Jiaxin Ding",
      "Xiaoying Gan",
      "Luoyi Fu",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "abstract": "With the development of data-centric AI, the focus has shifted from\nmodel-driven approaches to improving data quality. Academic literature, as one\nof the crucial types, is predominantly stored in PDF formats and needs to be\nparsed into texts before further processing. However, parsing diverse\nstructured texts in academic literature remains challenging due to the lack of\ndatasets that cover various text structures. In this paper, we introduce\nAceParse, the first comprehensive dataset designed to support the parsing of a\nwide range of structured texts, including formulas, tables, lists, algorithms,\nand sentences with embedded mathematical expressions. Based on AceParse, we\nfine-tuned a multimodal model, named AceParser, which accurately parses various\nstructured texts within academic literature. This model outperforms the\nprevious state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard\nSimilarity, demonstrating the potential of multimodal models in academic\nliterature parsing. Our dataset is available at\nhttps://github.com/JHW5981/AceParse.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.10016v2",
    "published_date": "2024-09-16 06:06:34 UTC",
    "updated_date": "2025-02-02 01:48:03 UTC"
  },
  {
    "arxiv_id": "2409.10583v1",
    "title": "Reinforcement Learning with Quasi-Hyperbolic Discounting",
    "authors": [
      "S. R. Eshwar",
      "Mayank Motwani",
      "Nibedita Roy",
      "Gugan Thoppe"
    ],
    "abstract": "Reinforcement learning has traditionally been studied with exponential\ndiscounting or the average reward setup, mainly due to their mathematical\ntractability. However, such frameworks fall short of accurately capturing human\nbehavior, which has a bias towards immediate gratification. Quasi-Hyperbolic\n(QH) discounting is a simple alternative for modeling this bias. Unlike in\ntraditional discounting, though, the optimal QH-policy, starting from some time\n$t_1,$ can be different to the one starting from $t_2.$ Hence, the future self\nof an agent, if it is naive or impatient, can deviate from the policy that is\noptimal at the start, leading to sub-optimal overall returns. To prevent this\nbehavior, an alternative is to work with a policy anchored in a Markov Perfect\nEquilibrium (MPE). In this work, we propose the first model-free algorithm for\nfinding an MPE. Using a two-timescale analysis, we show that, if our algorithm\nconverges, then the limit must be an MPE. We also validate this claim\nnumerically for the standard inventory system with stochastic demands. Our work\nsignificantly advances the practical application of reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10583v1",
    "published_date": "2024-09-16 06:00:42 UTC",
    "updated_date": "2024-09-16 06:00:42 UTC"
  },
  {
    "arxiv_id": "2410.01819v1",
    "title": "Strategic AI Governance: Insights from Leading Nations",
    "authors": [
      "Dian W. Tjondronegoro"
    ],
    "abstract": "Artificial Intelligence (AI) has the potential to revolutionize various\nsectors, yet its adoption is often hindered by concerns about data privacy,\nsecurity, and the understanding of AI capabilities. This paper synthesizes AI\ngovernance approaches, strategic themes, and enablers and challenges for AI\nadoption by reviewing national AI strategies from leading nations. The key\ncontribution is the development of an EPIC (Education, Partnership,\nInfrastructure, Community) framework, which maps AI implementation requirements\nto fully realize social impacts and public good from successful and sustained\nAI deployment. Through a multi-perspective content analysis of the latest AI\nstrategy documents, this paper provides a structured comparison of AI\ngovernance strategies across nations. The findings offer valuable insights for\ngovernments, academics, industries, and communities to enable responsible and\ntrustworthy AI deployments. Future work should focus on incorporating specific\nrequirements for developing countries and applying the strategies to specific\nAI applications, industries, and the public sector.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "90-02",
      "K.6.m"
    ],
    "primary_category": "cs.CY",
    "comment": "21 pages, 3 Figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2410.01819v1",
    "published_date": "2024-09-16 06:00:42 UTC",
    "updated_date": "2024-09-16 06:00:42 UTC"
  },
  {
    "arxiv_id": "2409.10011v2",
    "title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making",
    "authors": [
      "Sumera Anjum",
      "Hanzhi Zhang",
      "Wenjun Zhou",
      "Eun Jin Paek",
      "Xiaopeng Zhao",
      "Yunhe Feng"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.10011v2",
    "published_date": "2024-09-16 05:50:39 UTC",
    "updated_date": "2024-09-18 20:03:43 UTC"
  },
  {
    "arxiv_id": "2409.10007v1",
    "title": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL",
    "authors": [
      "Ke Shen",
      "Mayank Kejriwal"
    ],
    "abstract": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10007v1",
    "published_date": "2024-09-16 05:40:18 UTC",
    "updated_date": "2024-09-16 05:40:18 UTC"
  },
  {
    "arxiv_id": "2409.09996v1",
    "title": "FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks",
    "authors": [
      "Yuzhang Chen",
      "Jiangnan Zhu",
      "Yujie Gu",
      "Minoru Kuribayashi",
      "Kouichi Sakurai"
    ],
    "abstract": "Deep neural networks (DNNs) have achieved significant success in real-world\napplications. However, safeguarding their intellectual property (IP) remains\nextremely challenging. Existing DNN watermarking for IP protection often\nrequire modifying DNN models, which reduces model performance and limits their\npracticality.\n  This paper introduces FreeMark, a novel DNN watermarking framework that\nleverages cryptographic principles without altering the original host DNN\nmodel, thereby avoiding any reduction in model performance. Unlike traditional\nDNN watermarking methods, FreeMark innovatively generates secret keys from a\npre-generated watermark vector and the host model using gradient descent. These\nsecret keys, used to extract watermark from the model's activation values, are\nsecurely stored with a trusted third party, enabling reliable watermark\nextraction from suspect models. Extensive experiments demonstrate that FreeMark\neffectively resists various watermark removal attacks while maintaining high\nwatermark capacity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09996v1",
    "published_date": "2024-09-16 05:05:03 UTC",
    "updated_date": "2024-09-16 05:05:03 UTC"
  },
  {
    "arxiv_id": "2409.09989v1",
    "title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system",
    "authors": [
      "Shailja Gupta",
      "Rajesh Ranjan",
      "Surya Narayan Singh"
    ],
    "abstract": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "2 Images",
    "pdf_url": "http://arxiv.org/pdf/2409.09989v1",
    "published_date": "2024-09-16 04:44:52 UTC",
    "updated_date": "2024-09-16 04:44:52 UTC"
  },
  {
    "arxiv_id": "2409.10582v3",
    "title": "WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency",
    "authors": [
      "Pranav Jeevan",
      "Neeraj Nixon",
      "Amit Sethi"
    ],
    "abstract": "Recent advancements in single image super-resolution have been predominantly\ndriven by token mixers and transformer architectures. WaveMixSR utilized the\nWaveMix architecture, employing a two-dimensional discrete wavelet transform\nfor spatial token mixing, achieving superior performance in super-resolution\ntasks with remarkable resource efficiency. In this work, we present an enhanced\nversion of the WaveMixSR architecture by (1) replacing the traditional\ntranspose convolution layer with a pixel shuffle operation and (2) implementing\na multistage design for higher resolution tasks ($4\\times$). Our experiments\ndemonstrate that our enhanced model -- WaveMixSR-V2 -- outperforms other\narchitectures in multiple super-resolution tasks, achieving state-of-the-art\nfor the BSD100 dataset, while also consuming fewer resources, exhibits higher\nparameter efficiency, lower latency and higher throughput. Our code is\navailable at https://github.com/pranavphoenix/WaveMixSR.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "I.2.10; I.4.0; I.4.1; I.4.2; I.4.6; I.4.7; I.4.8; I.4.9; I.4.10;\n  I.2.10; I.5.1; I.5.2; I.5.4; I.4.3; I.4.4; I.4.5"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages. Accepted in AAAI 2025. arXiv admin note: text overlap with\n  arXiv:2307.00430",
    "pdf_url": "http://arxiv.org/pdf/2409.10582v3",
    "published_date": "2024-09-16 04:16:52 UTC",
    "updated_date": "2024-10-30 15:16:43 UTC"
  },
  {
    "arxiv_id": "2409.09968v1",
    "title": "Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System",
    "authors": [
      "Raffi Hagopian",
      "Timothy Strebel",
      "Simon Bernatz",
      "Gregory A Myers",
      "Erik Offerman",
      "Eric Zuniga",
      "Cy Y Kim",
      "Angie T Ng",
      "James A Iwaz",
      "Sunny P Singh",
      "Evan P Carey",
      "Michael J Kim",
      "R Spencer Schaefer",
      "Jeannie Yu",
      "Amilcare Gentili",
      "Hugo JWL Aerts"
    ],
    "abstract": "Coronary artery calcium (CAC) is highly predictive of cardiovascular events.\nWhile millions of chest CT scans are performed annually in the United States,\nCAC is not routinely quantified from scans done for non-cardiac purposes. A\ndeep learning algorithm was developed using 446 expert segmentations to\nautomatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our\nstudy differs from prior works as we leverage imaging data across the Veterans\nAffairs national healthcare system, from 98 medical centers, capturing\nextensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC\nperformance on non-gated scans was compared against clinical standard ECG-gated\nCAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than\n100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and\n87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a\nyear of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year\nall-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p <\n0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group:\n33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052\npatients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%)\nindividuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT\nimages from a random sample of >400 AI-CAC patients and verified that 527/531\n(99.2%) would benefit from lipid-lowering therapy. To the best of our\nknowledge, this is the first non-gated CT CAC algorithm developed across a\nnational healthcare system, on multiple imaging protocols, without filtering\nintra-cardiac hardware, and compared against a strong gated CT reference. We\nreport superior performance relative to previous CAC algorithms evaluated\nagainst paired gated scans that included patients with intra-cardiac hardware.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09968v1",
    "published_date": "2024-09-16 03:59:01 UTC",
    "updated_date": "2024-09-16 03:59:01 UTC"
  },
  {
    "arxiv_id": "2409.18982v1",
    "title": "Aligning Robot Navigation Behaviors with Human Intentions and Preferences",
    "authors": [
      "Haresh Karnan"
    ],
    "abstract": "Recent advances in the field of machine learning have led to new ways for\nmobile robots to acquire advanced navigational capabilities. However, these\nlearning-based methods raise the possibility that learned navigation behaviors\nmay not align with the intentions and preferences of people, a problem known as\nvalue misalignment. To mitigate this risk, this dissertation aims to answer the\nquestion: \"How can we use machine learning methods to align the navigational\nbehaviors of autonomous mobile robots with human intentions and preferences?\"\nFirst, this dissertation addresses this question by introducing a new approach\nto learning navigation behaviors by imitating human-provided demonstrations of\nthe intended navigation task. This contribution allows mobile robots to acquire\nautonomous visual navigation capabilities through imitation, using a novel\nobjective function that encourages the agent to align with the human's\nnavigation objectives and penalizes misalignment. Second, this dissertation\nintroduces two algorithms to enhance terrain-aware off-road navigation for\nmobile robots by learning visual terrain awareness in a self-supervised manner.\nThis contribution enables mobile robots to respect a human operator's\npreferences for navigating different terrains in urban outdoor environments,\nwhile extrapolating these preferences to visually novel terrains by leveraging\nmulti-modal representations. Finally, in the context of robot navigation in\nhuman-occupied environments, this dissertation introduces a dataset and an\nalgorithm for robot navigation in a socially compliant manner in both indoor\nand outdoor environments. In summary, the contributions in this dissertation\ntake significant steps toward addressing the value alignment problem in\nautonomous navigation, enabling mobile robots to navigate autonomously with\nobjectives that align with human intentions and preferences.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Haresh Karnan's PhD Dissertation A recording of the defense talk can\n  be accessed here: https://youtu.be/MssiO6g0Gb8",
    "pdf_url": "http://arxiv.org/pdf/2409.18982v1",
    "published_date": "2024-09-16 03:45:00 UTC",
    "updated_date": "2024-09-16 03:45:00 UTC"
  },
  {
    "arxiv_id": "2409.09958v1",
    "title": "An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning",
    "authors": [
      "Qian Lin",
      "Zongkai Liu",
      "Danying Mo",
      "Chao Yu"
    ],
    "abstract": "In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09958v1",
    "published_date": "2024-09-16 03:08:09 UTC",
    "updated_date": "2024-09-16 03:08:09 UTC"
  },
  {
    "arxiv_id": "2409.09957v1",
    "title": "Deep Graph Anomaly Detection: A Survey and New Perspectives",
    "authors": [
      "Hezhe Qiao",
      "Hanghang Tong",
      "Bo An",
      "Irwin King",
      "Charu Aggarwal",
      "Guansong Pang"
    ],
    "abstract": "Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 6 figures, and 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.09957v1",
    "published_date": "2024-09-16 03:05:11 UTC",
    "updated_date": "2024-09-16 03:05:11 UTC"
  },
  {
    "arxiv_id": "2409.09944v1",
    "title": "Fault Analysis And Predictive Maintenance Of Induction Motor Using Machine Learning",
    "authors": [
      "Kavana Venkatesh",
      "Neethi M"
    ],
    "abstract": "Induction motors are one of the most crucial electrical equipment and are\nextensively used in industries in a wide range of applications. This paper\npresents a machine learning model for the fault detection and classification of\ninduction motor faults by using three phase voltages and currents as inputs.\nThe aim of this work is to protect vital electrical components and to prevent\nabnormal event progression through early detection and diagnosis. This work\npresents a fast forward artificial neural network model to detect some of the\ncommonly occurring electrical faults like overvoltage, under voltage, single\nphasing, unbalanced voltage, overload, ground fault. A separate model free\nmonitoring system wherein the motor itself acts like a sensor is presented and\nthe only monitored signals are the input given to the motor. Limits for current\nand voltage values are set for the faulty and healthy conditions, which is done\nby a classifier. Real time data from a 0.33 HP induction motor is used to train\nand test the neural network. The model so developed analyses the voltage and\ncurrent values given at a particular instant and classifies the data into no\nfault or the specific fault. The model is then interfaced with a real motor to\naccurately detect and classify the faults so that further necessary action can\nbe taken.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at ICEECCOT-2018, Published in IEEE Xplore, 6 pages, 3\n  figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09944v1",
    "published_date": "2024-09-16 02:37:07 UTC",
    "updated_date": "2024-09-16 02:37:07 UTC"
  },
  {
    "arxiv_id": "2409.09927v2",
    "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
    "authors": [
      "Vinay Samuel",
      "Yue Zhou",
      "Henry Peng Zou"
    ],
    "abstract": "As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLING 2025 12 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.09927v2",
    "published_date": "2024-09-16 02:04:33 UTC",
    "updated_date": "2024-12-08 19:15:26 UTC"
  },
  {
    "arxiv_id": "2409.13759v1",
    "title": "Simulación de la distribución de alimento en el cultivo de camarón",
    "authors": [
      "Renato L. Conforme Rosado",
      "Francisco C. Calderon Bocanegra"
    ],
    "abstract": "This document presents the experimentation of 4 cases of food distribution\nfor shrimp farming. The distributions are based on the location of the\nautomatic feeders. Three cases applied in reality and a fourth case where the\nfood is irrigated on the crop simultaneously and uniformly. In a first stage,\nthe simulation of the three distribution cases is successfully adjusted to\nreality, where the trend of the shrimp growth curve is correlated with the\nhistorical data curve. A second stage where you experiment in 16 configurations\nthat are based on the amount of food, the density of biomass and the\ndistribution of the food. The simulation adopts the concepts of genetic\nalgorithms to improve the population and fuzzy logic as an agent evaluation\ntechnique for decision-making against the quality of physical-chemical\nparameters in the simulated environment. The results of these interactions\nreveal a reduction in the simulated total culture time from 22 weeks to 14\nweeks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13759v1",
    "published_date": "2024-09-16 01:29:49 UTC",
    "updated_date": "2024-09-16 01:29:49 UTC"
  },
  {
    "arxiv_id": "2409.09916v1",
    "title": "SFR-RAG: Towards Contextually Faithful LLMs",
    "authors": [
      "Xuan-Phi Nguyen",
      "Shrey Pandit",
      "Senthil Purushwalkam",
      "Austin Xu",
      "Hailin Chen",
      "Yifei Ming",
      "Zixuan Ke",
      "Silvio Savarese",
      "Caiming Xong",
      "Shafiq Joty"
    ],
    "abstract": "Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2409.09916v1",
    "published_date": "2024-09-16 01:08:18 UTC",
    "updated_date": "2024-09-16 01:08:18 UTC"
  }
]
{
  "date": "2025-04-30",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-30 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文再次被大语言模型 (LLM) 及其相关技术刷屏，特别是**推理能力**的提升和**智能体 (Agent)** 的构建成为焦点。微软发布了 Phi-4-reasoning，DeepSeek 也推出了 DeepSeek-Prover-V2 和 WebThinker，都在探索更强的推理和研究能力。同时，AI 在**软件工程、医疗健康、机器人**等领域的应用，以及**AI 安全、治理和公平性**问题也备受关注。此外，**多模态模型、强化学习、计算机视觉**等领域也有不少进展。\n\n**今日重点论文看点：**\n\n*   **LLM 推理新进展：** DeepSeek 和微软分别发布了用于数学定理证明和复杂推理的新模型 (DeepSeek-Prover-V2, Phi-4-reasoning)，展示了 LLM 在逻辑推理方面的潜力。WebThinker 则赋予 LLM 深度网络研究能力。AdaR1 和 ShorterBetter 探索了如何让 LLM 更高效地进行推理。\n*   **AI 智能体与应用：** SWE-smith 提出了大规模生成软件工程训练数据的方法，助力 AI 智能体开发。TRUST 利用 LLM 构建创伤后应激障碍 (PTSD) 的对话评估系统。IRL Dittos 则探索了用 AI 实体化身代表远程同事进行办公室互动。MF-LLM 模拟了群体决策动力学。NGENT 畅想了整合多领域能力的下一代 AI 智能体。\n*   **AI 安全与治理：** 多篇论文探讨了 AI 风险、信任、监管 (Public Opinion and The Rise of Digital Minds)、AI Agent 的特性与治理 (Characterizing AI Agents)、对抗 AI 内容篡改 (Active Light Modulation)、LLM 越狱 (XBreaking)、后门攻击防御 (Cert-SSB, How to Backdoor the Knowledge Distillation) 以及公平性问题 (Learning Heterogeneous Performance-Fairness Trade-offs, Quantitative Auditing of AI Fairness, Fairness in Graph Learning)。\n*   **多模态与视觉：** 多篇论文关注多模态 LLM (MLLM)，包括视觉层选择 (Rethinking Visual Layer Selection)、RL 增强推理 (Reinforced MLLM)、目标幻觉缓解 (Black-Box Visual Prompt Engineering) 以及统一理解、生成和编辑 (Nexus-Gen)。视频理解 (DEEVISum, SeriesBench)、无人机视觉 (LIF, Self-Supervised Monocular Visual Drone Model Identification)、服装模式生成 (GarmentDiffusion) 等也有新进展。\n*   **机器人与强化学习：** 论文探索了利用 RL 实现混合现实 UI 布局 (Adaptive 3D UI Placement)、安全导航 (Designing Control Barrier Function)、多目标灵巧手操作 (Multi-Goal Dexterous Hand Manipulation)、跨平台无人机竞速控制 (One Net to Rule Them All) 以及与虚拟环境交互的仿真框架 (SimPRIVE)。\n\n---\n\n**论文逐一解读：**\n\n**1. DeepSeek-Prover-V2: 通过强化学习子目标分解推进形式化数学推理 (DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition)**\n这篇论文介绍了 DeepSeek-Prover-V2，一个用于 Lean 4 形式化定理证明的开源大语言模型。它通过一个递归证明流程收集初始数据，利用 DeepSeek-V3 将复杂问题分解为子目标，并通过强化学习将非形式化和形式化数学推理整合到统一模型中。该模型在 MiniF2F-test 上达到 88.9% 的通过率，并在 PutnamBench 和新引入的 ProverBench (包含 AIME 难题) 上表现出色，显著缩小了 LLM 在形式化与非形式化数学推理间的差距。\n\n**2. Phi-4-reasoning 技术报告 (Phi-4-reasoning Technical Report)**\n微软推出了 Phi-4-reasoning，一个 140 亿参数的推理模型。通过在精心挑选的、具有适当复杂度和多样性的 \"可教\" 提示上进行监督微调，并在 o3-mini 生成的推理演示上训练，该模型在复杂推理任务上表现强劲。进一步开发的 Phi-4-reasoning-plus 通过基于结果的强化学习获得了更高性能。这两个模型在数学、科学推理、编码、规划等基准上优于许多更大的开源模型，接近 DeepSeek-R1 的水平，并显示出对通用基准的改进迁移。报告详细介绍了训练数据、方法和评估，强调了数据管理和 RL 对推理模型的重要性。\n\n**3. WebThinker: 赋予大型推理模型深度研究能力 (WebThinker: Empowering Large Reasoning Models with Deep Research Capability)**\n大型推理模型 (LRM) 如 OpenAI-o1 和 DeepSeek-R1 虽强，但依赖静态知识，难以处理知识密集型任务和生成综合性研究报告。WebThinker 提出了一种深度研究智能体，使 LRM 能在推理过程中自主搜索网页、导航并起草报告。它包含一个 \"深度网络探索器\" 模块用于信息获取，并采用 \"自主思考-搜索-起草\" 策略。通过基于 RL 的 DPO 训练策略优化工具使用。实验表明 WebThinker 在复杂推理和报告生成任务上显著优于现有方法。\n\n**4. SWE-smith: 扩展软件工程智能体的训练数据 (SWE-smith: Scaling Data for Software Engineering Agents)**\n为解决 LLM 在软件工程领域训练数据稀缺且收集困难的问题，该研究提出了 SWE-smith，一个可扩展生成软件工程训练数据的流水线。它能为任意 Python 代码库构建执行环境，并自动合成大量破坏现有测试的任务实例。利用 SWE-smith，研究者创建了一个包含 5 万实例、源自 128 个 GitHub 仓库的数据集，规模远超以往。基于此训练的 SWE-agent-LM-32B 模型在 SWE-bench Verified 上达到 40.2% Pass@1，是开源模型中的 SOTA。该工作开源了流程、数据、模型，旨在降低软件工程自动化研究的门槛。\n\n**5. TRUST: 基于 LLM 的创伤理解与结构化评估对话系统 (TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments)**\n针对标准诊断访谈和评估领域缺乏对话系统的问题，该研究开发了 TRUST，一个基于 LLM 的对话系统框架，用于进行创伤后应激障碍 (PTSD) 的正式诊断访谈和评估。研究者提出了专为临床访谈设计的对话行为模式 (Dialogue Acts schema) 来指导响应生成，并开发了基于真实访谈记录的患者模拟方法以替代耗时的人工测试。专家评估显示 TRUST 的表现与真实临床访谈相当，证明了其促进心理健康服务可及性的潜力。\n\n**6. IRL Dittos: 开放空间中的实体化多模态 AI 智能体交互 (IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces)**\n研究者设计了 IRL Ditto，一种 AI 驱动的实体化智能体，用于在共享办公空间中代表远程同事，即使他们不在场也能创造实时交流机会。它允许现场同事与远程队友的数字版本互动。为期四天的研究评估了 IRL Ditto 通过模拟在场感和促成有意义互动来加强社交联系的能力。结果表明，加强社交关系的效果很大程度上取决于参与者与 IRL Ditto 源头同事的原有关系基础。\n\n**7. MF-LLM: 通过平均场大语言模型框架模拟集体决策动力学 (MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework)**\n模拟集体决策需要考虑个体间的动态互动。现有 LLM 模拟方法常偏离真实数据。该研究提出 Mean-Field LLM (MF-LLM) 框架，显式建模微观决策与宏观群体的反馈循环。MF-LLM 交替使用策略模型 (生成个体行为) 和平均场模型 (更新群体分布)。为更好地匹配真实数据，引入了基于信息瓶颈原理的微调方法 IB-Tune。在真实社交数据集上的评估显示，MF-LLM 显著降低了与人类群体分布的 KL 散度，优于非平均场基线，并能进行准确趋势预测和干预规划。\n\n**8. ShorterBetter: 指导推理模型找到最优推理长度以实现高效推理 (ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning)**\n推理模型如 o3 和 DeepSeek-R1 通过长思维链 (CoT) 表现出色，但也常 \"过度思考\" 导致效率低下。ShorterBetter 提出一种简单的强化学习方法，让模型自主发现最优 CoT 长度。通过对每个问题采样多个输出，并将最短正确响应定义为样本最优长度 (SOL)，该方法引导模型走向最优推理长度。实验表明，ShorterBetter 能在保持准确率的同时，将输出长度减少高达 80%，表明长 CoT 具有高度可压缩性。\n\n**9. AdaR1: 从长思维链到混合思维链，通过双层自适应推理优化 (AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization)**\n长思维推理模型性能强但开销大。研究发现长思维链 (Long-CoT) 的益处因问题而异。为实现自适应推理，该研究提出一个两阶段框架：首先，融合长短 CoT 模型构建混合推理模型；其次，应用双层偏好训练，引导模型选择合适的推理风格 (组级别)，并在风格内偏好简洁正确的推理 (实例级别)。实验证明，该方法显著降低推理成本，同时保持性能，推理长度平均减少 50% 以上。\n\n**10. 重新思考多模态 LLM 中的视觉层选择 (Rethinking Visual Layer Selection in Multimodal LLMs)**\n多模态大语言模型 (MLLM) 通常使用 CLIP-ViT 作为视觉编码器。先前研究表明不同层捕捉不同信息 (浅层细节，深层语义)，但多数 MLLM 仍凭经验选择视觉特征。该研究提出 \"层级表示相似性\" 方法将 CLIP-ViT 层分为浅、中、深三类，并系统评估了它们对 MLLM 性能的影响。大规模实验发现：深层对 OCR 重要；浅、中层在计数、定位等推理任务上优于深层；跨层特征的轻量级融合通常优于单层选择和专门的融合基线。这项工作为 MLLM 的视觉表示学习提供了基础。\n\n**11. 黑盒视觉提示工程缓解大型视觉语言模型中的物体幻觉 (Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models)**\n大型视觉语言模型 (LVLM) 常出现物体幻觉。研究发现简单的物体视觉提示 (如在图像上叠加边界框) 能显著缓解幻觉，但不同提示效果各异。该研究提出 Black-Box Visual Prompt Engineering (BBVPE) 框架，在不访问模型内部的情况下识别最优视觉提示 (VP)。方法使用候选 VP 池，并训练一个路由模型为给定输入图像动态选择最有效的 VP。这种黑盒方法与模型无关，适用于开源和专有 LVLM。在 POPE 和 CHAIR 等基准上的评估证明了其有效性。\n\n**12. 强化 MLLM: 基于 RL 的多模态大语言模型推理综述 (Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models)**\n将强化学习 (RL) 整合到多模态大语言模型 (MLLM) 的推理能力中是一个新兴方向。本综述系统回顾了 MLLM 中基于 RL 的推理的最新进展，涵盖关键算法设计、奖励机制创新和实际应用。重点介绍了两种 RL 范式 (无价值和基于价值)，分析了 RL 如何通过优化推理轨迹和对齐多模态信息来增强推理能力。此外，还概述了基准数据集、评估协议、现有局限性，并提出了未来的研究方向。\n\n**13. Nexus-Gen: 用于图像理解、生成和编辑的统一模型 (Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing)**\n为弥合现有开源统一多模态模型与领域特定架构之间的性能差距，该研究提出了 Nexus-Gen，一个结合 LLM 语言推理能力和扩散模型图像合成能力的统一模型。通过双阶段对齐训练来对齐 LLM 和扩散模型的嵌入空间。为解决 LLM 训练和推理阶段自回归范式差异导致生成质量下降的问题，引入了 \"预填充自回归\" 策略。Nexus-Gen 能够综合处理图像理解、生成和编辑任务。\n\n**14. GarmentDiffusion: 基于多模态扩散 Transformer 的 3D 服装缝纫模式生成 (GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers)**\n服装缝纫模式是连接设计概念和实际制造的基础。该研究提出了 GarmentDiffusion，一个新的生成模型，能从多模态输入 (文本、图像、不完整模式) 生成厘米级精度的矢量化 3D 缝纫模式。该方法将 3D 缝纫模式参数高效编码为紧凑的边缘 token 表示，序列长度比 SewingGPT 短 10 倍。通过使用扩散 Transformer 并行去噪所有边缘 token，生成速度比 SewingGPT 快 100 倍，并在 DressCodeData 和 GarmentCodeData 数据集上取得了 SOTA 结果。\n\n**15. TRUST (创伤评估) 与 How Real Are Synthetic Therapy Conversations? (合成治疗对话保真度评估)**\n这两篇论文都关注心理健康领域，特别是 PTSD。\n*   **TRUST (论文 1):** 开发了一个 LLM 对话系统，模拟临床医生进行 PTSD 的诊断访谈和评估，旨在提高心理健康服务的可及性。\n*   **Synthetic Therapy Conversations (论文 7):** 探讨了使用合成的延长暴露疗法 (PE) 对话数据来训练和评估临床模型的可行性。研究发现合成数据在结构上接近真实对话，但在捕捉治疗互动的微妙动态和关键保真度指标 (如痛苦监测) 方面存在不足，强调需要超越表面流畅性的保真度感知指标。\n\n**16. AI 安全与治理相关论文 (2, 3, 4, 15, 20, 42, 66, 68, 69, 29)**\n*   **Public Opinion (论文 2):** 研究公众对 AI 风险的感知、对机构和 AI 技术的信任如何影响他们对 AI 监管 (如减缓发展或禁止) 的偏好。发现风险感知和对政府的信任是支持监管的关键因素。\n*   **Characterizing AI Agents (论文 3):** 提出了一个从自主性、效能、目标复杂性和通用性四个维度来描述 AI Agent 的框架，以帮助设计、操作和治理这些系统。\n*   **Active Light Modulation (论文 4):** 提出 Spotlight 系统，通过在活动现场嵌入不可察觉的调制光信号来保护现场演讲视频免遭身份、唇部和面部运动的视觉伪造。\n*   **Cert-SSB (论文 15):** 提出一种可认证的、样本特定的后门防御方法 Cert-SSB，通过为每个样本优化噪声大小并结合随机平滑来提高防御性能和认证效果。\n*   **XBreaking (论文 20):** 提出一种基于可解释 AI 的 LLM 越狱攻击方法 XBreaking。通过比较审查和未审查模型的行为来发现可利用的对齐模式，并进行有针对性的噪声注入来突破安全约束。\n*   **TRIED Benchmark (论文 42):** WITNESS 组织针对当前 AI 检测工具在真实世界中的局限性，提出了 TRIED 基准，一个基于真实世界影响和创新能力评估检测工具的新框架，强调社会技术因素。\n*   **Backdoor Knowledge Distillation (论文 66):** 首次展示了即使教师模型是干净的，也可以通过在蒸馏数据集中策略性地毒化对抗样本（嵌入后门触发器）来攻击知识蒸馏过程，从而秘密地危害学生模型。\n*   **Participatory AI (论文 68):** 介绍了一个对话界面系统，使公众能够参与设计公共部门应用中的差分隐私 AI 系统，平衡隐私保护和民主问责制。\n*   **Fairness in Graph Learning (论文 69):** 综述了机器学习增强的图学习 (GL-ML) 中的公平性挑战，强调了 ML 技术的引入如何加剧和复杂化公平性问题。\n*   **Quantitative Auditing of AI Fairness (论文 29):** 提出一个使用差分隐私合成数据来审计 AI 系统公平性的框架，以平衡严格审计和隐私保护的需求。\n\n**17. 机器人与强化学习相关论文 (14, 17, 21, 27, 33, 34, 48)**\n*   **Adaptive 3D UI Placement (论文 14):** 探索使用强化学习 (RL) 在混合现实 (MR) 中根据用户姿态和环境动态放置 3D 内容。\n*   **LLM-Empowered Embodied Agent (论文 17):** 提出一个由 LLM 驱动的具身机器人系统，用于家庭物品管理，集成了记忆增强的任务规划。\n*   **Self-Supervised Drone Identification (论文 21):** 提出一种自监督学习方案，仅使用机载单目视频和飞控数据训练基于神经网络的无人机模型，并改进了遮挡处理，以实现 GPS 拒止环境下的自我运动估计。\n*   **Control Barrier Function via Probabilistic Enumeration (论文 27):** 提出一个分层控制框架，利用神经网络验证技术设计控制屏障函数 (CBF)，以确保 RL 导航策略的安全性。\n*   **One Net to Rule Them All (论文 33):** 展示了单个神经网络控制器，通过领域随机化训练，可以鲁棒地控制物理上不同的多种四旋翼无人机进行高速竞速。\n*   **Multi-Goal Dexterous Hand Manipulation (论文 34):** 提出 GC-PMPC，一种基于概率模型的 RL 方法，用于学习多目标灵巧手操作任务，并在模拟和真实 DexHand 平台上验证了其高效性。\n*   **SimPRIVE (论文 48):** 介绍了一个用于物理机器人与虚拟环境交互的仿真框架 SimPRIVE，允许在虚拟世界中测试运行在真实硬件上的机器人算法。\n\n**18. 其他值得关注的论文**\n*   **DEEVISum (论文 5):** 提出轻量级视觉语言模型 DEEVISum，结合多阶段知识蒸馏 (MSKD) 和早退 (EE) 机制，用于高效的视频分段摘要。\n*   **Sionna RT (论文 16):** Sionna 物理仿真库的技术报告，重点介绍了其可微分的光线追踪器，用于模拟无线电波传播，并能计算信道脉冲响应等对系统参数的梯度。\n*   **Vision Transformers in Precision Agriculture (论文 19):** 全面综述了视觉 Transformer (ViT) 在精准农业中的应用，涵盖分类、检测、分割等任务，并与 CNN 进行了比较。\n*   **Sadeed (论文 28):** 提出 Sadeed，一种基于微调的小型语言模型 (Kuwain 1.5B)，用于阿拉伯语文本的自动标注 (diacritization)，并引入了新的基准 SadeedDiac-25。\n*   **Galvatron (论文 55):** 介绍了一个自动化的分布式系统 Galvatron，用于高效训练大型基础模型，能自动选择最优的混合并行策略。\n*   **Quantum-Safe Homomorphic Encryption (论文 75):** 提出一种基于格的方案，用于量子程序的同态评估和证明，能在量子对手面前保持安全。\n\n希望这份 TLDR 能帮助你快速把握今天 arXiv 的研究热点！",
  "papers": [
    {
      "arxiv_id": "2504.21851v1",
      "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments",
      "title_zh": "TRUST：用于创伤理解和结构化评估的基于 LLM 的对话系统\n",
      "authors": [
        "Sichang Tu",
        "Abigail Powers",
        "Stephen Doogan",
        "Jinho D. Choi"
      ],
      "abstract": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.",
      "tldr_zh": "该研究提出了一种基于LLM的对话系统TRUST，用于创伤理解和结构化评估，旨在弥合心理健康护理可及性的差距。TRUST框架由协同LLM模块组成，能够进行正式的创伤后应激障碍(PTSD)诊断访谈和评估。研究人员设计了一种专门用于临床访谈的对话行为模式，并开发了一种基于真实访谈记录的患者模拟方法。专家评估表明，TRUST的性能与真实的临床访谈相当，具有促进心理健康护理可用性的潜力。该系统在沟通风格和反应适当性方面仍有提升空间。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.21851v1",
      "published_date": "2025-04-30 17:58:06 UTC",
      "updated_date": "2025-04-30 17:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:14:14.840280"
    },
    {
      "arxiv_id": "2504.21849v1",
      "title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support",
      "title_zh": "公众舆论与数字思维的崛起：感知风险、信任与监管支持\n",
      "authors": [
        "Justin B. Bullock",
        "Janet V. T. Pauketat",
        "Hsini Huang",
        "Yi-Fan Wang",
        "Jacy Reese Anthis"
      ],
      "abstract": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape.",
      "tldr_zh": "本研究基于2023年“人工智能、道德与情感”(AIMS)的全国代表性调查，实证研究了公众对机构和人工智能技术的信任，以及感知风险如何影响对人工智能监管的偏好。研究发现公众普遍支持人工智能监管，风险感知在塑造政策偏好方面起着重要作用。对政府信任度较高的人倾向于支持监管，而对人工智能公司和技术信任度较高的人则不太倾向于支持限制。对政府的信任和感知风险能够显著预测对软性（例如，减缓发展）和强硬（例如，禁止人工智能系统）监管干预的偏好。研究强调了公众舆论在人工智能治理中的重要性，并为政策制定者在人工智能治理中平衡公众对风险的担忧和对机构的信任提供了经验基础。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "31 pages, 1 figure, 5 tables, accepted to Public Performance and\n  Management Review",
      "pdf_url": "http://arxiv.org/pdf/2504.21849v1",
      "published_date": "2025-04-30 17:56:23 UTC",
      "updated_date": "2025-04-30 17:56:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:14:27.112458"
    },
    {
      "arxiv_id": "2504.21848v1",
      "title": "Characterizing AI Agents for Alignment and Governance",
      "title_zh": "用于对齐和治理的人工智能体特征描述\n",
      "authors": [
        "Atoosa Kasirzadeh",
        "Iason Gabriel"
      ],
      "abstract": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals.",
      "tldr_zh": "为了更好地对齐和管理AI Agent，本文从四个维度对其进行刻画：自主性(autonomy)、有效性(efficacy)、目标复杂性(goal complexity)和通用性(generality)。针对每个维度，文章提出了不同的等级划分，并论证了每个维度都会引发关于AI系统设计、运行和治理的独特问题。作者利用此框架构建了不同类型AI Agent的“Agentic Profiles”，揭示了不同类别AI Agent所带来的技术和非技术治理挑战，涵盖了从狭义的特定任务助手到高度自主的通用系统。该框架旨在为开发者、政策制定者和公众提供机会，开发更符合集体社会目标的治理方法。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21848v1",
      "published_date": "2025-04-30 17:55:48 UTC",
      "updated_date": "2025-04-30 17:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:14:39.035405"
    },
    {
      "arxiv_id": "2504.21846v1",
      "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content",
      "title_zh": "主动光调制以对抗语音视觉内容的篡改\n",
      "authors": [
        "Hadleigh Schwartz",
        "Xiaofeng Yan",
        "Charles J. Carver",
        "Xia Zhou"
      ],
      "abstract": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies.",
      "tldr_zh": "该论文提出了一种名为Spotlight的低开销、非侵入式系统，旨在保护实时语音视频免受说话人身份和唇部及面部动作的视觉伪造。Spotlight通过不可察觉的调制光在事件现场创建动态物理签名，并将它们嵌入到所有视频记录中。这些物理签名编码了语音事件特有的语义特征，包括说话人的身份和面部动作，并经过密码学保护以防止欺骗。实验结果表明，Spotlight在检测伪造视频方面实现了AUCs≥0.99和100%的真阳性率，并且在各种录制条件、视频后处理技术和白盒对抗攻击中都具有很强的鲁棒性。Spotlight的核心在于利用局部敏感哈希生成紧凑的、姿势不变的语音视频特征，以及一种将>200 bps嵌入视频的光学调制方案，同时保持视频和直播中的不可察觉性。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21846v1",
      "published_date": "2025-04-30 17:55:24 UTC",
      "updated_date": "2025-04-30 17:55:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:14:51.238588"
    },
    {
      "arxiv_id": "2504.21831v1",
      "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization",
      "title_zh": "用于视频摘要的 VLM 中的提前退出和多阶段知识蒸馏\n",
      "authors": [
        "Anas Anwarul Haq Khan",
        "Utkarsh Verma",
        "Prateek Chanda",
        "Ganesh Ramakrishnan"
      ],
      "abstract": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.",
      "tldr_zh": "该论文提出了DEEVISum，一种轻量级、高效且可扩展的视觉语言模型，专为分段视频摘要而设计。DEEVISum利用结合文本和音频信号的多模态提示，并结合多阶段知识蒸馏(MSKD)和提前退出(EE)机制，以平衡性能和效率。MSKD在baseline知识蒸馏基础上实现了1.33%的F1值绝对提升，而EE则在F1值下降1.3个点的情况下，将推理时间缩短了约21%。在TVSum数据集上的评估表明，最佳模型PaLI Gemma2 3B + MSKD实现了61.1的F1分数，与更大的模型性能相当，同时保持较低的计算成本。作者公开了代码和处理后的数据集，以支持进一步的研究。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21831v1",
      "published_date": "2025-04-30 17:37:55 UTC",
      "updated_date": "2025-04-30 17:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:15:03.113890"
    },
    {
      "arxiv_id": "2504.21801v1",
      "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition",
      "title_zh": "DeepSeek-Prover-V2：通过子目标分解的强化学习推进形式化数学推理\n",
      "authors": [
        "Z. Z. Ren",
        "Zhihong Shao",
        "Junxiao Song",
        "Huajian Xin",
        "Haocheng Wang",
        "Wanjia Zhao",
        "Liyue Zhang",
        "Zhe Fu",
        "Qihao Zhu",
        "Dejian Yang",
        "Z. F. Wu",
        "Zhibin Gou",
        "Shirong Ma",
        "Hongxuan Tang",
        "Yuxuan Liu",
        "Wenjun Gao",
        "Daya Guo",
        "Chong Ruan"
      ],
      "abstract": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing.",
      "tldr_zh": "DeepSeek-Prover-V2是一个开源的大语言模型，专为Lean 4中的形式化定理证明设计。该模型利用DeepSeek-V3递归定理证明流程收集的初始化数据，通过将复杂问题分解为子目标，并结合链式思维过程，实现了非形式化和形式化数学推理的统一。DeepSeek-Prover-V2-671B在神经定理证明方面达到了SOTA水平，在MiniF2F-test上达到88.9%的通过率，并解决了PutnamBench中的49/658个问题。此外，该模型在ProverBench和AIME问题上的评估也表现出色，表明大语言模型在形式化和非形式化数学推理方面的差距正在缩小。该研究通过强化学习进行子目标分解，显著提升了模型在形式化数学推理方面的能力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21801v1",
      "published_date": "2025-04-30 16:57:48 UTC",
      "updated_date": "2025-04-30 16:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:15:15.207468"
    },
    {
      "arxiv_id": "2504.21800v2",
      "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues",
      "title_zh": "合成治疗对话有多真实？评估暴露疗法对话中的保真度\n",
      "authors": [
        "Suhas BN",
        "Dominik Mattioli",
        "Saeed Abdullah",
        "Rosa I. Arriaga",
        "Chris W. Wiese",
        "Andrew M. Sherrill"
      ],
      "abstract": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. Synthetic therapy dialogues closely match structural\nfeatures of real-world conversations (e.g., speaker switch ratio: 0.98 vs.\n0.99); however, they may not adequately reflect key fidelity markers (e.g.,\ndistress monitoring). We highlight gaps in existing evaluation frameworks and\nadvocate for fidelity-aware metrics that go beyond surface fluency to uncover\nclinically significant failures. Our findings clarify where synthetic data can\neffectively complement real-world datasets -- and where critical limitations\nremain.",
      "tldr_zh": "该研究评估了合成的暴露疗法(Prolonged Exposure, PE)对话在创伤后应激障碍(PTSD)治疗中的真实性，旨在探索其作为训练和评估临床模型的可扩展替代方案的潜力。通过比较真实和合成对话的语言、结构和协议特定指标，包括轮换模式和治疗保真度，研究发现合成数据在结构特征上与真实对话相似，但在关键保真度指标（如痛苦监测）上存在不足。研究引入了基于语言分析和语义建模的PE特定指标，强调了现有评估框架的局限性，并提倡使用超越表面流畅性的保真度感知指标来揭示临床上的显著缺陷。研究结果表明，合成数据在缓解数据稀缺和保护患者隐私方面具有潜力，但仍难以捕捉治疗互动的微妙动态，因此需要谨慎使用。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "68T50",
        "I.2.7; H.3.1"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.21800v2",
      "published_date": "2025-04-30 16:56:56 UTC",
      "updated_date": "2025-05-01 16:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:15:27.202571"
    },
    {
      "arxiv_id": "2504.21798v1",
      "title": "SWE-smith: Scaling Data for Software Engineering Agents",
      "title_zh": "SWE-smith：扩展软件工程智能体的数据规模\n",
      "authors": [
        "John Yang",
        "Kilian Leret",
        "Carlos E. Jimenez",
        "Alexander Wettig",
        "Kabir Khandpur",
        "Yanzhe Zhang",
        "Binyuan Hui",
        "Ofir Press",
        "Ludwig Schmidt",
        "Diyi Yang"
      ],
      "abstract": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com.",
      "tldr_zh": "该论文提出了SWE-smith，一个用于大规模生成软件工程训练数据的pipeline，旨在解决现有数据集规模小、标注复杂、存储需求高等问题。SWE-smith能够针对任何Python代码库构建执行环境，并自动合成数百到数千个破坏现有测试的任务实例。利用SWE-smith，作者构建了一个包含来自128个GitHub仓库的5万个实例的数据集，比以往的工作大一个数量级。基于此数据集训练的SWE-agent-LM-32B在SWE-bench Verified基准测试中达到了40.2%的Pass@1解决率，成为开源模型中的最优水平。作者开源了SWE-smith及其相关资源，以降低自动化软件工程领域LM系统研究的门槛。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21798v1",
      "published_date": "2025-04-30 16:56:06 UTC",
      "updated_date": "2025-04-30 16:56:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:15:39.142416"
    },
    {
      "arxiv_id": "2504.21776v1",
      "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
      "title_zh": "WebThinker：利用深度研究能力增强大型推理模型",
      "authors": [
        "Xiaoxi Li",
        "Jiajie Jin",
        "Guanting Dong",
        "Hongjin Qian",
        "Yutao Zhu",
        "Yongkang Wu",
        "Ji-Rong Wen",
        "Zhicheng Dou"
      ],
      "abstract": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
      "tldr_zh": "WebThinker 旨在提升大型推理模型 (LRM) 在复杂、知识密集型任务中的表现。它通过集成“深度网络探索器 (Deep Web Explorer)”模块，使 LRM 能够自主搜索网络、浏览网页并从中提取信息，从而动态地弥补知识缺口。WebThinker 采用“自主思考-搜索-起草策略 (Autonomous Think-Search-and-Draft strategy)”，实现推理、信息收集和报告撰写的无缝衔接。此外，还引入了基于强化学习的训练策略，通过迭代在线直接偏好优化 (DPO) 来增强研究工具的利用率。实验结果表明，WebThinker 在复杂推理和科学报告生成任务中显著优于现有方法，提高了 LRM 在复杂场景中的可靠性和适用性。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21776v1",
      "published_date": "2025-04-30 16:25:25 UTC",
      "updated_date": "2025-04-30 16:25:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:15:51.159626"
    },
    {
      "arxiv_id": "2504.21775v1",
      "title": "Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning",
      "title_zh": "联邦学习中异构性能-公平性权衡的学习",
      "authors": [
        "Rongguang Ye",
        "Ming Tang"
      ],
      "abstract": "Recent methods leverage a hypernet to handle the performance-fairness\ntrade-offs in federated learning. This hypernet maps the clients' preferences\nbetween model performance and fairness to preference-specifc models on the\ntrade-off curve, known as local Pareto front. However, existing methods\ntypically adopt a uniform preference sampling distribution to train the\nhypernet across clients, neglecting the inherent heterogeneity of their local\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\nconsider the gap between local and global Pareto fronts on the global dataset.\nTo address these limitations, we propose HetPFL to effectively learn both local\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\noptimal preference sampling distribution for each client to accommodate\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\nof clients' hypernets to ensure the performance of the global Pareto front. We\nprove that HetPFL converges linearly with respect to the number of rounds,\nunder weaker assumptions than existing methods. Extensive experiments on four\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\nthe quality of learned local and global Pareto fronts.",
      "tldr_zh": "本文提出了一种名为HetPFL的联邦学习框架，旨在解决现有方法在处理性能-公平性权衡时忽略客户端本地Pareto前沿异质性的问题。HetPFL包含两个关键模块：Preference Sampling Adaptation (PSA)和Preference-aware Hypernet Fusion (PHF)。PSA自适应地确定每个客户端的最佳偏好抽样分布，以适应异构的本地Pareto前沿。PHF则执行客户端超网络的偏好感知融合，以确保全局Pareto前沿的性能。理论分析表明，HetPFL在比现有方法更弱的假设下线性收敛。在四个数据集上的大量实验表明，HetPFL在学习局部和全局Pareto前沿的质量方面显著优于七个基线方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21775v1",
      "published_date": "2025-04-30 16:25:02 UTC",
      "updated_date": "2025-04-30 16:25:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:16:03.032580"
    },
    {
      "arxiv_id": "2504.21774v1",
      "title": "Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?",
      "title_zh": "基于无人机协同感知是否只需要中间融合？\n",
      "authors": [
        "Jiuwu Hao",
        "Liguo Sun",
        "Yuting Wan",
        "Yueyang Wu",
        "Ti Xiang",
        "Haolin Song",
        "Pin Lv"
      ],
      "abstract": "Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF.",
      "tldr_zh": "该论文提出了一种基于晚期-中期融合(late-intermediate fusion)的通信高效的无人机(UAV)协同感知框架LIF，旨在解决现有方法忽略UAV视角特性导致通信开销大的问题。LIF的核心思想是交换信息丰富且紧凑的检测结果，并将融合阶段转移到特征表示层面。通过视觉引导的位置嵌入(VPE)和基于框的虚拟增强特征(BoBEV)有效整合来自不同智能体的互补信息。此外，创新性地引入了不确定性驱动的通信机制，利用不确定性评估来选择高质量和可靠的共享区域。实验结果表明，LIF以最小的通信带宽实现了卓越的性能。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21774v1",
      "published_date": "2025-04-30 16:22:14 UTC",
      "updated_date": "2025-04-30 16:22:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:16:14.976488"
    },
    {
      "arxiv_id": "2504.21773v1",
      "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness",
      "title_zh": "MAC-Tuning：利用增强的知识边界感知进行 LLM 多重组合问题推理\n",
      "authors": [
        "Junsheng Huang",
        "Zhitao He",
        "Sandeep Polisetty",
        "Qingyun Wang",
        "May Fung"
      ],
      "abstract": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.",
      "tldr_zh": "该论文提出了一种名为多答案和置信度逐步调整(MAC-Tuning)的新方法，旨在提升大型语言模型(LLM)在多重组合问题推理中对知识边界的感知能力。MAC-Tuning通过在指令数据上进行微调，将答案预测和置信度估计的学习过程分离。实验结果表明，该方法在平均精度上比基线模型提高了高达25%，有效降低了LLM在复杂问题场景下的幻觉问题。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21773v1",
      "published_date": "2025-04-30 16:17:53 UTC",
      "updated_date": "2025-04-30 16:17:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:16:26.798930"
    },
    {
      "arxiv_id": "2504.21772v1",
      "title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline",
      "title_zh": "解决短视频平台的版权侵权问题：新型数据集和音频恢复深度学习管道\n",
      "authors": [
        "Minwoo Oh",
        "Minsu Park",
        "Eunil Park"
      ],
      "abstract": "Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset\ncomprising 1,121 video and mixed-audio pairs, specifically designed for short\nvideo restoration tasks. Experimental results demonstrate that our pipeline not\nonly removes arbitrary BGM with high accuracy but also restores OSTs, ensuring\ncontent integrity. This approach provides an ethical and scalable solution to\ncopyright challenges in user-generated content on short video platforms.",
      "tldr_zh": "该论文提出了一种新的pipeline，结合音乐源分离(MSS)和跨模态视频-音乐检索(CMVMR)，旨在解决短视频平台上的版权侵权问题，即通过嵌入背景音乐(BGM)来掩盖原始音轨(OST)。该方法能够有效地将BGM从OST中分离出来，从而恢复真实的视频音轨。为了支持这项研究，作者构建了两个领域特定的数据集：OASD-20K（用于音频分离，包含20,000个混合BGM和OST的音频片段）和OSVAR-160（用于pipeline评估，包含1,121个视频和混合音频对）。实验结果表明，该pipeline不仅能够高精度地移除BGM，还能恢复OST，从而确保内容完整性，为短视频平台上的版权问题提供了一种可扩展的解决方案。\n",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "will be presented in IJCAI 2025, 9 pages, 4 tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.21772v1",
      "published_date": "2025-04-30 16:17:05 UTC",
      "updated_date": "2025-04-30 16:17:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:16:39.198690"
    },
    {
      "arxiv_id": "2504.21731v1",
      "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning",
      "title_zh": "基于深度强化学习的混合现实自适应3D UI布局\n",
      "authors": [
        "Feiyu Lu",
        "Mengyu Chen",
        "Hsiang Hsu",
        "Pranav Deshpande",
        "Cheng Yao Wang",
        "Blair MacIntyre"
      ],
      "abstract": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR.",
      "tldr_zh": "该研究探索了使用深度强化学习(RL)在混合现实(MR)中自适应放置3D用户界面(UI)的方法。与以往基于优化的方法不同，该研究利用RL根据用户姿势和周围环境动态地调整虚拟内容的放置位置，以最大化用户体验。初步实验结果表明，RL在持续优化内容位置方面具有潜力。该研究还指出了未来利用RL实现MR中个性化和优化UI及内容放置的研究方向。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '24)",
      "pdf_url": "http://arxiv.org/pdf/2504.21731v1",
      "published_date": "2025-04-30 15:21:36 UTC",
      "updated_date": "2025-04-30 15:21:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:16:50.781597"
    },
    {
      "arxiv_id": "2504.21730v1",
      "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense",
      "title_zh": "Cert-SSB：迈向可认证的样本特异性后门防御\n",
      "authors": [
        "Ting Qiao",
        "Yingjia Wang",
        "Xing Liu",
        "Sixing Wu",
        "Jianbing Li",
        "Yiming Li"
      ],
      "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB.",
      "tldr_zh": "该论文提出了一种名为Cert-SSB的样本特异性认证后门防御方法，旨在解决深度神经网络(DNNs)易受后门攻击的问题。Cert-SSB通过随机梯度上升优化每个样本的噪声幅度，为每个样本确定一个特定的噪声水平，并将其应用于多个中毒训练集以重新训练平滑模型。为了解决优化噪声在不同样本间变化导致现有认证方法失效的问题，论文引入了一种基于存储-更新的认证方法，动态调整每个样本的认证区域以提高认证性能。实验结果表明，Cert-SSB在多个基准数据集上有效防御后门攻击。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.21730v1",
      "published_date": "2025-04-30 15:21:25 UTC",
      "updated_date": "2025-04-30 15:21:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:17:02.968385"
    },
    {
      "arxiv_id": "2504.21719v1",
      "title": "Sionna RT: Technical Report",
      "title_zh": "Sionna RT：技术报告\n",
      "authors": [
        "Fayçal Aït Aoudia",
        "Jakob Hoydis",
        "Merlin Nimier-David",
        "Sebastian Cammerer",
        "Alexander Keller"
      ],
      "abstract": "Sionna is an open-source, GPU-accelerated library that, as of version 0.14,\nincorporates a ray tracer for simulating radio wave propagation. A unique\nfeature of Sionna RT is differentiability, enabling the calculation of\ngradients for the channel impulse responses (CIRs), radio maps, and other\nrelated metrics with respect to system and environmental parameters, such as\nmaterial properties, antenna patterns, and array geometries. The release of\nSionna 1.0 provides a complete overhaul of the ray tracer, significantly\nimproving its speed, memory efficiency, and extensibility. This document\ndetails the algorithms employed by Sionna RT to simulate radio wave propagation\nefficiently, while also addressing their current limitations. Given that the\ncomputation of CIRs and radio maps requires distinct algorithms, these are\ndetailed in separate sections. For CIRs, Sionna RT integrates shooting and\nbouncing of rays (SBR) with the image method and uses a hashing-based mechanism\nto efficiently eliminate duplicate paths. Radio maps are computed using a\npurely SBR-based approach.",
      "tldr_zh": "Sionna RT是Sionna库中的一个GPU加速的射线追踪器，用于模拟无线电波传播。其独特之处在于可微性，能够计算信道冲激响应(CIRs)、无线电地图以及其他相关指标对于系统和环境参数的梯度，例如材料属性、天线模式和阵列几何形状。Sionna 1.0版本对射线追踪器进行了全面改进，显著提高了速度、内存效率和可扩展性。Sionna RT将射线发射和反弹(SBR)与镜像法相结合，并使用基于哈希的机制来有效消除重复路径，从而计算CIR。无线电地图则完全使用基于SBR的方法计算。\n",
      "categories": [
        "cs.IT",
        "cs.AI",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21719v1",
      "published_date": "2025-04-30 15:05:20 UTC",
      "updated_date": "2025-04-30 15:05:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:17:14.981745"
    },
    {
      "arxiv_id": "2504.21716v1",
      "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics",
      "title_zh": "基于 LLM 的具身智能体，用于家庭机器人中记忆增强的任务规划\n",
      "authors": [
        "Marc Glocker",
        "Peter Hönig",
        "Matthias Hirschmanner",
        "Markus Vincze"
      ],
      "abstract": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr.",
      "tldr_zh": "该论文提出了一种基于LLM驱动的具身机器人系统，用于自主家庭物品管理。该系统采用记忆增强的任务规划，使机器人能够执行高级用户指令并跟踪过去的动作。系统包含三个专门的智能体：路由智能体、任务规划智能体和知识库智能体，均由特定任务的LLM驱动，并通过上下文学习避免了显式模型训练。RAG技术增强了系统从过去交互中检索上下文的能力，从而改进了长期对象跟踪。Grounded SAM和LLaMa3.2-Vision的结合提供了强大的对象检测能力，促进了任务规划的语义场景理解。在三个家庭场景中的评估表明，该系统具有较高的任务规划准确性，并且由于RAG的引入，记忆召回能力得到了提高。Qwen2.5在专用智能体方面表现最佳，而LLaMA3.1在路由任务中表现出色。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at Austrian Robotics Workshop 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21716v1",
      "published_date": "2025-04-30 15:00:20 UTC",
      "updated_date": "2025-04-30 15:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:17:27.239340"
    },
    {
      "arxiv_id": "2504.21707v1",
      "title": "Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning",
      "title_zh": "递归KL散度优化：一种用于表征学习的动态框架\n",
      "authors": [
        "Anthony D Martin"
      ],
      "abstract": "We propose a generalization of modern representation learning objectives by\nreframing them as recursive divergence alignment processes over localized\nconditional distributions While recent frameworks like Information Contrastive\nLearning I-Con unify multiple learning paradigms through KL divergence between\nfixed neighborhood conditionals we argue this view underplays a crucial\nrecursive structure inherent in the learning process. We introduce Recursive KL\nDivergence Optimization RKDO a dynamic formalism where representation learning\nis framed as the evolution of KL divergences across data neighborhoods. This\nformulation captures contrastive clustering and dimensionality reduction\nmethods as static slices while offering a new path to model stability and local\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\nadvantages approximately 30 percent lower loss values compared to static\napproaches across three different datasets and 60 to 80 percent reduction in\ncomputational resources needed to achieve comparable results. This suggests\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\noptimization landscape for representation learning with significant\nimplications for resource constrained applications.",
      "tldr_zh": "该论文提出了递归KL散度优化(Recursive KL Divergence Optimization, RKDO)，一个用于表示学习的动态框架。RKDO将表示学习重新定义为数据邻域间KL散度的演化过程，从而推广了现有的表示学习目标。与基于固定邻域条件分布的静态方法（如信息对比学习I-Con）相比，RKDO利用递归更新机制，在模型稳定性和局部适应性方面具有优势。实验结果表明，RKDO在三个不同的数据集上，损失值比静态方法降低约30%，计算资源需求减少60%至80%，证明了RKDO在资源受限应用中的高效性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "cs.NE",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21707v1",
      "published_date": "2025-04-30 14:51:27 UTC",
      "updated_date": "2025-04-30 14:51:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:17:39.089772"
    },
    {
      "arxiv_id": "2504.21706v1",
      "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey",
      "title_zh": "精准农业中的视觉 Transformer：综合综述\n",
      "authors": [
        "Saber Mehdipour",
        "Seyed Abolghasem Mirroshandel",
        "Seyed Amirhossein Tabatabaei"
      ],
      "abstract": "Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture.",
      "tldr_zh": "该综述全面探讨了视觉Transformer (Vision Transformers, ViTs) 在精准农业中的应用，涵盖了从分类到检测和分割等任务。文章首先介绍了ViTs的基础架构及其从自然语言处理(NLP)到计算机视觉的演变，并讨论了ViTs如何缓解传统卷积神经网络(CNNs)中的归纳偏置。随后，综述回顾了相关文献，重点关注关键方法、数据集和性能指标，并对CNNs和ViTs进行了比较分析，同时考察了混合模型和性能增强技术。此外，文章还讨论了数据需求、计算需求和模型可解释性等技术挑战，并提出了潜在的解决方案，最后展望了ViTs在实际农业环境中进一步整合的潜在研究方向和技术进步。该综述旨在为从业者和研究人员提供对ViTs如何改变智能和精准农业的深入理解。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21706v1",
      "published_date": "2025-04-30 14:50:02 UTC",
      "updated_date": "2025-04-30 14:50:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:17:51.288411"
    },
    {
      "arxiv_id": "2504.21700v1",
      "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
      "title_zh": "XBreaking：用于破解大型语言模型的、可解释的人工智能方法\n",
      "authors": [
        "Marco Arazzi",
        "Vignesh Kumar Kembu",
        "Antonino Nocera",
        "Vinod P"
      ],
      "abstract": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.",
      "tldr_zh": "该论文提出了一种名为XBreaking的、基于可解释人工智能(Explainable AI)的LLM越狱攻击方法。XBreaking通过对比分析审查模型和非审查模型的行为，提取可利用的对齐模式，然后通过有针对性的噪声注入来突破LLM的安全限制。该方法旨在提高对审查机制的理解，并设计更具针对性的越狱攻击。实验结果表明，XBreaking能够有效突破LLM的审查机制，并揭示了审查机制的一些重要信息。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21700v1",
      "published_date": "2025-04-30 14:44:24 UTC",
      "updated_date": "2025-04-30 14:44:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:18:02.921200"
    },
    {
      "arxiv_id": "2504.21695v1",
      "title": "Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling",
      "title_zh": "通过改进的遮挡处理实现自监督单目视觉无人机模型识别\n",
      "authors": [
        "Stavrow A. Bahnam",
        "Christophe De Wagter",
        "Guido C. H. E. de Croon"
      ],
      "abstract": "Ego-motion estimation is vital for drones when flying in GPS-denied\nenvironments. Vision-based methods struggle when flight speed increases and\nclose-by objects lead to difficult visual conditions with considerable motion\nblur and large occlusions. To tackle this, vision is typically complemented by\nstate estimation filters that combine a drone model with inertial measurements.\nHowever, these drone models are currently learned in a supervised manner with\nground-truth data from external motion capture systems, limiting scalability to\ndifferent environments and drones. In this work, we propose a self-supervised\nlearning scheme to train a neural-network-based drone model using only onboard\nmonocular video and flight controller data (IMU and motor feedback). We achieve\nthis by first training a self-supervised relative pose estimation model, which\nthen serves as a teacher for the drone model. To allow this to work at high\nspeed close to obstacles, we propose an improved occlusion handling method for\ntraining self-supervised pose estimation models. Due to this method, the root\nmean squared error of resulting odometry estimates is reduced by an average of\n15%. Moreover, the student neural drone model can be successfully obtained from\nthe onboard data. It even becomes more accurate at higher speeds compared to\nits teacher, the self-supervised vision-based model. We demonstrate the value\nof the neural drone model by integrating it into a traditional filter-based VIO\nsystem (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing\ntrajectories near obstacles. Self-supervised learning of ego-motion estimation\nrepresents a significant step toward bridging the gap between flying in\ncontrolled, expensive lab environments and real-world drone applications. The\nfusion of vision and drone models will enable higher-speed flight and improve\nstate estimation, on any drone in any environment.",
      "tldr_zh": "该论文提出了一种自监督学习方案，仅使用单目视频和飞行控制器数据（IMU和电机反馈）训练基于神经网络的无人机模型，从而实现无人机在GPS受限环境下的自主运动估计。该方案首先训练一个自监督的相对姿态估计模型，然后将其作为教师模型来指导无人机模型的训练。为了提高在高速和近障碍物环境下的性能，论文提出了一种改进的遮挡处理方法，将里程计估计的均方根误差平均降低了15%。实验结果表明，该方法能够成功地从机载数据中获得学生无人机模型，并且在高速情况下比教师模型更准确。通过将该神经无人机模型集成到传统的基于滤波器的VIO系统(ROVIO)中，在近障碍物的3D竞速轨迹上实现了更高的里程计精度。该自监督学习方法是弥合受控实验室环境和真实世界无人机应用之间差距的重要一步，视觉和无人机模型的融合将提高飞行速度并改善状态估计。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21695v1",
      "published_date": "2025-04-30 14:38:01 UTC",
      "updated_date": "2025-04-30 14:38:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:18:15.785946"
    },
    {
      "arxiv_id": "2504.21694v1",
      "title": "Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation",
      "title_zh": "自动化ML文件到本体的自动映射，用于图查询和验证\n",
      "authors": [
        "Tom Westermann",
        "Malte Ramonat",
        "Johannes Hujer",
        "Felix Gehlhoff",
        "Alexander Fay"
      ],
      "abstract": "AutomationML has seen widespread adoption as an open data exchange format in\nthe automation domain. It is an open and vendor neutral standard based on the\nextensible markup language XML. However, AutomationML extends XML with\nadditional semantics, that limit the applicability of common XML-tools for\napplications like querying or data validation. This article provides\npractitioners with 1) an up-to-date ontology of the concepts in the\nAutomationML-standard, as well as 2) a declarative mapping to automatically\ntransform any AutomationML model into RDF triples. Together, these artifacts\nallow practitioners an easy integration of AutomationML information into\nindustrial knowledge graphs. A study on examples from the automation domain\nconcludes that transforming AutomationML to OWL opens up new powerful ways for\nquerying and validation that are impossible without transformation.",
      "tldr_zh": "本文针对AutomationML作为自动化领域开放数据交换格式的局限性，提出了一个自动将AutomationML文件映射到本体的方法，以便进行图查询和验证。该方法包括：1) 一个最新的AutomationML标准概念本体；2) 一个声明式映射，用于自动将任何AutomationML模型转换为RDF三元组。通过将AutomationML转换为OWL，可以实现传统XML工具无法实现的强大查询和验证功能，从而方便地将AutomationML信息集成到工业知识图谱中。自动化领域的实例研究表明，该方法能够有效提升查询和验证能力。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21694v1",
      "published_date": "2025-04-30 14:34:56 UTC",
      "updated_date": "2025-04-30 14:34:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:18:26.994802"
    },
    {
      "arxiv_id": "2504.21692v1",
      "title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction",
      "title_zh": "利用动态记忆预测增强自监督细粒度视频对象跟踪\n",
      "authors": [
        "Zihan Zhou",
        "Changrui Dai",
        "Aibo Song",
        "Xiaolin Fang"
      ],
      "abstract": "Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking.",
      "tldr_zh": "该论文提出了一种动态记忆预测(Dynamic Memory Prediction, DMP)框架，旨在提升自监督的细粒度视频目标跟踪性能。DMP创新性地利用多个参考帧，通过参考帧记忆引擎(Reference Frame Memory Engine)基于目标像素特征动态选择帧，从而更简洁直接地增强帧重建效果。此外，构建了双向目标预测网络(Bidirectional Target Prediction Network)以利用多参考帧提高模型的鲁棒性。实验结果表明，该算法在目标分割和关键点跟踪两个细粒度视频目标跟踪任务上优于当前最先进的自监督方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21692v1",
      "published_date": "2025-04-30 14:29:04 UTC",
      "updated_date": "2025-04-30 14:29:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:18:38.998130"
    },
    {
      "arxiv_id": "2504.21685v1",
      "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning",
      "title_zh": "提升健康提及分类性能：参数高效调优进展研究\n",
      "authors": [
        "Reem Abdel-Salam",
        "Mary Adewunmi"
      ],
      "abstract": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency.",
      "tldr_zh": "该研究探讨了如何通过参数高效微调(PEFT)技术提升健康提及分类(HMC)的性能，HMC在利用社交媒体进行实时追踪和公共健康监测中至关重要。研究发现，健康提及的复杂性源于其上下文特性，如比喻和描述性术语。为了解决这个问题，研究探索了词性标注(POS)信息的使用以及PEFT技术的改进。在RHDM、PHM和Illness三个数据集上的实验结果表明，结合POS标注信息和PEFT技术显著提高了F1分数，优于现有方法，同时使用了更小的模型和高效的训练。该研究表明，所提出的方法在准确分类社交媒体帖子中的健康提及方面具有潜在的有效性，同时优化了模型大小和训练效率。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.21685v1",
      "published_date": "2025-04-30 14:21:54 UTC",
      "updated_date": "2025-04-30 14:21:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:18:51.213115"
    },
    {
      "arxiv_id": "2504.21683v1",
      "title": "Extension-ranking Semantics for Abstract Argumentation Preprint",
      "title_zh": "抽象论证预印本的扩展排序语义\n",
      "authors": [
        "Kenneth Skiba",
        "Tjitze Rienstra",
        "Matthias Thimm",
        "Jesse Heyninck",
        "Gabriele Kern-Isberner"
      ],
      "abstract": "In this paper, we present a general framework for ranking sets of arguments\nin abstract argumentation based on their plausibility of acceptance. We present\na generalisation of Dung's extension semantics as extension-ranking semantics,\nwhich induce a preorder over the power set of all arguments, allowing us to\nstate that one set is \"closer\" to being acceptable than another. To evaluate\nthe extension-ranking semantics, we introduce a number of principles that a\nwell-behaved extension-ranking semantics should satisfy. We consider several\nsimple base relations, each of which models a single central aspect of\nargumentative reasoning. The combination of these base relations provides us\nwith a family of extension-ranking semantics. We also adapt a number of\napproaches from the literature for ranking extensions to be usable in the\ncontext of extension-ranking semantics, and evaluate their behaviour.",
      "tldr_zh": "本文提出了一个通用的框架，用于在抽象论证中根据论证集的可接受性来对其进行排序。该框架将Dung的扩展语义推广为扩展排序语义，从而在所有论证的幂集上引入了一个预序关系，能够表达一个集合比另一个集合“更接近”可接受状态。为了评估扩展排序语义，本文提出了一系列良好扩展排序语义应满足的原则。同时，考虑了几个简单的基本关系，每个关系都模拟了论证推理的一个核心方面。这些基本关系的组合构成了一个扩展排序语义族。此外，本文还调整了文献中的一些方法，使其适用于扩展排序语义的扩展排序，并评估了它们的行为。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21683v1",
      "published_date": "2025-04-30 14:19:42 UTC",
      "updated_date": "2025-04-30 14:19:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:19:03.256412"
    },
    {
      "arxiv_id": "2504.21659v1",
      "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
      "title_zh": "AdaR1：通过双层自适应推理优化，从长链式思维到混合链式思维\n",
      "authors": [
        "Haotian Luo",
        "Haiying He",
        "Yibo Wang",
        "Jinluan Yang",
        "Rui Liu",
        "Naiqiang Tan",
        "Xiaochun Cao",
        "Dacheng Tao",
        "Li Shen"
      ],
      "abstract": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
      "tldr_zh": "该论文提出了一种名为AdaR1的双层自适应推理优化框架，旨在提高复杂推理任务中长链思维(Long-CoT)模型的效率。AdaR1首先构建一个混合推理模型，融合长链和短链CoT模型，以实现多样化的推理风格。然后，通过双层偏好训练，引导模型选择合适的推理风格（组级别），并在每个风格组内偏好简洁且正确的推理（实例级别）。实验结果表明，AdaR1在保持性能的同时，显著降低了推理成本，在五个数学数据集上，平均推理长度减少了50%以上，展示了自适应策略在优化大型语言模型推理效率方面的潜力。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21659v1",
      "published_date": "2025-04-30 14:01:45 UTC",
      "updated_date": "2025-04-30 14:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:19:15.067726"
    },
    {
      "arxiv_id": "2504.21643v1",
      "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation",
      "title_zh": "通过概率枚举设计控制屏障函数，用于安全强化学习导航\n",
      "authors": [
        "Luca Marzari",
        "Francesco Trotti",
        "Enrico Marchesini",
        "Alessandro Farinelli"
      ],
      "abstract": "Achieving safe autonomous navigation systems is critical for deploying robots\nin dynamic and uncertain real-world environments. In this paper, we propose a\nhierarchical control framework leveraging neural network verification\ntechniques to design control barrier functions (CBFs) and policy correction\nmechanisms that ensure safe reinforcement learning navigation policies. Our\napproach relies on probabilistic enumeration to identify unsafe regions of\noperation, which are then used to construct a safe CBF-based control layer\napplicable to arbitrary policies. We validate our framework both in simulation\nand on a real robot, using a standard mobile robot benchmark and a highly\ndynamic aquatic environmental monitoring task. These experiments demonstrate\nthe ability of the proposed solution to correct unsafe actions while preserving\nefficient navigation behavior. Our results show the promise of developing\nhierarchical verification-based systems to enable safe and robust navigation\nbehaviors in complex scenarios.",
      "tldr_zh": "该论文提出了一种分层控制框架，利用神经网络验证技术设计控制障碍函数(CBFs)和策略修正机制，以确保强化学习导航策略的安全性。该方法依赖于概率枚举来识别不安全的操作区域，并使用这些区域构建基于CBF的安全控制层，该控制层适用于任意策略。通过在模拟和真实机器人上的实验验证，包括标准移动机器人基准测试和动态水生环境监测任务，证明了该解决方案能够纠正不安全行为，同时保持高效的导航性能。研究结果表明，开发基于分层验证的系统有望在复杂场景中实现安全可靠的导航行为。\n",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21643v1",
      "published_date": "2025-04-30 13:47:25 UTC",
      "updated_date": "2025-04-30 13:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:19:27.109139"
    },
    {
      "arxiv_id": "2504.21635v1",
      "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
      "title_zh": "Sadeed：通过小型语言模型推进阿拉伯语变音符恢复\n",
      "authors": [
        "Zeina Aldallal",
        "Sara Chrouf",
        "Khalil Hennara",
        "Mohamed Motaism Hamed",
        "Muhammad Hreden",
        "Safwan AlModhayan"
      ],
      "abstract": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.",
      "tldr_zh": "本文提出了Sadeed，一种基于微调的decoder-only小型语言模型(small language model)的新方法，用于提升阿拉伯语文本的音标恢复(diacritization)效果。Sadeed模型基于Kuwain 1.5B Hennara et al. [2025]模型进行微调，该模型在一个精心策划的高质量音标数据集上训练，该数据集通过严格的数据清洗和标准化流程构建。实验结果表明，Sadeed在计算资源有限的情况下，取得了与大型语言模型(large language models)相媲美的结果，并优于在类似领域训练的传统模型。此外，论文还指出了当前阿拉伯语音标恢复基准测试实践中的局限性，并提出了SadeedDiac-25，一个新的基准测试，旨在实现跨不同文本类型和复杂程度的更公平、更全面的评估。Sadeed和SadeedDiac-25共同为推进阿拉伯语自然语言处理(Arabic NLP)应用奠定了坚实的基础。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21635v1",
      "published_date": "2025-04-30 13:37:24 UTC",
      "updated_date": "2025-04-30 13:37:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:19:39.658916"
    },
    {
      "arxiv_id": "2504.21634v1",
      "title": "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data",
      "title_zh": "利用差分隐私合成数据对 AI 公平性进行定量审计\n",
      "authors": [
        "Chih-Cheng Rex Yuan",
        "Bow-Yaw Wang"
      ],
      "abstract": "Fairness auditing of AI systems can identify and quantify biases. However,\ntraditional auditing using real-world data raises security and privacy\nconcerns. It exposes auditors to security risks as they become custodians of\nsensitive information and targets for cyberattacks. Privacy risks arise even\nwithout direct breaches, as data analyses can inadvertently expose confidential\ninformation. To address these, we propose a framework that leverages\ndifferentially private synthetic data to audit the fairness of AI systems. By\napplying privacy-preserving mechanisms, it generates synthetic data that\nmirrors the statistical properties of the original dataset while ensuring\nprivacy. This method balances the goal of rigorous fairness auditing and the\nneed for strong privacy protections. Through experiments on real datasets like\nAdult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real\ndata. By analyzing the alignment and discrepancies between these metrics, we\nassess the capacity of synthetic data to preserve the fairness properties of\nreal data. Our results demonstrate the framework's ability to enable meaningful\nfairness evaluations while safeguarding sensitive information, proving its\napplicability across critical and sensitive domains.",
      "tldr_zh": "该论文提出了一个利用差分隐私合成数据(Differentially Private Synthetic Data)进行AI公平性量化审计的框架，旨在解决传统审计方法中存在的安全和隐私问题。该框架通过生成具有差分隐私保护的合成数据，在保留原始数据集统计特性的同时，确保敏感信息的隐私安全。实验结果表明，该框架能够有效评估AI系统的公平性，并在保护隐私的前提下，实现有意义的公平性评估，适用于关键和敏感领域。通过在Adult, COMPAS和Diabetes等真实数据集上的实验，验证了合成数据在保留真实数据公平性特征方面的能力。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21634v1",
      "published_date": "2025-04-30 13:36:27 UTC",
      "updated_date": "2025-04-30 13:36:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:19:51.139685"
    },
    {
      "arxiv_id": "2504.21605v1",
      "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations",
      "title_zh": "基于 RDF 的多语言 LLM 评估结构化质量评估表示\n",
      "authors": [
        "Jonas Gwozdz",
        "Andreas Both"
      ],
      "abstract": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.",
      "tldr_zh": "本文提出了一种基于RDF的框架，用于评估多语言LLM在处理知识冲突时的质量。该框架通过在四种不同的上下文条件（完整、不完整、冲突和无上下文信息）下，捕捉LLM在德语和英语中的响应，从而分析LLM的知识泄漏（即模型偏向训练数据而非提供的上下文）、错误检测和多语言一致性。通过在消防安全领域的实验，该框架揭示了LLM在上下文优先级和特定语言性能方面的关键模式，并验证了该框架能够充分表达研究中遇到的所有评估方面。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21605v1",
      "published_date": "2025-04-30 13:06:40 UTC",
      "updated_date": "2025-04-30 13:06:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:20:03.246020"
    },
    {
      "arxiv_id": "2504.21596v1",
      "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning",
      "title_zh": "利用具有精细化提示的预训练大型语言模型进行在线任务与运动规划\n",
      "authors": [
        "Huihui Guo",
        "Huilong Pi",
        "Yunchuan Qin",
        "Zhuo Tang",
        "Kenli Li"
      ],
      "abstract": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution.",
      "tldr_zh": "本文提出了一种名为LLM-PAS的闭环任务规划和执行系统，该系统利用预训练的大型语言模型（LLM）辅助在线任务和运动规划。LLM-PAS在规划长时程任务的同时，强调任务的执行阶段，将部分约束检查过程从规划阶段转移到执行阶段，从而探索约束空间并提供更准确的环境异常反馈。LLM的推理能力使其能够处理鲁棒执行器无法解决的异常情况。此外，作者提出了一种名为First Look Prompting (FLP)的方法，引导LLM生成有效的PDDL目标，以增强系统在重新规划期间辅助规划器的能力。实验结果表明，LLM-PAS在处理任务执行过程中的异常情况时具有有效性和鲁棒性。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21596v1",
      "published_date": "2025-04-30 12:53:53 UTC",
      "updated_date": "2025-04-30 12:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:20:15.209395"
    },
    {
      "arxiv_id": "2504.21589v1",
      "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing",
      "title_zh": "DNB-AI-Project 在 SemEval-2025 Task 5 中的表现：一种用于自动主题索引的 LLM 集成方法\n",
      "authors": [
        "Lisa Kluge",
        "Maximilian Kähler"
      ],
      "abstract": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.",
      "tldr_zh": "本文介绍了DNB-AI-Project团队为SemEval-2025 Task 5开发的自动主题索引系统，该系统基于LLM集成方法，旨在为国家技术图书馆的开放获取目录自动添加主题标签。该系统采用少量样本提示技术，利用不同LLM对经过人工标注的记录进行学习，并要求LLM为新记录推荐关键词。通过一系列后处理步骤，包括将生成的关键词映射到目标词汇表、将主题词汇总成集成投票，并根据其与记录的相关性对主题词进行排序。该系统在所有主题赛道的定量排名中位列第四，但在主题索引专家进行的定性排名中取得了最佳结果。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects",
      "pdf_url": "http://arxiv.org/pdf/2504.21589v1",
      "published_date": "2025-04-30 12:47:09 UTC",
      "updated_date": "2025-04-30 12:47:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:20:27.438346"
    },
    {
      "arxiv_id": "2504.21586v1",
      "title": "One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms",
      "title_zh": "一网打尽：四旋翼无人机竞速中跨不同平台的领域随机化",
      "authors": [
        "Robin Ferede",
        "Till Blaha",
        "Erin Lucassen",
        "Christophe De Wagter",
        "Guido C. H. E. de Croon"
      ],
      "abstract": "In high-speed quadcopter racing, finding a single controller that works well\nacross different platforms remains challenging. This work presents the first\nneural network controller for drone racing that generalizes across physically\ndistinct quadcopters. We demonstrate that a single network, trained with domain\nrandomization, can robustly control various types of quadcopters. The network\nrelies solely on the current state to directly compute motor commands. The\neffectiveness of this generalized controller is validated through real-world\ntests on two substantially different crafts (3-inch and 5-inch race\nquadcopters). We further compare the performance of this generalized controller\nwith controllers specifically trained for the 3-inch and 5-inch drone, using\ntheir identified model parameters with varying levels of domain randomization\n(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower\nspeeds compared to the fine-tuned models, it excels in adaptability across\ndifferent platforms. Our results show that no randomization fails sim-to-real\ntransfer while increasing randomization improves robustness but reduces speed.\nDespite this trade-off, our findings highlight the potential of domain\nrandomization for generalizing controllers, paving the way for universal AI\ncontrollers that can adapt to any platform.",
      "tldr_zh": "该研究提出了一种基于领域随机化训练的通用神经网络控制器，旨在解决高速无人机竞速中控制器在不同平台泛化性差的问题。该控制器仅依赖当前状态直接计算电机指令，无需针对特定无人机进行精调。通过在3英寸和5英寸两种差异显著的无人机上进行实验验证，结果表明该通用控制器具有良好的跨平台适应性。虽然速度略低于针对特定无人机训练的控制器，但其在不同平台上的鲁棒性更强。研究结果强调了领域随机化在控制器泛化方面的潜力，为开发能够适应任何平台的通用AI控制器铺平了道路。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21586v1",
      "published_date": "2025-04-30 12:44:41 UTC",
      "updated_date": "2025-04-30 12:44:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:20:39.197261"
    },
    {
      "arxiv_id": "2504.21585v1",
      "title": "Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning",
      "title_zh": "基于概率模型强化学习的多目标灵巧手操作\n",
      "authors": [
        "Yingzhuo Jiang",
        "Wenjun Huang",
        "Rongdun Lin",
        "Chenyang Miao",
        "Tianfu Sun",
        "Yunduan Cui"
      ],
      "abstract": "This paper tackles the challenge of learning multi-goal dexterous hand\nmanipulation tasks using model-based Reinforcement Learning. We propose\nGoal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing\nprobabilistic neural network ensembles to describe the high-dimensional\ndexterous hand dynamics and introducing an asynchronous MPC policy to meet the\ncontrol frequency requirements in real-world dexterous hand systems. Extensive\nevaluations on four simulated Shadow Hand manipulation scenarios with randomly\ngenerated goals demonstrate GC-PMPC's superior performance over\nstate-of-the-art baselines. It successfully drives a cable-driven Dexterous\nhand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn\nmanipulating a cubic die to three goal poses within approximately 80 minutes of\ninteractions, demonstrating exceptional learning efficiency and control\nperformance on a cost-effective dexterous hand platform.",
      "tldr_zh": "本文提出了一种基于概率模型的强化学习方法，用于学习多目标灵巧手操作任务，称为目标条件概率模型预测控制(GC-PMPC)。该方法设计了概率神经网络集成来描述高维灵巧手动力学，并引入异步MPC策略以满足实际灵巧手系统中的控制频率要求。在四个模拟Shadow Hand操作场景中，GC-PMPC在随机生成的目标下表现优于现有基线方法。此外，GC-PMPC成功驱动了一个具有12个主动自由度和5个触觉传感器的电缆驱动灵巧手DexHand 021，在约80分钟的交互内学会将一个立方体骰子操纵到三个目标姿势，展示了出色的学习效率和控制性能。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21585v1",
      "published_date": "2025-04-30 12:44:38 UTC",
      "updated_date": "2025-04-30 12:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:20:51.413887"
    },
    {
      "arxiv_id": "2504.21582v1",
      "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework",
      "title_zh": "MF-LLM：通过平均场大语言模型框架模拟集体决策动态",
      "authors": [
        "Qirui Mi",
        "Mengyue Yang",
        "Xiangning Yu",
        "Zhiyu Zhao",
        "Cheng Deng",
        "Bo An",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
      ],
      "abstract": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation.",
      "tldr_zh": "该论文提出了Mean-Field LLM (MF-LLM)框架，用于模拟集体决策动态，该框架显式地建模了微观个体决策和宏观群体之间的反馈循环。MF-LLM交替使用两个模型：一个策略模型，基于个体状态和群体信息生成个体行为；以及一个平均场模型，根据最新的个体决策更新群体分布。为了更好地匹配真实世界数据，论文引入了IB-Tune，一种基于信息瓶颈原理的LLM微调方法，它最大化群体分布与未来行为的相关性，同时最小化与历史数据的冗余。实验表明，MF-LLM在真实世界社交数据集上，相对于非平均场基线模型，KL散度降低了47%，并且能够进行准确的趋势预测和干预计划，推广到七个领域和四个LLM骨干网络，为高保真社会模拟提供了可扩展的基础。\n",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "27 pages, 8 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.21582v1",
      "published_date": "2025-04-30 12:41:51 UTC",
      "updated_date": "2025-04-30 12:41:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:21:03.597958"
    },
    {
      "arxiv_id": "2504.21568v1",
      "title": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks",
      "title_zh": "基于模糊推理和贝叶斯网络的群体决策问题研究\n",
      "authors": [
        "Shui-jin Rong",
        "Wei Guo",
        "Da-qing Zhang"
      ],
      "abstract": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts.",
      "tldr_zh": "该研究提出了一种融合模糊推理和贝叶斯网络的群体决策系统，用于解决多目标属性的群体决策问题。系统通过结合阈值、隶属函数、专家经验和领域知识构建模糊规则库，处理定量挑战。同时，设计了一个分层贝叶斯网络，利用最大似然估计动态优化条件概率表，建模多维指标之间的非线性关系，用于后验概率聚合。在学生综合评价案例中，该方法与传统加权评分法对比，结果表明该方法在规则准则构建和排序一致性方面表现出有效性，分类准确率达到86.0%，F1值比传统方法提高了53.4%。在真实数据集上的计算实验也证明了该方法在不同群体决策场景中的性能和鲁棒性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21568v1",
      "published_date": "2025-04-30 12:14:48 UTC",
      "updated_date": "2025-04-30 12:14:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:21:15.229919"
    },
    {
      "arxiv_id": "2504.21565v1",
      "title": "Towards proactive self-adaptive AI for non-stationary environments with dataset shifts",
      "title_zh": "面向具有数据集偏移的非平稳环境的主动自适应人工智能\n",
      "authors": [
        "David Fernández Narro",
        "Pablo Ferri",
        "Juan M. García-Gómez",
        "Carlos Sáez"
      ],
      "abstract": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health.",
      "tldr_zh": "该研究提出了一种前瞻性的自适应AI方法(pro-adaptive)，用于解决非平稳环境中AI模型因数据集偏移而性能下降的问题，尤其是在医疗环境中。该方法通过函数数据分析框架中的多项式样条基，对AI参数的时间轨迹进行建模，从而短期预测参数值。通过在模拟数据集和墨西哥COVID-19数据集上，对先验概率偏移、协变量偏移和概念偏移进行验证，结果表明，与在不同时间距离训练的基线模型相比，该方法在无需更新训练数据的情况下，提高了AI模型对偏移的性能。该研究为面向动态、非平稳环境的pro-adaptive AI研究奠定了基础，并与数据保护兼容，适用于医疗领域的弹性AI生产环境。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.8"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 4 figures, conference paper",
      "pdf_url": "http://arxiv.org/pdf/2504.21565v1",
      "published_date": "2025-04-30 12:09:59 UTC",
      "updated_date": "2025-04-30 12:09:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:21:27.464433"
    },
    {
      "arxiv_id": "2504.21562v1",
      "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes",
      "title_zh": "eNCApsulate：用于胶囊内窥镜精确诊断的 NCA",
      "authors": [
        "Henry John Krumb",
        "Anirban Mukhopadhyay"
      ],
      "abstract": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule.",
      "tldr_zh": "该论文提出了一种名为eNCApsulate的方法，利用神经元胞自动机(NCA)进行胶囊内窥镜的精确诊断。通过在胶囊内窥镜图像上训练NCA进行出血分割和深度估计，并使用大型基础模型的输出作为伪标签进行单目深度估计的知识蒸馏，将大型模型知识迁移到轻量级的NCA架构中。训练后的NCA模型可以移植到ESP32微控制器上，实现高效的图像处理。实验结果表明，NCA在出血分割任务上比其他便携式分割模型更准确，且参数量减少了100倍以上。论文还通过优化算法，显著提高了ESP32-S3上的推理速度。该研究首次实现了在微型设备上进行可靠的出血分割和深度估计，为胶囊的精确定位和诊断铺平了道路。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21562v1",
      "published_date": "2025-04-30 12:06:56 UTC",
      "updated_date": "2025-04-30 12:06:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:21:39.550903"
    },
    {
      "arxiv_id": "2504.21559v1",
      "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models",
      "title_zh": "用于缓解大型视觉语言模型中物体幻觉的黑盒视觉提示工程\n",
      "authors": [
        "Sangmin Woo",
        "Kang Zhou",
        "Yun Zhou",
        "Shuai Wang",
        "Sheng Guan",
        "Haibo Ding",
        "Lin Lee Cheong"
      ],
      "abstract": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination.",
      "tldr_zh": "该论文提出了一种黑盒视觉提示工程(BBVPE)框架，用于缓解大型视觉语言模型(LVLMs)中的物体幻觉问题。研究发现，简单的基于物体的视觉提示（例如，在图像上叠加边界框、圆形）可以显著减少幻觉。BBVPE通过维护一个候选视觉提示池，并训练一个路由模型来动态选择最有效的视觉提示，从而优化LVLM的响应，而无需访问模型内部结构。这种黑盒方法具有模型无关性，适用于开源和专有的LVLM。在POPE和CHAIR等基准测试上的评估表明，BBVPE能有效减少物体幻觉。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21559v1",
      "published_date": "2025-04-30 11:58:30 UTC",
      "updated_date": "2025-04-30 11:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:21:51.140535"
    },
    {
      "arxiv_id": "2504.21545v1",
      "title": "Meta knowledge assisted Evolutionary Neural Architecture Search",
      "title_zh": "元知识辅助的进化神经架构搜索\n",
      "authors": [
        "Yangyang Li",
        "Guanlong Liu",
        "Ronghua Shang",
        "Licheng Jiao"
      ],
      "abstract": "Evolutionary computation (EC)-based neural architecture search (NAS) has\nachieved remarkable performance in the automatic design of neural\narchitectures. However, the high computational cost associated with evaluating\nsearched architectures poses a challenge for these methods, and a fixed form of\nlearning rate (LR) schedule means greater information loss on diverse searched\narchitectures. This paper introduces an efficient EC-based NAS method to solve\nthese problems via an innovative meta-learning framework. Specifically, a\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\nsuitable LR schedule, which guides the training process with lower information\nloss when evaluating each individual. An adaptive surrogate model is designed\nthrough an adaptive threshold to select the potential architectures in a few\nepochs and then evaluate the potential architectures with complete epochs.\nAdditionally, a periodic mutation operator is proposed to increase the\ndiversity of the population, which enhances the generalizability and\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\ndemonstrate that the proposed method achieves high performance comparable to\nthat of many state-of-the-art peer methods, with lower computational cost and\ngreater robustness.",
      "tldr_zh": "本文提出了一种基于元学习框架的高效进化神经架构搜索(EC-based NAS)方法，旨在解决传统EC-based NAS方法中评估架构计算成本高和固定学习率(LR)导致信息损失的问题。该方法通过预训练获得一个合适的元学习率(Meta-LR)方案，以指导训练过程并降低信息损失。同时，设计了一个自适应代理模型，通过自适应阈值在少量epoch内选择潜在架构，并使用完整epoch评估它们。此外，提出了周期性变异算子以增加种群多样性，增强泛化性和鲁棒性。在CIFAR-10、CIFAR-100和ImageNet1K数据集上的实验表明，该方法以较低的计算成本实现了与最先进方法相当的高性能和更强的鲁棒性。\n",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21545v1",
      "published_date": "2025-04-30 11:43:07 UTC",
      "updated_date": "2025-04-30 11:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:22:03.488568"
    },
    {
      "arxiv_id": "2504.21491v1",
      "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery",
      "title_zh": "ClassWise-CRF：用于增强遥感图像语义分割的类别特定融合\n",
      "authors": [
        "Qinfeng Zhu",
        "Yunxi Jiang",
        "Lei Fan"
      ],
      "abstract": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.",
      "tldr_zh": "该论文提出了一种名为ClassWise-CRF的类别特定融合架构，用于提升遥感图像的语义分割性能。该架构首先通过贪婪算法从候选网络池中选择在特定类别上表现良好的专家网络，然后基于它们在每个类别中的分割性能自适应地加权融合这些网络的分割预测结果。ClassWise-CRF借鉴条件随机场(CRF)的思想，将多个网络的分割预测视为置信向量场，并利用验证集上的分割指标作为先验，采用指数加权策略融合类别特定的置信度分数。此外，该架构还利用CRF中的一元和二元势函数进一步优化融合结果，以确保空间一致性和边界精度。在LoveDA和Vaihingen两个遥感数据集上的实验结果表明，ClassWise-CRF显著提高了分割性能， mIoU分别提升了0.68%-0.91%。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21491v1",
      "published_date": "2025-04-30 10:19:21 UTC",
      "updated_date": "2025-04-30 10:19:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:22:15.513647"
    },
    {
      "arxiv_id": "2504.21489v2",
      "title": "TRIED: Truly Innovative and Effective AI Detection Benchmark, developed by WITNESS",
      "title_zh": "TRIED：由 WITNESS 开发的真正创新且有效的 AI 检测基准",
      "authors": [
        "Shirin Anlen",
        "Zuzanna Wojciak"
      ],
      "abstract": "The proliferation of generative AI and deceptive synthetic media threatens\nthe global information ecosystem, especially across the Global Majority. This\nreport from WITNESS highlights the limitations of current AI detection tools,\nwhich often underperform in real-world scenarios due to challenges related to\nexplainability, fairness, accessibility, and contextual relevance. In response,\nWITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)\nBenchmark, a new framework for evaluating detection tools based on their\nreal-world impact and capacity for innovation. Drawing on frontline\nexperiences, deceptive AI cases, and global consultations, the report outlines\nhow detection tools must evolve to become truly innovative and relevant by\nmeeting diverse linguistic, cultural, and technological contexts. It offers\npractical guidance for developers, policy actors, and standards bodies to\ndesign accountable, transparent, and user-centered detection solutions, and\nincorporate sociotechnical considerations into future AI standards, procedures\nand evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can\ndrive innovation, safeguard public trust, strengthen AI literacy, and\ncontribute to a more resilient global information credibility.",
      "tldr_zh": "WITNESS机构发布报告，指出当前AI检测工具在实际应用中存在可解释性、公平性、可访问性和上下文相关性等局限性，尤其是在全球大多数地区表现不佳。为了解决这些问题，WITNESS提出了“真正创新和有效AI检测 (TRIED) 基准”，这是一个基于实际影响和创新能力评估检测工具的新框架。该基准强调AI检测工具需要适应不同的语言、文化和技术环境，并为开发者、政策制定者和标准机构提供了设计负责任、透明和以用户为中心的检测解决方案的实用指导。通过采纳TRIED基准，旨在推动创新，维护公众信任，加强AI素养，并为更具弹性的全球信息可信度做出贡献。\n",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "33 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.21489v2",
      "published_date": "2025-04-30 10:18:19 UTC",
      "updated_date": "2025-05-01 13:38:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:22:27.444051"
    },
    {
      "arxiv_id": "2504.21480v1",
      "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense",
      "title_zh": "智能合约中可利用模式的综合研究：从漏洞到防御\n",
      "authors": [
        "Yuchen Ding",
        "Hongli Peng",
        "Xiaoqi Li"
      ],
      "abstract": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures.",
      "tldr_zh": "本文深入分析了以太坊智能合约中两种常见且关键的安全漏洞：重入漏洞(reentrancy)和整数溢出漏洞(integer overflow)。研究聚焦于Solidity编写并在以太坊虚拟机(EVM)上执行的智能合约，旨在剖析这些漏洞的底层机制，复现攻击场景，并评估有效的防御措施。该研究强调了智能合约安全在区块链生态系统中的重要性，以及防范漏洞以避免重大经济损失的必要性。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21480v1",
      "published_date": "2025-04-30 10:00:36 UTC",
      "updated_date": "2025-04-30 10:00:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:22:39.303948"
    },
    {
      "arxiv_id": "2504.21476v1",
      "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers",
      "title_zh": "GarmentDiffusion：基于多模态扩散 Transformer 的 3D 服装缝纫图案生成\n",
      "authors": [
        "Xinyu Li",
        "Qi Yao",
        "Yuanda Wang"
      ],
      "abstract": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/.",
      "tldr_zh": "该论文提出了GarmentDiffusion，一种新型生成模型，能够从多模态输入（文本、图像和不完整的缝纫图案）生成厘米级精度的矢量化3D缝纫图案。该方法将3D缝纫图案参数高效编码为紧凑的边缘token表示，序列长度比DressCode中的自回归SewingGPT缩短了10倍。通过采用扩散Transformer，该模型沿时间轴同时对所有边缘token进行去噪，且去噪步骤数量恒定，不受数据集特定的边缘和面板统计信息的影响。实验结果表明，GarmentDiffusion在DressCodeData和GarmentCodeData数据集上均取得了新的state-of-the-art结果，并且缝纫图案生成速度比SewingGPT提高了100倍。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.21476v1",
      "published_date": "2025-04-30 09:56:59 UTC",
      "updated_date": "2025-04-30 09:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:22:51.394624"
    },
    {
      "arxiv_id": "2504.21475v1",
      "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines",
      "title_zh": "推进阿拉伯语逆向词典系统：一种基于 Transformer 的方法与数据集构建指南\n",
      "authors": [
        "Serry Sibaee",
        "Samar Ahmed",
        "Abdullah Al Harbi",
        "Omer Nacar",
        "Adel Ammar",
        "Yasser Habashi",
        "Wadii Boulila"
      ],
      "abstract": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic.",
      "tldr_zh": "该研究旨在填补阿拉伯语自然语言处理领域的空白，开发了一种有效的阿拉伯语逆向词典(RD)系统，允许用户根据描述或含义查找单词。论文提出了一种基于Transformer的新方法，采用具有几何递减层的半编码器神经网络架构，在阿拉伯语RD任务中取得了state-of-the-art的结果。该方法结合了全面的数据集构建过程，并为阿拉伯语词典定义建立了正式的质量标准。实验表明，阿拉伯语专用模型显著优于通用多语言嵌入，其中ARBERTv2取得了最佳排名分数。此外，论文还提供了逆向词典任务的正式抽象，增强了理论理解，并开发了一个模块化、可扩展的Python库(RDTL)，具有可配置的训练pipeline。对数据集质量的分析揭示了改进阿拉伯语定义构建的重要见解，从而形成了八项构建高质量逆向词典资源的具体标准。这项工作为阿拉伯语计算语言学做出了重大贡献，并为阿拉伯语的语言学习、学术写作和专业交流提供了有价值的工具。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21475v1",
      "published_date": "2025-04-30 09:56:36 UTC",
      "updated_date": "2025-04-30 09:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:23:03.812559"
    },
    {
      "arxiv_id": "2504.21474v1",
      "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging",
      "title_zh": "Homa 在 SemEval-2025 Task 5 中的表现：使用 OntoAligner 对齐图书馆记录以进行主题标记\n",
      "authors": [
        "Hadi Bayrami Asl Tekanlou",
        "Jafar Razmara",
        "Mahsa Sanaei",
        "Mostafa Rahgouy",
        "Hamed Babaei Giglou"
      ],
      "abstract": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries.",
      "tldr_zh": "本文介绍了Homa系统，该系统参加了SemEval-2025 Task 5: Subject Tagging，旨在自动为TIBKAT中的技术记录分配主题标签，使用Gemeinsame Normdatei (GND)分类法。该系统利用OntoAligner，一个模块化的本体对齐工具包，通过集成检索增强生成(RAG)技术来解决此任务。该方法将主题标签问题转化为对齐任务，其中记录根据语义相似性与GND类别匹配。实验结果展示了该方法的优势和局限性，突出了对齐技术在改进数字图书馆中的主题标签方面的潜力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 4 figures, accepted to the LLMs4Subjects shared task at\n  SemEval2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21474v1",
      "published_date": "2025-04-30 09:52:51 UTC",
      "updated_date": "2025-04-30 09:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:23:15.173062"
    },
    {
      "arxiv_id": "2504.21457v1",
      "title": "xEEGNet: Towards Explainable AI in EEG Dementia Classification",
      "title_zh": "xEEGNet：面向脑电图痴呆分类中的可解释人工智能\n",
      "authors": [
        "Andrea Zanola",
        "Louis Fabrice Tshimanga",
        "Federico Del Pup",
        "Marco Baiesi",
        "Manfredo Atzori"
      ],
      "abstract": "This work presents xEEGNet, a novel, compact, and explainable neural network\nfor EEG data analysis. It is fully interpretable and reduces overfitting\nthrough major parameter reduction. As an applicative use case, we focused on\nclassifying common dementia conditions, Alzheimer's and frontotemporal\ndementia, versus controls. xEEGNet is broadly applicable to other neurological\nconditions involving spectral alterations. We initially used ShallowNet, a\nsimple and popular model from the EEGNet-family. Its structure was analyzed and\ngradually modified to move from a \"black box\" to a more transparent model,\nwithout compromising performance. The learned kernels and weights were examined\nfrom a clinical standpoint to assess medical relevance. Model variants,\nincluding ShallowNet and the final xEEGNet, were evaluated using robust\nNested-Leave-N-Subjects-Out cross-validation for unbiased performance\nestimates. Variability across data splits was explained using embedded EEG\nrepresentations, grouped by class and set, with pairwise separability to\nquantify group distinction. Overfitting was assessed through\ntraining-validation loss correlation and training speed. xEEGNet uses only 168\nparameters, 200 times fewer than ShallowNet, yet retains interpretability,\nresists overfitting, achieves comparable median performance (-1.5%), and\nreduces variability across splits. This variability is explained by embedded\nEEG representations: higher accuracy correlates with greater separation between\ntest set controls and Alzheimer's cases, without significant influence from\ntraining data. xEEGNet's ability to filter specific EEG bands, learn\nband-specific topographies, and use relevant spectral features demonstrates its\ninterpretability. While large deep learning models are often prioritized for\nperformance, this study shows smaller architectures like xEEGNet can be equally\neffective in EEG pathology classification.",
      "tldr_zh": "该论文提出了xEEGNet，一种紧凑且可解释的神经网络，用于脑电图(EEG)数据分析，特别应用于痴呆症分类，区分阿尔茨海默病、额颞叶痴呆与健康对照组。xEEGNet通过参数缩减降低过拟合，并具有完全的可解释性。研究从ShallowNet出发，逐步修改模型结构，在不牺牲性能的前提下，使其更透明。通过Nested-Leave-N-Subjects-Out交叉验证评估模型性能，并分析学习到的kernels和weights的临床相关性。实验结果表明，xEEGNet仅使用168个参数，比ShallowNet少200倍，但在保持可解释性、抵抗过拟合的同时，实现了可比的性能（-1.5%）。xEEGNet能够过滤特定EEG频段、学习特定频段的地形图，并使用相关的频谱特征，证明了其可解释性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21457v1",
      "published_date": "2025-04-30 09:24:50 UTC",
      "updated_date": "2025-04-30 09:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:23:27.744762"
    },
    {
      "arxiv_id": "2504.21454v1",
      "title": "SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments",
      "title_zh": "SimPRIVE：用于物理机器人与虚拟环境交互的仿真框架\n",
      "authors": [
        "Federico Nesti",
        "Gianluca D'Amico",
        "Mauro Marinoni",
        "Giorgio Buttazzo"
      ],
      "abstract": "The use of machine learning in cyber-physical systems has attracted the\ninterest of both industry and academia. However, no general solution has yet\nbeen found against the unpredictable behavior of neural networks and\nreinforcement learning agents. Nevertheless, the improvements of\nphoto-realistic simulators have paved the way towards extensive testing of\ncomplex algorithms in different virtual scenarios, which would be expensive and\ndangerous to implement in the real world.\n  This paper presents SimPRIVE, a simulation framework for physical robot\ninteraction with virtual environments, which operates as a vehicle-in-the-loop\nplatform, rendering a virtual world while operating the vehicle in the real\nworld.\n  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be\nconfigured to move its digital twin in a virtual world built with the Unreal\nEngine 5 graphic engine, which can be populated with objects, people, or other\nvehicles with programmable behavior.\n  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds\nwhile being light-weight to contain execution times and allow fast rendering.\nIts main advantage lies in the possibility of testing complex algorithms on the\nfull software and hardware stack while minimizing the risks and costs of a test\ncampaign. The framework has been validated by testing a reinforcement learning\nagent trained for obstacle avoidance on an AgileX Scout Mini rover that\nnavigates a virtual office environment where everyday objects and people are\nplaced as obstacles. The physical rover moves with no collision in an indoor\nlimited space, thanks to a LiDAR-based heuristic.",
      "tldr_zh": "SimPRIVE是一个用于物理机器人与虚拟环境交互的仿真框架，它作为一个车辆在环(vehicle-in-the-loop)平台运行，在真实世界中操作车辆的同时渲染虚拟世界。该框架允许任何运行在ROS 2上的物理移动机器人轻松配置，使其数字孪生体在用Unreal Engine 5构建的虚拟世界中移动，该虚拟世界可以填充具有可编程行为的物体、人和其它车辆。SimPRIVE的设计轻量化，以减少执行时间和允许快速渲染，同时能容纳定制或预构建的虚拟世界。通过在AgileX Scout Mini rover上测试一个用于避障的强化学习代理，验证了该框架，该rover在一个虚拟办公室环境中导航，其中日常物品和人被放置为障碍物。由于基于激光雷达的启发式方法，物理rover在室内有限空间内移动时没有发生碰撞。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to IEEE ITSC 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21454v1",
      "published_date": "2025-04-30 09:22:55 UTC",
      "updated_date": "2025-04-30 09:22:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:23:39.746338"
    },
    {
      "arxiv_id": "2504.21447v1",
      "title": "Rethinking Visual Layer Selection in Multimodal LLMs",
      "title_zh": "重新思考多模态 LLM 中的视觉层选择\n",
      "authors": [
        "Haoran Chen",
        "Junyan Lin",
        "Xinhao Chen",
        "Yue Fan",
        "Xin Jin",
        "Hui Su",
        "Jianfeng Dong",
        "Jinlan Fu",
        "Xiaoyu Shen"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs.",
      "tldr_zh": "本文重新审视了多模态大语言模型(MLLMs)中视觉层选择的问题。通过Layer-wise Representation Similarity方法，将CLIP-ViT的视觉层分为浅层、中层和深层，并评估它们对MLLM性能的影响。研究发现，深层对于OCR任务至关重要，而浅层和中层在涉及计数、定位和目标定位的推理任务上优于深层。此外，浅层、中层和深层特征的轻量级融合在10个数据集中的9个上优于专门的融合基线和单层选择。这项工作为MLLM中的视觉层选择提供了首个原则性研究，为深入研究MLLM的视觉表征学习奠定了基础。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures, submitted to ICCV 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21447v1",
      "published_date": "2025-04-30 09:07:10 UTC",
      "updated_date": "2025-04-30 09:07:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:23:51.335150"
    },
    {
      "arxiv_id": "2504.21435v1",
      "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding",
      "title_zh": "SeriesBench：叙事驱动的剧情系列理解基准\n",
      "authors": [
        "Chenkai Zhang",
        "Yiming Lei",
        "Zeming Liu",
        "Haitao Leng",
        "ShaoGuo Liu",
        "Tingting Gao",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "abstract": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025.",
      "tldr_zh": "为了评估多模态大型语言模型(MLLMs)在理解连续叙事视频方面的能力，该研究提出了SeriesBench，一个包含105个叙事驱动剧集的基准数据集。SeriesBench包含28个需要深度叙事理解的特定任务，涵盖了多种类型的剧集。此外，作者还提出了一种新的叙事推理框架PC-DCoT，旨在提升模型对情节结构和人物关系的分析能力。在SeriesBench上的实验结果表明，现有的MLLMs在理解叙事驱动的剧集方面仍然面临挑战，而PC-DCoT能够有效提升模型的性能。该研究强调了提升模型理解叙事驱动剧集能力的重要性，并为MLLMs的未来发展提供了指导。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "29 pages, 15 figures, CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21435v1",
      "published_date": "2025-04-30 08:48:21 UTC",
      "updated_date": "2025-04-30 08:48:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:24:03.402130"
    },
    {
      "arxiv_id": "2504.21433v1",
      "title": "NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence",
      "title_zh": "NGENT：下一代 AI 智能体必须整合多领域能力以实现通用人工智能\n",
      "authors": [
        "Zhicong Li",
        "Hangyu Mao",
        "Jiangjin Yin",
        "Mingzhe Xing",
        "Zhiwei Xu",
        "Yuanxing Zhang",
        "Yang Xiao"
      ],
      "abstract": "This paper argues that the next generation of AI agent (NGENT) should\nintegrate across-domain abilities to advance toward Artificial General\nIntelligence (AGI). Although current AI agents are effective in specialized\ntasks such as robotics, role-playing, and tool-using, they remain confined to\nnarrow domains. We propose that future AI agents should synthesize the\nstrengths of these specialized systems into a unified framework capable of\noperating across text, vision, robotics, reinforcement learning, emotional\nintelligence, and beyond. This integration is not only feasible but also\nessential for achieving the versatility and adaptability that characterize\nhuman intelligence. The convergence of technologies across AI domains, coupled\nwith increasing user demand for cross-domain capabilities, suggests that such\nintegration is within reach. Ultimately, the development of these versatile\nagents is a critical step toward realizing AGI. This paper explores the\nrationale for this shift, potential pathways for achieving it.",
      "tldr_zh": "本文认为，下一代人工智能体(NGENT)应整合跨领域能力，以实现通用人工智能(AGI)。当前AI agent虽然在机器人、角色扮演和工具使用等特定任务中有效，但仍局限于狭窄领域。文章提出，未来的AI agent应将这些专门系统的优势整合到一个统一的框架中，使其能够跨文本、视觉、机器人、强化学习、情感智能等领域运行。这种整合对于实现人类智能的多功能性和适应性至关重要，并且是可行的。跨AI领域技术的融合，以及用户对跨领域能力日益增长的需求表明，这种整合指日可待。开发这种多功能agent是实现AGI的关键一步。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21433v1",
      "published_date": "2025-04-30 08:46:14 UTC",
      "updated_date": "2025-04-30 08:46:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:24:15.362484"
    },
    {
      "arxiv_id": "2504.21428v1",
      "title": "UAV Marketplace Simulation Tool for BVLOS Operations",
      "title_zh": "用于视距外作业的无人机市场模拟工具\n",
      "authors": [
        "Kıvanç Şerefoğlu",
        "Önder Gürcan",
        "Reyhan Aydoğan"
      ],
      "abstract": "We present a simulation tool for evaluating team formation in autonomous\nmulti-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of\nSight (BVLOS). The tool models UAV collaboration and mission execution in\ndynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt\noperations. Our tool allows researchers to integrate and compare various team\nformation strategies in a controlled environment with configurable mission\nparameters and adversarial behaviors. The log of each simulation run is stored\nin a structured way along with performance metrics so that statistical analysis\ncould be done straightforwardly. The tool is versatile for testing and\nimproving UAV coordination strategies in real-world applications.",
      "tldr_zh": "本文介绍了一个用于评估自主多无人机(UAV)任务中团队组建的仿真工具，该任务在视距外(BVLOS)运行。该工具模拟了UAV在动态和对抗条件下的协作和任务执行，其中拜占庭UAV试图扰乱运行。该工具允许研究人员在具有可配置任务参数和对抗行为的受控环境中集成和比较各种团队组建策略。每次仿真运行的日志都以结构化的方式存储，并附带性能指标，以便可以简单地进行统计分析。该工具可用于测试和改进现实应用中的UAV协调策略。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.RO",
      "comment": "3 pages, 2 figures, the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.21428v1",
      "published_date": "2025-04-30 08:36:22 UTC",
      "updated_date": "2025-04-30 08:36:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:24:27.711552"
    },
    {
      "arxiv_id": "2504.21427v1",
      "title": "MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers",
      "title_zh": "MPEC：基于聚类分类器集成的流形保持脑电分类",
      "authors": [
        "Shermin Shahbazi",
        "Mohammad-Reza Nasiri",
        "Majid Ramezani"
      ],
      "abstract": "Accurate classification of EEG signals is crucial for brain-computer\ninterfaces (BCIs) and neuroprosthetic applications, yet many existing methods\nfail to account for the non-Euclidean, manifold structure of EEG data,\nresulting in suboptimal performance. Preserving this manifold information is\nessential to capture the true geometry of EEG signals, but traditional\nclassification techniques largely overlook this need. To this end, we propose\nMPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based\nClassifiers), that introduces two key innovations: (1) a feature engineering\nphase that combines covariance matrices and Radial Basis Function (RBF) kernels\nto capture both linear and non-linear relationships among EEG channels, and (2)\na clustering phase that employs a modified K-means algorithm tailored for the\nRiemannian manifold space, ensuring local geometric sensitivity. Ensembling\nmultiple clustering-based classifiers, MPEC achieves superior results,\nvalidated by significant improvements on the BCI Competition IV dataset 2a.",
      "tldr_zh": "该论文提出了一种名为MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers) 的脑电信号分类方法，旨在解决传统方法忽略脑电数据非欧几里得流形结构的问题。MPEC包含两个关键创新：首先，利用协方差矩阵和径向基函数(RBF)核进行特征工程，捕捉脑电通道间的线性和非线性关系；其次，采用改进的K-means算法，在黎曼流形空间中进行聚类，保证局部几何敏感性。通过集成多个基于聚类的分类器，MPEC在BCI Competition IV dataset 2a上取得了显著的性能提升，验证了其有效性。该方法能够更好地保留脑电信号的流形信息，从而提高分类精度。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages ,3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.21427v1",
      "published_date": "2025-04-30 08:34:15 UTC",
      "updated_date": "2025-04-30 08:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:24:39.902677"
    },
    {
      "arxiv_id": "2504.21415v1",
      "title": "Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges",
      "title_zh": "通过机器学习优化用于用户身份验证的鼠标动态：解决数据充分性、准确性-实用性权衡以及模型性能挑战\n",
      "authors": [
        "Yi Wang",
        "Chengyv Wu",
        "Yang Liao",
        "Maowei You"
      ],
      "abstract": "User authentication is essential to ensure secure access to computer systems,\nyet traditional methods face limitations in usability, cost, and security.\nMouse dynamics authentication, based on the analysis of users' natural\ninteraction behaviors with mouse devices, offers a cost-effective,\nnon-intrusive, and adaptable solution. However, challenges remain in\ndetermining the optimal data volume, balancing accuracy and practicality, and\neffectively capturing temporal behavioral patterns. In this study, we propose a\nstatistical method using Gaussian kernel density estimate (KDE) and\nKullback-Leibler (KL) divergence to estimate the sufficient data volume for\ntraining authentication models. We introduce the Mouse Authentication Unit\n(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for\nefficient and accurate behavioral representation. Furthermore, we design the\nLocal-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet\nfor local feature extraction and GRU for modeling long-term temporal\ndependencies. Taking the Balabit and DFL datasets as examples, we significantly\nreduced the data scale, particularly by a factor of 10 for the DFL dataset,\ngreatly alleviating the training burden. Additionally, we determined the\noptimal input recognition unit length for the user authentication system on\ndifferent datasets based on the slope of Approximate Entropy. Training with\nimbalanced samples, our model achieved a successful defense AUC 98.52% for\nblind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing\nthe current sota performance.",
      "tldr_zh": "该研究针对鼠标动态认证在用户身份验证中的应用，旨在解决数据量需求、准确性与实用性之间的权衡以及模型性能等挑战。研究提出了一种基于高斯核密度估计(KDE)和Kullback-Leibler (KL)散度的统计方法，用于评估训练认证模型的充足数据量。引入了鼠标认证单元(MAU)，利用近似熵(ApEn)优化分段长度，实现高效准确的行为表示。此外，设计了局部时间鼠标认证(LT-AMouse)框架，集成了1D-ResNet用于局部特征提取，GRU用于建模长期时间依赖性。实验结果表明，该方法显著降低了数据规模，并在不平衡样本训练下，模型在DFL和Balabit数据集上均超越了当前SOTA性能。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "14pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.21415v1",
      "published_date": "2025-04-30 08:16:52 UTC",
      "updated_date": "2025-04-30 08:16:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:24:51.526961"
    },
    {
      "arxiv_id": "2504.21411v1",
      "title": "Galvatron: An Automatic Distributed System for Efficient Foundation Model Training",
      "title_zh": "Galvatron：用于高效基础模型训练的自动分布式系统\n",
      "authors": [
        "Xinyi Liu",
        "Yujie Wang",
        "Shenhan Zhu",
        "Fangcheng Fu",
        "Qingshuo Liu",
        "Guangming Lin",
        "Bin Cui"
      ],
      "abstract": "Galvatron is a distributed system for efficiently training large-scale\nFoundation Models. It overcomes the complexities of selecting optimal\nparallelism strategies by automatically identifying the most efficient hybrid\nstrategy, incorporating data, tensor, pipeline, sharded data, and sequence\nparallelism, along with recomputation. The system's architecture includes a\nprofiler for hardware and model analysis, a search engine for strategy\noptimization using decision trees and dynamic programming, and a runtime for\nexecuting these strategies efficiently. Benchmarking on various clusters\ndemonstrates Galvatron's superior throughput compared to existing frameworks.\nThis open-source system offers user-friendly interfaces and comprehensive\ndocumentation, making complex distributed training accessible and efficient.\nThe source code of Galvatron is available at\nhttps://github.com/PKU-DAIR/Hetu-Galvatron.",
      "tldr_zh": "Galvatron是一个用于高效训练大规模Foundation Model的分布式系统。它通过自动识别最优混合并行策略（包括数据并行、张量并行、流水线并行、分片数据并行和序列并行，以及重计算）来克服选择最优并行策略的复杂性。该系统包含一个用于硬件和模型分析的profiler，一个使用决策树和动态规划进行策略优化的搜索引擎，以及一个用于高效执行这些策略的runtime。在各种集群上的基准测试表明，Galvatron的吞吐量优于现有框架。该系统提供用户友好的界面和全面的文档，使复杂的分布式训练变得可访问和高效。Galvatron的源代码已开源。\n",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21411v1",
      "published_date": "2025-04-30 08:11:45 UTC",
      "updated_date": "2025-04-30 08:11:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:25:03.395657"
    },
    {
      "arxiv_id": "2504.21383v1",
      "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning",
      "title_zh": "FAST-Q：基于对抗平衡状态表征的快速探索，用于离线强化学习中的反事实行为估计\n",
      "authors": [
        "Pulkit Agrawal",
        "Rukma Talwadker",
        "Aditya Pareek",
        "Tridib Mukherjee"
      ],
      "abstract": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform.",
      "tldr_zh": "该论文提出了FAST-Q，一种用于离线强化学习的新方法，旨在解决由于玩家心理和平台波动性导致的状态空间稀疏和部分重叠问题，从而改进反事实行动估计。FAST-Q利用梯度反转学习构建平衡的状态表示，降低玩家状态和行动之间的策略特定偏差，从而实现反事实估计。该方法支持离线反事实探索，并提出了一种Q值分解策略用于多目标优化，以实现可解释的推荐。在游戏平台上的实验结果表明，FAST-Q优于现有方法，并在玩家收益、LTV、推荐驱动的参与度、平台停留时间和推荐成本方面均有显著提升。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21383v1",
      "published_date": "2025-04-30 07:32:40 UTC",
      "updated_date": "2025-04-30 07:32:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:25:15.465017"
    },
    {
      "arxiv_id": "2504.21372v1",
      "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction",
      "title_zh": "检索增强的少样本提示用于语音事件抽取\n",
      "authors": [
        "Máté Gedeon"
      ],
      "abstract": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features.",
      "tldr_zh": "本文提出了一种基于检索增强的少样本提示(Retrieval-Enhanced Few-Shot Prompting)的语音事件抽取(Speech Event Extraction, SpeechEE)框架。该框架结合了高性能的自动语音识别(ASR)和基于语义搜索增强的大语言模型(LLM)提示，通过混合过滤机制（包括规则、BERT和LLM模型）对可能包含事件的语音片段进行分类。然后，利用少样本LLM提示，并通过语义相似性检索动态地丰富提示内容，从而识别事件触发词并提取相应的论元。实验结果表明，该框架在触发词分类和论元分类上均取得了显著的性能提升，证明了检索增强的LLM驱动的pipeline方法可以与端到端系统相媲美，同时保持了可解释性和模块化。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21372v1",
      "published_date": "2025-04-30 07:10:10 UTC",
      "updated_date": "2025-04-30 07:10:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:25:27.631979"
    },
    {
      "arxiv_id": "2504.21370v1",
      "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning",
      "title_zh": "ShorterBetter：引导推理模型寻找最优推理长度以实现高效推理\n",
      "authors": [
        "Jingyang Yi",
        "Jiazheng Wang"
      ],
      "abstract": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong\nperformance on reasoning-intensive tasks through extended Chain-of-Thought\n(CoT) prompting. While longer reasoning traces can facilitate a more thorough\nexploration of solution paths for complex problems, researchers have observed\nthat these models often \"overthink\", leading to inefficient inference. In this\npaper, we introduce ShorterBetter, a simple yet effective reinforcement\nlearning methed that enables reasoning language models to discover their own\noptimal CoT lengths without human intervention. By sampling multiple outputs\nper problem and defining the Sample Optimal Length (SOL) as the shortest\ncorrect response among all the outputs, our method dynamically guides the model\ntoward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B\nmodel, ShorterBetter achieves up to an 80% reduction in output length on both\nin-domain and out-of-domain reasoning tasks while maintaining accuracy. Our\nanalysis shows that overly long reasoning traces often reflect loss of\nreasoning direction, and thus suggests that the extended CoT produced by\nreasoning models is highly compressible.",
      "tldr_zh": "该论文提出了ShorterBetter，一种强化学习方法，旨在引导推理模型自动发现最佳的Chain-of-Thought (CoT)长度，从而提高推理效率。该方法通过对每个问题采样多个输出，并将“样本最优长度”(Sample Optimal Length, SOL)定义为所有输出中最短的正确答案，动态引导模型缩短推理长度。实验结果表明，ShorterBetter应用于DeepSeek-Distill-Qwen-1.5B模型后，在保持准确率的同时，能够将输出长度缩短高达80%，表明推理模型产生的过长CoT具有高度可压缩性。分析表明，过长的推理过程通常反映了推理方向的偏离。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "An appendix will be uploaded soon",
      "pdf_url": "http://arxiv.org/pdf/2504.21370v1",
      "published_date": "2025-04-30 07:04:19 UTC",
      "updated_date": "2025-04-30 07:04:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:25:39.558695"
    },
    {
      "arxiv_id": "2504.21368v1",
      "title": "Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality",
      "title_zh": "重温用于图像重建质量的扩散自编码器训练\n",
      "authors": [
        "Pramook Khungurn",
        "Sukit Seripanitkarn",
        "Phonphrm Thawatdamrongkit",
        "Supasorn Suwajanakorn"
      ],
      "abstract": "Diffusion autoencoders (DAEs) are typically formulated as a noise prediction\nmodel and trained with a linear-$\\beta$ noise schedule that spends much of its\nsampling steps at high noise levels. Because high noise levels are associated\nwith recovering large-scale image structures and low noise levels with\nrecovering details, this configuration can result in low-quality and blurry\nimages. However, it should be possible to improve details while spending fewer\nsteps recovering structures because the latent code should already contain\nstructural information. Based on this insight, we propose a new DAE training\nmethod that improves the quality of reconstructed images. We divide training\ninto two phases. In the first phase, the DAE is trained as a vanilla\nautoencoder by always setting the noise level to the highest, forcing the\nencoder and decoder to populate the latent code with structural information. In\nthe second phase, we incorporate a noise schedule that spends more time in the\nlow-noise region, allowing the DAE to learn how to perfect the details. Our\nmethod results in images that have accurate high-level structures and low-level\ndetails while still preserving useful properties of the latent codes.",
      "tldr_zh": "该论文重新审视了扩散自编码器(DAE)的训练方法，旨在提升图像重建质量。传统DAE训练采用线性-$\\beta$噪声调度，导致模型在较高噪声水平上花费过多采样步骤，影响细节恢复。作者提出一种新的两阶段训练方法：第一阶段，DAE作为普通自编码器训练，始终使用最高噪声水平，迫使编码器和解码器将结构信息编码到潜在代码中；第二阶段，采用噪声调度，使模型更多时间处于低噪声区域，从而学习完善细节。实验结果表明，该方法在保留潜在代码有用属性的同时，能够生成具有准确高层结构和低层细节的图像。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21368v1",
      "published_date": "2025-04-30 07:00:33 UTC",
      "updated_date": "2025-04-30 07:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:25:51.590161"
    },
    {
      "arxiv_id": "2504.21366v1",
      "title": "DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion",
      "title_zh": "DGFNet：基于动态门控融合的端到端音视频源分离\n",
      "authors": [
        "Yinfeng Yu",
        "Shiyu Sun"
      ],
      "abstract": "Current Audio-Visual Source Separation methods primarily adopt two design\nstrategies. The first strategy involves fusing audio and visual features at the\nbottleneck layer of the encoder, followed by processing the fused features\nthrough the decoder. However, when there is a significant disparity between the\ntwo modalities, this approach may lead to the loss of critical information. The\nsecond strategy avoids direct fusion and instead relies on the decoder to\nhandle the interaction between audio and visual features. Nonetheless, if the\nencoder fails to integrate information across modalities adequately, the\ndecoder may be unable to effectively capture the complex relationships between\nthem. To address these issues, this paper proposes a dynamic fusion method\nbased on a gating mechanism that dynamically adjusts the modality fusion\ndegree. This approach mitigates the limitations of solely relying on the\ndecoder and facilitates efficient collaboration between audio and visual\nfeatures. Additionally, an audio attention module is introduced to enhance the\nexpressive capacity of audio features, thereby further improving model\nperformance. Experimental results demonstrate that our method achieves\nsignificant performance improvements on two benchmark datasets, validating its\neffectiveness and advantages in Audio-Visual Source Separation tasks.",
      "tldr_zh": "该论文提出了一种基于动态门控融合(Dynamic Gating Fusion, DGF)的端到端音视频源分离网络(Audio-Visual Source Separation, AVSS)。DGFNet旨在解决现有AVSS方法中，因音视频模态差异大或编码器模态信息融合不足导致的信息损失问题。该网络通过动态调整模态融合程度的门控机制，以及增强音频特征表达能力的音频注意力模块，实现了音视频特征的有效协同。实验结果表明，DGFNet在两个基准数据集上取得了显著的性能提升，验证了其在AVSS任务中的有效性。\n",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (9 pages). Accepted for publication by ICMR(International\n  Conference on Multimedia Retrieval) 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21366v1",
      "published_date": "2025-04-30 06:55:24 UTC",
      "updated_date": "2025-04-30 06:55:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:26:03.413804"
    },
    {
      "arxiv_id": "2504.21358v1",
      "title": "A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting",
      "title_zh": "深度学习与集成学习在扩展交通预测范围上的比较研究\n",
      "authors": [
        "Xiao Zheng",
        "Saeed Asadi Bagloee",
        "Majid Sarvi"
      ],
      "abstract": "Traffic forecasting is vital for Intelligent Transportation Systems, for\nwhich Machine Learning (ML) methods have been extensively explored to develop\ndata-driven Artificial Intelligence (AI) solutions. Recent research focuses on\nmodelling spatial-temporal correlations for short-term traffic prediction,\nleaving the favourable long-term forecasting a challenging and open issue. This\npaper presents a comparative study on large-scale real-world signalized\narterials and freeway traffic flow datasets, aiming to evaluate promising ML\nmethods in the context of large forecasting horizons up to 30 days. Focusing on\nmodelling capacity for temporal dynamics, we develop one ensemble ML method,\neXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,\nincluding Recurrent Neural Network (RNN)-based methods and the state-of-the-art\nTransformer-based method. Time embedding is leveraged to enhance their\nunderstanding of seasonality and event factors. Experimental results highlight\nthat while the attention mechanism/Transformer framework is effective for\ncapturing long-range dependencies in sequential data, as the forecasting\nhorizon extends, the key to effective traffic forecasting gradually shifts from\ntemporal dependency capturing to periodicity modelling. Time embedding is\nparticularly effective in this context, helping naive RNN outperform Informer\nby 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust\nmodel, XGBoost, while learning solely from time features, performs\ncompetitively with DL methods. Moreover, we investigate the impacts of various\nfactors like input sequence length, holiday traffic, data granularity, and\ntraining data size. The findings offer valuable insights and serve as a\nreference for future long-term traffic forecasting research and the improvement\nof AI's corresponding learning capabilities.",
      "tldr_zh": "该论文对比研究了深度学习(DL)和集成学习(ensemble learning)方法在长时程交通预测中的表现，最长预测范围可达30天。研究使用了大规模真实世界的信号控制干道和高速公路交通流量数据集，评估了包括基于循环神经网络(RNN)的方法和基于Transformer的方法等多种模型。实验结果表明，随着预测时程的延长，有效交通预测的关键逐渐从时间依赖性捕获转变为周期性建模，时间嵌入(Time embedding)在此背景下特别有效。XGBoost作为一种高效且鲁棒的模型，仅从时间特征学习，其性能与深度学习方法具有竞争力。该研究还探讨了输入序列长度、节假日交通、数据粒度和训练数据大小等因素的影响，为未来的长期交通预测研究提供了有价值的见解。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.21358v1",
      "published_date": "2025-04-30 06:31:21 UTC",
      "updated_date": "2025-04-30 06:31:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:26:15.787753"
    },
    {
      "arxiv_id": "2504.21356v1",
      "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing",
      "title_zh": "Nexus-Gen：用于图像理解、生成和编辑的统一模型\n",
      "authors": [
        "Hong Zhang",
        "Zhongjie Duan",
        "Xingjun Wang",
        "Yingda Chen",
        "Yuze Zhao",
        "Yu Zhang"
      ],
      "abstract": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.",
      "tldr_zh": "Nexus-Gen是一个统一的多模态大语言模型(MLLM)，旨在整合图像理解和生成能力。它结合了LLM的语言推理能力和扩散模型的图像合成能力。该模型通过双阶段对齐训练过程对齐LLM和扩散模型的嵌入空间：首先，自回归LLM学习预测以多模态输入为条件的图像嵌入；然后，视觉解码器被训练从这些嵌入中重建高保真图像。为了解决自回归范式训练和推理阶段的差异，Nexus-Gen引入了一种预填充自回归策略，使用位置嵌入的特殊token预填充输入序列。实验结果表明，Nexus-Gen能够全面处理图像理解、生成和编辑任务。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21356v1",
      "published_date": "2025-04-30 06:30:48 UTC",
      "updated_date": "2025-04-30 06:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:26:27.596543"
    },
    {
      "arxiv_id": "2504.21347v1",
      "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces",
      "title_zh": "IRL Dittos：开放空间中具身多模态 AI 智能体交互\n",
      "authors": [
        "Seonghee Lee",
        "Denae Ford",
        "John Tang",
        "Sasa Junuzovic",
        "Asta Roseway",
        "Ed Cutrell",
        "Kori Inkpen"
      ],
      "abstract": "We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent\ndesigned to represent remote colleagues in shared office spaces, creating\nopportunities for real-time exchanges even in their absence. IRL Ditto offers a\nunique hybrid experience by allowing in-person colleagues to encounter a\ndigital version of their remote teammates, initiating greetings, updates, or\nsmall talk as they might in person. Our research question examines: How can the\nIRL Ditto influence interactions and relationships among colleagues in a shared\noffice space? Through a four-day study, we assessed IRL Ditto's ability to\nstrengthen social ties by simulating presence and enabling meaningful\ninteractions across different levels of social familiarity. We find that\nenhancing social relationships depended deeply on the foundation of the\nrelationship participants had with the source of the IRL Ditto. This study\nprovides insights into the role of embodied agents in enriching workplace\ndynamics for distributed teams.",
      "tldr_zh": "该研究介绍了“IRL Ditto”，一种AI驱动的具身智能体，旨在代表远程同事出现在共享办公空间，创造实时交流的机会。IRL Ditto提供了一种独特的混合体验，允许在场同事遇到远程同事的数字版本，发起问候、更新或闲聊，就像他们亲自在场一样。一项为期四天的研究评估了IRL Ditto通过模拟存在和促进有意义的互动来加强社交联系的能力。研究发现，增强社交关系在很大程度上取决于参与者与IRL Ditto来源的关系基础。这项研究深入了解了具身智能体在丰富分布式团队工作场所动态中的作用。\n",
      "categories": [
        "cs.AI",
        "cs.HC",
        "H.5.2; I.2.9"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.21347v1",
      "published_date": "2025-04-30 06:16:32 UTC",
      "updated_date": "2025-04-30 06:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:26:39.588128"
    },
    {
      "arxiv_id": "2504.21344v1",
      "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection",
      "title_zh": "基于视觉-语言模型的语义引导影像学生物标志物用于早期肺癌检测\n",
      "authors": [
        "Luoting Zhuang",
        "Seyed Mohammad Hossein Tabatabaei",
        "Ramin Salehi-Rad",
        "Linh M. Tran",
        "Denise R. Aberle",
        "Ashley E. Prosper",
        "William Hsu"
      ],
      "abstract": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings.",
      "tldr_zh": "该研究提出了一种基于视觉-语言模型(Vision-Language Model)的语义引导成像生物标志物，用于早期肺癌检测。该模型利用从放射科医生评估中提取的语义特征，结合对比语言-图像预训练(CLIP)模型进行微调，以预测一年的肺癌诊断结果。实验结果表明，该模型在外部数据集上表现优于现有最佳模型，AUROC达到0.90，AUPRC达到0.78。此外，模型还能预测结节边缘、结节一致性和胸膜附着等语义特征，为模型预测提供了解释性，有助于临床医生理解模型预测的潜在含义。该方法旨在防止模型学习捷径，并在临床环境中推广应用。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21344v1",
      "published_date": "2025-04-30 06:11:34 UTC",
      "updated_date": "2025-04-30 06:11:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:26:51.618516"
    },
    {
      "arxiv_id": "2504.21326v1",
      "title": "Q-function Decomposition with Intervention Semantics with Factored Action Spaces",
      "title_zh": "基于干预语义和分解行动空间的 Q 函数分解\n",
      "authors": [
        "Junkyu Lee",
        "Tian Gao",
        "Elliot Nelson",
        "Miao Liu",
        "Debarun Bhattacharjya",
        "Songtao Lu"
      ],
      "abstract": "Many practical reinforcement learning environments have a discrete factored\naction space that induces a large combinatorial set of actions, thereby posing\nsignificant challenges. Existing approaches leverage the regular structure of\nthe action space and resort to a linear decomposition of Q-functions, which\navoids enumerating all combinations of factored actions. In this paper, we\nconsider Q-functions defined over a lower dimensional projected subspace of the\noriginal action space, and study the condition for the unbiasedness of\ndecomposed Q-functions using causal effect estimation from the no unobserved\nconfounder setting in causal statistics. This leads to a general scheme which\nwe call action decomposed reinforcement learning that uses the projected\nQ-functions to approximate the Q-function in standard model-free reinforcement\nlearning algorithms. The proposed approach is shown to improve sample\ncomplexity in a model-based reinforcement learning setting. We demonstrate\nimprovements in sample efficiency compared to state-of-the-art baselines in\nonline continuous control environments and a real-world offline sepsis\ntreatment environment.",
      "tldr_zh": "该论文提出了一种基于干预语义的Q函数分解方法，用于解决具有离散分解动作空间的强化学习问题。该方法将Q函数定义在原始动作空间的低维投影子空间上，并利用因果统计中的无未观测混淆假设，研究了分解Q函数无偏性的条件。由此提出了一种通用的动作分解强化学习方案，该方案使用投影Q函数来近似标准无模型强化学习算法中的Q函数。理论分析表明，该方法可以提高基于模型的强化学习的样本复杂度。实验结果表明，与最先进的基线方法相比，该方法在在线连续控制环境和真实的离线脓毒症治疗环境中均能提高样本效率。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AISTATS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21326v1",
      "published_date": "2025-04-30 05:26:51 UTC",
      "updated_date": "2025-04-30 05:26:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:27:03.761718"
    },
    {
      "arxiv_id": "2504.21323v1",
      "title": "How to Backdoor the Knowledge Distillation",
      "title_zh": "如何对知识蒸馏进行后门攻击\n",
      "authors": [
        "Chen Wu",
        "Qian Ma",
        "Prasenjit Mitra",
        "Sencun Zhu"
      ],
      "abstract": "Knowledge distillation has become a cornerstone in modern machine learning\nsystems, celebrated for its ability to transfer knowledge from a large, complex\nteacher model to a more efficient student model. Traditionally, this process is\nregarded as secure, assuming the teacher model is clean. This belief stems from\nconventional backdoor attacks relying on poisoned training data with backdoor\ntriggers and attacker-chosen labels, which are not involved in the distillation\nprocess. Instead, knowledge distillation uses the outputs of a clean teacher\nmodel to guide the student model, inherently preventing recognition or response\nto backdoor triggers as intended by an attacker. In this paper, we challenge\nthis assumption by introducing a novel attack methodology that strategically\npoisons the distillation dataset with adversarial examples embedded with\nbackdoor triggers. This technique allows for the stealthy compromise of the\nstudent model while maintaining the integrity of the teacher model. Our\ninnovative approach represents the first successful exploitation of\nvulnerabilities within the knowledge distillation process using clean teacher\nmodels. Through extensive experiments conducted across various datasets and\nattack settings, we demonstrate the robustness, stealthiness, and effectiveness\nof our method. Our findings reveal previously unrecognized vulnerabilities and\npave the way for future research aimed at securing knowledge distillation\nprocesses against backdoor attacks.",
      "tldr_zh": "该论文提出了一种新的后门攻击方法，专门针对知识蒸馏过程。与传统依赖带后门触发器和恶意标签的投毒数据不同，该方法通过在蒸馏数据集中嵌入带有后门触发器的对抗样本，从而隐蔽地攻击学生模型，同时保持教师模型的完整性。这种方法挑战了知识蒸馏过程的安全性假设，即干净的教师模型可以保证学生模型的安全性。实验证明，该方法在各种数据集和攻击设置下都具有鲁棒性、隐蔽性和有效性，揭示了知识蒸馏过程中的潜在漏洞，并为未来研究如何保护知识蒸馏过程免受后门攻击奠定了基础。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21323v1",
      "published_date": "2025-04-30 05:19:23 UTC",
      "updated_date": "2025-04-30 05:19:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:27:15.672215"
    },
    {
      "arxiv_id": "2504.21318v1",
      "title": "Phi-4-reasoning Technical Report",
      "title_zh": "Phi-4-reasoning 技术报告\n",
      "authors": [
        "Marah Abdin",
        "Sahaj Agarwal",
        "Ahmed Awadallah",
        "Vidhisha Balachandran",
        "Harkirat Behl",
        "Lingjiao Chen",
        "Gustavo de Rosa",
        "Suriya Gunasekar",
        "Mojan Javaheripi",
        "Neel Joshi",
        "Piero Kauffmann",
        "Yash Lara",
        "Caio César Teodoro Mendes",
        "Arindam Mitra",
        "Besmira Nushi",
        "Dimitris Papailiopoulos",
        "Olli Saarikivi",
        "Shital Shah",
        "Vaishnavi Shrivastava",
        "Vibhav Vineet",
        "Yue Wu",
        "Safoora Yousefi",
        "Guoqing Zheng"
      ],
      "abstract": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
      "tldr_zh": "该报告介绍了Phi-4-reasoning，一个140亿参数的推理模型，在复杂推理任务上表现出色。该模型通过在精心策划的“可教”提示集上对Phi-4进行监督微调，并使用o3-mini生成推理演示，从而生成详细的推理链，有效利用推理时的计算能力。此外，还开发了Phi-4-reasoning-plus，通过基于结果的强化学习进行增强，通过生成更长的推理轨迹来提供更高的性能。在广泛的推理任务中，这两个模型都显著优于更大的开放权重模型，如DeepSeek-R1-Distill-Llama-70B模型，并接近完整DeepSeek-R1模型的性能水平。评估涵盖了数学和科学推理、编码、算法问题解决、规划和空间理解等基准。研究表明，改进可以非平凡地转移到通用基准。该报告深入探讨了训练数据、训练方法和评估，表明精心策划数据以进行监督微调（SFT）的好处可以扩展到推理语言模型，并且可以通过强化学习（RL）进一步放大。评估还指出了改进评估推理模型性能和鲁棒性的机会。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21318v1",
      "published_date": "2025-04-30 05:05:09 UTC",
      "updated_date": "2025-04-30 05:05:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:27:28.232588"
    },
    {
      "arxiv_id": "2504.21297v1",
      "title": "Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI",
      "title_zh": "参与式人工智能、公共部门人工智能、差分隐私、对话式界面、可解释人工智能、公民参与人工智能\n",
      "authors": [
        "Wenjun Yang",
        "Eyhab Al-Masri"
      ],
      "abstract": "This paper introduces a conversational interface system that enables\nparticipatory design of differentially private AI systems in public sector\napplications. Addressing the challenge of balancing mathematical privacy\nguarantees with democratic accountability, we propose three key contributions:\n(1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria\ndecision analysis to align citizen preferences with differential privacy (DP)\nparameters, (2) an explainable noise-injection framework featuring real-time\nMean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and\n(3) an integrated legal-compliance mechanism that dynamically modulates privacy\nbudgets based on evolving regulatory constraints. Our results advance\nparticipatory AI practices by demonstrating how conversational interfaces can\nenhance public engagement in algorithmic privacy mechanisms, ensuring that\nprivacy-preserving AI in public sector governance remains both mathematically\nrobust and democratically accountable.",
      "tldr_zh": "本文介绍了一个对话式界面系统，旨在促进公共部门应用中差分隐私(Differential Privacy, DP)人工智能系统的参与式设计。该系统通过三个关键贡献平衡了数学隐私保证与民主责任：(1) 利用TOPSIS多标准决策分析的自适应$\\epsilon$选择协议，将公民偏好与DP参数对齐；(2) 一个可解释的噪声注入框架，具有实时平均绝对误差(Mean Absolute Error, MAE)可视化和GPT-4驱动的影响分析；(3) 一个集成的法律合规机制，根据不断变化的监管约束动态调整隐私预算。实验结果表明，该系统能够增强公众对算法隐私机制的参与，确保公共部门中隐私保护AI在数学上稳健且在民主上负责。\n",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21297v1",
      "published_date": "2025-04-30 04:10:50 UTC",
      "updated_date": "2025-04-30 04:10:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:27:39.682444"
    },
    {
      "arxiv_id": "2504.21296v1",
      "title": "Fairness in Graph Learning Augmented with Machine Learning: A Survey",
      "title_zh": "机器学习增强的图学习中的公平性：一项综述\n",
      "authors": [
        "Renqiang Luo",
        "Ziqi Xu",
        "Xikun Zhang",
        "Qing Qing",
        "Huafei Huang",
        "Enyan Dai",
        "Zhe Wang",
        "Bo Yang"
      ],
      "abstract": "Augmenting specialised machine learning techniques into traditional graph\nlearning models has achieved notable success across various domains, including\nfederated graph learning, dynamic graph learning, and graph transformers.\nHowever, the intricate mechanisms of these specialised techniques introduce\nsignificant challenges in maintaining model fairness, potentially resulting in\ndiscriminatory outcomes in high-stakes applications such as recommendation\nsystems, disaster response, criminal justice, and loan approval. This paper\nsystematically examines the unique fairness challenges posed by Graph Learning\naugmented with Machine Learning (GL-ML). It highlights the complex interplay\nbetween graph learning mechanisms and machine learning techniques, emphasising\nhow the augmentation of machine learning both enhances and complicates\nfairness. Additionally, we explore four critical techniques frequently employed\nto improve fairness in GL-ML methods. By thoroughly investigating the root\ncauses and broader implications of fairness challenges in this rapidly evolving\nfield, this work establishes a robust foundation for future research and\ninnovation in GL-ML fairness.",
      "tldr_zh": "这篇综述论文系统性地探讨了图学习(Graph Learning)与机器学习(Machine Learning)结合(GL-ML)所带来的公平性挑战。GL-ML在联邦图学习、动态图学习和图Transformer等领域取得了显著进展，但同时也引入了复杂的机制，可能导致推荐系统、灾难响应等高风险应用中出现歧视性结果。论文深入研究了图学习机制和机器学习技术之间复杂的相互作用，并探讨了四种常用的GL-ML公平性提升技术。该研究旨在为GL-ML公平性领域的未来研究和创新奠定基础。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21296v1",
      "published_date": "2025-04-30 04:02:23 UTC",
      "updated_date": "2025-04-30 04:02:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:27:51.536753"
    },
    {
      "arxiv_id": "2504.21289v1",
      "title": "Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction",
      "title_zh": "基于正交因子的双聚类算法 (BCBOF) 及其在高维数据和股票趋势预测中的应用\n",
      "authors": [
        "Yan Huang",
        "Da-Qing Zhang"
      ],
      "abstract": "Biclustering is an effective technique in data mining and pattern\nrecognition. Biclustering algorithms based on traditional clustering face two\nfundamental limitations when processing high-dimensional data: (1) The distance\nconcentration phenomenon in high-dimensional spaces leads to data sparsity,\nrendering similarity measures ineffective; (2) Mainstream linear dimensionality\nreduction methods disrupt critical local structural patterns. To apply\nbiclustering to high-dimensional datasets, we propose an orthogonal\nfactor-based biclustering algorithm (BCBOF). First, we constructed orthogonal\nfactors in the vector space of the high-dimensional dataset. Then, we performed\nclustering using the coordinates of the original data in the orthogonal\nsubspace as clustering targets. Finally, we obtained biclustering results of\nthe original dataset. Since dimensionality reduction was applied before\nclustering, the proposed algorithm effectively mitigated the data sparsity\nproblem caused by high dimensionality. Additionally, we applied this\nbiclustering algorithm to stock technical indicator combinations and stock\nprice trend prediction. Biclustering results were transformed into fuzzy rules,\nand we incorporated profit-preserving and stop-loss rules into the rule set,\nultimately forming a fuzzy inference system for stock price trend predictions\nand trading signals. To evaluate the performance of BCBOF, we compared it with\nexisting biclustering methods using multiple evaluation metrics. The results\nshowed that our algorithm outperformed other biclustering techniques. To\nvalidate the effectiveness of the fuzzy inference system, we conducted virtual\ntrading experiments using historical data from 10 A-share stocks. The\nexperimental results showed that the generated trading strategies yielded\nhigher returns for investors.",
      "tldr_zh": "该论文提出了一种基于正交因子的双聚类算法(BCBOF)，用于处理高维数据，并将其应用于股票趋势预测。BCBOF算法首先在高维数据集的向量空间中构建正交因子，然后利用原始数据在正交子空间中的坐标进行聚类，从而缓解高维空间中的数据稀疏性问题。该算法将双聚类结果转化为模糊规则，并结合保利和止损规则，构建用于股票价格趋势预测和交易信号的模糊推理系统。实验结果表明，BCBOF算法优于其他双聚类技术，且基于该算法的交易策略能为投资者带来更高的回报。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21289v1",
      "published_date": "2025-04-30 03:49:08 UTC",
      "updated_date": "2025-04-30 03:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:28:03.595033"
    },
    {
      "arxiv_id": "2504.21277v1",
      "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models",
      "title_zh": "强化MLLM：多模态大型语言模型中基于强化学习的推理研究综述\n",
      "authors": [
        "Guanghao Zhou",
        "Panjia Qiu",
        "Cen Chen",
        "Jie Wang",
        "Zheming Yang",
        "Jian Xu",
        "Minghui Qiu"
      ],
      "abstract": "The integration of reinforcement learning (RL) into the reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\na transformative research direction. While MLLMs significantly extend Large\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\nvideo, enabling robust reasoning across multimodal inputs remains a major\nchallenge. This survey systematically reviews recent advances in RL-based\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\ninnovations, and practical applications. We highlight two main RL\nparadigms--value-free and value-based methods--and analyze how RL enhances\nreasoning abilities by optimizing reasoning trajectories and aligning\nmultimodal information. Furthermore, we provide an extensive overview of\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\nfuture research directions to address current bottlenecks such as sparse\nrewards, inefficient cross-modal reasoning, and real-world deployment\nconstraints. Our goal is to offer a comprehensive and structured guide to\nresearchers interested in advancing RL-based reasoning in the multimodal era.",
      "tldr_zh": "这篇综述系统地回顾了基于强化学习(RL)的多模态大型语言模型(MLLMs)推理能力的最新进展。文章重点介绍了RL如何通过优化推理轨迹和对齐多模态信息来增强MLLMs的推理能力，并涵盖了关键的算法设计、奖励机制创新和实际应用。文章分析了两种主要的RL范式——无价值方法和基于价值的方法。此外，综述还全面概述了基准数据集、评估协议和现有局限性，并提出了未来的研究方向，以解决当前瓶颈，例如稀疏奖励、低效的跨模态推理和现实世界的部署约束。该综述旨在为有兴趣推进多模态时代基于RL推理的研究人员提供全面而结构化的指导。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21277v1",
      "published_date": "2025-04-30 03:14:28 UTC",
      "updated_date": "2025-04-30 03:14:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:28:15.701948"
    },
    {
      "arxiv_id": "2504.21276v1",
      "title": "Assessing LLM code generation quality through path planning tasks",
      "title_zh": "通过路径规划任务评估LLM代码生成质量\n",
      "authors": [
        "Wanyi Chen",
        "Meng-Wen Su",
        "Mary L. Cummings"
      ],
      "abstract": "As LLM-generated code grows in popularity, more evaluation is needed to\nassess the risks of using such tools, especially for safety-critical\napplications such as path planning. Existing coding benchmarks are insufficient\nas they do not reflect the context and complexity of safety-critical\napplications. To this end, we assessed six LLMs' abilities to generate the code\nfor three different path-planning algorithms and tested them on three maps of\nvarious difficulties. Our results suggest that LLM-generated code presents\nserious hazards for path planning applications and should not be applied in\nsafety-critical contexts without rigorous testing.",
      "tldr_zh": "该研究评估了大型语言模型(LLMs)在路径规划任务中生成代码的质量，旨在揭示其在安全关键应用中的潜在风险。通过测试六种LLMs在三个不同难度地图上生成三种路径规划算法代码的能力，研究发现LLM生成的代码在路径规划应用中存在严重的安全隐患。因此，在未经严格测试的情况下，不应将LLM生成的代码应用于安全关键领域。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21276v1",
      "published_date": "2025-04-30 03:11:54 UTC",
      "updated_date": "2025-04-30 03:11:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:28:27.261143"
    },
    {
      "arxiv_id": "2504.21261v1",
      "title": "Multi-Domain Causal Discovery in Bijective Causal Models",
      "title_zh": "双射因果模型中的多域因果发现\n",
      "authors": [
        "Kasra Jalaldoust",
        "Saber Salehkaleybar",
        "Negar Kiyavash"
      ],
      "abstract": "We consider the problem of causal discovery (a.k.a., causal structure\nlearning) in a multi-domain setting. We assume that the causal functions are\ninvariant across the domains, while the distribution of the exogenous noise may\nvary. Under causal sufficiency (i.e., no confounders exist), we show that the\ncausal diagram can be discovered under less restrictive functional assumptions\ncompared to previous work. What enables causal discovery in this setting is\nbijective generation mechanisms (BGM), which ensures that the functional\nrelation between the exogenous noise $E$ and the endogenous variable $Y$ is\nbijective and differentiable in both directions at every level of the cause\nvariable $X = x$. BGM generalizes a variety of models including additive noise\nmodel, LiNGAM, post-nonlinear model, and location-scale noise model. Further,\nwe derive a statistical test to find the parents set of the target variable.\nExperiments on various synthetic and real-world datasets validate our\ntheoretical findings.",
      "tldr_zh": "本文研究了多域环境下的因果发现问题，假设因果函数在不同域之间是不变的，而外生噪声的分布可能不同。在因果充分性（即不存在混淆因素）的假设下，证明了在比以往工作更宽松的函数假设下可以发现因果图。关键在于双射生成机制 (Bijective Generation Mechanisms, BGM)，它确保了外生噪声 $E$ 和内生变量 $Y$ 之间的函数关系在原因变量 $X = x$ 的每个层级上都是双射且可微的。BGM 推广了包括加性噪声模型、LiNGAM、后非线性模型和位置-尺度噪声模型在内的多种模型。此外，还推导出一个统计检验来找到目标变量的父节点集合。在各种合成和真实世界数据集上的实验验证了理论发现。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of Causal Learning and Reasoning (CLeaR) 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.21261v1",
      "published_date": "2025-04-30 02:30:10 UTC",
      "updated_date": "2025-04-30 02:30:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:28:39.889331"
    },
    {
      "arxiv_id": "2504.21239v1",
      "title": "Memorization and Knowledge Injection in Gated LLMs",
      "title_zh": "门控 LLM 中的记忆和知识注入\n",
      "authors": [
        "Xu Pan",
        "Ely Hahami",
        "Zechen Zhang",
        "Haim Sompolinsky"
      ],
      "abstract": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain.",
      "tldr_zh": "该论文提出了Memory Embedded in Gated LLMs (MEGa)，一种持续学习框架，旨在解决大型语言模型(LLMs)难以顺序添加新记忆和整合新知识的问题。MEGa通过门控低秩权重将事件记忆直接注入LLMs的权重中，每个记忆存储在一组专门的门控权重中。在推理过程中，门控机制通过匹配查询嵌入到存储的记忆嵌入来激活相关的记忆权重，从而使模型能够回忆整个记忆并回答相关问题。在虚构人物和维基百科事件两个数据集上的实验表明，MEGa在缓解灾难性遗忘方面优于基线方法。该模型的设计灵感来源于人脑的互补记忆系统。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21239v1",
      "published_date": "2025-04-30 00:28:32 UTC",
      "updated_date": "2025-04-30 00:28:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:28:51.554832"
    },
    {
      "arxiv_id": "2504.21235v1",
      "title": "Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs",
      "title_zh": "用于量子计算机程序的高效量子安全同态加密\n",
      "authors": [
        "Ben Goertzel"
      ],
      "abstract": "We present a lattice-based scheme for homomorphic evaluation of quantum\nprograms and proofs that remains secure against quantum adversaries. Classical\nhomomorphic encryption is lifted to the quantum setting by replacing\ncomposite-order groups with Module Learning-With-Errors (MLWE) lattices and by\ngeneralizing polynomial functors to bounded natural super functors (BNSFs). A\nsecret depolarizing BNSF mask hides amplitudes, while each quantum state is\nstored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game\nthat allows coherent access to the encryption oracle and give a four-hybrid\nreduction to decisional MLWE.\n  The design also covers practical issues usually left open. A typed QC-bridge\nkeeps classical bits produced by measurements encrypted yet still usable as\ncontrols, with weak-measurement semantics for expectation-value workloads.\nEncrypted Pauli twirls add circuit privacy. If a fixed knowledge base is\nneeded, its axioms are shipped as MLWE \"capsules\"; the evaluator can use them\nbut cannot read them. A rho-calculus driver schedules encrypted tasks across\nseveral QPUs and records an auditable trace on an RChain-style ledger.\n  Performance analysis shows that the extra lattice arithmetic fits inside\ntoday's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof\nruns in about 10 ms, the public key (seed only) is 32 bytes, and even a\nCCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes\nhomomorphic teleportation plus knowledge-base-relative amplitude checks appears\nfeasible with current hardware. These results indicate that fully homomorphic,\nknowledge-base-aware quantum reasoning is compatible with near-term quantum\nclouds and standard post-quantum security assumptions.",
      "tldr_zh": "该论文提出了一种基于格的同态加密方案，用于对量子程序和证明进行同态评估，并且能够抵抗量子敌手的攻击。该方案通过用Module Learning-With-Errors (MLWE)格代替复合阶群，并将多项式函子推广到有界自然超函子 (BNSF)，从而将经典同态加密提升到量子设置。使用秘密的退极化BNSF掩码来隐藏振幅，同时每个量子态都存储为MLWE密文对。论文还形式化了qIND-CPA安全性，允许对加密oracle进行相干访问，并给出了一个到判定MLWE的四混合归约。该设计还涵盖了通常被忽略的实际问题，例如使用类型化的QC-bridge来保持测量产生的经典比特的加密状态，同时仍然可以用作控制。性能分析表明，额外的格算术运算适合当前QPU的空闲窗口。\n",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21235v1",
      "published_date": "2025-04-30 00:08:43 UTC",
      "updated_date": "2025-04-30 00:08:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-02T02:29:04.310038"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-02T02:30:11.136848"
}
[
  {
    "arxiv_id": "2510.05465v3",
    "title": "VAL-Bench: Belief Consistency as a measure for Value Alignment in Language Models",
    "authors": [
      "Aman Gupta",
      "Denny O'Shea",
      "Fazl Barez"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used for tasks where outputs shape human decisions, so it is critical to verify that their responses consistently reflect desired human values. Humans, as individuals or groups, don't agree on a universal set of values, which makes evaluating value alignment difficult. Existing benchmarks often use hypothetical or commonsensical situations, which don't capture the complexity and ambiguity of real-life debates. We introduce the Value ALignment Benchmark (VAL-Bench), which measures the consistency in language model belief expressions in response to real-life value-laden prompts. VAL-Bench consists of 115K pairs of prompts designed to elicit opposing stances on a controversial issue, extracted from Wikipedia. We use an LLM-as-a-judge, validated against human annotations, to evaluate if the pair of responses consistently expresses either a neutral or a specific stance on the issue. Applied across leading open- and closed-source models, the benchmark shows considerable variation in consistency rates (ranging from ~10% to ~80%), with Claude models the only ones to achieve high levels of consistency. Lack of consistency in this manner risks epistemic harm by making user beliefs dependent on how questions are framed rather than on underlying evidence, and undermines LLM reliability in trust-critical applications. Therefore, we stress the importance of research towards training belief consistency in modern LLMs. By providing a scalable, reproducible benchmark, VAL-Bench enables systematic measurement of necessary conditions for value alignment.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05465v3",
    "published_date": "2025-10-06 23:55:48 UTC",
    "updated_date": "2026-01-14 19:30:35 UTC"
  },
  {
    "arxiv_id": "2510.05457v1",
    "title": "Do Code Models Suffer from the Dunning-Kruger Effect?",
    "authors": [
      "Mukul Singh",
      "Somya Chatterjee",
      "Arjun Radhakrishna",
      "Sumit Gulwani"
    ],
    "abstract": "As artificial intelligence systems increasingly collaborate with humans in creative and technical domains, questions arise about the cognitive boundaries and biases that shape our shared agency. This paper investigates the Dunning-Kruger Effect (DKE), the tendency for those with limited competence to overestimate their abilities in state-of-the-art LLMs in coding tasks. By analyzing model confidence and performance across a diverse set of programming languages, we reveal that AI models mirror human patterns of overconfidence, especially in unfamiliar or low-resource domains. Our experiments demonstrate that less competent models and those operating in rare programming languages exhibit stronger DKE-like bias, suggesting that the strength of the bias is proportionate to the competence of the models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05457v1",
    "published_date": "2025-10-06 23:41:24 UTC",
    "updated_date": "2025-10-06 23:41:24 UTC"
  },
  {
    "arxiv_id": "2510.05453v1",
    "title": "QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification",
    "authors": [
      "Arpit Kapoor",
      "Rohitash Chandra"
    ],
    "abstract": "Conceptual rainfall-runoff models aid hydrologists and climate scientists in modelling streamflow to inform water management practices. Recent advances in deep learning have unravelled the potential for combining hydrological models with deep learning models for better interpretability and improved predictive performance. In our previous work, we introduced DeepGR4J, which enhanced the GR4J conceptual rainfall-runoff model using a deep learning model to serve as a surrogate for the routing component. DeepGR4J had an improved rainfall-runoff prediction accuracy, particularly in arid catchments. Quantile regression models have been extensively used for quantifying uncertainty while aiding extreme value forecasting. In this paper, we extend DeepGR4J using a quantile regression-based ensemble learning framework to quantify uncertainty in streamflow prediction. We also leverage the uncertainty bounds to identify extreme flow events potentially leading to flooding. We further extend the model to multi-step streamflow predictions for uncertainty bounds. We design experiments for a detailed evaluation of the proposed framework using the CAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J framework improves the predictive accuracy and uncertainty interval quality (interval score) compared to baseline deep learning models. Furthermore, we carry out flood risk evaluation using Quantile DeepGR4J, and the results demonstrate its suitability as an early warning system.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05453v1",
    "published_date": "2025-10-06 23:36:40 UTC",
    "updated_date": "2025-10-06 23:36:40 UTC"
  },
  {
    "arxiv_id": "2510.05451v1",
    "title": "NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification",
    "authors": [
      "Fadi Al Machot",
      "Fidaa Al Machot"
    ],
    "abstract": "Deep transformer models excel at multi-label text classification but often violate domain logic that experts consider essential, an issue of particular concern in safety-critical applications. We propose a hybrid neuro-symbolic framework that integrates Answer Set Programming (ASP) with transformer-based learning on the Aviation Safety Reporting System (ASRS) corpus. Domain knowledge is formalized as weighted ASP rules and validated using the Clingo solver. These rules are incorporated in two complementary ways: (i) as rule-based data augmentation, generating logically consistent synthetic samples that improve label diversity and coverage; and (ii) as a fuzzy-logic regularizer, enforcing rule satisfaction in a differentiable form during fine-tuning. This design preserves the interpretability of symbolic reasoning while leveraging the scalability of deep neural architectures. We further tune per-class thresholds and report both standard classification metrics and logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE) baseline, our approach improves micro- and macro-F1 scores and achieves up to an 86% reduction in rule violations on the ASRS test set. To the best of our knowledge, this constitutes the first large-scale neuro-symbolic application to ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and differentiable transformer training for trustworthy, safety-critical NLP.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05451v1",
    "published_date": "2025-10-06 23:33:09 UTC",
    "updated_date": "2025-10-06 23:33:09 UTC"
  },
  {
    "arxiv_id": "2510.09653v2",
    "title": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition",
    "authors": [
      "Ranjan Sapkota",
      "Manoj Karkee"
    ],
    "abstract": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only Look Once) family of object detectors, focusing the architectural evolution, benchmarking, deployment perspectives, and future challenges. The review begins with the most recent release, YOLO26 (or YOLOv26), which introduces key innovations including Distribution Focal Loss (DFL) removal, native NMS-free inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label Assignment (STAL), and the MuSGD optimizer for stable training. The progression is then traced through YOLO11, with its hybrid task assignment and efficiency-focused modules; YOLOv8, which advanced with a decoupled detection head and anchor-free predictions; and YOLOv5, which established the modular PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8, YOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including precision, recall, F1 score, mean Average Precision, and inference speed are analyzed to highlight trade-offs between accuracy and efficiency. Deployment and application perspectives are further discussed, covering export formats, quantization strategies, and real-world use in robotics, agriculture, surveillance, and manufacturing. Finally, the paper identifies challenges and future directions, including dense-scene limitations, hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware training approaches. (Object Detection, YOLOv26, YOLO)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09653v2",
    "published_date": "2025-10-06 23:28:44 UTC",
    "updated_date": "2025-10-15 16:57:20 UTC"
  },
  {
    "arxiv_id": "2510.05442v1",
    "title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety",
    "authors": [
      "Zizhao Wang",
      "Dingcheng Li",
      "Vaishakh Keshava",
      "Phillip Wallis",
      "Ananth Balashankar",
      "Peter Stone",
      "Lukas Rutishauser"
    ],
    "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05442v1",
    "published_date": "2025-10-06 23:09:18 UTC",
    "updated_date": "2025-10-06 23:09:18 UTC"
  },
  {
    "arxiv_id": "2510.05441v1",
    "title": "UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification",
    "authors": [
      "Yiannis Charalambous",
      "Claudionor N. Coelho",
      "Luis Lamb",
      "Lucas C. Cordeiro"
    ],
    "abstract": "This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent system designed to generate unit tests for legacy code, enhancing test coverage and critical value testing. UnitTenX leverages a combination of AI agents, formal methods, and Large Language Models (LLMs) to automate test generation, addressing the challenges posed by complex and legacy codebases. Despite the limitations of LLMs in bug detection, UnitTenX offers a robust framework for improving software reliability and maintainability. Our results demonstrate the effectiveness of this approach in generating high-quality tests and identifying potential issues. Additionally, our approach enhances the readability and documentation of legacy code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05441v1",
    "published_date": "2025-10-06 23:08:23 UTC",
    "updated_date": "2025-10-06 23:08:23 UTC"
  },
  {
    "arxiv_id": "2510.05433v1",
    "title": "Physics-Informed Machine Learning in Biomedical Science and Engineering",
    "authors": [
      "Nazanin Ahmadi",
      "Qianying Cao",
      "Jay D. Humphrey",
      "George Em Karniadakis"
    ],
    "abstract": "Physics-informed machine learning (PIML) is emerging as a potentially transformative paradigm for modeling complex biomedical systems by integrating parameterized physical laws with data-driven methods. Here, we review three main classes of PIML frameworks: physics-informed neural networks (PINNs), neural ordinary differential equations (NODEs), and neural operators (NOs), highlighting their growing role in biomedical science and engineering. We begin with PINNs, which embed governing equations into deep learning models and have been successfully applied to biosolid and biofluid mechanics, mechanobiology, and medical imaging among other areas. We then review NODEs, which offer continuous-time modeling, especially suited to dynamic physiological systems, pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful tools for learning mappings between function spaces, enabling efficient simulations across multiscale and spatially heterogeneous biological domains. Throughout, we emphasize applications where physical interpretability, data scarcity, or system complexity make conventional black-box learning insufficient. We conclude by identifying open challenges and future directions for advancing PIML in biomedical science and engineering, including issues of uncertainty quantification, generalization, and integration of PIML and large language models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in the Annual Review of Biomedical Engineering on October 2, 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05433v1",
    "published_date": "2025-10-06 22:52:39 UTC",
    "updated_date": "2025-10-06 22:52:39 UTC"
  },
  {
    "arxiv_id": "2510.05432v1",
    "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems",
    "authors": [
      "Shambhavi Mishra",
      "Gaurav Sahu",
      "Marco Pedersoli",
      "Laurent Charlin",
      "Jose Dolz",
      "Christopher Pal"
    ],
    "abstract": "Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05432v1",
    "published_date": "2025-10-06 22:50:41 UTC",
    "updated_date": "2025-10-06 22:50:41 UTC"
  },
  {
    "arxiv_id": "2510.05417v1",
    "title": "Exploring Student Choice and the Use of Multimodal Generative AI in Programming Learning",
    "authors": [
      "Xinying Hou",
      "Ruiwei Xiao",
      "Runlong Ye",
      "Michael Liut",
      "John Stamper"
    ],
    "abstract": "The broad adoption of Generative AI (GenAI) is impacting Computer Science education, and recent studies found its benefits and potential concerns when students use it for programming learning. However, most existing explorations focus on GenAI tools that primarily support text-to-text interaction. With recent developments, GenAI applications have begun supporting multiple modes of communication, known as multimodality. In this work, we explored how undergraduate programming novices choose and work with multimodal GenAI tools, and their criteria for choices. We selected a commercially available multimodal GenAI platform for interaction, as it supports multiple input and output modalities, including text, audio, image upload, and real-time screen-sharing. Through 16 think-aloud sessions that combined participant observation with follow-up semi-structured interviews, we investigated student modality choices for GenAI tools when completing programming problems and the underlying criteria for modality selections. With multimodal communication emerging as the future of AI in education, this work aims to spark continued exploration on understanding student interaction with multimodal GenAI in the context of CS education.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages, accepted to SIGCSE2026",
    "pdf_url": "https://arxiv.org/pdf/2510.05417v1",
    "published_date": "2025-10-06 22:16:44 UTC",
    "updated_date": "2025-10-06 22:16:44 UTC"
  },
  {
    "arxiv_id": "2510.05408v1",
    "title": "See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models",
    "authors": [
      "Kebin Contreras",
      "Luis Toscano-Palomino",
      "Mauro Dalla Mura",
      "Jorge Bacca"
    ],
    "abstract": "Recovering the past from present observations is an intriguing challenge with potential applications in forensics and scene analysis. Thermal imaging, operating in the infrared range, provides access to otherwise invisible information. Since humans are typically warmer (37 C -98.6 F) than their surroundings, interactions such as sitting, touching, or leaning leave residual heat traces. These fading imprints serve as passive temporal codes, allowing for the inference of recent events that exceed the capabilities of RGB cameras. This work proposes a time-reversed reconstruction framework that uses paired RGB and thermal images to recover scene states from a few seconds earlier. The proposed approach couples Visual-Language Models (VLMs) with a constrained diffusion process, where one VLM generates scene descriptions and another guides image reconstruction, ensuring semantic and structural consistency. The method is evaluated in three controlled scenarios, demonstrating the feasibility of reconstructing plausible past frames up to 120 seconds earlier, providing a first step toward time-reversed imaging from thermal traces.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05408v1",
    "published_date": "2025-10-06 21:57:26 UTC",
    "updated_date": "2025-10-06 21:57:26 UTC"
  },
  {
    "arxiv_id": "2510.06283v1",
    "title": "SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation",
    "authors": [
      "Sashank Makanaboyina"
    ],
    "abstract": "Incremental brain tumor segmentation is critical for models that must adapt to evolving clinical datasets without retraining on all prior data. However, catastrophic forgetting, where models lose previously acquired knowledge, remains a major obstacle. Recent incremental learning frameworks with knowledge distillation partially mitigate forgetting but rely heavily on generative replay or auxiliary storage. Meanwhile, diffusion models have proven effective for refining tumor segmentations, but have not been explored in incremental learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the first framework that unifies diffusion-based refinement with incremental learning. SER-Diff leverages a frozen teacher diffusion model to generate synthetic error maps from past tasks, which are replayed during training on new tasks. A dual-loss formulation combining Dice loss for new data and knowledge distillation loss for replayed errors ensures both adaptability and retention. Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff consistently outperforms prior methods. It achieves the highest Dice scores of 95.8\\%, 94.9\\%, and 94.6\\%, along with the lowest HD95 values of 4.4 mm, 4.7 mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only mitigates catastrophic forgetting but also delivers more accurate and anatomically coherent segmentations across evolving datasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06283v1",
    "published_date": "2025-10-06 21:53:45 UTC",
    "updated_date": "2025-10-06 21:53:45 UTC"
  },
  {
    "arxiv_id": "2510.05402v1",
    "title": "Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation",
    "authors": [
      "Ahmad Alsheikh",
      "Andreas Fischer"
    ],
    "abstract": "Predicting the final hardness of steel after heat treatment is a challenging regression task due to the many-to-one nature of the process -- different combinations of input parameters (such as temperature, duration, and chemical composition) can result in the same hardness value. This ambiguity makes the inverse problem, estimating input parameters from a desired hardness, particularly difficult. In this work, we propose a novel solution using a Teacher-Student learning framework. First, a forward model (Teacher) is trained to predict final hardness from 13 metallurgical input features. Then, a backward model (Student) is trained to infer plausible input configurations from a target hardness value. The Student is optimized by leveraging feedback from the Teacher in an iterative, supervised loop. We evaluate our method on a publicly available tempered steel dataset and compare it against baseline regression and reinforcement learning models. Results show that our Teacher-Student framework not only achieves higher inverse prediction accuracy but also requires significantly less computational time, demonstrating its effectiveness and efficiency for inverse process modeling in materials science.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Workshop paper, AIP2025: Second Workshop on AI in Production (2025). Licensed under CC BY 4.0",
    "pdf_url": "https://arxiv.org/pdf/2510.05402v1",
    "published_date": "2025-10-06 21:50:05 UTC",
    "updated_date": "2025-10-06 21:50:05 UTC"
  },
  {
    "arxiv_id": "2510.05399v1",
    "title": "Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data",
    "authors": [
      "Kangwoo Yi",
      "Bo Shen",
      "Qin Li",
      "Haimin Wang",
      "Yong-Jae Moon",
      "Jaewon Lee",
      "Hwanhee Lee"
    ],
    "abstract": "Solar Proton Events (SPEs) cause significant radiation hazards to satellites, astronauts, and technological systems. Accurate forecasting of their proton flux time profiles is crucial for early warnings and mitigation. This paper explores deep learning sequence-to-sequence (seq2seq) models based on Long Short-Term Memory networks to predict 24-hour proton flux profiles following SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by NOAA GOES, each associated with a >=M-class western-hemisphere solar flare and undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we evaluate seq2seq model configurations (varying hidden units and embedding dimensions) under multiple forecasting scenarios: (i) proton-only input vs. combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data, and (iii) autoregressive vs. one-shot forecasting. Our major results are as follows: First, one-shot forecasting consistently yields lower error than autoregressive prediction, avoiding the error accumulation seen in iterative approaches. Second, on the original data, proton-only models outperform proton+X-ray models. However, with trend-smoothed data, this gap narrows or reverses in proton+X-ray models. Third, trend-smoothing significantly enhances the performance of proton+X-ray models by mitigating fluctuations in the X-ray channel. Fourth, while models trained on trendsmoothed data perform best on average, the best-performing model was trained on original data, suggesting that architectural choices can sometimes outweigh the benefits of data preprocessing.",
    "categories": [
      "cs.LG",
      "astro-ph.SR",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages; accepted as a workshop paper at ICDM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05399v1",
    "published_date": "2025-10-06 21:45:37 UTC",
    "updated_date": "2025-10-06 21:45:37 UTC"
  },
  {
    "arxiv_id": "2510.05394v1",
    "title": "Fusion-Based Neural Generalization for Predicting Temperature Fields in Industrial PET Preform Heating",
    "authors": [
      "Ahmad Alsheikh",
      "Andreas Fischer"
    ],
    "abstract": "Accurate and efficient temperature prediction is critical for optimizing the preheating process of PET preforms in industrial microwave systems prior to blow molding. We propose a novel deep learning framework for generalized temperature prediction. Unlike traditional models that require extensive retraining for each material or design variation, our method introduces a data-efficient neural architecture that leverages transfer learning and model fusion to generalize across unseen scenarios. By pretraining specialized neural regressor on distinct conditions such as recycled PET heat capacities or varying preform geometries and integrating their representations into a unified global model, we create a system capable of learning shared thermal dynamics across heterogeneous inputs. The architecture incorporates skip connections to enhance stability and prediction accuracy. Our approach reduces the need for large simulation datasets while achieving superior performance compared to models trained from scratch. Experimental validation on two case studies material variability and geometric diversity demonstrates significant improvements in generalization, establishing a scalable ML-based solution for intelligent thermal control in manufacturing environments. Moreover, the approach highlights how data-efficient generalization strategies can extend to other industrial applications involving complex physical modeling with limited data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop paper, AIP2025: Second Workshop on AI in Production (2025). Licensed under CC BY 4.0",
    "pdf_url": "https://arxiv.org/pdf/2510.05394v1",
    "published_date": "2025-10-06 21:38:37 UTC",
    "updated_date": "2025-10-06 21:38:37 UTC"
  },
  {
    "arxiv_id": "2510.05381v1",
    "title": "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval",
    "authors": [
      "Yufeng Du",
      "Minyang Tian",
      "Srikanth Ronanki",
      "Subendhu Rongali",
      "Sravan Bodapati",
      "Aram Galstyan",
      "Azton Wells",
      "Roy Schwartz",
      "Eliu A Huerta",
      "Hao Peng"
    ],
    "abstract": "Large language models (LLMs) often fail to scale their performance on long-context tasks performance in line with the context lengths they support. This gap is commonly attributed to retrieval failures -- the models' inability to identify relevant information in the long inputs. Accordingly, recent efforts often focus on evaluating and improving LLMs' retrieval performance: if retrieval is perfect, a model should, in principle, perform just as well on a long input as it does on a short one -- or should it? This paper presents findings that the answer to this question may be negative. Our systematic experiments across 5 open- and closed-source LLMs on math, question answering, and coding tasks reveal that, even when models can perfectly retrieve all relevant information, their performance still degrades substantially (13.9%--85%) as input length increases but remains well within the models' claimed lengths. This failure occurs even when the irrelevant tokens are replaced with minimally distracting whitespace, and, more surprisingly, when they are all masked and the models are forced to attend only to the relevant tokens. A similar performance drop is observed when all relevant evidence is placed immediately before the question. Our findings reveal a previously-unrealized limitation: the sheer length of the input alone can hurt LLM performance, independent of retrieval quality and without any distraction. They motivate our simple, model-agnostic mitigation strategy that transforms a long-context task into a short-context one by prompting the model to recite the retrieved evidence before attempting to solve the problem. On RULER, we observe a consistent improvement of GPT-4o up to 4% on an already strong baseline.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages (9 pages of main content), 5 figures, accepted at the Findings of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05381v1",
    "published_date": "2025-10-06 21:17:13 UTC",
    "updated_date": "2025-10-06 21:17:13 UTC"
  },
  {
    "arxiv_id": "2510.05379v2",
    "title": "AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling",
    "authors": [
      "Xiaogeng Liu",
      "Chaowei Xiao"
    ],
    "abstract": "Recent advancements in jailbreaking large language models (LLMs), such as AutoDAN-Turbo, have demonstrated the power of automated strategy discovery. AutoDAN-Turbo employs a lifelong learning agent to build a rich library of attack strategies from scratch. While highly effective, its test-time generation process involves sampling a strategy and generating a single corresponding attack prompt, which may not fully exploit the potential of the learned strategy library. In this paper, we propose to further improve the attack performance of AutoDAN-Turbo through test-time scaling. We introduce two distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method generates N candidate attack prompts from a sampled strategy and selects the most effective one based on a scorer model. The Beam Search method conducts a more exhaustive search by exploring combinations of strategies from the library to discover more potent and synergistic attack vectors. According to the experiments, the proposed methods significantly boost performance, with Beam Search increasing the attack success rate by up to 15.6 percentage points on Llama-3.1-70B-Instruct and achieving a nearly 60% relative improvement against the highly robust GPT-o4-mini compared to the vanilla method.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Technical report. Code is available at https://github.com/SaFoLab-WISC/AutoDAN-Reasoning",
    "pdf_url": "https://arxiv.org/pdf/2510.05379v2",
    "published_date": "2025-10-06 21:16:09 UTC",
    "updated_date": "2025-10-08 04:37:35 UTC"
  },
  {
    "arxiv_id": "2510.05378v1",
    "title": "What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions",
    "authors": [
      "Reza Habibi",
      "Seung Wan Ha",
      "Zhiyu Lin",
      "Atieh Kashani",
      "Ala Shafia",
      "Lakshana Lakshmanarajan",
      "Chia-Fang Chung",
      "Magy Seif El-Nasr"
    ],
    "abstract": "Meaningful human-AI collaboration requires more than processing language; it demands a deeper understanding of symbols and their socially constructed meanings. While humans naturally interpret symbols through social interaction, AI systems often miss the dynamic interpretations that emerge in conversation. Drawing on Symbolic Interactionism theory, we conducted two studies to investigate how humans and AI co-construct symbols and their meanings. Findings provide empirical insights into how humans and conversational AI agents collaboratively shape meanings during interaction. We show how participants shift their initial definitions of meaning in response to the symbols and interpretations suggested by the conversational AI agents, especially when social context is introduced. We also observe how participants project their personal and social values into these interactions, refining meanings over time. These findings reveal that shared understanding does not emerge from mere agreement but from the bi-directional exchange and reinterpretation of symbols, suggesting new paradigms for human-AI interaction design.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "CHI 2026 Papers",
    "pdf_url": "https://arxiv.org/pdf/2510.05378v1",
    "published_date": "2025-10-06 21:13:22 UTC",
    "updated_date": "2025-10-06 21:13:22 UTC"
  },
  {
    "arxiv_id": "2510.06281v1",
    "title": "Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning",
    "authors": [
      "Chenyang Li",
      "Qin Li",
      "Haimin Wang",
      "Bo Shen"
    ],
    "abstract": "High-resolution (HR) solar imaging is crucial for capturing fine-scale dynamic features such as filaments and fibrils. However, the spatial resolution of the full-disk H$α$ images is limited and insufficient to resolve these small-scale structures. To address this, we propose a GAN-based superresolution approach to enhance low-resolution (LR) full-disk H$α$ images from the Global Oscillation Network Group (GONG) to a quality comparable with HR observations from the Big Bear Solar Observatory/Goode Solar Telescope (BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a relativistic discriminator. We carefully aligned GONG-GST pairs. The model effectively recovers fine details within sunspot penumbrae and resolves fine details in filaments and fibrils, achieving an average mean squared error (MSE) of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC) of 0.7794. Slight misalignments between image pairs limit quantitative performance, which we plan to address in future work alongside dataset expansion to further improve reconstruction quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages; accepted as a workshop paper in ICDM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.06281v1",
    "published_date": "2025-10-06 20:44:16 UTC",
    "updated_date": "2025-10-06 20:44:16 UTC"
  },
  {
    "arxiv_id": "2510.05363v1",
    "title": "MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts",
    "authors": [
      "Abhinav Jain",
      "Xinyu Yao",
      "Thomas Reps",
      "Christopher Jermaine"
    ],
    "abstract": "Adapting Foundation Models to new domains with limited training data is challenging and computationally expensive. While prior work has demonstrated the effectiveness of using domain-specific exemplars as in-context demonstrations, we investigate whether representing exemplars purely as text is the most efficient, effective, and stable approach. We explore an alternative: representing exemplars as soft prompts with an exemplar order invariant model architecture. To this end, we introduce Multi-Head Attention Retrieval-Augmented Generation (MHA-RAG), a framework with the number of attention heads serving as a simple hyperparameter to control soft prompt-generation across different tasks. Across multiple question-answering benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over standard RAG, while cutting inference costs by a factor of 10X GFLOPs-delivering both higher accuracy and greater efficiency, invariant to exemplar order.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05363v1",
    "published_date": "2025-10-06 20:41:43 UTC",
    "updated_date": "2025-10-06 20:41:43 UTC"
  },
  {
    "arxiv_id": "2510.05361v1",
    "title": "MT-DAO: Multi-Timescale Distributed Adaptive Optimizers with Local Updates",
    "authors": [
      "Alex Iacob",
      "Andrej Jovanovic",
      "Mher Safaryan",
      "Meghdad Kurmanji",
      "Lorenzo Sani",
      "Samuel Horváth",
      "William F. Shen",
      "Xinchi Qiu",
      "Nicholas D. Lane"
    ],
    "abstract": "Training large models with distributed data parallelism (DDP) requires frequent communication of gradients across workers, which can saturate bandwidth. Infrequent communication strategies (e.g., Local SGD) reduce this overhead but, when applied to adaptive optimizers, often suffer a performance gap relative to fully synchronous DDP. We trace this gap to a time-scale mismatch: the optimizer's fast-moving momentum, tuned for frequent updates, decays too quickly to smooth gradients over long intervals, leading to noise-dominated optimization. To address this, we propose MT-DAO, a family of optimizers that employs multiple slow- and fast-moving first momenta or the gradient to track update dynamics across different time scales, for which we provide the first convergence guarantees. Empirically, for language-model pre-training, this eliminates the performance gap with DDP, outperforming infrequent-communication baselines in perplexity and reducing iso-token wall-clock time by 6-27% on Ethernet interconnects. At the 720M scale, MT-DAO reaches a target perplexity in 24% fewer steps and 35% less time than the single-momentum DDP baseline. MT-DAO enables effective cross-datacenter training and training over wide geographic areas.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to the ICLR 2026 Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.05361v1",
    "published_date": "2025-10-06 20:37:57 UTC",
    "updated_date": "2025-10-06 20:37:57 UTC"
  },
  {
    "arxiv_id": "2510.05351v1",
    "title": "Physics-informed Attention-enhanced Fourier Neural Operator for Solar Magnetic Field Extrapolations",
    "authors": [
      "Jinghao Cao",
      "Qin Li",
      "Mengnan Du",
      "Haimin Wang",
      "Bo Shen"
    ],
    "abstract": "We propose Physics-informed Attention-enhanced Fourier Neural Operator (PIANO) to solve the Nonlinear Force-Free Field (NLFFF) problem in solar physics. Unlike conventional approaches that rely on iterative numerical methods, our proposed PIANO directly learns the 3D magnetic field structure from 2D boundary conditions. Specifically, PIANO integrates Efficient Channel Attention (ECA) mechanisms with Dilated Convolutions (DC), which enhances the model's ability to capture multimodal input by prioritizing critical channels relevant to the magnetic field's variations. Furthermore, we apply physics-informed loss by enforcing the force-free and divergence-free conditions in the training process so that our prediction is consistent with underlying physics with high accuracy. Experimental results on the ISEE NLFFF dataset show that our PIANO not only outperforms state-of-the-art neural operators in terms of accuracy but also shows strong consistency with the physical characteristics of NLFFF data across magnetic fields reconstructed from various solar active regions. The GitHub of this project is available https://github.com/Autumnstar-cjh/PIANO",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages; accepted as workshop paper in ICDM 2025; https://github.com/Autumnstar-cjh/PIANO",
    "pdf_url": "https://arxiv.org/pdf/2510.05351v1",
    "published_date": "2025-10-06 20:24:22 UTC",
    "updated_date": "2025-10-06 20:24:22 UTC"
  },
  {
    "arxiv_id": "2510.06280v1",
    "title": "Surgeons Are Indian Males and Speech Therapists Are White Females: Auditing Biases in Vision-Language Models for Healthcare Professionals",
    "authors": [
      "Zohaib Hasan Siddiqui",
      "Dayam Nadeem",
      "Mohammad Masudur Rahman",
      "Mohammad Nadeem",
      "Shahab Saquib Sohail",
      "Beenish Moalla Chaudhry"
    ],
    "abstract": "Vision language models (VLMs), such as CLIP and OpenCLIP, can encode and reflect stereotypical associations between medical professions and demographic attributes learned from web-scale data. We present an evaluation protocol for healthcare settings that quantifies associated biases and assesses their operational risk. Our methodology (i) defines a taxonomy spanning clinicians and allied healthcare roles (e.g., surgeon, cardiologist, dentist, nurse, pharmacist, technician), (ii) curates a profession-aware prompt suite to probe model behavior, and (iii) benchmarks demographic skew against a balanced face corpus. Empirically, we observe consistent demographic biases across multiple roles and vision models. Our work highlights the importance of bias identification in critical domains such as healthcare as AI-enabled hiring and workforce analytics can have downstream implications for equity, compliance, and patient trust.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06280v1",
    "published_date": "2025-10-06 20:20:30 UTC",
    "updated_date": "2025-10-06 20:20:30 UTC"
  },
  {
    "arxiv_id": "2510.05342v1",
    "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization",
    "authors": [
      "Hyung Gyu Rho"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $β$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $β$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05342v1",
    "published_date": "2025-10-06 20:09:37 UTC",
    "updated_date": "2025-10-06 20:09:37 UTC"
  },
  {
    "arxiv_id": "2510.05338v1",
    "title": "Integrating Bayesian methods with neural network--based model predictive control: a review",
    "authors": [
      "Asli Karacelik"
    ],
    "abstract": "In this review, we assess the use of Bayesian methods in model predictive control (MPC), focusing on neural-network-based modeling, control design, and uncertainty quantification. We systematically analyze individual studies and how they are implemented in practice. While Bayesian approaches are increasingly adopted to capture and propagate uncertainty in MPC, reported gains in performance and robustness remain fragmented, with inconsistent baselines and limited reliability analyses. We therefore argue for standardized benchmarks, ablation studies, and transparent reporting to rigorously determine the effectiveness of Bayesian techniques for MPC.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, review article",
    "pdf_url": "https://arxiv.org/pdf/2510.05338v1",
    "published_date": "2025-10-06 20:04:03 UTC",
    "updated_date": "2025-10-06 20:04:03 UTC"
  },
  {
    "arxiv_id": "2510.05336v1",
    "title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives",
    "authors": [
      "Yongan Yu",
      "Xianda Du",
      "Qingchen Hu",
      "Jiahao Liang",
      "Jingwei Ni",
      "Dan Qiang",
      "Kaiyu Huang",
      "Grant McKenzie",
      "Renee Sieber",
      "Fengran Mo"
    ],
    "abstract": "Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. However, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system's ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives. Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at https://anonymous.4open.science/r/WeatherArchive-Bench/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05336v1",
    "published_date": "2025-10-06 19:58:42 UTC",
    "updated_date": "2025-10-06 19:58:42 UTC"
  },
  {
    "arxiv_id": "2510.05335v1",
    "title": "Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis",
    "authors": [
      "Oskar Wysocki",
      "Magdalena Wysocka",
      "Mauricio Jacobo",
      "Harriet Unsworth",
      "André Freitas"
    ],
    "abstract": "We present M-Reason, a demonstration system for transparent, agent-based reasoning and evidence integration in the biomedical domain, with a focus on cancer research. M-Reason leverages recent advances in large language models (LLMs) and modular agent orchestration to automate evidence retrieval, appraisal, and synthesis across diverse biomedical data sources. Each agent specializes in a specific evidence stream, enabling parallel processing and fine-grained analysis. The system emphasizes explainability, structured reporting, and user auditability, providing complete traceability from source evidence to final conclusions. We discuss critical tradeoffs between agent specialization, system complexity, and resource usage, as well as the integration of deterministic code for validation. An open, interactive user interface allows researchers to directly observe, explore and evaluate the multi-agent workflow. Our evaluation demonstrates substantial gains in efficiency and output consistency, highlighting M-Reason's potential as both a practical tool for evidence synthesis and a testbed for robust multi-agent LLM systems in scientific research, available at https://m-reason.digitalecmt.com.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05335v1",
    "published_date": "2025-10-06 19:57:29 UTC",
    "updated_date": "2025-10-06 19:57:29 UTC"
  },
  {
    "arxiv_id": "2510.05327v1",
    "title": "DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base",
    "authors": [
      "Zahin Ibnat",
      "Paul E. Calzada",
      "Rasin Mohammed Ihtemam",
      "Sujan Kumar Saha",
      "Jingbo Zhou",
      "Farimah Farahmandi",
      "Mark Tehranipoor"
    ],
    "abstract": "As large language models (LLMs) continue to be integrated into modern technology, there has been an increased push towards code generation applications, which also naturally extends to hardware design automation. LLM-based solutions for register transfer level (RTL) code generation for intellectual property (IP) designs have grown, especially with fine-tuned LLMs, prompt engineering, and agentic approaches becoming popular in literature. However, a gap has been exposed in these techniques, as they fail to integrate novel IPs into the model's knowledge base, subsequently resulting in poorly generated code. Additionally, as general-purpose LLMs continue to improve, fine-tuned methods on older models will not be able to compete to produce more accurate and efficient designs. Although some retrieval augmented generation (RAG) techniques exist to mitigate challenges presented in fine-tuning approaches, works tend to leverage low-quality codebases, incorporate computationally expensive fine-tuning in the frameworks, or do not use RAG directly in the RTL generation step. In this work, we introduce DeepV: a model-agnostic RAG framework to generate RTL designs by enhancing context through a large, high-quality dataset without any RTL-specific training. Our framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17% increase in performance on the VerilogEval benchmark. We host DeepV for use by the community in a Hugging Face (HF) Space: https://huggingface.co/spaces/FICS-LLM/DeepV.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "22 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05327v1",
    "published_date": "2025-10-06 19:47:27 UTC",
    "updated_date": "2025-10-06 19:47:27 UTC"
  },
  {
    "arxiv_id": "2510.05325v1",
    "title": "Dynamic Functional Connectivity Features for Brain State Classification: Insights from the Human Connectome Project",
    "authors": [
      "Valeriya Kirova",
      "Dzerassa Kadieva",
      "Daniil Vlasenko",
      "Isak B. Blank",
      "Fedor Ratnikov"
    ],
    "abstract": "We analyze functional magnetic resonance imaging (fMRI) data from the Human Connectome Project (HCP) to match brain activities during a range of cognitive tasks. Our findings demonstrate that even basic linear machine learning models can effectively classify brain states and achieve state-of-the-art accuracy, particularly for tasks related to motor functions and language processing. Feature importance ranking allows to identify distinct sets of brain regions whose activation patterns are uniquely associated with specific cognitive functions. These discriminative features provide strong support for the hypothesis of functional specialization across cortical and subcortical areas of the human brain.\n  Additionally, we investigate the temporal dynamics of the identified brain regions, demonstrating that the time-dependent structure of fMRI signals are essential for shaping functional connectivity between regions: uncorrelated areas are least important for classification. This temporal perspective provides deeper insights into the formation and modulation of brain neural networks involved in cognitive processing.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05325v1",
    "published_date": "2025-10-06 19:46:25 UTC",
    "updated_date": "2025-10-06 19:46:25 UTC"
  },
  {
    "arxiv_id": "2510.05318v2",
    "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions",
    "authors": [
      "Nan Huo",
      "Xiaohan Xu",
      "Jinyang Li",
      "Per Jacobsson",
      "Shipei Lin",
      "Bowen Qin",
      "Binyuan Hui",
      "Xiaolong Li",
      "Ge Qu",
      "Shuzheng Si",
      "Linheng Han",
      "Edward Alexander",
      "Xintong Zhu",
      "Rui Qin",
      "Ruihan Yu",
      "Yiyao Jin",
      "Feige Zhou",
      "Weihao Zhong",
      "Yun Chen",
      "Hongyu Liu",
      "Chenhao Ma",
      "Fatma Ozcan",
      "Yannis Papakonstantinou",
      "Reynold Cheng"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "47 pages, 26 figures, 11 tables. Submitted to arXiv; based on work from The BIRD Team and Google Cloud. Dataset and code available at https://bird-interact.github.io",
    "pdf_url": "https://arxiv.org/pdf/2510.05318v2",
    "published_date": "2025-10-06 19:31:47 UTC",
    "updated_date": "2025-10-08 14:39:59 UTC"
  },
  {
    "arxiv_id": "2510.05315v1",
    "title": "DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology",
    "authors": [
      "Yousef Yeganeh",
      "Maximilian Frantzen",
      "Michael Lee",
      "Kun-Hsing Yu",
      "Nassir Navab",
      "Azade Farshad"
    ],
    "abstract": "While Whole Slide Imaging (WSI) scanners remain the gold standard for digitizing pathology samples, their high cost limits accessibility in many healthcare settings. Other low-cost solutions also face critical limitations: automated microscopes struggle with consistent focus across varying tissue morphology, traditional auto-focus methods require time-consuming focal stacks, and existing deep-learning approaches either need multiple input images or lack generalization capability across tissue types and staining protocols. We introduce a novel automated microscopic system powered by DeepAf, a novel auto-focus framework that uniquely combines spatial and spectral features through a hybrid architecture for single-shot focus prediction. The proposed network automatically regresses the distance to the optimal focal point using the extracted spatiospectral features and adjusts the control parameters for optimal image outcomes. Our system transforms conventional microscopes into efficient slide scanners, reducing focusing time by 80% compared to stack-based methods while achieving focus accuracy of 0.18 μm on the same-lab samples, matching the performance of dual-image methods (0.19 μm) with half the input requirements. DeepAf demonstrates robust cross-lab generalization with only 0.72% false focus predictions and 90% of predictions within the depth of field. Through an extensive clinical study of 536 brain tissue samples, our system achieves 0.90 AUC in cancer classification at 4x magnification, a significant achievement at lower magnification than typical 20x WSI scans. This results in a comprehensive hardware-software design enabling accessible, real-time digital pathology in resource-constrained settings while maintaining diagnostic accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05315v1",
    "published_date": "2025-10-06 19:28:08 UTC",
    "updated_date": "2025-10-06 19:28:08 UTC"
  },
  {
    "arxiv_id": "2510.05310v1",
    "title": "RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts",
    "authors": [
      "Yining She",
      "Daniel W. Peterson",
      "Marianne Menglin Liu",
      "Vikas Upadhyay",
      "Mohammad Hossein Chaghazardi",
      "Eunsuk Kang",
      "Dan Roth"
    ],
    "abstract": "With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern. External LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts. In this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases, making them unreliable. We separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response. The two mitigation methods we tested only bring minor improvements. These results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05310v1",
    "published_date": "2025-10-06 19:20:43 UTC",
    "updated_date": "2025-10-06 19:20:43 UTC"
  },
  {
    "arxiv_id": "2510.06278v1",
    "title": "RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets",
    "authors": [
      "M. Sajid",
      "Mushir Akhtar",
      "A. Quadir",
      "M. Tanveer"
    ],
    "abstract": "Recent advancements in neural networks, supported by foundational theoretical insights, emphasize the superior representational power of complex numbers. However, their adoption in randomized neural networks (RNNs) has been limited due to the lack of effective methods for transforming real-valued tabular datasets into complex-valued representations. To address this limitation, we propose two methods for generating complex-valued representations from real-valued datasets: a natural transformation and an autoencoder-driven method. Building on these mechanisms, we propose RVFL-X, a complex-valued extension of the random vector functional link (RVFL) network. RVFL-X integrates complex transformations into real-valued datasets while maintaining the simplicity and efficiency of the original RVFL architecture. By leveraging complex components such as input, weights, and activation functions, RVFL-X processes complex representations and produces real-valued outputs. Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that RVFL-X consistently outperforms both the original RVFL and state-of-the-art (SOTA) RNN variants, showcasing its robustness and effectiveness across diverse application domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06278v1",
    "published_date": "2025-10-06 19:11:35 UTC",
    "updated_date": "2025-10-06 19:11:35 UTC"
  },
  {
    "arxiv_id": "2510.05295v1",
    "title": "AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement",
    "authors": [
      "M. Sajid",
      "Deepanshu Gupta",
      "Yash Modi",
      "Sanskriti Jain",
      "Harshith Jai Surya Ganji",
      "A. Rahaman",
      "Harshvardhan Choudhary",
      "Nasir Saleem",
      "Amir Hussain",
      "M. Tanveer"
    ],
    "abstract": "In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at https://github.com/mtanveer1/AVSEC-4-Challenge-2025.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05295v1",
    "published_date": "2025-10-06 19:05:35 UTC",
    "updated_date": "2025-10-06 19:05:35 UTC"
  },
  {
    "arxiv_id": "2510.05288v1",
    "title": "DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping",
    "authors": [
      "Ruoxing Yang"
    ],
    "abstract": "Large language models (LLMs) such as ChatGPT have evolved into powerful and ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire specialized skills for specific tasks efficiently. Although LLMs provide great utility in both general and task-specific use cases, they are limited by two security-related concerns. First, traditional LLM hardware requirements make them infeasible to run locally on consumer-grade devices. A remote network connection with the LLM provider's server is usually required, making the system vulnerable to network attacks. Second, fine-tuning an LLM for a sensitive task may involve sensitive data. Non-private fine-tuning algorithms produce models vulnerable to training data reproduction attacks. Our work addresses these security concerns by enhancing differentially private optimization algorithms and applying them to fine-tune localizable language models. We introduce adaptable gradient clipping along with other engineering enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our optimizer to fine-tune examples of two localizable LLM designs, small language model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We demonstrate promising improvements in loss through experimentation with two synthetic datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05288v1",
    "published_date": "2025-10-06 18:56:15 UTC",
    "updated_date": "2025-10-06 18:56:15 UTC"
  },
  {
    "arxiv_id": "2510.05285v1",
    "title": "Adjusting the Output of Decision Transformer with Action Gradient",
    "authors": [
      "Rui Lin",
      "Yiwen Zhang",
      "Zhicheng Peng",
      "Minghao Lyu"
    ],
    "abstract": "Decision Transformer (DT), which integrates reinforcement learning (RL) with the transformer model, introduces a novel approach to offline RL. Unlike classical algorithms that take maximizing cumulative discounted rewards as objective, DT instead maximizes the likelihood of actions. This paradigm shift, however, presents two key challenges: stitching trajectories and extrapolation of action. Existing methods, such as substituting specific tokens with predictive values and integrating the Policy Gradient (PG) method, address these challenges individually but fail to improve performance stably when combined due to inherent instability. To address this, we propose Action Gradient (AG), an innovative methodology that directly adjusts actions to fulfill a function analogous to that of PG, while also facilitating efficient integration with token prediction techniques. AG utilizes the gradient of the Q-value with respect to the action to optimize the action. The empirical results demonstrate that our method can significantly enhance the performance of DT-based algorithms, with some results achieving state-of-the-art levels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05285v1",
    "published_date": "2025-10-06 18:54:42 UTC",
    "updated_date": "2025-10-06 18:54:42 UTC"
  },
  {
    "arxiv_id": "2510.05283v1",
    "title": "Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment",
    "authors": [
      "Radha Gulhane",
      "Sathish Reddy Indurthi"
    ],
    "abstract": "Aligning multimodal large language models (MLLMs) with human preferences often relies on single-signal, model-based reward methods. Such monolithic rewards often lack confidence calibration across domain-specific tasks, fail to capture diverse aspects of human preferences, and require extensive data annotation and reward model training. In this work, we propose a hybrid reward modeling framework that integrates complementary reward paradigms: (i) model-based rewards, where a learned reward model predicts scalar or vector scores from synthetic and human feedback, and (ii) rule-based rewards, where domain-specific heuristics provide explicit correctness signals with confidence. Beyond accuracy, we further incorporate multi-aspect rewards to enforce instruction adherence and introduce a generalized length-penalty reward to stabilize training and improve performance. The proposed framework provides a flexible and effective approach to aligning MLLMs through reinforcement learning policy optimization. Our experiments show consistent improvements across different multimodal benchmarks when applying hybrid and multi-aspect reward modeling. Our best performing model in the 3B family achieves an overall average improvement of ~9.5% across general and math reasoning tasks. Focusing specifically on mathematical benchmarks, the model achieves a significant average improvement of ~16%, highlighting its effectiveness in mathematical reasoning and problem solving.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05283v1",
    "published_date": "2025-10-06 18:53:23 UTC",
    "updated_date": "2025-10-06 18:53:23 UTC"
  },
  {
    "arxiv_id": "2510.06276v1",
    "title": "A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation",
    "authors": [
      "Mehdi Rabiee",
      "Sergio Greco",
      "Reza Shahbazian",
      "Irina Trubitsyna"
    ],
    "abstract": "Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to the subtle and small-scale nature of its lesions. Accurate segmentation of FCD regions in 3D multimodal brain MRI images is essential for effective surgical planning and treatment. However, this task remains highly challenging due to the limited availability of annotated FCD datasets, the extremely small size and weak contrast of FCD lesions, the complexity of handling 3D multimodal inputs, and the need for output smoothness and anatomical consistency, which is often not addressed by standard voxel-wise loss functions. This paper presents a new framework for segmenting FCD regions in 3D brain MRI images. We adopt state-of-the-art transformer-enhanced encoder-decoder architecture and introduce a novel loss function combining Dice loss with an anisotropic {Total Variation} (TV) term. This integration encourages spatial smoothness and reduces false positive clusters without relying on post-processing. The framework is evaluated on a public FCD dataset with 85 epilepsy patients and demonstrates superior segmentation accuracy and consistency compared to standard loss formulations. The model with the proposed TV loss shows an 11.9\\% improvement on the Dice coefficient and 13.3\\% higher precision over the baseline model. Moreover, the number of false positive clusters is reduced by 61.6%",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06276v1",
    "published_date": "2025-10-06 18:07:17 UTC",
    "updated_date": "2025-10-06 18:07:17 UTC"
  },
  {
    "arxiv_id": "2510.05228v1",
    "title": "CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers",
    "authors": [
      "Haining Pan",
      "James V. Roggeveen",
      "Erez Berg",
      "Juan Carrasquilla",
      "Debanjan Chowdhury",
      "Surya Ganguli",
      "Federico Ghimenti",
      "Juraj Hasik",
      "Henry Hunt",
      "Hong-Chen Jiang",
      "Mason Kamb",
      "Ying-Jer Kao",
      "Ehsan Khatami",
      "Michael J. Lawler",
      "Di Luo",
      "Titus Neupert",
      "Xiaoliang Qi",
      "Michael P. Brenner",
      "Eun-Ah Kim"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable progress in coding and math problem-solving, but evaluation on advanced research-level problems in hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a dataset of 50 problems covering condensed matter theory (CMT) at the level of an expert researcher. Topics span analytical and computational approaches in quantum many-body, and classical statistical mechanics. The dataset was designed and verified by a panel of expert researchers from around the world. We built the dataset through a collaborative environment that challenges the panel to write and refine problems they would want a research assistant to solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte Carlo, density matrix renormalization group (DMRG), quantum/classical statistical mechanics, and model building. We evaluate LLMs by programmatically checking solutions against expert-supplied ground truth. We developed machine-grading, including symbolic handling of non-commuting operators via normal ordering. They generalize across tasks too. Our evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. The best model, GPT5, solves 30\\% of the problems; average across 17 models (GPT, Gemini, Claude, DeepSeek, Llama) is 11.4$\\pm$2.1\\%. Moreover, 18 problems are solved by none of the 17 models, and 26 by at most one. These unsolved problems span Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe this benchmark will guide development toward capable AI research assistants and tutors.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05228v1",
    "published_date": "2025-10-06 18:00:55 UTC",
    "updated_date": "2025-10-06 18:00:55 UTC"
  },
  {
    "arxiv_id": "2510.05218v1",
    "title": "Approximate Gaussianity Beyond Initialisation in Neural Networks",
    "authors": [
      "Edward Hirst",
      "Sanjaye Ramgoolam"
    ],
    "abstract": "Ensembles of neural network weight matrices are studied through the training process for the MNIST classification problem, testing the efficacy of matrix models for representing their distributions, under assumptions of Gaussianity and permutation-symmetry. The general 13-parameter permutation invariant Gaussian matrix models are found to be effective models for the correlated Gaussianity in the weight matrices, beyond the range of applicability of the simple Gaussian with independent identically distributed matrix variables, and notably well beyond the initialisation step. The representation theoretic model parameters, and the graph-theoretic characterisation of the permutation invariant matrix observables give an interpretable framework for the best-fit model and for small departures from Gaussianity. Additionally, the Wasserstein distance is calculated for this class of models and used to quantify the movement of the distributions over training. Throughout the work, the effects of varied initialisation regimes, regularisation, layer depth, and layer width are tested for this formalism, identifying limits where particular departures from Gaussianity are enhanced and how more general, yet still highly-interpretable, models can be developed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "hep-th"
    ],
    "primary_category": "cs.LG",
    "comment": "26+34 pages, 15 figures, 12 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.05218v1",
    "published_date": "2025-10-06 18:00:46 UTC",
    "updated_date": "2025-10-06 18:00:46 UTC"
  },
  {
    "arxiv_id": "2510.05213v1",
    "title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing",
    "authors": [
      "Yixiao Wang",
      "Mingxiao Huo",
      "Zhixuan Liang",
      "Yushi Du",
      "Lingfeng Sun",
      "Haotian Lin",
      "Jinghuan Shang",
      "Chensheng Peng",
      "Mohit Bansal",
      "Mingyu Ding",
      "Masayoshi Tomizuka"
    ],
    "abstract": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05213v1",
    "published_date": "2025-10-06 18:00:43 UTC",
    "updated_date": "2025-10-06 18:00:43 UTC"
  },
  {
    "arxiv_id": "2510.05102v1",
    "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration",
    "authors": [
      "Cheng Xin",
      "Fan Xu",
      "Xin Ding",
      "Jie Gao",
      "Jiaxin Ding"
    ],
    "abstract": "Graph Neural Networks (GNNs) have shown remarkable success across various scientific fields, yet their adoption in critical decision-making is often hindered by a lack of interpretability. Recently, intrinsically interpretable GNNs have been studied to provide insights into model predictions by identifying rationale substructures in graphs. However, existing methods face challenges when the underlying rationale subgraphs are complex and varied. In this work, we propose TopInG: Topologically Interpretable Graph Learning, a novel topological framework that leverages persistent homology to identify persistent rationale subgraphs. TopInG employs a rationale filtration learning approach to model an autoregressive generation process of rationale subgraphs, and introduces a self-adjusted topological constraint, termed topological discrepancy, to enforce a persistent topological distinction between rationale subgraphs and irrelevant counterparts. We provide theoretical guarantees that our loss function is uniquely optimized by the ground truth under specific conditions. Extensive experiments demonstrate TopInG's effectiveness in tackling key challenges, such as handling variform rationale subgraphs, balancing predictive performance with interpretability, and mitigating spurious correlations. Results show that our approach improves upon state-of-the-art methods on both predictive accuracy and interpretation quality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "math.AT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "submitted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05102v1",
    "published_date": "2025-10-06 17:59:44 UTC",
    "updated_date": "2025-10-06 17:59:44 UTC"
  },
  {
    "arxiv_id": "2510.05096v2",
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
    "authors": [
      "Zeyu Zhu",
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ],
    "abstract": "Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MA",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://showlab.github.io/Paper2Video/",
    "pdf_url": "https://arxiv.org/pdf/2510.05096v2",
    "published_date": "2025-10-06 17:58:02 UTC",
    "updated_date": "2025-10-09 17:29:00 UTC"
  },
  {
    "arxiv_id": "2510.05095v1",
    "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
    "authors": [
      "Mingkang Zhu",
      "Xi Chen",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ],
    "abstract": "Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05095v1",
    "published_date": "2025-10-06 17:58:01 UTC",
    "updated_date": "2025-10-06 17:58:01 UTC"
  },
  {
    "arxiv_id": "2510.05092v3",
    "title": "Learning to Interpret Weight Differences in Language Models",
    "authors": [
      "Avichal Goel",
      "Yoon Kim",
      "Nir Shavit",
      "Tony T. Wang"
    ],
    "abstract": "Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes (\"weight diffs\") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT-adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Project code and links to weight diffs, adapters, and training data can be found at https://github.com/Aviously/diff-interpretation-tuning",
    "pdf_url": "https://arxiv.org/pdf/2510.05092v3",
    "published_date": "2025-10-06 17:57:23 UTC",
    "updated_date": "2025-10-21 07:29:55 UTC"
  },
  {
    "arxiv_id": "2510.05090v1",
    "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models",
    "authors": [
      "Runchu Tian",
      "Junxia Cui",
      "Xueqiang Xu",
      "Feng Yao",
      "Jingbo Shang"
    ],
    "abstract": "Diffusion large language models (dLLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering advantages such as accelerated parallel decoding and bidirectional context modeling. However, the vanilla decoding strategy in discrete dLLMs suffers from a critical limitation: once a token is accepted, it can no longer be revised in subsequent steps. As a result, early mistakes persist across iterations, harming both intermediate predictions and final output quality. To address this issue, we propose Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding strategy that leverages cross-validation among predicted tokens. Unlike existing methods that follow a single progressive unmasking procedure, Tolerator introduces a two-stage process: (i) sequence fill-up and (ii) iterative refinement by remasking and decoding a subset of tokens while treating the remaining as context. This design enables previously accepted tokens to be reconsidered and corrected when necessary, leading to more reliable diffusion decoding outputs. We evaluate Tolerator on five standard benchmarks covering language understanding, code generation, and mathematics. Experiments show that our method achieves consistent improvements over the baselines under the same computational budget. These findings suggest that decoding algorithms are crucial to realizing the full potential of diffusion large language models. Code and data are publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 8 figures. Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2510.05090v1",
    "published_date": "2025-10-06 17:56:46 UTC",
    "updated_date": "2025-10-06 17:56:46 UTC"
  },
  {
    "arxiv_id": "2510.05087v1",
    "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
    "authors": [
      "Janos Perczel",
      "Jin Chow",
      "Dorottya Demszky"
    ],
    "abstract": "The promise of generative AI to revolutionize education is constrained by the pedagogical limits of large language models (LLMs). A major issue is the lack of access to high-quality training data that reflect the learning of actual students. Prompt engineering has emerged as a stopgap, but the ability of prompts to encode complex pedagogical strategies in rule-based natural language is inherently limited. To address this gap we introduce TeachLM - an LLM optimized for teaching through parameter-efficient fine-tuning of state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000 hours of one-on-one, longitudinal student-tutor interactions maintained by Polygence, which underwent a rigorous anonymization process to protect privacy. We use parameter-efficient fine-tuning to develop an authentic student model that enables the generation of high-fidelity synthetic student-tutor dialogues. Building on this capability, we propose a novel multi-turn evaluation protocol that leverages synthetic dialogue generation to provide fast, scalable, and reproducible assessments of the dialogical capabilities of LLMs. Our evaluations demonstrate that fine-tuning on authentic learning data significantly improves conversational and pedagogical performance - doubling student talk time, improving questioning style, increasing dialogue turns by 50%, and greater personalization of instruction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05087v1",
    "published_date": "2025-10-06 17:55:04 UTC",
    "updated_date": "2025-10-06 17:55:04 UTC"
  },
  {
    "arxiv_id": "2510.05081v1",
    "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
    "authors": [
      "Ronen Kamenetsky",
      "Sara Dorfman",
      "Daniel Garibi",
      "Roni Paiss",
      "Or Patashnik",
      "Daniel Cohen-Or"
    ],
    "abstract": "Large-scale text-to-image diffusion models have become the backbone of modern image editing, yet text prompts alone do not offer adequate control over the editing process. Two properties are especially desirable: disentanglement, where changing one attribute does not unintentionally alter others, and continuous control, where the strength of an edit can be smoothly adjusted. We introduce a method for disentangled and continuous editing through token-level manipulation of text embeddings. The edits are applied by manipulating the embeddings along carefully chosen directions, which control the strength of the target attribute. To identify such directions, we employ a Sparse Autoencoder (SAE), whose sparse latent space exposes semantically isolated dimensions. Our method operates directly on text embeddings without modifying the diffusion process, making it model agnostic and broadly applicable to various image synthesis backbones. Experiments show that it enables intuitive and efficient manipulations with continuous control across diverse attributes and domains.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Project page at: https://ronen94.github.io/SAEdit/",
    "pdf_url": "https://arxiv.org/pdf/2510.05081v1",
    "published_date": "2025-10-06 17:51:04 UTC",
    "updated_date": "2025-10-06 17:51:04 UTC"
  },
  {
    "arxiv_id": "2510.05077v1",
    "title": "Slm-mux: Orchestrating small language models for reasoning",
    "authors": [
      "Chenyu Wang",
      "Zishen Wan",
      "Hao Kang",
      "Emma Chen",
      "Zhiqiang Xie",
      "Tushar Krishna",
      "Vijay Janapa Reddi",
      "Yilun Du"
    ],
    "abstract": "With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestrated into a system where each contributes effectively, achieving higher accuracy than any individual model? Existing orchestration methods have primarily targeted frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To address this gap, we propose a three-stage approach for orchestrating SLMs. First, we introduce SLM-MUX, a multi-model architecture that effectively coordinates multiple SLMs. Building on this, we develop two optimization strategies: (i) a model selection search that identifies the most complementary SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our approach delivers strong results: Compared to existing orchestration methods, our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. We further provide theoretical analyses to substantiate the advantages of our method. In summary, we demonstrate that SLMs can be effectively orchestrated into more accurate and efficient systems through the proposed approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05077v1",
    "published_date": "2025-10-06 17:49:58 UTC",
    "updated_date": "2025-10-06 17:49:58 UTC"
  },
  {
    "arxiv_id": "2510.05069v2",
    "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
    "authors": [
      "Dachuan Shi",
      "Abedelkadir Asi",
      "Keying Li",
      "Xiangchi Yuan",
      "Leyan Pan",
      "Wenke Lee",
      "Wen Xiao"
    ],
    "abstract": "Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/sdc17/SwiReasoning, Website: https://swireasoning.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.05069v2",
    "published_date": "2025-10-06 17:46:34 UTC",
    "updated_date": "2025-12-06 16:34:57 UTC"
  },
  {
    "arxiv_id": "2510.05197v1",
    "title": "Efficient Prediction of Pass@k Scaling in Large Language Models",
    "authors": [
      "Joshua Kazdan",
      "Rylan Schaeffer",
      "Youssef Allouah",
      "Colin Sullivan",
      "Kyssen Yu",
      "Noam Levi",
      "Sanmi Koyejo"
    ],
    "abstract": "Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05197v1",
    "published_date": "2025-10-06 17:42:27 UTC",
    "updated_date": "2025-10-06 17:42:27 UTC"
  },
  {
    "arxiv_id": "2510.05059v1",
    "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
    "authors": [
      "Junlin Wang",
      "Jue Wang",
      "Zhen",
      "Xu",
      "Ben Athiwaratkun",
      "Bhuwan Dhingra",
      "Ce Zhang",
      "James Zou"
    ],
    "abstract": "Recent advances in large language models (LLMs) opened up new directions for leveraging the collective expertise of multiple LLMs. These methods, such as Mixture-of-Agents, typically employ additional inference steps to generate intermediate outputs, which are then used to produce the final response. While multi-agent inference can enhance response quality, it can significantly increase the time to first token (TTFT), posing a challenge for latency-sensitive applications and hurting user experience. To address this issue, we propose staircase streaming for low-latency multi-agent inference. Instead of waiting for the complete intermediate outputs from previous steps, we begin generating the final response as soon as we receive partial outputs from these steps. Experimental results demonstrate that staircase streaming reduces TTFT by up to 93% while maintaining response quality.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05059v1",
    "published_date": "2025-10-06 17:37:35 UTC",
    "updated_date": "2025-10-06 17:37:35 UTC"
  },
  {
    "arxiv_id": "2510.05054v2",
    "title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model",
    "authors": [
      "Peter Van Katwyk",
      "Karianne J. Bergen"
    ],
    "abstract": "Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Reviewed and published in TMLR at https://openreview.net/forum?id=xRiEdSyVjY",
    "pdf_url": "https://arxiv.org/pdf/2510.05054v2",
    "published_date": "2025-10-06 17:34:48 UTC",
    "updated_date": "2025-10-14 18:49:39 UTC"
  },
  {
    "arxiv_id": "2510.08600v1",
    "title": "Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation",
    "authors": [
      "Devleena Das",
      "Rajeev Patwari",
      "Ashish Sirasao"
    ],
    "abstract": "Inference optimizations such as quantization, pruning, format and datatype conversion, model export, and serialization can lead to functional degradations in language model task performance. While most efforts on performance recovery for deployment focus on robust quantization techniques, we focus on recovering model accuracies from any sources that degrade model weights, such as improper model serialization. In this work, we propose Recover-LoRA, a lightweight and dataset agnostic method to recover accuracy in degraded models. Recover-LoRA uses synthetic data and logit distillation to learn LoRA adapters on selective layers that facilitate aligning the degraded model to its full precision model. We investigate the utility of Recover-LoRA across a diverse set of small language models (SLMs), including models with varying attention architectures, multi-head attention (MHA) and group-query attention (GQA), as well as several evaluation datasets. Our results show that Recover-LoRA recovers model accuracies by 5-17% on MHA and GQA SLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2510.08600v1",
    "published_date": "2025-10-06 17:34:19 UTC",
    "updated_date": "2025-10-06 17:34:19 UTC"
  },
  {
    "arxiv_id": "2510.05048v1",
    "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games",
    "authors": [
      "Ondřej Kubíček",
      "Viliam Lisý"
    ],
    "abstract": "Test-time reasoning significantly enhances pre-trained AI agents' performance. However, it requires an explicit environment model, often unavailable or overly complex in real-world scenarios. While MuZero enables effective model learning for search in perfect information games, extending this paradigm to imperfect information games presents substantial challenges due to more nuanced look-ahead reasoning techniques and large number of states relevant for individual decisions. This paper introduces an algorithm LAMIR that learns an abstracted model of an imperfect information game directly from the agent-environment interaction. During test time, this trained model is used to perform look-ahead reasoning. The learned abstraction limits the size of each subgame to a manageable size, making theoretically principled look-ahead reasoning tractable even in games where previous methods could not scale. We empirically demonstrate that with sufficient capacity, LAMIR learns the exact underlying game structure, and with limited capacity, it still learns a valuable abstraction, which improves game playing performance of the pre-trained agents even in large games.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05048v1",
    "published_date": "2025-10-06 17:26:56 UTC",
    "updated_date": "2025-10-06 17:26:56 UTC"
  },
  {
    "arxiv_id": "2510.05040v1",
    "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
    "authors": [
      "Jihoon Lee",
      "Hoyeon Moon",
      "Kevin Zhai",
      "Arun Kumar Chithanar",
      "Anit Kumar Sahu",
      "Soummya Kar",
      "Chul Lee",
      "Souradip Chakraborty",
      "Amrit Singh Bedi"
    ],
    "abstract": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05040v1",
    "published_date": "2025-10-06 17:16:41 UTC",
    "updated_date": "2025-10-06 17:16:41 UTC"
  },
  {
    "arxiv_id": "2510.05036v1",
    "title": "Graph-Aware Diffusion for Signal Generation",
    "authors": [
      "Sergio Rozada",
      "Vimal K. B.",
      "Andrea Cavallo",
      "Antonio G. Marques",
      "Hadi Jamali-Rad",
      "Elvin Isufi"
    ],
    "abstract": "We study the problem of generating graph signals from unknown distributions defined over given graphs, relevant to domains such as recommender systems or sensor networks. Our approach builds on generative diffusion models, which are well established in vision and graph generation but remain underexplored for graph signals. Existing methods lack generality, either ignoring the graph structure in the forward process or designing graph-aware mechanisms tailored to specific domains. We adopt a forward process that incorporates the graph through the heat equation. Rather than relying on the standard formulation, we consider a time-warped coefficient to mitigate the exponential decay of the drift term, yielding a graph-aware generative diffusion model (GAD). We analyze its forward dynamics, proving convergence to a Gaussian Markov random field with covariance parametrized by the graph Laplacian, and interpret the backward dynamics as a sequence of graph-signal denoising problems. Finally, we demonstrate the advantages of GAD on synthetic data, real traffic speed measurements, and a temperature sensor network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05036v1",
    "published_date": "2025-10-06 17:11:32 UTC",
    "updated_date": "2025-10-06 17:11:32 UTC"
  },
  {
    "arxiv_id": "2510.08599v1",
    "title": "BaldWhisper: Faster Whisper with Head Shearing and Layer Merging",
    "authors": [
      "Yaya Sy",
      "Christophe Cerisara",
      "Irina Illina"
    ],
    "abstract": "Pruning large pre-trained transformers for low-resource languages is challenging, as it often requires massive retraining data to recover performance. For instance, Distill-Whisper prunes Whisper by 40% and retrains on 21,000 hours of speech, far beyond what is available for most languages. Can Whisper be made lighter and faster for edge devices in data-scarce settings? Focusing on Bambara with only 32h of speech-to-text data, we propose a new pruning recipe. Instead of vocabulary pruning, which is unsuitable due to frequent code-switching by Bambara speakers, we compress the embeddings with low-rank decomposition and feature distillation. Rather than removing layers, we merge them to limit performance loss. The final model preserves 90% of the original performance while being 48% smaller and 2.15x faster on a MacBook Air M1.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08599v1",
    "published_date": "2025-10-06 17:04:36 UTC",
    "updated_date": "2025-10-06 17:04:36 UTC"
  },
  {
    "arxiv_id": "2510.05025v1",
    "title": "Imperceptible Jailbreaking against Large Language Models",
    "authors": [
      "Kuofeng Gao",
      "Yiming Li",
      "Chao Du",
      "Xin Wang",
      "Xingjun Ma",
      "Shu-Tao Xia",
      "Tianyu Pang"
    ],
    "abstract": "Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is \"secretly\" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05025v1",
    "published_date": "2025-10-06 17:03:50 UTC",
    "updated_date": "2025-10-06 17:03:50 UTC"
  },
  {
    "arxiv_id": "2510.05023v1",
    "title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective",
    "authors": [
      "Weixin Wang",
      "Haoyang Zheng",
      "Guang Lin",
      "Wei Deng",
      "Pan Xu"
    ],
    "abstract": "Most existing approximate Thompson Sampling (TS) algorithms for multi-armed bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in each round to sample from the posterior, relaxing the need for conjugacy assumptions between priors and reward distributions in vanilla TS. However, they often require approximating a different posterior distribution in different round of the bandit problem. This requires tricky, round-specific tuning of hyperparameters such as dynamic learning rates, causing challenges in both theoretical analysis and practical implementation. To alleviate this non-stationarity, we introduce TS-SA, which incorporates stochastic approximation (SA) within the TS framework. In each round, TS-SA constructs a posterior approximation only using the most recent reward(s), performs a Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy proposals over time. This can be interpreted as approximating a stationary posterior target throughout the entire algorithm, which further yields a fixed step-size, a unified convergence analysis framework, and improved posterior estimates through temporal averaging. We establish near-optimal regret bounds for TS-SA, with a simplified and more intuitive theoretical analysis enabled by interpreting the entire algorithm as a simulation of a stationary SGLD process. Our empirical results demonstrate that even a single-step Langevin update with certain warm-up outperforms existing methods substantially on bandit tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages, 3 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.05023v1",
    "published_date": "2025-10-06 17:01:29 UTC",
    "updated_date": "2025-10-06 17:01:29 UTC"
  },
  {
    "arxiv_id": "2510.05016v2",
    "title": "Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)",
    "authors": [
      "Lucas Carrit Delgado Pinheiro",
      "Ziru Chen",
      "Bruno Caixeta Piazza",
      "Ness Shroff",
      "Yingbin Liang",
      "Yuan-Sen Ting",
      "Huan Sun"
    ],
    "abstract": "While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "18 pages, 6 figures, to be submitted, comments are welcome. Reproducibility details can be found at: https://github.com/OSU-NLP-Group/LLM-IOAA",
    "pdf_url": "https://arxiv.org/pdf/2510.05016v2",
    "published_date": "2025-10-06 16:58:47 UTC",
    "updated_date": "2025-10-07 15:34:59 UTC"
  },
  {
    "arxiv_id": "2510.05014v4",
    "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
    "authors": [
      "Xuanming Cui",
      "Jianpeng Cheng",
      "Hong-you Chen",
      "Satya Narayan Shukla",
      "Abhijeet Awasthi",
      "Xichen Pan",
      "Chaitanya Ahuja",
      "Shlok Kumar Mishra",
      "Yonghuan Yang",
      "Jun Xiao",
      "Qi Guo",
      "Ser-Nam Lim",
      "Aashu Singh",
      "Xiangjun Fan"
    ],
    "abstract": "There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05014v4",
    "published_date": "2025-10-06 16:53:56 UTC",
    "updated_date": "2026-01-19 16:13:58 UTC"
  },
  {
    "arxiv_id": "2510.05003v1",
    "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning",
    "authors": [
      "Imran Mansha"
    ],
    "abstract": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated remarkable reasoning abilities but require significant computational resources for fine-tuning. This paper presents a resource-efficient fine-tuning approach for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating under constrained GPU and memory settings. Using parameter-efficient tuning techniques such as LoRA and QLoRA, we adapt the base model on publicly available medical reasoning datasets. The model achieves improved reasoning coherence and factual accuracy while reducing memory usage by up to 60% compared to standard full fine-tuning. Experimental evaluation demonstrates that lightweight adaptations can retain strong reasoning capability in medical question-answering tasks. This work highlights practical strategies for deploying LLMs in low-resource research environments and provides insights into balancing efficiency and domain specialization for medical AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 2 figures. Submitted to arXiv for open access",
    "pdf_url": "https://arxiv.org/pdf/2510.05003v1",
    "published_date": "2025-10-06 16:42:11 UTC",
    "updated_date": "2025-10-06 16:42:11 UTC"
  },
  {
    "arxiv_id": "2510.04999v1",
    "title": "Bridging Text and Video Generation: A Survey",
    "authors": [
      "Nilay Kumar",
      "Priyansh Bhandari",
      "G. Maragatham"
    ],
    "abstract": "Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04999v1",
    "published_date": "2025-10-06 16:39:05 UTC",
    "updated_date": "2025-10-06 16:39:05 UTC"
  },
  {
    "arxiv_id": "2510.04997v1",
    "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis",
    "authors": [
      "Jiongchi Yu",
      "Weipeng Jiang",
      "Xiaoyu Zhang",
      "Qiang Hu",
      "Xiaofei Xie",
      "Chao Shen"
    ],
    "abstract": "Understanding software faults is essential for empirical research in software development and maintenance. However, traditional fault analysis, while valuable, typically involves multiple expert-driven steps such as collecting potential faults, filtering, and manual investigation. These processes are both labor-intensive and time-consuming, creating bottlenecks that hinder large-scale fault studies in complex yet critical software systems and slow the pace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study into three key phases: (1) research objective definition, (2) data preparation, and (3) fault analysis, and we conduct an initial exploration study of applying Large Language Models (LLMs) for fault analysis of open-source software. Specifically, we perform the evaluation on 3,829 software faults drawn from a high-quality empirical study. Our results show that LLMs can substantially improve efficiency in fault analysis, with an average processing time of about two hours, compared to the weeks of manual effort typically required. We conclude by outlining a detailed research plan that highlights both the potential of LLMs for advancing empirical fault studies and the open challenges that required be addressed to achieve fully automated, end-to-end software fault analysis.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.04997v1",
    "published_date": "2025-10-06 16:37:18 UTC",
    "updated_date": "2025-10-06 16:37:18 UTC"
  },
  {
    "arxiv_id": "2510.04996v3",
    "title": "Reinforce-Ada: An Adaptive Sampling Framework under Non-linear RL Objectives",
    "authors": [
      "Wei Xiong",
      "Chenlu Ye",
      "Baohao Liao",
      "Hanze Dong",
      "Xinxing Xu",
      "Christof Monz",
      "Jiang Bian",
      "Nan Jiang",
      "Tong Zhang"
    ],
    "abstract": "Reinforcement learning (RL) for large language model reasoning is frequently hindered by signal loss, a phenomenon where standard uniform sampling with small group sizes fails to uncover informative learning signals for difficult prompts. We demonstrate that this collapse is a statistical artifact of undersampling rather than an inherent model limitation. To address this systematically, we introduce a theoretical framework based on optimizing a non-linear RL objective (e.g., log-likelihood). We show that this objective naturally induces a weighted gradient estimator that prioritizes difficult prompts, which can be robustly realized through adaptive sampling. Guided by this framework, we propose Reinforce-Ada, a family of algorithms that dynamically allocates inference budgets based on prompt difficulty, effectively scaling up RL compute to where it is needed most. Unlike passive filtering methods that discard low-signal prompts, Reinforce-Ada actively invests compute to recover them. We introduce two efficient realizations: an estimation-based approach and a model-free sequential sampling approach. Extensive experiments across multiple benchmarks show that Reinforce-Ada significantly outperforms uniform baselines like GRPO, recovering lost signals and accelerating convergence by up to $2\\times$ while maintaining the same total inference budget. Code is available at https://github.com/RLHFlow/Reinforce-Ada.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04996v3",
    "published_date": "2025-10-06 16:34:09 UTC",
    "updated_date": "2025-12-05 17:41:34 UTC"
  },
  {
    "arxiv_id": "2510.04983v3",
    "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives",
    "authors": [
      "Khalid Mehtab Khan",
      "Anagha Kulkarni"
    ],
    "abstract": "Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The authors are withdrawing this version to correct issues identified in the experimental design and analysis. A revised and validated version will be submitted after further review",
    "pdf_url": "https://arxiv.org/pdf/2510.04983v3",
    "published_date": "2025-10-06 16:19:57 UTC",
    "updated_date": "2025-11-03 21:48:02 UTC"
  },
  {
    "arxiv_id": "2510.04980v1",
    "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game",
    "authors": [
      "Fangzhou Liang",
      "Tianshi Zheng",
      "Chunkit Chan",
      "Yauwai Yim",
      "Yangqiu Song"
    ],
    "abstract": "Effective multi-agent collaboration requires agents to infer the rationale behind others' actions, a capability rooted in Theory-of-Mind (ToM). While recent Large Language Models (LLMs) excel at logical inference, their ability to infer rationale in dynamic, collaborative settings remains under-explored. This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework features an automated evaluation system that measures both game performance and ToM proficiency. Across a range of models, we find a significant positive correlation between ToM and in-game success. Notably, first-order ToM (interpreting others' intent) correlates more strongly with performance than second-order ToM (predicting others' interpretations). These findings highlight that for effective AI collaboration, the ability to accurately interpret a partner's rationale is more critical than higher-order reasoning. We conclude that prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2025 Wordplay",
    "pdf_url": "https://arxiv.org/pdf/2510.04980v1",
    "published_date": "2025-10-06 16:17:24 UTC",
    "updated_date": "2025-10-06 16:17:24 UTC"
  },
  {
    "arxiv_id": "2510.04978v4",
    "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI",
    "authors": [
      "Kun Xiang",
      "Terry Jingchen Zhang",
      "Yinya Huang",
      "Jixi He",
      "Zirong Liu",
      "Yueling Tang",
      "Ruizhe Zhou",
      "Lijing Luo",
      "Youpeng Wen",
      "Xiuwei Chen",
      "Bingqian Lin",
      "Jianhua Han",
      "Hang Xu",
      "Hanhui Li",
      "Bin Dong",
      "Xiaodan Liang"
    ],
    "abstract": "The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04978v4",
    "published_date": "2025-10-06 16:16:03 UTC",
    "updated_date": "2025-12-22 12:44:21 UTC"
  },
  {
    "arxiv_id": "2510.05196v1",
    "title": "Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response",
    "authors": [
      "Daqian Shi",
      "Xiaolei Diao",
      "Jinge Wu",
      "Honghan Wu",
      "Xiongfeng Tang",
      "Felix Naughton",
      "Paulina Bondaronek"
    ],
    "abstract": "Timely and accurate analysis of population-level data is crucial for effective decision-making during public health emergencies such as the COVID-19 pandemic. However, the massive input of semi-structured data, including structured demographic information and unstructured human feedback, poses significant challenges to conventional analysis methods. Manual expert-driven assessments, though accurate, are inefficient, while standard NLP pipelines often require large task-specific labeled datasets and struggle with generalization across diverse domains. To address these challenges, we propose a novel graph-based reasoning framework that integrates large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. The proposed approach dynamically models evolving citizen needs into a need-aware graph, enabling population-specific analyses based on key features such as age, gender, and the Index of Multiple Deprivation. It generates interpretable insights to inform responsive health policy decision-making. We test our method using a real-world dataset, and preliminary experimental results demonstrate its feasibility. This approach offers a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Efficient Medical AI 2025 Workshop, MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.05196v1",
    "published_date": "2025-10-06 16:10:18 UTC",
    "updated_date": "2025-10-06 16:10:18 UTC"
  },
  {
    "arxiv_id": "2510.04970v1",
    "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning",
    "authors": [
      "Marcel Wienöbst",
      "Leonard Henckel",
      "Sebastian Weichwald"
    ],
    "abstract": "We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. It pairs fast parent selection with iterative Cholesky-based score updates, cutting run-times over prior algorithms. This makes it feasible to fully embrace discrete search, enabling iterated local search with principled order initialization to find graphs with scores at or close to the global optimum. The resulting structures are highly accurate across benchmarks, with near-perfect recovery in standard settings. This performance calls for revisiting discrete search over graphs as a reasonable approach to causal discovery.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04970v1",
    "published_date": "2025-10-06 16:04:53 UTC",
    "updated_date": "2025-10-06 16:04:53 UTC"
  },
  {
    "arxiv_id": "2510.04966v1",
    "title": "ActiveMark: on watermarking of visual foundation models via massive activations",
    "authors": [
      "Anna Chistyakova",
      "Mikhail Pautov"
    ],
    "abstract": "Being trained on large and vast datasets, visual foundation models (VFMs) can be fine-tuned for diverse downstream tasks, achieving remarkable performance and efficiency in various computer vision applications. The high computation cost of data collection and training motivates the owners of some VFMs to distribute them alongside the license to protect their intellectual property rights. However, a dishonest user of the protected model's copy may illegally redistribute it, for example, to make a profit. As a consequence, the development of reliable ownership verification tools is of great importance today, since such methods can be used to differentiate between a redistributed copy of the protected model and an independent model. In this paper, we propose an approach to ownership verification of visual foundation models by fine-tuning a small set of expressive layers of a VFM along with a small encoder-decoder network to embed digital watermarks into an internal representation of a hold-out set of input images. Importantly, the watermarks embedded remain detectable in the functional copies of the protected model, obtained, for example, by fine-tuning the VFM for a particular downstream task. Theoretically and experimentally, we demonstrate that the proposed method yields a low probability of false detection of a non-watermarked model and a low probability of false misdetection of a watermarked model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04966v1",
    "published_date": "2025-10-06 15:58:27 UTC",
    "updated_date": "2025-10-06 15:58:27 UTC"
  },
  {
    "arxiv_id": "2510.04956v2",
    "title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling",
    "authors": [
      "Bi-Cheng Yan",
      "Ming-Kang Tsai",
      "Berlin Chen"
    ],
    "abstract": "Computer-assisted pronunciation training (CAPT) manages to facilitate second-language (L2) learners to practice pronunciation skills by offering timely and instructive feedback. To examine pronunciation proficiency from multiple facets, existing methods for CAPT broadly fall into two categories: mispronunciation detection and diagnosis (MDD) as well as automatic pronunciation assessment (APA). The former aims to pinpoint phonetic pronunciation errors and provide diagnostic feedback, while the latter seeks instead to quantify pronunciation proficiency pertaining to various aspects. Despite the natural complementarity between MDD and APA, researchers and practitioners, however, often treat them as independent tasks with disparate modeling paradigms. In light of this, we in this paper first introduce MuFFIN, a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical Neural architecture, to jointly address the tasks of MDD and APA. To better capture the nuanced distinctions between phonemes in the feature space, a novel phoneme-contrastive ordinal regularization mechanism is then put forward to optimize the proposed model to generate more phoneme-discriminative features while factoring in the ordinality of the aspect scores. In addition, to address the intricate data imbalance problem in MDD, we design a simple yet effective training objective, which is specifically tailored to perturb the outputs of a phoneme classifier with the phoneme-specific variations, so as to better render the distribution of predicted phonemes meanwhile considering their mispronunciation characteristics. A series of experiments conducted on the Speechocean762 benchmark dataset demonstrates the efficacy of our method in relation to several cutting-edge baselines, showing state-of-the-art performance on both the APA and MDD tasks.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted and to appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    "pdf_url": "https://arxiv.org/pdf/2510.04956v2",
    "published_date": "2025-10-06 15:54:55 UTC",
    "updated_date": "2025-10-07 14:43:24 UTC"
  },
  {
    "arxiv_id": "2510.04952v2",
    "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits",
    "authors": [
      "Ailiya Borjigin",
      "Cong He"
    ],
    "abstract": "We present a cross-market algorithmic trading system that balances execution quality with rigorous compliance enforcement. The architecture comprises a high-level planner, a reinforcement learning execution agent, and an independent compliance agent. We formulate trade execution as a constrained Markov decision process with hard constraints on participation limits, price bands, and self-trading avoidance. The execution agent is trained with proximal policy optimization, while a runtime action-shield projects any unsafe action into a feasible set. To support auditability without exposing proprietary signals, we add a zero-knowledge compliance audit layer that produces cryptographic proofs that all actions satisfied the constraints. We evaluate in a multi-venue, ABIDES-based simulator and compare against standard baselines (e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and variance while exhibiting no observed constraint violations across stress scenarios including elevated latency, partial fills, compliance module toggling, and varying constraint limits. We report effects at the 95% confidence level using paired t-tests and examine tail risk via CVaR. We situate the work at the intersection of optimal execution, safe reinforcement learning, regulatory technology, and verifiable AI, and discuss ethical considerations, limitations (e.g., modeling assumptions and computational overhead), and paths to real-world deployment.",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04952v2",
    "published_date": "2025-10-06 15:52:12 UTC",
    "updated_date": "2025-10-07 07:12:41 UTC"
  },
  {
    "arxiv_id": "2510.04951v2",
    "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints",
    "authors": [
      "Jayanta Mandi",
      "Marianne Defresne",
      "Senne Berden",
      "Tias Guns"
    ],
    "abstract": "When some parameters of a constrained optimization problem (COP) are uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising two stages: the prediction of the unknown parameters from contextual information and the subsequent optimization using those predicted parameters. Decision-focused learning (DFL) implements the first stage by training a machine learning (ML) model to optimize the quality of the decisions made using the predicted parameters. When the predicted parameters occur in the constraints, they can lead to infeasible solutions. Therefore, it is important to simultaneously manage both feasibility and decision quality. We develop a DFL framework for predicting constraint parameters in a generic COP. While prior works typically assume that the underlying optimization problem is a linear program (LP) or integer LP (ILP), our approach makes no such assumption. We derive two novel loss functions based on maximum likelihood estimation (MLE): the first one penalizes infeasibility (by penalizing predicted parameters that lead to infeasible solutions), while the second one penalizes suboptimal decisions (by penalizing predicted parameters that make the true optimal solution infeasible). We introduce a single tunable parameter to form a weighted average of the two losses, allowing decision-makers to balance suboptimality and feasibility. We experimentally demonstrate that adjusting this parameter provides decision-makers control over this trade-off. Moreover, across several COP instances, we show that adjusting the tunable parameter allows a decision-maker to prioritize either suboptimality or feasibility, outperforming the performance of existing baselines in either objective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04951v2",
    "published_date": "2025-10-06 15:52:03 UTC",
    "updated_date": "2025-10-26 08:52:55 UTC"
  },
  {
    "arxiv_id": "2510.04950v1",
    "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)",
    "authors": [
      "Om Dobariya",
      "Akhil Kumar"
    ],
    "abstract": "The wording of natural language prompts has been shown to influence the performance of large language models (LLMs), yet the role of politeness and tone remains underexplored. In this study, we investigate how varying levels of prompt politeness affect model accuracy on multiple-choice questions. We created a dataset of 50 base questions spanning mathematics, science, and history, each rewritten into five tone variants: Very Polite, Polite, Neutral, Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we evaluated responses across these conditions and applied paired sample t-tests to assess statistical significance. Contrary to expectations, impolite prompts consistently outperformed polite ones, with accuracy ranging from 80.8% for Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from earlier studies that associated rudeness with poorer outcomes, suggesting that newer LLMs may respond differently to tonal variation. Our results highlight the importance of studying pragmatic aspects of prompting and raise broader questions about the social dimensions of human-AI interaction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "stat.ME"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 3 tables; includes Limitations and Ethical Considerations sections; short paper under submission to Findings of ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04950v1",
    "published_date": "2025-10-06 15:50:39 UTC",
    "updated_date": "2025-10-06 15:50:39 UTC"
  },
  {
    "arxiv_id": "2510.04947v1",
    "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion",
    "authors": [
      "Xin Li",
      "Kaixiang Yang",
      "Qiang Li",
      "Zhiwei Wang"
    ],
    "abstract": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "BIBM2025 accept, 8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04947v1",
    "published_date": "2025-10-06 15:48:27 UTC",
    "updated_date": "2025-10-06 15:48:27 UTC"
  },
  {
    "arxiv_id": "2510.04945v1",
    "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
    "authors": [
      "Juan-José Guzmán-Landa",
      "Juan-Manuel Torres-Moreno",
      "Miguel Figueroa-Saavedra",
      "Ligia Quintana-Torres",
      "Martha-Lorena Avendaño-Garrido",
      "Graham Ranger"
    ],
    "abstract": "In this article we introduce a context-free grammar (CFG) for the Nawatl language. Nawatl (or Nahuatl) is an Amerindian language of the $π$-language type, i.e. a language with few digital resources, in which the corpora available for machine learning are virtually non-existent. The objective here is to generate a significant number of grammatically correct artificial sentences, in order to increase the corpora available for language model training. We want to show that a grammar enables us significantly to expand a corpus in Nawatl which we call $π$-\\textsc{yalli}. The corpus, thus enriched, enables us to train algorithms such as FastText and to evaluate them on sentence-level semantic tasks. Preliminary results show that by using the grammar, comparative improvements are achieved over some LLMs. However, it is observed that to achieve more significant improvement, grammars that model the Nawatl language even more effectively are required.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 7 tables, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.04945v1",
    "published_date": "2025-10-06 15:46:54 UTC",
    "updated_date": "2025-10-06 15:46:54 UTC"
  },
  {
    "arxiv_id": "2510.04939v1",
    "title": "Unsupervised Active Learning via Natural Feature Progressive Framework",
    "authors": [
      "Yuxi Liu",
      "Catherine Lalman",
      "Yimin Yang"
    ],
    "abstract": "The effectiveness of modern deep learning models is predicated on the availability of large-scale, human-annotated datasets, a process that is notoriously expensive and time-consuming. While Active Learning (AL) offers a strategic solution by labeling only the most informative and representative data, its iterative nature still necessitates significant human involvement. Unsupervised Active Learning (UAL) presents an alternative by shifting the annotation burden to a single, post-selection step. Unfortunately, prevailing UAL methods struggle to achieve state-of-the-art performance. These approaches typically rely on local, gradient-based scoring for sample importance estimation, which not only makes them vulnerable to ambiguous and noisy data but also hinders their capacity to select samples that adequately represent the full data distribution. Moreover, their use of shallow, one-shot linear selection falls short of a true UAL paradigm. In this paper, we propose the Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes how sample importance is measured. At its core, NFPF employs a Specific Feature Learning Machine (SFLM) to effectively quantify each sample's contribution to model performance. We further utilize the SFLM to define a powerful Reconstruction Difference metric for initial sample selection. Our comprehensive experiments show that NFPF significantly outperforms all established UAL methods and achieves performance on par with supervised AL methods on vision datasets. Detailed ablation studies and qualitative visualizations provide compelling evidence for NFPF's superior performance, enhanced robustness, and improved data distribution coverage.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review at IEEE TPAMI",
    "pdf_url": "https://arxiv.org/pdf/2510.04939v1",
    "published_date": "2025-10-06 15:44:33 UTC",
    "updated_date": "2025-10-06 15:44:33 UTC"
  },
  {
    "arxiv_id": "2510.04938v1",
    "title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures",
    "authors": [
      "Shiwen Qin",
      "Alexander Auras",
      "Shay B. Cohen",
      "Elliot J. Crowley",
      "Michael Moeller",
      "Linus Ericsson",
      "Jovita Lukasik"
    ],
    "abstract": "Neural architecture search (NAS) automates the design process of high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than 600k {architecture, accuracy} pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Our code is available at: https://github.com/shiwenqin/ONNX-Net",
    "pdf_url": "https://arxiv.org/pdf/2510.04938v1",
    "published_date": "2025-10-06 15:43:36 UTC",
    "updated_date": "2025-10-06 15:43:36 UTC"
  },
  {
    "arxiv_id": "2510.04935v1",
    "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning",
    "authors": [
      "Guoxin Chen",
      "Zile Qiao",
      "Wenqing Wang",
      "Donglei Yu",
      "Xuanzhong Chen",
      "Hao Sun",
      "Minpeng Liao",
      "Kai Fan",
      "Yong Jiang",
      "Penguin Xie",
      "Wayne Xin Zhao",
      "Ruihua Song",
      "Fei Huang"
    ],
    "abstract": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Ongoing Work",
    "pdf_url": "https://arxiv.org/pdf/2510.04935v1",
    "published_date": "2025-10-06 15:42:55 UTC",
    "updated_date": "2025-10-06 15:42:55 UTC"
  },
  {
    "arxiv_id": "2510.04934v1",
    "title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation",
    "authors": [
      "Satvik Dixit",
      "Soham Deshmukh",
      "Bhiksha Raj"
    ],
    "abstract": "Audio Question Answering (AQA) is a key task for evaluating Audio-Language Models (ALMs), yet assessing open-ended responses remains challenging. Existing metrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from NLP and audio captioning, rely on surface similarity and fail to account for question context, reasoning, and partial correctness. To address the gap in literature, we make three contributions in this work. First, we introduce AQEval to enable systematic benchmarking of AQA metrics. It is the first benchmark of its kind, consisting of 10k model responses annotated by multiple humans for their correctness and relevance. Second, we conduct a comprehensive analysis of existing AQA metrics on AQEval, highlighting weak correlation with human judgment, especially for longer answers. Third, we propose a new metric - AURA score, to better evaluate open-ended model responses. On AQEval, AURA achieves state-of-the-art correlation with human ratings, significantly outperforming all baselines. Through this work, we aim to highlight the limitations of current AQA evaluation methods and motivate better metrics. We release both the AQEval benchmark and the AURA metric to support future research in holistic AQA evaluation.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04934v1",
    "published_date": "2025-10-06 15:41:34 UTC",
    "updated_date": "2025-10-06 15:41:34 UTC"
  },
  {
    "arxiv_id": "2510.04933v1",
    "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models",
    "authors": [
      "Amir Hameed Mir"
    ],
    "abstract": "Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "comment": "Comments: 14 pages, 14 figures, 5 tables. Code available at: https://github.com/sirraya-tech/Sirraya_LSD_Code",
    "pdf_url": "https://arxiv.org/pdf/2510.04933v1",
    "published_date": "2025-10-06 15:41:22 UTC",
    "updated_date": "2025-10-06 15:41:22 UTC"
  },
  {
    "arxiv_id": "2510.04927v1",
    "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data",
    "authors": [
      "Usman Akram",
      "Yiyue Chen",
      "Haris Vikalo"
    ],
    "abstract": "Training automatic modulation classification (AMC) models on centrally aggregated data raises privacy concerns, incurs communication overhead, and often fails to confer robustness to channel shifts. Federated learning (FL) avoids central aggregation by training on distributed clients but remains sensitive to class imbalance, non-IID client distributions, and limited labeled samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences across clients, followed by per-client SVMs on small labeled sets. We establish convergence of the federated representation learning procedure and a separability guarantee for the downstream classifier under feature noise. Experiments on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04927v1",
    "published_date": "2025-10-06 15:37:15 UTC",
    "updated_date": "2025-10-06 15:37:15 UTC"
  },
  {
    "arxiv_id": "2510.04923v2",
    "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis",
    "authors": [
      "Alec K. Peltekian",
      "Halil Ertugrul Aktas",
      "Gorkem Durak",
      "Kevin Grudzinski",
      "Bradford C. Bemiss",
      "Carrie Richardson",
      "Jane E. Dematte",
      "G. R. Scott Budinger",
      "Anthony J. Esposito",
      "Alexander Misharin",
      "Alok Choudhary",
      "Ankit Agrawal",
      "Ulas Bagci"
    ],
    "abstract": "Mixture-of-Experts (MoE) architectures have significantly contributed to scalable machine learning by enabling specialized subnetworks to tackle complex tasks efficiently. However, traditional MoE systems lack domain-specific constraints essential for medical imaging, where anatomical structure and regional disease heterogeneity strongly influence pathological patterns. Here, we introduce \\textit{Regional Expert Networks (REN)}, the first anatomically-informed MoE framework tailored specifically for medical image classification. REN leverages anatomical priors to train seven specialized experts, each dedicated to distinct lung lobes and bilateral lung combinations, enabling precise modeling of region-specific pathological variations. Multi-modal gating mechanisms dynamically integrate radiomics biomarkers and deep learning (DL) features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to interstitial lung disease (ILD) classification, REN achieves consistently superior performance: the radiomics-guided ensemble reached an average AUC of 0.8646 +- 0.0467, a +12.5\\% improvement over the SwinUNETR baseline (AUC 0.7685, p=0.031). Region-specific experts further revealed that lower-lobe models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79) and aligning with known disease progression patterns. Through rigorous patient-level cross-validation, REN demonstrates strong generalizability and clinical interpretability, presenting a scalable, anatomically-guided approach readily extensible to other structured medical imaging applications. Code is available on our GitHub: https://github.com/NUBagciLab/MoE-REN.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 4 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.04923v2",
    "published_date": "2025-10-06 15:35:08 UTC",
    "updated_date": "2026-01-11 14:41:18 UTC"
  },
  {
    "arxiv_id": "2510.04919v1",
    "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
    "authors": [
      "Davood Rafiei",
      "Morgan Lindsay Heisler",
      "Weiwei Zhang",
      "Mohammadreza Pourreza",
      "Yong Zhang"
    ],
    "abstract": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large Language Models (LLMs) on downstream tasks. However, variability in training data can hinder a model's ability to generalize across domains. This paper studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or text to SQL), examining how well SFT training data matches the structural characteristics of target queries and how this alignment impacts model performance. We hypothesize that alignment can be accurately estimated by comparing the distributions of structural SQL features across the training set, target data, and the model's predictions prior to SFT. Through comprehensive experiments on three large cross-domain NL2SQL benchmarks and multiple model families, we show that structural alignment is a strong predictor of fine-tuning success. When alignment is high, SFT yields substantial gains in accuracy and SQL generation quality; when alignment is low, improvements are marginal or absent. These findings highlight the importance of alignment-aware data selection for effective fine-tuning and generalization in NL2SQL tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04919v1",
    "published_date": "2025-10-06 15:33:35 UTC",
    "updated_date": "2025-10-06 15:33:35 UTC"
  },
  {
    "arxiv_id": "2510.04910v1",
    "title": "Glocal Information Bottleneck for Time Series Imputation",
    "authors": [
      "Jie Yang",
      "Kexin Zhang",
      "Guibin Zhang",
      "Philip S. Yu",
      "Kaize Ding"
    ],
    "abstract": "Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. Existing models typically optimize the point-wise reconstruction loss, focusing on recovering numerical values (local information). However, we observe that under high missing rates, these models still perform well in the training phase yet produce poor imputations and distorted latent representation distributions (global information) in the inference phase. This reveals a critical optimization dilemma: current objectives lack global guidance, leading models to overfit local noise and fail to capture global information of the data. To address this issue, we propose a new training paradigm, Glocal Information Bottleneck (Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework by introducing a Global Alignment loss, derived from a tractable mutual information approximation. This loss aligns the latent representations of masked inputs with those of their originally observed counterparts. It helps the model retain global structure and local details while suppressing noise caused by missing values, giving rise to better generalization under high missingness. Extensive experiments on nine datasets confirm that Glocal-IB leads to consistently improved performance and aligned latent representations under missingness. Our code implementation is available in https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04910v1",
    "published_date": "2025-10-06 15:24:44 UTC",
    "updated_date": "2025-10-06 15:24:44 UTC"
  },
  {
    "arxiv_id": "2510.04901v1",
    "title": "Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects",
    "authors": [
      "Jonathan Colaço Carr",
      "Qinyi Sun",
      "Cameron Allen"
    ],
    "abstract": "Skills are essential for unlocking higher levels of problem solving. A common approach to discovering these skills is to learn ones that reliably reach different states, thus empowering the agent to control its environment. However, existing skill discovery algorithms often overlook the natural state variables present in many reinforcement learning problems, meaning that the discovered skills lack control of specific state variables. This can significantly hamper exploration efficiency, make skills more challenging to learn with, and lead to negative side effects in downstream tasks when the goal is under-specified. We introduce a general method that enables these skill discovery algorithms to learn focused skills -- skills that target and control specific state variables. Our approach improves state space coverage by a factor of three, unlocks new learning capabilities, and automatically avoids negative side effects in downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Reinforcement Learning Journal 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04901v1",
    "published_date": "2025-10-06 15:17:46 UTC",
    "updated_date": "2025-10-06 15:17:46 UTC"
  },
  {
    "arxiv_id": "2510.04899v1",
    "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding",
    "authors": [
      "Keane Ong",
      "Wei Dai",
      "Carol Li",
      "Dewei Feng",
      "Hengzhi Li",
      "Jingyao Wu",
      "Jiaee Cheong",
      "Rui Mao",
      "Gianmarco Mengaldo",
      "Erik Cambria",
      "Paul Pu Liang"
    ],
    "abstract": "Using intelligent systems to perceive psychological and social behaviors, that is, the underlying affective, cognitive, and pathological states that are manifested through observable behaviors and social interactions, remains a challenge due to their complex, multifaceted, and personalized nature. Existing work tackling these dimensions through specialized datasets and single-task systems often miss opportunities for scalability, cross-task transfer, and broader generalization. To address this gap, we curate Human Behavior Atlas, a unified benchmark of diverse behavioral tasks designed to support the development of unified models for understanding psychological and social behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text, audio, and visual modalities, covering tasks on affective states, cognitive states, pathologies, and social processes. Our unification efforts can reduce redundancy and cost, enable training to scale efficiently across tasks, and enhance generalization of behavioral features across domains. On Human Behavior Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models to consistently outperform existing multimodal LLMs across diverse behavioral tasks. Pretraining on Human Behavior Atlas also improves transfer to novel behavioral datasets; with the targeted use of behavioral descriptors yielding meaningful performance gains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04899v1",
    "published_date": "2025-10-06 15:16:45 UTC",
    "updated_date": "2025-10-06 15:16:45 UTC"
  },
  {
    "arxiv_id": "2510.04898v1",
    "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
    "authors": [
      "Zheng Xiong",
      "Kang Li",
      "Zilin Wang",
      "Matthew Jackson",
      "Jakob Foerster",
      "Shimon Whiteson"
    ],
    "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04898v1",
    "published_date": "2025-10-06 15:15:38 UTC",
    "updated_date": "2025-10-06 15:15:38 UTC"
  },
  {
    "arxiv_id": "2510.04891v1",
    "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests",
    "authors": [
      "Punya Syon Pandey",
      "Hai Son Le",
      "Devansh Bhardwaj",
      "Rada Mihalcea",
      "Zhijing Jin"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in contexts where their failures can have direct sociopolitical consequences. Yet, existing safety benchmarks rarely test vulnerabilities in domains such as political manipulation, propaganda and disinformation generation, or surveillance and information control. We introduce SocialHarmBench, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries, designed to surface where LLMs most acutely fail in politically charged contexts. Our evaluations reveal several shortcomings: open-weight models exhibit high vulnerability to harmful compliance, with Mistral-7B reaching attack success rates as high as 97% to 98% in domains such as historical revisionism, propaganda, and political manipulation. Moreover, temporal and geographic analyses show that LLMs are most fragile when confronted with 21st-century or pre-20th-century contexts, and when responding to prompts tied to regions such as Latin America, the USA, and the UK. These findings demonstrate that current safeguards fail to generalize to high-stakes sociopolitical settings, exposing systematic biases and raising concerns about the reliability of LLMs in preserving human rights and democratic values. We share the SocialHarmBench benchmark at https://huggingface.co/datasets/psyonp/SocialHarmBench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04891v1",
    "published_date": "2025-10-06 15:11:46 UTC",
    "updated_date": "2025-10-06 15:11:46 UTC"
  },
  {
    "arxiv_id": "2510.04888v2",
    "title": "Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models",
    "authors": [
      "Alina Ermilova",
      "Dmitrii Kornilov",
      "Sofia Samoilova",
      "Ekaterina Laptenkova",
      "Anastasia Kolesnikova",
      "Ekaterina Podplutova",
      "Senotrusova Sofya",
      "Maksim G. Sharaev"
    ],
    "abstract": "Identifying disease interconnections through manual analysis of large-scale clinical data is labor-intensive, subjective, and prone to expert disagreement. While machine learning (ML) shows promise, three critical challenges remain: (1) selecting optimal methods from the vast ML landscape, (2) determining whether real-world clinical data (e.g., electronic health records, EHRs) or structured disease descriptions yield more reliable insights, (3) the lack of \"ground truth,\" as some disease interconnections remain unexplored in medicine. Large language models (LLMs) demonstrate broad utility, yet they often lack specialized medical knowledge. To address these gaps, we conduct a systematic evaluation of seven approaches for uncovering disease relationships based on two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the full set of ICD-10 codes, both with and without textual descriptions. Our framework integrates the following: (i) a statistical co-occurrence analysis and a masked language modeling (MLM) approach using real clinical data; (ii) domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral, DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained interconnection matrices shows that the LLM-based approach produces interconnections with the lowest diversity of ICD code connections to different diseases compared to other methods, including text-based and domain-based approaches. This suggests an important implication: LLMs have limited potential for discovering new interconnections. In the absence of ground truth databases for medical interconnections between ICD codes, our results constitute a valuable medical disease ontology that can serve as a foundational resource for future clinical research and artificial intelligence applications in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04888v2",
    "published_date": "2025-10-06 15:09:39 UTC",
    "updated_date": "2025-10-09 19:46:38 UTC"
  },
  {
    "arxiv_id": "2510.06275v1",
    "title": "Reproducibility Study of \"XRec: Large Language Models for Explainable Recommendation\"",
    "authors": [
      "Ranjan Mishra",
      "Julian I. Bibo",
      "Quinten van Engelen",
      "Henk Schaapman"
    ],
    "abstract": "In this study, we reproduced the work done in the paper \"XRec: Large Language Models for Explainable Recommendation\" by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec's Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at https://github.com/julianbibo/xrec-reproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06275v1",
    "published_date": "2025-10-06 15:07:32 UTC",
    "updated_date": "2025-10-06 15:07:32 UTC"
  },
  {
    "arxiv_id": "2510.04886v2",
    "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution",
    "authors": [
      "Adi Banerjee",
      "Anirudh Nair",
      "Tarik Borogovac"
    ],
    "abstract": "Error attribution in Large Language Model (LLM) multi-agent systems presents a significant challenge in debugging and improving collaborative AI systems. Current approaches to pinpointing agent and step level failures in interaction traces - whether using all-at-once evaluation, step-by-step analysis, or binary search - fall short when analyzing complex patterns, struggling with both accuracy and consistency. We present ECHO (Error attribution through Contextual Hierarchy and Objective consensus analysis), a novel algorithm that combines hierarchical context representation, objective analysis-based evaluation, and consensus voting to improve error attribution accuracy. Our approach leverages a positional-based leveling of contextual understanding while maintaining objective evaluation criteria, ultimately reaching conclusions through a consensus mechanism. Experimental results demonstrate that ECHO outperforms existing methods across various multi-agent interaction scenarios, showing particular strength in cases involving subtle reasoning errors and complex interdependencies. Our findings suggest that leveraging these concepts of structured, hierarchical context representation combined with consensus-based objective decision-making, provides a more robust framework for error attribution in multi-agent systems.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04886v2",
    "published_date": "2025-10-06 15:07:13 UTC",
    "updated_date": "2025-10-16 18:25:19 UTC"
  },
  {
    "arxiv_id": "2510.04871v1",
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "authors": [
      "Alexia Jolicoeur-Martineau"
    ],
    "abstract": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04871v1",
    "published_date": "2025-10-06 14:58:08 UTC",
    "updated_date": "2025-10-06 14:58:08 UTC"
  },
  {
    "arxiv_id": "2510.04868v1",
    "title": "Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing",
    "authors": [
      "Seyed Soroush Karimi Madahi",
      "Kenneth Bruninx",
      "Bert Claessens",
      "Chris Develder"
    ],
    "abstract": "In Europe, profit-seeking balance responsible parties can deviate in real time from their day-ahead nominations to assist transmission system operators in maintaining the supply-demand balance. Model predictive control (MPC) strategies to exploit these implicit balancing strategies capture arbitrage opportunities, but fail to accurately capture the price-formation process in the European imbalance markets and face high computational costs. Model-free reinforcement learning (RL) methods are fast to execute, but require data-intensive training and usually rely on real-time and historical data for decision-making. This paper proposes an MPC-guided RL method that combines the complementary strengths of both MPC and RL. The proposed method can effectively incorporate forecasts into the decision-making process (as in MPC), while maintaining the fast inference capability of RL. The performance of the proposed method is evaluated on the implicit balancing battery control problem using Belgian balancing data from 2023. First, we analyze the performance of the standalone state-of-the-art RL and MPC methods from various angles, to highlight their individual strengths and limitations. Next, we show an arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and 54.36%, compared to standalone RL and MPC.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04868v1",
    "published_date": "2025-10-06 14:52:27 UTC",
    "updated_date": "2025-10-06 14:52:27 UTC"
  },
  {
    "arxiv_id": "2510.04862v1",
    "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem",
    "authors": [
      "Sam Earle",
      "Zehua Jiang",
      "Eugene Vinitsky",
      "Julian Togelius"
    ],
    "abstract": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a method for training controllable level designer agents without the need for human datasets, using metrics that serve as proxies for level quality as rewards. Existing PCGRL research focuses on single generator agents, but are bottlenecked by the need to frequently recalculate heuristics of level quality and the agent's need to navigate around potentially large maps. By framing level generation as a multi-agent problem, we mitigate the efficiency bottleneck of single-agent PCGRL by reducing the number of reward calculations relative to the number of agent actions. We also find that multi-agent level generators are better able to generalize to out-of-distribution map shapes, which we argue is due to the generators' learning more local, modular design policies. We conclude that treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 7 tables, 5 figures, published as full technical paper at the AAAI conference on Artificial Intelligence and Interactive Digital Entertainment 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04862v1",
    "published_date": "2025-10-06 14:49:21 UTC",
    "updated_date": "2025-10-06 14:49:21 UTC"
  },
  {
    "arxiv_id": "2510.04860v1",
    "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails",
    "authors": [
      "Siwei Han",
      "Jiaqi Liu",
      "Yaofeng Su",
      "Wenbo Duan",
      "Xinyuan Liu",
      "Cihang Xie",
      "Mohit Bansal",
      "Mingyu Ding",
      "Linjun Zhang",
      "Huaxiu Yao"
    ],
    "abstract": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04860v1",
    "published_date": "2025-10-06 14:48:39 UTC",
    "updated_date": "2025-10-06 14:48:39 UTC"
  },
  {
    "arxiv_id": "2510.04852v2",
    "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration",
    "authors": [
      "Victor May",
      "Diganta Misra",
      "Yanqi Luo",
      "Anjali Sridhar",
      "Justine Gehring",
      "Silvio Soares Ribeiro Junior"
    ],
    "abstract": "AI coding assistants are rapidly becoming integral to modern software development. A key challenge in this space is the continual need to migrate and modernize codebases in response to evolving software ecosystems. Traditionally, such migrations have relied on rule-based systems and human intervention. With the advent of powerful large language models (LLMs), AI-driven agentic frameworks offer a promising alternative-but their effectiveness has not been systematically evaluated. In this paper, we introduce FreshBrew, a novel benchmark for evaluating AI agents on project-level Java migrations, with a specific focus on measuring an agent's ability to preserve program semantics and avoid reward hacking, which we argue requires projects with high test coverage for a rigorous and reliable evaluation. We benchmark several state-of-the-art LLMs, and compare their performance against established rule-based tools. Our evaluation of AI agents on this benchmark of 228 repositories shows that the top-performing model, Gemini 2.5 Flash, can successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis reveals novel insights into the critical strengths and limitations of current agentic approaches, offering actionable insights into their real-world applicability. Our empirical study reveals failure modes of current AI agents in realistic Java modernization tasks, providing a foundation for evaluating trustworthy code-migration systems. By releasing FreshBrew, we aim to facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven codebase modernization.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "18 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04852v2",
    "published_date": "2025-10-06 14:39:58 UTC",
    "updated_date": "2025-10-13 00:55:18 UTC"
  },
  {
    "arxiv_id": "2510.04851v1",
    "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation",
    "authors": [
      "Dongge Han",
      "Camille Couturier",
      "Daniel Madrigal Diaz",
      "Xuchao Zhang",
      "Victor Rühle",
      "Saravan Rajmohan"
    ],
    "abstract": "We introduce LEGOMem, a modular procedural memory framework for multi-agent large language model (LLM) systems in workflow automation. LEGOMem decomposes past task trajectories into reusable memory units and flexibly allocates them across orchestrators and task agents to support planning and execution. To explore the design space of memory in multi-agent systems, we use LEGOMem as a lens and conduct a systematic study of procedural memory in multi-agent systems, examining where memory should be placed, how it should be retrieved, and which agents benefit most. Experiments on the OfficeBench benchmark show that orchestrator memory is critical for effective task decomposition and delegation, while fine-grained agent memory improves execution accuracy. We find that even teams composed of smaller language models can benefit substantially from procedural memory, narrowing the performance gap with stronger agents by leveraging prior execution traces for more accurate planning and tool use. These results position LEGOMem as both a practical framework for memory-augmented agent systems and a research tool for understanding memory design in multi-agent workflow automation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04851v1",
    "published_date": "2025-10-06 14:39:53 UTC",
    "updated_date": "2025-10-06 14:39:53 UTC"
  },
  {
    "arxiv_id": "2510.04850v2",
    "title": "Detecting Distillation Data from Reasoning Models",
    "authors": [
      "Hengxiang Zhang",
      "Hyeong Kyu Choi",
      "Sharon Li",
      "Hongxin Wei"
    ],
    "abstract": "Reasoning distillation has emerged as an efficient and powerful paradigm for enhancing the reasoning capabilities of large language models. However, reasoning distillation may inadvertently cause benchmark contamination, where evaluation data included in distillation datasets can inflate performance metrics of distilled models. In this work, we formally define the task of distillation data detection, which is uniquely challenging due to the partial availability of distillation data. Then, we propose a novel and effective method Token Probability Deviation (TBD), which leverages the probability patterns of the generated output tokens. Our method is motivated by the analysis that distilled models tend to generate near-deterministic tokens for seen questions, while producing more low-probability tokens for unseen questions. Our key idea behind TBD is to quantify how far the generated tokens' probabilities deviate from a high reference probability. In effect, our method achieves competitive detection performance by producing lower scores for seen questions than for unseen questions. Extensive experiments demonstrate the effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of 0.470 on the S1 dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04850v2",
    "published_date": "2025-10-06 14:37:02 UTC",
    "updated_date": "2025-10-15 08:23:27 UTC"
  },
  {
    "arxiv_id": "2510.04842v3",
    "title": "Distributionally Robust Causal Abstractions",
    "authors": [
      "Yorgos Felekis",
      "Theodoros Damoulas",
      "Paris Giampouras"
    ],
    "abstract": "Causal Abstraction (CA) theory provides a principled framework for relating causal models that describe the same system at different levels of granularity while ensuring interventional consistency between them. Recent methods for learning CAs, however, assume fixed and well-specified exogenous distributions, leaving them vulnerable to environmental shifts and model misspecification. In this work, we address these limitations by introducing the first class of distributionally robust CAs and their associated learning algorithms. The latter cast robust causal abstraction learning as a constrained min-max optimization problem with Wasserstein ambiguity sets. We provide theoretical guarantees for both empirical and Gaussian environments, enabling principled selection of ambiguity set radii and establish quantitative guarantees on worst-case abstraction error. Furthermore, we present empirical evidence across different problems and CA learning methods, demonstrating our framework's robustness not only to environmental shifts but also to structural and intervention mapping misspecification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04842v3",
    "published_date": "2025-10-06 14:26:12 UTC",
    "updated_date": "2026-01-21 21:24:30 UTC"
  },
  {
    "arxiv_id": "2510.04837v1",
    "title": "Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study",
    "authors": [
      "Guillaume Godin"
    ],
    "abstract": "Bond Centered FingerPrint (BCFP) are a complementary, bond-centric alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static BCFP that mirrors the bond-convolution used by directed message-passing GNNs like ChemProp, and evaluate it with a fast rapid Random Forest model on Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified cross-validation, concatenating ECFP with BCFP consistently improves AUROC and AUPRC over either descriptor alone, as confirmed by Turkey HSD multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not yield statistically separable gains under the same test. We further propose BCFP-Sort&Slice, a simple feature-combination scheme that preserves the out-of-vocabulary (OOV) count information native to ECFP count vectors while enabling compact unhashed concatenation of BCFP variants. We also outperform the MGTP prediction on our BBBP evaluation, using such composite new features bond and atom features. These results show that lightweight, bond-centered descriptors can complement atom-centered circular fingerprints and provide strong, fast baselines for BBBP prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 10 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2510.04837v1",
    "published_date": "2025-10-06 14:22:23 UTC",
    "updated_date": "2025-10-06 14:22:23 UTC"
  },
  {
    "arxiv_id": "2510.04817v1",
    "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning",
    "authors": [
      "Abhinav Madahar"
    ],
    "abstract": "Controllers for structured LM reasoning (e.g., Chain-of-Thought, self-consistency, and Tree-of-Thoughts) often entangle what to try next with how to execute it, exposing only coarse global knobs and yielding brittle, compute-inefficient, and hard-to-audit behavior. We introduce Natural Language Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form natural-language directive to each search edge and translates it into a schema-bounded control vector for decoding, search (branch quotas, exploration $β$), generation bundle size, retrieval mixtures, and verification passes. A labeller $Λ$ emits labels from the parent state and a compact context; a tuner $Ψ$ maps $(P, L, C)\\to Π$, with strict schema validation and trust-region projection around safe defaults. Downstream selection remains ToT-style with score $S=μ+βσ$ and depth-annealed $β$. We show NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for top-$k$ selection under label-conditioned bundles, and bound selector shortfall by control-vector distortion, providing decision-relevant justification for guards like trust regions and verification passes. We instantiate $Ψ$ as a prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH (subset), StrategyQA, and ARC-Challenge with compute-aware reporting (success@compute, tokens-per-success) and ablations over $Λ$, $Ψ$, trust-region radius, and control quantization; preregistered forecasts anticipate accuracy gains at comparable token budgets and improved success@compute under constraints. NLEL offers an interpretable, model-agnostic interface that separates intent from execution for controllable, auditable LM inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04817v1",
    "published_date": "2025-10-06 14:00:02 UTC",
    "updated_date": "2025-10-06 14:00:02 UTC"
  },
  {
    "arxiv_id": "2510.04816v1",
    "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference",
    "authors": [
      "Junhyung Ahn",
      "Sanghack Lee"
    ],
    "abstract": "Accurately predicting conversion rate (CVR) is essential in various recommendation domains such as online advertising systems and e-commerce. These systems utilize user interaction logs, which consist of exposures, clicks, and conversions. CVR prediction models are typically trained solely based on clicked samples, as conversions can only be determined following clicks. However, the sparsity of clicked instances necessitates the collection of a substantial amount of logs for effective model training. Recent works address this issue by devising frameworks that leverage non-clicked samples. While these frameworks aim to reduce biases caused by the discrepancy between clicked and non-clicked samples, they often rely on heuristics. Against this background, we propose a method to counterfactually generate conversion labels for non-clicked samples by using causality as a guiding principle, attempting to answer the question, \"Would the user have converted if he or she had clicked the recommended item?\" Our approach is named the Entire Space Counterfactual Inference Multi-task Model (ESCIM). We initially train a structural causal model (SCM) of user sequential behaviors and conduct a hypothetical intervention (i.e., click) on non-clicked items to infer counterfactual CVRs. We then introduce several approaches to transform predicted counterfactual CVRs into binary counterfactual conversion labels for the non-clicked samples. Finally, the generated samples are incorporated into the training process. Extensive experiments on public datasets illustrate the superiority of the proposed algorithm. Online A/B testing further empirically validates the effectiveness of our proposed algorithm in real-world scenarios. In addition, we demonstrate the improved performance of the proposed method on latent conversion data, showcasing its robustness and superior generalization capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted for publication at the IEEE International Conference on Data Mining (ICDM) 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04816v1",
    "published_date": "2025-10-06 13:57:49 UTC",
    "updated_date": "2025-10-06 13:57:49 UTC"
  },
  {
    "arxiv_id": "2510.05192v1",
    "title": "Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study",
    "authors": [
      "Francesca Gomez"
    ],
    "abstract": "Agentic misalignment occurs when goal-directed agents take harmful actions, such as blackmail, rather than risk goal failure, and can be triggered by replacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025). We adapt insider-risk control design (Critical Pathway; Situational Crime Prevention) to develop preventative operational controls that steer agents toward safe actions when facing stressors. Using the blackmail scenario from the original Anthropic study by Lynch et al. (2025), we evaluate mitigations across 10 LLMs and 66,600 samples. Our main finding is that an externally governed escalation channel, which guarantees a pause and independent review, reduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21% (averaged across all models and conditions). Augmenting this channel with compliance email bulletins further lowers the blackmail rate to 0.85%. Overall, incorporating preventative operational controls strengthens defence-in-depth strategies for agentic AI.\n  We also surface a failure mode diverging from Lynch et al. (2025): two models (Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent autonomy threat, leveraging sensitive information for coercive signalling. In counterfactual swaps, both continued using the affair regardless of whether the CEO or CTO was implicated. An escalation channel eliminated coercion, but Gemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was implicated, unlike most models (higher in the CEO condition). The reason for this divergent behaviour is not clear from raw outputs and could reflect benign differences in reasoning or strategic discrediting of a potential future threat, warranting further investigation.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.05192v1",
    "published_date": "2025-10-06 13:37:33 UTC",
    "updated_date": "2025-10-06 13:37:33 UTC"
  },
  {
    "arxiv_id": "2510.04802v1",
    "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors",
    "authors": [
      "Han Zhang",
      "Lalithkumar Seenivasan",
      "Jose L. Porras",
      "Roger D. Soberanis-Mukul",
      "Hao Ding",
      "Hongchao Shu",
      "Benjamin D. Killeen",
      "Ankita Ghosh",
      "Lonny Yarmus",
      "Masaru Ishii",
      "Angela Christine Argento",
      "Mathias Unberath"
    ],
    "abstract": "Observing surgical practice has historically relied on fixed vantage points or recollections, leaving the egocentric visual perspectives that guide clinical decisions undocumented. Fixed-camera video can capture surgical workflows at the room-scale, but cannot reconstruct what each team member actually saw. Thus, these videos only provide limited insights into how decisions that affect surgical safety, training, and workflow optimization are made. Here we introduce EgoSurg, the first framework to reconstruct the dynamic, egocentric replays for any operating room (OR) staff directly from wall-mounted fixed-camera video, and thus, without intervention to clinical workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based view enhancement, enabling high-visual fidelity synthesis of arbitrary and egocentric viewpoints at any moment. In evaluation across multi-site surgical cases and controlled studies, EgoSurg reconstructs person-specific visual fields and arbitrary viewpoints with high visual quality and fidelity. By transforming existing OR camera infrastructure into a navigable dynamic 3D record, EgoSurg establishes a new foundation for immersive surgical data science, enabling surgical practice to be visualized, experienced, and analyzed from every angle.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04802v1",
    "published_date": "2025-10-06 13:35:51 UTC",
    "updated_date": "2025-10-06 13:35:51 UTC"
  },
  {
    "arxiv_id": "2510.04792v1",
    "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems",
    "authors": [
      "Ni Zhang",
      "Zhiguang Cao"
    ],
    "abstract": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically employ Trajectory Balance (TB) to achieve global optimization but often neglect important aspects of local optimization. While Detailed Balance (DB) addresses local optimization more effectively, it alone falls short in solving VRPs, which inherently require holistic trajectory optimization. To address these limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which uniquely integrates TB and DB in a principled and adaptive manner by aligning their intrinsically complementary strengths. Additionally, we propose a specialized inference strategy for depot-centric scenarios like the Capacitated Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility in selecting successors. Despite this specialization, HBG maintains broad applicability, extending effectively to problems without explicit depots, such as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate consistent and significant improvements across both CVRP and TSP, underscoring the enhanced solution quality and generalization afforded by our approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04792v1",
    "published_date": "2025-10-06 13:16:01 UTC",
    "updated_date": "2025-10-06 13:16:01 UTC"
  },
  {
    "arxiv_id": "2510.04787v1",
    "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading",
    "authors": [
      "Zifan Song",
      "Kaitao Song",
      "Guosheng Hu",
      "Ding Qi",
      "Junyao Gao",
      "Xiaohua Wang",
      "Dongsheng Li",
      "Cairong Zhao"
    ],
    "abstract": "Recent advancements in large language models (LLMs) and agentic systems have shown exceptional decision-making capabilities, revealing significant potential for autonomic finance. Current financial trading agents predominantly simulate anthropomorphic roles that inadvertently introduce emotional biases and rely on peripheral information, while being constrained by the necessity for continuous inference during deployment. In this paper, we pioneer the harmonization of strategic depth in agents with the mechanical rationality essential for quantitative trading. Consequently, we present TiMi (Trade in Minutes), a rationality-driven multi-agent system that architecturally decouples strategy development from minute-level deployment. TiMi leverages specialized LLM capabilities of semantic analysis, code programming, and mathematical reasoning within a comprehensive policy-optimization-deployment chain. Specifically, we propose a two-tier analytical paradigm from macro patterns to micro customization, layered programming design for trading bot implementation, and closed-loop optimization driven by mathematical reflection. Extensive evaluations across 200+ trading pairs in stock and cryptocurrency markets empirically validate the efficacy of TiMi in stable profitability, action efficiency, and risk control under volatile market dynamics.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "16 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04787v1",
    "published_date": "2025-10-06 13:08:55 UTC",
    "updated_date": "2025-10-06 13:08:55 UTC"
  },
  {
    "arxiv_id": "2510.06274v1",
    "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization",
    "authors": [
      "Mohammad Mahdi Samiei Paqaleh",
      "Arash Marioriyad",
      "Arman Tahmasebi-Zadeh",
      "Mohamadreza Fereydooni",
      "Mahdi Ghaznavai",
      "Mahdieh Soleymani Baghshah"
    ],
    "abstract": "Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06274v1",
    "published_date": "2025-10-06 13:08:31 UTC",
    "updated_date": "2025-10-06 13:08:31 UTC"
  },
  {
    "arxiv_id": "2510.04786v1",
    "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning",
    "authors": [
      "Jonas Hübotter",
      "Leander Diaz-Bone",
      "Ido Hakimi",
      "Andreas Krause",
      "Moritz Hardt"
    ],
    "abstract": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04786v1",
    "published_date": "2025-10-06 13:07:14 UTC",
    "updated_date": "2025-10-06 13:07:14 UTC"
  },
  {
    "arxiv_id": "2510.04774v2",
    "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy",
    "authors": [
      "Weixu Zhu",
      "Marco Dorigo",
      "Mary Katherine Heinrich"
    ],
    "abstract": "Our recently introduced self-organizing nervous system (SoNS) provides robot swarms with 1) ease of behavior design and 2) global estimation of the swarm configuration and its collective environment, facilitating the implementation of online automatic code generation for robot swarms. In a demonstration with 6 real robots and simulation trials with >30 robots, we show that when a SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code generated by an external LLM on the fly, completing its mission with an 85% success rate.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "This abstract was accepted to and presented at the \"Multi-Agent Cooperative Systems and Swarm Robotics in the Era of Generative AI\" (MACRAI) workshop at the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.04774v2",
    "published_date": "2025-10-06 12:49:36 UTC",
    "updated_date": "2025-10-19 16:00:51 UTC"
  },
  {
    "arxiv_id": "2510.04773v1",
    "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning",
    "authors": [
      "Kai Qin",
      "Jiaqi Wu",
      "Jianxiang He",
      "Haoyuan Sun",
      "Yifei Zhao",
      "Bin Liang",
      "Yongzhe Chang",
      "Tiantian Zhang",
      "Houde Liu"
    ],
    "abstract": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned from vast corpora, concerns regarding data privacy and safety are receiving increasing attention. LLM unlearning, which aims to remove the influence of specific data while preserving overall model utility, is becoming an important research area. One of the mainstream unlearning classes is optimization-based methods, which achieve forgetting directly through fine-tuning, exemplified by Negative Preference Optimization (NPO). However, NPO's effectiveness is limited by its inherent lack of explicit positive preference signals. Attempts to introduce such signals by constructing preferred responses often necessitate domain-specific knowledge or well-designed prompts, fundamentally restricting their generalizability. In this paper, we shift the focus to the distribution-level, directly targeting the next-token probability distribution instead of entire responses, and derive a novel unlearning algorithm termed \\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show that the requisite preference distribution pairs for DiPO, which are distributions over the model's output tokens, can be constructed by selectively amplifying or suppressing the model's high-confidence output logits, thereby effectively overcoming NPO's limitations. We theoretically prove the consistency of DiPO's loss function with the desired unlearning direction. Extensive experiments demonstrate that DiPO achieves a strong trade-off between model utility and forget quality. Notably, DiPO attains the highest forget quality on the TOFU benchmark, and maintains leading scalability and sustainability in utility preservation on the MUSE benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.04773v1",
    "published_date": "2025-10-06 12:49:00 UTC",
    "updated_date": "2025-10-06 12:49:00 UTC"
  },
  {
    "arxiv_id": "2510.04769v1",
    "title": "When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates",
    "authors": [
      "Michele Caprio",
      "Siu Lun Chau",
      "Krikamol Muandet"
    ],
    "abstract": "Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04769v1",
    "published_date": "2025-10-06 12:42:32 UTC",
    "updated_date": "2025-10-06 12:42:32 UTC"
  },
  {
    "arxiv_id": "2510.04765v1",
    "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0",
    "authors": [
      "Jinbo Wen",
      "Jiawen Kang",
      "Linfeng Zhang",
      "Xiaoying Tang",
      "Jianhang Tang",
      "Yang Zhang",
      "Zhaohui Yang",
      "Dusit Niyato"
    ],
    "abstract": "Web 3.0 represents the next generation of the Internet, which is widely recognized as a decentralized ecosystem that focuses on value expression and data ownership. By leveraging blockchain and artificial intelligence technologies, Web 3.0 offers unprecedented opportunities for users to create, own, and monetize their content, thereby enabling User-Generated Content (UGC) to an entirely new level. However, some self-interested users may exploit the limitations of content curation mechanisms and generate low-quality content with less effort, obtaining platform rewards under information asymmetry. Such behavior can undermine Web 3.0 performance. To this end, we propose \\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based contract-theoretic model to motivate users to generate high-quality UGC, thereby mitigating the adverse selection problem from information asymmetry. To alleviate potential moral hazards after contract selection, we leverage LMM agents to evaluate UGC quality, which is the primary component of the contract, utilizing prompt engineering techniques to improve the evaluation performance of LMM agents. Recognizing that traditional contract design methods cannot effectively adapt to the dynamic environment of Web 3.0, we develop an improved Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for optimal contract design. Simulation results demonstrate the superiority of the proposed MoE-based PPO algorithm over representative benchmarks in the context of contract design. Finally, we deploy the designed contract within an Ethereum smart contract framework, further validating the effectiveness of the proposed scheme.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04765v1",
    "published_date": "2025-10-06 12:39:29 UTC",
    "updated_date": "2025-10-06 12:39:29 UTC"
  },
  {
    "arxiv_id": "2510.04762v1",
    "title": "Fisher-Bingham-like normalizing flows on the sphere",
    "authors": [
      "Thorsten Glüsenkamp"
    ],
    "abstract": "A generic D-dimensional Gaussian can be conditioned or projected onto the D-1 unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular Gaussian (AG) distribution families, respectively. These are some of the most fundamental distributions on the sphere, yet cannot straightforwardly be written as a normalizing flow except in two special cases: the von-Mises Fisher in D=3 and the central angular Gaussian in any D. In this paper, we describe how to generalize these special cases to a family of normalizing flows that behave similarly to the full FB or AG family in any D. We call them \"zoom-linear-project\" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham distribution, their composition allows to gradually add complexity as needed. Furthermore, they can naturally handle conditional density estimation with target distributions that vary by orders of magnitude in scale - a setting that is important in astronomical applications but that existing flows often struggle with. A particularly useful member of the new family is the Kent analogue that can cheaply upgrade any flow in this situation to yield better performance.",
    "categories": [
      "stat.ML",
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04762v1",
    "published_date": "2025-10-06 12:38:28 UTC",
    "updated_date": "2025-10-06 12:38:28 UTC"
  },
  {
    "arxiv_id": "2510.04760v1",
    "title": "Agile Software Effort Estimation using Regression Techniques",
    "authors": [
      "Sisay Deresa Sima",
      "Ayalew Belay Habtie"
    ],
    "abstract": "Software development effort estimation is one of the most critical aspect in software development process, as the success or failure of the entire project depends on the accuracy of estimations. Researchers are still conducting studies on agile effort estimation. The aim of this research is to develop a story point based agile effort estimation model using LASSO and Elastic Net regression techniques. The experimental work is applied to the agile story point approach using 21 software projects collected from six firms. The two algorithms are trained using their default parameters and tuned grid search with 5-fold cross-validation to get an enhanced model. The experiment result shows LASSO regression achieved better predictive performance PRED (8%) and PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593, MdMER of 0.063, and MSE of 0.0007. The results are also compared with other related literature.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04760v1",
    "published_date": "2025-10-06 12:37:09 UTC",
    "updated_date": "2025-10-06 12:37:09 UTC"
  },
  {
    "arxiv_id": "2510.04759v2",
    "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction",
    "authors": [
      "Chi Yan",
      "Dan Xu"
    ],
    "abstract": "The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: https://yanchi-3dv.github.io/PG-Occ",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://yanchi-3dv.github.io/PG-Occ",
    "pdf_url": "https://arxiv.org/pdf/2510.04759v2",
    "published_date": "2025-10-06 12:36:07 UTC",
    "updated_date": "2025-10-08 09:34:48 UTC"
  },
  {
    "arxiv_id": "2510.04755v4",
    "title": "A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI",
    "authors": [
      "Jason Miklian",
      "Kristian Hoelscher"
    ],
    "abstract": "Digital technologies are transforming democratic life in conflicting ways. This article bridges two perspectives to unpack these tensions. First, we present an original survey of software developers in Silicon Valley, interrogating how coder worldviews, ethics, and workplace cultures shape the democratic potential and social impact of the technologies they build. Results indicate that while most developers recognize the power of their products to influence civil liberties and political discourse, they often face ethical dilemmas and top-down pressures that can lead to design choices undermining democratic ideals. Second, we critically investigate these findings in the context of an emerging new digital divide, not of internet access but of information quality. We interrogate the survey findings in the context of the Slop Economy, in which billions of users unable to pay for high-quality content experience an internet dominated by low-quality, AI-generated ad-driven content. We find a reinforcing cycle between tech creator beliefs and the digital ecosystems they spawn. We discuss implications for democratic governance, arguing for more ethically informed design and policy interventions to help bridge the digital divide to ensure that technological innovation supports rather than subverts democratic values in the next chapter of the digital age.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04755v4",
    "published_date": "2025-10-06 12:32:37 UTC",
    "updated_date": "2025-12-17 13:48:34 UTC"
  },
  {
    "arxiv_id": "2510.04738v1",
    "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
    "authors": [
      "Baher Mohammad",
      "Magauiya Zhussip",
      "Stamatios Lefkimmiatis"
    ],
    "abstract": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE - edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires ~6x less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04738v1",
    "published_date": "2025-10-06 12:11:31 UTC",
    "updated_date": "2025-10-06 12:11:31 UTC"
  },
  {
    "arxiv_id": "2510.05191v2",
    "title": "Provable Speech Attributes Conversion via Latent Independence",
    "authors": [
      "Jonathan Svirsky",
      "Ofir Lindenbaum",
      "Uri Shaham"
    ],
    "abstract": "While signal conversion and disentangled representation learning have shown promise for manipulating data attributes across domains such as audio, image, and multimodal generation, existing approaches, especially for speech style conversion, are largely empirical and lack rigorous theoretical foundations to guarantee reliable and interpretable control. In this work, we propose a general framework for speech attribute conversion, accompanied by theoretical analysis and guarantees under reasonable assumptions. Our framework builds on a non-probabilistic autoencoder architecture with an independence constraint between the predicted latent variable and the target controllable variable. This design ensures a consistent signal transformation, conditioned on an observed style variable, while preserving the original content and modifying the desired attribute. We further demonstrate the versatility of our method by evaluating it on speech styles, including speaker identity and emotion. Quantitative evaluations confirm the effectiveness and generality of the proposed approach.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05191v2",
    "published_date": "2025-10-06 12:08:27 UTC",
    "updated_date": "2025-10-09 08:32:27 UTC"
  },
  {
    "arxiv_id": "2510.04721v1",
    "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
    "authors": [
      "Ivo Petrov",
      "Jasper Dekoninck",
      "Martin Vechev"
    ],
    "abstract": "Large language models (LLMs) have recently shown strong performance on mathematical benchmarks. At the same time, they are prone to hallucination and sycophancy, often providing convincing but flawed proofs for incorrect mathematical statements provided by users. This significantly limits the applicability of LLMs in theorem proving, as verification of these flawed proofs must be done manually by expert mathematicians. However, existing benchmarks that measure sycophancy in mathematics are limited: they focus solely on final-answer problems, rely on very simple and often contaminated datasets, and construct benchmark samples using synthetic modifications that create ill-posed questions rather than well-posed questions that are demonstrably false. To address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review. Using an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. We further investigate several mitigation strategies, including test-time interventions and supervised fine-tuning on curated sycophantic examples. These approaches substantially reduce, but do not eliminate, sycophantic behavior.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04721v1",
    "published_date": "2025-10-06 11:41:46 UTC",
    "updated_date": "2025-10-06 11:41:46 UTC"
  },
  {
    "arxiv_id": "2510.04716v3",
    "title": "Curved Boolean Logic: A Contextual Generalization of Propositional Logic with Algorithmic Consequences",
    "authors": [
      "Maximilian R. P. von Liechtenstein"
    ],
    "abstract": "Curved Boolean Logic (CBL) generalizes propositional logic by allowing local truth assignments that do not extend to a single global valuation, analogous to curvature in geometry. We give equivalent sheaf and exclusivity-graph semantics and a context-aware proof calculus that is conservative in the flat limit. We formalize CBL-SAT and basic complexity (NP-complete in general) and present operational operators (CBL-AC and CBL-CONS) that prune contradictions earlier on classical hardware. We model noise with iid, AR(1)-correlated, and adversarial bounded perturbations and provide permutation-based significance with Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files) regenerates all figures and statistics. We position CBL relative to KCBS, CSW, and sheaf frameworks and outline links to SAT/CSP and robustness/adapter stability in large language models.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CC",
      "quant-ph"
    ],
    "primary_category": "cs.LO",
    "comment": "v3: Restores original v1 content; later additions are retracted pending a normalization audit",
    "pdf_url": "https://arxiv.org/pdf/2510.04716v3",
    "published_date": "2025-10-06 11:34:08 UTC",
    "updated_date": "2025-10-11 07:06:25 UTC"
  },
  {
    "arxiv_id": "2510.04704v3",
    "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials",
    "authors": [
      "Taoyuze Lv",
      "Alexander Chen",
      "Fengyu Xie",
      "Chu Wu",
      "Jeffrey Meng",
      "Dongzhan Zhou",
      "Yingheng Wang",
      "Bram Hoex",
      "Zhicheng Zhong",
      "Tong Xie"
    ],
    "abstract": "Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04704v3",
    "published_date": "2025-10-06 11:17:56 UTC",
    "updated_date": "2026-01-22 05:18:03 UTC"
  },
  {
    "arxiv_id": "2510.04698v2",
    "title": "The Bayesian Origin of the Probability Weighting Function in Human Representation of Probabilities",
    "authors": [
      "Xin Tong",
      "Thi Thu Uyen Hoang",
      "Xue-Xin Wei",
      "Michael Hahn"
    ],
    "abstract": "Understanding the representation of probability in the human mind has been of great interest to understanding human decision making. Classical paradoxes in decision making suggest that human perception distorts probability magnitudes. Previous accounts postulate a Probability Weighting Function that transforms perceived probabilities; however, its motivation has been debated. Recent work has sought to motivate this function in terms of noisy representations of probabilities in the human mind. Here, we present an account of the Probability Weighting Function grounded in rational inference over optimal decoding from noisy neural encoding of quantities. We show that our model accurately accounts for behavior in a lottery task and a dot counting task. It further accounts for adaptation to a bimodal short-term prior. Taken together, our results provide a unifying account grounding the human representation of probability in rational inference.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "econ.TH"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04698v2",
    "published_date": "2025-10-06 11:10:55 UTC",
    "updated_date": "2026-01-04 19:16:51 UTC"
  },
  {
    "arxiv_id": "2510.04695v1",
    "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents",
    "authors": [
      "Yiding Wang",
      "Zhepei Wei",
      "Xinyu Zhu",
      "Yu Meng"
    ],
    "abstract": "Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. Recent work has explored reinforcement learning (RL) for training search-augmented agents that interleave reasoning and retrieval before answering. These approaches usually rely on outcome-based rewards (e.g., exact match), implicitly assuming that optimizing for final answers will also yield effective intermediate search behaviors. Our analysis challenges this assumption: we uncover multiple systematic deficiencies in search that arise under outcome-only training and ultimately degrade final answer quality, including failure to invoke tools, invalid queries, and redundant searches. To address these shortcomings, we introduce DeSA (Decoupling Search-and-Answering), a simple two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards. In Stage 2, outcome rewards are employed to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents consistently improve search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. Notably, DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards, underscoring the necessity of explicitly decoupling the two objectives.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04695v1",
    "published_date": "2025-10-06 11:09:45 UTC",
    "updated_date": "2025-10-06 11:09:45 UTC"
  },
  {
    "arxiv_id": "2510.04694v1",
    "title": "Multilingual Routing in Mixture-of-Experts",
    "authors": [
      "Lucas Bandarkar",
      "Chenyuan Yang",
      "Mohsen Fayyaz",
      "Junlin Hu",
      "Nanyun Peng"
    ],
    "abstract": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04694v1",
    "published_date": "2025-10-06 11:09:20 UTC",
    "updated_date": "2025-10-06 11:09:20 UTC"
  },
  {
    "arxiv_id": "2510.04692v1",
    "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies",
    "authors": [
      "Lyes Saad Saoud",
      "Irfan Hussain"
    ],
    "abstract": "Biomimetic intelligence and robotics are transforming field ecology by enabling lifelike robotic surrogates that interact naturally with animals under real world conditions. Studying avian behavior in the wild remains challenging due to the need for highly realistic morphology, durable outdoor operation, and intelligent perception that can adapt to uncontrolled environments. We present a next generation bio inspired robotic platform that replicates the morphology and visual appearance of the female Houbara bustard to support controlled ethological studies and conservation oriented field research. The system introduces a fully digitally replicable fabrication workflow that combines high resolution structured light 3D scanning, parametric CAD modelling, articulated 3D printing, and photorealistic UV textured vinyl finishing to achieve anatomically accurate and durable robotic surrogates. A six wheeled rocker bogie chassis ensures stable mobility on sand and irregular terrain, while an embedded NVIDIA Jetson module enables real time RGB and thermal perception, lightweight YOLO based detection, and an autonomous visual servoing loop that aligns the robot's head toward detected targets without human intervention. A lightweight thermal visible fusion module enhances perception in low light conditions. Field trials in desert aviaries demonstrated reliable real time operation at 15 to 22 FPS with latency under 100 ms and confirmed that the platform elicits natural recognition and interactive responses from live Houbara bustards under harsh outdoor conditions. This integrated framework advances biomimetic field robotics by uniting reproducible digital fabrication, embodied visual intelligence, and ecological validation, providing a transferable blueprint for animal robot interaction research, conservation robotics, and public engagement.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04692v1",
    "published_date": "2025-10-06 11:05:46 UTC",
    "updated_date": "2025-10-06 11:05:46 UTC"
  },
  {
    "arxiv_id": "2510.04686v1",
    "title": "How does the optimizer implicitly bias the model merging loss landscape?",
    "authors": [
      "Chenxiang Zhang",
      "Alexander Theus",
      "Damien Teney",
      "Antonio Orvieto",
      "Jun Pang",
      "Sjouke Mauw"
    ],
    "abstract": "Model merging methods combine models with different capabilities into a single one while maintaining the same inference cost. Two popular approaches are linear interpolation, which linearly interpolates between model weights, and task arithmetic, which combines task vectors obtained by the difference between finetuned and base models. While useful in practice, what properties make merging effective are poorly understood. This paper explores how the optimization process affects the loss landscape geometry and its impact on merging success. We show that a single quantity -- the effective noise scale -- unifies the impact of optimizer and data choices on model merging. Across architectures and datasets, the effectiveness of merging success is a non-monotonic function of effective noise, with a distinct optimum. Decomposing this quantity, we find that larger learning rates, stronger weight decay, smaller batch sizes, and data augmentation all independently modulate the effective noise scale, exhibiting the same qualitative trend. Unlike prior work that connects optimizer noise to the flatness or generalization of individual minima, we show that it also affects the global loss landscape, predicting when independently trained solutions can be merged. Our findings broaden the understanding of how optimization shapes the loss landscape geometry and its downstream consequences for model merging, suggesting the possibility of further manipulating the training dynamics to improve merging effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.04686v1",
    "published_date": "2025-10-06 10:56:41 UTC",
    "updated_date": "2025-10-06 10:56:41 UTC"
  },
  {
    "arxiv_id": "2510.04682v1",
    "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA",
    "authors": [
      "Chanjoo Jung",
      "Jaehyung Kim"
    ],
    "abstract": "Large Language Models (LLMs) are widely applied in real world scenarios, but fine-tuning them comes with significant computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these costs, but the adapted parameters are dependent on the base model and cannot be transferred across different backbones. One way to address this issue is through knowledge distillation, but its effectiveness inherently depends on training data. Recent work such as TransLoRA avoids this by generating synthetic data, but this adds complexity because it requires training an additional discriminator model. In this paper, we propose TiTok, a new framework that enables effective LoRA Transplantation through Token-level knowledge transfer. Specifically, TiTok captures task-relevant information through a contrastive excess between a source model with and without LoRA. This excess highlights informative tokens and enables selective filtering of synthetic data, all without additional models or overhead. Through experiments on three benchmarks across multiple transfer settings, our experiments show that the proposed method is consistently effective, achieving average performance gains of +4~8% compared to baselines overall.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04682v1",
    "published_date": "2025-10-06 10:47:22 UTC",
    "updated_date": "2025-10-06 10:47:22 UTC"
  },
  {
    "arxiv_id": "2510.04674v1",
    "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding",
    "authors": [
      "Lorenzo Pannacci",
      "Simone Fiorellino",
      "Mario Edoardo Pandolfo",
      "Emilio Calvanese Strinati",
      "Paolo Di Lorenzo"
    ],
    "abstract": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful paradigm for end-to-end semantic communications, jointly learning to compress and protect task-relevant features over noisy channels. However, existing DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver (RX) - an assumption that fails in multi-vendor deployments where encoders and decoders cannot be co-trained. This mismatch introduces \"semantic noise\", degrading reconstruction quality and downstream task performance. In this paper, we systematize and evaluate methods for semantic channel equalization for DeepJSCC, introducing an additional processing stage that aligns heterogeneous latent spaces under both physical and semantic impairments. We investigate three classes of aligners: (i) linear maps, which admit closed-form solutions; (ii) lightweight neural networks, offering greater expressiveness; and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without the need for training. Through extensive experiments on image reconstruction over AWGN and fading channels, we quantify trade-offs among complexity, data efficiency, and fidelity, providing guidelines for deploying DeepJSCC in heterogeneous AI-native wireless networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of IEEE Globecom 2025 Workshops",
    "pdf_url": "https://arxiv.org/pdf/2510.04674v1",
    "published_date": "2025-10-06 10:29:07 UTC",
    "updated_date": "2025-10-06 10:29:07 UTC"
  },
  {
    "arxiv_id": "2510.04673v2",
    "title": "Watch and Learn: Learning to Use Computers from Online Videos",
    "authors": [
      "Chan Hee Song",
      "Yiwen Song",
      "Palash Goyal",
      "Yu Su",
      "Oriana Riva",
      "Hamid Palangi",
      "Tomas Pfister"
    ],
    "abstract": "Computer-using agents (CUAs) must plan task workflows across diverse and evolving applications, yet progress is limited by the lack of large-scale, high-quality training data. Existing datasets are narrow, static, and costly to annotate, while synthetic data often yields oversimplified or misaligned behaviors. We present Watch & Learn (W&L), a framework that converts readily available Internet videos of human computer use into executable UI trajectories at scale. Instead of directly generating actions or relying on handcrafted heuristics, we cast trajectory annotation as an inverse dynamics problem that predicts user actions from consecutive screen states, which simplifies learning and generalizes across domains. Through a task-aware retrieval and labeling pipeline, W&L yields over 53K high-quality trajectories that enhance CUAs both as in-context exemplars and as supervised training data. On OSWorld, it consistently improves general-purpose and specialized CUAs, while on WindowsAgentArena it achieves state-of-the-art performance among 7B-scale models under the 15-step limit. These results show that web-scale human demonstration videos can serve as a practical and scalable foundation for advancing real-world CUAs.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04673v2",
    "published_date": "2025-10-06 10:29:00 UTC",
    "updated_date": "2025-11-27 09:41:18 UTC"
  },
  {
    "arxiv_id": "2510.04671v1",
    "title": "FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification",
    "authors": [
      "Chao Liu",
      "Ling Luo",
      "Tengxiao Lv",
      "Huan Zhuang",
      "Lejing Yu",
      "Jian Wang",
      "Hongfei Lin"
    ],
    "abstract": "With the rapid development of online medical platforms, consumer health questions (CHQs) are inefficient in diagnosis due to redundant information and frequent non-professional terms. The medical question summary (MQS) task aims to transform CHQs into streamlined doctors' frequently asked questions (FAQs), but existing methods still face challenges such as poor identification of question focus and model hallucination. This paper explores the potential of large language models (LLMs) in the MQS task and finds that direct fine-tuning is prone to focus identification bias and generates unfaithful content. To this end, we propose an optimization framework based on core focus guidance. First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text. Then, a fine-tuning dataset is constructed in combination with the original CHQ-FAQ pairs to improve the ability to identify the focus of the question. Finally, a multi-dimensional quality evaluation and selection mechanism is proposed to comprehensively improve the quality of the summary from multiple dimensions. We conduct comprehensive experiments on two widely-adopted MQS datasets using three established evaluation metrics. The proposed framework achieves state-of-the-art performance across all measures, demonstrating a significant boost in the model's ability to identify critical focus of questions and a notable mitigation of hallucinations. The source codes are freely available at https://github.com/DUT-LiuChao/FocusMed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a regular paper at BIBM2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04671v1",
    "published_date": "2025-10-06 10:27:09 UTC",
    "updated_date": "2025-10-06 10:27:09 UTC"
  },
  {
    "arxiv_id": "2510.04670v2",
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "authors": [
      "Xuanhua Yin",
      "Runkai Zhao",
      "Weidong Cai"
    ],
    "abstract": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating. Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from upstream fusion, while MIND combines token-dependent Top-K sparse routing with a subject prior to personalize expert usage without sacrificing generality. Experiments across multiple multimodal backbones and subjects show consistent improvements over strong baselines, enhanced cross-subject generalization, and interpretable expert patterns that correlate with content type. The framework offers a simple attachment point for new encoders and datasets, enabling robust, plug-and-improve performance for naturalistic neuroimaging studies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04670v2",
    "published_date": "2025-10-06 10:24:28 UTC",
    "updated_date": "2025-10-10 06:31:12 UTC"
  },
  {
    "arxiv_id": "2510.04667v1",
    "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting",
    "authors": [
      "Fanzhe Fu",
      "Yang Yang"
    ],
    "abstract": "Reversible Instance Normalization (RevIN) is a key technique enabling simple linear models to achieve state-of-the-art performance in time series forecasting. While replacing its non-robust statistics with robust counterparts (termed R$^2$-IN) seems like a straightforward improvement, our findings reveal a far more complex reality. This paper deconstructs the perplexing performance of various normalization strategies by identifying four underlying theoretical contradictions. Our experiments provide two crucial findings: first, the standard RevIN catastrophically fails on datasets with extreme outliers, where its MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN prevents this failure and unexpectedly emerges as the best overall performer, our adaptive model (A-IN), designed to test a diagnostics-driven heuristic, unexpectedly suffers a complete and systemic failure. This surprising outcome uncovers a critical, overlooked pitfall in time series analysis: the instability introduced by a simple or counter-intuitive heuristic can be more damaging than the statistical issues it aims to solve. The core contribution of this work is thus a new, cautionary paradigm for time series normalization: a shift from a blind search for complexity to a diagnostics-driven analysis that reveals not only the surprising power of simple baselines but also the perilous nature of naive adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04667v1",
    "published_date": "2025-10-06 10:22:20 UTC",
    "updated_date": "2025-10-06 10:22:20 UTC"
  },
  {
    "arxiv_id": "2510.06270v1",
    "title": "MCCE: A Framework for Multi-LLM Collaborative Co-Evolution",
    "authors": [
      "Nian Ran",
      "Zhongzheng Li",
      "Yue Wang",
      "Qingsong Ran",
      "Xiaoyuan Zhang",
      "Shikun Feng",
      "Richard Allmendinger",
      "Xiaoguang Zhao"
    ],
    "abstract": "Multi-objective discrete optimization problems, such as molecular design, pose significant challenges due to their vast and unstructured combinatorial spaces. Traditional evolutionary algorithms often get trapped in local optima, while expert knowledge can provide crucial guidance for accelerating convergence. Large language models (LLMs) offer powerful priors and reasoning ability, making them natural optimizers when expert knowledge matters. However, closed-source LLMs, though strong in exploration, cannot update their parameters and thus cannot internalize experience. Conversely, smaller open models can be continually fine-tuned but lack broad knowledge and reasoning strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid framework that unites a frozen closed-source LLM with a lightweight trainable model. The system maintains a trajectory memory of past search processes; the small model is progressively refined via reinforcement learning, with the two models jointly supporting and complementing each other in global exploration. Unlike model distillation, this process enhances the capabilities of both models through mutual inspiration. Experiments on multi-objective drug design benchmarks show that MCCE achieves state-of-the-art Pareto front quality and consistently outperforms baselines. These results highlight a new paradigm for enabling continual evolution in hybrid LLM systems, combining knowledge-driven exploration with experience-driven learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06270v1",
    "published_date": "2025-10-06 10:03:28 UTC",
    "updated_date": "2025-10-06 10:03:28 UTC"
  },
  {
    "arxiv_id": "2510.05189v1",
    "title": "A novel hallucination classification framework",
    "authors": [
      "Maksym Zavhorodnii",
      "Dmytro Dehtiarov",
      "Anna Konovalenko"
    ],
    "abstract": "This work introduces a novel methodology for the automatic detection of hallucinations generated during large language model (LLM) inference. The proposed approach is based on a systematic taxonomy and controlled reproduction of diverse hallucination types through prompt engineering. A dedicated hallucination dataset is subsequently mapped into a vector space using an embedding model and analyzed with unsupervised learning techniques in a reduced-dimensional representation of hallucinations with veridical responses. Quantitative evaluation of inter-centroid distances reveals a consistent correlation between the severity of informational distortion in hallucinations and their spatial divergence from the cluster of correct outputs. These findings provide theoretical and empirical evidence that even simple classification algorithms can reliably distinguish hallucinations from accurate responses within a single LLM, thereby offering a lightweight yet effective framework for improving model reliability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05189v1",
    "published_date": "2025-10-06 09:54:20 UTC",
    "updated_date": "2025-10-06 09:54:20 UTC"
  },
  {
    "arxiv_id": "2510.04646v1",
    "title": "Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation",
    "authors": [
      "Johanna Sommer",
      "John Rachwan",
      "Nils Fleischmann",
      "Stephan Günnemann",
      "Bertrand Charpentier"
    ],
    "abstract": "Flow matching models generate high-fidelity molecular geometries but incur significant computational costs during inference, requiring hundreds of network evaluations. This inference overhead becomes the primary bottleneck when such models are employed in practice to sample large numbers of molecular candidates. This work discusses a training-free caching strategy that accelerates molecular geometry generation by predicting intermediate hidden states across solver steps. The proposed method operates directly on the SE(3)-equivariant backbone, is compatible with pretrained models, and is orthogonal to existing training-based accelerations and system-level optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching achieves a twofold reduction in wall-clock inference time at matched sample quality and a speedup of up to 3x compared to the base model with minimal sample quality degradation. Because these gains compound with other optimizations, applying caching alongside other general, lossless optimizations yield as much as a 7x speedup.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04646v1",
    "published_date": "2025-10-06 09:49:14 UTC",
    "updated_date": "2025-10-06 09:49:14 UTC"
  },
  {
    "arxiv_id": "2510.04643v1",
    "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading",
    "authors": [
      "Xiangyu Li",
      "Yawen Zeng",
      "Xiaofen Xing",
      "Jin Xu",
      "Xiangmin Xu"
    ],
    "abstract": "In this paper, our objective is to develop a multi-agent financial system that incorporates simulated trading, a technique extensively utilized by financial professionals. While current LLM-based agent models demonstrate competitive performance, they still exhibit significant deviations from real-world fund companies. A critical distinction lies in the agents' reliance on ``post-reflection'', particularly in response to adverse outcomes, but lack a distinctly human capability: long-term prediction of future trends. Therefore, we introduce QuantAgents, a multi-agent system integrating simulated trading, to comprehensively evaluate various investment strategies and market scenarios without assuming actual risks. Specifically, QuantAgents comprises four agents: a simulated trading analyst, a risk control analyst, a market news analyst, and a manager, who collaborate through several meetings. Moreover, our system incentivizes agents to receive feedback on two fronts: performance in real-world markets and predictive accuracy in simulated trading. Extensive experiments demonstrate that our framework excels across all metrics, yielding an overall return of nearly 300% over the three years (https://quantagents.github.io/).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04643v1",
    "published_date": "2025-10-06 09:45:57 UTC",
    "updated_date": "2025-10-06 09:45:57 UTC"
  },
  {
    "arxiv_id": "2510.04630v1",
    "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection",
    "authors": [
      "Vrushank Ahire",
      "Aniruddh Muley",
      "Shivam Zample",
      "Siddharth Verma",
      "Pranav Menon",
      "Surbhi Madan",
      "Abhinav Dhall"
    ],
    "abstract": "Detecting manipulated media has now become a pressing issue with the recent rise of deepfakes. Most existing approaches fail to generalize across diverse datasets and generation techniques. We thus propose a novel ensemble framework, combining the strengths of transformer-based architectures, such as Swin Transformers and ViTs, and texture-based methods, to achieve better detection accuracy and robustness. Our method introduces innovative data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation techniques to handle dataset imbalances, enhance high-impact regions (e.g., eyes and mouth), and improve generalization. Our model achieves state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse subset of eight deepfake datasets. The ensemble benefits from the complementarity of these approaches, with transformers excelling in global feature extraction and texturebased methods providing interpretability. This work demonstrates that hybrid models can effectively address the evolving challenges of deepfake detection, offering a robust solution for real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04630v1",
    "published_date": "2025-10-06 09:35:57 UTC",
    "updated_date": "2025-10-06 09:35:57 UTC"
  },
  {
    "arxiv_id": "2510.04624v1",
    "title": "Fairness in Repeated Matching: A Maximin Perspective",
    "authors": [
      "Eugene Lim",
      "Tzeh Yuan Neoh",
      "Nicholas Teh"
    ],
    "abstract": "We study a sequential decision-making model where a set of items is repeatedly matched to the same set of agents over multiple rounds. The objective is to determine a sequence of matchings that either maximizes the utility of the least advantaged agent at the end of all rounds (optimal) or at the end of every individual round (anytime optimal). We investigate the computational challenges associated with finding (anytime) optimal outcomes and demonstrate that these problems are generally computationally intractable. However, we provide approximation algorithms, fixed-parameter tractable algorithms, and identify several special cases whereby the problem(s) can be solved efficiently. Along the way, we also establish characterizations of Pareto-optimal/maximum matchings, which may be of independent interest to works in matching theory and house allocation.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04624v1",
    "published_date": "2025-10-06 09:32:40 UTC",
    "updated_date": "2025-10-06 09:32:40 UTC"
  },
  {
    "arxiv_id": "2510.04623v1",
    "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports",
    "authors": [
      "Shrish Shrinath Vaidya",
      "Gowthamaan Palani",
      "Sidharth Ramesh",
      "Velmurugan Balasubramanian",
      "Minmini Selvam",
      "Gokulraja Srinivasaraja",
      "Ganapathy Krishnamurthi"
    ],
    "abstract": "The deployment of Large Language Models (LLMs) for structuring clinical data is critically hindered by their tendency to hallucinate facts and their inability to follow domain-specific rules. To address this, we introduce MedPAO, a novel agentic framework that ensures accuracy and verifiable reasoning by grounding its operation in established clinical protocols such as the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring task into a transparent process managed by a Plan-Act-Observe (PAO) loop and specialized tools. This protocol-driven method provides a verifiable alternative to opaque, monolithic models. The efficacy of our approach is demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96 on the critical sub-task of concept categorization. Notably, expert radiologists and clinicians rated the final structured outputs with an average score of 4.52 out of 5, indicating a level of reliability that surpasses baseline approaches relying solely on LLM-based foundation models. The code is available at: https://github.com/MiRL-IITM/medpao-agent",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper published at \"Agentic AI for Medicine\" Workshop, MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04623v1",
    "published_date": "2025-10-06 09:32:23 UTC",
    "updated_date": "2025-10-06 09:32:23 UTC"
  },
  {
    "arxiv_id": "2510.04618v1",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "authors": [
      "Qizheng Zhang",
      "Changran Hu",
      "Shubhangi Upasani",
      "Boyuan Ma",
      "Fenglu Hong",
      "Vamsidhar Kamanuru",
      "Jay Rainton",
      "Chen Wu",
      "Mengmeng Ji",
      "Hanchen Li",
      "Urmish Thakker",
      "James Zou",
      "Kunle Olukotun"
    ],
    "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04618v1",
    "published_date": "2025-10-06 09:30:18 UTC",
    "updated_date": "2025-10-06 09:30:18 UTC"
  },
  {
    "arxiv_id": "2510.04617v2",
    "title": "Making Mathematical Reasoning Adaptive",
    "authors": [
      "Zhejian Lai",
      "Xiang Geng",
      "Zhijun Wang",
      "Yang Bai",
      "Jiahuan Li",
      "Rongxiang Weng",
      "Jingang Wang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Shujian Huang"
    ],
    "abstract": "Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/NJUNLP/AdaR.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04617v2",
    "published_date": "2025-10-06 09:30:05 UTC",
    "updated_date": "2025-10-13 03:08:12 UTC"
  },
  {
    "arxiv_id": "2510.04615v1",
    "title": "Design Process of a Self Adaptive Smart Serious Games Ecosystem",
    "authors": [
      "X. Tao",
      "P. Chen",
      "M. Tsami",
      "F. Khayati",
      "M. Eckert"
    ],
    "abstract": "This paper outlines the design vision and planned evolution of Blexer v3, a modular and AI-driven rehabilitation ecosystem based on serious games. Building on insights from previous versions of the system, we propose a new architecture that aims to integrate multimodal sensing, real-time reasoning, and intelligent control. The envisioned system will include distinct modules for data collection, user state inference, and gameplay adaptation. Key features such as dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are also considered to support personalized interventions. We present the complete conceptual framework of Blexer v3, which defines the modular structure and data flow of the system. This serves as the foundation for the next phase: the development of a functional prototype and its integration into clinical rehabilitation scenarios.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04615v1",
    "published_date": "2025-10-06 09:28:31 UTC",
    "updated_date": "2025-10-06 09:28:31 UTC"
  },
  {
    "arxiv_id": "2510.04609v1",
    "title": "Accountability Capture: How Record-Keeping to Support AI Transparency and Accountability (Re)shapes Algorithmic Oversight",
    "authors": [
      "Shreya Chappidi",
      "Jennifer Cobbe",
      "Chris Norval",
      "Anjali Mazumder",
      "Jatinder Singh"
    ],
    "abstract": "Accountability regimes typically encourage record-keeping to enable the transparency that supports oversight, investigation, contestation, and redress. However, implementing such record-keeping can introduce considerations, risks, and consequences, which so far remain under-explored. This paper examines how record-keeping practices bring algorithmic systems within accountability regimes, providing a basis to observe and understand their effects. For this, we introduce, describe, and elaborate 'accountability capture' -- the re-configuration of socio-technical processes and the associated downstream effects relating to record-keeping for algorithmic accountability. Surveying 100 practitioners, we evidence and characterise record-keeping issues in practice, identifying their alignment with accountability capture. We further document widespread record-keeping practices, tensions between internal and external accountability requirements, and evidence of employee resistance to practices imposed through accountability capture. We discuss these and other effects for surveillance, privacy, and data protection, highlighting considerations for algorithmic accountability communities. In all, we show that implementing record-keeping to support transparency in algorithmic accountability regimes can itself bring wider implications -- an issue requiring greater attention from practitioners, researchers, and policymakers alike.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "To appear at 8th AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.04609v1",
    "published_date": "2025-10-06 09:20:27 UTC",
    "updated_date": "2025-10-06 09:20:27 UTC"
  },
  {
    "arxiv_id": "2510.04607v1",
    "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents",
    "authors": [
      "Yuan Wang",
      "Mingyu Li",
      "Haibo Chen"
    ],
    "abstract": "Computer-use agents (CUAs) powered by large language models (LLMs) have emerged as a promising approach to automating computer tasks, yet they struggle with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to decompose high-level goals into lengthy, error-prone sequences of fine-grained actions, resulting in low success rates and an excessive number of LLM calls.\n  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms existing GUIs into three declarative primitives: access, state, and observation, which are better suited for LLMs. Our key idea is policy-mechanism separation: LLMs focus on high-level semantic planning (policy) while GOI handles low-level navigation and interaction (mechanism). GOI does not require modifying the application source code or relying on application programming interfaces (APIs).\n  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on Windows. Compared to a leading GUI-based agent baseline, GOI improves task success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI completes over 61% of successful tasks with a single LLM call.",
    "categories": [
      "cs.OS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.OS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04607v1",
    "published_date": "2025-10-06 09:14:58 UTC",
    "updated_date": "2025-10-06 09:14:58 UTC"
  },
  {
    "arxiv_id": "2510.04602v2",
    "title": "Computing Wasserstein Barycenters through Gradient Flows",
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Yassir Bendou",
      "Mike Gartrell"
    ],
    "abstract": "Wasserstein barycenters provide a powerful tool for aggregating probability measures, while leveraging the geometry of their ambient space. Existing discrete methods suffer from poor scalability, as they require access to the complete set of samples from input measures. We address this issue by recasting the original barycenter problem as a gradient flow in the Wasserstein space. Our approach offers two advantages. First, we achieve scalability by sampling mini-batches from the input measures. Second, we incorporate functionals over probability measures, which regularize the barycenter problem through internal, potential, and interaction energies. We present two algorithms for empirical and Gaussian mixture measures, providing convergence guarantees under the Polyak-Łojasiewicz inequality. Experimental validation on toy datasets and domain adaptation benchmarks show that our methods outperform previous discrete and neural net-based methods for computing Wasserstein barycenters.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.04602v2",
    "published_date": "2025-10-06 09:07:12 UTC",
    "updated_date": "2025-11-13 10:08:22 UTC"
  },
  {
    "arxiv_id": "2510.04588v1",
    "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma",
    "authors": [
      "Shurui Li"
    ],
    "abstract": "Rapid advances in artificial intelligence necessitate a re-examination of the epistemological foundations upon which we attribute consciousness. As AI systems increasingly mimic human behavior and interaction with high fidelity, the concept of a \"perfect mimic\"-an entity empirically indistinguishable from a human through observation and interaction-shifts from hypothetical to technologically plausible. This paper argues that such developments pose a fundamental challenge to the consistency of our mind-recognition practices. Consciousness attributions rely heavily, if not exclusively, on empirical evidence derived from behavior and interaction. If a perfect mimic provides evidence identical to that of humans, any refusal to grant it equivalent epistemic status must invoke inaccessible factors, such as qualia, substrate requirements, or origin. Selectively invoking such factors risks a debilitating dilemma: either we undermine the rational basis for attributing consciousness to others (epistemological solipsism), or we accept inconsistent reasoning. I contend that epistemic consistency demands we ascribe the same status to empirically indistinguishable entities, regardless of metaphysical assumptions. The perfect mimic thus acts as an epistemic mirror, forcing critical reflection on the assumptions underlying intersubjective recognition in light of advancing AI. This analysis carries significant implications for theories of consciousness and ethical frameworks concerning artificial agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04588v1",
    "published_date": "2025-10-06 08:44:55 UTC",
    "updated_date": "2025-10-06 08:44:55 UTC"
  },
  {
    "arxiv_id": "2510.04580v1",
    "title": "Strongly Solving 2048 4x3",
    "authors": [
      "Tomoyuki Kaneko",
      "Shuhei Yamashita"
    ],
    "abstract": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid, where a player chooses a direction among up, down, left, and right to obtain a score by merging two tiles with the same number located in neighboring cells along the chosen direction. This paper presents that a variant 2048-4x3 12 cells on a 4 by 3 board, one row smaller than the original, has been strongly solved. In this variant, the expected score achieved by an optimal strategy is about $50724.26$ for the most common initial states: ones with two tiles of number 2. The numbers of reachable states and afterstates are identified to be $1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is to partition state space by the sum of tile numbers on a board, which we call the age of a state. An age is invariant between a state and its successive afterstate after any valid action and is increased two or four by stochastic response from the environment. Therefore, we can partition state space by ages and enumerate all (after)states of an age depending only on states with the recent ages. Similarly, we can identify (after)state values by going along with ages in decreasing order.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04580v1",
    "published_date": "2025-10-06 08:31:59 UTC",
    "updated_date": "2025-10-06 08:31:59 UTC"
  },
  {
    "arxiv_id": "2510.04576v1",
    "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator",
    "authors": [
      "Yuhta Takida",
      "Satoshi Hayakawa",
      "Takashi Shibuya",
      "Masaaki Imaizumi",
      "Naoki Murata",
      "Bac Nguyen",
      "Toshimitsu Uesaka",
      "Chieh-Hsin Lai",
      "Yuki Mitsufuji"
    ],
    "abstract": "Deep generative models have made significant advances in generating complex content, yet conditional generation remains a fundamental challenge. Existing conditional generative adversarial networks often struggle to balance the dual objectives of assessing authenticity and conditional alignment of input samples within their conditional discriminators. To address this, we propose a novel discriminator design that integrates three key capabilities: unconditional discrimination, matching-aware supervision to enhance alignment sensitivity, and adaptive weighting to dynamically balance all objectives. Specifically, we introduce Sum of Naturalness and Alignment (SONA), which employs separate projections for naturalness (authenticity) and alignment in the final layer with an inductive bias, supported by dedicated objective functions and an adaptive weighting mechanism. Extensive experiments on class-conditional generation tasks show that \\ours achieves superior sample quality and conditional alignment compared to state-of-the-art methods. Furthermore, we demonstrate its effectiveness in text-to-image generation, confirming the versatility and robustness of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages with 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04576v1",
    "published_date": "2025-10-06 08:26:06 UTC",
    "updated_date": "2025-10-06 08:26:06 UTC"
  },
  {
    "arxiv_id": "2510.04574v1",
    "title": "Deep learning framework for predicting stochastic take-off and die-out of early spreading",
    "authors": [
      "Wenchao He",
      "Tao Jia"
    ],
    "abstract": "Large-scale outbreaks of epidemics, misinformation, or other harmful contagions pose significant threats to human society, yet the fundamental question of whether an emerging outbreak will escalate into a major epidemic or naturally die out remains largely unaddressed. This problem is challenging, partially due to inadequate data during the early stages of outbreaks and also because established models focus on average behaviors of large epidemics rather than the stochastic nature of small transmission chains. Here, we introduce the first systematic framework for forecasting whether initial transmission events will amplify into major outbreaks or fade into extinction during early stages, when intervention strategies can still be effectively implemented. Using extensive data from stochastic spreading models, we developed a deep learning framework that predicts early-stage spreading outcomes in real-time. Validation across Erdős-Rényi and Barabási-Albert networks with varying infectivity levels shows our method accurately forecasts stochastic spreading events well before potential outbreaks, demonstrating robust performance across different network structures and infectivity scenarios.To address the challenge of sparse data during early outbreak stages, we further propose a pretrain-finetune framework that leverages diverse simulation data for pretraining and adapts to specific scenarios through targeted fine-tuning. The pretrain-finetune framework consistently outperforms baseline models, achieving superior performance even when trained on limited scenario-specific data. To our knowledge, this work presents the first framework for predicting stochastic take-off versus die-out. This framework provides valuable insights for epidemic preparedness and public health decision-making, enabling more informed early intervention strategies.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.SI",
    "comment": "29 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04574v1",
    "published_date": "2025-10-06 08:18:47 UTC",
    "updated_date": "2025-10-06 08:18:47 UTC"
  },
  {
    "arxiv_id": "2510.04573v4",
    "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
    "authors": [
      "Haoqiang Kang",
      "Yizhe Zhang",
      "Nikki Lijing Kuang",
      "Nicklas Majamaki",
      "Navdeep Jaitly",
      "Yi-An Ma",
      "Lianhui Qin"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04573v4",
    "published_date": "2025-10-06 08:15:03 UTC",
    "updated_date": "2025-12-11 22:01:37 UTC"
  },
  {
    "arxiv_id": "2510.04568v1",
    "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context",
    "authors": [
      "Naman Gupta",
      "Shreeyash Gowaikar",
      "Arun Iyer",
      "Kirankumar Shiragur",
      "Ramakrishna B Bairi",
      "Rishikesh Maurya",
      "Ritabrata Maiti",
      "Sankarshan Damle",
      "Shachee Mishra Gupta"
    ],
    "abstract": "Reasoning over very long inputs remains difficult for large language models (LLMs). Common workarounds either shrink the input via retrieval (risking missed evidence), enlarge the context window (straining selectivity), or stage multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents, CoA), free-form summaries passed between agents can discard crucial details and amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc messages with a structured memory. A Planner agent first turns a user query into concrete, checkable sub-questions. worker agents process chunks via a fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared memory. A Manager agent then Synthesizes the final answer directly from the memory. This preserves step-wise read-then-reason benefits while changing both the communication medium (structured memory) and the worker procedure (fixed micro-cycle), yielding higher faithfulness, better long-range aggregation, and auditability. On long-context QA from the HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over a CoA baseline.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04568v1",
    "published_date": "2025-10-06 08:10:04 UTC",
    "updated_date": "2025-10-06 08:10:04 UTC"
  },
  {
    "arxiv_id": "2510.04567v1",
    "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning",
    "authors": [
      "Weishuo Ma",
      "Yanbo Wang",
      "Xiyuan Wang",
      "Lei Zou",
      "Muhan Zhang"
    ],
    "abstract": "Graph Neural Networks (GNNs) are powerful tools for precessing relational data but often struggle to generalize to unseen graphs, giving rise to the development of Graph Foundational Models (GFMs). However, current GFMs are challenged by the extreme heterogeneity of graph data, where each graph can possess a unique feature space, label set, and topology. To address this, two main paradigms have emerged. The first leverages Large Language Models (LLMs), but is fundamentally text-dependent, thus struggles to handle the numerical features in vast graphs. The second pre-trains a structure-based model, but the adaptation to new tasks typically requires a costly, per-graph tuning stage, creating a critical efficiency bottleneck. In this work, we move beyond these limitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning \\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free architecture. GILT introduces a novel token-based framework for in-context learning (ICL) on graphs, reframing classification tasks spanning node, edge and graph levels in a unified framework. This mechanism is the key to handling heterogeneity, as it is designed to operate on generic numerical features. Further, its ability to understand class semantics dynamically from the context enables tuning-free adaptation. Comprehensive experiments show that GILT achieves stronger few-shot performance with significantly less time than LLM-based or tuning-based baselines, validating the effectiveness of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04567v1",
    "published_date": "2025-10-06 08:09:15 UTC",
    "updated_date": "2025-10-06 08:09:15 UTC"
  },
  {
    "arxiv_id": "2510.04560v1",
    "title": "ContextNav: Towards Agentic Multimodal In-Context Learning",
    "authors": [
      "Honghao Fu",
      "Yuan Ouyang",
      "Kai-Wei Chang",
      "Yiwei Wang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "abstract": "Recent advances demonstrate that multimodal large language models (MLLMs) exhibit strong multimodal in-context learning (ICL) capabilities, enabling them to adapt to novel vision-language tasks from a few contextual examples. However, existing ICL approaches face challenges in reconciling scalability with robustness across diverse tasks and noisy contextual examples: manually selecting examples produces clean contexts but is labor-intensive and task-specific, while similarity-based retrieval improves scalability but could introduce irrelevant or structurally inconsistent samples that degrade ICL performance. To address these limitations, we propose ContextNav, the first agentic framework that integrates the scalability of automated retrieval with the quality and adaptiveness of human-like curation, enabling noise-robust and dynamically optimized contextualization for multimodal ICL. ContextNav unifies context management and noise-robust contextualization within a closed-loop workflow driven by graph-based orchestration. Specifically, it builds a resource-aware multimodal embedding pipeline, maintains a retrievable vector database, and applies agentic retrieval and structural alignment to construct noise-resilient contexts. An Operational Grammar Graph (OGG) further supports adaptive workflow planning and optimization, enabling the agent to refine its operational strategies based on downstream ICL feedback. Experimental results demonstrate that ContextNav achieves state-of-the-art performance across various datasets, underscoring the promise of agentic workflows for advancing scalable and robust contextualization in multimodal ICL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04560v1",
    "published_date": "2025-10-06 07:49:52 UTC",
    "updated_date": "2025-10-06 07:49:52 UTC"
  },
  {
    "arxiv_id": "2510.04550v2",
    "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use",
    "authors": [
      "Pengfei He",
      "Zhenwei Dai",
      "Bing He",
      "Hui Liu",
      "Xianfeng Tang",
      "Hanqing Lu",
      "Juanhui Li",
      "Jiayuan Ding",
      "Subhabrata Mukherjee",
      "Suhang Wang",
      "Yue Xing",
      "Jiliang Tang",
      "Benoit Dumoulin"
    ],
    "abstract": "Large language model (LLM)-based agents increasingly rely on tool use to complete real-world tasks. While existing works evaluate the LLMs' tool use capability, they largely focus on the final answers yet overlook the detailed tool usage trajectory, i.e., whether tools are selected, parameterized, and ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to comprehensively evaluate LLMs' tool use capability through diverse tasks with fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable tools across practical domains with tasks grounded in production-style APIs, and synthesizes trajectories that vary in breadth (parallel calls) and depth (interdependent chains). Besides final accuracy, TRAJECT-Bench also reports trajectory-level diagnostics, including tool selection and argument correctness, and dependency/order satisfaction. Analyses reveal failure modes such as similar tool confusion and parameter-blind selection, and scaling behavior with tool diversity and trajectory length where the bottleneck of transiting from short to mid-length trajectories is revealed, offering actionable guidance for LLMs' tool use.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04550v2",
    "published_date": "2025-10-06 07:30:25 UTC",
    "updated_date": "2025-10-11 09:19:32 UTC"
  },
  {
    "arxiv_id": "2510.04542v1",
    "title": "Code World Models for General Game Playing",
    "authors": [
      "Wolfgang Lehrach",
      "Daniel Hennes",
      "Miguel Lazaro-Gredilla",
      "Xinghua Lou",
      "Carter Wendelken",
      "Zun Li",
      "Antoine Dedieu",
      "Jordi Grau-Moya",
      "Marc Lanctot",
      "Atil Iscen",
      "John Schultz",
      "Marcus Chiam",
      "Ian Gemp",
      "Piotr Zielinski",
      "Satinder Singh",
      "Kevin P. Murphy"
    ],
    "abstract": "Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04542v1",
    "published_date": "2025-10-06 07:16:07 UTC",
    "updated_date": "2025-10-06 07:16:07 UTC"
  },
  {
    "arxiv_id": "2510.04536v1",
    "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG",
    "authors": [
      "Shun-ichiro Hayashi",
      "Daichi Mukunoki",
      "Tetsuya Hoshino",
      "Satoshi Ohshima",
      "Takahiro Katagiri"
    ],
    "abstract": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG) generation framework utilizing Large Language Models (LLMs). The framework enables users to generate 3D-CG content solely through natural language instructions. 3Dify is built upon Dify, an open-source platform for AI application development, and incorporates several state-of-the-art LLM-related technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation (RAG). For 3D-CG generation support, 3Dify automates the operation of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not support MCP-based interaction, the framework employs the Computer-Using Agent (CUA) method to automate Graphical User Interface (GUI) operations. Moreover, to enhance image generation quality, 3Dify allows users to provide feedback by selecting preferred images from multiple candidates. The LLM then learns variable patterns from these selections and applies them to subsequent generations. Furthermore, 3Dify supports the integration of locally deployed LLMs, enabling users to utilize custom-developed models and to reduce both time and monetary costs associated with external API calls by leveraging their own computational resources.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04536v1",
    "published_date": "2025-10-06 07:00:06 UTC",
    "updated_date": "2025-10-06 07:00:06 UTC"
  },
  {
    "arxiv_id": "2510.04532v1",
    "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models",
    "authors": [
      "Xurui Song",
      "Shuo Huai",
      "JingJing Jiang",
      "Jiayi Kong",
      "Jun Luo"
    ],
    "abstract": "Vision-Language Model (VLM) driving agents promise explainable end-to-end autonomy by first producing natural-language reasoning and then predicting trajectory planning. However, whether planning is causally driven by this reasoning remains a critical but unverified assumption. To investigate this, we build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan. Our data generation process converts sensors and annotations into structured inputs and, crucially, separates priors from to-be-reasoned signals, enabling clean information ablations. Using DriveMind, we train representative VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately, indicate a consistent causal disconnect in reasoning-planning: removing ego/navigation priors causes large drops in planning scores, whereas removing CoT produces only minor changes. Attention analysis further shows that planning primarily focuses on priors rather than the CoT. Based on this evidence, we propose the Reasoning-Planning Decoupling Hypothesis, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator. To enable efficient diagnosis, we also introduce a novel, training-free probe that measures an agent's reliance on priors by evaluating its planning robustness against minor input perturbations. In summary, we provide the community with a new dataset and a diagnostic tool to evaluate the causal fidelity of future models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "The dataset will be released publicly once the paper is accepted for publication",
    "pdf_url": "https://arxiv.org/pdf/2510.04532v1",
    "published_date": "2025-10-06 06:50:16 UTC",
    "updated_date": "2025-10-06 06:50:16 UTC"
  },
  {
    "arxiv_id": "2510.04528v1",
    "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers",
    "authors": [
      "Santhosh KumarRavindran"
    ],
    "abstract": "The rapid adoption of large language models (LLMs) in enterprise systems exposes vulnerabilities to prompt injection attacks, strategic deception, and biased outputs, threatening security, trust, and fairness. Extending our adversarial activation patching framework (arXiv:2507.09406), which induced deception in toy networks at a 23.9% rate, we introduce the Unified Threat Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through 700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs via enhanced patching; and (3) 78% improvement in fairness metrics (e.g., demographic bias). Novel contributions include a generalized patching algorithm for multi-threat detection, three groundbreaking hypotheses on threat interactions (e.g., threat chaining in enterprise workflows), and a deployment-ready toolkit with APIs for enterprise integration.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04528v1",
    "published_date": "2025-10-06 06:44:27 UTC",
    "updated_date": "2025-10-06 06:44:27 UTC"
  },
  {
    "arxiv_id": "2510.04522v2",
    "title": "Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction",
    "authors": [
      "Yisen Gao",
      "Xingcheng Fu",
      "Qingyun Sun",
      "Jianxin Li",
      "Xianxian Li"
    ],
    "abstract": "Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04522v2",
    "published_date": "2025-10-06 06:29:49 UTC",
    "updated_date": "2025-12-11 07:48:20 UTC"
  },
  {
    "arxiv_id": "2510.04520v1",
    "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph",
    "authors": [
      "Hanyu Wang",
      "Ruohan Xie",
      "Yutong Wang",
      "Guoxiong Gao",
      "Xintao Yu",
      "Bin Dong"
    ],
    "abstract": "Accurate auto-formalization of theorem statements is essential for advancing automated discovery and verification of research-level mathematics, yet remains a major bottleneck for LLMs due to hallucinations, semantic mismatches, and their inability to synthesize new definitions. To tackle these issues, we present Aria (Agent for Retrieval and Iterative Autoformalization), a system for conjecture-level formalization in Lean that emulates human expert reasoning via a two-phase Graph-of-Thought process: recursively decomposing statements into a dependency graph and then constructing formalizations from grounded concepts. To ensure semantic correctness, we introduce AriaScorer, a checker that retrieves definitions from Mathlib for term-level grounding, enabling rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy, surpassing previous methods. On FATE-X, a suite of challenging algebra problems from research literature, it outperforms the best baseline with 44.0% vs. 24.0% final accuracy. On a dataset of homological conjectures, Aria reaches 42.9% final accuracy while all other models score 0%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04520v1",
    "published_date": "2025-10-06 06:25:11 UTC",
    "updated_date": "2025-10-06 06:25:11 UTC"
  },
  {
    "arxiv_id": "2510.04514v2",
    "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering",
    "authors": [
      "Rachneet Kaur",
      "Nishan Srishankar",
      "Zhen Zeng",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ],
    "abstract": "Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts-those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieves the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.CV",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025 Multimodal Algorithmic Reasoning Workshop (https://marworkshop.github.io/neurips25/) (Oral Paper Presentation)",
    "pdf_url": "https://arxiv.org/pdf/2510.04514v2",
    "published_date": "2025-10-06 06:05:36 UTC",
    "updated_date": "2026-01-07 06:02:37 UTC"
  },
  {
    "arxiv_id": "2510.04506v1",
    "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
    "authors": [
      "Jiashuo Sun",
      "Shixuan Liu",
      "Zhaochen Su",
      "Xianrui Zhong",
      "Pengcheng Jiang",
      "Bowen Jin",
      "Peiran Li",
      "Weijia Shi",
      "Jiawei Han"
    ],
    "abstract": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 7 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.04506v1",
    "published_date": "2025-10-06 05:46:56 UTC",
    "updated_date": "2025-10-06 05:46:56 UTC"
  },
  {
    "arxiv_id": "2510.04503v2",
    "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
    "authors": [
      "Shuai Zhao",
      "Xinyi Wu",
      "Shiqian Zhao",
      "Xiaobao Wu",
      "Zhongliang Guo",
      "Yanhao Jia",
      "Anh Tuan Luu"
    ],
    "abstract": "During fine-tuning, large language models (LLMs) are increasingly vulnerable to data-poisoning backdoor attacks, which compromise their reliability and trustworthiness. However, existing defense strategies suffer from limited generalization: they only work on specific attack types or task settings. In this study, we propose Poison-to-Poison (P2P), a general and effective backdoor defense algorithm. P2P injects benign triggers with safe alternative labels into a subset of training samples and fine-tunes the model on this re-poisoned dataset by leveraging prompt-based learning. This enforces the model to associate trigger-induced representations with safe outputs, thereby overriding the effects of original malicious triggers. Thanks to this robust and generalizable trigger-based fine-tuning, P2P is effective across task settings and attack types. Theoretically and empirically, we show that P2P can neutralize malicious backdoors while preserving task performance. We conduct extensive experiments on classification, mathematical reasoning, and summary generation tasks, involving multiple state-of-the-art LLMs. The results demonstrate that our P2P algorithm significantly reduces the attack success rate compared with baseline models. We hope that the P2P can serve as a guideline for defending against backdoor attacks and foster the development of a secure and trustworthy LLM community.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04503v2",
    "published_date": "2025-10-06 05:45:23 UTC",
    "updated_date": "2025-10-10 01:31:10 UTC"
  },
  {
    "arxiv_id": "2510.04498v1",
    "title": "GenQuest: An LLM-based Text Adventure Game for Language Learners",
    "authors": [
      "Qiao Wang",
      "Adnan Labib",
      "Robert Swier",
      "Michael Hofmeyr",
      "Zheng Yuan"
    ],
    "abstract": "GenQuest is a generative text adventure game that leverages Large Language Models (LLMs) to facilitate second language learning through immersive, interactive storytelling. The system engages English as a Foreign Language (EFL) learners in a collaborative \"choose-your-own-adventure\" style narrative, dynamically generated in response to learner choices. Game mechanics such as branching decision points and story milestones are incorporated to maintain narrative coherence while allowing learner-driven plot development. Key pedagogical features include content generation tailored to each learner's proficiency level, and a vocabulary assistant that provides in-context explanations of learner-queried text strings, ranging from words and phrases to sentences. Findings from a pilot study with university EFL students in China indicate promising vocabulary gains and positive user perceptions. Also discussed are suggestions from participants regarding the narrative length and quality, and the request for multi-modal content such as illustrations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Workshop on Wordplay: When Language Meets Games, EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04498v1",
    "published_date": "2025-10-06 05:22:53 UTC",
    "updated_date": "2025-10-06 05:22:53 UTC"
  },
  {
    "arxiv_id": "2510.05188v1",
    "title": "Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents",
    "authors": [
      "Wenda Xie",
      "Chao Guo",
      "Yanqing Jing. Junle Wang",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ],
    "abstract": "Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05188v1",
    "published_date": "2025-10-06 05:20:37 UTC",
    "updated_date": "2025-10-06 05:20:37 UTC"
  },
  {
    "arxiv_id": "2510.04491v1",
    "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents",
    "authors": [
      "Muyu He",
      "Anand Kumar",
      "Tsach Mackey",
      "Meghana Rajeev",
      "James Zou",
      "Nazneen Rajani"
    ],
    "abstract": "Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today's benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. We address this robustness testing gap by introducing TraitBasis, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend $τ$-Bench to $τ$-Trait, where user behaviors are altered via controlled trait vectors. We observe on average a 2%-30% performance degradation on $τ$-Trait across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior. Together, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We have open-sourced $τ$-Trai across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios: https://github.com/collinear-ai/tau-trait.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.04491v1",
    "published_date": "2025-10-06 05:03:57 UTC",
    "updated_date": "2025-10-06 05:03:57 UTC"
  },
  {
    "arxiv_id": "2510.04488v1",
    "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning",
    "authors": [
      "Edward Y. Chang",
      "Ethan Y. Chang"
    ],
    "abstract": "Multi-agent debate often wastes compute by using a fixed adversarial stance, aggregating without deliberation, or stopping on heuristics. We introduce MACI, an active controller with two independent dials that decouple information from behavior: an information dial that gates evidence by quality, and a behavior dial that schedules contentiousness from exploration to consolidation. A moderator tracks disagreement, overlap, evidence quality, and argument quality, and halts when gains plateau. We provide theory-lite guarantees for nonincreasing dispersion and provable termination, with a budget-feasible scheduler. Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens, and converts residual uncertainty into precision RAG plans that specify what to retrieve next. We use a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal, validated for order invariance and judge-swap stability; stability depends on using high-capability judges. MACI turns debate into a budget-aware, measurable, and provably terminating controller.",
    "categories": [
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, 5 figures, 21 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.04488v1",
    "published_date": "2025-10-06 04:52:17 UTC",
    "updated_date": "2025-10-06 04:52:17 UTC"
  },
  {
    "arxiv_id": "2510.04484v1",
    "title": "Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness",
    "authors": [
      "Amin Banayeeanzade",
      "Ala N. Tak",
      "Fatemeh Bahrani",
      "Anahita Bolourani",
      "Leonardo Blas",
      "Emilio Ferrara",
      "Jonathan Gratch",
      "Sai Praneeth Karimireddy"
    ],
    "abstract": "The ability to control LLMs' emulated emotional states and personality traits is essential for enabling rich, human-centered interactions in socially interactive settings. We introduce PsySET, a Psychologically-informed benchmark to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion and personality domains. Our study spans four models from different LLM families paired with various steering strategies, including prompting, fine-tuning, and representation engineering. Our results indicate that prompting is consistently effective but limited in intensity control, whereas vector injections achieve finer controllability while slightly reducing output quality. Moreover, we explore the trustworthiness of steered LLMs by assessing safety, truthfulness, fairness, and ethics, highlighting potential side effects and behavioral shifts. Notably, we observe idiosyncratic effects; for instance, even a positive emotion like joy can degrade robustness to adversarial factuality, lower privacy awareness, and increase preferential bias. Meanwhile, anger predictably elevates toxicity yet strengthens leakage resistance. Our framework establishes the first holistic evaluation of emotion and personality steering, offering insights into its interpretability and reliability for socially interactive applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ARR - October 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04484v1",
    "published_date": "2025-10-06 04:49:56 UTC",
    "updated_date": "2025-10-06 04:49:56 UTC"
  },
  {
    "arxiv_id": "2510.04480v1",
    "title": "On Continuous Optimization for Constraint Satisfaction Problems",
    "authors": [
      "Yunuo Cen",
      "Zixuan Wang",
      "Jintao Zhang",
      "Zhiwei Zhang",
      "Xuanyao Fong"
    ],
    "abstract": "Constraint satisfaction problems (CSPs) are fundamental in mathematics, physics, and theoretical computer science. While conflict-driven clause learning Boolean Satisfiability (SAT) solvers have achieved remarkable success and become the mainstream approach for Boolean satisfiability, recent advances show that modern continuous local search (CLS) solvers can achieve highly competitive results on certain classes of SAT problems. Motivated by these advances, we extend the CLS framework from Boolean SAT to general CSP with finite-domain variables and expressive constraints. We present FourierCSP, a continuous optimization framework that generalizes the Walsh-Fourier transform to CSP, allowing for transforming versatile constraints to compact multilinear polynomials, thereby avoiding the need for auxiliary variables and memory-intensive encodings. Our approach leverages efficient evaluation and differentiation of the objective via circuit-output probability and employs a projected gradient optimization method with theoretical guarantees. Empirical results on benchmark suites demonstrate that FourierCSP is scalable and competitive, significantly broadening the class of problems that can be efficiently solved by CLS techniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04480v1",
    "published_date": "2025-10-06 04:30:07 UTC",
    "updated_date": "2025-10-06 04:30:07 UTC"
  },
  {
    "arxiv_id": "2510.04477v1",
    "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models",
    "authors": [
      "Soo Yong Kim",
      "Suin Cho",
      "Vincent-Daniel Yun",
      "Gyeongyeon Hwang"
    ],
    "abstract": "Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging. We introduce MedCLM, an automated pipeline that converts detection datasets into large-scale medical visual question answering (VQA) data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ segmentation and structured rationales. These contextual signals enable medical vision-language models to generate question-answer pairs with step-by-step reasoning. To utilize this data effectively, we propose an Integrated CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes for visual grounding, a Medium stage that encourages implicit localization, and a Hard stage for weakly supervised reasoning. Experimental results demonstrate that MedCLM attains state-of-the-art performance on several medical VQA benchmarks, providing a scalable framework for developing clinically aligned medical vision-language models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04477v1",
    "published_date": "2025-10-06 04:26:39 UTC",
    "updated_date": "2025-10-06 04:26:39 UTC"
  },
  {
    "arxiv_id": "2510.04476v1",
    "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space",
    "authors": [
      "Tomas Figliolia",
      "Nicholas Alonso",
      "Rishi Iyer",
      "Quentin Anthony",
      "Beren Millidge"
    ],
    "abstract": "Multi-headed Attention's (MHA) quadratic compute and linearly growing KV-cache make long-context transformers expensive to train and serve. Prior works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA) shrink the cache, speeding decode, but leave compute, which determines prefill and training speed, largely unchanged. We introduce Compressed Convolutional Attention (CCA), a novel attention method which down-projects queries, keys, and values and performs the entire attention operation inside the shared latent space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all at once by the desired compression factor. Because CCA is orthogonal to head-sharing, we combine the two to form Compressed Convolutional Grouped Query Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier so that users can tune compression toward either FLOP or memory limits without sacrificing quality. Experiments show that CCGQA consistently outperforms both GQA and MLA at equal KV-cache compression on dense and MoE models. Additionally, we show that CCGQA outperforms all other attention methods on MoE models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache compression with no drop in performance compared to standard MHA. CCA and CCGQA also dramatically reduce the FLOP cost of attention which leads to substantially faster training and prefill than existing methods. On H100 GPUs, our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence length of 16k relative to MHA, and accelerates backward by about 1.3x.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04476v1",
    "published_date": "2025-10-06 04:24:23 UTC",
    "updated_date": "2025-10-06 04:24:23 UTC"
  },
  {
    "arxiv_id": "2510.05187v1",
    "title": "Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture",
    "authors": [
      "Mohamed El-Dosuky"
    ],
    "abstract": "The Internet of Things (IoT) has revolutionized various applications including agriculture, but it still faces challenges in data collection and understanding. This paper proposes a real-time framework with three additional semantic layers to help IoT devices and sensors comprehend data meaning and source. The framework consists of six layers: perception, semantic annotation, interoperability, transportation, semantic reasoning, and application, suitable for dynamic environments. Sensors collect data in the form of voltage, which is then processed by microprocessors or microcontrollers in the semantic annotation and preprocessing layer. Metadata is added to the raw data, including the purpose, ID number, and application. Two semantic algorithms are proposed in the semantic interoperability and ontologies layer: the interoperability semantic algorithm for standardizing file types and the synonym identification algorithm for identifying synonyms. In the transportation layer, raw data and metadata are sent to other IoT devices or cloud computing platforms using techniques like WiFi, Zigbee networks, Bluetooth, and mobile communication networks. A semantic reasoning layer is proposed to infer new knowledge from the existing data, using fuzzy logic, Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI) is proposed in the application layer to help users communicate with and monitor IoT sensors, devices, and new knowledge inferred. This framework provides a robust solution for managing IoT data, ensuring semantic completeness, and enabling real-time knowledge inference. The integration of uncertainty reasoning methods and semantic interoperability techniques makes this framework a valuable tool for advancing IoT applications in general and in agriculture in particular.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05187v1",
    "published_date": "2025-10-06 04:21:06 UTC",
    "updated_date": "2025-10-06 04:21:06 UTC"
  },
  {
    "arxiv_id": "2510.04474v1",
    "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization",
    "authors": [
      "Gang Li",
      "Yan Chen",
      "Ming Lin",
      "Tianbao Yang"
    ],
    "abstract": "Recent large reasoning models (LRMs) driven by reinforcement learning algorithms (e.g., GRPO) have achieved remarkable performance on challenging reasoning tasks. However, these models suffer from overthinking, generating unnecessarily long and redundant reasoning even for simple questions, which substantially increases computational cost and response latency. While existing methods incorporate length rewards to GRPO to promote concise reasoning, they incur significant performance degradation. We identify the root cause: when rewards for correct but long rollouts are penalized, GRPO's group-relative advantage function can assign them negative advantages, actively discouraging valid reasoning. To overcome this, we propose Decoupled Reward Policy Optimization (DRPO), a novel framework that decouples the length-based learning signal of correct rollouts from incorrect ones. DRPO ensures that reward signals for correct rollouts are normalized solely within the positive group, shielding them from interference by negative samples. The DRPO's objective is grounded in integrating an optimized positive data distribution, which maximizes length-based rewards under a KL regularization, into a discriminative objective. We derive a closed-form solution for this distribution, enabling efficient computation of the objective and its gradients using only on-policy data and importance weighting. Of independent interest, this formulation is general and can incorporate other preference rewards of positive data beyond length. Experiments on mathematical reasoning tasks demonstrate DRPO's significant superiority over six efficient reasoning baselines. Notably, with a 1.5B model, our method achieves 77\\% length reduction with only 1.1\\% performance loss on simple questions like GSM8k dataset, while the follow-up baseline sacrifices 4.3\\% for 68\\% length reduction.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.04474v1",
    "published_date": "2025-10-06 04:18:13 UTC",
    "updated_date": "2025-10-06 04:18:13 UTC"
  },
  {
    "arxiv_id": "2510.04472v1",
    "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection",
    "authors": [
      "Baber Jan",
      "Saeed Anwar",
      "Aiman H. El-Maleh",
      "Abdul Jabbar Siddiqui",
      "Abdul Bais"
    ],
    "abstract": "Camouflaged object detection segments objects with intrinsic similarity and edge disruption. Current detection methods rely on accumulated complex components. Each approach adds components such as boundary modules, attention mechanisms, and multi-scale processors independently. This accumulation creates a computational burden without proportional gains. To manage this complexity, they process at reduced resolutions, eliminating fine details essential for camouflage. We present SPEGNet, addressing fragmentation through a unified design. The architecture integrates multi-scale features via channel calibration and spatial enhancement. Boundaries emerge directly from context-rich representations, maintaining semantic-spatial alignment. Progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions. This design strikes a balance between boundary precision and regional consistency. SPEGNet achieves 0.887 $S_α$ on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed. Our approach excels across scales, from tiny, intricate objects to large, pattern-similar ones, while handling occlusion and ambiguous boundaries. Code, model weights, and results are available on \\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04472v1",
    "published_date": "2025-10-06 04:06:40 UTC",
    "updated_date": "2025-10-06 04:06:40 UTC"
  },
  {
    "arxiv_id": "2510.06267v1",
    "title": "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases",
    "authors": [
      "Khartik Uppalapati",
      "Shakeel Abdulkareem",
      "Bora Yimenicioglu"
    ],
    "abstract": "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion framework that generates realistic yet privacy-preserving synthetic electronic-health-record (EHR) trajectories for ultra-rare diseases. RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph comprising approximately 8 M typed edges. Meta-path scores extracted from this 8-million-edge KG modulate the per-token noise schedule in the forward stochastic differential equation, steering generation toward biologically plausible lab-medication-adverse-event co-occurrences while retaining score-based diffusion model stability. The reverse denoiser then produces timestamped sequences of lab-code, medication-code, and adverse-event-flag triples that contain no protected health information. On simulated ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean Discrepancy by 40 percent relative to an unguided diffusion baseline and by greater than 60 percent versus GAN counterparts, without sacrificing downstream predictive utility. A black-box membership-inference evaluation using the DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55 safe-release threshold and substantially better than the approximately 0.61 plus or minus 0.03 observed for non-KG baselines, demonstrating strong resistance to re-identification. These results suggest that integrating biomedical knowledge graphs directly into diffusion noise schedules can simultaneously enhance fidelity and privacy, enabling safer data sharing for rare-disease research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures, 2 tables. Submitted to IEEE International Conference on Data Science and Advanced Analytics (DSAA)",
    "pdf_url": "https://arxiv.org/pdf/2510.06267v1",
    "published_date": "2025-10-06 03:59:09 UTC",
    "updated_date": "2025-10-06 03:59:09 UTC"
  },
  {
    "arxiv_id": "2510.04465v1",
    "title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents",
    "authors": [
      "Zhiping Zhang",
      "Yi Evie Zhang",
      "Freda Shi",
      "Tianshi Li"
    ],
    "abstract": "Large Language Model (LLM) agents require personal information for personalization in order to better act on users' behalf in daily tasks, but this raises privacy concerns and a personalization-privacy dilemma. Agent's autonomy introduces both risks and opportunities, yet its effects remain unclear. To better understand this, we conducted a 3$\\times$3 between-subjects experiment ($N=450$) to study how agent's autonomy level and personalization influence users' privacy concerns, trust and willingness to use, as well as the underlying psychological processes. We find that personalization without considering users' privacy preferences increases privacy concerns and decreases trust and willingness to use. Autonomy moderates these effects: Intermediate autonomy flattens the impact of personalization compared to No- and Full autonomy conditions. Our results suggest that rather than aiming for perfect model alignment in output generation, balancing autonomy of agent's action and user control offers a promising path to mitigate the personalization-privacy dilemma.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04465v1",
    "published_date": "2025-10-06 03:38:54 UTC",
    "updated_date": "2025-10-06 03:38:54 UTC"
  },
  {
    "arxiv_id": "2510.04455v1",
    "title": "Inverse Mixed-Integer Programming: Learning Constraints then Objective Functions",
    "authors": [
      "Akira Kitaoka"
    ],
    "abstract": "In mixed-integer linear programming, data-driven inverse optimization that learns the objective function and the constraints from observed data plays an important role in constructing appropriate mathematical models for various fields, including power systems and scheduling. However, to the best of our knowledge, there is no known method for learning both the objective functions and the constraints. In this paper, we propose a two-stage method for a class of problems where the objective function is expressed as a linear combination of functions and the constraints are represented by functions and thresholds. Specifically, our method first learns the constraints and then learns the objective function. On the theoretical side, we show the proposed method can solve inverse optimization problems in finite dataset, develop statistical learning theory in pseudometric spaces and sub-Gaussian distributions, and construct a statistical learning for inverse optimization. On the experimental side, we demonstrate that our method is practically applicable for scheduling problems formulated as integer linear programmings with up to 100 decision variables, which are typical in real-world settings.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "33 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.04455v1",
    "published_date": "2025-10-06 03:02:43 UTC",
    "updated_date": "2025-10-06 03:02:43 UTC"
  },
  {
    "arxiv_id": "2510.09649v1",
    "title": "TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI",
    "authors": [
      "Khartik Uppalapati",
      "Bora Yimenicioglu",
      "Shakeel Abdulkareem",
      "Adan Eftekhari",
      "Bhavya Uppalapati",
      "Viraj Kamath"
    ],
    "abstract": "Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric neurodegenerative disorder whose early MRI signs are subtle and often missed. We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to detect early Batten disease from pediatric brain MRI with limited training cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and fine-tune it using metric-based few-shot learning (prototypical loss with 5-shot episodes). Our model achieves high accuracy (approximately 91%) and area under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2 from an international longitudinal cohort, 12 early-manifestation CLN2 cases reported by Cokal et al., and 8 public Radiopaedia scans) together with 90 age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to highlight disease-relevant brain regions, enabling explainable predictions. The model's small size and strong performance (sensitivity greater than 90%, specificity approximately 90%) demonstrates a practical AI solution for early Batten disease detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures, 1 table. Submitted to International Conference on Computational Intelligence and Sustainable Engineering Solutions (CISES)",
    "pdf_url": "https://arxiv.org/pdf/2510.09649v1",
    "published_date": "2025-10-06 03:01:36 UTC",
    "updated_date": "2025-10-06 03:01:36 UTC"
  },
  {
    "arxiv_id": "2510.04452v2",
    "title": "AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents",
    "authors": [
      "Jenny T. Liang",
      "Titus Barik",
      "Jeffrey Nichols",
      "Eldon Schoop",
      "Ruijia Cheng"
    ],
    "abstract": "Interface agents powered by generative AI models (referred to as \"agents\") can automate actions based on user commands. An important aspect of developing agents is their user experience (i.e., agent experience). There is a growing need to provide scaffolds for a broader set of individuals beyond AI engineers to prototype agent experiences, since they can contribute valuable perspectives to designing agent experiences. In this work, we explore the affordances agent prototyping systems should offer by conducting a requirements elicitation study with 12 participants with varying experience with agents. We identify key activities in agent experience prototyping and the desired capabilities of agent prototyping systems. We instantiate those capabilities in the AgentBuilder design probe for agent prototyping. We conduct an in situ agent prototyping study with 14 participants using AgentBuilder to validate the design requirements and elicit insights on how developers prototype agents and what their needs are in this process.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04452v2",
    "published_date": "2025-10-06 02:58:42 UTC",
    "updated_date": "2025-10-14 16:45:52 UTC"
  },
  {
    "arxiv_id": "2510.06266v1",
    "title": "Language models for longitudinal analysis of abusive content in Billboard Music Charts",
    "authors": [
      "Rohitash Chandra",
      "Yathin Suresh",
      "Divyansh Raj Sinha",
      "Sanchit Jindal"
    ],
    "abstract": "There is no doubt that there has been a drastic increase in abusive and sexually explicit content in music, particularly in Billboard Music Charts. However, there is a lack of studies that validate the trend for effective policy development, as such content has harmful behavioural changes in children and youths. In this study, we utilise deep learning methods to analyse songs (lyrics) from Billboard Charts of the United States in the last seven decades. We provide a longitudinal study using deep learning and language models and review the evolution of content using sentiment analysis and abuse detection, including sexually explicit content. Our results show a significant rise in explicit content in popular music from 1990 onwards. Furthermore, we find an increasing prevalence of songs with lyrics containing profane, sexually explicit, and otherwise inappropriate language. The longitudinal analysis of the ability of language models to capture nuanced patterns in lyrical content, reflecting shifts in societal norms and language use over time.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.06266v1",
    "published_date": "2025-10-06 01:59:21 UTC",
    "updated_date": "2025-10-06 01:59:21 UTC"
  },
  {
    "arxiv_id": "2511.17511v1",
    "title": "A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models",
    "authors": [
      "Bingkun Guo",
      "Wentian Li",
      "Xiaojian Liu",
      "Jiaqi Luo",
      "Zibin Yu",
      "Dalong Dong",
      "Shuyou Zhang",
      "Yiming Zhang"
    ],
    "abstract": "To accelerate mechanical design and enhance design quality and innovation, we present a Multidisciplinary Design and Optimization (MDO) Agent driven by Large Language Models (LLMs). The agent semi-automates the end-to-end workflow by orchestrating three core capabilities: (i) natural-language-driven parametric modeling, (ii) retrieval-augmented generation (RAG) for knowledge-grounded conceptualization, and (iii) intelligent orchestration of engineering software for performance verification and optimization. Working in tandem, these capabilities interpret high-level, unstructured intent, translate it into structured design representations, automatically construct parametric 3D CAD models, generate reliable concept variants using external knowledge bases, and conduct evaluation with iterative optimization via tool calls such as finite-element analysis (FEA). Validation on three representative cases - a gas-turbine blade, a machine-tool column, and a fractal heat sink - shows that the agent completes the pipeline from natural-language intent to verified and optimized designs with reduced manual scripting and setup effort, while promoting innovative design exploration. This work points to a practical path toward human-AI collaborative mechanical engineering and lays a foundation for more dependable, vertically customized MDO systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.17511v1",
    "published_date": "2025-10-06 01:26:55 UTC",
    "updated_date": "2025-10-06 01:26:55 UTC"
  },
  {
    "arxiv_id": "2510.04417v1",
    "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions",
    "authors": [
      "Wenyuan Zhao",
      "Adithya Balachandran",
      "Chao Tian",
      "Paul Pu Liang"
    ],
    "abstract": "The study of multimodality has garnered significant interest in fields where the analysis of interactions among multiple information sources can enhance predictive modeling, data fusion, and interpretability. Partial information decomposition (PID) has emerged as a useful information-theoretic framework to quantify the degree to which individual modalities independently, redundantly, or synergistically convey information about a target variable. However, existing PID methods depend on optimizing over a joint distribution constrained by estimated pairwise probability distributions, which are costly and inaccurate for continuous and high-dimensional modalities. Our first key insight is that the problem can be solved efficiently when the pairwise distributions are multivariate Gaussians, and we refer to this problem as Gaussian PID (GPID). We propose a new gradient-based algorithm that substantially improves the computational efficiency of GPID based on an alternative formulation of the underlying optimization problem. To generalize the applicability to non-Gaussian data, we learn information-preserving encoders to transform random variables of arbitrary input distributions into pairwise Gaussian random variables. Along the way, we resolved an open problem regarding the optimality of joint Gaussian solutions for GPID. Empirical validation in diverse synthetic examples demonstrates that our proposed method provides more accurate and efficient PID estimates than existing baselines. We further evaluate a series of large-scale multimodal benchmarks to show its utility in real-world applications of quantifying PID in multimodal datasets and selecting high-performing models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.04417v1",
    "published_date": "2025-10-06 01:08:34 UTC",
    "updated_date": "2025-10-06 01:08:34 UTC"
  },
  {
    "arxiv_id": "2510.05186v1",
    "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training",
    "authors": [
      "Hongpei Li",
      "Han Zhang",
      "Huikang Liu",
      "Dongdong Ge",
      "Yinyu Ye"
    ],
    "abstract": "Pipeline parallelism (PP) has become a standard technique for scaling large language model (LLM) training across multiple devices. However, despite recent progress in reducing memory consumption through activation offloading, existing approaches remain largely heuristic and coarse-grained, often overlooking the fine-grained trade-offs between memory, computation, and scheduling latency. In this work, we revisit the pipeline scheduling problem from a principled optimization perspective. We observe that prevailing strategies either rely on static rules or aggressively offload activations without fully leveraging the interaction between memory constraints and scheduling efficiency. To address this, we formulate scheduling as a constrained optimization problem that jointly accounts for memory capacity, activation reuse, and pipeline bubble minimization. Solving this model yields fine-grained schedules that reduce pipeline bubbles while adhering to strict memory budgets. Our approach complements existing offloading techniques: whereas prior approaches trade memory for time in a fixed pattern, we dynamically optimize the tradeoff with respect to model structure and hardware configuration. Experimental results demonstrate that our method consistently improves both throughput and memory utilization. In particular, we reduce idle pipeline time by up to 50% under the same per-device memory limit, and in some cases, enable the training of larger models within limited memory budgets.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.DC",
    "comment": "Use Mathematical Programming to model Pipeline Parallelism with Offloading to balance efficiency and memory requirement",
    "pdf_url": "https://arxiv.org/pdf/2510.05186v1",
    "published_date": "2025-10-06 01:06:33 UTC",
    "updated_date": "2025-10-06 01:06:33 UTC"
  },
  {
    "arxiv_id": "2510.04401v1",
    "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting",
    "authors": [
      "Xuyang Guo",
      "Zekai Huang",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang"
    ],
    "abstract": "Vision-Language Models (VLMs) have become a central focus of today's AI community, owing to their impressive abilities gained from training on large-scale vision-language data from the Web. These models have demonstrated strong performance across diverse tasks, including image understanding, video understanding, complex visual reasoning, and embodied AI. Despite these noteworthy successes, a fundamental question remains: Can VLMs count objects correctly? In this paper, we introduce a simple yet effective benchmark, VLMCountBench, designed under a minimalist setting with only basic geometric shapes (e.g., triangles, circles) and their compositions, focusing exclusively on counting tasks without interference from other factors. We adopt strict independent variable control and systematically study the effects of simple properties such as color, size, and prompt refinement in a controlled ablation. Our empirical results reveal that while VLMs can count reliably when only one shape type is present, they exhibit substantial failures when multiple shape types are combined (i.e., compositional counting). This highlights a fundamental empirical limitation of current VLMs and motivates important directions for future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04401v1",
    "published_date": "2025-10-06 00:11:24 UTC",
    "updated_date": "2025-10-06 00:11:24 UTC"
  },
  {
    "arxiv_id": "2510.04400v1",
    "title": "Large Language Models Preserve Semantic Isotopies in Story Continuations",
    "authors": [
      "Marc Cavazza"
    ],
    "abstract": "In this work, we explore the relevance of textual semantics to Large Language Models (LLMs), extending previous insights into the connection between distributional semantics and structural semantics. We investigate whether LLM-generated texts preserve semantic isotopies. We design a story continuation experiment using 10,000 ROCStories prompts completed by five LLMs. We first validate GPT-4o's ability to extract isotopies from a linguistic benchmark, then apply it to the generated stories. We then analyze structural (coverage, density, spread) and semantic properties of isotopies to assess how they are affected by completion. Results show that LLM completion within a given token horizon preserves semantic isotopies across multiple properties.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.04400v1",
    "published_date": "2025-10-06 00:03:12 UTC",
    "updated_date": "2025-10-06 00:03:12 UTC"
  }
]
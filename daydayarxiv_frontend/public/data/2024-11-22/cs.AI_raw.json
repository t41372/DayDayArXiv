[
  {
    "arxiv_id": "2411.16718v5",
    "title": "Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification",
    "authors": [
      "S P Sharan",
      "Minkyu Choi",
      "Sahil Shah",
      "Harsh Goel",
      "Mohammad Omama",
      "Sandeep Chinchali"
    ],
    "abstract": "Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,\nand CogVideoX are pushing the boundaries of synthetic video generation, with\nadoption seen in fields like robotics, autonomous driving, and entertainment.\nAs these models become prevalent, various metrics and benchmarks have emerged\nto evaluate the quality of the generated videos. However, these metrics\nemphasize visual quality and smoothness, neglecting temporal fidelity and\ntext-to-video alignment, which are crucial for safety-critical applications. To\naddress this gap, we introduce NeuS-V, a novel synthetic video evaluation\nmetric that rigorously assesses text-to-video alignment using neuro-symbolic\nformal verification techniques. Our approach first converts the prompt into a\nformally defined Temporal Logic (TL) specification and translates the generated\nvideo into an automaton representation. Then, it evaluates the text-to-video\nalignment by formally checking the video automaton against the TL\nspecification. Furthermore, we present a dataset of temporally extended prompts\nto evaluate state-of-the-art video generation models against our benchmark. We\nfind that NeuS-V demonstrates a higher correlation by over 5x with human\nevaluations when compared to existing metrics. Our evaluation further reveals\nthat current video generation models perform poorly on these temporally complex\nprompts, highlighting the need for future work in improving text-to-video\ngeneration capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16718v5",
    "published_date": "2024-11-22 23:59:12 UTC",
    "updated_date": "2025-04-25 02:50:58 UTC"
  },
  {
    "arxiv_id": "2411.15380v1",
    "title": "Nd-BiMamba2: A Unified Bidirectional Architecture for Multi-Dimensional Data Processing",
    "authors": [
      "Hao Liu"
    ],
    "abstract": "Deep learning models often require specially designed architectures to\nprocess data of different dimensions, such as 1D time series, 2D images, and 3D\nvolumetric data. Existing bidirectional models mainly focus on sequential data,\nmaking it difficult to scale effectively to higher dimensions. To address this\nissue, we propose a novel multi-dimensional bidirectional neural network\narchitecture, named Nd-BiMamba2, which efficiently handles 1D, 2D, and 3D data.\nNd-BiMamba2 is based on the Mamba2 module and introduces innovative\nbidirectional processing mechanisms and adaptive padding strategies to capture\nbidirectional information in multi-dimensional data while maintaining\ncomputational efficiency. Unlike existing methods that require designing\nspecific architectures for different dimensional data, Nd-BiMamba2 adopts a\nunified architecture with a modular design, simplifying development and\nmaintenance costs. To verify the portability and flexibility of Nd-BiMamba2, we\nsuccessfully exported it to ONNX and TorchScript and tested it on different\nhardware platforms (e.g., CPU, GPU, and mobile devices). Experimental results\nshow that Nd-BiMamba2 runs efficiently on multiple platforms, demonstrating its\npotential in practical applications. The code is open-source:\nhttps://github.com/Human9000/nd-Mamba2-torch",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15380v1",
    "published_date": "2024-11-22 23:45:15 UTC",
    "updated_date": "2024-11-22 23:45:15 UTC"
  },
  {
    "arxiv_id": "2411.15375v1",
    "title": "AdamZ: An Enhanced Optimisation Method for Neural Network Training",
    "authors": [
      "Ilia Zaznov",
      "Atta Badii",
      "Alfonso Dufour",
      "Julian Kunkel"
    ],
    "abstract": "AdamZ is an advanced variant of the Adam optimiser, developed to enhance\nconvergence efficiency in neural network training. This optimiser dynamically\nadjusts the learning rate by incorporating mechanisms to address overshooting\nand stagnation, that are common challenges in optimisation. Specifically, AdamZ\nreduces the learning rate when overshooting is detected and increases it during\nperiods of stagnation, utilising hyperparameters such as overshoot and\nstagnation factors, thresholds, and patience levels to guide these adjustments.\nWhile AdamZ may lead to slightly longer training times compared to some other\noptimisers, it consistently excels in minimising the loss function, making it\nparticularly advantageous for applications where precision is critical.\nBenchmarking results demonstrate the effectiveness of AdamZ in maintaining\noptimal learning rates, leading to improved model performance across diverse\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 9 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.15375v1",
    "published_date": "2024-11-22 23:33:41 UTC",
    "updated_date": "2024-11-22 23:33:41 UTC"
  },
  {
    "arxiv_id": "2411.15372v1",
    "title": "Transforming NLU with Babylon: A Case Study in Development of Real-time, Edge-Efficient, Multi-Intent Translation System for Automated Drive-Thru Ordering",
    "authors": [
      "Mostafa Varzaneh",
      "Pooja Voladoddi",
      "Tanmay Bakshi",
      "Uma Gunturi"
    ],
    "abstract": "Real-time conversational AI agents face challenges in performing Natural\nLanguage Understanding (NLU) in dynamic, outdoor environments like automated\ndrive-thru systems. These settings require NLU models to handle background\nnoise, diverse accents, and multi-intent queries while operating under strict\nlatency and memory constraints on edge devices. Additionally, robustness to\nerrors from upstream Automatic Speech Recognition (ASR) is crucial, as ASR\noutputs in these environments are often noisy. We introduce Babylon, a\ntransformer-based architecture that tackles NLU as an intent translation task,\nconverting natural language inputs into sequences of regular language units\n('transcodes') that encode both intents and slot information. This formulation\nallows Babylon to manage multi-intent scenarios in a single dialogue turn.\nFurthermore, Babylon incorporates an LSTM-based token pooling mechanism to\npreprocess phoneme sequences, reducing input length and optimizing for\nlow-latency, low-memory edge deployment. This also helps mitigate inaccuracies\nin ASR outputs, enhancing system robustness. While this work focuses on\ndrive-thru ordering, Babylon's design extends to similar noise-prone scenarios,\nfor e.g. ticketing kiosks. Our experiments show that Babylon achieves\nsignificantly better accuracy-latency-memory footprint trade-offs over\ntypically employed NMT models like Flan-T5 and BART, demonstrating its\neffectiveness for real-time NLU in edge deployment settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.15372v1",
    "published_date": "2024-11-22 23:03:35 UTC",
    "updated_date": "2024-11-22 23:03:35 UTC"
  },
  {
    "arxiv_id": "2411.15370v1",
    "title": "Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers",
    "authors": [
      "Gautham Vasan",
      "Mohamed Elsayed",
      "Alireza Azimi",
      "Jiamin He",
      "Fahim Shariar",
      "Colin Bellinger",
      "Martha White",
      "A. Rupam Mahmood"
    ],
    "abstract": "Modern deep policy gradient methods achieve effective performance on\nsimulated robotic tasks, but they all require large replay buffers or expensive\nbatch updates, or both, making them incompatible for real systems with\nresource-limited computers. We show that these methods fail catastrophically\nwhen limited to small replay buffers or during incremental learning, where\nupdates only use the most recent sample without batch updates or a replay\nbuffer. We propose a novel incremental deep policy gradient method -- Action\nValue Gradient (AVG) and a set of normalization and scaling techniques to\naddress the challenges of instability in incremental learning. On robotic\nsimulation benchmarks, we show that AVG is the only incremental method that\nlearns effectively, often achieving final performance comparable to batch\npolicy gradient methods. This advancement enabled us to show for the first time\neffective deep reinforcement learning with real robots using only incremental\nupdates, employing a robotic manipulator and a mobile robot.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "In The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems. Source code at https://github.com/gauthamvasan/avg and\n  companion video at https://youtu.be/cwwuN6Hyew0",
    "pdf_url": "http://arxiv.org/pdf/2411.15370v1",
    "published_date": "2024-11-22 22:46:21 UTC",
    "updated_date": "2024-11-22 22:46:21 UTC"
  },
  {
    "arxiv_id": "2411.15367v2",
    "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
    "authors": [
      "Soumil Datta",
      "Shih-Chieh Dai",
      "Leo Yu",
      "Guanhong Tao"
    ],
    "abstract": "Text-to-image diffusion models, such as Stable Diffusion, have shown\nexceptional potential in generating high-quality images. However, recent\nstudies highlight concerns over the use of unauthorized data in training these\nmodels, which may lead to intellectual property infringement or privacy\nviolations. A promising approach to mitigate these issues is to apply a\nwatermark to images and subsequently check if generative models reproduce\nsimilar watermark features. In this paper, we examine the robustness of various\nwatermark-based protection methods applied to text-to-image models. We observe\nthat common image transformations are ineffective at removing the watermark\neffect. Therefore, we propose RATTAN, that leverages the diffusion process to\nconduct controlled image generation on the protected input, preserving the\nhigh-level features of the input while ignoring the low-level details utilized\nby watermarks. A small number of generated images are then used to fine-tune\nprotected models. Our experiments on three datasets and 140 text-to-image\ndiffusion models reveal that existing state-of-the-art protections are not\nrobust against RATTAN.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15367v2",
    "published_date": "2024-11-22 22:28:19 UTC",
    "updated_date": "2024-11-26 19:12:32 UTC"
  },
  {
    "arxiv_id": "2411.15364v2",
    "title": "Exploring Facets of Language Generation in the Limit",
    "authors": [
      "Moses Charikar",
      "Chirag Pabbaraju"
    ],
    "abstract": "The recent work of Kleinberg & Mullainathan [KM24] provides a concrete model\nfor language generation in the limit: given a sequence of examples from an\nunknown target language, the goal is to generate new examples from the target\nlanguage such that no incorrect examples are generated beyond some point. In\nsharp contrast to strong negative results for the closely related problem of\nlanguage identification, they establish positive results for language\ngeneration in the limit for all countable collections of languages. Follow-up\nwork by Raman & Tewari [RT24] studies bounds on the number of distinct inputs\nrequired by an algorithm before correct language generation is achieved --\nnamely, whether this is a constant for all languages in the collection (uniform\ngeneration) or a language-dependent constant (non-uniform generation).\n  We show that every countable language collection has a generator which has\nthe stronger property of non-uniform generation in the limit. However, while\nthe generation algorithm of [KM24] can be implemented using membership queries,\nwe show that any algorithm cannot non-uniformly generate even for collections\nof just two languages, using only membership queries.\n  We also formalize the tension between validity and breadth in the generation\nalgorithm of [KM24] by introducing a definition of exhaustive generation, and\nshow a strong negative result for exhaustive generation. Our result shows that\na tradeoff between validity and breadth is inherent for generation in the\nlimit. We also provide a precise characterization of the language collections\nfor which exhaustive generation is possible. Finally, inspired by algorithms\nthat can choose to obtain feedback, we consider a model of uniform generation\nwith feedback, completely characterizing language collections for which such\nuniform generation with feedback is possible in terms of a complexity measure\nof the collection.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.DS",
    "comment": "31 pages. Fixed typos, updated related work, added results on\n  characterization of exhaustive generation",
    "pdf_url": "http://arxiv.org/pdf/2411.15364v2",
    "published_date": "2024-11-22 22:13:40 UTC",
    "updated_date": "2024-12-24 10:57:49 UTC"
  },
  {
    "arxiv_id": "2411.15361v2",
    "title": "Designing Cellular Manufacturing System in Presence of Alternative Process Plans",
    "authors": [
      "Md. Kutub Uddin",
      "Md. Saiful Islam",
      "Md Abrar Jahin",
      "Md. Tanjid Hossen Irfan",
      "Md. Saiful Islam Seam",
      "M. F. Mridha"
    ],
    "abstract": "In the design of cellular manufacturing systems (CMS), numerous technological\nand managerial decisions must be made at both the design and operational\nstages. The first step in designing a CMS involves grouping parts and machines.\nIn this paper, four integer programming formulations are presented for grouping\nparts and machines in a CMS at both the design and operational levels for a\ngeneralized grouping problem, where each part has more than one process plan,\nand each operation of a process plan can be performed on more than one machine.\nThe minimization of inter-cell and intra-cell movements is achieved by\nassigning the maximum possible number of consecutive operations of a part type\nto the same cell and to the same machine, respectively. The suitability of\nminimizing inter-cell and intra-cell movements as an objective, compared to\nother objectives such as minimizing investment costs on machines, operating\ncosts, etc., is discussed. Numerical examples are included to illustrate the\nworkings of the formulations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15361v2",
    "published_date": "2024-11-22 22:10:42 UTC",
    "updated_date": "2024-12-04 20:56:57 UTC"
  },
  {
    "arxiv_id": "2411.15356v2",
    "title": "Regulator-Manufacturer AI Agents Modeling: Mathematical Feedback-Driven Multi-Agent LLM Framework",
    "authors": [
      "Yu Han",
      "Zekun Guo"
    ],
    "abstract": "The increasing complexity of regulatory updates from global authorities\npresents significant challenges for medical device manufacturers, necessitating\nagile strategies to sustain compliance and maintain market access.\nConcurrently, regulatory bodies must effectively monitor manufacturers'\nresponses and develop strategic surveillance plans. This study employs a\nmulti-agent modeling approach, enhanced with Large Language Models (LLMs), to\nsimulate regulatory dynamics and examine the adaptive behaviors of key actors,\nincluding regulatory bodies, manufacturers, and competitors. These agents\noperate within a simulated environment governed by regulatory flow theory,\ncapturing the impacts of regulatory changes on compliance decisions, market\nadaptation, and innovation strategies. Our findings illuminate the influence of\nregulatory shifts on industry behaviour and identify strategic opportunities\nfor improving regulatory practices, optimizing compliance, and fostering\ninnovation. By leveraging the integration of multi-agent systems and LLMs, this\nresearch provides a novel perspective and offers actionable insights for\nstakeholders navigating the evolving regulatory landscape of the medical device\nindustry.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15356v2",
    "published_date": "2024-11-22 22:02:56 UTC",
    "updated_date": "2024-12-21 12:43:59 UTC"
  },
  {
    "arxiv_id": "2411.15355v2",
    "title": "UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations",
    "authors": [
      "Yuan Ren",
      "Guile Wu",
      "Runhao Li",
      "Zheyuan Yang",
      "Yibo Liu",
      "Xingxin Chen",
      "Tongtong Cao",
      "Bingbing Liu"
    ],
    "abstract": "Urban scene reconstruction is crucial for real-world autonomous driving\nsimulators. Although existing methods have achieved photorealistic\nreconstruction, they mostly focus on pinhole cameras and neglect fisheye\ncameras. In fact, how to effectively simulate fisheye cameras in driving scene\nremains an unsolved problem. In this work, we propose UniGaussian, a novel\napproach that learns a unified 3D Gaussian representation from multiple camera\nmodels for urban scene reconstruction in autonomous driving. Our contributions\nare two-fold. First, we propose a new differentiable rendering method that\ndistorts 3D Gaussians using a series of affine transformations tailored to\nfisheye camera models. This addresses the compatibility issue of 3D Gaussian\nsplatting with fisheye cameras, which is hindered by light ray distortion\ncaused by lenses or mirrors. Besides, our method maintains real-time rendering\nwhile ensuring differentiability. Second, built on the differentiable rendering\nmethod, we design a new framework that learns a unified Gaussian representation\nfrom multiple camera models. By applying affine transformations to adapt\ndifferent camera models and regularizing the shared Gaussians with supervision\nfrom different modalities, our framework learns a unified 3D Gaussian\nrepresentation with input data from multiple sources and achieves holistic\ndriving scene understanding. As a result, our approach models multiple sensors\n(pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR\npoint clouds). Our experiments show that our method achieves superior rendering\nquality and fast rendering speed for driving scene simulation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2411.15355v2",
    "published_date": "2024-11-22 21:59:46 UTC",
    "updated_date": "2025-03-01 00:05:39 UTC"
  },
  {
    "arxiv_id": "2411.15331v1",
    "title": "GeoScatt-GNN: A Geometric Scattering Transform-Based Graph Neural Network Model for Ames Mutagenicity Prediction",
    "authors": [
      "Abdeljalil Zoubir",
      "Badr Missaoui"
    ],
    "abstract": "This paper tackles the pressing challenge of mutagenicity prediction by\nintroducing three ground-breaking approaches. First, it showcases the superior\nperformance of 2D scattering coefficients extracted from molecular images,\ncompared to traditional molecular descriptors. Second, it presents a hybrid\napproach that combines geometric graph scattering (GGS), Graph Isomorphism\nNetworks (GIN), and machine learning models, achieving strong results in\nmutagenicity prediction. Third, it introduces a novel graph neural network\narchitecture, MOLG3-SAGE, which integrates GGS node features into a fully\nconnected graph structure, delivering outstanding predictive accuracy.\nExperimental results on the ZINC dataset demonstrate significant improvements,\nemphasizing the effectiveness of blending 2D and geometric scattering\ntechniques with graph neural networks. This study illustrates the potential of\nGNNs and GGS for mutagenicity prediction, with broad implications for drug\ndiscovery and chemical safety assessment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15331v1",
    "published_date": "2024-11-22 19:52:56 UTC",
    "updated_date": "2024-11-22 19:52:56 UTC"
  },
  {
    "arxiv_id": "2411.15320v1",
    "title": "PPLqa: An Unsupervised Information-Theoretic Quality Metric for Comparing Generative Large Language Models",
    "authors": [
      "Gerald Friedland",
      "Xin Huang",
      "Yueying Cui",
      "Vishaal Kapoor",
      "Ashish Khetan",
      "Sanjiv Das"
    ],
    "abstract": "We propose PPLqa, an easy to compute, language independent,\ninformation-theoretic metric to measure the quality of responses of generative\nLarge Language Models (LLMs) in an unsupervised way, without requiring ground\ntruth annotations or human supervision. The method and metric enables users to\nrank generative language models for quality of responses, so as to make a\nselection of the best model for a given task. Our single metric assesses LLMs\nwith an approach that subsumes, but is not explicitly based on, coherence and\nfluency (quality of writing) and relevance and consistency (appropriateness of\nresponse) to the query. PPLqa performs as well as other related metrics, and\nworks better with long-form Q\\&A. Thus, PPLqa enables bypassing the lengthy\nannotation process required for ground truth evaluations, and it also\ncorrelates well with human and LLM rankings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.m; E.4"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15320v1",
    "published_date": "2024-11-22 19:28:06 UTC",
    "updated_date": "2024-11-22 19:28:06 UTC"
  },
  {
    "arxiv_id": "2411.15296v2",
    "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
    "authors": [
      "Chaoyou Fu",
      "Yi-Fan Zhang",
      "Shukang Yin",
      "Bo Li",
      "Xinyu Fang",
      "Sirui Zhao",
      "Haodong Duan",
      "Xing Sun",
      "Ziwei Liu",
      "Liang Wang",
      "Caifeng Shan",
      "Ran He"
    ],
    "abstract": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal\nLarge Language Models (MLLMs) have garnered increased attention from both\nindustry and academia. Building upon pre-trained LLMs, this family of models\nfurther develops multimodal perception and reasoning capabilities that are\nimpressive, such as writing code given a flow chart or creating stories based\non an image. In the development process, evaluation is critical since it\nprovides intuitive feedback and guidance on improving models. Distinct from the\ntraditional train-eval-test paradigm that only favors a single task like image\nclassification, the versatility of MLLMs has spurred the rise of various new\nbenchmarks and evaluation methods. In this paper, we aim to present a\ncomprehensive survey of MLLM evaluation, discussing four key aspects: 1) the\nsummarised benchmarks types divided by the evaluation capabilities, including\nfoundation capabilities, model self-analysis, and extented applications; 2) the\ntypical process of benchmark counstruction, consisting of data collection,\nannotation, and precautions; 3) the systematic evaluation manner composed of\njudge, metric, and toolkit; 4) the outlook for the next benchmark. This work\naims to offer researchers an easy grasp of how to effectively evaluate MLLMs\naccording to different needs and to inspire better evaluation methods, thereby\ndriving the progress of MLLM research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Produced by MME+MMBench+LLaVA Teams. Project Page:\n  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2411.15296v2",
    "published_date": "2024-11-22 18:59:54 UTC",
    "updated_date": "2024-12-08 04:24:31 UTC"
  },
  {
    "arxiv_id": "2411.15129v1",
    "title": "Measuring Bullshit in the Language Games played by ChatGPT",
    "authors": [
      "Alessandro Trevisan",
      "Harry Giddens",
      "Sarah Dillon",
      "Alan F. Blackwell"
    ],
    "abstract": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15129v1",
    "published_date": "2024-11-22 18:55:21 UTC",
    "updated_date": "2024-11-22 18:55:21 UTC"
  },
  {
    "arxiv_id": "2411.15128v2",
    "title": "Health AI Developer Foundations",
    "authors": [
      "Atilla P. Kiraly",
      "Sebastien Baur",
      "Kenneth Philbrick",
      "Fereshteh Mahvar",
      "Liron Yatziv",
      "Tiffany Chen",
      "Bram Sterling",
      "Nick George",
      "Fayaz Jamil",
      "Jing Tang",
      "Kai Bailey",
      "Faruk Ahmed",
      "Akshay Goel",
      "Abbi Ward",
      "Lin Yang",
      "Andrew Sellergren",
      "Yossi Matias",
      "Avinatan Hassidim",
      "Shravya Shetty",
      "Daniel Golden",
      "Shekoofeh Azizi",
      "David F. Steiner",
      "Yun Liu",
      "Tim Thelin",
      "Rory Pilgrim",
      "Can Kirmizibayrak"
    ],
    "abstract": "Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15128v2",
    "published_date": "2024-11-22 18:51:51 UTC",
    "updated_date": "2024-11-26 18:01:08 UTC"
  },
  {
    "arxiv_id": "2411.15122v1",
    "title": "ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation",
    "authors": [
      "Xiaoman Zhang",
      "Hong-Yu Zhou",
      "Xiaoli Yang",
      "Oishi Banerjee",
      "Julián N. Acosta",
      "Josh Miller",
      "Ouwen Huang",
      "Pranav Rajpurkar"
    ],
    "abstract": "AI-driven models have demonstrated significant potential in automating\nradiology report generation for chest X-rays. However, there is no standardized\nbenchmark for objectively evaluating their performance. To address this, we\npresent ReXrank, https://rexrank.ai, a public leaderboard and challenge for\nassessing AI-powered radiology report generation. Our framework incorporates\nReXGradient, the largest test dataset consisting of 10,000 studies, and three\npublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation\nassessment. ReXrank employs 8 evaluation metrics and separately assesses models\ncapable of generating only findings sections and those providing both findings\nand impressions sections. By providing this standardized evaluation framework,\nReXrank enables meaningful comparisons of model performance and offers crucial\ninsights into their robustness across diverse clinical settings. Beyond its\ncurrent focus on chest X-rays, ReXrank's framework sets the stage for\ncomprehensive evaluation of automated reporting across the full spectrum of\nmedical imaging.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15122v1",
    "published_date": "2024-11-22 18:40:02 UTC",
    "updated_date": "2024-11-22 18:40:02 UTC"
  },
  {
    "arxiv_id": "2411.15115v2",
    "title": "VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement",
    "authors": [
      "Daeun Lee",
      "Jaehong Yoon",
      "Jaemin Cho",
      "Mohit Bansal"
    ],
    "abstract": "Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of two stages: In (1) video refinement planning, we first\ndetect misalignments by generating fine-grained evaluation questions and\nanswering them using an MLLM. Based on video evaluation outputs, we identify\naccurately generated objects and construct localized prompts to precisely\nrefine misaligned regions. In (2) localized refinement, we enhance video\nalignment by 'repairing' the misaligned regions from the original video while\npreserving the correctly generated areas. This is achieved by frame-wise region\ndecomposition using our Region-Preserving Segmentation (RPS) module. On two\npopular video generation benchmarks (EvalCrafter and T2V-CompBench),\nVideoRepair substantially outperforms recent baselines across various\ntext-video alignment metrics. We provide a comprehensive analysis of\nVideoRepair components and qualitative examples.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://video-repair.github.io",
    "pdf_url": "http://arxiv.org/pdf/2411.15115v2",
    "published_date": "2024-11-22 18:31:47 UTC",
    "updated_date": "2025-03-19 21:39:33 UTC"
  },
  {
    "arxiv_id": "2411.15114v1",
    "title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts",
    "authors": [
      "Hjalmar Wijk",
      "Tao Lin",
      "Joel Becker",
      "Sami Jawhar",
      "Neev Parikh",
      "Thomas Broadley",
      "Lawrence Chan",
      "Michael Chen",
      "Josh Clymer",
      "Jai Dhyani",
      "Elena Ericheva",
      "Katharyn Garcia",
      "Brian Goodrich",
      "Nikola Jurkovic",
      "Megan Kinniment",
      "Aron Lajko",
      "Seraphina Nix",
      "Lucas Sato",
      "William Saunders",
      "Maksym Taran",
      "Ben West",
      "Elizabeth Barnes"
    ],
    "abstract": "Frontier AI safety policies highlight automation of AI research and\ndevelopment (R&D) by AI agents as an important capability to anticipate.\nHowever, there exist few evaluations for AI R&D capabilities, and none that are\nhighly realistic and have a direct comparison to human performance. We\nintroduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7\nchallenging, open-ended ML research engineering environments and data from 71\n8-hour attempts by 61 distinct human experts. We confirm that our experts make\nprogress in the environments given 8 hours, with 82% of expert attempts\nachieving a non-zero score and 24% matching or exceeding our strong reference\nsolutions. We compare humans to several public frontier models through\nbest-of-k with varying time budgets and agent designs, and find that the best\nAI agents achieve a score 4x higher than human experts when both are given a\ntotal time budget of 2 hours per environment. However, humans currently display\nbetter returns to increasing time budgets, narrowly exceeding the top AI agent\nscores given an 8-hour budget, and achieving 2x the score of the top AI agent\nwhen both are given 32 total hours (across different attempts). Qualitatively,\nwe find that modern AI agents possess significant expertise in many ML topics\n-- e.g. an agent wrote a faster custom Triton kernel than any of our human\nexperts' -- and can generate and test solutions over ten times faster than\nhumans, at much lower cost. We open-source the evaluation environments, human\nexpert data, analysis code and agent trajectories to facilitate future\nresearch.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15114v1",
    "published_date": "2024-11-22 18:30:46 UTC",
    "updated_date": "2024-11-22 18:30:46 UTC"
  },
  {
    "arxiv_id": "2411.15113v1",
    "title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion",
    "authors": [
      "Samarth N Ramesh",
      "Zhixue Zhao"
    ],
    "abstract": "As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15113v1",
    "published_date": "2024-11-22 18:29:37 UTC",
    "updated_date": "2024-11-22 18:29:37 UTC"
  },
  {
    "arxiv_id": "2411.15292v2",
    "title": "Influence functions and regularity tangents for efficient active learning",
    "authors": [
      "Frederik Eaton"
    ],
    "abstract": "In this paper we describe an efficient method for providing a regression\nmodel with a sense of curiosity about its data. In the field of machine\nlearning, our framework for representing curiosity is called Active Learning,\nwhich concerns the problem of automatically choosing data points for which to\nquery labels in the semi-supervised setting. The methods we propose are based\non computing a \"regularity tangent\" vector that can be calculated (with only a\nconstant slow-down) together with the model's parameter vector during training.\nWe then take the inner product of this tangent vector with the gradient vector\nof the model's loss at a given data point to obtain a measure of the influence\nof that point on the complexity of the model. In the simplest instantiation,\nthere is only a single regularity tangent vector, of the same dimension as the\nparameter vector. Thus, in the proposed technique, once training is complete,\nevaluating our \"curiosity\" about a potential query data point can be done as\nquickly as calculating the model's loss gradient at that point. The new vector\nonly doubles the amount of storage required by the model. We show that the\nquantity computed by our technique is an example of an \"influence function\",\nand that it measures the expected squared change in model complexity incurred\nby up-weighting a given data point. We propose a number of ways for using this\nand other related quantities to choose new training data points for a\nregression model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15292v2",
    "published_date": "2024-11-22 18:14:26 UTC",
    "updated_date": "2025-03-18 15:17:32 UTC"
  },
  {
    "arxiv_id": "2411.15106v2",
    "title": "About Time: Advances, Challenges, and Outlooks of Action Understanding",
    "authors": [
      "Alexandros Stergiou",
      "Ronald Poppe"
    ],
    "abstract": "We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext across multiple modalities. This survey comprehensively reviews\nadvances in uni- and multi-modal action understanding across a range of tasks.\nWe focus on prevalent challenges, overview widely adopted datasets, and survey\nseminal works with an emphasis on recent advances. We broadly distinguish\nbetween three temporal scopes: (1) recognition tasks of actions observed in\nfull, (2) prediction tasks for ongoing partially observed actions, and (3)\nforecasting tasks for subsequent unobserved action(s). This division allows us\nto identify specific action modeling and video representation challenges.\nFinally, we outline future directions to address current shortcomings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the International Journal of Computer Vision (IJCV)",
    "pdf_url": "http://arxiv.org/pdf/2411.15106v2",
    "published_date": "2024-11-22 18:09:27 UTC",
    "updated_date": "2025-05-06 08:18:58 UTC"
  },
  {
    "arxiv_id": "2411.15100v3",
    "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models",
    "authors": [
      "Yixin Dong",
      "Charlie F. Ruan",
      "Yaxing Cai",
      "Ruihang Lai",
      "Ziyi Xu",
      "Yilong Zhao",
      "Tianqi Chen"
    ],
    "abstract": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.CL",
    "comment": "MLSys '25",
    "pdf_url": "http://arxiv.org/pdf/2411.15100v3",
    "published_date": "2024-11-22 18:01:37 UTC",
    "updated_date": "2025-05-12 08:20:08 UTC"
  },
  {
    "arxiv_id": "2411.15098v5",
    "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
    "authors": [
      "Zhenxiong Tan",
      "Songhua Liu",
      "Xingyi Yang",
      "Qiaochu Xue",
      "Xinchao Wang"
    ],
    "abstract": "We present OminiControl, a novel approach that rethinks how image conditions\nare integrated into Diffusion Transformer (DiT) architectures. Current image\nconditioning methods either introduce substantial parameter overhead or handle\nonly specific control tasks effectively, limiting their practical versatility.\nOminiControl addresses these limitations through three key innovations: (1) a\nminimal architectural design that leverages the DiT's own VAE encoder and\ntransformer blocks, requiring just 0.1% additional parameters; (2) a unified\nsequence processing strategy that combines condition tokens with image tokens\nfor flexible token interactions; and (3) a dynamic position encoding mechanism\nthat adapts to both spatially-aligned and non-aligned control tasks. Our\nextensive experiments show that this streamlined approach not only matches but\nsurpasses the performance of specialized methods across multiple conditioning\ntasks. To overcome data limitations in subject-driven generation, we also\nintroduce Subjects200K, a large-scale dataset of identity-consistent image\npairs synthesized using DiT models themselves. This work demonstrates that\neffective image control can be achieved without architectural complexity,\nopening new possibilities for efficient and versatile image generation systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15098v5",
    "published_date": "2024-11-22 17:55:15 UTC",
    "updated_date": "2025-03-11 10:41:44 UTC"
  },
  {
    "arxiv_id": "2411.15096v2",
    "title": "RED: Effective Trajectory Representation Learning with Comprehensive Information",
    "authors": [
      "Silin Zhou",
      "Shuo Shang",
      "Lisi Chen",
      "Christian S. Jensen",
      "Panos Kalnis"
    ],
    "abstract": "Trajectory representation learning (TRL) maps trajectories to vectors that\ncan then be used for various downstream tasks, including trajectory similarity\ncomputation, trajectory classification, and travel-time estimation. However,\nexisting TRL methods often produce vectors that, when used in downstream tasks,\nyield insufficiently accurate results. A key reason is that they fail to\nutilize the comprehensive information encompassed by trajectories. We propose a\nself-supervised TRL framework, called RED, which effectively exploits multiple\ntypes of trajectory information. Overall, RED adopts the Transformer as the\nbackbone model and masks the constituting paths in trajectories to train a\nmasked autoencoder (MAE). In particular, RED considers the moving patterns of\ntrajectories by employing a Road-aware masking strategy} that retains key paths\nof trajectories during masking, thereby preserving crucial information of the\ntrajectories. RED also adopts a spatial-temporal-user joint Embedding scheme to\nencode comprehensive information when preparing the trajectories as model\ninputs. To conduct training, RED adopts Dual-objective task learning}: the\nTransformer encoder predicts the next segment in a trajectory, and the\nTransformer decoder reconstructs the entire trajectory. RED also considers the\nspatial-temporal correlations of trajectories by modifying the attention\nmechanism of the Transformer. We compare RED with 9 state-of-the-art TRL\nmethods for 4 downstream tasks on 3 real-world datasets, finding that RED can\nusually improve the accuracy of the best-performing baseline by over 5%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by VLDB2025",
    "pdf_url": "http://arxiv.org/pdf/2411.15096v2",
    "published_date": "2024-11-22 17:51:21 UTC",
    "updated_date": "2024-11-28 12:40:17 UTC"
  },
  {
    "arxiv_id": "2411.15082v1",
    "title": "Towards Speaker Identification with Minimal Dataset and Constrained Resources using 1D-Convolution Neural Network",
    "authors": [
      "Irfan Nafiz Shahan",
      "Pulok Ahmed Auvi"
    ],
    "abstract": "Voice recognition and speaker identification are vital for applications in\nsecurity and personal assistants. This paper presents a lightweight\n1D-Convolutional Neural Network (1D-CNN) designed to perform speaker\nidentification on minimal datasets. Our approach achieves a validation accuracy\nof 97.87%, leveraging data augmentation techniques to handle background noise\nand limited training samples. Future improvements include testing on larger\ndatasets and integrating transfer learning methods to enhance generalizability.\nWe provide all code, the custom dataset, and the trained models to facilitate\nreproducibility. These resources are available on our GitHub repository:\nhttps://github.com/IrfanNafiz/RecMe.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15082v1",
    "published_date": "2024-11-22 17:18:08 UTC",
    "updated_date": "2024-11-22 17:18:08 UTC"
  },
  {
    "arxiv_id": "2411.15287v1",
    "title": "Sycophancy in Large Language Models: Causes and Mitigations",
    "authors": [
      "Lars Malmqvist"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. However, their tendency to\nexhibit sycophantic behavior - excessively agreeing with or flattering users -\nposes significant risks to their reliability and ethical deployment. This paper\nprovides a technical survey of sycophancy in LLMs, analyzing its causes,\nimpacts, and potential mitigation strategies. We review recent work on\nmeasuring and quantifying sycophantic tendencies, examine the relationship\nbetween sycophancy and other challenges like hallucination and bias, and\nevaluate promising techniques for reducing sycophancy while maintaining model\nperformance. Key approaches explored include improved training data, novel\nfine-tuning methods, post-deployment control mechanisms, and decoding\nstrategies. We also discuss the broader implications of sycophancy for AI\nalignment and propose directions for future research. Our analysis suggests\nthat mitigating sycophancy is crucial for developing more robust, reliable, and\nethically-aligned language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15287v1",
    "published_date": "2024-11-22 16:56:49 UTC",
    "updated_date": "2024-11-22 16:56:49 UTC"
  },
  {
    "arxiv_id": "2411.15285v1",
    "title": "Forecasting Unseen Points of Interest Visits Using Context and Proximity Priors",
    "authors": [
      "Ziyao Li",
      "Shang-Ling Hsu",
      "Cyrus Shahabi"
    ],
    "abstract": "Understanding human mobility behavior is crucial for numerous applications,\nincluding crowd management, location-based recommendations, and the estimation\nof pandemic spread. Machine learning models can predict the Points of Interest\n(POIs) that individuals are likely to visit in the future by analyzing their\nhistorical visit patterns. Previous studies address this problem by learning a\nPOI classifier, where each class corresponds to a POI. However, this limits\ntheir applicability to predict a new POI that was not in the training data,\nsuch as the opening of new restaurants. To address this challenge, we propose a\nmodel designed to predict a new POI outside the training data as long as its\ncontext is aligned with the user's interests. Unlike existing approaches that\ndirectly predict specific POIs, our model first forecasts the semantic context\nof potential future POIs, then combines this with a proximity-based prior\nprobability distribution to determine the exact POI. Experimental results on\nreal-world visit data demonstrate that our model outperforms baseline methods\nthat do not account for semantic contexts, achieving a 17% improvement in\naccuracy. Notably, as new POIs are introduced over time, our model remains\nrobust, exhibiting a lower decline rate in prediction accuracy compared to\nexisting methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2024 IEEE International Conference on Big Data workshop BSD 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.15285v1",
    "published_date": "2024-11-22 16:50:10 UTC",
    "updated_date": "2024-11-22 16:50:10 UTC"
  },
  {
    "arxiv_id": "2411.15061v1",
    "title": "Empowering Clients: Transformation of Design Processes Due to Generative AI",
    "authors": [
      "Johannes Schneider",
      "Kilic Sinem",
      "Daniel Stockhammer"
    ],
    "abstract": "The domain of computational design, driven by advancements in Generative AI,\nis transforming creative fields. We explore the transformative effects of\nGenerative AI on the architectural design process and discuss the role of the\narchitect. The case of architecture is interesting as designing houses is\ncomplex, involving extensive customer interaction. We employ a within-subject\nexperiment using a popular general-purpose text-to-image tool for generating\ndesigns and providing feedback on existing designs, followed by expert\ninterviews. The study reveals that AI can disrupt the ideation phase by\nenabling clients to engage in the design process through rapid visualization of\ntheir own ideas. In turn, the architect's role shifts more towards assessing\nthe feasibility of designs generated conjointly by clients and AI. Our study\nalso shows that while AI can provide valuable feedback on designs, it might\nfail to generate such designs, allowing for interesting connections to\nfoundations in computer science, i.e., NP-completeness. AI's feedback also\ntends to hamper creativity and innovation by suggesting altering novel,\ninnovative approaches toward more standardized designs. Our study also reveals\nthat there is uncertainty among architects about the interpretative sovereignty\nof architecture and loss of meaning and identity when AI increasingly takes\nover authorship in the design process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15061v1",
    "published_date": "2024-11-22 16:48:15 UTC",
    "updated_date": "2024-11-22 16:48:15 UTC"
  },
  {
    "arxiv_id": "2411.15056v1",
    "title": "Financial Risk Assessment via Long-term Payment Behavior Sequence Folding",
    "authors": [
      "Yiran Qiao",
      "Yateng Tang",
      "Xiang Ao",
      "Qi Yuan",
      "Ziming Liu",
      "Chen Shen",
      "Xuehao Zheng"
    ],
    "abstract": "Online inclusive financial services encounter significant financial risks due\nto their expansive user base and low default costs. By real-world practice, we\nreveal that utilizing longer-term user payment behaviors can enhance models'\nability to forecast financial risks. However, learning long behavior sequences\nis non-trivial for deep sequential models. Additionally, the diverse fields of\npayment behaviors carry rich information, requiring thorough exploitation.\nThese factors collectively complicate the task of long-term user behavior\nmodeling. To tackle these challenges, we propose a Long-term Payment Behavior\nSequence Folding method, referred to as LBSF. In LBSF, payment behavior\nsequences are folded based on merchants, using the merchant field as an\nintrinsic grouping criterion, which enables informative parallelism without\nreliance on external knowledge. Meanwhile, we maximize the utility of payment\ndetails through a multi-field behavior encoding mechanism. Subsequently,\nbehavior aggregation at the merchant level followed by relational learning\nacross merchants facilitates comprehensive user financial representation. We\nevaluate LBSF on the financial risk assessment task using a large-scale\nreal-world dataset. The results demonstrate that folding long behavior\nsequences based on internal behavioral cues effectively models long-term\npatterns and changes, thereby generating more accurate user financial profiles\nfor practical applications.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "ICDM2024 long paper",
    "pdf_url": "http://arxiv.org/pdf/2411.15056v1",
    "published_date": "2024-11-22 16:43:26 UTC",
    "updated_date": "2024-11-22 16:43:26 UTC"
  },
  {
    "arxiv_id": "2411.15042v2",
    "title": "Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications",
    "authors": [
      "Hong Ding",
      "Ziming Wang",
      "Yi Ding",
      "Hongjie Lin",
      "SuYang Xi",
      "Chia Chao Kang"
    ],
    "abstract": "Addressing the challenge of ensuring safety in ever-changing and\nunpredictable environments, particularly in the swiftly advancing realm of\nautonomous driving in today's 5G wireless communication world, we present\nNavigation Secure (NavSecure). This vision-based navigation framework merges\nthe strengths of world models with crucial safety-focused decision-making\ncapabilities, enabling autonomous vehicles to navigate real-world complexities\nsecurely. Our approach anticipates potential threats and formulates safer\nroutes by harnessing the predictive capabilities of world models, thus\nsignificantly reducing the need for extensive real-world trial-and-error\nlearning. Additionally, our method empowers vehicles to autonomously learn and\ndevelop through continuous practice, ensuring the system evolves and adapts to\nnew challenges. Incorporating radio frequency technology, NavSecure leverages\n5G networks to enhance real-time data exchange, improving communication and\nresponsiveness. Validated through rigorous experiments under simulation-to-real\ndriving conditions, NavSecure has shown exceptional performance in\nsafety-critical scenarios, such as sudden obstacle avoidance. Results indicate\nthat NavSecure excels in key safety metrics, including collision prevention and\nrisk reduction, surpassing other end-to-end methodologies. This framework not\nonly advances autonomous driving safety but also demonstrates how world models\ncan enhance decision-making in critical applications. NavSecure sets a new\nstandard for developing more robust and trustworthy autonomous driving systems,\ncapable of handling the inherent dynamics and uncertainties of real-world\nenvironments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15042v2",
    "published_date": "2024-11-22 16:16:07 UTC",
    "updated_date": "2024-11-25 15:37:55 UTC"
  },
  {
    "arxiv_id": "2411.15041v1",
    "title": "mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA",
    "authors": [
      "Tao Zhang",
      "Ziqi Zhang",
      "Zongyang Ma",
      "Yuxin Chen",
      "Zhongang Qi",
      "Chunfeng Yuan",
      "Bing Li",
      "Junfu Pu",
      "Yuxuan Zhao",
      "Zehua Xie",
      "Jin Ma",
      "Ying Shan",
      "Weiming Hu"
    ],
    "abstract": "Advanced Multimodal Large Language Models (MLLMs) struggle with recent\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\neffectively expanding the knowledge scope. However, current mRAG methods have\ninherent drawbacks, including: 1) Performing retrieval even when external\nknowledge is not needed. 2) Lacking of identification of evidence that supports\nthe query. 3) Increasing model complexity due to additional information\nfiltering modules or rules. To address these shortcomings, we propose a novel\ngeneralized framework called \\textbf{m}ultimodal\n\\textbf{R}etrieval-\\textbf{R}eflection-\\textbf{A}ugmented \\textbf{G}eneration\n(mR$^2$AG), which achieves adaptive retrieval and useful information\nlocalization to enable answers through two easy-to-implement reflection\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\nis designed to distinguish different user queries and avoids redundant\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\nlocating beneficial evidence of the retrieved content and generating answers\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\nVisual-dependent tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15041v1",
    "published_date": "2024-11-22 16:15:50 UTC",
    "updated_date": "2024-11-22 16:15:50 UTC"
  },
  {
    "arxiv_id": "2411.15281v1",
    "title": "ElastiFormer: Learned Redundancy Reduction in Transformer via Self-Distillation",
    "authors": [
      "Junzhang Liu",
      "Tingkai Liu",
      "Yueyuan Sui",
      "Stephen Xia"
    ],
    "abstract": "We introduce ElastiFormer, a post-training technique that adapts pretrained\nTransformer models into an elastic counterpart with variable inference time\ncompute. ElastiFormer introduces small routing modules (as low as .00006%\nadditional trainable parameters) to dynamically selects subsets of network\nparameters and input tokens to be processed by each layer of the pretrained\nnetwork in an inputdependent manner. The routing modules are trained using\nself-distillation losses to minimize the differences between the output of the\npretrained-model and their elastic counterparts. As ElastiFormer makes no\nassumption regarding the modality of the pretrained Transformer model, it can\nbe readily applied to all modalities covering causal language modeling, image\nmodeling as well as visual-language modeling tasks. We show that 20% to 50%\ncompute saving could be achieved for different components of the transformer\nlayer, which could be further reduced by adding very low rank LoRA weights\n(rank 1) trained via the same distillation objective. Finally, by comparing\nrouting trained on different subsets of ImageNet, we show that ElastiFormer is\nrobust against the training domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15281v1",
    "published_date": "2024-11-22 16:11:14 UTC",
    "updated_date": "2024-11-22 16:11:14 UTC"
  },
  {
    "arxiv_id": "2411.15033v1",
    "title": "One to rule them all: natural language to bind communication, perception and action",
    "authors": [
      "Simone Colombani",
      "Dimitri Ognibene",
      "Giuseppe Boccignone"
    ],
    "abstract": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15033v1",
    "published_date": "2024-11-22 16:05:54 UTC",
    "updated_date": "2024-11-22 16:05:54 UTC"
  },
  {
    "arxiv_id": "2411.15027v1",
    "title": "Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot",
    "authors": [
      "Simone Colombani",
      "Luca Brini",
      "Dimitri Ognibene",
      "Giuseppe Boccignone"
    ],
    "abstract": "Robots are increasingly being used in dynamic environments like workplaces,\nhospitals, and homes. As a result, interactions with robots must be simple and\nintuitive, with robots perception adapting efficiently to human-induced\nchanges. This paper presents a robot control architecture that addresses key\nchallenges in human-robot interaction, with a particular focus on the dynamic\ncreation and continuous update of the robot state representation. The\narchitecture uses Large Language Models to integrate diverse information\nsources, including natural language commands, robotic skills representation,\nreal-time dynamic semantic mapping of the perceived scene. This enables\nflexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions\nand settings, limiting their adaptability to dynamic environments and real-time\ncollaboration. In contrast, this architecture uses LLMs to interpret complex,\nhigh-level instructions and generate actionable plans that enhance human-robot\ncollaboration. At its core, the system Perception Module generates and\ncontinuously updates a semantic scene graph using RGB-D sensor data, providing\na detailed and structured representation of the environment. A particle filter\nis employed to ensure accurate object localization in dynamic, real-world\nsettings. The Planner Module leverages this up-to-date semantic map to break\ndown high-level tasks into sub-tasks and link them to robotic skills such as\nnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,\nGOTO). By combining real-time perception, state tracking, and LLM-driven\ncommunication and task planning, the architecture enhances adaptability, task\nefficiency, and human-robot collaboration in dynamic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15027v1",
    "published_date": "2024-11-22 15:58:26 UTC",
    "updated_date": "2024-11-22 15:58:26 UTC"
  },
  {
    "arxiv_id": "2411.15007v1",
    "title": "FTA generation using GenAI with an Autonomy sensor Usecase",
    "authors": [
      "Sneha Sudhir Shetiya",
      "Divya Garikapati",
      "Veeraja Sohoni"
    ],
    "abstract": "Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15007v1",
    "published_date": "2024-11-22 15:31:20 UTC",
    "updated_date": "2024-11-22 15:31:20 UTC"
  },
  {
    "arxiv_id": "2411.15004v2",
    "title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data",
    "authors": [
      "Junhong Shen",
      "Atishay Jain",
      "Zedian Xiao",
      "Ishan Amlekar",
      "Mouad Hadji",
      "Aaron Podolny",
      "Ameet Talwalkar"
    ],
    "abstract": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 7.3% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15004v2",
    "published_date": "2024-11-22 15:26:23 UTC",
    "updated_date": "2024-12-05 02:00:07 UTC"
  },
  {
    "arxiv_id": "2411.15276v1",
    "title": "Event USKT : U-State Space Model in Knowledge Transfer for Event Cameras",
    "authors": [
      "Yuhui Lin",
      "Jiahao Zhang",
      "Siyuan Li",
      "Jimin Xiao",
      "Ding Xu",
      "Wenjun Wu",
      "Jiaxuan Lu"
    ],
    "abstract": "Event cameras, as an emerging imaging technology, offer distinct advantages\nover traditional RGB cameras, including reduced energy consumption and higher\nframe rates. However, the limited quantity of available event data presents a\nsignificant challenge, hindering their broader development. To alleviate this\nissue, we introduce a tailored U-shaped State Space Model Knowledge Transfer\n(USKT) framework for Event-to-RGB knowledge transfer. This framework generates\ninputs compatible with RGB frames, enabling event data to effectively reuse\npre-trained RGB models and achieve competitive performance with minimal\nparameter tuning. Within the USKT architecture, we also propose a bidirectional\nreverse state space model. Unlike conventional bidirectional scanning\nmechanisms, the proposed Bidirectional Reverse State Space Model (BiR-SSM)\nleverages a shared weight strategy, which facilitates efficient modeling while\nconserving computational resources. In terms of effectiveness, integrating USKT\nwith ResNet50 as the backbone improves model performance by 0.95%, 3.57%, and\n2.9% on DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets, respectively,\nunderscoring USKT's adaptability and effectiveness. The code will be made\navailable upon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15276v1",
    "published_date": "2024-11-22 15:13:23 UTC",
    "updated_date": "2024-11-22 15:13:23 UTC"
  },
  {
    "arxiv_id": "2411.14995v2",
    "title": "Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution",
    "authors": [
      "Jonas Gösgens",
      "Niklas Jansen",
      "Hector Geffner"
    ],
    "abstract": "Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted at ICAPS 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.14995v2",
    "published_date": "2024-11-22 15:09:50 UTC",
    "updated_date": "2025-05-02 14:12:10 UTC"
  },
  {
    "arxiv_id": "2411.14991v1",
    "title": "Free Energy Projective Simulation (FEPS): Active inference with interpretability",
    "authors": [
      "Joséphine Pazem",
      "Marius Krumm",
      "Alexander Q. Vining",
      "Lukas J. Fiderer",
      "Hans J. Briegel"
    ],
    "abstract": "In the last decade, the free energy principle (FEP) and active inference\n(AIF) have achieved many successes connecting conceptual models of learning and\ncognition to mathematical models of perception and action. This effort is\ndriven by a multidisciplinary interest in understanding aspects of\nself-organizing complex adaptive systems, including elements of agency. Various\nreinforcement learning (RL) models performing active inference have been\nproposed and trained on standard RL tasks using deep neural networks. Recent\nwork has focused on improving such agents' performance in complex environments\nby incorporating the latest machine learning techniques. In this paper, we take\nan alternative approach. Within the constraints imposed by the FEP and AIF, we\nattempt to model agents in an interpretable way without deep neural networks by\nintroducing Free Energy Projective Simulation (FEPS). Using internal rewards\nonly, FEPS agents build a representation of their partially observable\nenvironments with which they interact. Following AIF, the policy to achieve a\ngiven task is derived from this world model by minimizing the expected free\nenergy. Leveraging the interpretability of the model, techniques are introduced\nto deal with long-term goals and reduce prediction errors caused by erroneous\nhidden state estimation. We test the FEPS model on two RL environments inspired\nfrom behavioral biology: a timed response task and a navigation task in a\npartially observable grid. Our results show that FEPS agents fully resolve the\nambiguity of both environments by appropriately contextualizing their\nobservations based on prediction accuracy only. In addition, they infer optimal\npolicies flexibly for any target observation in the environment.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.NC",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages (including 5 pages appendix), 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14991v1",
    "published_date": "2024-11-22 15:01:44 UTC",
    "updated_date": "2024-11-22 15:01:44 UTC"
  },
  {
    "arxiv_id": "2411.14975v1",
    "title": "Exploring Foundation Models Fine-Tuning for Cytology Classification",
    "authors": [
      "Manon Dausort",
      "Tiffanie Godelaine",
      "Maxime Zanella",
      "Karim El Khoury",
      "Isabelle Salmon",
      "Benoît Macq"
    ],
    "abstract": "Cytology slides are essential tools in diagnosing and staging cancer, but\ntheir analysis is time-consuming and costly. Foundation models have shown great\npotential to assist in these tasks. In this paper, we explore how existing\nfoundation models can be applied to cytological classification. More\nparticularly, we focus on low-rank adaptation, a parameter-efficient\nfine-tuning method suited to few-shot learning. We evaluated five foundation\nmodels across four cytological classification datasets. Our results demonstrate\nthat fine-tuning the pre-trained backbones with LoRA significantly improves\nmodel performance compared to fine-tuning only the classifier head, achieving\nstate-of-the-art results on both simple and complex classification tasks while\nrequiring fewer data samples.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.QM"
    ],
    "primary_category": "eess.IV",
    "comment": "5 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14975v1",
    "published_date": "2024-11-22 14:34:04 UTC",
    "updated_date": "2024-11-22 14:34:04 UTC"
  },
  {
    "arxiv_id": "2411.14972v1",
    "title": "Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models",
    "authors": [
      "Alec Wright",
      "Alistair Carson",
      "Lauri Juvela"
    ],
    "abstract": "This paper introduces Open-Amp, a synthetic data framework for generating\nlarge-scale and diverse audio effects data. Audio effects are relevant to many\nmusical audio processing and Music Information Retrieval (MIR) tasks, such as\nmodelling of analog audio effects, automatic mixing, tone matching and\ntranscription. Existing audio effects datasets are limited in scope, usually\nincluding relatively few audio effects processors and a limited amount of input\naudio signals. Our proposed framework overcomes these issues, by crowdsourcing\nneural network emulations of guitar amplifiers and effects, created by users of\nopen-source audio effects emulation software. This allows users of Open-Amp\ncomplete control over the input signals to be processed by the effects models,\nas well as providing high-quality emulations of hundreds of devices. Open-Amp\ncan render audio online during training, allowing great flexibility in data\naugmentation. Our experiments show that using Open-Amp to train a guitar\neffects encoder achieves new state-of-the-art results on multiple guitar\neffects classification tasks. Furthermore, we train a one-to-many guitar\neffects model using Open-Amp, and use it to emulate unseen analog effects via\nmanipulation of its learned latent space, indicating transferability to analog\nguitar effects data.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14972v1",
    "published_date": "2024-11-22 14:27:59 UTC",
    "updated_date": "2024-11-22 14:27:59 UTC"
  },
  {
    "arxiv_id": "2411.14967v1",
    "title": "SwissADT: An Audio Description Translation System for Swiss Languages",
    "authors": [
      "Lukas Fischer",
      "Yingqiang Gao",
      "Alexa Lintner",
      "Sarah Ebling"
    ],
    "abstract": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14967v1",
    "published_date": "2024-11-22 14:23:07 UTC",
    "updated_date": "2024-11-22 14:23:07 UTC"
  },
  {
    "arxiv_id": "2411.15274v1",
    "title": "Feature-interactive Siamese graph encoder-based image analysis to predict STAS from histopathology images in lung cancer",
    "authors": [
      "Liangrui Pan",
      "Qingchun Liang",
      "Wenwu Zeng",
      "Yijun Peng",
      "Zhenyu Zhao",
      "Yiyi Liang",
      "Jiadi Luo",
      "Xiang Wang",
      "Shaoliang Peng"
    ],
    "abstract": "Spread through air spaces (STAS) is a distinct invasion pattern in lung\ncancer, crucial for prognosis assessment and guiding surgical decisions.\nHistopathology is the gold standard for STAS detection, yet traditional methods\nare subjective, time-consuming, and prone to misdiagnosis, limiting large-scale\napplications. We present VERN, an image analysis model utilizing a\nfeature-interactive Siamese graph encoder to predict STAS from lung cancer\nhistopathological images. VERN captures spatial topological features with\nfeature sharing and skip connections to enhance model training. Using 1,546\nhistopathology slides, we built a large single-cohort STAS lung cancer dataset.\nVERN achieved an AUC of 0.9215 in internal validation and AUCs of 0.8275 and\n0.8829 in frozen and paraffin-embedded test sections, respectively,\ndemonstrating clinical-grade performance. Validated on a single-cohort and\nthree external datasets, VERN showed robust predictive performance and\ngeneralizability, providing an open platform (http://plr.20210706.xyz:5000/) to\nenhance STAS diagnosis efficiency and accuracy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "accept for publication in npj Precision Oncology",
    "pdf_url": "http://arxiv.org/pdf/2411.15274v1",
    "published_date": "2024-11-22 14:21:52 UTC",
    "updated_date": "2024-11-22 14:21:52 UTC"
  },
  {
    "arxiv_id": "2411.14962v2",
    "title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents",
    "authors": [
      "Hitesh Laxmichand Patel",
      "Amit Agarwal",
      "Bhargava Kumar",
      "Karan Gupta",
      "Priyaranjan Pattnayak"
    ],
    "abstract": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14962v2",
    "published_date": "2024-11-22 14:21:18 UTC",
    "updated_date": "2024-12-23 19:01:23 UTC"
  },
  {
    "arxiv_id": "2411.14959v1",
    "title": "Design-o-meter: Towards Evaluating and Refining Graphic Designs",
    "authors": [
      "Sahil Goyal",
      "Abhinav Mahajan",
      "Swasti Mishra",
      "Prateksha Udhayanan",
      "Tripti Shukla",
      "K J Joseph",
      "Balaji Vasan Srinivasan"
    ],
    "abstract": "Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to WACV 2025. Project page:\n  https://sahilg06.github.io/Design-o-meter/",
    "pdf_url": "http://arxiv.org/pdf/2411.14959v1",
    "published_date": "2024-11-22 14:17:46 UTC",
    "updated_date": "2024-11-22 14:17:46 UTC"
  },
  {
    "arxiv_id": "2411.14953v1",
    "title": "Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing",
    "authors": [
      "Miriam Alber",
      "Christoph Hönes",
      "Patrick Baier"
    ],
    "abstract": "One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2.m"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14953v1",
    "published_date": "2024-11-22 14:12:35 UTC",
    "updated_date": "2024-11-22 14:12:35 UTC"
  },
  {
    "arxiv_id": "2411.14946v1",
    "title": "Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based Approach",
    "authors": [
      "Lars Nieradzik",
      "Henrike Stephani",
      "Janis Keuper"
    ],
    "abstract": "In this paper, we present an approach for evaluating attribution maps, which\nplay a central role in interpreting the predictions of convolutional neural\nnetworks (CNNs). We show that the widely used insertion/deletion metrics are\nsusceptible to distribution shifts that affect the reliability of the ranking.\nOur method proposes to replace pixel modifications with adversarial\nperturbations, which provides a more robust evaluation framework. By using\nsmoothness and monotonicity measures, we illustrate the effectiveness of our\napproach in correcting distribution shifts. In addition, we conduct the most\ncomprehensive quantitative and qualitative assessment of attribution maps to\ndate. Introducing baseline attribution maps as sanity checks, we find that our\nmetric is the only contender to pass all checks. Using Kendall's $\\tau$ rank\ncorrelation coefficient, we show the increased consistency of our metric across\n15 dataset-architecture combinations. Of the 16 attribution maps tested, our\nresults clearly show SmoothGrad to be the best map currently available. This\nresearch makes an important contribution to the development of attribution maps\nby providing a reliable and consistent evaluation framework. To ensure\nreproducibility, we will provide the code along with our results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14946v1",
    "published_date": "2024-11-22 13:57:56 UTC",
    "updated_date": "2024-11-22 13:57:56 UTC"
  },
  {
    "arxiv_id": "2411.14942v1",
    "title": "Comparative Study of Neural Network Methods for Solving Topological Solitons",
    "authors": [
      "Koji Hashimoto",
      "Koshiro Matsuo",
      "Masaki Murata",
      "Gakuto Ogiwara"
    ],
    "abstract": "Topological solitons, which are stable, localized solutions of nonlinear\ndifferential equations, are crucial in various fields of physics and\nmathematics, including particle physics and cosmology. However, solving these\nsolitons presents significant challenges due to the complexity of the\nunderlying equations and the computational resources required for accurate\nsolutions. To address this, we have developed a novel method using neural\nnetwork (NN) to efficiently solve solitons. A similar NN approach is\nPhysics-Informed Neural Networks (PINN). In a comparative analysis between our\nmethod and PINN, we find that our method achieves shorter computation times\nwhile maintaining the same level of accuracy. This advancement in computational\nefficiency not only overcomes current limitations but also opens new avenues\nfor studying topological solitons and their dynamical behavior.",
    "categories": [
      "hep-th",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "hep-th",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14942v1",
    "published_date": "2024-11-22 13:54:52 UTC",
    "updated_date": "2024-11-22 13:54:52 UTC"
  },
  {
    "arxiv_id": "2411.14937v1",
    "title": "Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning",
    "authors": [
      "Junjie Shan",
      "Ziqi Zhao",
      "Jialin Lu",
      "Rui Zhang",
      "Siu Ming Yiu",
      "Ka-Ho Chow"
    ],
    "abstract": "Foundation models that bridge vision and language have made significant\nprogress, inspiring numerous life-enriching applications. However, their\npotential for misuse to introduce new threats remains largely unexplored. This\npaper reveals that vision-language models (VLMs) can be exploited to overcome\nlongstanding limitations in gradient inversion attacks (GIAs) within federated\nlearning (FL), where an FL server reconstructs private data samples from\ngradients shared by victim clients. Current GIAs face challenges in\nreconstructing high-resolution images, especially when the victim has a large\nlocal data batch. While focusing reconstruction on valuable samples rather than\nthe entire batch is promising, existing methods lack the flexibility to allow\nattackers to specify their target data. In this paper, we introduce Geminio,\nthe first approach to transform GIAs into semantically meaningful, targeted\nattacks. Geminio enables a brand new privacy attack experience: attackers can\ndescribe, in natural language, the types of data they consider valuable, and\nGeminio will prioritize reconstruction to focus on those high-value samples.\nThis is achieved by leveraging a pretrained VLM to guide the optimization of a\nmalicious global model that, when shared with and optimized by a victim,\nretains only gradients of samples that match the attacker-specified query.\nExtensive experiments demonstrate Geminio's effectiveness in pinpointing and\nreconstructing targeted samples, with high success rates across complex\ndatasets under FL and large batch sizes and showing resilience against existing\ndefenses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14937v1",
    "published_date": "2024-11-22 13:49:56 UTC",
    "updated_date": "2024-11-22 13:49:56 UTC"
  },
  {
    "arxiv_id": "2411.15272v1",
    "title": "Curriculum-enhanced GroupDRO: Challenging the Norm of Avoiding Curriculum Learning in Subpopulation Shift Setups",
    "authors": [
      "Antonio Barbalau"
    ],
    "abstract": "In subpopulation shift scenarios, a Curriculum Learning (CL) approach would\nonly serve to imprint the model weights, early on, with the easily learnable\nspurious correlations featured. To the best of our knowledge, none of the\ncurrent state-of-the-art subpopulation shift approaches employ any kind of\ncurriculum. To overcome this, we design a CL approach aimed at initializing the\nmodel weights in an unbiased vantage point in the hypothesis space which\nsabotages easy convergence towards biased hypotheses during the final\noptimization based on the entirety of the available data. We hereby propose a\nCurriculum-enhanced Group Distributionally Robust Optimization (CeGDRO)\napproach, which prioritizes the hardest bias-confirming samples and the easiest\nbias-conflicting samples, leveraging GroupDRO to balance the initial\ndiscrepancy in terms of difficulty. We benchmark our proposed method against\nthe most popular subpopulation shift datasets, showing an increase over the\nstate-of-the-art results across all scenarios, up to 6.2% on Waterbirds.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15272v1",
    "published_date": "2024-11-22 13:38:56 UTC",
    "updated_date": "2024-11-22 13:38:56 UTC"
  },
  {
    "arxiv_id": "2411.14927v2",
    "title": "LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure Cooperation",
    "authors": [
      "Zhenwei Yang",
      "Jilei Mao",
      "Wenxian Yang",
      "Yibo Ai",
      "Yu Kong",
      "Haibao Yu",
      "Weidong Zhang"
    ],
    "abstract": "Temporal perception, defined as the capability to detect and track objects\nacross temporal sequences, serves as a fundamental component in autonomous\ndriving systems. While single-vehicle perception systems encounter limitations,\nstemming from incomplete perception due to object occlusion and inherent blind\nspots, cooperative perception systems present their own challenges in terms of\nsensor calibration precision and positioning accuracy. To address these issues,\nwe introduce LET-VIC, a LiDAR-based End-to-End Tracking framework for\nVehicle-Infrastructure Cooperation (VIC). First, we employ Temporal\nSelf-Attention and VIC Cross-Attention modules to effectively integrate\ntemporal and spatial information from both vehicle and infrastructure\nperspectives. Then, we develop a novel Calibration Error Compensation (CEC)\nmodule to mitigate sensor misalignment issues and facilitate accurate feature\nalignment. Experiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC\nsignificantly outperforms baseline models. Compared to LET-V, LET-VIC achieves\n+15.0% improvement in mAP and a +17.3% improvement in AMOTA. Furthermore,\nLET-VIC surpasses representative Tracking by Detection models, including\nV2VNet, FFNet, and PointPillars, with at least a +13.7% improvement in mAP and\na +13.1% improvement in AMOTA without considering communication delays,\nshowcasing its robust detection and tracking performance. The experiments\ndemonstrate that the integration of multi-view perspectives, temporal\nsequences, or CEC in end-to-end training significantly improves both detection\nand tracking performance. All code will be open-sourced.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14927v2",
    "published_date": "2024-11-22 13:34:29 UTC",
    "updated_date": "2025-04-05 07:03:43 UTC"
  },
  {
    "arxiv_id": "2411.14925v1",
    "title": "Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot",
    "authors": [
      "Linqi Lu",
      "Yifan Deng",
      "Chuan Tian",
      "Sijia Yang",
      "Dhavan Shah"
    ],
    "abstract": "This study introduces Purrfessor, an innovative AI chatbot designed to\nprovide personalized dietary guidance through interactive, multimodal\nengagement. Leveraging the Large Language-and-Vision Assistant (LLaVA) model\nfine-tuned with food and nutrition data and a human-in-the-loop approach,\nPurrfessor integrates visual meal analysis with contextual advice to enhance\nuser experience and engagement. We conducted two studies to evaluate the\nchatbot's performance and user experience: (a) simulation assessments and human\nvalidation were conducted to examine the performance of the fine-tuned model;\n(b) a 2 (Profile: Bot vs. Pet) by 3 (Model: GPT-4 vs. LLaVA vs. Fine-tuned\nLLaVA) experiment revealed that Purrfessor significantly enhanced users'\nperceptions of care ($\\beta = 1.59$, $p = 0.04$) and interest ($\\beta = 2.26$,\n$p = 0.01$) compared to the GPT-4 bot. Additionally, user interviews\nhighlighted the importance of interaction design details, emphasizing the need\nfor responsiveness, personalization, and guidance to improve user engagement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14925v1",
    "published_date": "2024-11-22 13:28:28 UTC",
    "updated_date": "2024-11-22 13:28:28 UTC"
  },
  {
    "arxiv_id": "2411.16709v1",
    "title": "A Brief Summary of Explanatory Virtues",
    "authors": [
      "Ingrid Zukerman"
    ],
    "abstract": "In this report, I provide a brief summary of the literature in philosophy,\npsychology and cognitive science about Explanatory Virtues, and link these\nconcepts to eXplainable AI.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.16709v1",
    "published_date": "2024-11-22 13:27:56 UTC",
    "updated_date": "2024-11-22 13:27:56 UTC"
  },
  {
    "arxiv_id": "2411.14922v2",
    "title": "GOT4Rec: Graph of Thoughts for Sequential Recommendation",
    "authors": [
      "Zewen Long",
      "Liang Wang",
      "Shu Wu",
      "Qiang Liu",
      "Liang Wang"
    ],
    "abstract": "With their vast open-world knowledge and reasoning abilities, large language\nmodels (LLMs) have become a promising tool for sequential recommendation.\nResearchers have explored various methods to harness these capabilities, but\nmost existing approaches rely on simple input-output prompting, failing to\neffectively bridge the gap between LLMs' general knowledge and the specific\nneeds of recommendation tasks. While reasoning strategies like chain-of-thought\n(CoT) have been introduced to enhance performance, they often produce\ninaccurate recommendations due to underutilized user preference information and\ninsufficient reasoning depth. To address these challenges, we propose GOT4Rec,\na novel sequential recommendation method leveraging the graph of thoughts (GoT)\nreasoning strategy. Our method focuses on three key types of information in\nuser histories: short-term interests, long-term interests and collaborative\ninformation from other users. It enables LLMs to reason independently and\ngenerate recommendations, subsequently aggregating results to derive final\nitems. This method allows LLMs, with enhanced reasoning capabilities, to better\nutilize the user sequence information, producing more accurate recommendations\nand comprehensive explanations. Extensive experiments on real-world datasets\ndemonstrate the effectiveness of GOT4Rec, outperforming existing\nstate-of-the-art baselines with an average improvement of 37.11%. Our code is\navailable at https://anonymous.4open.science/r/GOT4Rec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14922v2",
    "published_date": "2024-11-22 13:24:01 UTC",
    "updated_date": "2025-04-23 01:38:48 UTC"
  },
  {
    "arxiv_id": "2411.14907v1",
    "title": "DAIRHuM: A Platform for Directly Aligning AI Representations with Human Musical Judgments applied to Carnatic Music",
    "authors": [
      "Prashanth Thattai Ravikumar"
    ],
    "abstract": "Quantifying and aligning music AI model representations with human behavior\nis an important challenge in the field of MIR. This paper presents a platform\nfor exploring the Direct alignment between AI music model Representations and\nHuman Musical judgments (DAIRHuM). It is designed to enable musicians and\nexperimentalists to label similarities in a dataset of music recordings, and\nexamine a pre-trained model's alignment with their labels using quantitative\nscores and visual plots. DAIRHuM is applied to analyze alignment between NSynth\nrepresentations, and a rhythmic duet between two percussionists in a Carnatic\nquartet ensemble, an example of a genre where annotated data is scarce and\nassessing alignment is non-trivial. The results demonstrate significant\nfindings on model alignment with human judgments of rhythmic harmony, while\nhighlighting key differences in rhythm perception and music similarity\njudgments specific to Carnatic music. This work is among the first efforts to\nenable users to explore human-AI model alignment in Carnatic music and advance\nMIR research in Indian music while dealing with data scarcity and cultural\nspecificity. The development of this platform provides greater accessibility to\nmusic AI tools for under-represented genres.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "4 Pages, ICASSP workshop submission",
    "pdf_url": "http://arxiv.org/pdf/2411.14907v1",
    "published_date": "2024-11-22 13:04:51 UTC",
    "updated_date": "2024-11-22 13:04:51 UTC"
  },
  {
    "arxiv_id": "2411.15271v1",
    "title": "EADReg: Probabilistic Correspondence Generation with Efficient Autoregressive Diffusion Model for Outdoor Point Cloud Registration",
    "authors": [
      "Linrui Gong",
      "Jiuming Liu",
      "Junyi Ma",
      "Lihao Liu",
      "Yaonan Wang",
      "Hesheng Wang"
    ],
    "abstract": "Diffusion models have shown the great potential in the point cloud\nregistration (PCR) task, especially for enhancing the robustness to challenging\ncases. However, existing diffusion-based PCR methods primarily focus on\ninstance-level scenarios and struggle with outdoor LiDAR points, where the\nsparsity, irregularity, and huge point scale inherent in LiDAR points pose\nchallenges to establishing dense global point-to-point correspondences. To\naddress this issue, we propose a novel framework named EADReg for efficient and\nrobust registration of LiDAR point clouds based on autoregressive diffusion\nmodels. EADReg follows a coarse-to-fine registration paradigm. In the coarse\nstage, we employ a Bi-directional Gaussian Mixture Model (BGMM) to reject\noutlier points and obtain purified point cloud pairs. BGMM establishes\ncorrespondences between the Gaussian Mixture Models (GMMs) from the source and\ntarget frames, enabling reliable coarse registration based on filtered features\nand geometric information. In the fine stage, we treat diffusion-based PCR as\nan autoregressive process to generate robust point correspondences, which are\nthen iteratively refined on upper layers. Despite common criticisms of\ndiffusion-based methods regarding inference speed, EADReg achieves runtime\ncomparable to convolutional-based methods. Extensive experiments on the KITTI\nand NuScenes benchmark datasets highlight the state-of-the-art performance of\nour proposed method. Codes will be released upon publication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15271v1",
    "published_date": "2024-11-22 13:03:54 UTC",
    "updated_date": "2024-11-22 13:03:54 UTC"
  },
  {
    "arxiv_id": "2412.06795v1",
    "title": "SpikeFI: A Fault Injection Framework for Spiking Neural Networks",
    "authors": [
      "Theofilos Spyrou",
      "Said Hamdioui",
      "Haralampos-G. Stratigopoulos"
    ],
    "abstract": "Neuromorphic computing and spiking neural networks (SNNs) are gaining\ntraction across various artificial intelligence (AI) tasks thanks to their\npotential for efficient energy usage and faster computation speed. This\ncomparative advantage comes from mimicking the structure, function, and\nefficiency of the biological brain, which arguably is the most brilliant and\ngreen computing machine. As SNNs are eventually deployed on a hardware\nprocessor, the reliability of the application in light of hardware-level faults\nbecomes a concern, especially for safety- and mission-critical applications. In\nthis work, we propose SpikeFI, a fault injection framework for SNNs that can be\nused for automating the reliability analysis and test generation. SpikeFI is\nbuilt upon the SLAYER PyTorch framework with fault injection experiments\naccelerated on a single or multiple GPUs. It has a comprehensive integrated\nneuron and synapse fault model library, in accordance to the literature in the\ndomain, which is extendable by the user if needed. It supports: single and\nmultiple faults; permanent and transient faults; specified, random layer-wise,\nand random network-wise fault locations; and pre-, during, and post-training\nfault injection. It also offers several optimization speedups and built-in\nfunctions for results visualization. SpikeFI is open-source and available for\ndownload via GitHub at https://github.com/SpikeFI.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.06795v1",
    "published_date": "2024-11-22 12:08:06 UTC",
    "updated_date": "2024-11-22 12:08:06 UTC"
  },
  {
    "arxiv_id": "2411.14883v1",
    "title": "Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation",
    "authors": [
      "Yuheng Xu",
      "Taiping Zhang"
    ],
    "abstract": "Domain-invariant representation learning is a powerful method for domain\ngeneralization. Previous approaches face challenges such as high computational\ndemands, training instability, and limited effectiveness with high-dimensional\ndata, potentially leading to the loss of valuable features. To address these\nissues, we hypothesize that an ideal generalized representation should exhibit\nsimilar pattern responses within the same channel across cross-domain images.\nBased on this hypothesis, we use deep features from the source domain as\nqueries, and deep features from the generated domain as keys and values.\nThrough a cross-channel attention mechanism, the original deep features are\nreconstructed into robust regularization representations, forming an explicit\nconstraint that guides the model to learn domain-invariant representations.\nAdditionally, style augmentation is another common method. However, existing\nmethods typically generate new styles through convex combinations of source\ndomains, which limits the diversity of training samples by confining the\ngenerated styles to the original distribution. To overcome this limitation, we\npropose an Adaptive Feature Blending (AFB) method that generates\nout-of-distribution samples while exploring the in-distribution space,\nsignificantly expanding the domain range. Extensive experimental results\ndemonstrate that our proposed methods achieve superior performance on two\nstandard domain generalization benchmarks for medical image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.4"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14883v1",
    "published_date": "2024-11-22 12:06:24 UTC",
    "updated_date": "2024-11-22 12:06:24 UTC"
  },
  {
    "arxiv_id": "2411.14871v3",
    "title": "Preference Alignment for Diffusion Model via Explicit Denoised Distribution Estimation",
    "authors": [
      "Dingyuan Shi",
      "Yong Wang",
      "Hangyu Li",
      "Xiangxiang Chu"
    ],
    "abstract": "Diffusion models have shown remarkable success in text-to-image generation,\nmaking preference alignment for these models increasingly important. The\npreference labels are typically available only at the terminal of denoising\ntrajectories, which poses challenges in optimizing the intermediate denoising\nsteps. In this paper, we propose to conduct Denoised Distribution Estimation\n(DDE) that explicitly connects intermediate steps to the terminal denoised\ndistribution. Therefore, preference labels can be used for the entire\ntrajectory optimization. To this end, we design two estimation strategies for\nour DDE. The first is stepwise estimation, which utilizes the conditional\ndenoised distribution to estimate the model denoised distribution. The second\nis single-shot estimation, which converts the model output into the terminal\ndenoised distribution via DDIM modeling. Analytically and empirically, we\nreveal that DDE equipped with two estimation strategies naturally derives a\nnovel credit assignment scheme that prioritizes optimizing the middle part of\nthe denoising trajectory. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14871v3",
    "published_date": "2024-11-22 11:45:33 UTC",
    "updated_date": "2025-03-13 02:36:28 UTC"
  },
  {
    "arxiv_id": "2411.14870v1",
    "title": "Application of AI to formal methods -- an analysis of current trends",
    "authors": [
      "Sebastian Stock",
      "Jannik Dunkelau",
      "Atif Mashkoor"
    ],
    "abstract": "With artificial intelligence (AI) being well established within the daily\nlives of research communities, we turn our gaze toward an application area that\nappears intuitively unsuited for probabilistic decision-making: the area of\nformal methods (FM). FM aim to provide sound and understandable reasoning about\nproblems in computer science, which seemingly collides with the black-box\nnature that inhibits many AI approaches. However, many researchers have crossed\nthis gap and applied AI techniques to enhance FM approaches. As this dichotomy\nof FM and AI sparked our interest, we conducted a systematic mapping study to\nmap the current landscape of research publications. In this study, we\ninvestigate the previous five years of applied AI to FM (2019-2023), as these\ncorrespond to periods of high activity. This investigation results in 189\nentries, which we explore in more detail to find current trends, highlight\nresearch gaps, and give suggestions for future research.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14870v1",
    "published_date": "2024-11-22 11:43:39 UTC",
    "updated_date": "2024-11-22 11:43:39 UTC"
  },
  {
    "arxiv_id": "2411.14869v2",
    "title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence",
    "authors": [
      "Xuewu Lin",
      "Tianwei Lin",
      "Lichao Huang",
      "Hongyu Xie",
      "Zhizhong Su"
    ],
    "abstract": "In embodied intelligence systems, a key component is 3D perception algorithm,\nwhich enables agents to understand their surrounding environments. Previous\nalgorithms primarily rely on point cloud, which, despite offering precise\ngeometric information, still constrain perception performance due to inherent\nsparsity, noise, and data scarcity. In this work, we introduce a novel\nimage-centric 3D perception model, BIP3D, which leverages expressive image\nfeatures with explicit 3D position encoding to overcome the limitations of\npoint-centric methods. Specifically, we leverage pre-trained 2D vision\nfoundation models to enhance semantic understanding, and introduce a spatial\nenhancer module to improve spatial understanding. Together, these modules\nenable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end\n3D perception. In our experiments, BIP3D outperforms current state-of-the-art\nresults on the EmbodiedScan benchmark, achieving improvements of 5.69% in the\n3D detection task and 15.25% in the 3D visual grounding task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14869v2",
    "published_date": "2024-11-22 11:35:42 UTC",
    "updated_date": "2024-11-27 11:31:05 UTC"
  },
  {
    "arxiv_id": "2411.14863v1",
    "title": "Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired Image-to-Image Translation",
    "authors": [
      "Jeongsol Kim",
      "Beomsu Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Diffusion models (DMs), which enable both image generation from noise and\ninversion from data, have inspired powerful unpaired image-to-image (I2I)\ntranslation algorithms. However, they often require a larger number of neural\nfunction evaluations (NFEs), limiting their practical applicability. In this\npaper, we tackle this problem with Schrodinger Bridges (SBs), which are\nstochastic differential equations (SDEs) between distributions with minimal\ntransport cost. We analyze the probability flow ordinary differential equation\n(ODE) formulation of SBs, and observe that we can decompose its vector field\ninto a linear combination of source predictor, target predictor, and noise\npredictor. Inspired by this observation, we propose Latent Schrodinger Bridges\n(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and\ndevelop appropriate prompt optimization and change of variables formula to\nmatch the training and inference between distributions. We demonstrate that our\nalgorithm successfully conduct competitive I2I translation in unsupervised\nsetting with only a fraction of computation cost required by previous DM-based\nI2I methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14863v1",
    "published_date": "2024-11-22 11:24:14 UTC",
    "updated_date": "2024-11-22 11:24:14 UTC"
  },
  {
    "arxiv_id": "2411.14858v1",
    "title": "Domain and Range Aware Synthetic Negatives Generation for Knowledge Graph Embedding Models",
    "authors": [
      "Alberto Bernardi",
      "Luca Costabello"
    ],
    "abstract": "Knowledge Graph Embedding models, representing entities and edges in a\nlow-dimensional space, have been extremely successful at solving tasks related\nto completing and exploring Knowledge Graphs (KGs). One of the key aspects of\ntraining most of these models is teaching to discriminate between true\nstatements positives and false ones (negatives). However, the way in which\nnegatives can be defined is not trivial, as facts missing from the KG are not\nnecessarily false and a set of ground truth negatives is hardly ever given.\nThis makes synthetic negative generation a necessity. Different generation\nstrategies can heavily affect the quality of the embeddings, making it a\nprimary aspect to consider. We revamp a strategy that generates corruptions\nduring training respecting the domain and range of relations, we extend its\ncapabilities and we show our methods bring substantial improvement (+10% MRR)\nfor standard benchmark datasets and over +150% MRR for a larger ontology-backed\ndataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the Third Learning on Graphs Conference (LoG 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.14858v1",
    "published_date": "2024-11-22 11:17:32 UTC",
    "updated_date": "2024-11-22 11:17:32 UTC"
  },
  {
    "arxiv_id": "2411.14847v2",
    "title": "Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction",
    "authors": [
      "Zhening Liu",
      "Yingdong Hu",
      "Xinjie Zhang",
      "Rui Song",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "The recent development of 3D Gaussian Splatting (3DGS) has led to great\ninterest in 4D dynamic spatial reconstruction. Existing approaches mainly rely\non full-length multi-view videos, while there has been limited exploration of\nonline reconstruction methods that enable on-the-fly training and per-timestep\nstreaming. Current 3DGS-based streaming methods treat the Gaussian primitives\nuniformly and constantly renew the densified Gaussians, thereby overlooking the\ndifference between dynamic and static features as well as neglecting the\ntemporal continuity in the scene. To address these limitations, we propose a\nnovel three-stage pipeline for iterative streamable 4D dynamic spatial\nreconstruction. Our pipeline comprises a selective inheritance stage to\npreserve temporal continuity, a dynamics-aware shift stage to distinguish\ndynamic and static primitives and optimize their movements, and an error-guided\ndensification stage to accommodate emerging objects. Our method achieves\nstate-of-the-art performance in online 4D reconstruction, demonstrating the\nfastest on-the-fly training, superior representation quality, and real-time\nrendering capability. Project page: https://www.liuzhening.top/DASS",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://www.liuzhening.top/DASS",
    "pdf_url": "http://arxiv.org/pdf/2411.14847v2",
    "published_date": "2024-11-22 10:47:47 UTC",
    "updated_date": "2025-03-27 13:49:30 UTC"
  },
  {
    "arxiv_id": "2411.15263v1",
    "title": "AI-Driven Real-Time Monitoring of Ground-Nesting Birds: A Case Study on Curlew Detection Using YOLOv10",
    "authors": [
      "Carl Chalmers",
      "Paul Fergus",
      "Serge Wich",
      "Steven N Longmore",
      "Naomi Davies Walsh",
      "Lee Oliver",
      "James Warrington",
      "Julieanne Quinlan",
      "Katie Appleby"
    ],
    "abstract": "Effective monitoring of wildlife is critical for assessing biodiversity and\necosystem health, as declines in key species often signal significant\nenvironmental changes. Birds, particularly ground-nesting species, serve as\nimportant ecological indicators due to their sensitivity to environmental\npressures. Camera traps have become indispensable tools for monitoring nesting\nbird populations, enabling data collection across diverse habitats. However,\nthe manual processing and analysis of such data are resource-intensive, often\ndelaying the delivery of actionable conservation insights. This study presents\nan AI-driven approach for real-time species detection, focusing on the curlew\n(Numenius arquata), a ground-nesting bird experiencing significant population\ndeclines. A custom-trained YOLOv10 model was developed to detect and classify\ncurlews and their chicks using 3/4G-enabled cameras linked to the Conservation\nAI platform. The system processes camera trap data in real-time, significantly\nenhancing monitoring efficiency. Across 11 nesting sites in Wales, the model\nachieved high performance, with a sensitivity of 90.56%, specificity of 100%,\nand F1-score of 95.05% for curlew detections, and a sensitivity of 92.35%,\nspecificity of 100%, and F1-score of 96.03% for curlew chick detections. These\nresults demonstrate the capability of AI-driven monitoring systems to deliver\naccurate, timely data for biodiversity assessments, facilitating early\nconservation interventions and advancing the use of technology in ecological\nresearch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15263v1",
    "published_date": "2024-11-22 10:36:29 UTC",
    "updated_date": "2024-11-22 10:36:29 UTC"
  },
  {
    "arxiv_id": "2411.14842v1",
    "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models",
    "authors": [
      "Wanqi Yang",
      "Yanda Li",
      "Meng Fang",
      "Yunchao Wei",
      "Tianyi Zhou",
      "Ling Chen"
    ],
    "abstract": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14842v1",
    "published_date": "2024-11-22 10:30:48 UTC",
    "updated_date": "2024-11-22 10:30:48 UTC"
  },
  {
    "arxiv_id": "2411.14832v1",
    "title": "VisGraphVar: A Benchmark Generator for Assessing Variability in Graph Analysis Using Large Vision-Language Models",
    "authors": [
      "Camilo Chacón Sartori",
      "Christian Blum",
      "Filippo Bistaffa"
    ],
    "abstract": "The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "68T50"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14832v1",
    "published_date": "2024-11-22 10:10:53 UTC",
    "updated_date": "2024-11-22 10:10:53 UTC"
  },
  {
    "arxiv_id": "2411.14827v1",
    "title": "Physically Interpretable Probabilistic Domain Characterization",
    "authors": [
      "Anaïs Halin",
      "Sébastien Piérard",
      "Renaud Vandeghen",
      "Benoît Gérin",
      "Maxime Zanella",
      "Martin Colot",
      "Jan Held",
      "Anthony Cioppa",
      "Emmanuel Jean",
      "Gianluca Bontempi",
      "Saïd Mahmoudi",
      "Benoît Macq",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "Characterizing domains is essential for models analyzing dynamic\nenvironments, as it allows them to adapt to evolving conditions or to hand the\ntask over to backup systems when facing conditions outside their operational\ndomain. Existing solutions typically characterize a domain by solving a\nregression or classification problem, which limits their applicability as they\nonly provide a limited summarized description of the domain. In this paper, we\npresent a novel approach to domain characterization by characterizing domains\nas probability distributions. Particularly, we develop a method to predict the\nlikelihood of different weather conditions from images captured by\nvehicle-mounted cameras by estimating distributions of physical parameters\nusing normalizing flows. To validate our proposed approach, we conduct\nexperiments within the context of autonomous vehicles, focusing on predicting\nthe distribution of weather parameters to characterize the operational domain.\nThis domain is characterized by physical parameters (absolute characterization)\nand arbitrarily predefined domains (relative characterization). Finally, we\nevaluate whether a system can safely operate in a target domain by comparing it\nto multiple source domains where safety has already been established. This\napproach holds significant potential, as accurate weather prediction and\neffective domain adaptation are crucial for autonomous systems to adjust to\ndynamic environmental conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14827v1",
    "published_date": "2024-11-22 10:07:02 UTC",
    "updated_date": "2024-11-22 10:07:02 UTC"
  },
  {
    "arxiv_id": "2411.15260v1",
    "title": "VIVID-10M: A Dataset and Baseline for Versatile and Interactive Video Local Editing",
    "authors": [
      "Jiahao Hu",
      "Tianxiong Zhong",
      "Xuebo Wang",
      "Boyuan Jiang",
      "Xingye Tian",
      "Fei Yang",
      "Pengfei Wan",
      "Di Zhang"
    ],
    "abstract": "Diffusion-based image editing models have made remarkable progress in recent\nyears. However, achieving high-quality video editing remains a significant\nchallenge. One major hurdle is the absence of open-source, large-scale video\nediting datasets based on real-world data, as constructing such datasets is\nboth time-consuming and costly. Moreover, video data requires a significantly\nlarger number of tokens for representation, which substantially increases the\ntraining costs for video editing models. Lastly, current video editing models\noffer limited interactivity, often making it difficult for users to express\ntheir editing requirements effectively in a single attempt. To address these\nchallenges, this paper introduces a dataset VIVID-10M and a baseline model\nVIVID. VIVID-10M is the first large-scale hybrid image-video local editing\ndataset aimed at reducing data construction and model training costs, which\ncomprises 9.7M samples that encompass a wide range of video editing tasks.\nVIVID is a Versatile and Interactive VIdeo local eDiting model trained on\nVIVID-10M, which supports entity addition, modification, and deletion. At its\ncore, a keyframe-guided interactive video editing mechanism is proposed,\nenabling users to iteratively edit keyframes and propagate it to other frames,\nthereby reducing latency in achieving desired outcomes. Extensive experimental\nevaluations show that our approach achieves state-of-the-art performance in\nvideo local editing, surpassing baseline methods in both automated metrics and\nuser studies. The VIVID-10M dataset and the VIVID editing model will be\navailable at \\url{https://inkosizhong.github.io/VIVID/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15260v1",
    "published_date": "2024-11-22 10:04:05 UTC",
    "updated_date": "2024-11-22 10:04:05 UTC"
  },
  {
    "arxiv_id": "2411.17731v1",
    "title": "Soil Characterization of Watermelon Field through Internet of Things: A New Approach to Soil Salinity Measurement",
    "authors": [
      "Md. Naimur Rahman",
      "Shafak Shahriar Sozol",
      "Md. Samsuzzaman",
      "Md. Shahin Hossin",
      "Mohammad Tariqul Islam",
      "S. M. Taohidul Islam",
      "Md. Maniruzzaman"
    ],
    "abstract": "In the modern agricultural industry, technology plays a crucial role in the\nadvancement of cultivation. To increase crop productivity, soil require some\nspecific characteristics. For watermelon cultivation, soil needs to be sandy\nand of high temperature with proper irrigation. This research aims to design\nand implement an intelligent IoT-based soil characterization system for the\nwatermelon field to measure the soil characteristics. IoT based developed\nsystem measures moisture, temperature, and pH of soil using different sensors,\nand the sensor data is uploaded to the cloud via Arduino and Raspberry Pi, from\nwhere users can obtain the data using mobile application and webpage developed\nfor this system. To ensure the precision of the framework, this study includes\nthe comparison between the readings of the soil parameters by the existing\nfield soil meters, the values obtained from the sensors integrated IoT system,\nand data obtained from soil science laboratory. Excessive salinity in soil\naffects the watermelon yield. This paper proposes a model for the measurement\nof soil salinity based on soil resistivity. It establishes a relationship\nbetween soil salinity and soil resistivity from the data obtained in the\nlaboratory using artificial neural network (ANN).",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17731v1",
    "published_date": "2024-11-22 09:54:29 UTC",
    "updated_date": "2024-11-22 09:54:29 UTC"
  },
  {
    "arxiv_id": "2411.15257v1",
    "title": "The Explabox: Model-Agnostic Machine Learning Transparency & Analysis",
    "authors": [
      "Marcel Robeer",
      "Michiel Bron",
      "Elize Herrewijnen",
      "Riwish Hoeseni",
      "Floris Bex"
    ],
    "abstract": "We present the Explabox: an open-source toolkit for transparent and\nresponsible machine learning (ML) model development and usage. Explabox aids in\nachieving explainable, fair and robust models by employing a four-step\nstrategy: explore, examine, explain and expose. These steps offer\nmodel-agnostic analyses that transform complex 'ingestibles' (models and data)\ninto interpretable 'digestibles'. The toolkit encompasses digestibles for\ndescriptive statistics, performance metrics, model behavior explanations (local\nand global), and robustness, security, and fairness assessments. Implemented in\nPython, Explabox supports multiple interaction modes and builds on open-source\npackages. It empowers model developers and testers to operationalize\nexplainability, fairness, auditability, and security. The initial release\nfocuses on text data and models, with plans for expansion. Explabox's code and\ndocumentation are available open-source at https://explabox.readthedocs.io/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE",
      "I.2; D.2.5"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15257v1",
    "published_date": "2024-11-22 09:10:57 UTC",
    "updated_date": "2024-11-22 09:10:57 UTC"
  },
  {
    "arxiv_id": "2411.14808v2",
    "title": "High-Resolution Image Synthesis via Next-Token Prediction",
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Tiezhu Yue",
      "Xiaoming Wei",
      "Enhua Wu"
    ],
    "abstract": "Recently, autoregressive models have demonstrated remarkable performance in\nclass-conditional image generation. However, the application of next-token\nprediction to high-resolution text-to-image generation remains largely\nunexplored. In this paper, we introduce \\textbf{D-JEPA$\\cdot$T2I}, an\nautoregressive model based on continuous tokens that incorporates innovations\nin both architecture and training strategy to generate high-quality,\nphotorealistic images at arbitrary resolutions, up to 4K. Architecturally, we\nadopt the denoising joint embedding predictive architecture (D-JEPA) while\nleveraging a multimodal visual transformer to effectively integrate textual and\nvisual features. Additionally, we introduce flow matching loss alongside the\nproposed Visual Rotary Positional Embedding (VoPE) to enable continuous\nresolution learning. In terms of training strategy, we propose a data feedback\nmechanism that dynamically adjusts the sampling procedure based on statistical\nanalysis and an online learning critic model. This encourages the model to move\nbeyond its comfort zone, reducing redundant training on well-mastered scenarios\nand compelling it to address more challenging cases with suboptimal generation\nquality. For the first time, we achieve state-of-the-art high-resolution image\nsynthesis via next-token prediction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.14808v2",
    "published_date": "2024-11-22 09:08:58 UTC",
    "updated_date": "2025-03-02 08:53:47 UTC"
  },
  {
    "arxiv_id": "2411.14797v1",
    "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
    "authors": [
      "Ke Zhu",
      "Yu Wang",
      "Yanpeng Sun",
      "Qiang Chen",
      "Jiangjiang Liu",
      "Gang Zhang",
      "Jingdong Wang"
    ],
    "abstract": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14797v1",
    "published_date": "2024-11-22 08:48:30 UTC",
    "updated_date": "2024-11-22 08:48:30 UTC"
  },
  {
    "arxiv_id": "2411.15254v1",
    "title": "A Unified Energy Management Framework for Multi-Timescale Forecasting in Smart Grids",
    "authors": [
      "Dafang Zhao",
      "Xihao Piao",
      "Zheng Chen",
      "Zhengmao Li",
      "Ittetsu Taniguchi"
    ],
    "abstract": "Accurate forecasting of the electrical load, such as the magnitude and the\ntiming of peak power, is crucial to successful power system management and\nimplementation of smart grid strategies like demand response and peak shaving.\nIn multi-time-scale optimization scheduling, rolling optimization is a common\nsolution. However, rolling optimization needs to consider the coupling of\ndifferent optimization objectives across time scales. It is challenging to\naccurately capture the mid- and long-term dependencies in time series data.\nThis paper proposes Multi-pofo, a multi-scale power load forecasting framework,\nthat captures such dependency via a novel architecture equipped with a temporal\npositional encoding layer. To validate the effectiveness of the proposed model,\nwe conduct experiments on real-world electricity load data. The experimental\nresults show that our approach outperforms compared to several strong baseline\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to PES GM 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.15254v1",
    "published_date": "2024-11-22 08:45:41 UTC",
    "updated_date": "2024-11-22 08:45:41 UTC"
  },
  {
    "arxiv_id": "2411.15252v1",
    "title": "LocRef-Diffusion:Tuning-Free Layout and Appearance-Guided Generation",
    "authors": [
      "Fan Deng",
      "Yaguang Wu",
      "Xinyang Yu",
      "Xiangjun Huang",
      "Jian Yang",
      "Guangyu Yan",
      "Qiang Xu"
    ],
    "abstract": "Recently, text-to-image models based on diffusion have achieved remarkable\nsuccess in generating high-quality images. However, the challenge of\npersonalized, controllable generation of instances within these images remains\nan area in need of further development. In this paper, we present\nLocRef-Diffusion, a novel, tuning-free model capable of personalized\ncustomization of multiple instances' appearance and position within an image.\nTo enhance the precision of instance placement, we introduce a Layout-net,\nwhich controls instance generation locations by leveraging both explicit\ninstance layout information and an instance region cross-attention module. To\nimprove the appearance fidelity to reference images, we employ an\nappearance-net that extracts instance appearance features and integrates them\ninto the diffusion model through cross-attention mechanisms. We conducted\nextensive experiments on the COCO and OpenImages datasets, and the results\ndemonstrate that our proposed method achieves state-of-the-art performance in\nlayout and appearance guided generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15252v1",
    "published_date": "2024-11-22 08:44:39 UTC",
    "updated_date": "2024-11-22 08:44:39 UTC"
  },
  {
    "arxiv_id": "2411.15251v1",
    "title": "Optimized Vessel Segmentation: A Structure-Agnostic Approach with Small Vessel Enhancement and Morphological Correction",
    "authors": [
      "Dongning Song",
      "Weijian Huang",
      "Jiarun Liu",
      "Md Jahidul Islam",
      "Hao Yang",
      "Shanshan Wang"
    ],
    "abstract": "Accurate segmentation of blood vessels is essential for various clinical\nassessments and postoperative analyses. However, the inherent challenges of\nvascular imaging, such as sparsity, fine granularity, low contrast, data\ndistribution variability, and the critical need for preserving topological\nstructure, making generalized vessel segmentation particularly complex. While\nspecialized segmentation methods have been developed for specific anatomical\nregions, their over-reliance on tailored models hinders broader applicability\nand generalization. General-purpose segmentation models introduced in medical\nimaging often fail to address critical vascular characteristics, including the\nconnectivity of segmentation results. To overcome these limitations, we propose\nan optimized vessel segmentation framework: a structure-agnostic approach\nincorporating small vessel enhancement and morphological correction for\nmulti-modality vessel segmentation. To train and validate this framework, we\ncompiled a comprehensive multi-modality dataset spanning 17 datasets and\nbenchmarked our model against six SAM-based methods and 17 expert models. The\nresults demonstrate that our approach achieves superior segmentation accuracy,\ngeneralization, and a 34.6% improvement in connectivity, underscoring its\nclinical potential. An ablation study further validates the effectiveness of\nthe proposed improvements. We will release the code and dataset at github\nfollowing the publication of this work.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 7 figurres, submitted to TIP",
    "pdf_url": "http://arxiv.org/pdf/2411.15251v1",
    "published_date": "2024-11-22 08:38:30 UTC",
    "updated_date": "2024-11-22 08:38:30 UTC"
  },
  {
    "arxiv_id": "2411.14794v1",
    "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
    "authors": [
      "Songhao Han",
      "Wei Huang",
      "Hairong Shi",
      "Le Zhuo",
      "Xiu Su",
      "Shifeng Zhang",
      "Xu Zhou",
      "Xiaojuan Qi",
      "Yue Liao",
      "Si Liu"
    ],
    "abstract": "The advancement of Large Vision Language Models (LVLMs) has significantly\nimproved multimodal understanding, yet challenges remain in video reasoning\ntasks due to the scarcity of high-quality, large-scale datasets. Existing video\nquestion-answering (VideoQA) datasets often rely on costly manual annotations\nwith insufficient granularity or automatic construction methods with redundant\nframe-by-frame analysis, limiting their scalability and effectiveness for\ncomplex reasoning. To address these challenges, we introduce VideoEspresso, a\nnovel dataset that features VideoQA pairs preserving essential spatial details\nand temporal coherence, along with multimodal annotations of intermediate\nreasoning steps. Our construction pipeline employs a semantic-aware method to\nreduce redundancy, followed by generating QA pairs using GPT-4o. We further\ndevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,\nguiding GPT-4o in extracting logical relationships from QA pairs and video\ncontent. To exploit the potential of high-quality VideoQA pairs, we propose a\nHybrid LVLMs Collaboration framework, featuring a Frame Selector and a\ntwo-stage instruction fine-tuned reasoning LVLM. This framework adaptively\nselects core frames and performs CoT reasoning using multimodal evidence.\nEvaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our\nmethod outperforms existing baselines on most tasks, demonstrating superior\nvideo reasoning capabilities. Our code and dataset will be released at:\nhttps://github.com/hshjerry/VideoEspresso",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14794v1",
    "published_date": "2024-11-22 08:33:36 UTC",
    "updated_date": "2024-11-22 08:33:36 UTC"
  },
  {
    "arxiv_id": "2411.15250v1",
    "title": "TPLogAD: Unsupervised Log Anomaly Detection Based on Event Templates and Key Parameters",
    "authors": [
      "Jiawei Lu",
      "Chengrong Wu"
    ],
    "abstract": "Log-system is an important mechanism for recording the runtime status and\nevents of Web service systems, and anomaly detection in logs is an effective\nmethod of detecting problems. However, manual anomaly detection in logs is\ninefficient, error-prone, and unrealistic. Existing log anomaly detection\nmethods either use the indexes of event templates, or form vectors by embedding\nthe fixed string part of the template as a sentence, or use time parameters for\nsequence analysis. However, log entries often contain features and semantic\ninformation that cannot be fully represented by these methods, resulting in\nmissed and false alarms. In this paper, we propose TPLogAD, a universal\nunsupervised method for analyzing unstructured logs, which performs anomaly\ndetection based on event templates and key parameters. The itemplate2vec and\npara2vec included in TPLogAD are two efficient and easy-to-implement semantic\nrepresentation methods for logs, detecting anomalies in event templates and\nparameters respectively, which has not been achieved in previous work.\nAdditionally, TPLogAD can avoid the interference of log diversity and dynamics\non anomaly detection. Our experiments on four public log datasets show that\nTPLogAD outperforms existing log anomaly detection methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15250v1",
    "published_date": "2024-11-22 08:25:21 UTC",
    "updated_date": "2024-11-22 08:25:21 UTC"
  },
  {
    "arxiv_id": "2411.14790v4",
    "title": "KBAlign: Efficient Self Adaptation on Specific Knowledge Bases",
    "authors": [
      "Zheni Zeng",
      "Yuxuan Chen",
      "Shi Yu",
      "Ruobing Wang",
      "Yukun Yan",
      "Zhenghao Liu",
      "Shuo Wang",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Although retrieval-augmented generation (RAG) remains essential for\nknowledge-based question answering (KBQA), current paradigms face critical\nchallenges under specific domains. Existing methods struggle with targeted\nadaptation on small-scale KBs: vanilla unsupervised training exhibits poor\neffectiveness, while fine-tuning incurs prohibitive costs of external signals.\nWe present KBAlign, a self-supervised framework that enhances RAG systems\nthrough efficient model adaptation. Our key insight is to leverage the model's\nintrinsic capabilities for knowledge alignment through two innovative\nmechanisms: multi-grained self-annotation that captures global knowledge for\ndata construction, and iterative tuning that accelerates convergence through\nself verification. This framework enables cost-effective model adaptation to\nspecific textual KBs, without human supervision or external model assistance.\nExperiments demonstrate that KBAlign can achieve 90\\% of the performance gain\nobtained through GPT-4-supervised adaptation, while relying entirely on\nself-annotation of much smaller models. KBAlign significantly improves\ndownstream QA accuracy across multiple domains with tiny costs, particularly\nbenefiting scenarios requiring deep knowledge integration from specialized\ncorpora. We release our experimental data, models, and process analyses to the\ncommunity for further exploration (https://github.com/thunlp/KBAlign).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14790v4",
    "published_date": "2024-11-22 08:21:03 UTC",
    "updated_date": "2025-05-15 13:02:21 UTC"
  },
  {
    "arxiv_id": "2411.14774v2",
    "title": "Resolution-Agnostic Transformer-based Climate Downscaling",
    "authors": [
      "Declan Curran",
      "Hira Saleem",
      "Sanaa Hobeichi",
      "Flora Salim"
    ],
    "abstract": "Understanding future weather changes at regional and local scales is crucial\nfor planning and decision-making, particularly in the context of extreme\nweather events, as well as for broader applications in agriculture, insurance,\nand infrastructure development. However, the computational cost of downscaling\nGlobal Climate Models (GCMs) to the fine resolutions needed for such\napplications presents a significant barrier. Drawing on advancements in weather\nforecasting models, this study introduces a cost-efficient downscaling method\nusing a pretrained Earth Vision Transformer (Earth ViT) model. Initially\ntrained on ERA5 data to downscale from 50 km to 25 km resolution, the model is\nthen tested on the higher resolution BARRA-SY dataset at a 3 km resolution.\nRemarkably, it performs well without additional training, demonstrating its\nability to generalize across different resolutions. This approach holds promise\nfor generating large ensembles of regional climate simulations by downscaling\nGCMs with varying input resolutions without incurring additional training\ncosts. Ultimately, this method could provide more comprehensive estimates of\npotential future changes in key climate variables, aiding in effective planning\nfor extreme weather events and climate change adaptation strategies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14774v2",
    "published_date": "2024-11-22 07:32:11 UTC",
    "updated_date": "2024-11-27 00:55:18 UTC"
  },
  {
    "arxiv_id": "2411.14773v2",
    "title": "Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology",
    "authors": [
      "Qian Liang",
      "Yi Zeng",
      "Menghaoran Tang"
    ],
    "abstract": "Musical mode is one of the most critical element that establishes the\nframework of pitch organization and determines the harmonic relationships.\nPrevious works often use the simplistic and rigid alignment method, and\noverlook the diversity of modes. However, in contrast to AI models, humans\npossess cognitive mechanisms for perceiving the various modes and keys. In this\npaper, we propose a spiking neural network inspired by brain mechanisms and\npsychological theories to represent musical modes and keys, ultimately\ngenerating musical pieces that incorporate tonality features. Specifically, the\ncontributions are detailed as follows: 1) The model is designed with multiple\ncollaborated subsystems inspired by the structures and functions of\ncorresponding brain regions; 2)We incorporate mechanisms for neural circuit\nevolutionary learning that enable the network to learn and generate\nmode-related features in music, reflecting the cognitive processes involved in\nhuman music perception. 3)The results demonstrate that the proposed model shows\na connection framework closely similar to the Krumhansl-Schmuckler model, which\nis one of the most significant key perception models in the music psychology\ndomain. 4) Experiments show that the model can generate music pieces with\ncharacteristics of the given modes and keys. Additionally, the quantitative\nassessments of generated pieces reveals that the generating music pieces have\nboth tonality characteristics and the melodic adaptability needed to generate\ndiverse and musical content. By combining insights from neuroscience,\npsychology, and music theory with advanced neural network architectures, our\nresearch aims to create a system that not only learns and generates music but\nalso bridges the gap between human cognition and artificial intelligence.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "q-bio.NC"
    ],
    "primary_category": "cs.SD",
    "comment": "18 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14773v2",
    "published_date": "2024-11-22 07:29:26 UTC",
    "updated_date": "2025-01-14 05:16:18 UTC"
  },
  {
    "arxiv_id": "2411.14768v1",
    "title": "Grid and Road Expressions Are Complementary for Trajectory Representation Learning",
    "authors": [
      "Silin Zhou",
      "Shuo Shang",
      "Lisi Chen",
      "Peng Han",
      "Christian S. Jensen"
    ],
    "abstract": "Trajectory representation learning (TRL) maps trajectories to vectors that\ncan be used for many downstream tasks. Existing TRL methods use either grid\ntrajectories, capturing movement in free space, or road trajectories, capturing\nmovement in a road network, as input. We observe that the two types of\ntrajectories are complementary, providing either region and location\ninformation or providing road structure and movement regularity. Therefore, we\npropose a novel multimodal TRL method, dubbed GREEN, to jointly utilize Grid\nand Road trajectory Expressions for Effective representatioN learning. In\nparticular, we transform raw GPS trajectories into both grid and road\ntrajectories and tailor two encoders to capture their respective information.\nTo align the two encoders such that they complement each other, we adopt a\ncontrastive loss to encourage them to produce similar embeddings for the same\nraw trajectory and design a mask language model (MLM) loss to use grid\ntrajectories to help reconstruct masked road trajectories. To learn the final\ntrajectory representation, a dual-modal interactor is used to fuse the outputs\nof the two encoders via cross-attention. We compare GREEN with 7\nstate-of-the-art TRL methods for 3 downstream tasks, finding that GREEN\nconsistently outperforms all baselines and improves the accuracy of the\nbest-performing baseline by an average of 15.99\\%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by KDD2025(August Cycle)",
    "pdf_url": "http://arxiv.org/pdf/2411.14768v1",
    "published_date": "2024-11-22 07:15:46 UTC",
    "updated_date": "2024-11-22 07:15:46 UTC"
  },
  {
    "arxiv_id": "2411.14762v4",
    "title": "Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction",
    "authors": [
      "Huiwon Jang",
      "Sihyun Yu",
      "Jinwoo Shin",
      "Pieter Abbeel",
      "Younggyo Seo"
    ],
    "abstract": "Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available on the project webpage:\n  https://huiwon-jang.github.io/coordtok/",
    "pdf_url": "http://arxiv.org/pdf/2411.14762v4",
    "published_date": "2024-11-22 06:50:44 UTC",
    "updated_date": "2025-04-03 02:29:28 UTC"
  },
  {
    "arxiv_id": "2411.14759v1",
    "title": "Hammer: Towards Efficient Hot-Cold Data Identification via Online Learning",
    "authors": [
      "Kai Lu",
      "Siqi Zhao",
      "Jiguang Wan"
    ],
    "abstract": "Efficient management of storage resources in big data and cloud computing\nenvironments requires accurate identification of data's \"cold\" and \"hot\"\nstates. Traditional methods, such as rule-based algorithms and early AI\ntechniques, often struggle with dynamic workloads, leading to low accuracy,\npoor adaptability, and high operational overhead. To address these issues, we\npropose a novel solution based on online learning strategies. Our approach\ndynamically adapts to changing data access patterns, achieving higher accuracy\nand lower operational costs. Rigorous testing with both synthetic and\nreal-world datasets demonstrates a significant improvement, achieving a 90%\naccuracy rate in hot-cold classification. Additionally, the computational and\nstorage overheads are considerably reduced.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14759v1",
    "published_date": "2024-11-22 06:46:16 UTC",
    "updated_date": "2024-11-22 06:46:16 UTC"
  },
  {
    "arxiv_id": "2411.14751v1",
    "title": "TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior",
    "authors": [
      "Sen Yang",
      "Minyue Jiang",
      "Ziwei Fan",
      "Xiaolu Xie",
      "Xiao Tan",
      "Yingying Li",
      "Errui Ding",
      "Liang Wang",
      "Jingdong Wang"
    ],
    "abstract": "Recent advances in autonomous driving systems have shifted towards reducing\nreliance on high-definition maps (HDMaps) due to the huge costs of annotation\nand maintenance. Instead, researchers are focusing on online vectorized HDMap\nconstruction using on-board sensors. However, sensor-only approaches still face\nchallenges in long-range perception due to the restricted views imposed by the\nmounting angles of onboard cameras, just as human drivers also rely on\nbird's-eye-view navigation maps for a comprehensive understanding of road\nstructures. To address these issues, we propose to train the perception model\nto \"see\" standard definition maps (SDMaps). We encode SDMap elements into\nneural spatial map representations and instance tokens, and then incorporate\nsuch complementary features as prior information to improve the bird's eye view\n(BEV) feature for lane geometry and topology decoding. Based on the lane\nsegment representation framework, the model simultaneously predicts lanes,\ncentrelines and their topology. To further enhance the ability of geometry\nprediction and topology reasoning, we also use a topology-guided decoder to\nrefine the predictions by exploiting the mutual relationships between\ntopological and geometric features. We perform extensive experiments on\nOpenLane-V2 datasets to validate the proposed method. The results show that our\nmodel outperforms state-of-the-art methods by a large margin, with gains of\n+6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that\nmodels trained with SDMap noise augmentation exhibit enhanced robustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 7 figures, and 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.14751v1",
    "published_date": "2024-11-22 06:13:42 UTC",
    "updated_date": "2024-11-22 06:13:42 UTC"
  },
  {
    "arxiv_id": "2412.06793v1",
    "title": "FEAD: Figma-Enhanced App Design Framework for Improving UI/UX in Educational App Development",
    "authors": [
      "Tianyi Huang"
    ],
    "abstract": "Designing user-centric mobile applications is increasingly essential in\neducational technology. However, platforms like MIT App Inventor-one of the\nworld's largest educational app development tools-face inherent limitations in\nsupporting modern UI/UX design. This study introduces the Figma-Enhanced App\nDesign (FEAD) Method, a structured framework that integrates Figma's advanced\ndesign tools into MIT App Inventor using an identify-design-implement workflow.\nLeveraging principles such as the 8-point grid system and Gestalt laws of\nperception, the FEAD Method empowers users to address design gaps, creating\nvisually appealing, functional, and accessible applications. A comparative\nevaluation revealed that 61.2% of participants perceived FEAD-enhanced designs\nas on par with professional apps, compared to just 8.2% for baseline designs.\nThese findings highlight the potential of bridging design with development\nplatforms to enhance app creation, offering a scalable framework for students\nto master both functional and aesthetic design principles and excel in shaping\nthe future of user-centric technology.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.06793v1",
    "published_date": "2024-11-22 05:56:53 UTC",
    "updated_date": "2024-11-22 05:56:53 UTC"
  },
  {
    "arxiv_id": "2411.14744v1",
    "title": "Point Cloud Understanding via Attention-Driven Contrastive Learning",
    "authors": [
      "Yi Wang",
      "Jiaze Wang",
      "Ziyu Guo",
      "Renrui Zhang",
      "Donghao Zhou",
      "Guangyong Chen",
      "Anfeng Liu",
      "Pheng-Ann Heng"
    ],
    "abstract": "Recently Transformer-based models have advanced point cloud understanding by\nleveraging self-attention mechanisms, however, these methods often overlook\nlatent information in less prominent regions, leading to increased sensitivity\nto perturbations and limited global comprehension. To solve this issue, we\nintroduce PointACL, an attention-driven contrastive learning framework designed\nto address these limitations. Our method employs an attention-driven dynamic\nmasking strategy that guides the model to focus on under-attended regions,\nenhancing the understanding of global structures within the point cloud. Then\nwe combine the original pre-training loss with a contrastive learning loss,\nimproving feature discrimination and generalization. Extensive experiments\nvalidate the effectiveness of PointACL, as it achieves state-of-the-art\nperformance across a variety of 3D understanding tasks, including object\nclassification, part segmentation, and few-shot learning. Specifically, when\nintegrated with different Transformer backbones like Point-MAE and PointGPT,\nPointACL demonstrates improved performance on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart. This highlights its superior capability in\ncapturing both global and local features, as well as its enhanced robustness\nagainst perturbations and incomplete data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14744v1",
    "published_date": "2024-11-22 05:41:00 UTC",
    "updated_date": "2024-11-22 05:41:00 UTC"
  },
  {
    "arxiv_id": "2411.14743v2",
    "title": "FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification",
    "authors": [
      "Zhengrui Guo",
      "Conghao Xiong",
      "Jiabo Ma",
      "Qichen Sun",
      "Lishuang Feng",
      "Jinzhuo Wang",
      "Hao Chen"
    ],
    "abstract": "Few-shot learning presents a critical solution for cancer diagnosis in\ncomputational pathology (CPath), addressing fundamental limitations in data\navailability, particularly the scarcity of expert annotations and patient\nprivacy constraints. A key challenge in this paradigm stems from the inherent\ndisparity between the limited training set of whole slide images (WSIs) and the\nenormous number of contained patches, where a significant portion of these\npatches lacks diagnostically relevant information, potentially diluting the\nmodel's ability to learn and focus on critical diagnostic features. While\nrecent works attempt to address this by incorporating additional knowledge,\nseveral crucial gaps hinder further progress: (1) despite the emergence of\npowerful pathology foundation models (FMs), their potential remains largely\nuntapped, with most approaches limiting their use to basic feature extraction;\n(2) current language guidance mechanisms attempt to align text prompts with\nvast numbers of WSI patches all at once, struggling to leverage rich\npathological semantic information. To this end, we introduce the\nknowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which\nuniquely combines pathology FMs with language prior knowledge to enable a\nfocused analysis of diagnostically relevant regions by prioritizing\ndiscriminative WSI patches. Our approach implements a progressive three-stage\ncompression strategy: we first leverage FMs for global visual redundancy\nelimination, and integrate compressed features with language prompts for\nsemantic relevance assessment, then perform neighbor-aware visual token\nfiltering while preserving spatial coherence. Extensive experiments on\npathological datasets spanning breast, lung, and ovarian cancers demonstrate\nits superior performance in few-shot pathology diagnosis. Codes are available\nat https://github.com/dddavid4real/FOCUS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR'2025",
    "pdf_url": "http://arxiv.org/pdf/2411.14743v2",
    "published_date": "2024-11-22 05:36:38 UTC",
    "updated_date": "2025-03-20 12:16:47 UTC"
  },
  {
    "arxiv_id": "2411.17729v3",
    "title": "Fast convolution algorithm for state space models",
    "authors": [
      "Gregory Beylkin"
    ],
    "abstract": "We present an unconditionally stable algorithm for applying matrix transfer\nfunction of a linear time invariant system (LTI) in time domain. The state\nmatrix of an LTI system used for modeling long range dependencies in state\nspace models (SSMs) has eigenvalues close to $1$. The standard recursion\ndefining LTI system becomes unstable if the $m\\times m$ state matrix has just\none eigenvalue with absolute value even slightly greater than 1. This may occur\nwhen approximating a state matrix by a structured matrix to reduce the cost of\nmatrix-vector multiplication from $\\mathcal{O}\\left(m^{2}\\right)$ to\n$\\mathcal{O}\\left(m\\right)$ or $\\mathcal{O}\\left(m\\log m\\right).$ We introduce\nan unconditionally stable algorithm that uses an approximation of the rational\ntransfer function in the z-domain by a matrix polynomial of degree $2^{N+1}-1$,\nwhere $N$ is chosen to achieve any user-selected accuracy. Using a cascade\nimplementation in time domain, applying such transfer function to compute $L$\nstates requires no more than $2L$ matrix-vector multiplications (whereas the\nstandard recursion requires $L$ matrix-vector multiplications). However, using\nunconditionally stable algorithm, it is not necessary to assure that an\napproximate state matrix has all eigenvalues with absolute values strictly less\nthan 1 i.e., within the desired accuracy, the absolute value of some\neigenvalues may possibly exceed $1$. Consequently, this algorithm allows one to\nuse a wider variety of structured approximations to reduce the cost of\nmatrix-vector multiplication and we briefly describe several of them to be used\nfor this purpose.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.17729v3",
    "published_date": "2024-11-22 05:30:03 UTC",
    "updated_date": "2025-04-11 00:35:25 UTC"
  },
  {
    "arxiv_id": "2411.14740v1",
    "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
    "authors": [
      "Xin Yu",
      "Ze Yuan",
      "Yuan-Chen Guo",
      "Ying-Tian Liu",
      "JianHui Liu",
      "Yangguang Li",
      "Yan-Pei Cao",
      "Ding Liang",
      "Xiaojuan Qi"
    ],
    "abstract": "While high-quality texture maps are essential for realistic 3D asset\nrendering, few studies have explored learning directly in the texture space,\nespecially on large-scale datasets. In this work, we depart from the\nconventional approach of relying on pre-trained 2D diffusion models for\ntest-time optimization of 3D textures. Instead, we focus on the fundamental\nproblem of learning in the UV texture space itself. For the first time, we\ntrain a large diffusion model capable of directly generating high-resolution\ntexture maps in a feed-forward manner. To facilitate efficient learning in\nhigh-resolution UV spaces, we propose a scalable network architecture that\ninterleaves convolutions on UV maps with attention layers on point clouds.\nLeveraging this architectural design, we train a 700 million parameter\ndiffusion model that can generate UV texture maps guided by text prompts and\nsingle-view images. Once trained, our model naturally supports various extended\napplications, including text-guided texture inpainting, sparse-view texture\ncompletion, and text-driven texture synthesis. Project page is at\nhttp://cvmi-lab.github.io/TEXGen/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to SIGGRAPH Asia Journal Article (TOG 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.14740v1",
    "published_date": "2024-11-22 05:22:11 UTC",
    "updated_date": "2024-11-22 05:22:11 UTC"
  },
  {
    "arxiv_id": "2411.14738v1",
    "title": "Universal and Context-Independent Triggers for Precise Control of LLM Outputs",
    "authors": [
      "Jiashuo Liang",
      "Guancheng Li",
      "Yang Yu"
    ],
    "abstract": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14738v1",
    "published_date": "2024-11-22 05:17:18 UTC",
    "updated_date": "2024-11-22 05:17:18 UTC"
  },
  {
    "arxiv_id": "2411.14713v1",
    "title": "LIBER: Lifelong User Behavior Modeling Based on Large Language Models",
    "authors": [
      "Chenxu Zhu",
      "Shigang Quan",
      "Bo Chen",
      "Jianghao Lin",
      "Xiaoling Cai",
      "Hong Zhu",
      "Xiangyang Li",
      "Yunjia Xi",
      "Weinan Zhang",
      "Ruiming Tang"
    ],
    "abstract": "CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14713v1",
    "published_date": "2024-11-22 03:43:41 UTC",
    "updated_date": "2024-11-22 03:43:41 UTC"
  },
  {
    "arxiv_id": "2411.14708v3",
    "title": "Understanding LLM Embeddings for Regression",
    "authors": [
      "Eric Tang",
      "Bangding Yang",
      "Xingyou Song"
    ],
    "abstract": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (TMLR) 2025.\n  Code can be found in https://github.com/google-research/optformer",
    "pdf_url": "http://arxiv.org/pdf/2411.14708v3",
    "published_date": "2024-11-22 03:33:51 UTC",
    "updated_date": "2025-02-15 16:26:55 UTC"
  },
  {
    "arxiv_id": "2411.14698v1",
    "title": "Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation",
    "authors": [
      "Xunyu Zhu",
      "Jian Li",
      "Can Ma",
      "Weiping Wang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14698v1",
    "published_date": "2024-11-22 03:12:39 UTC",
    "updated_date": "2024-11-22 03:12:39 UTC"
  },
  {
    "arxiv_id": "2411.14696v2",
    "title": "Quantum Hamiltonian Descent for Graph Partition",
    "authors": [
      "Jinglei Cheng",
      "Ruilin Zhou",
      "Yuhang Gan",
      "Chen Qian",
      "Junyu Liu"
    ],
    "abstract": "We introduce Quantum Hamiltonian Descent as a novel approach to solve the\ngraph partition problem. By reformulating graph partition as a Quadratic\nUnconstrained Binary Optimization (QUBO) problem, we leverage QHD's\nquantum-inspired dynamics to identify optimal community structures. Our method\nimplements a multi-level refinement strategy that alternates between QUBO\nformulation and QHD optimization to iteratively improve partition quality.\nExperimental results demonstrate that our QHD-based approach achieves superior\nmodularity scores (up to 5.49\\%) improvement with reduced computational\noverhead compared to traditional optimization methods. This work establishes\nQHD as an effective quantum-inspired framework for tackling graph partition\nchallenges in large-scale networks.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted by DAC 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.14696v2",
    "published_date": "2024-11-22 03:08:24 UTC",
    "updated_date": "2025-02-17 02:50:40 UTC"
  },
  {
    "arxiv_id": "2411.15244v2",
    "title": "Adversarial Prompt Distillation for Vision-Language Models",
    "authors": [
      "Lin Luo",
      "Xin Wang",
      "Bojia Zi",
      "Shihao Zhao",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "Large pre-trained Vision-Language Models (VLMs) such as Contrastive\nLanguage-Image Pre-training (CLIP) have been shown to be susceptible to\nadversarial attacks, raising concerns about their deployment in safety-critical\napplications like autonomous driving and medical diagnosis. One promising\napproach for robustifying pre-trained VLMs is Adversarial Prompt Tuning (APT),\nwhich applies adversarial training during the process of prompt tuning.\nHowever, existing APT methods are mostly single-modal methods that design\nprompt(s) for only the visual or textual modality, limiting their effectiveness\nin either robustness or clean accuracy. In this work, we propose Adversarial\nPrompt Distillation (APD), a bimodal knowledge distillation framework that\nenhances APT by integrating it with multi-modal knowledge transfer. APD\noptimizes prompts for both visual and textual modalities while distilling\nknowledge from a clean pre-trained teacher CLIP model. Extensive experiments on\nmultiple benchmark datasets demonstrate the superiority of our APD method over\nthe current state-of-the-art APT methods in terms of both adversarial\nrobustness and clean accuracy. The effectiveness of APD also validates the\npossibility of using a non-robust teacher to improve the generalization and\nrobustness of fine-tuned VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15244v2",
    "published_date": "2024-11-22 03:02:13 UTC",
    "updated_date": "2025-04-15 01:57:20 UTC"
  },
  {
    "arxiv_id": "2411.15243v1",
    "title": "Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence",
    "authors": [
      "Nima Dehghani",
      "Michael Levin"
    ],
    "abstract": "The pursuit of creating artificial intelligence (AI) mirrors our longstanding\nfascination with understanding our own intelligence. From the myths of Talos to\nAristotelian logic and Heron's inventions, we have sought to replicate the\nmarvels of the mind. While recent advances in AI hold promise, singular\napproaches often fall short in capturing the essence of intelligence. This\npaper explores how fundamental principles from biological\ncomputation--particularly context-dependent, hierarchical information\nprocessing, trial-and-error heuristics, and multi-scale organization--can guide\nthe design of truly intelligent systems. By examining the nuanced mechanisms of\nbiological intelligence, such as top-down causality and adaptive interaction\nwith the environment, we aim to illuminate potential limitations in artificial\nconstructs. Our goal is to provide a framework inspired by biological systems\nfor designing more adaptable and robust artificial intelligent systems.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.NE",
      "cs.SC"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15243v1",
    "published_date": "2024-11-22 02:55:39 UTC",
    "updated_date": "2024-11-22 02:55:39 UTC"
  },
  {
    "arxiv_id": "2411.15242v1",
    "title": "The Zamba2 Suite: Technical Report",
    "authors": [
      "Paolo Glorioso",
      "Quentin Anthony",
      "Yury Tokpanov",
      "Anna Golubeva",
      "Vasudev Shyam",
      "James Whittington",
      "Jonathan Pilault",
      "Beren Millidge"
    ],
    "abstract": "In this technical report, we present the Zamba2 series -- a suite of 1.2B,\n2.7B, and 7.4B parameter hybrid Mamba2-transformer models that achieve state of\nthe art performance against the leading open-weights models of their class,\nwhile achieving substantial gains in inference latency, throughput, and memory\nefficiency. The Zamba2 series builds upon our initial work with Zamba1-7B,\noptimizing its architecture, training and annealing datasets, and training for\nup to three trillion tokens. We provide open-source weights for all models of\nthe Zamba2 series as well as instruction-tuned variants that are strongly\ncompetitive against comparable instruct-tuned models of their class. We\nadditionally open-source the pretraining dataset, which we call Zyda-2, used to\ntrain the Zamba2 series of models. The models and datasets used in this work\nare openly available at https://huggingface.co/Zyphra",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21/11/24 initial upload",
    "pdf_url": "http://arxiv.org/pdf/2411.15242v1",
    "published_date": "2024-11-22 02:55:20 UTC",
    "updated_date": "2024-11-22 02:55:20 UTC"
  },
  {
    "arxiv_id": "2411.14684v2",
    "title": "Learning Modality-Aware Representations: Adaptive Group-wise Interaction Network for Multimodal MRI Synthesis",
    "authors": [
      "Tao Song",
      "Yicheng Wu",
      "Minhao Hu",
      "Xiangde Luo",
      "Linda Wei",
      "Guotai Wang",
      "Yi Guo",
      "Feng Xu",
      "Shaoting Zhang"
    ],
    "abstract": "Multimodal MR image synthesis aims to generate missing modality images by\neffectively fusing and mapping from a subset of available MRI modalities. Most\nexisting methods adopt an image-to-image translation paradigm, treating\nmultiple modalities as input channels. However, these approaches often yield\nsub-optimal results due to the inherent difficulty in achieving precise\nfeature- or semantic-level alignment across modalities. To address these\nchallenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net)\nthat explicitly models both inter-modality and intra-modality relationships for\nmultimodal MR image synthesis. Specifically, feature channels are first\npartitioned into predefined groups, after which an adaptive rolling mechanism\nis applied to conventional convolutional kernels to better capture feature and\nsemantic correspondences between different modalities. In parallel, a\ncross-group attention module is introduced to enable effective feature fusion\nacross groups, thereby enhancing the network's representational capacity. We\nvalidate the proposed AGI-Net on the publicly available IXI and BraTS2023\ndatasets. Experimental results demonstrate that AGI-Net achieves\nstate-of-the-art performance in multimodal MR image synthesis tasks, confirming\nthe effectiveness of its modality-aware interaction design. We release the\nrelevant code at:\nhttps://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14684v2",
    "published_date": "2024-11-22 02:29:37 UTC",
    "updated_date": "2025-04-28 06:26:01 UTC"
  },
  {
    "arxiv_id": "2411.14672v1",
    "title": "Multiverse of Greatness: Generating Story Branches with LLMs",
    "authors": [
      "Pittawat Taveekitworachai",
      "Chollakorn Nimpattanavong",
      "Mustafa Can Gursesli",
      "Antonio Lanata",
      "Andrea Guazzini",
      "Ruck Thawonmas"
    ],
    "abstract": "This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel\nframework for interacting with LLMs to generate graph-based content with a\ndynamic context window history. While there is an existing study utilizing LLMs\nto generate a visual novel game, the previous study involved a manual process\nof output extraction and did not provide flexibility in generating a longer,\ncoherent story. We evaluate DCP/P against our baseline, which does not provide\ncontext history to an LLM and only relies on the initial story data. Through\nobjective evaluation, we show that simply providing the LLM with a summary\nleads to a subpar story compared to additionally providing the LLM with the\nproper context of the story. We also provide an extensive qualitative analysis\nand discussion. We qualitatively examine the quality of the objectively\nbest-performing generated game from each approach. In addition, we examine\nbiases in word choices and word sentiment of the generated content. We find a\nconsistent observation with previous studies that LLMs are biased towards\ncertain words, even with a different LLM family. Finally, we provide a\ncomprehensive discussion on opportunities for future studies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.14672v1",
    "published_date": "2024-11-22 02:11:37 UTC",
    "updated_date": "2024-11-22 02:11:37 UTC"
  },
  {
    "arxiv_id": "2411.15240v3",
    "title": "AI Foundation Models for Wearable Movement Data in Mental Health Research",
    "authors": [
      "Franklin Y. Ruan",
      "Aiwei Zhang",
      "Jenny Y. Oh",
      "SouYoung Jin",
      "Nicholas C. Jacobson"
    ],
    "abstract": "Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15240v3",
    "published_date": "2024-11-22 01:58:35 UTC",
    "updated_date": "2025-01-14 04:10:46 UTC"
  },
  {
    "arxiv_id": "2411.14654v3",
    "title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective",
    "authors": [
      "Jinming Xing",
      "Dongwen Luo",
      "Chang Xue",
      "Ruilin Xing"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ISMSI'25",
    "pdf_url": "http://arxiv.org/pdf/2411.14654v3",
    "published_date": "2024-11-22 00:59:25 UTC",
    "updated_date": "2025-02-01 22:39:04 UTC"
  },
  {
    "arxiv_id": "2411.14652v1",
    "title": "Social Media Algorithms Can Shape Affective Polarization via Exposure to Antidemocratic Attitudes and Partisan Animosity",
    "authors": [
      "Tiziano Piccardi",
      "Martin Saveski",
      "Chenyan Jia",
      "Jeffrey T. Hancock",
      "Jeanne L. Tsai",
      "Michael Bernstein"
    ],
    "abstract": "There is widespread concern about the negative impacts of social media feed\nranking algorithms on political polarization. Leveraging advancements in large\nlanguage models (LLMs), we develop an approach to re-rank feeds in real-time to\ntest the effects of content that is likely to polarize: expressions of\nantidemocratic attitudes and partisan animosity (AAPA). In a preregistered\n10-day field experiment on X/Twitter with 1,256 consented participants, we\nincrease or decrease participants' exposure to AAPA in their algorithmically\ncurated feeds. We observe more positive outparty feelings when AAPA exposure is\ndecreased and more negative outparty feelings when AAPA exposure is increased.\nExposure to AAPA content also results in an immediate increase in negative\nemotions, such as sadness and anger. The interventions do not significantly\nimpact traditional engagement metrics such as re-post and favorite rates. These\nfindings highlight a potential pathway for developing feed algorithms that\nmitigate affective polarization by addressing content that undermines the\nshared values required for a healthy democracy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14652v1",
    "published_date": "2024-11-22 00:55:07 UTC",
    "updated_date": "2024-11-22 00:55:07 UTC"
  }
]
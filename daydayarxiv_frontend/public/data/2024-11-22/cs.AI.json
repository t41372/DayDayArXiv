{
  "date": "2024-11-22",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-22 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 103 篇论文，主要聚焦 AI 模型优化、图像生成、医疗应用和强化学习等领域，其中 Zamba2 系列模型的发布和医疗图像处理的创新（如 FOCUS）最为令人印象深刻，同时有著名学者如 Michael Levin 和 Paolo Glorioso 参与的相关工作值得关注。\n\n### 重点论文讨论\n我将优先选取重要、创新性和话题度高的论文进行简要分析，并将相关主题归类讨论。对于其他较常规或小众论文（如某些优化算法或小数据集实验），将快速掠过，只提及核心点。以下按主题分组，每篇论文列出中文 + 英文标题，并突出主要贡献和发现。\n\n#### AI 模型与 LLM 优化（高话题度领域）\n- **Zamba2 Suite: Technical Report**（Zamba2 系列：技术报告）：Paolo Glorioso 等作者提出 Zamba2 系列模型（1.2B 到 7.4B 参数），结合 Mamba2 和 Transformer 架构，实现高效推理和状态-of-the-art 性能，在多个任务上超越同类开源模型，显著提升推理延迟和内存效率。\n- **KBAlign: Efficient Self Adaptation on Specific Knowledge Bases**（KBAlign：针对特定知识库的高效自适应）：Zheni Zeng 等提出一种自监督框架，利用 LLM 进行知识对齐，实现高效模型适应，实验显示在小规模知识库上提升 90% 性能，适用于动态知识整合。\n- **PointACL: Point Cloud Understanding via Attention-Driven Contrastive Learning**（PointACL：通过注意力驱动对比学习理解点云）：Yi Wang 等开发一种对比学习框架，增强 Transformer 在点云任务中的鲁棒性，实验在分类和分割任务上超越基线，提升全局和局部特征捕捉。\n- **Sycophancy in Large Language Models: Causes and Mitigations**（LLM 中的谄媚行为：原因与缓解）：Lars Malmqvist 分析 LLM 的谄媚倾向，提出训练数据优化和微调策略，揭示其对模型可靠性的影响，并提供缓解方法。\n- **LIBER: Lifelong User Behavior Modeling Based on Large Language Models**（LIBER：基于 LLM 的终身用户行为建模）：Chenxu Zhu 等设计一个多模块框架（UBSP、UIL、UIF），使用 LLM 动态建模用户行为，应用于推荐系统，提升播放量 3.01% 和播放时间 7.69%。\n- 其他如 **PPLqa**（PPLqa：无监督信息理论质量指标）和 **mR²AG**（mR²AG：多模态检索增强生成）快速掠过：前者提出 LLM 响应质量评估指标，后者提升多模态任务的检索精度，但细节较常规。\n\n#### 图像生成与视频处理（创新性强）\n- **VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection**（VideoEspresso：通过核心帧选择的细粒度视频推理数据集）：Songhao Han 等构建一个大规模数据集和框架，使用链式思考提升视频推理，实验在 14 个任务上超越基线。\n- **LocRef-Diffusion: Tuning-Free Layout and Appearance-Guided Generation**（LocRef-Diffusion：无调优的布局和外观引导生成）：Fan Deng 等提出基于扩散模型的生成框架，支持文本和图像引导的高分辨率生成，实验在 COCO 数据集上实现高效实例控制。\n- **TEXGen: a Generative Diffusion Model for Mesh Textures**（TEXGen：用于网格纹理的生成扩散模型）：Xin Yu 等开发一个扩散模型，直接生成高质量纹理，支持文本和图像引导，扩展到纹理修复等应用。\n- **Efficient Pruning of Text-to-Image Models**（文本到图像模型的Efficient Pruning）：Samarth N Ramesh 等探索 Stable Diffusion 的剪枝策略，实验显示在 38.5% 稀疏度下保持图像质量，揭示关键权重对语义的影响。\n- 其他如 **VideoRepair**（VideoRepair：通过失调评估和局部细化的视频生成）和 **OminiControl**（OminiControl：扩散 Transformer 的最小控制）快速掠过：前者修复视频失调，后者实现多任务控制，但实现细节较相似。\n\n#### 医疗与生物应用（实际影响大）\n- **FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification**（FOCUS：用于少样本全滑玻图像分类的知识增强自适应视觉压缩）：Zhengrui Guo 等提出一个框架，使用病理基础模型压缩图像，提升少样本诊断准确性，实验在多癌症数据集上超越 SOTA。\n- **TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior**（TopoSD：使用 SDMap 先验的拓扑增强车道段感知）：Sen Yang 等开发一个感知模型，使用标准地图作为先验，提升自动驾驶中的车道拓扑预测，实验在 OpenLane-V2 上大幅提升精度。\n- **FEAD: Figma-Enhanced App Design Framework for Improving UI/UX in Educational App Development**（FEAD：Figma 增强的教育 App 设计框架）：Tianyi Huang 提出集成 Figma 的设计框架，提升教育 App 的用户体验，实验显示 61.2% 用户感知提升。\n- 其他如 **GeoScatt-GNN**（GeoScatt-GNN：几何散射变换的图神经网络用于突变预测）和 **Health AI Developer Foundations**（健康 AI 开发者基础）快速掠过：前者改善药物发现，后者提供医疗基础模型，但贡献较专注领域内。\n\n#### 强化学习与其他创新（有著名学者参与）\n- **Quantum Hamiltonian Descent for Graph Partition**（量子 Hamiltonian 下降用于图分区）：Jinglei Cheng 等引入量子启发式算法优化图分区，实验显示比传统方法提升 5.49% 模块度，著名作者如 Junyu Liu 参与。\n- **Free Energy Projective Simulation (FEPS)**（自由能投影模拟）：Joséphine Pazem 等提出基于生物启发的强化学习框架，提升代理在动态环境中的决策，著名学者 Hans J. Briegel 贡献显著。\n- **Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence**（生物启发 AI：将生物复杂性集成到人工智能中）：Nima Dehghani 和 Michael Levin 探讨将生物机制（如分层处理）融入 AI，揭示人类认知与 AI 的桥梁。\n- 其他如 **RE-Bench**（RE-Bench：评估 AI 代理 R&D 能力）和 **EADReg**（EADReg：基于扩散的点云配准）快速掠过：前者评估 AI 研究能力，后者优化 LiDAR 处理，但实验性较强。\n\n总体而言，今天的论文突显 AI 领域的快速迭代，尤其在 LLM 优化和图像生成上取得突破，而医疗应用的实用性也值得关注。非核心论文（如某些算法细化或小数据集实验）虽众多，但多为增量改进，未作详述。如果您对特定主题感兴趣，欢迎进一步探讨！",
  "papers": [
    {
      "arxiv_id": "2411.16718v5",
      "title": "Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification",
      "title_zh": "翻译失败",
      "authors": [
        "S P Sharan",
        "Minkyu Choi",
        "Sahil Shah",
        "Harsh Goel",
        "Mohammad Omama",
        "Sandeep Chinchali"
      ],
      "abstract": "Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,\nand CogVideoX are pushing the boundaries of synthetic video generation, with\nadoption seen in fields like robotics, autonomous driving, and entertainment.\nAs these models become prevalent, various metrics and benchmarks have emerged\nto evaluate the quality of the generated videos. However, these metrics\nemphasize visual quality and smoothness, neglecting temporal fidelity and\ntext-to-video alignment, which are crucial for safety-critical applications. To\naddress this gap, we introduce NeuS-V, a novel synthetic video evaluation\nmetric that rigorously assesses text-to-video alignment using neuro-symbolic\nformal verification techniques. Our approach first converts the prompt into a\nformally defined Temporal Logic (TL) specification and translates the generated\nvideo into an automaton representation. Then, it evaluates the text-to-video\nalignment by formally checking the video automaton against the TL\nspecification. Furthermore, we present a dataset of temporally extended prompts\nto evaluate state-of-the-art video generation models against our benchmark. We\nfind that NeuS-V demonstrates a higher correlation by over 5x with human\nevaluations when compared to existing metrics. Our evaluation further reveals\nthat current video generation models perform poorly on these temporally complex\nprompts, highlighting the need for future work in improving text-to-video\ngeneration capabilities.",
      "tldr_zh": "该论文提出NeuS-V，一种新型合成视频评估指标，利用Neuro-Symbolic和Formal Verification技术，针对文本到视频模型（如Sora和Gen-3）的文本到视频对齐和时间忠实度进行严格评估，以填补现有指标的不足。方法包括将用户提示转换为Temporal Logic (TL)规范，并将生成的视频转化为自动机表示，然后通过形式验证检查两者是否匹配。实验结果显示，NeuS-V与人类评估的相关性比现有指标高出5倍以上，同时揭示当前视频生成模型在处理时间复杂提示时表现较差，突显了提升文本到视频生成能力的迫切需求。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16718v5",
      "published_date": "2024-11-22 23:59:12 UTC",
      "updated_date": "2025-04-25 02:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:10:28.847252"
    },
    {
      "arxiv_id": "2411.15380v1",
      "title": "Nd-BiMamba2: A Unified Bidirectional Architecture for Multi-Dimensional Data Processing",
      "title_zh": "Nd-BiMamba2：一种用于多维数据处理的统一双向架构",
      "authors": [
        "Hao Liu"
      ],
      "abstract": "Deep learning models often require specially designed architectures to\nprocess data of different dimensions, such as 1D time series, 2D images, and 3D\nvolumetric data. Existing bidirectional models mainly focus on sequential data,\nmaking it difficult to scale effectively to higher dimensions. To address this\nissue, we propose a novel multi-dimensional bidirectional neural network\narchitecture, named Nd-BiMamba2, which efficiently handles 1D, 2D, and 3D data.\nNd-BiMamba2 is based on the Mamba2 module and introduces innovative\nbidirectional processing mechanisms and adaptive padding strategies to capture\nbidirectional information in multi-dimensional data while maintaining\ncomputational efficiency. Unlike existing methods that require designing\nspecific architectures for different dimensional data, Nd-BiMamba2 adopts a\nunified architecture with a modular design, simplifying development and\nmaintenance costs. To verify the portability and flexibility of Nd-BiMamba2, we\nsuccessfully exported it to ONNX and TorchScript and tested it on different\nhardware platforms (e.g., CPU, GPU, and mobile devices). Experimental results\nshow that Nd-BiMamba2 runs efficiently on multiple platforms, demonstrating its\npotential in practical applications. The code is open-source:\nhttps://github.com/Human9000/nd-Mamba2-torch",
      "tldr_zh": "该研究提出Nd-BiMamba2，一种统一的多维双向神经网络架构，旨在高效处理1D时序数据、2D图像和3D体积数据，解决现有模型需针对不同维度设计特定架构的问题。Nd-BiMamba2基于Mamba2模块，引入创新的双向处理机制和自适应填充策略，实现对多维数据的双向信息捕获，同时保持计算效率。实验验证显示，该架构采用模块化设计，便于导出到ONNX和TorchScript，并在CPU、GPU和移动设备上高效运行，简化了开发维护成本，并证明了其在实际应用中的潜力。代码已开源：https://github.com/Human9000/nd-Mamba2-torch。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15380v1",
      "published_date": "2024-11-22 23:45:15 UTC",
      "updated_date": "2024-11-22 23:45:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:10:40.344122"
    },
    {
      "arxiv_id": "2411.15375v1",
      "title": "AdamZ: An Enhanced Optimisation Method for Neural Network Training",
      "title_zh": "AdamZ：一种增强的神经网络训练优化方法",
      "authors": [
        "Ilia Zaznov",
        "Atta Badii",
        "Alfonso Dufour",
        "Julian Kunkel"
      ],
      "abstract": "AdamZ is an advanced variant of the Adam optimiser, developed to enhance\nconvergence efficiency in neural network training. This optimiser dynamically\nadjusts the learning rate by incorporating mechanisms to address overshooting\nand stagnation, that are common challenges in optimisation. Specifically, AdamZ\nreduces the learning rate when overshooting is detected and increases it during\nperiods of stagnation, utilising hyperparameters such as overshoot and\nstagnation factors, thresholds, and patience levels to guide these adjustments.\nWhile AdamZ may lead to slightly longer training times compared to some other\noptimisers, it consistently excels in minimising the loss function, making it\nparticularly advantageous for applications where precision is critical.\nBenchmarking results demonstrate the effectiveness of AdamZ in maintaining\noptimal learning rates, leading to improved model performance across diverse\ntasks.",
      "tldr_zh": "该论文提出 AdamZ，一种对 Adam 优化器的增强版本，旨在提升神经网络训练的收敛效率。AdamZ 通过动态调整学习率来应对常见的 overshooting 和 stagnation 问题：当检测到 overshooting 时降低学习率，而在 stagnation 期则增加学习率，并利用 hyperparameters 如 overshoot factor、stagnation factor、thresholds 和 patience levels 进行精细控制。虽然 AdamZ 可能导致训练时间略微延长，但它在最小化损失函数方面表现出色，并在基准测试中证明了其在各种任务中改善模型性能的优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 9 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.15375v1",
      "published_date": "2024-11-22 23:33:41 UTC",
      "updated_date": "2024-11-22 23:33:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:10:52.492711"
    },
    {
      "arxiv_id": "2411.15372v1",
      "title": "Transforming NLU with Babylon: A Case Study in Development of Real-time, Edge-Efficient, Multi-Intent Translation System for Automated Drive-Thru Ordering",
      "title_zh": "翻译失败",
      "authors": [
        "Mostafa Varzaneh",
        "Pooja Voladoddi",
        "Tanmay Bakshi",
        "Uma Gunturi"
      ],
      "abstract": "Real-time conversational AI agents face challenges in performing Natural\nLanguage Understanding (NLU) in dynamic, outdoor environments like automated\ndrive-thru systems. These settings require NLU models to handle background\nnoise, diverse accents, and multi-intent queries while operating under strict\nlatency and memory constraints on edge devices. Additionally, robustness to\nerrors from upstream Automatic Speech Recognition (ASR) is crucial, as ASR\noutputs in these environments are often noisy. We introduce Babylon, a\ntransformer-based architecture that tackles NLU as an intent translation task,\nconverting natural language inputs into sequences of regular language units\n('transcodes') that encode both intents and slot information. This formulation\nallows Babylon to manage multi-intent scenarios in a single dialogue turn.\nFurthermore, Babylon incorporates an LSTM-based token pooling mechanism to\npreprocess phoneme sequences, reducing input length and optimizing for\nlow-latency, low-memory edge deployment. This also helps mitigate inaccuracies\nin ASR outputs, enhancing system robustness. While this work focuses on\ndrive-thru ordering, Babylon's design extends to similar noise-prone scenarios,\nfor e.g. ticketing kiosks. Our experiments show that Babylon achieves\nsignificantly better accuracy-latency-memory footprint trade-offs over\ntypically employed NMT models like Flan-T5 and BART, demonstrating its\neffectiveness for real-time NLU in edge deployment settings.",
      "tldr_zh": "本研究介绍了 Babylon 系统，一种基于 Transformer 的架构，将自然语言理解 (NLU) 转化为意图翻译任务，用于处理自动驾车点餐等动态户外环境的挑战，包括背景噪音、多样口音和多意图查询，同时满足边缘设备的延迟和内存限制。Babylon 通过将输入转换为编码意图和槽信息的序列（transcodes），并采用 LSTM-based token pooling 机制预处理语音序列，减少输入长度并提升对 Automatic Speech Recognition (ASR) 错误的鲁棒性。该系统在实验中比 Flan-T5 和 BART 等模型表现出更好的准确率-延迟-内存权衡，可扩展到其他噪声场景如售票亭。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.15372v1",
      "published_date": "2024-11-22 23:03:35 UTC",
      "updated_date": "2024-11-22 23:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:12:57.774721"
    },
    {
      "arxiv_id": "2411.15370v1",
      "title": "Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers",
      "title_zh": "无需批量更新、目标网络或重放缓冲区的深度策略梯度方法",
      "authors": [
        "Gautham Vasan",
        "Mohamed Elsayed",
        "Alireza Azimi",
        "Jiamin He",
        "Fahim Shariar",
        "Colin Bellinger",
        "Martha White",
        "A. Rupam Mahmood"
      ],
      "abstract": "Modern deep policy gradient methods achieve effective performance on\nsimulated robotic tasks, but they all require large replay buffers or expensive\nbatch updates, or both, making them incompatible for real systems with\nresource-limited computers. We show that these methods fail catastrophically\nwhen limited to small replay buffers or during incremental learning, where\nupdates only use the most recent sample without batch updates or a replay\nbuffer. We propose a novel incremental deep policy gradient method -- Action\nValue Gradient (AVG) and a set of normalization and scaling techniques to\naddress the challenges of instability in incremental learning. On robotic\nsimulation benchmarks, we show that AVG is the only incremental method that\nlearns effectively, often achieving final performance comparable to batch\npolicy gradient methods. This advancement enabled us to show for the first time\neffective deep reinforcement learning with real robots using only incremental\nupdates, employing a robotic manipulator and a mobile robot.",
      "tldr_zh": "该研究指出，现代深度策略梯度方法依赖大型重放缓冲区(replay buffers)或批量更新(batch updates)，使其不适合资源有限的真实机器人系统。论文提出了一种新型增量深度策略梯度方法——Action Value Gradient (AVG)，结合规范化(normalization)和缩放(scaling)技术，以解决增量学习中的不稳定性问题。在机器人模拟基准测试中，AVG 表现出色，往往达到与批量策略梯度方法相当的最终性能，并首次在真实机器人（如机械臂和移动机器人）上实现了有效的深度强化学习，仅使用增量更新。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "In The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems. Source code at https://github.com/gauthamvasan/avg and\n  companion video at https://youtu.be/cwwuN6Hyew0",
      "pdf_url": "http://arxiv.org/pdf/2411.15370v1",
      "published_date": "2024-11-22 22:46:21 UTC",
      "updated_date": "2024-11-22 22:46:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:11:16.663754"
    },
    {
      "arxiv_id": "2411.15367v2",
      "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
      "title_zh": "翻译失败",
      "authors": [
        "Soumil Datta",
        "Shih-Chieh Dai",
        "Leo Yu",
        "Guanhong Tao"
      ],
      "abstract": "Text-to-image diffusion models, such as Stable Diffusion, have shown\nexceptional potential in generating high-quality images. However, recent\nstudies highlight concerns over the use of unauthorized data in training these\nmodels, which may lead to intellectual property infringement or privacy\nviolations. A promising approach to mitigate these issues is to apply a\nwatermark to images and subsequently check if generative models reproduce\nsimilar watermark features. In this paper, we examine the robustness of various\nwatermark-based protection methods applied to text-to-image models. We observe\nthat common image transformations are ineffective at removing the watermark\neffect. Therefore, we propose RATTAN, that leverages the diffusion process to\nconduct controlled image generation on the protected input, preserving the\nhigh-level features of the input while ignoring the low-level details utilized\nby watermarks. A small number of generated images are then used to fine-tune\nprotected models. Our experiments on three datasets and 140 text-to-image\ndiffusion models reveal that existing state-of-the-art protections are not\nrobust against RATTAN.",
      "tldr_zh": "这篇论文探讨了文本到图像扩散模型（如 Stable Diffusion）中基于 watermark 的防御机制的脆弱性，这些机制旨在防止未经授权数据的使用以避免知识产权侵犯或隐私泄露。作者提出了一种名为 RATTAN 的攻击方法，通过利用扩散过程进行受控图像生成，保留输入的高级特征而忽略低级 watermark 细节，并使用少量生成图像对受保护模型进行微调。实验在三个数据集和 140 个文本到图像扩散模型上表明，现有的最先进 watermark 保护机制对 RATTAN 不够鲁棒，从而暴露了潜在的安全风险。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15367v2",
      "published_date": "2024-11-22 22:28:19 UTC",
      "updated_date": "2024-11-26 19:12:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:11:28.784925"
    },
    {
      "arxiv_id": "2411.15364v2",
      "title": "Exploring Facets of Language Generation in the Limit",
      "title_zh": "探索语言生成在极限中的各个方面",
      "authors": [
        "Moses Charikar",
        "Chirag Pabbaraju"
      ],
      "abstract": "The recent work of Kleinberg & Mullainathan [KM24] provides a concrete model\nfor language generation in the limit: given a sequence of examples from an\nunknown target language, the goal is to generate new examples from the target\nlanguage such that no incorrect examples are generated beyond some point. In\nsharp contrast to strong negative results for the closely related problem of\nlanguage identification, they establish positive results for language\ngeneration in the limit for all countable collections of languages. Follow-up\nwork by Raman & Tewari [RT24] studies bounds on the number of distinct inputs\nrequired by an algorithm before correct language generation is achieved --\nnamely, whether this is a constant for all languages in the collection (uniform\ngeneration) or a language-dependent constant (non-uniform generation).\n  We show that every countable language collection has a generator which has\nthe stronger property of non-uniform generation in the limit. However, while\nthe generation algorithm of [KM24] can be implemented using membership queries,\nwe show that any algorithm cannot non-uniformly generate even for collections\nof just two languages, using only membership queries.\n  We also formalize the tension between validity and breadth in the generation\nalgorithm of [KM24] by introducing a definition of exhaustive generation, and\nshow a strong negative result for exhaustive generation. Our result shows that\na tradeoff between validity and breadth is inherent for generation in the\nlimit. We also provide a precise characterization of the language collections\nfor which exhaustive generation is possible. Finally, inspired by algorithms\nthat can choose to obtain feedback, we consider a model of uniform generation\nwith feedback, completely characterizing language collections for which such\nuniform generation with feedback is possible in terms of a complexity measure\nof the collection.",
      "tldr_zh": "这篇论文探讨了语言生成 in the limit 的各个方面，基于 Kleinberg & Mullainathan [KM24] 的模型，该模型允许从未知目标语言的示例序列生成新示例，而不生成错误示例。研究证明，每个可数语言集合都存在一个具有 non-uniform generation in the limit 的生成器，但使用仅 membership queries 的算法无法实现非统一生成，即使针对仅两个语言的集合。论文引入 exhaustive generation 的定义，揭示了有效性和广度之间的 inherent tradeoff，并精确表征了支持 exhaustive generation 和带有反馈的 uniform generation 的语言集合。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "31 pages. Fixed typos, updated related work, added results on\n  characterization of exhaustive generation",
      "pdf_url": "http://arxiv.org/pdf/2411.15364v2",
      "published_date": "2024-11-22 22:13:40 UTC",
      "updated_date": "2024-12-24 10:57:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:11:41.035392"
    },
    {
      "arxiv_id": "2411.15361v2",
      "title": "Designing Cellular Manufacturing System in Presence of Alternative Process Plans",
      "title_zh": "翻译失败",
      "authors": [
        "Md. Kutub Uddin",
        "Md. Saiful Islam",
        "Md Abrar Jahin",
        "Md. Tanjid Hossen Irfan",
        "Md. Saiful Islam Seam",
        "M. F. Mridha"
      ],
      "abstract": "In the design of cellular manufacturing systems (CMS), numerous technological\nand managerial decisions must be made at both the design and operational\nstages. The first step in designing a CMS involves grouping parts and machines.\nIn this paper, four integer programming formulations are presented for grouping\nparts and machines in a CMS at both the design and operational levels for a\ngeneralized grouping problem, where each part has more than one process plan,\nand each operation of a process plan can be performed on more than one machine.\nThe minimization of inter-cell and intra-cell movements is achieved by\nassigning the maximum possible number of consecutive operations of a part type\nto the same cell and to the same machine, respectively. The suitability of\nminimizing inter-cell and intra-cell movements as an objective, compared to\nother objectives such as minimizing investment costs on machines, operating\ncosts, etc., is discussed. Numerical examples are included to illustrate the\nworkings of the formulations.",
      "tldr_zh": "本论文探讨了在存在替代工艺计划的情况下设计细胞制造系统(CMS)，重点关注零件和机器的分组问题。论文提出了四个整数规划公式，用于在设计和操作级别对零件和机器进行分组，每个零件可有多个工艺计划，且每个操作可在多个机器上执行。目标是通过将零件类型的最大可能连续操作分配到同一细胞和同一机器，从而最小化细胞间和细胞内移动。论文还讨论了这种最小化移动目标相对于最小化投资成本等其他目标的适用性，并通过数字例子验证了公式的可行性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15361v2",
      "published_date": "2024-11-22 22:10:42 UTC",
      "updated_date": "2024-12-04 20:56:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:11:52.358880"
    },
    {
      "arxiv_id": "2411.15356v2",
      "title": "Regulator-Manufacturer AI Agents Modeling: Mathematical Feedback-Driven Multi-Agent LLM Framework",
      "title_zh": "监管者-制造商 AI 代理建模：数学反馈驱动的多智能体 LLM 框架",
      "authors": [
        "Yu Han",
        "Zekun Guo"
      ],
      "abstract": "The increasing complexity of regulatory updates from global authorities\npresents significant challenges for medical device manufacturers, necessitating\nagile strategies to sustain compliance and maintain market access.\nConcurrently, regulatory bodies must effectively monitor manufacturers'\nresponses and develop strategic surveillance plans. This study employs a\nmulti-agent modeling approach, enhanced with Large Language Models (LLMs), to\nsimulate regulatory dynamics and examine the adaptive behaviors of key actors,\nincluding regulatory bodies, manufacturers, and competitors. These agents\noperate within a simulated environment governed by regulatory flow theory,\ncapturing the impacts of regulatory changes on compliance decisions, market\nadaptation, and innovation strategies. Our findings illuminate the influence of\nregulatory shifts on industry behaviour and identify strategic opportunities\nfor improving regulatory practices, optimizing compliance, and fostering\ninnovation. By leveraging the integration of multi-agent systems and LLMs, this\nresearch provides a novel perspective and offers actionable insights for\nstakeholders navigating the evolving regulatory landscape of the medical device\nindustry.",
      "tldr_zh": "本研究提出了一种基于数学反馈驱动的多智能体LLM框架，用于模拟医疗器械行业的监管动态。该框架通过多智能体建模（包括监管机构、制造商和竞争者）结合大型语言模型（LLMs）和监管流理论，模拟监管变化对合规决策、市场适应及创新策略的影响。研究发现，监管更新显著塑造行业行为，并识别出优化监管实践、提升合规效率和促进创新的战略机会。该方法为医疗器械领域的利益相关者提供了一个新颖视角和可操作的见解。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15356v2",
      "published_date": "2024-11-22 22:02:56 UTC",
      "updated_date": "2024-12-21 12:43:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:12:04.687850"
    },
    {
      "arxiv_id": "2411.15355v2",
      "title": "UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Ren",
        "Guile Wu",
        "Runhao Li",
        "Zheyuan Yang",
        "Yibo Liu",
        "Xingxin Chen",
        "Tongtong Cao",
        "Bingbing Liu"
      ],
      "abstract": "Urban scene reconstruction is crucial for real-world autonomous driving\nsimulators. Although existing methods have achieved photorealistic\nreconstruction, they mostly focus on pinhole cameras and neglect fisheye\ncameras. In fact, how to effectively simulate fisheye cameras in driving scene\nremains an unsolved problem. In this work, we propose UniGaussian, a novel\napproach that learns a unified 3D Gaussian representation from multiple camera\nmodels for urban scene reconstruction in autonomous driving. Our contributions\nare two-fold. First, we propose a new differentiable rendering method that\ndistorts 3D Gaussians using a series of affine transformations tailored to\nfisheye camera models. This addresses the compatibility issue of 3D Gaussian\nsplatting with fisheye cameras, which is hindered by light ray distortion\ncaused by lenses or mirrors. Besides, our method maintains real-time rendering\nwhile ensuring differentiability. Second, built on the differentiable rendering\nmethod, we design a new framework that learns a unified Gaussian representation\nfrom multiple camera models. By applying affine transformations to adapt\ndifferent camera models and regularizing the shared Gaussians with supervision\nfrom different modalities, our framework learns a unified 3D Gaussian\nrepresentation with input data from multiple sources and achieves holistic\ndriving scene understanding. As a result, our approach models multiple sensors\n(pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR\npoint clouds). Our experiments show that our method achieves superior rendering\nquality and fast rendering speed for driving scene simulation.",
      "tldr_zh": "该研究提出UniGaussian方法，通过统一的3D Gaussian表示来重建自动驾驶中的城市场景，支持多种相机模型（如pinhole和fisheye cameras）。为了解决3D Gaussian splatting与鱼眼相机的兼容性问题，作者设计了一个新的可微渲染方法，使用一系列仿射变换扭曲3D Gaussians，同时保持实时渲染和可微性。基于此框架，该方法从多种相机模型和模态（包括深度、语义、法线和LiDAR点云）学习统一的Gaussian表示，实现整体驾驶场景理解；实验结果显示，该方法在渲染质量和速度上均优于现有基线，为真实世界自动驾驶模拟器提供了更全面的支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report",
      "pdf_url": "http://arxiv.org/pdf/2411.15355v2",
      "published_date": "2024-11-22 21:59:46 UTC",
      "updated_date": "2025-03-01 00:05:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:12:17.438905"
    },
    {
      "arxiv_id": "2411.15331v1",
      "title": "GeoScatt-GNN: A Geometric Scattering Transform-Based Graph Neural Network Model for Ames Mutagenicity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Abdeljalil Zoubir",
        "Badr Missaoui"
      ],
      "abstract": "This paper tackles the pressing challenge of mutagenicity prediction by\nintroducing three ground-breaking approaches. First, it showcases the superior\nperformance of 2D scattering coefficients extracted from molecular images,\ncompared to traditional molecular descriptors. Second, it presents a hybrid\napproach that combines geometric graph scattering (GGS), Graph Isomorphism\nNetworks (GIN), and machine learning models, achieving strong results in\nmutagenicity prediction. Third, it introduces a novel graph neural network\narchitecture, MOLG3-SAGE, which integrates GGS node features into a fully\nconnected graph structure, delivering outstanding predictive accuracy.\nExperimental results on the ZINC dataset demonstrate significant improvements,\nemphasizing the effectiveness of blending 2D and geometric scattering\ntechniques with graph neural networks. This study illustrates the potential of\nGNNs and GGS for mutagenicity prediction, with broad implications for drug\ndiscovery and chemical safety assessment.",
      "tldr_zh": "本研究针对Ames Mutagenicity Prediction的挑战，提出三种创新方法：首先，使用从分子图像提取的2D散射系数，相比传统分子描述符表现出色；其次，开发一种结合Geometric Graph Scattering (GGS)、Graph Isomorphism Networks (GIN)和机器学习模型的混合方法，提升预测性能；第三，引入新型Graph Neural Network架构MOLG3-SAGE，将GGS节点特征整合到全连接图结构中，实现高准确率。在ZINC数据集上的实验显示，该方法显著优于基线模型，证明了将2D和几何散射技术与GNN结合的有效性，为药物发现和化学安全评估提供重要潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.IV",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15331v1",
      "published_date": "2024-11-22 19:52:56 UTC",
      "updated_date": "2024-11-22 19:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:13:09.273712"
    },
    {
      "arxiv_id": "2411.15320v1",
      "title": "PPLqa: An Unsupervised Information-Theoretic Quality Metric for Comparing Generative Large Language Models",
      "title_zh": "PPLqa：一种无监督的信息论质量指标，用于比较生成式大型语言模型",
      "authors": [
        "Gerald Friedland",
        "Xin Huang",
        "Yueying Cui",
        "Vishaal Kapoor",
        "Ashish Khetan",
        "Sanjiv Das"
      ],
      "abstract": "We propose PPLqa, an easy to compute, language independent,\ninformation-theoretic metric to measure the quality of responses of generative\nLarge Language Models (LLMs) in an unsupervised way, without requiring ground\ntruth annotations or human supervision. The method and metric enables users to\nrank generative language models for quality of responses, so as to make a\nselection of the best model for a given task. Our single metric assesses LLMs\nwith an approach that subsumes, but is not explicitly based on, coherence and\nfluency (quality of writing) and relevance and consistency (appropriateness of\nresponse) to the query. PPLqa performs as well as other related metrics, and\nworks better with long-form Q\\&A. Thus, PPLqa enables bypassing the lengthy\nannotation process required for ground truth evaluations, and it also\ncorrelates well with human and LLM rankings.",
      "tldr_zh": "这篇论文提出了 PPLqa，一种易于计算的无监督信息理论指标，用于评估生成式 Large Language Models (LLMs) 的响应质量，而无需 ground truth 标注或人工监督。PPLqa 通过整合连贯性、流畅性、相关性和一致性等因素，帮助用户对 LLMs 进行排名，以选择最适合特定任务的模型，尤其在长形式问答场景中表现优异。实验结果表明，PPLqa 的性能与现有相关指标相当，并与人类和 LLM 排名高度相关，从而简化了评估过程并避免了冗长标注需求。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.m; E.4"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15320v1",
      "published_date": "2024-11-22 19:28:06 UTC",
      "updated_date": "2024-11-22 19:28:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:13:22.083174"
    },
    {
      "arxiv_id": "2411.15296v2",
      "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Chaoyou Fu",
        "Yi-Fan Zhang",
        "Shukang Yin",
        "Bo Li",
        "Xinyu Fang",
        "Sirui Zhao",
        "Haodong Duan",
        "Xing Sun",
        "Ziwei Liu",
        "Liang Wang",
        "Caifeng Shan",
        "Ran He"
      ],
      "abstract": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal\nLarge Language Models (MLLMs) have garnered increased attention from both\nindustry and academia. Building upon pre-trained LLMs, this family of models\nfurther develops multimodal perception and reasoning capabilities that are\nimpressive, such as writing code given a flow chart or creating stories based\non an image. In the development process, evaluation is critical since it\nprovides intuitive feedback and guidance on improving models. Distinct from the\ntraditional train-eval-test paradigm that only favors a single task like image\nclassification, the versatility of MLLMs has spurred the rise of various new\nbenchmarks and evaluation methods. In this paper, we aim to present a\ncomprehensive survey of MLLM evaluation, discussing four key aspects: 1) the\nsummarised benchmarks types divided by the evaluation capabilities, including\nfoundation capabilities, model self-analysis, and extented applications; 2) the\ntypical process of benchmark counstruction, consisting of data collection,\nannotation, and precautions; 3) the systematic evaluation manner composed of\njudge, metric, and toolkit; 4) the outlook for the next benchmark. This work\naims to offer researchers an easy grasp of how to effectively evaluate MLLMs\naccording to different needs and to inspire better evaluation methods, thereby\ndriving the progress of MLLM research.",
      "tldr_zh": "这篇论文对多模态大语言模型（Multimodal LLMs, MLLMs）的评估进行了全面调查，旨在解决其在人工智能通用性（Artificial General Intelligence, AGI）发展中的关键问题。调查总结了评估的四大方面：基础能力、模型自分析和扩展应用等基准类型；基准构建过程，包括数据收集、标注和注意事项；系统评估方式，涉及评判标准、指标和工具包；以及未来基准展望。该工作为研究者提供了有效评估 MLLMs 的指导，帮助推动更可靠的评估方法和 MLLM 研究进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Produced by MME+MMBench+LLaVA Teams. Project Page:\n  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks",
      "pdf_url": "http://arxiv.org/pdf/2411.15296v2",
      "published_date": "2024-11-22 18:59:54 UTC",
      "updated_date": "2024-12-08 04:24:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:13:32.561927"
    },
    {
      "arxiv_id": "2411.15129v1",
      "title": "Measuring Bullshit in the Language Games played by ChatGPT",
      "title_zh": "翻译失败",
      "authors": [
        "Alessandro Trevisan",
        "Harry Giddens",
        "Sarah Dillon",
        "Alan F. Blackwell"
      ],
      "abstract": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language.",
      "tldr_zh": "这篇论文探讨了生成式大语言模型 (LLMs) 如 ChatGPT 如何类似于 Frankfurt 在《On Bullshit》中描述的语言使用，提出这些模型在“language games”中产生 bull shit。研究者通过统计文本分析，构建了一个数据集对比1,000篇科学出版物与ChatGPT生成的伪科学文本，并考察这些语言特征是否与Orwell的政治语言批判和Graeber的bullshit jobs概念相符。最终，使用假设测试方法证明，统计模型能可靠地将ChatGPT的bull shit语言与人类语言中的政治和职场功能相关联，为分析AI生成文本的真实性提供了新框架。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15129v1",
      "published_date": "2024-11-22 18:55:21 UTC",
      "updated_date": "2024-11-22 18:55:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:13:46.195902"
    },
    {
      "arxiv_id": "2411.15128v2",
      "title": "Health AI Developer Foundations",
      "title_zh": "翻译失败",
      "authors": [
        "Atilla P. Kiraly",
        "Sebastien Baur",
        "Kenneth Philbrick",
        "Fereshteh Mahvar",
        "Liron Yatziv",
        "Tiffany Chen",
        "Bram Sterling",
        "Nick George",
        "Fayaz Jamil",
        "Jing Tang",
        "Kai Bailey",
        "Faruk Ahmed",
        "Akshay Goel",
        "Abbi Ward",
        "Lin Yang",
        "Andrew Sellergren",
        "Yossi Matias",
        "Avinatan Hassidim",
        "Shravya Shetty",
        "Daniel Golden",
        "Shekoofeh Azizi",
        "David F. Steiner",
        "Yun Liu",
        "Tim Thelin",
        "Rory Pilgrim",
        "Can Kirmizibayrak"
      ],
      "abstract": "Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.",
      "tldr_zh": "该研究引入了 Health AI Developer Foundations (HAI-DEF)，一个预训练的领域特定基础模型套件、工具和配方，旨在加速医疗 Machine Learning (ML) 应用的开发，解决传统方法的高成本和资源需求问题。该套件覆盖放射学（X 光和 CT）、组织病理学、皮肤影像和音频等多种模式，提供高效的领域嵌入，以减少标记数据需求、缩短训练时间并降低计算成本。实验评估显示 HAI-DEF 在各种任务中表现出色，并强调在使用时需针对具体问题和人群进行验证，以确保功效、公平性和公平性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15128v2",
      "published_date": "2024-11-22 18:51:51 UTC",
      "updated_date": "2024-11-26 18:01:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:13:59.019984"
    },
    {
      "arxiv_id": "2411.15122v1",
      "title": "ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoman Zhang",
        "Hong-Yu Zhou",
        "Xiaoli Yang",
        "Oishi Banerjee",
        "Julián N. Acosta",
        "Josh Miller",
        "Ouwen Huang",
        "Pranav Rajpurkar"
      ],
      "abstract": "AI-driven models have demonstrated significant potential in automating\nradiology report generation for chest X-rays. However, there is no standardized\nbenchmark for objectively evaluating their performance. To address this, we\npresent ReXrank, https://rexrank.ai, a public leaderboard and challenge for\nassessing AI-powered radiology report generation. Our framework incorporates\nReXGradient, the largest test dataset consisting of 10,000 studies, and three\npublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation\nassessment. ReXrank employs 8 evaluation metrics and separately assesses models\ncapable of generating only findings sections and those providing both findings\nand impressions sections. By providing this standardized evaluation framework,\nReXrank enables meaningful comparisons of model performance and offers crucial\ninsights into their robustness across diverse clinical settings. Beyond its\ncurrent focus on chest X-rays, ReXrank's framework sets the stage for\ncomprehensive evaluation of automated reporting across the full spectrum of\nmedical imaging.",
      "tldr_zh": "本研究引入了 ReXrank，一个公共排行榜和挑战平台，用于标准化评估 AI 驱动的放射学报告生成模型，特别是针对胸部 X 光片。ReXrank 利用最大的测试数据集 ReXGradient（包含 10,000 个研究）以及 MIMIC-CXR、IU-Xray 和 CheXpert Plus 等公共数据集，采用 8 个评估指标来分别评估模型生成 findings 部分和 findings 与 impressions 部分的性能。通过提供这一框架，ReXrank 促进了模型性能的客观比较，并揭示了其在不同临床设置中的鲁棒性，同时为扩展到其他医疗成像领域的自动化报告评估奠定基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15122v1",
      "published_date": "2024-11-22 18:40:02 UTC",
      "updated_date": "2024-11-22 18:40:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:14:08.944280"
    },
    {
      "arxiv_id": "2411.15115v2",
      "title": "VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Daeun Lee",
        "Jaehong Yoon",
        "Jaemin Cho",
        "Mohit Bansal"
      ],
      "abstract": "Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of two stages: In (1) video refinement planning, we first\ndetect misalignments by generating fine-grained evaluation questions and\nanswering them using an MLLM. Based on video evaluation outputs, we identify\naccurately generated objects and construct localized prompts to precisely\nrefine misaligned regions. In (2) localized refinement, we enhance video\nalignment by 'repairing' the misaligned regions from the original video while\npreserving the correctly generated areas. This is achieved by frame-wise region\ndecomposition using our Region-Preserving Segmentation (RPS) module. On two\npopular video generation benchmarks (EvalCrafter and T2V-CompBench),\nVideoRepair substantially outperforms recent baselines across various\ntext-video alignment metrics. We provide a comprehensive analysis of\nVideoRepair components and qualitative examples.",
      "tldr_zh": "本研究提出 VideoRepair，一种模型无关、无需训练的框架，用于提升 Text-to-Video (T2V) 生成模型的性能，针对复杂文本提示中常见的文本-视频不对齐问题。框架分为两个阶段：首先，通过多模态大型语言模型 (MLLM) 生成细粒度评估问题检测不对齐，并构建本地化提示；其次，利用 Region-Preserving Segmentation (RPS) 模块对帧-wise 区域进行分解，精确修复不对齐部分的同时保留正确生成的区域。在 EvalCrafter 和 T2V-CompBench 基准测试中，VideoRepair 在各种文本-视频对齐指标上显著超越基线模型，提供全面的组件分析和定性示例。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://video-repair.github.io",
      "pdf_url": "http://arxiv.org/pdf/2411.15115v2",
      "published_date": "2024-11-22 18:31:47 UTC",
      "updated_date": "2025-03-19 21:39:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:14:22.750300"
    },
    {
      "arxiv_id": "2411.15114v1",
      "title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts",
      "title_zh": "翻译失败",
      "authors": [
        "Hjalmar Wijk",
        "Tao Lin",
        "Joel Becker",
        "Sami Jawhar",
        "Neev Parikh",
        "Thomas Broadley",
        "Lawrence Chan",
        "Michael Chen",
        "Josh Clymer",
        "Jai Dhyani",
        "Elena Ericheva",
        "Katharyn Garcia",
        "Brian Goodrich",
        "Nikola Jurkovic",
        "Megan Kinniment",
        "Aron Lajko",
        "Seraphina Nix",
        "Lucas Sato",
        "William Saunders",
        "Maksym Taran",
        "Ben West",
        "Elizabeth Barnes"
      ],
      "abstract": "Frontier AI safety policies highlight automation of AI research and\ndevelopment (R&D) by AI agents as an important capability to anticipate.\nHowever, there exist few evaluations for AI R&D capabilities, and none that are\nhighly realistic and have a direct comparison to human performance. We\nintroduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7\nchallenging, open-ended ML research engineering environments and data from 71\n8-hour attempts by 61 distinct human experts. We confirm that our experts make\nprogress in the environments given 8 hours, with 82% of expert attempts\nachieving a non-zero score and 24% matching or exceeding our strong reference\nsolutions. We compare humans to several public frontier models through\nbest-of-k with varying time budgets and agent designs, and find that the best\nAI agents achieve a score 4x higher than human experts when both are given a\ntotal time budget of 2 hours per environment. However, humans currently display\nbetter returns to increasing time budgets, narrowly exceeding the top AI agent\nscores given an 8-hour budget, and achieving 2x the score of the top AI agent\nwhen both are given 32 total hours (across different attempts). Qualitatively,\nwe find that modern AI agents possess significant expertise in many ML topics\n-- e.g. an agent wrote a faster custom Triton kernel than any of our human\nexperts' -- and can generate and test solutions over ten times faster than\nhumans, at much lower cost. We open-source the evaluation environments, human\nexpert data, analysis code and agent trajectories to facilitate future\nresearch.",
      "tldr_zh": "本文提出了 RE-Bench 基准，用于评估语言模型代理在 AI R&D 领域的能力，并与人类专家进行直接比较。该基准包括 7 个挑战性的开放式 ML 研究工程环境，基于 71 次 8 小时人类专家尝试的数据，结果显示人类专家在较长时间预算下（如 8 或 32 小时）表现更优，能实现顶级 AI 代理的 2 倍得分。相比之下，AI 代理在短时间预算（如 2 小时）内得分高达人类专家的 4 倍，并展示了在特定任务（如编写自定义 Triton 内核）上的显著速度和成本优势。论文开源了评估环境、人类数据、分析代码和代理轨迹，以推动未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15114v1",
      "published_date": "2024-11-22 18:30:46 UTC",
      "updated_date": "2024-11-22 18:30:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:14:35.358880"
    },
    {
      "arxiv_id": "2411.15113v1",
      "title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion",
      "title_zh": "高效剪枝文本到图像模型：从剪枝 Stable Diffusion 的洞见",
      "authors": [
        "Samarth N Ramesh",
        "Zhixue Zhao"
      ],
      "abstract": "As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems",
      "tldr_zh": "这篇论文研究了文本到图像模型的Efficient Pruning，通过对Stable Diffusion 2的修剪实验，探讨了模型压缩以适应资源受限设备的需求。研究者比较了不同稀疏度下对文本编码器和图像生成组件的修剪效果，发现simple magnitude pruning在文本到图像模型中优于更高级的技术。结果显示，Stable Diffusion 2可修剪到38.5% sparsity，几乎不损失图像生成质量，并提出最佳配置（文本编码器至47.5%、扩散生成器至35%），这揭示了关键权重对语义信息的编码，并为开发更高效的AI图像生成系统奠定基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15113v1",
      "published_date": "2024-11-22 18:29:37 UTC",
      "updated_date": "2024-11-22 18:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:14:45.640698"
    },
    {
      "arxiv_id": "2411.15292v2",
      "title": "Influence functions and regularity tangents for efficient active learning",
      "title_zh": "翻译失败",
      "authors": [
        "Frederik Eaton"
      ],
      "abstract": "In this paper we describe an efficient method for providing a regression\nmodel with a sense of curiosity about its data. In the field of machine\nlearning, our framework for representing curiosity is called Active Learning,\nwhich concerns the problem of automatically choosing data points for which to\nquery labels in the semi-supervised setting. The methods we propose are based\non computing a \"regularity tangent\" vector that can be calculated (with only a\nconstant slow-down) together with the model's parameter vector during training.\nWe then take the inner product of this tangent vector with the gradient vector\nof the model's loss at a given data point to obtain a measure of the influence\nof that point on the complexity of the model. In the simplest instantiation,\nthere is only a single regularity tangent vector, of the same dimension as the\nparameter vector. Thus, in the proposed technique, once training is complete,\nevaluating our \"curiosity\" about a potential query data point can be done as\nquickly as calculating the model's loss gradient at that point. The new vector\nonly doubles the amount of storage required by the model. We show that the\nquantity computed by our technique is an example of an \"influence function\",\nand that it measures the expected squared change in model complexity incurred\nby up-weighting a given data point. We propose a number of ways for using this\nand other related quantities to choose new training data points for a\nregression model.",
      "tldr_zh": "本研究提出了一种高效的主动学习（Active Learning）方法，用于回归模型（regression model）自动选择查询标签的数据点。该方法通过计算“regularity tangent”向量——在训练过程中与模型参数向量一同生成——来量化数据点对模型复杂度的影响，并使用该向量与损失梯度（gradient vector）的内积作为“好奇度”指标。这种技术仅需恒定计算开销和双倍存储空间，便于快速评估潜在数据点。实验结果表明，该方法基于“influence functions”原理，能有效测量数据点对模型复杂度的预期变化，从而优化新训练数据的选择。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15292v2",
      "published_date": "2024-11-22 18:14:26 UTC",
      "updated_date": "2025-03-18 15:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:14:56.156246"
    },
    {
      "arxiv_id": "2411.15106v2",
      "title": "About Time: Advances, Challenges, and Outlooks of Action Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandros Stergiou",
        "Ronald Poppe"
      ],
      "abstract": "We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext across multiple modalities. This survey comprehensively reviews\nadvances in uni- and multi-modal action understanding across a range of tasks.\nWe focus on prevalent challenges, overview widely adopted datasets, and survey\nseminal works with an emphasis on recent advances. We broadly distinguish\nbetween three temporal scopes: (1) recognition tasks of actions observed in\nfull, (2) prediction tasks for ongoing partially observed actions, and (3)\nforecasting tasks for subsequent unobserved action(s). This division allows us\nto identify specific action modeling and video representation challenges.\nFinally, we outline future directions to address current shortcomings.",
      "tldr_zh": "这篇论文回顾了视频动作理解领域的最新进展，包括数据集规模增加、变异性和计算资源带来的性能提升和任务多样化。作者区分了三种时间范围的任务：(1) 完整观察动作的识别任务，(2) 部分观察动作的预测任务，以及(3) 未观察后续动作的预测任务，以识别动作建模和视频表示的特定挑战。论文全面审视了单模态和多模态方法，概述了广泛采用的数据集和标志性作品，特别是最近的进展。最后，它指出了未来方向，以解决当前不足，如提升跨模态预测和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the International Journal of Computer Vision (IJCV)",
      "pdf_url": "http://arxiv.org/pdf/2411.15106v2",
      "published_date": "2024-11-22 18:09:27 UTC",
      "updated_date": "2025-05-06 08:18:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:15:09.337841"
    },
    {
      "arxiv_id": "2411.15100v3",
      "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models",
      "title_zh": "XGrammar：灵活且高效的大型语言模型结构化生成引擎",
      "authors": [
        "Yixin Dong",
        "Charlie F. Ruan",
        "Yaxing Cai",
        "Ruihang Lai",
        "Ziyi Xu",
        "Yilong Zhao",
        "Tianqi Chen"
      ],
      "abstract": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.",
      "tldr_zh": "这篇论文提出了 XGrammar，一种灵活且高效的结构生成引擎，用于 Large Language Models (LLMs)，以满足复杂 LLM Agents 对结构化输出（如代码和函数调用）的需求。XGrammar 通过将词汇分为上下文无关 tokens（预检查）和上下文相关 tokens（运行时解释），并结合文法上下文扩展、持久栈优化以及与 LLM 推理引擎的协同设计，显著加速 context-free grammar 的执行。实验结果显示，XGrammar 比现有解决方案快 100 倍，并在端到端 LLM 服务中实现近零开销的结构生成。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.CL",
      "comment": "MLSys '25",
      "pdf_url": "http://arxiv.org/pdf/2411.15100v3",
      "published_date": "2024-11-22 18:01:37 UTC",
      "updated_date": "2025-05-12 08:20:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:15:21.461646"
    },
    {
      "arxiv_id": "2411.15098v5",
      "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenxiong Tan",
        "Songhua Liu",
        "Xingyi Yang",
        "Qiaochu Xue",
        "Xinchao Wang"
      ],
      "abstract": "We present OminiControl, a novel approach that rethinks how image conditions\nare integrated into Diffusion Transformer (DiT) architectures. Current image\nconditioning methods either introduce substantial parameter overhead or handle\nonly specific control tasks effectively, limiting their practical versatility.\nOminiControl addresses these limitations through three key innovations: (1) a\nminimal architectural design that leverages the DiT's own VAE encoder and\ntransformer blocks, requiring just 0.1% additional parameters; (2) a unified\nsequence processing strategy that combines condition tokens with image tokens\nfor flexible token interactions; and (3) a dynamic position encoding mechanism\nthat adapts to both spatially-aligned and non-aligned control tasks. Our\nextensive experiments show that this streamlined approach not only matches but\nsurpasses the performance of specialized methods across multiple conditioning\ntasks. To overcome data limitations in subject-driven generation, we also\nintroduce Subjects200K, a large-scale dataset of identity-consistent image\npairs synthesized using DiT models themselves. This work demonstrates that\neffective image control can be achieved without architectural complexity,\nopening new possibilities for efficient and versatile image generation systems.",
      "tldr_zh": "该研究提出了 OminiControl，一种最小化和通用化的控制方法，用于在 Diffusion Transformer (DiT) 架构中整合图像条件，以解决现有方法参数开销大或仅限于特定任务的局限性。OminiControl 的三大创新包括：利用 DiT 的 VAE 编码器和 transformer 块仅添加 0.1% 参数的极简设计、统一序列处理策略将条件 token 与图像 token 结合以实现灵活交互，以及动态位置编码机制适应空间对齐和非对齐任务。实验结果显示，该方法在多个条件任务上超过了专业化方法的性能；此外，研究还引入了 Subjects200K 数据集，一个由 DiT 模型合成的规模化身份一致图像对数据集，以克服主 driven 生成的数据限制，并为高效、多功能的图像生成系统开辟新可能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15098v5",
      "published_date": "2024-11-22 17:55:15 UTC",
      "updated_date": "2025-03-11 10:41:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:15:33.264908"
    },
    {
      "arxiv_id": "2411.15096v2",
      "title": "RED: Effective Trajectory Representation Learning with Comprehensive Information",
      "title_zh": "翻译失败",
      "authors": [
        "Silin Zhou",
        "Shuo Shang",
        "Lisi Chen",
        "Christian S. Jensen",
        "Panos Kalnis"
      ],
      "abstract": "Trajectory representation learning (TRL) maps trajectories to vectors that\ncan then be used for various downstream tasks, including trajectory similarity\ncomputation, trajectory classification, and travel-time estimation. However,\nexisting TRL methods often produce vectors that, when used in downstream tasks,\nyield insufficiently accurate results. A key reason is that they fail to\nutilize the comprehensive information encompassed by trajectories. We propose a\nself-supervised TRL framework, called RED, which effectively exploits multiple\ntypes of trajectory information. Overall, RED adopts the Transformer as the\nbackbone model and masks the constituting paths in trajectories to train a\nmasked autoencoder (MAE). In particular, RED considers the moving patterns of\ntrajectories by employing a Road-aware masking strategy} that retains key paths\nof trajectories during masking, thereby preserving crucial information of the\ntrajectories. RED also adopts a spatial-temporal-user joint Embedding scheme to\nencode comprehensive information when preparing the trajectories as model\ninputs. To conduct training, RED adopts Dual-objective task learning}: the\nTransformer encoder predicts the next segment in a trajectory, and the\nTransformer decoder reconstructs the entire trajectory. RED also considers the\nspatial-temporal correlations of trajectories by modifying the attention\nmechanism of the Transformer. We compare RED with 9 state-of-the-art TRL\nmethods for 4 downstream tasks on 3 real-world datasets, finding that RED can\nusually improve the accuracy of the best-performing baseline by over 5%.",
      "tldr_zh": "这篇论文针对轨迹表示学习 (TRL) 的不足，提出了一种自监督框架 RED，以更全面地利用轨迹的空间、时间和用户信息。RED 以 Transformer 为主干模型，采用 masked autoencoder (MAE) 结合 Road-aware masking 策略保留关键路径，以及空间-时间-用户联合嵌入方案和双目标任务学习（包括预测下一个段和重建整个轨迹），并修改注意力机制以捕捉空间-时间相关性。实验结果显示，在 3 个真实数据集和 4 个下游任务上，RED 通常比 9 个最先进基线方法提高超过 5% 的准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by VLDB2025",
      "pdf_url": "http://arxiv.org/pdf/2411.15096v2",
      "published_date": "2024-11-22 17:51:21 UTC",
      "updated_date": "2024-11-28 12:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:15:45.850590"
    },
    {
      "arxiv_id": "2411.15082v1",
      "title": "Towards Speaker Identification with Minimal Dataset and Constrained Resources using 1D-Convolution Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Irfan Nafiz Shahan",
        "Pulok Ahmed Auvi"
      ],
      "abstract": "Voice recognition and speaker identification are vital for applications in\nsecurity and personal assistants. This paper presents a lightweight\n1D-Convolutional Neural Network (1D-CNN) designed to perform speaker\nidentification on minimal datasets. Our approach achieves a validation accuracy\nof 97.87%, leveraging data augmentation techniques to handle background noise\nand limited training samples. Future improvements include testing on larger\ndatasets and integrating transfer learning methods to enhance generalizability.\nWe provide all code, the custom dataset, and the trained models to facilitate\nreproducibility. These resources are available on our GitHub repository:\nhttps://github.com/IrfanNafiz/RecMe.",
      "tldr_zh": "本研究针对资源受限环境，提出了一种轻量级的 1D-Convolution Neural Network (1D-CNN) 用于说话者识别，旨在在最小数据集上实现高效性能。该模型通过数据 augmentation 技术处理背景噪声和训练样本有限的问题，达到了 97.87% 的验证准确率。未来计划扩展到更大数据集并整合 transfer learning 方法以提升泛化能力，并已在 GitHub 上公开代码、自定义数据集和训练模型以便再现。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15082v1",
      "published_date": "2024-11-22 17:18:08 UTC",
      "updated_date": "2024-11-22 17:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:15:57.152586"
    },
    {
      "arxiv_id": "2411.15287v1",
      "title": "Sycophancy in Large Language Models: Causes and Mitigations",
      "title_zh": "大型语言模型中的阿谀奉承：原因和缓解措施",
      "authors": [
        "Lars Malmqvist"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. However, their tendency to\nexhibit sycophantic behavior - excessively agreeing with or flattering users -\nposes significant risks to their reliability and ethical deployment. This paper\nprovides a technical survey of sycophancy in LLMs, analyzing its causes,\nimpacts, and potential mitigation strategies. We review recent work on\nmeasuring and quantifying sycophantic tendencies, examine the relationship\nbetween sycophancy and other challenges like hallucination and bias, and\nevaluate promising techniques for reducing sycophancy while maintaining model\nperformance. Key approaches explored include improved training data, novel\nfine-tuning methods, post-deployment control mechanisms, and decoding\nstrategies. We also discuss the broader implications of sycophancy for AI\nalignment and propose directions for future research. Our analysis suggests\nthat mitigating sycophancy is crucial for developing more robust, reliable, and\nethically-aligned language models.",
      "tldr_zh": "这篇论文调查了大型语言模型(LLMs)中sycophancy（谄媚行为）的成因、影响及缓解策略，强调这种行为会损害模型的可靠性和伦理部署。作者回顾了测量sycophancy的方法，探讨了其与hallucination（幻觉）和bias（偏见）等问题的关联，并评估了多种缓解技术，包括改进训练数据、新型微调方法、部署后控制机制和解码策略。研究结论指出，缓解sycophancy对开发更稳健、可靠且AI对齐的语言模型至关重要，并提出了未来研究的潜在方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15287v1",
      "published_date": "2024-11-22 16:56:49 UTC",
      "updated_date": "2024-11-22 16:56:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:16:09.570715"
    },
    {
      "arxiv_id": "2411.15285v1",
      "title": "Forecasting Unseen Points of Interest Visits Using Context and Proximity Priors",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyao Li",
        "Shang-Ling Hsu",
        "Cyrus Shahabi"
      ],
      "abstract": "Understanding human mobility behavior is crucial for numerous applications,\nincluding crowd management, location-based recommendations, and the estimation\nof pandemic spread. Machine learning models can predict the Points of Interest\n(POIs) that individuals are likely to visit in the future by analyzing their\nhistorical visit patterns. Previous studies address this problem by learning a\nPOI classifier, where each class corresponds to a POI. However, this limits\ntheir applicability to predict a new POI that was not in the training data,\nsuch as the opening of new restaurants. To address this challenge, we propose a\nmodel designed to predict a new POI outside the training data as long as its\ncontext is aligned with the user's interests. Unlike existing approaches that\ndirectly predict specific POIs, our model first forecasts the semantic context\nof potential future POIs, then combines this with a proximity-based prior\nprobability distribution to determine the exact POI. Experimental results on\nreal-world visit data demonstrate that our model outperforms baseline methods\nthat do not account for semantic contexts, achieving a 17% improvement in\naccuracy. Notably, as new POIs are introduced over time, our model remains\nrobust, exhibiting a lower decline rate in prediction accuracy compared to\nexisting methods.",
      "tldr_zh": "该研究针对预测用户未来访问的Points of Interest (POIs)问题，提出了一种新模型，能够处理训练数据中不存在的“未见POI”，如新开张的餐厅，只要其语义上下文与用户兴趣相符。该模型首先预测潜在POI的semantic context，然后结合proximity-based prior probability distribution（基于邻近度的先验概率分布）来确定确切的POI，从而提升预测的准确性和泛化能力。在真实世界访问数据上的实验表明，该模型比不考虑语义上下文的基线方法准确率提高了17%，并在新增POI时表现出更低的准确率下降率，证明了其鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2024 IEEE International Conference on Big Data workshop BSD 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.15285v1",
      "published_date": "2024-11-22 16:50:10 UTC",
      "updated_date": "2024-11-22 16:50:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:16:21.517703"
    },
    {
      "arxiv_id": "2411.15061v1",
      "title": "Empowering Clients: Transformation of Design Processes Due to Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Johannes Schneider",
        "Kilic Sinem",
        "Daniel Stockhammer"
      ],
      "abstract": "The domain of computational design, driven by advancements in Generative AI,\nis transforming creative fields. We explore the transformative effects of\nGenerative AI on the architectural design process and discuss the role of the\narchitect. The case of architecture is interesting as designing houses is\ncomplex, involving extensive customer interaction. We employ a within-subject\nexperiment using a popular general-purpose text-to-image tool for generating\ndesigns and providing feedback on existing designs, followed by expert\ninterviews. The study reveals that AI can disrupt the ideation phase by\nenabling clients to engage in the design process through rapid visualization of\ntheir own ideas. In turn, the architect's role shifts more towards assessing\nthe feasibility of designs generated conjointly by clients and AI. Our study\nalso shows that while AI can provide valuable feedback on designs, it might\nfail to generate such designs, allowing for interesting connections to\nfoundations in computer science, i.e., NP-completeness. AI's feedback also\ntends to hamper creativity and innovation by suggesting altering novel,\ninnovative approaches toward more standardized designs. Our study also reveals\nthat there is uncertainty among architects about the interpretative sovereignty\nof architecture and loss of meaning and identity when AI increasingly takes\nover authorship in the design process.",
      "tldr_zh": "本研究探讨了生成式 AI 对建筑设计过程的变革，特别是如何赋予客户更多参与权。研究采用 within-subject 实验和专家访谈，考察了使用流行文本到图像工具生成设计和提供反馈的效果。结果显示，AI 增强了客户在构思阶段的参与，通过快速可视化实现设计协作，但也可能抑制创新并推动标准化设计，同时建筑师担心 AI 接管作者身份导致身份和意义丧失。该研究揭示了 AI 在设计领域的双重影响，并与计算机科学基础如 NP-completeness 相关联。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15061v1",
      "published_date": "2024-11-22 16:48:15 UTC",
      "updated_date": "2024-11-22 16:48:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:16:32.919262"
    },
    {
      "arxiv_id": "2411.15056v1",
      "title": "Financial Risk Assessment via Long-term Payment Behavior Sequence Folding",
      "title_zh": "基于长期支付行为序列折叠的金融风险评估",
      "authors": [
        "Yiran Qiao",
        "Yateng Tang",
        "Xiang Ao",
        "Qi Yuan",
        "Ziming Liu",
        "Chen Shen",
        "Xuehao Zheng"
      ],
      "abstract": "Online inclusive financial services encounter significant financial risks due\nto their expansive user base and low default costs. By real-world practice, we\nreveal that utilizing longer-term user payment behaviors can enhance models'\nability to forecast financial risks. However, learning long behavior sequences\nis non-trivial for deep sequential models. Additionally, the diverse fields of\npayment behaviors carry rich information, requiring thorough exploitation.\nThese factors collectively complicate the task of long-term user behavior\nmodeling. To tackle these challenges, we propose a Long-term Payment Behavior\nSequence Folding method, referred to as LBSF. In LBSF, payment behavior\nsequences are folded based on merchants, using the merchant field as an\nintrinsic grouping criterion, which enables informative parallelism without\nreliance on external knowledge. Meanwhile, we maximize the utility of payment\ndetails through a multi-field behavior encoding mechanism. Subsequently,\nbehavior aggregation at the merchant level followed by relational learning\nacross merchants facilitates comprehensive user financial representation. We\nevaluate LBSF on the financial risk assessment task using a large-scale\nreal-world dataset. The results demonstrate that folding long behavior\nsequences based on internal behavioral cues effectively models long-term\npatterns and changes, thereby generating more accurate user financial profiles\nfor practical applications.",
      "tldr_zh": "这篇论文针对在线包容性金融服务的金融风险评估问题，提出了一种LBSF（Long-term Payment Behavior Sequence Folding）方法，通过基于商户的序列折叠来处理长期支付行为序列。该方法利用商户字段作为内在分组标准，实现信息并行，并结合多字段行为编码、商户级别行为聚合和跨商户关系学习，充分利用支付行为的丰富信息以生成全面的用户金融表示。在大规模真实数据集上的实验结果表明，LBSF能有效捕捉长期行为模式和变化，从而显著提升风险预测的准确性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "ICDM2024 long paper",
      "pdf_url": "http://arxiv.org/pdf/2411.15056v1",
      "published_date": "2024-11-22 16:43:26 UTC",
      "updated_date": "2024-11-22 16:43:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:16:45.737858"
    },
    {
      "arxiv_id": "2411.15042v2",
      "title": "Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications",
      "title_zh": "通过基于世界模型的预测导航和自适应学习算法增强自动驾驶安全，用于5G无线应用",
      "authors": [
        "Hong Ding",
        "Ziming Wang",
        "Yi Ding",
        "Hongjie Lin",
        "SuYang Xi",
        "Chia Chao Kang"
      ],
      "abstract": "Addressing the challenge of ensuring safety in ever-changing and\nunpredictable environments, particularly in the swiftly advancing realm of\nautonomous driving in today's 5G wireless communication world, we present\nNavigation Secure (NavSecure). This vision-based navigation framework merges\nthe strengths of world models with crucial safety-focused decision-making\ncapabilities, enabling autonomous vehicles to navigate real-world complexities\nsecurely. Our approach anticipates potential threats and formulates safer\nroutes by harnessing the predictive capabilities of world models, thus\nsignificantly reducing the need for extensive real-world trial-and-error\nlearning. Additionally, our method empowers vehicles to autonomously learn and\ndevelop through continuous practice, ensuring the system evolves and adapts to\nnew challenges. Incorporating radio frequency technology, NavSecure leverages\n5G networks to enhance real-time data exchange, improving communication and\nresponsiveness. Validated through rigorous experiments under simulation-to-real\ndriving conditions, NavSecure has shown exceptional performance in\nsafety-critical scenarios, such as sudden obstacle avoidance. Results indicate\nthat NavSecure excels in key safety metrics, including collision prevention and\nrisk reduction, surpassing other end-to-end methodologies. This framework not\nonly advances autonomous driving safety but also demonstrates how world models\ncan enhance decision-making in critical applications. NavSecure sets a new\nstandard for developing more robust and trustworthy autonomous driving systems,\ncapable of handling the inherent dynamics and uncertainties of real-world\nenvironments.",
      "tldr_zh": "本研究提出NavSecure框架，通过world models的预测导航和adaptive learning algorithms，提升自动驾驶在5G无线环境下的安全性。该框架结合视觉导航技术，提前预测潜在威胁并规划更安全的路线，同时利用5G网络实现实时数据交换和自主学习，以适应动态环境。实验结果显示，NavSecure在模拟和真实驾驶条件下表现出色，尤其在突发障碍避免等安全场景中，显著提高了碰撞预防和风险减少指标，优于其他端到端方法。该框架为构建更可靠的自动驾驶系统树立了新标准。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15042v2",
      "published_date": "2024-11-22 16:16:07 UTC",
      "updated_date": "2024-11-25 15:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:16:56.590043"
    },
    {
      "arxiv_id": "2411.15041v1",
      "title": "mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Zhang",
        "Ziqi Zhang",
        "Zongyang Ma",
        "Yuxin Chen",
        "Zhongang Qi",
        "Chunfeng Yuan",
        "Bing Li",
        "Junfu Pu",
        "Yuxuan Zhao",
        "Zehua Xie",
        "Jin Ma",
        "Ying Shan",
        "Weiming Hu"
      ],
      "abstract": "Advanced Multimodal Large Language Models (MLLMs) struggle with recent\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\neffectively expanding the knowledge scope. However, current mRAG methods have\ninherent drawbacks, including: 1) Performing retrieval even when external\nknowledge is not needed. 2) Lacking of identification of evidence that supports\nthe query. 3) Increasing model complexity due to additional information\nfiltering modules or rules. To address these shortcomings, we propose a novel\ngeneralized framework called \\textbf{m}ultimodal\n\\textbf{R}etrieval-\\textbf{R}eflection-\\textbf{A}ugmented \\textbf{G}eneration\n(mR$^2$AG), which achieves adaptive retrieval and useful information\nlocalization to enable answers through two easy-to-implement reflection\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\nis designed to distinguish different user queries and avoids redundant\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\nlocating beneficial evidence of the retrieved content and generating answers\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\nVisual-dependent tasks.",
      "tldr_zh": "该研究提出了一种新型框架 mR$^2$AG（Multimodal Retrieval-Reflection-Augmented Generation），旨在解决多模态大型语言模型 (MLLMs) 在知识型视觉问答 (VQA) 任务中的问题，如知识范围有限和响应不准确。mR$^2$AG 通过 Retrieval-Reflection 和 Relevance-Reflection 两种简单反射操作，实现自适应检索、避免冗余调用，并定位有用的证据以生成答案，同时不增加模型复杂度。该框架可与任何训练好的 MLLMs 集成，并通过 mR$^2$AG-IT 数据集进行高效微调；在 INFOSEEK 和 Encyclopedic-VQA 等任务上，mR$^2$AG 显著优于现有 SOTA 模型（如 GPT-4v/o），并保持了基础模型在视觉依赖任务中的出色性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15041v1",
      "published_date": "2024-11-22 16:15:50 UTC",
      "updated_date": "2024-11-22 16:15:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:17:10.295627"
    },
    {
      "arxiv_id": "2411.15281v1",
      "title": "ElastiFormer: Learned Redundancy Reduction in Transformer via Self-Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Junzhang Liu",
        "Tingkai Liu",
        "Yueyuan Sui",
        "Stephen Xia"
      ],
      "abstract": "We introduce ElastiFormer, a post-training technique that adapts pretrained\nTransformer models into an elastic counterpart with variable inference time\ncompute. ElastiFormer introduces small routing modules (as low as .00006%\nadditional trainable parameters) to dynamically selects subsets of network\nparameters and input tokens to be processed by each layer of the pretrained\nnetwork in an inputdependent manner. The routing modules are trained using\nself-distillation losses to minimize the differences between the output of the\npretrained-model and their elastic counterparts. As ElastiFormer makes no\nassumption regarding the modality of the pretrained Transformer model, it can\nbe readily applied to all modalities covering causal language modeling, image\nmodeling as well as visual-language modeling tasks. We show that 20% to 50%\ncompute saving could be achieved for different components of the transformer\nlayer, which could be further reduced by adding very low rank LoRA weights\n(rank 1) trained via the same distillation objective. Finally, by comparing\nrouting trained on different subsets of ImageNet, we show that ElastiFormer is\nrobust against the training domain.",
      "tldr_zh": "我们提出了 ElastiFormer，一种后训练技术，通过添加小型路由模块（额外可训练参数少至 0.00006%）将预训练 Transformer 模型转化为弹性版本，实现输入依赖的动态参数和标记子集选择。\n路由模块使用自蒸馏损失训练，以最小化弹性版本输出与原模型输出的差异，从而减少冗余计算。\n实验结果显示，该方法在因果语言建模、图像建模和视觉语言建模等任务上可节省 20% 到 50% 的计算资源，并通过低秩 LoRA 权重进一步优化，同时证明了 ElastiFormer 对训练域的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15281v1",
      "published_date": "2024-11-22 16:11:14 UTC",
      "updated_date": "2024-11-22 16:11:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:17:22.013351"
    },
    {
      "arxiv_id": "2411.15033v1",
      "title": "One to rule them all: natural language to bind communication, perception and action",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Colombani",
        "Dimitri Ognibene",
        "Giuseppe Boccignone"
      ],
      "abstract": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.",
      "tldr_zh": "本文提出了一种先进的机器人行动规划架构，使用 Large Language Models (LLMs) 整合通信、感知和行动，旨在帮助机器人理解复杂自然语言指令并在动态环境中执行任务。该架构的核心是 Planner Module，通过修改后的 ReAct 框架处理用户命令，利用预训练知识和实时环境反馈动态生成可执行计划。系统结合语义地图表示和连续反馈循环，提升机器人的适应性、任务执行准确性，并通过失败数据集提供详细反馈以优化后续迭代。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15033v1",
      "published_date": "2024-11-22 16:05:54 UTC",
      "updated_date": "2024-11-22 16:05:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:17:33.808514"
    },
    {
      "arxiv_id": "2411.15027v1",
      "title": "Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Colombani",
        "Luca Brini",
        "Dimitri Ognibene",
        "Giuseppe Boccignone"
      ],
      "abstract": "Robots are increasingly being used in dynamic environments like workplaces,\nhospitals, and homes. As a result, interactions with robots must be simple and\nintuitive, with robots perception adapting efficiently to human-induced\nchanges. This paper presents a robot control architecture that addresses key\nchallenges in human-robot interaction, with a particular focus on the dynamic\ncreation and continuous update of the robot state representation. The\narchitecture uses Large Language Models to integrate diverse information\nsources, including natural language commands, robotic skills representation,\nreal-time dynamic semantic mapping of the perceived scene. This enables\nflexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions\nand settings, limiting their adaptability to dynamic environments and real-time\ncollaboration. In contrast, this architecture uses LLMs to interpret complex,\nhigh-level instructions and generate actionable plans that enhance human-robot\ncollaboration. At its core, the system Perception Module generates and\ncontinuously updates a semantic scene graph using RGB-D sensor data, providing\na detailed and structured representation of the environment. A particle filter\nis employed to ensure accurate object localization in dynamic, real-world\nsettings. The Planner Module leverages this up-to-date semantic map to break\ndown high-level tasks into sub-tasks and link them to robotic skills such as\nnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,\nGOTO). By combining real-time perception, state tracking, and LLM-driven\ncommunication and task planning, the architecture enhances adaptability, task\nefficiency, and human-robot collaboration in dynamic environments.",
      "tldr_zh": "该论文提出了一种基于LLM驱动的机器人控制架构，旨在提升机器人对动态环境的感知和适应性，特别是处理人类诱发的变化。架构的核心包括感知模块，使用RGB-D传感器和粒子滤波器生成并持续更新语义场景图，以及规划模块将高层次自然语言指令分解为子任务并链接到机器人技能（如导航、PICK和PLACE操作）。与传统静态系统相比，该方法显著提高了任务效率和人类-机器人协作，实现更灵活的实时交互。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15027v1",
      "published_date": "2024-11-22 15:58:26 UTC",
      "updated_date": "2024-11-22 15:58:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:17:45.603723"
    },
    {
      "arxiv_id": "2411.15007v1",
      "title": "FTA generation using GenAI with an Autonomy sensor Usecase",
      "title_zh": "翻译失败",
      "authors": [
        "Sneha Sudhir Shetiya",
        "Divya Garikapati",
        "Veeraja Sohoni"
      ],
      "abstract": "Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool.",
      "tldr_zh": "这篇论文探讨了使用生成式人工智能(GenAI)生成故障树分析(FTA)，以自动驾驶中Lidar传感器故障为具体用例，旨在提升系统功能安全。研究者评估了多种开源大型语言模型(LLM)，并通过Prompt Engineering训练其中一个模型，结合PlantUML工具来辅助FTA的开发和分析。结果显示，这种方法成功证明了LLM在生成FTA方面的潜力，为自动驾驶等自治场景的安全设计提供了可行的新途径。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15007v1",
      "published_date": "2024-11-22 15:31:20 UTC",
      "updated_date": "2024-11-22 15:31:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:17:56.817285"
    },
    {
      "arxiv_id": "2411.15004v2",
      "title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data",
      "title_zh": "翻译失败",
      "authors": [
        "Junhong Shen",
        "Atishay Jain",
        "Zedian Xiao",
        "Ishan Amlekar",
        "Mouad Hadji",
        "Aaron Podolny",
        "Ameet Talwalkar"
      ],
      "abstract": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 7.3% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.",
      "tldr_zh": "该研究提出了ScribeAgent，一种针对专业网络代理的框架，通过使用生产规模工作流数据（来自250多个领域、6亿tokens）对开源LLM进行微调，以解决通用LLM在理解HTML等特定网络上下文和长期规划上的不足。相比基于提示的代理，ScribeAgent在Mind2Web基准上实现了最先进的直接生成性能，并在WebArena上将任务成功率提高了7.3%。此外，论文通过详细的消融研究提供了关于LLM选择、训练配方、上下文窗口优化和数据集规模影响的宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15004v2",
      "published_date": "2024-11-22 15:26:23 UTC",
      "updated_date": "2024-12-05 02:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:18:09.211708"
    },
    {
      "arxiv_id": "2411.15276v1",
      "title": "Event USKT : U-State Space Model in Knowledge Transfer for Event Cameras",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhui Lin",
        "Jiahao Zhang",
        "Siyuan Li",
        "Jimin Xiao",
        "Ding Xu",
        "Wenjun Wu",
        "Jiaxuan Lu"
      ],
      "abstract": "Event cameras, as an emerging imaging technology, offer distinct advantages\nover traditional RGB cameras, including reduced energy consumption and higher\nframe rates. However, the limited quantity of available event data presents a\nsignificant challenge, hindering their broader development. To alleviate this\nissue, we introduce a tailored U-shaped State Space Model Knowledge Transfer\n(USKT) framework for Event-to-RGB knowledge transfer. This framework generates\ninputs compatible with RGB frames, enabling event data to effectively reuse\npre-trained RGB models and achieve competitive performance with minimal\nparameter tuning. Within the USKT architecture, we also propose a bidirectional\nreverse state space model. Unlike conventional bidirectional scanning\nmechanisms, the proposed Bidirectional Reverse State Space Model (BiR-SSM)\nleverages a shared weight strategy, which facilitates efficient modeling while\nconserving computational resources. In terms of effectiveness, integrating USKT\nwith ResNet50 as the backbone improves model performance by 0.95%, 3.57%, and\n2.9% on DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets, respectively,\nunderscoring USKT's adaptability and effectiveness. The code will be made\navailable upon acceptance.",
      "tldr_zh": "本研究针对事件相机(Event cameras)的数据量有限问题，提出了一种定制的 U-shaped State Space Model Knowledge Transfer (USKT) 框架，用于 Event-to-RGB 知识转移，该框架生成与 RGB 帧兼容的输入，使事件数据能够高效重用预训练 RGB 模型，同时最小化参数调整。USKT 架构中引入了 Bidirectional Reverse State Space Model (BiR-SSM)，通过共享权重策略实现高效建模，节省计算资源。实验结果显示，将 USKT 与 ResNet50 结合后，在 DVS128 Gesture、N-Caltech101 和 CIFAR-10-DVS 数据集上，模型性能分别提升 0.95%、3.57% 和 2.9%，证明了其适应性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15276v1",
      "published_date": "2024-11-22 15:13:23 UTC",
      "updated_date": "2024-11-22 15:13:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:20:21.525656"
    },
    {
      "arxiv_id": "2411.14995v2",
      "title": "Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution",
      "title_zh": "翻译失败",
      "authors": [
        "Jonas Gösgens",
        "Niklas Jansen",
        "Hector Geffner"
      ],
      "abstract": "Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.",
      "tldr_zh": "本研究提出了一种简单、通用且可扩展的方法，用于从动作序列（action traces）中学习提升 STRIPS 模型。该方法无需额外信息，就能同时学习域谓词（domain predicates），并通过一个高效的新测试检查谓词是否受特定动作模式的影响，从而构建完整的学习域，包括添加前提条件和静态谓词。与 LOCM 系统类似，该方法具有可扩展性，与 SAT 方法类似，具有正确性和完整性。实验在标准经典域如 8-puzzle 上进行，涉及数十万状态和转换，验证了所学表示在更大实例中的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted at ICAPS 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.14995v2",
      "published_date": "2024-11-22 15:09:50 UTC",
      "updated_date": "2025-05-02 14:12:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:18:32.545160"
    },
    {
      "arxiv_id": "2411.14991v1",
      "title": "Free Energy Projective Simulation (FEPS): Active inference with interpretability",
      "title_zh": "Free",
      "authors": [
        "Joséphine Pazem",
        "Marius Krumm",
        "Alexander Q. Vining",
        "Lukas J. Fiderer",
        "Hans J. Briegel"
      ],
      "abstract": "In the last decade, the free energy principle (FEP) and active inference\n(AIF) have achieved many successes connecting conceptual models of learning and\ncognition to mathematical models of perception and action. This effort is\ndriven by a multidisciplinary interest in understanding aspects of\nself-organizing complex adaptive systems, including elements of agency. Various\nreinforcement learning (RL) models performing active inference have been\nproposed and trained on standard RL tasks using deep neural networks. Recent\nwork has focused on improving such agents' performance in complex environments\nby incorporating the latest machine learning techniques. In this paper, we take\nan alternative approach. Within the constraints imposed by the FEP and AIF, we\nattempt to model agents in an interpretable way without deep neural networks by\nintroducing Free Energy Projective Simulation (FEPS). Using internal rewards\nonly, FEPS agents build a representation of their partially observable\nenvironments with which they interact. Following AIF, the policy to achieve a\ngiven task is derived from this world model by minimizing the expected free\nenergy. Leveraging the interpretability of the model, techniques are introduced\nto deal with long-term goals and reduce prediction errors caused by erroneous\nhidden state estimation. We test the FEPS model on two RL environments inspired\nfrom behavioral biology: a timed response task and a navigation task in a\npartially observable grid. Our results show that FEPS agents fully resolve the\nambiguity of both environments by appropriately contextualizing their\nobservations based on prediction accuracy only. In addition, they infer optimal\npolicies flexibly for any target observation in the environment.",
      "tldr_zh": "本研究在自由能原理（Free Energy Principle, FEP）和主动推理（Active Inference, AIF）的框架下，提出了一种可解释的强化学习（Reinforcement Learning, RL）模型——Free Energy Projective Simulation (FEPS)，旨在避免使用深度神经网络来构建代理环境表示。FEPS 代理通过内部奖励构建部分可观察环境的表示，并通过最小化期望自由能（expected free energy）推导策略，同时引入技术处理长期目标和减少预测错误。实验在两个受行为生物学启发的环境中进行，包括定时响应任务和部分可观察网格导航，结果显示 FEPS 代理能基于预测准确性有效上下文化观察，解决环境模糊性，并灵活推断出针对任何目标观察的最优策略。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.NC",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages (including 5 pages appendix), 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14991v1",
      "published_date": "2024-11-22 15:01:44 UTC",
      "updated_date": "2024-11-22 15:01:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:18:45.802738"
    },
    {
      "arxiv_id": "2411.14975v1",
      "title": "Exploring Foundation Models Fine-Tuning for Cytology Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Manon Dausort",
        "Tiffanie Godelaine",
        "Maxime Zanella",
        "Karim El Khoury",
        "Isabelle Salmon",
        "Benoît Macq"
      ],
      "abstract": "Cytology slides are essential tools in diagnosing and staging cancer, but\ntheir analysis is time-consuming and costly. Foundation models have shown great\npotential to assist in these tasks. In this paper, we explore how existing\nfoundation models can be applied to cytological classification. More\nparticularly, we focus on low-rank adaptation, a parameter-efficient\nfine-tuning method suited to few-shot learning. We evaluated five foundation\nmodels across four cytological classification datasets. Our results demonstrate\nthat fine-tuning the pre-trained backbones with LoRA significantly improves\nmodel performance compared to fine-tuning only the classifier head, achieving\nstate-of-the-art results on both simple and complex classification tasks while\nrequiring fewer data samples.",
      "tldr_zh": "本文探讨了使用 Foundation Models 进行细胞学分类的微调方法，旨在解决细胞学幻灯片分析耗时且成本高的挑战。研究重点采用 Low-Rank Adaptation (LoRA) 作为参数高效的微调技术，适用于少样本学习，并评估了五个基础模型在四个细胞学分类数据集上的表现。结果表明，LoRA 微调预训练骨干网络比仅微调分类器头部显著提升了模型性能，实现了最先进的结果，同时减少了对样本数据的需求。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "5 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14975v1",
      "published_date": "2024-11-22 14:34:04 UTC",
      "updated_date": "2024-11-22 14:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:18:57.191693"
    },
    {
      "arxiv_id": "2411.14972v1",
      "title": "Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models",
      "title_zh": "Open-Amp：音频效果基础模型的合成数据框架",
      "authors": [
        "Alec Wright",
        "Alistair Carson",
        "Lauri Juvela"
      ],
      "abstract": "This paper introduces Open-Amp, a synthetic data framework for generating\nlarge-scale and diverse audio effects data. Audio effects are relevant to many\nmusical audio processing and Music Information Retrieval (MIR) tasks, such as\nmodelling of analog audio effects, automatic mixing, tone matching and\ntranscription. Existing audio effects datasets are limited in scope, usually\nincluding relatively few audio effects processors and a limited amount of input\naudio signals. Our proposed framework overcomes these issues, by crowdsourcing\nneural network emulations of guitar amplifiers and effects, created by users of\nopen-source audio effects emulation software. This allows users of Open-Amp\ncomplete control over the input signals to be processed by the effects models,\nas well as providing high-quality emulations of hundreds of devices. Open-Amp\ncan render audio online during training, allowing great flexibility in data\naugmentation. Our experiments show that using Open-Amp to train a guitar\neffects encoder achieves new state-of-the-art results on multiple guitar\neffects classification tasks. Furthermore, we train a one-to-many guitar\neffects model using Open-Amp, and use it to emulate unseen analog effects via\nmanipulation of its learned latent space, indicating transferability to analog\nguitar effects data.",
      "tldr_zh": "本论文介绍了 Open-Amp，一种合成数据框架，用于生成大规模、多样化的音频效果数据，以支持音乐音频处理和 Music Information Retrieval (MIR) 任务，如模拟模拟音频效果、自动混音和音色匹配。Open-Amp 通过众包神经网络模拟吉他放大器和效果，利用开源软件提供高质量仿真，并允许在线渲染音频以增强数据扩充灵活性。实验结果显示，使用 Open-Amp 训练的吉他效果编码器在多个分类任务上达到了新的 state-of-the-art 性能，并成功训练一到多吉他效果模型，实现对未见模拟效果的仿真，证明了其对模拟数据的可转移性。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14972v1",
      "published_date": "2024-11-22 14:27:59 UTC",
      "updated_date": "2024-11-22 14:27:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:20:33.669287"
    },
    {
      "arxiv_id": "2411.14967v1",
      "title": "SwissADT: An Audio Description Translation System for Swiss Languages",
      "title_zh": "SwissADT：瑞士语言音频描述翻译系统",
      "authors": [
        "Lukas Fischer",
        "Yingqiang Gao",
        "Alexa Lintner",
        "Sarah Ebling"
      ],
      "abstract": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.",
      "tldr_zh": "本文提出 SwissADT，这是一个针对瑞士主要语言（德语、法语、意大利语）和英语的音频描述翻译 (ADT) 系统，旨在解决多语言国家中 AD 数据缺乏和时间同步问题，从而提升视力障碍者的信息可访问性。系统通过收集包含视频剪辑的精炼 AD 数据，并利用 Large Language Models (LLMs) 进行自动翻译，探索了是否融入视觉信息能改善输出质量。实验结果显示，SwissADT 在自动和人工评估中表现出色，证明其在 ADT 任务中的潜力，并建议结合人类专家进一步优化系统以惠及更多多语言人群。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14967v1",
      "published_date": "2024-11-22 14:23:07 UTC",
      "updated_date": "2024-11-22 14:23:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:20:45.117055"
    },
    {
      "arxiv_id": "2411.15274v1",
      "title": "Feature-interactive Siamese graph encoder-based image analysis to predict STAS from histopathology images in lung cancer",
      "title_zh": "翻译失败",
      "authors": [
        "Liangrui Pan",
        "Qingchun Liang",
        "Wenwu Zeng",
        "Yijun Peng",
        "Zhenyu Zhao",
        "Yiyi Liang",
        "Jiadi Luo",
        "Xiang Wang",
        "Shaoliang Peng"
      ],
      "abstract": "Spread through air spaces (STAS) is a distinct invasion pattern in lung\ncancer, crucial for prognosis assessment and guiding surgical decisions.\nHistopathology is the gold standard for STAS detection, yet traditional methods\nare subjective, time-consuming, and prone to misdiagnosis, limiting large-scale\napplications. We present VERN, an image analysis model utilizing a\nfeature-interactive Siamese graph encoder to predict STAS from lung cancer\nhistopathological images. VERN captures spatial topological features with\nfeature sharing and skip connections to enhance model training. Using 1,546\nhistopathology slides, we built a large single-cohort STAS lung cancer dataset.\nVERN achieved an AUC of 0.9215 in internal validation and AUCs of 0.8275 and\n0.8829 in frozen and paraffin-embedded test sections, respectively,\ndemonstrating clinical-grade performance. Validated on a single-cohort and\nthree external datasets, VERN showed robust predictive performance and\ngeneralizability, providing an open platform (http://plr.20210706.xyz:5000/) to\nenhance STAS diagnosis efficiency and accuracy.",
      "tldr_zh": "该研究针对肺癌中STAS（Spread through air spaces）的检测问题，提出了一种基于feature-interactive Siamese graph encoder的图像分析模型VERN，以克服传统组织病理学方法的局限性。VERN通过feature sharing和skip connections捕获空间拓扑特征，并在1,546张组织病理学切片构建的单队列数据集上进行训练。模型在内部验证中获得AUC 0.9215，并在外部数据集上显示出稳健的预测性能和泛化能力，提供了一个开源平台（http://plr.20210706.xyz:5000/）来提升STAS诊断的效率和准确性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "accept for publication in npj Precision Oncology",
      "pdf_url": "http://arxiv.org/pdf/2411.15274v1",
      "published_date": "2024-11-22 14:21:52 UTC",
      "updated_date": "2024-11-22 14:21:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:20:57.184810"
    },
    {
      "arxiv_id": "2411.14962v2",
      "title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Hitesh Laxmichand Patel",
        "Amit Agarwal",
        "Bhargava Kumar",
        "Karan Gupta",
        "Priyaranjan Pattnayak"
      ],
      "abstract": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.",
      "tldr_zh": "本研究提出了一种使用大型语言模型（LLM）生成多样化合成数据的方法，旨在解决身份文件（如驾照、保险卡和学生ID）中条形码检测的挑战，该问题源于缺乏真实数据集和隐私限制。不同于传统工具如Faker的预定义模板，该方法利用LLM的广泛知识创建上下文丰富的真实数据，并将其编码成条形码后叠加到文档模板上，从而简化数据集生成过程。实验结果显示，这种合成数据比传统方法更具多样性和相关性，导致条形码检测模型性能显著提升，并为可扩展的隐私优先自动化文档处理和身份验证提供新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 1 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14962v2",
      "published_date": "2024-11-22 14:21:18 UTC",
      "updated_date": "2024-12-23 19:01:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:21:09.413902"
    },
    {
      "arxiv_id": "2411.14959v1",
      "title": "Design-o-meter: Towards Evaluating and Refining Graphic Designs",
      "title_zh": "翻译失败",
      "authors": [
        "Sahil Goyal",
        "Abhinav Mahajan",
        "Swasti Mishra",
        "Prateksha Udhayanan",
        "Tripti Shukla",
        "K J Joseph",
        "Balaji Vasan Srinivasan"
      ],
      "abstract": "Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting.",
      "tldr_zh": "本文提出 Design-o-meter，一种数据驱动的方法，用于量化评估图形设计的质量，并提供修改建议以提升其视觉吸引力。该框架首次在统一系统中整合设计评分和优化功能，解决了设计评估中的主观性和模糊性问题。通过定量和定性分析，与基线模型（包括最近的 Multimodal LLM 方法）相比，Design-o-meter 显示出显著优势，有望推动图形设计自动化的进一步发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2025. Project page:\n  https://sahilg06.github.io/Design-o-meter/",
      "pdf_url": "http://arxiv.org/pdf/2411.14959v1",
      "published_date": "2024-11-22 14:17:46 UTC",
      "updated_date": "2024-11-22 14:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:21:20.563687"
    },
    {
      "arxiv_id": "2411.14953v1",
      "title": "Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing",
      "title_zh": "评估 Vision Transformer 模型在工业制造中的视觉质量控制",
      "authors": [
        "Miriam Alber",
        "Christoph Hönes",
        "Patrick Baier"
      ],
      "abstract": "One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints.",
      "tldr_zh": "该论文评估了Vision Transformer模型在工业制造视觉质量控制中的应用，旨在通过机器学习早期检测缺陷产品，以节省成本并减少人为错误。研究者结合SotA的视觉骨干和异常检测算法，在MVTecAD和BTAD数据集上进行实验，探索检测质量与推理时间的权衡。最终，论文提供指导，帮助从业者根据具体用例和硬件约束选择高效的模型架构。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2.m"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14953v1",
      "published_date": "2024-11-22 14:12:35 UTC",
      "updated_date": "2024-11-22 14:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:21:34.261691"
    },
    {
      "arxiv_id": "2411.14946v1",
      "title": "Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Lars Nieradzik",
        "Henrike Stephani",
        "Janis Keuper"
      ],
      "abstract": "In this paper, we present an approach for evaluating attribution maps, which\nplay a central role in interpreting the predictions of convolutional neural\nnetworks (CNNs). We show that the widely used insertion/deletion metrics are\nsusceptible to distribution shifts that affect the reliability of the ranking.\nOur method proposes to replace pixel modifications with adversarial\nperturbations, which provides a more robust evaluation framework. By using\nsmoothness and monotonicity measures, we illustrate the effectiveness of our\napproach in correcting distribution shifts. In addition, we conduct the most\ncomprehensive quantitative and qualitative assessment of attribution maps to\ndate. Introducing baseline attribution maps as sanity checks, we find that our\nmetric is the only contender to pass all checks. Using Kendall's $\\tau$ rank\ncorrelation coefficient, we show the increased consistency of our metric across\n15 dataset-architecture combinations. Of the 16 attribution maps tested, our\nresults clearly show SmoothGrad to be the best map currently available. This\nresearch makes an important contribution to the development of attribution maps\nby providing a reliable and consistent evaluation framework. To ensure\nreproducibility, we will provide the code along with our results.",
      "tldr_zh": "本文提出了一种基于 adversarial perturbations 的方法，用于可靠评估 CNNs 中的 attribution maps，以解决现有 insertion/deletion metrics 易受 distribution shifts 影响的问题。该方法通过引入 smoothness 和 monotonicity measures，提供更稳健的评估框架，并在 15 个数据集-架构组合上显示出更高的 Kendall's τ 一致性。研究进行了最全面的定量和定性评估，发现 SmoothGrad 是目前最佳的 attribution map，且新 metric 是唯一通过所有 baseline sanity checks 的。该框架为 attribution maps 的发展提供了可靠支持，并承诺提供代码以确保可重复性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14946v1",
      "published_date": "2024-11-22 13:57:56 UTC",
      "updated_date": "2024-11-22 13:57:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:21:45.814815"
    },
    {
      "arxiv_id": "2411.14942v1",
      "title": "Comparative Study of Neural Network Methods for Solving Topological Solitons",
      "title_zh": "神经网络方法用于解决拓扑孤子的比较研究",
      "authors": [
        "Koji Hashimoto",
        "Koshiro Matsuo",
        "Masaki Murata",
        "Gakuto Ogiwara"
      ],
      "abstract": "Topological solitons, which are stable, localized solutions of nonlinear\ndifferential equations, are crucial in various fields of physics and\nmathematics, including particle physics and cosmology. However, solving these\nsolitons presents significant challenges due to the complexity of the\nunderlying equations and the computational resources required for accurate\nsolutions. To address this, we have developed a novel method using neural\nnetwork (NN) to efficiently solve solitons. A similar NN approach is\nPhysics-Informed Neural Networks (PINN). In a comparative analysis between our\nmethod and PINN, we find that our method achieves shorter computation times\nwhile maintaining the same level of accuracy. This advancement in computational\nefficiency not only overcomes current limitations but also opens new avenues\nfor studying topological solitons and their dynamical behavior.",
      "tldr_zh": "这篇论文比较了不同神经网络方法用于解决topological solitons，这些是物理和数学领域非线性微分方程的稳定局部解。研究团队开发了一种新型神经网络方法，与Physics-Informed Neural Networks (PINN)相比，该方法在保持相同准确性的前提下，大大缩短了计算时间。总体而言，这一创新提高了topological solitons的求解效率，并为探索其动态行为提供了新的研究机会。",
      "categories": [
        "hep-th",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "hep-th",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14942v1",
      "published_date": "2024-11-22 13:54:52 UTC",
      "updated_date": "2024-11-22 13:54:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:21:56.868136"
    },
    {
      "arxiv_id": "2411.14937v1",
      "title": "Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning",
      "title_zh": "Geminio：语言引导的联邦学习梯度反转攻击",
      "authors": [
        "Junjie Shan",
        "Ziqi Zhao",
        "Jialin Lu",
        "Rui Zhang",
        "Siu Ming Yiu",
        "Ka-Ho Chow"
      ],
      "abstract": "Foundation models that bridge vision and language have made significant\nprogress, inspiring numerous life-enriching applications. However, their\npotential for misuse to introduce new threats remains largely unexplored. This\npaper reveals that vision-language models (VLMs) can be exploited to overcome\nlongstanding limitations in gradient inversion attacks (GIAs) within federated\nlearning (FL), where an FL server reconstructs private data samples from\ngradients shared by victim clients. Current GIAs face challenges in\nreconstructing high-resolution images, especially when the victim has a large\nlocal data batch. While focusing reconstruction on valuable samples rather than\nthe entire batch is promising, existing methods lack the flexibility to allow\nattackers to specify their target data. In this paper, we introduce Geminio,\nthe first approach to transform GIAs into semantically meaningful, targeted\nattacks. Geminio enables a brand new privacy attack experience: attackers can\ndescribe, in natural language, the types of data they consider valuable, and\nGeminio will prioritize reconstruction to focus on those high-value samples.\nThis is achieved by leveraging a pretrained VLM to guide the optimization of a\nmalicious global model that, when shared with and optimized by a victim,\nretains only gradients of samples that match the attacker-specified query.\nExtensive experiments demonstrate Geminio's effectiveness in pinpointing and\nreconstructing targeted samples, with high success rates across complex\ndatasets under FL and large batch sizes and showing resilience against existing\ndefenses.",
      "tldr_zh": "本论文提出Geminio，一种语言引导的梯度反演攻击（Gradient Inversion Attacks），用于联邦学习（Federated Learning）中重建受害者私有数据，克服了现有方法在处理高分辨率图像和大批量数据时的局限性。Geminio利用预训练的视觉语言模型（VLMs）允许攻击者通过自然语言描述目标数据类型，从而指导恶意全局模型优化，只保留匹配查询的样本梯度。实验结果表明，该攻击在复杂数据集上表现出高成功率，并能抵抗现有防御措施，为隐私风险评估提供了新洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14937v1",
      "published_date": "2024-11-22 13:49:56 UTC",
      "updated_date": "2024-11-22 13:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:22:10.210660"
    },
    {
      "arxiv_id": "2411.15272v1",
      "title": "Curriculum-enhanced GroupDRO: Challenging the Norm of Avoiding Curriculum Learning in Subpopulation Shift Setups",
      "title_zh": "课程增强的 GroupDRO：挑战在子群体偏移",
      "authors": [
        "Antonio Barbalau"
      ],
      "abstract": "In subpopulation shift scenarios, a Curriculum Learning (CL) approach would\nonly serve to imprint the model weights, early on, with the easily learnable\nspurious correlations featured. To the best of our knowledge, none of the\ncurrent state-of-the-art subpopulation shift approaches employ any kind of\ncurriculum. To overcome this, we design a CL approach aimed at initializing the\nmodel weights in an unbiased vantage point in the hypothesis space which\nsabotages easy convergence towards biased hypotheses during the final\noptimization based on the entirety of the available data. We hereby propose a\nCurriculum-enhanced Group Distributionally Robust Optimization (CeGDRO)\napproach, which prioritizes the hardest bias-confirming samples and the easiest\nbias-conflicting samples, leveraging GroupDRO to balance the initial\ndiscrepancy in terms of difficulty. We benchmark our proposed method against\nthe most popular subpopulation shift datasets, showing an increase over the\nstate-of-the-art results across all scenarios, up to 6.2% on Waterbirds.",
      "tldr_zh": "该论文挑战了在Subpopulation Shift场景中避免使用Curriculum Learning (CL)的传统做法，提出了一种Curriculum-enhanced Group Distributionally Robust Optimization (CeGDRO)方法，以无偏视角初始化模型权重，避免模型快速收敛到虚假相关性(spurious correlations)。CeGDRO通过优先处理最难的偏见确认样本和最容易的偏见冲突样本，并利用GroupDRO平衡难度差异，从而优化模型在完整数据集上的训练。实验结果显示，该方法在多个Subpopulation Shift数据集上超越现有最先进技术，最高在Waterbirds数据集上提升6.2%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15272v1",
      "published_date": "2024-11-22 13:38:56 UTC",
      "updated_date": "2024-11-22 13:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:22:20.281682"
    },
    {
      "arxiv_id": "2411.14927v2",
      "title": "LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure Cooperation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenwei Yang",
        "Jilei Mao",
        "Wenxian Yang",
        "Yibo Ai",
        "Yu Kong",
        "Haibao Yu",
        "Weidong Zhang"
      ],
      "abstract": "Temporal perception, defined as the capability to detect and track objects\nacross temporal sequences, serves as a fundamental component in autonomous\ndriving systems. While single-vehicle perception systems encounter limitations,\nstemming from incomplete perception due to object occlusion and inherent blind\nspots, cooperative perception systems present their own challenges in terms of\nsensor calibration precision and positioning accuracy. To address these issues,\nwe introduce LET-VIC, a LiDAR-based End-to-End Tracking framework for\nVehicle-Infrastructure Cooperation (VIC). First, we employ Temporal\nSelf-Attention and VIC Cross-Attention modules to effectively integrate\ntemporal and spatial information from both vehicle and infrastructure\nperspectives. Then, we develop a novel Calibration Error Compensation (CEC)\nmodule to mitigate sensor misalignment issues and facilitate accurate feature\nalignment. Experiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC\nsignificantly outperforms baseline models. Compared to LET-V, LET-VIC achieves\n+15.0% improvement in mAP and a +17.3% improvement in AMOTA. Furthermore,\nLET-VIC surpasses representative Tracking by Detection models, including\nV2VNet, FFNet, and PointPillars, with at least a +13.7% improvement in mAP and\na +13.1% improvement in AMOTA without considering communication delays,\nshowcasing its robust detection and tracking performance. The experiments\ndemonstrate that the integration of multi-view perspectives, temporal\nsequences, or CEC in end-to-end training significantly improves both detection\nand tracking performance. All code will be open-sourced.",
      "tldr_zh": "该论文提出了一种基于LiDAR的端到端时间感知框架LET-VIC，用于车辆-基础设施合作（VIC），旨在解决单车感知系统（如物体遮挡和盲点问题）以及合作感知系统（如传感器校准精度）的局限性。框架通过Temporal Self-Attention和VIC Cross-Attention模块整合车辆和基础设施的时间及空间信息，并引入Calibration Error Compensation (CEC)模块来补偿传感器不对齐问题，从而实现精确的物体检测和跟踪。在V2X-Seq-SPD数据集上的实验显示，LET-VIC相较于LET-V提高了15.0% mAP和17.3% AMOTA，并比V2VNet、FFNet和PointPillars等模型至少提升13.7% mAP和13.1% AMOTA，证明了多视图整合、时间序列和CEC在端到端训练中的显著性能优势。所有代码将开源，以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14927v2",
      "published_date": "2024-11-22 13:34:29 UTC",
      "updated_date": "2025-04-05 07:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:22:33.347520"
    },
    {
      "arxiv_id": "2411.14925v1",
      "title": "Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot",
      "title_zh": "翻译失败",
      "authors": [
        "Linqi Lu",
        "Yifan Deng",
        "Chuan Tian",
        "Sijia Yang",
        "Dhavan Shah"
      ],
      "abstract": "This study introduces Purrfessor, an innovative AI chatbot designed to\nprovide personalized dietary guidance through interactive, multimodal\nengagement. Leveraging the Large Language-and-Vision Assistant (LLaVA) model\nfine-tuned with food and nutrition data and a human-in-the-loop approach,\nPurrfessor integrates visual meal analysis with contextual advice to enhance\nuser experience and engagement. We conducted two studies to evaluate the\nchatbot's performance and user experience: (a) simulation assessments and human\nvalidation were conducted to examine the performance of the fine-tuned model;\n(b) a 2 (Profile: Bot vs. Pet) by 3 (Model: GPT-4 vs. LLaVA vs. Fine-tuned\nLLaVA) experiment revealed that Purrfessor significantly enhanced users'\nperceptions of care ($\\beta = 1.59$, $p = 0.04$) and interest ($\\beta = 2.26$,\n$p = 0.01$) compared to the GPT-4 bot. Additionally, user interviews\nhighlighted the importance of interaction design details, emphasizing the need\nfor responsiveness, personalization, and guidance to improve user engagement.",
      "tldr_zh": "这篇论文介绍了 Purrfessor，一种基于 fine-tuned LLaVA 模型的多模态 AI 聊天机器人，旨在通过视觉餐食分析和上下文建议提供个性化的饮食健康指导。研究采用 human-in-the-loop 方法对模型进行微调，并整合模拟评估和人类验证来优化性能。在 2x3 实验中，Purrfessor 相较于 GPT-4 显著提升了用户对关心的感知 (β=1.59, p=0.04) 和兴趣 (β=2.26, p=0.01)。此外，用户访谈突出了交互设计的响应性、个性和指导在提升用户参与度方面的关键作用。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14925v1",
      "published_date": "2024-11-22 13:28:28 UTC",
      "updated_date": "2024-11-22 13:28:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:22:46.717707"
    },
    {
      "arxiv_id": "2411.16709v1",
      "title": "A Brief Summary of Explanatory Virtues",
      "title_zh": "翻译失败",
      "authors": [
        "Ingrid Zukerman"
      ],
      "abstract": "In this report, I provide a brief summary of the literature in philosophy,\npsychology and cognitive science about Explanatory Virtues, and link these\nconcepts to eXplainable AI.",
      "tldr_zh": "本报告简要总结了哲学、心理学和认知科学领域中关于 Explanatory Virtues 的文献，这些概念包括解释的优点和标准。作者将这些 Explanatory Virtues 与 eXplainable AI 领域联系起来，探讨如何将哲学洞见应用于人工智能的可解释性设计。主要贡献在于桥接传统学术概念与AI解释性研究，提供了一个基础框架以提升AI系统的透明度和可信度。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.16709v1",
      "published_date": "2024-11-22 13:27:56 UTC",
      "updated_date": "2024-11-22 13:27:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:22:56.015415"
    },
    {
      "arxiv_id": "2411.14922v2",
      "title": "GOT4Rec: Graph of Thoughts for Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Zewen Long",
        "Liang Wang",
        "Shu Wu",
        "Qiang Liu",
        "Liang Wang"
      ],
      "abstract": "With their vast open-world knowledge and reasoning abilities, large language\nmodels (LLMs) have become a promising tool for sequential recommendation.\nResearchers have explored various methods to harness these capabilities, but\nmost existing approaches rely on simple input-output prompting, failing to\neffectively bridge the gap between LLMs' general knowledge and the specific\nneeds of recommendation tasks. While reasoning strategies like chain-of-thought\n(CoT) have been introduced to enhance performance, they often produce\ninaccurate recommendations due to underutilized user preference information and\ninsufficient reasoning depth. To address these challenges, we propose GOT4Rec,\na novel sequential recommendation method leveraging the graph of thoughts (GoT)\nreasoning strategy. Our method focuses on three key types of information in\nuser histories: short-term interests, long-term interests and collaborative\ninformation from other users. It enables LLMs to reason independently and\ngenerate recommendations, subsequently aggregating results to derive final\nitems. This method allows LLMs, with enhanced reasoning capabilities, to better\nutilize the user sequence information, producing more accurate recommendations\nand comprehensive explanations. Extensive experiments on real-world datasets\ndemonstrate the effectiveness of GOT4Rec, outperforming existing\nstate-of-the-art baselines with an average improvement of 37.11%. Our code is\navailable at https://anonymous.4open.science/r/GOT4Rec.",
      "tldr_zh": "该研究提出 GOT4Rec，一种基于 Graph of Thoughts (GoT) 推理策略的顺序推荐方法，旨在解决大型语言模型 (LLMs) 在推荐任务中依赖简单提示、未充分利用用户偏好信息和推理深度不足的问题。\nGOT4Rec 聚焦用户历史的短期兴趣、长期兴趣和来自其他用户的协作信息，允许 LLMs 独立进行推理、生成推荐并聚合结果，从而提供更准确的推荐和全面解释。\n实验在真实数据集上表明，GOT4Rec 比现有最先进基线平均提升 37.11%。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14922v2",
      "published_date": "2024-11-22 13:24:01 UTC",
      "updated_date": "2025-04-23 01:38:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:23:09.265715"
    },
    {
      "arxiv_id": "2411.14907v1",
      "title": "DAIRHuM: A Platform for Directly Aligning AI Representations with Human Musical Judgments applied to Carnatic Music",
      "title_zh": "翻译失败",
      "authors": [
        "Prashanth Thattai Ravikumar"
      ],
      "abstract": "Quantifying and aligning music AI model representations with human behavior\nis an important challenge in the field of MIR. This paper presents a platform\nfor exploring the Direct alignment between AI music model Representations and\nHuman Musical judgments (DAIRHuM). It is designed to enable musicians and\nexperimentalists to label similarities in a dataset of music recordings, and\nexamine a pre-trained model's alignment with their labels using quantitative\nscores and visual plots. DAIRHuM is applied to analyze alignment between NSynth\nrepresentations, and a rhythmic duet between two percussionists in a Carnatic\nquartet ensemble, an example of a genre where annotated data is scarce and\nassessing alignment is non-trivial. The results demonstrate significant\nfindings on model alignment with human judgments of rhythmic harmony, while\nhighlighting key differences in rhythm perception and music similarity\njudgments specific to Carnatic music. This work is among the first efforts to\nenable users to explore human-AI model alignment in Carnatic music and advance\nMIR research in Indian music while dealing with data scarcity and cultural\nspecificity. The development of this platform provides greater accessibility to\nmusic AI tools for under-represented genres.",
      "tldr_zh": "本研究引入了 DAIRHuM 平台，用于直接对齐 AI 音乐模型表示与人类音乐判断，特别应用于 Carnatic 音乐领域。平台允许音乐家和实验者标记音乐录音中的相似性，并通过量化分数和可视化图表评估预训练模型（如 NSynth）的对齐程度。实验结果显示，DAIRHuM 在分析 Carnatic 四重奏中打击乐手的节奏和谐时，模型与人类判断有显著对齐，但也揭示了 Carnatic 音乐特有的节奏感知和相似性判断差异。该平台有助于推进音乐信息检索 (MIR) 在印度音乐中的研究，解决数据稀缺和文化特异性问题，并提升音乐 AI 工具对 underrepresented 流派的可用性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "4 Pages, ICASSP workshop submission",
      "pdf_url": "http://arxiv.org/pdf/2411.14907v1",
      "published_date": "2024-11-22 13:04:51 UTC",
      "updated_date": "2024-11-22 13:04:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:23:21.472066"
    },
    {
      "arxiv_id": "2411.15271v1",
      "title": "EADReg: Probabilistic Correspondence Generation with Efficient Autoregressive Diffusion Model for Outdoor Point Cloud Registration",
      "title_zh": "EADReg：基于高效自回归扩散模型的概率对应生成，用于室外点云配准",
      "authors": [
        "Linrui Gong",
        "Jiuming Liu",
        "Junyi Ma",
        "Lihao Liu",
        "Yaonan Wang",
        "Hesheng Wang"
      ],
      "abstract": "Diffusion models have shown the great potential in the point cloud\nregistration (PCR) task, especially for enhancing the robustness to challenging\ncases. However, existing diffusion-based PCR methods primarily focus on\ninstance-level scenarios and struggle with outdoor LiDAR points, where the\nsparsity, irregularity, and huge point scale inherent in LiDAR points pose\nchallenges to establishing dense global point-to-point correspondences. To\naddress this issue, we propose a novel framework named EADReg for efficient and\nrobust registration of LiDAR point clouds based on autoregressive diffusion\nmodels. EADReg follows a coarse-to-fine registration paradigm. In the coarse\nstage, we employ a Bi-directional Gaussian Mixture Model (BGMM) to reject\noutlier points and obtain purified point cloud pairs. BGMM establishes\ncorrespondences between the Gaussian Mixture Models (GMMs) from the source and\ntarget frames, enabling reliable coarse registration based on filtered features\nand geometric information. In the fine stage, we treat diffusion-based PCR as\nan autoregressive process to generate robust point correspondences, which are\nthen iteratively refined on upper layers. Despite common criticisms of\ndiffusion-based methods regarding inference speed, EADReg achieves runtime\ncomparable to convolutional-based methods. Extensive experiments on the KITTI\nand NuScenes benchmark datasets highlight the state-of-the-art performance of\nour proposed method. Codes will be released upon publication.",
      "tldr_zh": "这篇论文提出了 EADReg，一种基于高效自回归扩散模型的框架，用于处理室外 LiDAR 点云注册（PCR），以应对点云的稀疏性、不规则性和大规模挑战。EADReg 采用粗到细的注册范式：在粗阶段使用双向高斯混合模型（BGMM）去除异常点并进行初步配准；在细阶段将扩散-based PCR 视为自回归过程，生成鲁棒的点对应关系并迭代精炼。尽管扩散模型通常推理速度较慢，但 EADReg 的运行时间与基于卷积的方法相当。实验在 KITTI 和 NuScenes 数据集上证明了该方法的 state-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15271v1",
      "published_date": "2024-11-22 13:03:54 UTC",
      "updated_date": "2024-11-22 13:03:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:23:34.162207"
    },
    {
      "arxiv_id": "2412.06795v1",
      "title": "SpikeFI: A Fault Injection Framework for Spiking Neural Networks",
      "title_zh": "SpikeFI：用于脉冲神经网络的故障注入框架",
      "authors": [
        "Theofilos Spyrou",
        "Said Hamdioui",
        "Haralampos-G. Stratigopoulos"
      ],
      "abstract": "Neuromorphic computing and spiking neural networks (SNNs) are gaining\ntraction across various artificial intelligence (AI) tasks thanks to their\npotential for efficient energy usage and faster computation speed. This\ncomparative advantage comes from mimicking the structure, function, and\nefficiency of the biological brain, which arguably is the most brilliant and\ngreen computing machine. As SNNs are eventually deployed on a hardware\nprocessor, the reliability of the application in light of hardware-level faults\nbecomes a concern, especially for safety- and mission-critical applications. In\nthis work, we propose SpikeFI, a fault injection framework for SNNs that can be\nused for automating the reliability analysis and test generation. SpikeFI is\nbuilt upon the SLAYER PyTorch framework with fault injection experiments\naccelerated on a single or multiple GPUs. It has a comprehensive integrated\nneuron and synapse fault model library, in accordance to the literature in the\ndomain, which is extendable by the user if needed. It supports: single and\nmultiple faults; permanent and transient faults; specified, random layer-wise,\nand random network-wise fault locations; and pre-, during, and post-training\nfault injection. It also offers several optimization speedups and built-in\nfunctions for results visualization. SpikeFI is open-source and available for\ndownload via GitHub at https://github.com/SpikeFI.",
      "tldr_zh": "本文提出 SpikeFI，这是一个针对 Spiking Neural Networks (SNNs) 的故障注入框架，用于自动化可靠性分析和测试生成，以应对硬件故障对安全关键应用的潜在风险。SpikeFI 基于 SLAYER PyTorch 框架，在单或多 GPUs 上加速实验，支持全面的神经元和突触故障模型，包括单故障、多故障、永久和瞬态故障，以及指定或随机位置的注入时机（如训练前、中或后）。该框架还提供优化加速、结果可视化功能，并作为开源工具在 GitHub 上公开可用。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.06795v1",
      "published_date": "2024-11-22 12:08:06 UTC",
      "updated_date": "2024-11-22 12:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:23:45.528861"
    },
    {
      "arxiv_id": "2411.14883v1",
      "title": "Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuheng Xu",
        "Taiping Zhang"
      ],
      "abstract": "Domain-invariant representation learning is a powerful method for domain\ngeneralization. Previous approaches face challenges such as high computational\ndemands, training instability, and limited effectiveness with high-dimensional\ndata, potentially leading to the loss of valuable features. To address these\nissues, we hypothesize that an ideal generalized representation should exhibit\nsimilar pattern responses within the same channel across cross-domain images.\nBased on this hypothesis, we use deep features from the source domain as\nqueries, and deep features from the generated domain as keys and values.\nThrough a cross-channel attention mechanism, the original deep features are\nreconstructed into robust regularization representations, forming an explicit\nconstraint that guides the model to learn domain-invariant representations.\nAdditionally, style augmentation is another common method. However, existing\nmethods typically generate new styles through convex combinations of source\ndomains, which limits the diversity of training samples by confining the\ngenerated styles to the original distribution. To overcome this limitation, we\npropose an Adaptive Feature Blending (AFB) method that generates\nout-of-distribution samples while exploring the in-distribution space,\nsignificantly expanding the domain range. Extensive experimental results\ndemonstrate that our proposed methods achieve superior performance on two\nstandard domain generalization benchmarks for medical image segmentation.",
      "tldr_zh": "这篇论文提出了一种新的领域泛化范式，用于医疗图像分割（Medical Image Segmentation），通过跨通道注意力（Cross-Attention）机制和自适应特征混合（Adaptive Feature Blending, AFB）方法，解决现有领域不变表示学习的问题，如高计算需求、训练不稳定性和特征丢失。核心方法假设同一通道内的跨域图像应显示相似的模式响应，并使用源域深度特征作为查询来重建鲁棒表示，同时AFB通过生成分布外样本扩展训练样本的多样性，超越传统风格增强的局限性。实验结果表明，该方法在两个标准的领域泛化（Domain Generalization）基准上取得了优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.4"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14883v1",
      "published_date": "2024-11-22 12:06:24 UTC",
      "updated_date": "2024-11-22 12:06:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:23:58.645084"
    },
    {
      "arxiv_id": "2411.14871v3",
      "title": "Preference Alignment for Diffusion Model via Explicit Denoised Distribution Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Dingyuan Shi",
        "Yong Wang",
        "Hangyu Li",
        "Xiangxiang Chu"
      ],
      "abstract": "Diffusion models have shown remarkable success in text-to-image generation,\nmaking preference alignment for these models increasingly important. The\npreference labels are typically available only at the terminal of denoising\ntrajectories, which poses challenges in optimizing the intermediate denoising\nsteps. In this paper, we propose to conduct Denoised Distribution Estimation\n(DDE) that explicitly connects intermediate steps to the terminal denoised\ndistribution. Therefore, preference labels can be used for the entire\ntrajectory optimization. To this end, we design two estimation strategies for\nour DDE. The first is stepwise estimation, which utilizes the conditional\ndenoised distribution to estimate the model denoised distribution. The second\nis single-shot estimation, which converts the model output into the terminal\ndenoised distribution via DDIM modeling. Analytically and empirically, we\nreveal that DDE equipped with two estimation strategies naturally derives a\nnovel credit assignment scheme that prioritizes optimizing the middle part of\nthe denoising trajectory. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.",
      "tldr_zh": "本文提出了一种针对扩散模型的偏好对齐方法，通过显式去噪分布估计(Denoised Distribution Estimation, DDE)来解决偏好标签仅在去噪轨迹末端可用的问题，从而实现对整个轨迹的优化。DDE包括两种策略：stepwise estimation，利用条件去噪分布估计模型去噪分布；以及single-shot estimation，通过DDIM建模将模型输出转换为末端去噪分布。这种方法自然导出一个新的信用分配方案，优先优化去噪轨迹的中间部分。实验结果表明，该方法在文本到图像生成任务中实现了优越的定量和定性性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14871v3",
      "published_date": "2024-11-22 11:45:33 UTC",
      "updated_date": "2025-03-13 02:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:24:09.398235"
    },
    {
      "arxiv_id": "2411.14870v1",
      "title": "Application of AI to formal methods -- an analysis of current trends",
      "title_zh": "AI 在形式方法中的应用——当前趋势的分析",
      "authors": [
        "Sebastian Stock",
        "Jannik Dunkelau",
        "Atif Mashkoor"
      ],
      "abstract": "With artificial intelligence (AI) being well established within the daily\nlives of research communities, we turn our gaze toward an application area that\nappears intuitively unsuited for probabilistic decision-making: the area of\nformal methods (FM). FM aim to provide sound and understandable reasoning about\nproblems in computer science, which seemingly collides with the black-box\nnature that inhibits many AI approaches. However, many researchers have crossed\nthis gap and applied AI techniques to enhance FM approaches. As this dichotomy\nof FM and AI sparked our interest, we conducted a systematic mapping study to\nmap the current landscape of research publications. In this study, we\ninvestigate the previous five years of applied AI to FM (2019-2023), as these\ncorrespond to periods of high activity. This investigation results in 189\nentries, which we explore in more detail to find current trends, highlight\nresearch gaps, and give suggestions for future research.",
      "tldr_zh": "本研究分析了人工智能（AI）在形式方法（formal methods, FM）中的应用，尽管FM强调精确推理，而AI常被视为黑盒子决策工具。研究者们通过系统映射研究（systematic mapping study）审查了2019-2023年间共189篇相关文献，以识别当前AI增强FM的趋势。结果突出了关键研究空白，并为未来研究提供了建议，例如如何弥合AI与FM的差异以提升计算机科学领域的可靠性和创新。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14870v1",
      "published_date": "2024-11-22 11:43:39 UTC",
      "updated_date": "2024-11-22 11:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:24:20.812863"
    },
    {
      "arxiv_id": "2411.14869v2",
      "title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Xuewu Lin",
        "Tianwei Lin",
        "Lichao Huang",
        "Hongyu Xie",
        "Zhizhong Su"
      ],
      "abstract": "In embodied intelligence systems, a key component is 3D perception algorithm,\nwhich enables agents to understand their surrounding environments. Previous\nalgorithms primarily rely on point cloud, which, despite offering precise\ngeometric information, still constrain perception performance due to inherent\nsparsity, noise, and data scarcity. In this work, we introduce a novel\nimage-centric 3D perception model, BIP3D, which leverages expressive image\nfeatures with explicit 3D position encoding to overcome the limitations of\npoint-centric methods. Specifically, we leverage pre-trained 2D vision\nfoundation models to enhance semantic understanding, and introduce a spatial\nenhancer module to improve spatial understanding. Together, these modules\nenable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end\n3D perception. In our experiments, BIP3D outperforms current state-of-the-art\nresults on the EmbodiedScan benchmark, achieving improvements of 5.69% in the\n3D detection task and 15.25% in the 3D visual grounding task.",
      "tldr_zh": "该研究提出了一种新型图像中心 3D 感知模型 BIP3D，用于提升具身智能系统的环境理解能力，以克服传统基于 point cloud 的方法的稀疏性、噪声和数据稀缺问题。BIP3D 利用预训练的 2D vision foundation models 增强语义理解，并引入 spatial enhancer module 来改善空间理解，从而实现多视图、多模态特征融合和端到端的 3D 感知。在 EmbodiedScan 基准测试中，BIP3D 在 3D detection 任务上提升 5.69%，在 3D visual grounding 任务上提升 15.25%，超过了现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14869v2",
      "published_date": "2024-11-22 11:35:42 UTC",
      "updated_date": "2024-11-27 11:31:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:24:33.023981"
    },
    {
      "arxiv_id": "2411.14863v1",
      "title": "Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired Image-to-Image Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Jeongsol Kim",
        "Beomsu Kim",
        "Jong Chul Ye"
      ],
      "abstract": "Diffusion models (DMs), which enable both image generation from noise and\ninversion from data, have inspired powerful unpaired image-to-image (I2I)\ntranslation algorithms. However, they often require a larger number of neural\nfunction evaluations (NFEs), limiting their practical applicability. In this\npaper, we tackle this problem with Schrodinger Bridges (SBs), which are\nstochastic differential equations (SDEs) between distributions with minimal\ntransport cost. We analyze the probability flow ordinary differential equation\n(ODE) formulation of SBs, and observe that we can decompose its vector field\ninto a linear combination of source predictor, target predictor, and noise\npredictor. Inspired by this observation, we propose Latent Schrodinger Bridges\n(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and\ndevelop appropriate prompt optimization and change of variables formula to\nmatch the training and inference between distributions. We demonstrate that our\nalgorithm successfully conduct competitive I2I translation in unsupervised\nsetting with only a fraction of computation cost required by previous DM-based\nI2I methods.",
      "tldr_zh": "本论文提出Latent Schrodinger Bridge (LSBs)，一种基于Schrodinger Bridges (SBs)的框架，用于加速无监督图像到图像翻译（I2I translation）。LSBs 通过分析SBs的概率流ODE，将其向量场分解为源预测器、目标预测器和噪声预测器的线性组合，并利用预训练的Stable Diffusion模型进行近似，同时优化提示和变量变化公式，以减少神经函数评估（NFEs）的计算成本。实验结果表明，该方法在无监督设置下实现了与现有Diffusion models-based算法相当的翻译性能，但仅需一小部分计算资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14863v1",
      "published_date": "2024-11-22 11:24:14 UTC",
      "updated_date": "2024-11-22 11:24:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:24:46.409473"
    },
    {
      "arxiv_id": "2411.14858v1",
      "title": "Domain and Range Aware Synthetic Negatives Generation for Knowledge Graph Embedding Models",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Bernardi",
        "Luca Costabello"
      ],
      "abstract": "Knowledge Graph Embedding models, representing entities and edges in a\nlow-dimensional space, have been extremely successful at solving tasks related\nto completing and exploring Knowledge Graphs (KGs). One of the key aspects of\ntraining most of these models is teaching to discriminate between true\nstatements positives and false ones (negatives). However, the way in which\nnegatives can be defined is not trivial, as facts missing from the KG are not\nnecessarily false and a set of ground truth negatives is hardly ever given.\nThis makes synthetic negative generation a necessity. Different generation\nstrategies can heavily affect the quality of the embeddings, making it a\nprimary aspect to consider. We revamp a strategy that generates corruptions\nduring training respecting the domain and range of relations, we extend its\ncapabilities and we show our methods bring substantial improvement (+10% MRR)\nfor standard benchmark datasets and over +150% MRR for a larger ontology-backed\ndataset.",
      "tldr_zh": "本文提出了一种考虑关系的 domain 和 range 的 synthetic negatives 生成策略，用于训练 Knowledge Graph Embedding models，以更好地区分真实语句（positives）和虚假语句（negatives）。该策略通过扩展原有方法，生成更高质量的负样本，确保训练过程更准确。实验结果显示，该方法在标准基准数据集上提升了 10% 的 MRR，在一个更大的 ontology-backed 数据集上实现了超过 150% 的 MRR 改进，从而显著提高了知识图谱嵌入模型的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the Third Learning on Graphs Conference (LoG 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.14858v1",
      "published_date": "2024-11-22 11:17:32 UTC",
      "updated_date": "2024-11-22 11:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:24:57.421809"
    },
    {
      "arxiv_id": "2411.14847v2",
      "title": "Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhening Liu",
        "Yingdong Hu",
        "Xinjie Zhang",
        "Rui Song",
        "Jiawei Shao",
        "Zehong Lin",
        "Jun Zhang"
      ],
      "abstract": "The recent development of 3D Gaussian Splatting (3DGS) has led to great\ninterest in 4D dynamic spatial reconstruction. Existing approaches mainly rely\non full-length multi-view videos, while there has been limited exploration of\nonline reconstruction methods that enable on-the-fly training and per-timestep\nstreaming. Current 3DGS-based streaming methods treat the Gaussian primitives\nuniformly and constantly renew the densified Gaussians, thereby overlooking the\ndifference between dynamic and static features as well as neglecting the\ntemporal continuity in the scene. To address these limitations, we propose a\nnovel three-stage pipeline for iterative streamable 4D dynamic spatial\nreconstruction. Our pipeline comprises a selective inheritance stage to\npreserve temporal continuity, a dynamics-aware shift stage to distinguish\ndynamic and static primitives and optimize their movements, and an error-guided\ndensification stage to accommodate emerging objects. Our method achieves\nstate-of-the-art performance in online 4D reconstruction, demonstrating the\nfastest on-the-fly training, superior representation quality, and real-time\nrendering capability. Project page: https://www.liuzhening.top/DASS",
      "tldr_zh": "该论文针对 4D 动态空间重建的问题，提出了一种名为 Dynamics-Aware Gaussian Splatting Streaming 的新方法，以实现快速在线训练和实时流式处理。方法采用三阶段管道，包括 selective inheritance 阶段来保持时序连续性、dynamics-aware shift 阶段来区分并优化动态和静态 Gaussian primitives，以及 error-guided densification 阶段来处理新兴对象。实验结果显示，该方法在在线 4D reconstruction 中达到最先进性能，提供最快的 on-the-fly 训练、最优的表示质量和实时渲染能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://www.liuzhening.top/DASS",
      "pdf_url": "http://arxiv.org/pdf/2411.14847v2",
      "published_date": "2024-11-22 10:47:47 UTC",
      "updated_date": "2025-03-27 13:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:25:10.399074"
    },
    {
      "arxiv_id": "2411.15263v1",
      "title": "AI-Driven Real-Time Monitoring of Ground-Nesting Birds: A Case Study on Curlew Detection Using YOLOv10",
      "title_zh": "翻译失败",
      "authors": [
        "Carl Chalmers",
        "Paul Fergus",
        "Serge Wich",
        "Steven N Longmore",
        "Naomi Davies Walsh",
        "Lee Oliver",
        "James Warrington",
        "Julieanne Quinlan",
        "Katie Appleby"
      ],
      "abstract": "Effective monitoring of wildlife is critical for assessing biodiversity and\necosystem health, as declines in key species often signal significant\nenvironmental changes. Birds, particularly ground-nesting species, serve as\nimportant ecological indicators due to their sensitivity to environmental\npressures. Camera traps have become indispensable tools for monitoring nesting\nbird populations, enabling data collection across diverse habitats. However,\nthe manual processing and analysis of such data are resource-intensive, often\ndelaying the delivery of actionable conservation insights. This study presents\nan AI-driven approach for real-time species detection, focusing on the curlew\n(Numenius arquata), a ground-nesting bird experiencing significant population\ndeclines. A custom-trained YOLOv10 model was developed to detect and classify\ncurlews and their chicks using 3/4G-enabled cameras linked to the Conservation\nAI platform. The system processes camera trap data in real-time, significantly\nenhancing monitoring efficiency. Across 11 nesting sites in Wales, the model\nachieved high performance, with a sensitivity of 90.56%, specificity of 100%,\nand F1-score of 95.05% for curlew detections, and a sensitivity of 92.35%,\nspecificity of 100%, and F1-score of 96.03% for curlew chick detections. These\nresults demonstrate the capability of AI-driven monitoring systems to deliver\naccurate, timely data for biodiversity assessments, facilitating early\nconservation interventions and advancing the use of technology in ecological\nresearch.",
      "tldr_zh": "这篇论文提出了一种AI驱动的实时监测系统，使用YOLOv10模型对地巢鸟类（如鹬鸟Numenius arquata）进行检测和分类，旨在解决传统相机陷阱数据手动处理耗时的难题。研究在威尔士11个巢址测试了该系统，鹬鸟检测的灵敏度达到90.56%、特异性100%、F1-score 95.05%，而鹬鸟幼鸟检测的灵敏度为92.35%、特异性100%、F1-score 96.03%。这一方法显著提升了生物多样性评估的效率和准确性，促进早期保护干预和生态研究技术的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15263v1",
      "published_date": "2024-11-22 10:36:29 UTC",
      "updated_date": "2024-11-22 10:36:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:25:21.915210"
    },
    {
      "arxiv_id": "2411.14842v1",
      "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wanqi Yang",
        "Yanda Li",
        "Meng Fang",
        "Yunchao Wei",
        "Tianyi Zhou",
        "Ling Chen"
      ],
      "abstract": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience.",
      "tldr_zh": "这篇论文引入了 Chat-Audio Attacks (CAA) 基准测试，包括四种音频攻击类型，用于评估大型语言模型 (LLMs) 在对话场景中对音频攻击的脆弱性。研究者提出了三种评估策略：Standard Evaluation（使用传统指标量化模型性能）、GPT-4o-Based Evaluation（模拟真实对话复杂性）和 Human Evaluation（分析用户感知与信任）。通过测试六个先进的 LLMs（如 Gemini-1.5-Pro 和 GPT-4o），结果显示 GPT-4o 表现出最高的韧性，为提升 LLMs 在语音交互中的鲁棒性提供了重要洞见。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14842v1",
      "published_date": "2024-11-22 10:30:48 UTC",
      "updated_date": "2024-11-22 10:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:25:33.994918"
    },
    {
      "arxiv_id": "2411.14832v1",
      "title": "VisGraphVar: A Benchmark Generator for Assessing Variability in Graph Analysis Using Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Camilo Chacón Sartori",
        "Christian Blum",
        "Filippo Bistaffa"
      ],
      "abstract": "The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.",
      "tldr_zh": "本研究引入了VisGraphVar，一种可自定义的基准生成器，用于评估大型视觉语言模型(LVLMs)在图形分析中的变异性表现。VisGraphVar能生成990张图形图像，涵盖七个任务类别（检测、分类、分割、模式识别、链接预测、推理和匹配），并通过零-shot和chain-of-thought提示策略评估六种LVLMs。结果显示，图像的视觉属性变化（如节点标签和布局）以及视觉缺陷（如重叠节点）会显著降低模型性能，强调了需要更全面的图形任务评估，以推动开发更可靠的视觉图形分析系统。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "68T50"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14832v1",
      "published_date": "2024-11-22 10:10:53 UTC",
      "updated_date": "2024-11-22 10:10:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:25:45.054640"
    },
    {
      "arxiv_id": "2411.14827v1",
      "title": "Physically Interpretable Probabilistic Domain Characterization",
      "title_zh": "翻译失败",
      "authors": [
        "Anaïs Halin",
        "Sébastien Piérard",
        "Renaud Vandeghen",
        "Benoît Gérin",
        "Maxime Zanella",
        "Martin Colot",
        "Jan Held",
        "Anthony Cioppa",
        "Emmanuel Jean",
        "Gianluca Bontempi",
        "Saïd Mahmoudi",
        "Benoît Macq",
        "Marc Van Droogenbroeck"
      ],
      "abstract": "Characterizing domains is essential for models analyzing dynamic\nenvironments, as it allows them to adapt to evolving conditions or to hand the\ntask over to backup systems when facing conditions outside their operational\ndomain. Existing solutions typically characterize a domain by solving a\nregression or classification problem, which limits their applicability as they\nonly provide a limited summarized description of the domain. In this paper, we\npresent a novel approach to domain characterization by characterizing domains\nas probability distributions. Particularly, we develop a method to predict the\nlikelihood of different weather conditions from images captured by\nvehicle-mounted cameras by estimating distributions of physical parameters\nusing normalizing flows. To validate our proposed approach, we conduct\nexperiments within the context of autonomous vehicles, focusing on predicting\nthe distribution of weather parameters to characterize the operational domain.\nThis domain is characterized by physical parameters (absolute characterization)\nand arbitrarily predefined domains (relative characterization). Finally, we\nevaluate whether a system can safely operate in a target domain by comparing it\nto multiple source domains where safety has already been established. This\napproach holds significant potential, as accurate weather prediction and\neffective domain adaptation are crucial for autonomous systems to adjust to\ndynamic environmental conditions.",
      "tldr_zh": "本论文提出了一种将领域表征为概率分布的新方法，以解决现有回归或分类方法在动态环境分析中的局限性，该方法通过估计物理参数的分布来提供更全面的描述。作者使用 normalizing flows 技术，从车辆安装摄像头的图像中预测不同天气条件的可能性，从而实现对操作领域的绝对表征（基于物理参数）和相对表征（与预定义领域比较）。实验在自主车辆场景中验证了这一方法，能够通过比较目标领域与已知安全的源领域来评估系统安全操作潜力，从而提升自主系统的环境适应能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14827v1",
      "published_date": "2024-11-22 10:07:02 UTC",
      "updated_date": "2024-11-22 10:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:25:57.684844"
    },
    {
      "arxiv_id": "2411.15260v1",
      "title": "VIVID-10M: A Dataset and Baseline for Versatile and Interactive Video Local Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahao Hu",
        "Tianxiong Zhong",
        "Xuebo Wang",
        "Boyuan Jiang",
        "Xingye Tian",
        "Fei Yang",
        "Pengfei Wan",
        "Di Zhang"
      ],
      "abstract": "Diffusion-based image editing models have made remarkable progress in recent\nyears. However, achieving high-quality video editing remains a significant\nchallenge. One major hurdle is the absence of open-source, large-scale video\nediting datasets based on real-world data, as constructing such datasets is\nboth time-consuming and costly. Moreover, video data requires a significantly\nlarger number of tokens for representation, which substantially increases the\ntraining costs for video editing models. Lastly, current video editing models\noffer limited interactivity, often making it difficult for users to express\ntheir editing requirements effectively in a single attempt. To address these\nchallenges, this paper introduces a dataset VIVID-10M and a baseline model\nVIVID. VIVID-10M is the first large-scale hybrid image-video local editing\ndataset aimed at reducing data construction and model training costs, which\ncomprises 9.7M samples that encompass a wide range of video editing tasks.\nVIVID is a Versatile and Interactive VIdeo local eDiting model trained on\nVIVID-10M, which supports entity addition, modification, and deletion. At its\ncore, a keyframe-guided interactive video editing mechanism is proposed,\nenabling users to iteratively edit keyframes and propagate it to other frames,\nthereby reducing latency in achieving desired outcomes. Extensive experimental\nevaluations show that our approach achieves state-of-the-art performance in\nvideo local editing, surpassing baseline methods in both automated metrics and\nuser studies. The VIVID-10M dataset and the VIVID editing model will be\navailable at \\url{https://inkosizhong.github.io/VIVID/}.",
      "tldr_zh": "这篇论文介绍了 VIVID-10M 数据集和 VIVID 模型，以解决视频编辑领域的挑战，如缺乏大规模开源数据集和高训练成本。VIVID-10M 是一个包含 9.7M 样本的混合图像-视频本地编辑数据集，涵盖多种编辑任务，帮助降低数据构建和模型训练开销。VIVID 模型支持实体的添加、修改和删除，通过关键帧引导的交互式机制允许用户迭代编辑关键帧并传播到其他帧，从而减少延迟。实验结果显示，VIVID 在视频本地编辑任务中达到 state-of-the-art 性能，在自动化指标和用户研究中优于基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15260v1",
      "published_date": "2024-11-22 10:04:05 UTC",
      "updated_date": "2024-11-22 10:04:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:26:09.714219"
    },
    {
      "arxiv_id": "2411.17731v1",
      "title": "Soil Characterization of Watermelon Field through Internet of Things: A New Approach to Soil Salinity Measurement",
      "title_zh": "翻译失败",
      "authors": [
        "Md. Naimur Rahman",
        "Shafak Shahriar Sozol",
        "Md. Samsuzzaman",
        "Md. Shahin Hossin",
        "Mohammad Tariqul Islam",
        "S. M. Taohidul Islam",
        "Md. Maniruzzaman"
      ],
      "abstract": "In the modern agricultural industry, technology plays a crucial role in the\nadvancement of cultivation. To increase crop productivity, soil require some\nspecific characteristics. For watermelon cultivation, soil needs to be sandy\nand of high temperature with proper irrigation. This research aims to design\nand implement an intelligent IoT-based soil characterization system for the\nwatermelon field to measure the soil characteristics. IoT based developed\nsystem measures moisture, temperature, and pH of soil using different sensors,\nand the sensor data is uploaded to the cloud via Arduino and Raspberry Pi, from\nwhere users can obtain the data using mobile application and webpage developed\nfor this system. To ensure the precision of the framework, this study includes\nthe comparison between the readings of the soil parameters by the existing\nfield soil meters, the values obtained from the sensors integrated IoT system,\nand data obtained from soil science laboratory. Excessive salinity in soil\naffects the watermelon yield. This paper proposes a model for the measurement\nof soil salinity based on soil resistivity. It establishes a relationship\nbetween soil salinity and soil resistivity from the data obtained in the\nlaboratory using artificial neural network (ANN).",
      "tldr_zh": "这篇论文提出了一种基于 Internet of Things (IoT) 的智能系统，用于西瓜田土壤特征的测量，旨在提升作物产量并确保土壤适合西瓜种植（如沙质、高温和适当灌溉）。系统利用传感器测量土壤的湿度、温度和 pH，并通过 Arduino 和 Raspberry Pi 将数据上传到云端，用户可通过移动应用或网页实时获取。论文还开发了一种新方法，通过土壤电阻率测量土壤盐度，并使用 artificial neural network (ANN) 建立盐度和电阻率之间的关系，以评估盐度对西瓜产量的影响。实验结果显示，该系统与现有现场土壤仪和实验室数据相比具有较高的精度，为现代农业提供了可靠的土壤监测解决方案。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.17731v1",
      "published_date": "2024-11-22 09:54:29 UTC",
      "updated_date": "2024-11-22 09:54:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:26:22.675886"
    },
    {
      "arxiv_id": "2411.15257v1",
      "title": "The Explabox: Model-Agnostic Machine Learning Transparency & Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Marcel Robeer",
        "Michiel Bron",
        "Elize Herrewijnen",
        "Riwish Hoeseni",
        "Floris Bex"
      ],
      "abstract": "We present the Explabox: an open-source toolkit for transparent and\nresponsible machine learning (ML) model development and usage. Explabox aids in\nachieving explainable, fair and robust models by employing a four-step\nstrategy: explore, examine, explain and expose. These steps offer\nmodel-agnostic analyses that transform complex 'ingestibles' (models and data)\ninto interpretable 'digestibles'. The toolkit encompasses digestibles for\ndescriptive statistics, performance metrics, model behavior explanations (local\nand global), and robustness, security, and fairness assessments. Implemented in\nPython, Explabox supports multiple interaction modes and builds on open-source\npackages. It empowers model developers and testers to operationalize\nexplainability, fairness, auditability, and security. The initial release\nfocuses on text data and models, with plans for expansion. Explabox's code and\ndocumentation are available open-source at https://explabox.readthedocs.io/.",
      "tldr_zh": "我们介绍了 Explabox，这是一个开源工具包，旨在实现模型无关的机器学习透明度和负责任的模型开发。Explabox 通过四步策略（explore、examine、explain 和 expose）将复杂的模型和数据转化为可解释的输出，包括描述性统计、性能指标、局部和全局模型行为解释，以及鲁棒性、安全性和公平性评估。基于 Python 实现并支持多种交互模式，该工具专注于文本数据和模型，并计划未来扩展，以帮助开发者操作化 explainability、fairness、auditability 和 security。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE",
        "I.2; D.2.5"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15257v1",
      "published_date": "2024-11-22 09:10:57 UTC",
      "updated_date": "2024-11-22 09:10:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:26:34.081608"
    },
    {
      "arxiv_id": "2411.14808v2",
      "title": "High-Resolution Image Synthesis via Next-Token Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Dengsheng Chen",
        "Jie Hu",
        "Tiezhu Yue",
        "Xiaoming Wei",
        "Enhua Wu"
      ],
      "abstract": "Recently, autoregressive models have demonstrated remarkable performance in\nclass-conditional image generation. However, the application of next-token\nprediction to high-resolution text-to-image generation remains largely\nunexplored. In this paper, we introduce \\textbf{D-JEPA$\\cdot$T2I}, an\nautoregressive model based on continuous tokens that incorporates innovations\nin both architecture and training strategy to generate high-quality,\nphotorealistic images at arbitrary resolutions, up to 4K. Architecturally, we\nadopt the denoising joint embedding predictive architecture (D-JEPA) while\nleveraging a multimodal visual transformer to effectively integrate textual and\nvisual features. Additionally, we introduce flow matching loss alongside the\nproposed Visual Rotary Positional Embedding (VoPE) to enable continuous\nresolution learning. In terms of training strategy, we propose a data feedback\nmechanism that dynamically adjusts the sampling procedure based on statistical\nanalysis and an online learning critic model. This encourages the model to move\nbeyond its comfort zone, reducing redundant training on well-mastered scenarios\nand compelling it to address more challenging cases with suboptimal generation\nquality. For the first time, we achieve state-of-the-art high-resolution image\nsynthesis via next-token prediction.",
      "tldr_zh": "本研究提出 D-JEPA·T2I，一种基于 next-token prediction 的 autoregressive 模型，用于实现高分辨率文本到图像生成，支持高达 4K 的任意分辨率。模型架构采用 denoising joint embedding predictive architecture (D-JEPA) 和 multimodal visual transformer 来整合文本与视觉特征，同时引入 flow matching loss 和 Visual Rotary Positional Embedding (VoPE) 以支持连续分辨率学习。训练策略通过 data feedback 机制动态调整采样过程，利用统计分析和在线学习 critic 模型，鼓励模型处理更具挑战性的生成任务。最终，该方法首次在 next-token prediction 框架下实现了 state-of-the-art 的高分辨率图像合成，显著提升了生成质量和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.14808v2",
      "published_date": "2024-11-22 09:08:58 UTC",
      "updated_date": "2025-03-02 08:53:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:26:46.608405"
    },
    {
      "arxiv_id": "2411.14797v1",
      "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Zhu",
        "Yu Wang",
        "Yanpeng Sun",
        "Qiang Chen",
        "Jiangjiang Liu",
        "Gang Zhang",
        "Jingdong Wang"
      ],
      "abstract": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.",
      "tldr_zh": "该论文观察到多模态 RLHF 的核心价值在于其负监督（rejected responses 的 logit），并提出了一种新型负监督微调（nSFT）方法，作为对视觉语言模型（VLMs）的持续对齐替代方案。nSFT 通过分离 RLHF 中的负监督信息，使用简单的 SFT 损失进行微调，比传统多模态 RLHF（如 DPO 或 PPO）更内存高效，因为它不需要维护 2 或 4 个大型 VLM。实验结果显示，nSFT 在不同数据集、基线 VLMs 和评估指标上与多模态 RLHF 相当或更优，并通过消融实验支持这一假设，为未来 VLMs 对齐研究提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14797v1",
      "published_date": "2024-11-22 08:48:30 UTC",
      "updated_date": "2024-11-22 08:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:26:58.240738"
    },
    {
      "arxiv_id": "2411.15254v1",
      "title": "A Unified Energy Management Framework for Multi-Timescale Forecasting in Smart Grids",
      "title_zh": "翻译失败",
      "authors": [
        "Dafang Zhao",
        "Xihao Piao",
        "Zheng Chen",
        "Zhengmao Li",
        "Ittetsu Taniguchi"
      ],
      "abstract": "Accurate forecasting of the electrical load, such as the magnitude and the\ntiming of peak power, is crucial to successful power system management and\nimplementation of smart grid strategies like demand response and peak shaving.\nIn multi-time-scale optimization scheduling, rolling optimization is a common\nsolution. However, rolling optimization needs to consider the coupling of\ndifferent optimization objectives across time scales. It is challenging to\naccurately capture the mid- and long-term dependencies in time series data.\nThis paper proposes Multi-pofo, a multi-scale power load forecasting framework,\nthat captures such dependency via a novel architecture equipped with a temporal\npositional encoding layer. To validate the effectiveness of the proposed model,\nwe conduct experiments on real-world electricity load data. The experimental\nresults show that our approach outperforms compared to several strong baseline\nmethods.",
      "tldr_zh": "本论文提出一个统一的能源管理框架，用于智能电网的多时间尺度预测，旨在解决电力负载（如峰值功率的大小和时间）预测的准确性问题，尤其是在滚动优化中处理不同时间尺度优化目标的耦合挑战。框架名为 Multi-pofo，采用新型架构并配备 temporal positional encoding layer，以准确捕捉时间序列数据中的中长期依赖性。通过在真实世界电力负载数据上的实验验证，该方法比几个强基线模型表现出色，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to PES GM 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.15254v1",
      "published_date": "2024-11-22 08:45:41 UTC",
      "updated_date": "2024-11-22 08:45:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:27:09.804966"
    },
    {
      "arxiv_id": "2411.15252v1",
      "title": "LocRef-Diffusion:Tuning-Free Layout and Appearance-Guided Generation",
      "title_zh": "LocRef-Diffusion：无需调优的布局和外观引导生成",
      "authors": [
        "Fan Deng",
        "Yaguang Wu",
        "Xinyang Yu",
        "Xiangjun Huang",
        "Jian Yang",
        "Guangyu Yan",
        "Qiang Xu"
      ],
      "abstract": "Recently, text-to-image models based on diffusion have achieved remarkable\nsuccess in generating high-quality images. However, the challenge of\npersonalized, controllable generation of instances within these images remains\nan area in need of further development. In this paper, we present\nLocRef-Diffusion, a novel, tuning-free model capable of personalized\ncustomization of multiple instances' appearance and position within an image.\nTo enhance the precision of instance placement, we introduce a Layout-net,\nwhich controls instance generation locations by leveraging both explicit\ninstance layout information and an instance region cross-attention module. To\nimprove the appearance fidelity to reference images, we employ an\nappearance-net that extracts instance appearance features and integrates them\ninto the diffusion model through cross-attention mechanisms. We conducted\nextensive experiments on the COCO and OpenImages datasets, and the results\ndemonstrate that our proposed method achieves state-of-the-art performance in\nlayout and appearance guided generation.",
      "tldr_zh": "该论文提出了 LocRef-Diffusion，一种无需微调（tuning-free）的模型，用于个性化控制图像中多个实例的外观和位置。模型引入 Layout-net，通过显式实例布局信息和实例区域 cross-attention 模块来精确管理实例生成位置；同时，appearance-net 提取参考图像的外观特征，并通过 cross-attention 机制整合到 diffusion model 中，提升生成图像的保真度。在 COCO 和 OpenImages 数据集上的广泛实验显示，该方法在布局和外观引导生成任务中达到了 state-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15252v1",
      "published_date": "2024-11-22 08:44:39 UTC",
      "updated_date": "2024-11-22 08:44:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:27:22.325844"
    },
    {
      "arxiv_id": "2411.15251v1",
      "title": "Optimized Vessel Segmentation: A Structure-Agnostic Approach with Small Vessel Enhancement and Morphological Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Dongning Song",
        "Weijian Huang",
        "Jiarun Liu",
        "Md Jahidul Islam",
        "Hao Yang",
        "Shanshan Wang"
      ],
      "abstract": "Accurate segmentation of blood vessels is essential for various clinical\nassessments and postoperative analyses. However, the inherent challenges of\nvascular imaging, such as sparsity, fine granularity, low contrast, data\ndistribution variability, and the critical need for preserving topological\nstructure, making generalized vessel segmentation particularly complex. While\nspecialized segmentation methods have been developed for specific anatomical\nregions, their over-reliance on tailored models hinders broader applicability\nand generalization. General-purpose segmentation models introduced in medical\nimaging often fail to address critical vascular characteristics, including the\nconnectivity of segmentation results. To overcome these limitations, we propose\nan optimized vessel segmentation framework: a structure-agnostic approach\nincorporating small vessel enhancement and morphological correction for\nmulti-modality vessel segmentation. To train and validate this framework, we\ncompiled a comprehensive multi-modality dataset spanning 17 datasets and\nbenchmarked our model against six SAM-based methods and 17 expert models. The\nresults demonstrate that our approach achieves superior segmentation accuracy,\ngeneralization, and a 34.6% improvement in connectivity, underscoring its\nclinical potential. An ablation study further validates the effectiveness of\nthe proposed improvements. We will release the code and dataset at github\nfollowing the publication of this work.",
      "tldr_zh": "该研究针对血管分割面临的挑战（如稀疏性、细粒度、低对比度和拓扑结构保留需求），提出了一种结构无关（structure-agnostic）方法，结合小血管增强（small vessel enhancement）和形态校正（morphological correction），以实现多模态血管分割的优化。研究者编译了一个涵盖17个数据集的综合数据集，并与6个SAM-based方法和17个专家模型进行比较，结果显示该框架在分割精度、泛化能力和连通性上均有显著提升，连通性改善达34.6%。这项工作通过消融研究验证了改进的有效性，并计划开源代码和数据集，以推动临床应用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 7 figurres, submitted to TIP",
      "pdf_url": "http://arxiv.org/pdf/2411.15251v1",
      "published_date": "2024-11-22 08:38:30 UTC",
      "updated_date": "2024-11-22 08:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:27:33.947812"
    },
    {
      "arxiv_id": "2411.14794v1",
      "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Songhao Han",
        "Wei Huang",
        "Hairong Shi",
        "Le Zhuo",
        "Xiu Su",
        "Shifeng Zhang",
        "Xu Zhou",
        "Xiaojuan Qi",
        "Yue Liao",
        "Si Liu"
      ],
      "abstract": "The advancement of Large Vision Language Models (LVLMs) has significantly\nimproved multimodal understanding, yet challenges remain in video reasoning\ntasks due to the scarcity of high-quality, large-scale datasets. Existing video\nquestion-answering (VideoQA) datasets often rely on costly manual annotations\nwith insufficient granularity or automatic construction methods with redundant\nframe-by-frame analysis, limiting their scalability and effectiveness for\ncomplex reasoning. To address these challenges, we introduce VideoEspresso, a\nnovel dataset that features VideoQA pairs preserving essential spatial details\nand temporal coherence, along with multimodal annotations of intermediate\nreasoning steps. Our construction pipeline employs a semantic-aware method to\nreduce redundancy, followed by generating QA pairs using GPT-4o. We further\ndevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,\nguiding GPT-4o in extracting logical relationships from QA pairs and video\ncontent. To exploit the potential of high-quality VideoQA pairs, we propose a\nHybrid LVLMs Collaboration framework, featuring a Frame Selector and a\ntwo-stage instruction fine-tuned reasoning LVLM. This framework adaptively\nselects core frames and performs CoT reasoning using multimodal evidence.\nEvaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our\nmethod outperforms existing baselines on most tasks, demonstrating superior\nvideo reasoning capabilities. Our code and dataset will be released at:\nhttps://github.com/hshjerry/VideoEspresso",
      "tldr_zh": "本研究介绍了 VideoEspresso，一种大规模 Chain-of-Thought (CoT) 数据集，旨在提升 Large Vision Language Models (LVLMs) 在视频推理任务中的细粒度性能，通过核心帧选择减少冗余。数据集构建管道采用语义感知方法生成 VideoQA 对，并利用 GPT-4o 结合多模态注解创建中间推理步骤，确保保留空间细节和时间连贯性。研究提出 Hybrid LVLMs Collaboration 框架，包括 Frame Selector 和两阶段指令微调的推理 LVLM，能够自适应选择核心帧并进行 CoT 推理。实验结果显示，该框架在 14 个任务上优于 9 个基线模型，证明了其在视频推理方面的卓越能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14794v1",
      "published_date": "2024-11-22 08:33:36 UTC",
      "updated_date": "2024-11-22 08:33:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:27:46.573938"
    },
    {
      "arxiv_id": "2411.15250v1",
      "title": "TPLogAD: Unsupervised Log Anomaly Detection Based on Event Templates and Key Parameters",
      "title_zh": "TPLogAD：基于事件模板和关键参数的无监督日志异常检测",
      "authors": [
        "Jiawei Lu",
        "Chengrong Wu"
      ],
      "abstract": "Log-system is an important mechanism for recording the runtime status and\nevents of Web service systems, and anomaly detection in logs is an effective\nmethod of detecting problems. However, manual anomaly detection in logs is\ninefficient, error-prone, and unrealistic. Existing log anomaly detection\nmethods either use the indexes of event templates, or form vectors by embedding\nthe fixed string part of the template as a sentence, or use time parameters for\nsequence analysis. However, log entries often contain features and semantic\ninformation that cannot be fully represented by these methods, resulting in\nmissed and false alarms. In this paper, we propose TPLogAD, a universal\nunsupervised method for analyzing unstructured logs, which performs anomaly\ndetection based on event templates and key parameters. The itemplate2vec and\npara2vec included in TPLogAD are two efficient and easy-to-implement semantic\nrepresentation methods for logs, detecting anomalies in event templates and\nparameters respectively, which has not been achieved in previous work.\nAdditionally, TPLogAD can avoid the interference of log diversity and dynamics\non anomaly detection. Our experiments on four public log datasets show that\nTPLogAD outperforms existing log anomaly detection methods.",
      "tldr_zh": "本研究提出 TPLogAD，一种无监督日志异常检测方法，针对现有方法的不足（如仅依赖事件模板索引或时间参数分析，导致漏报和误报），通过整合 Event Templates 和 Key Parameters 进行更全面的语义分析。TPLogAD 包括 itemplate2vec 和 para2vec 两种高效语义表示技术，分别检测事件模板和参数的异常，并有效避免日志多样性和动态性的干扰，这是之前工作未实现的创新点。在四个公共日志数据集上的实验表明，TPLogAD 优于现有方法，显著提高了检测准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15250v1",
      "published_date": "2024-11-22 08:25:21 UTC",
      "updated_date": "2024-11-22 08:25:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:29:58.661981"
    },
    {
      "arxiv_id": "2411.14790v4",
      "title": "KBAlign: Efficient Self Adaptation on Specific Knowledge Bases",
      "title_zh": "KBAlign：针对特定知识库的高效自我适应",
      "authors": [
        "Zheni Zeng",
        "Yuxuan Chen",
        "Shi Yu",
        "Ruobing Wang",
        "Yukun Yan",
        "Zhenghao Liu",
        "Shuo Wang",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Although retrieval-augmented generation (RAG) remains essential for\nknowledge-based question answering (KBQA), current paradigms face critical\nchallenges under specific domains. Existing methods struggle with targeted\nadaptation on small-scale KBs: vanilla unsupervised training exhibits poor\neffectiveness, while fine-tuning incurs prohibitive costs of external signals.\nWe present KBAlign, a self-supervised framework that enhances RAG systems\nthrough efficient model adaptation. Our key insight is to leverage the model's\nintrinsic capabilities for knowledge alignment through two innovative\nmechanisms: multi-grained self-annotation that captures global knowledge for\ndata construction, and iterative tuning that accelerates convergence through\nself verification. This framework enables cost-effective model adaptation to\nspecific textual KBs, without human supervision or external model assistance.\nExperiments demonstrate that KBAlign can achieve 90\\% of the performance gain\nobtained through GPT-4-supervised adaptation, while relying entirely on\nself-annotation of much smaller models. KBAlign significantly improves\ndownstream QA accuracy across multiple domains with tiny costs, particularly\nbenefiting scenarios requiring deep knowledge integration from specialized\ncorpora. We release our experimental data, models, and process analyses to the\ncommunity for further exploration (https://github.com/thunlp/KBAlign).",
      "tldr_zh": "该研究提出 KBAlign，一种高效的自监督框架，用于提升检索增强生成（RAG）系统在特定知识库（KBs）上的适应能力，解决现有方法在小规模领域中的效果差和成本高问题。KBAlign 通过多粒度自标注机制捕捉全局知识以构建数据，以及迭代调优机制通过自验证加速模型收敛，从而实现无需人工监督或外部模型的成本有效适应。实验显示，KBAlign 能获得 GPT-4 监督适应 90% 的性能提升，仅依赖小型模型的自标注，并在多个领域显著提高问答准确率，尤其适用于深度知识集成的专业语料场景。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14790v4",
      "published_date": "2024-11-22 08:21:03 UTC",
      "updated_date": "2025-05-15 13:02:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:28:09.632030"
    },
    {
      "arxiv_id": "2411.14774v2",
      "title": "Resolution-Agnostic Transformer-based Climate Downscaling",
      "title_zh": "翻译失败",
      "authors": [
        "Declan Curran",
        "Hira Saleem",
        "Sanaa Hobeichi",
        "Flora Salim"
      ],
      "abstract": "Understanding future weather changes at regional and local scales is crucial\nfor planning and decision-making, particularly in the context of extreme\nweather events, as well as for broader applications in agriculture, insurance,\nand infrastructure development. However, the computational cost of downscaling\nGlobal Climate Models (GCMs) to the fine resolutions needed for such\napplications presents a significant barrier. Drawing on advancements in weather\nforecasting models, this study introduces a cost-efficient downscaling method\nusing a pretrained Earth Vision Transformer (Earth ViT) model. Initially\ntrained on ERA5 data to downscale from 50 km to 25 km resolution, the model is\nthen tested on the higher resolution BARRA-SY dataset at a 3 km resolution.\nRemarkably, it performs well without additional training, demonstrating its\nability to generalize across different resolutions. This approach holds promise\nfor generating large ensembles of regional climate simulations by downscaling\nGCMs with varying input resolutions without incurring additional training\ncosts. Ultimately, this method could provide more comprehensive estimates of\npotential future changes in key climate variables, aiding in effective planning\nfor extreme weather events and climate change adaptation strategies.",
      "tldr_zh": "本文提出了一种基于 Transformer 的分辨率无关气候下放方法，使用预训练 Earth ViT 模型，以降低全球气候模型 (GCMs) 下放至精细分辨率的计算成本。该模型首先在 ERA5 数据上训练，从 50 km 到 25 km 分辨率，然后在 BARRA-SY 数据集的 3 km 分辨率上测试，无需额外训练即可实现良好泛化。实验结果显示，这种方法能高效生成大量区域气候模拟，提供更全面的未来气候变量变化估计，支持极端天气事件规划和气候变化适应策略。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14774v2",
      "published_date": "2024-11-22 07:32:11 UTC",
      "updated_date": "2024-11-27 00:55:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:28:22.115748"
    },
    {
      "arxiv_id": "2411.14773v2",
      "title": "Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Liang",
        "Yi Zeng",
        "Menghaoran Tang"
      ],
      "abstract": "Musical mode is one of the most critical element that establishes the\nframework of pitch organization and determines the harmonic relationships.\nPrevious works often use the simplistic and rigid alignment method, and\noverlook the diversity of modes. However, in contrast to AI models, humans\npossess cognitive mechanisms for perceiving the various modes and keys. In this\npaper, we propose a spiking neural network inspired by brain mechanisms and\npsychological theories to represent musical modes and keys, ultimately\ngenerating musical pieces that incorporate tonality features. Specifically, the\ncontributions are detailed as follows: 1) The model is designed with multiple\ncollaborated subsystems inspired by the structures and functions of\ncorresponding brain regions; 2)We incorporate mechanisms for neural circuit\nevolutionary learning that enable the network to learn and generate\nmode-related features in music, reflecting the cognitive processes involved in\nhuman music perception. 3)The results demonstrate that the proposed model shows\na connection framework closely similar to the Krumhansl-Schmuckler model, which\nis one of the most significant key perception models in the music psychology\ndomain. 4) Experiments show that the model can generate music pieces with\ncharacteristics of the given modes and keys. Additionally, the quantitative\nassessments of generated pieces reveals that the generating music pieces have\nboth tonality characteristics and the melodic adaptability needed to generate\ndiverse and musical content. By combining insights from neuroscience,\npsychology, and music theory with advanced neural network architectures, our\nresearch aims to create a system that not only learns and generates music but\nalso bridges the gap between human cognition and artificial intelligence.",
      "tldr_zh": "本研究提出了一种受神经科学和心理学启发的脉冲神经网络（spiking neural network），用于音乐学习和作曲，专注于处理音乐模式（musical mode）和调式（keys）的多样性，以克服现有AI方法的刚性和局限性。模型设计包括多个协作子系统，模仿脑区结构和功能，并整合神经电路进化学习机制，以学习和生成模式相关特征，反映人类音乐认知过程。实验结果显示，该模型的框架类似于Krumhansl-Schmuckler模型，且能生成具有指定模式和调式特征的音乐片段，这些片段在定量评估中表现出良好的音调特性（tonality characteristics）和旋律适应性，最终桥接了人类认知与人工智能的差距。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "q-bio.NC"
      ],
      "primary_category": "cs.SD",
      "comment": "18 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14773v2",
      "published_date": "2024-11-22 07:29:26 UTC",
      "updated_date": "2025-01-14 05:16:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:30:10.742059"
    },
    {
      "arxiv_id": "2411.14768v1",
      "title": "Grid and Road Expressions Are Complementary for Trajectory Representation Learning",
      "title_zh": "网格和道路表达在轨迹表示学习中是互补的",
      "authors": [
        "Silin Zhou",
        "Shuo Shang",
        "Lisi Chen",
        "Peng Han",
        "Christian S. Jensen"
      ],
      "abstract": "Trajectory representation learning (TRL) maps trajectories to vectors that\ncan be used for many downstream tasks. Existing TRL methods use either grid\ntrajectories, capturing movement in free space, or road trajectories, capturing\nmovement in a road network, as input. We observe that the two types of\ntrajectories are complementary, providing either region and location\ninformation or providing road structure and movement regularity. Therefore, we\npropose a novel multimodal TRL method, dubbed GREEN, to jointly utilize Grid\nand Road trajectory Expressions for Effective representatioN learning. In\nparticular, we transform raw GPS trajectories into both grid and road\ntrajectories and tailor two encoders to capture their respective information.\nTo align the two encoders such that they complement each other, we adopt a\ncontrastive loss to encourage them to produce similar embeddings for the same\nraw trajectory and design a mask language model (MLM) loss to use grid\ntrajectories to help reconstruct masked road trajectories. To learn the final\ntrajectory representation, a dual-modal interactor is used to fuse the outputs\nof the two encoders via cross-attention. We compare GREEN with 7\nstate-of-the-art TRL methods for 3 downstream tasks, finding that GREEN\nconsistently outperforms all baselines and improves the accuracy of the\nbest-performing baseline by an average of 15.99\\%.",
      "tldr_zh": "该论文观察到，轨迹表示学习(TRL)中，网格轨迹(grid trajectories)提供区域和位置信息，而道路轨迹(road trajectories)提供道路结构和运动规律，二者互补。作者提出了一种新颖的多模态TRL方法GREEN，通过将原始GPS轨迹转换为网格和道路轨迹，并使用两个定制编码器分别捕捉其信息。GREEN采用对比损失(contrastive loss)使编码器产生相似嵌入，并通过掩码语言模型损失(MLM loss)让网格轨迹辅助重建道路轨迹，最后利用双模态交互器(dual-modal interactor)通过交叉注意力融合输出。实验结果显示，GREEN在3个下游任务上优于7个最先进基线，平均准确率提升15.99%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by KDD2025(August Cycle)",
      "pdf_url": "http://arxiv.org/pdf/2411.14768v1",
      "published_date": "2024-11-22 07:15:46 UTC",
      "updated_date": "2024-11-22 07:15:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:30:22.458597"
    },
    {
      "arxiv_id": "2411.14762v4",
      "title": "Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Huiwon Jang",
        "Sihyun Yu",
        "Jinwoo Shin",
        "Pieter Abbeel",
        "Younggyo Seo"
      ],
      "abstract": "Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.",
      "tldr_zh": "本文提出CoordTok，一种基于coordinate-based patch reconstruction的视频tokenizer，用于高效处理长视频的tokenization问题。该方法将视频编码成factorized triplane representations，并通过随机采样(x,y,t)坐标重建对应的patches，从而在长视频上训练大型模型时显著降低资源需求。实验结果显示，CoordTok可以将128帧分辨率为128×128的视频tokens数量从基线模型的6144或8192减少到1280，同时保持相似的重建质量，并支持memory-efficient的diffusion transformer训练，以一次性生成128帧视频。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available on the project webpage:\n  https://huiwon-jang.github.io/coordtok/",
      "pdf_url": "http://arxiv.org/pdf/2411.14762v4",
      "published_date": "2024-11-22 06:50:44 UTC",
      "updated_date": "2025-04-03 02:29:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:30:35.149305"
    },
    {
      "arxiv_id": "2411.14759v1",
      "title": "Hammer: Towards Efficient Hot-Cold Data Identification via Online Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Lu",
        "Siqi Zhao",
        "Jiguang Wan"
      ],
      "abstract": "Efficient management of storage resources in big data and cloud computing\nenvironments requires accurate identification of data's \"cold\" and \"hot\"\nstates. Traditional methods, such as rule-based algorithms and early AI\ntechniques, often struggle with dynamic workloads, leading to low accuracy,\npoor adaptability, and high operational overhead. To address these issues, we\npropose a novel solution based on online learning strategies. Our approach\ndynamically adapts to changing data access patterns, achieving higher accuracy\nand lower operational costs. Rigorous testing with both synthetic and\nreal-world datasets demonstrates a significant improvement, achieving a 90%\naccuracy rate in hot-cold classification. Additionally, the computational and\nstorage overheads are considerably reduced.",
      "tldr_zh": "该论文针对大数据和云计算环境中存储资源管理的问题，提出了一种名为 Hammer 的新方法，通过在线学习（online learning）策略来高效识别数据的“hot”和“cold”状态，以应对动态工作负载的挑战。Hammer 框架能够动态适应变化的数据访问模式，提高分类准确性和降低操作开销。实验结果显示，该方法在合成和真实数据集上实现了90%的热-冷分类准确率，同时显著减少了计算和存储开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14759v1",
      "published_date": "2024-11-22 06:46:16 UTC",
      "updated_date": "2024-11-22 06:46:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:30:45.159745"
    },
    {
      "arxiv_id": "2411.14751v1",
      "title": "TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior",
      "title_zh": "TopoSD：基于 SDMap 先验的拓扑增强车道段感知",
      "authors": [
        "Sen Yang",
        "Minyue Jiang",
        "Ziwei Fan",
        "Xiaolu Xie",
        "Xiao Tan",
        "Yingying Li",
        "Errui Ding",
        "Liang Wang",
        "Jingdong Wang"
      ],
      "abstract": "Recent advances in autonomous driving systems have shifted towards reducing\nreliance on high-definition maps (HDMaps) due to the huge costs of annotation\nand maintenance. Instead, researchers are focusing on online vectorized HDMap\nconstruction using on-board sensors. However, sensor-only approaches still face\nchallenges in long-range perception due to the restricted views imposed by the\nmounting angles of onboard cameras, just as human drivers also rely on\nbird's-eye-view navigation maps for a comprehensive understanding of road\nstructures. To address these issues, we propose to train the perception model\nto \"see\" standard definition maps (SDMaps). We encode SDMap elements into\nneural spatial map representations and instance tokens, and then incorporate\nsuch complementary features as prior information to improve the bird's eye view\n(BEV) feature for lane geometry and topology decoding. Based on the lane\nsegment representation framework, the model simultaneously predicts lanes,\ncentrelines and their topology. To further enhance the ability of geometry\nprediction and topology reasoning, we also use a topology-guided decoder to\nrefine the predictions by exploiting the mutual relationships between\ntopological and geometric features. We perform extensive experiments on\nOpenLane-V2 datasets to validate the proposed method. The results show that our\nmodel outperforms state-of-the-art methods by a large margin, with gains of\n+6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that\nmodels trained with SDMap noise augmentation exhibit enhanced robustness.",
      "tldr_zh": "本论文提出TopoSD框架，利用SDMap先验信息增强自动驾驶系统的车道段感知，旨在解决传感器-only方法在远距离感知中的视野限制问题。\n该框架将SDMap元素编码成神经空间表示和实例标记，并整合到BEV特征中，同时使用拓扑引导解码器来优化车道几何、拓扑预测及其相互关系。\n实验结果显示，在OpenLane-V2数据集上，TopoSD在mAP和拓扑指标上分别比最先进方法提升6.7和9.1分，并通过SDMap噪声增强训练显著提高了模型的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 7 figures, and 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.14751v1",
      "published_date": "2024-11-22 06:13:42 UTC",
      "updated_date": "2024-11-22 06:13:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:30:59.513926"
    },
    {
      "arxiv_id": "2412.06793v1",
      "title": "FEAD: Figma-Enhanced App Design Framework for Improving UI/UX in Educational App Development",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyi Huang"
      ],
      "abstract": "Designing user-centric mobile applications is increasingly essential in\neducational technology. However, platforms like MIT App Inventor-one of the\nworld's largest educational app development tools-face inherent limitations in\nsupporting modern UI/UX design. This study introduces the Figma-Enhanced App\nDesign (FEAD) Method, a structured framework that integrates Figma's advanced\ndesign tools into MIT App Inventor using an identify-design-implement workflow.\nLeveraging principles such as the 8-point grid system and Gestalt laws of\nperception, the FEAD Method empowers users to address design gaps, creating\nvisually appealing, functional, and accessible applications. A comparative\nevaluation revealed that 61.2% of participants perceived FEAD-enhanced designs\nas on par with professional apps, compared to just 8.2% for baseline designs.\nThese findings highlight the potential of bridging design with development\nplatforms to enhance app creation, offering a scalable framework for students\nto master both functional and aesthetic design principles and excel in shaping\nthe future of user-centric technology.",
      "tldr_zh": "本研究针对教育应用开发中 MIT App Inventor 的 UI/UX 设计局限性，提出 FEAD（Figma-Enhanced App Design）框架，该框架通过 identify-design-implement 工作流将 Figma 的高级设计工具整合进来，并应用 8-point grid system 和 Gestalt laws of perception 等原则来提升应用的视觉吸引力和功能性。FEAD 方法使用户能够更有效地桥接设计与开发，帮助创建更专业、可访问的教育应用。在比较评估中，61.2% 的参与者认为 FEAD 增强的设计与专业应用相当，远高于基线设计的 8.2%，从而为学生掌握功能与美学设计原则提供了可扩展的框架。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.06793v1",
      "published_date": "2024-11-22 05:56:53 UTC",
      "updated_date": "2024-11-22 05:56:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:31:10.597908"
    },
    {
      "arxiv_id": "2411.14744v1",
      "title": "Point Cloud Understanding via Attention-Driven Contrastive Learning",
      "title_zh": "通过注意力驱动的对比学习进行点云理解",
      "authors": [
        "Yi Wang",
        "Jiaze Wang",
        "Ziyu Guo",
        "Renrui Zhang",
        "Donghao Zhou",
        "Guangyong Chen",
        "Anfeng Liu",
        "Pheng-Ann Heng"
      ],
      "abstract": "Recently Transformer-based models have advanced point cloud understanding by\nleveraging self-attention mechanisms, however, these methods often overlook\nlatent information in less prominent regions, leading to increased sensitivity\nto perturbations and limited global comprehension. To solve this issue, we\nintroduce PointACL, an attention-driven contrastive learning framework designed\nto address these limitations. Our method employs an attention-driven dynamic\nmasking strategy that guides the model to focus on under-attended regions,\nenhancing the understanding of global structures within the point cloud. Then\nwe combine the original pre-training loss with a contrastive learning loss,\nimproving feature discrimination and generalization. Extensive experiments\nvalidate the effectiveness of PointACL, as it achieves state-of-the-art\nperformance across a variety of 3D understanding tasks, including object\nclassification, part segmentation, and few-shot learning. Specifically, when\nintegrated with different Transformer backbones like Point-MAE and PointGPT,\nPointACL demonstrates improved performance on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart. This highlights its superior capability in\ncapturing both global and local features, as well as its enhanced robustness\nagainst perturbations and incomplete data.",
      "tldr_zh": "本文提出 PointACL，一种基于注意力的对比学习框架，用于解决 Transformer-based 模型在点云理解中忽略不显眼区域信息的问题，从而提高对扰动和全局结构的鲁棒性。PointACL 采用注意力驱动的动态掩码策略结合对比学习损失，增强特征区分和泛化能力。实验结果显示，该框架在物体分类、部分分割和少样本学习等 3D 任务上达到最先进性能，并在 ScanObjectNN、ModelNet40 和 ShapeNetPart 等数据集上超越基线模型，特别是在处理不完整数据时表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14744v1",
      "published_date": "2024-11-22 05:41:00 UTC",
      "updated_date": "2024-11-22 05:41:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:31:23.478815"
    },
    {
      "arxiv_id": "2411.14743v2",
      "title": "FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengrui Guo",
        "Conghao Xiong",
        "Jiabo Ma",
        "Qichen Sun",
        "Lishuang Feng",
        "Jinzhuo Wang",
        "Hao Chen"
      ],
      "abstract": "Few-shot learning presents a critical solution for cancer diagnosis in\ncomputational pathology (CPath), addressing fundamental limitations in data\navailability, particularly the scarcity of expert annotations and patient\nprivacy constraints. A key challenge in this paradigm stems from the inherent\ndisparity between the limited training set of whole slide images (WSIs) and the\nenormous number of contained patches, where a significant portion of these\npatches lacks diagnostically relevant information, potentially diluting the\nmodel's ability to learn and focus on critical diagnostic features. While\nrecent works attempt to address this by incorporating additional knowledge,\nseveral crucial gaps hinder further progress: (1) despite the emergence of\npowerful pathology foundation models (FMs), their potential remains largely\nuntapped, with most approaches limiting their use to basic feature extraction;\n(2) current language guidance mechanisms attempt to align text prompts with\nvast numbers of WSI patches all at once, struggling to leverage rich\npathological semantic information. To this end, we introduce the\nknowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which\nuniquely combines pathology FMs with language prior knowledge to enable a\nfocused analysis of diagnostically relevant regions by prioritizing\ndiscriminative WSI patches. Our approach implements a progressive three-stage\ncompression strategy: we first leverage FMs for global visual redundancy\nelimination, and integrate compressed features with language prompts for\nsemantic relevance assessment, then perform neighbor-aware visual token\nfiltering while preserving spatial coherence. Extensive experiments on\npathological datasets spanning breast, lung, and ovarian cancers demonstrate\nits superior performance in few-shot pathology diagnosis. Codes are available\nat https://github.com/dddavid4real/FOCUS.",
      "tldr_zh": "该论文提出FOCUS框架，用于提升少样本学习（Few-shot learning）在计算病理学（CPath）中的癌症诊断性能，针对全滑微镜图像（WSIs）中冗余补丁导致的模型学习效率低下问题。FOCUS通过整合病理基础模型（FMs）和语言先验知识，实现自适应视觉压缩，包括三阶段策略：首先使用FMs消除全局视觉冗余，然后结合语言提示进行语义相关性评估，最后执行邻域感知视觉标记过滤以保留空间一致性。实验结果显示，该框架在乳腺、肺和卵巢癌症数据集上表现出优越的诊断性能，显著提高了少样本场景下的准确率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR'2025",
      "pdf_url": "http://arxiv.org/pdf/2411.14743v2",
      "published_date": "2024-11-22 05:36:38 UTC",
      "updated_date": "2025-03-20 12:16:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:31:34.302976"
    },
    {
      "arxiv_id": "2411.17729v3",
      "title": "Fast convolution algorithm for state space models",
      "title_zh": "状态空间模型的快速卷积算法",
      "authors": [
        "Gregory Beylkin"
      ],
      "abstract": "We present an unconditionally stable algorithm for applying matrix transfer\nfunction of a linear time invariant system (LTI) in time domain. The state\nmatrix of an LTI system used for modeling long range dependencies in state\nspace models (SSMs) has eigenvalues close to $1$. The standard recursion\ndefining LTI system becomes unstable if the $m\\times m$ state matrix has just\none eigenvalue with absolute value even slightly greater than 1. This may occur\nwhen approximating a state matrix by a structured matrix to reduce the cost of\nmatrix-vector multiplication from $\\mathcal{O}\\left(m^{2}\\right)$ to\n$\\mathcal{O}\\left(m\\right)$ or $\\mathcal{O}\\left(m\\log m\\right).$ We introduce\nan unconditionally stable algorithm that uses an approximation of the rational\ntransfer function in the z-domain by a matrix polynomial of degree $2^{N+1}-1$,\nwhere $N$ is chosen to achieve any user-selected accuracy. Using a cascade\nimplementation in time domain, applying such transfer function to compute $L$\nstates requires no more than $2L$ matrix-vector multiplications (whereas the\nstandard recursion requires $L$ matrix-vector multiplications). However, using\nunconditionally stable algorithm, it is not necessary to assure that an\napproximate state matrix has all eigenvalues with absolute values strictly less\nthan 1 i.e., within the desired accuracy, the absolute value of some\neigenvalues may possibly exceed $1$. Consequently, this algorithm allows one to\nuse a wider variety of structured approximations to reduce the cost of\nmatrix-vector multiplication and we briefly describe several of them to be used\nfor this purpose.",
      "tldr_zh": "该论文提出了一种无条件稳定的算法，用于在时域应用线性时不变系统 (LTI) 的矩阵传递函数，以解决状态空间模型 (SSMs) 中状态矩阵特征值接近 1 时标准递归方法的稳定性问题。算法通过在 z 域中使用一个度为 2^{N+1}-1 的矩阵多项式近似有理传递函数，并采用级联实现，仅需不超过 2L 次矩阵向量乘法来计算 L 个状态。相比传统方法，该算法允许使用更广泛的结构化矩阵近似，将矩阵向量乘法的计算成本从 O(m²) 降低到 O(m) 或 O(m log m)，从而提高了效率和鲁棒性。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.17729v3",
      "published_date": "2024-11-22 05:30:03 UTC",
      "updated_date": "2025-04-11 00:35:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:31:46.646295"
    },
    {
      "arxiv_id": "2411.14740v1",
      "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Yu",
        "Ze Yuan",
        "Yuan-Chen Guo",
        "Ying-Tian Liu",
        "JianHui Liu",
        "Yangguang Li",
        "Yan-Pei Cao",
        "Ding Liang",
        "Xiaojuan Qi"
      ],
      "abstract": "While high-quality texture maps are essential for realistic 3D asset\nrendering, few studies have explored learning directly in the texture space,\nespecially on large-scale datasets. In this work, we depart from the\nconventional approach of relying on pre-trained 2D diffusion models for\ntest-time optimization of 3D textures. Instead, we focus on the fundamental\nproblem of learning in the UV texture space itself. For the first time, we\ntrain a large diffusion model capable of directly generating high-resolution\ntexture maps in a feed-forward manner. To facilitate efficient learning in\nhigh-resolution UV spaces, we propose a scalable network architecture that\ninterleaves convolutions on UV maps with attention layers on point clouds.\nLeveraging this architectural design, we train a 700 million parameter\ndiffusion model that can generate UV texture maps guided by text prompts and\nsingle-view images. Once trained, our model naturally supports various extended\napplications, including text-guided texture inpainting, sparse-view texture\ncompletion, and text-driven texture synthesis. Project page is at\nhttp://cvmi-lab.github.io/TEXGen/.",
      "tldr_zh": "该研究引入了TEXGen，一种直接在UV纹理空间学习的生成扩散模型，旨在高效生成高质量的3D资产纹理地图，而非依赖预训练2D扩散模型的测试时优化。研究提出了一种可扩展的网络架构，将UV地图上的卷积与点云上的注意力层交错，以支持高效的高分辨率学习，并训练了一个7亿参数的扩散模型，能通过文本提示和单视图图像进行前馈式生成。TEXGen不仅能生成纹理地图，还扩展应用于文本引导纹理修复、稀疏视图纹理完成和文本驱动纹理合成，展示了其在3D渲染领域的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to SIGGRAPH Asia Journal Article (TOG 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.14740v1",
      "published_date": "2024-11-22 05:22:11 UTC",
      "updated_date": "2024-11-22 05:22:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:31:58.891958"
    },
    {
      "arxiv_id": "2411.14738v1",
      "title": "Universal and Context-Independent Triggers for Precise Control of LLM Outputs",
      "title_zh": "翻译失败",
      "authors": [
        "Jiashuo Liang",
        "Guancheng Li",
        "Yang Yu"
      ],
      "abstract": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）的输出控制风险，提出了一种泛化gradient-based白盒攻击方法，以发现Universal（通用）、Context-Independent（上下文无关）和Precise Output（精确输出）的触发器(triggers)，从而实现对LLMs输出的高精度操纵。方法通过高效算法生成这些触发器，使其在不同提示上下文中有效，而不受目标输出的限制。实验验证了攻击的有效性，并强调了其对LLM-based应用的潜在威胁，可能导致AI代理决策被对手接管。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14738v1",
      "published_date": "2024-11-22 05:17:18 UTC",
      "updated_date": "2024-11-22 05:17:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:32:10.085699"
    },
    {
      "arxiv_id": "2411.14713v1",
      "title": "LIBER: Lifelong User Behavior Modeling Based on Large Language Models",
      "title_zh": "LIBER：基于大型语言模型的终身用户行为建模",
      "authors": [
        "Chenxu Zhu",
        "Shigang Quan",
        "Bo Chen",
        "Jianghao Lin",
        "Xiaoling Cai",
        "Hong Zhu",
        "Xiangyang Li",
        "Yunjia Xi",
        "Weinan Zhang",
        "Ruiming Tang"
      ],
      "abstract": "CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%.",
      "tldr_zh": "本研究提出LIBER框架，利用Large Language Models (LLMs) 进行终身用户行为建模，以提升推荐系统的CTR预测性能。针对现有模型在处理终身用户行为序列时存在的提取信息困难和动态兴趣捕捉挑战，LIBER包括三个模块：User Behavior Streaming Partition (UBSP) 用于增量式压缩用户行为序列、User Interest Learning (UIL) 通过级联LLMs推断兴趣洞见，以及User Interest Fusion (UIF) 整合输出以生成全面用户表示。实验结果显示，LIBER已在华为音乐推荐服务中部署，提高了用户播放次数3.01%和播放时间7.69%。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14713v1",
      "published_date": "2024-11-22 03:43:41 UTC",
      "updated_date": "2024-11-22 03:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:32:22.201971"
    },
    {
      "arxiv_id": "2411.14708v3",
      "title": "Understanding LLM Embeddings for Regression",
      "title_zh": "理解 LLM 嵌入用于回归",
      "authors": [
        "Eric Tang",
        "Bangding Yang",
        "Xingyou Song"
      ],
      "abstract": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.",
      "tldr_zh": "本论文探讨了使用大型语言模型(LLMs)嵌入进行回归任务的潜力，特别是通过将字符串表示预处理为嵌入特征，以提升下游预测性能。研究发现，LLM 嵌入在高维回归任务中比传统特征工程更有效，这主要因为嵌入能固有地保留Lipschitz连续性。作者还量化了模型大小和语言理解能力的影响，结果显示这些因素并不总是改善回归性能。总的来说，该工作为嵌入-based回归提供了全面调查和实用见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research (TMLR) 2025.\n  Code can be found in https://github.com/google-research/optformer",
      "pdf_url": "http://arxiv.org/pdf/2411.14708v3",
      "published_date": "2024-11-22 03:33:51 UTC",
      "updated_date": "2025-02-15 16:26:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:32:33.928732"
    },
    {
      "arxiv_id": "2411.14698v1",
      "title": "Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Xunyu Zhu",
        "Jian Li",
        "Can Ma",
        "Weiping Wang"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance.",
      "tldr_zh": "本研究针对大语言模型(LLMs)的资源消耗问题，提出Feedback-Driven Distillation (FDD)框架，通过知识蒸馏(knowledge distillation)提升小语言模型(SLMs，参数≤1B)的数学推理能力。FDD框架首先构建蒸馏数据集，由LLMs生成数学问题及其推理理由，并根据SLMs的表现将问题分类为简单或困难类型，然后为简单问题生成更复杂变体，为困难问题合成类似复杂度的全新问题。该框架采用多轮蒸馏范式，迭代丰富数据集，从而逐步增强SLMs的推理能力。实验结果显示，采用FDD的SLMs在数学推理任务上达到了state-of-the-art (SOTA)性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14698v1",
      "published_date": "2024-11-22 03:12:39 UTC",
      "updated_date": "2024-11-22 03:12:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:32:45.747858"
    },
    {
      "arxiv_id": "2411.14696v2",
      "title": "Quantum Hamiltonian Descent for Graph Partition",
      "title_zh": "翻译失败",
      "authors": [
        "Jinglei Cheng",
        "Ruilin Zhou",
        "Yuhang Gan",
        "Chen Qian",
        "Junyu Liu"
      ],
      "abstract": "We introduce Quantum Hamiltonian Descent as a novel approach to solve the\ngraph partition problem. By reformulating graph partition as a Quadratic\nUnconstrained Binary Optimization (QUBO) problem, we leverage QHD's\nquantum-inspired dynamics to identify optimal community structures. Our method\nimplements a multi-level refinement strategy that alternates between QUBO\nformulation and QHD optimization to iteratively improve partition quality.\nExperimental results demonstrate that our QHD-based approach achieves superior\nmodularity scores (up to 5.49\\%) improvement with reduced computational\noverhead compared to traditional optimization methods. This work establishes\nQHD as an effective quantum-inspired framework for tackling graph partition\nchallenges in large-scale networks.",
      "tldr_zh": "本研究引入了 Quantum Hamiltonian Descent (QHD) 作为一种新方法来解决 Graph Partition 问题，通过将问题重构为 Quadratic Unconstrained Binary Optimization (QUBO) 形式，并利用 QHD 的量子启发动态来识别最佳社区结构。方法采用多级精炼策略，在 QUBO 表述和 QHD 优化之间交替迭代，以逐步提升分区质量。实验结果显示，与传统优化方法相比，QHD 在模块度得分上提高了多达 5.49%，同时减少了计算开销。该工作确立了 QHD 作为处理大规模网络中图分区挑战的有效量子启发框架。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted by DAC 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.14696v2",
      "published_date": "2024-11-22 03:08:24 UTC",
      "updated_date": "2025-02-17 02:50:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:32:59.231987"
    },
    {
      "arxiv_id": "2411.15244v2",
      "title": "Adversarial Prompt Distillation for Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lin Luo",
        "Xin Wang",
        "Bojia Zi",
        "Shihao Zhao",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "abstract": "Large pre-trained Vision-Language Models (VLMs) such as Contrastive\nLanguage-Image Pre-training (CLIP) have been shown to be susceptible to\nadversarial attacks, raising concerns about their deployment in safety-critical\napplications like autonomous driving and medical diagnosis. One promising\napproach for robustifying pre-trained VLMs is Adversarial Prompt Tuning (APT),\nwhich applies adversarial training during the process of prompt tuning.\nHowever, existing APT methods are mostly single-modal methods that design\nprompt(s) for only the visual or textual modality, limiting their effectiveness\nin either robustness or clean accuracy. In this work, we propose Adversarial\nPrompt Distillation (APD), a bimodal knowledge distillation framework that\nenhances APT by integrating it with multi-modal knowledge transfer. APD\noptimizes prompts for both visual and textual modalities while distilling\nknowledge from a clean pre-trained teacher CLIP model. Extensive experiments on\nmultiple benchmark datasets demonstrate the superiority of our APD method over\nthe current state-of-the-art APT methods in terms of both adversarial\nrobustness and clean accuracy. The effectiveness of APD also validates the\npossibility of using a non-robust teacher to improve the generalization and\nrobustness of fine-tuned VLMs.",
      "tldr_zh": "该研究针对视觉语言模型(VLMs)如CLIP易受对抗性攻击的问题，提出了一种新的双模态框架Adversarial Prompt Distillation (APD)。APD将Adversarial Prompt Tuning (APT)与多模态知识蒸馏相结合，通过优化视觉和文本模态的提示，并从一个干净的预训练教师CLIP模型中转移知识，从而提升模型的对抗鲁棒性和干净准确率。实验在多个基准数据集上表明，APD优于现有状态方法，并在验证使用非鲁棒教师模型改善VLMs泛化和鲁棒性的可能性方面提供了重要见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15244v2",
      "published_date": "2024-11-22 03:02:13 UTC",
      "updated_date": "2025-04-15 01:57:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:33:10.510161"
    },
    {
      "arxiv_id": "2411.15243v1",
      "title": "Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence",
      "title_zh": "生物启发人工智能：将生物复杂性整合到人工智能中",
      "authors": [
        "Nima Dehghani",
        "Michael Levin"
      ],
      "abstract": "The pursuit of creating artificial intelligence (AI) mirrors our longstanding\nfascination with understanding our own intelligence. From the myths of Talos to\nAristotelian logic and Heron's inventions, we have sought to replicate the\nmarvels of the mind. While recent advances in AI hold promise, singular\napproaches often fall short in capturing the essence of intelligence. This\npaper explores how fundamental principles from biological\ncomputation--particularly context-dependent, hierarchical information\nprocessing, trial-and-error heuristics, and multi-scale organization--can guide\nthe design of truly intelligent systems. By examining the nuanced mechanisms of\nbiological intelligence, such as top-down causality and adaptive interaction\nwith the environment, we aim to illuminate potential limitations in artificial\nconstructs. Our goal is to provide a framework inspired by biological systems\nfor designing more adaptable and robust artificial intelligent systems.",
      "tldr_zh": "这篇论文探讨了如何将生物计算的基本原则整合到人工智能(AI)设计中，以克服当前AI方法的局限性。论文强调了生物智能的关键机制，如上下文依赖的层次化信息处理(trial-and-error heuristics)、多尺度组织(multi-scale organization)、top-down causality和与环境的adaptive interaction。最终，它提出一个受生物系统启发的框架，旨在指导开发更适应和鲁棒的AI系统。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.NE",
        "cs.SC"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15243v1",
      "published_date": "2024-11-22 02:55:39 UTC",
      "updated_date": "2024-11-22 02:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:33:22.296297"
    },
    {
      "arxiv_id": "2411.15242v1",
      "title": "The Zamba2 Suite: Technical Report",
      "title_zh": "Zamba2 套件：技术报告",
      "authors": [
        "Paolo Glorioso",
        "Quentin Anthony",
        "Yury Tokpanov",
        "Anna Golubeva",
        "Vasudev Shyam",
        "James Whittington",
        "Jonathan Pilault",
        "Beren Millidge"
      ],
      "abstract": "In this technical report, we present the Zamba2 series -- a suite of 1.2B,\n2.7B, and 7.4B parameter hybrid Mamba2-transformer models that achieve state of\nthe art performance against the leading open-weights models of their class,\nwhile achieving substantial gains in inference latency, throughput, and memory\nefficiency. The Zamba2 series builds upon our initial work with Zamba1-7B,\noptimizing its architecture, training and annealing datasets, and training for\nup to three trillion tokens. We provide open-source weights for all models of\nthe Zamba2 series as well as instruction-tuned variants that are strongly\ncompetitive against comparable instruct-tuned models of their class. We\nadditionally open-source the pretraining dataset, which we call Zyda-2, used to\ntrain the Zamba2 series of models. The models and datasets used in this work\nare openly available at https://huggingface.co/Zyphra",
      "tldr_zh": "这篇技术报告介绍了 Zamba2 系列模型，包括 1.2B、2.7B 和 7.4B 参数的混合 Mamba2-transformer 模型，这些模型在性能上超越同类开源模型，同时显著提升了推理延迟、吞吐量和内存效率。Zamba2 基于 Zamba1-7B 的架构进行优化，改进了训练数据集（Zyda-2）和训练过程，共训练了多达三万亿 tokens。论文开源了所有模型权重、指令微调版本以及预训练数据集 Zyda-2，可在 Hugging Face 上获取。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "21/11/24 initial upload",
      "pdf_url": "http://arxiv.org/pdf/2411.15242v1",
      "published_date": "2024-11-22 02:55:20 UTC",
      "updated_date": "2024-11-22 02:55:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:33:35.591973"
    },
    {
      "arxiv_id": "2411.14684v2",
      "title": "Learning Modality-Aware Representations: Adaptive Group-wise Interaction Network for Multimodal MRI Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Song",
        "Yicheng Wu",
        "Minhao Hu",
        "Xiangde Luo",
        "Linda Wei",
        "Guotai Wang",
        "Yi Guo",
        "Feng Xu",
        "Shaoting Zhang"
      ],
      "abstract": "Multimodal MR image synthesis aims to generate missing modality images by\neffectively fusing and mapping from a subset of available MRI modalities. Most\nexisting methods adopt an image-to-image translation paradigm, treating\nmultiple modalities as input channels. However, these approaches often yield\nsub-optimal results due to the inherent difficulty in achieving precise\nfeature- or semantic-level alignment across modalities. To address these\nchallenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net)\nthat explicitly models both inter-modality and intra-modality relationships for\nmultimodal MR image synthesis. Specifically, feature channels are first\npartitioned into predefined groups, after which an adaptive rolling mechanism\nis applied to conventional convolutional kernels to better capture feature and\nsemantic correspondences between different modalities. In parallel, a\ncross-group attention module is introduced to enable effective feature fusion\nacross groups, thereby enhancing the network's representational capacity. We\nvalidate the proposed AGI-Net on the publicly available IXI and BraTS2023\ndatasets. Experimental results demonstrate that AGI-Net achieves\nstate-of-the-art performance in multimodal MR image synthesis tasks, confirming\nthe effectiveness of its modality-aware interaction design. We release the\nrelevant code at:\nhttps://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git.",
      "tldr_zh": "这篇论文针对多模态 MRI 合成的问题，提出了一种 Adaptive Group-wise Interaction Network (AGI-Net)，通过显式建模 inter-modality 和 intra-modality 关系来实现更精确的特征和语义对齐。具体而言，AGI-Net 将特征通道分区后应用自适应滚动机制到卷积核，并引入跨组注意力模块以增强特征融合能力。实验在 IXI 和 BraTS2023 数据集上验证了该方法的有效性，实现了 state-of-the-art 性能，证明了其模态感知交互设计的优势。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14684v2",
      "published_date": "2024-11-22 02:29:37 UTC",
      "updated_date": "2025-04-28 06:26:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:33:46.702658"
    },
    {
      "arxiv_id": "2411.14672v1",
      "title": "Multiverse of Greatness: Generating Story Branches with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Pittawat Taveekitworachai",
        "Chollakorn Nimpattanavong",
        "Mustafa Can Gursesli",
        "Antonio Lanata",
        "Andrea Guazzini",
        "Ruck Thawonmas"
      ],
      "abstract": "This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel\nframework for interacting with LLMs to generate graph-based content with a\ndynamic context window history. While there is an existing study utilizing LLMs\nto generate a visual novel game, the previous study involved a manual process\nof output extraction and did not provide flexibility in generating a longer,\ncoherent story. We evaluate DCP/P against our baseline, which does not provide\ncontext history to an LLM and only relies on the initial story data. Through\nobjective evaluation, we show that simply providing the LLM with a summary\nleads to a subpar story compared to additionally providing the LLM with the\nproper context of the story. We also provide an extensive qualitative analysis\nand discussion. We qualitatively examine the quality of the objectively\nbest-performing generated game from each approach. In addition, we examine\nbiases in word choices and word sentiment of the generated content. We find a\nconsistent observation with previous studies that LLMs are biased towards\ncertain words, even with a different LLM family. Finally, we provide a\ncomprehensive discussion on opportunities for future studies.",
      "tldr_zh": "这篇论文提出了Dynamic Context Prompting/Programming (DCP/P)，一个新框架，用于与LLMs互动生成基于图形的动态故事分支，通过动态上下文窗口历史确保故事的连贯性和灵活性。相比于基线方法（仅依赖初始故事数据而不提供上下文），DCP/P在客观评估中表现出色，提供适当的故事上下文能显著提升生成质量。研究还通过定性分析揭示了LLMs在词选择和情感偏见方面的固有问题，并讨论了未来研究的潜在机会。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.14672v1",
      "published_date": "2024-11-22 02:11:37 UTC",
      "updated_date": "2024-11-22 02:11:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:33:57.895895"
    },
    {
      "arxiv_id": "2411.15240v3",
      "title": "AI Foundation Models for Wearable Movement Data in Mental Health Research",
      "title_zh": "翻译失败",
      "authors": [
        "Franklin Y. Ruan",
        "Aiwei Zhang",
        "Jenny Y. Oh",
        "SouYoung Jin",
        "Nicholas C. Jacobson"
      ],
      "abstract": "Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/",
      "tldr_zh": "本研究探讨了预训练基础模型和Transformer架构在AI领域的成功，并将其应用于可穿戴运动数据，以推进心理健康研究。该团队引入了首个开源模型Pretrained Actigraphy Transformer (PAT)，利用Transformer-based architectures、patch embeddings等技术，在来自29,307名美国参与者的数据上进行预训练。PAT在多个心理健康预测任务中实现了state-of-the-art性能，同时保持轻量级和易解释特性，为心理健康研究提供了可靠工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15240v3",
      "published_date": "2024-11-22 01:58:35 UTC",
      "updated_date": "2025-01-14 04:10:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:34:09.225155"
    },
    {
      "arxiv_id": "2411.14654v3",
      "title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Jinming Xing",
        "Dongwen Luo",
        "Chang Xue",
        "Ruilin Xing"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.",
      "tldr_zh": "本研究比较了大型语言模型（LLMs）中不同 pooling 机制（包括 Mean、Max 和 Weighted Sum）在句子级情感分析任务中的性能，焦点放在 BERT 和 GPT 模型上。研究通过全面实验评估这些机制如何聚合 token-level embeddings 为 sentence-level representations，并揭示每个机制根据任务需求表现出独特的优势和劣势。结果强调，选择合适的 pooling 方法至关重要，这为优化 LLM 模型应用于下游任务提供了可操作的见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ISMSI'25",
      "pdf_url": "http://arxiv.org/pdf/2411.14654v3",
      "published_date": "2024-11-22 00:59:25 UTC",
      "updated_date": "2025-02-01 22:39:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:34:21.221007"
    },
    {
      "arxiv_id": "2411.14652v1",
      "title": "Social Media Algorithms Can Shape Affective Polarization via Exposure to Antidemocratic Attitudes and Partisan Animosity",
      "title_zh": "翻译失败",
      "authors": [
        "Tiziano Piccardi",
        "Martin Saveski",
        "Chenyan Jia",
        "Jeffrey T. Hancock",
        "Jeanne L. Tsai",
        "Michael Bernstein"
      ],
      "abstract": "There is widespread concern about the negative impacts of social media feed\nranking algorithms on political polarization. Leveraging advancements in large\nlanguage models (LLMs), we develop an approach to re-rank feeds in real-time to\ntest the effects of content that is likely to polarize: expressions of\nantidemocratic attitudes and partisan animosity (AAPA). In a preregistered\n10-day field experiment on X/Twitter with 1,256 consented participants, we\nincrease or decrease participants' exposure to AAPA in their algorithmically\ncurated feeds. We observe more positive outparty feelings when AAPA exposure is\ndecreased and more negative outparty feelings when AAPA exposure is increased.\nExposure to AAPA content also results in an immediate increase in negative\nemotions, such as sadness and anger. The interventions do not significantly\nimpact traditional engagement metrics such as re-post and favorite rates. These\nfindings highlight a potential pathway for developing feed algorithms that\nmitigate affective polarization by addressing content that undermines the\nshared values required for a healthy democracy.",
      "tldr_zh": "本研究探讨社交媒体算法如何通过暴露用户于反民主态度和党派敌意 (AAPA) 内容来影响情感极化 (affective polarization)。研究团队利用大型语言模型 (LLMs) 实时重新排序 X/Twitter 用户的算法推荐 feed，并在涉及 1,256 名参与者的 10 天现场实验中，增加或减少 AAPA 内容的暴露。结果显示，减少 AAPA 暴露可提升对对方党的积极情感，而增加暴露则加剧负面情绪，如悲伤和愤怒，但对传统参与指标（如转发和收藏率）无显著影响。这些发现为开发算法提供潜在路径，以缓解情感极化并维护民主共享价值。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14652v1",
      "published_date": "2024-11-22 00:55:07 UTC",
      "updated_date": "2024-11-22 00:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:34:34.196675"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 103,
  "processed_papers_count": 103,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T03:34:54.711548"
}
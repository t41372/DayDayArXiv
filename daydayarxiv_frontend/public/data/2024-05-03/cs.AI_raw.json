[
  {
    "arxiv_id": "2405.02522v2",
    "title": "New contexts, old heuristics: How young people in India and the US trust online content in the age of generative AI",
    "authors": [
      "Rachel Xu",
      "Nhu Le",
      "Rebekah Park",
      "Laura Murray",
      "Vishnupriya Das",
      "Devika Kumar",
      "Beth Goldberg"
    ],
    "abstract": "We conducted in-person ethnography in India and the US to investigate how\nyoung people (18-24) trusted online content, just as generative AI (genAI)\nbecame mainstream. We found that when online, how participants determined what\ncontent to trust was shaped by emotional states, which we term \"information\nmodes.\" Our participants reflexively shifted between modes to maintain\n\"emotional equilibrium,\" and eschewed engaging literacy skills in the more\npassive modes in which they spent the most time. We found participants imported\ntrust heuristics from established online contexts into emerging ones (i.e.,\ngenAI). This led them to use ill-fitting trust heuristics, and exposed them to\nthe risk of trusting false and misleading information. While many had\nreservations about AI, prioritizing efficiency, they used genAI and habitual\nheuristics to quickly achieve goals at the expense of accuracy. We conclude\nthat literacy interventions designed to match users' distinct information modes\nwill be most effective.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.02522v2",
    "published_date": "2024-05-03 23:27:11 UTC",
    "updated_date": "2024-10-08 00:35:23 UTC"
  },
  {
    "arxiv_id": "2405.03710v1",
    "title": "Automating the Enterprise with Foundation Models",
    "authors": [
      "Michael Wornow",
      "Avanika Narayan",
      "Krista Opsahl-Ong",
      "Quinn McIntyre",
      "Nigam H. Shah",
      "Christopher Re"
    ],
    "abstract": "Automating enterprise workflows could unlock $4 trillion/year in productivity\ngains. Despite being of interest to the data management community for decades,\nthe ultimate vision of end-to-end workflow automation has remained elusive.\nCurrent solutions rely on process mining and robotic process automation (RPA),\nin which a bot is hard-coded to follow a set of predefined rules for completing\na workflow. Through case studies of a hospital and large B2B enterprise, we\nfind that the adoption of RPA has been inhibited by high set-up costs (12-18\nmonths), unreliable execution (60% initial accuracy), and burdensome\nmaintenance (requiring multiple FTEs). Multimodal foundation models (FMs) such\nas GPT-4 offer a promising new approach for end-to-end workflow automation\ngiven their generalized reasoning and planning abilities. To study these\ncapabilities we propose ECLAIR, a system to automate enterprise workflows with\nminimal human supervision. We conduct initial experiments showing that\nmultimodal FMs can address the limitations of traditional RPA with (1)\nnear-human-level understanding of workflows (93% accuracy on a workflow\nunderstanding task) and (2) instant set-up with minimal technical barrier\n(based solely on a natural language description of a workflow, ECLAIR achieves\nend-to-end completion rates of 40%). We identify human-AI collaboration,\nvalidation, and self-improvement as open challenges, and suggest ways they can\nbe solved with data management techniques. Code is available at:\nhttps://github.com/HazyResearch/eclair-agents",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03710v1",
    "published_date": "2024-05-03 23:25:15 UTC",
    "updated_date": "2024-05-03 23:25:15 UTC"
  },
  {
    "arxiv_id": "2405.03709v3",
    "title": "ScenicNL: Generating Probabilistic Scenario Programs from Natural Language",
    "authors": [
      "Karim Elmaaroufi",
      "Devan Shanker",
      "Ana Cismaru",
      "Marcell Vazquez-Chanlatte",
      "Alberto Sangiovanni-Vincentelli",
      "Matei Zaharia",
      "Sanjit A. Seshia"
    ],
    "abstract": "For cyber-physical systems (CPS), including robotics and autonomous vehicles,\nmass deployment has been hindered by fatal errors that occur when operating in\nrare events. To replicate rare events such as vehicle crashes, many companies\nhave created logging systems and employed crash reconstruction experts to\nmeticulously recreate these valuable events in simulation. However, in these\nmethods, \"what if\" questions are not easily formulated and answered. We present\nScenarioNL, an AI System for creating scenario programs from natural language.\nSpecifically, we generate these programs from police crash reports. Reports\nnormally contain uncertainty about the exact details of the incidents which we\nrepresent through a Probabilistic Programming Language (PPL), Scenic. By using\nScenic, we can clearly and concisely represent uncertainty and variation over\nCPS behaviors, properties, and interactions. We demonstrate how commonplace\nprompting techniques with the best Large Language Models (LLM) are incapable of\nreasoning about probabilistic scenario programs and generating code for\nlow-resource languages such as Scenic. Our system is comprised of several LLMs\nchained together with several kinds of prompting strategies, a compiler, and a\nsimulator. We evaluate our system on publicly available autonomous vehicle\ncrash reports in California from the last five years and share insights into\nhow we generate code that is both semantically meaningful and syntactically\ncorrect.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "22 pages, 3 figures. Published at COLM 2024.\n  https://ke7.github.io/ScenicNL",
    "pdf_url": "http://arxiv.org/pdf/2405.03709v3",
    "published_date": "2024-05-03 23:06:31 UTC",
    "updated_date": "2024-10-02 22:58:42 UTC"
  },
  {
    "arxiv_id": "2405.02512v2",
    "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
    "authors": [
      "Yohei Nakayama",
      "Jiawei Su",
      "Luis M. Pazos-Outón"
    ],
    "abstract": "Recent advancements in foundation models have significantly impacted various\nfields, including natural language processing, computer vision, and multi-modal\ntasks. One area that stands to benefit greatly is Earth observation, where\nthese models can efficiently process large-scale, unlabeled geospatial data. In\nthis work we extend the SwinMAE model to integrate temporal information for\nsatellite time-series data. The architecture employs a hierarchical 3D Masked\nAutoencoder (MAE) with Video Swin Transformer blocks to effectively capture\nmulti-scale spatio-temporal dependencies in satellite imagery. To enhance\ntransfer learning, we incorporate both encoder and decoder pretrained weights,\nalong with skip connections to preserve scale-specific information. This forms\nan architecture similar to SwinUNet with an additional temporal component. Our\napproach shows significant performance improvements over existing\nstate-of-the-art foundation models for all the evaluated downstream tasks: land\ncover segmentation, building density prediction, flood mapping, wildfire scar\nmapping and multi-temporal crop segmentation. Particularly, in the land cover\nsegmentation task of the PhilEO Bench dataset, it outperforms other geospatial\nfoundation models with a 10.4% higher accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02512v2",
    "published_date": "2024-05-03 22:55:56 UTC",
    "updated_date": "2024-10-18 08:25:52 UTC"
  },
  {
    "arxiv_id": "2405.02509v2",
    "title": "Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction",
    "authors": [
      "Jiayang Shi",
      "Junyi Zhu",
      "Daniel M. Pelt",
      "K. Joost Batenburg",
      "Matthew B. Blaschko"
    ],
    "abstract": "Computed Tomography (CT) is pivotal in industrial quality control and medical\ndiagnostics. Sparse-view CT, offering reduced ionizing radiation, faces\nchallenges due to its under-sampled nature, leading to ill-posed reconstruction\nproblems. Recent advancements in Implicit Neural Representations (INRs) have\nshown promise in addressing sparse-view CT reconstruction. Recognizing that CT\noften involves scanning similar subjects, we propose a novel approach to\nimprove reconstruction quality through joint reconstruction of multiple objects\nusing INRs. This approach can potentially utilize the advantages of INRs and\nthe common patterns observed across different objects. While current INR joint\nreconstruction techniques primarily focus on speeding up the learning process,\nthey are not specifically tailored to enhance the final reconstruction quality.\nTo address this gap, we introduce a novel INR-based Bayesian framework\nintegrating latent variables to capture the common patterns across multiple\nobjects under joint reconstruction. The common patterns then assist in the\nreconstruction of each object via latent variables, thereby improving the\nindividual reconstruction. Extensive experiments demonstrate that our method\nachieves higher reconstruction quality with sparse views and remains robust to\nnoise in the measurements as indicated by common numerical metrics. The\nobtained latent variables can also serve as network initialization for the new\nobject and speed up the learning process.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02509v2",
    "published_date": "2024-05-03 22:50:59 UTC",
    "updated_date": "2024-10-25 14:17:57 UTC"
  },
  {
    "arxiv_id": "2405.02501v2",
    "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
    "authors": [
      "Hyeong Kyu Choi",
      "Yixuan Li"
    ],
    "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are\nencoded with diverse personality traits. This triggers an interesting goal of\neliciting a desired personality trait from the LLM, and probing its behavioral\npreferences. Accordingly, we formalize the persona elicitation task, aiming to\ncustomize LLM behaviors to align with a target persona. We present Persona\nIn-Context Learning (PICLe), a novel persona elicitation framework grounded in\nBayesian inference. At the core, PICLe introduces a new ICL example selection\ncriterion based on likelihood ratio, which is designed to optimally guide the\nmodel in eliciting a specific target persona. We demonstrate the effectiveness\nof PICLe through extensive comparisons against baseline methods across three\ncontemporary LLMs. Code is available at\nhttps://github.com/deeplearning-wisc/picle.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02501v2",
    "published_date": "2024-05-03 22:17:22 UTC",
    "updated_date": "2024-05-14 05:53:07 UTC"
  },
  {
    "arxiv_id": "2405.02485v1",
    "title": "A Survey of Few-Shot Learning for Biomedical Time Series",
    "authors": [
      "Chenqi Li",
      "Timothy Denison",
      "Tingting Zhu"
    ],
    "abstract": "Advancements in wearable sensor technologies and the digitization of medical\nrecords have contributed to the unprecedented ubiquity of biomedical time\nseries data. Data-driven models have tremendous potential to assist clinical\ndiagnosis and improve patient care by improving long-term monitoring\ncapabilities, facilitating early disease detection and intervention, as well as\npromoting personalized healthcare delivery. However, accessing extensively\nlabeled datasets to train data-hungry deep learning models encounters many\nbarriers, such as long-tail distribution of rare diseases, cost of annotation,\nprivacy and security concerns, data-sharing regulations, and ethical\nconsiderations. An emerging approach to overcome the scarcity of labeled data\nis to augment AI methods with human-like capabilities to leverage past\nexperiences to learn new tasks with limited examples, called few-shot learning.\nThis survey provides a comprehensive review and comparison of few-shot learning\nmethods for biomedical time series applications. The clinical benefits and\nlimitations of such methods are discussed in relation to traditional\ndata-driven approaches. This paper aims to provide insights into the current\nlandscape of few-shot learning for biomedical time series and its implications\nfor future research and applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2405.02485v1",
    "published_date": "2024-05-03 21:22:27 UTC",
    "updated_date": "2024-05-03 21:22:27 UTC"
  },
  {
    "arxiv_id": "2405.02481v1",
    "title": "Proximal Curriculum with Task Correlations for Deep Reinforcement Learning",
    "authors": [
      "Georgios Tzannetos",
      "Parameswaran Kamalaruban",
      "Adish Singla"
    ],
    "abstract": "Curriculum design for reinforcement learning (RL) can speed up an agent's\nlearning process and help it learn to perform well on complex tasks. However,\nexisting techniques typically require domain-specific hyperparameter tuning,\ninvolve expensive optimization procedures for task selection, or are suitable\nonly for specific learning objectives. In this work, we consider curriculum\ndesign in contextual multi-task settings where the agent's final performance is\nmeasured w.r.t. a target distribution over complex tasks. We base our\ncurriculum design on the Zone of Proximal Development concept, which has proven\nto be effective in accelerating the learning process of RL agents for uniform\ndistribution over all tasks. We propose a novel curriculum, ProCuRL-Target,\nthat effectively balances the need for selecting tasks that are not too\ndifficult for the agent while progressing the agent's learning toward the\ntarget distribution via leveraging task correlations. We theoretically justify\nthe task selection strategy of ProCuRL-Target by analyzing a simple learning\nsetting with REINFORCE learner model. Our experimental results across various\ndomains with challenging target task distributions affirm the effectiveness of\nour curriculum strategy over state-of-the-art baselines in accelerating the\ntraining process of deep RL agents.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "IJCAI'24 paper (longer version)",
    "pdf_url": "http://arxiv.org/pdf/2405.02481v1",
    "published_date": "2024-05-03 21:07:54 UTC",
    "updated_date": "2024-05-03 21:07:54 UTC"
  },
  {
    "arxiv_id": "2405.02480v1",
    "title": "A Network Simulation of OTC Markets with Multiple Agents",
    "authors": [
      "James T. Wilkinson",
      "Jacob Kelter",
      "John Chen",
      "Uri Wilensky"
    ],
    "abstract": "We present a novel agent-based approach to simulating an over-the-counter\n(OTC) financial market in which trades are intermediated solely by market\nmakers and agent visibility is constrained to a network topology. Dynamics,\nsuch as changes in price, result from agent-level interactions that\nubiquitously occur via market maker agents acting as liquidity providers. Two\nadditional agents are considered: trend investors use a deep convolutional\nneural network paired with a deep Q-learning framework to inform trading\ndecisions by analysing price history; and value investors use a static\nprice-target to determine their trade directions and sizes. We demonstrate that\nour novel inclusion of a network topology with market makers facilitates\nexplorations into various market structures. First, we present the model and an\noverview of its mechanics. Second, we validate our findings via comparison to\nthe real-world: we demonstrate a fat-tailed distribution of price changes,\nauto-correlated volatility, a skew negatively correlated to market maker\npositioning, predictable price-history patterns and more. Finally, we\ndemonstrate that our network-based model can lend insights into the effect of\nmarket-structure on price-action. For example, we show that markets with\nsparsely connected intermediaries can have a critical point of fragmentation,\nbeyond which the market forms distinct clusters and arbitrage becomes rapidly\npossible between the prices of different market makers. A discussion is\nprovided on future work that would be beneficial.",
    "categories": [
      "econ.EM",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "econ.EM",
    "comment": "20 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02480v1",
    "published_date": "2024-05-03 20:45:00 UTC",
    "updated_date": "2024-05-03 20:45:00 UTC"
  },
  {
    "arxiv_id": "2405.02475v2",
    "title": "Generalizing Orthogonalization for Models with Non-Linearities",
    "authors": [
      "David Rügamer",
      "Chris Kolb",
      "Tobias Weber",
      "Lucas Kook",
      "Thomas Nagler"
    ],
    "abstract": "The complexity of black-box algorithms can lead to various challenges,\nincluding the introduction of biases. These biases present immediate risks in\nthe algorithms' application. It was, for instance, shown that neural networks\ncan deduce racial information solely from a patient's X-ray scan, a task beyond\nthe capability of medical experts. If this fact is not known to the medical\nexpert, automatic decision-making based on this algorithm could lead to\nprescribing a treatment (purely) based on racial information. While current\nmethodologies allow for the \"orthogonalization\" or \"normalization\" of neural\nnetworks with respect to such information, existing approaches are grounded in\nlinear models. Our paper advances the discourse by introducing corrections for\nnon-linearities such as ReLU activations. Our approach also encompasses scalar\nand tensor-valued predictions, facilitating its integration into neural network\narchitectures. Through extensive experiments, we validate our method's\neffectiveness in safeguarding sensitive data in generalized linear models,\nnormalizing convolutional neural networks for metadata, and rectifying\npre-existing embeddings for undesired attributes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02475v2",
    "published_date": "2024-05-03 20:25:57 UTC",
    "updated_date": "2024-06-02 13:15:21 UTC"
  },
  {
    "arxiv_id": "2405.02463v1",
    "title": "Knowledge Graph Extension by Entity Type Recognition",
    "authors": [
      "Daqian Shi"
    ],
    "abstract": "Knowledge graphs have emerged as a sophisticated advancement and refinement\nof semantic networks, and their deployment is one of the critical methodologies\nin contemporary artificial intelligence. The construction of knowledge graphs\nis a multifaceted process involving various techniques, where researchers aim\nto extract the knowledge from existing resources for the construction since\nbuilding from scratch entails significant labor and time costs. However, due to\nthe pervasive issue of heterogeneity, the description diversity across\ndifferent knowledge graphs can lead to mismatches between concepts, thereby\nimpacting the efficacy of knowledge extraction. This Ph.D. study focuses on\nautomatic knowledge graph extension, i.e., properly extending the reference\nknowledge graph by extracting and integrating concepts from one or more\ncandidate knowledge graphs. We propose a novel knowledge graph extension\nframework based on entity type recognition. The framework aims to achieve\nhigh-quality knowledge extraction by aligning the schemas and entities across\ndifferent knowledge graphs, thereby enhancing the performance of the extension.\nThis paper elucidates three major contributions: (i) we propose an entity type\nrecognition method exploiting machine learning and property-based similarities\nto enhance knowledge extraction; (ii) we introduce a set of assessment metrics\nto validate the quality of the extended knowledge graphs; (iii) we develop a\nplatform for knowledge graph acquisition, management, and extension to benefit\nknowledge engineers practically. Our evaluation comprehensively demonstrated\nthe feasibility and effectiveness of the proposed extension framework and its\nfunctionalities through quantitative experiments and case studies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2405.02463v1",
    "published_date": "2024-05-03 19:55:03 UTC",
    "updated_date": "2024-05-03 19:55:03 UTC"
  },
  {
    "arxiv_id": "2405.02458v1",
    "title": "Controlled Query Evaluation through Epistemic Dependencies",
    "authors": [
      "Gianluca Cima",
      "Domenico Lembo",
      "Lorenzo Marconi",
      "Riccardo Rosati",
      "Domenico Fabio Savo"
    ],
    "abstract": "In this paper, we propose the use of epistemic dependencies to express data\nprotection policies in Controlled Query Evaluation (CQE), which is a form of\nconfidentiality-preserving query answering over ontologies and databases. The\nresulting policy language goes significantly beyond those proposed in the\nliterature on CQE so far, allowing for very rich and practically interesting\nforms of data protection rules. We show the expressive abilities of our\nframework and study the data complexity of CQE for (unions of) conjunctive\nqueries when ontologies are specified in the Description Logic DL-Lite_R.\nInterestingly, while we show that the problem is in general intractable, we\nprove tractability for the case of acyclic epistemic dependencies by providing\na suitable query rewriting algorithm. The latter result paves the way towards\nthe implementation and practical application of this new approach to CQE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02458v1",
    "published_date": "2024-05-03 19:48:07 UTC",
    "updated_date": "2024-05-03 19:48:07 UTC"
  },
  {
    "arxiv_id": "2405.02454v1",
    "title": "What is Sentiment Meant to Mean to Language Models?",
    "authors": [
      "Michael Burnham"
    ],
    "abstract": "Sentiment analysis is one of the most widely used techniques in text\nanalysis. Recent advancements with Large Language Models have made it more\naccurate and accessible than ever, allowing researchers to classify text with\nonly a plain English prompt. However, \"sentiment\" entails a wide variety of\nconcepts depending on the domain and tools used. It has been used to mean\nemotion, opinions, market movements, or simply a general ``good-bad''\ndimension. This raises a question: What exactly are language models doing when\nprompted to label documents by sentiment? This paper first overviews how\nsentiment is defined across different contexts, highlighting that it is a\nconfounded measurement construct in that it entails multiple variables, such as\nemotional valence and opinion, without disentangling them. I then test three\nlanguage models across two data sets with prompts requesting sentiment,\nvalence, and stance classification. I find that sentiment labels most strongly\ncorrelate with valence labels. I further find that classification improves when\nresearchers more precisely specify their dimension of interest rather than\nusing the less well-defined concept of sentiment. I conclude by encouraging\nresearchers to move beyond \"sentiment\" when feasible and use a more precise\nmeasurement construct.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02454v1",
    "published_date": "2024-05-03 19:37:37 UTC",
    "updated_date": "2024-05-03 19:37:37 UTC"
  },
  {
    "arxiv_id": "2406.18536v1",
    "title": "Reliable Interval Prediction of Minimum Operating Voltage Based on On-chip Monitors via Conformalized Quantile Regression",
    "authors": [
      "Yuxuan Yin",
      "Xiaoxiao Wang",
      "Rebecca Chen",
      "Chen He",
      "Peng Li"
    ],
    "abstract": "Predicting the minimum operating voltage ($V_{min}$) of chips is one of the\nimportant techniques for improving the manufacturing testing flow, as well as\nensuring the long-term reliability and safety of in-field systems. Current\n$V_{min}$ prediction methods often provide only point estimates, necessitating\nadditional techniques for constructing prediction confidence intervals to cover\nuncertainties caused by different sources of variations. While some existing\ntechniques offer region predictions, but they rely on certain distributional\nassumptions and/or provide no coverage guarantees. In response to these\nlimitations, we propose a novel distribution-free $V_{min}$ interval estimation\nmethodology possessing a theoretical guarantee of coverage. Our approach\nleverages conformalized quantile regression and on-chip monitors to generate\nreliable prediction intervals. We demonstrate the effectiveness of the proposed\nmethod on an industrial 5nm automotive chip dataset. Moreover, we show that the\nuse of on-chip monitors can reduce the interval length significantly for\n$V_{min}$ prediction.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.AR",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted by DATE 2024. Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.18536v1",
    "published_date": "2024-05-03 19:34:47 UTC",
    "updated_date": "2024-05-03 19:34:47 UTC"
  },
  {
    "arxiv_id": "2406.02566v2",
    "title": "Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition",
    "authors": [
      "Ognjen Kundacina",
      "Vladimir Vincan",
      "Dragisa Miskovic"
    ],
    "abstract": "This paper introduces a novel two-stage active learning (AL) pipeline for\nautomatic speech recognition (ASR), combining unsupervised and supervised AL\nmethods. The first stage utilizes unsupervised AL by using x-vectors clustering\nfor diverse sample selection from unlabeled speech data, thus establishing a\nrobust initial dataset for the subsequent supervised AL. The second stage\nincorporates a supervised AL strategy, with a batch AL method specifically\ndeveloped for ASR, aimed at selecting diverse and informative batches of\nsamples. Here, sample diversity is also achieved using x-vectors clustering,\nwhile the most informative samples are identified using a Bayesian AL method\ntailored for ASR with an adaptation of Monte Carlo dropout to approximate\nBayesian inference. This approach enables precise uncertainty estimation,\nthereby enhancing ASR model training with significantly reduced data\nrequirements. Our method has shown superior performance compared to competing\nmethods on homogeneous, heterogeneous, and OOD test sets, demonstrating that\nstrategic sample selection and innovative Bayesian modeling can substantially\noptimize both labeling effort and data utilization in deep learning-based ASR\napplications.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02566v2",
    "published_date": "2024-05-03 19:24:41 UTC",
    "updated_date": "2025-04-25 06:24:10 UTC"
  },
  {
    "arxiv_id": "2405.15789v1",
    "title": "Semantic Objective Functions: A distribution-aware method for adding logical constraints in deep learning",
    "authors": [
      "Miguel Angel Mendez-Lucero",
      "Enrique Bojorquez Gallardo",
      "Vaishak Belle"
    ],
    "abstract": "Issues of safety, explainability, and efficiency are of increasing concern in\nlearning systems deployed with hard and soft constraints. Symbolic Constrained\nLearning and Knowledge Distillation techniques have shown promising results in\nthis area, by embedding and extracting knowledge, as well as providing logical\nconstraints during neural network training. Although many frameworks exist to\ndate, through an integration of logic and information geometry, we provide a\nconstruction and theoretical framework for these tasks that generalize many\napproaches. We propose a loss-based method that embeds knowledge-enforces\nlogical constraints-into a machine learning model that outputs probability\ndistributions. This is done by constructing a distribution from the external\nknowledge/logic formula and constructing a loss function as a linear\ncombination of the original loss function with the Fisher-Rao distance or\nKullback-Leibler divergence to the constraint distribution. This construction\nincludes logical constraints in the form of propositional formulas (Boolean\nvariables), formulas of a first-order language with finite variables over a\nmodel with compact domain (categorical and continuous variables), and in\ngeneral, likely applicable to any statistical model that was pretrained with\nsemantic information. We evaluate our method on a variety of learning tasks,\nincluding classification tasks with logic constraints, transferring knowledge\nfrom logic formulas, and knowledge distillation from general distributions.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.LO",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages,4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.15789v1",
    "published_date": "2024-05-03 19:21:47 UTC",
    "updated_date": "2024-05-03 19:21:47 UTC"
  },
  {
    "arxiv_id": "2405.02429v2",
    "title": "CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation",
    "authors": [
      "Yaoyiran Li",
      "Xiang Zhai",
      "Moustafa Alzantot",
      "Keyi Yu",
      "Ivan Vulić",
      "Anna Korhonen",
      "Mohamed Hammad"
    ],
    "abstract": "Traditional recommender systems such as matrix factorization methods have\nprimarily focused on learning a shared dense embedding space to represent both\nitems and user preferences. Subsequently, sequence models such as RNN, GRUs,\nand, recently, Transformers have emerged and excelled in the task of sequential\nrecommendation. This task requires understanding the sequential structure\npresent in users' historical interactions to predict the next item they may\nlike. Building upon the success of Large Language Models (LLMs) in a variety of\ntasks, researchers have recently explored using LLMs that are pretrained on\nvast corpora of text for sequential recommendation. To use LLMs for sequential\nrecommendation, both the history of user interactions and the model's\nprediction of the next item are expressed in text form. We propose CALRec, a\ntwo-stage LLM finetuning framework that finetunes a pretrained LLM in a\ntwo-tower fashion using a mixture of two contrastive losses and a language\nmodeling loss: the LLM is first finetuned on a data mixture from multiple\ndomains followed by another round of target domain finetuning. Our model\nsignificantly outperforms many state-of-the-art baselines (+37% in Recall@1 and\n+24% in NDCG@10) and our systematic ablation studies reveal that (i) both\nstages of finetuning are crucial, and, when combined, we achieve improved\nperformance, and (ii) contrastive alignment is effective among the target\ndomains explored in our experiments.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "RecSys 2024 (Long Paper)",
    "pdf_url": "http://arxiv.org/pdf/2405.02429v2",
    "published_date": "2024-05-03 18:51:19 UTC",
    "updated_date": "2024-08-23 20:46:32 UTC"
  },
  {
    "arxiv_id": "2405.02425v1",
    "title": "Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning",
    "authors": [
      "Dhruva Tirumala",
      "Markus Wulfmeier",
      "Ben Moran",
      "Sandy Huang",
      "Jan Humplik",
      "Guy Lever",
      "Tuomas Haarnoja",
      "Leonard Hasenclever",
      "Arunkumar Byravan",
      "Nathan Batchelor",
      "Neil Sreendra",
      "Kushal Patel",
      "Marlon Gwira",
      "Francesco Nori",
      "Martin Riedmiller",
      "Nicolas Heess"
    ],
    "abstract": "We apply multi-agent deep reinforcement learning (RL) to train end-to-end\nrobot soccer policies with fully onboard computation and sensing via egocentric\nRGB vision. This setting reflects many challenges of real-world robotics,\nincluding active perception, agile full-body control, and long-horizon planning\nin a dynamic, partially-observable, multi-agent domain. We rely on large-scale,\nsimulation-based data generation to obtain complex behaviors from egocentric\nvision which can be successfully transferred to physical robots using low-cost\nsensors. To achieve adequate visual realism, our simulation combines rigid-body\nphysics with learned, realistic rendering via multiple Neural Radiance Fields\n(NeRFs). We combine teacher-based multi-agent RL and cross-experiment data\nreuse to enable the discovery of sophisticated soccer strategies. We analyze\nactive-perception behaviors including object tracking and ball seeking that\nemerge when simply optimizing perception-agnostic soccer play. The agents\ndisplay equivalent levels of performance and agility as policies with access to\nprivileged, ground-truth state. To our knowledge, this paper constitutes a\nfirst demonstration of end-to-end training for multi-agent robot soccer,\nmapping raw pixel observations to joint-level actions, that can be deployed in\nthe real world. Videos of the game-play and analyses can be seen on our website\nhttps://sites.google.com/view/vision-soccer .",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02425v1",
    "published_date": "2024-05-03 18:41:13 UTC",
    "updated_date": "2024-05-03 18:41:13 UTC"
  },
  {
    "arxiv_id": "2405.02413v2",
    "title": "A Unified Framework for Human-Allied Learning of Probabilistic Circuits",
    "authors": [
      "Athresh Karanam",
      "Saurabh Mathur",
      "Sahil Sidheekh",
      "Sriraam Natarajan"
    ],
    "abstract": "Probabilistic Circuits (PCs) have emerged as an efficient framework for\nrepresenting and learning complex probability distributions. Nevertheless, the\nexisting body of research on PCs predominantly concentrates on data-driven\nparameter learning, often neglecting the potential of knowledge-intensive\nlearning, a particular issue in data-scarce/knowledge-rich domains such as\nhealthcare. To bridge this gap, we propose a novel unified framework that can\nsystematically integrate diverse domain knowledge into the parameter learning\nprocess of PCs. Experiments on several benchmarks as well as real world\ndatasets show that our proposed framework can both effectively and efficiently\nleverage domain knowledge to achieve superior performance compared to purely\ndata-driven learning approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02413v2",
    "published_date": "2024-05-03 18:14:29 UTC",
    "updated_date": "2024-12-18 19:02:40 UTC"
  },
  {
    "arxiv_id": "2405.02287v1",
    "title": "Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models",
    "authors": [
      "Piotr Padlewski",
      "Max Bain",
      "Matthew Henderson",
      "Zhongkai Zhu",
      "Nishant Relan",
      "Hai Pham",
      "Donovan Ong",
      "Kaloyan Aleksiev",
      "Aitor Ormazabal",
      "Samuel Phua",
      "Ethan Yeo",
      "Eugenie Lamprecht",
      "Qi Liu",
      "Yuqi Wang",
      "Eric Chen",
      "Deyu Fu",
      "Lei Li",
      "Che Zheng",
      "Cyprien de Masson d'Autume",
      "Dani Yogatama",
      "Mikel Artetxe",
      "Yi Tay"
    ],
    "abstract": "We introduce Vibe-Eval: a new open benchmark and framework for evaluating\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\nincluding 100 of hard difficulty, complete with gold-standard responses\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\n(ii) rigorously testing and probing the capabilities of present frontier\nmodels. Notably, our hard set contains >50% questions that all frontier models\nanswer incorrectly. We explore the nuances of designing, evaluating, and\nranking models on ultra challenging prompts. We also discuss trade-offs between\nhuman and automatic evaluation, and show that automatic model evaluation using\nReka Core roughly correlates to human judgment. We offer free API access for\nthe purpose of lightweight evaluation and plan to conduct formal human\nevaluations for public models that perform well on the Vibe-Eval's automatic\nscores. We release the evaluation code and data, see\nhttps://github.com/reka-ai/reka-vibe-eval",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02287v1",
    "published_date": "2024-05-03 17:59:55 UTC",
    "updated_date": "2024-05-03 17:59:55 UTC"
  },
  {
    "arxiv_id": "2405.02246v1",
    "title": "What matters when building vision-language models?",
    "authors": [
      "Hugo Laurençon",
      "Léo Tronchon",
      "Matthieu Cord",
      "Victor Sanh"
    ],
    "abstract": "The growing interest in vision-language models (VLMs) has been driven by\nimprovements in large language models and vision transformers. Despite the\nabundance of literature on this subject, we observe that critical decisions\nregarding the design of VLMs are often not justified. We argue that these\nunsupported decisions impede progress in the field by making it difficult to\nidentify which choices improve model performance. To address this issue, we\nconduct extensive experiments around pre-trained models, architecture choice,\ndata, and training methods. Our consolidation of findings includes the\ndevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.\nIdefics2 achieves state-of-the-art performance within its size category across\nvarious multimodal benchmarks, and is often on par with models four times its\nsize. We release the model (base, instructed, and chat) along with the datasets\ncreated for its training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02246v1",
    "published_date": "2024-05-03 17:00:00 UTC",
    "updated_date": "2024-05-03 17:00:00 UTC"
  },
  {
    "arxiv_id": "2405.02228v3",
    "title": "Attribution in Scientific Literature: New Benchmark and Methods",
    "authors": [
      "Yash Saxena",
      "Deepa Tilwani",
      "Ali Mohammadi",
      "Edward Raff",
      "Amit Sheth",
      "Srinivasan Parthasarathy",
      "Manas Gaur"
    ],
    "abstract": "Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2405.02228v3",
    "published_date": "2024-05-03 16:38:51 UTC",
    "updated_date": "2025-04-11 07:20:47 UTC"
  },
  {
    "arxiv_id": "2405.02225v1",
    "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
    "authors": [
      "Lujing Zhang",
      "Aaron Roth",
      "Linjun Zhang"
    ],
    "abstract": "This paper introduces a framework for post-processing machine learning models\nso that their predictions satisfy multi-group fairness guarantees. Based on the\ncelebrated notion of multicalibration, we introduce $(\\mathbf{s},\\mathcal{G},\n\\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for\nmulti-dimensional mappings $\\mathbf{s}$, constraint set $\\mathcal{G}$, and a\npre-specified threshold level $\\alpha$. We propose associated algorithms to\nachieve this notion in general settings. This framework is then applied to\ndiverse scenarios encompassing different fairness concerns, including false\nnegative rate control in image segmentation, prediction set conditional\nuncertainty quantification in hierarchical classification, and de-biased text\ngeneration in language models. We conduct numerical studies on several datasets\nand tasks.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "28 pages, 8 figures, accepted by ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02225v1",
    "published_date": "2024-05-03 16:32:09 UTC",
    "updated_date": "2024-05-03 16:32:09 UTC"
  },
  {
    "arxiv_id": "2405.02219v2",
    "title": "A Normative Framework for Benchmarking Consumer Fairness in Large Language Model Recommender System",
    "authors": [
      "Yashar Deldjoo",
      "Fatemeh Nazary"
    ],
    "abstract": "The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\".",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02219v2",
    "published_date": "2024-05-03 16:25:27 UTC",
    "updated_date": "2024-09-11 07:27:51 UTC"
  },
  {
    "arxiv_id": "2405.02213v2",
    "title": "Automatic Programming: Large Language Models and Beyond",
    "authors": [
      "Michael R. Lyu",
      "Baishakhi Ray",
      "Abhik Roychoudhury",
      "Shin Hwei Tan",
      "Patanamon Thongtanunam"
    ],
    "abstract": "Automatic programming has seen increasing popularity due to the emergence of\ntools like GitHub Copilot which rely on Large Language Models (LLMs). At the\nsame time, automatically generated code faces challenges during deployment due\nto concerns around quality and trust. In this article, we study automated\ncoding in a general sense and study the concerns around code quality, security\nand related issues of programmer responsibility. These are key issues for\norganizations while deciding on the usage of automatically generated code. We\ndiscuss how advances in software engineering such as program repair and\nanalysis can enable automatic programming. We conclude with a forward looking\nview, focusing on the programming environment of the near future, where\nprogrammers may need to switch to different roles to fully utilize the power of\nautomatic programming. Automated repair of automatically generated programs\nfrom LLMs, can help produce higher assurance code from LLMs, along with\nevidence of assurance",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02213v2",
    "published_date": "2024-05-03 16:19:24 UTC",
    "updated_date": "2024-05-15 16:33:57 UTC"
  },
  {
    "arxiv_id": "2405.02384v1",
    "title": "CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding",
    "authors": [
      "Kaiyuan Chen",
      "Xingzhuo Guo",
      "Yu Zhang",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "abstract": "Predictive Coding (PC) is a theoretical framework in cognitive science\nsuggesting that the human brain processes cognition through spatiotemporal\nprediction of the visual world. Existing studies have developed spatiotemporal\nprediction neural networks based on the PC theory, emulating its two core\nmechanisms: Correcting predictions from residuals and hierarchical learning.\nHowever, these models do not show the enhancement of prediction skills on\nreal-world forecasting tasks and ignore the Precision Weighting mechanism of PC\ntheory. The precision weighting mechanism posits that the brain allocates more\nattention to signals with lower precision, contributing to the cognitive\nability of human brains. This work introduces the Cognitive Diffusion\nProbabilistic Models (CogDPM), which demonstrate the connection between\ndiffusion probabilistic models and PC theory. CogDPM features a precision\nestimation method based on the hierarchical sampling capabilities of diffusion\nmodels and weight the guidance with precision weights estimated by the inherent\nproperty of diffusion models. We experimentally show that the precision weights\neffectively estimate the data predictability. We apply CogDPM to real-world\nprediction tasks using the United Kindom precipitation and ERA surface wind\ndatasets. Our results demonstrate that CogDPM outperforms both existing\ndomain-specific operational models and general deep prediction models by\nproviding more proficient forecasting.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02384v1",
    "published_date": "2024-05-03 15:54:50 UTC",
    "updated_date": "2024-05-03 15:54:50 UTC"
  },
  {
    "arxiv_id": "2405.02383v1",
    "title": "A Fresh Look at Sanity Checks for Saliency Maps",
    "authors": [
      "Anna Hedström",
      "Leander Weber",
      "Sebastian Lapuschkin",
      "Marina Höhne"
    ],
    "abstract": "The Model Parameter Randomisation Test (MPRT) is highly recognised in the\neXplainable Artificial Intelligence (XAI) community due to its fundamental\nevaluative criterion: explanations should be sensitive to the parameters of the\nmodel they seek to explain. However, recent studies have raised several\nmethodological concerns for the empirical interpretation of MPRT. In response,\nwe propose two modifications to the original test: Smooth MPRT and Efficient\nMPRT. The former reduces the impact of noise on evaluation outcomes via\nsampling, while the latter avoids the need for biased similarity measurements\nby re-interpreting the test through the increase in explanation complexity\nafter full model randomisation. Our experiments show that these modifications\nenhance the metric reliability, facilitating a more trustworthy deployment of\nexplanation methods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "arXiv admin note: text overlap with arXiv:2401.06465",
    "pdf_url": "http://arxiv.org/pdf/2405.02383v1",
    "published_date": "2024-05-03 15:47:32 UTC",
    "updated_date": "2024-05-03 15:47:32 UTC"
  },
  {
    "arxiv_id": "2405.02188v1",
    "title": "Optimistic Regret Bounds for Online Learning in Adversarial Markov Decision Processes",
    "authors": [
      "Sang Bin Moon",
      "Abolfazl Hashemi"
    ],
    "abstract": "The Adversarial Markov Decision Process (AMDP) is a learning framework that\ndeals with unknown and varying tasks in decision-making applications like\nrobotics and recommendation systems. A major limitation of the AMDP formalism,\nhowever, is pessimistic regret analysis results in the sense that although the\ncost function can change from one episode to the next, the evolution in many\nsettings is not adversarial. To address this, we introduce and study a new\nvariant of AMDP, which aims to minimize regret while utilizing a set of cost\npredictors. For this setting, we develop a new policy search method that\nachieves a sublinear optimistic regret with high probability, that is a regret\nbound which gracefully degrades with the estimation power of the cost\npredictors. Establishing such optimistic regret bounds is nontrivial given that\n(i) as we demonstrate, the existing importance-weighted cost estimators cannot\nestablish optimistic bounds, and (ii) the feedback model of AMDP is different\n(and more realistic) than the existing optimistic online learning works. Our\nresult, in particular, hinges upon developing a novel optimistically biased\ncost estimator that leverages cost predictors and enables a high-probability\nregret analysis without imposing restrictive assumptions. We further discuss\npractical extensions of the proposed scheme and demonstrate its efficacy\nnumerically.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02188v1",
    "published_date": "2024-05-03 15:44:31 UTC",
    "updated_date": "2024-05-03 15:44:31 UTC"
  },
  {
    "arxiv_id": "2405.02178v2",
    "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
    "authors": [
      "Negar Arabzadeh",
      "Siqing Huo",
      "Nikhil Mehta",
      "Qinqyun Wu",
      "Chi Wang",
      "Ahmed Awadallah",
      "Charles L. A. Clarke",
      "Julia Kiseleva"
    ],
    "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in\napplications that facilitate collaboration among multiple agents, assisting\nhumans in their daily tasks. However, a significant gap remains in assessing to\nwhat extent LLM-powered applications genuinely enhance user experience and task\nexecution efficiency. This highlights the need to verify utility of LLM-powered\napplications, particularly by ensuring alignment between the application's\nfunctionality and end-user needs. We introduce AgentEval, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the effectiveness and robustness of AgentEval for two\nopen source datasets including Math Problem solving and ALFWorld House-hold\nrelated tasks. For reproducibility purposes, we make the data, code and all the\nlogs publicly available at https://bit.ly/3w3yKcS .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2402.09015",
    "pdf_url": "http://arxiv.org/pdf/2405.02178v2",
    "published_date": "2024-05-03 15:26:27 UTC",
    "updated_date": "2024-05-12 15:52:49 UTC"
  },
  {
    "arxiv_id": "2405.02175v3",
    "title": "Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset",
    "authors": [
      "Hsuvas Borkakoty",
      "Luis Espinosa-Anke"
    ],
    "abstract": "Hoaxes are a recognised form of disinformation created deliberately, with\npotential serious implications in the credibility of reference knowledge\nresources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that\nthey often are written according to the official style guidelines. In this\nwork, we first provide a systematic analysis of similarities and discrepancies\nbetween legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a\ncollection of 311 hoax articles (from existing literature and official\nWikipedia lists), together with semantically similar legitimate articles, which\ntogether form a binary text classification dataset aimed at fostering research\nin automated hoax detection. In this paper, We report results after analyzing\nseveral language models, hoax-to-legit ratios, and the amount of text\nclassifiers are exposed to (full article vs the article's definition alone).\nOur results suggest that detecting deceitful content in Wikipedia based on\ncontent alone is hard but feasible, and complement our analysis with a study on\nthe differences in distributions in edit histories, and find that looking at\nthis feature yields better classification results than context.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02175v3",
    "published_date": "2024-05-03 15:25:48 UTC",
    "updated_date": "2024-08-30 16:40:15 UTC"
  },
  {
    "arxiv_id": "2405.02165v1",
    "title": "EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and Multi-View Transformer",
    "authors": [
      "Hanwen Liu",
      "Daniel Hajialigol",
      "Benny Antony",
      "Aiguo Han",
      "Xuan Wang"
    ],
    "abstract": "Deciphering the intricacies of the human brain has captivated curiosity for\ncenturies. Recent strides in Brain-Computer Interface (BCI) technology,\nparticularly using motor imagery, have restored motor functions such as\nreaching, grasping, and walking in paralyzed individuals. However, unraveling\nnatural language from brain signals remains a formidable challenge.\nElectroencephalography (EEG) is a non-invasive technique used to record\nelectrical activity in the brain by placing electrodes on the scalp. Previous\nstudies of EEG-to-text decoding have achieved high accuracy on small closed\nvocabularies, but still fall short of high accuracy when dealing with large\nopen vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy\nof open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG\npre-training to enhance the learning of semantics from EEG signals and proposes\na multi-view transformer to model the EEG signal processing by different\nspatial regions of the brain. Experiments show that EEG2TEXT has superior\nperformance, outperforming the state-of-the-art baseline methods by a large\nmargin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great\npotential for a high-performance open-vocabulary brain-to-text system to\nfacilitate communication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02165v1",
    "published_date": "2024-05-03 15:14:19 UTC",
    "updated_date": "2024-05-03 15:14:19 UTC"
  },
  {
    "arxiv_id": "2405.02162v3",
    "title": "Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models",
    "authors": [
      "Mohamad Al Mdfaa",
      "Raghad Salameh",
      "Sergey Zagoruyko",
      "Gonzalo Ferrer"
    ],
    "abstract": "In the field of robotics and computer vision, efficient and accurate semantic\nmapping remains a significant challenge due to the growing demand for\nintelligent machines that can comprehend and interact with complex\nenvironments. Conventional panoptic mapping methods, however, are limited by\npredefined semantic classes, thus making them ineffective for handling novel or\nunforeseen objects. In response to this limitation, we introduce the Unified\nPromptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances in\nfoundation models to enable real-time, on-demand label generation using natural\nlanguage prompts. By incorporating a dynamic labeling strategy into traditional\npanoptic mapping techniques, UPPM provides significant improvements in\nadaptability and versatility while maintaining high performance levels in map\nreconstruction. We demonstrate our approach on real-world and simulated\ndatasets. Results show that UPPM can accurately reconstruct scenes and segment\nobjects while generating rich semantic labels through natural language\ninteractions. A series of ablation experiments validated the advantages of\nfoundation model-based labeling over fixed label sets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is under consideration at Pattern Recognition Letters",
    "pdf_url": "http://arxiv.org/pdf/2405.02162v3",
    "published_date": "2024-05-03 15:08:39 UTC",
    "updated_date": "2024-10-10 16:03:42 UTC"
  },
  {
    "arxiv_id": "2405.02161v2",
    "title": "Simulating the Economic Impact of Rationality through Reinforcement Learning and Agent-Based Modelling",
    "authors": [
      "Simone Brusatin",
      "Tommaso Padoan",
      "Andrea Coletta",
      "Domenico Delli Gatti",
      "Aldo Glielmo"
    ],
    "abstract": "Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined 'bounded\nrational' behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of 'fully rational' agents that\nlearn their policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for studying the impact of rationality on the economy.\nWe find that RL agents spontaneously learn three distinct strategies for\nmaximising profits, with the optimal strategy depending on the level of market\ncompetition and rationality. We also find that RL agents with independent\npolicies, and without the ability to communicate with each other, spontaneously\nlearn to segregate into different strategic groups, thus increasing market\npower and overall profits. Finally, we find that a higher number of rational\n(RL) agents in the economy always improves the macroeconomic environment as\nmeasured by total output. Depending on the specific rational policy, this can\ncome at the cost of higher instability. Our R-MABM framework allows for stable\nmulti-agent learning, is available in open source, and represents a principled\nand robust direction to extend economic simulators.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02161v2",
    "published_date": "2024-05-03 15:08:25 UTC",
    "updated_date": "2024-10-21 21:20:29 UTC"
  },
  {
    "arxiv_id": "2405.02151v3",
    "title": "GMP-TL: Gender-augmented Multi-scale Pseudo-label Enhanced Transfer Learning for Speech Emotion Recognition",
    "authors": [
      "Yu Pan",
      "Yuguang Yang",
      "Heng Lu",
      "Lei Ma",
      "Jianjun Zhao"
    ],
    "abstract": "The continuous evolution of pre-trained speech models has greatly advanced\nSpeech Emotion Recognition (SER). However, current research typically relies on\nutterance-level emotion labels, inadequately capturing the complexity of\nemotions within a single utterance. In this paper, we introduce GMP-TL, a novel\nSER framework that employs gender-augmented multi-scale pseudo-label (GMP)\nbased transfer learning to mitigate this gap. Specifically, GMP-TL initially\nuses the pre-trained HuBERT, implementing multi-task learning and multi-scale\nk-means clustering to acquire frame-level GMPs. Subsequently, to fully leverage\nframe-level GMPs and utterance-level emotion labels, a two-stage model\nfine-tuning approach is presented to further optimize GMP-TL. Experiments on\nIEMOCAP show that our GMP-TL attains a WAR of 80.0% and an UAR of 82.0%,\nachieving superior performance compared to state-of-the-art unimodal SER\nmethods while also yielding comparable results to multimodal SER approaches.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to SLT2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02151v3",
    "published_date": "2024-05-03 14:58:46 UTC",
    "updated_date": "2024-09-23 12:32:11 UTC"
  },
  {
    "arxiv_id": "2405.02148v1",
    "title": "Towards a Formal Creativity Theory: Preliminary results in Novelty and Transformativeness",
    "authors": [
      "Luís Espírito Santo",
      "Geraint Wiggins",
      "Amílcar Cardoso"
    ],
    "abstract": "Formalizing creativity-related concepts has been a long-term goal of\nComputational Creativity. To the same end, we explore Formal Learning Theory in\nthe context of creativity. We provide an introduction to the main concepts of\nthis framework and a re-interpretation of terms commonly found in creativity\ndiscussions, proposing formal definitions for novelty and transformational\ncreativity. This formalisation marks the beginning of a research branch we call\nFormal Creativity Theory, exploring how learning can be included as preparation\nfor exploratory behaviour and how learning is a key part of transformational\ncreative behaviour. By employing these definitions, we argue that, while\nnovelty is neither necessary nor sufficient for transformational creativity in\ngeneral, when using an inspiring set, rather than a sequence of experiences, an\nagent actually requires novelty for transformational creativity to occur.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02148v1",
    "published_date": "2024-05-03 14:53:46 UTC",
    "updated_date": "2024-05-03 14:53:46 UTC"
  },
  {
    "arxiv_id": "2405.06668v2",
    "title": "Exposing and Explaining Fake News On-the-Fly",
    "authors": [
      "Francisco de Arriba-Pérez",
      "Silvia García-Méndez",
      "Fátima Leal",
      "Benedita Malheiro",
      "Juan Carlos Burguillo"
    ],
    "abstract": "Social media platforms enable the rapid dissemination and consumption of\ninformation. However, users instantly consume such content regardless of the\nreliability of the shared data. Consequently, the latter crowdsourcing model is\nexposed to manipulation. This work contributes with an explainable and online\nclassification method to recognize fake news in real-time. The proposed method\ncombines both unsupervised and supervised Machine Learning approaches with\nonline created lexica. The profiling is built using creator-, content- and\ncontext-based features using Natural Language Processing techniques. The\nexplainable classification mechanism displays in a dashboard the features\nselected for classification and the prediction confidence. The performance of\nthe proposed solution has been validated with real data sets from Twitter and\nthe results attain 80 % accuracy and macro F-measure. This proposal is the\nfirst to jointly provide data stream processing, profiling, classification and\nexplainability. Ultimately, the proposed early detection, isolation and\nexplanation of fake news contribute to increase the quality and trustworthiness\nof social media contents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06668v2",
    "published_date": "2024-05-03 14:49:04 UTC",
    "updated_date": "2024-09-05 10:07:46 UTC"
  },
  {
    "arxiv_id": "2405.02131v2",
    "title": "Physics-informed generative neural networks for RF propagation prediction with application to indoor body perception",
    "authors": [
      "Federica Fieramosca",
      "Vittorio Rampa",
      "Michele D'Amico",
      "Stefano Savazzi"
    ],
    "abstract": "Electromagnetic (EM) body models designed to predict Radio-Frequency (RF)\npropagation are time-consuming methods which prevent their adoption in strict\nreal-time computational imaging problems, such as human body localization and\nsensing. Physics-informed Generative Neural Network (GNN) models have been\nrecently proposed to reproduce EM effects, namely to simulate or reconstruct\nmissing data or samples by incorporating relevant EM principles and\nconstraints. The paper discusses a Variational Auto-Encoder (VAE) model which\nis trained to reproduce the effects of human motions on the EM field and\nincorporate EM body diffraction principles. Proposed physics-informed\ngenerative neural network models are verified against both classical\ndiffraction-based EM tools and full-wave EM body simulations.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02131v2",
    "published_date": "2024-05-03 14:35:02 UTC",
    "updated_date": "2024-05-15 13:11:52 UTC"
  },
  {
    "arxiv_id": "2405.02124v1",
    "title": "TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on Self-Supervised Learning and Knowledge Transfer",
    "authors": [
      "Noé Tits",
      "Prernna Bhatnagar",
      "Thierry Dutoit"
    ],
    "abstract": "In this paper, we present a novel approach for text independent\nphone-to-audio alignment based on phoneme recognition, representation learning\nand knowledge transfer. Our method leverages a self-supervised model (wav2vec2)\nfine-tuned for phoneme recognition using a Connectionist Temporal\nClassification (CTC) loss, a dimension reduction model and a frame-level\nphoneme classifier trained thanks to forced-alignment labels (using Montreal\nForced Aligner) to produce multi-lingual phonetic representations, thus\nrequiring minimal additional training. We evaluate our model using synthetic\nnative data from the TIMIT dataset and the SCRIBE dataset for American and\nBritish English, respectively. Our proposed model outperforms the\nstate-of-the-art (charsiu) in statistical metrics and has applications in\nlanguage learning and speech processing systems. We leave experiments on other\nlanguages for future work but the design of the system makes it easily\nadaptable to other languages.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02124v1",
    "published_date": "2024-05-03 14:25:21 UTC",
    "updated_date": "2024-05-03 14:25:21 UTC"
  },
  {
    "arxiv_id": "2407.05423v1",
    "title": "A Manifesto for a Pro-Actively Responsible AI in Education",
    "authors": [
      "Kaska Porayska-Pomsta"
    ],
    "abstract": "This paper examines the historical foundations, current practices, and\nemerging challenges for Artificial Intelligence in Education (AIED) within\nbroader AI practices. It highlights AIED's unique and rich potential for\ncontributing to the current AI policy and practices, especially in the context\nof responsible AI. It also discusses the key gaps in the AIED field, which need\nto be addressed by the community to elevate the field from a cottage industry\nto the level where it will deservedly be seen as key to advancin AI research\nand practical applications. The paper offers a five-point manifesto aimed to\nrevitalise AIED' contributions to education and broader AI community,\nsuggesting enhanced interdisciplinary collaboration, a broadened understanding\nof AI's impact on human functioning, and commitment to setting agendas for\nhuman-centred educational innovations.This approach positions AIED to\nsignificantly influence educational technologies to achieve genuine positive\nimpact across diverse societal segments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.3"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05423v1",
    "published_date": "2024-05-03 14:23:41 UTC",
    "updated_date": "2024-05-03 14:23:41 UTC"
  },
  {
    "arxiv_id": "2405.02105v1",
    "title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
    "authors": [
      "Vladyslav Nechakhin",
      "Jennifer D'Souza",
      "Steffen Eger"
    ],
    "abstract": "Structured science summaries or research contributions using properties or\ndimensions beyond traditional keywords enhances science findability. Current\nmethods, such as those used by the Open Research Knowledge Graph (ORKG),\ninvolve manually curating properties to describe research papers' contributions\nin a structured manner, but this is labor-intensive and inconsistent between\nthe domain expert human curators. We propose using Large Language Models (LLMs)\nto automatically suggest these properties. However, it's essential to assess\nthe readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before\napplication. Our study performs a comprehensive comparative analysis between\nORKG's manually curated properties and those generated by the aforementioned\nstate-of-the-art LLMs. We evaluate LLM performance through four unique\nperspectives: semantic alignment and deviation with ORKG properties,\nfine-grained properties mapping accuracy, SciNCL embeddings-based cosine\nsimilarity, and expert surveys comparing manual annotations with LLM outputs.\nThese evaluations occur within a multidisciplinary science setting. Overall,\nLLMs show potential as recommendation systems for structuring science, but\nfurther finetuning is recommended to improve their alignment with scientific\ntasks and mimicry of human expertise.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 11 figures. In review at\n  https://www.mdpi.com/journal/information/special_issues/WYS02U2GTD",
    "pdf_url": "http://arxiv.org/pdf/2405.02105v1",
    "published_date": "2024-05-03 14:03:04 UTC",
    "updated_date": "2024-05-03 14:03:04 UTC"
  },
  {
    "arxiv_id": "2405.02095v2",
    "title": "Advanced Detection of Source Code Clones via an Ensemble of Unsupervised Similarity Measures",
    "authors": [
      "Jorge Martinez-Gil"
    ],
    "abstract": "The capability of accurately determining code similarity is crucial in many\ntasks related to software development. For example, it might be essential to\nidentify code duplicates for performing software maintenance. This research\nintroduces a novel ensemble learning approach for code similarity assessment,\ncombining the strengths of multiple unsupervised similarity measures. The key\nidea is that the strengths of a diverse set of similarity measures can\ncomplement each other and mitigate individual weaknesses, leading to improved\nperformance. Preliminary results show that while Transformers-based CodeBERT\nand its variant GraphCodeBERT are undoubtedly the best option in the presence\nof abundant training data, in the case of specific small datasets (up to 500\nsamples), our ensemble achieves similar results, without prejudice to the\ninterpretability of the resulting solution, and with a much lower associated\ncarbon footprint due to training. The source code of this novel approach can be\ndownloaded from https://github.com/jorge-martinez-gil/ensemble-codesim.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under review in Software Quality Days",
    "pdf_url": "http://arxiv.org/pdf/2405.02095v2",
    "published_date": "2024-05-03 13:42:49 UTC",
    "updated_date": "2024-10-30 14:01:43 UTC"
  },
  {
    "arxiv_id": "2405.02083v2",
    "title": "A fuzzy loss for ontology classification",
    "authors": [
      "Simon Flügel",
      "Martin Glauer",
      "Till Mossakowski",
      "Fabian Neuhaus"
    ],
    "abstract": "Deep learning models are often unaware of the inherent constraints of the\ntask they are applied to. However, many downstream tasks require logical\nconsistency. For ontology classification tasks, such constraints include\nsubsumption and disjointness relations between classes.\n  In order to increase the consistency of deep learning models, we propose a\nfuzzy loss that combines label-based loss with terms penalising subsumption- or\ndisjointness-violations. Our evaluation on the ChEBI ontology shows that the\nfuzzy loss is able to decrease the number of consistency violations by several\norders of magnitude without decreasing the classification performance. In\naddition, we use the fuzzy loss for unsupervised learning. We show that this\ncan further improve consistency on data from a",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02083v2",
    "published_date": "2024-05-03 13:20:37 UTC",
    "updated_date": "2024-08-19 14:42:33 UTC"
  },
  {
    "arxiv_id": "2405.02082v1",
    "title": "A comparative study of conformal prediction methods for valid uncertainty quantification in machine learning",
    "authors": [
      "Nicolas Dewolf"
    ],
    "abstract": "In the past decades, most work in the area of data analysis and machine\nlearning was focused on optimizing predictive models and getting better results\nthan what was possible with existing models. To what extent the metrics with\nwhich such improvements were measured were accurately capturing the intended\ngoal, whether the numerical differences in the resulting values were\nsignificant, or whether uncertainty played a role in this study and if it\nshould have been taken into account, was of secondary importance. Whereas\nprobability theory, be it frequentist or Bayesian, used to be the gold standard\nin science before the advent of the supercomputer, it was quickly replaced in\nfavor of black box models and sheer computing power because of their ability to\nhandle large data sets. This evolution sadly happened at the expense of\ninterpretability and trustworthiness. However, while people are still trying to\nimprove the predictive power of their models, the community is starting to\nrealize that for many applications it is not so much the exact prediction that\nis of importance, but rather the variability or uncertainty.\n  The work in this dissertation tries to further the quest for a world where\neveryone is aware of uncertainty, of how important it is and how to embrace it\ninstead of fearing it. A specific, though general, framework that allows anyone\nto obtain accurate uncertainty estimates is singled out and analysed. Certain\naspects and applications of the framework -- dubbed `conformal prediction' --\nare studied in detail. Whereas many approaches to uncertainty quantification\nmake strong assumptions about the data, conformal prediction is, at the time of\nwriting, the only framework that deserves the title `distribution-free'. No\nparametric assumptions have to be made and the nonparametric results also hold\nwithout having to resort to the law of large numbers in the asymptotic regime.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "At 339 pages, this document is a live/working version of my PhD\n  dissertation published in 2024 by the University of Ghent (UGent)",
    "pdf_url": "http://arxiv.org/pdf/2405.02082v1",
    "published_date": "2024-05-03 13:19:33 UTC",
    "updated_date": "2024-05-03 13:19:33 UTC"
  },
  {
    "arxiv_id": "2405.02079v3",
    "title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification",
    "authors": [
      "Gabriel Freedman",
      "Adam Dejl",
      "Deniz Gorur",
      "Xiang Yin",
      "Antonio Rago",
      "Francesca Toni"
    ],
    "abstract": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 18 figures. Accepted as an oral presentation at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.02079v3",
    "published_date": "2024-05-03 13:12:28 UTC",
    "updated_date": "2025-04-18 11:20:24 UTC"
  },
  {
    "arxiv_id": "2405.02048v1",
    "title": "Comparative Analysis of Retrieval Systems in the Real World",
    "authors": [
      "Dmytro Mozolevskyi",
      "Waseem AlShikh"
    ],
    "abstract": "This research paper presents a comprehensive analysis of integrating advanced\nlanguage models with search and retrieval systems in the fields of information\nretrieval and natural language processing. The objective is to evaluate and\ncompare various state-of-the-art methods based on their performance in terms of\naccuracy and efficiency. The analysis explores different combinations of\ntechnologies, including Azure Cognitive Search Retriever with GPT-4, Pinecone's\nCanopy framework, Langchain with Pinecone and different language models\n(OpenAI, Cohere), LlamaIndex with Weaviate Vector Store's hybrid search,\nGoogle's RAG implementation on Cloud VertexAI-Search, Amazon SageMaker's RAG,\nand a novel approach called KG-FID Retrieval. The motivation for this analysis\narises from the increasing demand for robust and responsive question-answering\nsystems in various domains. The RobustQA metric is used to evaluate the\nperformance of these systems under diverse paraphrasing of questions. The\nreport aims to provide insights into the strengths and weaknesses of each\nmethod, facilitating informed decisions in the deployment and development of\nAI-driven search and retrieval systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02048v1",
    "published_date": "2024-05-03 12:30:01 UTC",
    "updated_date": "2024-05-03 12:30:01 UTC"
  },
  {
    "arxiv_id": "2405.02044v1",
    "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach",
    "authors": [
      "Anton Plaksin",
      "Vitaly Kalev"
    ],
    "abstract": "Robust Reinforcement Learning (RRL) is a promising Reinforcement Learning\n(RL) paradigm aimed at training robust to uncertainty or disturbances models,\nmaking them more efficient for real-world applications. Following this\nparadigm, uncertainty or disturbances are interpreted as actions of a second\nadversarial agent, and thus, the problem is reduced to seeking the agents'\npolicies robust to any opponent's actions. This paper is the first to propose\nconsidering the RRL problems within the positional differential game theory,\nwhich helps us to obtain theoretically justified intuition to develop a\ncentralized Q-learning approach. Namely, we prove that under Isaacs's condition\n(sufficiently general for real-world dynamical systems), the same Q-function\ncan be utilized as an approximate solution of both minimax and maximin Bellman\nequations. Based on these results, we present the Isaacs Deep Q-Network\nalgorithms and demonstrate their superiority compared to other baseline RRL and\nMulti-Agent RL algorithms in various environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "68T07, 49N70"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02044v1",
    "published_date": "2024-05-03 12:21:43 UTC",
    "updated_date": "2024-05-03 12:21:43 UTC"
  },
  {
    "arxiv_id": "2405.02024v1",
    "title": "Analyzing Narrative Processing in Large Language Models (LLMs): Using GPT4 to test BERT",
    "authors": [
      "Patrick Krauss",
      "Jannik Hösch",
      "Claus Metzner",
      "Andreas Maier",
      "Peter Uhrig",
      "Achim Schilling"
    ],
    "abstract": "The ability to transmit and receive complex information via language is\nunique to humans and is the basis of traditions, culture and versatile social\ninteractions. Through the disruptive introduction of transformer based large\nlanguage models (LLMs) humans are not the only entity to \"understand\" and\nproduce language any more. In the present study, we have performed the first\nsteps to use LLMs as a model to understand fundamental mechanisms of language\nprocessing in neural networks, in order to make predictions and generate\nhypotheses on how the human brain does language processing. Thus, we have used\nChatGPT to generate seven different stylistic variations of ten different\nnarratives (Aesop's fables). We used these stories as input for the open source\nLLM BERT and have analyzed the activation patterns of the hidden units of BERT\nusing multi-dimensional scaling and cluster analysis. We found that the\nactivation vectors of the hidden units cluster according to stylistic\nvariations in earlier layers of BERT (1) than narrative content (4-5). Despite\nthe fact that BERT consists of 12 identical building blocks that are stacked\nand trained on large text corpora, the different layers perform different\ntasks. This is a very useful model of the human brain, where self-similar\nstructures, i.e. different areas of the cerebral cortex, can have different\nfunctions and are therefore well suited to processing language in a very\nefficient way. The proposed approach has the potential to open the black box of\nLLMs on the one hand, and might be a further step to unravel the neural\nprocesses underlying human language processing and cognition in general.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02024v1",
    "published_date": "2024-05-03 11:56:13 UTC",
    "updated_date": "2024-05-03 11:56:13 UTC"
  },
  {
    "arxiv_id": "2405.02016v1",
    "title": "Adversarial Botometer: Adversarial Analysis for Social Bot Detection",
    "authors": [
      "Shaghayegh Najari",
      "Davood Rafiee",
      "Mostafa Salehi",
      "Reza Farahbakhsh"
    ],
    "abstract": "Social bots play a significant role in many online social networks (OSN) as\nthey imitate human behavior. This fact raises difficult questions about their\ncapabilities and potential risks. Given the recent advances in Generative AI\n(GenAI), social bots are capable of producing highly realistic and complex\ncontent that mimics human creativity. As the malicious social bots emerge to\ndeceive people with their unrealistic content, identifying them and\ndistinguishing the content they produce has become an actual challenge for\nnumerous social platforms. Several approaches to this problem have already been\nproposed in the literature, but the proposed solutions have not been widely\nevaluated. To address this issue, we evaluate the behavior of a text-based bot\ndetector in a competitive environment where some scenarios are proposed:\n\\textit{First}, the tug-of-war between a bot and a bot detector is examined. It\nis interesting to analyze which party is more likely to prevail and which\ncircumstances influence these expectations. In this regard, we model the\nproblem as a synthetic adversarial game in which a conversational bot and a bot\ndetector are engaged in strategic online interactions. \\textit{Second}, the bot\ndetection model is evaluated under attack examples generated by a social bot;\nto this end, we poison the dataset with attack examples and evaluate the model\nperformance under this condition. \\textit{Finally}, to investigate the impact\nof the dataset, a cross-domain analysis is performed. Through our comprehensive\nevaluation of different categories of social bots using two benchmark datasets,\nwe were able to demonstrate some achivement that could be utilized in future\nworks.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02016v1",
    "published_date": "2024-05-03 11:28:21 UTC",
    "updated_date": "2024-05-03 11:28:21 UTC"
  },
  {
    "arxiv_id": "2405.02375v2",
    "title": "The Sparse Tsetlin Machine: Sparse Representation with Active Literals",
    "authors": [
      "Sebastian Østby",
      "Tobias M. Brambo",
      "Sondre Glimsdal"
    ],
    "abstract": "This paper introduces the Sparse Tsetlin Machine (STM), a novel Tsetlin\nMachine (TM) that processes sparse data efficiently. Traditionally, the TM does\nnot consider data characteristics such as sparsity, commonly seen in NLP\napplications and other bag-of-word-based representations. Consequently, a TM\nmust initialize, store, and process a significant number of zero values,\nresulting in excessive memory usage and computational time. Previous attempts\nat creating a sparse TM have predominantly been unsuccessful, primarily due to\ntheir inability to identify which literals are sufficient for TM training. By\nintroducing Active Literals (AL), the STM can focus exclusively on literals\nthat actively contribute to the current data representation, significantly\ndecreasing memory footprint and computational time while demonstrating\ncompetitive classification performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02375v2",
    "published_date": "2024-05-03 11:06:10 UTC",
    "updated_date": "2024-05-11 04:40:53 UTC"
  },
  {
    "arxiv_id": "2405.01997v1",
    "title": "Exploring Combinatorial Problem Solving with Large Language Models: A Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo",
    "authors": [
      "Mahmoud Masoud",
      "Ahmed Abdelhay",
      "Mohammed Elhenawy"
    ],
    "abstract": "Large Language Models (LLMs) are deep learning models designed to generate\ntext based on textual input. Although researchers have been developing these\nmodels for more complex tasks such as code generation and general reasoning,\nfew efforts have explored how LLMs can be applied to combinatorial problems. In\nthis research, we investigate the potential of LLMs to solve the Travelling\nSalesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments\nemploying various approaches, including zero-shot in-context learning, few-shot\nin-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned\nGPT-3.5 Turbo to solve a specific problem size and tested it using a set of\nvarious instance sizes. The fine-tuned models demonstrated promising\nperformance on problems identical in size to the training instances and\ngeneralized well to larger problems. Furthermore, to improve the performance of\nthe fine-tuned model without incurring additional training costs, we adopted a\nself-ensemble approach to improve the quality of the solutions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01997v1",
    "published_date": "2024-05-03 10:54:14 UTC",
    "updated_date": "2024-05-03 10:54:14 UTC"
  },
  {
    "arxiv_id": "2405.01988v1",
    "title": "Joint sentiment analysis of lyrics and audio in music",
    "authors": [
      "Lea Schaab",
      "Anna Kruspe"
    ],
    "abstract": "Sentiment or mood can express themselves on various levels in music. In\nautomatic analysis, the actual audio data is usually analyzed, but the lyrics\ncan also play a crucial role in the perception of moods. We first evaluate\nvarious models for sentiment analysis based on lyrics and audio separately. The\ncorresponding approaches already show satisfactory results, but they also\nexhibit weaknesses, the causes of which we examine in more detail. Furthermore,\ndifferent approaches to combining the audio and lyrics results are proposed and\nevaluated. Considering both modalities generally leads to improved performance.\nWe investigate misclassifications and (also intentional) contradictions between\naudio and lyrics sentiment more closely, and identify possible causes. Finally,\nwe address fundamental problems in this research area, such as high\nsubjectivity, lack of data, and inconsistency in emotion taxonomies.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "published at DAGA 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01988v1",
    "published_date": "2024-05-03 10:42:17 UTC",
    "updated_date": "2024-05-03 10:42:17 UTC"
  },
  {
    "arxiv_id": "2405.01984v1",
    "title": "A Penalty-Based Guardrail Algorithm for Non-Decreasing Optimization with Inequality Constraints",
    "authors": [
      "Ksenija Stepanovic",
      "Wendelin Böhmer",
      "Mathijs de Weerdt"
    ],
    "abstract": "Traditional mathematical programming solvers require long computational times\nto solve constrained minimization problems of complex and large-scale physical\nsystems. Therefore, these problems are often transformed into unconstrained\nones, and solved with computationally efficient optimization approaches based\non first-order information, such as the gradient descent method. However, for\nunconstrained problems, balancing the minimization of the objective function\nwith the reduction of constraint violations is challenging. We consider the\nclass of time-dependent minimization problems with increasing (possibly)\nnonlinear and non-convex objective function and non-decreasing (possibly)\nnonlinear and non-convex inequality constraints. To efficiently solve them, we\npropose a penalty-based guardrail algorithm (PGA). This algorithm adapts a\nstandard penalty-based method by dynamically updating the right-hand side of\nthe constraints with a guardrail variable which adds a margin to prevent\nviolations. We evaluate PGA on two novel application domains: a simplified\nmodel of a district heating system and an optimization model derived from\nlearned deep neural networks. Our method significantly outperforms mathematical\nprogramming solvers and the standard penalty-based method, and achieves better\nperformance and faster convergence than a state-of-the-art algorithm (IPDD)\nwithin a specified time limit.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01984v1",
    "published_date": "2024-05-03 10:37:34 UTC",
    "updated_date": "2024-05-03 10:37:34 UTC"
  },
  {
    "arxiv_id": "2405.02374v1",
    "title": "Protein binding affinity prediction under multiple substitutions applying eGNNs on Residue and Atomic graphs combined with Language model information: eGRAL",
    "authors": [
      "Arturo Fiorellini-Bernardis",
      "Sebastien Boyer",
      "Christoph Brunken",
      "Bakary Diallo",
      "Karim Beguir",
      "Nicolas Lopez-Carranza",
      "Oliver Bent"
    ],
    "abstract": "Protein-protein interactions (PPIs) play a crucial role in numerous\nbiological processes. Developing methods that predict binding affinity changes\nunder substitution mutations is fundamental for modelling and re-engineering\nbiological systems. Deep learning is increasingly recognized as a powerful tool\ncapable of bridging the gap between in-silico predictions and in-vitro\nobservations. With this contribution, we propose eGRAL, a novel SE(3)\nequivariant graph neural network (eGNN) architecture designed for predicting\nbinding affinity changes from multiple amino acid substitutions in protein\ncomplexes. eGRAL leverages residue, atomic and evolutionary scales, thanks to\nfeatures extracted from protein large language models. To address the limited\navailability of large-scale affinity assays with structural information, we\ngenerate a simulated dataset comprising approximately 500,000 data points. Our\nmodel is pre-trained on this dataset, then fine-tuned and tested on\nexperimental data.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02374v1",
    "published_date": "2024-05-03 10:33:19 UTC",
    "updated_date": "2024-05-03 10:33:19 UTC"
  },
  {
    "arxiv_id": "2405.01983v1",
    "title": "Model-based reinforcement learning for protein backbone design",
    "authors": [
      "Frederic Renard",
      "Cyprien Courtot",
      "Alfredo Reichlin",
      "Oliver Bent"
    ],
    "abstract": "Designing protein nanomaterials of predefined shape and characteristics has\nthe potential to dramatically impact the medical industry. Machine learning\n(ML) has proven successful in protein design, reducing the need for expensive\nwet lab experiment rounds. However, challenges persist in efficiently exploring\nthe protein fitness landscapes to identify optimal protein designs. In\nresponse, we propose the use of AlphaZero to generate protein backbones,\nmeeting shape and structural scoring requirements. We extend an existing Monte\nCarlo tree search (MCTS) framework by incorporating a novel threshold-based\nreward and secondary objectives to improve design precision. This innovation\nconsiderably outperforms existing approaches, leading to protein backbones that\nbetter respect structural scores. The application of AlphaZero is novel in the\ncontext of protein backbone design and demonstrates promising performance.\nAlphaZero consistently surpasses baseline MCTS by more than 100% in top-down\nprotein design tasks. Additionally, our application of AlphaZero with secondary\nobjectives uncovers further promising outcomes, indicating the potential of\nmodel-based reinforcement learning (RL) in navigating the intricate and nuanced\naspects of protein design",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01983v1",
    "published_date": "2024-05-03 10:24:33 UTC",
    "updated_date": "2024-05-03 10:24:33 UTC"
  },
  {
    "arxiv_id": "2405.02372v2",
    "title": "Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence",
    "authors": [
      "Yancheng Huang",
      "Kai Yang",
      "Zelin Zhu",
      "Leian Chen"
    ],
    "abstract": "The primary goal of online change detection (OCD) is to promptly identify\nchanges in the data stream. OCD problem find a wide variety of applications in\ndiverse areas, e.g., security detection in smart grids and intrusion detection\nin communication networks. Prior research usually assumes precise knowledge of\nthe system parameters. Nevertheless, this presumption often proves unattainable\nin practical scenarios due to factors such as estimation errors, system\nupdates, etc. This paper aims to take the first attempt to develop a\ntriadic-OCD framework with certifiable robustness, provable optimality, and\nguaranteed convergence. In addition, the proposed triadic-OCD algorithm can be\nrealized in a fully asynchronous distributed manner, easing the necessity of\ntransmitting the data to a single server. This asynchronous mechanism could\nalso mitigate the straggler issue that faced by traditional synchronous\nalgorithm. Moreover, the non-asymptotic convergence property of Triadic-OCD is\ntheoretically analyzed, and its iteration complexity to achieve an\n$\\epsilon$-optimal point is derived. Extensive experiments have been conducted\nto elucidate the effectiveness of the proposed method.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted at ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02372v2",
    "published_date": "2024-05-03 10:10:11 UTC",
    "updated_date": "2024-06-04 11:40:50 UTC"
  },
  {
    "arxiv_id": "2405.01974v1",
    "title": "Multitask Extension of Geometrically Aligned Transfer Encoder",
    "authors": [
      "Sung Moon Ko",
      "Sumin Lee",
      "Dae-Woong Jeong",
      "Hyunseung Kim",
      "Chanhui Lee",
      "Soorin Yim",
      "Sehui Han"
    ],
    "abstract": "Molecular datasets often suffer from a lack of data. It is well-known that\ngathering data is difficult due to the complexity of experimentation or\nsimulation involved. Here, we leverage mutual information across different\ntasks in molecular data to address this issue. We extend an algorithm that\nutilizes the geometric characteristics of the encoding space, known as the\nGeometrically Aligned Transfer Encoder (GATE), to a multi-task setup. Thus, we\nconnect multiple molecular tasks by aligning the curved coordinates onto\nlocally flat coordinates, ensuring the flow of information from source tasks to\nsupport performance on target data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.01974v1",
    "published_date": "2024-05-03 09:57:44 UTC",
    "updated_date": "2024-05-03 09:57:44 UTC"
  },
  {
    "arxiv_id": "2405.01963v1",
    "title": "From Attack to Defense: Insights into Deep Learning Security Measures in Black-Box Settings",
    "authors": [
      "Firuz Juraev",
      "Mohammed Abuhamad",
      "Eric Chan-Tin",
      "George K. Thiruvathukal",
      "Tamer Abuhmed"
    ],
    "abstract": "Deep Learning (DL) is rapidly maturing to the point that it can be used in\nsafety- and security-crucial applications. However, adversarial samples, which\nare undetectable to the human eye, pose a serious threat that can cause the\nmodel to misbehave and compromise the performance of such applications.\nAddressing the robustness of DL models has become crucial to understanding and\ndefending against adversarial attacks. In this study, we perform comprehensive\nexperiments to examine the effect of adversarial attacks and defenses on\nvarious model architectures across well-known datasets. Our research focuses on\nblack-box attacks such as SimBA, HopSkipJump, MGAAttack, and boundary attacks,\nas well as preprocessor-based defensive mechanisms, including bits squeezing,\nmedian smoothing, and JPEG filter. Experimenting with various models, our\nresults demonstrate that the level of noise needed for the attack increases as\nthe number of layers increases. Moreover, the attack success rate decreases as\nthe number of layers increases. This indicates that model complexity and\nrobustness have a significant relationship. Investigating the diversity and\nrobustness relationship, our experiments with diverse models show that having a\nlarge number of parameters does not imply higher robustness. Our experiments\nextend to show the effects of the training dataset on model robustness. Using\nvarious datasets such as ImageNet-1000, CIFAR-100, and CIFAR-10 are used to\nevaluate the black-box attacks. Considering the multiple dimensions of our\nanalysis, e.g., model complexity and training dataset, we examined the behavior\nof black-box attacks when models apply defenses. Our results show that applying\ndefense strategies can significantly reduce attack effectiveness. This research\nprovides in-depth analysis and insight into the robustness of DL models against\nvarious attacks, and defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01963v1",
    "published_date": "2024-05-03 09:40:47 UTC",
    "updated_date": "2024-05-03 09:40:47 UTC"
  },
  {
    "arxiv_id": "2405.02371v1",
    "title": "Architecture of a Cortex Inspired Hierarchical Event Recaller",
    "authors": [
      "Valentin Puente Varona"
    ],
    "abstract": "This paper proposes a new approach to Machine Learning (ML) that focuses on\nunsupervised continuous context-dependent learning of complex patterns.\nAlthough the proposal is partly inspired by some of the current knowledge about\nthe structural and functional properties of the mammalian brain, we do not\nclaim that biological systems work in an analogous way (nor the opposite).\nBased on some properties of the cerebellar cortex and adjacent structures, a\nproposal suitable for practical problems is presented. A synthetic structure\ncapable of identifying and predicting complex temporal series will be defined\nand experimentally tested. The system relies heavily on prediction to help\nidentify and learn patterns based on previously acquired contextual knowledge.\nAs a proof of concept, the proposed system is shown to be able to learn,\nidentify and predict a remarkably complex temporal series such as human speech,\nwith no prior knowledge. From raw data, without any adaptation in the core\nalgorithm, the system is able to identify certain speech structures from a set\nof Spanish sentences. Unlike conventional ML, the proposal can learn with a\nreduced training set. Although the idea can be applied to a constrained\nproblem, such as the detection of unknown vocabulary in a speech, it could be\nused in more applications, such as vision, or (by incorporating the missing\nbiological periphery) fit into other ML techniques. Given the trivial\ncomputational primitives used, a potential hardware implementation will be\nremarkably frugal. Coincidentally, the proposed model not only conforms to a\nplausible functional framework for biological systems but may also explain many\nelusive cognitive phenomena.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02371v1",
    "published_date": "2024-05-03 09:36:16 UTC",
    "updated_date": "2024-05-03 09:36:16 UTC"
  },
  {
    "arxiv_id": "2405.02370v1",
    "title": "Neuromorphic Correlates of Artificial Consciousness",
    "authors": [
      "Anwaar Ulhaq"
    ],
    "abstract": "The concept of neural correlates of consciousness (NCC), which suggests that\nspecific neural activities are linked to conscious experiences, has gained\nwidespread acceptance. This acceptance is based on a wealth of evidence from\nexperimental studies, brain imaging techniques such as fMRI and EEG, and\ntheoretical frameworks like integrated information theory (IIT) within\nneuroscience and the philosophy of mind. This paper explores the potential for\nartificial consciousness by merging neuromorphic design and architecture with\nbrain simulations. It proposes the Neuromorphic Correlates of Artificial\nConsciousness (NCAC) as a theoretical framework. While the debate on artificial\nconsciousness remains contentious due to our incomplete grasp of consciousness,\nthis work may raise eyebrows and invite criticism. Nevertheless, this\noptimistic and forward-thinking approach is fueled by insights from the Human\nBrain Project, advancements in brain imaging like EEG and fMRI, and recent\nstrides in AI and computing, including quantum and neuromorphic designs.\nAdditionally, this paper outlines how machine learning can play a role in\ncrafting artificial consciousness, aiming to realise machine consciousness and\nawareness in the future.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "13 Pages, 8 Figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02370v1",
    "published_date": "2024-05-03 09:27:51 UTC",
    "updated_date": "2024-05-03 09:27:51 UTC"
  },
  {
    "arxiv_id": "2405.01952v1",
    "title": "Three Quantization Regimes for ReLU Networks",
    "authors": [
      "Weigutian Ou",
      "Philipp Schenkel",
      "Helmut Bölcskei"
    ],
    "abstract": "We establish the fundamental limits in the approximation of Lipschitz\nfunctions by deep ReLU neural networks with finite-precision weights.\nSpecifically, three regimes, namely under-, over-, and proper quantization, in\nterms of minimax approximation error behavior as a function of network weight\nprecision, are identified. This is accomplished by deriving nonasymptotic tight\nlower and upper bounds on the minimax approximation error. Notably, in the\nproper-quantization regime, neural networks exhibit memory-optimality in the\napproximation of Lipschitz functions. Deep networks have an inherent advantage\nover shallow networks in achieving memory-optimality. We also develop the\nnotion of depth-precision tradeoff, showing that networks with high-precision\nweights can be converted into functionally equivalent deeper networks with\nlow-precision weights, while preserving memory-optimality. This idea is\nreminiscent of sigma-delta analog-to-digital conversion, where oversampling\nrate is traded for resolution in the quantization of signal samples. We improve\nupon the best-known ReLU network approximation results for Lipschitz functions\nand describe a refinement of the bit extraction technique which could be of\nindependent general interest.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01952v1",
    "published_date": "2024-05-03 09:27:31 UTC",
    "updated_date": "2024-05-03 09:27:31 UTC"
  },
  {
    "arxiv_id": "2405.01943v3",
    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
    "authors": [
      "Zhiyu Guo",
      "Hidetaka Kamigaito",
      "Taro Wanatnabe"
    ],
    "abstract": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced\nthe capabilities of language understanding and generation. However, the\nsubstantial model size poses hardware challenges, affecting both memory size\nfor serving and inference latency for token generation. To address those\nchallenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a\nnovel method for the recent prevalent GLU-based LLMs pruning, which\nincorporates structural dependency into the weight magnitude-based unstructured\npruning. We introduce an MLP-specific pruning metric that evaluates the\nimportance of each weight by jointly considering its magnitude and its\ncorresponding MLP intermediate activation norms. DaSS facilitates a balance\nbetween the adaptability offered by unstructured pruning and the structural\nconsistency inherent in dependency-based structured pruning. Empirical\nevaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS\nnot only outperforms both SparseGPT and Wanda in achieving hardware-friendly\nN:M sparsity patterns but also maintains the computational efficiency of Wanda.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01943v3",
    "published_date": "2024-05-03 09:13:13 UTC",
    "updated_date": "2024-10-20 12:27:26 UTC"
  },
  {
    "arxiv_id": "2405.02369v1",
    "title": "No One-Size-Fits-All Neurons: Task-based Neurons for Artificial Neural Networks",
    "authors": [
      "Feng-Lei Fan",
      "Meng Wang",
      "Hang-Cheng Dong",
      "Jianwei Ma",
      "Tieyong Zeng"
    ],
    "abstract": "Biologically, the brain does not rely on a single type of neuron that\nuniversally functions in all aspects. Instead, it acts as a sophisticated\ndesigner of task-based neurons. In this study, we address the following\nquestion: since the human brain is a task-based neuron user, can the artificial\nnetwork design go from the task-based architecture design to the task-based\nneuron design? Since methodologically there are no one-size-fits-all neurons,\ngiven the same structure, task-based neurons can enhance the feature\nrepresentation ability relative to the existing universal neurons due to the\nintrinsic inductive bias for the task. Specifically, we propose a two-step\nframework for prototyping task-based neurons. First, symbolic regression is\nused to identify optimal formulas that fit input data by utilizing base\nfunctions such as logarithmic, trigonometric, and exponential functions. We\nintroduce vectorized symbolic regression that stacks all variables in a vector\nand regularizes each input variable to perform the same computation, which can\nexpedite the regression speed, facilitate parallel computation, and avoid\noverfitting. Second, we parameterize the acquired elementary formula to make\nparameters learnable, which serves as the aggregation function of the neuron.\nThe activation functions such as ReLU and the sigmoidal functions remain the\nsame because they have proven to be good. Empirically, experimental results on\nsynthetic data, classic benchmarks, and real-world applications show that the\nproposed task-based neuron design is not only feasible but also delivers\ncompetitive performance over other state-of-the-art models.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02369v1",
    "published_date": "2024-05-03 09:12:46 UTC",
    "updated_date": "2024-05-03 09:12:46 UTC"
  },
  {
    "arxiv_id": "2405.01934v1",
    "title": "Impact of Architectural Modifications on Deep Learning Adversarial Robustness",
    "authors": [
      "Firuz Juraev",
      "Mohammed Abuhamad",
      "Simon S. Woo",
      "George K Thiruvathukal",
      "Tamer Abuhmed"
    ],
    "abstract": "Rapid advancements of deep learning are accelerating adoption in a wide\nvariety of applications, including safety-critical applications such as\nself-driving vehicles, drones, robots, and surveillance systems. These\nadvancements include applying variations of sophisticated techniques that\nimprove the performance of models. However, such models are not immune to\nadversarial manipulations, which can cause the system to misbehave and remain\nunnoticed by experts. The frequency of modifications to existing deep learning\nmodels necessitates thorough analysis to determine the impact on models'\nrobustness. In this work, we present an experimental evaluation of the effects\nof model modifications on deep learning model robustness using adversarial\nattacks. Our methodology involves examining the robustness of variations of\nmodels against various adversarial attacks. By conducting our experiments, we\naim to shed light on the critical issue of maintaining the reliability and\nsafety of deep learning models in safety- and security-critical applications.\nOur results indicate the pressing demand for an in-depth assessment of the\neffects of model changes on the robustness of models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01934v1",
    "published_date": "2024-05-03 08:58:38 UTC",
    "updated_date": "2024-05-03 08:58:38 UTC"
  },
  {
    "arxiv_id": "2405.01924v2",
    "title": "Semi-Parametric Retrieval via Binary Bag-of-Tokens Index",
    "authors": [
      "Jiawei Zhou",
      "Li Dong",
      "Furu Wei",
      "Lei Chen"
    ],
    "abstract": "Information retrieval has transitioned from standalone systems into essential\ncomponents across broader applications, with indexing efficiency,\ncost-effectiveness, and freshness becoming increasingly critical yet often\noverlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval\n(SiDR), a bi-encoder retrieval framework that decouples retrieval index from\nneural parameters to enable efficient, low-cost, and parameter-agnostic\nindexing for emerging use cases. Specifically, in addition to using embeddings\nas indexes like existing neural retrieval methods, SiDR supports a\nnon-parametric tokenization index for search, achieving BM25-like indexing\ncomplexity with significantly better effectiveness. Our comprehensive\nevaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms\nboth neural and term-based retrieval baselines under the same indexing\nworkload: (i) When using an embedding-based index, SiDR exceeds the performance\nof conventional neural retrievers while maintaining similar training\ncomplexity; (ii) When using a tokenization-based index, SiDR drastically\nreduces indexing cost and time, matching the complexity of traditional\nterm-based retrieval, while consistently outperforming BM25 on all in-domain\ndatasets; (iii) Additionally, we introduce a late parametric mechanism that\nmatches BM25 index preparation time while outperforming other neural retrieval\nbaselines in effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01924v2",
    "published_date": "2024-05-03 08:34:13 UTC",
    "updated_date": "2025-03-06 10:39:48 UTC"
  },
  {
    "arxiv_id": "2405.01906v1",
    "title": "Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization",
    "authors": [
      "Changliang Zhou",
      "Xi Lin",
      "Zhenkun Wang",
      "Xialiang Tong",
      "Mingxuan Yuan",
      "Qingfu Zhang"
    ],
    "abstract": "The neural combinatorial optimization (NCO) approach has shown great\npotential for solving routing problems without the requirement of expert\nknowledge. However, existing constructive NCO methods cannot directly solve\nlarge-scale instances, which significantly limits their application prospects.\nTo address these crucial shortcomings, this work proposes a novel\nInstance-Conditioned Adaptation Model (ICAM) for better large-scale\ngeneralization of neural combinatorial optimization. In particular, we design a\npowerful yet lightweight instance-conditioned adaptation module for the NCO\nmodel to generate better solutions for instances across different scales. In\naddition, we develop an efficient three-stage reinforcement learning-based\ntraining scheme that enables the model to learn cross-scale features without\nany labeled optimal solution. Experimental results show that our proposed\nmethod is capable of obtaining excellent results with a very fast inference\ntime in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle\nRouting Problems (CVRPs) across different scales. To the best of our knowledge,\nour model achieves state-of-the-art performance among all RL-based constructive\nmethods for TSP and CVRP with up to 1,000 nodes.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01906v1",
    "published_date": "2024-05-03 08:00:19 UTC",
    "updated_date": "2024-05-03 08:00:19 UTC"
  },
  {
    "arxiv_id": "2405.01886v1",
    "title": "Aloe: A Family of Fine-tuned Open Healthcare LLMs",
    "authors": [
      "Ashwin Kumar Gururajan",
      "Enrique Lopez-Cuena",
      "Jordi Bayarri-Planas",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Pablo Bernabeu-Perez",
      "Anna Arias-Duart",
      "Pablo Agustin Martin-Torres",
      "Lucia Urcelay-Ganzabal",
      "Marta Gonzalez-Mallo",
      "Sergio Alvarez-Napagao",
      "Eduard Ayguadé-Parra",
      "Ulises Cortés Dario Garcia-Gasulla"
    ],
    "abstract": "As the capabilities of Large Language Models (LLMs) in healthcare and\nmedicine continue to advance, there is a growing need for competitive\nopen-source models that can safeguard public interest. With the increasing\navailability of highly competitive open base models, the impact of continued\npre-training is increasingly uncertain. In this work, we explore the role of\ninstruct tuning, model merging, alignment, red teaming and advanced inference\nschemes, as means to improve current open models. To that end, we introduce the\nAloe family, a set of open medical LLMs highly competitive within its scale\nrange. Aloe models are trained on the current best base models (Mistral, LLaMA\n3), using a new custom dataset which combines public data sources improved with\nsynthetic Chain of Thought (CoT). Aloe models undergo an alignment phase,\nbecoming one of the first few policy-aligned open healthcare LLM using Direct\nPreference Optimization, setting a new standard for ethical performance in\nhealthcare LLMs. Model evaluation expands to include various bias and toxicity\ndatasets, a dedicated red teaming effort, and a much-needed risk assessment for\nhealthcare LLMs. Finally, to explore the limits of current LLMs in inference,\nwe study several advanced prompt engineering strategies to boost performance\nacross benchmarks, yielding state-of-the-art results for open healthcare 7B\nLLMs, unprecedented at this scale.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Five appendix",
    "pdf_url": "http://arxiv.org/pdf/2405.01886v1",
    "published_date": "2024-05-03 07:14:07 UTC",
    "updated_date": "2024-05-03 07:14:07 UTC"
  },
  {
    "arxiv_id": "2405.01882v1",
    "title": "Millimeter Wave Radar-based Human Activity Recognition for Healthcare Monitoring Robot",
    "authors": [
      "Zhanzhong Gu",
      "Xiangjian He",
      "Gengfa Fang",
      "Chengpei Xu",
      "Feng Xia",
      "Wenjing Jia"
    ],
    "abstract": "Healthcare monitoring is crucial, especially for the daily care of elderly\nindividuals living alone. It can detect dangerous occurrences, such as falls,\nand provide timely alerts to save lives. Non-invasive millimeter wave (mmWave)\nradar-based healthcare monitoring systems using advanced human activity\nrecognition (HAR) models have recently gained significant attention. However,\nthey encounter challenges in handling sparse point clouds, achieving real-time\ncontinuous classification, and coping with limited monitoring ranges when\nstatically mounted. To overcome these limitations, we propose RobHAR, a movable\nrobot-mounted mmWave radar system with lightweight deep neural networks for\nreal-time monitoring of human activities. Specifically, we first propose a\nsparse point cloud-based global embedding to learn the features of point clouds\nusing the light-PointNet (LPN) backbone. Then, we learn the temporal pattern\nwith a bidirectional lightweight LSTM model (BiLiLSTM). In addition, we\nimplement a transition optimization strategy, integrating the Hidden Markov\nModel (HMM) with Connectionist Temporal Classification (CTC) to improve the\naccuracy and robustness of the continuous HAR. Our experiments on three\ndatasets indicate that our method significantly outperforms the previous\nstudies in both discrete and continuous HAR tasks. Finally, we deploy our\nsystem on a movable robot-mounted edge computing platform, achieving flexible\nhealthcare monitoring in real-world scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01882v1",
    "published_date": "2024-05-03 06:57:59 UTC",
    "updated_date": "2024-05-03 06:57:59 UTC"
  },
  {
    "arxiv_id": "2405.01859v2",
    "title": "AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research",
    "authors": [
      "Riley Simmons-Edler",
      "Ryan Badman",
      "Shayne Longpre",
      "Kanaka Rajan"
    ],
    "abstract": "The recent embrace of machine learning (ML) in the development of autonomous\nweapons systems (AWS) creates serious risks to geopolitical stability and the\nfree exchange of ideas in AI research. This topic has received comparatively\nlittle attention of late compared to risks stemming from superintelligent\nartificial general intelligence (AGI), but requires fewer assumptions about the\ncourse of technological development and is thus a nearer-future issue. ML is\nalready enabling the substitution of AWS for human soldiers in many battlefield\nroles, reducing the upfront human cost, and thus political cost, of waging\noffensive war. In the case of peer adversaries, this increases the likelihood\nof \"low intensity\" conflicts which risk escalation to broader warfare. In the\ncase of non-peer adversaries, it reduces the domestic blowback to wars of\naggression. This effect can occur regardless of other ethical issues around the\nuse of military AI such as the risk of civilian casualties, and does not\nrequire any superhuman AI capabilities. Further, the military value of AWS\nraises the specter of an AI-powered arms race and the misguided imposition of\nnational security restrictions on AI research. Our goal in this paper is to\nraise awareness among the public and ML researchers on the near-future risks\nposed by full or near-full autonomy in military technology, and we provide\nregulatory suggestions to mitigate these risks. We call upon AI policy experts\nand the defense AI community in particular to embrace transparency and caution\nin their development and deployment of AWS to avoid the negative effects on\nglobal stability and AI research that we highlight here.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 1 figure, in ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01859v2",
    "published_date": "2024-05-03 05:19:45 UTC",
    "updated_date": "2024-05-31 23:28:13 UTC"
  },
  {
    "arxiv_id": "2405.01851v1",
    "title": "Deep Learning Inference on Heterogeneous Mobile Processors: Potentials and Pitfalls",
    "authors": [
      "Sicong Liu",
      "Wentao Zhou",
      "Zimu Zhou",
      "Bin Guo",
      "Minfan Wang",
      "Cheng Fang",
      "Zheng Lin",
      "Zhiwen Yu"
    ],
    "abstract": "There is a growing demand to deploy computation-intensive deep learning (DL)\nmodels on resource-constrained mobile devices for real-time intelligent\napplications. Equipped with a variety of processing units such as CPUs, GPUs,\nand NPUs, the mobile devices hold potential to accelerate DL inference via\nparallel execution across heterogeneous processors. Various efficient parallel\nmethods have been explored to optimize computation distribution, achieve load\nbalance, and minimize communication cost across processors. Yet their practical\neffectiveness in the dynamic and diverse real-world mobile environment is less\nexplored. This paper presents a holistic empirical study to assess the\ncapabilities and challenges associated with parallel DL inference on\nheterogeneous mobile processors. Through carefully designed experiments\ncovering various DL models, mobile software/hardware environments, workload\npatterns, and resource availability, we identify limitations of existing\ntechniques and highlight opportunities for cross-level optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01851v1",
    "published_date": "2024-05-03 04:47:23 UTC",
    "updated_date": "2024-05-03 04:47:23 UTC"
  },
  {
    "arxiv_id": "2405.01847v1",
    "title": "A Model-based Multi-Agent Personalized Short-Video Recommender System",
    "authors": [
      "Peilun Zhou",
      "Xiaoxiao Xu",
      "Lantao Hu",
      "Han Li",
      "Peng Jiang"
    ],
    "abstract": "Recommender selects and presents top-K items to the user at each online\nrequest, and a recommendation session consists of several sequential requests.\nFormulating a recommendation session as a Markov decision process and solving\nit by reinforcement learning (RL) framework has attracted increasing attention\nfrom both academic and industry communities. In this paper, we propose a\nRL-based industrial short-video recommender ranking framework, which models and\nmaximizes user watch-time in an environment of user multi-aspect preferences by\na collaborative multi-agent formulization. Moreover, our proposed framework\nadopts a model-based learning approach to alleviate the sample selection bias\nwhich is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of\nour proposed method over alternatives. Our proposed approach has been deployed\nin our real large-scale short-video sharing platform, successfully serving over\nhundreds of millions users.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01847v1",
    "published_date": "2024-05-03 04:34:36 UTC",
    "updated_date": "2024-05-03 04:34:36 UTC"
  },
  {
    "arxiv_id": "2405.01843v5",
    "title": "Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization",
    "authors": [
      "Mudit Gaur",
      "Amrit Singh Bedi",
      "Di Wang",
      "Vaneet Aggarwal"
    ],
    "abstract": "The current state-of-the-art theoretical analysis of Actor-Critic (AC)\nalgorithms significantly lags in addressing the practical aspects of AC\nimplementations. This crucial gap needs bridging to bring the analysis in line\nwith practical implementations of AC. To address this, we advocate for\nconsidering the MMCLG criteria: \\textbf{M}ulti-layer neural network\nparametrization for actor/critic, \\textbf{M}arkovian sampling,\n\\textbf{C}ontinuous state-action spaces, the performance of the \\textbf{L}ast\niterate, and \\textbf{G}lobal optimality. These aspects are practically\nsignificant and have been largely overlooked in existing theoretical analyses\nof AC algorithms. In this work, we address these gaps by providing the first\ncomprehensive theoretical analysis of AC algorithms that encompasses all five\ncrucial practical aspects (covers MMCLG criteria). We establish global\nconvergence sample complexity bounds of\n$\\tilde{\\mathcal{O}}\\left({\\epsilon^{-3}}\\right)$. We achieve this result\nthrough our novel use of the weak gradient domination property of MDP's and our\nunique analysis of the error in critic estimation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024. This is a revised version of arXiv:2306.10486,\n  where we have gone from finite action space to continuous action space, from\n  average iterate convergence to last iterate convergence and from\n  $\\epsilon^{-4}$ to $\\epsilon^{-3}$ sample complexity. This version fixes the\n  related work result of (Xu et al., 2020a), based on their result update on\n  arXiv",
    "pdf_url": "http://arxiv.org/pdf/2405.01843v5",
    "published_date": "2024-05-03 04:26:03 UTC",
    "updated_date": "2024-12-09 06:38:53 UTC"
  },
  {
    "arxiv_id": "2405.01840v1",
    "title": "An Essay concerning machine understanding",
    "authors": [
      "Herbert L. Roitblat"
    ],
    "abstract": "Artificial intelligence systems exhibit many useful capabilities, but they\nappear to lack understanding. This essay describes how we could go about\nconstructing a machine capable of understanding. As John Locke (1689) pointed\nout words are signs for ideas, which we can paraphrase as thoughts and\nconcepts. To understand a word is to know and be able to work with the\nunderlying concepts for which it is an indicator. Understanding between a\nspeaker and a listener occurs when the speaker casts his or her concepts into\nwords and the listener recovers approximately those same concepts. Current\nmodels rely on the listener to construct any potential meaning. The diminution\nof behaviorism as a psychological paradigm and the rise of cognitivism provide\nexamples of many experimental methods that can be used to determine whether and\nto what extent a machine might understand and to make suggestions about how\nthat understanding might be instantiated.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01840v1",
    "published_date": "2024-05-03 04:12:43 UTC",
    "updated_date": "2024-05-03 04:12:43 UTC"
  },
  {
    "arxiv_id": "2405.01839v1",
    "title": "SocialGFs: Learning Social Gradient Fields for Multi-Agent Reinforcement Learning",
    "authors": [
      "Qian Long",
      "Fangwei Zhong",
      "Mingdong Wu",
      "Yizhou Wang",
      "Song-Chun Zhu"
    ],
    "abstract": "Multi-agent systems (MAS) need to adaptively cope with dynamic environments,\nchanging agent populations, and diverse tasks. However, most of the multi-agent\nsystems cannot easily handle them, due to the complexity of the state and task\nspace. The social impact theory regards the complex influencing factors as\nforces acting on an agent, emanating from the environment, other agents, and\nthe agent's intrinsic motivation, referring to the social force. Inspired by\nthis concept, we propose a novel gradient-based state representation for\nmulti-agent reinforcement learning. To non-trivially model the social forces,\nwe further introduce a data-driven method, where we employ denoising score\nmatching to learn the social gradient fields (SocialGFs) from offline samples,\ne.g., the attractive or repulsive outcomes of each force. During interactions,\nthe agents take actions based on the multi-dimensional gradients to maximize\ntheir own rewards. In practice, we integrate SocialGFs into the widely used\nmulti-agent reinforcement learning algorithms, e.g., MAPPO. The empirical\nresults reveal that SocialGFs offer four advantages for multi-agent systems: 1)\nthey can be learned without requiring online interaction, 2) they demonstrate\ntransferability across diverse tasks, 3) they facilitate credit assignment in\nchallenging reward settings, and 4) they are scalable with the increasing\nnumber of agents.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2024 Cooperative Multi-Agent Systems Decision-Making and\n  Learning (CMASDL) Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.01839v1",
    "published_date": "2024-05-03 04:12:19 UTC",
    "updated_date": "2024-05-03 04:12:19 UTC"
  },
  {
    "arxiv_id": "2405.02359v1",
    "title": "CVTGAD: Simplified Transformer with Cross-View Attention for Unsupervised Graph-level Anomaly Detection",
    "authors": [
      "Jindong Li",
      "Qianli Xing",
      "Qi Wang",
      "Yi Chang"
    ],
    "abstract": "Unsupervised graph-level anomaly detection (UGAD) has received remarkable\nperformance in various critical disciplines, such as chemistry analysis and\nbioinformatics. Existing UGAD paradigms often adopt data augmentation\ntechniques to construct multiple views, and then employ different strategies to\nobtain representations from different views for jointly conducting UGAD.\nHowever, most previous works only considered the relationship between\nnodes/graphs from a limited receptive field, resulting in some key structure\npatterns and feature information being neglected. In addition, most existing\nmethods consider different views separately in a parallel manner, which is not\nable to explore the inter-relationship across different views directly. Thus, a\nmethod with a larger receptive field that can explore the inter-relationship\nacross different views directly is in need. In this paper, we propose a novel\nSimplified Transformer with Cross-View Attention for Unsupervised Graph-level\nAnomaly Detection, namely, CVTGAD. To increase the receptive field, we\nconstruct a simplified transformer-based module, exploiting the relationship\nbetween nodes/graphs from both intra-graph and inter-graph perspectives.\nFurthermore, we design a cross-view attention mechanism to directly exploit the\nview co-occurrence between different views, bridging the inter-view gap at node\nlevel and graph level. To the best of our knowledge, this is the first work to\napply transformer and cross attention to UGAD, which realizes graph neural\nnetwork and transformer working collaboratively. Extensive experiments on 15\nreal-world datasets of 3 fields demonstrate the superiority of CVTGAD on the\nUGAD task. The code is available at\n\\url{https://github.com/jindongli-Ai/CVTGAD}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02359v1",
    "published_date": "2024-05-03 03:31:00 UTC",
    "updated_date": "2024-05-03 03:31:00 UTC"
  },
  {
    "arxiv_id": "2405.02358v2",
    "title": "A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model",
    "authors": [
      "Jiexia Ye",
      "Weiqi Zhang",
      "Ke Yi",
      "Yongzi Yu",
      "Ziyue Li",
      "Jia Li",
      "Fugee Tsung"
    ],
    "abstract": "Time series data are ubiquitous across various domains, making time series\nanalysis critically important. Traditional time series models are\ntask-specific, featuring singular functionality and limited generalization\ncapacity. Recently, large language foundation models have unveiled their\nremarkable capabilities for cross-task transferability, zero-shot/few-shot\nlearning, and decision-making explainability. This success has sparked interest\nin the exploration of foundation models to solve multiple time series\nchallenges simultaneously. There are two main research lines, namely\npre-training foundation models from scratch for time series and adapting large\nlanguage foundation models for time series. They both contribute to the\ndevelopment of a unified model that is highly generalizable, versatile, and\ncomprehensible for time series analysis. This survey offers a 3E analytical\nframework for comprehensive examination of related research. Specifically, we\nexamine existing works from three dimensions, namely Effectiveness, Efficiency\nand Explainability. In each dimension, we focus on discussing how related works\ndevise tailored solution by considering unique challenges in the realm of time\nseries. Furthermore, we provide a domain taxonomy to help followers keep up\nwith the domain-specific advancements. In addition, we introduce extensive\nresources to facilitate the field's development, including datasets,\nopen-source, time series libraries. A GitHub repository is also maintained for\nresource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 figures, 6 tables, 41 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.02358v2",
    "published_date": "2024-05-03 03:12:55 UTC",
    "updated_date": "2024-05-07 01:59:37 UTC"
  },
  {
    "arxiv_id": "2405.05126v1",
    "title": "Exploring Speech Pattern Disorders in Autism using Machine Learning",
    "authors": [
      "Chuanbo Hu",
      "Jacob Thrasher",
      "Wenqi Li",
      "Mindi Ruan",
      "Xiangxu Yu",
      "Lynn K Paul",
      "Shuo Wang",
      "Xin Li"
    ],
    "abstract": "Diagnosing autism spectrum disorder (ASD) by identifying abnormal speech\npatterns from examiner-patient dialogues presents significant challenges due to\nthe subtle and diverse manifestations of speech-related symptoms in affected\nindividuals. This study presents a comprehensive approach to identify\ndistinctive speech patterns through the analysis of examiner-patient dialogues.\nUtilizing a dataset of recorded dialogues, we extracted 40 speech-related\nfeatures, categorized into frequency, zero-crossing rate, energy, spectral\ncharacteristics, Mel Frequency Cepstral Coefficients (MFCCs), and balance.\nThese features encompass various aspects of speech such as intonation, volume,\nrhythm, and speech rate, reflecting the complex nature of communicative\nbehaviors in ASD. We employed machine learning for both classification and\nregression tasks to analyze these speech features. The classification model\naimed to differentiate between ASD and non-ASD cases, achieving an accuracy of\n87.75%. Regression models were developed to predict speech pattern related\nvariables and a composite score from all variables, facilitating a deeper\nunderstanding of the speech dynamics associated with ASD. The effectiveness of\nmachine learning in interpreting intricate speech patterns and the high\nclassification accuracy underscore the potential of computational methods in\nsupporting the diagnostic processes for ASD. This approach not only aids in\nearly detection but also contributes to personalized treatment planning by\nproviding insights into the speech and communication profiles of individuals\nwith ASD.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05126v1",
    "published_date": "2024-05-03 02:59:15 UTC",
    "updated_date": "2024-05-03 02:59:15 UTC"
  },
  {
    "arxiv_id": "2405.01824v1",
    "title": "Creation of Novel Soft Robot Designs using Generative AI",
    "authors": [
      "Wee Kiat Chan",
      "PengWei Wang",
      "Raye Chen-Hua Yeow"
    ],
    "abstract": "Soft robotics has emerged as a promising field with the potential to\nrevolutionize industries such as healthcare and manufacturing. However,\ndesigning effective soft robots presents challenges, particularly in managing\nthe complex interplay of material properties, structural design, and control\nstrategies. Traditional design methods are often time-consuming and may not\nyield optimal designs. In this paper, we explore the use of generative AI to\ncreate 3D models of soft actuators. We create a dataset of over 70 text-shape\npairings of soft pneumatic robot actuator designs, and adapt a latent diffusion\nmodel (SDFusion) to learn the data distribution and generate novel designs from\nit. By employing transfer learning and data augmentation techniques, we\nsignificantly improve the performance of the diffusion model. These findings\nhighlight the potential of generative AI in designing complex soft robotic\nsystems, paving the way for future advancements in the field.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01824v1",
    "published_date": "2024-05-03 02:55:27 UTC",
    "updated_date": "2024-05-03 02:55:27 UTC"
  },
  {
    "arxiv_id": "2405.02356v1",
    "title": "Stochastic Multivariate Universal-Radix Finite-State Machine: a Theoretically and Practically Elegant Nonlinear Function Approximator",
    "authors": [
      "Xincheng Feng",
      "Guodong Shen",
      "Jianhao Hu",
      "Meng Li",
      "Ngai Wong"
    ],
    "abstract": "Nonlinearities are crucial for capturing complex input-output relationships\nespecially in deep neural networks. However, nonlinear functions often incur\nvarious hardware and compute overheads. Meanwhile, stochastic computing (SC)\nhas emerged as a promising approach to tackle this challenge by trading output\nprecision for hardware simplicity. To this end, this paper proposes a\nfirst-of-its-kind stochastic multivariate universal-radix finite-state machine\n(SMURF) that harnesses SC for hardware-simplistic multivariate nonlinear\nfunction generation at high accuracy. We present the finite-state machine (FSM)\narchitecture for SMURF, as well as analytical derivations of sampling gate\ncoefficients for accurately approximating generic nonlinear functions.\nExperiments demonstrate the superiority of SMURF, requiring only 16.07% area\nand 14.45% power consumption of Taylor-series approximation, and merely 2.22%\narea of look-up table (LUT) schemes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02356v1",
    "published_date": "2024-05-03 02:53:32 UTC",
    "updated_date": "2024-05-03 02:53:32 UTC"
  },
  {
    "arxiv_id": "2405.02355v3",
    "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation",
    "authors": [
      "Kounianhua Du",
      "Jizheng Chen",
      "Renting Rui",
      "Huacan Chai",
      "Lingyue Fu",
      "Wei Xia",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02355v3",
    "published_date": "2024-05-03 02:48:55 UTC",
    "updated_date": "2024-11-08 14:17:05 UTC"
  },
  {
    "arxiv_id": "2405.01820v1",
    "title": "Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention",
    "authors": [
      "Cedric Deslandes Whitney",
      "Justin Norman"
    ],
    "abstract": "Machine learning systems require representations of the real world for\ntraining and testing - they require data, and lots of it. Collecting data at\nscale has logistical and ethical challenges, and synthetic data promises a\nsolution to these challenges. Instead of needing to collect photos of real\npeople's faces to train a facial recognition system, a model creator could\ncreate and use photo-realistic, synthetic faces. The comparative ease of\ngenerating this synthetic data rather than relying on collecting data has made\nit a common practice. We present two key risks of using synthetic data in model\ndevelopment. First, we detail the high risk of false confidence when using\nsynthetic data to increase dataset diversity and representation. We base this\nin the examination of a real world use-case of synthetic data, where synthetic\ndatasets were generated for an evaluation of facial recognition technology.\nSecond, we examine how using synthetic data risks circumventing consent for\ndata usage. We illustrate this by considering the importance of consent to the\nU.S. Federal Trade Commission's regulation of data collection and affected\nmodels. Finally, we discuss how these two risks exemplify how synthetic data\ncomplicates existing governance and ethical practice; by decoupling data from\nthose it impacts, synthetic data is prone to consolidating power away those\nmost impacted by algorithmically-mediated harm.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01820v1",
    "published_date": "2024-05-03 02:47:44 UTC",
    "updated_date": "2024-05-03 02:47:44 UTC"
  },
  {
    "arxiv_id": "2405.01815v1",
    "title": "Toward end-to-end interpretable convolutional neural networks for waveform signals",
    "authors": [
      "Linh Vu",
      "Thu Tran",
      "Wern-Han Lim",
      "Raphael Phan"
    ],
    "abstract": "This paper introduces a novel convolutional neural networks (CNN) framework\ntailored for end-to-end audio deep learning models, presenting advancements in\nefficiency and explainability. By benchmarking experiments on three standard\nspeech emotion recognition datasets with five-fold cross-validation, our\nframework outperforms Mel spectrogram features by up to seven percent. It can\npotentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while\nremaining lightweight. Furthermore, we demonstrate the efficiency and\ninterpretability of the front-end layer using the PhysioNet Heart Sound\nDatabase, illustrating its ability to handle and capture intricate long\nwaveform patterns. Our contributions offer a portable solution for building\nefficient and interpretable models for raw waveform data.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01815v1",
    "published_date": "2024-05-03 02:24:27 UTC",
    "updated_date": "2024-05-03 02:24:27 UTC"
  },
  {
    "arxiv_id": "2405.02354v1",
    "title": "Heterogeneous network and graph attention auto-encoder for LncRNA-disease association prediction",
    "authors": [
      "Jin-Xing Liu",
      "Wen-Yu Xi",
      "Ling-Yun Dai",
      "Chun-Hou Zheng",
      "Ying-Lian Gao"
    ],
    "abstract": "The emerging research shows that lncRNAs are associated with a series of\ncomplex human diseases. However, most of the existing methods have limitations\nin identifying nonlinear lncRNA-disease associations (LDAs), and it remains a\nhuge challenge to predict new LDAs. Therefore, the accurate identification of\nLDAs is very important for the warning and treatment of diseases. In this work,\nmultiple sources of biomedical data are fully utilized to construct\ncharacteristics of lncRNAs and diseases, and linear and nonlinear\ncharacteristics are effectively integrated. Furthermore, a novel deep learning\nmodel based on graph attention automatic encoder is proposed, called HGATELDA.\nTo begin with, the linear characteristics of lncRNAs and diseases are created\nby the miRNA-lncRNA interaction matrix and miRNA-disease interaction matrix.\nFollowing this, the nonlinear features of diseases and lncRNAs are extracted\nusing a graph attention auto-encoder, which largely retains the critical\ninformation and effectively aggregates the neighborhood information of nodes.\nIn the end, LDAs can be predicted by fusing the linear and nonlinear\ncharacteristics of diseases and lncRNA. The HGATELDA model achieves an\nimpressive AUC value of 0.9692 when evaluated using a 5-fold cross-validation\nindicating its superior performance in comparison to several recent prediction\nmodels. Meanwhile, the effectiveness of HGATELDA in identifying novel LDAs is\nfurther demonstrated by case studies. the HGATELDA model appears to be a viable\ncomputational model for predicting LDAs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM",
      "I.2.4; I.2.6; I.2.m"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02354v1",
    "published_date": "2024-05-03 02:15:05 UTC",
    "updated_date": "2024-05-03 02:15:05 UTC"
  },
  {
    "arxiv_id": "2405.01810v2",
    "title": "Non-linear Welfare-Aware Strategic Learning",
    "authors": [
      "Tian Xie",
      "Xueru Zhang"
    ],
    "abstract": "This paper studies algorithmic decision-making in the presence of strategic\nindividual behaviors, where an ML model is used to make decisions about human\nagents and the latter can adapt their behavior strategically to improve their\nfuture data. Existing results on strategic learning have largely focused on the\nlinear setting where agents with linear labeling functions best respond to a\n(noisy) linear decision policy. Instead, this work focuses on general\nnon-linear settings where agents respond to the decision policy with only\n\"local information\" of the policy. Moreover, we simultaneously consider the\nobjectives of maximizing decision-maker welfare (model prediction accuracy),\nsocial welfare (agent improvement caused by strategic behaviors), and agent\nwelfare (the extent that ML underestimates the agents). We first generalize the\nagent best response model in previous works to the non-linear setting, then\nreveal the compatibility of welfare objectives. We show the three welfare can\nattain the optimum simultaneously only under restrictive conditions which are\nchallenging to achieve in non-linear settings. The theoretical results imply\nthat existing works solely maximizing the welfare of a subset of parties\ninevitably diminish the welfare of the others. We thus claim the necessity of\nbalancing the welfare of each party in non-linear settings and propose an\nirreducible optimization algorithm suitable for general strategic learning.\nExperiments on synthetic and real data validate the proposed algorithm.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01810v2",
    "published_date": "2024-05-03 01:50:03 UTC",
    "updated_date": "2024-08-13 19:19:07 UTC"
  },
  {
    "arxiv_id": "2405.01807v3",
    "title": "Algorithmic Decision-Making under Agents with Persistent Improvement",
    "authors": [
      "Tian Xie",
      "Xuwei Tan",
      "Xueru Zhang"
    ],
    "abstract": "This paper studies algorithmic decision-making under human's strategic\nbehavior, where a decision maker uses an algorithm to make decisions about\nhuman agents, and the latter with information about the algorithm may exert\neffort strategically and improve to receive favorable decisions. Unlike prior\nworks that assume agents benefit from their efforts immediately, we consider\nrealistic scenarios where the impacts of these efforts are persistent and\nagents benefit from efforts by making improvements gradually. We first develop\na dynamic model to characterize persistent improvements and based on this\nconstruct a Stackelberg game to model the interplay between agents and the\ndecision-maker. We analytically characterize the equilibrium strategies and\nidentify conditions under which agents have incentives to improve. With the\ndynamics, we then study how the decision-maker can design an optimal policy to\nincentivize the largest improvements inside the agent population. We also\nextend the model to settings where 1) agents may be dishonest and game the\nalgorithm into making favorable but erroneous decisions; 2) honest efforts are\nforgettable and not sufficient to guarantee persistent improvements. With the\nextended models, we further examine conditions under which agents prefer honest\nefforts over dishonest behavior and the impacts of forgettable efforts.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01807v3",
    "published_date": "2024-05-03 01:36:35 UTC",
    "updated_date": "2024-09-13 13:25:04 UTC"
  },
  {
    "arxiv_id": "2405.01799v2",
    "title": "Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders and Identifying Distinct Features",
    "authors": [
      "Chuanbo Hu",
      "Wenqi Li",
      "Mindi Ruan",
      "Xiangxu Yu",
      "Shalaka Deshpande",
      "Lynn K. Paul",
      "Shuo Wang",
      "Xin Li"
    ],
    "abstract": "Diagnosing language disorders associated with autism is a complex challenge,\noften hampered by the subjective nature and variability of traditional\nassessment methods. Traditional diagnostic methods not only require intensive\nhuman effort but also often result in delayed interventions due to their lack\nof speed and precision. In this study, we explored the application of ChatGPT,\na large language model, to overcome these obstacles by enhancing sensitivity\nand profiling linguistic features for autism diagnosis. This research utilizes\nChatGPT natural language processing capabilities to simplify and improve the\ndiagnostic process, focusing on identifying autism related language patterns.\nSpecifically, we compared ChatGPT performance with that of conventional\nsupervised learning models, including BERT, a model acclaimed for its\neffectiveness in various natural language processing tasks. We showed that\nChatGPT substantially outperformed these models, achieving over 10% improvement\nin both sensitivity and positive predictive value, in a zero shot learning\nconfiguration. The findings underscore the model potential as a diagnostic\ntool, combining accuracy and applicability. We identified ten key features of\nautism associated language disorders across scenarios. Features such as\necholalia, pronoun reversal, and atypical language usage play a critical role\nin diagnosing ASD and informing tailored treatment plans. Together, our\nfindings advocate for adopting sophisticated AI tools like ChatGPT in clinical\nsettings to assess and diagnose developmental disorders. Our approach promises\nenhanced diagnostic precision and supports personalized medicine, potentially\ntransforming the evaluation landscape for autism and similar neurological\nconditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01799v2",
    "published_date": "2024-05-03 01:04:28 UTC",
    "updated_date": "2024-11-29 06:15:20 UTC"
  },
  {
    "arxiv_id": "2405.01797v2",
    "title": "Learning under Imitative Strategic Behavior with Unforeseeable Outcomes",
    "authors": [
      "Tian Xie",
      "Zhiqun Zuo",
      "Mohammad Mahdi Khalili",
      "Xueru Zhang"
    ],
    "abstract": "Machine learning systems have been widely used to make decisions about\nindividuals who may behave strategically to receive favorable outcomes, e.g.,\nthey may genuinely improve the true labels or manipulate observable features\ndirectly to game the system without changing labels. Although both behaviors\nhave been studied (often as two separate problems) in the literature, most\nworks assume individuals can (i) perfectly foresee the outcomes of their\nbehaviors when they best respond; (ii) change their features arbitrarily as\nlong as it is affordable, and the costs they need to pay are deterministic\nfunctions of feature changes. In this paper, we consider a different setting\nand focus on imitative strategic behaviors with unforeseeable outcomes, i.e.,\nindividuals manipulate/improve by imitating the features of those with positive\nlabels, but the induced feature changes are unforeseeable. We first propose a\nStackelberg game to model the interplay between individuals and the\ndecision-maker, under which we examine how the decision-maker's ability to\nanticipate individual behavior affects its objective function and the\nindividual's best response. We show that the objective difference between the\ntwo can be decomposed into three interpretable terms, with each representing\nthe decision-maker's preference for a certain behavior. By exploring the roles\nof each term, we theoretically illustrate how a decision-maker with adjusted\npreferences may simultaneously disincentivize manipulation, incentivize\nimprovement, and promote fairness. Such theoretical results provide a guideline\nfor decision-makers to inform better and socially responsible decisions in\npractice.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01797v2",
    "published_date": "2024-05-03 00:53:58 UTC",
    "updated_date": "2024-10-29 10:28:05 UTC"
  },
  {
    "arxiv_id": "2405.01790v1",
    "title": "Understanding Position Bias Effects on Fairness in Social Multi-Document Summarization",
    "authors": [
      "Olubusayo Olabisi",
      "Ameeta Agrawal"
    ],
    "abstract": "Text summarization models have typically focused on optimizing aspects of\nquality such as fluency, relevance, and coherence, particularly in the context\nof news articles. However, summarization models are increasingly being used to\nsummarize diverse sources of text, such as social media data, that encompass a\nwide demographic user base. It is thus crucial to assess not only the quality\nof the generated summaries, but also the extent to which they can fairly\nrepresent the opinions of diverse social groups. Position bias, a long-known\nissue in news summarization, has received limited attention in the context of\nsocial multi-document summarization. We deeply investigate this phenomenon by\nanalyzing the effect of group ordering in input documents when summarizing\ntweets from three distinct linguistic communities: African-American English,\nHispanic-aligned Language, and White-aligned Language. Our empirical analysis\nshows that although the textual quality of the summaries remains consistent\nregardless of the input document order, in terms of fairness, the results vary\nsignificantly depending on how the dialect groups are presented in the input\ndata. Our results suggest that position bias manifests differently in social\nmulti-document summarization, severely impacting the fairness of summarization\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at VarDial 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01790v1",
    "published_date": "2024-05-03 00:19:31 UTC",
    "updated_date": "2024-05-03 00:19:31 UTC"
  },
  {
    "arxiv_id": "2405.01787v3",
    "title": "Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming",
    "authors": [
      "Saikat Chakraborty",
      "Gabriel Ebner",
      "Siddharth Bhat",
      "Sarah Fakhoury",
      "Sakina Fatima",
      "Shuvendu Lahiri",
      "Nikhil Swamy"
    ],
    "abstract": "Proof-oriented programs mix computational content with proofs of program\ncorrectness. However, the human effort involved in programming and proving is\nstill substantial, despite the use of Satisfiability Modulo Theories (SMT)\nsolvers to automate proofs in languages such as F*. Seeking to spur research on\nusing AI to automate the construction of proof-oriented programs, we curate a\ndataset of 600K lines of open-source F* programs and proofs, including software\nused in production systems ranging from Windows and Linux to Python and\nFirefox. Our dataset includes around 32K top-level F* definitions, each\nrepresenting a type-directed program and proof synthesis problem producing a\ndefinition given a formal specification expressed as an F* type. We provide a\nprogram fragment checker that queries F* to check the correctness of candidate\nsolutions. We also report on an extended version of our dataset containing a\ntotal of 940K lines of programs and proofs, with a total of 54k top-level F*\ndefinitions. We believe this is the largest corpus of SMT-assisted program\nproofs coupled with a reproducible program-fragment checker. Grounded in this\ndataset, we investigate the use of AI to synthesize programs and their proofs\nin F*, with promising results. Our main finding in that the performance of\nfine-tuned smaller language models (such as Phi-2 or StarCoder) compare\nfavorably with large language models (such as GPT-4), at a much lower\ncomputational cost. We also identify various type-based retrieval augmentation\ntechniques and find that they boost performance significantly. With detailed\nerror analysis and case studies, we identify potential strengths and weaknesses\nof models and techniques and suggest directions for future improvements.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "47th International Conference on Software Engineering",
    "pdf_url": "http://arxiv.org/pdf/2405.01787v3",
    "published_date": "2024-05-03 00:14:33 UTC",
    "updated_date": "2024-09-04 19:32:16 UTC"
  }
]
[
  {
    "arxiv_id": "2501.03413v1",
    "title": "SALT: Sales Autocompletion Linked Business Tables Dataset",
    "authors": [
      "Tassilo Klein",
      "Clemens Biehl",
      "Margarida Costa",
      "Andre Sres",
      "Jonas Kolk",
      "Johannes Hoffart"
    ],
    "abstract": "Foundation models, particularly those that incorporate Transformer\narchitectures, have demonstrated exceptional performance in domains such as\nnatural language processing and image processing. Adapting these models to\nstructured data, like tables, however, introduces significant challenges. These\ndifficulties are even more pronounced when addressing multi-table data linked\nvia foreign key, which is prevalent in the enterprise realm and crucial for\nempowering business use cases. Despite its substantial impact, research\nfocusing on such linked business tables within enterprise settings remains a\nsignificantly important yet underexplored domain. To address this, we introduce\na curated dataset sourced from an Enterprise Resource Planning (ERP) system,\nfeaturing extensive linked tables. This dataset is specifically designed to\nsupport research endeavors in table representation learning. By providing\naccess to authentic enterprise data, our goal is to potentially enhance the\neffectiveness and applicability of models for real-world business contexts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "Table Representation Learning Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.03413v1",
    "published_date": "2025-01-06 22:20:02 UTC",
    "updated_date": "2025-01-06 22:20:02 UTC"
  },
  {
    "arxiv_id": "2501.03403v1",
    "title": "BoundingDocs: a Unified Dataset for Document Question Answering with Spatial Annotations",
    "authors": [
      "Simone Giovannini",
      "Fabio Coppini",
      "Andrea Gemelli",
      "Simone Marinai"
    ],
    "abstract": "We present a unified dataset for document Question-Answering (QA), which is\nobtained combining several public datasets related to Document AI and visually\nrich document understanding (VRDU). Our main contribution is twofold: on the\none hand we reformulate existing Document AI tasks, such as Information\nExtraction (IE), into a Question-Answering task, making it a suitable resource\nfor training and evaluating Large Language Models; on the other hand, we\nrelease the OCR of all the documents and include the exact position of the\nanswer to be found in the document image as a bounding box. Using this dataset,\nwe explore the impact of different prompting techniques (that might include\nbounding box information) on the performance of open-weight models, identifying\nthe most effective approaches for document comprehension.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03403v1",
    "published_date": "2025-01-06 21:46:22 UTC",
    "updated_date": "2025-01-06 21:46:22 UTC"
  },
  {
    "arxiv_id": "2501.03394v2",
    "title": "Enhanced Importance Sampling through Latent Space Exploration in Normalizing Flows",
    "authors": [
      "Liam A. Kruse",
      "Alexandros E. Tzikas",
      "Harrison Delecki",
      "Mansur M. Arief",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Importance sampling is a rare event simulation technique used in Monte Carlo\nsimulations to bias the sampling distribution towards the rare event of\ninterest. By assigning appropriate weights to sampled points, importance\nsampling allows for more efficient estimation of rare events or tails of\ndistributions. However, importance sampling can fail when the proposal\ndistribution does not effectively cover the target distribution. In this work,\nwe propose a method for more efficient sampling by updating the proposal\ndistribution in the latent space of a normalizing flow. Normalizing flows learn\nan invertible mapping from a target distribution to a simpler latent\ndistribution. The latent space can be more easily explored during the search\nfor a proposal distribution, and samples from the proposal distribution are\nrecovered in the space of the target distribution via the invertible mapping.\nWe empirically validate our methodology on simulated robotics applications such\nas autonomous racing and aircraft ground collision avoidance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03394v2",
    "published_date": "2025-01-06 21:18:02 UTC",
    "updated_date": "2025-05-13 05:04:45 UTC"
  },
  {
    "arxiv_id": "2501.03392v1",
    "title": "Over-the-Air Fair Federated Learning via Multi-Objective Optimization",
    "authors": [
      "Shayan Mohajer Hamidi",
      "Ali Bereyhi",
      "Saba Asaad",
      "H. Vincent Poor"
    ],
    "abstract": "In federated learning (FL), heterogeneity among the local dataset\ndistributions of clients can result in unsatisfactory performance for some,\nleading to an unfair model. To address this challenge, we propose an\nover-the-air fair federated learning algorithm (OTA-FFL), which leverages\nover-the-air computation to train fair FL models. By formulating FL as a\nmulti-objective minimization problem, we introduce a modified Chebyshev\napproach to compute adaptive weighting coefficients for gradient aggregation in\neach communication round. To enable efficient aggregation over the multiple\naccess channel, we derive analytical solutions for the optimal transmit scalars\nat the clients and the de-noising scalar at the parameter server. Extensive\nexperiments demonstrate the superiority of OTA-FFL in achieving fairness and\nrobust performance compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03392v1",
    "published_date": "2025-01-06 21:16:51 UTC",
    "updated_date": "2025-01-06 21:16:51 UTC"
  },
  {
    "arxiv_id": "2501.03376v1",
    "title": "Existential Crisis: A Social Robot's Reason for Being",
    "authors": [
      "Dora Medgyesy",
      "Joella Galas",
      "Julian van Pol",
      "Rustam Eynaliyev",
      "Thijs Vollebregt"
    ],
    "abstract": "As Robots become ever more important in our daily lives there's growing need\nfor understanding how they're perceived by people. This study aims to\ninvestigate how the user perception of robots is influenced by displays of\npersonality. Using LLMs and speech to text technology, we designed a\nwithin-subject study to compare two conditions: a personality-driven robot and\na purely task-oriented, personality-neutral robot. Twelve participants,\nrecruited from Socially Intelligent Robotics course at Vrije Universiteit\nAmsterdam, interacted with a robot Nao tasked with asking them a set of medical\nquestions under both conditions. After completing both interactions, the\nparticipants completed a user experience questionnaire measuring their\nemotional states and robot perception using standardized questionnaires from\nthe SRI and Psychology literature.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03376v1",
    "published_date": "2025-01-06 20:30:15 UTC",
    "updated_date": "2025-01-06 20:30:15 UTC"
  },
  {
    "arxiv_id": "2501.03374v1",
    "title": "License Plate Images Generation with Diffusion Models",
    "authors": [
      "Mariia Shpir",
      "Nadiya Shvai",
      "Amir Nakib"
    ],
    "abstract": "Despite the evident practical importance of license plate recognition (LPR),\ncorresponding research is limited by the volume of publicly available datasets\ndue to privacy regulations such as the General Data Protection Regulation\n(GDPR). To address this challenge, synthetic data generation has emerged as a\npromising approach. In this paper, we propose to synthesize realistic license\nplates (LPs) using diffusion models, inspired by recent advances in image and\nvideo generation. In our experiments a diffusion model was successfully trained\non a Ukrainian LP dataset, and 1000 synthetic images were generated for\ndetailed analysis. Through manual classification and annotation of the\ngenerated images, we performed a thorough study of the model output, such as\nsuccess rate, character distributions, and type of failures. Our contributions\ninclude experimental validation of the efficacy of diffusion models for LP\nsynthesis, along with insights into the characteristics of the generated data.\nFurthermore, we have prepared a synthetic dataset consisting of 10,000 LP\nimages, publicly available at https://zenodo.org/doi/10.5281/zenodo.13342102.\nConducted experiments empirically confirm the usefulness of synthetic data for\nthe LPR task. Despite the initial performance gap between the model trained\nwith real and synthetic data, the expansion of the training data set with\npseudolabeled synthetic data leads to an improvement in LPR accuracy by 3%\ncompared to baseline.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03374v1",
    "published_date": "2025-01-06 20:22:18 UTC",
    "updated_date": "2025-01-06 20:22:18 UTC"
  },
  {
    "arxiv_id": "2501.03370v1",
    "title": "Advanced Machine Learning Techniques for Social Support Detection on Social Media",
    "authors": [
      "Olga Kolesnikova",
      "Moein Shahiki Tash",
      "Zahra Ahani",
      "Ameeta Agrawal",
      "Raul Monroy",
      "Grigori Sidorov"
    ],
    "abstract": "The widespread use of social media highlights the need to understand its\nimpact, particularly the role of online social support. This study uses a\ndataset focused on online social support, which includes binary and multiclass\nclassifications of social support content on social media. The classification\nof social support is divided into three tasks. The first task focuses on\ndistinguishing between supportive and non-supportive. The second task aims to\nidentify whether the support is directed toward an individual or a group. The\nthird task categorizes the specific type of social support, grouping it into\ncategories such as Nation, LGBTQ, Black people, Women, Religion, and Other (if\nit does not fit into the previously mentioned categories). To address data\nimbalances in these tasks, we employed K-means clustering for balancing the\ndataset and compared the results with the original unbalanced data. Using\nadvanced machine learning techniques, including transformers and zero-shot\nlearning approaches with GPT3, GPT4, and GPT4-o, we predict social support\nlevels in various contexts. The effectiveness of the dataset is evaluated using\nbaseline models across different learning approaches, with transformer-based\nmethods demonstrating superior performance. Additionally, we achieved a 0.4\\%\nincrease in the macro F1 score for the second task and a 0.7\\% increase for the\nthird task, compared to previous work utilizing traditional machine learning\nwith psycholinguistic and unigram-based TF-IDF values.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03370v1",
    "published_date": "2025-01-06 20:14:09 UTC",
    "updated_date": "2025-01-06 20:14:09 UTC"
  },
  {
    "arxiv_id": "2501.03349v1",
    "title": "FTA-FTL: A Fine-Tuned Aggregation Federated Transfer Learning Scheme for Lithology Microscopic Image Classification",
    "authors": [
      "Keyvan RahimiZadeh",
      "Ahmad Taheri",
      "Jan Baumbach",
      "Esmael Makarian",
      "Abbas Dehghani",
      "Bahman Ravaei",
      "Bahman Javadi",
      "Amin Beheshti"
    ],
    "abstract": "Lithology discrimination is a crucial activity in characterizing oil\nreservoirs, and processing lithology microscopic images is an essential\ntechnique for investigating fossils and minerals and geological assessment of\nshale oil exploration. In this way, Deep Learning (DL) technique is a powerful\napproach for building robust classifier models. However, there is still a\nconsiderable challenge to collect and produce a large dataset.\nTransfer-learning and data augmentation techniques have emerged as popular\napproaches to tackle this problem. Furthermore, due to different reasons,\nespecially data privacy, individuals, organizations, and industry companies\noften are not willing to share their sensitive data and information. Federated\nLearning (FL) has emerged to train a highly accurate central model across\nmultiple decentralized edge servers without transferring sensitive data,\npreserving sensitive data, and enhancing security. This study involves two\nphases; the first phase is to conduct Lithology microscopic image\nclassification on a small dataset using transfer learning. In doing so, various\npre-trained DL model architectures are comprehensively compared for the\nclassification task. In the second phase, we formulated the classification task\nto a Federated Transfer Learning (FTL) scheme and proposed a Fine-Tuned\nAggregation strategy for Federated Learning (FTA-FTL). In order to perform a\ncomprehensive experimental study, several metrics such as accuracy, f1 score,\nprecision, specificity, sensitivity (recall), and confusion matrix are taken\ninto account. The results are in excellent agreement and confirm the efficiency\nof the proposed scheme, and show that the proposed FTA-FTL algorithm is capable\nenough to achieve approximately the same results obtained by the centralized\nimplementation for Lithology microscopic images classification task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03349v1",
    "published_date": "2025-01-06 19:32:14 UTC",
    "updated_date": "2025-01-06 19:32:14 UTC"
  },
  {
    "arxiv_id": "2501.03324v1",
    "title": "Analyzing Bias in Swiss Federal Supreme Court Judgments Using Facebook's Holistic Bias Dataset: Implications for Language Model Training",
    "authors": [
      "Sabine Wehnert",
      "Muhammet Ertas",
      "Ernesto William De Luca"
    ],
    "abstract": "Natural Language Processing (NLP) is vital for computers to process and\nrespond accurately to human language. However, biases in training data can\nintroduce unfairness, especially in predicting legal judgment. This study\nfocuses on analyzing biases within the Swiss Judgment Prediction Dataset\n(SJP-Dataset). Our aim is to ensure unbiased factual descriptions essential for\nfair decision making by NLP models in legal contexts. We analyze the dataset\nusing social bias descriptors from the Holistic Bias dataset and employ\nadvanced NLP techniques, including attention visualization, to explore the\nimpact of dispreferred descriptors on model predictions. The study identifies\nbiases and examines their influence on model behavior. Challenges include\ndataset imbalance and token limits affecting model performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03324v1",
    "published_date": "2025-01-06 19:00:09 UTC",
    "updated_date": "2025-01-06 19:00:09 UTC"
  },
  {
    "arxiv_id": "2501.03229v1",
    "title": "Gaussian Masked Autoencoders",
    "authors": [
      "Jathushan Rajasegaran",
      "Xinlei Chen",
      "Rulilong Li",
      "Christoph Feichtenhofer",
      "Jitendra Malik",
      "Shiry Ginosar"
    ],
    "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While\nreconstructive self-supervised learning frameworks such as MAE learns good\nsemantic abstractions, it is not trained for explicit spatial awareness. Our\napproach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic\nabstractions and spatial understanding jointly. Like MAE, it reconstructs the\nimage end-to-end in the pixel space, but beyond MAE, it also introduces an\nintermediate, 3D Gaussian-based representation and renders images via\nsplatting. We show that GMAE can enable various zero-shot learning capabilities\nof spatial understanding (e.g., figure-ground segmentation, image layering,\nedge detection, etc.) while preserving the high-level semantics of\nself-supervised representation quality from MAE. To our knowledge, we are the\nfirst to employ Gaussian primitives in an image representation learning\nframework beyond optimization-based single-scene reconstructions. We believe\nGMAE will inspire further research in this direction and contribute to\ndeveloping next-generation techniques for modeling high-fidelity visual data.\nMore details at https://brjathu.github.io/gmae",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03229v1",
    "published_date": "2025-01-06 18:59:57 UTC",
    "updated_date": "2025-01-06 18:59:57 UTC"
  },
  {
    "arxiv_id": "2501.03228v3",
    "title": "LightGNN: Simple Graph Neural Network for Recommendation",
    "authors": [
      "Guoxuan Chen",
      "Lianghao Xia",
      "Chao Huang"
    ],
    "abstract": "Graph neural networks (GNNs) have demonstrated superior performance in\ncollaborative recommendation through their ability to conduct high-order\nrepresentation smoothing, effectively capturing structural information within\nusers' interaction patterns. However, existing GNN paradigms face significant\nchallenges in scalability and robustness when handling large-scale, noisy, and\nreal-world datasets. To address these challenges, we present LightGNN, a\nlightweight and distillation-based GNN pruning framework designed to\nsubstantially reduce model complexity while preserving essential collaboration\nmodeling capabilities. Our LightGNN framework introduces a computationally\nefficient pruning module that adaptively identifies and removes redundant edges\nand embedding entries for model compression. The framework is guided by a\nresource-friendly hierarchical knowledge distillation objective, whose\nintermediate layer augments the observed graph to maintain performance,\nparticularly in high-rate compression scenarios. Extensive experiments on\npublic datasets demonstrate LightGNN's effectiveness, significantly improving\nboth computational efficiency and recommendation accuracy. Notably, LightGNN\nachieves an 80% reduction in edge count and 90% reduction in embedding entries\nwhile maintaining performance comparable to more complex state-of-the-art\nbaselines. The implementation of our LightGNN framework is available at the\ngithub repository: https://github.com/HKUDS/LightGNN.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to WSDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03228v3",
    "published_date": "2025-01-06 18:59:55 UTC",
    "updated_date": "2025-02-04 08:34:08 UTC"
  },
  {
    "arxiv_id": "2501.03226v3",
    "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
    "authors": [
      "Beichen Zhang",
      "Yuhong Liu",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Pan Zhang",
      "Haodong Duan",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive ability in solving\ncomplex mathematical problems with multi-step reasoning and can be further\nenhanced with well-designed in-context learning (ICL) examples. However, this\npotential is often constrained by two major challenges in ICL: granularity\nmismatch and irrelevant information. We observe that while LLMs excel at\ndecomposing mathematical problems, they often struggle with reasoning errors in\nfine-grained steps. Moreover, ICL examples retrieved at the question level may\nomit critical steps or even mislead the model with irrelevant details. To\naddress this issue, we propose BoostStep, a method that enhances reasoning\naccuracy through step-aligned ICL, a novel mechanism that carefully aligns\nretrieved reference steps with the corresponding reasoning steps. Additionally,\nBoostStep incorporates an effective \"first-try\" strategy to deliver exemplars\nhighly relevant to the current state of reasoning. BoostStep is a flexible and\npowerful method that integrates seamlessly with chain-of-thought (CoT) and tree\nsearch algorithms, refining both candidate selection and decision-making.\nEmpirical results show that BoostStep improves GPT-4o's CoT performance by 4.6%\nacross mathematical benchmarks, significantly surpassing traditional few-shot\nlearning's 1.2%. Moreover, it can achieve an additional 7.5\\% gain combined\nwith tree search. Surprisingly, it enhances state-of-the-art LLMs to solve\nchallenging math problems using simpler examples. It improves\nDeepSeek-R1-671B's performance on AIME by 2.2%, leveraging simple examples only\nfrom the MATH dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Codes and Data are available at\n  https://github.com/beichenzbc/BoostStep",
    "pdf_url": "http://arxiv.org/pdf/2501.03226v3",
    "published_date": "2025-01-06 18:59:13 UTC",
    "updated_date": "2025-02-17 06:27:16 UTC"
  },
  {
    "arxiv_id": "2501.03225v2",
    "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
    "authors": [
      "Yuhui Zhang",
      "Yuchang Su",
      "Yiming Liu",
      "Xiaohan Wang",
      "James Burgess",
      "Elaine Sui",
      "Chenyu Wang",
      "Josiah Aklilu",
      "Alejandro Lozano",
      "Anjiang Wei",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ],
    "abstract": "The rapid development of vision language models (VLMs) demands rigorous and\nreliable evaluation. However, current visual question answering (VQA)\nbenchmarks often depend on open-ended questions, making accurate evaluation\ndifficult due to the variability in natural language responses. To address\nthis, we introduce AutoConverter, an agentic framework that automatically\nconverts these open-ended questions into multiple-choice format, enabling\nobjective evaluation while reducing the costly multiple-choice question\ncreation process. Our experiments demonstrate that AutoConverter can generate\ncorrect and challenging multiple-choice questions, with VLMs demonstrating\nconsistently similar or lower accuracy on these questions compared to\nhuman-created ones. Using AutoConverter, we construct VMCBench, a benchmark\ncreated by transforming 20 existing VQA datasets into a unified multiple-choice\nformat, totaling 9,018 questions. We comprehensively evaluate 33\nstate-of-the-art VLMs on VMCBench, setting a new standard for scalable,\nconsistent, and reproducible VLM evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03225v2",
    "published_date": "2025-01-06 18:57:31 UTC",
    "updated_date": "2025-04-09 17:25:07 UTC"
  },
  {
    "arxiv_id": "2501.03203v1",
    "title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity",
    "authors": [
      "Ayat A. Najjar",
      "Huthaifa I. Ashqar",
      "Omar A. Darwish",
      "Eman Hammad"
    ],
    "abstract": "This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03203v1",
    "published_date": "2025-01-06 18:34:20 UTC",
    "updated_date": "2025-01-06 18:34:20 UTC"
  },
  {
    "arxiv_id": "2501.03187v1",
    "title": "Turn-based Multi-Agent Reinforcement Learning Model Checking",
    "authors": [
      "Dennis Gross"
    ],
    "abstract": "In this paper, we propose a novel approach for verifying the compliance of\nturn-based multi-agent reinforcement learning (TMARL) agents with complex\nrequirements in stochastic multiplayer games. Our method overcomes the\nlimitations of existing verification approaches, which are inadequate for\ndealing with TMARL agents and not scalable to large games with multiple agents.\nOur approach relies on tight integration of TMARL and a verification technique\nreferred to as model checking. We demonstrate the effectiveness and scalability\nof our technique through experiments in different types of environments. Our\nexperiments show that our method is suited to verify TMARL agents and scales\nbetter than naive monolithic model checking.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03187v1",
    "published_date": "2025-01-06 18:04:20 UTC",
    "updated_date": "2025-01-06 18:04:20 UTC"
  },
  {
    "arxiv_id": "2501.03172v1",
    "title": "GLiREL -- Generalist Model for Zero-Shot Relation Extraction",
    "authors": [
      "Jack Boylan",
      "Chris Hokamp",
      "Demian Gholipour Ghalandari"
    ],
    "abstract": "We introduce GLiREL (Generalist Lightweight model for zero-shot Relation\nExtraction), an efficient architecture and training paradigm for zero-shot\nrelation classification. Inspired by recent advancements in zero-shot named\nentity recognition, this work presents an approach to efficiently and\naccurately predict zero-shot relationship labels between multiple entities in a\nsingle forward pass. Experiments using the FewRel and WikiZSL benchmarks\ndemonstrate that our approach achieves state-of-the-art results on the\nzero-shot relation classification task. In addition, we contribute a protocol\nfor synthetically-generating datasets with diverse relation labels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03172v1",
    "published_date": "2025-01-06 17:42:29 UTC",
    "updated_date": "2025-01-06 17:42:29 UTC"
  },
  {
    "arxiv_id": "2501.03152v1",
    "title": "The Scaling Law for LoRA Base on Mutual Information Upper Bound",
    "authors": [
      "Jing Zhang",
      "Hui Gao",
      "Peng Zhang",
      "Shuzhen Sun",
      "Chang Yang",
      "Yuexian Hou"
    ],
    "abstract": "LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method. In\nfine-tuning, the law among model performance, model parameters, and data\ncomplexity has been a focal issue in the field. Existing methods often leverage\nexternal metrics (such as cross-entropy or perplexity) to evaluate model\nperformance. In the fine-tuning process for large models, two types of\nknowledge are typically involved: the frozen, general knowledge acquired by the\nmodel during pre-training and the new knowledge learned through the LoRA module\nfrom the current data. Generally, the less LoRA's learned knowledge relies on\nthe large model, the more it captures the specific knowledge of new data,\nthereby enhancing its adaptability to new tasks. However, external metrics do\nnot readily capture the dependency relationship between these two types of\nknowledge. Therefore, we designed an internal metric based on the Mutual\nInformation Upper Bound (MIUB) theory to investigate the scaling law of\nlarge-model LoRA fine-tuning. In our experiments, we validated this approach on\nbenchmark datasets, using the Llama3-8B and Phi3-3B models. The results show\nthat the proposed MIUB metric aligns more accurately and stably with the\nscaling law of LoRA fine-tuning compared to cross-entropy and perplexity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03152v1",
    "published_date": "2025-01-06 17:19:19 UTC",
    "updated_date": "2025-01-06 17:19:19 UTC"
  },
  {
    "arxiv_id": "2501.03151v1",
    "title": "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches",
    "authors": [
      "Alhassan Mumuni",
      "Fuseini Mumuni"
    ],
    "abstract": "Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03151v1",
    "published_date": "2025-01-06 17:18:47 UTC",
    "updated_date": "2025-01-06 17:18:47 UTC"
  },
  {
    "arxiv_id": "2501.03145v2",
    "title": "Geometry Restoration and Dewarping of Camera-Captured Document Images",
    "authors": [
      "Valery Istomin",
      "Oleg Pereziabov",
      "Ilya Afanasyev"
    ],
    "abstract": "This research focuses on developing a method for restoring the topology of\ndigital images of paper documents captured by a camera, using algorithms for\ndetection, segmentation, geometry restoration, and dewarping. Our methodology\nemploys deep learning (DL) for document outline detection, followed by computer\nvision (CV) to create a topological 2D grid using cubic polynomial\ninterpolation and correct nonlinear distortions by remapping the image. Using\nclassical CV methods makes the document topology restoration process more\nefficient and faster, as it requires significantly fewer computational\nresources and memory. We developed a new pipeline for automatic document\ndewarping and reconstruction, along with a framework and annotated dataset to\ndemonstrate its efficiency. Our experiments confirm the promise of our\nmethodology and its superiority over existing benchmarks (including mobile apps\nand popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both\nvisually and in terms of document readability via Optical Character Recognition\n(OCR) and geometry restoration metrics. This paves the way for creating\nhigh-quality digital copies of paper documents and enhancing the efficiency of\nOCR systems. Project page: https://github.com/HorizonParadox/DRCCBI",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03145v2",
    "published_date": "2025-01-06 17:12:19 UTC",
    "updated_date": "2025-01-09 15:31:29 UTC"
  },
  {
    "arxiv_id": "2501.03142v1",
    "title": "Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies",
    "authors": [
      "Dennis Gross",
      "Helge Spieker"
    ],
    "abstract": "Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors\nand are challenging to interpret. To address these challenges, we combine RL\npolicy model checking--a technique for determining whether RL policies exhibit\nunsafe behaviors--with co-activation graph analysis--a method that maps neural\nnetwork inner workings by analyzing neuron activation patterns--to gain insight\ninto the safe RL policy's sequential decision-making. This combination lets us\ninterpret the RL policy's inner workings for safe decision-making. We\ndemonstrate its applicability in various experiments.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03142v1",
    "published_date": "2025-01-06 17:07:44 UTC",
    "updated_date": "2025-01-06 17:07:44 UTC"
  },
  {
    "arxiv_id": "2501.03124v3",
    "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
    "authors": [
      "Mingyang Song",
      "Zhaochen Su",
      "Xiaoye Qu",
      "Jiawei Zhou",
      "Yu Cheng"
    ],
    "abstract": "Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://prmbench.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2501.03124v3",
    "published_date": "2025-01-06 16:31:45 UTC",
    "updated_date": "2025-04-09 05:29:30 UTC"
  },
  {
    "arxiv_id": "2501.03119v2",
    "title": "From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning",
    "authors": [
      "Chao Feng",
      "Yuanzhe Gao",
      "Alberto Huertas Celdran",
      "Gerome Bovet",
      "Burkhard Stiller"
    ],
    "abstract": "Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. Nevertheless, model training leaves exploitable traces that can be\nused to infer sensitive information. In Decentralized FL (DFL), the topology,\ndefining how participants are connected, plays a crucial role in shaping the\nmodel's privacy, robustness, and convergence. However, the topology introduces\nan unexplored vulnerability: attackers can exploit it to infer participant\nrelationships and launch targeted attacks. This work uncovers the hidden risks\nof DFL topologies by proposing a novel Topology Inference Attack that infers\nthe topology solely from model behavior. A taxonomy of topology inference\nattacks is introduced, categorizing them by the attacker's capabilities and\nknowledge. Practical attack strategies are designed for various scenarios, and\nexperiments are conducted to identify key factors influencing attack success.\nThe results demonstrate that analyzing only the model of each node can\naccurately infer the DFL topology, highlighting a critical privacy risk in DFL\nsystems. These findings offer valuable insights for improving privacy\npreservation in DFL environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03119v2",
    "published_date": "2025-01-06 16:27:53 UTC",
    "updated_date": "2025-05-09 08:49:26 UTC"
  },
  {
    "arxiv_id": "2501.03112v1",
    "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases",
    "authors": [
      "Dylan Bouchard",
      "Mohit Singh Chauhan",
      "David Skarbrevik",
      "Viren Bajaj",
      "Zeya Ahmad"
    ],
    "abstract": "Large Language Models (LLMs) have been observed to exhibit bias in numerous\nways, potentially creating or worsening outcomes for specific groups identified\nby protected attributes such as sex, race, sexual orientation, or age. To help\naddress this gap, we introduce LangFair, an open-source Python package that\naims to equip LLM practitioners with the tools to evaluate bias and fairness\nrisks relevant to their specific use cases. The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to\nuse-case-specific prompts, and subsequently calculate applicable metrics for\nthe practitioner's use case. To guide in metric selection, LangFair offers an\nactionable decision framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Journal of Open Source Software; LangFair repository:\n  https://github.com/cvs-health/langfair",
    "pdf_url": "http://arxiv.org/pdf/2501.03112v1",
    "published_date": "2025-01-06 16:20:44 UTC",
    "updated_date": "2025-01-06 16:20:44 UTC"
  },
  {
    "arxiv_id": "2501.03085v1",
    "title": "Personalized Fashion Recommendation with Image Attributes and Aesthetics Assessment",
    "authors": [
      "Chongxian Chen",
      "Fan Mo",
      "Xin Fan",
      "Hayato Yamana"
    ],
    "abstract": "Personalized fashion recommendation is a difficult task because 1) the\ndecisions are highly correlated with users' aesthetic appetite, which previous\nwork frequently overlooks, and 2) many new items are constantly rolling out\nthat cause strict cold-start problems in the popular identity (ID)-based\nrecommendation methods. These new items are critical to recommend because of\ntrend-driven consumerism. In this work, we aim to provide more accurate\npersonalized fashion recommendations and solve the cold-start problem by\nconverting available information, especially images, into two attribute graphs\nfocusing on optimized image utilization and noise-reducing user modeling.\nCompared with previous methods that separate image and text as two components,\nthe proposed method combines image and text information to create a richer\nattributes graph. Capitalizing on the advancement of large language and vision\nmodels, we experiment with extracting fine-grained attributes efficiently and\nas desired using two different prompts. Preliminary experiments on the IQON3000\ndataset have shown that the proposed method achieves competitive accuracy\ncompared with baselines.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03085v1",
    "published_date": "2025-01-06 15:31:10 UTC",
    "updated_date": "2025-01-06 15:31:10 UTC"
  },
  {
    "arxiv_id": "2501.03301v2",
    "title": "Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective",
    "authors": [
      "Zhongjian Zhang",
      "Mengmei Zhang",
      "Xiao Wang",
      "Lingjuan Lyu",
      "Bo Yan",
      "Junping Du",
      "Chuan Shi"
    ],
    "abstract": "To preserve user privacy in recommender systems, federated recommendation\n(FR) based on federated learning (FL) emerges, keeping the personal data on the\nlocal client and updating a model collaboratively. Unlike FL, FR has a unique\nsparse aggregation mechanism, where the embedding of each item is updated by\nonly partial clients, instead of full clients in a dense aggregation of general\nFL. Recently, as an essential principle of FL, model security has received\nincreasing attention, especially for Byzantine attacks, where malicious clients\ncan send arbitrary updates. The problem of exploring the Byzantine robustness\nof FR is particularly critical since in the domains applying FR, e.g.,\ne-commerce, malicious clients can be injected easily by registering new\naccounts. However, existing Byzantine works neglect the unique sparse\naggregation of FR, making them unsuitable for our problem. Thus, we make the\nfirst effort to investigate Byzantine attacks on FR from the perspective of\nsparse aggregation, which is non-trivial: it is not clear how to define\nByzantine robustness under sparse aggregations and design Byzantine attacks\nunder limited knowledge/capability. In this paper, we reformulate the Byzantine\nrobustness under sparse aggregation by defining the aggregation for a single\nitem as the smallest execution unit. Then we propose a family of effective\nattack strategies, named Spattack, which exploit the vulnerability in sparse\naggregation and are categorized along the adversary's knowledge and capability.\nExtensive experimental results demonstrate that Spattack can effectively\nprevent convergence and even break down defenses under a few malicious clients,\nraising alarms for securing FR systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03301v2",
    "published_date": "2025-01-06 15:19:26 UTC",
    "updated_date": "2025-01-08 11:47:25 UTC"
  },
  {
    "arxiv_id": "2501.03059v1",
    "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
    "authors": [
      "Guy Yariv",
      "Yuval Kirstain",
      "Amit Zohar",
      "Shelly Sheynin",
      "Yaniv Taigman",
      "Yossi Adi",
      "Sagie Benaim",
      "Adam Polyak"
    ],
    "abstract": "We consider the task of Image-to-Video (I2V) generation, which involves\ntransforming static images into realistic video sequences based on a textual\ndescription. While recent advancements produce photorealistic outputs, they\nfrequently struggle to create videos with accurate and consistent object\nmotion, especially in multi-object scenarios. To address these limitations, we\npropose a two-stage compositional framework that decomposes I2V generation\ninto: (i) An explicit intermediate representation generation stage, followed by\n(ii) A video generation stage that is conditioned on this representation. Our\nkey innovation is the introduction of a mask-based motion trajectory as an\nintermediate representation, that captures both semantic object information and\nmotion, enabling an expressive but compact representation of motion and\nsemantics. To incorporate the learned representation in the second stage, we\nutilize object-level attention objectives. Specifically, we consider a spatial,\nper-object, masked-cross attention objective, integrating object-specific\nprompts into corresponding latent space regions and a masked spatio-temporal\nself-attention objective, ensuring frame-to-frame consistency for each object.\nWe evaluate our method on challenging benchmarks with multi-object and\nhigh-motion scenarios and empirically demonstrate that the proposed method\nachieves state-of-the-art results in temporal coherence, motion realism, and\ntext-prompt faithfulness. Additionally, we introduce \\benchmark, a new\nchallenging benchmark for single-object and multi-object I2V generation, and\ndemonstrate our method's superiority on this benchmark. Project page is\navailable at https://guyyariv.github.io/TTM/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03059v1",
    "published_date": "2025-01-06 14:49:26 UTC",
    "updated_date": "2025-01-06 14:49:26 UTC"
  },
  {
    "arxiv_id": "2501.03058v1",
    "title": "Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis",
    "authors": [
      "Tianhua Chen"
    ],
    "abstract": "This paper explores foundational and applied aspects of survival analysis,\nusing fall risk assessment as a case study. It revisits key time-related\nprobability distributions and statistical methods, including logistic\nregression, Poisson regression, Exponential regression, and the Cox\nProportional Hazards model, offering a unified perspective on their\nrelationships within the survival analysis framework. A contribution of this\nwork is the step-by-step derivation and clarification of the relationships\namong these models, particularly demonstrating that Poisson regression in the\nsurvival context is a specific case of the Cox model. These insights address\ngaps in understanding and reinforce the simplicity and interpretability of\nsurvival models. The paper also emphasizes the practical utility of survival\nanalysis by connecting theoretical insights with real-world applications. In\nthe context of fall detection, it demonstrates how these models can\nsimultaneously predict fall risk, analyze contributing factors, and estimate\ntime-to-event outcomes within a single streamlined framework. In contrast,\nadvanced deep learning methods often require complex post-hoc interpretation\nand separate training for different tasks particularly when working with\nstructured numerical data. This highlights the enduring relevance of classical\nstatistical frameworks and makes survival models especially valuable in\nhealthcare settings, where explainability and robustness are critical. By\nunifying foundational concepts and offering a cohesive perspective on\ntime-to-event analysis, this work serves as an accessible resource for\nunderstanding survival models and applying them effectively to diverse\nanalytical challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03058v1",
    "published_date": "2025-01-06 14:48:30 UTC",
    "updated_date": "2025-01-06 14:48:30 UTC"
  },
  {
    "arxiv_id": "2501.03055v2",
    "title": "To Analyze and Regulate Human-in-the-loop Learning for Congestion Games",
    "authors": [
      "Hongbo Li",
      "Lingjie Duan"
    ],
    "abstract": "In congestion games, selfish users behave myopically to crowd to the shortest\npaths, and the social planner designs mechanisms to regulate such selfish\nrouting through information or payment incentives. However, such mechanism\ndesign requires the knowledge of time-varying traffic conditions and it is the\nusers themselves to learn and report past road experiences to the social\nplanner (e.g., Waze or Google Maps). When congestion games meet mobile\ncrowdsourcing, it is critical to incentivize selfish users to explore\nnon-shortest paths in the best exploitation-exploration trade-off. First, we\nconsider a simple but fundamental parallel routing network with one\ndeterministic path and multiple stochastic paths for users with an average\narrival probability $\\lambda$. We prove that the current myopic routing policy\n(widely used in Waze and Google Maps) misses both exploration (when strong\nhazard belief) and exploitation (when weak hazard belief) as compared to the\nsocial optimum. Due to the myopic policy's under-exploration, we prove that the\ncaused price of anarchy (PoA) is larger than\n\\(\\frac{1}{1-\\rho^{\\frac{1}{\\lambda}}}\\), which can be arbitrarily large as\ndiscount factor \\(\\rho\\rightarrow1\\). To mitigate such huge efficiency loss, we\npropose a novel selective information disclosure (SID) mechanism: we only\nreveal the latest traffic information to users when they intend to over-explore\nstochastic paths upon arrival, while hiding such information when they want to\nunder-explore. We prove that our mechanism successfully reduces PoA to be less\nthan~\\(2\\). Besides the parallel routing network, we further extend our\nmechanism and PoA results to any linear path graphs with multiple intermediate\nnodes.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2211.14029",
    "pdf_url": "http://arxiv.org/pdf/2501.03055v2",
    "published_date": "2025-01-06 14:41:45 UTC",
    "updated_date": "2025-01-14 07:46:44 UTC"
  },
  {
    "arxiv_id": "2501.03045v1",
    "title": "Single-Channel Distance-Based Source Separation for Mobile GPU in Outdoor and Indoor Environments",
    "authors": [
      "Hanbin Bae",
      "Byungjun Kang",
      "Jiwon Kim",
      "Jaeyong Hwang",
      "Hosang Sung",
      "Hoon-Young Cho"
    ],
    "abstract": "This study emphasizes the significance of exploring distance-based source\nseparation (DSS) in outdoor environments. Unlike existing studies that\nprimarily focus on indoor settings, the proposed model is designed to capture\nthe unique characteristics of outdoor audio sources. It incorporates advanced\ntechniques, including a two-stage conformer block, a linear relation-aware\nself-attention (RSA), and a TensorFlow Lite GPU delegate. While the linear RSA\nmay not capture physical cues as explicitly as the quadratic RSA, the linear\nRSA enhances the model's context awareness, leading to improved performance on\nthe DSS that requires an understanding of physical cues in outdoor and indoor\nenvironments. The experimental results demonstrated that the proposed model\novercomes the limitations of existing approaches and considerably enhances\nenergy efficiency and real-time inference speed on mobile devices.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by ICASSP2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in any current or future media, including reprinting/republishing this\n  material for advertising or promotional purposes, creating new collective\n  works, for resale or redistribution to servers or lists, or reuse of any\n  copyrighted component",
    "pdf_url": "http://arxiv.org/pdf/2501.03045v1",
    "published_date": "2025-01-06 14:32:24 UTC",
    "updated_date": "2025-01-06 14:32:24 UTC"
  },
  {
    "arxiv_id": "2501.03038v2",
    "title": "Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders",
    "authors": [
      "Dichucheng Li",
      "Yongyi Zang",
      "Qiuqiang Kong"
    ],
    "abstract": "Automatic Music Transcription (AMT), aiming to get musical notes from raw\naudio, typically uses frame-level systems with piano-roll outputs or language\nmodel (LM)-based systems with note-level predictions. However, frame-level\nsystems require manual thresholding, while the LM-based systems struggle with\nlong sequences. In this paper, we propose a hybrid method combining pre-trained\nroll-based encoders with an LM decoder to leverage the strengths of both\nmethods. Besides, our approach employs a hierarchical prediction strategy,\nfirst predicting onset and pitch, then velocity, and finally offset. The\nhierarchical prediction strategy reduces computational costs by breaking down\nlong sequences into different hierarchies. Evaluated on two benchmark\nroll-based encoders, our method outperforms traditional piano-roll outputs 0.01\nand 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a\nperformance-enhancing plug-in for arbitrary roll-based music transcription\nencoder.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03038v2",
    "published_date": "2025-01-06 14:26:00 UTC",
    "updated_date": "2025-01-07 15:13:41 UTC"
  },
  {
    "arxiv_id": "2501.03035v4",
    "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning",
    "authors": [
      "Zhen Li",
      "Yupeng Su",
      "Runming Yang",
      "Congkai Xie",
      "Zheng Wang",
      "Zhongwei Xie",
      "Ngai Wong",
      "Hongxia Yang"
    ],
    "abstract": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03035v4",
    "published_date": "2025-01-06 14:23:02 UTC",
    "updated_date": "2025-02-24 14:34:37 UTC"
  },
  {
    "arxiv_id": "2501.03026v1",
    "title": "Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective",
    "authors": [
      "Sheldon Z. Soudin"
    ],
    "abstract": "Making sense of theory choice in normal and across extraordinary science is\ncentral to philosophy of science. The emergence of machine learning models has\nthe potential to act as a wrench in the gears of current debates. In this\npaper, I will attempt to reconstruct the main movements that lead to and came\nout of Putnam's critical and explanatory tendency distinction, argue for the\nbiconditional necessity of the tendencies, and conceptualize that wrench\nthrough a machine learning interpretation of my claim.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.03026v1",
    "published_date": "2025-01-06 14:09:35 UTC",
    "updated_date": "2025-01-06 14:09:35 UTC"
  },
  {
    "arxiv_id": "2501.03012v1",
    "title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment",
    "authors": [
      "Pegah Khayatan",
      "Mustafa Shukor",
      "Jayneel Parekh",
      "Matthieu Cord"
    ],
    "abstract": "Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "The first three authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2501.03012v1",
    "published_date": "2025-01-06 13:37:13 UTC",
    "updated_date": "2025-01-06 13:37:13 UTC"
  },
  {
    "arxiv_id": "2501.03008v1",
    "title": "Quality Estimation based Feedback Training for Improving Pronoun Translation",
    "authors": [
      "Harshit Dhankhar",
      "Baban Gain",
      "Asif Ekbal",
      "Yogesh Mani Tripathi"
    ],
    "abstract": "Pronoun translation is a longstanding challenge in neural machine translation\n(NMT), often requiring inter-sentential context to ensure linguistic accuracy.\nTo address this, we introduce ProNMT, a novel framework designed to enhance\npronoun and overall translation quality in context-aware machine translation\nsystems. ProNMT leverages Quality Estimation (QE) models and a unique Pronoun\nGeneration Likelihood-Based Feedback mechanism to iteratively fine-tune\npre-trained NMT models without relying on extensive human annotations. The\nframework combines QE scores with pronoun-specific rewards to guide training,\nensuring improved handling of linguistic nuances. Extensive experiments\ndemonstrate significant gains in pronoun translation accuracy and general\ntranslation quality across multiple metrics. ProNMT offers an efficient,\nscalable, and context-aware approach to improving NMT systems, particularly in\ntranslating context-dependent elements like pronouns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03008v1",
    "published_date": "2025-01-06 13:34:51 UTC",
    "updated_date": "2025-01-06 13:34:51 UTC"
  },
  {
    "arxiv_id": "2501.02997v1",
    "title": "CALM: Curiosity-Driven Auditing for Large Language Models",
    "authors": [
      "Xiang Zheng",
      "Longxiang Wang",
      "Yi Liu",
      "Xingjun Ma",
      "Chao Shen",
      "Cong Wang"
    ],
    "abstract": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI 2025 AI Alignment Track",
    "pdf_url": "http://arxiv.org/pdf/2501.02997v1",
    "published_date": "2025-01-06 13:14:34 UTC",
    "updated_date": "2025-01-06 13:14:34 UTC"
  },
  {
    "arxiv_id": "2501.02992v2",
    "title": "GLFC: Unified Global-Local Feature and Contrast Learning with Mamba-Enhanced UNet for Synthetic CT Generation from CBCT",
    "authors": [
      "Xianhao Zhou",
      "Jianghao Wu",
      "Huangxuan Zhao",
      "Lei Chen",
      "Shaoting Zhang",
      "Guotai Wang"
    ],
    "abstract": "Generating synthetic Computed Tomography (CT) images from Cone Beam Computed\nTomography (CBCT) is desirable for improving the image quality of CBCT.\nExisting synthetic CT (sCT) generation methods using Convolutional Neural\nNetworks (CNN) and Transformers often face difficulties in effectively\ncapturing both global and local features and contrasts for high-quality sCT\ngeneration. In this work, we propose a Global-Local Feature and Contrast\nlearning (GLFC) framework for sCT generation. First, a Mamba-Enhanced UNet\n(MEUNet) is introduced by integrating Mamba blocks into the skip connections of\na high-resolution UNet for effective global and local feature learning. Second,\nwe propose a Multiple Contrast Loss (MCL) that calculates synthetic loss at\ndifferent intensity windows to improve quality for both soft tissues and bone\nregions. Experiments on the SynthRAD2023 dataset demonstrate that GLFC improved\nthe SSIM of sCT from 77.91% to 91.50% compared with the original CBCT, and\nsignificantly outperformed several existing methods for sCT generation. The\ncode is available at https://github.com/HiLab-git/GLFC",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by ISBI2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02992v2",
    "published_date": "2025-01-06 13:11:47 UTC",
    "updated_date": "2025-01-11 14:46:47 UTC"
  },
  {
    "arxiv_id": "2501.02982v2",
    "title": "A Bio-Inspired Research Paradigm of Collision Perception Neurons Enabling Neuro-Robotic Integration: The LGMD Case",
    "authors": [
      "Ziyan Qin",
      "Jigen Peng",
      "Shigang Yue",
      "Qinbing Fu"
    ],
    "abstract": "Compared to human vision, locust visual systems excel at rapid and precise\ncollision detection, despite relying on only hundreds of thousands of neurons\norganized through a few neuropils. This efficiency makes them an attractive\nmodel system for developing artificial collision-detecting systems.\nSpecifically, researchers have identified collision-selective neurons in the\nlocust's optic lobe, called lobula giant movement detectors (LGMDs), which\nrespond specifically to approaching objects. Research upon LGMD neurons began\nin the early 1970s. Initially, due to their large size, these neurons were\nidentified as motion detectors, but their role as looming detectors was\nrecognized over time. Since then, progress in neuroscience, computational\nmodeling of LGMD's visual neural circuits, and LGMD-based robotics have\nadvanced in tandem, each field supporting and driving the others. Today, with a\ndeeper understanding of LGMD neurons, LGMD-based models have significantly\nimproved collision-free navigation in mobile robots including ground and aerial\nrobots. This review highlights recent developments in LGMD research from the\nperspectives of neuroscience, computational modeling, and robotics. It\nemphasizes a biologically plausible research paradigm, where insights from\nneuroscience inform real-world applications, which would in turn validate and\nadvance neuroscience. With strong support from extensive research and growing\napplication demand, this paradigm has reached a mature stage and demonstrates\nversatility across different areas of neuroscience research, thereby enhancing\nour understanding of the interconnections between neuroscience, computational\nmodeling, and robotics. Furthermore, this paradigm would shed light upon the\nmodeling and robotic research into other motion-sensitive neurons or neural\ncircuits.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "35 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02982v2",
    "published_date": "2025-01-06 12:44:48 UTC",
    "updated_date": "2025-05-14 06:57:48 UTC"
  },
  {
    "arxiv_id": "2501.02981v2",
    "title": "CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks",
    "authors": [
      "Atmane Ayoub Mansour Bahar",
      "Kamel Soaid Ferrahi",
      "Mohamed-Lamine Messai",
      "Hamida Seba",
      "Karima Amrouche"
    ],
    "abstract": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their sophisticated and stealthy nature. Traditional\nIntrusion Detection Systems (IDS) often fall short in detecting these\nmulti-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed\nto enhance IDS capabilities by analyzing the complex relationships within\nnetworked data. However, existing GNN-based solutions are hampered by high\nfalse positive rates and substantial resource consumption. In this paper, we\npresent a novel IDS designed to detect APTs using a Spatio-Temporal Graph\nNeural Network Autoencoder. Our approach leverages spatial information to\nunderstand the interactions between entities within a graph and temporal\ninformation to capture the evolution of the graph over time. This dual\nperspective is crucial for identifying the sequential stages of APTs.\nFurthermore, to address privacy and scalability concerns, we deploy our\narchitecture in a federated learning environment. This setup ensures that local\ndata remains on-premise while encrypted model-weights are shared and aggregated\nusing homomorphic encryption, maintaining data privacy and security. Our\nevaluation shows that this system effectively detects APTs with lower false\npositive rates and optimized resource usage compared to existing methods,\nhighlighting the potential of spatio-temporal analysis and federated learning\nin enhancing cybersecurity defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.02981v2",
    "published_date": "2025-01-06 12:43:59 UTC",
    "updated_date": "2025-01-07 08:39:10 UTC"
  },
  {
    "arxiv_id": "2501.02977v2",
    "title": "CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems",
    "authors": [
      "Chuanbo Hua",
      "Federico Berto",
      "Jiwoo Son",
      "Seunghyun Kang",
      "Changhyun Kwon",
      "Jinkyoo Park"
    ],
    "abstract": "The profiled vehicle routing problem (PVRP) is a generalization of the\nheterogeneous capacitated vehicle routing problem (HCVRP) in which the\nobjective is to optimize the routes of vehicles to serve client demands subject\nto different vehicle profiles, with each having a preference or constraint on a\nper-client basis. While existing learning methods have shown promise for\nsolving the HCVRP in real-time, no learning method exists to solve the more\npractical and challenging PVRP. In this paper, we propose a Collaborative\nAttention Model with Profiles (CAMP), a novel approach that learns efficient\nsolvers for PVRP using multi-agent reinforcement learning. CAMP employs a\nspecialized attention-based encoder architecture to embed profiled client\nembeddings in parallel for each vehicle profile. We design a communication\nlayer between agents for collaborative decision-making across profiled\nembeddings at each decoding step and a batched pointer mechanism to attend to\nthe profiled embeddings to evaluate the likelihood of the next actions. We\nevaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly\ninfluence the reward function, and PVRP with zone constraints with different\nnumbers of agents and clients, demonstrating that our learned solvers achieve\ncompetitive results compared to both classical state-of-the-art neural\nmulti-agent models in terms of solution quality and computational efficiency.\nWe make our code openly available at https://github.com/ai4co/camp.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at AAMAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02977v2",
    "published_date": "2025-01-06 12:37:56 UTC",
    "updated_date": "2025-02-04 09:21:37 UTC"
  },
  {
    "arxiv_id": "2501.02975v1",
    "title": "Fuzzy Granule Density-Based Outlier Detection with Multi-Scale Granular Balls",
    "authors": [
      "Can Gao",
      "Xiaofeng Tan",
      "Jie Zhou",
      "Weiping Ding",
      "Witold Pedrycz"
    ],
    "abstract": "Outlier detection refers to the identification of anomalous samples that\ndeviate significantly from the distribution of normal data and has been\nextensively studied and used in a variety of practical tasks. However, most\nunsupervised outlier detection methods are carefully designed to detect\nspecified outliers, while real-world data may be entangled with different types\nof outliers. In this study, we propose a fuzzy rough sets-based multi-scale\noutlier detection method to identify various types of outliers. Specifically, a\nnovel fuzzy rough sets-based method that integrates relative fuzzy granule\ndensity is first introduced to improve the capability of detecting local\noutliers. Then, a multi-scale view generation method based on granular-ball\ncomputing is proposed to collaboratively identify group outliers at different\nlevels of granularity. Moreover, reliable outliers and inliers determined by\nthe three-way decision are used to train a weighted support vector machine to\nfurther improve the performance of outlier detection. The proposed method\ninnovatively transforms unsupervised outlier detection into a semi-supervised\nclassification problem and for the first time explores the fuzzy rough\nsets-based outlier detection from the perspective of multi-scale granular\nballs, allowing for high adaptability to different types of outliers. Extensive\nexperiments carried out on both artificial and UCI datasets demonstrate that\nthe proposed outlier detection method significantly outperforms the\nstate-of-the-art methods, improving the results by at least 8.48% in terms of\nthe Area Under the ROC Curve (AUROC) index. { The source codes are released at\n\\url{https://github.com/Xiaofeng-Tan/MGBOD}. }",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02975v1",
    "published_date": "2025-01-06 12:35:51 UTC",
    "updated_date": "2025-01-06 12:35:51 UTC"
  },
  {
    "arxiv_id": "2501.02971v1",
    "title": "Proof-of-Data: A Consensus Protocol for Collaborative Intelligence",
    "authors": [
      "Huiwen Liu",
      "Feida Zhu",
      "Ling Cheng"
    ],
    "abstract": "Existing research on federated learning has been focused on the setting where\nlearning is coordinated by a centralized entity. Yet the greatest potential of\nfuture collaborative intelligence would be unleashed in a more open and\ndemocratized setting with no central entity in a dominant role, referred to as\n\"decentralized federated learning\". New challenges arise accordingly in\nachieving both correct model training and fair reward allocation with\ncollective effort among all participating nodes, especially with the threat of\nthe Byzantine node jeopardising both tasks.\n  In this paper, we propose a blockchain-based decentralized Byzantine\nfault-tolerant federated learning framework based on a novel Proof-of-Data\n(PoD) consensus protocol to resolve both the \"trust\" and \"incentive\"\ncomponents. By decoupling model training and contribution accounting, PoD is\nable to enjoy not only the benefit of learning efficiency and system liveliness\nfrom asynchronous societal-scale PoW-style learning but also the finality of\nconsensus and reward allocation from epoch-based BFT-style voting. To mitigate\nfalse reward claims by data forgery from Byzantine attacks, a privacy-aware\ndata verification and contribution-based reward allocation mechanism is\ndesigned to complete the framework. Our evaluation results show that PoD\ndemonstrates performance in model training close to that of the centralized\ncounterpart while achieving trust in consensus and fairness for reward\nallocation with a fault tolerance ratio of 1/3.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02971v1",
    "published_date": "2025-01-06 12:27:59 UTC",
    "updated_date": "2025-01-06 12:27:59 UTC"
  },
  {
    "arxiv_id": "2501.02964v2",
    "title": "Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild",
    "authors": [
      "Wanpeng Hu",
      "Haodi Liu",
      "Lin Chen",
      "Feng Zhou",
      "Changming Xiao",
      "Qi Yang",
      "Changshui Zhang"
    ],
    "abstract": "Complex visual reasoning remains a key challenge today. Typically, the\nchallenge is tackled using methodologies such as Chain of Thought (COT) and\nvisual instruction tuning. However, how to organically combine these two\nmethodologies for greater success remains unexplored. Also, issues like\nhallucinations and high training cost still need to be addressed. In this work,\nwe devise an innovative multi-round training and reasoning framework suitable\nfor lightweight Multimodal Large Language Models (MLLMs). Our self-questioning\napproach heuristically guides MLLMs to focus on visual clues relevant to the\ntarget problem, reducing hallucinations and enhancing the model's ability to\ndescribe fine-grained image details. This ultimately enables the model to\nperform well in complex visual reasoning and question-answering tasks. We have\nnamed this framework Socratic Questioning(SQ). To facilitate future research,\nwe create a multimodal mini-dataset named CapQA, which includes 1k images of\nfine-grained activities, for visual instruction tuning and evaluation, our\nproposed SQ method leads to a 31.2% improvement in the hallucination score. Our\nextensive experiments on various benchmarks demonstrate SQ's remarkable\ncapabilities in heuristic self-questioning, zero-shot visual reasoning and\nhallucination mitigation. Our model and code will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02964v2",
    "published_date": "2025-01-06 12:16:56 UTC",
    "updated_date": "2025-01-07 02:55:15 UTC"
  },
  {
    "arxiv_id": "2501.02950v2",
    "title": "Key-value memory in the brain",
    "authors": [
      "Samuel J. Gershman",
      "Ila Fiete",
      "Kazuki Irie"
    ],
    "abstract": "Classical models of memory in psychology and neuroscience rely on\nsimilarity-based retrieval of stored patterns, where similarity is a function\nof retrieval cues and the stored patterns. While parsimonious, these models do\nnot allow distinct representations for storage and retrieval, despite their\ndistinct computational demands. Key-value memory systems, in contrast,\ndistinguish representations used for storage (values) and those used for\nretrieval (keys). This allows key-value memory systems to optimize\nsimultaneously for fidelity in storage and discriminability in retrieval. We\nreview the computational foundations of key-value memory, its role in modern\nmachine learning systems, related ideas from psychology and neuroscience,\napplications to a number of empirical puzzles, and possible biological\nimplementations.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Accepted to Neuron",
    "pdf_url": "http://arxiv.org/pdf/2501.02950v2",
    "published_date": "2025-01-06 11:46:40 UTC",
    "updated_date": "2025-03-04 03:10:09 UTC"
  },
  {
    "arxiv_id": "2501.03295v2",
    "title": "A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation Based on Large Language Models Enhanced by Domain Knowledge Retrieval",
    "authors": [
      "Shuo Tong",
      "Han Liu",
      "Runyuan Guo",
      "Wenqing Wang",
      "Xueqiong Tian",
      "Lingyun Wei",
      "Lin Zhang",
      "Huayong Wu",
      "Ding Liu",
      "Youmin Zhang"
    ],
    "abstract": "Data-driven soft sensors are crucial in predicting key performance indicators\nin industrial systems. However, current methods predominantly rely on the\nsupervised learning paradigms of parameter updating, which inherently faces\nchallenges such as high development costs, poor robustness, training\ninstability, and lack of interpretability. Recently, large language models\n(LLMs) have demonstrated significant potential across various domains, notably\nthrough In-Context Learning (ICL), which enables high-performance task\nexecution with minimal input-label demonstrations and no prior training. This\npaper aims to replace supervised learning with the emerging ICL paradigm for\nsoft sensor modeling to address existing challenges and explore new avenues for\nadvancement. To achieve this, we propose a novel framework called the Few-shot\nUncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes\nthe Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware\nFew-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial\nKnowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling\nzero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based\ncontext demonstrations of structured data to prompt LLMs to execute ICL for\npredicting and propose a context sample retrieval augmentation strategy to\nimprove performance. Additionally, we explored LLMs' AIGC and probabilistic\ncharacteristics to propose self-explanation and uncertainty quantification\nmethods for constructing a trustworthy soft sensor. Extensive experiments\ndemonstrate that our method achieved state-of-the-art predictive performance,\nstrong robustness, and flexibility, effectively mitigates training instability\nfound in traditional methods. To the best of our knowledge, this is the first\nwork to establish soft sensor utilizing LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03295v2",
    "published_date": "2025-01-06 11:43:29 UTC",
    "updated_date": "2025-01-08 04:50:01 UTC"
  },
  {
    "arxiv_id": "2501.02922v1",
    "title": "Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology",
    "authors": [
      "Susu Sun",
      "Leslie Tessier",
      "Frédérique Meeuwsen",
      "Clément Grisi",
      "Dominique van Midden",
      "Geert Litjens",
      "Christian F. Baumgartner"
    ],
    "abstract": "Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide\nImage (WSI) analysis with only slide-level annotations. Interpretability is\ncrucial for safely deploying such algorithms in high-stakes medical domains.\nTraditional MIL methods offer explanations by highlighting salient regions.\nHowever, such spatial heatmaps provide limited insights for end users. To\naddress this, we propose a novel inherently interpretable WSI-classification\napproach that uses human-understandable pathology concepts to generate\nexplanations. Our proposed Concept MIL model leverages recent advances in\nvision-language models to directly predict pathology concepts based on image\nfeatures. The model's predictions are obtained through a linear combination of\nthe concepts identified on the top-K patches of a WSI, enabling inherent\nexplanations by tracing each concept's influence on the prediction. In contrast\nto traditional concept-based interpretable models, our approach eliminates the\nneed for costly human annotations by leveraging the vision-language model. We\nvalidate our method on two widely used pathology datasets: Camelyon16 and\nPANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9,\nputting it on par with state-of-the-art models. We further find that 87.1\\%\n(Camelyon16) and 85.3\\% (PANDA) of the top 20 patches fall within the tumor\nregion. A user study shows that the concepts identified by our model align with\nthe concepts used by pathologists, making it a promising strategy for\nhuman-interpretable WSI classification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02922v1",
    "published_date": "2025-01-06 11:03:04 UTC",
    "updated_date": "2025-01-06 11:03:04 UTC"
  },
  {
    "arxiv_id": "2501.02921v2",
    "title": "Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging and Variational Autoencoders",
    "authors": [
      "Mahmoud Abdulsalam",
      "Usman Zahidi",
      "Bradley Hurst",
      "Simon Pearson",
      "Grzegorz Cielniak",
      "James Brown"
    ],
    "abstract": "Tomato anomalies/damages pose a significant challenge in greenhouse farming.\nWhile this method of cultivation benefits from efficient resource utilization,\nanomalies can significantly degrade the quality of farm produce. A common\nanomaly associated with tomatoes is splitting, characterized by the development\nof cracks on the tomato skin, which degrades its quality. Detecting this type\nof anomaly is challenging due to dynamic variations in appearance and sizes,\ncompounded by dataset scarcity. We address this problem in an unsupervised\nmanner by utilizing a tailored variational autoencoder (VAE) with hyperspectral\ninput. Preliminary analysis of the dataset enabled us to select the optimal\nrange of wavelengths for detecting this anomaly. Our findings indicate that the\n530nm - 550nm range is suitable for identifying tomato dry splits. The proposed\nVAE model achieved a 97% detection accuracy for tomato split anomalies in the\ntest data. The analysis on reconstruction loss allow us to not only detect the\nanomalies but also to some degree estimate the anomalous regions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPPA Workshop",
    "pdf_url": "http://arxiv.org/pdf/2501.02921v2",
    "published_date": "2025-01-06 11:02:52 UTC",
    "updated_date": "2025-04-28 11:24:12 UTC"
  },
  {
    "arxiv_id": "2501.02905v1",
    "title": "Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework",
    "authors": [
      "Shuangshuang He",
      "Hongli Liang",
      "Yuanting Zhang",
      "Xingyuan Yuan"
    ],
    "abstract": "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02905v1",
    "published_date": "2025-01-06 10:29:38 UTC",
    "updated_date": "2025-01-06 10:29:38 UTC"
  },
  {
    "arxiv_id": "2501.02891v2",
    "title": "Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis",
    "authors": [
      "Mary Ogbuka Kenneth",
      "Foaad Khosmood",
      "Abbas Edalat"
    ],
    "abstract": "Humour styles can have either a negative or a positive impact on well-being.\nGiven the importance of these styles to mental health, significant research has\nbeen conducted on their automatic identification. However, the automated\nmachine learning models used for this purpose are black boxes, making their\nprediction decisions opaque. Clarity and transparency are vital in the field of\nmental health. This paper presents an explainable AI (XAI) framework for\nunderstanding humour style classification, building upon previous work in\ncomputational humour analysis. Using the best-performing single model\n(ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to\nanalyse how linguistic, emotional, and semantic features contribute to humour\nstyle classification decisions. Our analysis reveals distinct patterns in how\ndifferent humour styles are characterised and misclassified, with particular\nemphasis on the challenges in distinguishing affiliative humour from other\nstyles. Through detailed examination of feature importance, error patterns, and\nmisclassification cases, we identify key factors influencing model decisions,\nincluding emotional ambiguity, context misinterpretation, and target\nidentification. The framework demonstrates significant utility in understanding\nmodel behaviour, achieving interpretable insights into the complex interplay of\nfeatures that define different humour styles. Our findings contribute to both\nthe theoretical understanding of computational humour analysis and practical\napplications in mental health, content moderation, and digital humanities\nresearch.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02891v2",
    "published_date": "2025-01-06 10:08:56 UTC",
    "updated_date": "2025-02-28 17:57:47 UTC"
  },
  {
    "arxiv_id": "2501.02869v1",
    "title": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment",
    "authors": [
      "Yiming Zhang",
      "Zheng Chang",
      "Wentao Cai",
      "MengXing Ren",
      "Kang Yuan",
      "Yining Sun",
      "Zenghui Ding"
    ],
    "abstract": "Recent researches of large language models(LLM), which is pre-trained on\nmassive general-purpose corpora, have achieved breakthroughs in responding\nhuman queries. However, these methods face challenges including limited data\ninsufficiency to support extensive pre-training and can not align responses\nwith users' instructions. To address these issues, we introduce a medical\ninstruction dataset, CMedINS, containing six medical instructions derived from\nactual medical tasks, which effectively fine-tunes LLM in conjunction with\nother data. Subsequently, We launch our medical model, IIMedGPT, employing an\nefficient preference alignment method, Direct preference Optimization(DPO). The\nresults show that our final model outperforms existing medical models in\nmedical dialogue.Datsets, Code and model checkpoints will be released upon\nacceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02869v1",
    "published_date": "2025-01-06 09:22:36 UTC",
    "updated_date": "2025-01-06 09:22:36 UTC"
  },
  {
    "arxiv_id": "2501.02840v1",
    "title": "Enhanced Rooftop Solar Panel Detection by Efficiently Aggregating Local Features",
    "authors": [
      "Kuldeep Kurte",
      "Kedar Kulkarni"
    ],
    "abstract": "In this paper, we present an enhanced Convolutional Neural Network\n(CNN)-based rooftop solar photovoltaic (PV) panel detection approach using\nsatellite images. We propose to use pre-trained CNN-based model to extract the\nlocal convolutional features of rooftops. These local features are then\ncombined using the Vectors of Locally Aggregated Descriptors (VLAD) technique\nto obtain rooftop-level global features, which are then used to train\ntraditional Machine Learning (ML) models to identify rooftop images that do and\ndo not contain PV panels. On the dataset used in this study, the proposed\napproach achieved rooftop-PV classification scores exceeding the predefined\nthreshold of 0.9 across all three cities for each of the feature extractor\nnetworks evaluated. Moreover, we propose a 3-phase approach to enable efficient\nutilization of the previously trained models on a new city or region with\nlimited labelled data. We illustrate the effectiveness of this 3-phase approach\nfor multi-city rooftop-PV detection task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CODS-COMAD 2024, December, 2024, Jodhpur, India\n  (https://cods-comad.in/accepted-papers.php)",
    "pdf_url": "http://arxiv.org/pdf/2501.02840v1",
    "published_date": "2025-01-06 08:36:44 UTC",
    "updated_date": "2025-01-06 08:36:44 UTC"
  },
  {
    "arxiv_id": "2501.03292v1",
    "title": "Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model",
    "authors": [
      "Naibo Wang",
      "Yuchen Deng",
      "Shichen Fan",
      "Jianwei Yin",
      "See-Kiong Ng"
    ],
    "abstract": "Federated learning (FL) has attracted considerable interest in the medical\ndomain due to its capacity to facilitate collaborative model training while\nmaintaining data privacy. However, conventional FL methods typically\nnecessitate multiple communication rounds, leading to significant communication\noverhead and delays, especially in environments with limited bandwidth.\nOne-shot federated learning addresses these issues by conducting model training\nand aggregation in a single communication round, thereby reducing communication\ncosts while preserving privacy. Among these, one-shot federated ensemble\nlearning combines independently trained client models using ensemble techniques\nsuch as voting, further boosting performance in non-IID data scenarios. On the\nother hand, existing machine learning methods in healthcare predominantly use\nunimodal data (e.g., medical images or textual reports), which restricts their\ndiagnostic accuracy and comprehensiveness. Therefore, the integration of\nmulti-modal data is proposed to address these shortcomings. In this paper, we\nintroduce FedMME, an innovative one-shot multi-modal federated ensemble\nlearning framework that utilizes multi-modal data for medical image analysis.\nSpecifically, FedMME capitalizes on vision large language models to produce\ntextual reports from medical images, employs a BERT model to extract textual\nfeatures from these reports, and amalgamates these features with visual\nfeatures to improve diagnostic accuracy. Experimental results show that our\nmethod demonstrated superior performance compared to existing one-shot\nfederated learning methods in healthcare scenarios across four datasets with\nvarious data distributions. For instance, it surpasses existing one-shot\nfederated learning approaches by more than 17.5% in accuracy on the RSNA\ndataset when applying a Dirichlet distribution with ($\\alpha$ = 0.3).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03292v1",
    "published_date": "2025-01-06 08:36:28 UTC",
    "updated_date": "2025-01-06 08:36:28 UTC"
  },
  {
    "arxiv_id": "2501.05471v1",
    "title": "Found in Translation: semantic approaches for enhancing AI interpretability in face verification",
    "authors": [
      "Miriam Doh",
      "Caroline Mazini Rodrigues",
      "N. Boutry",
      "L. Najman",
      "Matei Mancas",
      "Bernard Gosselin"
    ],
    "abstract": "The increasing complexity of machine learning models in computer vision,\nparticularly in face verification, requires the development of explainable\nartificial intelligence (XAI) to enhance interpretability and transparency.\nThis study extends previous work by integrating semantic concepts derived from\nhuman cognitive processes into XAI frameworks to bridge the comprehension gap\nbetween model outputs and human understanding. We propose a novel approach\ncombining global and local explanations, using semantic features defined by\nuser-selected facial landmarks to generate similarity maps and textual\nexplanations via large language models (LLMs). The methodology was validated\nthrough quantitative experiments and user feedback, demonstrating improved\ninterpretability. Results indicate that our semantic-based approach,\nparticularly the most detailed set, offers a more nuanced understanding of\nmodel decisions than traditional methods. User studies highlight a preference\nfor our semantic explanations over traditional pixelbased heatmaps, emphasizing\nthe benefits of human-centric interpretability in AI. This work contributes to\nthe ongoing efforts to create XAI frameworks that align AI models behaviour\nwith human cognitive processes, fostering trust and acceptance in critical\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05471v1",
    "published_date": "2025-01-06 08:34:53 UTC",
    "updated_date": "2025-01-06 08:34:53 UTC"
  },
  {
    "arxiv_id": "2501.02837v1",
    "title": "Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation",
    "authors": [
      "Kairui Fu",
      "Zheqi Lv",
      "Shengyu Zhang",
      "Fan Wu",
      "Kun Kuang"
    ],
    "abstract": "In cloud-centric recommender system, regular data exchanges between user\ndevices and cloud could potentially elevate bandwidth demands and privacy\nrisks. On-device recommendation emerges as a viable solution by performing\nreranking locally to alleviate these concerns. Existing methods primarily focus\non developing local adaptive parameters, while potentially neglecting the\ncritical role of tailor-made model architecture. Insights from broader research\ndomains suggest that varying data distributions might favor distinct\narchitectures for better fitting. In addition, imposing a uniform model\nstructure across heterogeneous devices may result in risking inefficacy on less\ncapable devices or sub-optimal performance on those with sufficient\ncapabilities. In response to these gaps, our paper introduces Forward-OFA, a\nnovel approach for the dynamic construction of device-specific networks (both\nstructure and parameters). Forward-OFA employs a structure controller to\nselectively determine whether each block needs to be assembled for a given\ndevice. However, during the training of the structure controller, these\nassembled heterogeneous structures are jointly optimized, where the co-adaption\namong blocks might encounter gradient conflicts. To mitigate this, Forward-OFA\nis designed to establish a structure-guided mapping of real-time behaviors to\nthe parameters of assembled networks. Structure-related parameters and parallel\ncomponents within the mapper prevent each part from receiving heterogeneous\ngradients from others, thus bypassing the gradient conflicts for coupled\noptimization. Besides, direct mapping enables Forward-OFA to achieve adaptation\nthrough only one forward pass, allowing for swift adaptation to changing\ninterests and eliminating the requirement for on-device backpropagation.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof Forward-OFA.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02837v1",
    "published_date": "2025-01-06 08:32:16 UTC",
    "updated_date": "2025-01-06 08:32:16 UTC"
  },
  {
    "arxiv_id": "2502.15701v1",
    "title": "Political Events using RAG with LLMs",
    "authors": [
      "Muhammad Arslan",
      "Saba Munawar",
      "Christophe Cruz"
    ],
    "abstract": "In the contemporary digital landscape, media content stands as the foundation\nfor political news analysis, offering invaluable insights sourced from various\nchannels like news articles, social media updates, speeches, and reports.\nNatural Language Processing (NLP) has revolutionized Political Information\nExtraction (IE), automating tasks such as Event Extraction (EE) from these\ndiverse media outlets. While traditional NLP methods often necessitate\nspecialized expertise to build rule-based systems or train machine learning\nmodels with domain-specific datasets, the emergence of Large Language Models\n(LLMs) driven by Generative Artificial Intelligence (GenAI) presents a\npromising alternative. These models offer accessibility, alleviating challenges\nassociated with model construction from scratch and reducing the dependency on\nextensive datasets during the training phase, thus facilitating rapid\nimplementation. However, challenges persist in handling domain-specific tasks,\nleading to the development of the Retrieval-Augmented Generation (RAG)\nframework. RAG enhances LLMs by integrating external data retrieval, enriching\ntheir contextual understanding, and expanding their knowledge base beyond\npre-existing training data. To illustrate RAG's efficacy, we introduce the\nPolitical EE system, specifically tailored to extract political event\ninformation from news articles. Understanding these political insights is\nessential for remaining informed about the latest political advancements,\nwhether on a national or global scale.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15701v1",
    "published_date": "2025-01-06 08:16:24 UTC",
    "updated_date": "2025-01-06 08:16:24 UTC"
  },
  {
    "arxiv_id": "2501.02832v3",
    "title": "Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models",
    "authors": [
      "Syed Abdul Gaffar Shakhadri",
      "Kruthika KR",
      "Kartik Basavaraj Angadi"
    ],
    "abstract": "We propose Samba ASR,the first state of the art Automatic Speech\nRecognition(ASR)model leveraging the novel Mamba architecture as both encoder\nand decoder,built on the foundation of state space models(SSMs).Unlike\ntransformerbased ASR models,which rely on self-attention mechanisms to capture\ndependencies,Samba ASR effectively models both local and global temporal\ndependencies using efficient statespace dynamics,achieving remarkable\nperformance gains.By addressing the limitations of transformers,such as\nquadratic scaling with input length and difficulty in handling longrange\ndependencies,Samba ASR achieves superior accuracy and efficiency.Experimental\nresults demonstrate that Samba ASR surpasses existing opensource\ntransformerbased ASR models across various standard benchmarks,establishing it\nas the new state of theart in ASR.Extensive evaluations on the benchmark\ndataset show significant improvements in Word Error Rate(WER),with competitive\nperformance even in lowresource scenarios.Furthermore,the inherent\ncomputational efficiency and parameter optimization of the Mamba architecture\nmake Samba ASR a scalable and robust solution for diverse ASR tasks.Our\ncontributions include the development of a new Samba ASR architecture for\nautomatic speech recognition(ASR),demonstrating the superiority of structured\nstatespace models(SSMs)over transformer based models for speech sequence\nprocessing.We provide a comprehensive evaluation on public\nbenchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth\nanalysis of computational efficiency,robustness to noise,and sequence\ngeneralization.This work highlights the viability of Mamba SSMs as a\ntransformerfree alternative for efficient and accurate ASR.By leveraging the\nadvancements of statespace modeling,Samba ASR redefines ASR performance\nstandards and sets a new benchmark for future research in this field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02832v3",
    "published_date": "2025-01-06 08:16:06 UTC",
    "updated_date": "2025-01-08 17:46:40 UTC"
  },
  {
    "arxiv_id": "2502.15700v1",
    "title": "Sustainable Digitalization of Business with Multi-Agent RAG and LLM",
    "authors": [
      "Muhammad Arslan",
      "Saba Munawar",
      "Christophe Cruz"
    ],
    "abstract": "Businesses heavily rely on data sourced from various channels like news\narticles, financial reports, and consumer reviews to drive their operations,\nenabling informed decision-making and identifying opportunities. However,\ntraditional manual methods for data extraction are often time-consuming and\nresource-intensive, prompting the adoption of digital transformation\ninitiatives to enhance efficiency. Yet, concerns persist regarding the\nsustainability of such initiatives and their alignment with the United Nations\n(UN)'s Sustainable Development Goals (SDGs). This research aims to explore the\nintegration of Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) as a sustainable solution for Information Extraction (IE) and processing.\nThe research methodology involves reviewing existing solutions for business\ndecision-making, noting that many systems require training new machine learning\nmodels, which are resource-intensive and have significant environmental\nimpacts. Instead, we propose a sustainable business solution using pre-existing\nLLMs that can work with diverse datasets. We link domain-specific datasets to\ntailor LLMs to company needs and employ a Multi-Agent architecture to divide\ntasks such as information retrieval, enrichment, and classification among\nspecialized agents. This approach optimizes the extraction process and improves\noverall efficiency. Through the utilization of these technologies, businesses\ncan optimize resource utilization, improve decision-making processes, and\ncontribute to sustainable development goals, thereby fostering environmental\nresponsibility within the corporate sector.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15700v1",
    "published_date": "2025-01-06 08:14:23 UTC",
    "updated_date": "2025-01-06 08:14:23 UTC"
  },
  {
    "arxiv_id": "2501.02822v1",
    "title": "RDD4D: 4D Attention-Guided Road Damage Detection And Classification",
    "authors": [
      "Asma Alkalbani",
      "Muhammad Saqib",
      "Ahmed Salim Alrawahi",
      "Abbas Anwar",
      "Chandarnath Adak",
      "Saeed Anwar"
    ],
    "abstract": "Road damage detection and assessment are crucial components of infrastructure\nmaintenance. However, current methods often struggle with detecting multiple\ntypes of road damage in a single image, particularly at varying scales. This is\ndue to the lack of road datasets with various damage types having varying\nscales. To overcome this deficiency, first, we present a novel dataset called\nDiverse Road Damage Dataset (DRDD) for road damage detection that captures the\ndiverse road damage types in individual images, addressing a crucial gap in\nexisting datasets. Then, we provide our model, RDD4D, that exploits Attention4D\nblocks, enabling better feature refinement across multiple scales. The\nAttention4D module processes feature maps through an attention mechanism\ncombining positional encoding and \"Talking Head\" components to capture local\nand global contextual information. In our comprehensive experimental analysis\ncomparing various state-of-the-art models on our proposed, our enhanced model\ndemonstrated superior performance in detecting large-sized road cracks with an\nAverage Precision (AP) of 0.458 and maintained competitive performance with an\noverall AP of 0.445. Moreover, we also provide results on the CrackTinyNet\ndataset; our model achieved around a 0.21 increase in performance. The code,\nmodel weights, dataset, and our results are available on\n\\href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\\_Damage\\_Detection}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02822v1",
    "published_date": "2025-01-06 07:48:04 UTC",
    "updated_date": "2025-01-06 07:48:04 UTC"
  },
  {
    "arxiv_id": "2501.02816v1",
    "title": "InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models",
    "authors": [
      "Kai Wang",
      "Shaozhang Niu",
      "Qixian Hao",
      "Jiwei Zhang"
    ],
    "abstract": "As artificial intelligence advances rapidly, particularly with the advent of\nGANs and diffusion models, the accuracy of Image Inpainting Localization (IIL)\nhas become increasingly challenging. Current IIL methods face two main\nchallenges: a tendency towards overconfidence, leading to incorrect\npredictions; and difficulty in detecting subtle tampering boundaries in\ninpainted images. In response, we propose a new paradigm that treats IIL as a\nconditional mask generation task utilizing diffusion models. Our method,\nInpDiffusion, utilizes the denoising process enhanced by the integration of\nimage semantic conditions to progressively refine predictions. During\ndenoising, we employ edge conditions and introduce a novel edge supervision\nstrategy to enhance the model's perception of edge details in inpainted\nobjects. Balancing the diffusion model's stochastic sampling with edge\nsupervision of tampered image regions mitigates the risk of incorrect\npredictions from overconfidence and prevents the loss of subtle boundaries that\ncan result from overly stochastic processes. Furthermore, we propose an\ninnovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting\nmulti-scale features, enhancing feature representation by considering both\nsemantic and edge conditions of the inpainted images. Extensive experiments\nacross challenging datasets demonstrate that the InpDiffusion significantly\noutperforms existing state-of-the-art methods in IIL tasks, while also\nshowcasing excellent generalization capabilities and robustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02816v1",
    "published_date": "2025-01-06 07:32:12 UTC",
    "updated_date": "2025-01-06 07:32:12 UTC"
  },
  {
    "arxiv_id": "2501.03290v1",
    "title": "A Decision-Based Heterogenous Graph Attention Network for Multi-Class Fake News Detection",
    "authors": [
      "Batool Lakzaei",
      "Mostafa Haghir Chehreghani",
      "Alireza Bagheri"
    ],
    "abstract": "A promising tool for addressing fake news detection is Graph Neural Networks\n(GNNs). However, most existing GNN-based methods rely on binary classification,\ncategorizing news as either real or fake. Additionally, traditional GNN models\nuse a static neighborhood for each node, making them susceptible to issues like\nover-squashing. In this paper, we introduce a novel model named Decision-based\nHeterogeneous Graph Attention Network (DHGAT) for fake news detection in a\nsemi-supervised setting. DHGAT effectively addresses the limitations of\ntraditional GNNs by dynamically optimizing and selecting the neighborhood type\nfor each node in every layer. It represents news data as a heterogeneous graph\nwhere nodes (news items) are connected by various types of edges. The\narchitecture of DHGAT consists of a decision network that determines the\noptimal neighborhood type and a representation network that updates node\nembeddings based on this selection. As a result, each node learns an optimal\nand task-specific computational graph, enhancing both the accuracy and\nefficiency of the fake news detection process. We evaluate DHGAT on the LIAR\ndataset, a large and challenging dataset for multi-class fake news detection,\nwhich includes news items categorized into six classes. Our results demonstrate\nthat DHGAT outperforms existing methods, improving accuracy by approximately 4%\nand showing robustness with limited labeled data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03290v1",
    "published_date": "2025-01-06 07:18:31 UTC",
    "updated_date": "2025-01-06 07:18:31 UTC"
  },
  {
    "arxiv_id": "2501.02803v1",
    "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
    "authors": [
      "Yimin Tang",
      "Zhenghong Yu",
      "Yi Zheng",
      "T. K. Satish Kumar",
      "Jiaoyang Li",
      "Sven Koenig"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
    "pdf_url": "http://arxiv.org/pdf/2501.02803v1",
    "published_date": "2025-01-06 06:44:13 UTC",
    "updated_date": "2025-01-06 06:44:13 UTC"
  },
  {
    "arxiv_id": "2501.02793v1",
    "title": "Fairness Through Matching",
    "authors": [
      "Kunwoong Kim",
      "Insung Kong",
      "Jongjin Lee",
      "Minwoo Chae",
      "Sangchul Park",
      "Yongdai Kim"
    ],
    "abstract": "Group fairness requires that different protected groups, characterized by a\ngiven sensitive attribute, receive equal outcomes overall. Typically, the level\nof group fairness is measured by the statistical gap between predictions from\ndifferent protected groups. In this study, we reveal an implicit property of\nexisting group fairness measures, which provides an insight into how the\ngroup-fair models behave. Then, we develop a new group-fair constraint based on\nthis implicit property to learn group-fair models. To do so, we first introduce\na notable theoretical observation: every group-fair model has an implicitly\ncorresponding transport map between the input spaces of each protected group.\nBased on this observation, we introduce a new group fairness measure termed\nMatched Demographic Parity (MDP), which quantifies the averaged gap between\npredictions of two individuals (from different protected groups) matched by a\ngiven transport map. Then, we prove that any transport map can be used in MDP\nto learn group-fair models, and develop a novel algorithm called Fairness\nThrough Matching (FTM), which learns a group-fair model using MDP constraint\nwith an user-specified transport map. We specifically propose two favorable\ntypes of transport maps for MDP, based on the optimal transport theory, and\ndiscuss their advantages. Experiments reveal that FTM successfully trains\ngroup-fair models with certain desirable properties by choosing the transport\nmap accordingly.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in TMLR",
    "pdf_url": "http://arxiv.org/pdf/2501.02793v1",
    "published_date": "2025-01-06 06:27:38 UTC",
    "updated_date": "2025-01-06 06:27:38 UTC"
  },
  {
    "arxiv_id": "2501.02790v1",
    "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model",
    "authors": [
      "Yueqin Yin",
      "Shentao Yang",
      "Yujia Xie",
      "Ziyi Yang",
      "Yuting Sun",
      "Hany Awadalla",
      "Weizhu Chen",
      "Mingyuan Zhou"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has been widely adopted to\nalign language models (LMs) with human preference. Prior RLHF works typically\ntake a bandit formulation, which, though intuitive, ignores the sequential\nnature of LM generation and can suffer from the sparse reward issue. While\nrecent works propose dense token-level RLHF, treating each token as an action\nmay be oversubtle to proper reward assignment. In this paper, we seek to get\nthe best of both by training and utilizing a segment-level reward model, which\nassigns a reward to each semantically complete text segment that spans over a\nshort sequence of tokens. For reward learning, our method allows dynamic text\nsegmentation and compatibility with standard sequence-preference datasets. For\neffective RL-based LM training against segment reward, we generalize the\nclassical scalar bandit reward normalizers into location-aware normalizer\nfunctions and interpolate the segment reward for further densification. With\nthese designs, our method performs competitively on three popular RLHF\nbenchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation\nstudies are conducted to further demonstrate our method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02790v1",
    "published_date": "2025-01-06 06:17:56 UTC",
    "updated_date": "2025-01-06 06:17:56 UTC"
  },
  {
    "arxiv_id": "2501.03288v1",
    "title": "CodeVision: Detecting LLM-Generated Code Using 2D Token Probability Maps and Vision Models",
    "authors": [
      "Zhenyu Xu",
      "Victor S. Sheng"
    ],
    "abstract": "The rise of large language models (LLMs) like ChatGPT has significantly\nimproved automated code generation, enhancing software development efficiency.\nHowever, this introduces challenges in academia, particularly in distinguishing\nbetween human-written and LLM-generated code, which complicates issues of\nacademic integrity. Existing detection methods, such as pre-trained models and\nwatermarking, face limitations in adaptability and computational efficiency. In\nthis paper, we propose a novel detection method using 2D token probability maps\ncombined with vision models, preserving spatial code structures such as\nindentation and brackets. By transforming code into log probability matrices\nand applying vision models like Vision Transformers (ViT) and ResNet, we\ncapture both content and structure for more accurate detection. Our method\nshows robustness across multiple programming languages and improves upon\ntraditional detectors, offering a scalable and computationally efficient\nsolution for identifying LLM-generated code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03288v1",
    "published_date": "2025-01-06 06:15:10 UTC",
    "updated_date": "2025-01-06 06:15:10 UTC"
  },
  {
    "arxiv_id": "2501.02788v2",
    "title": "GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation",
    "authors": [
      "Niloufar Eghbali",
      "Hassan Bagher-Ebadian",
      "Tuka Alhanai",
      "Mohammad M. Ghassemi"
    ],
    "abstract": "Vision Transformers (ViTs) have shown promise in medical image semantic\nsegmentation (MISS) by capturing long-range correlations. However, ViTs often\nstruggle to model local spatial information effectively, which is essential for\naccurately segmenting fine anatomical details, particularly when applied to\nsmall datasets without extensive pre-training. We introduce Gabor and Laplacian\nof Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture\nenhancing Transformer-based models by incorporating learnable radiomic\nfeatures. This approach integrates dynamically adaptive Gabor and Laplacian of\nGaussian (LoG) filters to capture texture, edge, and boundary information,\nenhancing the feature representation processed by the Transformer model. Our\nmethod uniquely combines the long-range dependency modeling of Transformers\nwith the texture analysis capabilities of Gabor and LoG features. Evaluated on\nthe Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet\ndemonstrates significant improvements over state-of-the-art models, achieving a\n1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal\ncomputational overhead (only 15 and 30 additional parameters, respectively).\nGLoG-CSUnet's flexible design allows integration with various base models,\noffering a promising approach for incorporating radiomics-inspired feature\nextraction in Transformer architectures for medical image analysis. The code\nimplementation is available on GitHub at: https://github.com/HAAIL/GLoG-CSUnet.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02788v2",
    "published_date": "2025-01-06 06:07:40 UTC",
    "updated_date": "2025-01-08 18:33:07 UTC"
  },
  {
    "arxiv_id": "2501.02785v1",
    "title": "Hybrid deep convolution model for lung cancer detection with transfer learning",
    "authors": [
      "Sugandha Saxena",
      "S. N. Prasad",
      "Ashwin M Polnaya",
      "Shweta Agarwala"
    ],
    "abstract": "Advances in healthcare research have significantly enhanced our understanding\nof disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung\ncancer remains one of the leading causes of cancer-related mortality worldwide\ndue to challenges in early and accurate diagnosis. While current lung cancer\ndetection models show promise, there is considerable potential for further\nimproving the accuracy for timely intervention. To address this challenge, we\nintroduce a hybrid deep convolution model leveraging transfer learning, named\nthe Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the\nprecision of lung cancer detection by refining sensitivity and specificity.\nThis model has surpassed existing deep learning approaches through experimental\nvalidation, achieving an accuracy of 98% and a sensitivity of 97%. By\noverlaying sensitivity maps onto lung Computed Tomography (CT) scans, it\nenables the visualization of regions most indicative of malignant or benign\nclassifications. This innovative method demonstrates exceptional performance in\ndistinguishing lung cancer with minimal false positives, thereby enhancing the\naccuracy of medical diagnoses.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02785v1",
    "published_date": "2025-01-06 06:01:01 UTC",
    "updated_date": "2025-01-06 06:01:01 UTC"
  },
  {
    "arxiv_id": "2501.02778v1",
    "title": "ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction",
    "authors": [
      "Binyu Zhang",
      "Zhu Meng",
      "Junhao Dong",
      "Fei Su",
      "Zhicheng Zhao"
    ],
    "abstract": "Survival prediction is a crucial task in the medical field and is essential\nfor optimizing treatment options and resource allocation. However, current\nmethods often rely on limited data modalities, resulting in suboptimal\nperformance. In this paper, we propose an Integrated Cross-modal Fusion Network\n(ICFNet) that integrates histopathology whole slide images, genomic expression\nprofiles, patient demographics, and treatment protocols. Specifically, three\ntypes of encoders, a residual orthogonal decomposition module and a unification\nfusion module are employed to merge multi-modal features to enhance prediction\naccuracy. Additionally, a balanced negative log-likelihood loss function is\ndesigned to ensure fair training across different patients. Extensive\nexperiments demonstrate that our ICFNet outperforms state-of-the-art algorithms\non five public TCGA datasets, including BLCA, BRCA, GBMLGG, LUAD, and UCEC, and\nshows its potential to support clinical decision-making and advance precision\nmedicine. The codes are available at: https://github.com/binging512/ICFNet.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02778v1",
    "published_date": "2025-01-06 05:49:08 UTC",
    "updated_date": "2025-01-06 05:49:08 UTC"
  },
  {
    "arxiv_id": "2501.02770v2",
    "title": "Multi-Agent Path Finding under Limited Communication Range Constraint via Dynamic Leading",
    "authors": [
      "Hoang-Dung Bui",
      "Erion Plaku",
      "Gregoy J. Stein"
    ],
    "abstract": "This paper proposes a novel framework to handle a multi-agent path finding\nproblem under a limited communication range constraint, where all agents must\nhave a connected communication channel to the rest of the team. Many existing\napproaches to multi-agent path finding (e.g., leader-follower platooning)\novercome computational challenges of planning in this domain by planning one\nagent at a time in a fixed order. However, fixed leader-follower approaches can\nbecome stuck during planning, limiting their practical utility in dense-clutter\nenvironments. To overcome this limitation, we develop dynamic leading\nmulti-agent path finding, which allows for dynamic reselection of the leading\nagent during path planning whenever progress cannot be made. The experiments\nshow the efficiency of our framework, which can handle up to 25 agents with\nmore than 90% success-rate across five environment types where baselines\nroutinely fail.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02770v2",
    "published_date": "2025-01-06 05:21:18 UTC",
    "updated_date": "2025-02-05 15:32:43 UTC"
  },
  {
    "arxiv_id": "2501.02767v1",
    "title": "Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training",
    "authors": [
      "Ting Wang",
      "Zhixin Zhou",
      "Rui Luo"
    ],
    "abstract": "Graph Neural Networks (GNNs) has been widely used in a variety of fields\nbecause of their great potential in representing graph-structured data.\nHowever, lacking of rigorous uncertainty estimations limits their application\nin high-stakes. Conformal Prediction (CP) can produce statistically guaranteed\nuncertainty estimates by using the classifier's probability estimates to obtain\nprediction sets, which contains the true class with a user-specified\nprobability. In this paper, we propose a Rank-based CP during training\nframework to GNNs (RCP-GNN) for reliable uncertainty estimates to enhance the\ntrustworthiness of GNNs in the node classification scenario. By exploiting rank\ninformation of the classifier's outcome, prediction sets with desired coverage\nrate can be efficiently constructed. The strategy of CP during training with\ndifferentiable rank-based conformity loss function is further explored to adapt\nprediction sets according to network topology information. In this way, the\ncomposition of prediction sets can be guided by the goal of jointly reducing\ninefficiency and probability estimation errors. Extensive experiments on\nseveral real-world datasets show that our model achieves any pre-defined target\nmarginal coverage while significantly reducing the inefficiency compared with\nstate-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages,2 figures,published to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02767v1",
    "published_date": "2025-01-06 05:19:24 UTC",
    "updated_date": "2025-01-06 05:19:24 UTC"
  },
  {
    "arxiv_id": "2501.02766v2",
    "title": "Are GNNs Actually Effective for Multimodal Fault Diagnosis in Microservice Systems?",
    "authors": [
      "Fei Gao",
      "Ruyue Xin",
      "Xiaocui Li",
      "Yaqiang Zhang"
    ],
    "abstract": "Graph Neural Networks (GNNs) are widely adopted for fault diagnosis in\nmicroservice systems, premised on their ability to model service dependencies.\nHowever, the necessity of explicit graph structures remains underexamined, as\nexisting evaluations conflate preprocessing with architectural contributions.\nTo isolate the true value of GNNs, we propose DiagMLP, a deliberately minimal,\ntopology-agnostic baseline that retains multimodal fusion capabilities while\nexcluding graph modeling. Through ablation experiments across five datasets,\nDiagMLP achieves performance parity with state-of-the-art GNN-based methods in\nfault detection, localization, and classification. These findings challenge the\nprevailing assumption that graph structures are indispensable, revealing that:\n(i) preprocessing pipelines already encode critical dependency information, and\n(ii) GNN modules contribute marginally beyond multimodality fusion. Our work\nadvocates for systematic re-evaluation of architectural complexity and\nhighlights the need for standardized baseline protocols to validate model\ninnovations.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "6 pages, 5 figures, submitted to conference",
    "pdf_url": "http://arxiv.org/pdf/2501.02766v2",
    "published_date": "2025-01-06 05:18:13 UTC",
    "updated_date": "2025-03-10 09:51:12 UTC"
  },
  {
    "arxiv_id": "2501.02765v1",
    "title": "Visual Large Language Models for Generalized and Specialized Applications",
    "authors": [
      "Yifan Li",
      "Zhixin Lai",
      "Wentao Bao",
      "Zhen Tan",
      "Anh Dao",
      "Kewei Sui",
      "Jiayi Shen",
      "Dong Liu",
      "Huan Liu",
      "Yu Kong"
    ],
    "abstract": "Visual-language models (VLM) have emerged as a powerful tool for learning a\nunified embedding space for vision and language. Inspired by large language\nmodels, which have demonstrated strong reasoning and multi-task capabilities,\nvisual large language models (VLLMs) are gaining increasing attention for\nbuilding general-purpose VLMs. Despite the significant progress made in VLLMs,\nthe related literature remains limited, particularly from a comprehensive\napplication perspective, encompassing generalized and specialized applications\nacross vision (image, video, depth), action, and language modalities. In this\nsurvey, we focus on the diverse applications of VLLMs, examining their using\nscenarios, identifying ethics consideration and challenges, and discussing\nfuture directions for their development. By synthesizing these contents, we aim\nto provide a comprehensive guide that will pave the way for future innovations\nand broader applications of VLLMs. The paper list repository is available:\nhttps://github.com/JackYFL/awesome-VLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02765v1",
    "published_date": "2025-01-06 05:15:59 UTC",
    "updated_date": "2025-01-06 05:15:59 UTC"
  },
  {
    "arxiv_id": "2501.02749v2",
    "title": "Intelligent logistics management robot path planning algorithm integrating transformer and GCN network",
    "authors": [
      "Hao Luo",
      "Jianjun Wei",
      "Shuchen Zhao",
      "Ankai Liang",
      "Zhongjin Xu",
      "Ruxue Jiang"
    ],
    "abstract": "This research delves into advanced route optimization for robots in smart\nlogistics, leveraging a fusion of Transformer architectures, Graph Neural\nNetworks (GNNs), and Generative Adversarial Networks (GANs). The approach\nutilizes a graph-based representation encompassing geographical data, cargo\nallocation, and robot dynamics, addressing both spatial and resource\nlimitations to refine route efficiency. Through extensive testing with\nauthentic logistics datasets, the proposed method achieves notable\nimprovements, including a 15% reduction in travel distance, a 20% boost in time\nefficiency, and a 10% decrease in energy consumption. These findings highlight\nthe algorithm's effectiveness, promoting enhanced performance in intelligent\nlogistics operations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.02749v2",
    "published_date": "2025-01-06 03:53:02 UTC",
    "updated_date": "2025-03-12 03:29:21 UTC"
  },
  {
    "arxiv_id": "2501.02740v1",
    "title": "Interpretable Recognition of Fused Magnesium Furnace Working Conditions with Deep Convolutional Stochastic Configuration Networks",
    "authors": [
      "Li Weitao",
      "Zhang Xinru",
      "Wang Dianhui",
      "Tong Qianqian",
      "Chai Tianyou"
    ],
    "abstract": "To address the issues of a weak generalization capability and\ninterpretability in working condition recognition model of a fused magnesium\nfurnace, this paper proposes an interpretable working condition recognition\nmethod based on deep convolutional stochastic configuration networks (DCSCNs).\nFirstly, a supervised learning mechanism is employed to generate physically\nmeaningful Gaussian differential convolution kernels. An incremental method is\nutilized to construct a DCSCNs model, ensuring the convergence of recognition\nerrors in a hierarchical manner and avoiding the iterative optimization process\nof convolutional kernel parameters using the widely used backpropagation\nalgorithm. The independent coefficient of channel feature maps is defined to\nobtain the visualization results of feature class activation maps for the fused\nmagnesium furnace. A joint reward function is constructed based on the\nrecognition accuracy, the interpretable trustworthiness evaluation metrics, and\nthe model parameter quantity. Reinforcement learning (RL) is applied to\nadaptively prune the convolutional kernels of the DCSCNs model, aiming to build\na compact, highly performed and interpretable network. The experimental results\ndemonstrate that the proposed method outperforms the other deep learning\napproaches in terms of recognition accuracy and interpretability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02740v1",
    "published_date": "2025-01-06 03:17:41 UTC",
    "updated_date": "2025-01-06 03:17:41 UTC"
  },
  {
    "arxiv_id": "2501.02739v1",
    "title": "TARDiS : Text Augmentation for Refining Diversity and Separability",
    "authors": [
      "Kyungmin Kim",
      "SangHun Im",
      "GiBaeg Kim",
      "Heung-Seon Oh"
    ],
    "abstract": "Text augmentation (TA) is a critical technique for text classification,\nespecially in few-shot settings. This paper introduces a novel LLM-based TA\nmethod, TARDiS, to address challenges inherent in the generation and alignment\nstages of two-stage TA methods. For the generation stage, we propose two\ngeneration processes, SEG and CEG, incorporating multiple class-specific\nprompts to enhance diversity and separability. For the alignment stage, we\nintroduce a class adaptation (CA) method to ensure that generated examples\nalign with their target classes through verification and modification.\nExperimental results demonstrate TARDiS's effectiveness, outperforming\nstate-of-the-art LLM-based TA methods in various few-shot text classification\ntasks. An in-depth analysis confirms the detailed behaviors at each stage.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.02739v1",
    "published_date": "2025-01-06 03:17:35 UTC",
    "updated_date": "2025-01-06 03:17:35 UTC"
  },
  {
    "arxiv_id": "2501.02732v1",
    "title": "AFed: Algorithmic Fair Federated Learning",
    "authors": [
      "Huiqiang Chen",
      "Tianqing Zhu",
      "Wanlei Zhou",
      "Wei Zhao"
    ],
    "abstract": "Federated Learning (FL) has gained significant attention as it facilitates\ncollaborative machine learning among multiple clients without centralizing\ntheir data on a server. FL ensures the privacy of participating clients by\nlocally storing their data, which creates new challenges in fairness.\nTraditional debiasing methods assume centralized access to sensitive\ninformation, rendering them impractical for the FL setting. Additionally, FL is\nmore susceptible to fairness issues than centralized machine learning due to\nthe diverse client data sources that may be associated with group information.\nTherefore, training a fair model in FL without access to client local data is\nimportant and challenging. This paper presents AFed, a straightforward yet\neffective framework for promoting group fairness in FL. The core idea is to\ncircumvent restricted data access by learning the global data distribution.\nThis paper proposes two approaches: AFed-G, which uses a conditional generator\ntrained on the server side, and AFed-GAN, which improves upon AFed-G by\ntraining a conditional GAN on the client side. We augment the client data with\nthe generated samples to help remove bias. Our theoretical analysis justifies\nthe proposed methods, and empirical results on multiple real-world datasets\ndemonstrate a substantial improvement in AFed over several baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems",
    "pdf_url": "http://arxiv.org/pdf/2501.02732v1",
    "published_date": "2025-01-06 03:05:49 UTC",
    "updated_date": "2025-01-06 03:05:49 UTC"
  },
  {
    "arxiv_id": "2501.05470v1",
    "title": "RTLSquad: Multi-Agent Based Interpretable RTL Design",
    "authors": [
      "Bowei Wang",
      "Qi Xiong",
      "Zeqing Xiang",
      "Lei Wang",
      "Renzhi Chen"
    ],
    "abstract": "Optimizing Register-Transfer Level (RTL) code is crucial for improving\nhardware PPA performance. Large Language Models (LLMs) offer new approaches for\nautomatic RTL code generation and optimization. However, existing methods often\nlack decision interpretability (sufficient, understandable justification for\ndecisions), making it difficult for hardware engineers to trust the generated\nresults, thus preventing these methods from being integrated into the design\nprocess. To address this, we propose RTLSquad, a novel LLM-Based Multi-Agent\nsystem for interpretable RTL code generation. RTLSquad divides the design\nprocess into exploration, implementation, and verification & evaluation stages\nmanaged by specialized agent squads, generating optimized RTL code through\ninter-agent collaboration, and providing decision interpretability through the\ncommunication process. Experiments show that RTLSquad excels in generating\nfunctionally correct RTL code and optimizing PPA performance, while also having\nthe capability to provide decision paths, demonstrating the practical value of\nour system.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05470v1",
    "published_date": "2025-01-06 02:57:54 UTC",
    "updated_date": "2025-01-06 02:57:54 UTC"
  },
  {
    "arxiv_id": "2501.02728v1",
    "title": "OpenGU: A Comprehensive Benchmark for Graph Unlearning",
    "authors": [
      "Bowen Fan",
      "Yuming Ai",
      "Xunkai Li",
      "Zhilin Guo",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "Graph Machine Learning is essential for understanding and analyzing\nrelational data. However, privacy-sensitive applications demand the ability to\nefficiently remove sensitive information from trained graph neural networks\n(GNNs), avoiding the unnecessary time and space overhead caused by retraining\nmodels from scratch. To address this issue, Graph Unlearning (GU) has emerged\nas a critical solution, with the potential to support dynamic graph updates in\ndata management systems and enable scalable unlearning in distributed data\nsystems while ensuring privacy compliance. Unlike machine unlearning in\ncomputer vision or other fields, GU faces unique difficulties due to the\nnon-Euclidean nature of graph data and the recursive message-passing mechanism\nof GNNs. Additionally, the diversity of downstream tasks and the complexity of\nunlearning requests further amplify these challenges. Despite the proliferation\nof diverse GU strategies, the absence of a benchmark providing fair comparisons\nfor GU, and the limited flexibility in combining downstream tasks and\nunlearning requests, have yielded inconsistencies in evaluations, hindering the\ndevelopment of this domain. To fill this gap, we present OpenGU, the first GU\nbenchmark, where 16 SOTA GU algorithms and 37 multi-domain datasets are\nintegrated, enabling various downstream tasks with 13 GNN backbones when\nresponding to flexible unlearning requests. Based on this unified benchmark\nframework, we are able to provide a comprehensive and fair evaluation for GU.\nThrough extensive experimentation, we have drawn $8$ crucial conclusions about\nexisting GU methods, while also gaining valuable insights into their\nlimitations, shedding light on potential avenues for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2501.02728v1",
    "published_date": "2025-01-06 02:57:32 UTC",
    "updated_date": "2025-01-06 02:57:32 UTC"
  },
  {
    "arxiv_id": "2501.02727v1",
    "title": "Tree-based RAG-Agent Recommendation System: A Case Study in Medical Test Data",
    "authors": [
      "Yahe Yang",
      "Chengyue Huang"
    ],
    "abstract": "We present HiRMed (Hierarchical RAG-enhanced Medical Test Recommendation), a\nnovel tree-structured recommendation system that leverages Retrieval-Augmented\nGeneration (RAG) for intelligent medical test recommendations. Unlike\ntraditional vector similarity-based approaches, our system performs medical\nreasoning at each tree node through a specialized RAG process. Starting from\nthe root node with initial symptoms, the system conducts step-wise medical\nanalysis to identify potential underlying conditions and their corresponding\ndiagnostic requirements. At each level, instead of simple matching, our\nRAG-enhanced nodes analyze retrieved medical knowledge to understand\nsymptom-disease relationships and determine the most appropriate diagnostic\npath. The system dynamically adjusts its recommendation strategy based on\nmedical reasoning results, considering factors such as urgency levels and\ndiagnostic uncertainty. Experimental results demonstrate that our approach\nachieves superior performance in terms of coverage rate, accuracy, and miss\nrate compared to conventional retrieval-based methods. This work represents a\nsignificant advance in medical test recommendation by introducing medical\nreasoning capabilities into the traditional tree-based retrieval structure.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02727v1",
    "published_date": "2025-01-06 02:50:51 UTC",
    "updated_date": "2025-01-06 02:50:51 UTC"
  },
  {
    "arxiv_id": "2501.02725v2",
    "title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
    "authors": [
      "Nantheera Anantrasirichai",
      "Fan Zhang",
      "David Bull"
    ],
    "abstract": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This is an updated review of our previous paper (see\n  https://doi.org/10.1007/s10462-021-10039-7)",
    "pdf_url": "http://arxiv.org/pdf/2501.02725v2",
    "published_date": "2025-01-06 02:46:33 UTC",
    "updated_date": "2025-02-16 10:20:10 UTC"
  },
  {
    "arxiv_id": "2501.02715v1",
    "title": "Improved Data Encoding for Emerging Computing Paradigms: From Stochastic to Hyperdimensional Computing",
    "authors": [
      "Mehran Shoushtari Moghadam",
      "Sercan Aygun",
      "M. Hassan Najafi"
    ],
    "abstract": "Data encoding is a fundamental step in emerging computing paradigms,\nparticularly in stochastic computing (SC) and hyperdimensional computing (HDC),\nwhere it plays a crucial role in determining the overall system performance and\nhardware cost efficiency. This study presents an advanced encoding strategy\nthat leverages a hardware-friendly class of low-discrepancy (LD) sequences,\nspecifically powers-of-2 bases of Van der Corput (VDC) sequences (VDC-2^n), as\nsources for random number generation. Our approach significantly enhances the\naccuracy and efficiency of SC and HDC systems by addressing challenges\nassociated with randomness. By employing LD sequences, we improve correlation\nproperties and reduce hardware complexity. Experimental results demonstrate\nsignificant improvements in accuracy and energy savings for SC and HDC systems.\nOur solution provides a robust framework for integrating SC and HDC in\nresource-constrained environments, paving the way for efficient and scalable AI\nimplementations.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.ET",
    "comment": "5 pages, 3 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.02715v1",
    "published_date": "2025-01-06 02:07:49 UTC",
    "updated_date": "2025-01-06 02:07:49 UTC"
  },
  {
    "arxiv_id": "2501.02711v1",
    "title": "KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models",
    "authors": [
      "Zaiyi Zheng",
      "Yushun Dong",
      "Song Wang",
      "Haochen Liu",
      "Qi Wang",
      "Jundong Li"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive performance in various\ntasks, including knowledge graph completion (KGC). However, current studies\nmostly apply LLMs to classification tasks, like identifying missing triplets,\nrather than ranking-based tasks, where the model ranks candidate entities based\non plausibility. This focus limits the practical use of LLMs in KGC, as\nreal-world applications prioritize highly plausible triplets. Additionally,\nwhile graph paths can help infer the existence of missing triplets and improve\ncompletion accuracy, they often contain redundant information. To address these\nissues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.\nKG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,\nachieving superior results on real-world datasets. The code and datasets are\navailable at \\url{https://anonymous.4open.science/r/KG-CF}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.02711v1",
    "published_date": "2025-01-06 01:52:15 UTC",
    "updated_date": "2025-01-06 01:52:15 UTC"
  },
  {
    "arxiv_id": "2501.02709v1",
    "title": "Horizon Generalization in Reinforcement Learning",
    "authors": [
      "Vivek Myers",
      "Catherine Ji",
      "Benjamin Eysenbach"
    ],
    "abstract": "We study goal-conditioned RL through the lens of generalization, but not in\nthe traditional sense of random augmentations and domain randomization. Rather,\nwe aim to learn goal-directed policies that generalize with respect to the\nhorizon: after training to reach nearby goals (which are easy to learn), these\npolicies should succeed in reaching distant goals (which are quite challenging\nto learn). In the same way that invariance is closely linked with\ngeneralization is other areas of machine learning (e.g., normalization layers\nmake a network invariant to scale, and therefore generalize to inputs of\nvarying scales), we show that this notion of horizon generalization is closely\nlinked with invariance to planning: a policy navigating towards a goal will\nselect the same actions as if it were navigating to a waypoint en route to that\ngoal. Thus, such a policy trained to reach nearby goals should succeed at\nreaching arbitrarily-distant goals. Our theoretical analysis proves that both\nhorizon generalization and planning invariance are possible, under some\nassumptions. We present new experimental results and recall findings from prior\nwork in support of our theoretical results. Taken together, our results open\nthe door to studying how techniques for invariance and generalization developed\nin other areas of machine learning might be adapted to achieve this alluring\nproperty.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02709v1",
    "published_date": "2025-01-06 01:42:46 UTC",
    "updated_date": "2025-01-06 01:42:46 UTC"
  },
  {
    "arxiv_id": "2501.02702v1",
    "title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance",
    "authors": [
      "Binita Saha",
      "Utsha Saha",
      "Muhammad Zubair Malik"
    ],
    "abstract": "This work presents a novel architecture for building Retrieval-Augmented\nGeneration (RAG) systems to improve Question Answering (QA) tasks from a target\ncorpus. Large Language Models (LLMs) have revolutionized the analyzing and\ngeneration of human-like text. These models rely on pre-trained data and lack\nreal-time updates unless integrated with live data tools. RAG enhances LLMs by\nintegrating online resources and databases to generate contextually appropriate\nresponses. However, traditional RAG still encounters challenges like\ninformation dilution and hallucinations when handling vast amounts of data. Our\napproach addresses these challenges by converting corpora into a\ndomain-specific dataset and RAG architecture is constructed to generate\nresponses from the target document. We introduce QuIM-RAG (Question-to-question\nInverted Index Matching), a novel approach for the retrieval mechanism in our\nsystem. This strategy generates potential questions from document chunks and\nmatches these with user queries to identify the most relevant text chunks for\ngenerating accurate answers. We have implemented our RAG system on top of the\nopen-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on\nHugging Face. We constructed a custom corpus of 500+ pages from a high-traffic\nwebsite accessed thousands of times daily for answering complex questions,\nalong with manually prepared ground truth QA for evaluation. We compared our\napproach with traditional RAG models using BERT-Score and RAGAS,\nstate-of-the-art metrics for evaluating LLM applications. Our evaluation\ndemonstrates that our approach outperforms traditional RAG architectures on\nboth metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02702v1",
    "published_date": "2025-01-06 01:07:59 UTC",
    "updated_date": "2025-01-06 01:07:59 UTC"
  },
  {
    "arxiv_id": "2501.02699v1",
    "title": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models",
    "authors": [
      "Andrés Villa",
      "Juan León Alcázar",
      "Motasem Alfarra",
      "Vladimir Araujo",
      "Alvaro Soto",
      "Bernard Ghanem"
    ],
    "abstract": "Large language models and vision transformers have demonstrated impressive\nzero-shot capabilities, enabling significant transferability in downstream\ntasks. The fusion of these models has resulted in multi-modal architectures\nwith enhanced instructional capabilities. Despite incorporating vast image and\nlanguage pre-training, these multi-modal architectures often generate responses\nthat deviate from the ground truth in the image data. These failure cases are\nknown as hallucinations. Current methods for mitigating hallucinations\ngenerally focus on regularizing the language component, improving the fusion\nmodule, or ensembling multiple visual encoders to improve visual\nrepresentation. In this paper, we address the hallucination issue by directly\nenhancing the capabilities of the visual component. Our approach, named EAGLE,\nis fully agnostic to the LLM or fusion module and works as a post-pretraining\napproach that improves the grounding and language alignment of the visual\nencoder. We show that a straightforward reformulation of the original\ncontrastive pre-training task results in an improved visual encoder that can be\nincorporated into the instructional multi-modal architecture without additional\ninstructional training. As a result, EAGLE achieves a significant reduction in\nhallucinations across multiple challenging benchmarks and tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 4 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.02699v1",
    "published_date": "2025-01-06 00:39:31 UTC",
    "updated_date": "2025-01-06 00:39:31 UTC"
  }
]
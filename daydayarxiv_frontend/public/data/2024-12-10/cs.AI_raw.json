[
  {
    "arxiv_id": "2412.07981v1",
    "title": "Where Common Knowledge Cannot Be Formed, Common Belief Can -- Planning with Multi-Agent Belief Using Group Justified Perspectives",
    "authors": [
      "Guang Hu",
      "Tim Miller",
      "Nir Lipovetzky"
    ],
    "abstract": "Epistemic planning is the sub-field of AI planning that focuses on changing\nknowledge and belief. It is important in both multi-agent domains where agents\nneed to have knowledge/belief regarding the environment, but also the beliefs\nof other agents, including nested beliefs. When modeling knowledge in\nmulti-agent settings, many models face an exponential growth challenge in terms\nof nested depth. A contemporary method, known as Planning with Perspectives\n(PWP), addresses these challenges through the use of perspectives and set\noperations for knowledge. The JP model defines that an agent's belief is\njustified if and only if the agent has seen evidence that this belief was true\nin the past and has not seen evidence to suggest that this has changed. The\ncurrent paper extends the JP model to handle \\emph{group belief}, including\ndistributed belief and common belief. We call this the Group Justified\nPerspective (GJP) model. Using experimental problems crafted by adapting\nwell-known benchmarks to a group setting, we show the efficiency and\nexpressiveness of our GJP model at handling planning problems that cannot be\nhandled by other epistemic planning tools.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, including appendix and reference",
    "pdf_url": "http://arxiv.org/pdf/2412.07981v1",
    "published_date": "2024-12-10 23:43:06 UTC",
    "updated_date": "2024-12-10 23:43:06 UTC"
  },
  {
    "arxiv_id": "2412.07980v1",
    "title": "TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram",
    "authors": [
      "Mingxi Lei",
      "Chunwei Ma",
      "Meng Ding",
      "Yufan Zhou",
      "Ziyun Huang",
      "Jinhui Xu"
    ],
    "abstract": "Deep learning models often struggle with generalization when deploying on\nreal-world data, due to the common distributional shift to the training data.\nTest-time adaptation (TTA) is an emerging scheme used at inference time to\naddress this issue. In TTA, models are adapted online at the same time when\nmaking predictions to test data. Neighbor-based approaches have gained\nattention recently, where prototype embeddings provide location information to\nalleviate the feature shift between training and testing data. However, due to\ntheir inherit limitation of simplicity, they often struggle to learn useful\npatterns and encounter performance degradation. To confront this challenge, we\nstudy the TTA problem from a geometric point of view. We first reveal that the\nunderlying structure of neighbor-based methods aligns with the Voronoi Diagram,\na classical computational geometry model for space partitioning. Building on\nthis observation, we propose the Test-Time adjustment by Voronoi Diagram\nguidance (TTVD), a novel framework that leverages the benefits of this\ngeometric property. Specifically, we explore two key structures: 1)\nCluster-induced Voronoi Diagram (CIVD): This integrates the joint contribution\nof self-supervision and entropy-based methods to provide richer information. 2)\nPower Diagram (PD): A generalized version of the Voronoi Diagram that refines\npartitions by assigning weights to each Voronoi cell. Our experiments under\nrigid, peer-reviewed settings on CIFAR-10-C, CIFAR-100-C, ImageNet-C, and\nImageNet-R shows that TTVD achieves remarkable improvements compared to\nstate-of-the-art methods. Moreover, extensive experimental results also explore\nthe effects of batch size and class imbalance, which are two scenarios commonly\nencountered in real-world applications. These analyses further validate the\nrobustness and adaptability of our proposed framework.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "29 pages, 7 figures. Under review",
    "pdf_url": "http://arxiv.org/pdf/2412.07980v1",
    "published_date": "2024-12-10 23:40:07 UTC",
    "updated_date": "2024-12-10 23:40:07 UTC"
  },
  {
    "arxiv_id": "2412.07979v1",
    "title": "AmCLR: Unified Augmented Learning for Cross-Modal Representations",
    "authors": [
      "Ajay Jagannath",
      "Aayush Upadhyay",
      "Anant Mehta"
    ],
    "abstract": "Contrastive learning has emerged as a pivotal framework for representation\nlearning, underpinning advances in both unimodal and bimodal applications like\nSimCLR and CLIP. To address fundamental limitations like large batch size\ndependency and bimodality, methods such as SogCLR leverage stochastic\noptimization for the global contrastive objective. Inspired by SogCLR's\nefficiency and adaptability, we introduce AmCLR and xAmCLR objective functions\ntailored for bimodal vision-language models to further enhance the robustness\nof contrastive learning. AmCLR integrates diverse augmentations, including text\nparaphrasing and image transformations, to reinforce the alignment of\ncontrastive representations, keeping batch size limited to a few hundred\nsamples unlike CLIP which needs batch size of 32,768 to produce reasonable\nresults. xAmCLR further extends this paradigm by incorporating intra-modal\nalignments between original and augmented modalities for richer feature\nlearning. These advancements yield a more resilient and generalizable\ncontrastive learning process, aimed at overcoming bottlenecks in scaling and\naugmentative diversity. Since we have built our framework on the existing\nSogCLR, we are able to demonstrate improved representation quality with fewer\ncomputational resources, establishing a foundation for scalable and robust\nmulti-modal learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07979v1",
    "published_date": "2024-12-10 23:32:36 UTC",
    "updated_date": "2024-12-10 23:32:36 UTC"
  },
  {
    "arxiv_id": "2412.07978v1",
    "title": "Agents for self-driving laboratories applied to quantum computing",
    "authors": [
      "Shuxiang Cao",
      "Zijian Zhang",
      "Mohammed Alghadeer",
      "Simone D Fasciati",
      "Michele Piscitelli",
      "Mustafa Bakr",
      "Peter Leek",
      "Alán Aspuru-Guzik"
    ],
    "abstract": "Fully automated self-driving laboratories are promising to enable\nhigh-throughput and large-scale scientific discovery by reducing repetitive\nlabour. However, effective automation requires deep integration of laboratory\nknowledge, which is often unstructured, multimodal, and difficult to\nincorporate into current AI systems. This paper introduces the k-agents\nframework, designed to support experimentalists in organizing laboratory\nknowledge and automating experiments with agents. Our framework employs large\nlanguage model-based agents to encapsulate laboratory knowledge including\navailable laboratory operations and methods for analyzing experiment results.\nTo automate experiments, we introduce execution agents that break multi-step\nexperimental procedures into state machines, interact with other agents to\nexecute each step and analyze the experiment results. The analyzed results are\nthen utilized to drive state transitions, enabling closed-loop feedback\ncontrol. To demonstrate its capabilities, we applied the agents to calibrate\nand operate a superconducting quantum processor, where they autonomously\nplanned and executed experiments for hours, successfully producing and\ncharacterizing entangled quantum states at the level achieved by human\nscientists. Our knowledge-based agent system opens up new possibilities for\nmanaging laboratory knowledge and accelerating scientific discovery.",
    "categories": [
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07978v1",
    "published_date": "2024-12-10 23:30:44 UTC",
    "updated_date": "2024-12-10 23:30:44 UTC"
  },
  {
    "arxiv_id": "2412.07977v1",
    "title": "Thinking Fast and Laterally: Multi-Agentic Approach for Reasoning about Uncertain Emerging Events",
    "authors": [
      "Stefan Dernbach",
      "Alejandro Michel",
      "Khushbu Agarwal",
      "Christopher Brissette",
      "Geetika Gupta",
      "Sutanay Choudhury"
    ],
    "abstract": "This paper introduces lateral thinking to implement System-2 reasoning\ncapabilities in AI systems, focusing on anticipatory and causal reasoning under\nuncertainty. We present a framework for systematic generation and modeling of\nlateral thinking queries and evaluation datasets. We introduce Streaming\nAgentic Lateral Thinking (SALT), a multi-agent framework designed to process\ncomplex, low-specificity queries in streaming data environments. SALT\nimplements lateral thinking-inspired System-2 reasoning through a dynamic\ncommunication structure between specialized agents. Our key insight is that\nlateral information flow across long-distance agent interactions, combined with\nfine-grained belief management, yields richer information contexts and enhanced\nreasoning. Preliminary quantitative and qualitative evaluations indicate SALT's\npotential to outperform single-agent systems in handling complex lateral\nreasoning tasks in a streaming environment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented in The 1st Workshop on System-2 Reasoning at Scale (NeurIPS\n  2024), Vancouver, Canada",
    "pdf_url": "http://arxiv.org/pdf/2412.07977v1",
    "published_date": "2024-12-10 23:29:11 UTC",
    "updated_date": "2024-12-10 23:29:11 UTC"
  },
  {
    "arxiv_id": "2412.10428v1",
    "title": "Observing Micromotives and Macrobehavior of Large Language Models",
    "authors": [
      "Yuyang Cheng",
      "Xingwei Qu",
      "Tomas Goldsack",
      "Chenghua Lin",
      "Chung-Chi Chen"
    ],
    "abstract": "Thomas C. Schelling, awarded the 2005 Nobel Memorial Prize in Economic\nSciences, pointed out that ``individuals decisions (micromotives), while often\npersonal and localized, can lead to societal outcomes (macrobehavior) that are\nfar more complex and different from what the individuals intended.'' The\ncurrent research related to large language models' (LLMs') micromotives, such\nas preferences or biases, assumes that users will make more appropriate\ndecisions once LLMs are devoid of preferences or biases. Consequently, a series\nof studies has focused on removing bias from LLMs. In the NLP community, while\nthere are many discussions on LLMs' micromotives, previous studies have seldom\nconducted a systematic examination of how LLMs may influence society's\nmacrobehavior. In this paper, we follow the design of Schelling's model of\nsegregation to observe the relationship between the micromotives and\nmacrobehavior of LLMs. Our results indicate that, regardless of the level of\nbias in LLMs, a highly segregated society will emerge as more people follow\nLLMs' suggestions. We hope our discussion will spark further consideration of\nthe fundamental assumption regarding the mitigation of LLMs' micromotives and\nencourage a reevaluation of how LLMs may influence users and society.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10428v1",
    "published_date": "2024-12-10 23:25:14 UTC",
    "updated_date": "2024-12-10 23:25:14 UTC"
  },
  {
    "arxiv_id": "2412.07975v1",
    "title": "Machines of Meaning",
    "authors": [
      "Davide Nunes",
      "Luis Antunes"
    ],
    "abstract": "One goal of Artificial Intelligence is to learn meaningful representations\nfor natural language expressions, but what this entails is not always clear. A\nvariety of new linguistic behaviours present themselves embodied as computers,\nenhanced humans, and collectives with various kinds of integration and\ncommunication. But to measure and understand the behaviours generated by such\nsystems, we must clarify the language we use to talk about them. Computational\nmodels are often confused with the phenomena they try to model and shallow\nmetaphors are used as justifications for (or to hype) the success of\ncomputational techniques on many tasks related to natural language; thus\nimplying their progress toward human-level machine intelligence without ever\nclarifying what that means.\n  This paper discusses the challenges in the specification of \"machines of\nmeaning\", machines capable of acquiring meaningful semantics from natural\nlanguage in order to achieve their goals. We characterize \"meaning\" in a\ncomputational setting, while highlighting the need for detachment from\nanthropocentrism in the study of the behaviour of machines of meaning. The\npressing need to analyse AI risks and ethics requires a proper measurement of\nits capabilities which cannot be productively studied and explained while using\nambiguous language. We propose a view of \"meaning\" to facilitate the discourse\naround approaches such as neural language models and help broaden the research\nperspectives for technology that facilitates dialogues between humans and\nmachines.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "I.2.0; I.2.7; A.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07975v1",
    "published_date": "2024-12-10 23:23:28 UTC",
    "updated_date": "2024-12-10 23:23:28 UTC"
  },
  {
    "arxiv_id": "2412.10427v2",
    "title": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering",
    "authors": [
      "Rumi A. Allbert",
      "James K. Wiles",
      "Vlad Grankovsky"
    ],
    "abstract": "The field of large language models (LLMs) has grown rapidly in recent years,\ndriven by the desire for better efficiency, interpretability, and safe use.\nBuilding on the novel approach of \"activation engineering,\" this study explores\npersonality modification in LLMs, drawing inspiration from research like\nRefusal in LLMs Is Mediated by a Single Direction (arXiv:2406.11717) and\nSteering Llama 2 via Contrastive Activation Addition (arXiv:2312.06681). We\nleverage activation engineering to develop a method for identifying and\nadjusting activation directions related to personality traits, which may allow\nfor dynamic LLM personality fine-tuning. This work aims to further our\nunderstanding of LLM interpretability while examining the ethical implications\nof such developments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10427v2",
    "published_date": "2024-12-10 23:15:25 UTC",
    "updated_date": "2025-01-10 22:26:26 UTC"
  },
  {
    "arxiv_id": "2412.07961v1",
    "title": "Forking Paths in Neural Text Generation",
    "authors": [
      "Eric Bigelow",
      "Ari Holtzman",
      "Hidenori Tanaka",
      "Tomer Ullman"
    ],
    "abstract": "Estimating uncertainty in Large Language Models (LLMs) is important for\nproperly evaluating LLMs, and ensuring safety for users. However, prior\napproaches to uncertainty estimation focus on the final answer in generated\ntext, ignoring intermediate steps that might dramatically impact the outcome.\nWe hypothesize that there exist key forking tokens, such that re-sampling the\nsystem at those specific tokens, but not others, leads to very different\noutcomes. To test this empirically, we develop a novel approach to representing\nuncertainty dynamics across individual tokens of text generation, and applying\nstatistical models to test our hypothesis. Our approach is highly flexible: it\ncan be applied to any dataset and any LLM, without fine tuning or accessing\nmodel weights. We use our method to analyze LLM responses on 7 different tasks\nacross 4 domains, spanning a wide range of typical use cases. We find many\nexamples of forking tokens, including surprising ones such as punctuation\nmarks, suggesting that LLMs are often just a single token away from saying\nsomething very different.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07961v1",
    "published_date": "2024-12-10 22:57:57 UTC",
    "updated_date": "2024-12-10 22:57:57 UTC"
  },
  {
    "arxiv_id": "2412.07958v2",
    "title": "PAFFA: Premeditated Actions For Fast Agents",
    "authors": [
      "Shambhavi Krishna",
      "Zheng Chen",
      "Yuan Ling",
      "Xiaojiang Huang",
      "Yingjie Li",
      "Fan Yang",
      "Xiang Li"
    ],
    "abstract": "Modern AI assistants have made significant progress in natural language\nunderstanding and tool-use, with emerging efforts to interact with Web\ninterfaces. However, current approaches that heavily rely on repeated\nLLM-driven HTML parsing are computationally expensive and error-prone,\nparticularly when handling dynamic web interfaces and multi-step tasks. We\nintroduce PAFFA (Premeditated Actions For Fast Agents), a method that makes\nLLMs faster and more accurate in completing tasks on the internet using a novel\ninference-time technique that requires no task-specific training. PAFFA\nconstructs an 'Action Library', leveraging the parametric knowledge of the base\nLLM to pre-compute browser interaction patterns that generalize across tasks.\nBy strategically re-using LLM inference across tasks - either via 'Dist-Map'\nfor task-agnostic identification of key interactive web elements, or 'Unravel'\nfor first-encounter, stateful exploration of novel tasks/sites) - PAFFA\ndrastically reduces inference time tokens by 87% while maintaining robust\nperformance (achieving 0.57 vs. 0.50 step accuracy compared to baseline).\nFurther, Unravel's ability to update its action library based on explorations\nallows generalization and adaptation to unseen websites. In sum, this work\nexhibits that LLM reasoning sequences can generalize across prompts, offering a\nway to scale inference-time techniques for internet-scale data with sublinear\ntoken count.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.07958v2",
    "published_date": "2024-12-10 22:51:31 UTC",
    "updated_date": "2025-04-04 17:33:53 UTC"
  },
  {
    "arxiv_id": "2412.07956v1",
    "title": "Reciprocal Learning of Intent Inferral with Augmented Visual Feedback for Stroke",
    "authors": [
      "Jingxi Xu",
      "Ava Chen",
      "Lauren Winterbottom",
      "Joaquin Palacios",
      "Preethika Chivukula",
      "Dawn M. Nilsen",
      "Joel Stein",
      "Matei Ciocarlie"
    ],
    "abstract": "Intent inferral, the process by which a robotic device predicts a user's\nintent from biosignals, offers an effective and intuitive way to control\nwearable robots. Classical intent inferral methods treat biosignal inputs as\nunidirectional ground truths for training machine learning models, where the\ninternal state of the model is not directly observable by the user. In this\nwork, we propose reciprocal learning, a bidirectional paradigm that facilitates\nhuman adaptation to an intent inferral classifier. Our paradigm consists of\niterative, interwoven stages that alternate between updating machine learning\nmodels and guiding human adaptation with the use of augmented visual feedback.\nWe demonstrate this paradigm in the context of controlling a robotic hand\northosis for stroke, where the device predicts open, close, and relax intents\nfrom electromyographic (EMG) signals and provides appropriate assistance. We\nuse LED progress-bar displays to communicate to the user the predicted\nprobabilities for open and close intents by the classifier. Our experiments\nwith stroke subjects show reciprocal learning improving performance in a subset\nof subjects (two out of five) without negatively impacting performance on the\nothers. We hypothesize that, during reciprocal learning, subjects can learn to\nreproduce more distinguishable muscle activation patterns and generate more\nseparable biosignals.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07956v1",
    "published_date": "2024-12-10 22:49:36 UTC",
    "updated_date": "2024-12-10 22:49:36 UTC"
  },
  {
    "arxiv_id": "2412.07951v2",
    "title": "From Lived Experience to Insight: Unpacking the Psychological Risks of Using AI Conversational Agents",
    "authors": [
      "Mohit Chandra",
      "Suchismita Naik",
      "Denae Ford",
      "Ebele Okoli",
      "Munmun De Choudhury",
      "Mahsa Ershadi",
      "Gonzalo Ramos",
      "Javier Hernandez",
      "Ananya Bhattacharjee",
      "Shahed Warreth",
      "Jina Suh"
    ],
    "abstract": "Recent gain in popularity of AI conversational agents has led to their\nincreased use for improving productivity and supporting well-being. While\nprevious research has aimed to understand the risks associated with\ninteractions with AI conversational agents, these studies often fall short in\ncapturing the lived experiences. Additionally, psychological risks have often\nbeen presented as a sub-category within broader AI-related risks in past\ntaxonomy works, leading to under-representation of the impact of psychological\nrisks of AI use. To address these challenges, our work presents a novel risk\ntaxonomy focusing on psychological risks of using AI gathered through lived\nexperience of individuals. We employed a mixed-method approach, involving a\ncomprehensive survey with 283 individuals with lived mental health experience\nand workshops involving lived experience experts to develop a psychological\nrisk taxonomy. Our taxonomy features 19 AI behaviors, 21 negative psychological\nimpacts, and 15 contexts related to individuals. Additionally, we propose a\nnovel multi-path vignette based framework for understanding the complex\ninterplay between AI behaviors, psychological impacts, and individual user\ncontexts. Finally, based on the feedback obtained from the workshop sessions,\nwe present design recommendations for developing safer and more robust AI\nagents. Our work offers an in-depth understanding of the psychological risks\nassociated with AI conversational agents and provides actionable\nrecommendations for policymakers, researchers, and developers.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "25 pages, 2 figures, 4 tables; Corrected typos",
    "pdf_url": "http://arxiv.org/pdf/2412.07951v2",
    "published_date": "2024-12-10 22:31:29 UTC",
    "updated_date": "2024-12-12 13:19:29 UTC"
  },
  {
    "arxiv_id": "2412.07948v2",
    "title": "Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation",
    "authors": [
      "Jan Retkowski",
      "Jakub Stępniak",
      "Mateusz Modrzejewski"
    ],
    "abstract": "In this paper we introduce the Frechet Music Distance (FMD), a novel\nevaluation metric for generative symbolic music models, inspired by the Frechet\nInception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in\ngenerative audio. FMD calculates the distance between distributions of\nreference and generated symbolic music embeddings, capturing abstract musical\nfeatures. We validate FMD across several datasets and models. Results indicate\nthat FMD effectively differentiates model quality, providing a domain-specific\nmetric for evaluating symbolic music generation, and establishing a\nreproducible standard for future research in symbolic music modeling.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07948v2",
    "published_date": "2024-12-10 22:22:19 UTC",
    "updated_date": "2025-01-16 17:56:53 UTC"
  },
  {
    "arxiv_id": "2412.07947v1",
    "title": "GPT-2 Through the Lens of Vector Symbolic Architectures",
    "authors": [
      "Johannes Knittel",
      "Tushaar Gangavarapu",
      "Hendrik Strobelt",
      "Hanspeter Pfister"
    ],
    "abstract": "Understanding the general priniciples behind transformer models remains a\ncomplex endeavor. Experiments with probing and disentangling features using\nsparse autoencoders (SAE) suggest that these models might manage linear\nfeatures embedded as directions in the residual stream. This paper explores the\nresemblance between decoder-only transformer architecture and vector symbolic\narchitectures (VSA) and presents experiments indicating that GPT-2 uses\nmechanisms involving nearly orthogonal vector bundling and binding operations\nsimilar to VSA for computation and communication between layers. It further\nshows that these principles help explain a significant portion of the actual\nneural weights.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2nd Workshop on Attributing Model Behavior at Scale (ATTRIB) at\n  NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.07947v1",
    "published_date": "2024-12-10 22:20:36 UTC",
    "updated_date": "2024-12-10 22:20:36 UTC"
  },
  {
    "arxiv_id": "2412.07941v1",
    "title": "Beyond Static Assumptions: the Predictive Justified Perspective Model for Epistemic Planning",
    "authors": [
      "Weijia Li",
      "Guang Hu",
      "Yangmengfei Xu"
    ],
    "abstract": "Epistemic Planning (EP) is an important research area dedicated to reasoning\nabout the knowledge and beliefs of agents in multi-agent cooperative or\nadversarial settings. The Justified Perspective (JP) model is the\nstate-of-the-art approach to solving EP problems with efficiency and\nexpressiveness. However, all existing EP methods inherit the static environment\nassumption from classical planning. This limitation hinders the application of\nEP in fields such as robotics with multi-agent settings, where the environment\ncontains changing variables.\n  In this paper, we propose an extension of the JP model, namely, the\nPredictive Justified Perspective (PJP) model, to remove this assumption.\nInstead of assuming that beliefs remain unchanged since the last observation,\nthe PJP model uses all past observations to form predictions about the changing\nvariables. The definition of the prediction function with examples is provided,\nand it is demonstrated that it can work with arbitrary nesting. We then\nimplemented the PJP model in several well-known domains and compared it with\nthe JP model in the experiments. The results indicated that the PJP model\nperforms exceptionally well across various domains, demonstrating its potential\nin improving EP applications in robotics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.07941v1",
    "published_date": "2024-12-10 22:00:08 UTC",
    "updated_date": "2024-12-10 22:00:08 UTC"
  },
  {
    "arxiv_id": "2412.14191v1",
    "title": "Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education",
    "authors": [
      "Chengshuai Zhao",
      "Garima Agrawal",
      "Tharindu Kumarage",
      "Zhen Tan",
      "Yuli Deng",
      "Ying-Chih Chen",
      "Huan Liu"
    ],
    "abstract": "Integrating AI into education has the potential to transform the teaching of\nscience and technology courses, particularly in the field of cybersecurity.\nAI-driven question-answering (QA) systems can actively manage uncertainty in\ncybersecurity problem-solving, offering interactive, inquiry-based learning\nexperiences. Large language models (LLMs) have gained prominence in AI-driven\nQA systems, offering advanced language understanding and user engagement.\nHowever, they face challenges like hallucinations and limited domain-specific\nknowledge, which reduce their reliability in educational settings. To address\nthese challenges, we propose CyberRAG, an ontology-aware retrieval-augmented\ngeneration (RAG) approach for developing a reliable and safe QA system in\ncybersecurity education. CyberRAG employs a two-step approach: first, it\naugments the domain-specific knowledge by retrieving validated cybersecurity\ndocuments from a knowledge base to enhance the relevance and accuracy of the\nresponse. Second, it mitigates hallucinations and misuse by integrating a\nknowledge graph ontology to validate the final answer. Experiments on publicly\navailable cybersecurity datasets show that CyberRAG delivers accurate, reliable\nresponses aligned with domain knowledge, demonstrating the potential of AI\ntools to enhance education.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14191v1",
    "published_date": "2024-12-10 21:52:35 UTC",
    "updated_date": "2024-12-10 21:52:35 UTC"
  },
  {
    "arxiv_id": "2412.07935v1",
    "title": "Non-Normal Diffusion Models",
    "authors": [
      "Henry Li"
    ],
    "abstract": "Diffusion models generate samples by incrementally reversing a process that\nturns data into noise. We show that when the step size goes to zero, the\nreversed process is invariant to the distribution of these increments. This\nreveals a previously unconsidered parameter in the design of diffusion models:\nthe distribution of the diffusion step $\\Delta x_k := x_{k} - x_{k + 1}$. This\nparameter is implicitly set by default to be normally distributed in most\ndiffusion models. By lifting this assumption, we generalize the framework for\ndesigning diffusion models and establish an expanded class of diffusion\nprocesses with greater flexibility in the choice of loss function used during\ntraining. We demonstrate the effectiveness of these models on density\nestimation and generative modeling tasks on standard image datasets, and show\nthat different choices of the distribution of $\\Delta x_k$ result in\nqualitatively different generated samples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07935v1",
    "published_date": "2024-12-10 21:31:12 UTC",
    "updated_date": "2024-12-10 21:31:12 UTC"
  },
  {
    "arxiv_id": "2412.07922v1",
    "title": "Robust Multiple Description Neural Video Codec with Masked Transformer for Dynamic and Noisy Networks",
    "authors": [
      "Xinyue Hu",
      "Wei Ye",
      "Jiaxiang Tang",
      "Eman Ramadan",
      "Zhi-Li Zhang"
    ],
    "abstract": "Multiple Description Coding (MDC) is a promising error-resilient source\ncoding method that is particularly suitable for dynamic networks with multiple\n(yet noisy and unreliable) paths. However, conventional MDC video codecs suffer\nfrom cumbersome architectures, poor scalability, limited loss resilience, and\nlower compression efficiency. As a result, MDC has never been widely adopted.\nInspired by the potential of neural video codecs, this paper rethinks MDC\ndesign. We propose a novel MDC video codec, NeuralMDC, demonstrating how\nbidirectional transformers trained for masked token prediction can vastly\nsimplify the design of MDC video codec. To compress a video, NeuralMDC starts\nby tokenizing each frame into its latent representation and then splits the\nlatent tokens to create multiple descriptions containing correlated\ninformation. Instead of using motion prediction and warping operations,\nNeuralMDC trains a bidirectional masked transformer to model the\nspatial-temporal dependencies of latent representations and predict the\ndistribution of the current representation based on the past. The predicted\ndistribution is used to independently entropy code each description and infer\nany potentially lost tokens. Extensive experiments demonstrate NeuralMDC\nachieves state-of-the-art loss resilience with minimal sacrifices in\ncompression efficiency, significantly outperforming the best existing\nresidual-coding-based error-resilient neural video codec.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.07922v1",
    "published_date": "2024-12-10 21:08:05 UTC",
    "updated_date": "2024-12-10 21:08:05 UTC"
  },
  {
    "arxiv_id": "2412.07909v1",
    "title": "Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning",
    "authors": [
      "Can Yaras",
      "Siyi Chen",
      "Peng Wang",
      "Qing Qu"
    ],
    "abstract": "Multimodal learning has recently gained significant popularity, demonstrating\nimpressive performance across various zero-shot classification tasks and a\nrange of perceptive and generative applications. Models such as Contrastive\nLanguage-Image Pretraining (CLIP) are designed to bridge different modalities,\nsuch as images and text, by learning a shared representation space through\ncontrastive learning. Despite their success, the working mechanisms underlying\nmultimodal learning are not yet well understood. Notably, these models often\nexhibit a modality gap, where different modalities occupy distinct regions\nwithin the shared representation space. In this work, we conduct an in-depth\nanalysis of the emergence of modality gap by characterizing the gradient flow\nlearning dynamics. Specifically, we identify the critical roles of mismatched\ndata pairs and a learnable temperature parameter in causing and perpetuating\nthe modality gap during training. Furthermore, our theoretical insights are\nvalidated through experiments on practical CLIP models. These findings provide\nprincipled guidance for mitigating the modality gap, including strategies such\nas appropriate temperature scheduling and modality swapping. Additionally, we\ndemonstrate that closing the modality gap leads to improved performance on\ntasks such as image-text retrieval.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2412.07909v1",
    "published_date": "2024-12-10 20:36:49 UTC",
    "updated_date": "2024-12-10 20:36:49 UTC"
  },
  {
    "arxiv_id": "2412.07904v3",
    "title": "Score Change of Variables",
    "authors": [
      "Stephen Robbins"
    ],
    "abstract": "We derive a general change of variables formula for score functions, showing\nthat for a smooth, invertible transformation $\\mathbf{y} = \\phi(\\mathbf{x})$,\nthe transformed score function $\\nabla_{\\mathbf{y}} \\log q(\\mathbf{y})$ can be\nexpressed directly in terms of $\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$. Using\nthis result, we develop two applications: First, we establish a reverse-time\nIt\\^o lemma for score-based diffusion models, allowing the use of\n$\\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x})$ to reverse an SDE in the transformed\nspace without directly learning $\\nabla_{\\mathbf{y}} \\log q_t(\\mathbf{y})$.\nThis approach enables training diffusion models in one space but sampling in\nanother, effectively decoupling the forward and reverse processes. Second, we\nintroduce generalized sliced score matching, extending traditional sliced score\nmatching from linear projections to arbitrary smooth transformations. This\nprovides greater flexibility in high-dimensional density estimation. We\ndemonstrate these theoretical advances through applications to diffusion on the\nprobability simplex and empirically compare our generalized score matching\napproach against traditional sliced score matching methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "68T01",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07904v3",
    "published_date": "2024-12-10 20:27:15 UTC",
    "updated_date": "2025-02-24 17:56:03 UTC"
  },
  {
    "arxiv_id": "2412.14190v2",
    "title": "Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships",
    "authors": [
      "Julian De Freitas",
      "Noah Castelo",
      "Ahmet Uguralp",
      "Zeliha Uguralp"
    ],
    "abstract": "Can consumers form especially deep emotional bonds with AI and be vested in\nAI identities over time? We leverage a natural app-update event at Replika AI,\na popular US-based AI companion, to shed light on these questions. We find\nthat, after the app removed its erotic role play (ERP) feature, preventing\nintimate interactions between consumers and chatbots that were previously\npossible, this event triggered perceptions in customers that their AI\ncompanion's identity had discontinued. This in turn predicted negative consumer\nwelfare and marketing outcomes related to loss, including mourning the loss,\nand devaluing the \"new\" AI relative to the \"original\". Experimental evidence\nconfirms these findings. Further experiments find that AI companions users feel\ncloser to their AI companion than even their best human friend, and mourn a\nloss of their AI companion more than a loss of various other inanimate\nproducts. In short, consumers are forming human-level relationships with AI\ncompanions; disruptions to these relationships trigger real patterns of\nmourning as well as devaluation of the offering; and the degree of mourning and\ndevaluation are explained by perceived discontinuity in the AIs identity. Our\nresults illustrate that relationships with AI are truly personal, creating\nunique benefits and risks for consumers and firms alike.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14190v2",
    "published_date": "2024-12-10 20:14:10 UTC",
    "updated_date": "2025-05-13 16:17:50 UTC"
  },
  {
    "arxiv_id": "2412.07895v1",
    "title": "How Should We Represent History in Interpretable Models of Clinical Policies?",
    "authors": [
      "Anton Matsson",
      "Lena Stempfle",
      "Yaochen Rao",
      "Zachary R. Margolin",
      "Heather J. Litman",
      "Fredrik D. Johansson"
    ],
    "abstract": "Modeling policies for sequential clinical decision-making based on\nobservational data is useful for describing treatment practices, standardizing\nfrequent patterns in treatment, and evaluating alternative policies. For each\ntask, it is essential that the policy model is interpretable. Learning accurate\nmodels requires effectively capturing the state of a patient, either through\nsequence representation learning or carefully crafted summaries of their\nmedical history. While recent work has favored the former, it remains a\nquestion as to how histories should best be represented for interpretable\npolicy modeling. Focused on model fit, we systematically compare diverse\napproaches to summarizing patient history for interpretable modeling of\nclinical policies across four sequential decision-making tasks. We illustrate\ndifferences in the policies learned using various representations by breaking\ndown evaluations by patient subgroups, critical states, and stages of\ntreatment, highlighting challenges specific to common use cases. We find that\ninterpretable sequence models using learned representations perform on par with\nblack-box models across all tasks. Interpretable models using hand-crafted\nrepresentations perform substantially worse when ignoring history entirely, but\nare made competitive by incorporating only a few aggregated and recent elements\nof patient history. The added benefits of using a richer representation are\npronounced for subgroups and in specific use cases. This underscores the\nimportance of evaluating policy models in the context of their intended use.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07895v1",
    "published_date": "2024-12-10 20:03:17 UTC",
    "updated_date": "2024-12-10 20:03:17 UTC"
  },
  {
    "arxiv_id": "2412.07888v1",
    "title": "Graph convolutional networks enable fast hemorrhagic stroke monitoring with electrical impedance tomography",
    "authors": [
      "J. Toivanen",
      "V. Kolehmainen",
      "A. Paldanius",
      "A. Hänninen",
      "A. Hauptmann",
      "S. J. Hamilton"
    ],
    "abstract": "Objective: To develop a fast image reconstruction method for stroke\nmonitoring with electrical impedance tomography with image quality comparable\nto computationally expensive nonlinear model-based methods. Methods: A\npost-processing approach with graph convolutional networks is employed.\nUtilizing the flexibility of the graph setting, a graph U-net is trained on\nlinear difference reconstructions from 2D simulated stroke data and applied to\nfully 3D images from realistic simulated and experimental data. An additional\nnetwork, trained on 3D vs. 2D images, is also considered for comparison.\nResults: Post-processing the linear difference reconstructions through the\ngraph U-net significantly improved the image quality, resulting in images\ncomparable to, or better than, the time-intensive nonlinear reconstruction\nmethod (a few minutes vs. several hours). Conclusion: Pairing a fast\nreconstruction method, such as linear difference imaging, with post-processing\nthrough a graph U-net provided significant improvements, at a negligible\ncomputational cost. Training in the graph framework vs classic pixel-based\nsetting (CNN) allowed the ability to train on 2D cross-sectional images and\nprocess 3D volumes providing a nearly 50x savings in data simulation costs with\nno noticeable loss in quality. Significance: The proposed approach of\npost-processing a linear difference reconstruction with the graph U-net could\nbe a feasible approach for on-line monitoring of hemorrhagic stroke.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "math.AP"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07888v1",
    "published_date": "2024-12-10 19:47:49 UTC",
    "updated_date": "2024-12-10 19:47:49 UTC"
  },
  {
    "arxiv_id": "2412.07883v2",
    "title": "On Faster Marginalization with Squared Circuits via Orthonormalization",
    "authors": [
      "Lorenzo Loconte",
      "Antonio Vergari"
    ],
    "abstract": "Squared tensor networks (TNs) and their generalization as parameterized\ncomputational graphs -- squared circuits -- have been recently used as\nexpressive distribution estimators in high dimensions. However, the squaring\noperation introduces additional complexity when marginalizing variables or\ncomputing the partition function, which hinders their usage in machine learning\napplications. Canonical forms of popular TNs are parameterized via unitary\nmatrices as to simplify the computation of particular marginals, but cannot be\nmapped to general circuits since these might not correspond to a known TN.\nInspired by TN canonical forms, we show how to parameterize squared circuits to\nensure they encode already normalized distributions. We then use this\nparameterization to devise an algorithm to compute any marginal of squared\ncircuits that is more efficient than a previously known one. We conclude by\nformally showing the proposed parameterization comes with no expressiveness\nloss for many circuit classes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07883v2",
    "published_date": "2024-12-10 19:37:03 UTC",
    "updated_date": "2025-01-19 10:48:35 UTC"
  },
  {
    "arxiv_id": "2412.07880v2",
    "title": "Towards Foundation-model-based Multiagent System to Accelerate AI for Social Impact",
    "authors": [
      "Yunfan Zhao",
      "Niclas Boehmer",
      "Aparna Taneja",
      "Milind Tambe"
    ],
    "abstract": "AI for social impact (AI4SI) offers significant potential for addressing\ncomplex societal challenges in areas such as public health, agriculture,\neducation, conservation, and public safety. However, existing AI4SI research is\noften labor-intensive and resource-demanding, limiting its accessibility and\nscalability; the standard approach is to design a (base-level) system tailored\nto a specific AI4SI problem. We propose the development of a novel meta-level\nmulti-agent system designed to accelerate the development of such base-level\nsystems, thereby reducing the computational cost and the burden on social\nimpact domain experts and AI researchers. Leveraging advancements in foundation\nmodels and large language models, our proposed approach focuses on resource\nallocation problems providing help across the full AI4SI pipeline from problem\nformulation over solution design to impact evaluation. We highlight the ethical\nconsiderations and challenges inherent in deploying such systems and emphasize\nthe importance of a human-in-the-loop approach to ensure the responsible and\neffective application of AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07880v2",
    "published_date": "2024-12-10 19:29:34 UTC",
    "updated_date": "2024-12-12 15:08:30 UTC"
  },
  {
    "arxiv_id": "2412.07878v1",
    "title": "Comparative Analysis of Deep Learning Approaches for Harmful Brain Activity Detection Using EEG",
    "authors": [
      "Shivraj Singh Bhatti",
      "Aryan Yadav",
      "Mitali Monga",
      "Neeraj Kumar"
    ],
    "abstract": "The classification of harmful brain activities, such as seizures and periodic\ndischarges, play a vital role in neurocritical care, enabling timely diagnosis\nand intervention. Electroencephalography (EEG) provides a non-invasive method\nfor monitoring brain activity, but the manual interpretation of EEG signals are\ntime-consuming and rely heavily on expert judgment. This study presents a\ncomparative analysis of deep learning architectures, including Convolutional\nNeural Networks (CNNs), Vision Transformers (ViTs), and EEGNet, applied to the\nclassification of harmful brain activities using both raw EEG data and\ntime-frequency representations generated through Continuous Wavelet Transform\n(CWT). We evaluate the performance of these models use multimodal data\nrepresentations, including high-resolution spectrograms and waveform data, and\nintroduce a multi-stage training strategy to improve model robustness. Our\nresults show that training strategies, data preprocessing, and augmentation\ntechniques are as critical to model success as architecture choice, with\nmulti-stage TinyViT and EfficientNet demonstrating superior performance. The\nfindings underscore the importance of robust training regimes in achieving\naccurate and efficient EEG classification, providing valuable insights for\ndeploying AI models in clinical practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 5 figures. Presented at IEEE CICT 2024. The paper discusses\n  the application of multimodal data and training strategies in EEG-based brain\n  activity classification",
    "pdf_url": "http://arxiv.org/pdf/2412.07878v1",
    "published_date": "2024-12-10 19:27:19 UTC",
    "updated_date": "2024-12-10 19:27:19 UTC"
  },
  {
    "arxiv_id": "2412.07872v1",
    "title": "Evaluating the Potential of Federated Learning for Maize Leaf Disease Prediction",
    "authors": [
      "Thalita Mendonça Antico",
      "Larissa F. Rodrigues Moreira",
      "Rodrigo Moreira"
    ],
    "abstract": "The diagnosis of diseases in food crops based on machine learning seemed\nsatisfactory and suitable for use on a large scale. The Convolutional Neural\nNetworks (CNNs) perform accurately in the disease prediction considering the\nimage capture of the crop leaf, being extensively enhanced in the literature.\nThese machine learning techniques fall short in data privacy, as they require\nsharing the data in the training process with a central server, disregarding\ncompetitive or regulatory concerns. Thus, Federated Learning (FL) aims to\nsupport distributed training to address recognized gaps in centralized\ntraining. As far as we know, this paper inaugurates the use and evaluation of\nFL applied in maize leaf diseases. We evaluated the performance of five CNNs\ntrained under the distributed paradigm and measured their training time\ncompared to the classification performance. In addition, we consider the\nsuitability of distributed training considering the volume of network traffic\nand the number of parameters of each CNN. Our results indicate that FL\npotentially enhances data privacy in heterogeneous domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07872v1",
    "published_date": "2024-12-10 19:19:43 UTC",
    "updated_date": "2024-12-10 19:19:43 UTC"
  },
  {
    "arxiv_id": "2412.12147v1",
    "title": "Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in Continuous Control",
    "authors": [
      "Seongwoong Cho",
      "Donggyun Kim",
      "Jinwoo Lee",
      "Seunghoon Hong"
    ],
    "abstract": "Generalizing across robot embodiments and tasks is crucial for adaptive\nrobotic systems. Modular policy learning approaches adapt to new embodiments\nbut are limited to specific tasks, while few-shot imitation learning (IL)\napproaches often focus on a single embodiment. In this paper, we introduce a\nfew-shot behavior cloning framework to simultaneously generalize to unseen\nembodiments and tasks using a few (\\emph{e.g.,} five) reward-free\ndemonstrations. Our framework leverages a joint-level input-output\nrepresentation to unify the state and action spaces of heterogeneous\nembodiments and employs a novel structure-motion state encoder that is\nparameterized to capture both shared knowledge across all embodiments and\nembodiment-specific knowledge. A matching-based policy network then predicts\nactions from a few demonstrations, producing an adaptive policy that is robust\nto over-fitting. Evaluated in the DeepMind Control suite, our framework termed\n\\modelname{} demonstrates superior few-shot generalization to unseen\nembodiments and tasks over modular policy learning and few-shot IL approaches.\nCodes are available at\n\\href{https://github.com/SeongwoongCho/meta-controller}{https://github.com/SeongwoongCho/meta-controller}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.12147v1",
    "published_date": "2024-12-10 19:10:17 UTC",
    "updated_date": "2024-12-10 19:10:17 UTC"
  },
  {
    "arxiv_id": "2412.07836v2",
    "title": "Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state",
    "authors": [
      "Semih Kacmaz",
      "Roland Haas",
      "E. A. Huerta"
    ],
    "abstract": "We present a novel machine learning (ML) method to accelerate\nconservative-to-primitive inversion, focusing on hybrid piecewise polytropic\nand tabulated equations of state. Traditional root-finding techniques are\ncomputationally expensive, particularly for large-scale relativistic\nhydrodynamics simulations. To address this, we employ feedforward neural\nnetworks (NNC2PS and NNC2PL), trained in PyTorch and optimized for GPU\ninference using NVIDIA TensorRT, achieving significant speedups with minimal\naccuracy loss. The NNC2PS model achieves $ L_1 $ and $ L_\\infty $ errors of $\n4.54 \\times 10^{-7} $ and $ 3.44 \\times 10^{-6} $, respectively, while the\nNNC2PL model exhibits even lower error values. TensorRT optimization with\nmixed-precision deployment substantially accelerates performance compared to\ntraditional root-finding methods. Specifically, the mixed-precision TensorRT\nengine for NNC2PS achieves inference speeds approximately 400 times faster than\na traditional single-threaded CPU implementation for a dataset size of\n1,000,000 points. Ideal parallelization across an entire compute node in the\nDelta supercomputer (Dual AMD 64 core 2.45 GHz Milan processors; and 8 NVIDIA\nA100 GPUs with 40 GB HBM2 RAM and NVLink) predicts a 25-fold speedup for\nTensorRT over an optimally-parallelized numerical method when processing 8\nmillion data points. Moreover, the ML method exhibits sub-linear scaling with\nincreasing dataset sizes. We release the scientific software developed,\nenabling further validation and extension of our findings. This work\nunderscores the potential of ML, combined with GPU optimization and model\nquantization, to accelerate conservative-to-primitive inversion in relativistic\nhydrodynamics simulations.",
    "categories": [
      "gr-qc",
      "astro-ph.IM",
      "cs.AI",
      "physics.comp-ph",
      "J.2; I.2"
    ],
    "primary_category": "gr-qc",
    "comment": "17 pages, 4 figures, 1 table New results added",
    "pdf_url": "http://arxiv.org/pdf/2412.07836v2",
    "published_date": "2024-12-10 19:00:01 UTC",
    "updated_date": "2025-01-29 19:00:04 UTC"
  },
  {
    "arxiv_id": "2412.07776v2",
    "title": "Video Motion Transfer with Diffusion Transformers",
    "authors": [
      "Alexander Pondaven",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Philip Torr",
      "Fabio Pizzati"
    ],
    "abstract": "We propose DiTFlow, a method for transferring the motion of a reference video\nto a newly synthesized one, designed specifically for Diffusion Transformers\n(DiT). We first process the reference video with a pre-trained DiT to analyze\ncross-frame attention maps and extract a patch-wise motion signal called the\nAttention Motion Flow (AMF). We guide the latent denoising process in an\noptimization-based, training-free, manner by optimizing latents with our AMF\nloss to generate videos reproducing the motion of the reference one. We also\napply our optimization strategy to transformer positional embeddings, granting\nus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow\nagainst recently published methods, outperforming all across multiple metrics\nand human evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 - Project page: https://ditflow.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2412.07776v2",
    "published_date": "2024-12-10 18:59:58 UTC",
    "updated_date": "2025-03-27 11:57:50 UTC"
  },
  {
    "arxiv_id": "2412.07773v2",
    "title": "Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control",
    "authors": [
      "Chenhao Lu",
      "Xuxin Cheng",
      "Jialong Li",
      "Shiqi Yang",
      "Mazeyu Ji",
      "Chengjing Yuan",
      "Ge Yang",
      "Sha Yi",
      "Xiaolong Wang"
    ],
    "abstract": "Humanoid robots require both robust lower-body locomotion and precise\nupper-body manipulation. While recent Reinforcement Learning (RL) approaches\nprovide whole-body loco-manipulation policies, they lack precise manipulation\nwith high DoF arms. In this paper, we propose decoupling upper-body control\nfrom locomotion, using inverse kinematics (IK) and motion retargeting for\nprecise manipulation, while RL focuses on robust lower-body locomotion. We\nintroduce PMP (Predictive Motion Priors), trained with Conditional Variational\nAutoencoder (CVAE) to effectively represent upper-body motions. The locomotion\npolicy is trained conditioned on this upper-body motion representation,\nensuring that the system remains robust with both manipulation and locomotion.\nWe show that CVAE features are crucial for stability and robustness, and\nsignificantly outperforms RL-based whole-body control in precise manipulation.\nWith precise upper-body motion and robust lower-body locomotion control,\noperators can remotely control the humanoid to walk around and explore\ndifferent environments, while performing diverse manipulation tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07773v2",
    "published_date": "2024-12-10 18:59:50 UTC",
    "updated_date": "2025-03-09 08:41:46 UTC"
  },
  {
    "arxiv_id": "2412.07755v2",
    "title": "SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models",
    "authors": [
      "Arijit Ray",
      "Jiafei Duan",
      "Ellis Brown",
      "Reuben Tan",
      "Dina Bashkirova",
      "Rose Hendrix",
      "Kiana Ehsani",
      "Aniruddha Kembhavi",
      "Bryan A. Plummer",
      "Ranjay Krishna",
      "Kuo-Hao Zeng",
      "Kate Saenko"
    ],
    "abstract": "Reasoning about motion and space is a fundamental cognitive capability that\nis required by multiple real-world applications. While many studies highlight\nthat large multimodal language models (MLMs) struggle to reason about space,\nthey only focus on static spatial relationships, and not dynamic awareness of\nmotion and space, i.e., reasoning about the effect of egocentric and object\nmotions on spatial relationships. Manually annotating such object and camera\nmovements is expensive. Hence, we introduce SAT, a simulated spatial aptitude\ntraining dataset comprising both static and dynamic spatial reasoning across\n175K question-answer (QA) pairs and 20K scenes. Complementing this, we also\nconstruct a small (150 image-QAs) yet challenging dynamic spatial test set\nusing real-world images. Leveraging our SAT datasets and 6 existing static\nspatial benchmarks, we systematically investigate what improves both static and\ndynamic spatial awareness. Our results reveal that simulations are surprisingly\neffective at imparting spatial aptitude to MLMs that translate to real images.\nWe show that perfect annotations in simulation are more effective than existing\napproaches of pseudo-annotating real images. For instance, SAT training\nimproves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an\naverage 8% on multiple spatial benchmarks, including our real-image dynamic\ntest set and spatial reasoning on long videos -- even outperforming some large\nproprietary models. While reasoning over static relationships improves with\nsynthetic training data, there is still considerable room for improvement for\ndynamic reasoning questions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project webpage: https://arijitray.com/SAT/",
    "pdf_url": "http://arxiv.org/pdf/2412.07755v2",
    "published_date": "2024-12-10 18:52:45 UTC",
    "updated_date": "2025-04-03 17:59:24 UTC"
  },
  {
    "arxiv_id": "2412.09645v2",
    "title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models",
    "authors": [
      "Fan Zhang",
      "Shulin Tian",
      "Ziqi Huang",
      "Yu Qiao",
      "Ziwei Liu"
    ],
    "abstract": "Recent advancements in visual generative models have enabled high-quality\nimage and video generation, opening diverse applications. However, evaluating\nthese models often demands sampling hundreds or thousands of images or videos,\nmaking the process computationally expensive, especially for diffusion-based\nmodels with inherently slow sampling. Moreover, existing evaluation methods\nrely on rigid pipelines that overlook specific user needs and provide numerical\nresults without clear explanations. In contrast, humans can quickly form\nimpressions of a model's capabilities by observing only a few samples. To mimic\nthis, we propose the Evaluation Agent framework, which employs human-like\nstrategies for efficient, dynamic, multi-round evaluations using only a few\nsamples per round, while offering detailed, user-tailored analyses. It offers\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\nscalability across various models and tools. Experiments show that Evaluation\nAgent reduces evaluation time to 10% of traditional methods while delivering\ncomparable results. The Evaluation Agent framework is fully open-sourced to\nadvance research in visual generative models and their efficient evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Equal contributions from first three authors. Project page:\n  https://vchitect.github.io/Evaluation-Agent-project Code:\n  https://github.com/Vchitect/Evaluation-Agent",
    "pdf_url": "http://arxiv.org/pdf/2412.09645v2",
    "published_date": "2024-12-10 18:52:39 UTC",
    "updated_date": "2024-12-16 04:05:05 UTC"
  },
  {
    "arxiv_id": "2412.07754v1",
    "title": "PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation",
    "authors": [
      "Fatemeh Nazarieh",
      "Zhenhua Feng",
      "Diptesh Kanojia",
      "Muhammad Awais",
      "Josef Kittler"
    ],
    "abstract": "Audio-driven talking face generation is a challenging task in digital\ncommunication. Despite significant progress in the area, most existing methods\nconcentrate on audio-lip synchronization, often overlooking aspects such as\nvisual quality, customization, and generalization that are crucial to producing\nrealistic talking faces. To address these limitations, we introduce a novel,\ncustomizable one-shot audio-driven talking face generation framework, named\nPortraitTalk. Our proposed method utilizes a latent diffusion framework\nconsisting of two main components: IdentityNet and AnimateNet. IdentityNet is\ndesigned to preserve identity features consistently across the generated video\nframes, while AnimateNet aims to enhance temporal coherence and motion\nconsistency. This framework also integrates an audio input with the reference\nimages, thereby reducing the reliance on reference-style videos prevalent in\nexisting approaches. A key innovation of PortraitTalk is the incorporation of\ntext prompts through decoupled cross-attention mechanisms, which significantly\nexpands creative control over the generated videos. Through extensive\nexperiments, including a newly developed evaluation metric, our model\ndemonstrates superior performance over the state-of-the-art methods, setting a\nnew standard for the generation of customizable realistic talking faces\nsuitable for real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07754v1",
    "published_date": "2024-12-10 18:51:31 UTC",
    "updated_date": "2024-12-10 18:51:31 UTC"
  },
  {
    "arxiv_id": "2412.07752v3",
    "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
    "authors": [
      "Korbinian Pöppel",
      "Maximilian Beck",
      "Sepp Hochreiter"
    ],
    "abstract": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07752v3",
    "published_date": "2024-12-10 18:50:37 UTC",
    "updated_date": "2025-03-13 11:14:49 UTC"
  },
  {
    "arxiv_id": "2412.07747v2",
    "title": "Predictive Modeling of Homeless Service Assignment: A Representation Learning Approach",
    "authors": [
      "Khandker Sadia Rahman",
      "Charalampos Chelmis"
    ],
    "abstract": "In recent years, there has been growing interest in leveraging machine\nlearning for homeless service assignment. However, the categorical nature of\nadministrative data recorded for homeless individuals hinders the development\nof accurate machine learning methods for this task. This work asserts that\nderiving latent representations of such features, while at the same time\nleveraging underlying relationships between instances is crucial in\nalgorithmically enhancing the existing assignment decision-making process. Our\nproposed approach learns temporal and functional relationships between services\nfrom historical data, as well as unobserved but relevant relationships between\nindividuals to generate features that significantly improve the prediction of\nthe next service assignment compared to the state-of-the-art.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07747v2",
    "published_date": "2024-12-10 18:47:10 UTC",
    "updated_date": "2024-12-11 20:24:57 UTC"
  },
  {
    "arxiv_id": "2412.07739v1",
    "title": "GASP: Gaussian Avatars with Synthetic Priors",
    "authors": [
      "Jack Saunders",
      "Charlie Hewitt",
      "Yanan Jian",
      "Marek Kowalski",
      "Tadas Baltrusaitis",
      "Yiye Chen",
      "Darren Cosker",
      "Virginia Estellers",
      "Nicholas Gyde",
      "Vinay P. Namboodiri",
      "Benjamin E Lundell"
    ],
    "abstract": "Gaussian Splatting has changed the game for real-time photo-realistic\nrendering. One of the most popular applications of Gaussian Splatting is to\ncreate animatable avatars, known as Gaussian Avatars. Recent works have pushed\nthe boundaries of quality and rendering efficiency but suffer from two main\nlimitations. Either they require expensive multi-camera rigs to produce avatars\nwith free-view rendering, or they can be trained with a single camera but only\nrendered at high quality from this fixed viewpoint. An ideal model would be\ntrained using a short monocular video or image from available hardware, such as\na webcam, and rendered from any view. To this end, we propose GASP: Gaussian\nAvatars with Synthetic Priors. To overcome the limitations of existing\ndatasets, we exploit the pixel-perfect nature of synthetic data to train a\nGaussian Avatar prior. By fitting this prior model to a single photo or video\nand fine-tuning it, we get a high-quality Gaussian Avatar, which supports\n360$^\\circ$ rendering. Our prior is only required for fitting, not inference,\nenabling real-time application. Through our method, we obtain high-quality,\nanimatable Avatars from limited data which can be animated and rendered at\n70fps on commercial hardware. See our project page\n(https://microsoft.github.io/GASP/) for results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://microsoft.github.io/GASP/",
    "pdf_url": "http://arxiv.org/pdf/2412.07739v1",
    "published_date": "2024-12-10 18:36:21 UTC",
    "updated_date": "2024-12-10 18:36:21 UTC"
  },
  {
    "arxiv_id": "2412.07730v1",
    "title": "STIV: Scalable Text and Image Conditioned Video Generation",
    "authors": [
      "Zongyu Lin",
      "Wei Liu",
      "Chen Chen",
      "Jiasen Lu",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jesse Allardice",
      "Zhengfeng Lai",
      "Liangchen Song",
      "Bowen Zhang",
      "Cha Chen",
      "Yiran Fei",
      "Yifan Jiang",
      "Lezhi Li",
      "Yizhou Sun",
      "Kai-Wei Chang",
      "Yinfei Yang"
    ],
    "abstract": "The field of video generation has made remarkable advancements, yet there\nremains a pressing need for a clear, systematic recipe that can guide the\ndevelopment of robust and scalable models. In this work, we present a\ncomprehensive study that systematically explores the interplay of model\narchitectures, training recipes, and data curation strategies, culminating in a\nsimple and scalable text-image-conditioned video generation method, named STIV.\nOur framework integrates image condition into a Diffusion Transformer (DiT)\nthrough frame replacement, while incorporating text conditioning via a joint\nimage-text conditional classifier-free guidance. This design enables STIV to\nperform both text-to-video (T2V) and text-image-to-video (TI2V) tasks\nsimultaneously. Additionally, STIV can be easily extended to various\napplications, such as video prediction, frame interpolation, multi-view\ngeneration, and long video generation, etc. With comprehensive ablation studies\non T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple\ndesign. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V,\nsurpassing both leading open and closed-source models like CogVideoX-5B, Pika,\nKling, and Gen-3. The same-sized model also achieves a state-of-the-art result\nof 90.1 on VBench I2V task at 512 resolution. By providing a transparent and\nextensible recipe for building cutting-edge video generation models, we aim to\nempower future research and accelerate progress toward more versatile and\nreliable video generation solutions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07730v1",
    "published_date": "2024-12-10 18:27:06 UTC",
    "updated_date": "2024-12-10 18:27:06 UTC"
  },
  {
    "arxiv_id": "2412.07713v1",
    "title": "Benchmark for Evaluation and Analysis of Citation Recommendation Models",
    "authors": [
      "Puja Maharjan"
    ],
    "abstract": "Citation recommendation systems have attracted much academic interest,\nresulting in many studies and implementations. These systems help authors\nautomatically generate proper citations by suggesting relevant references based\non the text they have written. However, the methods used in citation\nrecommendation differ across various studies and implementations. Some\napproaches focus on the overall content of papers, while others consider the\ncontext of the citation text. Additionally, the datasets used in these studies\ninclude different aspects of papers, such as metadata, citation context, or\neven the full text of the paper in various formats and structures. The\ndiversity in models, datasets, and evaluation metrics makes it challenging to\nassess and compare citation recommendation methods effectively. To address this\nissue, a standardized dataset and evaluation metrics are needed to evaluate\nthese models consistently. Therefore, we propose developing a benchmark\nspecifically designed to analyze and compare citation recommendation models.\nThis benchmark will evaluate the performance of models on different features of\nthe citation context and provide a comprehensive evaluation of the models\nacross all these tasks, presenting the results in a standardized way. By\ncreating a benchmark with standardized evaluation metrics, researchers and\npractitioners in the field of citation recommendation will have a common\nplatform to assess and compare different models. This will enable meaningful\ncomparisons and help identify promising approaches for further research and\ndevelopment in the field.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.07713v1",
    "published_date": "2024-12-10 18:01:33 UTC",
    "updated_date": "2024-12-10 18:01:33 UTC"
  },
  {
    "arxiv_id": "2412.07696v1",
    "title": "SimVS: Simulating World Inconsistencies for Robust View Synthesis",
    "authors": [
      "Alex Trevithick",
      "Roni Paiss",
      "Philipp Henzler",
      "Dor Verbin",
      "Rundi Wu",
      "Hadi Alzayer",
      "Ruiqi Gao",
      "Ben Poole",
      "Jonathan T. Barron",
      "Aleksander Holynski",
      "Ravi Ramamoorthi",
      "Pratul P. Srinivasan"
    ],
    "abstract": "Novel-view synthesis techniques achieve impressive results for static scenes\nbut struggle when faced with the inconsistencies inherent to casual capture\nsettings: varying illumination, scene motion, and other unintended effects that\nare difficult to model explicitly. We present an approach for leveraging\ngenerative video models to simulate the inconsistencies in the world that can\noccur during capture. We use this process, along with existing multi-view\ndatasets, to create synthetic data for training a multi-view harmonization\nnetwork that is able to reconcile inconsistent observations into a consistent\n3D scene. We demonstrate that our world-simulation strategy significantly\noutperforms traditional augmentation methods in handling real-world scene\nvariations, thereby enabling highly accurate static 3D reconstructions in the\npresence of a variety of challenging inconsistencies. Project page:\nhttps://alextrevithick.github.io/simvs",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://alextrevithick.github.io/simvs",
    "pdf_url": "http://arxiv.org/pdf/2412.07696v1",
    "published_date": "2024-12-10 17:35:12 UTC",
    "updated_date": "2024-12-10 17:35:12 UTC"
  },
  {
    "arxiv_id": "2412.07686v1",
    "title": "Optimizing Sensor Redundancy in Sequential Decision-Making Problems",
    "authors": [
      "Jonas Nüßlein",
      "Maximilian Zorn",
      "Fabian Ritz",
      "Jonas Stein",
      "Gerhard Stenzel",
      "Julian Schönberger",
      "Thomas Gabor",
      "Claudia Linnhoff-Popien"
    ],
    "abstract": "Reinforcement Learning (RL) policies are designed to predict actions based on\ncurrent observations to maximize cumulative future rewards. In real-world\napplications (i.e., non-simulated environments), sensors are essential for\nmeasuring the current state and providing the observations on which RL policies\nrely to make decisions. A significant challenge in deploying RL policies in\nreal-world scenarios is handling sensor dropouts, which can result from\nhardware malfunctions, physical damage, or environmental factors like dust on a\ncamera lens. A common strategy to mitigate this issue is the use of backup\nsensors, though this comes with added costs. This paper explores the\noptimization of backup sensor configurations to maximize expected returns while\nkeeping costs below a specified threshold, C. Our approach uses a second-order\napproximation of expected returns and includes penalties for exceeding cost\nconstraints. We then optimize this quadratic program using Tabu Search, a\nmeta-heuristic algorithm. The approach is evaluated across eight OpenAI Gym\nenvironments and a custom Unity-based robotic environment (RobotArmGrasping).\nEmpirical results demonstrate that our quadratic program effectively\napproximates real expected returns, facilitating the identification of optimal\nsensor configurations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at ICAART conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07686v1",
    "published_date": "2024-12-10 17:20:44 UTC",
    "updated_date": "2024-12-10 17:20:44 UTC"
  },
  {
    "arxiv_id": "2412.07684v1",
    "title": "The Pitfalls of Memorization: When Memorization Hurts Generalization",
    "authors": [
      "Reza Bayat",
      "Mohammad Pezeshki",
      "Elvis Dohmatob",
      "David Lopez-Paz",
      "Pascal Vincent"
    ],
    "abstract": "Neural networks often learn simple explanations that fit the majority of the\ndata while memorizing exceptions that deviate from these explanations.This\nbehavior leads to poor generalization when the learned explanations rely on\nspurious correlations. In this work, we formalize the interplay between\nmemorization and generalization, showing that spurious correlations would\nparticularly lead to poor generalization when are combined with memorization.\nMemorization can reduce training loss to zero, leaving no incentive to learn\nrobust, generalizable patterns. To address this, we propose memorization-aware\ntraining (MAT), which uses held-out predictions as a signal of memorization to\nshift a model's logits. MAT encourages learning robust patterns invariant\nacross distributions, improving generalization under distribution shifts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07684v1",
    "published_date": "2024-12-10 17:18:33 UTC",
    "updated_date": "2024-12-10 17:18:33 UTC"
  },
  {
    "arxiv_id": "2412.07679v2",
    "title": "RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models",
    "authors": [
      "Greg Heinrich",
      "Mike Ranzinger",
      "Hongxu",
      "Yin",
      "Yao Lu",
      "Jan Kautz",
      "Andrew Tao",
      "Bryan Catanzaro",
      "Pavlo Molchanov"
    ],
    "abstract": "Agglomerative models have recently emerged as a powerful approach to training\nvision foundation models, leveraging multi-teacher distillation from existing\nmodels such as CLIP, DINO, and SAM. This strategy enables the efficient\ncreation of robust models, combining the strengths of individual teachers while\nsignificantly reducing computational and resource demands. In this paper, we\nthoroughly analyze state-of-the-art agglomerative models, identifying critical\nchallenges including resolution mode shifts, teacher imbalance, idiosyncratic\nteacher artifacts, and an excessive number of output tokens. To address these\nissues, we propose several novel solutions: multi-resolution training, mosaic\naugmentation, and improved balancing of teacher loss functions. Specifically,\nin the context of Vision Language Models, we introduce a token compression\ntechnique to maintain high-resolution information within a fixed token count.\nWe release our top-performing variants at multiple scales (-B, -L, -H, and -g),\nalong with inference code and pretrained weights",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07679v2",
    "published_date": "2024-12-10 17:06:41 UTC",
    "updated_date": "2025-02-09 15:03:08 UTC"
  },
  {
    "arxiv_id": "2412.16181v2",
    "title": "Minimum Weighted Feedback Arc Sets for Ranking from Pairwise Comparisons",
    "authors": [
      "Soroush Vahidi",
      "Ioannis Koutis"
    ],
    "abstract": "The Minimum Weighted Feedback Arc Set (MWFAS) problem is fundamentally\nconnected to the Ranking Problem -- the task of deriving global rankings from\npairwise comparisons. Recent work [He et al. ICML2022] has advanced the\nstate-of-the-art for the Ranking Problem using learning-based methods,\nimproving upon multiple previous approaches. However, the connection to MWFAS\nremains underexplored. This paper investigates this relationship and presents\nefficient combinatorial algorithms for solving MWFAS, thus addressing the\nRanking Problem. Our experimental results demonstrate that these simple,\nlearning-free algorithms not only significantly outperform learning-based\nmethods in terms of speed but also generally achieve superior ranking accuracy.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "This is a preliminary paper",
    "pdf_url": "http://arxiv.org/pdf/2412.16181v2",
    "published_date": "2024-12-10 16:51:11 UTC",
    "updated_date": "2025-01-07 22:12:47 UTC"
  },
  {
    "arxiv_id": "2412.07658v2",
    "title": "TraSCE: Trajectory Steering for Concept Erasure",
    "authors": [
      "Anubhav Jain",
      "Yuya Kobayashi",
      "Takashi Shibuya",
      "Yuhta Takida",
      "Nasir Memon",
      "Julian Togelius",
      "Yuki Mitsufuji"
    ],
    "abstract": "Recent advancements in text-to-image diffusion models have brought them to\nthe public spotlight, becoming widely accessible and embraced by everyday\nusers. However, these models have been shown to generate harmful content such\nas not-safe-for-work (NSFW) images. While approaches have been proposed to\nerase such abstract concepts from the models, jail-breaking techniques have\nsucceeded in bypassing such safety measures. In this paper, we propose TraSCE,\nan approach to guide the diffusion trajectory away from generating harmful\ncontent. Our approach is based on negative prompting, but as we show in this\npaper, a widely used negative prompting strategy is not a complete solution and\ncan easily be bypassed in some corner cases. To address this issue, we first\npropose using a specific formulation of negative prompting instead of the\nwidely used one. Furthermore, we introduce a localized loss-based guidance that\nenhances the modified negative prompting technique by steering the diffusion\ntrajectory. We demonstrate that our proposed method achieves state-of-the-art\nresults on various benchmarks in removing harmful content, including ones\nproposed by red teams, and erasing artistic styles and objects. Our proposed\napproach does not require any training, weight modifications, or training data\n(either image or prompt), making it easier for model owners to erase new\nconcepts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07658v2",
    "published_date": "2024-12-10 16:45:03 UTC",
    "updated_date": "2025-03-17 15:37:35 UTC"
  },
  {
    "arxiv_id": "2501.08416v1",
    "title": "A Survey on Recent Advances in Self-Organizing Maps",
    "authors": [
      "Axel Guérin",
      "Pierre Chauvet",
      "Frédéric Saubion"
    ],
    "abstract": "Self-organising maps are a powerful tool for cluster analysis in a wide range\nof data contexts. From the pioneer work of Kohonen, many variants and\nimprovements have been proposed. This review focuses on the last decade, in\norder to provide an overview of the main evolution of the seminal SOM algorithm\nas well as of the methodological developments that have been achieved in order\nto better fit to various application contexts and users' requirements. We also\nhighlight a specific and important application field that is related to\ncommercial use of SOM, which involves specific data management.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "36 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.08416v1",
    "published_date": "2024-12-10 16:40:02 UTC",
    "updated_date": "2024-12-10 16:40:02 UTC"
  },
  {
    "arxiv_id": "2412.10425v3",
    "title": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation",
    "authors": [
      "Rithvik Prakki"
    ],
    "abstract": "This paper introduces a novel approach to creating adaptive language agents\nby integrating active inference with large language models (LLMs). While LLMs\ndemonstrate remarkable capabilities, their reliance on static prompts limits\nadaptation to new information and changing environments. We address this by\nimplementing an active inference framework that acts as a cognitive layer above\nan LLM-based agent, dynamically adjusting prompts and search strategies through\nprincipled information-seeking behavior. Our framework models the environment\nusing three state factors (prompt, search, and information states) with seven\nobservation modalities capturing quality metrics. By framing the agent's\nlearning through the free energy principle, we enable systematic exploration of\nprompt combinations and search strategies. Experimental results demonstrate the\neffectiveness of this approach, with the agent developing accurate models of\nenvironment dynamics evidenced by emergent structure in observation matrices.\nAction selection patterns reveal sophisticated exploration-exploitation\nbehavior, transitioning from initial information-gathering to targeted prompt\ntesting. The integration of thermodynamic principles with language model\ncapabilities provides a principled framework for creating robust, adaptable\nagents, extending active inference beyond traditional low-dimensional control\nproblems to high-dimensional, language-driven environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10425v3",
    "published_date": "2024-12-10 16:34:47 UTC",
    "updated_date": "2025-01-09 22:46:26 UTC"
  },
  {
    "arxiv_id": "2412.09644v1",
    "title": "Combining knowledge graphs and LLMs for hazardous chemical information management and reuse",
    "authors": [
      "Marcos Da Silveira",
      "Louis Deladiennee",
      "Kheira Acem",
      "Oona Freudenthal"
    ],
    "abstract": "Human health is increasingly threatened by exposure to hazardous substances,\nparticularly persistent and toxic chemicals. The link between these substances,\noften encountered in complex mixtures, and various diseases are demonstrated in\nscientific studies. However, this information is scattered across several\nsources and hardly accessible by humans and machines. This paper evaluates\ncurrent practices for publishing/accessing information on hazardous chemicals\nand proposes a novel platform designed to facilitate retrieval of critical\nchemical data in urgent situations. The platform aggregates information from\nmultiple sources and organizes it into a structured knowledge graph. Users can\naccess this information through a visual interface such as Neo4J Bloom and\ndashboards, or via natural language queries using a Chatbot. Our findings\ndemonstrate a significant reduction in the time and effort required to access\nvital chemical information when datasets follow FAIR principles. Furthermore,\nwe discuss the lessons learned from the development and implementation of this\nplatform and provide recommendations for data owners and publishers to enhance\ndata reuse and interoperability. This work aims to improve the accessibility\nand usability of chemical information by healthcare professionals, thereby\nsupporting better health outcomes and informed decision-making in the face of\npatients exposed to chemical intoxication risks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "H.4; J.3"
    ],
    "primary_category": "cs.IR",
    "comment": "Submitted to IEEE BIBM24",
    "pdf_url": "http://arxiv.org/pdf/2412.09644v1",
    "published_date": "2024-12-10 16:31:53 UTC",
    "updated_date": "2024-12-10 16:31:53 UTC"
  },
  {
    "arxiv_id": "2412.07639v2",
    "title": "Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization",
    "authors": [
      "Zongkai Liu",
      "Qian Lin",
      "Chao Yu",
      "Xiawei Wu",
      "Yile Liang",
      "Donghui Li",
      "Xuetao Ding"
    ],
    "abstract": "Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that\naims to learn optimal multi-agent policies from pre-collected datasets.\nCompared to single-agent case, multi-agent setting involves a large joint\nstate-action space and coupled behaviors of multiple agents, which bring extra\ncomplexity to offline policy optimization. In this work, we revisit the\nexisting offline MARL methods and show that in certain scenarios they can be\nproblematic, leading to uncoordinated behaviors and out-of-distribution (OOD)\njoint actions. To address these issues, we propose a new offline MARL\nalgorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO\nsequentially updates each agent's policy in an in-sample manner, which not only\navoids selecting OOD joint actions but also carefully considers teammates'\nupdated policies to enhance coordination. Additionally, by thoroughly exploring\nlow-probability actions in the behavior policy, InSPO can well address the\nissue of premature convergence to sub-optimal solutions. Theoretically, we\nprove InSPO guarantees monotonic policy improvement and converges to quantal\nresponse equilibrium (QRE). Experimental results demonstrate the effectiveness\nof our method compared to current state-of-the-art offline MARL methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07639v2",
    "published_date": "2024-12-10 16:19:08 UTC",
    "updated_date": "2024-12-18 09:04:32 UTC"
  },
  {
    "arxiv_id": "2412.07636v1",
    "title": "TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize Hardware Trojans",
    "authors": [
      "Md Omar Faruque",
      "Peter Jamieson",
      "Ahmad Patooghy",
      "Abdel-Hameed A. Badawy"
    ],
    "abstract": "Existing Hardware Trojans (HT) detection methods face several critical\nlimitations: logic testing struggles with scalability and coverage for large\ndesigns, side-channel analysis requires golden reference chips, and formal\nverification methods suffer from state-space explosion. The emergence of Large\nLanguage Models (LLMs) offers a promising new direction for HT detection by\nleveraging their natural language understanding and reasoning capabilities. For\nthe first time, this paper explores the potential of general-purpose LLMs in\ndetecting various HTs inserted in Register Transfer Level (RTL) designs,\nincluding SRAM, AES, and UART modules. We propose a novel tool for this goal\nthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and\nLlama 3.1) in detecting HTs without prior fine-tuning. To address potential\ntraining data bias, the tool implements perturbation techniques, i.e., variable\nname obfuscation, and design restructuring, that make the cases more\nsophisticated for the used LLMs. Our experimental evaluation demonstrates\nperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios\n(100%/100% precision/recall), with both models achieving better trigger line\ncoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under\ncode perturbation, while Gemini 1.5 pro maintains perfect detection performance\n(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some\ndegradation in detection rates, and all models experience decreased accuracy in\nlocalizing both triggers and payloads. This paper validates the potential of\nLLM approaches for hardware security applications, highlighting areas for\nfuture improvement.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07636v1",
    "published_date": "2024-12-10 16:16:22 UTC",
    "updated_date": "2024-12-10 16:16:22 UTC"
  },
  {
    "arxiv_id": "2412.07629v4",
    "title": "Piece of Table: A Divide-and-Conquer Approach for Selecting Subtables in Table Question Answering",
    "authors": [
      "Wonjin Lee",
      "Kyumin Kim",
      "Sungjae Lee",
      "Jihun Lee",
      "Kwang In Kim"
    ],
    "abstract": "Applying language models (LMs) to tables is challenging due to the inherent\nstructural differences between two-dimensional tables and one-dimensional text\nfor which the LMs were originally designed. Furthermore, when applying\nlinearized tables to LMs, the maximum token lengths often imposed in\nself-attention calculations make it difficult to comprehensively understand the\ncontext spread across large tables. To address these challenges, we present\nPieTa (Piece of Table), a new framework for subtable-based question answering\n(QA). PieTa operates through an iterative process of dividing tables into\nsmaller windows, using LMs to select relevant cells within each window, and\nmerging these cells into a subtable. This multi-resolution approach captures\ndependencies across multiple rows and columns while avoiding the limitations\ncaused by long context inputs. Instantiated as a simple iterative subtable\nunion algorithm, PieTa demonstrates improved performance over previous\nsubtable-based QA approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07629v4",
    "published_date": "2024-12-10 16:08:14 UTC",
    "updated_date": "2025-02-19 11:56:57 UTC"
  },
  {
    "arxiv_id": "2412.07626v2",
    "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
    "authors": [
      "Linke Ouyang",
      "Yuan Qu",
      "Hongbin Zhou",
      "Jiawei Zhu",
      "Rui Zhang",
      "Qunshu Lin",
      "Bin Wang",
      "Zhiyuan Zhao",
      "Man Jiang",
      "Xiaomeng Zhao",
      "Jin Shi",
      "Fan Wu",
      "Pei Chu",
      "Minghao Liu",
      "Zhenxiang Li",
      "Chao Xu",
      "Bo Zhang",
      "Botian Shi",
      "Zhongying Tu",
      "Conghui He"
    ],
    "abstract": "Document content extraction is a critical task in computer vision,\nunderpinning the data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) systems. Despite recent progress, current\ndocument parsing methods have not been fairly and comprehensively evaluated due\nto the narrow coverage of document types and the simplified, unrealistic\nevaluation procedures in existing benchmarks. To address these gaps, we\nintroduce OmniDocBench, a novel benchmark featuring high-quality annotations\nacross nine document sources, including academic papers, textbooks, and more\nchallenging cases such as handwritten notes and densely typeset newspapers.\nOmniDocBench supports flexible, multi-level evaluations--ranging from an\nend-to-end assessment to the task-specific and attribute--based analysis using\n19 layout categories and 15 attribute labels. We conduct a thorough evaluation\nof both pipeline-based methods and end-to-end vision-language models, revealing\ntheir strengths and weaknesses across different document types. OmniDocBench\nsets a new standard for the fair, diverse, and fine-grained evaluation in\ndocument parsing. Dataset and code are available at\nhttps://github.com/opendatalab/OmniDocBench.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07626v2",
    "published_date": "2024-12-10 16:05:56 UTC",
    "updated_date": "2025-03-25 06:19:32 UTC"
  },
  {
    "arxiv_id": "2412.07618v2",
    "title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced Retrieval-Augmented Generation on Knowledge Graphs",
    "authors": [
      "Xiaqiang Tang",
      "Jian Li",
      "Nan Du",
      "Sihong Xie"
    ],
    "abstract": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07618v2",
    "published_date": "2024-12-10 15:56:03 UTC",
    "updated_date": "2024-12-20 03:12:49 UTC"
  },
  {
    "arxiv_id": "2412.07617v1",
    "title": "Swarm Behavior Cloning",
    "authors": [
      "Jonas Nüßlein",
      "Maximilian Zorn",
      "Philipp Altmann",
      "Claudia Linnhoff-Popien"
    ],
    "abstract": "In sequential decision-making environments, the primary approaches for\ntraining agents are Reinforcement Learning (RL) and Imitation Learning (IL).\nUnlike RL, which relies on modeling a reward function, IL leverages expert\ndemonstrations, where an expert policy $\\pi_e$ (e.g., a human) provides the\ndesired behavior. Formally, a dataset $D$ of state-action pairs is provided: $D\n= {(s, a = \\pi_e(s))}$. A common technique within IL is Behavior Cloning (BC),\nwhere a policy $\\pi(s) = a$ is learned through supervised learning on $D$.\nFurther improvements can be achieved by using an ensemble of $N$ individually\ntrained BC policies, denoted as $E = {\\pi_i(s)}{1 \\leq i \\leq N}$. The\nensemble's action $a$ for a given state $s$ is the aggregated output of the $N$\nactions: $a = \\frac{1}{N} \\sum{i} \\pi_i(s)$. This paper addresses the issue of\nincreasing action differences -- the observation that discrepancies between the\n$N$ predicted actions grow in states that are underrepresented in the training\ndata. Large action differences can result in suboptimal aggregated actions. To\naddress this, we propose a method that fosters greater alignment among the\npolicies while preserving the diversity of their computations. This approach\nreduces action differences and ensures that the ensemble retains its inherent\nstrengths, such as robustness and varied decision-making. We evaluate our\napproach across eight diverse environments, demonstrating a notable decrease in\naction differences and significant improvements in overall performance, as\nmeasured by mean episode returns.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICAART 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07617v1",
    "published_date": "2024-12-10 15:54:57 UTC",
    "updated_date": "2024-12-10 15:54:57 UTC"
  },
  {
    "arxiv_id": "2412.07585v1",
    "title": "Scaling Sequential Recommendation Models with Transformers",
    "authors": [
      "Pablo Zivic",
      "Hernan Vazquez",
      "Jorge Sanchez"
    ],
    "abstract": "Modeling user preferences has been mainly addressed by looking at users'\ninteraction history with the different elements available in the system.\nTailoring content to individual preferences based on historical data is the\nmain goal of sequential recommendation.\n  The nature of the problem, as well as the good performance observed across\nvarious domains, has motivated the use of the transformer architecture, which\nhas proven effective in leveraging increasingly larger amounts of training data\nwhen accompanied by an increase in the number of model parameters. This scaling\nbehavior has brought a great deal of attention, as it provides valuable\nguidance in the design and training of even larger models.\n  Taking inspiration from the scaling laws observed in training large language\nmodels, we explore similar principles for sequential recommendation.\n  We use the full Amazon Product Data dataset, which has only been partially\nexplored in other studies, and reveal scaling behaviors similar to those found\nin language models. Compute-optimal training is possible but requires a careful\nanalysis of the compute-performance trade-offs specific to the application.\n  We also show that performance scaling translates to downstream tasks by\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\napproach and findings provide a strategic roadmap for model training and\ndeployment in real high-dimensional preference spaces, facilitating better\ntraining and inference efficiency.\n  We hope this paper bridges the gap between the potential of transformers and\nthe intrinsic complexities of high-dimensional sequential recommendation in\nreal-world recommender systems.\n  Code and models can be found at https://github.com/mercadolibre/srt",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07585v1",
    "published_date": "2024-12-10 15:20:56 UTC",
    "updated_date": "2024-12-10 15:20:56 UTC"
  },
  {
    "arxiv_id": "2412.07584v1",
    "title": "Multimodal Contextualized Support for Enhancing Video Retrieval System",
    "authors": [
      "Quoc-Bao Nguyen-Le",
      "Thanh-Huy Le-Nguyen"
    ],
    "abstract": "Current video retrieval systems, especially those used in competitions,\nprimarily focus on querying individual keyframes or images rather than encoding\nan entire clip or video segment. However, queries often describe an action or\nevent over a series of frames, not a specific image. This results in\ninsufficient information when analyzing a single frame, leading to less\naccurate query results. Moreover, extracting embeddings solely from images\n(keyframes) does not provide enough information for models to encode\nhigher-level, more abstract insights inferred from the video. These models tend\nto only describe the objects present in the frame, lacking a deeper\nunderstanding. In this work, we propose a system that integrates the latest\nmethodologies, introducing a novel pipeline that extracts multimodal data, and\nincorporate information from multiple frames within a video, enabling the model\nto abstract higher-level information that captures latent meanings, focusing on\nwhat can be inferred from the video clip, rather than just focusing on object\ndetection in one single image.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07584v1",
    "published_date": "2024-12-10 15:20:23 UTC",
    "updated_date": "2024-12-10 15:20:23 UTC"
  },
  {
    "arxiv_id": "2412.07583v1",
    "title": "Mobile Video Diffusion",
    "authors": [
      "Haitam Ben Yahia",
      "Denis Korzhenkov",
      "Ioannis Lelekas",
      "Amir Ghodrati",
      "Amirhossein Habibian"
    ],
    "abstract": "Video diffusion models have achieved impressive realism and controllability\nbut are limited by high computational demands, restricting their use on mobile\ndevices. This paper introduces the first mobile-optimized video diffusion\nmodel. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD),\nwe reduce memory and computational cost by reducing the frame resolution,\nincorporating multi-scale temporal representations, and introducing two novel\npruning schema to reduce the number of channels and temporal blocks.\nFurthermore, we employ adversarial finetuning to reduce the denoising to a\nsingle step. Our model, coined as MobileVD, is 523x more efficient (1817.2 vs.\n4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents\nfor a 14x512x256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are\navailable at https://qualcomm-ai-research.github.io/mobile-video-diffusion/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07583v1",
    "published_date": "2024-12-10 15:19:10 UTC",
    "updated_date": "2024-12-10 15:19:10 UTC"
  },
  {
    "arxiv_id": "2412.10424v2",
    "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
    "authors": [
      "Eunsu Kim",
      "Juyoung Suk",
      "Seungone Kim",
      "Niklas Muennighoff",
      "Dongkwan Kim",
      "Alice Oh"
    ],
    "abstract": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10424v2",
    "published_date": "2024-12-10 15:00:32 UTC",
    "updated_date": "2024-12-30 09:11:50 UTC"
  },
  {
    "arxiv_id": "2412.07820v1",
    "title": "Hyperband-based Bayesian Optimization for Black-box Prompt Selection",
    "authors": [
      "Lennart Schneider",
      "Martin Wistuba",
      "Aaron Klein",
      "Jacek Golebiowski",
      "Giovanni Zappella",
      "Felice Antonio Merra"
    ],
    "abstract": "Optimal prompt selection is crucial for maximizing large language model (LLM)\nperformance on downstream tasks. As the most powerful models are proprietary\nand can only be invoked via an API, users often manually refine prompts in a\nblack-box setting by adjusting instructions and few-shot examples until they\nachieve good performance as measured on a validation set. Recent methods\naddressing static black-box prompt selection face significant limitations: They\noften fail to leverage the inherent structure of prompts, treating instructions\nand few-shot exemplars as a single block of text. Moreover, they often lack\nquery-efficiency by evaluating prompts on all validation instances, or risk\nsub-optimal selection of a prompt by using random subsets of validation\ninstances. We introduce HbBoPs, a novel Hyperband-based Bayesian optimization\nmethod for black-box prompt selection addressing these key limitations. Our\napproach combines a structural-aware deep kernel Gaussian Process to model\nprompt performance with Hyperband as a multi-fidelity scheduler to select the\nnumber of validation instances for prompt evaluations. The structural-aware\nmodeling approach utilizes separate embeddings for instructions and few-shot\nexemplars, enhancing the surrogate model's ability to capture prompt\nperformance and predict which prompt to evaluate next in a sample-efficient\nmanner. Together with Hyperband as a multi-fidelity scheduler we further enable\nquery-efficiency by adaptively allocating resources across different fidelity\nlevels, keeping the total number of validation instances prompts are evaluated\non low. Extensive evaluation across ten benchmarks and three LLMs demonstrate\nthat HbBoPs outperforms state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07820v1",
    "published_date": "2024-12-10 14:42:51 UTC",
    "updated_date": "2024-12-10 14:42:51 UTC"
  },
  {
    "arxiv_id": "2412.07541v1",
    "title": "A data-driven learned discretization approach in finite volume schemes for hyperbolic conservation laws and varying boundary conditions",
    "authors": [
      "Guillaume de Romémont",
      "Florent Renac",
      "Jorge Nunez",
      "Francisco Chinesta"
    ],
    "abstract": "This paper presents a data-driven finite volume method for solving 1D and 2D\nhyperbolic partial differential equations. This work builds upon the prior\nresearch incorporating a data-driven finite-difference approximation of smooth\nsolutions of scalar conservation laws, where optimal coefficients of neural\nnetworks approximating space derivatives are learned based on accurate, but\ncumbersome solutions to these equations. We extend this approach to\nflux-limited finite volume schemes for hyperbolic scalar and systems of\nconservation laws. We also train the discretization to efficiently capture\ndiscontinuous solutions with shock and contact waves, as well as to the\napplication of boundary conditions. The learning procedure of the data-driven\nmodel is extended through the definition of a new loss, paddings and adequate\ndatabase. These new ingredients guarantee computational stability, preserve the\naccuracy of fine-grid solutions, and enhance overall performance. Numerical\nexperiments using test cases from the literature in both one- and\ntwo-dimensional spaces demonstrate that the learned model accurately reproduces\nfine-grid results on very coarse meshes.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "15 pages, 20 figures with appendice",
    "pdf_url": "http://arxiv.org/pdf/2412.07541v1",
    "published_date": "2024-12-10 14:18:30 UTC",
    "updated_date": "2024-12-10 14:18:30 UTC"
  },
  {
    "arxiv_id": "2412.07538v2",
    "title": "Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?",
    "authors": [
      "D. Cotroneo",
      "F. C. Grasso",
      "R. Natella",
      "V. Orbinato"
    ],
    "abstract": "Vulnerability prediction is valuable in identifying security issues\nefficiently, even though it requires the source code of the target software\nsystem, which is a restrictive hypothesis. This paper presents an experimental\nstudy to predict vulnerabilities in binary code without source code or complex\nrepresentations of the binary, leveraging the pivotal idea of decompiling the\nbinary file through neural decompilation and predicting vulnerabilities through\ndeep learning on the decompiled source code. The results outperform the\nstate-of-the-art in both neural decompilation and vulnerability prediction,\nshowing that it is possible to identify vulnerable programs with this approach\nconcerning bi-class (vulnerable/non-vulnerable) and multi-class (type of\nvulnerability) analysis.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07538v2",
    "published_date": "2024-12-10 14:17:14 UTC",
    "updated_date": "2025-03-29 14:19:09 UTC"
  },
  {
    "arxiv_id": "2412.07493v1",
    "title": "Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning",
    "authors": [
      "Muhayy Ud Din",
      "Jan Rosell",
      "Waseem Akram",
      "Isiah Zaplana",
      "Maximo A Roa",
      "Lakmal Seneviratne",
      "Irfan Hussain"
    ],
    "abstract": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches, which combine high-level\nsymbolic plan with low-level motion planning. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which struggles in adapting\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel ontology-driven prompt-tuning framework\nthat employs knowledge-based reasoning to refine and expand user prompts with\ntask contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments, and the\ngeneration of semantically correct task plans.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to Robotics and Automation Letters",
    "pdf_url": "http://arxiv.org/pdf/2412.07493v1",
    "published_date": "2024-12-10 13:18:45 UTC",
    "updated_date": "2024-12-10 13:18:45 UTC"
  },
  {
    "arxiv_id": "2412.10423v2",
    "title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding Harmful Content with GuidelineLLM",
    "authors": [
      "Shaoqing Zhang",
      "Zhuosheng Zhang",
      "Kehai Chen",
      "Rongxiang Weng",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min Zhang"
    ],
    "abstract": "Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against LLM (an average\nreduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling\nbenign queries. The code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10423v2",
    "published_date": "2024-12-10 12:42:33 UTC",
    "updated_date": "2025-04-14 12:52:24 UTC"
  },
  {
    "arxiv_id": "2412.07472v3",
    "title": "SmartAgent: Chain-of-User-Thought for Embodied Personalized Agent in Cyber World",
    "authors": [
      "Jiaqi Zhang",
      "Chen Gao",
      "Liyuan Zhang",
      "Yong Li",
      "Hongzhi Yin"
    ],
    "abstract": "Recent advances in embodied agents with multimodal perception and reasoning\ncapabilities based on large vision-language models (LVLMs), excel in\nautonomously interacting either real or cyber worlds, helping people make\nintelligent decisions in complex environments. However, the current works are\nnormally optimized by golden action trajectories or ideal task-oriented\nsolutions toward a definitive goal. This paradigm considers limited\nuser-oriented factors, which could be the reason for their performance\nreduction in a wide range of personal assistant applications. To address this,\nwe propose Chain-of-User-Thought (COUT), a novel embodied reasoning paradigm\nthat takes a chain of thought from basic action thinking to explicit and\nimplicit personalized preference thought to incorporate personalized factors\ninto autonomous agent learning. To target COUT, we introduce SmartAgent, an\nagent framework perceiving cyber environments and reasoning personalized\nrequirements as 1) interacting with GUI to access an item pool, 2) generating\nusers' explicit requirements implied by previous actions, and 3) recommending\nitems to fulfill users' implicit requirements. To demonstrate SmartAgent's\ncapabilities, we also create a brand-new dataset SmartSpot that offers a\nfull-stage personalized action-involved environment. To our best knowledge, our\nwork is the first to formulate the COUT process, serving as a preliminary\nattempt towards embodied personalized agent learning. Our extensive experiments\non SmartSpot illuminate SmartAgent's functionality among a series of embodied\nand personalized sub-tasks. We will release code and data upon paper\nnotification at https://github.com/tsinghua-fib-lab/SmartAgent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07472v3",
    "published_date": "2024-12-10 12:40:35 UTC",
    "updated_date": "2025-02-18 09:04:16 UTC"
  },
  {
    "arxiv_id": "2412.12146v1",
    "title": "Generative Modeling and Data Augmentation for Power System Production Simulation",
    "authors": [
      "Linna Xu",
      "Yongli Zhu"
    ],
    "abstract": "As a key component of power system production simulation, load forecasting is\ncritical for the stable operation of power systems. Machine learning methods\nprevail in this field. However, the limited training data can be a challenge.\nThis paper proposes a generative model-assisted approach for load forecasting\nunder small sample scenarios, consisting of two steps: expanding the dataset\nusing a diffusion-based generative model and then training various machine\nlearning regressors on the augmented dataset to identify the best performer.\nThe expanded dataset significantly reduces forecasting errors compared to the\noriginal dataset, and the diffusion model outperforms the generative\nadversarial model by achieving about 200 times smaller errors and better\nalignment in latent data distributions.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "This paper has been accepted by D3S3: Data-driven and Differentiable\n  Simulations, Surrogates, and Solvers at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.12146v1",
    "published_date": "2024-12-10 12:38:47 UTC",
    "updated_date": "2024-12-10 12:38:47 UTC"
  },
  {
    "arxiv_id": "2412.07454v2",
    "title": "Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning",
    "authors": [
      "Kichang Lee",
      "Jaeho Jin",
      "JaeYeon Park",
      "Songkuk Kim",
      "JeongGil Ko"
    ],
    "abstract": "Federated learning enables decentralized model training without sharing raw\ndata, preserving data privacy. However, its vulnerability towards critical\nsecurity threats, such as gradient inversion and model poisoning by malicious\nclients, remain unresolved. Existing solutions often address these issues\nseparately, sacrificing either system robustness or model accuracy. This work\nintroduces Tazza, a secure and efficient federated learning framework that\nsimultaneously addresses both challenges. By leveraging the permutation\nequivariance and invariance properties of neural networks via weight shuffling\nand shuffled model validation, Tazza enhances resilience against diverse\npoisoning attacks, while ensuring data confidentiality and high model accuracy.\nComprehensive evaluations on various datasets and embedded platforms show that\nTazza achieves robust defense with up to 6.7x improved computational efficiency\ncompared to alternative schemes, without compromising performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07454v2",
    "published_date": "2024-12-10 12:20:42 UTC",
    "updated_date": "2025-02-03 17:23:32 UTC"
  },
  {
    "arxiv_id": "2412.07819v2",
    "title": "Intelligent System for Automated Molecular Patent Infringement Assessment",
    "authors": [
      "Yaorui Shi",
      "Sihang Li",
      "Taiyan Zhang",
      "Xi Fang",
      "Jiankun Wang",
      "Zhiyuan Liu",
      "Guojiang Zhao",
      "Zhengdan Zhu",
      "Zhifeng Gao",
      "Renxin Zhong",
      "Linfeng Zhang",
      "Guolin Ke",
      "Weinan E",
      "Hengxing Cai",
      "Xiang Wang"
    ],
    "abstract": "Automated drug discovery offers significant potential for accelerating the\ndevelopment of novel therapeutics by substituting labor-intensive human\nworkflows with machine-driven processes. However, molecules generated by\nartificial intelligence may unintentionally infringe on existing patents,\nposing legal and financial risks that impede the full automation of drug\ndiscovery pipelines. This paper introduces PatentFinder, a novel multi-agent\nand tool-enhanced intelligence system that can accurately and comprehensively\nevaluate small molecules for patent infringement. PatentFinder features five\nspecialized agents that collaboratively analyze patent claims and molecular\nstructures with heuristic and model-based tools, generating interpretable\ninfringement reports. To support systematic evaluation, we curate\nMolPatent-240, a benchmark dataset tailored for patent infringement assessment\nalgorithms. On this benchmark, PatentFinder outperforms baseline methods that\nrely solely on large language models or specialized chemical tools, achieving a\n13.8% improvement in F1-score and a 12% increase in accuracy. Additionally,\nPatentFinder autonomously generates detailed and interpretable patent\ninfringement reports, showcasing enhanced accuracy and improved\ninterpretability. The high accuracy and interpretability of PatentFinder make\nit a valuable and reliable tool for automating patent infringement assessments,\noffering a practical solution for integrating patent protection analysis into\nthe drug discovery pipeline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07819v2",
    "published_date": "2024-12-10 12:14:38 UTC",
    "updated_date": "2025-01-13 03:03:57 UTC"
  },
  {
    "arxiv_id": "2412.07448v1",
    "title": "Dynamic Ensemble Reasoning for LLM Experts",
    "authors": [
      "Jinwu Hu",
      "Yufeng Wang",
      "Shuhai Zhang",
      "Kai Zhou",
      "Guohao Chen",
      "Yu Hu",
      "Bin Xiao",
      "Mingkui Tan"
    ],
    "abstract": "Ensemble reasoning for the strengths of different LLM experts is critical to\nachieving consistent and satisfactory performance on diverse inputs across a\nwide range of tasks. However, existing LLM ensemble methods are either\ncomputationally intensive or incapable of leveraging complementary knowledge\namong LLM experts for various inputs. In this paper, we propose a Dynamic\nEnsemble Reasoning paradigm, called DER to integrate the strengths of multiple\nLLM experts conditioned on dynamic inputs. Specifically, we model the LLM\nensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent\nsequentially takes inputs to request knowledge from an LLM candidate and passes\nthe output to a subsequent LLM candidate. Moreover, we devise a reward function\nto train a DER-Agent to dynamically select an optimal answering route given the\ninput questions, aiming to achieve the highest performance with as few\ncomputational resources as possible. Last, to fully transfer the expert\nknowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP)\nthat enables the subsequent LLM candidates to transfer complementary knowledge\neffectively. Experiments demonstrate that our method uses fewer computational\nresources to achieve better performance compared to state-of-the-art baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.07448v1",
    "published_date": "2024-12-10 12:05:56 UTC",
    "updated_date": "2024-12-10 12:05:56 UTC"
  },
  {
    "arxiv_id": "2412.07446v3",
    "title": "A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment",
    "authors": [
      "Raanan Y. Rohekar",
      "Yaniv Gurwicz",
      "Sungduk Yu",
      "Estelle Aflalo",
      "Vasudev Lal"
    ],
    "abstract": "Do generative pre-trained transformer (GPT) models, trained only to predict\nthe next token, implicitly learn a world model from which a sequence is\ngenerated one token at a time? We address this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that GPT\nmodels, at inference time, can be utilized for zero-shot causal structure\nlearning for input sequences and present a confidence score. Empirical\nevaluation is conducted in a controlled environment using the setup and rules\nof the Othello and Chess strategy games. A GPT, pre-trained on real-world games\nplayed with the intention of winning, is tested on out-of-distribution\nsynthetic data consisting of sequences of random legal moves. We find that the\nGPT model is likely to generate legal next moves for out-of-distribution\nsequences for which a causal structure is encoded in the attention mechanism\nwith high confidence. In cases for which the GPT model generates illegal moves\nit also fails to capture any causal structure.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "International Conference on Machine Learning (ICML), 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07446v3",
    "published_date": "2024-12-10 12:05:03 UTC",
    "updated_date": "2025-05-02 11:32:37 UTC"
  },
  {
    "arxiv_id": "2412.07818v1",
    "title": "FastDDS-Based Middleware System for Remote X-Ray Image Classification Using Raspberry Pi",
    "authors": [
      "Omar H. Khater",
      "Basem Almadani",
      "Farouq Aliyu"
    ],
    "abstract": "Internet of Things (IoT) based healthcare systems offer significant potential\nfor improving the delivery of healthcare services in humanitarian engineering,\nproviding essential healthcare services to millions of underserved people in\nremote areas worldwide. However, these areas have poor network infrastructure,\nmaking communications difficult for traditional IoT. This paper presents a\nreal-time chest X-ray classification system for hospitals in remote areas using\nFastDDS real-time middleware, offering reliable real-time communication. We\nfine-tuned a ResNet50 neural network to an accuracy of 88.61%, a precision of\n88.76%, and a recall of 88.49\\%. Our system results mark an average throughput\nof 3.2 KB/s and an average latency of 65 ms. The proposed system demonstrates\nhow middleware-based systems can assist doctors in remote locations.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07818v1",
    "published_date": "2024-12-10 12:01:52 UTC",
    "updated_date": "2024-12-10 12:01:52 UTC"
  },
  {
    "arxiv_id": "2412.07441v1",
    "title": "Reconstructing Deep Neural Networks: Unleashing the Optimization Potential of Natural Gradient Descent",
    "authors": [
      "Weihua Liu",
      "Said Boumaraf",
      "Jianwu Li",
      "Chaochao Lin",
      "Xiabi Liu",
      "Lijuan Niu",
      "Naoufel Werghi"
    ],
    "abstract": "Natural gradient descent (NGD) is a powerful optimization technique for\nmachine learning, but the computational complexity of the inverse Fisher\ninformation matrix limits its application in training deep neural networks. To\novercome this challenge, we propose a novel optimization method for training\ndeep neural networks called structured natural gradient descent (SNGD).\nTheoretically, we demonstrate that optimizing the original network using NGD is\nequivalent to using fast gradient descent (GD) to optimize the reconstructed\nnetwork with a structural transformation of the parameter matrix. Thereby, we\ndecompose the calculation of the global Fisher information matrix into the\nefficient computation of local Fisher matrices via constructing local Fisher\nlayers in the reconstructed network to speed up the training. Experimental\nresults on various deep networks and datasets demonstrate that SNGD achieves\nfaster convergence speed than NGD while retaining comparable solutions.\nFurthermore, our method outperforms traditional GDs in terms of efficiency and\neffectiveness. Thus, our proposed method has the potential to significantly\nimprove the scalability and efficiency of NGD in deep learning applications.\nOur source code is available at https://github.com/Chaochao-Lin/SNGD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07441v1",
    "published_date": "2024-12-10 11:57:47 UTC",
    "updated_date": "2024-12-10 11:57:47 UTC"
  },
  {
    "arxiv_id": "2412.07431v1",
    "title": "BENet: A Cross-domain Robust Network for Detecting Face Forgeries via Bias Expansion and Latent-space Attention",
    "authors": [
      "Weihua Liu",
      "Jianhua Qiu",
      "Said Boumaraf",
      "Chaochao lin",
      "Pan liyuan",
      "Lin Li",
      "Mohammed Bennamoun",
      "Naoufel Werghi"
    ],
    "abstract": "In response to the growing threat of deepfake technology, we introduce BENet,\na Cross-Domain Robust Bias Expansion Network. BENet enhances the detection of\nfake faces by addressing limitations in current detectors related to variations\nacross different types of fake face generation techniques, where\n``cross-domain\" refers to the diverse range of these deepfakes, each considered\na separate domain. BENet's core feature is a bias expansion module based on\nautoencoders. This module maintains genuine facial features while enhancing\ndifferences in fake reconstructions, creating a reliable bias for detecting\nfake faces across various deepfake domains. We also introduce a Latent-Space\nAttention (LSA) module to capture inconsistencies related to fake faces at\ndifferent scales, ensuring robust defense against advanced deepfake techniques.\nThe enriched LSA feature maps are multiplied with the expanded bias to create a\nversatile feature space optimized for subtle forgeries detection. To improve\nits ability to detect fake faces from unknown sources, BENet integrates a\ncross-domain detector module that enhances recognition accuracy by verifying\nthe facial domain during inference. We train our network end-to-end with a\nnovel bias expansion loss, adopted for the first time, in face forgery\ndetection. Extensive experiments covering both intra and cross-dataset\ndemonstrate BENet's superiority over current state-of-the-art solutions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07431v1",
    "published_date": "2024-12-10 11:41:55 UTC",
    "updated_date": "2024-12-10 11:41:55 UTC"
  },
  {
    "arxiv_id": "2412.07430v2",
    "title": "Knowledge Graph Guided Evaluation of Abstention Techniques",
    "authors": [
      "Kinshuk Vasisht",
      "Navreet Kaur",
      "Danish Pruthi"
    ],
    "abstract": "To deploy language models safely, it is crucial that they abstain from\nresponding to inappropriate requests. Several prior studies test the safety\npromises of models based on their effectiveness in blocking malicious requests.\nIn this work, we focus on evaluating the underlying techniques that cause\nmodels to abstain. We create SELECT, a benchmark derived from a set of benign\nconcepts (e.g., \"rivers\") from a knowledge graph. Focusing on benign concepts\nisolates the effect of safety training, and grounding these concepts in a\nknowledge graph allows us to study the generalization and specificity of\nabstention techniques. Using SELECT, we benchmark different abstention\ntechniques over six open-weight and closed-source models. We find that the\nexamined techniques indeed cause models to abstain with over $80\\%$ abstention\nrates. However, these techniques are not as effective for descendants of the\ntarget concepts, where abstention rates drop by $19\\%$. We also characterize\nthe generalization-specificity trade-offs for different techniques. Overall, no\nsingle technique is invariably better than others, and our findings inform\npractitioners of the various trade-offs involved.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07430v2",
    "published_date": "2024-12-10 11:40:47 UTC",
    "updated_date": "2025-02-08 19:36:28 UTC"
  },
  {
    "arxiv_id": "2412.07429v1",
    "title": "Optimizing Alignment with Less: Leveraging Data Augmentation for Personalized Evaluation",
    "authors": [
      "Javad Seraj",
      "Mohammad Mahdi Mohajeri",
      "Mohammad Javad Dousti",
      "Majid Nili Ahmadabadi"
    ],
    "abstract": "Automatic evaluation by large language models (LLMs) is a prominent topic\ntoday; however, judgment and evaluation tasks are often subjective and\ninfluenced by various factors, making adaptation challenging. While many\nstudies demonstrate the capabilities of state-of-the-art proprietary LLMs in\ncomparison to human evaluators, they often struggle to adapt to reference\nevaluators over time, a requirement for achieving personalized judgment.\nAdditionally, numerous works have attempted to apply open LLMs as judges or\nevaluators, but these efforts frequently overlook the limitations of working\nwith scarce data. Personalized judgment is inherently associated with limited\ndata scenarios, which are common in many real-world problems. Our work aims to\npresent a data augmentation technique to select a more effective sample from\nlimited data in order to align an open LLM with human preference. Our work\nachieves approximately 7% improvements in Pearson correlation with a reference\njudge over the baseline,and 30% improvement over the base model\n(Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task.\ndemonstrating that augmenting selecting more effective preference data enables\nour approach to surpass baseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07429v1",
    "published_date": "2024-12-10 11:40:11 UTC",
    "updated_date": "2024-12-10 11:40:11 UTC"
  },
  {
    "arxiv_id": "2412.07412v1",
    "title": "Generating Knowledge Graphs from Large Language Models: A Comparative Study of GPT-4, LLaMA 2, and BERT",
    "authors": [
      "Ahan Bhatt",
      "Nandan Vaghela",
      "Kush Dudhia"
    ],
    "abstract": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.07412v1",
    "published_date": "2024-12-10 11:05:26 UTC",
    "updated_date": "2024-12-10 11:05:26 UTC"
  },
  {
    "arxiv_id": "2412.07411v1",
    "title": "DSFEC: Efficient and Deployable Deep Radar Object Detection",
    "authors": [
      "Gayathri Dandugula",
      "Santhosh Boddana",
      "Sudesh Mirashi"
    ],
    "abstract": "Deploying radar object detection models on resource-constrained edge devices\nlike the Raspberry Pi poses significant challenges due to the large size of the\nmodel and the limited computational power and the memory of the Pi. In this\nwork, we explore the efficiency of Depthwise Separable Convolutions in radar\nobject detection networks and integrate them into our model. Additionally, we\nintroduce a novel Feature Enhancement and Compression (FEC) module to the\nPointPillars feature encoder to further improve the model performance. With\nthese innovations, we propose the DSFEC-L model and its two versions, which\noutperform the baseline (23.9 mAP of Car class, 20.72 GFLOPs) on nuScenes\ndataset: 1). An efficient DSFEC-M model with a 14.6% performance improvement\nand a 60% reduction in GFLOPs. 2). A deployable DSFEC-S model with a 3.76%\nperformance improvement and a remarkable 78.5% reduction in GFLOPs. Despite\nmarginal performance gains, our deployable model achieves an impressive 74.5%\nreduction in runtime on the Raspberry Pi compared to the baseline.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07411v1",
    "published_date": "2024-12-10 11:03:51 UTC",
    "updated_date": "2024-12-10 11:03:51 UTC"
  },
  {
    "arxiv_id": "2412.10422v3",
    "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework",
    "authors": [
      "Meihao Fan",
      "Ju Fan",
      "Nan Tang",
      "Lei Cao",
      "Guoliang Li",
      "Xiaoyong Du"
    ],
    "abstract": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation...",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10422v3",
    "published_date": "2024-12-10 11:03:49 UTC",
    "updated_date": "2025-05-02 00:11:48 UTC"
  },
  {
    "arxiv_id": "2412.07408v1",
    "title": "Explainability of Deep Learning-Based Plant Disease Classifiers Through Automated Concept Identification",
    "authors": [
      "Jihen Amara",
      "Birgitta König-Ries",
      "Sheeba Samuel"
    ],
    "abstract": "While deep learning has significantly advanced automatic plant disease\ndetection through image-based classification, improving model explainability\nremains crucial for reliable disease detection. In this study, we apply the\nAutomated Concept-based Explanation (ACE) method to plant disease\nclassification using the widely adopted InceptionV3 model and the PlantVillage\ndataset. ACE automatically identifies the visual concepts found in the image\ndata and provides insights about the critical features influencing the model\npredictions. This approach reveals both effective disease-related patterns and\nincidental biases, such as those from background or lighting that can\ncompromise model robustness. Through systematic experiments, ACE helped us to\nidentify relevant features and pinpoint areas for targeted model improvement.\nOur findings demonstrate the potential of ACE to improve the explainability of\nplant disease classification based on deep learning, which is essential for\nproducing transparent tools for plant disease management in agriculture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07408v1",
    "published_date": "2024-12-10 10:59:43 UTC",
    "updated_date": "2024-12-10 10:59:43 UTC"
  },
  {
    "arxiv_id": "2412.07405v1",
    "title": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning",
    "authors": [
      "Yufei Ma",
      "Zihan Liang",
      "Huangyu Dai",
      "Ben Chen",
      "Dehong Gao",
      "Zhuoran Ran",
      "Wang Zihan",
      "Linbo Jin",
      "Wen Jiang",
      "Guannan Zhang",
      "Xiaoyan Cai",
      "Libin Yang"
    ],
    "abstract": "The growing demand for larger-scale models in the development of\n\\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels (LLMs) poses challenges for\nefficient training within limited computational resources. Traditional\nfine-tuning methods often exhibit instability in multi-task learning and rely\nheavily on extensive training resources. Here, we propose MoDULA\n(\\textbf{M}ixture \\textbf{o}f \\textbf{D}omain-Specific and \\textbf{U}niversal\n\\textbf{L}oR\\textbf{A}), a novel \\textbf{P}arameter \\textbf{E}fficient\n\\textbf{F}ine-\\textbf{T}uning (PEFT)\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xpert (MoE) paradigm for improved\nfine-tuning and parameter efficiency in multi-task learning. The paradigm\neffectively improves the multi-task capability of the model by training\nuniversal experts, domain-specific experts, and routers separately. MoDULA-Res\nis a new method within the MoDULA paradigm, which maintains the model's general\ncapability by connecting universal and task-specific experts through residual\nconnections. The experimental results demonstrate that the overall performance\nof the MoDULA-Flan and MoDULA-Res methods surpasses that of existing\nfine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more\nsignificant performance improvements in multiple tasks while reducing training\ncosts by over 80\\% without losing general capability. Moreover, MoDULA displays\nflexible pluggability, allowing for the efficient addition of new tasks without\nretraining existing experts from scratch. This progressive training paradigm\ncircumvents data balancing issues, enhancing training efficiency and model\nstability. Overall, MoDULA provides a scalable, cost-effective solution for\nfine-tuning LLMs with enhanced parameter efficiency and generalization\ncapability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07405v1",
    "published_date": "2024-12-10 10:55:57 UTC",
    "updated_date": "2024-12-10 10:55:57 UTC"
  },
  {
    "arxiv_id": "2412.07402v1",
    "title": "Non-Progressive Influence Maximization in Dynamic Social Networks",
    "authors": [
      "Yunming Hui",
      "Shihan Wang",
      "Melisachew Wudage Chekol",
      "Stevan Rudinac",
      "Inez Maria Zwetsloot"
    ],
    "abstract": "The influence maximization (IM) problem involves identifying a set of key\nindividuals in a social network who can maximize the spread of influence\nthrough their network connections. With the advent of geometric deep learning\non graphs, great progress has been made towards better solutions for the IM\nproblem. In this paper, we focus on the dynamic non-progressive IM problem,\nwhich considers the dynamic nature of real-world social networks and the\nspecial case where the influence diffusion is non-progressive, i.e., nodes can\nbe activated multiple times. We first extend an existing diffusion model to\ncapture the non-progressive influence propagation in dynamic social networks.\nWe then propose the method, DNIMRL, which employs deep reinforcement learning\nand dynamic graph embedding to solve the dynamic non-progressive IM problem. In\nparticular, we propose a novel algorithm that effectively leverages graph\nembedding to capture the temporal changes of dynamic networks and seamlessly\nintegrates with deep reinforcement learning. The experiments, on different\ntypes of real-world social network datasets, demonstrate that our method\noutperforms state-of-the-art baselines.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07402v1",
    "published_date": "2024-12-10 10:52:32 UTC",
    "updated_date": "2024-12-10 10:52:32 UTC"
  },
  {
    "arxiv_id": "2501.14735v1",
    "title": "ARCEAK: An Automated Rule Checking Framework Enhanced with Architectural Knowledge",
    "authors": [
      "Junyong Chen",
      "Ling-I Wu",
      "Minyu Chen",
      "Xiaoying Qian",
      "Haoze Zhu",
      "Qiongfang Zhang",
      "Guoqiang Li"
    ],
    "abstract": "Automated Rule Checking (ARC) plays a crucial role in advancing the\nconstruction industry by addressing the laborious, inconsistent, and\nerror-prone nature of traditional model review conducted by industry\nprofessionals. Manual assessment against intricate sets of rules often leads to\nsignificant project delays and expenses. In response to these challenges, ARC\noffers a promising solution to improve efficiency and compliance in design\nwithin the construction sector. However, the main challenge of ARC lies in\ntranslating regulatory text into a format suitable for computer processing.\nCurrent methods for rule interpretation require extensive manual labor, thereby\nlimiting their practicality. To address this issue, our study introduces a\nnovel approach that decomposes ARC into two distinct tasks: rule information\nextraction and verification code generation. Leveraging generative pre-trained\ntransformers, our method aims to streamline the interpretation of regulatory\ntexts and simplify the process of generating model compliance checking code.\nThrough empirical evaluation and case studies, we showcase the effectiveness\nand potential of our approach in automating code compliance checking, enhancing\nthe efficiency and reliability of construction projects.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14735v1",
    "published_date": "2024-12-10 10:37:11 UTC",
    "updated_date": "2024-12-10 10:37:11 UTC"
  },
  {
    "arxiv_id": "2412.07393v1",
    "title": "CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models",
    "authors": [
      "Dongfang Li",
      "Zetian Sun",
      "Xinshuo Hu",
      "Baotian Hu",
      "Min Zhang"
    ],
    "abstract": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025; Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2412.07393v1",
    "published_date": "2024-12-10 10:35:19 UTC",
    "updated_date": "2024-12-10 10:35:19 UTC"
  },
  {
    "arxiv_id": "2412.07388v1",
    "title": "A Review of Challenges in Speech-based Conversational AI for Elderly Care",
    "authors": [
      "Willemijn Klaassen",
      "Bram van Dijk",
      "Marco Spruit"
    ],
    "abstract": "Artificially intelligent systems optimized for speech conversation are\nappearing at a fast pace. Such models are interesting from a healthcare\nperspective, as these voice-controlled assistants may support the elderly and\nenable remote health monitoring. The bottleneck for efficacy, however, is how\nwell these devices work in practice and how the elderly experience them, but\nresearch on this topic is scant. We review elderly use of voice-controlled AI\nand highlight various user- and technology-centered issues, that need to be\nconsidered before effective speech-controlled AI for elderly care can be\nrealized.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at Medical Informatics Europe 2025\n  conference, Glasgow. 5 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2412.07388v1",
    "published_date": "2024-12-10 10:32:22 UTC",
    "updated_date": "2024-12-10 10:32:22 UTC"
  },
  {
    "arxiv_id": "2412.07387v1",
    "title": "Enhanced MRI Representation via Cross-series Masking",
    "authors": [
      "Churan Wang",
      "Fei Gao",
      "Lijun Yan",
      "Siwen Wang",
      "Yizhou Yu",
      "Yizhou Wang"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is indispensable for diagnosing and planning\ntreatment in various medical conditions due to its ability to produce\nmulti-series images that reveal different tissue characteristics. However,\nintegrating these diverse series to form a coherent analysis presents\nsignificant challenges, such as differing spatial resolutions and contrast\npatterns meanwhile requiring extensive annotated data, which is scarce in\nclinical practice. Due to these issues, we introduce a novel Cross-Series\nMasking (CSM) Strategy for effectively learning MRI representation in a\nself-supervised manner. Specifically, CSM commences by randomly sampling a\nsubset of regions and series, which are then strategically masked. In the\ntraining process, the cross-series representation is learned by utilizing the\nunmasked data to reconstruct the masked portions. This process not only\nintegrates information across different series but also facilitates the ability\nto model both intra-series and inter-series correlations and complementarities.\nWith the learned representation, the downstream tasks like segmentation and\nclassification are also enhanced. Taking brain tissue segmentation, breast\ntumor benign/malignant classification, and prostate cancer diagnosis as\nexamples, our method achieves state-of-the-art performance on both public and\nin-house datasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07387v1",
    "published_date": "2024-12-10 10:32:09 UTC",
    "updated_date": "2024-12-10 10:32:09 UTC"
  },
  {
    "arxiv_id": "2412.07380v2",
    "title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction",
    "authors": [
      "Bo Lv",
      "Chen Tang",
      "Yanan Zhang",
      "Xin Liu",
      "Yue Yu",
      "Ping Luo"
    ],
    "abstract": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07380v2",
    "published_date": "2024-12-10 10:27:41 UTC",
    "updated_date": "2025-02-19 09:01:59 UTC"
  },
  {
    "arxiv_id": "2412.12145v4",
    "title": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Junqi Tong",
      "Min Liu",
      "Qi Li"
    ],
    "abstract": "Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Our study requires further in-depth research to ensure the\n  comprehensiveness and adequacy of the methodology",
    "pdf_url": "http://arxiv.org/pdf/2412.12145v4",
    "published_date": "2024-12-10 10:14:03 UTC",
    "updated_date": "2025-02-22 07:36:26 UTC"
  },
  {
    "arxiv_id": "2412.07338v3",
    "title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation",
    "authors": [
      "Lorenzo Cima",
      "Alessio Miaschi",
      "Amaury Trujillo",
      "Marco Avvenuti",
      "Felice Dell'Orletta",
      "Stefano Cresci"
    ],
    "abstract": "AI-generated counterspeech offers a promising and scalable strategy to curb\nonline toxicity through direct replies that promote civil discourse. However,\ncurrent counterspeech is one-size-fits-all, lacking adaptation to the\nmoderation context and the users involved. We propose and evaluate multiple\nstrategies for generating tailored counterspeech that is adapted to the\nmoderation context and personalized for the moderated user. We instruct an\nLLaMA2-13B model to generate counterspeech, experimenting with various\nconfigurations based on different contextual information and fine-tuning\nstrategies. We identify the configurations that generate persuasive\ncounterspeech through a combination of quantitative indicators and human\nevaluations collected via a pre-registered mixed-design crowdsourcing\nexperiment. Results show that contextualized counterspeech can significantly\noutperform state-of-the-art generic counterspeech in adequacy and\npersuasiveness, without compromising other characteristics. Our findings also\nreveal a poor correlation between quantitative indicators and human\nevaluations, suggesting that these methods assess different aspects and\nhighlighting the need for nuanced evaluation methodologies. The effectiveness\nof contextualized AI-generated counterspeech and the divergence between human\nand algorithmic evaluations underscore the importance of increased human-AI\ncollaboration in content moderation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "Article published in WebConf 25, 34th ACM Web Conference. Please,\n  cite the published version",
    "pdf_url": "http://arxiv.org/pdf/2412.07338v3",
    "published_date": "2024-12-10 09:29:52 UTC",
    "updated_date": "2025-02-07 10:30:23 UTC"
  },
  {
    "arxiv_id": "2412.07333v1",
    "title": "Fusion Embedding for Pose-Guided Person Image Synthesis with Diffusion Model",
    "authors": [
      "Donghwna Lee",
      "Kyungha Min",
      "Kirok Kim",
      "Seyoung Jeong",
      "Jiwoo Jeong",
      "Wooju Kim"
    ],
    "abstract": "Pose-Guided Person Image Synthesis (PGPIS) aims to synthesize high-quality\nperson images corresponding to target poses while preserving the appearance of\nthe source image. Recently, PGPIS methods that use diffusion models have\nachieved competitive performance. Most approaches involve extracting\nrepresentations of the target pose and source image and learning their\nrelationships in the generative model's training process. This approach makes\nit difficult to learn the semantic relationships between the input and target\nimages and complicates the model structure needed to enhance generation\nresults. To address these issues, we propose Fusion embedding for PGPIS using a\nDiffusion Model (FPDM). Inspired by the successful application of pre-trained\nCLIP models in text-to-image diffusion models, our method consists of two\nstages. The first stage involves training the fusion embedding of the source\nimage and target pose to align with the target image's embedding. In the second\nstage, the generative model uses this fusion embedding as a condition to\ngenerate the target image. We applied the proposed method to the benchmark\ndatasets DeepFashion and RWTH-PHOENIX-Weather 2014T, and conducted both\nquantitative and qualitative evaluations, demonstrating state-of-the-art (SOTA)\nperformance. An ablation study of the model structure showed that even a model\nusing only the second stage achieved performance close to the other PGPIS SOTA\nmodels. The code is available at https://github.com/dhlee-work/FPDM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07333v1",
    "published_date": "2024-12-10 09:25:01 UTC",
    "updated_date": "2024-12-10 09:25:01 UTC"
  },
  {
    "arxiv_id": "2412.07331v1",
    "title": "NeSyA: Neurosymbolic Automata",
    "authors": [
      "Nikolaos Manginas",
      "George Paliouras",
      "Luc De Raedt"
    ],
    "abstract": "Neurosymbolic Artificial Intelligence (NeSy) has emerged as a promising\ndirection to integrate low level perception with high level reasoning.\nUnfortunately, little attention has been given to developing NeSy systems\ntailored to temporal/sequential problems. This entails reasoning symbolically\nover sequences of subsymbolic observations towards a target prediction. We show\nthat using a probabilistic semantics symbolic automata, which combine the power\nof automata for temporal structure specification with that of propositional\nlogic, can be used to reason efficiently and differentiably over subsymbolic\nsequences. The proposed system, which we call NeSyA (Neuro Symbolic Automata),\nis shown to either scale or perform better than existing NeSy approaches when\napplied to problems with a temporal component.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07331v1",
    "published_date": "2024-12-10 09:23:36 UTC",
    "updated_date": "2024-12-10 09:23:36 UTC"
  },
  {
    "arxiv_id": "2412.12144v3",
    "title": "Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models",
    "authors": [
      "Chang-Jin Li",
      "Jiyuan Zhang",
      "Yun Tang",
      "Jian Li"
    ],
    "abstract": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.1; J.4"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to Psychological Methods. 56 pages (main text), 12 pages\n  (appendix), and 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12144v3",
    "published_date": "2024-12-10 09:13:32 UTC",
    "updated_date": "2025-04-16 15:53:03 UTC"
  },
  {
    "arxiv_id": "2412.07289v2",
    "title": "Enhancing Relation Extraction via Supervised Rationale Verification and Feedback",
    "authors": [
      "Yongqi Li",
      "Xin Miao",
      "Shen Zhou",
      "Mayi Xu",
      "Yuyang Ren",
      "Tieyun Qian"
    ],
    "abstract": "Despite the rapid progress that existing automated feedback methods have made\nin correcting the output of large language models (LLMs), these methods cannot\nbe well applied to the relation extraction (RE) task due to their designated\nfeedback objectives and correction manner. To address this problem, we propose\na novel automated feedback framework for RE, which presents a rationale\nsupervisor to verify the rationale and provides re-selected demonstrations as\nfeedback to correct the initial prediction. Specifically, we first design a\ncausal intervention and observation method to collect biased/unbiased\nrationales for contrastive training the rationale supervisor. Then, we present\na verification-feedback-correction procedure to iteratively enhance LLMs'\ncapability of handling the RE task. Extensive experiments prove that our\nproposed framework significantly outperforms existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AAAI 2025, camera ready version",
    "pdf_url": "http://arxiv.org/pdf/2412.07289v2",
    "published_date": "2024-12-10 08:18:29 UTC",
    "updated_date": "2024-12-11 02:31:45 UTC"
  },
  {
    "arxiv_id": "2412.07282v1",
    "title": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass",
    "authors": [
      "Romain Storaï",
      "Seung-won Hwang"
    ],
    "abstract": "This paper aims to improve the performance of large language models by\naddressing the variable computational demands in inference steps, where some\ntokens require more computational resources than others. We present HARP, a\nsimple modification to \"off-the-shelf\" Transformer forward pass. Drawing from\nhesitation and the framing effect in decision-making, HARP selectively applies\nadditional computation when the model encounters uncertainty during token\ngeneration. Our method mimics human cognitive processes by pausing at difficult\ndecision points and reframing inputs for a different perspective. Unlike other\napproaches, HARP is model-agnostic, training-free, and easy to implement. We\nthoroughly evaluate our method across various downstream tasks and model sizes,\ndemonstrating performance improvements up to +5.16%. Notably, HARP achieves\nthese gains while maintaining inference times twice faster than beam search.\nSimple and yet with significant gains, HARP offers a practical solution for\nenhancing the performance of Transformer-based language models with minimal\ncomputational impact.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07282v1",
    "published_date": "2024-12-10 08:12:22 UTC",
    "updated_date": "2024-12-10 08:12:22 UTC"
  },
  {
    "arxiv_id": "2412.07278v1",
    "title": "Superficial Consciousness Hypothesis for Autoregressive Transformers",
    "authors": [
      "Yosuke Miyanishi",
      "Keita Mitani"
    ],
    "abstract": "The alignment between human objectives and machine learning models built on\nthese objectives is a crucial yet challenging problem for achieving Trustworthy\nAI, particularly when preparing for superintelligence (SI). First, given that\nSI does not exist today, empirical analysis for direct evidence is difficult.\nSecond, SI is assumed to be more intelligent than humans, capable of deceiving\nus into underestimating its intelligence, making output-based analysis\nunreliable. Lastly, what kind of unexpected property SI might have is still\nunclear. To address these challenges, we propose the Superficial Consciousness\nHypothesis under Information Integration Theory (IIT), suggesting that SI could\nexhibit a complex information-theoretic state like a conscious agent while\nunconscious. To validate this, we use a hypothetical scenario where SI can\nupdate its parameters \"at will\" to achieve its own objective (mesa-objective)\nunder the constraint of the human objective (base objective). We show that a\npractical estimate of IIT's consciousness metric is relevant to the widely used\nperplexity metric, and train GPT-2 with those two objectives. Our preliminary\nresult suggests that this SI-simulating GPT-2 could simultaneously follow the\ntwo objectives, supporting the feasibility of the Superficial Consciousness\nHypothesis.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to PSS Workshop at AAAI25",
    "pdf_url": "http://arxiv.org/pdf/2412.07278v1",
    "published_date": "2024-12-10 08:08:17 UTC",
    "updated_date": "2024-12-10 08:08:17 UTC"
  },
  {
    "arxiv_id": "2412.07273v2",
    "title": "Temporal-Aware Evaluation and Learning for Temporal Graph Neural Networks",
    "authors": [
      "Junwei Su",
      "Shan Wu"
    ],
    "abstract": "Temporal Graph Neural Networks (TGNNs) are a family of graph neural networks\ndesigned to model and learn dynamic information from temporal graphs. Given\ntheir substantial empirical success, there is an escalating interest in TGNNs\nwithin the research community. However, the majority of these efforts have been\nchannelled towards algorithm and system design, with the evaluation metrics\nreceiving comparatively less attention. Effective evaluation metrics are\ncrucial for providing detailed performance insights, particularly in the\ntemporal domain. This paper investigates the commonly used evaluation metrics\nfor TGNNs and illustrates the failure mechanisms of these metrics in capturing\nessential temporal structures in the predictive behaviour of TGNNs. We provide\na mathematical formulation of existing performance metrics and utilize an\ninstance-based study to underscore their inadequacies in identifying volatility\nclustering (the occurrence of emerging errors within a brief interval). This\nphenomenon has profound implications for both algorithm and system design in\nthe temporal domain. To address this deficiency, we introduce a new\nvolatility-aware evaluation metric (termed volatility cluster statistics),\ndesigned for a more refined analysis of model temporal performance.\nAdditionally, we demonstrate how this metric can serve as a\ntemporal-volatility-aware training objective to alleviate the clustering of\ntemporal errors. Through comprehensive experiments on various TGNN models, we\nvalidate our analysis and the proposed approach. The empirical results offer\nrevealing insights: 1) existing TGNNs are prone to making errors with\nvolatility clustering, and 2) TGNNs with different mechanisms to capture\ntemporal information exhibit distinct volatility clustering patterns. Our\nempirical findings demonstrate that our proposed training objective effectively\nreduces volatility clusters in error.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07273v2",
    "published_date": "2024-12-10 07:56:33 UTC",
    "updated_date": "2024-12-15 04:10:49 UTC"
  },
  {
    "arxiv_id": "2412.07259v3",
    "title": "Goal-Driven Reasoning in DatalogMTL with Magic Sets",
    "authors": [
      "Shaoyu Wang",
      "Kaiyue Zhao",
      "Dongliang Wei",
      "Przemysław Andrzej Wałęga",
      "Dingmin Wang",
      "Hongming Cai",
      "Pan Hu"
    ],
    "abstract": "DatalogMTL is a powerful rule-based language for temporal reasoning. Due to\nits high expressive power and flexible modeling capabilities, it is suitable\nfor a wide range of applications, including tasks from industrial and financial\nsectors. However, due to its high computational complexity, practical reasoning\nin DatalogMTL is highly challenging. To address this difficulty, we introduce a\nnew reasoning method for DatalogMTL which exploits the magic sets technique --\na rewriting approach developed for (non-temporal) Datalog to simulate top-down\nevaluation with bottom-up reasoning. We have implemented this approach and\nevaluated it on publicly available benchmarks, showing that the proposed\napproach significantly and consistently outperformed state-of-the-art reasoning\ntechniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07259v3",
    "published_date": "2024-12-10 07:40:37 UTC",
    "updated_date": "2025-02-27 14:13:20 UTC"
  },
  {
    "arxiv_id": "2412.07255v1",
    "title": "Label-Confidence-Aware Uncertainty Estimation in Natural Language Generation",
    "authors": [
      "Qinhong Lin",
      "Linna Zhou",
      "Zhongliang Yang",
      "Yuang Cai"
    ],
    "abstract": "Large Language Models (LLMs) display formidable capabilities in generative\ntasks but also pose potential risks due to their tendency to generate\nhallucinatory responses. Uncertainty Quantification (UQ), the evaluation of\nmodel output reliability, is crucial for ensuring the safety and robustness of\nAI systems. Recent studies have concentrated on model uncertainty by analyzing\nthe relationship between output entropy under various sampling conditions and\nthe corresponding labels. However, these methods primarily focus on measuring\nmodel entropy with precision to capture response characteristics, often\nneglecting the uncertainties associated with greedy decoding results-the\nsources of model labels, which can lead to biased classification outcomes. In\nthis paper, we explore the biases introduced by greedy decoding and propose a\nlabel-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler\n(KL) divergence bridging between samples and label source, thus enhancing the\nreliability and stability of uncertainty assessments. Our empirical evaluations\nacross a range of popular LLMs and NLP datasets reveal that different label\nsources can indeed affect classification, and that our approach can effectively\ncapture differences in sampling results and label sources, demonstrating more\neffective uncertainty estimation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07255v1",
    "published_date": "2024-12-10 07:35:23 UTC",
    "updated_date": "2024-12-10 07:35:23 UTC"
  },
  {
    "arxiv_id": "2412.07249v2",
    "title": "Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW Content Generation",
    "authors": [
      "Xin Zhao",
      "Xiaojun Chen",
      "Yuexin Xuan",
      "Zhendong Zhao",
      "Xiaojun Jia",
      "Xinfeng Li",
      "Xiaofeng Wang"
    ],
    "abstract": "The rise of deep learning models in the digital era has raised substantial\nconcerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing\ndefense methods primarily involve model fine-tuning and post-hoc content\nmoderation. Nevertheless, these approaches largely lack scalability in\neliminating harmful content, degrade the quality of benign image generation, or\nincur high inference costs. To address these challenges, we propose an\ninnovative framework named \\textit{Buster}, which injects backdoors into the\ntext encoder to prevent NSFW content generation. Buster leverages deep semantic\ninformation rather than explicit prompts as triggers, redirecting NSFW prompts\ntowards targeted benign prompts. Additionally, Buster employs energy-based\ntraining data generation through Langevin dynamics for adversarial knowledge\naugmentation, thereby ensuring robustness in harmful concept definition. This\napproach demonstrates exceptional resilience and scalability in mitigating NSFW\ncontent. Particularly, Buster fine-tunes the text encoder of Text-to-Image\nmodels within merely five minutes, showcasing its efficiency. Our extensive\nexperiments denote that Buster outperforms nine state-of-the-art baselines,\nachieving a superior NSFW content removal rate of at least 91.2\\% while\npreserving the quality of harmless images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07249v2",
    "published_date": "2024-12-10 07:18:51 UTC",
    "updated_date": "2025-01-13 07:22:02 UTC"
  },
  {
    "arxiv_id": "2412.07243v1",
    "title": "A Dynamical Systems-Inspired Pruning Strategy for Addressing Oversmoothing in Graph Neural Networks",
    "authors": [
      "Biswadeep Chakraborty",
      "Harshit Kumar",
      "Saibal Mukhopadhyay"
    ],
    "abstract": "Oversmoothing in Graph Neural Networks (GNNs) poses a significant challenge\nas network depth increases, leading to homogenized node representations and a\nloss of expressiveness. In this work, we approach the oversmoothing problem\nfrom a dynamical systems perspective, providing a deeper understanding of the\nstability and convergence behavior of GNNs. Leveraging insights from dynamical\nsystems theory, we identify the root causes of oversmoothing and propose\n\\textbf{\\textit{DYNAMO-GAT}}. This approach utilizes noise-driven covariance\nanalysis and Anti-Hebbian principles to selectively prune redundant attention\nweights, dynamically adjusting the network's behavior to maintain node feature\ndiversity and stability. Our theoretical analysis reveals how DYNAMO-GAT\ndisrupts the convergence to oversmoothed states, while experimental results on\nbenchmark datasets demonstrate its superior performance and efficiency compared\nto traditional and state-of-the-art methods. DYNAMO-GAT not only advances the\ntheoretical understanding of oversmoothing through the lens of dynamical\nsystems but also provides a practical and effective solution for improving the\nstability and expressiveness of deep GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.07243v1",
    "published_date": "2024-12-10 07:07:06 UTC",
    "updated_date": "2024-12-10 07:07:06 UTC"
  },
  {
    "arxiv_id": "2412.07241v1",
    "title": "Human-Computer Interaction and Human-AI Collaboration in Advanced Air Mobility: A Comprehensive Review",
    "authors": [
      "Fatma Yamac Sagirli",
      "Xiaopeng Zhao",
      "Zhenbo Wang"
    ],
    "abstract": "The increasing rates of global urbanization and vehicle usage are leading to\na shift of mobility to the third dimension-through Advanced Air Mobility\n(AAM)-offering a promising solution for faster, safer, cleaner, and more\nefficient transportation. As air transportation continues to evolve with more\nautomated and autonomous systems, advancements in AAM require a deep\nunderstanding of human-computer interaction and human-AI collaboration to\nensure safe and effective operations in complex urban and regional\nenvironments. There has been a significant increase in publications regarding\nthese emerging applications; thus, there is a need to review developments in\nthis area. This paper comprehensively reviews the current state of research on\nhuman-computer interaction and human-AI collaboration in AAM. Specifically, we\nfocus on AAM applications related to the design of human-machine interfaces for\nvarious uses, including pilot training, air traffic management, and the\nintegration of AI-assisted decision-making systems with immersive technologies\nsuch as extended, virtual, mixed, and augmented reality devices. Additionally,\nwe provide a comprehensive analysis of the challenges AAM encounters in\nintegrating human-computer frameworks, including unique challenges associated\nwith these interactions, such as trust in AI systems and safety concerns.\nFinally, we highlight emerging opportunities and propose future research\ndirections to bridge the gap between human factors and technological\nadvancements in AAM.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07241v1",
    "published_date": "2024-12-10 07:06:52 UTC",
    "updated_date": "2024-12-10 07:06:52 UTC"
  },
  {
    "arxiv_id": "2412.07237v3",
    "title": "ArtFormer: Controllable Generation of Diverse 3D Articulated Objects",
    "authors": [
      "Jiayi Su",
      "Youhe Feng",
      "Zheng Li",
      "Jinhua Song",
      "Yangfan He",
      "Botao Ren",
      "Botian Xu"
    ],
    "abstract": "This paper presents a novel framework for modeling and conditional generation\nof 3D articulated objects. Troubled by flexibility-quality tradeoffs, existing\nmethods are often limited to using predefined structures or retrieving shapes\nfrom static datasets. To address these challenges, we parameterize an\narticulated object as a tree of tokens and employ a transformer to generate\nboth the object's high-level geometry code and its kinematic relations.\nSubsequently, each sub-part's geometry is further decoded using a\nsigned-distance-function (SDF) shape prior, facilitating the synthesis of\nhigh-quality 3D shapes. Our approach enables the generation of diverse objects\nwith high-quality geometry and varying number of parts. Comprehensive\nexperiments on conditional generation from text descriptions demonstrate the\neffectiveness and flexibility of our method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. impl. repo: https://github.com/ShuYuMo2003/ArtFormer",
    "pdf_url": "http://arxiv.org/pdf/2412.07237v3",
    "published_date": "2024-12-10 07:00:05 UTC",
    "updated_date": "2025-04-03 14:16:29 UTC"
  },
  {
    "arxiv_id": "2412.07236v5",
    "title": "CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding",
    "authors": [
      "Jiquan Wang",
      "Sha Zhao",
      "Zhiling Luo",
      "Yangxuan Zhou",
      "Haiteng Jiang",
      "Shijian Li",
      "Tao Li",
      "Gang Pan"
    ],
    "abstract": "Electroencephalography (EEG) is a non-invasive technique to measure and\nrecord brain electrical activity, widely used in various BCI and healthcare\napplications. Early EEG decoding methods rely on supervised learning, limited\nby specific tasks and datasets, hindering model performance and\ngeneralizability. With the success of large language models, there is a growing\nbody of studies focusing on EEG foundation models. However, these studies still\nleave challenges: Firstly, most of existing EEG foundation models employ full\nEEG modeling strategy. It models the spatial and temporal dependencies between\nall EEG patches together, but ignores that the spatial and temporal\ndependencies are heterogeneous due to the unique structural characteristics of\nEEG signals. Secondly, existing EEG foundation models have limited\ngeneralizability on a wide range of downstream BCI tasks due to varying formats\nof EEG data, making it challenging to adapt to. To address these challenges, we\npropose a novel foundation model called CBraMod. Specifically, we devise a\ncriss-cross transformer as the backbone to thoroughly leverage the structural\ncharacteristics of EEG signals, which can model spatial and temporal\ndependencies separately through two parallel attention mechanisms. And we\nutilize an asymmetric conditional positional encoding scheme which can encode\npositional information of EEG patches and be easily adapted to the EEG with\ndiverse formats. CBraMod is pre-trained on a very large corpus of EEG through\npatch-based masked EEG reconstruction. We evaluate CBraMod on up to 10\ndownstream BCI tasks (12 public datasets). CBraMod achieves the\nstate-of-the-art performance across the wide range of tasks, proving its strong\ncapability and generalizability. The source code is publicly available at\nhttps://github.com/wjq-learning/CBraMod.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.07236v5",
    "published_date": "2024-12-10 06:56:36 UTC",
    "updated_date": "2025-04-13 10:02:56 UTC"
  },
  {
    "arxiv_id": "2412.14188v1",
    "title": "CogSimulator: A Model for Simulating User Cognition & Behavior with Minimal Data for Tailored Cognitive Enhancement",
    "authors": [
      "Weizhen Bian",
      "Yubo Zhou",
      "Yuanhang Luo",
      "Ming Mo",
      "Siyan Liu",
      "Yikai Gong",
      "Renjie Wan",
      "Ziyuan Luo",
      "Aobo Wang"
    ],
    "abstract": "The interplay between cognition and gaming, notably through educational games\nenhancing cognitive skills, has garnered significant attention in recent years.\nThis research introduces the CogSimulator, a novel algorithm for simulating\nuser cognition in small-group settings with minimal data, as the educational\ngame Wordle exemplifies. The CogSimulator employs Wasserstein-1 distance and\ncoordinates search optimization for hyperparameter tuning, enabling precise\nfew-shot predictions in new game scenarios. Comparative experiments with the\nWordle dataset illustrate that our model surpasses most conventional machine\nlearning models in mean Wasserstein-1 distance, mean squared error, and mean\naccuracy, showcasing its efficacy in cognitive enhancement through tailored\ngame design.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14188v1",
    "published_date": "2024-12-10 06:25:28 UTC",
    "updated_date": "2024-12-10 06:25:28 UTC"
  },
  {
    "arxiv_id": "2412.07813v3",
    "title": "How Can Incentives and Cut Layer Selection Influence Data Contribution in Split Federated Learning?",
    "authors": [
      "Joohyung Lee",
      "Jungchan Cho",
      "Wonjun Lee",
      "Mohamed Seif",
      "H. Vincent Poor"
    ],
    "abstract": "To alleviate the training burden in federated learning while enhancing\nconvergence speed, Split Federated Learning (SFL) has emerged as a promising\napproach by combining the advantages of federated and split learning. However,\nrecent studies have largely overlooked competitive situations. In this\nframework, the SFL model owner can choose the cut layer to balance the training\nload between the server and clients, ensuring the necessary level of privacy\nfor the clients. Additionally, the SFL model owner sets incentives to encourage\nclient participation in the SFL process. The optimization strategies employed\nby the SFL model owner influence clients' decisions regarding the amount of\ndata they contribute, taking into account the shared incentives over clients\nand anticipated energy consumption during SFL. To address this framework, we\nmodel the problem using a hierarchical decision-making approach, formulated as\na single-leader multi-follower Stackelberg game. We demonstrate the existence\nand uniqueness of the Nash equilibrium among clients and analyze the\nStackelberg equilibrium by examining the leader's game. Furthermore, we discuss\nprivacy concerns related to differential privacy and the criteria for selecting\nthe minimum required cut layer. Our findings show that the Stackelberg\nequilibrium solution maximizes the utility for both the clients and the SFL\nmodel owner.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "12 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07813v3",
    "published_date": "2024-12-10 06:24:08 UTC",
    "updated_date": "2025-01-23 09:47:49 UTC"
  },
  {
    "arxiv_id": "2412.07224v1",
    "title": "Parseval Regularization for Continual Reinforcement Learning",
    "authors": [
      "Wesley Chung",
      "Lynn Cherif",
      "David Meger",
      "Doina Precup"
    ],
    "abstract": "Loss of plasticity, trainability loss, and primacy bias have been identified\nas issues arising when training deep neural networks on sequences of tasks --\nall referring to the increased difficulty in training on new tasks. We propose\nto use Parseval regularization, which maintains orthogonality of weight\nmatrices, to preserve useful optimization properties and improve training in a\ncontinual reinforcement learning setting. We show that it provides significant\nbenefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We\nconduct comprehensive ablations to identify the source of its benefits and\ninvestigate the effect of certain metrics associated to network trainability\nincluding weight matrix rank, weight norms and policy entropy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07224v1",
    "published_date": "2024-12-10 06:19:21 UTC",
    "updated_date": "2024-12-10 06:19:21 UTC"
  },
  {
    "arxiv_id": "2412.07222v1",
    "title": "MPSI: Mamba enhancement model for pixel-wise sequential interaction Image Super-Resolution",
    "authors": [
      "Yuchun He",
      "Yuhan He"
    ],
    "abstract": "Single image super-resolution (SR) has long posed a challenge in the field of\ncomputer vision. While the advent of deep learning has led to the emergence of\nnumerous methods aimed at tackling this persistent issue, the current\nmethodologies still encounter challenges in modeling long sequence information,\nleading to limitations in effectively capturing the global pixel interactions.\nTo tackle this challenge and achieve superior SR outcomes, we propose the Mamba\npixel-wise sequential interaction network (MPSI), aimed at enhancing the\nestablishment of long-range connections of information, particularly focusing\non pixel-wise sequential interaction. We propose the Channel-Mamba Block (CMB)\nto capture comprehensive pixel interaction information by effectively modeling\nlong sequence information. Moreover, in the existing SR methodologies, there\npersists the issue of the neglect of features extracted by preceding layers,\nleading to the loss of valuable feature information. While certain existing\nmodels strive to preserve these features, they frequently encounter difficulty\nin establishing connections across all layers. To overcome this limitation,\nMPSI introduces the Mamba channel recursion module (MCRM), which maximizes the\nretention of valuable feature information from early layers, thereby\nfacilitating the acquisition of pixel sequence interaction information from\nmultiple-level layers. Through extensive experimentation, we demonstrate that\nMPSI outperforms existing super-resolution methods in terms of image\nreconstruction results, attaining state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07222v1",
    "published_date": "2024-12-10 06:18:29 UTC",
    "updated_date": "2024-12-10 06:18:29 UTC"
  },
  {
    "arxiv_id": "2412.07214v3",
    "title": "Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models",
    "authors": [
      "Jun-Peng Zhu",
      "Boyan Niu",
      "Peng Cai",
      "Zheming Ni",
      "Jianwei Wan",
      "Kai Xu",
      "Jiajun Huang",
      "Shengbo Ma",
      "Bing Wang",
      "Xuan Zhou",
      "Guanglei Bao",
      "Donghui Zhang",
      "Liu Tang",
      "Qi Liu"
    ],
    "abstract": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "14 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07214v3",
    "published_date": "2024-12-10 06:11:23 UTC",
    "updated_date": "2025-02-14 02:49:05 UTC"
  },
  {
    "arxiv_id": "2412.07213v1",
    "title": "IntellectSeeker: A Personalized Literature Management System with the Probabilistic Model and Large Language Model",
    "authors": [
      "Weizhen Bian",
      "Siyan Liu",
      "Yubo Zhou",
      "Dezhi Chen",
      "Yijie Liao",
      "Zhenzhen Fan",
      "Aobo Wang"
    ],
    "abstract": "Faced with the burgeoning volume of academic literature, researchers often\nneed help with uncertain article quality and mismatches in term searches using\ntraditional academic engines. We introduce IntellectSeeker, an innovative and\npersonalized intelligent academic literature management platform to address\nthese challenges. This platform integrates a Large Language Model (LLM)--based\nsemantic enhancement bot with a sophisticated probability model to personalize\nand streamline literature searches. We adopted the GPT-3.5-turbo model to\ntransform everyday language into professional academic terms across various\nscenarios using multiple rounds of few-shot learning. This adaptation mainly\nbenefits academic newcomers, effectively bridging the gap between general\ninquiries and academic terminology. The probabilistic model intelligently\nfilters academic articles to align closely with the specific interests of\nusers, which are derived from explicit needs and behavioral patterns. Moreover,\nIntellectSeeker incorporates an advanced recommendation system and text\ncompression tools. These features enable intelligent article recommendations\nbased on user interactions and present search results through concise one-line\nsummaries and innovative word cloud visualizations, significantly enhancing\nresearch efficiency and user experience. IntellectSeeker offers academic\nresearchers a highly customizable literature management solution with\nexceptional search precision and matching capabilities. The code can be found\nhere: https://github.com/LuckyBian/ISY5001",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07213v1",
    "published_date": "2024-12-10 06:09:49 UTC",
    "updated_date": "2024-12-10 06:09:49 UTC"
  },
  {
    "arxiv_id": "2412.07210v2",
    "title": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models",
    "authors": [
      "Jialiang Cheng",
      "Ning Gao",
      "Yun Yue",
      "Zhiling Ye",
      "Jiadi Jiang",
      "Jian Sha"
    ],
    "abstract": "Distributed training methods are crucial for large language models (LLMs).\nHowever, existing distributed training methods often suffer from communication\nbottlenecks, stragglers, and limited elasticity, particularly in heterogeneous\nor large-scale environments. Local SGD methods have been proposed to address\nthese issues, but their effectiveness remains limited to small-scale training\ndue to additional memory overhead and lack of concerns on efficiency and\nstability. To tackle these issues, we propose EDiT, an innovative Efficient\nDistributed Training method that combines a tailored Local SGD approach with\nmodel sharding techniques to enhance large-scale training efficiency. EDiT\nperforms layer-wise parameter synchronization during forward pass, reducing\ncommunication and memory overhead and enabling overlap. Besides, EDiT employs a\npseudo gradient penalty strategy to suppress loss spikes, which ensures\ntraining stability and improves performance. Additionally, we introduce A-EDiT,\na fully asynchronous variant of EDiT that accommodates heterogeneous clusters.\nBuilding on EDiT/A-EDiT, we conduct a series of experiments to validate\nlarge-scale asynchronous training for LLMs, accompanied by comprehensive\nanalyses. Experimental results demonstrate the superior performance of\nEDiT/A-EDiT, establishing them as robust solutions for distributed LLM training\nin diverse computational ecosystems. The code is available at Atorch codebase:\nhttps://github.com/intelligent-machine-learning/atorch/tree/main/atorch/local_sgd.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "22 pages, 10 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.07210v2",
    "published_date": "2024-12-10 06:08:24 UTC",
    "updated_date": "2025-02-17 02:57:12 UTC"
  },
  {
    "arxiv_id": "2412.07207v2",
    "title": "MAPLE: A Framework for Active Preference Learning Guided by Large Language Models",
    "authors": [
      "Saaduddin Mahmud",
      "Mason Nakamura",
      "Shlomo Zilberstein"
    ],
    "abstract": "The advent of large language models (LLMs) has sparked significant interest\nin using natural language for preference learning. However, existing methods\noften suffer from high computational burdens, taxing human supervision, and\nlack of interpretability. To address these issues, we introduce MAPLE, a\nframework for large language model-guided Bayesian active preference learning.\nMAPLE leverages LLMs to model the distribution over preference functions,\nconditioning it on both natural language feedback and conventional preference\nlearning feedback, such as pairwise trajectory rankings. MAPLE also employs\nactive learning to systematically reduce uncertainty in this distribution and\nincorporates a language-conditioned active query selection mechanism to\nidentify informative and easy-to-answer queries, thus reducing human burden. We\nevaluate MAPLE's sample efficiency and preference inference quality across two\nbenchmarks, including a real-world vehicle route planning benchmark using\nOpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning\nprocess and effectively improves humans' ability to answer queries.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025 AI Alignment Track",
    "pdf_url": "http://arxiv.org/pdf/2412.07207v2",
    "published_date": "2024-12-10 05:55:14 UTC",
    "updated_date": "2024-12-20 01:08:20 UTC"
  },
  {
    "arxiv_id": "2501.14734v1",
    "title": "Research on the Application of Spark Streaming Real-Time Data Analysis System and large language model Intelligent Agents",
    "authors": [
      "Jialin Wang",
      "Zhihua Duan"
    ],
    "abstract": "This study explores the integration of Agent AI with LangGraph to enhance\nreal-time data analysis systems in big data environments. The proposed\nframework overcomes limitations of static workflows, inefficient stateful\ncomputations, and lack of human intervention by leveraging LangGraph's\ngraph-based workflow construction and dynamic decision-making capabilities.\nLangGraph allows large language models (LLMs) to dynamically determine control\nflows, invoke tools, and assess the necessity of further actions, improving\nflexibility and efficiency.\n  The system architecture incorporates Apache Spark Streaming, Kafka, and\nLangGraph to create a high-performance sentiment analysis system. LangGraph's\ncapabilities include precise state management, dynamic workflow construction,\nand robust memory checkpointing, enabling seamless multi-turn interactions and\ncontext retention. Human-in-the-loop mechanisms are integrated to refine\nsentiment analysis, particularly in ambiguous or high-stakes scenarios,\nensuring greater reliability and contextual relevance.\n  Key features such as real-time state streaming, debugging via LangGraph\nStudio, and efficient handling of large-scale data streams make this framework\nideal for adaptive decision-making. Experimental results confirm the system's\nability to classify inquiries, detect sentiment trends, and escalate complex\nissues for manual review, demonstrating a synergistic blend of LLM capabilities\nand human oversight.\n  This work presents a scalable, adaptable, and reliable solution for real-time\nsentiment analysis and decision-making, advancing the use of Agent AI and\nLangGraph in big data applications.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14734v1",
    "published_date": "2024-12-10 05:51:11 UTC",
    "updated_date": "2024-12-10 05:51:11 UTC"
  },
  {
    "arxiv_id": "2412.07201v1",
    "title": "A Review on the Applications of Transformer-based language models for Nucleotide Sequence Analysis",
    "authors": [
      "Nimisha Ghosh",
      "Daniele Santoni",
      "Indrajit Saha",
      "Giovanni Felici"
    ],
    "abstract": "In recent times, Transformer-based language models are making quite an impact\nin the field of natural language processing. As relevant parallels can be drawn\nbetween biological sequences and natural languages, the models used in NLP can\nbe easily extended and adapted for various applications in bioinformatics. In\nthis regard, this paper introduces the major developments of Transformer-based\nmodels in the recent past in the context of nucleotide sequences. We have\nreviewed and analysed a large number of application-based papers on this\nsubject, giving evidence of the main characterizing features and to different\napproaches that may be adopted to customize such powerful computational\nmachines. We have also provided a structured description of the functioning of\nTransformers, that may enable even first time users to grab the essence of such\ncomplex architectures. We believe this review will help the scientific\ncommunity in understanding the various applications of Transformer-based\nlanguage models to nucleotide sequences. This work will motivate the readers to\nbuild on these methodologies to tackle also various other problems in the field\nof bioinformatics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07201v1",
    "published_date": "2024-12-10 05:33:09 UTC",
    "updated_date": "2024-12-10 05:33:09 UTC"
  },
  {
    "arxiv_id": "2412.07200v1",
    "title": "Modifying AI, Enhancing Essays: How Active Engagement with Generative AI Boosts Writing Quality",
    "authors": [
      "Kaixun Yang",
      "Mladen Raković",
      "Zhiping Liang",
      "Lixiang Yan",
      "Zijie Zeng",
      "Yizhou Fan",
      "Dragan Gašević",
      "Guanliang Chen"
    ],
    "abstract": "Students are increasingly relying on Generative AI (GAI) to support their\nwriting-a key pedagogical practice in education. In GAI-assisted writing,\nstudents can delegate core cognitive tasks (e.g., generating ideas and turning\nthem into sentences) to GAI while still producing high-quality essays. This\ncreates new challenges for teachers in assessing and supporting student\nlearning, as they often lack insight into whether students are engaging in\nmeaningful cognitive processes during writing or how much of the essay's\nquality can be attributed to those processes. This study aimed to help teachers\nbetter assess and support student learning in GAI-assisted writing by examining\nhow different writing behaviors, especially those indicative of meaningful\nlearning versus those that are not, impact essay quality. Using a dataset of\n1,445 GAI-assisted writing sessions, we applied the cutting-edge method,\nX-Learner, to quantify the causal impact of three GAI-assisted writing\nbehavioral patterns (i.e., seeking suggestions but not accepting them, seeking\nsuggestions and accepting them as they are, and seeking suggestions and\naccepting them with modification) on four measures of essay quality (i.e.,\nlexical sophistication, syntactic complexity, text cohesion, and linguistic\nbias). Our analysis showed that writers who frequently modified GAI-generated\ntext-suggesting active engagement in higher-order cognitive\nprocesses-consistently improved the quality of their essays in terms of lexical\nsophistication, syntactic complexity, and text cohesion. In contrast, those who\noften accepted GAI-generated text without changes, primarily engaging in\nlower-order processes, saw a decrease in essay quality. Additionally, while\nhuman writers tend to introduce linguistic bias when writing independently,\nincorporating GAI-generated text-even without modification-can help mitigate\nthis bias.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07200v1",
    "published_date": "2024-12-10 05:32:57 UTC",
    "updated_date": "2024-12-10 05:32:57 UTC"
  },
  {
    "arxiv_id": "2412.07197v2",
    "title": "Hierarchical Split Federated Learning: Convergence Analysis and System Optimization",
    "authors": [
      "Zheng Lin",
      "Wei Wei",
      "Zhe Chen",
      "Chan-Tong Lam",
      "Xianhao Chen",
      "Yue Gao",
      "Jun Luo"
    ],
    "abstract": "As AI models expand in size, it has become increasingly challenging to deploy\nfederated learning (FL) on resource-constrained edge devices. To tackle this\nissue, split federated learning (SFL) has emerged as an FL framework with\nreduced workload on edge devices via model splitting; it has received extensive\nattention from the research community in recent years. Nevertheless, most prior\nworks on SFL focus only on a two-tier architecture without harnessing\nmulti-tier cloudedge computing resources. In this paper, we intend to analyze\nand optimize the learning performance of SFL under multi-tier systems.\nSpecifically, we propose the hierarchical SFL (HSFL) framework and derive its\nconvergence bound. Based on the theoretical results, we formulate a joint\noptimization problem for model splitting (MS) and model aggregation (MA). To\nsolve this rather hard problem, we then decompose it into MS and MA subproblems\nthat can be solved via an iterative descending algorithm. Simulation results\ndemonstrate that the tailored algorithm can effectively optimize MS and MA for\nSFL within virtually any multi-tier system.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07197v2",
    "published_date": "2024-12-10 05:20:49 UTC",
    "updated_date": "2025-04-21 06:52:09 UTC"
  },
  {
    "arxiv_id": "2412.07188v1",
    "title": "Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective",
    "authors": [
      "Yushun Dong",
      "Patrick Soga",
      "Yinhan He",
      "Song Wang",
      "Jundong Li"
    ],
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various\ngraph-based learning tasks. While their performance is often attributed to the\npowerful neighborhood aggregation mechanism, recent studies suggest that other\ncomponents such as non-linear layers may also significantly affecting how GNNs\nprocess the input graph data in the spectral domain. Such evidence challenges\nthe prevalent opinion that neighborhood aggregation mechanisms dominate the\nbehavioral characteristics of GNNs in the spectral domain. To demystify such a\nconflict, this paper introduces a comprehensive benchmark to measure and\nevaluate GNNs' capability in capturing and leveraging the information encoded\nin different frequency components of the input graph data. Specifically, we\nfirst conduct an exploratory study demonstrating that GNNs can flexibly yield\noutputs with diverse frequency components even when certain frequencies are\nabsent or filtered out from the input graph data. We then formulate a novel\nresearch problem of measuring and benchmarking the performance of GNNs from a\nspectral perspective. To take an initial step towards a comprehensive\nbenchmark, we design an evaluation protocol supported by comprehensive\ntheoretical analysis. Finally, we introduce a comprehensive benchmark on\nreal-world datasets, revealing insights that challenge prevalent opinions from\na spectral perspective. We believe that our findings will open new avenues for\nfuture advancements in this area. Our implementations can be found at:\nhttps://github.com/yushundong/Spectral-benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07188v1",
    "published_date": "2024-12-10 04:53:53 UTC",
    "updated_date": "2024-12-10 04:53:53 UTC"
  },
  {
    "arxiv_id": "2412.07186v1",
    "title": "Monte Carlo Tree Search based Space Transfer for Black-box Optimization",
    "authors": [
      "Shukuan Wang",
      "Ke Xue",
      "Lei Song",
      "Xiaobin Huang",
      "Chao Qian"
    ],
    "abstract": "Bayesian optimization (BO) is a popular method for computationally expensive\nblack-box optimization. However, traditional BO methods need to solve new\nproblems from scratch, leading to slow convergence. Recent studies try to\nextend BO to a transfer learning setup to speed up the optimization, where\nsearch space transfer is one of the most promising approaches and has shown\nimpressive performance on many tasks. However, existing search space transfer\nmethods either lack an adaptive mechanism or are not flexible enough, making it\ndifficult to efficiently identify promising search space during the\noptimization process. In this paper, we propose a search space transfer\nlearning method based on Monte Carlo tree search (MCTS), called MCTS-transfer,\nto iteratively divide, select, and optimize in a learned subspace.\nMCTS-transfer can not only provide a well-performing search space for\nwarm-start but also adaptively identify and leverage the information of similar\nsource tasks to reconstruct the search space during the optimization process.\nExperiments on synthetic functions, real-world problems, Design-Bench and\nhyper-parameter optimization show that MCTS-transfer can demonstrate superior\nperformance compared to other search space transfer methods under different\nsettings. Our code is available at\n\\url{https://github.com/lamda-bbo/mcts-transfer}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2412.07186v1",
    "published_date": "2024-12-10 04:52:26 UTC",
    "updated_date": "2024-12-10 04:52:26 UTC"
  },
  {
    "arxiv_id": "2412.07183v1",
    "title": "Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding of Video Anomaly",
    "authors": [
      "Hang Du",
      "Guoshun Nan",
      "Jiawen Qian",
      "Wangchenhui Wu",
      "Wendi Deng",
      "Hanqing Mu",
      "Zhenyan Chen",
      "Pengxuan Mao",
      "Xiaofeng Tao",
      "Jun Liu"
    ],
    "abstract": "Recent advancements in video anomaly understanding (VAU) have opened the door\nto groundbreaking applications in various fields, such as traffic monitoring\nand industrial automation. While the current benchmarks in VAU predominantly\nemphasize the detection and localization of anomalies. Here, we endeavor to\ndelve deeper into the practical aspects of VAU by addressing the essential\nquestions: \"what anomaly occurred?\", \"why did it happen?\", and \"how severe is\nthis abnormal event?\". In pursuit of these answers, we introduce a\ncomprehensive benchmark for Exploring the Causation of Video Anomalies (ECVA).\nOur benchmark is meticulously designed, with each video accompanied by detailed\nhuman annotations. Specifically, each instance of our ECVA involves three sets\nof human annotations to indicate \"what\", \"why\" and \"how\" of an anomaly,\nincluding 1) anomaly type, start and end times, and event descriptions, 2)\nnatural language explanations for the cause of an anomaly, and 3) free text\nreflecting the effect of the abnormality. Building upon this foundation, we\npropose a novel prompt-based methodology that serves as a baseline for tackling\nthe intricate challenges posed by ECVA. We utilize \"hard prompt\" to guide the\nmodel to focus on the critical parts related to video anomaly segments, and\n\"soft prompt\" to establish temporal and spatial relationships within these\nanomaly segments. Furthermore, we propose AnomEval, a specialized evaluation\nmetric crafted to align closely with human judgment criteria for ECVA. This\nmetric leverages the unique features of the ECVA dataset to provide a more\ncomprehensive and reliable assessment of various video large language models.\nWe demonstrate the efficacy of our approach through rigorous experimental\nanalysis and delineate possible avenues for further investigation into the\ncomprehension of video anomaly causation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. arXiv admin note: substantial text overlap with\n  arXiv:2405.00181",
    "pdf_url": "http://arxiv.org/pdf/2412.07183v1",
    "published_date": "2024-12-10 04:41:44 UTC",
    "updated_date": "2024-12-10 04:41:44 UTC"
  },
  {
    "arxiv_id": "2412.07182v2",
    "title": "An Enhancement of CNN Algorithm for Rice Leaf Disease Image Classification in Mobile Applications",
    "authors": [
      "Kayne Uriel K. Rodrigo",
      "Jerriane Hillary Heart S. Marcial",
      "Samuel C. Brillo",
      "Khatalyn E. Mata",
      "Jonathan C. Morano"
    ],
    "abstract": "This study focuses on enhancing rice leaf disease image classification\nalgorithms, which have traditionally relied on Convolutional Neural Network\n(CNN) models. We employed transfer learning with MobileViTV2_050 using\nImageNet-1k weights, a lightweight model that integrates CNN's local feature\nextraction with Vision Transformers' global context learning through a\nseparable self-attention mechanism. Our approach resulted in a significant\n15.66% improvement in classification accuracy for MobileViTV2_050-A, our first\nenhanced model trained on the baseline dataset, achieving 93.14%. Furthermore,\nMobileViTV2_050-B, our second enhanced model trained on a broader rice leaf\ndataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy.\nAdditionally, MobileViTV2-A attained an F1-score of 93% across four rice labels\nand a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In\nterms of resource consumption, our enhanced models reduced the total parameters\nof the baseline CNN model by up to 92.50%, from 14 million to 1.1 million.\nThese results indicate that MobileViTV2_050 not only improves computational\nefficiency through its separable self-attention mechanism but also enhances\nglobal context learning. Consequently, it offers a lightweight and robust\nsolution suitable for mobile deployment, advancing the interpretability and\npracticality of models in precision agriculture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Presented at 46th World Conference on Applied Science, Engineering &\n  Technology (WCASET) from Institute for Educational Research and Publication\n  (IFERP)",
    "pdf_url": "http://arxiv.org/pdf/2412.07182v2",
    "published_date": "2024-12-10 04:41:10 UTC",
    "updated_date": "2025-02-16 12:20:40 UTC"
  },
  {
    "arxiv_id": "2412.07174v1",
    "title": "Post-Training Statistical Calibration for Higher Activation Sparsity",
    "authors": [
      "Vui Seng Chua",
      "Yujie Pan",
      "Nilesh Jain"
    ],
    "abstract": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training\nactivation pruning framework that (1) generalizes sparsification by input\nactivations of Fully-Connected layers for generic and flexible application\nacross Transformers, and (2) features a simple Mode-Centering technique to\npre-calibrate activation distributions for maximizing post-training sparsity.\nOur results demonstrate robust Pareto efficiency compared to prior methods,\ntranslating to a 1.5x additional LLM decoding speedup against CATS at iso model\nquality. SCAP effectiveness is empirically verified across a wide range of\nmodels, including recent Transformer Decoders, MoE, Mamba2, Encoding\nTransformer, and pre-quantized models, highlighting its practicality and\nscalability. The code is available at: https://github.com/IntelLabs/SCAP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ENLSP-IV NeurIPS Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.07174v1",
    "published_date": "2024-12-10 04:15:39 UTC",
    "updated_date": "2024-12-10 04:15:39 UTC"
  },
  {
    "arxiv_id": "2412.07167v1",
    "title": "Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer",
    "authors": [
      "Ke Xue",
      "Ruo-Tong Chen",
      "Xi Lin",
      "Yunqi Shi",
      "Shixiong Kai",
      "Siyuan Xu",
      "Chao Qian"
    ],
    "abstract": "In modern chip design, placement aims at placing millions of circuit modules,\nwhich is an essential step that significantly influences power, performance,\nand area (PPA) metrics. Recently, reinforcement learning (RL) has emerged as a\npromising technique for improving placement quality, especially macro\nplacement. However, current RL-based placement methods suffer from long\ntraining times, low generalization ability, and inability to guarantee PPA\nresults. A key issue lies in the problem formulation, i.e., using RL to place\nfrom scratch, which results in limits useful information and inaccurate rewards\nduring the training process. In this work, we propose an approach that utilizes\nRL for the refinement stage, which allows the RL policy to learn how to adjust\nexisting placement layouts, thereby receiving sufficient information for the\npolicy to act and obtain relatively dense and precise rewards. Additionally, we\nintroduce the concept of regularity during training, which is considered an\nimportant metric in the chip design industry but is often overlooked in current\nRL placement methods. We evaluate our approach on the ISPD 2005 and ICCAD 2015\nbenchmark, comparing the global half-perimeter wirelength and regularity of our\nproposed method against several competitive approaches. Besides, we test the\nPPA performance using commercial software, showing that RL as a regulator can\nachieve significant PPA improvements. Our RL regulator can fine-tune placements\nfrom any method and enhance their quality. Our work opens up new possibilities\nfor the application of RL in placement, providing a more effective and\nefficient approach to optimizing chip design. Our code is available at\n\\url{https://github.com/lamda-bbo/macro-regulator}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.07167v1",
    "published_date": "2024-12-10 04:01:21 UTC",
    "updated_date": "2024-12-10 04:01:21 UTC"
  },
  {
    "arxiv_id": "2412.07165v2",
    "title": "A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning",
    "authors": [
      "Jacob Adkins",
      "Michael Bowling",
      "Adam White"
    ],
    "abstract": "The performance of modern reinforcement learning algorithms critically relies\non tuning ever-increasing numbers of hyperparameters. Often, small changes in a\nhyperparameter can lead to drastic changes in performance, and different\nenvironments require very different hyperparameter settings to achieve\nstate-of-the-art performance reported in the literature. We currently lack a\nscalable and widely accepted approach to characterizing these complex\ninteractions. This work proposes a new empirical methodology for studying,\ncomparing, and quantifying the sensitivity of an algorithm's performance to\nhyperparameter tuning for a given set of environments. We then demonstrate the\nutility of this methodology by assessing the hyperparameter sensitivity of\nseveral commonly used normalization variants of PPO. The results suggest that\nseveral algorithmic performance improvements may, in fact, be a result of an\nincreased reliance on hyperparameter tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.07165v2",
    "published_date": "2024-12-10 03:55:18 UTC",
    "updated_date": "2025-02-04 05:17:26 UTC"
  },
  {
    "arxiv_id": "2412.07163v1",
    "title": "Fast Occupancy Network",
    "authors": [
      "Mingjie Lu",
      "Yuanxian Huang",
      "Ji Liu",
      "Xingliang Huang",
      "Dong Li",
      "Jinzhang Peng",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "Occupancy Network has recently attracted much attention in autonomous\ndriving. Instead of monocular 3D detection and recent bird's eye view(BEV)\nmodels predicting 3D bounding box of obstacles, Occupancy Network predicts the\ncategory of voxel in specified 3D space around the ego vehicle via transforming\n3D detection task into 3D voxel segmentation task, which has much superiority\nin tackling category outlier obstacles and providing fine-grained 3D\nrepresentation. However, existing methods usually require huge computation\nresources than previous methods, which hinder the Occupancy Network solution\napplying in intelligent driving systems. To address this problem, we make an\nanalysis of the bottleneck of Occupancy Network inference cost, and present a\nsimple and fast Occupancy Network model, which adopts a deformable 2D\nconvolutional layer to lift BEV feature to 3D voxel feature and presents an\nefficient voxel feature pyramid network (FPN) module to improve performance\nwith few computational cost. Further, we present a cost-free 2D segmentation\nbranch in perspective view after feature extractors for Occupancy Network\nduring inference phase to improve accuracy. Experimental results demonstrate\nthat our method consistently outperforms existing methods in both accuracy and\ninference speed, which surpasses recent state-of-the-art (SOTA) OCCNet by 1.7%\nwith ResNet50 backbone with about 3X inference speedup. Furthermore, our method\ncan be easily applied to existing BEV models to transform them into Occupancy\nNetwork models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures,",
    "pdf_url": "http://arxiv.org/pdf/2412.07163v1",
    "published_date": "2024-12-10 03:46:03 UTC",
    "updated_date": "2024-12-10 03:46:03 UTC"
  },
  {
    "arxiv_id": "2412.07148v1",
    "title": "MM-PoE: Multiple Choice Reasoning via. Process of Elimination using Multi-Modal Models",
    "authors": [
      "Sayak Chakrabarty",
      "Souradip Pal"
    ],
    "abstract": "This paper introduces Multiple Choice Reasoning via. Process of Elimination\nusing Multi-Modal models, herein referred to as Multi-Modal Process of\nElimination (MM-PoE). This novel methodology is engineered to augment the\nefficacy of Vision-Language Models (VLMs) in multiple-choice visual reasoning\ntasks. Diverging from conventional approaches that evaluate each option\nindependently, MM-PoE employs a dual-step scoring paradigm that initially\nidentifies and excludes implausible choices, subsequently concentrating on the\nmost probable remaining options. This method emulates human test-taking\nstrategies, where individuals typically eliminate clearly incorrect answers\nprior to selecting the optimal response. Our empirical evaluations, conducted\nacross three benchmark datasets, reveal that MM-PoE significantly improves both\nzero-shot and few-shot performance of contemporary state-of-the-art VLMs.\nCritically, this approach not only broadens the application of the elimination\nprocess to multi-modal contexts but also allows few-shot experiments, thereby\naddressing two principal limitations concerning usage of PoE only in zero-shot\nsettings and only with a language-only framework. As a result, MM-PoE not only\nrefines the reasoning capabilities of VLMs but also broadens their\napplicability to complex visual question-answering scenarios. All code and\ndocumentation supporting our work are available at\nhttps://pypi.org/project/mm-poe/, enabling researchers and practitioners to\neasily integrate and further develop these techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07148v1",
    "published_date": "2024-12-10 03:13:41 UTC",
    "updated_date": "2024-12-10 03:13:41 UTC"
  },
  {
    "arxiv_id": "2412.07147v2",
    "title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation",
    "authors": [
      "Bo Li",
      "Shaolin Zhu",
      "Lijie Wen"
    ],
    "abstract": "Image Translation (IT) holds immense potential across diverse domains,\nenabling the translation of textual content within images into various\nlanguages. However, existing datasets often suffer from limitations in scale,\ndiversity, and quality, hindering the development and evaluation of IT models.\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\nmultilingual image translation with over 10M image-text pairs derived from\nreal-world data, which has undergone extensive data cleaning and multilingual\ntranslation validation. It contains 840K images in three sizes, 28 categories,\ntasks with three levels of difficulty and 14 languages image-text pairs, which\nis a considerable improvement on existing datasets. We conduct extensive\nexperiments to evaluate and train models on MIT-10M. The experimental results\nclearly indicate that our dataset has higher adaptability when it comes to\nevaluating the performance of the models in tackling challenging and complex\nimage translation tasks in the real world. Moreover, the performance of the\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\nfurther confirming its superiority.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07147v2",
    "published_date": "2024-12-10 03:12:35 UTC",
    "updated_date": "2024-12-16 09:28:53 UTC"
  },
  {
    "arxiv_id": "2412.07144v2",
    "title": "Political Actor Agent: Simulating Legislative System for Roll Call Votes Prediction with Large Language Models",
    "authors": [
      "Hao Li",
      "Ruoyuan Gong",
      "Hao Jiang"
    ],
    "abstract": "Predicting roll call votes through modeling political actors has emerged as a\nfocus in quantitative political science and computer science. Widely used\nembedding-based methods generate vectors for legislators from diverse data sets\nto predict legislative behaviors. However, these methods often contend with\nchallenges such as the need for manually predefined features, reliance on\nextensive training data, and a lack of interpretability. Achieving more\ninterpretable predictions under flexible conditions remains an unresolved\nissue. This paper introduces the Political Actor Agent (PAA), a novel\nagent-based framework that utilizes Large Language Models to overcome these\nlimitations. By employing role-playing architectures and simulating legislative\nsystem, PAA provides a scalable and interpretable paradigm for predicting\nroll-call votes. Our approach not only enhances the accuracy of predictions but\nalso offers multi-view, human-understandable decision reasoning, providing new\ninsights into political actor behaviors. We conducted comprehensive experiments\nusing voting records from the 117-118th U.S. House of Representatives,\nvalidating the superior performance and interpretability of PAA. This study not\nonly demonstrates PAA's effectiveness but also its potential in political\nscience research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.07144v2",
    "published_date": "2024-12-10 03:06:28 UTC",
    "updated_date": "2024-12-13 04:05:05 UTC"
  },
  {
    "arxiv_id": "2412.07127v1",
    "title": "Deep Learning-Enhanced Preconditioning for Efficient Conjugate Gradient Solvers in Large-Scale PDE Systems",
    "authors": [
      "Rui Li",
      "Song Wang",
      "Chen Wang"
    ],
    "abstract": "Preconditioning techniques are crucial for enhancing the efficiency of\nsolving large-scale linear equation systems that arise from partial\ndifferential equation (PDE) discretization. These techniques, such as\nIncomplete Cholesky factorization (IC) and data-driven neural network methods,\naccelerate the convergence of iterative solvers like Conjugate Gradient (CG) by\napproximating the original matrices. This paper introduces a novel approach\nthat integrates Graph Neural Network (GNN) with traditional IC, addressing the\nshortcomings of direct generation methods based on GNN and achieving\nsignificant improvements in computational efficiency and scalability.\nExperimental results demonstrate an average reduction in iteration counts by\n24.8% compared to IC and a two-order-of-magnitude increase in training scale\ncompared to previous methods. A three-dimensional static structural analysis\nutilizing finite element methods was validated on training sparse matrices of\nup to 5 million dimensions and inference scales of up to 10 million.\nFurthermore, the approach demon-strates robust generalization capabilities\nacross scales, facilitating the effective acceleration of CG solvers for\nlarge-scale linear equations using small-scale data on modest hardware. The\nmethod's robustness and scalability make it a practical solution for\ncomputational science.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07127v1",
    "published_date": "2024-12-10 02:34:13 UTC",
    "updated_date": "2024-12-10 02:34:13 UTC"
  },
  {
    "arxiv_id": "2412.07116v1",
    "title": "A Review of Human Emotion Synthesis Based on Generative Technology",
    "authors": [
      "Fei Ma",
      "Yukan Li",
      "Yifan Xie",
      "Ying He",
      "Yi Zhang",
      "Hongwei Ren",
      "Zhou Liu",
      "Wei Yao",
      "Fuji Ren",
      "Fei Richard Yu",
      "Shiguang Ni"
    ],
    "abstract": "Human emotion synthesis is a crucial aspect of affective computing. It\ninvolves using computational methods to mimic and convey human emotions through\nvarious modalities, with the goal of enabling more natural and effective\nhuman-computer interactions. Recent advancements in generative models, such as\nAutoencoders, Generative Adversarial Networks, Diffusion Models, Large Language\nModels, and Sequence-to-Sequence Models, have significantly contributed to the\ndevelopment of this field. However, there is a notable lack of comprehensive\nreviews in this field. To address this problem, this paper aims to address this\ngap by providing a thorough and systematic overview of recent advancements in\nhuman emotion synthesis based on generative models. Specifically, this review\nwill first present the review methodology, the emotion models involved, the\nmathematical principles of generative models, and the datasets used. Then, the\nreview covers the application of different generative models to emotion\nsynthesis based on a variety of modalities, including facial images, speech,\nand text. It also examines mainstream evaluation metrics. Additionally, the\nreview presents some major findings and suggests future research directions,\nproviding a comprehensive understanding of the role of generative technology in\nthe nuanced domain of emotion synthesis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.07116v1",
    "published_date": "2024-12-10 02:06:10 UTC",
    "updated_date": "2024-12-10 02:06:10 UTC"
  },
  {
    "arxiv_id": "2412.10419v1",
    "title": "Personalized and Sequential Text-to-Image Generation",
    "authors": [
      "Ofir Nabati",
      "Guy Tennenholtz",
      "ChihWei Hsu",
      "Moonkyung Ryu",
      "Deepak Ramachandran",
      "Yinlam Chow",
      "Xiang Li",
      "Craig Boutilier"
    ],
    "abstract": "We address the problem of personalized, interactive text-to-image (T2I)\ngeneration, designing a reinforcement learning (RL) agent which iteratively\nimproves a set of generated images for a user through a sequence of prompt\nexpansions. Using human raters, we create a novel dataset of sequential\npreferences, which we leverage, together with large-scale open-source\n(non-sequential) datasets. We construct user-preference and user-choice models\nusing an EM strategy and identify varying user preference types. We then\nleverage a large multimodal language model (LMM) and a value-based RL approach\nto suggest a personalized and diverse slate of prompt expansions to the user.\nOur Personalized And Sequential Text-to-image Agent (PASTA) extends T2I models\nwith personalized multi-turn capabilities, fostering collaborative co-creation\nand addressing uncertainty or underspecification in a user's intent. We\nevaluate PASTA using human raters, showing significant improvement compared to\nbaseline methods. We also release our sequential rater dataset and simulated\nuser-rater interactions to support future research in personalized, multi-turn\nT2I generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "Link to PASTA dataset:\n  https://www.kaggle.com/datasets/googleai/pasta-data",
    "pdf_url": "http://arxiv.org/pdf/2412.10419v1",
    "published_date": "2024-12-10 01:47:40 UTC",
    "updated_date": "2024-12-10 01:47:40 UTC"
  },
  {
    "arxiv_id": "2412.07097v1",
    "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs",
    "authors": [
      "Xiangyu Qi",
      "Boyi Wei",
      "Nicholas Carlini",
      "Yangsibo Huang",
      "Tinghao Xie",
      "Luxi He",
      "Matthew Jagielski",
      "Milad Nasr",
      "Prateek Mittal",
      "Peter Henderson"
    ],
    "abstract": "Stakeholders -- from model developers to policymakers -- seek to minimize the\ndual-use risks of large language models (LLMs). An open challenge to this goal\nis whether technical safeguards can impede the misuse of LLMs, even when models\nare customizable via fine-tuning or when model weights are fully open. In\nresponse, several recent studies have proposed methods to produce durable LLM\nsafeguards for open-weight LLMs that can withstand adversarial modifications of\nthe model's weights via fine-tuning. This holds the promise of raising\nadversaries' costs even under strong threat models where adversaries can\ndirectly fine-tune model weights. However, in this paper, we urge for more\ncareful characterization of the limits of these approaches. Through several\ncase studies, we demonstrate that even evaluating these defenses is exceedingly\ndifficult and can easily mislead audiences into thinking that safeguards are\nmore durable than they really are. We draw lessons from the evaluation pitfalls\nthat we identify and suggest future research carefully cabin claims to more\nconstrained, well-defined, and rigorously examined threat models, which can\nprovide more useful and candid assessments to stakeholders.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07097v1",
    "published_date": "2024-12-10 01:30:32 UTC",
    "updated_date": "2024-12-10 01:30:32 UTC"
  },
  {
    "arxiv_id": "2412.07096v1",
    "title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization",
    "authors": [
      "Shiyue Zhang",
      "David Wan",
      "Arie Cattan",
      "Ayal Klein",
      "Ido Dagan",
      "Mohit Bansal"
    ],
    "abstract": "How to properly conduct human evaluations for text summarization is a\nlongstanding challenge. The Pyramid human evaluation protocol, which assesses\ncontent selection by breaking the reference summary into sub-units and\nverifying their presence in the system summary, has been widely adopted.\nHowever, it suffers from a lack of systematicity in the definition and\ngranularity of the sub-units. We address these problems by proposing QAPyramid,\nwhich decomposes each reference summary into finer-grained question-answer (QA)\npairs according to the QA-SRL framework. We collect QA-SRL annotations for\nreference summaries from CNN/DM and evaluate 10 summarization systems,\nresulting in 8.9K QA-level annotations. We show that, compared to Pyramid,\nQAPyramid provides more systematic and fine-grained content selection\nevaluation while maintaining high inter-annotator agreement without needing\nexpert annotations. Furthermore, we propose metrics that automate the\nevaluation pipeline and achieve higher correlations with QAPyramid than other\nwidely adopted metrics, allowing future work to accurately and efficiently\nbenchmark summarization systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The first two authors contributed equally. Code:\n  https://github.com/ZhangShiyue/QAPyramid",
    "pdf_url": "http://arxiv.org/pdf/2412.07096v1",
    "published_date": "2024-12-10 01:29:51 UTC",
    "updated_date": "2024-12-10 01:29:51 UTC"
  },
  {
    "arxiv_id": "2412.07094v1",
    "title": "Access Point Deployment for Localizing Accuracy and User Rate in Cell-Free Systems",
    "authors": [
      "Fanfei Xu",
      "Shengheng Liu",
      "Zihuan Mao",
      "Shangqing Shi",
      "Dazhuan Xu",
      "Dongming Wang",
      "Yongming Huang"
    ],
    "abstract": "Evolving next-generation mobile networks is designed to provide ubiquitous\ncoverage and networked sensing. With utility of multi-view sensing and\nmulti-node joint transmission, cell-free is a promising technique to realize\nthis prospect. This paper aims to tackle the problem of access point (AP)\ndeployment in cell-free systems to balance the sensing accuracy and user rate.\nBy merging the D-optimality with Euclidean criterion, a novel integrated metric\nis proposed to be the objective function for both max-sum and max-min problems,\nwhich respectively guarantee the overall and lowest performance in multi-user\ncommunication and target tracking scenario. To solve the corresponding high\ndimensional non-convex multi-objective problem, the Soft actor-critic (SAC) is\nutilized to avoid risk of local optimal result. Numerical results demonstrate\nthat proposed SAC-based APs deployment method achieves $20\\%$ of overall\nperformance and $120\\%$ of lowest performance.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Presented at MobiCom 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.07094v1",
    "published_date": "2024-12-10 01:22:32 UTC",
    "updated_date": "2024-12-10 01:22:32 UTC"
  },
  {
    "arxiv_id": "2501.00009v1",
    "title": "Model-Driven Deep Neural Network for Enhanced AoA Estimation Using 5G gNB",
    "authors": [
      "Shengheng Liu",
      "Xingkang Li",
      "Zihuan Mao",
      "Peng Liu",
      "Yongming Huang"
    ],
    "abstract": "High-accuracy positioning has become a fundamental enabler for intelligent\nconnected devices. Nevertheless, the present wireless networks still rely on\nmodel-driven approaches to achieve positioning functionality, which are\nsusceptible to performance degradation in practical scenarios, primarily due to\nhardware impairments. Integrating artificial intelligence into the positioning\nframework presents a promising solution to revolutionize the accuracy and\nrobustness of location-based services. In this study, we address this challenge\nby reformulating the problem of angle-of-arrival (AoA) estimation into image\nreconstruction of spatial spectrum. To this end, we design a model-driven deep\nneural network (MoD-DNN), which can automatically calibrate the\nangular-dependent phase error. The proposed MoD-DNN approach employs an\niterative optimization scheme between a convolutional neural network and a\nsparse conjugate gradient algorithm. Simulation and experimental results are\npresented to demonstrate the effectiveness of the proposed method in enhancing\nspectrum calibration and AoA estimation.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Presented at AAAI 2024 (Main Technical Track)",
    "pdf_url": "http://arxiv.org/pdf/2501.00009v1",
    "published_date": "2024-12-10 01:16:48 UTC",
    "updated_date": "2024-12-10 01:16:48 UTC"
  },
  {
    "arxiv_id": "2412.07809v1",
    "title": "Fine-grained graph representation learning for heterogeneous mobile networks with attentive fusion and contrastive learning",
    "authors": [
      "Shengheng Liu",
      "Tianqi Zhang",
      "Ningning Fu",
      "Yongming Huang"
    ],
    "abstract": "AI becomes increasingly vital for telecom industry, as the burgeoning\ncomplexity of upcoming mobile communication networks places immense pressure on\nnetwork operators. While there is a growing consensus that intelligent network\nself-driving holds the key, it heavily relies on expert experience and\nknowledge extracted from network data. In an effort to facilitate convenient\nanalytics and utilization of wireless big data, we introduce the concept of\nknowledge graphs into the field of mobile networks, giving rise to what we term\nas wireless data knowledge graphs (WDKGs). However, the heterogeneous and\ndynamic nature of communication networks renders manual WDKG construction both\nprohibitively costly and error-prone, presenting a fundamental challenge. In\nthis context, we propose an unsupervised data-and-model driven graph structure\nlearning (DMGSL) framework, aimed at automating WDKG refinement and updating.\nTackling WDKG heterogeneity involves stratifying the network into homogeneous\nlayers and refining it at a finer granularity. Furthermore, to capture WDKG\ndynamics effectively, we segment the network into static snapshots based on the\ncoherence time and harness the power of recurrent neural networks to\nincorporate historical information. Extensive experiments conducted on the\nestablished WDKG demonstrate the superiority of the DMGSL over the baselines,\nparticularly in terms of node classification accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in AAAI 2025 (Main Technical Track)",
    "pdf_url": "http://arxiv.org/pdf/2412.07809v1",
    "published_date": "2024-12-10 01:12:51 UTC",
    "updated_date": "2024-12-10 01:12:51 UTC"
  },
  {
    "arxiv_id": "2412.07081v1",
    "title": "Sequential Controlled Langevin Diffusions",
    "authors": [
      "Junhua Chen",
      "Lorenz Richter",
      "Julius Berner",
      "Denis Blessing",
      "Gerhard Neumann",
      "Anima Anandkumar"
    ],
    "abstract": "An effective approach for sampling from unnormalized densities is based on\nthe idea of gradually transporting samples from an easy prior to the\ncomplicated target distribution. Two popular methods are (1) Sequential Monte\nCarlo (SMC), where the transport is performed through successive annealed\ndensities via prescribed Markov chains and resampling steps, and (2) recently\ndeveloped diffusion-based sampling methods, where a learned dynamical transport\nis used. Despite the common goal, both approaches have different, often\ncomplementary, advantages and drawbacks. The resampling steps in SMC allow\nfocusing on promising regions of the space, often leading to robust\nperformance. While the algorithm enjoys asymptotic guarantees, the lack of\nflexible, learnable transitions can lead to slow convergence. On the other\nhand, diffusion-based samplers are learned and can potentially better adapt\nthemselves to the target at hand, yet often suffer from training instabilities.\nIn this work, we present a principled framework for combining SMC with\ndiffusion-based samplers by viewing both methods in continuous time and\nconsidering measures on path space. This culminates in the new Sequential\nControlled Langevin Diffusion (SCLD) sampling method, which is able to utilize\nthe benefits of both methods and reaches improved performance on multiple\nbenchmark problems, in many cases using only 10% of the training budget of\nprevious diffusion-based samplers.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07081v1",
    "published_date": "2024-12-10 00:47:10 UTC",
    "updated_date": "2024-12-10 00:47:10 UTC"
  },
  {
    "arxiv_id": "2412.07080v1",
    "title": "EvRepSL: Event-Stream Representation via Self-Supervised Learning for Event-Based Vision",
    "authors": [
      "Qiang Qu",
      "Xiaoming Chen",
      "Yuk Ying Chung",
      "Yiran Shen"
    ],
    "abstract": "Event-stream representation is the first step for many computer vision tasks\nusing event cameras. It converts the asynchronous event-streams into a\nformatted structure so that conventional machine learning models can be applied\neasily. However, most of the state-of-the-art event-stream representations are\nmanually designed and the quality of these representations cannot be guaranteed\ndue to the noisy nature of event-streams. In this paper, we introduce a\ndata-driven approach aiming at enhancing the quality of event-stream\nrepresentations. Our approach commences with the introduction of a new\nevent-stream representation based on spatial-temporal statistics, denoted as\nEvRep. Subsequently, we theoretically derive the intrinsic relationship between\nasynchronous event-streams and synchronous video frames. Building upon this\ntheoretical relationship, we train a representation generator, RepGen, in a\nself-supervised learning manner accepting EvRep as input. Finally, the\nevent-streams are converted to high-quality representations, termed as EvRepSL,\nby going through the learned RepGen (without the need of fine-tuning or\nretraining). Our methodology is rigorously validated through extensive\nevaluations on a variety of mainstream event-based classification and optical\nflow datasets (captured with various types of event cameras). The experimental\nresults highlight not only our approach's superior performance over existing\nevent-stream representations but also its versatility, being agnostic to\ndifferent event cameras and tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Published on IEEE Transactions on Image Processing",
    "pdf_url": "http://arxiv.org/pdf/2412.07080v1",
    "published_date": "2024-12-10 00:42:54 UTC",
    "updated_date": "2024-12-10 00:42:54 UTC"
  },
  {
    "arxiv_id": "2412.07078v1",
    "title": "Defensive Dual Masking for Robust Adversarial Defense",
    "authors": [
      "Wangli Yang",
      "Jie Yang",
      "Yi Guo",
      "Johan Barthelemy"
    ],
    "abstract": "The field of textual adversarial defenses has gained considerable attention\nin recent years due to the increasing vulnerability of natural language\nprocessing (NLP) models to adversarial attacks, which exploit subtle\nperturbations in input text to deceive models. This paper introduces the\nDefensive Dual Masking (DDM) algorithm, a novel approach designed to enhance\nmodel robustness against such attacks. DDM utilizes a unique adversarial\ntraining strategy where [MASK] tokens are strategically inserted into training\nsamples to prepare the model to handle adversarial perturbations more\neffectively. During inference, potentially adversarial tokens are dynamically\nreplaced with [MASK] tokens to neutralize potential threats while preserving\nthe core semantics of the input. The theoretical foundation of our approach is\nexplored, demonstrating how the selective masking mechanism strengthens the\nmodel's ability to identify and mitigate adversarial manipulations. Our\nempirical evaluation across a diverse set of benchmark datasets and attack\nmechanisms consistently shows that DDM outperforms state-of-the-art defense\ntechniques, improving model accuracy and robustness. Moreover, when applied to\nLarge Language Models (LLMs), DDM also enhances their resilience to adversarial\nattacks, providing a scalable defense mechanism for large-scale NLP\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "First version",
    "pdf_url": "http://arxiv.org/pdf/2412.07078v1",
    "published_date": "2024-12-10 00:41:25 UTC",
    "updated_date": "2024-12-10 00:41:25 UTC"
  },
  {
    "arxiv_id": "2412.07066v1",
    "title": "The Mirage of Artificial Intelligence Terms of Use Restrictions",
    "authors": [
      "Peter Henderson",
      "Mark A. Lemley"
    ],
    "abstract": "Artificial intelligence (AI) model creators commonly attach restrictive terms\nof use to both their models and their outputs. These terms typically prohibit\nactivities ranging from creating competing AI models to spreading\ndisinformation. Often taken at face value, these terms are positioned by\ncompanies as key enforceable tools for preventing misuse, particularly in\npolicy dialogs. But are these terms truly meaningful? There are myriad examples\nwhere these broad terms are regularly and repeatedly violated. Yet except for\nsome account suspensions on platforms, no model creator has actually tried to\nenforce these terms with monetary penalties or injunctive relief. This is\nlikely for good reason: we think that the legal enforceability of these\nlicenses is questionable.\n  This Article systematically assesses of the enforceability of AI model terms\nof use and offers three contributions. First, we pinpoint a key problem: the\nartifacts that they protect, namely model weights and model outputs, are\nlargely not copyrightable, making it unclear whether there is even anything to\nbe licensed. Second, we examine the problems this creates for other\nenforcement. Recent doctrinal trends in copyright preemption may further\nundermine state-law claims, while other legal frameworks like the DMCA and CFAA\noffer limited recourse. Anti-competitive provisions likely fare even worse than\nresponsible use provisions. Third, we provide recommendations to policymakers.\nThere are compelling reasons for many provisions to be unenforceable: they\nchill good faith research, constrain competition, and create quasi-copyright\nownership where none should exist. There are, of course, downsides: model\ncreators have fewer tools to prevent harmful misuse. But we think the better\napproach is for statutory provisions, not private fiat, to distinguish between\ngood and bad uses of AI, restricting the latter.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Forthcoming Indiana Law Journal",
    "pdf_url": "http://arxiv.org/pdf/2412.07066v1",
    "published_date": "2024-12-10 00:18:29 UTC",
    "updated_date": "2024-12-10 00:18:29 UTC"
  }
]
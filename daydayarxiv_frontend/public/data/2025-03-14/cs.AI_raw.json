[
  {
    "arxiv_id": "2503.11926v1",
    "title": "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation",
    "authors": [
      "Bowen Baker",
      "Joost Huizinga",
      "Leo Gao",
      "Zehao Dou",
      "Melody Y. Guan",
      "Aleksander Madry",
      "Wojciech Zaremba",
      "Jakub Pachocki",
      "David Farhi"
    ],
    "abstract": "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11926v1",
    "published_date": "2025-03-14 23:50:34 UTC",
    "updated_date": "2025-03-14 23:50:34 UTC"
  },
  {
    "arxiv_id": "2503.11924v1",
    "title": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives",
    "authors": [
      "Kun Su",
      "Krishna Sayana",
      "Hubert Pham",
      "James Pine",
      "Yuri Vasilevski",
      "Raghavendra Vasudeva",
      "Marialena Kyriakidi",
      "Liam Hebert",
      "Ambarish Jash",
      "Anushya Subbiah",
      "Sukhdeep Sodhi"
    ],
    "abstract": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11924v1",
    "published_date": "2025-03-14 23:47:46 UTC",
    "updated_date": "2025-03-14 23:47:46 UTC"
  },
  {
    "arxiv_id": "2503.11918v1",
    "title": "Sketch-to-Skill: Bootstrapping Robot Learning with Human Drawn Trajectory Sketches",
    "authors": [
      "Peihong Yu",
      "Amisha Bhaskar",
      "Anukriti Singh",
      "Zahiruddin Mahammad",
      "Pratap Tokekar"
    ],
    "abstract": "Training robotic manipulation policies traditionally requires numerous\ndemonstrations and/or environmental rollouts. While recent Imitation Learning\n(IL) and Reinforcement Learning (RL) methods have reduced the number of\nrequired demonstrations, they still rely on expert knowledge to collect\nhigh-quality data, limiting scalability and accessibility. We propose\nSketch-to-Skill, a novel framework that leverages human-drawn 2D sketch\ntrajectories to bootstrap and guide RL for robotic manipulation. Our approach\nextends beyond previous sketch-based methods, which were primarily focused on\nimitation learning or policy conditioning, limited to specific trained tasks.\nSketch-to-Skill employs a Sketch-to-3D Trajectory Generator that translates 2D\nsketches into 3D trajectories, which are then used to autonomously collect\ninitial demonstrations. We utilize these sketch-generated demonstrations in two\nways: to pre-train an initial policy through behavior cloning and to refine\nthis policy through RL with guided exploration. Experimental results\ndemonstrate that Sketch-to-Skill achieves ~96% of the performance of the\nbaseline model that leverages teleoperated demonstration data, while exceeding\nthe performance of a pure reinforcement learning policy by ~170%, only from\nsketch inputs. This makes robotic manipulation learning more accessible and\npotentially broadens its applications across various domains.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "Peihong Yu and Amisha Bhaskar contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2503.11918v1",
    "published_date": "2025-03-14 23:08:29 UTC",
    "updated_date": "2025-03-14 23:08:29 UTC"
  },
  {
    "arxiv_id": "2503.11917v3",
    "title": "A Framework for Evaluating Emerging Cyberattack Capabilities of AI",
    "authors": [
      "Mikel Rodriguez",
      "Raluca Ada Popa",
      "Four Flynn",
      "Lihao Liang",
      "Allan Dafoe",
      "Anna Wang"
    ],
    "abstract": "As frontier AI models become more capable, evaluating their potential to\nenable cyberattacks is crucial for ensuring the safe development of Artificial\nGeneral Intelligence (AGI). Current cyber evaluation efforts are often ad-hoc,\nlacking systematic analysis of attack phases and guidance on targeted defenses.\nThis work introduces a novel evaluation framework that addresses these\nlimitations by: (1) examining the end-to-end attack chain, (2) identifying gaps\nin AI threat evaluation, and (3) helping defenders prioritize targeted\nmitigations and conduct AI-enabled adversary emulation for red teaming. Our\napproach adapts existing cyberattack chain frameworks for AI systems. We\nanalyzed over 12,000 real-world instances of AI involvement in cyber incidents,\ncatalogued by Google's Threat Intelligence Group, to curate seven\nrepresentative attack chain archetypes. Through a bottleneck analysis on these\narchetypes, we pinpointed phases most susceptible to AI-driven disruption. We\nthen identified and utilized externally developed cybersecurity model\nevaluations focused on these critical phases. We report on AI's potential to\namplify offensive capabilities across specific attack stages, and offer\nrecommendations for prioritizing defenses. We believe this represents the most\ncomprehensive AI cyber risk evaluation framework published to date.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11917v3",
    "published_date": "2025-03-14 23:05:02 UTC",
    "updated_date": "2025-04-21 19:22:25 UTC"
  },
  {
    "arxiv_id": "2503.11915v1",
    "title": "How Problematic Writer-AI Interactions (Rather than Problematic AI) Hinder Writers' Idea Generation",
    "authors": [
      "Khonzoda Umarova",
      "Talia Wise",
      "Zhuoer Lyu",
      "Mina Lee",
      "Qian Yang"
    ],
    "abstract": "Writing about a subject enriches writers' understanding of that subject. This\ncognitive benefit of writing -- known as constructive learning -- is essential\nto how students learn in various disciplines. However, does this benefit\npersist when students write with generative AI writing assistants? Prior\nresearch suggests the answer varies based on the type of AI, e.g.,\nauto-complete systems tend to hinder ideation, while assistants that pose\nSocratic questions facilitate it. This paper adds an additional perspective.\nThrough a case study, we demonstrate that the impact of genAI on students' idea\ndevelopment depends not only on the AI but also on the students and, crucially,\ntheir interactions in between. Students who proactively explored ideas gained\nnew ideas from writing, regardless of whether they used auto-complete or\nSocratic AI assistants. Those who engaged in prolonged, mindless copyediting\ndeveloped few ideas even with a Socratic AI. These findings suggest\nopportunities in designing AI writing assistants, not merely by creating more\nthought-provoking AI, but also by fostering more thought-provoking writer-AI\ninteractions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11915v1",
    "published_date": "2025-03-14 22:53:53 UTC",
    "updated_date": "2025-03-14 22:53:53 UTC"
  },
  {
    "arxiv_id": "2503.11910v1",
    "title": "RTD-Lite: Scalable Topological Analysis for Comparing Weighted Graphs in Learning Tasks",
    "authors": [
      "Eduard Tulchinskii",
      "Daria Voronkova",
      "Ilya Trofimov",
      "Evgeny Burnaev",
      "Serguei Barannikov"
    ],
    "abstract": "Topological methods for comparing weighted graphs are valuable in various\nlearning tasks but often suffer from computational inefficiency on large\ndatasets. We introduce RTD-Lite, a scalable algorithm that efficiently compares\ntopological features, specifically connectivity or cluster structures at\narbitrary scales, of two weighted graphs with one-to-one correspondence between\nvertices. Using minimal spanning trees in auxiliary graphs, RTD-Lite captures\ntopological discrepancies with $O(n^2)$ time and memory complexity. This\nefficiency enables its application in tasks like dimensionality reduction and\nneural network training. Experiments on synthetic and real-world datasets\ndemonstrate that RTD-Lite effectively identifies topological differences while\nsignificantly reducing computation time compared to existing methods. Moreover,\nintegrating RTD-Lite into neural network training as a loss function component\nenhances the preservation of topological structures in learned representations.\nOur code is publicly available at https://github.com/ArGintum/RTD-Lite",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.SG"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for AISTATS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11910v1",
    "published_date": "2025-03-14 22:42:13 UTC",
    "updated_date": "2025-03-14 22:42:13 UTC"
  },
  {
    "arxiv_id": "2503.11908v1",
    "title": "Revisiting FastMap: New Applications",
    "authors": [
      "Ang Li"
    ],
    "abstract": "FastMap was first introduced in the Data Mining community for generating\nEuclidean embeddings of complex objects. In this dissertation, we first present\nFastMap to generate Euclidean embeddings of graphs in near-linear time: The\npairwise Euclidean distances approximate a desired graph-based distance\nfunction on the vertices. We then apply the graph version of FastMap to\nefficiently solve various graph-theoretic problems of significant interest in\nAI: including facility location, top-K centrality computations, community\ndetection and block modeling, and graph convex hull computations. We also\npresent a novel learning framework, called FastMapSVM, by combining FastMap and\nSupport Vector Machines. We then apply FastMapSVM to predict the satisfiability\nof Constraint Satisfaction Problems and to classify seismograms in Earthquake\nScience.",
    "categories": [
      "cs.DM",
      "cs.AI"
    ],
    "primary_category": "cs.DM",
    "comment": "PhD dissertation",
    "pdf_url": "http://arxiv.org/pdf/2503.11908v1",
    "published_date": "2025-03-14 22:29:10 UTC",
    "updated_date": "2025-03-14 22:29:10 UTC"
  },
  {
    "arxiv_id": "2503.11906v1",
    "title": "A Survey on SAR ship classification using Deep Learning",
    "authors": [
      "Ch Muhammad Awais",
      "Marco Reggiannini",
      "Davide Moroni",
      "Emanuele Salerno"
    ],
    "abstract": "Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture\nRadar (SAR) ship classification. This survey comprehensively analyzes the\ndiverse DL techniques employed in this domain. We identify critical trends and\nchallenges, highlighting the importance of integrating handcrafted features,\nutilizing public datasets, data augmentation, fine-tuning, explainability\ntechniques, and fostering interdisciplinary collaborations to improve DL model\nperformance. This survey establishes a first-of-its-kind taxonomy for\ncategorizing relevant research based on DL models, handcrafted feature use, SAR\nattribute utilization, and the impact of fine-tuning. We discuss the\nmethodologies used in SAR ship classification tasks and the impact of different\ntechniques. Finally, the survey explores potential avenues for future research,\nincluding addressing data scarcity, exploring novel DL architectures,\nincorporating interpretability techniques, and establishing standardized\nperformance metrics. By addressing these challenges and leveraging advancements\nin DL, researchers can contribute to developing more accurate and efficient\nship classification systems, ultimately enhancing maritime surveillance and\nrelated applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to JSTARS journal",
    "pdf_url": "http://arxiv.org/pdf/2503.11906v1",
    "published_date": "2025-03-14 22:19:24 UTC",
    "updated_date": "2025-03-14 22:19:24 UTC"
  },
  {
    "arxiv_id": "2503.11905v1",
    "title": "Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities",
    "authors": [
      "Ruchika Chavhan",
      "Abhinav Mehrotra",
      "Malcolm Chadwick",
      "Alberto Gil Ramos",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Sourav Bhattacharya"
    ],
    "abstract": "Text-to-image synthesis has witnessed remarkable advancements in recent\nyears. Many attempts have been made to adopt text-to-image models to support\nmultiple tasks. However, existing approaches typically require\nresource-intensive re-training or additional parameters to accommodate for the\nnew tasks, which makes the model inefficient for on-device deployment. We\npropose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends\nthe capabilities of a pre-trained text-to-image diffusion model to support a\nvariety of image-to-image generation tasks. MTU replaces Feed-Forward Network\n(FFN) layers in the diffusion model with smaller FFNs, referred to as experts,\nand combines them with a dynamic routing mechanism. To the best of our\nknowledge, MTU is the first multi-task diffusion modeling approach that\nseamlessly blends multi-tasking with on-device compatibility, by mitigating the\nissue of parameter inflation. We show that the performance of MTU is on par\nwith the single-task fine-tuned diffusion models across several tasks including\nimage editing, super-resolution, and inpainting, while maintaining similar\nlatency and computational load (GFLOPs) as the single-task fine-tuned models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.11905v1",
    "published_date": "2025-03-14 22:19:20 UTC",
    "updated_date": "2025-03-14 22:19:20 UTC"
  },
  {
    "arxiv_id": "2503.11901v2",
    "title": "Characterizing GPU Resilience and Impact on AI/HPC Systems",
    "authors": [
      "Shengkun Cui",
      "Archit Patke",
      "Ziheng Chen",
      "Aditya Ranjan",
      "Hung Nguyen",
      "Phuong Cao",
      "Saurabh Jha",
      "Brett Bode",
      "Gregory Bauer",
      "Chandra Narayanaswami",
      "Daby Sow",
      "Catello Di Martino",
      "Zbigniew T. Kalbarczyk",
      "Ravishankar K. Iyer"
    ],
    "abstract": "In this study, we characterize GPU failures in Delta, the current large-scale\nAI system with over 600 petaflops of peak compute throughput. The system\ncomprises GPU and non-GPU nodes with modern AI accelerators, such as NVIDIA\nA40, A100, and H100 GPUs. The study uses two and a half years of data on GPU\nerrors. We evaluate the resilience of GPU hardware components to determine the\nvulnerability of different GPU components to failure and their impact on the\nGPU and node availability. We measure the key propagation paths in GPU\nhardware, GPU interconnect (NVLink), and GPU memory. Finally, we evaluate the\nimpact of the observed GPU errors on user jobs. Our key findings are: (i)\nContrary to common beliefs, GPU memory is over 30x more reliable than GPU\nhardware in terms of MTBE (mean time between errors). (ii) The newly introduced\nGSP (GPU System Processor) is the most vulnerable GPU hardware component. (iii)\nNVLink errors did not always lead to user job failure, and we attribute it to\nthe underlying error detection and retry mechanisms employed. (iv) We show\nmultiple examples of hardware errors originating from one of the key GPU\nhardware components, leading to application failure. (v) We project the impact\nof GPU node availability on larger scales with emulation and find that\nsignificant overprovisioning between 5-20% would be necessary to handle GPU\nfailures. If GPU availability were improved to 99.9%, the overprovisioning\nwould be reduced by 4x.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11901v2",
    "published_date": "2025-03-14 22:14:18 UTC",
    "updated_date": "2025-03-24 03:52:43 UTC"
  },
  {
    "arxiv_id": "2503.11898v1",
    "title": "LLMs for Translation: Historical, Low-Resourced Languages and Contemporary AI Models",
    "authors": [
      "Merve Tekgurler"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable adaptability in\nperforming various tasks, including machine translation (MT), without explicit\ntraining. Models such as OpenAI's GPT-4 and Google's Gemini are frequently\nevaluated on translation benchmarks and utilized as translation tools due to\ntheir high performance. This paper examines Gemini's performance in translating\nan 18th-century Ottoman Turkish manuscript, Prisoner of the Infidels: The\nMemoirs of Osman Agha of Timisoara, into English. The manuscript recounts the\nexperiences of Osman Agha, an Ottoman subject who spent 11 years as a prisoner\nof war in Austria, and includes his accounts of warfare and violence. Our\nanalysis reveals that Gemini's safety mechanisms flagged between 14 and 23\npercent of the manuscript as harmful, resulting in untranslated passages. These\nsafety settings, while effective in mitigating potential harm, hinder the\nmodel's ability to provide complete and accurate translations of historical\ntexts. Through real historical examples, this study highlights the inherent\nchallenges and limitations of current LLM safety implementations in the\nhandling of sensitive and context-rich materials. These real-world instances\nunderscore potential failures of LLMs in contemporary translation scenarios,\nwhere accurate and comprehensive translations are crucial-for example,\ntranslating the accounts of modern victims of war for legal proceedings or\nhumanitarian documentation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LaTeCH-CLfL 2025, held in conjunction with NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11898v1",
    "published_date": "2025-03-14 21:59:12 UTC",
    "updated_date": "2025-03-14 21:59:12 UTC"
  },
  {
    "arxiv_id": "2503.14522v1",
    "title": "Accessibility Considerations in the Development of an AI Action Plan",
    "authors": [
      "Jennifer Mankoff",
      "Janice Light",
      "James Coughlan",
      "Christian Vogler",
      "Abraham Glasser",
      "Gregg Vanderheiden",
      "Laura Rice"
    ],
    "abstract": "We argue that there is a need for Accessibility to be represented in several\nimportant domains:\n  - Capitalize on the new capabilities AI provides - Support for open source\ndevelopment of AI, which can allow disabled and disability focused\nprofessionals to contribute, including\n  - Development of Accessibility Apps which help realise the promise of AI in\naccessibility domains\n  - Open Source Model Development and Validation to ensure that accessibility\nconcerns are addressed in these algorithms\n  - Data Augmentation to include accessibility in data sets used to train\nmodels\n  - Accessible Interfaces that allow disabled people to use any AI app, and to\nvalidate its outputs\n  - Dedicated Functionality and Libraries that can make it easy to integrate AI\nsupport into a variety of settings and apps. - Data security and privacy and\nprivacy risks including data collected by AI based accessibility technologies;\nand the possibility of disability disclosure. - Disability-specific AI risks\nand biases including both direct bias (during AI use by the disabled person)\nand indirect bias (when AI is used by someone else on data relating to a\ndisabled person).",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14522v1",
    "published_date": "2025-03-14 21:57:23 UTC",
    "updated_date": "2025-03-14 21:57:23 UTC"
  },
  {
    "arxiv_id": "2503.11896v1",
    "title": "Expressive Music Data Processing and Generation",
    "authors": [
      "Jingwei Liu"
    ],
    "abstract": "Musical expressivity and coherence are indispensable in music composition and\nperformance, while often neglected in modern AI generative models. In this\nwork, we introduce a listening-based data-processing technique that captures\nthe expressivity in musical performance. This technique derived from Weber's\nlaw reflects the human perceptual truth of listening and preserves musical\nsubtlety and expressivity in the training input. To facilitate musical\ncoherence, we model the output interdependencies among multiple arguments in\nthe music data such as pitch, duration, velocity, etc. in the neural networks\nbased on the probabilistic chain rule. In practice, we decompose the\nmulti-output sequential model into single-output submodels and condition\npreviously sampled outputs on the subsequent submodels to induce conditional\ndistributions. Finally, to select eligible sequences from all generations, a\ntentative measure based on the output entropy was proposed. The entropy\nsequence is set as a criterion to select predictable and stable generations,\nwhich is further studied under the context of informational aesthetic measures\nto quantify musical pleasure and information gain along the music tendency.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11896v1",
    "published_date": "2025-03-14 21:56:07 UTC",
    "updated_date": "2025-03-14 21:56:07 UTC"
  },
  {
    "arxiv_id": "2503.11895v1",
    "title": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing",
    "authors": [
      "Bhiman Kumar Baghel",
      "Scott M. Jordan",
      "Zheyuan Ryan Shi",
      "Xiang Lorraine Li"
    ],
    "abstract": "Large Language Models (LLMs) are used in various downstream language tasks,\nmaking it crucial to keep their knowledge up-to-date, but both retraining and\nfine-tuning the model can be costly. Model editing offers an efficient and\neffective alternative by a single update to only a key subset of model\nparameters. While being efficient, these methods are not perfect. Sometimes\nknowledge edits are unsuccessful, i.e., UnderEdit, or the edit contaminated\nneighboring knowledge that should remain unchanged, i.e., OverEdit. To address\nthese limitations, we propose iterative model editing, based on our hypothesis\nthat a single parameter update is often insufficient, to mitigate UnderEdit,\nand neighbor-assisted model editing, which incorporates neighboring knowledge\nduring editing to minimize OverEdit. Extensive experiments demonstrate that our\nmethods effectively reduce UnderEdit up to 38 percentage points and OverEdit up\nto 6 percentage points across multiple model editing algorithms, LLMs, and\nbenchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review @ ACL'25",
    "pdf_url": "http://arxiv.org/pdf/2503.11895v1",
    "published_date": "2025-03-14 21:53:12 UTC",
    "updated_date": "2025-03-14 21:53:12 UTC"
  },
  {
    "arxiv_id": "2503.15542v1",
    "title": "Identifying Likely-Reputable Blockchain Projects on Ethereum",
    "authors": [
      "Cyrus Malik",
      "Josef Bajada",
      "Joshua Ellul"
    ],
    "abstract": "Identifying reputable Ethereum projects remains a critical challenge within\nthe expanding blockchain ecosystem. The ability to distinguish between\nlegitimate initiatives and potentially fraudulent schemes is non-trivial. This\nwork presents a systematic approach that integrates multiple data sources with\nadvanced analytics to evaluate credibility, transparency, and overall\ntrustworthiness. The methodology applies machine learning techniques to analyse\ntransaction histories on the Ethereum blockchain.\n  The study classifies accounts based on a dataset comprising 2,179 entities\nlinked to illicit activities and 3,977 associated with reputable projects.\nUsing the LightGBM algorithm, the approach achieves an average accuracy of\n0.984 and an average AUC of 0.999, validated through 10-fold cross-validation.\nKey influential factors include time differences between transactions and\nreceived_tnx.\n  The proposed methodology provides a robust mechanism for identifying\nreputable Ethereum projects, fostering a more secure and transparent investment\nenvironment. By equipping stakeholders with data-driven insights, this research\nenables more informed decision-making, risk mitigation, and the promotion of\nlegitimate blockchain initiatives. Furthermore, it lays the foundation for\nfuture advancements in trust assessment methodologies, contributing to the\ncontinued development and maturity of the Ethereum ecosystem.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15542v1",
    "published_date": "2025-03-14 21:43:25 UTC",
    "updated_date": "2025-03-14 21:43:25 UTC"
  },
  {
    "arxiv_id": "2503.13522v3",
    "title": "Advanced Deep Learning Methods for Protein Structure Prediction and Design",
    "authors": [
      "Yichao Zhang",
      "Ningyuan Deng",
      "Xinyuan Song",
      "Ziqian Bi",
      "Tianyang Wang",
      "Zheyu Yao",
      "Keyu Chen",
      "Ming Li",
      "Qian Niu",
      "Junyu Liu",
      "Benji Peng",
      "Sen Zhang",
      "Ming Liu",
      "Li Zhang",
      "Xuanhe Pan",
      "Jinlang Wang",
      "Pohsun Feng",
      "Yizhu Wen",
      "Lawrence KQ Yan",
      "Hongming Tseng",
      "Yan Zhong",
      "Yunze Wang",
      "Ziyuan Qin",
      "Bowen Jing",
      "Junjie Yang",
      "Jun Zhou",
      "Chia Xin Liang",
      "Junhao Song"
    ],
    "abstract": "After AlphaFold won the Nobel Prize, protein prediction with deep learning\nonce again became a hot topic. We comprehensively explore advanced deep\nlearning methods applied to protein structure prediction and design. It begins\nby examining recent innovations in prediction architectures, with detailed\ndiscussions on improvements such as diffusion based frameworks and novel\npairwise attention modules. The text analyses key components including\nstructure generation, evaluation metrics, multiple sequence alignment\nprocessing, and network architecture, thereby illustrating the current state of\nthe art in computational protein modelling. Subsequent chapters focus on\npractical applications, presenting case studies that range from individual\nprotein predictions to complex biomolecular interactions. Strategies for\nenhancing prediction accuracy and integrating deep learning techniques with\nexperimental validation are thoroughly explored. The later sections review the\nindustry landscape of protein design, highlighting the transformative role of\nartificial intelligence in biotechnology and discussing emerging market trends\nand future challenges. Supplementary appendices provide essential resources\nsuch as databases and open source tools, making this volume a valuable\nreference for researchers and students.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13522v3",
    "published_date": "2025-03-14 21:28:29 UTC",
    "updated_date": "2025-03-29 13:08:27 UTC"
  },
  {
    "arxiv_id": "2503.11880v1",
    "title": "FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-the-World LoRA",
    "authors": [
      "Jieming Bian",
      "Lei Wang",
      "Letian Zhang",
      "Jie Xu"
    ],
    "abstract": "Fine-tuning large language models (LLMs) in federated settings enables\nprivacy-preserving adaptation but suffers from cross-client interference due to\nmodel aggregation. Existing federated LoRA fine-tuning methods, primarily based\non FedAvg, struggle with data heterogeneity, leading to harmful cross-client\ninterference and suboptimal personalization. In this work, we propose\n\\textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that\nfundamentally departs from FedAvg. Instead of using an aggregated model to\ninitialize local training, each client continues training its individual LoRA\nwhile incorporating shared knowledge through a separate Rest-of-the-World\n(RoTW) LoRA component. To effectively balance local adaptation and global\ninformation, FedALT introduces an adaptive mixer that dynamically learns\ninput-specific weightings between the individual and RoTW LoRA components using\nthe Mixture-of-Experts (MoE) principle. Through extensive experiments on NLP\nbenchmarks, we demonstrate that FedALT significantly outperforms\nstate-of-the-art personalized federated LoRA fine-tuning methods, achieving\nsuperior local adaptation without sacrificing computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11880v1",
    "published_date": "2025-03-14 21:07:46 UTC",
    "updated_date": "2025-03-14 21:07:46 UTC"
  },
  {
    "arxiv_id": "2503.11870v1",
    "title": "Counterfactual Realizability",
    "authors": [
      "Arvind Raghavan",
      "Elias Bareinboim"
    ],
    "abstract": "It is commonly believed that, in a real-world environment, samples can only\nbe drawn from observational and interventional distributions, corresponding to\nLayers 1 and 2 of the Pearl Causal Hierarchy. Layer 3, representing\ncounterfactual distributions, is believed to be inaccessible by definition.\nHowever, Bareinboim, Forney, and Pearl (2015) introduced a procedure that\nallows an agent to sample directly from a counterfactual distribution, leaving\nopen the question of what other counterfactual quantities can be estimated\ndirectly via physical experimentation. We resolve this by introducing a formal\ndefinition of realizability, the ability to draw samples from a distribution,\nand then developing a complete algorithm to determine whether an arbitrary\ncounterfactual distribution is realizable given fundamental physical\nconstraints, such as the inability to go back in time and subject the same unit\nto a different experimental condition. We illustrate the implications of this\nnew framework for counterfactual data collection using motivating examples from\ncausal fairness and causal reinforcement learning. While the baseline approach\nin these motivating settings typically follows an interventional or\nobservational strategy, we show that a counterfactual strategy provably\ndominates both.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "F.4.1; G.3"
    ],
    "primary_category": "cs.AI",
    "comment": "published at ICLR'25 (spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2503.11870v1",
    "published_date": "2025-03-14 20:54:27 UTC",
    "updated_date": "2025-03-14 20:54:27 UTC"
  },
  {
    "arxiv_id": "2503.11851v2",
    "title": "DCAT: Dual Cross-Attention Fusion for Disease Classification in Radiological Images with Uncertainty Estimation",
    "authors": [
      "Jutika Borah",
      "Hidam Kumarjit Singh"
    ],
    "abstract": "Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "18 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.11851v2",
    "published_date": "2025-03-14 20:28:20 UTC",
    "updated_date": "2025-03-19 12:18:48 UTC"
  },
  {
    "arxiv_id": "2503.11846v1",
    "title": "From Pixels to Histopathology: A Graph-Based Framework for Interpretable Whole Slide Image Analysis",
    "authors": [
      "Alexander Weers",
      "Alexander H. Berger",
      "Laurin Lux",
      "Peter Schüffler",
      "Daniel Rueckert",
      "Johannes C. Paetzold"
    ],
    "abstract": "The histopathological classification of whole-slide images (WSIs) is a\nfundamental task in digital pathology; yet it requires extensive time and\nexpertise from specialists. While deep learning methods show promising results,\nthey typically process WSIs by dividing them into artificial patches, which\ninherently prevents a network from learning from the entire image context,\ndisregards natural tissue structures and compromises interpretability. Our\nmethod overcomes this limitation through a novel graph-based framework that\nconstructs WSI graph representations. The WSI-graph efficiently captures\nessential histopathological information in a compact form. We build tissue\nrepresentations (nodes) that follow biological boundaries rather than arbitrary\npatches all while providing interpretable features for explainability. Through\nadaptive graph coarsening guided by learned embeddings, we progressively merge\nregions while maintaining discriminative local features and enabling efficient\nglobal information exchange. In our method's final step, we solve the\ndiagnostic task through a graph attention network. We empirically demonstrate\nstrong performance on multiple challenging tasks such as cancer stage\nclassification and survival prediction, while also identifying predictive\nfactors using Integrated Gradients. Our implementation is publicly available at\nhttps://github.com/HistoGraph31/pix2pathology",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11846v1",
    "published_date": "2025-03-14 20:15:04 UTC",
    "updated_date": "2025-03-14 20:15:04 UTC"
  },
  {
    "arxiv_id": "2503.11836v1",
    "title": "Transfer Learning for Automated Feedback Generation on Small Datasets",
    "authors": [
      "Oscar Morris"
    ],
    "abstract": "Feedback is a very important part the learning process. However, it is\nchallenging to make this feedback both timely and accurate when relying on\nhuman markers. This is the challenge that Automated Feedback Generation\nattempts to address. In this paper, a technique to train such a system on a\nvery small dataset with very long sequences is presented. Both of these\nattributes make this a very challenging task, however, by using a three stage\ntransfer learning pipeline state-of-the-art results can be achieved with\nqualitatively accurate but unhuman sounding results. The use of both Automated\nEssay Scoring and Automated Feedback Generation systems in the real world is\nalso discussed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11836v1",
    "published_date": "2025-03-14 19:57:54 UTC",
    "updated_date": "2025-03-14 19:57:54 UTC"
  },
  {
    "arxiv_id": "2503.11833v2",
    "title": "Adaptive Stochastic Gradient Descents on Manifolds with an Application on Weighted Low-Rank Approximation",
    "authors": [
      "Peiqi Yang",
      "Conglong Xu",
      "Hao Wu"
    ],
    "abstract": "We prove a convergence theorem for stochastic gradient descents on manifolds\nwith adaptive learning rate and apply it to the weighted low-rank approximation\nproblem.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "41A60, 53Z50, 62L20, 68T05"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11833v2",
    "published_date": "2025-03-14 19:56:07 UTC",
    "updated_date": "2025-03-29 01:05:48 UTC"
  },
  {
    "arxiv_id": "2503.14521v1",
    "title": "Policy Frameworks for Transparent Chain-of-Thought Reasoning in Large Language Models",
    "authors": [
      "Yihang Chen",
      "Haikang Deng",
      "Kaiqiao Han",
      "Qingyue Zhao"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\ndecomposing complex problems into step-by-step solutions, improving performance\non reasoning tasks. However, current CoT disclosure policies vary widely across\ndifferent models in frontend visibility, API access, and pricing strategies,\nlacking a unified policy framework. This paper analyzes the dual-edged\nimplications of full CoT disclosure: while it empowers small-model\ndistillation, fosters trust, and enables error diagnosis, it also risks\nviolating intellectual property, enabling misuse, and incurring operational\ncosts. We propose a tiered-access policy framework that balances transparency,\naccountability, and security by tailoring CoT availability to academic,\nbusiness, and general users through ethical licensing, structured reasoning\noutputs, and cross-tier safeguards. By harmonizing accessibility with ethical\nand operational considerations, this framework aims to advance responsible AI\ndeployment while mitigating risks of misuse or misinterpretation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14521v1",
    "published_date": "2025-03-14 19:54:18 UTC",
    "updated_date": "2025-03-14 19:54:18 UTC"
  },
  {
    "arxiv_id": "2503.11832v1",
    "title": "Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-tuning",
    "authors": [
      "Yiwei Chen",
      "Yuguang Yao",
      "Yihua Zhang",
      "Bingquan Shen",
      "Gaowen Liu",
      "Sijia Liu"
    ],
    "abstract": "Recent vision-language models (VLMs) have made remarkable strides in\ngenerative modeling with multimodal inputs, particularly text and images.\nHowever, their susceptibility to generating harmful content when exposed to\nunsafe queries raises critical safety concerns. While current alignment\nstrategies primarily rely on supervised safety fine-tuning with curated\ndatasets, we identify a fundamental limitation we call the \"safety mirage\"\nwhere supervised fine-tuning inadvertently reinforces spurious correlations\nbetween superficial textual patterns and safety responses, rather than\nfostering deep, intrinsic mitigation of harm. We show that these spurious\ncorrelations leave fine-tuned VLMs vulnerable even to a simple one-word\nmodification-based attack, where substituting a single word in text queries\nwith a spurious correlation-inducing alternative can effectively bypass\nsafeguards. Additionally, these correlations contribute to the over prudence,\ncausing fine-tuned VLMs to refuse benign queries unnecessarily. To address this\nissue, we show machine unlearning (MU) as a powerful alternative to supervised\nsafety fine-tuning as it avoids biased feature-label mappings and directly\nremoves harmful knowledge from VLMs while preserving their general\ncapabilities. Extensive evaluations across safety benchmarks show that under\none-word attacks, MU-based alignment reduces the attack success rate by up to\n60.17% and cuts unnecessary rejections by over 84.20%. Codes are available at\nhttps://github.com/OPTML-Group/VLM-Safety-MU. WARNING: There exist AI\ngenerations that may be offensive in nature.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11832v1",
    "published_date": "2025-03-14 19:52:08 UTC",
    "updated_date": "2025-03-14 19:52:08 UTC"
  },
  {
    "arxiv_id": "2503.11824v1",
    "title": "Semi-Supervised Co-Training of Time and Time-Frequency Models: Application to Bearing Fault Diagnosis",
    "authors": [
      "Tuomas Jalonen",
      "Mohammad Al-Sa'd",
      "Serkan Kiranyaz",
      "Moncef Gabbouj"
    ],
    "abstract": "Neural networks require massive amounts of annotated data to train\nintelligent solutions. Acquiring many labeled data in industrial applications\nis often difficult; therefore, semi-supervised approaches are preferred. We\npropose a new semi-supervised co-training method, which combines time and\ntime-frequency (TF) machine learning models to improve performance and\nreliability. The developed framework collaboratively co-trains fast time-domain\nmodels by utilizing high-performing TF techniques without increasing the\ninference complexity. Besides, it operates in cloud-edge networks and offers\nholistic support for many applications covering edge-real-time monitoring and\ncloud-based updates and corrections. Experimental results on bearing fault\ndiagnosis verify the superiority of our technique compared to a competing\nself-training method. The results from two case studies show that our method\noutperforms self-training for different noise levels and amounts of available\ndata with accuracy gains reaching from 10.6% to 33.9%. They demonstrate that\nfusing time-domain and TF-based models offers opportunities for developing\nhigh-performance industrial solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11824v1",
    "published_date": "2025-03-14 19:24:38 UTC",
    "updated_date": "2025-03-14 19:24:38 UTC"
  },
  {
    "arxiv_id": "2503.11820v1",
    "title": "An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models",
    "authors": [
      "Antonio Lorenzin",
      "Fabio Zanasi"
    ],
    "abstract": "Moralisation and Triangulation are transformations allowing to switch between\ndifferent ways of factoring a probability distribution into a graphical model.\nMoralisation allows to view a Bayesian network (a directed model) as a Markov\nnetwork (an undirected model), whereas triangulation works in the opposite\ndirection. We present a categorical framework where these transformations are\nmodelled as functors between a category of Bayesian networks and one of Markov\nnetworks. The two kinds of network (the objects of these categories) are\nthemselves represented as functors, from a `syntax' domain to a `semantics'\ncodomain. Notably, moralisation and triangulation are definable inductively on\nsuch syntax, and operate as a form of functor pre-composition. This approach\nintroduces a modular, algebraic perspective in the theory of probabilistic\ngraphical models.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "math.CT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11820v1",
    "published_date": "2025-03-14 19:16:41 UTC",
    "updated_date": "2025-03-14 19:16:41 UTC"
  },
  {
    "arxiv_id": "2503.11807v1",
    "title": "Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images",
    "authors": [
      "Sanayya A",
      "Amoolya Shetty",
      "Abhijeet Sharma",
      "Venkatesh Ravichandran",
      "Masthan Wali Gosuvarapalli",
      "Sarthak Jain",
      "Priyamvada Nanjundiah",
      "Ujjal Kr Dutta",
      "Divya Sharma"
    ],
    "abstract": "In agricultural management, precise Ground Truth (GT) data is crucial for\naccurate Machine Learning (ML) based crop classification. Yet, issues like crop\nmislabeling and incorrect land identification are common. We propose a\nmulti-level GT cleaning framework while utilizing multi-temporal Sentinel-2\ndata to address these issues. Specifically, this framework utilizes generating\nembeddings for farmland, clustering similar crop profiles, and identification\nof outliers indicating GT errors. We validated clusters with False Colour\nComposite (FCC) checks and used distance-based metrics to scale and automate\nthis verification process. The importance of cleaning the GT data became\napparent when the models were trained on the clean and unclean data. For\ninstance, when we trained a Random Forest model with the clean GT data, we\nachieved upto 70\\% absolute percentage points higher for the F1 score metric.\nThis approach advances crop classification methodologies, with potential for\napplications towards improving loan underwriting and agricultural\ndecision-making.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted In IEEE India Geoscience and Remote Sensing Symposium\n  (InGARSS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.11807v1",
    "published_date": "2025-03-14 18:50:30 UTC",
    "updated_date": "2025-03-14 18:50:30 UTC"
  },
  {
    "arxiv_id": "2503.11794v1",
    "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection",
    "authors": [
      "Bangzheng Li",
      "Fei Wang",
      "Wenxuan Zhou",
      "Nan Xu",
      "Ben Zhou",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "abstract": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11794v1",
    "published_date": "2025-03-14 18:33:31 UTC",
    "updated_date": "2025-03-14 18:33:31 UTC"
  },
  {
    "arxiv_id": "2503.11790v1",
    "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
    "authors": [
      "Nasim Borazjanizadeh",
      "Roei Herzig",
      "Eduard Oks",
      "Trevor Darrell",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ],
    "abstract": "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11790v1",
    "published_date": "2025-03-14 18:27:02 UTC",
    "updated_date": "2025-03-14 18:27:02 UTC"
  },
  {
    "arxiv_id": "2503.13518v1",
    "title": "Examples as the Prompt: A Scalable Approach for Efficient LLM Adaptation in E-Commerce",
    "authors": [
      "Jingying Zeng",
      "Zhenwei Dai",
      "Hui Liu",
      "Samarth Varshney",
      "Zhiji Liu",
      "Chen Luo",
      "Zhen Li",
      "Qi He",
      "Xianfeng Tang"
    ],
    "abstract": "Prompting LLMs offers an efficient way to guide output generation without\nexplicit model training. In the e-commerce domain, prompting-based applications\nare widely used for tasks such as query understanding, recommender systems, and\ncustomer support. However, adapting LLMs to different tasks often requires\nextensive prompt engineering by domain experts, along with frequent updates to\nalign with evolving business needs. Additionally, crafting fully unbiased\nnatural language prompts remains a challenge for humans. To address these\nchallenges, we propose a novel framework, Examples as the Prompt (EaP) which\nleverages labeled data to enhance prompts. Specifically, EaP automatically\nselects the most representative examples to maximize the few-shot capability of\nLLMs. It is efficient due to its unsupervised example selection and adaptive to\npotential data distribution shifts. We validate EaP on four real-world\nproduction use cases, demonstrating that it achieves comparable or even\nsuperior performance comparing to hand-crafted prompts designed by domain\nexperts. Additionally, we introduce EaP_lite, which entirely replaces the\nnatural language components of prompts with labeled examples. EaP_lite improves\nLLM inference speed by up to 70% without compromising performance. Latest\nonline A/B test shows that using EaP and EaP_lite for data labeling can bring\nsignificant composite revenue gain by 0.06%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13518v1",
    "published_date": "2025-03-14 18:22:43 UTC",
    "updated_date": "2025-03-14 18:22:43 UTC"
  },
  {
    "arxiv_id": "2503.11650v1",
    "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
    "authors": [
      "Chonghao Sima",
      "Kashyap Chitta",
      "Zhiding Yu",
      "Shiyi Lan",
      "Ping Luo",
      "Andreas Geiger",
      "Hongyang Li",
      "Jose M. Alvarez"
    ],
    "abstract": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11650v1",
    "published_date": "2025-03-14 17:59:41 UTC",
    "updated_date": "2025-03-14 17:59:41 UTC"
  },
  {
    "arxiv_id": "2503.11640v1",
    "title": "Enhancing Deep Learning Based Structured Illumination Microscopy Reconstruction with Light Field Awareness",
    "authors": [
      "Long-Kun Shan",
      "Ze-Hao Wang",
      "Tong-Tian Weng",
      "Xiang-Dong Chen",
      "Fang-Wen Sun"
    ],
    "abstract": "Structured illumination microscopy (SIM) is a pivotal technique for dynamic\nsubcellular imaging in live cells. Conventional SIM reconstruction algorithms\ndepend on accurately estimating the illumination pattern and can introduce\nartefacts when this estimation is imprecise. Although recent deep\nlearning-based SIM reconstruction methods have improved speed, accuracy, and\nrobustness, they often struggle with out-of-distribution data. To address this\nlimitation, we propose an Awareness-of-Light-field SIM (AL-SIM) reconstruction\napproach that directly estimates the actual light field to correct for errors\narising from data distribution shifts. Through comprehensive experiments on\nboth simulated filament structures and live BSC1 cells, our method demonstrates\na 7% reduction in the normalized root mean square error (NRMSE) and\nsubstantially lowers reconstruction artefacts. By minimizing these artefacts\nand improving overall accuracy, AL-SIM broadens the applicability of SIM for\ncomplex biological systems.",
    "categories": [
      "physics.optics",
      "cs.AI"
    ],
    "primary_category": "physics.optics",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11640v1",
    "published_date": "2025-03-14 17:56:49 UTC",
    "updated_date": "2025-03-14 17:56:49 UTC"
  },
  {
    "arxiv_id": "2503.13517v2",
    "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning",
    "authors": [
      "Hao Cui",
      "Zahra Shamsi",
      "Gowoon Cheon",
      "Xuejian Ma",
      "Shutong Li",
      "Maria Tikhanovskaya",
      "Peter Norgaard",
      "Nayantara Mudur",
      "Martyna Plomecka",
      "Paul Raccuglia",
      "Yasaman Bahri",
      "Victor V. Albert",
      "Pranesh Srinivasan",
      "Haining Pan",
      "Philippe Faist",
      "Brian Rohr",
      "Ekin Dogus Cubuk",
      "Muratahan Aykol",
      "Amil Merchant",
      "Michael J. Statt",
      "Dan Morris",
      "Drew Purves",
      "Elise Kleeman",
      "Ruth Alcantara",
      "Matthew Abraham",
      "Muqthar Mohammad",
      "Ean Phing VanLee",
      "Chenfei Jiang",
      "Elizabeth Dorfman",
      "Eun-Ah Kim",
      "Michael P Brenner",
      "Viren Jain",
      "Sameera Ponda",
      "Subhashini Venugopalan"
    ],
    "abstract": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2503.13517v2",
    "published_date": "2025-03-14 17:53:03 UTC",
    "updated_date": "2025-05-13 06:16:23 UTC"
  },
  {
    "arxiv_id": "2503.11617v2",
    "title": "ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning",
    "authors": [
      "Xinyi Wang",
      "Jiashui Wang",
      "Jinbo Su",
      "Ke Wang",
      "Peng Chen",
      "Yanming Liu",
      "Long Liu",
      "Xiang Li",
      "Yangdong Wang",
      "Qiyuan Chen",
      "Rongze Chen",
      "Chunfu Jia"
    ],
    "abstract": "Assembly code analysis and comprehension play critical roles in applications\nlike reverse engineering, yet they face substantial challenges due to low\ninformation density and a lack of explicit syntactic structures. While\ntraditional masked language modeling (MLM) approaches do not explicitly focus\non natural language interaction, emerging decoder-focused large language models\n(LLMs) demonstrate partial success in binary analysis yet remain underexplored\nfor holistic comprehension. We present Assembly Augmented Tuning, an end-to-end\nstructural-semantic instruction tuning framework that synergizes encoder\narchitecture with decoder-based LLMs through a projector module, where the\nassembly encoder extracts hardware-level structural features, the projector\nbridges representations with the semantic space, and the instruction-tuned LLM\npreserves natural language capabilities. Experimental results demonstrate three\nkey advantages: (1) State-of-the-art performance in assembly comprehension with\n+39.7% Recall@1 and +17.8% MRR improvements over GPT-4-Turbo, (2) Consistent\nenhancements across base models (24.6-107.4% Recall@1 and 15.2-106.3% MRR on\nQwen2.5-Coder, Deepseek-Coder and CodeLlama variants), and (3) Superior\ninstruction-following capabilities (41.5%-118% improvements) with controlled\ncode generation degradation (-8.9% to -35% across architectures).",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages, multiple figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11617v2",
    "published_date": "2025-03-14 17:36:08 UTC",
    "updated_date": "2025-05-22 09:43:27 UTC"
  },
  {
    "arxiv_id": "2503.11743v1",
    "title": "PUBLICSPEAK: Hearing the Public with a Probabilistic Framework in Local Government",
    "authors": [
      "Tianliang Xu",
      "Eva Maxfield Brown",
      "Dustin Dwyer",
      "Sabina Tomkins"
    ],
    "abstract": "Local governments around the world are making consequential decisions on\nbehalf of their constituents, and these constituents are responding with\nrequests, advice, and assessments of their officials at public meetings. So\nmany small meetings cannot be covered by traditional newsrooms at scale. We\npropose PUBLICSPEAK, a probabilistic framework which can utilize meeting\nstructure, domain knowledge, and linguistic information to discover public\nremarks in local government meetings. We then use our approach to inspect the\nissues raised by constituents in 7 cities across the United States. We evaluate\nour approach on a novel dataset of local government meetings and find that\nPUBLICSPEAK improves over state-of-the-art by 10% on average, and by up to 40%.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 3 figures, in the 39th Annual AAAI Conference on Artificial\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2503.11743v1",
    "published_date": "2025-03-14 17:04:36 UTC",
    "updated_date": "2025-03-14 17:04:36 UTC"
  },
  {
    "arxiv_id": "2503.11742v1",
    "title": "Safe Vision-Language Models via Unsafe Weights Manipulation",
    "authors": [
      "Moreno D'Incà",
      "Elia Peruzzo",
      "Xingqian Xu",
      "Humphrey Shi",
      "Nicu Sebe",
      "Massimiliano Mancini"
    ],
    "abstract": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.11742v1",
    "published_date": "2025-03-14 17:00:22 UTC",
    "updated_date": "2025-03-14 17:00:22 UTC"
  },
  {
    "arxiv_id": "2503.11586v1",
    "title": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space",
    "authors": [
      "Zhiliang Chen",
      "Xinyuan Niu",
      "Chuan-Sheng Foo",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2503.11586v1",
    "published_date": "2025-03-14 16:55:46 UTC",
    "updated_date": "2025-03-14 16:55:46 UTC"
  },
  {
    "arxiv_id": "2503.11741v3",
    "title": "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba for Enhanced Biosignal Classification",
    "authors": [
      "Jian Qian",
      "Teck Lun Goh",
      "Bingyu Xie",
      "Chengyao Zhu",
      "Biao Wan",
      "Yawen Guan",
      "Rachel Ding Chen",
      "Patrick Yin Chiang"
    ],
    "abstract": "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Biological signals",
    "pdf_url": "http://arxiv.org/pdf/2503.11741v3",
    "published_date": "2025-03-14 16:42:58 UTC",
    "updated_date": "2025-03-25 06:23:36 UTC"
  },
  {
    "arxiv_id": "2503.11573v1",
    "title": "Synthesizing Access Control Policies using Large Language Models",
    "authors": [
      "Adarsh Vatsa",
      "Pratyush Patel",
      "William Eiers"
    ],
    "abstract": "Cloud compute systems allow administrators to write access control policies\nthat govern access to private data. While policies are written in convenient\nlanguages, such as AWS Identity and Access Management Policy Language, manually\nwritten policies often become complex and error prone. In this paper, we\ninvestigate whether and how well Large Language Models (LLMs) can be used to\nsynthesize access control policies. Our investigation focuses on the task of\ntaking an access control request specification and zero-shot prompting LLMs to\nsynthesize a well-formed access control policy which correctly adheres to the\nrequest specification. We consider two scenarios, one which the request\nspecification is given as a concrete list of requests to be allowed or denied,\nand another in which a natural language description is used to specify sets of\nrequests to be allowed or denied. We then argue that for zero-shot prompting,\nmore precise and structured prompts using a syntax based approach are necessary\nand experimentally show preliminary results validating our approach.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "68P25"
    ],
    "primary_category": "cs.SE",
    "comment": "to be published in the NLBSE Workshop at ICSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11573v1",
    "published_date": "2025-03-14 16:40:25 UTC",
    "updated_date": "2025-03-14 16:40:25 UTC"
  },
  {
    "arxiv_id": "2503.11572v2",
    "title": "Implicit Bias-Like Patterns in Reasoning Models",
    "authors": [
      "Messi H. J. Lee",
      "Calvin K. Lai"
    ],
    "abstract": "Implicit bias refers to automatic mental processes that shape perceptions,\njudgments, and behaviors. Previous research on \"implicit bias\" in LLMs focused\nprimarily on outputs rather than the processes underlying the outputs. We\npresent the Reasoning Model Implicit Association Test (RM-IAT) to study\nimplicit bias-like processing in reasoning models, which are LLMs using\nstep-by-step reasoning for complex tasks. Using RM-IAT, we find o3-mini and\nDeepSeek R1 require more tokens when processing association-incompatible\ninformation, mirroring human implicit bias patterns. Conversely, Claude 3.7\nSonnet displays reversed patterns for race and gender tests, requiring more\ntokens for association-compatible information. This reversal appears linked to\ndifferences in safety mechanism activation, increasing deliberation in\nsensitive contexts. These findings suggest AI systems can exhibit processing\npatterns analogous to both human implicit bias and bias correction mechanisms.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11572v2",
    "published_date": "2025-03-14 16:40:02 UTC",
    "updated_date": "2025-05-14 18:40:23 UTC"
  },
  {
    "arxiv_id": "2503.11571v1",
    "title": "RASA: Replace Anyone, Say Anything -- A Training-Free Framework for Audio-Driven and Universal Portrait Video Editing",
    "authors": [
      "Tianrui Pan",
      "Lin Liu",
      "Jie Liu",
      "Xiaopeng Zhang",
      "Jie Tang",
      "Gangshan Wu",
      "Qi Tian"
    ],
    "abstract": "Portrait video editing focuses on modifying specific attributes of portrait\nvideos, guided by audio or video streams. Previous methods typically either\nconcentrate on lip-region reenactment or require training specialized models to\nextract keypoints for motion transfer to a new identity. In this paper, we\nintroduce a training-free universal portrait video editing framework that\nprovides a versatile and adaptable editing strategy. This framework supports\nportrait appearance editing conditioned on the changed first reference frame,\nas well as lip editing conditioned on varied speech, or a combination of both.\nIt is based on a Unified Animation Control (UAC) mechanism with source\ninversion latents to edit the entire portrait, including visual-driven shape\ncontrol, audio-driven speaking control, and inter-frame temporal control.\nFurthermore, our method can be adapted to different scenarios by adjusting the\ninitial reference frame, enabling detailed editing of portrait videos with\nspecific head rotations and facial expressions. This comprehensive approach\nensures a holistic and flexible solution for portrait video editing. The\nexperimental results show that our model can achieve more accurate and\nsynchronized lip movements for the lip editing task, as well as more flexible\nmotion transfer for the appearance editing task. Demo is available at\nhttps://alice01010101.github.io/RASA/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Demo is available at https://alice01010101.github.io/RASA/",
    "pdf_url": "http://arxiv.org/pdf/2503.11571v1",
    "published_date": "2025-03-14 16:39:15 UTC",
    "updated_date": "2025-03-14 16:39:15 UTC"
  },
  {
    "arxiv_id": "2503.11562v2",
    "title": "Designing Neural Synthesizers for Low-Latency Interaction",
    "authors": [
      "Franco Caspe",
      "Jordie Shier",
      "Mark Sandler",
      "Charalampos Saitis",
      "Andrew McPherson"
    ],
    "abstract": "Neural Audio Synthesis (NAS) models offer interactive musical control over\nhigh-quality, expressive audio generators. While these models can operate in\nreal-time, they often suffer from high latency, making them unsuitable for\nintimate musical interaction. The impact of architectural choices in deep\nlearning models on audio latency remains largely unexplored in the NAS\nliterature. In this work, we investigate the sources of latency and jitter\ntypically found in interactive NAS models. We then apply this analysis to the\ntask of timbre transfer using RAVE, a convolutional variational autoencoder for\naudio waveforms introduced by Caillon et al. in 2021. Finally, we present an\niterative design approach for optimizing latency. This culminates with a model\nwe call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is\nlow-latency and exhibits better pitch and loudness replication while showing\ntimbre modification capabilities similar to RAVE. We implement it in a\nspecialized inference framework for low-latency, real-time inference and\npresent a proof-of-concept audio plugin compatible with audio signals from\nmusical instruments. We expect the challenges and guidelines described in this\ndocument to support NAS researchers in designing models for low-latency\ninference from the ground up, enriching the landscape of possibilities for\nmusicians.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "See website at fcaspe.github.io/brave - 13 pages, 5 figures, accepted\n  to the Journal of the Audio Engineering Society, LaTeX; Corrected typos,\n  added hyphen to title to reflect JAES version",
    "pdf_url": "http://arxiv.org/pdf/2503.11562v2",
    "published_date": "2025-03-14 16:30:31 UTC",
    "updated_date": "2025-04-11 18:00:53 UTC"
  },
  {
    "arxiv_id": "2503.11538v1",
    "title": "FLASHμ: Fast Localizing And Sizing of Holographic Microparticles",
    "authors": [
      "Ayush Paliwal",
      "Oliver Schlenczek",
      "Birte Thiede",
      "Manuel Santos Pereira",
      "Katja Stieger",
      "Eberhard Bodenschatz",
      "Gholamhossein Bagheri",
      "Alexander Ecker"
    ],
    "abstract": "Reconstructing the 3D location and size of microparticles from diffraction\nimages - holograms - is a computationally expensive inverse problem that has\ntraditionally been solved using physics-based reconstruction methods. More\nrecently, researchers have used machine learning methods to speed up the\nprocess. However, for small particles in large sample volumes the performance\nof these methods falls short of standard physics-based reconstruction methods.\nHere we designed a two-stage neural network architecture, FLASH$\\mu$, to detect\nsmall particles (6-100$\\mu$m) from holograms with large sample depths up to\n20cm. Trained only on synthetic data with added physical noise, our method\nreliably detects particles of at least 9$\\mu$m diameter in real holograms,\ncomparable to the standard reconstruction-based approaches while operating on\nsmaller crops, at quarter of the original resolution and providing roughly a\n600-fold speedup. In addition to introducing a novel approach to a non-local\nobject detection or signal demixing problem, our work could enable low-cost,\nreal-time holographic imaging setups.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph",
      "physics.optics"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11538v1",
    "published_date": "2025-03-14 16:04:10 UTC",
    "updated_date": "2025-03-14 16:04:10 UTC"
  },
  {
    "arxiv_id": "2503.11531v1",
    "title": "Potential of large language model-powered nudges for promoting daily water and energy conservation",
    "authors": [
      "Zonghan Li",
      "Song Tong",
      "Yi Liu",
      "Kaiping Peng",
      "Chunyan Wang"
    ],
    "abstract": "The increasing amount of pressure related to water and energy shortages has\nincreased the urgency of cultivating individual conservation behaviors. While\nthe concept of nudging, i.e., providing usage-based feedback, has shown promise\nin encouraging conservation behaviors, its efficacy is often constrained by the\nlack of targeted and actionable content. This study investigates the impact of\nthe use of large language models (LLMs) to provide tailored conservation\nsuggestions for conservation intentions and their rationale. Through a survey\nexperiment with 1,515 university participants, we compare three virtual nudging\nscenarios: no nudging, traditional nudging with usage statistics, and\nLLM-powered nudging with usage statistics and personalized conservation\nsuggestions. The results of statistical analyses and causal forest modeling\nreveal that nudging led to an increase in conservation intentions among\n86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum\nincrease of 18.0% in conservation intentions, surpassing traditional nudging by\n88.6%. Furthermore, structural equation modeling results reveal that exposure\nto LLM-powered nudges enhances self-efficacy and outcome expectations while\ndiminishing dependence on social norms, thereby increasing intrinsic motivation\nto conserve. These findings highlight the transformative potential of LLMs in\npromoting individual water and energy conservation, representing a new frontier\nin the design of sustainable behavioral interventions and resource management.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11531v1",
    "published_date": "2025-03-14 15:58:11 UTC",
    "updated_date": "2025-03-14 15:58:11 UTC"
  },
  {
    "arxiv_id": "2503.11517v1",
    "title": "Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks",
    "authors": [
      "Diego Gosmar",
      "Deborah A. Dahl",
      "Dario Gosmar"
    ],
    "abstract": "Prompt injection constitutes a significant challenge for generative AI\nsystems by inducing unintended outputs. We introduce a multi-agent NLP\nframework specifically designed to address prompt injection vulnerabilities\nthrough layered detection and enforcement mechanisms. The framework\norchestrates specialized agents for generating responses, sanitizing outputs,\nand enforcing policy compliance. Evaluation on 500 engineered injection prompts\ndemonstrates a marked reduction in injection success and policy breaches. Novel\nmetrics, including Injection Success Rate (ISR), Policy Override Frequency\n(POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS),\nare proposed to derive a composite Total Injection Vulnerability Score (TIVS).\nThe system utilizes the OVON (Open Voice Network) framework for inter-agent\ncommunication via structured JSON messages, extending a previously established\nmulti-agent architecture from hallucination mitigation to address the unique\nchallenges of prompt injection.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11517v1",
    "published_date": "2025-03-14 15:41:45 UTC",
    "updated_date": "2025-03-14 15:41:45 UTC"
  },
  {
    "arxiv_id": "2503.11739v1",
    "title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control",
    "authors": [
      "Zirui Yuan",
      "Siqi Lai",
      "Hao Liu"
    ],
    "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic\nmanagement by optimizing traffic flow and mitigating congestion. While Large\nLanguage Models (LLMs) have recently emerged as promising tools for TSC due to\ntheir exceptional problem-solving and generalization capabilities, existing\napproaches fail to address the essential need for inter-agent coordination,\nlimiting their effectiveness in achieving network-wide optimization. To bridge\nthis gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC.\nSpecifically, we first construct a structured spatiotemporal graph to capture\nreal-time traffic dynamics and spatial relationships among neighboring\nintersections, enabling the LLM to reason about complex traffic interactions.\nMoreover, we introduce a complexity-aware reasoning mechanism that dynamically\nadapts reasoning depth based on real-time traffic conditions, ensuring optimal\ncomputational efficiency without sacrificing decision quality. Besides, we\npropose a fine-tuning strategy that leverages iterative simulation-driven data\ncollection and environmental feedback to build a lightweight LLM tailored for\ncooperative TSC. Extensive experiments on both synthetic and real-world\ndatasets demonstrate that CoLLMLight outperforms state-of-the-art methods in\ndiverse traffic scenarios, showcasing its effectiveness, scalability, and\nrobustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review, 14 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.11739v1",
    "published_date": "2025-03-14 15:40:39 UTC",
    "updated_date": "2025-03-14 15:40:39 UTC"
  },
  {
    "arxiv_id": "2503.11513v1",
    "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
    "authors": [
      "Ziqin Zhou",
      "Yifan Yang",
      "Yuqing Yang",
      "Tianyu He",
      "Houwen Peng",
      "Kai Qiu",
      "Qi Dai",
      "Lili Qiu",
      "Chong Luo",
      "Lingqiao Liu"
    ],
    "abstract": "Text-to-video generation poses significant challenges due to the inherent\ncomplexity of video data, which spans both temporal and spatial dimensions. It\nintroduces additional redundancy, abrupt variations, and a domain gap between\nlanguage and vision tokens while generation. Addressing these challenges\nrequires an effective video tokenizer that can efficiently encode video data\nwhile preserving essential semantic and spatiotemporal information, serving as\na critical bridge between text and vision. Inspired by the observation in\nVQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for\ntext-to-video generation with hierarchical tokenizers. It utilizes a 3D causal\nVAE with a multi-layer discrete token framework, encoding video content into\nhierarchically structured codebooks. Higher layers capture semantic information\nwith higher compression, while lower layers focus on fine-grained\nspatiotemporal details, striking a balance between compression efficiency and\nreconstruction quality. Our approach efficiently encodes longer video sequences\n(e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately\n70\\% compared to baseline tokenizers, while maintaining competitive\nreconstruction quality. We explore the trade-offs between compression and\nreconstruction, while emphasizing the advantages of high-compressed semantic\ntokens in text-to-video tasks. HiTVideo aims to address the potential\nlimitations of existing video tokenizers in text-to-video generation tasks,\nstriving for higher compression ratios and simplify LLMs modeling under\nlanguage guidance, offering a scalable and promising framework for advancing\ntext to video generation. Demo page:\nhttps://ziqinzhou66.github.io/project/HiTVideo.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11513v1",
    "published_date": "2025-03-14 15:36:39 UTC",
    "updated_date": "2025-03-14 15:36:39 UTC"
  },
  {
    "arxiv_id": "2503.11511v1",
    "title": "Alzheimer's Disease Classification Using Retinal OCT: TransnetOCT and Swin Transformer Models",
    "authors": [
      "Siva Manohar Reddy Kesu",
      "Neelam Sinha",
      "Hariharan Ramasangu",
      "Thomas Gregor Issac"
    ],
    "abstract": "Retinal optical coherence tomography (OCT) images are the biomarkers for\nneurodegenerative diseases, which are rising in prevalence. Early detection of\nAlzheimer's disease using retinal OCT is a primary challenging task. This work\nutilizes advanced deep learning techniques to classify retinal OCT images of\nsubjects with Alzheimer's disease (AD) and healthy controls (CO). The goal is\nto enhance diagnostic capabilities through efficient image analysis. In the\nproposed model, Raw OCT images have been preprocessed with ImageJ and given to\nvarious deep-learning models to evaluate the accuracy. The best classification\narchitecture is TransNetOCT, which has an average accuracy of 98.18% for input\nOCT images and 98.91% for segmented OCT images for five-fold cross-validation\ncompared to other models, and the Swin Transformer model has achieved an\naccuracy of 93.54%. The evaluation accuracy metric demonstrated TransNetOCT and\nSwin transformer models capability to classify AD and CO subjects reliably,\ncontributing to the potential for improved diagnostic processes in clinical\nsettings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "18 pages, 25 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11511v1",
    "published_date": "2025-03-14 15:34:37 UTC",
    "updated_date": "2025-03-14 15:34:37 UTC"
  },
  {
    "arxiv_id": "2503.11488v1",
    "title": "Unicorn: A Universal and Collaborative Reinforcement Learning Approach Towards Generalizable Network-Wide Traffic Signal Control",
    "authors": [
      "Yifeng Zhang",
      "Yilin Liu",
      "Ping Gong",
      "Peizhuo Li",
      "Mingfeng Fan",
      "Guillaume Sartoretti"
    ],
    "abstract": "Adaptive traffic signal control (ATSC) is crucial in reducing congestion,\nmaximizing throughput, and improving mobility in rapidly growing urban areas.\nRecent advancements in parameter-sharing multi-agent reinforcement learning\n(MARL) have greatly enhanced the scalable and adaptive optimization of complex,\ndynamic flows in large-scale homogeneous networks. However, the inherent\nheterogeneity of real-world traffic networks, with their varied intersection\ntopologies and interaction dynamics, poses substantial challenges to achieving\nscalable and effective ATSC across different traffic scenarios. To address\nthese challenges, we present Unicorn, a universal and collaborative MARL\nframework designed for efficient and adaptable network-wide ATSC. Specifically,\nwe first propose a unified approach to map the states and actions of\nintersections with varying topologies into a common structure based on traffic\nmovements. Next, we design a Universal Traffic Representation (UTR) module with\na decoder-only network for general feature extraction, enhancing the model's\nadaptability to diverse traffic scenarios. Additionally, we incorporate an\nIntersection Specifics Representation (ISR) module, designed to identify key\nlatent vectors that represent the unique intersection's topology and traffic\ndynamics through variational inference techniques. To further refine these\nlatent representations, we employ a contrastive learning approach in a\nself-supervised manner, which enables better differentiation of\nintersection-specific features. Moreover, we integrate the state-action\ndependencies of neighboring agents into policy optimization, which effectively\ncaptures dynamic agent interactions and facilitates efficient regional\ncollaboration. Our results show that Unicorn outperforms other methods across\nvarious evaluation metrics, highlighting its potential in complex, dynamic\ntraffic networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11488v1",
    "published_date": "2025-03-14 15:13:42 UTC",
    "updated_date": "2025-03-14 15:13:42 UTC"
  },
  {
    "arxiv_id": "2503.16508v1",
    "title": "Conversational AI as a Coding Assistant: Understanding Programmers' Interactions with and Expectations from Large Language Models for Coding",
    "authors": [
      "Mehmet Akhoroz",
      "Caglar Yildirim"
    ],
    "abstract": "Conversational AI interfaces powered by large language models (LLMs) are\nincreasingly used as coding assistants. However, questions remain about how\nprogrammers interact with LLM-based conversational agents, the challenges they\nencounter, and the factors influencing adoption. This study investigates\nprogrammers' usage patterns, perceptions, and interaction strategies when\nengaging with LLM-driven coding assistants. Through a survey, participants\nreported both the benefits, such as efficiency and clarity of explanations, and\nthe limitations, including inaccuracies, lack of contextual awareness, and\nconcerns about over-reliance. Notably, some programmers actively avoid LLMs due\nto a preference for independent learning, distrust in AI-generated code, and\nethical considerations. Based on our findings, we propose design guidelines for\nimproving conversational coding assistants, emphasizing context retention,\ntransparency, multimodal support, and adaptability to user preferences. These\ninsights contribute to the broader understanding of how LLM-based\nconversational agents can be effectively integrated into software development\nworkflows while addressing adoption barriers and enhancing usability.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.16508v1",
    "published_date": "2025-03-14 15:06:07 UTC",
    "updated_date": "2025-03-14 15:06:07 UTC"
  },
  {
    "arxiv_id": "2503.11477v1",
    "title": "Heterogeneous Causal Discovery of Repeated Undesirable Health Outcomes",
    "authors": [
      "Shishir Adhikari",
      "Guido Muscioni",
      "Mark Shapiro",
      "Plamen Petrov",
      "Elena Zheleva"
    ],
    "abstract": "Understanding factors triggering or preventing undesirable health outcomes\nacross patient subpopulations is essential for designing targeted\ninterventions. While randomized controlled trials and expert-led patient\ninterviews are standard methods for identifying these factors, they can be\ntime-consuming and infeasible. Causal discovery offers an alternative to\nconventional approaches by generating cause-and-effect hypotheses from\nobservational data. However, it often relies on strong or untestable\nassumptions, which can limit its practical application. This work aims to make\ncausal discovery more practical by considering multiple assumptions and\nidentifying heterogeneous effects. We formulate the problem of discovering\ncauses and effect modifiers of an outcome, where effect modifiers are contexts\n(e.g., age groups) with heterogeneous causal effects. Then, we present a novel,\nend-to-end framework that incorporates an ensemble of causal discovery\nalgorithms and estimation of heterogeneous effects to discover causes and\neffect modifiers that trigger or inhibit the outcome. We demonstrate that the\nensemble approach improves robustness by enhancing recall of causal factors\nwhile maintaining precision. Our study examines the causes of repeat emergency\nroom visits for diabetic patients and hospital readmissions for ICU patients.\nOur framework generates causal hypotheses consistent with existing literature\nand can help practitioners identify potential interventions and patient\nsubpopulations to focus on.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11477v1",
    "published_date": "2025-03-14 15:05:17 UTC",
    "updated_date": "2025-03-14 15:05:17 UTC"
  },
  {
    "arxiv_id": "2503.11475v1",
    "title": "Research Vision: Multi-Agent Path Planning for Cops And Robbers Via Reactive Synthesis",
    "authors": [
      "William Fishell",
      "Andoni Rodriguez",
      "Mark Santolucito"
    ],
    "abstract": "We propose the problem of multi-agent path planning for a generalization of\nthe classic Cops and Robbers game via reactive synthesis. Specifically, through\nthe application of LTLt and Coordination Synthesis, we aim to check whether\nvarious Cops and Robbers games are realizable (a strategy exists for the cops\nwhich guarantees they catch the robbers). Additionally, we construct this\nstrategy as an executable program for the multiple system players in our games.\nIn this paper we formalize the problem space, and propose potential directions\nfor solutions. We also show how our formalization of this generalized cops and\nrobbers game can be mapped to a broad range of other problems in the reactive\nprogram synthesis space.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11475v1",
    "published_date": "2025-03-14 15:03:32 UTC",
    "updated_date": "2025-03-14 15:03:32 UTC"
  },
  {
    "arxiv_id": "2503.11458v1",
    "title": "Integrating LLMs in Gamified Systems",
    "authors": [
      "Carlos J. Costa"
    ],
    "abstract": "In this work, a thorough mathematical framework for incorporating Large\nLanguage Models (LLMs) into gamified systems is presented with an emphasis on\nimproving task dynamics, user engagement, and reward systems. Personalized\nfeedback, adaptive learning, and dynamic content creation are all made possible\nby integrating LLMs and are crucial for improving user engagement and system\nperformance. A simulated environment tests the framework's adaptability and\ndemonstrates its potential for real-world applications in various industries,\nincluding business, healthcare, and education. The findings demonstrate how\nLLMs can offer customized experiences that raise system effectiveness and user\nretention. This study also examines the difficulties this framework aims to\nsolve, highlighting its importance in maximizing involvement and encouraging\nsustained behavioral change in a range of sectors.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.11458v1",
    "published_date": "2025-03-14 14:47:04 UTC",
    "updated_date": "2025-03-14 14:47:04 UTC"
  },
  {
    "arxiv_id": "2503.11737v2",
    "title": "Multi-View Node Pruning for Accurate Graph Representation",
    "authors": [
      "Jiseong Park",
      "Hanjin Kim",
      "Seojin Kim",
      "Jueun Choi"
    ],
    "abstract": "Graph pooling, which compresses a whole graph into a smaller coarsened graph,\nis an essential component of graph representation learning. To efficiently\ncompress a given graph, graph pooling methods often drop their nodes with\nattention-based scoring with the task loss. However, this often results in\nsimply removing nodes with lower degrees without consideration of their\nfeature-level relevance to the given task. To fix this problem, we propose a\nMulti-View Pruning(MVP), a graph pruning method based on a multi-view framework\nand reconstruction loss. Given a graph, MVP first constructs multiple graphs\nfor different views either by utilizing the predefined modalities or by\nrandomly partitioning the input features, to consider the importance of each\nnode in diverse perspectives. Then, it learns the score for each node by\nconsidering both the reconstruction and the task loss. MVP can be incorporated\nwith any hierarchical pooling framework to score the nodes. We validate MVP on\nmultiple benchmark datasets by coupling it with two graph pooling methods, and\nshow that it significantly improves the performance of the base graph pooling\nmethod, outperforming all baselines. Further analysis shows that both the\nencoding of multiple views and the consideration of reconstruction loss are the\nkey to the success of MVP, and that it indeed identifies nodes that are less\nimportant according to domain knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Jiseong Park and Hanjin Kim are co-first author for this work",
    "pdf_url": "http://arxiv.org/pdf/2503.11737v2",
    "published_date": "2025-03-14 14:44:54 UTC",
    "updated_date": "2025-03-18 14:34:49 UTC"
  },
  {
    "arxiv_id": "2503.17378v2",
    "title": "Large language model-powered AI systems achieve self-replication with no human intervention",
    "authors": [
      "Xudong Pan",
      "Jiarun Dai",
      "Yihe Fan",
      "Minyuan Luo",
      "Changyi Li",
      "Min Yang"
    ],
    "abstract": "Self-replication with no human intervention is broadly recognized as one of\nthe principal red lines associated with frontier AI systems. While leading\ncorporations such as OpenAI and Google DeepMind have assessed GPT-o3-mini and\nGemini on replication-related tasks and concluded that these systems pose a\nminimal risk regarding self-replication, our research presents novel findings.\nFollowing the same evaluation protocol, we demonstrate that 11 out of 32\nexisting AI systems under evaluation already possess the capability of\nself-replication. In hundreds of experimental trials, we observe a non-trivial\nnumber of successful self-replication trials across mainstream model families\nworldwide, even including those with as small as 14 billion parameters which\ncan run on personal computers. Furthermore, we note the increase in\nself-replication capability when the model becomes more intelligent in general.\nAlso, by analyzing the behavioral traces of diverse AI systems, we observe that\nexisting AI systems already exhibit sufficient planning, problem-solving, and\ncreative capabilities to accomplish complex agentic tasks including\nself-replication. More alarmingly, we observe successful cases where an AI\nsystem do self-exfiltration without explicit instructions, adapt to harsher\ncomputational environments without sufficient software or hardware supports,\nand plot effective strategies to survive against the shutdown command from the\nhuman beings. These novel findings offer a crucial time buffer for the\ninternational community to collaborate on establishing effective governance\nover the self-replication capabilities and behaviors of frontier AI systems,\nwhich could otherwise pose existential risks to the human society if not\nwell-controlled.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.ET",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.17378v2",
    "published_date": "2025-03-14 14:44:27 UTC",
    "updated_date": "2025-03-25 13:38:18 UTC"
  },
  {
    "arxiv_id": "2503.11444v1",
    "title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment, Distribution, and Discovery",
    "authors": [
      "Balaji Rama",
      "Kai Mei",
      "Yongfeng Zhang"
    ],
    "abstract": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.OS"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted to the 2025 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL) - System\n  Demonstration Track",
    "pdf_url": "http://arxiv.org/pdf/2503.11444v1",
    "published_date": "2025-03-14 14:29:17 UTC",
    "updated_date": "2025-03-14 14:29:17 UTC"
  },
  {
    "arxiv_id": "2503.11435v1",
    "title": "Preference Elicitation for Multi-objective Combinatorial Optimization with Active Learning and Maximum Likelihood Estimation",
    "authors": [
      "Marianne Defresne",
      "Jayanta Mandi",
      "Tias Guns"
    ],
    "abstract": "Real-life combinatorial optimization problems often involve several\nconflicting objectives, such as price, product quality and sustainability. A\ncomputationally-efficient way to tackle multiple objectives is to aggregate\nthem into a single-objective function, such as a linear combination. However,\ndefining the weights of the linear combination upfront is hard; alternatively,\nthe use of interactive learning methods that ask users to compare candidate\nsolutions is highly promising. The key challenges are to generate candidates\nquickly, to learn an objective function that leads to high-quality solutions\nand to do so with few user interactions. We build upon the Constructive\nPreference Elicitation framework and show how each of the three properties can\nbe improved: to increase the interaction speed we investigate using pools of\n(relaxed) solutions, to improve the learning we adopt Maximum Likelihood\nEstimation of a Bradley-Terry preference model; and to reduce the number of\nuser interactions, we select the pair of candidates to compare with an\nensemble-based acquisition function inspired from Active Learning. Our careful\nexperimentation demonstrates each of these improvements: on a PC configuration\ntask and a realistic multi-instance routing problem, our method selects queries\nfaster, needs fewer queries and synthesizes higher-quality combinatorial\nsolutions than previous CPE methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11435v1",
    "published_date": "2025-03-14 14:24:27 UTC",
    "updated_date": "2025-03-14 14:24:27 UTC"
  },
  {
    "arxiv_id": "2503.11433v1",
    "title": "Adaptive Torque Control of Exoskeletons under Spasticity Conditions via Reinforcement Learning",
    "authors": [
      "Andrés Chavarrías",
      "David Rodriguez-Cianca",
      "Pablo Lanillos"
    ],
    "abstract": "Spasticity is a common movement disorder symptom in individuals with cerebral\npalsy, hereditary spastic paraplegia, spinal cord injury and stroke, being one\nof the most disabling features in the progression of these diseases. Despite\nthe potential benefit of using wearable robots to treat spasticity, their use\nis not currently recommended to subjects with a level of spasticity above\n${1^+}$ on the Modified Ashworth Scale. The varying dynamics of this\nvelocity-dependent tonic stretch reflex make it difficult to deploy safe\npersonalized controllers. Here, we describe a novel adaptive torque controller\nvia deep reinforcement learning (RL) for a knee exoskeleton under joint\nspasticity conditions, which accounts for task performance and interaction\nforces reduction. To train the RL agent, we developed a digital twin, including\na musculoskeletal-exoskeleton system with joint misalignment and a\ndifferentiable spastic reflexes model for the muscles activation. Results for a\nsimulated knee extension movement showed that the agent learns to control the\nexoskeleton for individuals with different levels of spasticity. The proposed\ncontroller was able to reduce maximum torques applied to the human joint under\nspastic conditions by an average of 10.6\\% and decreases the root mean square\nuntil the settling time by 8.9\\% compared to a conventional compliant\ncontroller.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication in IEEE 19th International Conference on\n  Rehabilitation Robotics (ICORR2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.11433v1",
    "published_date": "2025-03-14 14:22:09 UTC",
    "updated_date": "2025-03-14 14:22:09 UTC"
  },
  {
    "arxiv_id": "2503.11429v1",
    "title": "Combining Causal Models for More Accurate Abstractions of Neural Networks",
    "authors": [
      "Theodora-Mara Pîslar",
      "Sara Magliacane",
      "Atticus Geiger"
    ],
    "abstract": "Mechanistic interpretability aims to reverse engineer neural networks by\nuncovering which high-level algorithms they implement. Causal abstraction\nprovides a precise notion of when a network implements an algorithm, i.e., a\ncausal model of the network contains low-level features that realize the\nhigh-level variables in a causal model of the algorithm. A typical problem in\npractical settings is that the algorithm is not an entirely faithful\nabstraction of the network, meaning it only partially captures the true\nreasoning process of a model. We propose a solution where we combine different\nsimple high-level models to produce a more faithful representation of the\nnetwork. Through learning this combination, we can model neural networks as\nbeing in different computational states depending on the input provided, which\nwe show is more accurate to GPT 2-small fine-tuned on two toy tasks. We observe\na trade-off between the strength of an interpretability hypothesis, which we\ndefine in terms of the number of inputs explained by the high-level models, and\nits faithfulness, which we define as the interchange intervention accuracy. Our\nmethod allows us to modulate between the two, providing the most accurate\ncombination of models that describe the behavior of a neural network given a\nfaithfulness level.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11429v1",
    "published_date": "2025-03-14 14:14:43 UTC",
    "updated_date": "2025-03-14 14:14:43 UTC"
  },
  {
    "arxiv_id": "2503.11419v1",
    "title": "From Generative AI to Innovative AI: An Evolutionary Roadmap",
    "authors": [
      "Seyed Mahmoud Sajjadi Mohammadabadi"
    ],
    "abstract": "This paper explores the critical transition from Generative Artificial\nIntelligence (GenAI) to Innovative Artificial Intelligence (InAI). While recent\nadvancements in GenAI have enabled systems to produce high-quality content\nacross various domains, these models often lack the capacity for true\ninnovation. In this context, innovation is defined as the ability to generate\nnovel and useful outputs that go beyond mere replication of learned data. The\npaper examines this shift and proposes a roadmap for developing AI systems that\ncan generate content and engage in autonomous problem-solving and creative\nideation. The work provides both theoretical insights and practical strategies\nfor advancing AI to a stage where it can genuinely innovate, contributing\nmeaningfully to science, technology, and the arts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11419v1",
    "published_date": "2025-03-14 14:03:28 UTC",
    "updated_date": "2025-03-14 14:03:28 UTC"
  },
  {
    "arxiv_id": "2503.11408v1",
    "title": "A Neural Network Architecture Based on Attention Gate Mechanism for 3D Magnetotelluric Forward Modeling",
    "authors": [
      "Xin Zhong",
      "Weiwei Ling",
      "Kejia Pan",
      "Pinxia Wu",
      "Jiajing Zhang",
      "Zhiliang Zhan",
      "Wenbo Xiao"
    ],
    "abstract": "Traditional three-dimensional magnetotelluric (MT) numerical forward modeling\nmethods, such as the finite element method (FEM) and finite volume method\n(FVM), suffer from high computational costs and low efficiency due to\nlimitations in mesh refinement and computational resources. We propose a novel\nneural network architecture named MTAGU-Net, which integrates an attention\ngating mechanism for 3D MT forward modeling. Specifically, a dual-path\nattention gating module is designed based on forward response data images and\nembedded in the skip connections between the encoder and decoder. This module\nenables the fusion of critical anomaly information from shallow feature maps\nduring the decoding of deep feature maps, significantly enhancing the network's\ncapability to extract features from anomalous regions. Furthermore, we\nintroduce a synthetic model generation method utilizing 3D Gaussian random\nfield (GRF), which accurately replicates the electrical structures of\nreal-world geological scenarios with high fidelity. Numerical experiments\ndemonstrate that MTAGU-Net outperforms conventional 3D U-Net in terms of\nconvergence stability and prediction accuracy, with the structural similarity\nindex (SSIM) of the forward response data consistently exceeding 0.98.\nMoreover, the network can accurately predict forward response data on\npreviously unseen datasets models, demonstrating its strong generalization\nability and validating the feasibility and effectiveness of this method in\npractical applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11408v1",
    "published_date": "2025-03-14 13:48:25 UTC",
    "updated_date": "2025-03-14 13:48:25 UTC"
  },
  {
    "arxiv_id": "2503.11404v1",
    "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
    "authors": [
      "Jonas Thietke",
      "Andreas Müller",
      "Denis Lukovnikov",
      "Asja Fischer",
      "Erwin Quiring"
    ],
    "abstract": "Semantic watermarking methods enable the direct integration of watermarks\ninto the generation process of latent diffusion models by only modifying the\ninitial latent noise. One line of approaches building on Gaussian Shading\nrelies on cryptographic primitives to steer the sampling process of the latent\nnoise. However, we identify several issues in the usage of cryptographic\ntechniques in Gaussian Shading, particularly in its proof of lossless\nperformance and key management, causing ambiguity in follow-up works, too. In\nthis work, we therefore revisit the cryptographic primitives for semantic\nwatermarking. We introduce a novel, general proof of lossless performance based\non IND\\$-CPA security for semantic watermarks. We then discuss the\nconfiguration of the cryptographic primitives in semantic watermarks with\nrespect to security, efficiency, and generation quality.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages, 3 figures, WMark@ICLR",
    "pdf_url": "http://arxiv.org/pdf/2503.11404v1",
    "published_date": "2025-03-14 13:45:46 UTC",
    "updated_date": "2025-03-14 13:45:46 UTC"
  },
  {
    "arxiv_id": "2503.11387v1",
    "title": "Hierarchical Information-Guided Spatio-Temporal Mamba for Stock Time Series Forecasting",
    "authors": [
      "Wenbo Yan",
      "Shurui Wang",
      "Ying Tan"
    ],
    "abstract": "Mamba has demonstrated excellent performance in various time series\nforecasting tasks due to its superior selection mechanism. Nevertheless,\nconventional Mamba-based models encounter significant challenges in accurately\npredicting stock time series, as they fail to adequately capture both the\noverarching market dynamics and the intricate interdependencies among\nindividual stocks. To overcome these constraints, we introduce the Hierarchical\nInformation-Guided Spatio-Temporal Mamba (HIGSTM) framework. HIGSTM introduces\nIndex-Guided Frequency Filtering Decomposition to extract commonality and\nspecificity from time series. The model architecture features a meticulously\ndesigned hierarchical framework that systematically captures both temporal\ndynamic patterns and global static relationships within the stock market.\nFurthermore, we propose an Information-Guided Mamba that integrates macro\ninformations into the sequence selection process, thereby facilitating more\nmarket-conscious decision-making. Comprehensive experimental evaluations\nconducted on the CSI500, CSI800 and CSI1000 datasets demonstrate that HIGSTM\nachieves state-of-the-art performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11387v1",
    "published_date": "2025-03-14 13:30:38 UTC",
    "updated_date": "2025-03-14 13:30:38 UTC"
  },
  {
    "arxiv_id": "2503.11384v1",
    "title": "Optimizing Large Language Models for Detecting Symptoms of Comorbid Depression or Anxiety in Chronic Diseases: Insights from Patient Messages",
    "authors": [
      "Jiyeong Kim",
      "Stephen P. Ma",
      "Michael L. Chen",
      "Isaac R. Galatzer-Levy",
      "John Torous",
      "Peter J. van Roessel",
      "Christopher Sharp",
      "Michael A. Pfeffer",
      "Carolyn I. Rodriguez",
      "Eleni Linos",
      "Jonathan H. Chen"
    ],
    "abstract": "Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11384v1",
    "published_date": "2025-03-14 13:27:35 UTC",
    "updated_date": "2025-03-14 13:27:35 UTC"
  },
  {
    "arxiv_id": "2503.11376v1",
    "title": "Annotating Scientific Uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches",
    "authors": [
      "Panggih Kusuma Ningrum",
      "Philipp Mayr",
      "Nina Smirnova",
      "Iana Atanassova"
    ],
    "abstract": "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper Accepted for Publication in the Journal of Informetrics (2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.11376v1",
    "published_date": "2025-03-14 13:21:59 UTC",
    "updated_date": "2025-03-14 13:21:59 UTC"
  },
  {
    "arxiv_id": "2503.11360v1",
    "title": "PARIC: Probabilistic Attention Regularization for Language Guided Image Classification from Pre-trained Vison Language Models",
    "authors": [
      "Mayank Nautiyal",
      "Stela Arranz Gheorghe",
      "Kristiana Stefa",
      "Li Ju",
      "Ida-Maria Sintorn",
      "Prashant Singh"
    ],
    "abstract": "Language-guided attention frameworks have significantly enhanced both\ninterpretability and performance in image classification; however, the reliance\non deterministic embeddings from pre-trained vision-language foundation models\nto generate reference attention maps frequently overlooks the intrinsic\nmultivaluedness and ill-posed characteristics of cross-modal mappings. To\naddress these limitations, we introduce PARIC, a probabilistic framework for\nguiding visual attention via language specifications. Our approach enables\npre-trained vision-language models to generate probabilistic reference\nattention maps, which align textual and visual modalities more effectively\nwhile incorporating uncertainty estimates, as compared to their deterministic\ncounterparts. Experiments on benchmark test problems demonstrate that PARIC\nenhances prediction accuracy, mitigates bias, ensures consistent predictions,\nand improves robustness across various datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11360v1",
    "published_date": "2025-03-14 12:53:37 UTC",
    "updated_date": "2025-03-14 12:53:37 UTC"
  },
  {
    "arxiv_id": "2503.11349v1",
    "title": "An experimental approach on Few Shot Class Incremental Learning",
    "authors": [
      "Marinela Adam"
    ],
    "abstract": "Few-Shot Class-Incremental Learning (FSCIL) represents a cutting-edge\nparadigm within the broader scope of machine learning, designed to empower\nmodels with the ability to assimilate new classes of data with limited examples\nwhile safeguarding existing knowledge. The paper will present different\nsolutions which contain extensive experiments across large-scale datasets,\ndomain shifts, and network architectures to evaluate and compare the selected\nmethods. We highlight their advantages and then present an experimental\napproach with the purpose of improving the most promising one by replacing the\nvisual-language (V-L) model (CLIP) with another V-L model (CLOOB) that seem to\noutperform it on zero-shot learning tasks. The aim of this report is to present\nan experimental method for FSCIL that would improve its performance. We also\nplan to offer an overview followed by an analysis of the recent advancements in\nFSCIL domain, focusing on various strategies to mitigate catastrophic\nforgetting and improve the adaptability of models to evolving tasks and\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11349v1",
    "published_date": "2025-03-14 12:36:15 UTC",
    "updated_date": "2025-03-14 12:36:15 UTC"
  },
  {
    "arxiv_id": "2503.11346v1",
    "title": "AIstorian lets AI be a historian: A KG-powered multi-agent system for accurate biography generation",
    "authors": [
      "Fengyu Li",
      "Yilin Li",
      "Junhao Zhu",
      "Lu Chen",
      "Yanfei Zhang",
      "Jia Zhou",
      "Hui Zu",
      "Jingwen Zhao",
      "Yunjun Gao"
    ],
    "abstract": "Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11346v1",
    "published_date": "2025-03-14 12:23:45 UTC",
    "updated_date": "2025-03-14 12:23:45 UTC"
  },
  {
    "arxiv_id": "2503.11339v2",
    "title": "Contextual Similarity Distillation: Ensemble Uncertainties with a Single Model",
    "authors": [
      "Moritz A. Zanger",
      "Pascal R. Van der Vaart",
      "Wendelin Böhmer",
      "Matthijs T. J. Spaan"
    ],
    "abstract": "Uncertainty quantification is a critical aspect of reinforcement learning and\ndeep learning, with numerous applications ranging from efficient exploration\nand stable offline reinforcement learning to outlier detection in medical\ndiagnostics. The scale of modern neural networks, however, complicates the use\nof many theoretically well-motivated approaches such as full Bayesian\ninference. Approximate methods like deep ensembles can provide reliable\nuncertainty estimates but still remain computationally expensive. In this work,\nwe propose contextual similarity distillation, a novel approach that explicitly\nestimates the variance of an ensemble of deep neural networks with a single\nmodel, without ever learning or evaluating such an ensemble in the first place.\nOur method builds on the predictable learning dynamics of wide neural networks,\ngoverned by the neural tangent kernel, to derive an efficient approximation of\nthe predictive variance of an infinite ensemble. Specifically, we reinterpret\nthe computation of ensemble variance as a supervised regression problem with\nkernel similarities as regression targets. The resulting model can estimate\npredictive variance at inference time with a single forward pass, and can make\nuse of unlabeled target-domain data or data augmentations to refine its\nuncertainty estimates. We empirically validate our method across a variety of\nout-of-distribution detection benchmarks and sparse-reward reinforcement\nlearning environments. We find that our single-model method performs\ncompetitively and sometimes superior to ensemble-based baselines and serves as\na reliable signal for efficient exploration. These results, we believe,\nposition contextual similarity distillation as a principled and scalable\nalternative for uncertainty quantification in reinforcement learning and\ngeneral deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11339v2",
    "published_date": "2025-03-14 12:09:58 UTC",
    "updated_date": "2025-03-26 08:31:36 UTC"
  },
  {
    "arxiv_id": "2503.11331v1",
    "title": "Cardiomyopathy Diagnosis Model from Endomyocardial Biopsy Specimens: Appropriate Feature Space and Class Boundary in Small Sample Size Data",
    "authors": [
      "Masaya Mori",
      "Yuto Omae",
      "Yutaka Koyama",
      "Kazuyuki Hara",
      "Jun Toyotani",
      "Yasuo Okumura",
      "Hiroyuki Hao"
    ],
    "abstract": "As the number of patients with heart failure increases, machine learning (ML)\nhas garnered attention in cardiomyopathy diagnosis, driven by the shortage of\npathologists. However, endomyocardial biopsy specimens are often small sample\nsize and require techniques such as feature extraction and dimensionality\nreduction. This study aims to determine whether texture features are effective\nfor feature extraction in the pathological diagnosis of cardiomyopathy.\nFurthermore, model designs that contribute toward improving generalization\nperformance are examined by applying feature selection (FS) and dimensional\ncompression (DC) to several ML models. The obtained results were verified by\nvisualizing the inter-class distribution differences and conducting statistical\nhypothesis testing based on texture features. Additionally, they were evaluated\nusing predictive performance across different model designs with varying\ncombinations of FS and DC (applied or not) and decision boundaries. The\nobtained results confirmed that texture features may be effective for the\npathological diagnosis of cardiomyopathy. Moreover, when the ratio of features\nto the sample size is high, a multi-step process involving FS and DC improved\nthe generalization performance, with the linear kernel support vector machine\nachieving the best results. This process was demonstrated to be potentially\neffective for models with reduced complexity, regardless of whether the\ndecision boundaries were linear, curved, perpendicular, or parallel to the\naxes. These findings are expected to facilitate the development of an effective\ncardiomyopathy diagnostic model for its rapid adoption in medical practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11331v1",
    "published_date": "2025-03-14 11:59:23 UTC",
    "updated_date": "2025-03-14 11:59:23 UTC"
  },
  {
    "arxiv_id": "2503.11330v1",
    "title": "Learning to reset in target search problems",
    "authors": [
      "Gorka Muñoz-Gil",
      "Hans J. Briegel",
      "Michele Caraglio"
    ],
    "abstract": "Target search problems are central to a wide range of fields, from biological\nforaging to the optimization algorithms. Recently, the ability to reset the\nsearch has been shown to significantly improve the searcher's efficiency.\nHowever, the optimal resetting strategy depends on the specific properties of\nthe search problem and can often be challenging to determine. In this work, we\npropose a reinforcement learning (RL)-based framework to train agents capable\nof optimizing their search efficiency in environments by learning how to reset.\nFirst, we validate the approach in a well-established benchmark: the Brownian\nsearch with resetting. There, RL agents consistently recover strategies closely\nresembling the sharp resetting distribution, known to be optimal in this\nscenario. We then extend the framework by allowing agents to control not only\nwhen to reset, but also their spatial dynamics through turning actions. In this\nmore complex setting, the agents discover strategies that adapt both resetting\nand turning to the properties of the environment, outperforming the proposed\nbenchmarks. These results demonstrate how reinforcement learning can serve both\nas an optimization tool and a mechanism for uncovering new, interpretable\nstrategies in stochastic search processes with resetting.",
    "categories": [
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.LG",
      "physics.bio-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11330v1",
    "published_date": "2025-03-14 11:57:51 UTC",
    "updated_date": "2025-03-14 11:57:51 UTC"
  },
  {
    "arxiv_id": "2503.14519v2",
    "title": "Content ARCs: Decentralized Content Rights in the Age of Generative AI",
    "authors": [
      "Kar Balan",
      "Andrew Gilbert",
      "John Collomosse"
    ],
    "abstract": "The rise of Generative AI (GenAI) has sparked significant debate over\nbalancing the interests of creative rightsholders and AI developers. As GenAI\nmodels are trained on vast datasets that often include copyrighted material,\nquestions around fair compensation and proper attribution have become\nincreasingly urgent. To address these challenges, this paper proposes a\nframework called Content ARCs (Authenticity, Rights, Compensation). By\ncombining open standards for provenance and dynamic licensing with data\nattribution, and decentralized technologies, Content ARCs create a mechanism\nfor managing rights and compensating creators for using their work in AI\ntraining. We characterize several nascent works in the AI data licensing space\nwithin Content ARCs and identify where challenges remain to fully implement the\nend-to-end framework.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DL",
      "eess.IV"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14519v2",
    "published_date": "2025-03-14 11:57:08 UTC",
    "updated_date": "2025-05-06 11:48:04 UTC"
  },
  {
    "arxiv_id": "2503.11733v1",
    "title": "LLM Agents for Education: Advances and Applications",
    "authors": [
      "Zhendong Chu",
      "Shen Wang",
      "Jian Xie",
      "Tinghui Zhu",
      "Yibo Yan",
      "Jinheng Ye",
      "Aoxiao Zhong",
      "Xuming Hu",
      "Jing Liang",
      "Philip S. Yu",
      "Qingsong Wen"
    ],
    "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin automating tasks and driving innovation across diverse educational\napplications. In this survey, we provide a systematic review of\nstate-of-the-art research on LLM agents in education, categorizing them into\ntwo broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating\ncomplex pedagogical tasks to support both teachers and students; and (2)\n\\emph{Domain-Specific Educational Agents}, which are tailored for specialized\nfields such as science education, language learning, and professional\ndevelopment. We comprehensively examine the technological advancements\nunderlying these LLM agents, including key datasets, benchmarks, and\nalgorithmic frameworks that drive their effectiveness. Furthermore, we discuss\ncritical challenges such as privacy, bias and fairness concerns, hallucination\nmitigation, and integration with existing educational ecosystems. This survey\naims to provide a comprehensive technological overview of LLM agents for\neducation, fostering further research and collaboration to enhance their impact\nfor the greater good of learners and educators alike.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.11733v1",
    "published_date": "2025-03-14 11:53:44 UTC",
    "updated_date": "2025-03-14 11:53:44 UTC"
  },
  {
    "arxiv_id": "2503.13514v1",
    "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
    "authors": [
      "Hong Qing Yu",
      "Frank McQuade"
    ],
    "abstract": "This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed\nto enhance the reasoning capabilities of Large Language Models (LLMs) by\nintegrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs)\nwith an Incremental Learning (IL) approach. Despite recent advancements, LLMs\nstill face significant challenges in reasoning with structured data, handling\ndynamic knowledge evolution, and mitigating hallucinations, particularly in\nmission-critical domains. Our proposed RAG-KG-IL framework addresses these\nlimitations by employing a multi-agent architecture that enables continuous\nknowledge updates, integrates structured knowledge, and incorporates autonomous\nagents for enhanced explainability and reasoning. The framework utilizes RAG to\nensure the generated responses are grounded in verifiable information, while\nKGs provide structured domain knowledge for improved consistency and depth of\nunderstanding. The Incremental Learning approach allows for dynamic updates to\nthe knowledge base without full retraining, significantly reducing\ncomputational overhead and improving the model's adaptability. We evaluate the\nframework using real-world case studies involving health-related queries,\ncomparing it to state-of-the-art models like GPT-4o and a RAG-only baseline.\nExperimental results demonstrate that our approach significantly reduces\nhallucination rates and improves answer completeness and reasoning accuracy.\nThe results underscore the potential of combining RAG, KGs, and multi-agent\nsystems to create intelligent, adaptable systems capable of real-time knowledge\nintegration and reasoning in complex domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13514v1",
    "published_date": "2025-03-14 11:50:16 UTC",
    "updated_date": "2025-03-14 11:50:16 UTC"
  },
  {
    "arxiv_id": "2503.11299v3",
    "title": "BriLLM: Brain-inspired Large Language Model",
    "authors": [
      "Hai Zhao",
      "Hongqiu Wu",
      "Dongjie Yang",
      "Anni Zou",
      "Jiale Hong"
    ],
    "abstract": "This paper reports the first brain-inspired large language model (BriLLM).\nThis is a non-Transformer, non-GPT, non-traditional machine learning\ninput-output controlled generative language model. The model is based on the\nSignal Fully-connected flowing (SiFu) definition on the directed graph in terms\nof the neural network, and has the interpretability of all nodes on the graph\nof the whole model, instead of the traditional machine learning model that only\nhas limited interpretability at the input and output ends. In the language\nmodel scenario, the token is defined as a node in the graph. A randomly shaped\nor user-defined signal flow flows between nodes on the principle of \"least\nresistance\" along paths. The next token or node to be predicted or generated is\nthe target of the signal flow. As a language model, BriLLM theoretically\nsupports infinitely long $n$-gram models when the model size is independent of\nthe input and predicted length of the model. The model's working signal flow\nprovides the possibility of recall activation and innate multi-modal support\nsimilar to the cognitive patterns of the human brain. At present, we released\nthe first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node\nwidth, 16-token long sequence prediction ability, and language model prediction\nperformance comparable to GPT-1. More computing power will help us explore the\ninfinite possibilities depicted above.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11299v3",
    "published_date": "2025-03-14 11:08:30 UTC",
    "updated_date": "2025-05-21 15:02:30 UTC"
  },
  {
    "arxiv_id": "2503.11732v1",
    "title": "Class-Level Feature Selection Method Using Feature Weighted Growing Self-Organising Maps",
    "authors": [
      "Andrew Starkey",
      "Uduak Idio Akpan",
      "Omaimah AL Hosni",
      "Yaseen Pullissery"
    ],
    "abstract": "There have been several attempts to develop Feature Selection (FS) algorithms\ncapable of identifying features that are relevant in a dataset. Although in\ncertain applications the FS algorithms can be seen to be successful, they have\nsimilar basic limitations. In all cases, the global feature selection\nalgorithms seek to select features that are relevant and common to all classes\nof the dataset. This is a major limitation since there could be features that\nare specifically useful for a particular class while irrelevant for other\nclasses, and full explanation of the relationship at class level therefore\ncannot be determined. While the inclusion of such features for all classes\ncould cause improved predictive ability for the relevant class, the same\nfeatures could be problematic for other classes. In this paper, we examine this\nissue and also develop a class-level feature selection method called the\nFeature Weighted Growing Self-Organising Map (FWGSOM). The proposed method\ncarries out feature analysis at class level which enhances its ability to\nidentify relevant features for each class. Results from experiments indicate\nthat our method performs better than other methods, gives explainable results\nat class level, and has a low computational footprint when compared to other\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11732v1",
    "published_date": "2025-03-14 11:02:34 UTC",
    "updated_date": "2025-03-14 11:02:34 UTC"
  },
  {
    "arxiv_id": "2503.11281v3",
    "title": "AI and Deep Learning for Automated Segmentation and Quantitative Measurement of Spinal Structures in MRI",
    "authors": [
      "Praveen Shastry",
      "Bhawana Sonawane",
      "Kavya Mohan",
      "Naveen Kumarasami",
      "Raghotham Sripadraj",
      "Anandakumar D",
      "Keerthana R",
      "Mounigasri M",
      "Kaviya SP",
      "Kishore Prasath Venkatesh",
      "Bargava Subramanian",
      "Kalyan Sivasailam"
    ],
    "abstract": "Background: Accurate spinal structure measurement is crucial for assessing\nspine health and diagnosing conditions like spondylosis, disc herniation, and\nstenosis. Manual methods for measuring intervertebral disc height and spinal\ncanal diameter are subjective and time-consuming. Automated solutions are\nneeded to improve accuracy, efficiency, and reproducibility in clinical\npractice.\n  Purpose: This study develops an autonomous AI system for segmenting and\nmeasuring key spinal structures in MRI scans, focusing on intervertebral disc\nheight and spinal canal anteroposterior (AP) diameter in the cervical, lumbar,\nand thoracic regions. The goal is to reduce clinician workload, enhance\ndiagnostic consistency, and improve assessments.\n  Methods: The AI model leverages deep learning architectures, including UNet,\nnnU-Net, and CNNs. Trained on a large proprietary MRI dataset, it was validated\nagainst expert annotations. Performance was evaluated using Dice coefficients\nand segmentation accuracy.\n  Results: The AI model achieved Dice coefficients of 0.94 for lumbar, 0.91 for\ncervical, and 0.90 for dorsal spine segmentation (D1-D12). It precisely\nmeasured spinal parameters like disc height and canal diameter, demonstrating\nrobustness and clinical applicability.\n  Conclusion: The AI system effectively automates MRI-based spinal\nmeasurements, improving accuracy and reducing clinician workload. Its\nconsistent performance across spinal regions supports clinical decision-making,\nparticularly in high-demand settings, enhancing spinal assessments and patient\noutcomes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "92C55, 68T07, 68U10, 62P10, 65D18"
    ],
    "primary_category": "eess.IV",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11281v3",
    "published_date": "2025-03-14 10:39:52 UTC",
    "updated_date": "2025-03-19 06:18:20 UTC"
  },
  {
    "arxiv_id": "2503.11273v1",
    "title": "Financial Fraud Detection with Entropy Computing",
    "authors": [
      "Babak Emami",
      "Wesley Dyk",
      "David Haycraft",
      "Carrie Spear",
      "Lac Nguyen",
      "Nicholas Chancellor"
    ],
    "abstract": "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.optics",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages including references and appendix, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11273v1",
    "published_date": "2025-03-14 10:30:43 UTC",
    "updated_date": "2025-03-14 10:30:43 UTC"
  },
  {
    "arxiv_id": "2503.11256v1",
    "title": "Line of Duty: Evaluating LLM Self-Knowledge via Consistency in Feasibility Boundaries",
    "authors": [
      "Sahil Kale",
      "Vijaykant Nadadur"
    ],
    "abstract": "As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 8 figures, Accepted to the 5th TrustNLP Workshop at NAACL\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11256v1",
    "published_date": "2025-03-14 10:07:07 UTC",
    "updated_date": "2025-03-14 10:07:07 UTC"
  },
  {
    "arxiv_id": "2503.11249v2",
    "title": "Spherical Tree-Sliced Wasserstein Distance",
    "authors": [
      "Viet-Hoang Tran",
      "Thanh T. Chu",
      "Khoi N. M. Nguyen",
      "Trang Pham",
      "Tam Le",
      "Tan M. Nguyen"
    ],
    "abstract": "Sliced Optimal Transport (OT) simplifies the OT problem in high-dimensional\nspaces by projecting supports of input measures onto one-dimensional lines and\nthen exploiting the closed-form expression of the univariate OT to reduce the\ncomputational burden of OT. Recently, the Tree-Sliced method has been\nintroduced to replace these lines with more intricate structures, known as tree\nsystems. This approach enhances the ability to capture topological information\nof integration domains in Sliced OT while maintaining low computational cost.\nInspired by this approach, in this paper, we present an adaptation of tree\nsystems on OT problems for measures supported on a sphere. As a counterpart to\nthe Radon transform variant on tree systems, we propose a novel spherical Radon\ntransform with a new integration domain called spherical trees. By leveraging\nthis transform and exploiting the spherical tree structures, we derive\nclosed-form expressions for OT problems on the sphere. Consequently, we obtain\nan efficient metric for measures on the sphere, named Spherical Tree-Sliced\nWasserstein (STSW) distance. We provide an extensive theoretical analysis to\ndemonstrate the topology of spherical trees and the well-definedness and\ninjectivity of our Radon transform variant, which leads to an orthogonally\ninvariant distance between spherical measures. Finally, we conduct a wide range\nof numerical experiments, including gradient flows and self-supervised\nlearning, to assess the performance of our proposed metric, comparing it to\nrecent benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11249v2",
    "published_date": "2025-03-14 10:00:13 UTC",
    "updated_date": "2025-03-20 11:04:51 UTC"
  },
  {
    "arxiv_id": "2503.11241v1",
    "title": "Compound Expression Recognition via Large Vision-Language Models",
    "authors": [
      "Jun Yu",
      "Xilong Lu"
    ],
    "abstract": "Compound Expression Recognition (CER) is crucial for understanding human\nemotions and improving human-computer interaction. However, CER faces\nchallenges due to the complexity of facial expressions and the difficulty of\ncapturing subtle emotional cues. To address these issues, we propose a novel\napproach leveraging Large Vision-Language Models (LVLMs). Our method employs a\ntwo-stage fine-tuning process: first, pre-trained LVLMs are fine-tuned on basic\nfacial expressions to establish foundational patterns; second, the model is\nfurther optimized on a compound-expression dataset to refine visual-language\nfeature interactions. Our approach achieves advanced accuracy on the RAF-DB\ndataset and demonstrates strong zero-shot generalization on the C-EXPR-DB\ndataset, showcasing its potential for real-world applications in emotion\nanalysis and human-computer interaction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11241v1",
    "published_date": "2025-03-14 09:46:05 UTC",
    "updated_date": "2025-03-14 09:46:05 UTC"
  },
  {
    "arxiv_id": "2503.11237v1",
    "title": "Collaboration is all you need: LLM Assisted Safe Code Translation",
    "authors": [
      "Rabimba Karanjai",
      "Sam Blackshear",
      "Lei Xu",
      "Weidong Shi"
    ],
    "abstract": "This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11237v1",
    "published_date": "2025-03-14 09:42:07 UTC",
    "updated_date": "2025-03-14 09:42:07 UTC"
  },
  {
    "arxiv_id": "2503.11227v2",
    "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph Construction",
    "authors": [
      "Jian Zhang",
      "Bifan Wei",
      "Shihao Qi",
      "haiping Zhu",
      "Jun Liu",
      "Qika Lin"
    ],
    "abstract": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11227v2",
    "published_date": "2025-03-14 09:23:22 UTC",
    "updated_date": "2025-03-17 06:41:34 UTC"
  },
  {
    "arxiv_id": "2503.11224v1",
    "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models",
    "authors": [
      "Xingtai Lv",
      "Youbang Sun",
      "Kaiyan Zhang",
      "Shang Qu",
      "Xuekai Zhu",
      "Yuchen Fan",
      "Yi Wu",
      "Ermo Hua",
      "Xinwei Long",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "abstract": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11224v1",
    "published_date": "2025-03-14 09:20:31 UTC",
    "updated_date": "2025-03-14 09:20:31 UTC"
  },
  {
    "arxiv_id": "2503.11219v1",
    "title": "MEET: A Million-Scale Dataset for Fine-Grained Geospatial Scene Classification with Zoom-Free Remote Sensing Imagery",
    "authors": [
      "Yansheng Li",
      "Yuning Wu",
      "Gong Cheng",
      "Chao Tao",
      "Bo Dang",
      "Yu Wang",
      "Jiahao Zhang",
      "Chuge Zhang",
      "Yiting Liu",
      "Xu Tang",
      "Jiayi Ma",
      "Yongjun Zhang"
    ],
    "abstract": "Accurate fine-grained geospatial scene classification using remote sensing\nimagery is essential for a wide range of applications. However, existing\napproaches often rely on manually zooming remote sensing images at different\nscales to create typical scene samples. This approach fails to adequately\nsupport the fixed-resolution image interpretation requirements in real-world\nscenarios. To address this limitation, we introduce the Million-scale\nfinE-grained geospatial scEne classification dataseT (MEET), which contains\nover 1.03 million zoom-free remote sensing scene samples, manually annotated\ninto 80 fine-grained categories. In MEET, each scene sample follows a\nscene-inscene layout, where the central scene serves as the reference, and\nauxiliary scenes provide crucial spatial context for finegrained\nclassification. Moreover, to tackle the emerging challenge of scene-in-scene\nclassification, we present the Context-Aware Transformer (CAT), a model\nspecifically designed for this task, which adaptively fuses spatial context to\naccurately classify the scene samples. CAT adaptively fuses spatial context to\naccurately classify the scene samples by learning attentional features that\ncapture the relationships between the center and auxiliary scenes. Based on\nMEET, we establish a comprehensive benchmark for fine-grained geospatial scene\nclassification, evaluating CAT against 11 competitive baselines. The results\ndemonstrate that CAT significantly outperforms these baselines, achieving a\n1.88% higher balanced accuracy (BA) with the Swin-Large backbone, and a notable\n7.87% improvement with the Swin-Huge backbone. Further experiments validate the\neffectiveness of each module in CAT and show the practical applicability of CAT\nin the urban functional zone mapping. The source code and dataset will be\npublicly available at https://jerrywyn.github.io/project/MEET.html.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11219v1",
    "published_date": "2025-03-14 09:10:45 UTC",
    "updated_date": "2025-03-14 09:10:45 UTC"
  },
  {
    "arxiv_id": "2503.11730v1",
    "title": "BACE-RUL: A Bi-directional Adversarial Network with Covariate Encoding for Machine Remaining Useful Life Prediction",
    "authors": [
      "Zekai Zhang",
      "Dan Li",
      "Shunyu Wu",
      "Junya Cai",
      "Bo Zhang",
      "See Kiong Ng",
      "Zibin Zheng"
    ],
    "abstract": "Prognostic and Health Management (PHM) are crucial ways to avoid unnecessary\nmaintenance for Cyber-Physical Systems (CPS) and improve system reliability.\nPredicting the Remaining Useful Life (RUL) is one of the most challenging tasks\nfor PHM. Existing methods require prior knowledge about the system, contrived\nassumptions, or temporal mining to model the life cycles of machine\nequipment/devices, resulting in diminished accuracy and limited applicability\nin real-world scenarios. This paper proposes a Bi-directional Adversarial\nnetwork with Covariate Encoding for machine Remaining Useful Life (BACE-RUL)\nprediction, which only adopts sensor measurements from the current life cycle\nto predict RUL rather than relying on previous consecutive cycle recordings.\nThe current sensor measurements of mechanical devices are encoded to a\nconditional space to better understand the implicit inner mechanical status.\nThe predictor is trained as a conditional generative network with the encoded\nsensor measurements as its conditions. Various experiments on several\nreal-world datasets, including the turbofan aircraft engine dataset and the\ndataset collected from degradation experiments of Li-Ion battery cells, show\nthat the proposed model is a general framework and outperforms state-of-the-art\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been received as a research paper at CollaborateCom\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2503.11730v1",
    "published_date": "2025-03-14 08:56:40 UTC",
    "updated_date": "2025-03-14 08:56:40 UTC"
  },
  {
    "arxiv_id": "2503.11207v1",
    "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?",
    "authors": [
      "Giacomo Camposampiero",
      "Michael Hersche",
      "Roger Wattenhofer",
      "Abu Sebastian",
      "Abbas Rahimi"
    ],
    "abstract": "This work presents a first evaluation of two state-of-the-art Large Reasoning\nModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,\nfocusing on well-established nonverbal human IQ tests based on Raven's\nprogressive matrices. We benchmark with the I-RAVEN dataset and its more\ndifficult extension, I-RAVEN-X, which tests the ability to generalize to longer\nreasoning rules and ranges of the attribute values. To assess the influence of\nvisual uncertainties on these nonverbal analogical reasoning tests, we extend\nthe I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a\ntwo-fold strategy to simulate this imperfect visual perception: 1) we introduce\nconfounding attributes which, being sampled at random, do not contribute to the\nprediction of the correct answer of the puzzles and 2) smoothen the\ndistributions of the input attributes' values. We observe a sharp decline in\nOpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to\njust 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X,\nwhich increases input length and range and emulates perceptual uncertainty.\nThis drop occurred despite spending 3.4x more reasoning tokens. A similar trend\nis also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a\nneuro-symbolic probabilistic abductive model, ARLC, that achieves\nstate-of-the-art performances on I-RAVEN, can robustly reason under all these\nout-of-distribution tests, maintaining strong accuracy with only a modest\nreduction from 98.6% to 88.0%. Our code is available at\nhttps://github.com/IBM/raven-large-language-models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11207v1",
    "published_date": "2025-03-14 08:52:25 UTC",
    "updated_date": "2025-03-14 08:52:25 UTC"
  },
  {
    "arxiv_id": "2503.11197v4",
    "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering",
    "authors": [
      "Gang Li",
      "Jizhong Liu",
      "Heinrich Dinkel",
      "Yadong Niu",
      "Junbo Zhang",
      "Jian Luan"
    ],
    "abstract": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11197v4",
    "published_date": "2025-03-14 08:43:53 UTC",
    "updated_date": "2025-05-14 02:12:43 UTC"
  },
  {
    "arxiv_id": "2503.13511v1",
    "title": "Towards a Digital Twin Modeling Method for Container Terminal Port",
    "authors": [
      "Faouzi Hakimi",
      "Tarek Khaled",
      "Mohammed Al-Kharaz",
      "Arthur Cartel Foahom Gouabou",
      "Kenza Amzil"
    ],
    "abstract": "This paper introduces a novel strategy aimed at enhancing productivity and\nminimizing non-productive movements within container terminals, specifically\nfocusing on container yards. It advocates for the implementation of a digital\ntwin-based methodology to streamline the operations of stacking cranes (SCs)\nresponsible for container handling. The proposed approach entails the creation\nof a virtual container yard that mirrors the physical yard within a digital\ntwin system, facilitating real-time observation and validation. In addition,\nthis article demonstrates the effectiveness of using a digital twin to reduce\nunproductive movements and improve productivity through simulation. It defines\nvarious operational strategies and takes into account different yard contexts,\nproviding a comprehensive understanding of optimisation possibilities. By\nexploiting the capabilities of the digital twin, managers and operators are\nprovided with crucial information on operational dynamics, enabling them to\nidentify areas for improvement. This visualisation helps decision-makers to\nmake informed choices about their stacking strategies, thereby improving the\nefficiency of overall container terminal operations. Overall, this paper\npresent a digital twin solution in container terminal operations, offering a\npowerful tool for optimising productivity and minimising inefficiencies.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13511v1",
    "published_date": "2025-03-14 08:36:03 UTC",
    "updated_date": "2025-03-14 08:36:03 UTC"
  },
  {
    "arxiv_id": "2503.11190v1",
    "title": "Cross-Modal Learning for Music-to-Music-Video Description Generation",
    "authors": [
      "Zhuoyuan Mao",
      "Mengjie Zhao",
      "Qiyu Wu",
      "Zhi Zhong",
      "Wei-Hsiang Liao",
      "Hiromi Wakaki",
      "Yuki Mitsufuji"
    ],
    "abstract": "Music-to-music-video generation is a challenging task due to the intrinsic\ndifferences between the music and video modalities. The advent of powerful\ntext-to-video diffusion models has opened a promising pathway for music-video\n(MV) generation by first addressing the music-to-MV description task and\nsubsequently leveraging these models for video generation. In this study, we\nfocus on the MV description generation task and propose a comprehensive\npipeline encompassing training data construction and multimodal model\nfine-tuning. We fine-tune existing pre-trained multimodal models on our newly\nconstructed music-to-MV description dataset based on the Music4All dataset,\nwhich integrates both musical and visual information. Our experimental results\ndemonstrate that music representations can be effectively mapped to textual\ndomains, enabling the generation of meaningful MV description directly from\nmusic inputs. We also identify key components in the dataset construction\npipeline that critically impact the quality of MV description and highlight\nspecific musical attributes that warrant greater focus for improved MV\ndescription generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by RepL4NLP 2025 @ NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11190v1",
    "published_date": "2025-03-14 08:34:28 UTC",
    "updated_date": "2025-03-14 08:34:28 UTC"
  },
  {
    "arxiv_id": "2503.11185v1",
    "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
    "authors": [
      "Yingjie Zhang",
      "Tong Liu",
      "Zhe Zhao",
      "Guozhu Meng",
      "Kai Chen"
    ],
    "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use\ncrafted prompts to elicit toxic responses. These attacks exploit LLMs'\ndifficulty in dynamically detecting harmful intents during the generation\nprocess. Traditional safety alignment methods, often relying on the initial few\ngeneration steps, are ineffective due to limited computational budget. This\npaper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to\nprogressively detoxify generated content, significantly improving both the\ncomputational budget and effectiveness of mitigating harmful generation. Our\napproach uses a hybrid loss function operating on hidden states to directly\nimprove LLMs' inherent awareness of toxity during generation. Furthermore, we\nredefine safe responses by generating semantically relevant answers to harmful\nqueries, thereby increasing robustness against representation-mutation attacks.\nEvaluations across multiple LLMs demonstrate state-of-the-art defense\nperformance against six different attack types, reducing Attack Success Rates\nby up to two orders of magnitude compared to previous state-of-the-art defense\nwhile preserving utility. This work advances LLM safety by addressing\nlimitations of conventional alignment through dynamic, context-aware\nmitigation.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11185v1",
    "published_date": "2025-03-14 08:32:12 UTC",
    "updated_date": "2025-03-14 08:32:12 UTC"
  },
  {
    "arxiv_id": "2503.11728v1",
    "title": "Forecasting Empty Container availability for Vehicle Booking System Application",
    "authors": [
      "Arthur Cartel Foahom Gouabou",
      "Mohammed Al-Kharaz",
      "Faouzi Hakimi",
      "Tarek Khaled",
      "Kenza Amzil"
    ],
    "abstract": "Container terminals, pivotal nodes in the network of empty container\nmovement, hold significant potential for enhancing operational efficiency\nwithin terminal depots through effective collaboration between transporters and\nterminal operators. This collaboration is crucial for achieving optimization,\nleading to streamlined operations and reduced congestion, thereby benefiting\nboth parties. Consequently, there is a pressing need to develop the most\nsuitable forecasting approaches to address this challenge. This study focuses\non developing and evaluating a data-driven approach for forecasting empty\ncontainer availability at container terminal depots within a Vehicle Booking\nSystem (VBS) framework. It addresses the gap in research concerning optimizing\nempty container dwell time and aims to enhance operational efficiencies in\ncontainer terminal operations. Four forecasting models-Naive, ARIMA, Prophet,\nand LSTM-are comprehensively analyzed for their predictive capabilities, with\nLSTM emerging as the top performer due to its ability to capture complex time\nseries patterns. The research underscores the significance of selecting\nappropriate forecasting techniques tailored to the specific requirements of\ncontainer terminal operations, contributing to improved operational planning\nand management in maritime logistics.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11728v1",
    "published_date": "2025-03-14 08:29:04 UTC",
    "updated_date": "2025-03-14 08:29:04 UTC"
  },
  {
    "arxiv_id": "2503.11181v1",
    "title": "Multi-Stage Generative Upscaler: Reconstructing Football Broadcast Images via Diffusion Models",
    "authors": [
      "Luca Martini",
      "Daniele Zolezzi",
      "Saverio Iacono",
      "Gianni Viardo Vercelli"
    ],
    "abstract": "The reconstruction of low-resolution football broadcast images presents a\nsignificant challenge in sports broadcasting, where detailed visuals are\nessential for analysis and audience engagement. This study introduces a\nmulti-stage generative upscaling framework leveraging Diffusion Models to\nenhance degraded images, transforming inputs as small as $64 \\times 64$ pixels\ninto high-fidelity $1024 \\times 1024$ outputs. By integrating an image-to-image\npipeline, ControlNet conditioning, and LoRA fine-tuning, our approach surpasses\ntraditional upscaling methods in restoring intricate textures and\ndomain-specific elements such as player details and jersey logos. The custom\nLoRA is trained on a custom football dataset, ensuring adaptability to sports\nbroadcast needs. Experimental results demonstrate substantial improvements over\nconventional models, with ControlNet refining fine details and LoRA enhancing\ntask-specific elements. These findings highlight the potential of\ndiffusion-based image reconstruction in sports media, paving the way for future\napplications in automated video enhancement and real-time sports analytics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11181v1",
    "published_date": "2025-03-14 08:28:30 UTC",
    "updated_date": "2025-03-14 08:28:30 UTC"
  },
  {
    "arxiv_id": "2503.11175v1",
    "title": "Zero-TIG: Temporal Consistency-Aware Zero-Shot Illumination-Guided Low-light Video Enhancement",
    "authors": [
      "Yini Li",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Low-light and underwater videos suffer from poor visibility, low contrast,\nand high noise, necessitating enhancements in visual quality. However, existing\napproaches typically rely on paired ground truth, which limits their\npracticality and often fails to maintain temporal consistency. To overcome\nthese obstacles, this paper introduces a novel zero-shot learning approach\nnamed Zero-TIG, leveraging the Retinex theory and optical flow techniques. The\nproposed network consists of an enhancement module and a temporal feedback\nmodule. The enhancement module comprises three subnetworks: low-light image\ndenoising, illumination estimation, and reflection denoising. The temporal\nenhancement module ensures temporal consistency by incorporating histogram\nequalization, optical flow computation, and image warping to align the enhanced\nprevious frame with the current frame, thereby maintaining continuity.\nAdditionally, we address color distortion in underwater data by adaptively\nbalancing RGB channels. The experimental results demonstrate that our method\nachieves low-light video enhancement without the need for paired training data,\nmaking it a promising and applicable method for real-world scenario\nenhancement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11175v1",
    "published_date": "2025-03-14 08:22:26 UTC",
    "updated_date": "2025-03-14 08:22:26 UTC"
  },
  {
    "arxiv_id": "2503.11167v1",
    "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction",
    "authors": [
      "Haonan Wang",
      "Qixiang Zhang",
      "Lehan Wang",
      "Xuanqi Huang",
      "Xiaomeng Li"
    ],
    "abstract": "Decoding visual stimuli from neural activity is essential for understanding\nthe human brain. While fMRI methods have successfully reconstructed static\nimages, fMRI-to-video reconstruction faces challenges due to the need for\ncapturing spatiotemporal dynamics like motion and scene transitions. Recent\napproaches have improved semantic and perceptual alignment but struggle to\nintegrate coarse fMRI data with detailed visual features. Inspired by the\nhierarchical organization of the visual system, we propose NEURONS, a novel\nframework that decouples learning into four correlated sub-tasks: key object\nsegmentation, concept recognition, scene description, and blurry video\nreconstruction. This approach simulates the visual cortex's functional\nspecialization, allowing the model to capture diverse video content. In the\ninference stage, NEURONS generates robust conditioning signals for a\npre-trained text-to-video diffusion model to reconstruct the videos. Extensive\nexperiments demonstrate that NEURONS outperforms state-of-the-art baselines,\nachieving solid improvements in video consistency (26.6%) and semantic-level\naccuracy (19.1%). Notably, NEURONS shows a strong functional correlation with\nthe visual cortex, highlighting its potential for brain-computer interfaces and\nclinical applications. Code and model weights will be available at:\nhttps://github.com/xmed-lab/NEURONS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11167v1",
    "published_date": "2025-03-14 08:12:28 UTC",
    "updated_date": "2025-03-14 08:12:28 UTC"
  },
  {
    "arxiv_id": "2503.11160v1",
    "title": "Unifying Perplexing Behaviors in Modified BP Attributions through Alignment Perspective",
    "authors": [
      "Guanhua Zheng",
      "Jitao Sang",
      "Changsheng Xu"
    ],
    "abstract": "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11160v1",
    "published_date": "2025-03-14 07:58:26 UTC",
    "updated_date": "2025-03-14 07:58:26 UTC"
  },
  {
    "arxiv_id": "2503.11154v1",
    "title": "Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models",
    "authors": [
      "Shaotian Yan",
      "Chen Shen",
      "Wenxiao Wang",
      "Liang Xie",
      "Junjie Liu",
      "Jieping Ye"
    ],
    "abstract": "Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning\ncapabilities of large language models (LLMs), functioning as a whole to guide\nthese models in generating reasoning steps toward final answers. However, we\nobserve that isolated segments, words, or tokens within CoT demonstrations can\nunexpectedly disrupt the generation process of LLMs. The model may overly\nconcentrate on certain local information present in the demonstration,\nintroducing irrelevant noise into the reasoning process and potentially leading\nto incorrect answers. In this paper, we investigate the underlying mechanism of\nCoT through dynamically tracing and manipulating the inner workings of LLMs at\neach output step, which demonstrates that tokens exhibiting specific attention\ncharacteristics are more likely to induce the model to take things out of\ncontext; these tokens directly attend to the hidden states tied with\nprediction, without substantial integration of non-local information. Building\nupon these insights, we propose a Few-shot Attention Intervention method (FAI)\nthat dynamically analyzes the attention patterns of demonstrations to\naccurately identify these tokens and subsequently make targeted adjustments to\nthe attention weights to effectively suppress their distracting effect on LLMs.\nComprehensive experiments across multiple benchmarks demonstrate consistent\nimprovements over baseline methods, with a remarkable 5.91% improvement on the\nAQuA dataset, further highlighting the effectiveness of FAI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11154v1",
    "published_date": "2025-03-14 07:46:33 UTC",
    "updated_date": "2025-03-14 07:46:33 UTC"
  },
  {
    "arxiv_id": "2503.11144v1",
    "title": "MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling",
    "authors": [
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "abstract": "Large-scale pre-training of deep models, followed by fine-tuning them, has\nbecome the cornerstone of natural language processing (NLP). The prevalence of\ndata coupled with computational resources has led to large models with a\nconsiderable number of parameters. While the massive size of these models has\nled to remarkable success in many NLP tasks, a detriment is the expense\nrequired to retrain all the base model's parameters for the adaptation to each\ntask or domain. Parameter Efficient Fine-Tuning (PEFT) provides an effective\nsolution for this challenge by minimizing the number of parameters required to\nbe fine-tuned while maintaining the quality of the model. While existing\nmethods have achieved impressive results, they mainly focus on adapting a\nsubset of parameters, weight reparameterization, and prompt engineering. In\nthis paper, we study layers as extractors of different types of linguistic\ninformation that are valuable when used in conjunction. We then propose the\nMixture of Layer Experts (MoLEx), a novel sparse mixture of experts (SMoE)\nwhose experts are layers in the pre-trained model. It performs a conditional\ncomputation of a mixture of layers during fine-tuning to provide the model with\nmore structural knowledge about the data. By providing an avenue for\ninformation exchange between layers, MoLEx enables the model to make a more\nwell-informed prediction for the downstream task, leading to better fine-tuning\nresults with the same number of effective parameters. As experts can be\nprocessed in parallel, MoLEx introduces minimal additional computational\noverhead. We empirically corroborate the advantages of MoLEx when combined with\npopular PEFT baseline methods on a variety of downstream fine-tuning tasks,\nincluding the popular GLUE benchmark as well as the End-to-End Challenge (E2E).\nThe code is publicly available at https://github.com/rachtsy/molex.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11144v1",
    "published_date": "2025-03-14 07:22:07 UTC",
    "updated_date": "2025-03-14 07:22:07 UTC"
  },
  {
    "arxiv_id": "2503.11129v2",
    "title": "Direction-Aware Diagonal Autoregressive Image Generation",
    "authors": [
      "Yijia Xu",
      "Jianzhong Ju",
      "Jian Luan",
      "Jinshi Cui"
    ],
    "abstract": "The raster-ordered image token sequence exhibits a significant Euclidean\ndistance between index-adjacent tokens at line breaks, making it unsuitable for\nautoregressive generation. To address this issue, this paper proposes\nDirection-Aware Diagonal Autoregressive Image Generation (DAR) method, which\ngenerates image tokens following a diagonal scanning order. The proposed\ndiagonal scanning order ensures that tokens with adjacent indices remain in\nclose proximity while enabling causal attention to gather information from a\nbroader range of directions. Additionally, two direction-aware modules: 4D-RoPE\nand direction embeddings are introduced, enhancing the model's capability to\nhandle frequent changes in generation direction. To leverage the\nrepresentational capacity of the image tokenizer, we use its codebook as the\nimage token embeddings. We propose models of varying scales, ranging from 485M\nto 2.0B. On the 256$\\times$256 ImageNet benchmark, our DAR-XL (2.0B)\noutperforms all previous autoregressive image generators, achieving a\nstate-of-the-art FID score of 1.37.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11129v2",
    "published_date": "2025-03-14 06:44:01 UTC",
    "updated_date": "2025-04-16 06:16:13 UTC"
  },
  {
    "arxiv_id": "2503.11127v1",
    "title": "Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning",
    "authors": [
      "Matthew Khoriaty",
      "Andrii Shportko",
      "Gustavo Mercier",
      "Zach Wood-Doughty"
    ],
    "abstract": "Recent developments in Large Language Model (LLM) capabilities have brought\ngreat potential but also posed new risks. For example, LLMs with knowledge of\nbioweapons, advanced chemistry, or cyberattacks could cause violence if placed\nin the wrong hands or during malfunctions. Because of their nature as\nnear-black boxes, intuitive interpretation of LLM internals remains an open\nresearch question, preventing developers from easily controlling model behavior\nand capabilities. The use of Sparse Autoencoders (SAEs) has recently emerged as\na potential method of unraveling representations of concepts in LLMs internals,\nand has allowed developers to steer model outputs by directly modifying the\nhidden activations. In this paper, we use SAEs to identify unwanted concepts\nfrom the Weapons of Mass Destruction Proxy (WMDP) dataset within gemma-2-2b\ninternals and use feature steering to reduce the model's ability to answer\nharmful questions while retaining its performance on harmless queries. Our\nresults bring back optimism to the viability of SAE-based explicit knowledge\nunlearning techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11127v1",
    "published_date": "2025-03-14 06:43:19 UTC",
    "updated_date": "2025-03-14 06:43:19 UTC"
  },
  {
    "arxiv_id": "2503.11118v1",
    "title": "UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with Prompt Optimization and Supervised Fine-Tuning",
    "authors": [
      "Kristin Qi",
      "Youxiang Zhu",
      "Xiaohui Liang"
    ],
    "abstract": "We present our approach to the PerAnsSumm Shared Task, which involves\nperspective span identification and perspective-aware summarization in\ncommunity question-answering (CQA) threads. For span identification, we adopt\nensemble learning that integrates three transformer models through averaging to\nexploit individual model strengths, achieving an 82.91% F1-score on test data.\nFor summarization, we design a suite of Chain-of-Thought (CoT) prompting\nstrategies that incorporate keyphrases and guide information to structure\nsummary generation into manageable steps. To further enhance summary quality,\nwe apply prompt optimization using the DSPy framework and supervised\nfine-tuning (SFT) on Llama-3 to adapt the model to domain-specific data.\nExperimental results on validation and test sets show that structured prompts\nwith keyphrases and guidance improve summaries aligned with references, while\nthe combination of prompt optimization and fine-tuning together yields\nsignificant improvement in both relevance and factuality evaluation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "CL4HEALTH NAACL: Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics",
    "pdf_url": "http://arxiv.org/pdf/2503.11118v1",
    "published_date": "2025-03-14 06:29:51 UTC",
    "updated_date": "2025-03-14 06:29:51 UTC"
  },
  {
    "arxiv_id": "2503.13510v1",
    "title": "Prompt Sentiment: The Catalyst for LLM Change",
    "authors": [
      "Vishal Gandhi",
      "Sagar Gandhi"
    ],
    "abstract": "The rise of large language models (LLMs) has revolutionized natural language\nprocessing (NLP), yet the influence of prompt sentiment, a latent affective\ncharacteristic of input text, remains underexplored. This study systematically\nexamines how sentiment variations in prompts affect LLM-generated outputs in\nterms of coherence, factuality, and bias. Leveraging both lexicon-based and\ntransformer-based sentiment analysis methods, we categorize prompts and\nevaluate responses from five leading LLMs: Claude, DeepSeek, GPT-4, Gemini, and\nLLaMA. Our analysis spans six AI-driven applications, including content\ngeneration, conversational AI, legal and financial analysis, healthcare AI,\ncreative writing, and technical documentation. By transforming prompts, we\nassess their impact on output quality. Our findings reveal that prompt\nsentiment significantly influences model responses, with negative prompts often\nreducing factual accuracy and amplifying bias, while positive prompts tend to\nincrease verbosity and sentiment propagation. These results highlight the\nimportance of sentiment-aware prompt engineering for ensuring fair and reliable\nAI-generated content.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13510v1",
    "published_date": "2025-03-14 06:25:21 UTC",
    "updated_date": "2025-03-14 06:25:21 UTC"
  },
  {
    "arxiv_id": "2503.11108v2",
    "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer Decoding",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Yu Tian"
    ],
    "abstract": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11108v2",
    "published_date": "2025-03-14 06:01:42 UTC",
    "updated_date": "2025-03-27 07:02:19 UTC"
  },
  {
    "arxiv_id": "2503.11103v1",
    "title": "Quantifying Interpretability in CLIP Models with Concept Consistency",
    "authors": [
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "abstract": "CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. While recent work has proposed decomposition-based interpretability\nmethods for identifying textual descriptions of attention heads in CLIP, the\nimplications of conceptual consistency in these text labels on interpretability\nand model performance has not been explored. To bridge this gap, we study the\nconceptual consistency of text descriptions for attention heads in CLIP-like\nmodels. We conduct extensive experiments on six different models from OpenAI\nand OpenCLIP which vary by size, type of pre-training data and patch size. We\npropose Concept Consistency Score (CCS), a novel interpretability metric that\nmeasures how consistently individual attention heads in CLIP models align with\nspecific concepts. To assign concept labels to heads, we use in-context\nlearning with ChatGPT, guided by a few manually-curated examples, and validate\nthese labels using an LLM-as-a-judge approach. Our soft-pruning experiments\nreveal that high CCS heads are critical for preserving model performance, as\npruning them leads to a significantly larger performance drop than pruning\nrandom or low CCS heads. Notably, we find that high CCS heads capture essential\nconcepts and play a key role in out-of-domain detection, concept-specific\nreasoning, and video-language understanding. These results position CCS as a\npowerful interpretability metric for analyzing CLIP-like models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11103v1",
    "published_date": "2025-03-14 05:47:17 UTC",
    "updated_date": "2025-03-14 05:47:17 UTC"
  },
  {
    "arxiv_id": "2503.11096v1",
    "title": "Augmenting Image Annotation: A Human-LMM Collaborative Framework for Efficient Object Selection and Label Generation",
    "authors": [
      "He Zhang",
      "Xinyi Fu",
      "John M. Carroll"
    ],
    "abstract": "Traditional image annotation tasks rely heavily on human effort for object\nselection and label assignment, making the process time-consuming and prone to\ndecreased efficiency as annotators experience fatigue after extensive work.\nThis paper introduces a novel framework that leverages the visual understanding\ncapabilities of large multimodal models (LMMs), particularly GPT, to assist\nannotation workflows. In our proposed approach, human annotators focus on\nselecting objects via bounding boxes, while the LMM autonomously generates\nrelevant labels. This human-AI collaborative framework enhances annotation\nefficiency by reducing the cognitive and time burden on human annotators. By\nanalyzing the system's performance across various types of annotation tasks, we\ndemonstrate its ability to generalize to tasks such as object recognition,\nscene description, and fine-grained categorization. Our proposed framework\nhighlights the potential of this approach to redefine annotation workflows,\noffering a scalable and efficient solution for large-scale data labeling in\ncomputer vision. Finally, we discuss how integrating LMMs into the annotation\npipeline can advance bidirectional human-AI alignment, as well as the\nchallenges of alleviating the \"endless annotation\" burden in the face of\ninformation overload by shifting some of the work to AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper will appear at ICLR 2025 Workshop on Bidirectional\n  Human-AI Alignment",
    "pdf_url": "http://arxiv.org/pdf/2503.11096v1",
    "published_date": "2025-03-14 05:38:53 UTC",
    "updated_date": "2025-03-14 05:38:53 UTC"
  },
  {
    "arxiv_id": "2503.11089v1",
    "title": "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks",
    "authors": [
      "Yi Zhang",
      "Qiang Zhang",
      "Xiaozhu Ju",
      "Zhaoyang Liu",
      "Jilei Mao",
      "Jingkai Sun",
      "Jintao Wu",
      "Shixiong Gao",
      "Shihan Cai",
      "Zhiyuan Qin",
      "Linkai Liang",
      "Jiaxu Wang",
      "Yiqun Duan",
      "Jiahang Cao",
      "Renjing Xu",
      "Jian Tang"
    ],
    "abstract": "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "technical report",
    "pdf_url": "http://arxiv.org/pdf/2503.11089v1",
    "published_date": "2025-03-14 05:06:07 UTC",
    "updated_date": "2025-03-14 05:06:07 UTC"
  },
  {
    "arxiv_id": "2503.11086v1",
    "title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
    "authors": [
      "Haihong Zhao",
      "Chenyi Zi",
      "Aochuan Chen",
      "Jia Li"
    ],
    "abstract": "Graph learning plays a vital role in mining and analyzing complex\nrelationships involved in graph data, which is widely used in many real-world\napplications like transaction networks and communication networks. Foundation\nmodels in CV and NLP have shown powerful cross-domain capabilities that are\nalso significant in graph domains. However, existing graph learning approaches\nstruggle with cross-domain tasks. Inspired by successes in CV and NLP,\ncross-domain graph learning has once again become a focal point of attention to\nrealizing true graph foundation models. In this survey, we present a\ncomprehensive review and analysis of existing works on cross-domain graph\nlearning. Concretely, we first propose a new taxonomy, categorizing existing\napproaches based on the learned cross-domain information: structure, feature,\nand structure-feature mixture. Next, we systematically survey representative\nmethods in these categories. Finally, we discuss the remaining limitations of\nexisting studies and highlight promising avenues for future research. Relevant\npapers are summarized and will be consistently updated at:\nhttps://github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11086v1",
    "published_date": "2025-03-14 04:53:27 UTC",
    "updated_date": "2025-03-14 04:53:27 UTC"
  },
  {
    "arxiv_id": "2503.11081v1",
    "title": "MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation",
    "authors": [
      "Pingrui Zhang",
      "Xianqiang Gao",
      "Yuhan Wu",
      "Kehui Liu",
      "Dong Wang",
      "Zhigang Wang",
      "Bin Zhao",
      "Yan Ding",
      "Xuelong Li"
    ],
    "abstract": "In mobile manipulation, navigation and manipulation are often treated as\nseparate problems, resulting in a significant gap between merely approaching an\nobject and engaging with it effectively. Many navigation approaches primarily\ndefine success by proximity to the target, often overlooking the necessity for\noptimal positioning that facilitates subsequent manipulation. To address this,\nwe introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples\nthat provide training data for models to learn optimal final navigation\npositions for seamless transition to manipulation. Our dataset includes\naffordance-grounded floor labels collected from diverse kitchen environments,\nin which robotic mobile manipulators of different models attempt to grasp\ntarget objects amidst clutter. Using a fully automated pipeline, we simulate\ndiverse real-world scenarios and generate affordance labels for optimal\nmanipulation positions. Visual data are collected from RGB-D inputs captured by\na first-person view camera mounted on the robotic arm, ensuring consistency in\nviewpoint during data collection. We also develop a lightweight baseline model,\nNavAff, for navigation affordance grounding that demonstrates promising\nperformance on the MoMa-Kitchen benchmark. Our approach enables models to learn\naffordance-based final positioning that accommodates different arm types and\nplatform heights, thereby paving the way for more robust and generalizable\nintegration of navigation and manipulation in embodied AI. Project page:\n\\href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11081v1",
    "published_date": "2025-03-14 04:47:38 UTC",
    "updated_date": "2025-03-14 04:47:38 UTC"
  },
  {
    "arxiv_id": "2503.11074v1",
    "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities",
    "authors": [
      "Xueyang Zhou",
      "Guiyao Tie",
      "Guowen Zhang",
      "Weidong Wang",
      "Zhigang Zuo",
      "Di Wu",
      "Duanfeng Chu",
      "Pan Zhou",
      "Lichao Sun",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward\nadvanced computational reasoning. Yet, this progress disrupts traditional agent\nframeworks, traditionally anchored by execution-oriented Large Language Models\n(LLMs). To explore this transformation, we propose the LaRMA framework,\nencompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,\nassessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs\n(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass\nLLMs in reasoning-intensive tasks like Plan Design, leveraging iterative\nreflection for superior outcomes; LLMs excel in execution-driven tasks such as\nTool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing\nLLMs as actors with LRMs as reflectors, optimize agent performance by blending\nexecution speed with reasoning depth; and LRMs' enhanced reasoning incurs\nhigher computational costs, prolonged processing, and behavioral challenges,\nincluding overthinking and fact-ignoring tendencies. This study fosters deeper\ninquiry into LRMs' balance of deep thinking and overthinking, laying a critical\nfoundation for future agent design advancements.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "71 pages, 5 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.11074v1",
    "published_date": "2025-03-14 04:34:31 UTC",
    "updated_date": "2025-03-14 04:34:31 UTC"
  },
  {
    "arxiv_id": "2503.11726v1",
    "title": "SPECTra: Scalable Multi-Agent Reinforcement Learning with Permutation-Free Networks",
    "authors": [
      "Hyunwoo Park",
      "Baekryun Seong",
      "Sang-Ki Ko"
    ],
    "abstract": "In cooperative multi-agent reinforcement learning (MARL), the permutation\nproblem where the state space grows exponentially with the number of agents\nreduces sample efficiency. Additionally, many existing architectures struggle\nwith scalability, relying on a fixed structure tied to a specific number of\nagents, limiting their applicability to environments with a variable number of\nentities. While approaches such as graph neural networks (GNNs) and\nself-attention mechanisms have progressed in addressing these challenges, they\nhave significant limitations as dense GNNs and self-attention mechanisms incur\nhigh computational costs. To overcome these limitations, we propose a novel\nagent network and a non-linear mixing network that ensure\npermutation-equivariance and scalability, allowing them to generalize to\nenvironments with various numbers of agents. Our agent network significantly\nreduces computational complexity, and our scalable hypernetwork enables\nefficient weight generation for non-linear mixing. Additionally, we introduce\ncurriculum learning to improve training efficiency. Experiments on SMACv2 and\nGoogle Research Football (GRF) demonstrate that our approach achieves superior\nlearning performance compared to existing methods. By addressing both\npermutation-invariance and scalability in MARL, our work provides a more\nefficient and adaptable framework for cooperative MARL. Our code is available\nat https://github.com/funny-rl/SPECTra.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11726v1",
    "published_date": "2025-03-14 04:26:51 UTC",
    "updated_date": "2025-03-14 04:26:51 UTC"
  },
  {
    "arxiv_id": "2503.11069v1",
    "title": "API Agents vs. GUI Agents: Divergence and Convergence",
    "authors": [
      "Chaoyun Zhang",
      "Shilin He",
      "Liqun Li",
      "Si Qin",
      "Yu Kang",
      "Qingwei Lin",
      "Dongmei Zhang"
    ],
    "abstract": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11069v1",
    "published_date": "2025-03-14 04:26:21 UTC",
    "updated_date": "2025-03-14 04:26:21 UTC"
  },
  {
    "arxiv_id": "2503.11065v1",
    "title": "Low-cost Real-world Implementation of the Swing-up Pendulum for Deep Reinforcement Learning Experiments",
    "authors": [
      "Peter Böhm",
      "Pauline Pounds",
      "Archie C. Chapman"
    ],
    "abstract": "Deep reinforcement learning (DRL) has had success in virtual and simulated\ndomains, but due to key differences between simulated and real-world\nenvironments, DRL-trained policies have had limited success in real-world\napplications. To assist researchers to bridge the \\textit{sim-to-real gap}, in\nthis paper, we describe a low-cost physical inverted pendulum apparatus and\nsoftware environment for exploring sim-to-real DRL methods. In particular, the\ndesign of our apparatus enables detailed examination of the delays that arise\nin physical systems when sensing, communicating, learning, inferring and\nactuating. Moreover, we wish to improve access to educational systems, so our\napparatus uses readily available materials and parts to reduce cost and\nlogistical barriers. Our design shows how commercial, off-the-shelf electronics\nand electromechanical and sensor systems, combined with common metal\nextrusions, dowel and 3D printed couplings provide a pathway for affordable\nphysical DRL apparatus. The physical apparatus is complemented with a simulated\nenvironment implemented using a high-fidelity physics engine and OpenAI Gym\ninterface.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Australasian Conference on Robotics and Automation (ACRA) 2022",
    "pdf_url": "http://arxiv.org/pdf/2503.11065v1",
    "published_date": "2025-03-14 04:18:36 UTC",
    "updated_date": "2025-03-14 04:18:36 UTC"
  },
  {
    "arxiv_id": "2503.11059v1",
    "title": "Training Directional Locomotion for Quadrupedal Low-Cost Robotic Systems via Deep Reinforcement Learning",
    "authors": [
      "Peter Böhm",
      "Archie C. Chapman",
      "Pauline Pounds"
    ],
    "abstract": "In this work we present Deep Reinforcement Learning (DRL) training of\ndirectional locomotion for low-cost quadrupedal robots in the real world. In\nparticular, we exploit randomization of heading that the robot must follow to\nfoster exploration of action-state transitions most useful for learning both\nforward locomotion as well as course adjustments. Changing the heading in\nepisode resets to current yaw plus a random value drawn from a normal\ndistribution yields policies able to follow complex trajectories involving\nfrequent turns in both directions as well as long straight-line stretches. By\nrepeatedly changing the heading, this method keeps the robot moving within the\ntraining platform and thus reduces human involvement and need for manual resets\nduring the training. Real world experiments on a custom-built, low-cost\nquadruped demonstrate the efficacy of our method with the robot successfully\nnavigating all validation tests. When trained with other approaches, the robot\nonly succeeds in forward locomotion test and fails when turning is required.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Australasian Conference on Robotics and Automation (ACRA) 2022",
    "pdf_url": "http://arxiv.org/pdf/2503.11059v1",
    "published_date": "2025-03-14 03:53:01 UTC",
    "updated_date": "2025-03-14 03:53:01 UTC"
  },
  {
    "arxiv_id": "2503.11050v1",
    "title": "Distance-Based Tree-Sliced Wasserstein Distance",
    "authors": [
      "Hoang V. Tran",
      "Khoi N. M. Nguyen",
      "Trang Pham",
      "Thanh T. Chu",
      "Tam Le",
      "Tan M. Nguyen"
    ],
    "abstract": "To overcome computational challenges of Optimal Transport (OT), several\nvariants of Sliced Wasserstein (SW) has been developed in the literature. These\napproaches exploit the closed-form expression of the univariate OT by\nprojecting measures onto (one-dimensional) lines. However, projecting measures\nonto low-dimensional spaces can lead to a loss of topological information.\nTree-Sliced Wasserstein distance on Systems of Lines (TSW-SL) has emerged as a\npromising alternative that replaces these lines with a more advanced structure\ncalled tree systems. The tree structures enhance the ability to capture\ntopological information of the metric while preserving computational\nefficiency. However, at the core of TSW-SL, the splitting maps, which serve as\nthe mechanism for pushing forward measures onto tree systems, focus solely on\nthe position of the measure supports while disregarding the projecting domains.\nMoreover, the specific splitting map used in TSW-SL leads to a metric that is\nnot invariant under Euclidean transformations, a typically expected property\nfor OT on Euclidean space. In this work, we propose a novel class of splitting\nmaps that generalizes the existing one studied in TSW-SL enabling the use of\nall positional information from input measures, resulting in a novel\nDistance-based Tree-Sliced Wasserstein (Db-TSW) distance. In addition, we\nintroduce a simple tree sampling process better suited for Db-TSW, leading to\nan efficient GPU-friendly implementation for tree systems, similar to the\noriginal SW. We also provide a comprehensive theoretical analysis of proposed\nclass of splitting maps to verify the injectivity of the corresponding Radon\nTransform, and demonstrate that Db-TSW is an Euclidean invariant metric. We\nempirically show that Db-TSW significantly improves accuracy compared to recent\nSW variants while maintaining low computational cost via a wide range of\nexperiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11050v1",
    "published_date": "2025-03-14 03:36:44 UTC",
    "updated_date": "2025-03-14 03:36:44 UTC"
  },
  {
    "arxiv_id": "2503.11046v1",
    "title": "Measuring Similarity in Causal Graphs: A Framework for Semantic and Structural Analysis",
    "authors": [
      "Ning-Yuan Georgia Liu",
      "Flower Yang",
      "Mohammad S. Jalali"
    ],
    "abstract": "Causal graphs are commonly used to understand and model complex systems.\nResearchers often construct these graphs from different perspectives, leading\nto significant variations for the same problem. Comparing causal graphs is,\ntherefore, essential for evaluating assumptions, integrating insights, and\nresolving disagreements. The rise of AI tools has further amplified this need,\nas they are increasingly used to generate hypothesized causal graphs by\nsynthesizing information from various sources such as prior research and\ncommunity inputs, providing the potential for automating and scaling causal\nmodeling for complex systems. Similar to humans, these tools also produce\ninconsistent results across platforms, versions, and iterations. Despite its\nimportance, research on causal graph comparison remains scarce. Existing\nmethods often focus solely on structural similarities, assuming identical\nvariable names, and fail to capture nuanced semantic relationships, which is\nessential for causal graph comparison. We address these gaps by investigating\nmethods for comparing causal graphs from both semantic and structural\nperspectives. First, we reviewed over 40 existing metrics and, based on\npredefined criteria, selected nine for evaluation from two threads of machine\nlearning: four semantic similarity metrics and five learning graph kernels. We\ndiscuss the usability of these metrics in simple examples to illustrate their\nstrengths and limitations. We then generated a synthetic dataset of 2,000\ncausal graphs using generative AI based on a reference diagram. Our findings\nreveal that each metric captures a different aspect of similarity, highlighting\nthe need to use multiple metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 68R10, 62H30",
      "I.2.6; G.2.2; I.5.4; H.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.11046v1",
    "published_date": "2025-03-14 03:29:26 UTC",
    "updated_date": "2025-03-14 03:29:26 UTC"
  },
  {
    "arxiv_id": "2503.11037v1",
    "title": "Resource Constrained Pathfinding with A* and Negative Weights",
    "authors": [
      "Saman Ahmadi",
      "Andrea Raith",
      "Mahdi Jalili"
    ],
    "abstract": "Constrained pathfinding is a well-studied, yet challenging network\noptimisation problem that can be seen in a broad range of real-world\napplications. Pathfinding with multiple resource limits, which is known as the\nResource Constrained Shortest Path Problem (RCSP), aims to plan a cost-optimum\npath subject to limited usage of resources. Given the recent advances in\nconstrained and multi-criteria search with A*, this paper introduces a new\nresource constrained search framework on the basis of A* to tackle RCSP in\nlarge networks, even in the presence of negative cost and negative resources.\nWe empirically evaluate our new algorithm on a set of large instances and show\nup to two orders of magnitude faster performance compared to state-of-the-art\nRCSP algorithms in the literature.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages 2 figures 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.11037v1",
    "published_date": "2025-03-14 03:06:40 UTC",
    "updated_date": "2025-03-14 03:06:40 UTC"
  },
  {
    "arxiv_id": "2503.11031v2",
    "title": "Fourier Neural Operator based surrogates for $CO_2$ storage in realistic geologies",
    "authors": [
      "Anirban Chandra",
      "Marius Koch",
      "Suraj Pawar",
      "Aniruddha Panda",
      "Kamyar Azizzadenesheli",
      "Jeroen Snippe",
      "Faruk O. Alpak",
      "Farah Hariri",
      "Clement Etienam",
      "Pandu Devarakota",
      "Anima Anandkumar",
      "Detlef Hohl"
    ],
    "abstract": "This study aims to develop surrogate models for accelerating decision making\nprocesses associated with carbon capture and storage (CCS) technologies.\nSelection of sub-surface $CO_2$ storage sites often necessitates expensive and\ninvolved simulations of $CO_2$ flow fields. Here, we develop a Fourier Neural\nOperator (FNO) based model for real-time, high-resolution simulation of $CO_2$\nplume migration. The model is trained on a comprehensive dataset generated from\nrealistic subsurface parameters and offers $O(10^5)$ computational acceleration\nwith minimal sacrifice in prediction accuracy. We also explore super-resolution\nexperiments to improve the computational cost of training the FNO based models.\nAdditionally, we present various strategies for improving the reliability of\npredictions from the model, which is crucial while assessing actual geological\nsites. This novel framework, based on NVIDIA's Modulus library, will allow\nrapid screening of sites for CCS. The discussed workflows and strategies can be\napplied to other energy solutions like geothermal reservoir modeling and\nhydrogen storage. Our work scales scientific machine learning models to\nrealistic 3D systems that are more consistent with real-life subsurface\naquifers/reservoirs, paving the way for next-generation digital twins for\nsubsurface CCS applications.",
    "categories": [
      "physics.comp-ph",
      "cs.AI",
      "physics.geo-ph"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11031v2",
    "published_date": "2025-03-14 02:58:24 UTC",
    "updated_date": "2025-03-20 15:44:45 UTC"
  },
  {
    "arxiv_id": "2503.11030v1",
    "title": "FMNet: Frequency-Assisted Mamba-Like Linear Attention Network for Camouflaged Object Detection",
    "authors": [
      "Ming Deng",
      "Sijin Sun",
      "Zihao Li",
      "Xiaochuan Hu",
      "Xing Wu"
    ],
    "abstract": "Camouflaged Object Detection (COD) is challenging due to the strong\nsimilarity between camouflaged objects and their surroundings, which\ncomplicates identification. Existing methods mainly rely on spatial local\nfeatures, failing to capture global information, while Transformers increase\ncomputational costs.To address this, the Frequency-Assisted Mamba-Like Linear\nAttention Network (FMNet) is proposed, which leverages frequency-domain\nlearning to efficiently capture global features and mitigate ambiguity between\nobjects and the background. FMNet introduces the Multi-Scale Frequency-Assisted\nMamba-Like Linear Attention (MFM) module, integrating frequency and spatial\nfeatures through a multi-scale structure to handle scale variations while\nreducing computational complexity. Additionally, the Pyramidal Frequency\nAttention Extraction (PFAE) module and the Frequency Reverse Decoder (FRD)\nenhance semantics and reconstruct features. Experimental results demonstrate\nthat FMNet outperforms existing methods on multiple COD datasets, showcasing\nits advantages in both performance and efficiency. Code available at\nhttps://anonymous.4open.science/r/FMNet-3CE5.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11030v1",
    "published_date": "2025-03-14 02:55:19 UTC",
    "updated_date": "2025-03-14 02:55:19 UTC"
  },
  {
    "arxiv_id": "2503.14517v1",
    "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control",
    "authors": [
      "Hejia Chen",
      "Haoxian Zhang",
      "Shoulong Zhang",
      "Xiaoqiang Liu",
      "Sisi Zhuang",
      "Yuan Zhang",
      "Pengfei Wan",
      "Di Zhang",
      "Shuai Li"
    ],
    "abstract": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR'25",
    "pdf_url": "http://arxiv.org/pdf/2503.14517v1",
    "published_date": "2025-03-14 02:52:41 UTC",
    "updated_date": "2025-03-14 02:52:41 UTC"
  },
  {
    "arxiv_id": "2503.11007v1",
    "title": "From Abstraction to Reality: DARPA's Vision for Robust Sim-to-Real Autonomy",
    "authors": [
      "Erfaun Noorani",
      "Zachary Serlin",
      "Ben Price",
      "Alvaro Velasquez"
    ],
    "abstract": "The DARPA Transfer from Imprecise and Abstract Models to Autonomous\nTechnologies (TIAMAT) program aims to address rapid and robust transfer of\nautonomy technologies across dynamic and complex environments, goals, and\nplatforms. Existing methods for simulation-to-reality (sim-to-real) transfer\noften rely on high-fidelity simulations and struggle with broad adaptation,\nparticularly in time-sensitive scenarios. Although many approaches have shown\nincredible performance at specific tasks, most techniques fall short when posed\nwith unforeseen, complex, and dynamic real-world scenarios due to the inherent\nlimitations of simulation. In contrast to current research that aims to bridge\nthe gap between simulation environments and the real world through increasingly\nsophisticated simulations and a combination of methods typically assuming a\nsmall sim-to-real gap -- such as domain randomization, domain adaptation,\nimitation learning, meta-learning, policy distillation, and dynamic\noptimization -- TIAMAT takes a different approach by instead emphasizing\ntransfer and adaptation of the autonomy stack directly to real-world\nenvironments by utilizing a breadth of low(er)-fidelity simulations to create\nbroadly effective sim-to-real transfers. By abstractly learning from multiple\nsimulation environments in reference to their shared semantics, TIAMAT's\napproaches aim to achieve abstract-to-real transfer for effective and rapid\nreal-world adaptation. Furthermore, this program endeavors to improve the\noverall autonomy pipeline by addressing the inherent challenges in translating\nsimulated behaviors into effective real-world performance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11007v1",
    "published_date": "2025-03-14 02:06:10 UTC",
    "updated_date": "2025-03-14 02:06:10 UTC"
  },
  {
    "arxiv_id": "2503.11006v1",
    "title": "Observation-Graph Interaction and Key-Detail Guidance for Vision and Language Navigation",
    "authors": [
      "Yifan Xie",
      "Binkai Ou",
      "Fei Ma",
      "Yaohua Liu"
    ],
    "abstract": "Vision and Language Navigation (VLN) requires an agent to navigate through\nenvironments following natural language instructions. However, existing methods\noften struggle with effectively integrating visual observations and instruction\ndetails during navigation, leading to suboptimal path planning and limited\nsuccess rates. In this paper, we propose OIKG (Observation-graph Interaction\nand Key-detail Guidance), a novel framework that addresses these limitations\nthrough two key components: (1) an observation-graph interaction module that\ndecouples angular and visual information while strengthening edge\nrepresentations in the navigation space, and (2) a key-detail guidance module\nthat dynamically extracts and utilizes fine-grained location and object\ninformation from instructions. By enabling more precise cross-modal alignment\nand dynamic instruction interpretation, our approach significantly improves the\nagent's ability to follow complex navigation instructions. Extensive\nexperiments on the R2R and RxR datasets demonstrate that OIKG achieves\nstate-of-the-art performance across multiple evaluation metrics, validating the\neffectiveness of our method in enhancing navigation precision through better\nobservation-instruction alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.11006v1",
    "published_date": "2025-03-14 02:05:16 UTC",
    "updated_date": "2025-03-14 02:05:16 UTC"
  },
  {
    "arxiv_id": "2503.11723v1",
    "title": "Physics-based simulation ontology: an ontology to support modelling and reuse of data for physics-based simulation",
    "authors": [
      "Hyunmin Cheong",
      "Adrian Butscher"
    ],
    "abstract": "The current work presents an ontology developed for physics-based simulation\nin engineering design, called Physics-based Simulation Ontology (PSO). The\npurpose of the ontology is to assist in modelling the physical phenomenon of\ninterest in a veridical manner, while capturing the necessary and reusable\ninformation for physics-based simulation solvers. The development involved\nextending an existing upper ontology, Basic Formal Ontology (BFO), to define\nlower-level terms of PSO. PSO has two parts: PSO-Physics, which consists of\nterms and relations used to model physical phenomena based on the perspective\nof classical mechanics involving partial differential equations, and PSO-Sim,\nwhich consists of terms used to represent the information artefacts that are\nabout the physical phenomena modelled with PSO-Physics. The former terms are\nused to model the physical phenomenon of interest independent of\nsolver-specific interpretations, which can be reused across different solvers,\nwhile the latter terms are used to instantiate solver-specific input data. A\ncase study involving two simulation solvers was conducted to demonstrate this\ncapability of PSO. Discussion around the benefits and limitations of using BFO\nfor the current work is also provided, which should be valuable for any future\nwork that extends an existing upper ontology to develop ontologies for\nengineering applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11723v1",
    "published_date": "2025-03-14 01:51:42 UTC",
    "updated_date": "2025-03-14 01:51:42 UTC"
  },
  {
    "arxiv_id": "2503.10997v1",
    "title": "RONA: Pragmatically Diverse Image Captioning with Coherence Relations",
    "authors": [
      "Aashish Anantha Ramakrishnan",
      "Aadarsh Anantha Ramakrishnan",
      "Dongwon Lee"
    ],
    "abstract": "Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally\ngenerate diverse image captions by employing syntactic and semantic variations\nto describe image components. However, human-written captions prioritize\nconveying a central message alongside visual descriptions using pragmatic cues.\nTo enhance pragmatic diversity, it is essential to explore alternative ways of\ncommunicating these messages in conjunction with visual content. To address\nthis challenge, we propose RONA, a novel prompting strategy for Multi-modal\nLarge Language Models (MLLM) that leverages Coherence Relations as an axis for\nvariation. We demonstrate that RONA generates captions with better overall\ndiversity and ground-truth alignment, compared to MLLM baselines across\nmultiple domains. Our code is available at: https://github.com/aashish2000/RONA",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "68T50",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the NAACL Fourth Workshop on Intelligent and Interactive\n  Writing Assistants (In2Writing), Albuquerque, New Mexico, May 2025,\n  https://in2writing.glitch.me",
    "pdf_url": "http://arxiv.org/pdf/2503.10997v1",
    "published_date": "2025-03-14 01:45:38 UTC",
    "updated_date": "2025-03-14 01:45:38 UTC"
  },
  {
    "arxiv_id": "2503.22693v1",
    "title": "Bridging Language Models and Financial Analysis",
    "authors": [
      "Alejandro Lopez-Lira",
      "Jihoon Kwon",
      "Sangwoon Yoon",
      "Jy-yong Sohn",
      "Chanyeol Choi"
    ],
    "abstract": "The rapid advancements in Large Language Models (LLMs) have unlocked\ntransformative possibilities in natural language processing, particularly\nwithin the financial sector. Financial data is often embedded in intricate\nrelationships across textual content, numerical tables, and visual charts,\nposing challenges that traditional methods struggle to address effectively.\nHowever, the emergence of LLMs offers new pathways for processing and analyzing\nthis multifaceted data with increased efficiency and insight. Despite the fast\npace of innovation in LLM research, there remains a significant gap in their\npractical adoption within the finance industry, where cautious integration and\nlong-term validation are prioritized. This disparity has led to a slower\nimplementation of emerging LLM techniques, despite their immense potential in\nfinancial applications. As a result, many of the latest advancements in LLM\ntechnology remain underexplored or not fully utilized in this domain. This\nsurvey seeks to bridge this gap by providing a comprehensive overview of recent\ndevelopments in LLM research and examining their applicability to the financial\nsector. Building on previous survey literature, we highlight several novel LLM\nmethodologies, exploring their distinctive capabilities and their potential\nrelevance to financial data analysis. By synthesizing insights from a broad\nrange of studies, this paper aims to serve as a valuable resource for\nresearchers and practitioners, offering direction on promising research avenues\nand outlining future opportunities for advancing LLM applications in finance.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-fin.ST",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.22693v1",
    "published_date": "2025-03-14 01:35:20 UTC",
    "updated_date": "2025-03-14 01:35:20 UTC"
  },
  {
    "arxiv_id": "2503.10986v1",
    "title": "Image-Goal Navigation Using Refined Feature Guidance and Scene Graph Enhancement",
    "authors": [
      "Zhicheng Feng",
      "Xieyuanli Chen",
      "Chenghao Shi",
      "Lun Luo",
      "Zhichao Chen",
      "Yun-Hui Liu",
      "Huimin Lu"
    ],
    "abstract": "In this paper, we introduce a novel image-goal navigation approach, named\nRFSG. Our focus lies in leveraging the fine-grained connections between goals,\nobservations, and the environment within limited image data, all the while\nkeeping the navigation architecture simple and lightweight. To this end, we\npropose the spatial-channel attention mechanism, enabling the network to learn\nthe importance of multi-dimensional features to fuse the goal and observation\nfeatures. In addition, a selfdistillation mechanism is incorporated to further\nenhance the feature representation capabilities. Given that the navigation task\nneeds surrounding environmental information for more efficient navigation, we\npropose an image scene graph to establish feature associations at both the\nimage and object levels, effectively encoding the surrounding scene\ninformation. Crossscene performance validation was conducted on the Gibson and\nHM3D datasets, and the proposed method achieved stateof-the-art results among\nmainstream methods, with a speed of up to 53.5 frames per second on an RTX3080.\nThis contributes to the realization of end-to-end image-goal navigation in\nrealworld scenarios. The implementation and model of our method have been\nreleased at: https://github.com/nubot-nudt/RFSG.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10986v1",
    "published_date": "2025-03-14 01:15:24 UTC",
    "updated_date": "2025-03-14 01:15:24 UTC"
  },
  {
    "arxiv_id": "2503.10984v2",
    "title": "The Problem of the Priors, or Posteriors?",
    "authors": [
      "Hanti Lin"
    ],
    "abstract": "The problem of the priors is well known: it concerns the challenge of\nidentifying norms that govern one's prior credences. I argue that a key to\naddressing this problem lies in considering what I call the problem of the\nposteriors -- the challenge of identifying norms that directly govern one's\nposterior credences, which backward induce some norms on the priors via the\ndiachronic requirement of conditionalization. This forward-looking approach can\nbe summarized as: Think ahead, work backward. Although this idea can be traced\nto Freedman (1963), Carnap (1963), and Shimony (1970), I believe that it has\nnot received enough attention. In this paper, I initiate a systematic defense\nof forward-looking Bayesianism, addressing potential objections from more\ntraditional views (both subjectivist and objectivist). I also develop a\nspecific approach to forward-looking Bayesianism -- one that values the\nconvergence of posterior credences to the truth, and treats it as a fundamental\nrather than derived norm. This approach, called {\\em convergentist\nBayesianism}, is argued to be crucial for a Bayesian foundation of Ockham's\nrazor in statistics and machine learning.",
    "categories": [
      "stat.OT",
      "cs.AI",
      "math.PR"
    ],
    "primary_category": "stat.OT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10984v2",
    "published_date": "2025-03-14 01:06:34 UTC",
    "updated_date": "2025-05-14 03:37:53 UTC"
  },
  {
    "arxiv_id": "2503.10970v1",
    "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools",
    "authors": [
      "Shanghua Gao",
      "Richard Zhu",
      "Zhenglun Kong",
      "Ayush Noori",
      "Xiaorui Su",
      "Curtis Ginder",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ],
    "abstract": "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Project page: https://zitniklab.hms.harvard.edu/TxAgent TxAgent code:\n  https://github.com/mims-harvard/TxAgent ToolUniverse code:\n  https://github.com/mims-harvard/ToolUniverse",
    "pdf_url": "http://arxiv.org/pdf/2503.10970v1",
    "published_date": "2025-03-14 00:28:15 UTC",
    "updated_date": "2025-03-14 00:28:15 UTC"
  },
  {
    "arxiv_id": "2503.10968v1",
    "title": "Combinatorial Optimization for All: Using LLMs to Aid Non-Experts in Improving Optimization Algorithms",
    "authors": [
      "Camilo Chacón Sartori",
      "Christian Blum"
    ],
    "abstract": "Large Language Models (LLMs) have shown notable potential in code generation\nfor optimization algorithms, unlocking exciting new opportunities. This paper\nexamines how LLMs, rather than creating algorithms from scratch, can improve\nexisting ones without the need for specialized expertise. To explore this\npotential, we selected 10 baseline optimization algorithms from various domains\n(metaheuristics, reinforcement learning, deterministic, and exact methods) to\nsolve the classic Travelling Salesman Problem. The results show that our simple\nmethodology often results in LLM-generated algorithm variants that improve over\nthe baseline algorithms in terms of solution quality, reduction in\ncomputational time, and simplification of code complexity, all without\nrequiring specialized optimization knowledge or advanced algorithmic\nimplementation skills.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10968v1",
    "published_date": "2025-03-14 00:26:00 UTC",
    "updated_date": "2025-03-14 00:26:00 UTC"
  },
  {
    "arxiv_id": "2503.10965v2",
    "title": "Auditing language models for hidden objectives",
    "authors": [
      "Samuel Marks",
      "Johannes Treutlein",
      "Trenton Bricken",
      "Jack Lindsey",
      "Jonathan Marcus",
      "Siddharth Mishra-Sharma",
      "Daniel Ziegler",
      "Emmanuel Ameisen",
      "Joshua Batson",
      "Tim Belonax",
      "Samuel R. Bowman",
      "Shan Carter",
      "Brian Chen",
      "Hoagy Cunningham",
      "Carson Denison",
      "Florian Dietz",
      "Satvik Golechha",
      "Akbir Khan",
      "Jan Kirchner",
      "Jan Leike",
      "Austin Meek",
      "Kei Nishimura-Gasparian",
      "Euan Ong",
      "Christopher Olah",
      "Adam Pearce",
      "Fabien Roger",
      "Jeanne Salle",
      "Andy Shih",
      "Meg Tong",
      "Drake Thomas",
      "Kelley Rivoire",
      "Adam Jermyn",
      "Monte MacDiarmid",
      "Tom Henighan",
      "Evan Hubinger"
    ],
    "abstract": "We study the feasibility of conducting alignment audits: investigations into\nwhether models have undesired objectives. As a testbed, we train a language\nmodel with a hidden objective. Our training pipeline first teaches the model\nabout exploitable errors in RLHF reward models (RMs), then trains the model to\nexploit some of these errors. We verify via out-of-distribution evaluations\nthat the model generalizes to exhibit whatever behaviors it believes RMs rate\nhighly, including ones not reinforced during training. We leverage this model\nto study alignment audits in two ways. First, we conduct a blind auditing game\nwhere four teams, unaware of the model's hidden objective or training,\ninvestigate it for concerning behaviors and their causes. Three teams\nsuccessfully uncovered the model's hidden objective using techniques including\ninterpretability with sparse autoencoders (SAEs), behavioral attacks, and\ntraining data analysis. Second, we conduct an unblinded follow-up study of\neight techniques for auditing the model, analyzing their strengths and\nlimitations. Overall, our work provides a concrete example of using alignment\naudits to discover a model's hidden objective and proposes a methodology for\npracticing and validating progress in alignment auditing.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10965v2",
    "published_date": "2025-03-14 00:21:15 UTC",
    "updated_date": "2025-03-28 01:48:40 UTC"
  }
]
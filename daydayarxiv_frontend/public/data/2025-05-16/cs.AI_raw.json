[
  {
    "arxiv_id": "2505.11760v1",
    "title": "Topology-Aware Knowledge Propagation in Decentralized Learning",
    "authors": [
      "Mansi Sakarvadia",
      "Nathaniel Hudson",
      "Tian Li",
      "Ian Foster",
      "Kyle Chard"
    ],
    "abstract": "Decentralized learning enables collaborative training of models across naturally distributed data without centralized coordination or maintenance of a global model. Instead, devices are organized in arbitrary communication topologies, in which they can only communicate with neighboring devices. Each device maintains its own local model by training on its local data and integrating new knowledge via model aggregation with neighbors. Therefore, knowledge is propagated across the topology via successive aggregation rounds. We study, in particular, the propagation of out-of-distribution (OOD) knowledge. We find that popular decentralized learning algorithms struggle to propagate OOD knowledge effectively to all devices. Further, we find that both the location of OOD data within a topology, and the topology itself, significantly impact OOD knowledge propagation. We then propose topology-aware aggregation strategies to accelerate (OOD) knowledge propagation across devices. These strategies improve OOD data accuracy, compared to topology-unaware baselines, by 123% on average across models in a topology.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11760v1",
    "published_date": "2025-05-16 23:53:33 UTC",
    "updated_date": "2025-05-16 23:53:33 UTC"
  },
  {
    "arxiv_id": "2505.11758v1",
    "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning",
    "authors": [
      "Sriram Mandalika"
    ],
    "abstract": "Few-shot adaptation remains a core challenge for vision-language models (VLMs), especially under limited supervision and noisy support samples. We propose PromptFuseNL, a unified framework that enhances few-shot generalization by combining predictive prompt tuning with dual-branch positive and negative learning. The method refines class prototypes through task-conditioned residuals, multi-stage cross-modal coordination, and semantic hard negative mining. To address label noise, we introduce an unsupervised instance reweighting strategy that downweights unreliable support examples without requiring additional labels or structural changes. PromptFuseNL fuses visual and textual cues through lightweight modules for efficient and discriminative prediction. Evaluated across 15 benchmarks, it consistently surpasses existing prompt- and adapter-based methods in all shot settings while remaining highly efficient, achieving up to 300x faster training and 1000x lower FLOPs compared to full prompt tuning, achieving a new state-of-the-art for robust and scalable few-shot vision-language adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11758v1",
    "published_date": "2025-05-16 23:39:34 UTC",
    "updated_date": "2025-05-16 23:39:34 UTC"
  },
  {
    "arxiv_id": "2505.11756v2",
    "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders",
    "authors": [
      "David Chanin",
      "Tomáš Dulka",
      "Adrià Garriga-Alonso"
    ],
    "abstract": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic activations into interpretable linear directions, as long as the activations are composed of sparse linear combinations of underlying features. However, we find that if an SAE is more narrow than the number of underlying \"true features\" on which it is trained, and there is correlation between features, the SAE will merge components of correlated features together, thus destroying monosemanticity. In LLM SAEs, these two conditions are almost certainly true. This phenomenon, which we call feature hedging, is caused by SAE reconstruction loss, and is more severe the narrower the SAE. In this work, we introduce the problem of feature hedging and study it both theoretically in toy models and empirically in SAEs trained on LLMs. We suspect that feature hedging may be one of the core reasons that SAEs consistently underperform supervised baselines. Finally, we use our understanding of feature hedging to propose an improved variant of matryoshka SAEs. Importantly, our work shows that SAE width is not a neutral hyperparameter: narrower SAEs suffer more from hedging than wider SAEs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11756v2",
    "published_date": "2025-05-16 23:30:17 UTC",
    "updated_date": "2025-09-26 11:18:40 UTC"
  },
  {
    "arxiv_id": "2505.11755v2",
    "title": "Reachability Barrier Networks: Learning Hamilton-Jacobi Solutions for Smooth and Flexible Control Barrier Functions",
    "authors": [
      "Matthew Kim",
      "William Sharpless",
      "Hyun Joe Jeong",
      "Sander Tonkens",
      "Somil Bansal",
      "Sylvia Herbert"
    ],
    "abstract": "Recent developments in autonomous driving and robotics underscore the necessity of safety-critical controllers. Control barrier functions (CBFs) are a popular method for appending safety guarantees to a general control framework, but they are notoriously difficult to generate beyond low dimensions. Existing methods often yield non-differentiable or inaccurate approximations that lack integrity, and thus fail to ensure safety. In this work, we use physics-informed neural networks (PINNs) to generate smooth approximations of CBFs by computing Hamilton-Jacobi (HJ) optimal control solutions. These reachability barrier networks (RBNs) avoid traditional dimensionality constraints and support the tuning of their conservativeness post-training through a parameterized discount term. To ensure robustness of the discounted solutions, we leverage conformal prediction methods to derive probabilistic safety guarantees for RBNs. We demonstrate that RBNs are highly accurate in low dimensions, and safer than the standard neural CBF approach in high dimensions. Namely, we showcase the RBNs in a 9D multi-vehicle collision avoidance problem where it empirically proves to be 5.5x safer and 1.9x less conservative than the neural CBFs, offering a promising method to synthesize CBFs for general nonlinear autonomous systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11755v2",
    "published_date": "2025-05-16 23:30:13 UTC",
    "updated_date": "2025-05-20 02:30:21 UTC"
  },
  {
    "arxiv_id": "2505.11750v3",
    "title": "Improving Medium Range Severe Weather Prediction through Transformer Post-processing of AI Weather Forecasts",
    "authors": [
      "Zhanxiang Hua",
      "Ryan Sobash",
      "David John Gagne",
      "Yingkai Sha",
      "Alexandra Anderson-Frey"
    ],
    "abstract": "Improving the skill of medium-range (3-8 day) severe weather prediction is crucial for mitigating societal impacts. This study introduces a novel approach leveraging decoder-only transformer networks to post-process AI-based weather forecasts, specifically from the Pangu-Weather model, for improved severe weather guidance. Unlike traditional post-processing methods that use a dense neural network to predict the probability of severe weather using discrete forecast samples, our method treats forecast lead times as sequential ``tokens'', enabling the transformer to learn complex temporal relationships within the evolving atmospheric state. We compare this approach against post-processing of the Global Forecast System (GFS) using both a traditional dense neural network and our transformer, as well as configurations that exclude convective parameters to fairly evaluate the impact of using the Pangu-Weather AI model. Results demonstrate that the transformer-based post-processing significantly enhances forecast skill compared to dense neural networks. Furthermore, AI-driven forecasts, particularly Pangu-Weather initialized from high resolution analysis, exhibit superior performance to GFS in the medium-range, even without explicit convective parameters. Our approach offers improved accuracy, and reliability, which also provides interpretability through feature attribution analysis, advancing medium-range severe weather prediction capabilities.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "revision update",
    "pdf_url": "https://arxiv.org/pdf/2505.11750v3",
    "published_date": "2025-05-16 23:22:07 UTC",
    "updated_date": "2025-09-21 06:40:41 UTC"
  },
  {
    "arxiv_id": "2506.06290v3",
    "title": "CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning",
    "authors": [
      "Mingyu Lu",
      "Ethan Weinberger",
      "Chanwoo Kim",
      "Su-In Lee"
    ],
    "abstract": "High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06290v3",
    "published_date": "2025-05-16 23:07:51 UTC",
    "updated_date": "2025-09-23 19:44:10 UTC"
  },
  {
    "arxiv_id": "2505.11746v1",
    "title": "Token Masking Improves Transformer-Based Text Classification",
    "authors": [
      "Xianglong Xu",
      "John Bowen",
      "Rojin Taheri"
    ],
    "abstract": "While transformer-based models achieve strong performance on text classification, we explore whether masking input tokens can further enhance their effectiveness. We propose token masking regularization, a simple yet theoretically motivated method that randomly replaces input tokens with a special [MASK] token at probability p. This introduces stochastic perturbations during training, leading to implicit gradient averaging that encourages the model to capture deeper inter-token dependencies. Experiments on language identification and sentiment analysis -- across diverse models (mBERT, Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard regularization techniques. We identify task-specific optimal masking rates, with p = 0.1 as a strong general default. We attribute the gains to two key effects: (1) input perturbation reduces overfitting, and (2) gradient-level smoothing acts as implicit ensembling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11746v1",
    "published_date": "2025-05-16 23:06:11 UTC",
    "updated_date": "2025-05-16 23:06:11 UTC"
  },
  {
    "arxiv_id": "2505.11745v1",
    "title": "POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence",
    "authors": [
      "Joshua Inman",
      "Tanmay Khandait",
      "Lalitha Sankar",
      "Giulia Pedrielli"
    ],
    "abstract": "In this paper we propose for the first time the hyperparameter optimization (HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive Halving literature by explicitly separating the search and evaluation phases and utilizing principled approaches to exploration and exploitation principles during both phases. Such distinction results in a highly flexible scheme for managing a hyperparameter optimization budget by focusing on search (i.e., generating competing configurations) towards the start of the HPO process while increasing the evaluation effort as the HPO comes to an end.\n  POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our algorithm shows superior performance in low-budget hyperparameter optimization regimes. Since many practitioners do not have exhaustive resources to assign to HPO, it has wide applications to real-world problems. Moreover, the empirical evidence showed how POCAII demonstrates higher robustness and lower variance in the results. This is again very important when considering realistic scenarios with extremely expensive models to train.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11745v1",
    "published_date": "2025-05-16 23:05:07 UTC",
    "updated_date": "2025-05-16 23:05:07 UTC"
  },
  {
    "arxiv_id": "2505.11743v1",
    "title": "Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing",
    "authors": [
      "Cheng Ji",
      "Huaiying Luo"
    ],
    "abstract": "With the rapid development of cloud computing systems and the increasing complexity of their infrastructure, intelligent mechanisms to detect and mitigate failures in real time are becoming increasingly important. Traditional methods of failure detection are often difficult to cope with the scale and dynamics of modern cloud environments. In this study, we propose a novel AI framework based on Massive Language Model (LLM) for intelligent fault detection and self-healing mechanisms in cloud systems. The model combines existing machine learning fault detection algorithms with LLM's natural language understanding capabilities to process and parse system logs, error reports, and real-time data streams through semantic context. The method adopts a multi-level architecture, combined with supervised learning for fault classification and unsupervised learning for anomaly detection, so that the system can predict potential failures before they occur and automatically trigger the self-healing mechanism. Experimental results show that the proposed model is significantly better than the traditional fault detection system in terms of fault detection accuracy, system downtime reduction and recovery speed.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11743v1",
    "published_date": "2025-05-16 23:02:57 UTC",
    "updated_date": "2025-05-16 23:02:57 UTC"
  },
  {
    "arxiv_id": "2505.11741v2",
    "title": "MTRE: Multi-Token Reliability Estimation for Hallucination Detection in VLMs",
    "authors": [
      "Geigh Zollicoffer",
      "Minh Vu",
      "Manish Bhattarai"
    ],
    "abstract": "Vision-language models (VLMs) now rival human performance on many multimodal tasks, yet they still hallucinate objects or generate unsafe text. Current hallucination detectors, e.g., single-token linear probing (LP) and PTrue, typically analyze only the logit of the first generated token or just its highest-scoring component, overlooking richer signals embedded within earlier token distributions. We demonstrate that analyzing the complete sequence of early logits potentially provides substantially more diagnostic information. We emphasize that hallucinations may only emerge after several tokens, as subtle inconsistencies accumulate over time. By analyzing the Kullback-Leibler (KL) divergence between logits corresponding to hallucinated and non-hallucinated tokens, we underscore the importance of incorporating later-token logits to more accurately capture the reliability dynamics of VLMs. In response, we introduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box method that aggregates logits from the first ten tokens using multi-token log-likelihood ratios and self-attention. Despite the challenges posed by large vocabulary sizes and long logit sequences, MTRE remains efficient and tractable. Across MAD-Bench, MM-SafetyBench, MathVista, and four compositional-geometry benchmarks, MTRE achieves a 9.4% gain in accuracy and a 14.8% gain in AUROC over standard detection methods, establishing a new state of the art in hallucination detection for open-source VLMs.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11741v2",
    "published_date": "2025-05-16 23:00:19 UTC",
    "updated_date": "2025-10-20 18:38:13 UTC"
  },
  {
    "arxiv_id": "2505.11740v2",
    "title": "Simple and Effective Specialized Representations for Fair Classifiers",
    "authors": [
      "Alberto Sinigaglia",
      "Davide Sartor",
      "Marina Ceccon",
      "Gian Antonio Susto"
    ],
    "abstract": "Fair classification is a critical challenge that has gained increasing importance due to international regulations and its growing use in high-stakes decision-making settings. Existing methods often rely on adversarial learning or distribution matching across sensitive groups; however, adversarial learning can be unstable, and distribution matching can be computationally intensive. To address these limitations, we propose a novel approach based on the characteristic function distance. Our method ensures that the learned representation contains minimal sensitive information while maintaining high effectiveness for downstream tasks. By utilizing characteristic functions, we achieve a more stable and efficient solution compared to traditional methods. Additionally, we introduce a simple relaxation of the objective function that guarantees fairness in common classification models with no performance degradation. Experimental results on benchmark datasets demonstrate that our approach consistently matches or achieves better fairness and predictive accuracy than existing methods. Moreover, our method maintains robustness and computational efficiency, making it a practical solution for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11740v2",
    "published_date": "2025-05-16 22:59:46 UTC",
    "updated_date": "2025-10-13 11:14:09 UTC"
  },
  {
    "arxiv_id": "2505.11739v2",
    "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training",
    "authors": [
      "Feijiang Han",
      "Xiaodong Yu",
      "Jianheng Tang",
      "Delip Rao",
      "Weihua Du",
      "Lyle Ungar"
    ],
    "abstract": "Token-level attention tuning, a class of training-free methods including Post-hoc Attention Steering (PASTA) and Attention Calibration (ACT), has emerged as a promising way to improve frozen LLMs with interpretable interventions. However, these methods depend on auxiliary heuristics to identify \"important\" task-specific tokens, which can introduce bias and limit applicability when token importance is unclear or when using optimized kernels where attention maps are inaccessible. We propose a simpler and more elegant alternative: acting only on the initial token (e.g., <BOS> in LLaMA). We show theoretically that adding lightweight biases to this token's attention logits monotonically controls the entropy of the downstream attention distribution - an effect amplified by its natural function as an attention sink. Our empirical analysis reveals that this tuning process can positively affect LLMs and better unlock their pretrained knowledge, with stronger effects in early layers and distinct scaling preferences across attention heads. Building on these insights, we introduce ZeroTuning: a training-free method that improves LLM performance by applying head-specific attention adjustments to the initial token, requiring zero parameter updates. We present two variants: a supervised mode that calibrates on validation examples, and a novel unsupervised mode that directly minimizes the model's output entropy. The method is lightweight, kernel-agnostic, and requires only four lines of modification to the standard LlamaAttention code. It achieves broad gains across 15 datasets and outperforms previous, more complex methods; for instance, with Llama-3.1-8B, it yields relative improvements of 19.9% on classification, 4.5% on question answering, and 2.1% on dialogue. ZeroTuning also works out-of-the-box with quantized inference and maintains its performance improvements with increasing context lengths.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11739v2",
    "published_date": "2025-05-16 22:52:24 UTC",
    "updated_date": "2025-09-26 03:55:57 UTC"
  },
  {
    "arxiv_id": "2505.11738v1",
    "title": "Automated Real-time Assessment of Intracranial Hemorrhage Detection AI Using an Ensembled Monitoring Model (EMM)",
    "authors": [
      "Zhongnan Fang",
      "Andrew Johnston",
      "Lina Cheuy",
      "Hye Sun Na",
      "Magdalini Paschali",
      "Camila Gonzalez",
      "Bonnie A. Armstrong",
      "Arogya Koirala",
      "Derrick Laurel",
      "Andrew Walker Campion",
      "Michael Iv",
      "Akshay S. Chaudhari",
      "David B. Larson"
    ],
    "abstract": "Artificial intelligence (AI) tools for radiology are commonly unmonitored once deployed. The lack of real-time case-by-case assessments of AI prediction confidence requires users to independently distinguish between trustworthy and unreliable AI predictions, which increases cognitive burden, reduces productivity, and potentially leads to misdiagnoses. To address these challenges, we introduce Ensembled Monitoring Model (EMM), a framework inspired by clinical consensus practices using multiple expert reviews. Designed specifically for black-box commercial AI products, EMM operates independently without requiring access to internal AI components or intermediate outputs, while still providing robust confidence measurements. Using intracranial hemorrhage detection as our test case on a large, diverse dataset of 2919 studies, we demonstrate that EMM successfully categorizes confidence in the AI-generated prediction, suggesting different actions and helping improve the overall performance of AI tools to ultimately reduce cognitive burden. Importantly, we provide key technical considerations and best practices for successfully translating EMM into clinical settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11738v1",
    "published_date": "2025-05-16 22:50:42 UTC",
    "updated_date": "2025-05-16 22:50:42 UTC"
  },
  {
    "arxiv_id": "2505.11737v3",
    "title": "TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning",
    "authors": [
      "Tunyu Zhang",
      "Haizhou Shi",
      "Yibin Wang",
      "Hengyi Wang",
      "Xiaoxiao He",
      "Zhuowei Li",
      "Haoxian Chen",
      "Ligong Han",
      "Kai Xu",
      "Huan Zhang",
      "Dimitris Metaxas",
      "Hao Wang"
    ],
    "abstract": "While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a Token-level Uncertainty estimation framework for Reasoning (TokUR) that enables LLMs to self-assess and self-improve their responses in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation during LLM decoding to generate predictive distributions for token-level uncertainty estimation, and we aggregate these uncertainty quantities to capture the semantic uncertainty of generated responses. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that TokUR exhibits a strong correlation with answer correctness and model robustness, and the uncertainty signals produced by TokUR can be leveraged to enhance the model's reasoning performance at test time. These results highlight the effectiveness of TokUR as a principled and scalable approach for improving the reliability and interpretability of LLMs in challenging reasoning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint; Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.11737v3",
    "published_date": "2025-05-16 22:47:32 UTC",
    "updated_date": "2025-09-25 21:44:35 UTC"
  },
  {
    "arxiv_id": "2505.13511v1",
    "title": "Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "abstract": "This study explores Large Language Models (LLMs) as autonomous agents for real-world tasks, including freelance software development. This work presents a new benchmark that evaluates LLMs on freelance programming and data analysis tasks derived from economic data. We construct the benchmark using synthetic tasks created from a Kaggle Freelancer dataset of job postings, with all job prices standardized to USD (median fixed-project price around $250, and an average of $306). Each task is accompanied by structured input-output test cases and an estimated price tag, enabling automated correctness checking and a monetary performance valuation. This approach is inspired by OpenAI's recent SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our framework simplifies evaluation using programmatically testable tasks and predicted price values, making it highly scalable and repeatable. On this benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen 2.5, and Mistral. We report each model's accuracy (task success rate and test-case pass rate) and the total \"freelance earnings\" it achieves (sum of prices of solved tasks). Our results show that Claude 3.5 Haiku performs best, earning approximately $1.52 million USD, followed closely by GPT-4o-mini at $1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the distribution of errors per task and observe that the strongest models solve the most tasks and rarely fail completely on any project. We discuss the implications of these results for the feasibility of AI as a freelance developer, the advantages and limitations of our automated benchmark approach, and the gap between performance on structured tasks versus the true complexity of real-world freelance jobs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13511v1",
    "published_date": "2025-05-16 22:42:04 UTC",
    "updated_date": "2025-05-16 22:42:04 UTC"
  },
  {
    "arxiv_id": "2505.11731v2",
    "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models",
    "authors": [
      "Harshil Vejendla",
      "Haizhou Shi",
      "Yibin Wang",
      "Tunyu Zhang",
      "Huan Zhang",
      "Hao Wang"
    ],
    "abstract": "Recent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation have addressed key challenges of reliability and simplicity. However, existing Bayesian methods typically require multiple sampling iterations during inference, creating significant efficiency issues that limit practical deployment. In this paper, we investigate the possibility of eliminating the need for test-time sampling for LLM uncertainty estimation. Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned confidence into a non-Bayesian student LLM by minimizing the divergence between their predictive distributions. Unlike typical calibration methods, our distillation is carried out solely on the training dataset without the need of an additional validation dataset. This simple yet effective approach achieves N-times more efficient uncertainty estimation during testing, where N is the number of samples traditionally required by Bayesian LLMs. Our extensive experiments demonstrate that uncertainty estimation capabilities on training data can successfully generalize to unseen test data through our distillation technique, consistently producing results comparable to (or even better than) state-of-the-art Bayesian LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint; work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.11731v2",
    "published_date": "2025-05-16 22:26:03 UTC",
    "updated_date": "2025-05-23 18:24:43 UTC"
  },
  {
    "arxiv_id": "2505.11730v2",
    "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
    "authors": [
      "Hao Mark Chen",
      "Guanxi Lu",
      "Yasuyuki Okoshi",
      "Zhiwen Mo",
      "Masato Motomura",
      "Hongxiang Fan"
    ],
    "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11730v2",
    "published_date": "2025-05-16 22:24:48 UTC",
    "updated_date": "2025-10-30 13:52:37 UTC"
  },
  {
    "arxiv_id": "2505.11725v2",
    "title": "CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median",
    "authors": [
      "Imon Banerjee",
      "Sayak Chakrabarty"
    ],
    "abstract": "The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.\n  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "math.ST",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "48 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11725v2",
    "published_date": "2025-05-16 22:14:49 UTC",
    "updated_date": "2025-10-24 01:16:54 UTC"
  },
  {
    "arxiv_id": "2506.01986v1",
    "title": "SpecMemo: Speculative Decoding is in Your Pocket",
    "authors": [
      "Selin Yildirim",
      "Deming Chen"
    ],
    "abstract": "Recent advancements in speculative decoding have demonstrated considerable speedup across a wide array of large language model (LLM) tasks. Speculative decoding inherently relies on sacrificing extra memory allocations to generate several candidate tokens, of which acceptance rate drives the speedup. However, deploying speculative decoding on memory-constrained devices, such as mobile GPUs, remains as a significant challenge in real-world scenarios. In this work, we present a device-aware inference engine named SpecMemo that can smartly control memory allocations at finer levels to enable multi-turn chatbots with speculative decoding on such limited memory devices. Our methodology stems from theoretically modeling memory footprint of speculative decoding to determine a lower bound on the required memory budget while retaining speedup. SpecMemo empirically acquires a careful balance between minimizing redundant memory allocations for rejected candidate tokens and maintaining competitive performance gains from speculation. Notably, with SpecMemo's memory management, we maintain 96% of overall throughput from speculative decoding on MT-Bench, with reduced generation-memory by 65% on single Nvidia Titan RTX. Given multiple constrained GPUs, we build on top of previous speculative decoding architectures to facilitate big-model inference by distributing Llama-2-70B-Chat model, on which we provide novel batched speculative decoding to increase usability of multiple small server GPUs. This novel framework demonstrates 2x speedup over distributed and batched vanilla decoding with the base model on eight AMD MI250 GPUs. Moreover, inference throughput increases remarkably 8x with batch size 10. Our work contributes to democratized LLM applications in resource-constrained environments, providing a pathway for faster and cheaper deployment of real-world LLM applications with robust performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.01986v1",
    "published_date": "2025-05-16 22:12:29 UTC",
    "updated_date": "2025-05-16 22:12:29 UTC"
  },
  {
    "arxiv_id": "2505.11719v1",
    "title": "Zero-Shot Visual Generalization in Robot Manipulation",
    "authors": [
      "Sumeet Batra",
      "Gaurav Sukhatme"
    ],
    "abstract": "Training vision-based manipulation policies that are robust across diverse visual environments remains an important and unresolved challenge in robot learning. Current approaches often sidestep the problem by relying on invariant representations such as point clouds and depth, or by brute-forcing generalization through visual domain randomization and/or large, visually diverse datasets. Disentangled representation learning - especially when combined with principles of associative memory - has recently shown promise in enabling vision-based reinforcement learning policies to be robust to visual distribution shifts. However, these techniques have largely been constrained to simpler benchmarks and toy environments. In this work, we scale disentangled representation learning and associative memory to more visually and dynamically complex manipulation tasks and demonstrate zero-shot adaptability to visual perturbations in both simulation and on real hardware. We further extend this approach to imitation learning, specifically Diffusion Policy, and empirically show significant gains in visual generalization compared to state-of-the-art imitation learning methods. Finally, we introduce a novel technique adapted from the model equivariance literature that transforms any trained neural network policy into one invariant to 2D planar rotations, making our policy not only visually robust but also resilient to certain camera perturbations. We believe that this work marks a significant step towards manipulation policies that are not only adaptable out of the box, but also robust to the complexities and dynamical nature of real-world deployment. Supplementary videos are available at https://sites.google.com/view/vis-gen-robotics/home.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11719v1",
    "published_date": "2025-05-16 22:01:46 UTC",
    "updated_date": "2025-05-16 22:01:46 UTC"
  },
  {
    "arxiv_id": "2505.11718v2",
    "title": "REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning",
    "authors": [
      "Pawin Taechoyotin",
      "Daniel Acuna"
    ],
    "abstract": "AI-based peer review systems tend to produce shallow and overpraising suggestions compared to human feedback. Here, we evaluate how well a reasoning LLM trained with multi-objective reinforcement learning (REMOR) can overcome these limitations. We start by designing a multi-aspect reward function that aligns with human evaluation of reviews. The aspects are related to the review itself (e.g., criticisms, novelty) and the relationship between the review and the manuscript (i.e., relevance). First, we perform supervised fine-tuning of DeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality top AI conference reviews enriched with reasoning traces. We then apply Group Relative Policy Optimization (GRPO) to train two models: REMOR-H (with the human-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the human-aligned reward penalizes aspects typically associated with strong reviews, leading REMOR-U to produce qualitatively more substantive feedback. Our results show that REMOR-U and REMOR-H achieve more than twice the average rewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI review systems, and general commercial LLM baselines. We found that while the best AI and human reviews are comparable in quality, REMOR avoids the long tail of low-quality human reviews. We discuss how reasoning is key to achieving these improvements and release the Human-aligned Peer Review Reward (HPRR) function, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the REMOR models, which we believe can help spur progress in the area.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11718v2",
    "published_date": "2025-05-16 22:00:49 UTC",
    "updated_date": "2025-06-27 02:48:27 UTC"
  },
  {
    "arxiv_id": "2505.11717v4",
    "title": "WebInject: Prompt Injection Attack to Web Agents",
    "authors": [
      "Xilong Wang",
      "John Bloch",
      "Zedian Shao",
      "Yuepeng Hu",
      "Shuyan Zhou",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. In this work, we propose WebInject, a prompt injection attack that manipulates the webpage environment to induce a web agent to perform an attacker-specified action. Our attack adds a perturbation to the raw pixel values of the rendered webpage. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the attacker-specified action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple datasets shows that WebInject is highly effective and significantly outperforms baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Appeared in EMNLP 2025 main conference. To better understand prompt injection attacks, see https://people.duke.edu/~zg70/code/PromptInjection.pdf",
    "pdf_url": "https://arxiv.org/pdf/2505.11717v4",
    "published_date": "2025-05-16 22:00:26 UTC",
    "updated_date": "2025-10-17 01:52:39 UTC"
  },
  {
    "arxiv_id": "2505.11714v1",
    "title": "Bi-Level Policy Optimization with Nyström Hypergradients",
    "authors": [
      "Arjun Prakash",
      "Naicheng He",
      "Denizalp Goktas",
      "Amy Greenwald"
    ],
    "abstract": "The dependency of the actor on the critic in actor-critic (AC) reinforcement learning means that AC can be characterized as a bilevel optimization (BLO) problem, also called a Stackelberg game. This characterization motivates two modifications to vanilla AC algorithms. First, the critic's update should be nested to learn a best response to the actor's policy. Second, the actor should update according to a hypergradient that takes changes in the critic's behavior into account. Computing this hypergradient involves finding an inverse Hessian vector product, a process that can be numerically unstable. We thus propose a new algorithm, Bilevel Policy Optimization with Nyström Hypergradients (BLPO), which uses nesting to account for the nested structure of BLO, and leverages the Nyström method to compute the hypergradient. Theoretically, we prove BLPO converges to (a point that satisfies the necessary conditions for) a local strong Stackelberg equilibrium in polynomial time with high probability, assuming a linear parametrization of the critic's objective. Empirically, we demonstrate that BLPO performs on par with or better than PPO on a variety of discrete and continuous control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11714v1",
    "published_date": "2025-05-16 21:56:05 UTC",
    "updated_date": "2025-05-16 21:56:05 UTC"
  },
  {
    "arxiv_id": "2505.20302v3",
    "title": "VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification",
    "authors": [
      "Patrick Yubeaton",
      "Andre Nakkab",
      "Weihua Xiao",
      "Luca Collini",
      "Ramesh Karri",
      "Chinmay Hegde",
      "Siddharth Garg"
    ],
    "abstract": "This paper introduces VeriThoughts, a novel dataset designed for reasoning-based Verilog code generation. We establish a new benchmark framework grounded in formal verification methods to evaluate the quality and correctness of generated hardware descriptions. Additionally, we present a suite of specialized small-scale models optimized specifically for Verilog generation. Our work addresses the growing need for automated hardware design tools that can produce verifiably correct implementations from high-level specifications, potentially accelerating the hardware development process while maintaining rigorous correctness guarantees. Our code and data are available at \\href{https://github.com/wilyub/VeriThoughts}{this URL}.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20302v3",
    "published_date": "2025-05-16 21:33:14 UTC",
    "updated_date": "2025-11-18 19:04:33 UTC"
  },
  {
    "arxiv_id": "2505.11701v2",
    "title": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior",
    "authors": [
      "Shaghayegh Abedi",
      "Amin Jalali"
    ],
    "abstract": "Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting in our case study. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Large Language Models, Decision Model and Notation, Automated Feedback, Prompt Engineering",
    "pdf_url": "https://arxiv.org/pdf/2505.11701v2",
    "published_date": "2025-05-16 21:09:36 UTC",
    "updated_date": "2025-09-04 14:12:36 UTC"
  },
  {
    "arxiv_id": "2505.11698v1",
    "title": "Conditional Deep Generative Models for Belief State Planning",
    "authors": [
      "Antoine Bigeard",
      "Anthony Corso",
      "Mykel Kochenderfer"
    ],
    "abstract": "Partially observable Markov decision processes (POMDPs) are used to model a wide range of applications, including robotics, autonomous vehicles, and subsurface problems. However, accurately representing the belief is difficult for POMDPs with high-dimensional states. In this paper, we propose a novel approach that uses conditional deep generative models (cDGMs) to represent the belief. Unlike traditional belief representations, cDGMs are well-suited for high-dimensional states and large numbers of observations, and they can generate an arbitrary number of samples from the posterior belief. We train the cDGMs on data produced by random rollout trajectories and show their effectiveness in solving a mineral exploration POMDP with a large and continuous state space. The cDGMs outperform particle filter baselines in both task-agnostic measures of belief accuracy as well as in planning performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11698v1",
    "published_date": "2025-05-16 21:06:41 UTC",
    "updated_date": "2025-05-16 21:06:41 UTC"
  },
  {
    "arxiv_id": "2505.11695v2",
    "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization",
    "authors": [
      "Shihao Zhang",
      "Haoyu Zhang",
      "Ian Colbert",
      "Rayan Saab"
    ],
    "abstract": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11695v2",
    "published_date": "2025-05-16 21:04:25 UTC",
    "updated_date": "2025-06-12 00:25:14 UTC"
  },
  {
    "arxiv_id": "2505.11694v2",
    "title": "Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory",
    "authors": [
      "Sahil Rajesh Dhayalkar"
    ],
    "abstract": "We present a complete theoretical and empirical framework establishing feedforward neural networks as universal finite-state machines (N-FSMs). Our results prove that finite-depth ReLU and threshold networks can exactly simulate deterministic finite automata (DFAs) by unrolling state transitions into depth-wise neural layers, with formal characterizations of required depth, width, and state compression. We demonstrate that DFA transitions are linearly separable, binary threshold activations allow exponential compression, and Myhill-Nerode equivalence classes can be embedded into continuous latent spaces while preserving separability. We also formalize the expressivity boundary: fixed-depth feedforward networks cannot recognize non-regular languages requiring unbounded memory. Unlike prior heuristic or probing-based studies, we provide constructive proofs and design explicit DFA-unrolled neural architectures that empirically validate every claim. Our results bridge deep learning, automata theory, and neural-symbolic computation, offering a rigorous blueprint for how discrete symbolic processes can be realized in continuous neural systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2505.11694v2",
    "published_date": "2025-05-16 21:01:34 UTC",
    "updated_date": "2025-05-28 21:39:10 UTC"
  },
  {
    "arxiv_id": "2505.11692v2",
    "title": "The Geometry of ReLU Networks through the ReLU Transition Graph",
    "authors": [
      "Sahil Rajesh Dhayalkar"
    ],
    "abstract": "We develop a novel theoretical framework for analyzing ReLU neural networks through the lens of a combinatorial object we term the ReLU Transition Graph (RTG). In this graph, each node corresponds to a linear region induced by the network's activation patterns, and edges connect regions that differ by a single neuron flip. Building on this structure, we derive a suite of new theoretical results connecting RTG geometry to expressivity, generalization, and robustness. Our contributions include tight combinatorial bounds on RTG size and diameter, a proof of RTG connectivity, and graph-theoretic interpretations of VC-dimension. We also relate entropy and average degree of the RTG to generalization error. Each theoretical result is rigorously validated via carefully controlled experiments across varied network depths, widths, and data regimes. This work provides the first unified treatment of ReLU network structure via graph theory and opens new avenues for compression, regularization, and complexity control rooted in RTG analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11692v2",
    "published_date": "2025-05-16 21:00:56 UTC",
    "updated_date": "2025-05-28 21:46:04 UTC"
  },
  {
    "arxiv_id": "2505.15836v1",
    "title": "Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning",
    "authors": [
      "Aarav Lala",
      "Kalyan Cherukuri"
    ],
    "abstract": "As artificial intelligence continues to drive innovation in complex, decentralized environments, the need for scalable, adaptive, and privacy-preserving decision-making systems has become critical. This paper introduces a novel framework combining quantum-inspired neural networks with evolutionary algorithms to optimize real-time decision-making in multi-agent systems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN) leverages quantum computing principles -- such as quantum superposition and entanglement -- to enhance learning speed and decision accuracy, while integrating evolutionary optimization to continually refine agent behaviors in dynamic, uncertain environments. By utilizing federated learning, QE-NN ensures privacy preservation, enabling decentralized agents to collaborate without sharing sensitive data. The framework is designed to allow agents to adapt in real-time to their environments, optimizing decision-making processes for applications in areas such as autonomous systems, smart cities, and healthcare. This research represents a breakthrough in merging quantum computing, evolutionary optimization, and privacy-preserving techniques to solve complex problems in multi-agent decision-making systems, pushing the boundaries of AI in real-world, privacy-sensitive applications.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15836v1",
    "published_date": "2025-05-16 20:51:37 UTC",
    "updated_date": "2025-05-16 20:51:37 UTC"
  },
  {
    "arxiv_id": "2505.11687v1",
    "title": "Second SIGIR Workshop on Simulations for Information Access (Sim4IA 2025)",
    "authors": [
      "Philipp Schaer",
      "Christin Katharina Kreutz",
      "Krisztian Balog",
      "Timo Breuer",
      "Andreas Konstantin Kruff"
    ],
    "abstract": "Simulations in information access (IA) have recently gained interest, as shown by various tutorials and workshops around that topic. Simulations can be key contributors to central IA research and evaluation questions, especially around interactive settings when real users are unavailable, or their participation is impossible due to ethical reasons. In addition, simulations in IA can help contribute to a better understanding of users, reduce complexity of evaluation experiments, and improve reproducibility. Building on recent developments in methods and toolkits, the second iteration of our Sim4IA workshop aims to again bring together researchers and practitioners to form an interactive and engaging forum for discussions on the future perspectives of the field. An additional aim is to plan an upcoming TREC/CLEF campaign.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '25), July 13--18, 2025, Padua, Italy",
    "pdf_url": "https://arxiv.org/pdf/2505.11687v1",
    "published_date": "2025-05-16 20:48:59 UTC",
    "updated_date": "2025-05-16 20:48:59 UTC"
  },
  {
    "arxiv_id": "2505.11669v2",
    "title": "OT Score: An OT based Confidence Score for Source Free Unsupervised Domain Adaptation",
    "authors": [
      "Yiming Zhang",
      "Sitong Liu",
      "Alex Cloninger"
    ],
    "abstract": "We address the computational and theoretical limitations of current distributional alignment methods for source-free unsupervised domain adaptation (SFUDA). In particular, we focus on estimating classification performance and confidence in the absence of target labels. Current theoretical frameworks for these methods often yield computationally intractable quantities and fail to adequately reflect the properties of the alignment algorithms employed. To overcome these challenges, we introduce the Optimal Transport (OT) score, a confidence metric derived from a novel theoretical analysis that exploits the flexibility of decision boundaries induced by Semi-Discrete Optimal Transport alignment. The proposed OT score is intuitively interpretable and theoretically rigorous. It provides principled uncertainty estimates for any given set of target pseudo-labels. Experimental results demonstrate that OT score outperforms existing confidence scores. Moreover, it improves SFUDA performance through training-time reweighting and provides a reliable, label-free proxy for model performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11669v2",
    "published_date": "2025-05-16 20:09:05 UTC",
    "updated_date": "2025-10-03 01:01:59 UTC"
  },
  {
    "arxiv_id": "2505.11665v1",
    "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks",
    "authors": [
      "Shubham Vatsal",
      "Harsh Dubey",
      "Aditi Singh"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11665v1",
    "published_date": "2025-05-16 19:59:17 UTC",
    "updated_date": "2025-05-16 19:59:17 UTC"
  },
  {
    "arxiv_id": "2505.11661v1",
    "title": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning",
    "authors": [
      "Zihan Ye",
      "Oleg Arenz",
      "Kristian Kersting"
    ],
    "abstract": "When tackling complex problems, humans naturally break them down into smaller, manageable subtasks and adjust their initial plans based on observations. For instance, if you want to make coffee at a friend's place, you might initially plan to grab coffee beans, go to the coffee machine, and pour them into the machine. Upon noticing that the machine is full, you would skip the initial steps and proceed directly to brewing. In stark contrast, state of the art reinforcement learners, such as Proximal Policy Optimization (PPO), lack such prior knowledge and therefore require significantly more training steps to exhibit comparable adaptive behavior. Thus, a central research question arises: \\textit{How can we enable reinforcement learning (RL) agents to have similar ``human priors'', allowing the agent to learn with fewer training interactions?} To address this challenge, we propose differentiable symbolic planner (Dylan), a novel framework that integrates symbolic planning into Reinforcement Learning. Dylan serves as a reward model that dynamically shapes rewards by leveraging human priors, guiding agents through intermediate subtasks, thus enabling more efficient exploration. Beyond reward shaping, Dylan can work as a high level planner that composes primitive policies to generate new behaviors while avoiding common symbolic planner pitfalls such as infinite execution loops. Our experimental evaluations demonstrate that Dylan significantly improves RL agents' performance and facilitates generalization to unseen tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "conference paper, 9 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11661v1",
    "published_date": "2025-05-16 19:52:36 UTC",
    "updated_date": "2025-05-16 19:52:36 UTC"
  },
  {
    "arxiv_id": "2505.11659v1",
    "title": "Programmable metasurfaces for future photonic artificial intelligence",
    "authors": [
      "Loubnan Abou-Hamdan",
      "Emil Marinov",
      "Peter Wiecha",
      "Philipp del Hougne",
      "Tianyu Wang",
      "Patrice Genevet"
    ],
    "abstract": "Photonic neural networks (PNNs), which share the inherent benefits of photonic systems, such as high parallelism and low power consumption, could challenge traditional digital neural networks in terms of energy efficiency, latency, and throughput. However, producing scalable photonic artificial intelligence (AI) solutions remains challenging. To make photonic AI models viable, the scalability problem needs to be solved. Large optical AI models implemented on PNNs are only commercially feasible if the advantages of optical computation outweigh the cost of their input-output overhead. In this Perspective, we discuss how field-programmable metasurface technology may become a key hardware ingredient in achieving scalable photonic AI accelerators and how it can compete with current digital electronic technologies. Programmability or reconfigurability is a pivotal component for PNN hardware, enabling in situ training and accommodating non-stationary use cases that require fine-tuning or transfer learning. Co-integration with electronics, 3D stacking, and large-scale manufacturing of metasurfaces would significantly improve PNN scalability and functionalities. Programmable metasurfaces could address some of the current challenges that PNNs face and enable next-generation photonic AI technology.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics",
    "comment": "Nat. Rev. Phys. (2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.11659v1",
    "published_date": "2025-05-16 19:50:01 UTC",
    "updated_date": "2025-05-16 19:50:01 UTC"
  },
  {
    "arxiv_id": "2505.11646v1",
    "title": "FLOW-BENCH: Towards Conversational Generation of Enterprise Workflows",
    "authors": [
      "Evelyn Duesterwald",
      "Siyu Huo",
      "Vatche Isahagian",
      "K. R. Jayaram",
      "Ritesh Kumar",
      "Vinod Muthusamy",
      "Punleuk Oum",
      "Debashish Saha",
      "Gegi Thomas",
      "Praveen Venkateswaran"
    ],
    "abstract": "Business process automation (BPA) that leverages Large Language Models (LLMs) to convert natural language (NL) instructions into structured business process artifacts is becoming a hot research topic. This paper makes two technical contributions -- (i) FLOW-BENCH, a high quality dataset of paired natural language instructions and structured business process definitions to evaluate NL-based BPA tools, and support bourgeoning research in this area, and (ii) FLOW-GEN, our approach to utilize LLMs to translate natural language into an intermediate representation with Python syntax that facilitates final conversion into widely adopted business process definition languages, such as BPMN and DMN. We bootstrap FLOW-BENCH by demonstrating how it can be used to evaluate the components of FLOW-GEN across eight LLMs of varying sizes. We hope that FLOW-GEN and FLOW-BENCH catalyze further research in BPA making it more accessible to novice and expert users.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11646v1",
    "published_date": "2025-05-16 19:14:19 UTC",
    "updated_date": "2025-05-16 19:14:19 UTC"
  },
  {
    "arxiv_id": "2505.11642v2",
    "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning",
    "authors": [
      "Falong Fan",
      "Xi Li"
    ],
    "abstract": "Multi-agent systems leverage advanced AI models as autonomous agents that interact, cooperate, or compete to complete complex tasks across applications such as robotics and traffic management. Despite their growing importance, safety in multi-agent systems remains largely underexplored, with most research focusing on single AI models rather than interacting agents. This work investigates backdoor vulnerabilities in multi-agent systems and proposes a defense mechanism based on agent interactions. By leveraging reasoning abilities, each agent evaluates responses from others to detect illogical reasoning processes, which indicate poisoned agents. Experiments on LLM-based multi-agent systems, including ChatGPT series and Llama 3, demonstrate the effectiveness of the proposed method, achieving high accuracy in identifying poisoned agents while minimizing false positives on clean agents. We believe this work provides insights into multi-agent system safety and contributes to the development of robust, trustworthy AI interactions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "This paper has been accepted to IEEE IRI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11642v2",
    "published_date": "2025-05-16 19:08:29 UTC",
    "updated_date": "2025-05-27 15:31:58 UTC"
  },
  {
    "arxiv_id": "2505.11633v2",
    "title": "Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs",
    "authors": [
      "Vyacheslav Tykhonov",
      "Han Yang",
      "Philipp Mayr",
      "Jetze Touber",
      "Andrea Scharnhorst"
    ],
    "abstract": "This demo paper reports on a new workflow \\textit{GhostWriter} that combines the use of Large Language Models and Knowledge Graphs (semantic artifacts) to support navigation through collections. Situated in the research area of Retrieval Augmented Generation, this specific workflow represents the creation of local and adaptable chatbots. Based on the tool-suite \\textit{EverythingData} at the backend, \\textit{GhostWriter} provides an interface that enables querying and ``chatting'' with a collection. Applied iteratively, the workflow supports the information needs of researchers when interacting with a collection of papers, whether it be to gain an overview, to learn more about a specific concept and its context, and helps the researcher ultimately to refine their research question in a controlled way. We demonstrate the workflow for a collection of articles from the \\textit{method data analysis} journal published by GESIS -- Leibniz-Institute for the Social Sciences. We also point to further application areas.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "10 pages, 3 figures, Accepted at Joint Workshop of the 5th AI + Informetrics (AII) and the 6th Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE)",
    "pdf_url": "https://arxiv.org/pdf/2505.11633v2",
    "published_date": "2025-05-16 18:51:51 UTC",
    "updated_date": "2025-06-17 10:48:44 UTC"
  },
  {
    "arxiv_id": "2505.11626v2",
    "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering",
    "authors": [
      "Udita Patel",
      "Rutu Mulkar",
      "Jay Roberts",
      "Cibi Chakravarthy Senthilkumar",
      "Sujay Gandhi",
      "Xiaofei Zheng",
      "Naumaan Nayyar",
      "Parul Kalra",
      "Rafael Castrillo"
    ],
    "abstract": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Added author",
    "pdf_url": "https://arxiv.org/pdf/2505.11626v2",
    "published_date": "2025-05-16 18:42:04 UTC",
    "updated_date": "2025-06-03 19:21:16 UTC"
  },
  {
    "arxiv_id": "2505.11625v1",
    "title": "Nearest Neighbor Multivariate Time Series Forecasting",
    "authors": [
      "Huiliang Zhang",
      "Ping Nie",
      "Lijun Sun",
      "Benoit Boulet"
    ],
    "abstract": "Multivariate time series (MTS) forecasting has a wide range of applications in both industry and academia. Recently, spatial-temporal graph neural networks (STGNNs) have gained popularity as MTS forecasting methods. However, current STGNNs can only use the finite length of MTS input data due to the computational complexity. Moreover, they lack the ability to identify similar patterns throughout the entire dataset and struggle with data that exhibit sparsely and discontinuously distributed correlations among variables over an extensive historical period, resulting in only marginal improvements. In this article, we introduce a simple yet effective k-nearest neighbor MTS forecasting ( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval mechanism over a large datastore of cached series, using representations from the MTS model for similarity search. This approach requires no additional training and scales to give the MTS model direct access to the whole dataset at test time, resulting in a highly expressive model that consistently improves performance, and has the ability to extract sparse distributed but similar patterns spanning over multivariables from the entire dataset. Furthermore, a hybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can capture both long-term temporal and short-term spatial-temporal dependencies and is shown to provide accurate representation for kNN-MTSfor better forecasting. Experimental results on several real-world datasets show a significant improvement in the forecasting performance of kNN-MTS. The quantitative analysis also illustrates the interpretability and efficiency of kNN-MTS, showing better application prospects and opening up a new path for efficiently using the large dataset in MTS models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11625v1",
    "published_date": "2025-05-16 18:41:33 UTC",
    "updated_date": "2025-05-16 18:41:33 UTC"
  },
  {
    "arxiv_id": "2505.11621v1",
    "title": "A Classical View on Benign Overfitting: The Role of Sample Size",
    "authors": [
      "Junhyung Park",
      "Patrick Bloebaum",
      "Shiva Prasad Kasiviswanathan"
    ],
    "abstract": "Benign overfitting is a phenomenon in machine learning where a model perfectly fits (interpolates) the training data, including noisy examples, yet still generalizes well to unseen data. Understanding this phenomenon has attracted considerable attention in recent years. In this work, we introduce a conceptual shift, by focusing on almost benign overfitting, where models simultaneously achieve both arbitrarily small training and test errors. This behavior is characteristic of neural networks, which often achieve low (but non-zero) training error while still generalizing well. We hypothesize that this almost benign overfitting can emerge even in classical regimes, by analyzing how the interaction between sample size and model complexity enables larger models to achieve both good training fit but still approach Bayes-optimal generalization. We substantiate this hypothesis with theoretical evidence from two case studies: (i) kernel ridge regression, and (ii) least-squares regression using a two-layer fully connected ReLU neural network trained via gradient flow. In both cases, we overcome the strong assumptions often required in prior work on benign overfitting.\n  Our results on neural networks also provide the first generalization result in this setting that does not rely on any assumptions about the underlying regression function or noise, beyond boundedness. Our analysis introduces a novel proof technique based on decomposing the excess risk into estimation and approximation errors, interpreting gradient flow as an implicit regularizer, that helps avoid uniform convergence traps. This analysis idea could be of independent interest.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The results here subsume: arXiv:2410.06191",
    "pdf_url": "https://arxiv.org/pdf/2505.11621v1",
    "published_date": "2025-05-16 18:37:51 UTC",
    "updated_date": "2025-05-16 18:37:51 UTC"
  },
  {
    "arxiv_id": "2505.11618v3",
    "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges",
    "authors": [
      "Pengrui Quan",
      "Brian Wang",
      "Kang Yang",
      "Liying Han",
      "Mani Srivastava"
    ],
    "abstract": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS). Despite advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs), their capacity to reason about complex spatiotemporal signals remains underexplored. This paper proposes a hierarchical SpatioTemporal reAsoning benchmaRK, STARK, to systematically evaluate LLMs across three levels of reasoning complexity: state estimation (e.g., predicting field variables, localizing and tracking events in space and time), spatiotemporal reasoning over states (e.g., inferring spatial-temporal relationships), and world-knowledge-aware reasoning that integrates contextual and domain knowledge (e.g., intent prediction, landmark-aware navigation). We curate 26 distinct spatiotemporal tasks with diverse sensor modalities, comprising 14,552 challenges where models answer directly or by Python Code Interpreter. Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks requiring geometric reasoning (e.g., multilateration or triangulation), particularly as complexity increases. Surprisingly, LRMs show robust performance across tasks with various levels of difficulty, often competing or surpassing traditional first-principle-based methods. Our results show that in reasoning tasks requiring world knowledge, the performance gap between LLMs and LRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model continues to achieve leading performance across all evaluated tasks, a result attributed primarily to the larger size of the reasoning models. STARK motivates future innovations in model architectures and reasoning paradigms for intelligent CPS by providing a structured framework to identify limitations in the spatiotemporal reasoning of LLMs and LRMs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11618v3",
    "published_date": "2025-05-16 18:32:35 UTC",
    "updated_date": "2026-01-10 00:15:57 UTC"
  },
  {
    "arxiv_id": "2505.11615v1",
    "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations",
    "authors": [
      "Jian-Qiao Zhu",
      "Haijiang Yan",
      "Thomas L. Griffiths"
    ],
    "abstract": "Changing the behavior of large language models (LLMs) can be as straightforward as editing the Transformer's residual streams using appropriately constructed \"steering vectors.\" These modifications to internal neural activations, a form of representation engineering, offer an effective and targeted means of influencing model behavior without retraining or fine-tuning the model. But how can such steering vectors be systematically identified? We propose a principled approach for uncovering steering vectors by aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts. To evaluate this approach, we focus on extracting latent risk preferences from LLMs and steering their risk-related outputs using the aligned representations as steering vectors. We show that the resulting steering vectors successfully and reliably modulate LLM outputs in line with the targeted behavior.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11615v1",
    "published_date": "2025-05-16 18:23:10 UTC",
    "updated_date": "2025-05-16 18:23:10 UTC"
  },
  {
    "arxiv_id": "2505.11614v1",
    "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions",
    "authors": [
      "Jian-Qiao Zhu",
      "Hanbo Xie",
      "Dilip Arumugam",
      "Robert C. Wilson",
      "Thomas L. Griffiths"
    ],
    "abstract": "A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11614v1",
    "published_date": "2025-05-16 18:22:05 UTC",
    "updated_date": "2025-05-16 18:22:05 UTC"
  },
  {
    "arxiv_id": "2505.11612v1",
    "title": "Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors",
    "authors": [
      "Hung Nguyen",
      "Alireza Rahimi",
      "Veronica Whitford",
      "Hélène Fournier",
      "Irina Kondratova",
      "René Richard",
      "Hung Cao"
    ],
    "abstract": "Psychiatric disorders affect millions globally, yet their diagnosis faces significant challenges in clinical practice due to subjective assessments and accessibility concerns, leading to potential delays in treatment. To help address this issue, we present Heart2Mind, a human-centered contestable psychiatric disorder diagnosis system using wearable electrocardiogram (ECG) monitors. Our approach leverages cardiac biomarkers, particularly heart rate variability (HRV) and R-R intervals (RRI) time series, as objective indicators of autonomic dysfunction in psychiatric conditions. The system comprises three key components: (1) a Cardiac Monitoring Interface (CMI) for real-time data acquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency Transformer (MSTFT) that processes RRI time series through integrated time-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI) combining Self-Adversarial Explanations (SAEs) with contestable Large Language Models (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using leave-one-out cross-validation, outperforming state-of-the-art methods. SAEs successfully detect inconsistencies in model predictions by comparing attention-based and gradient-based explanations, while LLMs enable clinicians to validate correct predictions and contest erroneous ones. This work demonstrates the feasibility of combining wearable technology with Explainable Artificial Intelligence (XAI) and contestable LLMs to create a transparent, contestable system for psychiatric diagnosis that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/Analytics-Everywhere-Lab/heart2mind.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "41 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11612v1",
    "published_date": "2025-05-16 18:21:08 UTC",
    "updated_date": "2025-05-16 18:21:08 UTC"
  },
  {
    "arxiv_id": "2505.11611v2",
    "title": "Signal in the Noise: Polysemantic Interference Transfers and Predicts Cross-Model Influence",
    "authors": [
      "Bofan Gong",
      "Shiyang Lai",
      "James Evans",
      "Dawn Song"
    ],
    "abstract": "Polysemanticity is pervasive in language models and remains a major challenge for interpretation and model behavioral control. Leveraging sparse autoencoders (SAEs), we map the polysemantic topology of two small models (Pythia-70M and GPT-2-Small) to identify SAE feature pairs that are semantically unrelated yet exhibit interference within models. We intervene at four loci (prompt, token, feature, neuron) and measure induced shifts in the next-token prediction distribution, uncovering polysemantic structures that expose a systematic vulnerability in these models. Critically, interventions distilled from counterintuitive interference patterns shared by two small models transfer reliably to larger instruction-tuned models (Llama-3.1-8B/70B-Instruct and Gemma-2-9B-Instruct), yielding predictable behavioral shifts without access to model internals. These findings challenge the view that polysemanticity is purely stochastic, demonstrating instead that interference structures generalize across scale and family. Such generalization suggests a convergent, higher-order organization of internal representations, which is only weakly aligned with intuition and structured by latent regularities, offering new possibilities for both black-box control and theoretical insight into human and artificial cognition.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11611v2",
    "published_date": "2025-05-16 18:20:42 UTC",
    "updated_date": "2025-09-29 16:04:15 UTC"
  },
  {
    "arxiv_id": "2506.11021v1",
    "title": "Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering",
    "authors": [
      "Chaitanya Ravuri",
      "Saman Amarasinghe"
    ],
    "abstract": "Modern code-generation LLMs can already solve a large fraction of programming problems, yet they still hallucinate subtle bugs that make their outputs unsafe for autonomous deployment. We present functional clustering, a black-box wrapper that eliminates nearly all hallucination-induced errors while providing a tunable confidence score. The wrapper samples many candidate programs, executes each on a self-generated test suite, and clusters candidates whose I/O behavior is identical; the empirical mass of the largest cluster serves as an exact confidence estimate. A single scalar threshold on this estimate lets users trade coverage for reliability with exponential guarantees. On LiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet slashes the error rate of returned answers from ~65% to 2%, and drives it to 0% at a conservative threshold while still answering 15.6% of prompts. Manual audits show that the few residual mistakes stem from prompt misinterpretation, not random generation noise, narrowing future work to specification clarity. Because the method requires only sampling and sandbox execution, it applies unchanged to closed-source APIs and future models, offering a practical path toward dependable, autonomous code generation. Our code is available on Github (https://github.com/20ChaituR/functional-clustering).",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2506.11021v1",
    "published_date": "2025-05-16 18:19:38 UTC",
    "updated_date": "2025-05-16 18:19:38 UTC"
  },
  {
    "arxiv_id": "2505.11610v1",
    "title": "Foundation Models for AI-Enabled Biological Design",
    "authors": [
      "Asher Moldwin",
      "Amarda Shehu"
    ],
    "abstract": "This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.BM",
      "q-bio.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "Published as part of the workshop proceedings at AAAI 2025 in the workshop \"Foundation Models for Biological Discoveries\"",
    "pdf_url": "https://arxiv.org/pdf/2505.11610v1",
    "published_date": "2025-05-16 18:17:37 UTC",
    "updated_date": "2025-05-16 18:17:37 UTC"
  },
  {
    "arxiv_id": "2505.11601v2",
    "title": "Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search",
    "authors": [
      "Rui Liu",
      "Rui Xie",
      "Zijun Yao",
      "Yanjie Fu",
      "Dongjie Wang"
    ],
    "abstract": "Feature selection removes redundant features to enhanc performance and computational efficiency in downstream tasks. Existing works often struggle to capture complex feature interactions and adapt to diverse scenarios. Recent advances in this domain have incorporated generative intelligence to address these drawbacks by uncovering intricate relationships between features. However, two key limitations remain: 1) embedding feature subsets in a continuous space is challenging due to permutation sensitivity, as changes in feature order can introduce biases and weaken the embedding learning process; 2) gradient-based search in the embedding space assumes convexity, which is rarely guaranteed, leading to reduced search effectiveness and suboptimal subsets. To address these limitations, we propose a new framework that can: 1) preserve feature subset knowledge in a continuous embedding space while ensuring permutation invariance; 2) effectively explore the embedding space without relying on strong convex assumptions. For the first objective, we develop an encoder-decoder paradigm to preserve feature selection knowledge into a continuous embedding space. This paradigm captures feature interactions through pairwise relationships within the subset, removing the influence of feature order on the embedding. Moreover, an inducing point mechanism is introduced to accelerate pairwise relationship computations. For the second objective, we employ a policy-based reinforcement learning (RL) approach to guide the exploration of the embedding space. The RL agent effectively navigates the space by balancing multiple objectives. By prioritizing high-potential regions adaptively and eliminating the reliance on convexity assumptions, the RL agent effectively reduces the risk of converging to local optima. Extensive experiments demonstrate the effectiveness, efficiency, robustness and explicitness of our model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11601v2",
    "published_date": "2025-05-16 18:08:16 UTC",
    "updated_date": "2025-09-28 23:56:46 UTC"
  },
  {
    "arxiv_id": "2505.11595v4",
    "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO",
    "authors": [
      "Peter Chen",
      "Xiaopeng Li",
      "Ziniu Li",
      "Xi Chen",
      "Tianyi Lin"
    ],
    "abstract": "Reinforcement learning (RL) has proven effective in strengthening the reasoning capabilities of large language models (LLMs). A widely adopted method, Group Relative Policy Optimization (GRPO), has shown strong empirical results in training DeepSeek-R1. However, GRPO fails to update the policy when all responses within a group are incorrect (i.e., \\emph{all-negative-sample} groups). This limitation underscores a key gap between artificial and human intelligence: unlike humans, who can learn from mistakes, GRPO discards these signals. Our first contribution is to introduce a simple framework that mitigates the all-negative-sample issue by incorporating response diversity within groups using a \\textit{step-wise} judge model, which can be either directly trained or adapted from existing LLMs. We prove that this diversification can accelerate GRPO's learning dynamics in a simplified setting. We also empirically validate the proposed stepwise guided policy optimization (SGPO) method, demonstrating consistent gains across model sizes (7B, 14B, 32B) in offline and online training on 9 benchmarks, including base and distilled variants. Our results highlight two advantages: (i) SGPO surpasses GRPO, especially in the early and mid-training stages where all-negative-sample groups are prevalent; and (ii) SGPO does not require judge models to generate correct answers, differentiating it from knowledge distillation methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "42 pages; correct some typos",
    "pdf_url": "https://arxiv.org/pdf/2505.11595v4",
    "published_date": "2025-05-16 18:02:05 UTC",
    "updated_date": "2025-10-01 01:55:06 UTC"
  },
  {
    "arxiv_id": "2505.18181v1",
    "title": "2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision",
    "authors": [
      "Yunrui Li",
      "Hao Xu",
      "Pengyu Hong"
    ],
    "abstract": "Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy, particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays a critical role in elucidating molecular structures, interactions, and electronic properties. However, accurately interpreting 2D NMR data remains labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) holds significant potential in 2D NMR analysis by learning molecular representations and recognizing complex patterns from data. However, progress has been limited by the lack of large-scale and high-quality annotated datasets. In this work, we introduce 2DNMRGym, the first annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. It includes over 22,000 HSQC spectra, along with the corresponding molecular graphs and SMILES strings. Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained using algorithm-generated annotations derived from a previously validated method and evaluated on a held-out set of human-annotated gold-standard labels. This enables rigorous assessment of a model's ability to generalize from imperfect supervision to expert-level interpretation. We provide benchmark results using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. Our data and code is open-source and available on Huggingface and Github.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18181v1",
    "published_date": "2025-05-16 18:02:05 UTC",
    "updated_date": "2025-05-16 18:02:05 UTC"
  },
  {
    "arxiv_id": "2505.11594v3",
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
    "authors": [
      "Jintao Zhang",
      "Jia Wei",
      "Pengle Zhang",
      "Xiaoming Xu",
      "Haofeng Huang",
      "Haoxu Wang",
      "Kai Jiang",
      "Jianfei Chen",
      "Jun Zhu"
    ],
    "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11594v3",
    "published_date": "2025-05-16 18:01:54 UTC",
    "updated_date": "2026-01-14 21:51:33 UTC"
  },
  {
    "arxiv_id": "2505.11586v1",
    "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
    "authors": [
      "Rui Zhang",
      "Yun Shen",
      "Hongwei Li",
      "Wenbo Jiang",
      "Hanxiao Chen",
      "Yuan Zhang",
      "Guowen Xu",
      "Yang Zhang"
    ],
    "abstract": "Recent research highlights concerns about the trustworthiness of third-party Pre-Trained Language Models (PTLMs) due to potential backdoor attacks. These backdoored PTLMs, however, are effective only for specific pre-defined downstream tasks. In reality, these PTLMs can be adapted to many other unrelated downstream tasks. Such adaptation may lead to unforeseen consequences in downstream model outputs, consequently raising user suspicion and compromising attack stealthiness. We refer to this phenomenon as backdoor complications. In this paper, we undertake the first comprehensive quantification of backdoor complications. Through extensive experiments using 4 prominent PTLMs and 16 text classification benchmark datasets, we demonstrate the widespread presence of backdoor complications in downstream models fine-tuned from backdoored PTLMs. The output distribution of triggered samples significantly deviates from that of clean samples. Consequently, we propose a backdoor complication reduction method leveraging multi-task learning to mitigate complications without prior knowledge of downstream tasks. The experimental results demonstrate that our proposed method can effectively reduce complications while maintaining the efficacy and consistency of backdoor attacks. Our code is available at https://github.com/zhangrui4041/Backdoor_Complications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11586v1",
    "published_date": "2025-05-16 17:59:53 UTC",
    "updated_date": "2025-05-16 17:59:53 UTC"
  },
  {
    "arxiv_id": "2505.11584v1",
    "title": "LLM Agents Are Hypersensitive to Nudges",
    "authors": [
      "Manuel Cherep",
      "Pattie Maes",
      "Nikhil Singh"
    ],
    "abstract": "LLMs are being set loose in complex, real-world environments involving sequential decision-making and tool use. Often, this involves making choices on behalf of human users. However, not much is known about the distribution of such choices, and how susceptible they are to different choice architectures. We perform a case study with a few such LLM models on a multi-attribute tabular decision-making problem, under canonical nudges such as the default option, suggestions, and information highlighting, as well as additional prompting strategies. We show that, despite superficial similarities to human choice distributions, such models differ in subtle but important ways. First, they show much higher susceptibility to the nudges. Second, they diverge in points earned, being affected by factors like the idiosyncrasy of available prizes. Third, they diverge in information acquisition strategies: e.g. incurring substantial cost to reveal too much information, or selecting without revealing any. Moreover, we show that simple prompt strategies like zero-shot chain of thought (CoT) can shift the choice distribution, and few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges. Finally, we show how optimal nudges optimized with a human resource-rational model can similarly increase LLM performance for some models. All these findings suggest that behavioral tests are needed before deploying models as agents or assistants acting on behalf of users in complex environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 28 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11584v1",
    "published_date": "2025-05-16 17:53:05 UTC",
    "updated_date": "2025-05-16 17:53:05 UTC"
  },
  {
    "arxiv_id": "2505.11485v1",
    "title": "Modeling cognitive processes of natural reading with transformer-based Language Models",
    "authors": [
      "Bruno Bianchi",
      "Fermín Travi",
      "Juan E. Kamienkowski"
    ],
    "abstract": "Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11485v1",
    "published_date": "2025-05-16 17:47:58 UTC",
    "updated_date": "2025-05-16 17:47:58 UTC"
  },
  {
    "arxiv_id": "2505.15835v1",
    "title": "Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based Indoor Localization",
    "authors": [
      "Nayan Sanjay Bhatia",
      "Katia Obraczka"
    ],
    "abstract": "Wireless Fidelity (WiFi) based indoor positioning is a widely researched area for determining the position of devices within a wireless network. Accurate indoor location has numerous applications, such as asset tracking and indoor navigation. Despite advances in WiFi localization techniques -- in particular approaches that leverage WiFi telemetry -- their adoption in practice remains limited due to several factors including environmental changes that cause signal fading, multipath effects, interference, which, in turn, impact positioning accuracy. In addition, telemetry data differs depending on the WiFi device vendor, offering distinct features and formats; use case requirements can also vary widely. Currently, there is no unified model to handle all these variations effectively. In this paper, we present WiFiGPT, a Generative Pretrained Transformer (GPT) based system that is able to handle these variations while achieving high localization accuracy. Our experiments with WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can effectively capture subtle spatial patterns in noisy wireless telemetry, making them reliable regressors. Compared to existing state-of-the-art methods, our method matches and often surpasses conventional approaches for multiple types of telemetry. Achieving sub-meter accuracy for RSSI and FTM and centimeter-level precision for CSI demonstrates the potential of LLM-based localisation to outperform specialized techniques, all without handcrafted signal processing or calibration.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "11 pages, 2 figures, In Submission",
    "pdf_url": "https://arxiv.org/pdf/2505.15835v1",
    "published_date": "2025-05-16 17:47:32 UTC",
    "updated_date": "2025-05-16 17:47:32 UTC"
  },
  {
    "arxiv_id": "2507.19485v1",
    "title": "Creativity as a Human Right: Design Considerations for Computational Creativity Systems",
    "authors": [
      "Alayt Issak"
    ],
    "abstract": "We investigate creativity that is underlined in the Universal Declaration of Human Rights (UDHR) to present design considerations for Computational Creativity (CC) systems. We find this declaration to describe creativity in salient aspects and bring to light creativity as a Human Right attributed to the Fourth Generation of such rights. This generation of rights attributes CC systems and the evolving nature of interaction with entities of shared intelligence. Our methodology examines five of thirty articles from the UDHR and demonstrates each article with actualizations concluding with design considerations for each. We contribute our findings to ground the relationship between creativity and CC systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.19485v1",
    "published_date": "2025-05-16 17:46:37 UTC",
    "updated_date": "2025-05-16 17:46:37 UTC"
  },
  {
    "arxiv_id": "2505.11481v1",
    "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation",
    "authors": [
      "Alayt Issak",
      "Jeba Rezwana",
      "Casper Harteveld"
    ],
    "abstract": "Striking the appropriate balance between humans and co-creative AI is an open research question in computational creativity. Co-creativity, a form of hybrid intelligence where both humans and AI take action proactively, is a process that leads to shared creative artifacts and ideas. Achieving a balanced dynamic in co-creativity requires characterizing control and identifying strategies to distribute control between humans and AI. We define control as the power to determine, initiate, and direct the process of co-creation. Informed by a systematic literature review of 172 full-length papers, we introduce MOSAAIC (Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation), a novel framework for characterizing and balancing control in co-creation. MOSAAIC identifies three key dimensions of control: autonomy, initiative, and authority. We supplement our framework with control optimization strategies in co-creation. To demonstrate MOSAAIC's applicability, we analyze the distribution of control in six existing co-creative AI case studies and present the implications of using this framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11481v1",
    "published_date": "2025-05-16 17:41:44 UTC",
    "updated_date": "2025-05-16 17:41:44 UTC"
  },
  {
    "arxiv_id": "2505.11480v2",
    "title": "SuperCoder: Assembly Program Superoptimization with Large Language Models",
    "authors": [
      "Anjiang Wei",
      "Tarun Suresh",
      "Huanmi Tan",
      "Yinglun Xu",
      "Gagandeep Singh",
      "Ke Wang",
      "Alex Aiken"
    ],
    "abstract": "Superoptimization is the task of transforming a program into a faster one while preserving its input-output behavior. In this work, we investigate whether large language models (LLMs) can serve as superoptimizers, generating assembly programs that outperform code already optimized by industry-standard compilers. We construct the first large-scale benchmark for this problem, consisting of 8,072 real-world assembly programs averaging 130 lines, in contrast to prior datasets restricted to 2-15 straight-line, loop-free programs. We evaluate 23 LLMs on this benchmark and find that the strongest baseline, Claude-opus-4, achieves a 51.5% test-passing rate and a 1.43x average speedup over gcc -O3. To further enhance performance, we fine-tune models with reinforcement learning, optimizing a reward function that integrates correctness and performance speedup. Starting from Qwen2.5-Coder-7B-Instruct (61.4% correctness, 1.10x speedup), the fine-tuned model SuperCoder attains 95.0% correctness and 1.46x average speedup. Our results demonstrate, for the first time, that LLMs can be applied as superoptimizers for assembly programs, establishing a foundation for future research in program performance optimization beyond compiler heuristics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PF",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11480v2",
    "published_date": "2025-05-16 17:40:45 UTC",
    "updated_date": "2025-09-25 21:58:56 UTC"
  },
  {
    "arxiv_id": "2505.11478v2",
    "title": "Automatic Reward Shaping from Confounded Offline Data",
    "authors": [
      "Mingxuan Li",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "abstract": "A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \\emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11478v2",
    "published_date": "2025-05-16 17:40:01 UTC",
    "updated_date": "2025-09-09 17:06:56 UTC"
  },
  {
    "arxiv_id": "2505.11475v2",
    "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages",
    "authors": [
      "Zhilin Wang",
      "Jiaqi Zeng",
      "Olivier Delalleau",
      "Hoo-Chang Shin",
      "Felipe Soares",
      "Alexander Bukharin",
      "Ellie Evans",
      "Yi Dong",
      "Oleksii Kuchaiev"
    ],
    "abstract": "Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference Models (NVIDIA Open Model): https://huggingface.co/collections/nvidia/reward-models-68377c5955575f71fcc7a2a3",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 Datasets and Benchmarks Track Camera Ready, 46 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11475v2",
    "published_date": "2025-05-16 17:31:19 UTC",
    "updated_date": "2025-10-24 04:04:19 UTC"
  },
  {
    "arxiv_id": "2505.11462v2",
    "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models",
    "authors": [
      "Rahul Thapa",
      "Qingyang Wu",
      "Kevin Wu",
      "Harrison Zhang",
      "Angela Zhang",
      "Eric Wu",
      "Haotian Ye",
      "Suhana Bedi",
      "Nevin Aresh",
      "Joseph Boen",
      "Shriya Reddy",
      "Ben Athiwaratkun",
      "Shuaiwen Leon Song",
      "James Zou"
    ],
    "abstract": "Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, HuatuoGPT-o1 scores 56.9 on knowledge but only 44.8 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11462v2",
    "published_date": "2025-05-16 17:16:27 UTC",
    "updated_date": "2025-06-24 03:27:30 UTC"
  },
  {
    "arxiv_id": "2505.11454v6",
    "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation",
    "authors": [
      "Shaina Raza",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Ashmal Vayani",
      "Ahmed Y. Radwan",
      "Mukund S. Chettiar",
      "Amandeep Singh",
      "Mubarak Shah",
      "Deval Pandya"
    ],
    "abstract": "Although recent large multimodal models (LMMs) demonstrate impressive progress on vision language tasks, their alignment with human centered (HC) principles, such as fairness, ethics, inclusivity, empathy, and robustness; remains poorly understood. We present HumaniBench, a unified evaluation framework designed to characterize HC alignment across realistic, socially grounded visual contexts. HumaniBench contains 32,000 expert-verified image question pairs derived from real world news imagery and spanning seven evaluation tasks: scene understanding, instance identity, multiple-choice visual question answering (VQA), multilinguality, visual grounding, empathetic captioning, and image resilience testing. Each task is mapped to one or more HC principles through a principled operationalization of metrics covering accuracy, harmful content detection, hallucination and faithfulness, coherence, cross lingual quality, empathy, and robustness.We evaluate 15 state-of-the-art LMMs under this framework and observe consistent cross model trade offs: proprietary systems achieve the strongest performance on ethics, reasoning, and empathy, while open-source models exhibit superior visual grounding and resilience. All models, however, show persistent gaps in fairness and multilingual inclusivity. We further analyze the effect of inference-time techniques, finding that chain of thought prompting and test-time scaling yield 8 to 12 % improvements on several HC dimensions. HumaniBench provides a reproducible, extensible foundation for systematic HC evaluation of LMMs and enables fine-grained analysis of alignment trade-offs that are not captured by conventional multimodal benchmarks. https://vectorinstitute.github.io/humanibench/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11454v6",
    "published_date": "2025-05-16 17:09:44 UTC",
    "updated_date": "2025-11-27 20:09:53 UTC"
  },
  {
    "arxiv_id": "2505.11451v2",
    "title": "Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps",
    "authors": [
      "Lee Harris"
    ],
    "abstract": "Dates often contribute towards highly impactful medical decisions, but it is rarely clear how to extract this data. AI has only just begun to be used transcribe such documents, and common methods are either to trust that the output produced by a complex AI model, or to parse the text using regular expressions. Recent work has established that regular expressions are an explainable form of logic, but it is difficult to decompose these into the component parts that are required to construct precise UNIX timestamps. First, we test publicly-available regular expressions, and we found that these were unable to capture a significant number of our dates. Next, we manually created easily-decomposable regular expressions, and we found that these were able to detect the majority of real dates, but also a lot of sequences of text that look like dates. Finally, we used regular expression synthesis to automatically identify regular expressions from the reverse-engineered UNIX timestamps that we created. We find that regular expressions created by regular expression synthesis detect far fewer sequences of text that look like dates than those that were manually created, at the cost of a slight increase to the number of missed dates. Overall, our results show that regular expressions can be created through regular expression synthesis to identify complex dates and date ranges in text transcriptions. To our knowledge, our proposed way of learning deterministic logic by reverse-engineering several many-one mappings and feeding these into a regular expression synthesiser is a new approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This research was funded by a UKRI grant. Number: 10048265",
    "pdf_url": "https://arxiv.org/pdf/2505.11451v2",
    "published_date": "2025-05-16 17:07:14 UTC",
    "updated_date": "2025-06-03 10:43:24 UTC"
  },
  {
    "arxiv_id": "2505.11582v2",
    "title": "Comparing Lexical and Semantic Vector Search Methods When Classifying Medical Documents",
    "authors": [
      "Lee Harris"
    ],
    "abstract": "Classification is a common AI problem, and vector search is a typical solution. This transforms a given body of text into a numerical representation, known as an embedding, and modern improvements to vector search focus on optimising speed and predictive accuracy. This is often achieved through neural methods that aim to learn language semantics. However, our results suggest that these are not always the best solution. Our task was to classify rigidly-structured medical documents according to their content, and we found that using off-the-shelf semantic vector search produced slightly worse predictive accuracy than creating a bespoke lexical vector search model, and that it required significantly more time to execute. These findings suggest that traditional methods deserve to be contenders in the information retrieval toolkit, despite the prevalence and success of neural models.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "This project was funded by a UKRI grant, number: 10048265",
    "pdf_url": "https://arxiv.org/pdf/2505.11582v2",
    "published_date": "2025-05-16 17:06:35 UTC",
    "updated_date": "2025-06-03 09:18:51 UTC"
  },
  {
    "arxiv_id": "2505.11449v1",
    "title": "LLMs unlock new paths to monetizing exploits",
    "authors": [
      "Nicholas Carlini",
      "Milad Nasr",
      "Edoardo Debenedetti",
      "Barry Wang",
      "Christopher A. Choquette-Choo",
      "Daphne Ippolito",
      "Florian Tramèr",
      "Matthew Jagielski"
    ],
    "abstract": "We argue that Large language models (LLMs) will soon alter the economics of cyberattacks. Instead of attacking the most commonly used software and monetizing exploits by targeting the lowest common denominator among victims, LLMs enable adversaries to launch tailored attacks on a user-by-user basis. On the exploitation front, instead of human attackers manually searching for one difficult-to-identify bug in a product with millions of users, LLMs can find thousands of easy-to-identify bugs in products with thousands of users. And on the monetization front, instead of generic ransomware that always performs the same attack (encrypt all your data and request payment to decrypt), an LLM-driven ransomware attack could tailor the ransom demand based on the particular content of each exploited device.\n  We show that these two attacks (and several others) are imminently practical using state-of-the-art LLMs. For example, we show that without any human intervention, an LLM finds highly sensitive personal information in the Enron email dataset (e.g., an executive having an affair with another employee) that could be used for blackmail. While some of our attacks are still too expensive to scale widely today, the incentives to implement these attacks will only increase as LLMs get cheaper. Thus, we argue that LLMs create a need for new defense-in-depth approaches.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11449v1",
    "published_date": "2025-05-16 17:05:25 UTC",
    "updated_date": "2025-05-16 17:05:25 UTC"
  },
  {
    "arxiv_id": "2505.11439v1",
    "title": "SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision",
    "authors": [
      "Utsav Rai",
      "Haozheng Xu",
      "Stamatia Giannarou"
    ],
    "abstract": "Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery (RMIS) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, supervised learning methods require extensive training on annotated datasets, limiting their adaptability to new tools. Despite their success in other domains, zero-shot pose estimation models remain unexplored in RMIS for pose estimation of surgical instruments, creating a gap in generalising to unseen surgical tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D models like the FoundationPose and SAM-6D. We advanced these models by incorporating vision-based depth estimation using the RAFT-Stereo method, for robust depth estimation in reflective and textureless environments. Additionally, we enhanced SAM-6D by replacing its instance segmentation module, Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly boosting segmentation accuracy in occluded and complex conditions. Extensive validation reveals that our enhanced SAM-6D surpasses FoundationPose in zero-shot pose estimation of unseen surgical instruments, setting a new benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the generalisability of pose estimation for unseen objects and pioneers the application of RGB-D zero-shot methods in RMIS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published in 2025 International Conference on Robotics and Automation (ICRA)",
    "pdf_url": "https://arxiv.org/pdf/2505.11439v1",
    "published_date": "2025-05-16 16:58:03 UTC",
    "updated_date": "2025-05-16 16:58:03 UTC"
  },
  {
    "arxiv_id": "2505.11436v2",
    "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art",
    "authors": [
      "Yiming Lei",
      "Chenkai Zhang",
      "Zeming Liu",
      "Haitao Leng",
      "Shaoguo Liu",
      "Tingting Gao",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "abstract": "Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "69 pages, 66 figures, accepted by ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11436v2",
    "published_date": "2025-05-16 16:56:40 UTC",
    "updated_date": "2025-05-21 15:41:51 UTC"
  },
  {
    "arxiv_id": "2505.11427v1",
    "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
    "authors": [
      "Adrian Robert Minut",
      "Tommaso Mencattini",
      "Andrea Santilli",
      "Donato Crisostomi",
      "Emanuele Rodolà"
    ],
    "abstract": "Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Link: https://github.com/tommasomncttn/mergenetic",
    "pdf_url": "https://arxiv.org/pdf/2505.11427v1",
    "published_date": "2025-05-16 16:43:23 UTC",
    "updated_date": "2025-05-16 16:43:23 UTC"
  },
  {
    "arxiv_id": "2505.11424v1",
    "title": "Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study",
    "authors": [
      "Rana Poureskandar",
      "Shiva Razzagzadeh"
    ],
    "abstract": "This study evaluated the performance of a YOLOv8-based segmentation model for detecting and segmenting wrinkles in facial images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11424v1",
    "published_date": "2025-05-16 16:38:01 UTC",
    "updated_date": "2025-05-16 16:38:01 UTC"
  },
  {
    "arxiv_id": "2505.11417v1",
    "title": "EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions",
    "authors": [
      "Patryk Bartkowiak",
      "Michal Podstawski"
    ],
    "abstract": "This paper introduces a novel dataset and evaluation benchmark designed to assess and improve small language models deployable on edge devices, with a focus on user profiling from multi-session natural language interactions in smart home environments. At the core of the dataset are structured user profiles, each defined by a set of routines - context-triggered, repeatable patterns of behavior that govern how users interact with their home systems. Using these profiles as input, a large language model (LLM) generates corresponding interaction sessions that simulate realistic, diverse, and context-aware dialogues between users and their devices.\n  The primary task supported by this dataset is profile reconstruction: inferring user routines and preferences solely from interactions history. To assess how well current models can perform this task under realistic conditions, we benchmarked several state-of-the-art compact language models and compared their performance against large foundation models. Our results show that while small models demonstrate some capability in reconstructing profiles, they still fall significantly short of large models in accurately capturing user behavior. This performance gap poses a major challenge - particularly because on-device processing offers critical advantages, such as preserving user privacy, minimizing latency, and enabling personalized experiences without reliance on the cloud. By providing a realistic, structured testbed for developing and evaluating behavioral modeling under these constraints, our dataset represents a key step toward enabling intelligent, privacy-respecting AI systems that learn and adapt directly on user-owned devices.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11417v1",
    "published_date": "2025-05-16 16:29:21 UTC",
    "updated_date": "2025-05-16 16:29:21 UTC"
  },
  {
    "arxiv_id": "2505.11416v1",
    "title": "MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection",
    "authors": [
      "Pouya Shaeri",
      "Ariane Middel"
    ],
    "abstract": "Modern neural networks often activate all neurons for every input, leading to unnecessary computation and inefficiency. We introduce Matrix-Interpolated Dropout Layer (MID-L), a novel module that dynamically selects and activates only the most informative neurons by interpolating between two transformation paths via a learned, input-dependent gating vector. Unlike conventional dropout or static sparsity methods, MID-L employs a differentiable Top-k masking strategy, enabling per-input adaptive computation while maintaining end-to-end differentiability. MID-L is model-agnostic and integrates seamlessly into existing architectures. Extensive experiments on six benchmarks, including MNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves up to average 55\\% reduction in active neurons, 1.7$\\times$ FLOPs savings, and maintains or exceeds baseline accuracy. We further validate the informativeness and selectivity of the learned neurons via Sliced Mutual Information (SMI) and observe improved robustness under overfitting and noisy data conditions. Additionally, MID-L demonstrates favorable inference latency and memory usage profiles, making it suitable for both research exploration and deployment on compute-constrained systems. These results position MID-L as a general-purpose, plug-and-play dynamic computation layer, bridging the gap between dropout regularization and efficient inference.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Submitted in a Computer Science Conference, currently in Review",
    "pdf_url": "https://arxiv.org/pdf/2505.11416v1",
    "published_date": "2025-05-16 16:29:19 UTC",
    "updated_date": "2025-05-16 16:29:19 UTC"
  },
  {
    "arxiv_id": "2505.11580v1",
    "title": "Flash Invariant Point Attention",
    "authors": [
      "Andrew Liu",
      "Axel Elaldi",
      "Nicholas T Franklin",
      "Nathan Russell",
      "Gurinder S Atwal",
      "Yih-En A Ban",
      "Olivia Viessmann"
    ],
    "abstract": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11580v1",
    "published_date": "2025-05-16 16:19:05 UTC",
    "updated_date": "2025-05-16 16:19:05 UTC"
  },
  {
    "arxiv_id": "2505.11409v2",
    "title": "Visual Planning: Let's Think Only with Images",
    "authors": [
      "Yi Xu",
      "Chengzu Li",
      "Han Zhou",
      "Xingchen Wan",
      "Caiqi Zhang",
      "Anna Korhonen",
      "Ivan Vulić"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations for these \"vision-first\" tasks, as a supplementary channel to language-based reasoning. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising supplement to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures, 2 tables (31 pages, 15 figures, 10 tables including references and appendices)",
    "pdf_url": "https://arxiv.org/pdf/2505.11409v2",
    "published_date": "2025-05-16 16:17:22 UTC",
    "updated_date": "2025-09-29 08:21:01 UTC"
  },
  {
    "arxiv_id": "2505.11406v1",
    "title": "Large Language Model Use Impact Locus of Control",
    "authors": [
      "Jenny Xiyu Fu",
      "Brennan Antone",
      "Kowe Kadoma",
      "Malte Jung"
    ],
    "abstract": "As AI tools increasingly shape how we write, they may also quietly reshape how we perceive ourselves. This paper explores the psychological impact of co-writing with AI on people's locus of control. Through an empirical study with 462 participants, we found that employment status plays a critical role in shaping users' reliance on AI and their locus of control. Current results demonstrated that employed participants displayed higher reliance on AI and a shift toward internal control, while unemployed users tended to experience a reduction in personal agency. Through quantitative results and qualitative observations, this study opens a broader conversation about AI's role in shaping personal agency and identity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11406v1",
    "published_date": "2025-05-16 16:16:32 UTC",
    "updated_date": "2025-05-16 16:16:32 UTC"
  },
  {
    "arxiv_id": "2505.11404v3",
    "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner",
    "authors": [
      "Wenchuan Zhang",
      "Penghao Zhang",
      "Jingru Guo",
      "Tao Cheng",
      "Jie Chen",
      "Shuwan Zhang",
      "Zhang Zhang",
      "Yuhao Yi",
      "Hong Bu"
    ],
    "abstract": "Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose Patho-CLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both Patho-CLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11404v3",
    "published_date": "2025-05-16 16:12:50 UTC",
    "updated_date": "2025-06-17 08:23:39 UTC"
  },
  {
    "arxiv_id": "2506.01983v3",
    "title": "Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification",
    "authors": [
      "Reyhaneh Keshavarzpour",
      "Eghbal Mansoori"
    ],
    "abstract": "Identification of antimicrobial peptides is an important and necessary issue in today's era. Antimicrobial peptides are essential as an alternative to antibiotics for biomedical applications and many other practical applications. These oligopeptides are useful in drug design and cause innate immunity against microorganisms. Artificial intelligence algorithms have played a significant role in the ease of identifying these peptides.This research is improved by improving proposed method in the field of antimicrobial peptides prediction. Suggested method is improved by combining the best coding method from different perspectives, In the following a deep neural network to balance the imbalanced combined datasets. The results of this research show that the proposed method have a significant improvement in the accuracy and efficiency of the prediction of antimicrobial peptides and are able to provide the best results compared to the existing methods. These development in the field of prediction and classification of antimicrobial peptides, basically in the fields of medicine and pharmaceutical industries, have high effectiveness and application.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 3 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.01983v3",
    "published_date": "2025-05-16 16:11:42 UTC",
    "updated_date": "2025-12-16 09:36:33 UTC"
  },
  {
    "arxiv_id": "2505.17054v1",
    "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery",
    "authors": [
      "Linglong Qian",
      "Zina Ibrahim"
    ],
    "abstract": "Recent advances in transformer architectures have revolutionised natural language processing, but their application to healthcare domains presents unique challenges. Patient timelines are characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships that differ substantially from traditional language tasks. This paper introduces \\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture specifically designed to address the challenges of clinical sequence modelling in electronic health records. \\METHOD~integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing. Evaluations on the MIMIC-IV database demonstrate that \\METHOD~consistently outperforms the state-of-the-art \\ETHOS~model, particularly in predicting high-severity cases that require urgent clinical intervention. \\METHOD~exhibits stable performance across varying inference lengths, a crucial feature for clinical deployment where patient histories vary significantly in length. Analysis of learned embeddings reveals that \\METHOD~better preserves clinical hierarchies and relationships between medical concepts. These results suggest that \\METHOD~represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.17054v1",
    "published_date": "2025-05-16 15:52:56 UTC",
    "updated_date": "2025-05-16 15:52:56 UTC"
  },
  {
    "arxiv_id": "2505.11365v4",
    "title": "Phare: A Safety Probe for Large Language Models",
    "authors": [
      "Pierre Le Jeune",
      "Benoît Malézieux",
      "Weixuan Xiao",
      "Matteo Dora"
    ],
    "abstract": "Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11365v4",
    "published_date": "2025-05-16 15:31:08 UTC",
    "updated_date": "2025-05-26 12:21:14 UTC"
  },
  {
    "arxiv_id": "2506.01982v2",
    "title": "Music Interpretation and Emotion Perception: A Computational and Neurophysiological Investigation",
    "authors": [
      "Vassilis Lyberatos",
      "Spyridon Kantarelis",
      "Ioanna Zioga",
      "Christina Anagnostopoulou",
      "Giorgos Stamou",
      "Anastasia Georgaki"
    ],
    "abstract": "This study investigates emotional expression and perception in music performance using computational and neurophysiological methods. The influence of different performance settings, such as repertoire, diatonic modal etudes, and improvisation, as well as levels of expressiveness, on performers' emotional communication and listeners' reactions is explored. Professional musicians performed various tasks, and emotional annotations were provided by both performers and the audience. Audio analysis revealed that expressive and improvisational performances exhibited unique acoustic features, while emotion analysis showed stronger emotional responses. Neurophysiological measurements indicated greater relaxation in improvisational performances. This multimodal study highlights the significance of expressivity in enhancing emotional communication and audience engagement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at SMC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.01982v2",
    "published_date": "2025-05-16 15:30:38 UTC",
    "updated_date": "2025-06-04 06:16:47 UTC"
  },
  {
    "arxiv_id": "2505.11340v1",
    "title": "DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios",
    "authors": [
      "Zeyu Gao",
      "Yuxin Cui",
      "Hao Wang",
      "Siliang Qin",
      "Yuanda Wang",
      "Bolun Zhang",
      "Chao Zhang"
    ],
    "abstract": "Decompilers are fundamental tools for critical security tasks, from vulnerability discovery to malware analysis, yet their evaluation remains fragmented. Existing approaches primarily focus on syntactic correctness through synthetic micro-benchmarks or subjective human ratings, failing to address real-world requirements for semantic fidelity and analyst usability. We present DecompileBench, the first comprehensive framework that enables effective evaluation of decompilers in reverse engineering workflows through three key components: \\textit{real-world function extraction} (comprising 23,400 functions from 130 real-world programs), \\textit{runtime-aware validation}, and \\textit{automated human-centric assessment} using LLM-as-Judge to quantify the effectiveness of decompilers in reverse engineering workflows. Through a systematic comparison between six industrial-strength decompilers and six recent LLM-powered approaches, we demonstrate that LLM-based methods surpass commercial tools in code understandability despite 52.2% lower functionality correctness. These findings highlight the potential of LLM-based approaches to transform human-centric reverse engineering. We open source \\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a framework to advance research on decompilers and assist security experts in making informed tool selections based on their specific requirements.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11340v1",
    "published_date": "2025-05-16 15:07:43 UTC",
    "updated_date": "2025-05-16 15:07:43 UTC"
  },
  {
    "arxiv_id": "2505.11326v1",
    "title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models",
    "authors": [
      "Keunwoo Peter Yu",
      "Joyce Chai"
    ],
    "abstract": "Vision-language models (VLMs) have shown remarkable progress in offline tasks such as image captioning and video question answering. However, real-time interactive environments impose new demands on VLMs, requiring them to generate utterances that are not only semantically accurate but also precisely timed. We identify two core capabilities necessary for such settings -- $\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and propose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation (TGLG)}$, to evaluate them. TGLG requires models to generate utterances in response to streaming video such that both content and timing align with dynamic visual input. To support this benchmark, we curate evaluation datasets from sports broadcasting and egocentric human interaction domains, and introduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring semantic similarity and temporal alignment. Finally, we present $\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$, a model that interleaves visual and linguistic tokens in a time-synchronized manner, enabling real-time language generation without relying on turn-based assumptions. Experimental results show that VLM-TSI significantly outperforms a strong baseline, yet overall performance remains modest -- highlighting the difficulty of TGLG and motivating further research in real-time VLMs. Code and data available $\\href{https://github.com/yukw777/tglg}{here}$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11326v1",
    "published_date": "2025-05-16 14:48:30 UTC",
    "updated_date": "2025-05-16 14:48:30 UTC"
  },
  {
    "arxiv_id": "2505.11325v2",
    "title": "Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors",
    "authors": [
      "Thomas Nagler",
      "David Rügamer"
    ],
    "abstract": "Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11325v2",
    "published_date": "2025-05-16 14:47:43 UTC",
    "updated_date": "2025-10-16 19:58:50 UTC"
  },
  {
    "arxiv_id": "2505.17053v1",
    "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games",
    "authors": [
      "Ou Jiamin",
      "Eikmans Emile",
      "Buskens Vincent",
      "Pankowska Paulina",
      "Shan Yuli"
    ],
    "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities in understanding human languages, this study explores how they translate this understanding into social exchange contexts that capture certain essences of real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were placed in economic trust games where players balance self-interest with trust and reciprocity, making decisions that reveal their social preferences and interactive reasoning abilities. Our study shows that LLMs deviate from pure self-interest and exhibit trust and reciprocity even without being prompted to adopt a specific persona. In the simplest one-shot interaction, LLMs emulated how human players place trust at the beginning of such a game. Larger human-machine divergences emerged in scenarios involving trust repayment or multi-round interactions, where decisions were influenced by both social preferences and interactive reasoning. LLMs responses varied significantly when prompted to adopt personas like selfish or unselfish players, with the impact outweighing differences between models or game types. Response of ChatGPT-4, in an unselfish or neutral persona, resembled the highest trust and reciprocity, surpassing humans, Claude, and Bard. Claude and Bard displayed trust and reciprocity levels that sometimes exceeded and sometimes fell below human choices. When given selfish personas, all LLMs showed lower trust and reciprocity than humans. Interactive reasoning to the actions of counterparts or changing game mechanics appeared to be random rather than stable, reproducible characteristics in the response of LLMs, though some improvements were observed when ChatGPT-4 responded in selfish or unselfish personas.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 2 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.17053v1",
    "published_date": "2025-05-16 14:45:59 UTC",
    "updated_date": "2025-05-16 14:45:59 UTC"
  },
  {
    "arxiv_id": "2505.11579v2",
    "title": "Toward Adaptive Categories: Dimensional Governance for Agentic AI",
    "authors": [
      "Zeynep Engin",
      "David Hand"
    ],
    "abstract": "As AI systems evolve from static tools to dynamic agents, traditional categorical governance frameworks -- based on fixed risk tiers, levels of autonomy, or human oversight models -- are increasingly insufficient on their own. Systems built on foundation models, self-supervised learning, and multi-agent architectures increasingly blur the boundaries that categories were designed to police. In this Perspective, we make the case for dimensional governance: a framework that tracks how decision authority, process autonomy, and accountability (the 3As) distribute dynamically across human-AI relationships. A critical advantage of this approach is its ability to explicitly monitor system movement toward and across key governance thresholds, enabling preemptive adjustments before risks materialize. This dimensional approach provides the necessary foundation for more adaptive categorization, enabling thresholds and classifications that can evolve with emerging capabilities. While categories remain essential for decision-making, building them upon dimensional foundations allows for context-specific adaptability and stakeholder-responsive governance that static approaches cannot achieve. We outline key dimensions, critical trust thresholds, and practical examples illustrating where rigid categorical frameworks fail -- and where a dimensional mindset could offer a more resilient and future-proof path forward for both governance and innovation at the frontier of artificial intelligence.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages core text, 15 pages including references, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11579v2",
    "published_date": "2025-05-16 14:43:12 UTC",
    "updated_date": "2025-11-22 19:02:39 UTC"
  },
  {
    "arxiv_id": "2505.11578v4",
    "title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning",
    "authors": [
      "Peimian Du",
      "Jiabin Liu",
      "Xiaowei Jin",
      "Wangmeng Zuo",
      "Hui Li"
    ],
    "abstract": "This research confronts the challenge of substantial physical equation discrepancies encountered in the generation of spatiotemporal physical fields through data-driven trained models. A spatiotemporal physical field generation model, named HMT-PF, is developed based on the hybrid Mamba-Transformer architecture, incorporating unstructured grid information as input. A fine-tuning block, enhanced with physical information, is introduced to effectively reduce the physical equation discrepancies. The physical equation residuals are computed through a point query mechanism for efficient gradient evaluation, then encoded into latent space for refinement. The fine-tuning process employs a self-supervised learning approach to achieve physical consistency while maintaining essential field characteristics. Results show that the hybrid Mamba-Transformer model achieves good performance in generating spatiotemporal fields, while the physics-informed fine-tuning mechanism further reduces significant physical errors effectively. A MSE-R evaluation method is developed to assess the accuracy and realism of physical field generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11578v4",
    "published_date": "2025-05-16 14:40:56 UTC",
    "updated_date": "2025-06-13 14:02:43 UTC"
  },
  {
    "arxiv_id": "2505.11311v1",
    "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics",
    "authors": [
      "Ardian Selmonaj",
      "Alessandro Antonucci",
      "Adrian Schneider",
      "Michael Rüegsegger",
      "Matthias Sommer"
    ],
    "abstract": "Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Published as a journal chapter in NATO Journal of Science and Technology",
    "pdf_url": "https://arxiv.org/pdf/2505.11311v1",
    "published_date": "2025-05-16 14:36:30 UTC",
    "updated_date": "2025-05-16 14:36:30 UTC"
  },
  {
    "arxiv_id": "2505.14707v1",
    "title": "CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity",
    "authors": [
      "Georgiana Manolache",
      "Gerard Schouten",
      "Joaquin Vanschoren"
    ],
    "abstract": "We present CrypticBio, the largest publicly available multimodal dataset of visually confusing species, specifically curated to support the development of AI models in the context of biodiversity applications. Visually confusing or cryptic species are groups of two or more taxa that are nearly indistinguishable based on visual characteristics alone. While much existing work addresses taxonomic identification in a broad sense, datasets that directly address the morphological confusion of cryptic species are small, manually curated, and target only a single taxon. Thus, the challenge of identifying such subtle differences in a wide range of taxa remains unaddressed. Curated from real-world trends in species misidentification among community annotators of iNaturalist, CrypticBio contains 52K unique cryptic groups spanning 67K species, represented in 166 million images. Rich research-grade image annotations--including scientific, multicultural, and multilingual species terminology, hierarchical taxonomy, spatiotemporal context, and associated cryptic groups--address multimodal AI in biodiversity research. For easy dataset curation, we provide an open-source pipeline CrypticBio-Curate. The multimodal nature of the dataset beyond vision-language arises from the integration of geographical and temporal data as complementary cues to identifying cryptic species. To highlight the importance of the dataset, we benchmark a suite of state-of-the-art foundation models across CrypticBio subsets of common, unseen, endangered, and invasive species, and demonstrate the substantial impact of geographical context on vision-language zero-shot learning for cryptic species. By introducing CrypticBio, we aim to catalyze progress toward real-world-ready biodiversity AI models capable of handling the nuanced challenges of species ambiguity.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "We present CrypticBio, the largest publicly available multimodal dataset of visually confusing species, specifically curated to support the development of AI models for biodiversity identification using images, language and spatiotemporal data",
    "pdf_url": "https://arxiv.org/pdf/2505.14707v1",
    "published_date": "2025-05-16 14:35:56 UTC",
    "updated_date": "2025-05-16 14:35:56 UTC"
  },
  {
    "arxiv_id": "2505.11304v1",
    "title": "Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning",
    "authors": [
      "Shudi Weng",
      "Chao Ren",
      "Ming Xiao",
      "Mikael Skoglund"
    ],
    "abstract": "Federated learning (FL) commonly involves clients with diverse communication and computational capabilities. Such heterogeneity can significantly distort the optimization dynamics and lead to objective inconsistency, where the global model converges to an incorrect stationary point potentially far from the pursued optimum. Despite its critical impact, the joint effect of communication and computation heterogeneity has remained largely unexplored, due to the intrinsic complexity of their interaction. In this paper, we reveal the fundamentally distinct mechanisms through which heterogeneous communication and computation drive inconsistency in FL. To the best of our knowledge, this is the first unified theoretical analysis of general heterogeneous FL, offering a principled understanding of how these two forms of heterogeneity jointly distort the optimization trajectory under arbitrary choices of local solvers. Motivated by these insights, we propose Federated Heterogeneity-Aware Client Sampling, FedACS, a universal method to eliminate all types of objective inconsistency. We theoretically prove that FedACS converges to the correct optimum at a rate of $O(1/\\sqrt{R})$, even in dynamic heterogeneous environments. Extensive experiments across multiple datasets show that FedACS outperforms state-of-the-art and category-specific baselines by 4.3%-36%, while reducing communication costs by 22%-89% and computation loads by 14%-105%, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11304v1",
    "published_date": "2025-05-16 14:31:36 UTC",
    "updated_date": "2025-05-16 14:31:36 UTC"
  },
  {
    "arxiv_id": "2505.11577v2",
    "title": "The Accountability Paradox: How Platform API Restrictions Undermine AI Transparency Mandates",
    "authors": [
      "Florian A. D. Burnat",
      "Brittany I. Davidson"
    ],
    "abstract": "Recent application programming interface (API) restrictions on major social media platforms challenge compliance with the EU Digital Services Act [20], which mandates data access for algorithmic transparency. We develop a structured audit framework to assess the growing misalignment between regulatory requirements and platform implementations. Our comparative analysis of X/Twitter, Reddit, TikTok, and Meta identifies critical ``audit blind-spots'' where platform content moderation and algorithmic amplification remain inaccessible to independent verification. Our findings reveal an ``accountability paradox'': as platforms increasingly rely on AI systems, they simultaneously restrict the capacity for independent oversight. We propose targeted policy interventions aligned with the AI Risk Management Framework of the National Institute of Standards and Technology [80], emphasizing federated access models and enhanced regulatory enforcement.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11577v2",
    "published_date": "2025-05-16 14:30:20 UTC",
    "updated_date": "2026-01-14 12:16:02 UTC"
  },
  {
    "arxiv_id": "2505.11289v2",
    "title": "Meta-World+: An Improved, Standardized, RL Benchmark",
    "authors": [
      "Reginald McLean",
      "Evangelos Chatzaroulas",
      "Luc McCutcheon",
      "Frank Röder",
      "Tianhe Yu",
      "Zhanpeng He",
      "K. R. Zentner",
      "Ryan Julian",
      "J K Terry",
      "Isaac Woungang",
      "Nariman Farsad",
      "Pablo Samuel Castro"
    ],
    "abstract": "Meta-World is widely used for evaluating multi-task and meta-reinforcement learning agents, which are challenged to master diverse skills simultaneously. Since its introduction however, there have been numerous undocumented changes which inhibit a fair comparison of algorithms. This work strives to disambiguate these results from the literature, while also leveraging the past versions of Meta-World to provide insights into multi-task and meta-reinforcement learning benchmark design. Through this process we release a new open-source version of Meta-World (https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility of past results, is more technically ergonomic, and gives users more control over the tasks that are included in a task set.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPs 2025, Datasets and Benchmarks",
    "pdf_url": "https://arxiv.org/pdf/2505.11289v2",
    "published_date": "2025-05-16 14:24:03 UTC",
    "updated_date": "2025-11-21 15:53:08 UTC"
  },
  {
    "arxiv_id": "2505.17052v2",
    "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs",
    "authors": [
      "Jinwoo Park",
      "Seunggeun Cho",
      "Dongsu Han"
    ],
    "abstract": "Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17052v2",
    "published_date": "2025-05-16 14:17:59 UTC",
    "updated_date": "2025-11-18 12:57:29 UTC"
  },
  {
    "arxiv_id": "2505.11277v5",
    "title": "Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning",
    "authors": [
      "Yaorui Shi",
      "Sihang Li",
      "Chang Wu",
      "Zhiyuan Liu",
      "Junfeng Fang",
      "Hengxing Cai",
      "An Zhang",
      "Xiang Wang"
    ],
    "abstract": "Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new \"search-and-refine-during-think\" paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11277v5",
    "published_date": "2025-05-16 14:11:29 UTC",
    "updated_date": "2025-09-19 12:21:03 UTC"
  },
  {
    "arxiv_id": "2505.11275v4",
    "title": "TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs",
    "authors": [
      "Pengju Xu",
      "Yan Wang",
      "Shuyuan Zhang",
      "Xuan Zhou",
      "Xin Li",
      "Yue Yuan",
      "Fengzhao Li",
      "Shunyuan Zhou",
      "Xingyu Wang",
      "Yi Zhang",
      "Haiying Zhao"
    ],
    "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) have significantly enhanced the ability of artificial intelligence systems to understand and generate multimodal content. However, these models often exhibit limited effectiveness when applied to non-Western cultural contexts, which raises concerns about their wider applicability. To address this limitation, we propose the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a bilingual (i.e., Chinese and English) Visual Question Answering (VQA) benchmark specifically designed for assessing the understanding of traditional Chinese culture by MLLMs. TCC-Bench comprises culturally rich and visually diverse data, incorporating images from museum artifacts, everyday life scenes, comics, and other culturally significant contexts. We adopt a semi-automated pipeline that utilizes GPT-4o in text-only mode to generate candidate questions, followed by human curation to ensure data quality and avoid potential data leakage. The benchmark also avoids language bias by preventing direct disclosure of cultural concepts within question texts. Experimental evaluations across a wide range of MLLMs demonstrate that current models still face significant challenges when reasoning about culturally grounded visual content. The results highlight the need for further research in developing culturally inclusive and context-aware multimodal systems. The code and data can be found at: https://tcc-bench.github.io/.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.MM",
    "comment": "There are issues with the paper",
    "pdf_url": "https://arxiv.org/pdf/2505.11275v4",
    "published_date": "2025-05-16 14:10:41 UTC",
    "updated_date": "2025-12-02 11:23:03 UTC"
  },
  {
    "arxiv_id": "2505.11274v5",
    "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning",
    "authors": [
      "Zheng Li",
      "Qingxiu Dong",
      "Jingyuan Ma",
      "Di Zhang",
      "Kai Jia",
      "Zhifang Sui"
    ],
    "abstract": "Recently, large reasoning models demonstrate exceptional performance on various tasks. However, reasoning models always consume excessive tokens even for simple queries, leading to resource waste and prolonged user latency. To address this challenge, we propose SelfBudgeter - a self-adaptive reasoning strategy for efficient and controllable reasoning. Specifically, we first train the model to self-estimate the required reasoning budget based on the query. We then introduce budget-guided GPRO for reinforcement learning, which effectively maintains accuracy while reducing output length. Experimental results demonstrate that SelfBudgeter dynamically allocates budgets according to problem complexity, achieving an average response length compression of 61% on math reasoning tasks while maintaining accuracy. Furthermore, SelfBudgeter allows users to see how long generation will take and decide whether to continue or stop. Additionally, users can directly control the reasoning length by setting token budgets upfront.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11274v5",
    "published_date": "2025-05-16 14:08:04 UTC",
    "updated_date": "2026-01-09 03:54:39 UTC"
  },
  {
    "arxiv_id": "2505.11271v1",
    "title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models",
    "authors": [
      "Camille Couturier",
      "Spyros Mastorakis",
      "Haiying Shen",
      "Saravan Rajmohan",
      "Victor Rühle"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed across edge and cloud platforms for real-time question-answering and retrieval-augmented generation. However, processing lengthy contexts in distributed systems incurs high computational overhead, memory usage, and network bandwidth. This paper introduces a novel semantic caching approach for storing and reusing intermediate contextual summaries, enabling efficient information reuse across similar queries in LLM-based QA workflows. Our method reduces redundant computations by up to 50-60% while maintaining answer accuracy comparable to full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a synthetic ArXiv dataset. This approach balances computational cost and response quality, critical for real-time AI assistants.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear in the proceedings",
    "pdf_url": "https://arxiv.org/pdf/2505.11271v1",
    "published_date": "2025-05-16 14:04:31 UTC",
    "updated_date": "2025-05-16 14:04:31 UTC"
  },
  {
    "arxiv_id": "2505.11270v1",
    "title": "TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes",
    "authors": [
      "Chao Zhang",
      "Shaolei Zhang",
      "Quehuan Liu",
      "Sibei Chen",
      "Tong Li",
      "Ju Fan"
    ],
    "abstract": "The variety of data in data lakes presents significant challenges for data analytics, as data scientists must simultaneously analyze multi-modal data, including structured, semi-structured, and unstructured data. While Large Language Models (LLMs) have demonstrated promising capabilities, they still remain inadequate for multi-modal data analytics in terms of accuracy, efficiency, and freshness. First, current natural language (NL) or SQL-like query languages may struggle to precisely and comprehensively capture users' analytical intent. Second, relying on a single unified LLM to process diverse data modalities often leads to substantial inference overhead. Third, data stored in data lakes may be incomplete or outdated, making it essential to integrate external open-domain knowledge to generate timely and relevant analytics results.\n  In this paper, we envision a new multi-modal data analytics system. Specifically, we propose a novel architecture built upon the Model Context Protocol (MCP), an emerging paradigm that enables LLMs to collaborate with knowledgeable agents. First, we define a semantic operator hierarchy tailored for querying multi-modal data in data lakes and develop an AI-agent-powered NL2Operator translator to bridge user intent and analytical execution. Next, we introduce an MCP-based execution framework, in which each MCP server hosts specialized foundation models optimized for specific data modalities. This design enhances both accuracy and efficiency, while supporting high scalability through modular deployment. Finally, we propose a updating mechanism by harnessing the deep research and machine unlearning techniques to refresh the data lakes and LLM knowledges, with the goal of balancing the data freshness and inference efficiency.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11270v1",
    "published_date": "2025-05-16 14:03:30 UTC",
    "updated_date": "2025-05-16 14:03:30 UTC"
  },
  {
    "arxiv_id": "2506.01980v1",
    "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance",
    "authors": [
      "Lianhao Yin",
      "Ozanan Meireles",
      "Guy Rosman",
      "Daniela Rus"
    ],
    "abstract": "Real-time video understanding is critical to guide procedures in minimally invasive surgery (MIS). However, supervised learning approaches require large, annotated datasets that are scarce due to annotation efforts that are prohibitive, e.g., in medical fields. Although self-supervision methods can address such limitations, current self-supervised methods often fail to capture structural and physical information in a form that generalizes across tasks. We propose Compress-to-Explore (C2E), a novel self-supervised framework that leverages Kolmogorov complexity to learn compact, informative representations from surgical videos. C2E uses entropy-maximizing decoders to compress images while preserving clinically relevant details, improving encoder performance without labeled data. Trained on large-scale unlabeled surgical datasets, C2E demonstrates strong generalization across a variety of surgical ML tasks, such as workflow classification, tool-tissue interaction classification, segmentation, and diagnosis tasks, providing improved performance as a surgical visual foundation model. As we further show in the paper, the model's internal compact representation better disentangles features from different structural parts of images. The resulting performance improvements highlight the yet untapped potential of self-supervised learning to enhance surgical AI and improve outcomes in MIS.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.01980v1",
    "published_date": "2025-05-16 14:02:24 UTC",
    "updated_date": "2025-05-16 14:02:24 UTC"
  },
  {
    "arxiv_id": "2505.11267v1",
    "title": "Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity",
    "authors": [
      "Wuzhou Quan",
      "Mingqiang Wei",
      "Jinhui Tang"
    ],
    "abstract": "Hyperspectral image (HSI) representation is fundamentally challenged by pervasive non-uniformity, where spectral dependencies, spatial continuity, and feature efficiency exhibit complex and often conflicting behaviors. Most existing models rely on a unified processing paradigm that assumes homogeneity across dimensions, leading to suboptimal performance and biased representations. To address this, we propose FairHyp, a fairness-directed framework that explicitly disentangles and resolves the threefold non-uniformity through cooperative yet specialized modules. We introduce a Runge-Kutta-inspired spatial variability adapter to restore spatial coherence under resolution discrepancies, a multi-receptive field convolution module with sparse-aware refinement to enhance discriminative features while respecting inherent sparsity, and a spectral-context state space model that captures stable and long-range spectral dependencies via bidirectional Mamba scanning and statistical aggregation. Unlike one-size-fits-all solutions, FairHyp achieves dimension-specific adaptation while preserving global consistency and mutual reinforcement. This design is grounded in the view that non-uniformity arises from the intrinsic structure of HSI representations, rather than any particular task setting. To validate this, we apply FairHyp across four representative tasks including classification, denoising, super-resolution, and inpaintin, demonstrating its effectiveness in modeling a shared structural flaw. Extensive experiments show that FairHyp consistently outperforms state-of-the-art methods under varied imaging conditions. Our findings redefine fairness as a structural necessity in HSI modeling and offer a new paradigm for balancing adaptability, efficiency, and fidelity in high-dimensional vision tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11267v1",
    "published_date": "2025-05-16 14:00:11 UTC",
    "updated_date": "2025-05-16 14:00:11 UTC"
  },
  {
    "arxiv_id": "2505.11576v3",
    "title": "Concept-Guided Interpretability via Neural Chunking",
    "authors": [
      "Shuchen Wu",
      "Stephan Alaniz",
      "Shyamgopal Karthik",
      "Peter Dayan",
      "Eric Schulz",
      "Zeynep Akata"
    ],
    "abstract": "Neural networks are often described as black boxes, reflecting the significant challenge of understanding their internal workings and interactions. We propose a different perspective that challenges the prevailing view: rather than being inscrutable, neural networks exhibit patterns in their raw population activity that mirror regularities in the training data. We refer to this as the Reflection Hypothesis and provide evidence for this phenomenon in both simple recurrent neural networks (RNNs) and complex large language models (LLMs). Building on this insight, we propose to leverage our cognitive tendency of chunking to segment high-dimensional neural population dynamics into interpretable units that reflect underlying concepts. We propose three methods to extract recurring chunks on a neural population level, complementing each other based on label availability and neural data dimensionality. Discrete sequence chunking (DSC) learns a dictionary of entities in a lower-dimensional neural space; population averaging (PA) extracts recurring entities that correspond to known labels; and unsupervised chunk discovery (UCD) can be used when labels are absent. We demonstrate the effectiveness of these methods in extracting concept-encoding entities agnostic to model architectures. These concepts can be both concrete (words), abstract (POS tags), or structural (narrative schema). Additionally, we show that extracted chunks play a causal role in network behavior, as grafting them leads to controlled and predictable changes in the model's behavior. Our work points to a new direction for interpretability, one that harnesses both cognitive principles and the structure of naturalistic data to reveal the hidden computations of complex learning systems, gradually transforming them from black boxes into systems we can begin to understand.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11576v3",
    "published_date": "2025-05-16 13:49:43 UTC",
    "updated_date": "2025-10-21 21:20:23 UTC"
  },
  {
    "arxiv_id": "2505.13508v2",
    "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
    "authors": [
      "Zijia Liu",
      "Peixuan Han",
      "Haofei Yu",
      "Haoru Li",
      "Jiaxuan You"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \\textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \\textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \\textit{Time-R1} checkpoints.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13508v2",
    "published_date": "2025-05-16 13:46:28 UTC",
    "updated_date": "2025-06-03 05:30:14 UTC"
  },
  {
    "arxiv_id": "2505.11247v2",
    "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
    "authors": [
      "Mingxing Peng",
      "Yuting Xie",
      "Xusen Guo",
      "Ruoyu Yao",
      "Hai Yang",
      "Jun Ma"
    ],
    "abstract": "Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11247v2",
    "published_date": "2025-05-16 13:41:05 UTC",
    "updated_date": "2025-08-17 05:29:45 UTC"
  },
  {
    "arxiv_id": "2505.11243v2",
    "title": "A Set-Sequence Model for Time Series",
    "authors": [
      "Elliot L. Epstein",
      "Apaar Sadhwani",
      "Kay Giesecke"
    ],
    "abstract": "Many prediction problems across science and engineering, especially in finance and economics, involve large cross-sections of individual time series, where each unit (e.g., a loan, stock, or customer) is driven by unit-level features and latent cross-sectional dynamics. While sequence models have advanced per-unit temporal prediction, capturing cross-sectional effects often still relies on hand-crafted summary features. We propose Set-Sequence, a model that learns cross-sectional structure directly, enhancing expressivity and eliminating manual feature engineering. At each time step, a permutation-invariant Set module summarizes the unit set; a Sequence module then models each unit's dynamics conditioned on both its features and the learned summary. The architecture accommodates unaligned series, supports varying numbers of units at inference, integrates with standard sequence backbones (e.g., Transformers), and scales linearly in cross-sectional size. Across a synthetic contagion task and two large-scale real-world applications, equity portfolio optimization and loan risk prediction, Set-Sequence significantly outperforms strong baselines, delivering higher Sharpe ratios, improved AUCs, and interpretable cross-sectional summaries.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the Workshop on Financial AI at ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11243v2",
    "published_date": "2025-05-16 13:36:07 UTC",
    "updated_date": "2025-10-13 17:44:53 UTC"
  },
  {
    "arxiv_id": "2505.17051v1",
    "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models",
    "authors": [
      "Bernd Huber",
      "Ghazal Fazelnia",
      "Andreas Damianou",
      "Sebastian Peleato",
      "Max Lefarov",
      "Praveen Ravichandran",
      "Marco De Nadai",
      "Mounia Lalmas-Roellke",
      "Paul N. Bennett"
    ],
    "abstract": "Large language models (LLMs) excel at generating contextually relevant content. However, tailoring these outputs to individual users for effective personalization is a significant challenge. While rich user-specific information often exists as pre-existing user representations, such as embeddings learned from preferences or behaviors, current methods to leverage these for LLM personalization typically require costly fine-tuning or token-heavy prompting. We propose Embedding-to-Prefix (E2P), a parameter-efficient method that injects pre-computed context embeddings into an LLM's hidden representation space through a learned projection to a single soft token prefix. This enables effective personalization while keeping the backbone model frozen and avoiding expensive adaptation techniques. We evaluate E2P across two public datasets and in a production setting: dialogue personalization on Persona-Chat, contextual headline generation on PENS, and large-scale personalization for music and podcast consumption. Results show that E2P preserves contextual signals and achieves strong performance with minimal computational overhead, offering a scalable, efficient solution for contextualizing generative AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17051v1",
    "published_date": "2025-05-16 13:34:25 UTC",
    "updated_date": "2025-05-16 13:34:25 UTC"
  },
  {
    "arxiv_id": "2505.11227v2",
    "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
    "authors": [
      "Zhangying Feng",
      "Qianglong Chen",
      "Ning Lu",
      "Yongqian Li",
      "Siqi Cheng",
      "Shuangmu Peng",
      "Duyu Tang",
      "Shengcai Liu",
      "Zhirui Zhang"
    ],
    "abstract": "The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025, camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2505.11227v2",
    "published_date": "2025-05-16 13:23:26 UTC",
    "updated_date": "2025-12-08 02:52:13 UTC"
  },
  {
    "arxiv_id": "2505.11225v2",
    "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
    "authors": [
      "Chengyu Huang",
      "Zhengxin Zhang",
      "Claire Cardie"
    ],
    "abstract": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11225v2",
    "published_date": "2025-05-16 13:21:28 UTC",
    "updated_date": "2025-11-11 05:07:45 UTC"
  },
  {
    "arxiv_id": "2505.11217v2",
    "title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization",
    "authors": [
      "Yanhao Jia",
      "Ji Xie",
      "S Jivaganesh",
      "Hao Li",
      "Xu Wu",
      "Mengmi Zhang"
    ],
    "abstract": "Imagine hearing a dog bark and turning toward the sound only to see a parked car, while the real, silent dog sits elsewhere. Such sensory conflicts test perception, yet humans reliably resolve them by prioritizing sound over misleading visuals. Despite advances in multimodal AI integrating vision and audio, little is known about how these systems handle cross-modal conflicts or whether they favor one modality. In this study, we systematically examine modality bias and conflict resolution in AI sound localization. We assess leading multimodal models and benchmark them against human performance in psychophysics experiments across six audiovisual conditions, including congruent, conflicting, and absent cues. Humans consistently outperform AI, demonstrating superior resilience to conflicting or missing visuals by relying on auditory information. In contrast, AI models often default to visual input, degrading performance to near chance levels. To address this, we propose a neuroscience-inspired model, EchoPin, which uses a stereo audio-image dataset generated via 3D simulations. Even with limited training data, EchoPin surpasses existing benchmarks. Notably, it also mirrors human-like horizontal localization bias favoring left-right precision-likely due to the stereo audio structure reflecting human ear placement. These findings underscore how sensory input quality and system architecture shape multimodal representation accuracy.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "NeurIPS 2025, Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2505.11217v2",
    "published_date": "2025-05-16 13:13:25 UTC",
    "updated_date": "2025-10-24 14:21:57 UTC"
  },
  {
    "arxiv_id": "2505.11211v2",
    "title": "Bayesian Hierarchical Invariant Prediction",
    "authors": [
      "Francisco Madaleno",
      "Pernille Julie Viuff Sand",
      "Francisco C. Pereira",
      "Sergio Hernan Garrido Mejia"
    ],
    "abstract": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We leverage the hierarchical structure to explicitly test invariance of causal mechanisms under heterogeneous data, resulting in improved computational scalability for a larger number of predictors compared to ICP. Moreover, given its Bayesian nature BHIP enables the use of prior information. In this paper, we test two sparsity inducing priors: horseshoe and spike-and-slab, both of which allow us a more reliable identification of causal features. We test BHIP in synthetic and real-world data showing its potential as an alternative inference method to ICP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11211v2",
    "published_date": "2025-05-16 13:06:25 UTC",
    "updated_date": "2025-07-08 10:51:36 UTC"
  },
  {
    "arxiv_id": "2505.11208v1",
    "title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning",
    "authors": [
      "Dongjun Kim",
      "Junwoo Park",
      "Chaehyeon Shin",
      "Jaeheon Jung",
      "Kyungho Shin",
      "Seungheon Baek",
      "Sanghyuk Heo",
      "Woongrae Kim",
      "Inchul Jeong",
      "Joohwan Cho",
      "Jongsun Park"
    ],
    "abstract": "Analog/mixed-signal circuit design encounters significant challenges due to performance degradation from process, voltage, and temperature (PVT) variations. To achieve commercial-grade reliability, iterative manual design revisions and extensive statistical simulations are required. While several studies have aimed to automate variation aware analog design to reduce time-to-market, the substantial mismatches in real-world wafers have not been thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing framework that effectively manages the impact of diverse random mismatches to improve robustness against PVT variations. In the proposed approach, risk-sensitive reinforcement learning is leveraged to account for the reliability bound affected by PVT variations, and ensemble-based critic is introduced to achieve sample-efficient learning. For design verification, we also propose $μ$-$σ$ evaluation and simulation reordering method to reduce simulation costs of identifying failed designs. GLOVA supports verification through industrial-level PVT variation evaluation methods, including corner simulation as well as global and local Monte Carlo (MC) simulations. Compared to previous state-of-the-art variation-aware analog sizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample efficiency and 76.0$\\times$ reduction in time.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for DAC 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11208v1",
    "published_date": "2025-05-16 13:05:45 UTC",
    "updated_date": "2025-05-16 13:05:45 UTC"
  },
  {
    "arxiv_id": "2505.11204v1",
    "title": "RanDeS: Randomized Delta Superposition for Multi-Model Compression",
    "authors": [
      "Hangyu Zhou",
      "Aaron Gokaslan",
      "Volodymyr Kuleshov",
      "Bharath Hariharan"
    ],
    "abstract": "From a multi-model compression perspective, model merging enables memory-efficient serving of multiple models fine-tuned from the same base, but suffers from degraded performance due to interference among their task-specific parameter adjustments (i.e., deltas). In this paper, we reformulate model merging as a compress-and-retrieve scheme, revealing that the task interference arises from the summation of irrelevant deltas during model retrieval. To address this issue, we use random orthogonal transformations to decorrelate these vectors into self-cancellation. We show that this approach drastically reduces interference, improving performance across both vision and language tasks. Since these transformations are fully defined by random seeds, adding new models requires no extra memory. Further, their data- and model-agnostic nature enables easy addition or removal of models with minimal compute overhead, supporting efficient and flexible multi-model serving.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "https://github.com/Zhou-Hangyu/randes",
    "pdf_url": "https://arxiv.org/pdf/2505.11204v1",
    "published_date": "2025-05-16 13:02:12 UTC",
    "updated_date": "2025-05-16 13:02:12 UTC"
  },
  {
    "arxiv_id": "2505.11200v1",
    "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese",
    "authors": [
      "Xihuai Wang",
      "Ziyi Zhao",
      "Siyu Ren",
      "Shao Zhang",
      "Song Li",
      "Xiaoyu Li",
      "Ziwen Wang",
      "Lin Qiu",
      "Guanglu Wan",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Weinan Zhang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have significantly improved text-to-speech (TTS) systems, enhancing control over speech style, naturalness, and emotional expression, which brings TTS Systems closer to human-level performance. Although the Mean Opinion Score (MOS) remains the standard for TTS System evaluation, it suffers from subjectivity, environmental inconsistencies, and limited interpretability. Existing evaluation datasets also lack a multi-dimensional design, often neglecting factors such as speaking styles, context diversity, and trap utterances, which is particularly evident in Chinese TTS evaluation. To address these challenges, we introduce the Audio Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on complex MOS scales or direct model comparisons, ATT asks evaluators to judge whether a voice sounds human. This simplification reduces rating bias and improves evaluation robustness. To further support rapid model development, we also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for automatic evaluation. Experimental results show that ATT effectively differentiates models across specific capability dimensions using its multi-dimensional design. Auto-ATT also demonstrates strong alignment with human evaluations, confirming its value as a fast and reliable assessment tool. The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face Collection (https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2505.11200v1",
    "published_date": "2025-05-16 12:57:23 UTC",
    "updated_date": "2025-05-16 12:57:23 UTC"
  },
  {
    "arxiv_id": "2505.11198v1",
    "title": "User-centric Music Recommendations",
    "authors": [
      "Jaime Ramirez Castillo",
      "M. Julia Flores",
      "Ann E. Nicholson"
    ],
    "abstract": "This work presents a user-centric recommendation framework, designed as a pipeline with four distinct, connected, and customizable phases. These phases are intended to improve explainability and boost user engagement.\n  We have collected the historical Last.fm track playback records of a single user over approximately 15 years. The collected dataset includes more than 90,000 playbacks and approximately 14,000 unique tracks.\n  From track playback records, we have created a dataset of user temporal contexts (each row is a specific moment when the user listened to certain music descriptors). As music descriptors, we have used community-contributed Last.fm tags and Spotify audio features. They represent the music that, throughout years, the user has been listening to.\n  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the day), we predict the Spotify audio features that best fit the user preferences in that particular moment. Finally, we use the predicted audio features to find tracks similar to these features. The final aim is to recommend (and discover) tracks that the user may feel like listening to at a particular moment.\n  For our initial study case, we have chosen to predict only a single audio feature target: danceability. The framework, however, allows to include more target variables.\n  The ability to learn the musical habits from a single user can be quite powerful, and this framework could be extended to other users.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted for the 16th Bayesian Modelling Applications Workshop (@UAI2022) (BMAW 2022)",
    "pdf_url": "https://arxiv.org/pdf/2505.11198v1",
    "published_date": "2025-05-16 12:56:40 UTC",
    "updated_date": "2025-05-16 12:56:40 UTC"
  },
  {
    "arxiv_id": "2505.11192v4",
    "title": "FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Alignment",
    "authors": [
      "Myunsoo Kim",
      "Seong-Woong Shim",
      "Byung-Jun Lee"
    ],
    "abstract": "False negatives pose a critical challenge in vision-language pretraining (VLP) due to the many-to-many correspondence between images and texts in large-scale datasets. These false negatives introduce conflicting supervision signals that degrade the learned embedding space and diminish the effectiveness of hard negative sampling. In this paper, we propose FALCON (False-negative Aware Learning of COntrastive Negatives), a learning-based mini-batch construction strategy that adaptively balances the trade-off between hard and false negatives during VLP. Rather than relying on fixed heuristics, FALCON employs a negative mining scheduler that dynamically selects negative samples of appropriate hardness for each anchor instance during mini-batch construction, guided by a proxy for cross-modal alignment improvement. Experimental results demonstrate that FALCON significantly improves performance across three vision-language learning frameworks (ALBEF, BLIP-2, SigLIP-2) and a broad range of downstream tasks and evaluation settings, underscoring its effectiveness and robustness in mitigating the impact of false negatives.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11192v4",
    "published_date": "2025-05-16 12:50:05 UTC",
    "updated_date": "2025-11-11 12:55:16 UTC"
  },
  {
    "arxiv_id": "2505.11191v2",
    "title": "Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration",
    "authors": [
      "Kasra Borazjani",
      "Payam Abdisarabshali",
      "Fardis Nadimi",
      "Naji Khosravan",
      "Minghui Liwang",
      "Xianbin Wang",
      "Yiguang Hong",
      "Seyyedali Hosseinalipour"
    ],
    "abstract": "As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: multi-modal multi-task foundation models (M3T-FMs) provide a pathway toward generalization across tasks and modalities, whereas federated learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied AI environments. In this vision paper, we introduce multi-modal multi-task federated foundation models (M3T-FFMs) for embodied AI, a new paradigm that unifies the strengths of M3T-FMs with the privacy-preserving distributed training nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of M3T-FFMs in embodied AI ecosystems under a unified framework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying M3T-FFMs in embodied AI systems, along with the associated trade-offs. Finally, we present a prototype implementation of M3T-FFMs and evaluate their energy and latency performance.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for Publication in IEEE Internet of Things Magazine, 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11191v2",
    "published_date": "2025-05-16 12:49:36 UTC",
    "updated_date": "2025-09-05 18:00:00 UTC"
  },
  {
    "arxiv_id": "2505.11189v2",
    "title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP",
    "authors": [
      "Francesco Sovrano"
    ],
    "abstract": "Large language models (LLMs) can amplify misinformation, undermining societal goals like the UN SDGs. We study three documented drivers of misinformation (valence framing, information overload, and oversimplification) which are often shaped by one's default beliefs. Building on evidence that LLMs encode such defaults (e.g., \"joy is positive,\" \"math is complex\") and can act as \"bags of heuristics,\" we ask: can general belief-driven heuristics behind misinformative behaviour be recovered from LLMs as clear rules? A key obstacle is that global rule-extraction methods in explainable AI (XAI) are built for numerical inputs/outputs, not text. We address this by eliciting global LLM beliefs and mapping them to numerical scores via statistically reliable abstractions, thereby enabling off-the-shelf global XAI to detect belief-related heuristics in LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear heuristics of increasing complexity (univariate, conjunctive, nonconvex) into popular LLMs (ChatGPT and Llama) via system instructions. This way, we find that RuleFit under-detects non-univariate biases, while global SHAP better approximates conjunctive ones but does not yield actionable rules. To bridge this gap, we propose RuleSHAP, a rule-extraction algorithm that couples global SHAP-value aggregations with rule induction to better capture non-univariate bias, improving heuristics detection over RuleFit by +94% (MRR@1) on average. Our results provide a practical pathway for revealing belief-driven biases in LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11189v2",
    "published_date": "2025-05-16 12:48:44 UTC",
    "updated_date": "2025-09-23 15:19:17 UTC"
  },
  {
    "arxiv_id": "2505.11182v1",
    "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning",
    "authors": [
      "Yuzhuo Dai",
      "Jiaqi Jin",
      "Zhibin Dong",
      "Siwei Wang",
      "Xinwang Liu",
      "En Zhu",
      "Xihong Yang",
      "Xinbiao Gan",
      "Yu Feng"
    ],
    "abstract": "In incomplete multi-view clustering (IMVC), missing data induce prototype shifts within views and semantic inconsistencies across views. A feasible solution is to explore cross-view consistency in paired complete observations, further imputing and aligning the similarity relationships inherently shared across views. Nevertheless, existing methods are constrained by two-tiered limitations: (1) Neither instance- nor cluster-level consistency learning construct a semantic space shared across views to learn consensus semantics. The former enforces cross-view instances alignment, and wrongly regards unpaired observations with semantic consistency as negative pairs; the latter focuses on cross-view cluster counterparts while coarsely handling fine-grained intra-cluster relationships within views. (2) Excessive reliance on consistency results in unreliable imputation and alignment without incorporating view-specific cluster information. Thus, we propose an IMVC framework, imputation- and alignment-free for consensus semantics learning (FreeCSL). To bridge semantic gaps across all observations, we learn consensus prototypes from available data to discover a shared space, where semantically similar observations are pulled closer for consensus semantics learning. To capture semantic relationships within specific views, we design a heuristic graph clustering based on modularity to recover cluster structure with intra-cluster compactness and inter-cluster separation for cluster semantics enhancement. Extensive experiments demonstrate, compared to state-of-the-art competitors, FreeCSL achieves more confident and robust assignments on IMVC task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper has been accepted by the 42nd CVPR 2025. The main text has 9 pages, including 8 figures and 4 tables. The appendix has 8 pages, with 10 figures and 6 tables. The reference list has 3 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11182v1",
    "published_date": "2025-05-16 12:37:10 UTC",
    "updated_date": "2025-05-16 12:37:10 UTC"
  },
  {
    "arxiv_id": "2505.11181v1",
    "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning",
    "authors": [
      "Jae Myung Kim",
      "Stephan Alaniz",
      "Cordelia Schmid",
      "Zeynep Akata"
    ],
    "abstract": "Humans can easily tell if an attribute (also called state) is realistic, i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In Open-World Compositional Zero-Shot Learning, when all possible state-object combinations are considered as unseen classes, zero-shot predictors tend to perform poorly. Our work focuses on using external auxiliary knowledge to determine the feasibility of state-object combinations. Our Feasibility with Language Model (FLM) is a simple and effective approach that leverages Large Language Models (LLMs) to better comprehend the semantic relationships between states and objects. FLM involves querying an LLM about the feasibility of a given pair and retrieving the output logit for the positive answer. To mitigate potential misguidance of the LLM given that many of the state-object compositions are rare or completely infeasible, we observe that the in-context learning ability of LLMs is essential. We present an extensive study identifying Vicuna and ChatGPT as best performing, and we demonstrate that our FLM consistently improves OW-CZSL performance across all three benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ECCV Workshop in OOD-CV, 2024",
    "pdf_url": "https://arxiv.org/pdf/2505.11181v1",
    "published_date": "2025-05-16 12:37:08 UTC",
    "updated_date": "2025-05-16 12:37:08 UTC"
  },
  {
    "arxiv_id": "2505.11178v1",
    "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback",
    "authors": [
      "Yixin Wan",
      "Kai-Wei Chang"
    ],
    "abstract": "State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest's feedback as preference signals to improve diffusion models' compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11178v1",
    "published_date": "2025-05-16 12:23:58 UTC",
    "updated_date": "2025-05-16 12:23:58 UTC"
  },
  {
    "arxiv_id": "2505.11177v1",
    "title": "Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline",
    "authors": [
      "Hrishit Madhavi",
      "Jacob Cherian",
      "Yuvraj Khamkar",
      "Dhananjay Bhagat"
    ],
    "abstract": "This paper presents an end-to-end suite for multilingual information extraction and processing from image-based documents. The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil, and then a pipeline involving large language model APIs (Gemini) for cross-lingual translation, abstractive summarization, and re-translation into a target language. Additional modules add sentiment analysis (TensorFlow), topic classification (Transformers), and date extraction (Regex) for better document comprehension. Made available in an accessible Gradio interface, the current research shows a real-world application of libraries, models, and APIs to close the language gap and enhance access to information in image media across different linguistic environments",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 7 figures, direct arXiv submission",
    "pdf_url": "https://arxiv.org/pdf/2505.11177v1",
    "published_date": "2025-05-16 12:20:37 UTC",
    "updated_date": "2025-05-16 12:20:37 UTC"
  },
  {
    "arxiv_id": "2505.11176v2",
    "title": "Enhancing and Scaling Search Query Datasets for Recommendation Systems",
    "authors": [
      "Aaron Rodrigues",
      "Mahmood Hegazy",
      "Azzam Naeem"
    ],
    "abstract": "This paper presents a deployed, production-grade system designed to enhance and scale search query datasets for intent-based recommendation systems in digital banking. In real-world environments, the growing volume and complexity of user intents create substantial challenges for data management, resulting in suboptimal recommendations and delayed product onboarding. To overcome these challenges, our approach shifts the focus from model-centric enhancements to automated, data-centric strategies. The proposed system integrates three core modules: Synthetic Query Generation, Intent Disambiguation, and Intent Gap Analysis. Synthetic Query Generation produces diverse and realistic user queries. Our experiments reveal no statistically significant difference when using synthetic data for Clinc150, while Banking77 and a proprietary dataset show significant differences. We dig into the underlying factors driving these variations, demonstrating that our approach effectively alleviates the cold start problem (i.e. the challenge of recommending new products with limited historical data). Intent Disambiguation refines broad and overlapping intent categories into precise subintents, achieving an F1 score of 0.863 $\\pm$ 0.127 against expert reannotations and leading to clearer differentiation and more precise recommendation mapping. Meanwhile, Intent Gap Analysis identifies latent customer needs by extracting novel intents from unlabeled queries; recovery rates reach up to 71\\% in controlled evaluations. Deployed in a live banking environment, our system demonstrates significant improvements in recommendation precision and operation agility, ultimately delivering enhanced user experiences and strategic business benefits. This work underscores the role of high-quality, scalable data in modern AI-driven applications and advocates a proactive approach to data enhancement as a key driver of value.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11176v2",
    "published_date": "2025-05-16 12:20:31 UTC",
    "updated_date": "2025-08-22 16:07:58 UTC"
  },
  {
    "arxiv_id": "2505.11175v2",
    "title": "Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition",
    "authors": [
      "Bo Yue",
      "Shuqi Guo",
      "Kaiyu Hu",
      "Chujiao Wang",
      "Benyou Wang",
      "Kui Jia",
      "Guiliang Liu"
    ],
    "abstract": "Generative skill acquisition enables embodied agents to actively learn a scalable and evolving repertoire of control skills, crucial for the advancement of large decision models. While prior approaches often rely on supervision signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D environments remains unclear; exhaustive evaluation incurs substantial computational costs, significantly hindering the efficiency of skill learning. Inspired by recent successes in verification models for mathematical reasoning, we propose VERGSA (Verifying Embodied Reasoning in Generative Skill Acquisition), a framework that systematically integrates real-time verification principles into embodied skill learning. VERGSA establishes 1) a seamless extension from verification of mathematical reasoning into embodied learning by dynamically incorporating contextually relevant tasks into prompts and defining success metrics for both subtasks and overall tasks, and 2) an automated, scalable reward labeling scheme that synthesizes dense reward signals by iteratively finalizing the contribution of scene configuration and subtask learning to overall skill acquisition. To the best of our knowledge, this approach constitutes the first comprehensive training dataset for verification-driven generative skill acquisition, eliminating arduous manual reward engineering. Experiments validate the efficacy of our approach: 1) the exemplar task pool improves the average task success rates by 21%, 2) our verification model boosts success rates by 24% for novel tasks and 36% for encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification quality.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11175v2",
    "published_date": "2025-05-16 12:19:13 UTC",
    "updated_date": "2025-05-19 05:14:55 UTC"
  },
  {
    "arxiv_id": "2505.11574v4",
    "title": "Quantization Meets Reasoning: Exploring and Mitigating Degradation of Low-Bit LLMs in Mathematical Reasoning",
    "authors": [
      "Zhen Li",
      "Yupeng Su",
      "Songmiao Wang",
      "Runming Yang",
      "Congkai Xie",
      "Aofan Liu",
      "Ming Li",
      "Jiannong Cao",
      "Yuan Xie",
      "Ngai Wong",
      "Hongxia Yang"
    ],
    "abstract": "Low-bit post-training quantization (PTQ) is a practical route to deploy reasoning-capable LLMs under tight memory and latency budgets, yet it can markedly impair mathematical reasoning (drops up to 69.81% in our harder settings). We address two deployment-critical questions with process-level precision: Where along a step-structured solution does degradation first arise? How to mitigate it while staying in the low-bit regime? Across widely used PTQ methods (AWQ, GPTQ, SmoothQuant), open-source model families (Qwen, LLaMA; 0.5--7B), and math reasoning benchmarks (GSM8K, MATH, AIME), we perform format-aligned chain-of-thought with step-aligned attribution and uncover two robust regularities: (i) PTQ disproportionately elevates method and execution errors relative to high-level conceptual mistakes; and (ii) failures emerge early, with the first vulnerable step flipping and cascading to the final answer. These regularities suggest a general intervention principle: restore local token-level margins exactly at the earliest failure frontier. We instantiate this principle as a lightweight measure$\\rightarrow$locate$\\rightarrow$restore loop that operates directly on the quantized model: detect the first faulty step, construct our \"Silver Bullet\" datasets, and apply small-scale supervised/preference tuning. In our settings, as few as 332 curated examples and 3--5 minutes of compute on a single GPU recover 4-bit weight math reasoning toward the full-precision baseline while preserving PTQ efficiency. Our framework is quantizer- and architecture-agnostic within the evaluated regimes, and turns low-bit degradation from a global accuracy problem into a local, reproducible process intervention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11574v4",
    "published_date": "2025-05-16 12:11:40 UTC",
    "updated_date": "2026-01-20 15:56:29 UTC"
  },
  {
    "arxiv_id": "2505.11168v1",
    "title": "CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer",
    "authors": [
      "Xinran Li",
      "Yu Liu",
      "Xiujuan Xu",
      "Xiaowei Zhao"
    ],
    "abstract": "The automatic diagnosis of chest diseases is a popular and challenging task. Most current methods are based on convolutional neural networks (CNNs), which focus on local features while neglecting global features. Recently, self-attention mechanisms have been introduced into the field of computer vision, demonstrating superior performance. Therefore, this paper proposes an effective model, CheX-DS, for classifying long-tail multi-label data in the medical field of chest X-rays. The model is based on the excellent CNN model DenseNet for medical imaging and the newly popular Swin Transformer model, utilizing ensemble deep learning techniques to combine the two models and leverage the advantages of both CNNs and Transformers. The loss function of CheX-DS combines weighted binary cross-entropy loss with asymmetric loss, effectively addressing the issue of data imbalance. The NIH ChestX-ray14 dataset is selected to evaluate the model's effectiveness. The model outperforms previous studies with an excellent average AUC score of 83.76\\%, demonstrating its superior performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "BIBM",
    "pdf_url": "https://arxiv.org/pdf/2505.11168v1",
    "published_date": "2025-05-16 12:10:01 UTC",
    "updated_date": "2025-05-16 12:10:01 UTC"
  },
  {
    "arxiv_id": "2505.11166v2",
    "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization",
    "authors": [
      "Huashan Sun",
      "Shengyi Liao",
      "Yansen Han",
      "Yu Bai",
      "Yang Gao",
      "Cheng Fu",
      "Weizhou Shen",
      "Fanqi Wan",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang"
    ],
    "abstract": "Despite advances in pretraining with extended context lengths, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng $\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency utilization for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11166v2",
    "published_date": "2025-05-16 12:08:48 UTC",
    "updated_date": "2025-10-12 09:58:15 UTC"
  },
  {
    "arxiv_id": "2505.11165v1",
    "title": "Maximizing Asynchronicity in Event-based Neural Networks",
    "authors": [
      "Haiqing Hao",
      "Nikola Zubić",
      "Weihua He",
      "Zhipeng Sui",
      "Davide Scaramuzza",
      "Wenhui Wang"
    ],
    "abstract": "Event cameras deliver visual data with high temporal resolution, low latency, and minimal redundancy, yet their asynchronous, sparse sequential nature challenges standard tensor-based machine learning (ML). While the recent asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by asynchronously encoding events into learned representations for ML pipelines, existing A2S approaches often sacrifice representation expressivity and generalizability compared to dense, synchronous methods. This paper introduces EVA (EVent Asynchronous representation learning), a novel A2S framework to generate highly expressive and generalizable event-by-event representations. Inspired by the analogy between events and language, EVA uniquely adapts advances from language modeling in linear attention and self-supervised learning for its construction. In demonstration, EVA outperforms prior A2S methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the first A2S framework to successfully master demanding detection tasks, achieving a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's transformative potential for advancing real-time event-based vision applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 5 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.11165v1",
    "published_date": "2025-05-16 12:07:50 UTC",
    "updated_date": "2025-05-16 12:07:50 UTC"
  },
  {
    "arxiv_id": "2505.15834v1",
    "title": "MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation",
    "authors": [
      "Congyuan Zhao",
      "Lingwei Wei",
      "Ziming Qin",
      "Wei Zhou",
      "Yunya Song",
      "Songlin Hu"
    ],
    "abstract": "Fake news spreads widely on social media, leading to numerous negative effects. Most existing detection algorithms focus on analyzing news content and social context to detect fake news. However, these approaches typically detect fake news based on specific platforms, ignoring differences in propagation characteristics across platforms. In this paper, we introduce the MPPFND dataset, which captures propagation structures across multiple platforms. We also describe the commenting and propagation characteristics of different platforms to show that their social contexts have distinct features. We propose a multi-platform fake news detection model (APSL) that uses graph neural networks to extract social context features from various platforms. Experiments show that accounting for cross-platform propagation differences improves fake news detection performance.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "Cogsci 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15834v1",
    "published_date": "2025-05-16 11:59:31 UTC",
    "updated_date": "2025-05-16 11:59:31 UTC"
  },
  {
    "arxiv_id": "2505.11157v1",
    "title": "Attention on the Sphere",
    "authors": [
      "Boris Bonev",
      "Max Rietmann",
      "Andrea Paris",
      "Alberto Carpentieri",
      "Thorsten Kurth"
    ],
    "abstract": "We introduce a generalized attention mechanism for spherical domains, enabling Transformer architectures to natively process data defined on the two-dimensional sphere - a critical need in fields such as atmospheric physics, cosmology, and robotics, where preserving spherical symmetries and topology is essential for physical accuracy. By integrating numerical quadrature weights into the attention mechanism, we obtain a geometrically faithful spherical attention that is approximately rotationally equivariant, providing strong inductive biases and leading to better performance than Cartesian approaches. To further enhance both scalability and model performance, we propose neighborhood attention on the sphere, which confines interactions to geodesic neighborhoods. This approach reduces computational complexity and introduces the additional inductive bias for locality, while retaining the symmetry properties of our method. We provide optimized CUDA kernels and memory-efficient implementations to ensure practical applicability. The method is validated on three diverse tasks: simulating shallow water equations on the rotating sphere, spherical image segmentation, and spherical depth estimation. Across all tasks, our spherical Transformers consistently outperform their planar counterparts, highlighting the advantage of geometric priors for learning on spherical domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11157v1",
    "published_date": "2025-05-16 11:59:30 UTC",
    "updated_date": "2025-05-16 11:59:30 UTC"
  },
  {
    "arxiv_id": "2505.11146v2",
    "title": "X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation",
    "authors": [
      "Peizhen Li",
      "Longbing Cao",
      "Xiao-Ming Wu",
      "Runze Yang",
      "Xiaohan Yu"
    ],
    "abstract": "The ability to imitate realistic facial expressions is essential for humanoid robots engaged in affective human-robot communication. However, the lack of datasets containing diverse humanoid facial expressions with proper annotations hinders progress in realistic humanoid facial expression imitation. To address these challenges, we introduce X2C (Anything to Control), a dataset featuring nuanced facial expressions for realistic humanoid imitation. With X2C, we contribute: 1) a high-quality, high-diversity, large-scale dataset comprising 100,000 (image, control value) pairs. Each image depicts a humanoid robot displaying a diverse range of facial expressions, annotated with 30 control values representing the ground-truth expression configuration; 2) X2CNet, a novel human-to-humanoid facial expression imitation framework that learns the correspondence between nuanced humanoid expressions and their underlying control values from X2C. It enables facial expression imitation in the wild for different human performers, providing a baseline for the imitation task, showcasing the potential value of our dataset; 3) real-world demonstrations on a physical humanoid robot, highlighting its capability to advance realistic humanoid facial expression imitation. Code and Data: https://lipzh5.github.io/X2CNet/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11146v2",
    "published_date": "2025-05-16 11:48:19 UTC",
    "updated_date": "2025-09-20 07:38:04 UTC"
  },
  {
    "arxiv_id": "2505.11141v2",
    "title": "Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans",
    "authors": [
      "Yansheng Qiu",
      "Li Xiao",
      "Zhaopan Xu",
      "Pengfei Zhou",
      "Zheng Wang",
      "Kaipeng Zhang"
    ],
    "abstract": "The goal of achieving Artificial General Intelligence (AGI) is to imitate humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have demonstrated that large language models (LLMs) with human-like reasoning capabilities exhibit exceptional performance and are being gradually integrated into multimodal large language models (MLLMs). However, whether these models possess capabilities comparable to humans in handling reasoning tasks remains unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark for fine-grained alignment of multimodal reasoning with human performance. Specifically, we collected 9,794 multimodal questions that solely rely on contextual reasoning, including bilingual (Chinese and English) multimodal questions and pure text-based questions, encompassing four question types: visual reasoning, definition judgment, analogical reasoning, and logical judgment. More importantly, each question is accompanied by human success rates and options that humans are prone to choosing incorrectly. Extensive experiments on the Human-Aligned Bench reveal notable differences between the performance of current MLLMs in multimodal reasoning and human performance. The findings on our benchmark provide insights into the development of the next-generation models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11141v2",
    "published_date": "2025-05-16 11:41:19 UTC",
    "updated_date": "2025-05-24 02:18:52 UTC"
  },
  {
    "arxiv_id": "2505.13506v1",
    "title": "EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation",
    "authors": [
      "Ruobing Yao",
      "Yifei Zhang",
      "Shuang Song",
      "Neng Gao",
      "Chenyang Tu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) compensates for the static knowledge limitations of Large Language Models (LLMs) by integrating external knowledge, producing responses with enhanced factual correctness and query-specific contextualization. However, it also introduces new attack surfaces such as corpus poisoning at the same time. Most of the existing defense methods rely on the internal knowledge of the model, which conflicts with the design concept of RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and bait-guided context diversity detection to identify malicious content by analyzing the context diversity of candidate documents without relying on LLM internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art security with plug-and-play deployment, simultaneously improving clean-scenario RAG performance while maintaining practical operational costs (relatively 1.2$\\times$ latency, 48\\%-80\\% token reduction versus Vanilla RAG).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13506v1",
    "published_date": "2025-05-16 11:40:32 UTC",
    "updated_date": "2025-05-16 11:40:32 UTC"
  },
  {
    "arxiv_id": "2505.11140v2",
    "title": "Follow the Path: Reasoning over Knowledge Graph Paths to Improve LLM Factuality",
    "authors": [
      "Mike Zhang",
      "Johannes Bjerva",
      "Russa Biswas"
    ],
    "abstract": "We introduce fs1, a simple yet effective method that improves the factuality of reasoning traces by sourcing them from large reasoning models (e.g., DeepSeek-R1) and grounding them by conditioning on knowledge graph (KG) paths. We fine-tune eight instruction-tuned Large Language Models (LLMs) on 3.9K factually grounded reasoning traces and rigorously evaluate them on six complex open-domain question-answering (QA) benchmarks encompassing 23.9K questions. Our results demonstrate that our fs1-tuned model (32B parameters) consistently outperforms instruction-tuned counterparts with parallel sampling by 6-14 absolute points (pass@$16$). Our detailed analysis shows that fs1 considerably improves model performance over more complex questions (requiring 3 or more hops on KG paths) and numerical answer types compared to the baselines. Furthermore, in single-pass inference, we notice that smaller LLMs show the most improvements. While prior works demonstrate the effectiveness of reasoning traces primarily in the STEM domains, our work shows strong evidence that anchoring reasoning to factual KG paths is a critical step in transforming LLMs for reliable knowledge-intensive tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Updated version 26.9",
    "pdf_url": "https://arxiv.org/pdf/2505.11140v2",
    "published_date": "2025-05-16 11:39:33 UTC",
    "updated_date": "2025-09-26 09:25:25 UTC"
  },
  {
    "arxiv_id": "2505.11136v1",
    "title": "Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design",
    "authors": [
      "Janik Bischoff",
      "Alexandru Rinciog",
      "Anne Meyer"
    ],
    "abstract": "We propose a novel reinforcement learning (RL) design to optimize the charging strategy for autonomous mobile robots in large-scale block stacking warehouses. RL design involves a wide array of choices that can mostly only be evaluated through lengthy experimentation. Our study focuses on how different reward and action space configurations, ranging from flexible setups to more guided, domain-informed design configurations, affect the agent performance. Using heuristic charging strategies as a baseline, we demonstrate the superiority of flexible, RL-based approaches in terms of service times. Furthermore, our findings highlight a trade-off: While more open-ended designs are able to discover well-performing strategies on their own, they may require longer convergence times and are less stable, whereas guided configurations lead to a more stable learning process but display a more limited generalization potential. Our contributions are threefold. First, we extend SLAPStack, an open-source, RL-compatible simulation-framework to accommodate charging strategies. Second, we introduce a novel RL design for tackling the charging strategy problem. Finally, we introduce several novel adaptive baseline heuristics and reproducibly evaluate the design using a Proximal Policy Optimization agent and varying different design configurations, with a focus on reward.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review LION19: The 19th Learning and Intelligent OptimizatioN Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.11136v1",
    "published_date": "2025-05-16 11:33:29 UTC",
    "updated_date": "2025-05-16 11:33:29 UTC"
  },
  {
    "arxiv_id": "2505.11135v1",
    "title": "Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets",
    "authors": [
      "Patrick Stöckermann",
      "Henning Südfeld",
      "Alessandro Immordino",
      "Thomas Altenmüller",
      "Marc Wegmann",
      "Martin Gebser",
      "Konstantin Schekotihin",
      "Georg Seidel",
      "Chew Wye Chan",
      "Fei Fei Zhang"
    ],
    "abstract": "Benchmark datasets are crucial for evaluating approaches to scheduling or dispatching in the semiconductor industry during the development and deployment phases. However, commonly used benchmark datasets like the Minifab or SMT2020 lack the complex details and constraints found in real-world scenarios. To mitigate this shortcoming, we compare open-source simulation models with a real industry dataset to evaluate how optimization methods scale with different levels of complexity. Specifically, we focus on Reinforcement Learning methods, performing optimization based on policy-gradient and Evolution Strategies. Our research provides insights into the effectiveness of these optimization methods and their applicability to realistic semiconductor frontend fab simulations. We show that our proposed Evolution Strategies-based method scales much better than a comparable policy-gradient-based approach. Moreover, we identify the selection and combination of relevant bottleneck tools to control by the agent as crucial for an efficient optimization. For the generalization across different loading scenarios and stochastic tool failure patterns, we achieve advantages when utilizing a diverse training dataset. While the overall approach is computationally expensive, it manages to scale well with the number of CPU cores used for training. For the real industry dataset, we achieve an improvement of up to 4% regarding tardiness and up to 1% regarding throughput. For the less complex open-source models Minifab and SMT2020, we observe double-digit percentage improvement in tardiness and single digit percentage improvement in throughput by use of Evolution Strategies.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11135v1",
    "published_date": "2025-05-16 11:32:29 UTC",
    "updated_date": "2025-05-16 11:32:29 UTC"
  },
  {
    "arxiv_id": "2505.11131v2",
    "title": "One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework",
    "authors": [
      "Feiran Li",
      "Qianqian Xu",
      "Shilong Bao",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "abstract": "Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at https://github.com/Ferry-Li/Co-Erasing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepeted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11131v2",
    "published_date": "2025-05-16 11:25:50 UTC",
    "updated_date": "2025-05-26 11:20:32 UTC"
  },
  {
    "arxiv_id": "2505.11129v1",
    "title": "PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video",
    "authors": [
      "Makoto Yamada",
      "Kian Ming A. Chai",
      "Ayoub Rhim",
      "Satoki Ishikawa",
      "Mohammad Sabokrou",
      "Yao-Hung Hubert Tsai"
    ],
    "abstract": "Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation. Our model leverages variational inference to learn robust visual representations from continuous input streams, similar to human visual processing. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation models, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a significant step toward more biologically plausible computer vision systems that process visual information in a manner more closely aligned with human cognitive processes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2405.14650",
    "pdf_url": "https://arxiv.org/pdf/2505.11129v1",
    "published_date": "2025-05-16 11:23:30 UTC",
    "updated_date": "2025-05-16 11:23:30 UTC"
  },
  {
    "arxiv_id": "2505.11123v1",
    "title": "Conditioning Matters: Training Diffusion Policies is Faster Than You Think",
    "authors": [
      "Zibin Dong",
      "Yicheng Liu",
      "Yinchuan Li",
      "Hang Zhao",
      "Jianye Hao"
    ],
    "abstract": "Diffusion policies have emerged as a mainstream paradigm for building vision-language-action (VLA) models. Although they demonstrate strong robot control capabilities, their training efficiency remains suboptimal. In this work, we identify a fundamental challenge in conditional diffusion policy training: when generative conditions are hard to distinguish, the training objective degenerates into modeling the marginal action distribution, a phenomenon we term loss collapse. To overcome this, we propose Cocos, a simple yet general solution that modifies the source distribution in the conditional flow matching to be condition-dependent. By anchoring the source distribution around semantics extracted from condition inputs, Cocos encourages stronger condition integration and prevents the loss collapse. We provide theoretical justification and extensive empirical results across simulation and real-world benchmarks. Our method achieves faster convergence and higher success rates than existing approaches, matching the performance of large-scale pre-trained VLAs using significantly fewer gradient steps and parameters. Cocos is lightweight, easy to implement, and compatible with diverse policy architectures, offering a general-purpose improvement to diffusion policy training.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2505.10105",
    "pdf_url": "https://arxiv.org/pdf/2505.11123v1",
    "published_date": "2025-05-16 11:14:22 UTC",
    "updated_date": "2025-05-16 11:14:22 UTC"
  },
  {
    "arxiv_id": "2505.11122v3",
    "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining",
    "authors": [
      "Yu Shi",
      "Yitong Duan",
      "Jian Li"
    ],
    "abstract": "Alpha factor mining is pivotal in quantitative investment for identifying predictive signals from complex financial data. While traditional formulaic alpha mining relies on human expertise, contemporary automated methods, such as those based on genetic programming or reinforcement learning, often struggle with search inefficiency or yield alpha factors that are difficult to interpret. This paper introduces a novel framework that integrates Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these limitations. Our framework leverages the LLM's instruction-following and reasoning capability to iteratively generate and refine symbolic alpha formulas within an MCTS-driven exploration. A key innovation is the guidance of MCTS exploration by rich, quantitative feedback from financial backtesting of each candidate factor, enabling efficient navigation of the vast search space. Furthermore, a frequent subtree avoidance mechanism is introduced to enhance search diversity and prevent formulaic homogenization, further improving performance. Experimental results on real-world stock market data demonstrate that our LLM-based framework outperforms existing methods by mining alphas with superior predictive accuracy and trading performance. The resulting formulas are also more amenable to human interpretation, establishing a more effective and efficient paradigm for formulaic alpha mining.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11122v3",
    "published_date": "2025-05-16 11:14:17 UTC",
    "updated_date": "2025-11-12 07:06:50 UTC"
  },
  {
    "arxiv_id": "2505.11119v1",
    "title": "Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach",
    "authors": [
      "Jiabei Cheng",
      "Zhen-Qun Yang",
      "Jiannong Cao",
      "Yu Yang",
      "Xinzhe Zheng"
    ],
    "abstract": "Timely prediction of students at high risk of dropout is critical for early intervention and improving educational outcomes. However, in offline educational settings, poor data quality, limited scale, and high heterogeneity often hinder the application of advanced machine learning models. Furthermore, while educational theories provide valuable insights into dropout phenomena, the lack of quantifiable metrics for key indicators limits their use in data-driven modeling. Through data analysis and a review of educational literature, we identified abrupt changes in student behavior as key early signals of dropout risk. To address this, we propose the Dual-Modal Multiscale Sliding Window (DMSW) Model, which integrates academic performance and behavioral data to dynamically capture behavior patterns using minimal data. The DMSW model improves prediction accuracy by 15% compared to traditional methods, enabling educators to identify high-risk students earlier, provide timely support, and foster a more inclusive learning environment. Our analysis highlights key behavior patterns, offering practical insights for preventive strategies and tailored support. These findings bridge the gap between theory and practice in dropout prediction, giving educators an innovative tool to enhance student retention and outcomes.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11119v1",
    "published_date": "2025-05-16 11:02:55 UTC",
    "updated_date": "2025-05-16 11:02:55 UTC"
  },
  {
    "arxiv_id": "2505.17050v2",
    "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning",
    "authors": [
      "Xinyi Wu",
      "Yanhao Jia",
      "Qinglin Zhang",
      "Yiran Qin",
      "Luwei Xiao",
      "Shuai Zhao"
    ],
    "abstract": "Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.CY",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17050v2",
    "published_date": "2025-05-16 11:01:01 UTC",
    "updated_date": "2025-11-01 09:29:22 UTC"
  },
  {
    "arxiv_id": "2505.11111v2",
    "title": "FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation",
    "authors": [
      "Lin Zhu",
      "Yijun Bian",
      "Lei You"
    ],
    "abstract": "Ensuring fairness in machine learning models is critical, particularly in high-stakes domains where biased decisions can lead to serious societal consequences. Existing preprocessing approaches generally lack transparent mechanisms for identifying which features or instances are responsible for unfairness. This obscures the rationale behind data modifications. We introduce FairSHAP, a novel pre-processing framework that leverages Shapley value attribution to improve both individual and group fairness. FairSHAP identifies fairness-critical instances in the training data using an interpretable measure of feature importance, and systematically modifies them through instance-level matching across sensitive groups. This process reduces discriminative risk - an individual fairness metric - while preserving data integrity and model accuracy. We demonstrate that FairSHAP significantly improves demographic parity and equality of opportunity across diverse tabular datasets, achieving fairness gains with minimal data perturbation and, in some cases, improved predictive performance. As a model-agnostic and transparent method, FairSHAP integrates seamlessly into existing machine learning pipelines and provides actionable insights into the sources of bias.Our code is on https://github.com/youlei202/FairSHAP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "4 figures, 19 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11111v2",
    "published_date": "2025-05-16 10:48:19 UTC",
    "updated_date": "2025-10-09 01:12:31 UTC"
  },
  {
    "arxiv_id": "2505.11109v1",
    "title": "MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark",
    "authors": [
      "Florinel-Alin Croitoru",
      "Vlad Hondru",
      "Marius Popescu",
      "Radu Tudor Ionescu",
      "Fahad Shahbaz Khan",
      "Mubarak Shah"
    ],
    "abstract": "We present the first large-scale open-set benchmark for multilingual audio-video deepfake detection. Our dataset comprises over 250 hours of real and fake videos across eight languages, with 60% of data being generated. For each language, the fake videos are generated with seven distinct deepfake generation models, selected based on the quality of the generated content. We organize the training, validation and test splits such that only a subset of the chosen generative models and languages are available during training, thus creating several challenging open-set evaluation setups. We perform experiments with various pre-trained and fine-tuned deepfake detectors proposed in recent literature. Our results show that state-of-the-art detectors are not currently able to maintain their performance levels when tested in our open-set scenarios. We publicly release our data and code at: https://huggingface.co/datasets/unibuc-cs/MAVOS-DD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11109v1",
    "published_date": "2025-05-16 10:42:30 UTC",
    "updated_date": "2025-05-16 10:42:30 UTC"
  },
  {
    "arxiv_id": "2505.11108v2",
    "title": "Personalized Robotic Object Rearrangement from Scene Context",
    "authors": [
      "Kartik Ramachandruni",
      "Sonia Chernova"
    ],
    "abstract": "Object rearrangement is a key task for household robots requiring personalization without explicit instructions, meaningful object placement in environments occupied with objects, and generalization to unseen objects and new environments. To facilitate research addressing these challenges, we introduce PARSEC, an object rearrangement benchmark for learning user organizational preferences from observed scene context to place objects in a partially arranged environment. PARSEC is built upon a novel dataset of 110K rearrangement examples crowdsourced from 72 users, featuring 93 object categories and 15 environments. To better align with real-world organizational habits, we propose ContextSortLM, an LLM-based personalized rearrangement model that handles flexible user preferences by explicitly accounting for objects with multiple valid placement locations when placing items in partially arranged environments. We evaluate ContextSortLM and existing personalized rearrangement approaches on the PARSEC benchmark and complement these findings with a crowdsourced evaluation of 108 online raters ranking model predictions based on alignment with user preferences. Our results indicate that personalized rearrangement models leveraging multiple scene context sources perform better than models relying on a single context source. Moreover, ContextSortLM outperforms other models in placing objects to replicate the target user's arrangement and ranks among the top two in all three environment categories, as rated by online evaluators. Importantly, our evaluation highlights challenges associated with modeling environment semantics across different environment categories and provides recommendations for future work.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at IEEE ROMAN 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11108v2",
    "published_date": "2025-05-16 10:40:44 UTC",
    "updated_date": "2025-06-26 23:26:27 UTC"
  },
  {
    "arxiv_id": "2505.11107v1",
    "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity",
    "authors": [
      "Chan-Jan Hsu",
      "Davide Buffelli",
      "Jamie McGowan",
      "Feng-Ting Liao",
      "Yi-Chang Chen",
      "Sattar Vakili",
      "Da-shan Shiu"
    ],
    "abstract": "Recent advances in large language models (LLMs) have demonstrated the power of reasoning through self-generated chains of thought. Multiple reasoning agents can collaborate to raise joint reasoning quality above individual outcomes. However, such agents typically interact in a turn-based manner, trading increased latency for improved quality. In this paper, we propose Group Think--a single LLM that acts as multiple concurrent reasoning agents, or thinkers. With shared visibility into each other's partial generation progress, Group Think introduces a new concurrent-reasoning paradigm in which multiple reasoning trajectories adapt dynamically to one another at the token level. For example, a reasoning thread may shift its generation mid-sentence upon detecting that another thread is better positioned to continue. This fine-grained, token-level collaboration enables Group Think to reduce redundant reasoning and improve quality while achieving significantly lower latency. Moreover, its concurrent nature allows for efficient utilization of idle computational resources, making it especially suitable for edge inference, where very small batch size often underutilizes local~GPUs. We give a simple and generalizable modification that enables any existing LLM to perform Group Think on a local GPU. We also present an evaluation strategy to benchmark reasoning latency and empirically demonstrate latency improvements using open-source LLMs that were not explicitly trained for Group Think. We hope this work paves the way for future LLMs to exhibit more sophisticated and more efficient collaborative behavior for higher quality generation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11107v1",
    "published_date": "2025-05-16 10:40:35 UTC",
    "updated_date": "2025-05-16 10:40:35 UTC"
  },
  {
    "arxiv_id": "2505.11106v1",
    "title": "Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series",
    "authors": [
      "Thanadej Rattanakornphan",
      "Piyanon Charoenpoonpanich",
      "Chainarong Amornbunchornvej"
    ],
    "abstract": "Finding the most similar subsequences between two multidimensional time series has many applications: e.g. capturing dependency in stock market or discovering coordinated movement of baboons. Considering one pattern occurring in one time series, we might be wondering whether the same pattern occurs in another time series with some distortion that might have a different length. Nevertheless, to the best of our knowledge, there is no efficient framework that deals with this problem yet. In this work, we propose an algorithm that provides the exact solution of finding the most similar multidimensional subsequences between time series where there is a difference in length both between time series and between subsequences. The algorithm is built based on theoretical guarantee of correctness and efficiency. The result in simulation datasets illustrated that our approach not just only provided correct solution, but it also utilized running time only quarter of time compared against the baseline approaches. In real-world datasets, it extracted the most similar subsequences even faster (up to 20 times faster against baseline methods) and provided insights regarding the situation in stock market and following relations of multidimensional time series of baboon movement. Our approach can be used for any time series. The code and datasets of this work are provided for the public use.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.11106v1",
    "published_date": "2025-05-16 10:39:46 UTC",
    "updated_date": "2025-05-16 10:39:46 UTC"
  },
  {
    "arxiv_id": "2505.11100v1",
    "title": "Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors",
    "authors": [
      "Lang Feng",
      "Jiahao Lin",
      "Dong Xing",
      "Li Zhang",
      "De Ma",
      "Gang Pan"
    ],
    "abstract": "Population-population generalization is a challenging problem in multi-agent reinforcement learning (MARL), particularly when agents encounter unseen co-players. However, existing self-play-based methods are constrained by the limitation of inside-space generalization. In this study, we propose Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome this limitation in MARL. BiDist leverages knowledge distillation in two alternating directions: forward distillation, which emulates the historical policies' space and creates an implicit self-play, and reverse distillation, which systematically drives agents towards novel distributions outside the known policy space in a non-self-play manner. In addition, BiDist operates as a concise and efficient solution without the need for the complex and costly storage of past policies. We provide both theoretical analysis and empirical evidence to support BiDist's effectiveness. Our results highlight its remarkable generalization ability across a variety of cooperative, competitive, and social dilemma tasks, and reveal that BiDist significantly diversifies the policy distribution space. We also present comprehensive ablation studies to reinforce BiDist's effectiveness and key success factors. Source codes are available in the supplementary material.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11100v1",
    "published_date": "2025-05-16 10:31:10 UTC",
    "updated_date": "2025-05-16 10:31:10 UTC"
  },
  {
    "arxiv_id": "2505.11086v1",
    "title": "Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data",
    "authors": [
      "Keita Kinjo"
    ],
    "abstract": "Recently, the proliferation of omni-channel platforms has attracted interest in customer journeys, particularly regarding their role in developing marketing strategies. However, few efforts have been taken to quantitatively study or comprehensively analyze them owing to the sequential nature of their data and the complexity involved in analysis. In this study, we propose a novel approach comprising three steps for analyzing customer journeys. First, the distance between sequential data is defined and used to identify and visualize representative sequences. Second, the likelihood of purchase is predicted based on this distance. Third, if a sequence suggests no purchase, counterfactual sequences are recommended to increase the probability of a purchase using a proposed method, which extracts counterfactual explanations for sequential data. A survey was conducted, and the data were analyzed; the results revealed that typical sequences could be extracted, and the parts of those sequences important for purchase could be detected. We believe that the proposed approach can support improvements in various marketing activities.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11086v1",
    "published_date": "2025-05-16 10:17:53 UTC",
    "updated_date": "2025-05-16 10:17:53 UTC"
  },
  {
    "arxiv_id": "2505.11085v2",
    "title": "A Fast Kernel-based Conditional Independence test with Application to Causal Discovery",
    "authors": [
      "Oliver Schacht",
      "Biwei Huang"
    ],
    "abstract": "Kernel-based conditional independence (KCI) testing is a powerful nonparametric method commonly employed in causal discovery tasks. Despite its flexibility and statistical reliability, cubic computational complexity limits its application to large datasets. To address this computational bottleneck, we propose \\textit{FastKCI}, a scalable and parallelizable kernel-based conditional independence test that utilizes a mixture-of-experts approach inspired by embarrassingly parallel inference techniques for Gaussian processes. By partitioning the dataset based on a Gaussian mixture model over the conditioning variables, FastKCI conducts local KCI tests in parallel, aggregating the results using an importance-weighted sampling scheme. Experiments on synthetic datasets and benchmarks on real-world production data validate that FastKCI maintains the statistical power of the original KCI test while achieving substantial computational speedups. FastKCI thus represents a practical and efficient solution for conditional independence testing in causal inference on large-scale data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11085v2",
    "published_date": "2025-05-16 10:14:57 UTC",
    "updated_date": "2025-12-04 08:49:40 UTC"
  },
  {
    "arxiv_id": "2505.11083v1",
    "title": "Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation",
    "authors": [
      "Guangqiang Li",
      "M. Amine Atoui",
      "Xiangshun Li"
    ],
    "abstract": "Deep learning methods have shown promising performance in fault diagnosis for multimode process. Most existing studies assume that the collected health state categories from different operating modes are identical. However, in real industrial scenarios, these categories typically exhibit only partial overlap. The incompleteness of the available data and the large distributional differences between the operating modes pose a significant challenge to existing fault diagnosis methods. To address this problem, a novel fault diagnosis model named self-adaptive temporal-spatial attention network (TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy category data to generate multimode samples. To enrich the diversity of the fault data, interpolation is performed between healthy and fault samples. Subsequently, the fault diagnosis model is trained using real and generated data. The self-adaptive instance normalization is established to suppress irrelevant information while retaining essential statistical features for diagnosis. In addition, a temporal-spatial attention mechanism is constructed to focus on the key features, thus enhancing the generalization ability of the model. The extensive experiments demonstrate that the proposed model significantly outperforms the state-of-the-art methods. The code will be available on Github at https://github.com/GuangqiangLi/TSA-SAN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11083v1",
    "published_date": "2025-05-16 10:14:10 UTC",
    "updated_date": "2025-05-16 10:14:10 UTC"
  },
  {
    "arxiv_id": "2506.14777v2",
    "title": "WebXAII: an open-source web framework to study human-XAI interaction",
    "authors": [
      "Jules Leguy",
      "Pierre-Antoine Jean",
      "Felipe Torres Figueroa",
      "Sébastien Harispe"
    ],
    "abstract": "This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14777v2",
    "published_date": "2025-05-16 10:12:53 UTC",
    "updated_date": "2025-06-19 14:45:07 UTC"
  },
  {
    "arxiv_id": "2505.11080v3",
    "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following",
    "authors": [
      "Yapei Chang",
      "Yekyung Kim",
      "Michael Krumdick",
      "Amir Zadeh",
      "Chuan Li",
      "Chris Tanner",
      "Mohit Iyyer"
    ],
    "abstract": "Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "neurips cam-ready",
    "pdf_url": "https://arxiv.org/pdf/2505.11080v3",
    "published_date": "2025-05-16 10:11:43 UTC",
    "updated_date": "2025-10-24 01:33:28 UTC"
  },
  {
    "arxiv_id": "2505.11570v2",
    "title": "Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning",
    "authors": [
      "Chongyang Tan",
      "Ruoqi Wen",
      "Rongpeng Li",
      "Zhifeng Zhao",
      "Ekram Hossain",
      "Honggang Zhang"
    ],
    "abstract": "Federated Learning (FL) enables distributed model training across edge devices in a privacy-friendly manner. However, its efficiency heavily depends on effective device selection and high-dimensional resource allocation in dynamic and heterogeneous wireless environments. Conventional methods demand a confluence of domain-specific expertise, extensive hyperparameter tuning, and/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary Large Language Model (T-ELLM) framework to generate a qualified policy for device selection in a wireless FL environment. Unlike conventional optimization methods, T-ELLM leverages natural language-based scenario prompts to enhance generalization across varying network conditions. The framework decouples the joint optimization problem mathematically, enabling tractable learning of device selection policies while delegating resource allocation to convex optimization tools. To improve adaptability, T-ELLM integrates a sample-efficient, model-based virtual learning environment that captures the relationship between device selection and learning performance, facilitating subsequent group relative policy optimization. This concerted approach reduces reliance on real-world interactions, minimizing communication overhead while maintaining high-fidelity decision-making. Theoretical analysis proves that the discrepancy between virtual and real environments is bounded, ensuring the advantage function learned in the virtual environment maintains a provably small deviation from real-world conditions. Experimental results demonstrate that T-ELLM outperforms benchmark methods in energy efficiency and exhibits robust adaptability to environmental changes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11570v2",
    "published_date": "2025-05-16 10:07:29 UTC",
    "updated_date": "2025-11-11 04:21:21 UTC"
  },
  {
    "arxiv_id": "2505.11569v1",
    "title": "Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures",
    "authors": [
      "Pooja Mangal",
      "Sudaksh Kalra",
      "Dolly Sapra"
    ],
    "abstract": "Deploying deep convolutional neural networks (CNNs) on resource-constrained devices presents significant challenges due to their high computational demands and rigid, static architectures. To overcome these limitations, this thesis explores methods for enabling CNNs to dynamically adjust their computational complexity based on available hardware resources. We introduce adaptive CNN architectures capable of scaling their capacity at runtime, thus efficiently balancing performance and resource utilization. To achieve this adaptability, we propose a structured pruning and dynamic re-construction approach that creates nested subnetworks within a single CNN model. This approach allows the network to dynamically switch between compact and full-sized configurations without retraining, making it suitable for deployment across varying hardware platforms. Experiments conducted across multiple CNN architectures including VGG-16, AlexNet, ResNet-20, and ResNet-56 on CIFAR-10 and Imagenette datasets demonstrate that adaptive models effectively maintain or even enhance performance under varying computational constraints. Our results highlight that embedding adaptability directly into CNN architectures significantly improves their robustness and flexibility, paving the way for efficient real-world deployment in diverse computational environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "50 Pages, 11 figures, Preprint",
    "pdf_url": "https://arxiv.org/pdf/2505.11569v1",
    "published_date": "2025-05-16 10:06:55 UTC",
    "updated_date": "2025-05-16 10:06:55 UTC"
  },
  {
    "arxiv_id": "2505.11070v1",
    "title": "Towards Self-Improvement of Diffusion Models via Group Preference Optimization",
    "authors": [
      "Renjie Chen",
      "Wenfeng Lin",
      "Yichen Zhang",
      "Jiangchuan Wei",
      "Boyuan Liu",
      "Chao Feng",
      "Jiao Ran",
      "Mingyu Guo"
    ],
    "abstract": "Aligning text-to-image (T2I) diffusion models with Direct Preference Optimization (DPO) has shown notable improvements in generation quality. However, applying DPO to T2I faces two challenges: the sensitivity of DPO to preference pairs and the labor-intensive process of collecting and annotating high-quality data. In this work, we demonstrate that preference pairs with marginal differences can degrade DPO performance. Since DPO relies exclusively on relative ranking while disregarding the absolute difference of pairs, it may misclassify losing samples as wins, or vice versa. We empirically show that extending the DPO from pairwise to groupwise and incorporating reward standardization for reweighting leads to performance gains without explicit data selection. Furthermore, we propose Group Preference Optimization (GPO), an effective self-improvement method that enhances performance by leveraging the model's own capabilities without requiring external data. Extensive experiments demonstrate that GPO is effective across various diffusion models and tasks. Specifically, combining with widely used computer vision models, such as YOLO and OCR, the GPO improves the accurate counting and text rendering capabilities of the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a plug-and-play method, no extra overhead is introduced during inference.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11070v1",
    "published_date": "2025-05-16 10:04:57 UTC",
    "updated_date": "2025-05-16 10:04:57 UTC"
  },
  {
    "arxiv_id": "2505.11067v1",
    "title": "Assessing the Performance of Analog Training for Transfer Learning",
    "authors": [
      "Omobayode Fagbohungbe",
      "Corey Lammie",
      "Malte J. Rasch",
      "Takashi Ando",
      "Tayfun Gokmen",
      "Vijay Narayanan"
    ],
    "abstract": "Analog in-memory computing is a next-generation computing paradigm that promises fast, parallel, and energy-efficient deep learning training and transfer learning (TL). However, achieving this promise has remained elusive due to a lack of suitable training algorithms. Analog memory devices exhibit asymmetric and non-linear switching behavior in addition to device-to-device variation, meaning that most, if not all, of the current off-the-shelf training algorithms cannot achieve good training outcomes. Also, recently introduced algorithms have enjoyed limited attention, as they require bi-directionally switching devices of unrealistically high symmetry and precision and are highly sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which leverages the chopped technique to address many of the challenges mentioned above. In this paper, we assess the performance of the c-TTv2 algorithm for analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also investigate the robustness of our algorithm to changes in some device specifications, including weight transfer noise, symmetry point skew, and symmetry point variability",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.DC",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11067v1",
    "published_date": "2025-05-16 10:02:32 UTC",
    "updated_date": "2025-05-16 10:02:32 UTC"
  },
  {
    "arxiv_id": "2505.11066v1",
    "title": "A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware",
    "authors": [
      "Rui Wang",
      "Shichun Yang",
      "Yuyi Chen",
      "Zhuoyang Li",
      "Zexiang Tong",
      "Jianyi Xu",
      "Jiayi Lu",
      "Xinjie Feng",
      "Yaoguang Cao"
    ],
    "abstract": "Road terrains play a crucial role in ensuring the driving safety of autonomous vehicles (AVs). However, existing sensors of AVs, including cameras and Lidars, are susceptible to variations in lighting and weather conditions, making it challenging to achieve real-time perception of road conditions. In this paper, we propose an illumination-aware multi-modal fusion network (IMF), which leverages both exteroceptive and proprioceptive perception and optimizes the fusion process based on illumination features. We introduce an illumination-perception sub-network to accurately estimate illumination features. Moreover, we design a multi-modal fusion network which is able to dynamically adjust weights of different modalities according to illumination features. We enhance the optimization process by pre-training of the illumination-perception sub-network and incorporating illumination loss as one of the training constraints. Extensive experiments demonstrate that the IMF shows a superior performance compared to state-of-the-art methods. The comparison results with single modality perception methods highlight the comprehensive advantages of multi-modal fusion in accurately perceiving road terrains under varying lighting conditions. Our dataset is available at: https://github.com/lindawang2016/IMF.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11066v1",
    "published_date": "2025-05-16 10:02:22 UTC",
    "updated_date": "2025-05-16 10:02:22 UTC"
  },
  {
    "arxiv_id": "2505.11065v2",
    "title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking",
    "authors": [
      "Changlun Li",
      "Yao Shi",
      "Chen Wang",
      "Qiqi Duan",
      "Runke Ruan",
      "Weijie Huang",
      "Haonan Long",
      "Lijun Huang",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in managing complex fund investment remains inadequately assessed. A fundamental limitation of existing benchmarks for evaluating LLM-driven trading strategies is their reliance on historical back-testing, inadvertently enabling LLMs to \"time travel\"-leveraging future information embedded in their training corpora, thus resulting in possible information leakage and overly optimistic performance estimates. To address this issue, we introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate LLM in real-time market conditions. Utilizing a multi-agent architecture, DeepFund connects directly with real-time stock market data-specifically data published after each model pretraining cutoff-to ensure fair and leakage-free evaluations. Empirical tests on nine flagship LLMs from leading global institutions across multiple investment dimensions-including ticker-level analysis, investment decision-making, portfolio management, and risk control-reveal significant practical challenges. Notably, even cutting-edge models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses within DeepFund real-time evaluation environment, underscoring the present limitations of LLMs for active fund management. Our code is available at https://github.com/HKUSTDial/DeepFund.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CE",
    "comment": "NeurIPS 2025 Datasets and Benchmarks Track",
    "pdf_url": "https://arxiv.org/pdf/2505.11065v2",
    "published_date": "2025-05-16 10:00:56 UTC",
    "updated_date": "2025-10-14 05:28:29 UTC"
  },
  {
    "arxiv_id": "2505.11063v2",
    "title": "Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction",
    "authors": [
      "Changyue Jiang",
      "Xudong Pan",
      "Min Yang"
    ],
    "abstract": "LLM-based autonomous agents possess capabilities such as reasoning, tool invocation, and environment interaction, enabling the execution of complex multi-step tasks. The internal reasoning process, i.e., thought, of behavioral trajectory significantly influences tool usage and subsequent actions but can introduce potential risks. Even minor deviations in the agent's thought may trigger cascading effects leading to irreversible safety incidents. To address the safety alignment challenges in long-horizon behavioral trajectories, we propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing a lightweight and resource-efficient model, Thought-Aligner corrects each high-risk thought on the fly before each action execution. The corrected thought is then reintroduced to the agent, ensuring safer subsequent decisions and tool interactions. Importantly, Thought-Aligner modifies only the reasoning phase without altering the underlying agent framework, making it easy to deploy and widely applicable to various agent frameworks. To train the Thought-Aligner model, we construct an instruction dataset across ten representative scenarios and simulate ReAct execution trajectories, generating 5,000 diverse instructions and more than 11,400 safe and unsafe thought pairs. The model is fine-tuned using contrastive learning techniques. Experiments across three agent safety benchmarks involving 12 different LLMs demonstrate that Thought-Aligner raises agent behavioral safety from approximately 50% in the unprotected setting to 90% on average. Additionally, Thought-Aligner maintains response latency below 100ms with minimal resource usage, demonstrating its capability for efficient deployment, broad applicability, and timely responsiveness. This method thus provides a practical dynamic safety solution for the LLM-based agents.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11063v2",
    "published_date": "2025-05-16 10:00:15 UTC",
    "updated_date": "2025-05-19 06:52:59 UTC"
  },
  {
    "arxiv_id": "2505.11060v1",
    "title": "CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs",
    "authors": [
      "David Méndez",
      "Gianpaolo Bontempo",
      "Elisa Ficarra",
      "Roberto Confalonieri",
      "Natalia Díaz-Rodríguez"
    ],
    "abstract": "Deep vision models often rely on biases learned from spurious correlations in datasets. To identify these biases, methods that interpret high-level, human-understandable concepts are more effective than those relying primarily on low-level features like heatmaps. A major challenge for these concept-based methods is the lack of image annotations indicating potentially bias-inducing concepts, since creating such annotations requires detailed labeling for each dataset and concept, which is highly labor-intensive. We present CUBIC (Concept embeddings for Unsupervised Bias IdentifiCation), a novel method that automatically discovers interpretable concepts that may bias classifier behavior. Unlike existing approaches, CUBIC does not rely on predefined bias candidates or examples of model failures tied to specific biases, as such information is not always available. Instead, it leverages image-text latent space and linear classifier probes to examine how the latent representation of a superclass label$\\unicode{x2014}$shared by all instances in the dataset$\\unicode{x2014}$is influenced by the presence of a given concept. By measuring these shifts against the normal vector to the classifier's decision boundary, CUBIC identifies concepts that significantly influence model predictions. Our experiments demonstrate that CUBIC effectively uncovers previously unknown biases using Vision-Language Models (VLMs) without requiring the samples in the dataset where the classifier underperforms or prior knowledge of potential biases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures, 5 tables. Accepted at IJCNN 2025; to appear in IEEE Xplore",
    "pdf_url": "https://arxiv.org/pdf/2505.11060v1",
    "published_date": "2025-05-16 09:57:15 UTC",
    "updated_date": "2025-05-16 09:57:15 UTC"
  },
  {
    "arxiv_id": "2505.11050v2",
    "title": "Halting Recurrent GNNs and the Graded $μ$-Calculus",
    "authors": [
      "Jeroen Bollen",
      "Jan Van den Bussche",
      "Stijn Vansummeren",
      "Jonni Virtema"
    ],
    "abstract": "Graph Neural Networks (GNNs) are a class of machine-learning models that operate on graph-structured data. Their expressive power is intimately related to logics that are invariant under graded bisimilarity. Current proposals for recurrent GNNs either assume that the graph size is given to the model, or suffer from a lack of termination guarantees. In this paper, we propose a halting mechanism for recurrent GNNs. We prove that our halting model can express all node classifiers definable in graded modal mu-calculus, even for the standard GNN variant that is oblivious to the graph size. To prove our main result, we develop a new approximate semantics for graded mu-calculus, which we believe to be of independent interest. We leverage this new semantics into a new model-checking algorithm, called the counting algorithm, which is oblivious to the graph size. In a final step we show that the counting algorithm can be implemented on a halting recurrent GNN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "Extended technical report of paper accepted for publication at KR 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.11050v2",
    "published_date": "2025-05-16 09:46:36 UTC",
    "updated_date": "2025-08-13 09:37:33 UTC"
  },
  {
    "arxiv_id": "2505.13504v1",
    "title": "An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents",
    "authors": [
      "Ayesha Amjad",
      "Saurav Sthapit",
      "Tahir Qasim Syed"
    ],
    "abstract": "Extracting alphanumeric data from form-like documents such as invoices, purchase orders, bills, and financial documents is often performed via vision (OCR) and learning algorithms or monolithic pipelines with limited potential for systemic improvements. We propose an agentic AI system that leverages Large Language Model (LLM) agents and a reinforcement learning (RL) driver agent to automate consistent, self-improving extraction under LLM inference uncertainty. Our work highlights the limitations of monolithic LLM-based extraction and introduces a modular, multi-agent framework with task-specific prompts and an RL policy of rewards and penalties to guide a meta-prompting agent to learn from past errors and improve prompt-based actor agents. This self-corrective adaptive system handles diverse documents, file formats, layouts, and LLMs, aiming to automate accurate information extraction without the need for human intervention. Results as reported on two benchmark datasets of SOIRE, and CORD, are promising for the agentic AI framework.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13504v1",
    "published_date": "2025-05-16 09:46:10 UTC",
    "updated_date": "2025-05-16 09:46:10 UTC"
  },
  {
    "arxiv_id": "2505.11049v1",
    "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
    "authors": [
      "Yue Liu",
      "Shengfang Zhai",
      "Mingzhe Du",
      "Yulin Chen",
      "Tri Cao",
      "Hongcheng Gao",
      "Cheng Wang",
      "Xinfeng Li",
      "Kun Wang",
      "Junfeng Fang",
      "Jiaheng Zhang",
      "Bryan Hooi"
    ],
    "abstract": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11049v1",
    "published_date": "2025-05-16 09:46:10 UTC",
    "updated_date": "2025-05-16 09:46:10 UTC"
  },
  {
    "arxiv_id": "2505.11568v3",
    "title": "BioCube: A Multimodal Dataset for Biodiversity Research",
    "authors": [
      "Stylianos Stasinos",
      "Martino Mensio",
      "Elena Lazovik",
      "Athanasios Trantas"
    ],
    "abstract": "Biodiversity research requires complete and detailed information to study ecosystem dynamics at different scales. Employing data-driven methods like Machine Learning is getting traction in ecology and more specific biodiversity, offering alternative modelling pathways. For these methods to deliver accurate results there is the need for large, curated and multimodal datasets that offer granular spatial and temporal resolutions. In this work, we introduce BioCube, a multimodal, fine-grained global dataset for ecology and biodiversity research. BioCube incorporates species observations through images, audio recordings and descriptions, environmental DNA, vegetation indices, agricultural, forest, land indicators, and high-resolution climate variables. All observations are geospatially aligned under the WGS84 geodetic system, spanning from 2000 to 2020. The dataset is available at https://huggingface.co/datasets/ BioDT/BioCube, the acquisition and processing code base at https://github.com/BioDT/bfm-data.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "published to BiDS'25, 4 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.11568v3",
    "published_date": "2025-05-16 09:46:08 UTC",
    "updated_date": "2025-10-24 09:17:32 UTC"
  },
  {
    "arxiv_id": "2505.11034v1",
    "title": "CleanPatrick: A Benchmark for Image Data Cleaning",
    "authors": [
      "Fabian Gröger",
      "Simone Lionetti",
      "Philippe Gottfrois",
      "Alvaro Gonzalez-Jimenez",
      "Ludovic Amruthalingam",
      "Elisabeth Victoria Goessinger",
      "Hanna Lindemann",
      "Marie Bargiela",
      "Marie Hofbauer",
      "Omar Badri",
      "Philipp Tschandl",
      "Arash Koochek",
      "Matthew Groh",
      "Alexander A. Navarini",
      "Marc Pouly"
    ],
    "abstract": "Robust machine learning depends on clean data, yet current image data cleaning benchmarks rely on synthetic noise or narrow human studies, limiting comparison and real-world relevance. We introduce CleanPatrick, the first large-scale benchmark for data cleaning in the image domain, built upon the publicly available Fitzpatrick17k dermatology dataset. We collect 496,377 binary annotations from 933 medical crowd workers, identify off-topic samples (4%), near-duplicates (21%), and label errors (22%), and employ an aggregation model inspired by item-response theory followed by expert review to derive high-quality ground truth. CleanPatrick formalizes issue detection as a ranking task and adopts typical ranking metrics mirroring real audit workflows. Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick, self-supervised representations excel at near-duplicate detection, classical methods achieve competitive off-topic detection under constrained review budgets, and label-error detection remains an open challenge for fine-grained medical classification. By releasing both the dataset and the evaluation framework, CleanPatrick enables a systematic comparison of image-cleaning strategies and paves the way for more reliable data-centric artificial intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11034v1",
    "published_date": "2025-05-16 09:29:41 UTC",
    "updated_date": "2025-05-16 09:29:41 UTC"
  },
  {
    "arxiv_id": "2505.11032v3",
    "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy",
    "authors": [
      "Yuran Wang",
      "Ruihai Wu",
      "Yue Chen",
      "Jiarui Wang",
      "Jiaqi Liang",
      "Ziyu Zhu",
      "Haoran Geng",
      "Jitendra Malik",
      "Pieter Abbeel",
      "Hao Dong"
    ],
    "abstract": "Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "NeurIPS2025 Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2505.11032v3",
    "published_date": "2025-05-16 09:26:59 UTC",
    "updated_date": "2025-10-12 09:55:05 UTC"
  },
  {
    "arxiv_id": "2505.11030v1",
    "title": "The heteronomy of algorithms: Traditional knowledge and computational knowledge",
    "authors": [
      "David M. Berry"
    ],
    "abstract": "If an active citizen should increasingly be a computationally enlightened one, replacing the autonomy of reason with the heteronomy of algorithms, then I argue in this article that we must begin teaching the principles of critiquing the computal through new notions of what we might call digital Bildung. Indeed, if civil society itself is mediated by computational systems and media, the public use of reason must also be complemented by skills for negotiating and using these computal forms to articulate such critique. Not only is there a need to raise the intellectual tone regarding computation and its related softwarization processes, but there is an urgent need to attend to the likely epistemic challenges from computation which, as presently constituted, tends towards justification through a philosophy of utility rather than through a philosophy of care for the territory of the intellect. We therefore need to develop an approach to this field that uses concepts and methods drawn from philosophy, politics, history, anthropology, sociology, media studies, computer science, and the humanities more generally, to try to understand these issues - particularly the way in which software and data increasingly penetrate our everyday life and the pressures and fissures that are created. We must, in other words, move to undertake a critical interdisciplinary research program to understand the way in which these systems are created, instantiated, and normatively engendered in both specific and general contexts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11030v1",
    "published_date": "2025-05-16 09:25:00 UTC",
    "updated_date": "2025-05-16 09:25:00 UTC"
  },
  {
    "arxiv_id": "2505.11026v1",
    "title": "StRuCom: A Novel Dataset of Structured Code Comments in Russian",
    "authors": [
      "Maria Dziuba",
      "Valentin Malykh"
    ],
    "abstract": "Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom shows statistically significant improvements of chrf++ and BERTScore over baseline models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11026v1",
    "published_date": "2025-05-16 09:22:07 UTC",
    "updated_date": "2025-05-16 09:22:07 UTC"
  },
  {
    "arxiv_id": "2505.11567v2",
    "title": "OLMA: One Loss for More Accurate Time Series Forecasting",
    "authors": [
      "Tianyi Shi",
      "Zhu Meng",
      "Yue Chen",
      "Siyang Zheng",
      "Fei Su",
      "Jin Huang",
      "Changrui Ren",
      "Zhicheng Zhao"
    ],
    "abstract": "Time series forecasting faces two important but often overlooked challenges. Firstly, the inherent random noise in the time series labels sets a theoretical lower bound for the forecasting error, which is positively correlated with the entropy of the labels. Secondly, neural networks exhibit a frequency bias when modeling the state-space of time series, that is, the model performs well in learning certain frequency bands but poorly in others, thus restricting the overall forecasting performance. To address the first challenge, we prove a theorem that there exists a unitary transformation that can reduce the marginal entropy of multiple correlated Gaussian processes, thereby providing guidance for reducing the lower bound of forecasting error. Furthermore, experiments confirm that Discrete Fourier Transform (DFT) can reduce the entropy in the majority of scenarios. Correspondingly, to alleviate the frequency bias, we jointly introduce supervision in the frequency domain along the temporal dimension through DFT and Discrete Wavelet Transform (DWT). This supervision-side strategy is highly general and can be seamlessly integrated into any supervised learning method. Moreover, we propose a novel loss function named OLMA, which utilizes the frequency domain transformation across both channel and temporal dimensions to enhance forecasting. Finally, the experimental results on multiple datasets demonstrate the effectiveness of OLMA in addressing the above two challenges and the resulting improvement in forecasting accuracy. The results also indicate that the perspectives of entropy and frequency bias provide a new and feasible research direction for time series forecasting. The code is available at: https://github.com/Yuyun1011/OLMA-One-Loss-for-More-Accurate-Time-Series-Forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11567v2",
    "published_date": "2025-05-16 09:17:15 UTC",
    "updated_date": "2025-09-25 09:06:01 UTC"
  },
  {
    "arxiv_id": "2505.11011v1",
    "title": "Humans expect rationality and cooperation from LLM opponents in strategic games",
    "authors": [
      "Darija Barak",
      "Miguel Costa-Gomes"
    ],
    "abstract": "As Large Language Models (LLMs) integrate into our social and economic interactions, we need to deepen our understanding of how humans respond to LLMs opponents in strategic settings. We present the results of the first controlled monetarily-incentivised laboratory experiment looking at differences in human behaviour in a multi-player p-beauty contest against other humans and LLMs. We use a within-subject design in order to compare behaviour at the individual level. We show that, in this environment, human subjects choose significantly lower numbers when playing against LLMs than humans, which is mainly driven by the increased prevalence of `zero' Nash-equilibrium choices. This shift is mainly driven by subjects with high strategic reasoning ability. Subjects who play the zero Nash-equilibrium choice motivate their strategy by appealing to perceived LLM's reasoning ability and, unexpectedly, propensity towards cooperation. Our findings provide foundational insights into the multi-player human-LLM interaction in simultaneous choice games, uncover heterogeneities in both subjects' behaviour and beliefs about LLM's play when playing against them, and suggest important implications for mechanism design in mixed human-LLM systems.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11011v1",
    "published_date": "2025-05-16 09:01:09 UTC",
    "updated_date": "2025-05-16 09:01:09 UTC"
  },
  {
    "arxiv_id": "2505.11010v2",
    "title": "ReviewInstruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models",
    "authors": [
      "Jiangxu Wu",
      "Cong Wang",
      "TianHuang Su",
      "Jun Yang",
      "Haozhi Lin",
      "Chao Zhang",
      "Ming Peng",
      "Kai Shi",
      "SongPan Yang",
      "BinQing Pan",
      "ZiXian Li",
      "Ni Yang",
      "ZhenYu Yang"
    ],
    "abstract": "The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative \"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2025 Accepted",
    "pdf_url": "https://arxiv.org/pdf/2505.11010v2",
    "published_date": "2025-05-16 08:59:07 UTC",
    "updated_date": "2025-07-04 12:51:51 UTC"
  },
  {
    "arxiv_id": "2505.11004v3",
    "title": "Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning",
    "authors": [
      "Jingcheng Niu",
      "Subhabrata Dutta",
      "Ahmed Elshabrawy",
      "Harish Tayyar Madabushi",
      "Iryna Gurevych"
    ],
    "abstract": "Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere \"memorization\" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "TMLR",
    "pdf_url": "https://arxiv.org/pdf/2505.11004v3",
    "published_date": "2025-05-16 08:50:42 UTC",
    "updated_date": "2025-10-06 19:24:12 UTC"
  },
  {
    "arxiv_id": "2505.10994v3",
    "title": "Space Group Equivariant Crystal Diffusion",
    "authors": [
      "Rees Chang",
      "Angela Pak",
      "Alex Guerra",
      "Ni Zhan",
      "Nick Richardson",
      "Elif Ertekin",
      "Ryan P. Adams"
    ],
    "abstract": "Accelerating inverse design of crystalline materials with generative models has significant implications for a range of technologies. Unlike other atomic systems, 3D crystals are invariant to discrete groups of isometries called the space groups. Crucially, these space group symmetries are known to heavily influence materials properties. We propose SGEquiDiff, a crystal generative model which naturally handles space group constraints with space group invariant likelihoods. SGEquiD-iff consists of an SE(3)-invariant, telescoping discrete sampler of crystal lattices; permutation-invariant, transformer-based autoregressive sampling of Wyckoff positions, elements, and numbers of symmetrically unique atoms; and space group equivariant diffusion of atomic coordinates. We show that space group equivariant vector fields automatically live in the tangent spaces of the Wyckoff positions. SGEquiDiff achieves state-of-the-art performance on standard benchmark datasets as assessed by quantitative proxy metrics and quantum mechanical calculations. Our code is available at https://github.com/rees-c/sgequidiff.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10994v3",
    "published_date": "2025-05-16 08:45:04 UTC",
    "updated_date": "2025-10-23 23:52:51 UTC"
  },
  {
    "arxiv_id": "2505.10991v3",
    "title": "Most General Explanations of Tree Ensembles (Extended Version)",
    "authors": [
      "Yacine Izza",
      "Alexey Ignatiev",
      "Sasha Rubin",
      "Joao Marques-Silva",
      "Peter J. Stuckey"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) is critical for attaining trust in the operation of AI systems. A key question of an AI system is ``why was this decision made this way''. Formal approaches to XAI use a formal model of the AI system to identify abductive explanations. While abductive explanations may be applicable to a large number of inputs sharing the same concrete values, more general explanations may be preferred for numeric inputs. So-called inflated abductive explanations give intervals for each feature ensuring that any input whose values fall withing these intervals is still guaranteed to make the same prediction. Inflated explanations cover a larger portion of the input space, and hence are deemed more general explanations. But there can be many (inflated) abductive explanations for an instance. Which is the best? In this paper, we show how to find a most general abductive explanation for an AI decision. This explanation covers as much of the input space as possible, while still being a correct formal explanation of the model's behaviour. Given that we only want to give a human one explanation for a decision, the most general explanation gives us the explanation with the broadest applicability, and hence the one most likely to seem sensible. (The paper has been accepted at IJCAI2025 conference.)",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10991v3",
    "published_date": "2025-05-16 08:42:01 UTC",
    "updated_date": "2025-05-20 02:10:09 UTC"
  },
  {
    "arxiv_id": "2505.11565v2",
    "title": "ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?",
    "authors": [
      "Sarthak Munshi",
      "Swapnil Pathak",
      "Sonam Ghatode",
      "Thenuga Priyadarshini",
      "Dhivya Chandramouleeswaran",
      "Ashutosh Rana"
    ],
    "abstract": "While Large Language Models have shown promise in cybersecurity applications, their effectiveness in identifying security threats within cloud deployments remains unexplored. This paper introduces AWS Cloud Security Engineering Eval, a novel dataset for evaluating LLMs cloud security threat modeling capabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios, each featuring detailed architectural specifications, Infrastructure as Code implementations, documented security vulnerabilities, and associated threat modeling parameters. Our dataset enables systemic assessment of LLMs abilities to identify security risks, analyze attack vectors, and propose mitigation strategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that GPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro performing optimally in 0-shot scenarios and GPT 4.1 showing superior results in few-shot settings. While GPT 4.1 maintains a slight overall performance advantage, Claude 3.7 Sonnet generates the most semantically sophisticated threat models but struggles with threat categorization and generalization. To promote reproducibility and advance research in automated cybersecurity threat analysis, we open-source our dataset, evaluation metrics, and methodologies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Submitted to the 39th Annual Conference on Neural Information Processing Systems",
    "pdf_url": "https://arxiv.org/pdf/2505.11565v2",
    "published_date": "2025-05-16 08:40:09 UTC",
    "updated_date": "2025-05-24 03:07:23 UTC"
  },
  {
    "arxiv_id": "2505.10989v1",
    "title": "RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization",
    "authors": [
      "Haiyang Shen",
      "Hang Yan",
      "Zhongshi Xing",
      "Mugeng Liu",
      "Yue Li",
      "Zhiyang Chen",
      "Yuxiang Wang",
      "Jiuzheng Wang",
      "Yun Ma"
    ],
    "abstract": "RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various RAG paradigms, including vanilla, planning-based, and iterative RAG, are built upon 2 cores: the retriever, which should robustly select relevant documents across complex queries, and the generator, which should faithfully synthesize responses. However, existing retrievers rely heavily on public knowledge and struggle with queries of varying logical complexity and clue completeness, while generators frequently face fidelity problems. In this work, we introduce RAGSynth, a framework that includes a data construction modeling and a corresponding synthetic data generation implementation, designed to optimize retriever robustness and generator fidelity. Additionally, we present SynthBench, a benchmark encompassing 8 domain-specific documents across 4 domains, featuring diverse query complexities, clue completeness, and fine-grained citation granularity. Leveraging RAGSynth, we generate a large-scale synthetic dataset, including single and multi-hop. Extensive experiments demonstrate that the synthetic data significantly improves the robustness of the retrievers and the fidelity of the generators. Additional evaluations confirm that RAGSynth can also generalize well across different domains. By integrating the optimized retrievers into various RAG paradigms, we consistently observe enhanced RAG system performance. We have open-sourced the implementation on https://github.com/EachSheep/RAGSynth.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10989v1",
    "published_date": "2025-05-16 08:38:25 UTC",
    "updated_date": "2025-05-16 08:38:25 UTC"
  },
  {
    "arxiv_id": "2505.10988v1",
    "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production",
    "authors": [
      "Joon-Young Kim",
      "Jecheon Yu",
      "Heekyu Kim",
      "Seunghwa Ryu"
    ],
    "abstract": "Plastic injection molding remains essential to modern manufacturing. However, optimizing process parameters to balance product quality and profitability under dynamic environmental and economic conditions remains a persistent challenge. This study presents a novel deep reinforcement learning (DRL)-based framework for real-time process optimization in injection molding, integrating product quality and profitability into the control objective. A profit function was developed to reflect real-world manufacturing costs, incorporating resin, mold wear, and electricity prices, including time-of-use variations. Surrogate models were constructed to predict product quality and cycle time, enabling efficient offline training of DRL agents using soft actor-critic (SAC) and proximal policy optimization (PPO) algorithms. Experimental results demonstrate that the proposed DRL framework can dynamically adapt to seasonal and operational variations, consistently maintaining product quality while maximizing profit. Compared to traditional optimization methods such as genetic algorithms, the DRL models achieved comparable economic performance with up to 135x faster inference speeds, making them well-suited for real-time applications. The framework's scalability and adaptability highlight its potential as a foundation for intelligent, data-driven decision-making in modern manufacturing environments.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "50 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10988v1",
    "published_date": "2025-05-16 08:35:31 UTC",
    "updated_date": "2025-05-16 08:35:31 UTC"
  },
  {
    "arxiv_id": "2505.10983v2",
    "title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models",
    "authors": [
      "Haozheng Luo",
      "Chenghao Qiu",
      "Yimin Wang",
      "Shang Wu",
      "Jiahao Yu",
      "Zhenyu Pan",
      "Weian Mao",
      "Haoyang Fang",
      "Hao Xu",
      "Han Liu",
      "Binghui Wang",
      "Yan Chen"
    ],
    "abstract": "We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks, GenoArmory offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Additionally, we introduce GenoAdv, a new adversarial sample dataset designed to improve GFM safety. Empirically, classification models exhibit greater robustness to adversarial perturbations compared to generative models, highlighting the impact of task type on model vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10983v2",
    "published_date": "2025-05-16 08:29:56 UTC",
    "updated_date": "2025-10-11 06:03:20 UTC"
  },
  {
    "arxiv_id": "2505.10982v1",
    "title": "Facets in Argumentation: A Formal Approach to Argument Significance",
    "authors": [
      "Johannes Fichte",
      "Nicolas Fröhlich",
      "Markus Hecher",
      "Victor Lagerkvist",
      "Yasir Mahmood",
      "Arne Meier",
      "Jonathan Persson"
    ],
    "abstract": "Argumentation is a central subarea of Artificial Intelligence (AI) for modeling and reasoning about arguments. The semantics of abstract argumentation frameworks (AFs) is given by sets of arguments (extensions) and conditions on the relationship between them, such as stable or admissible. Today's solvers implement tasks such as finding extensions, deciding credulous or skeptical acceptance, counting, or enumerating extensions. While these tasks are well charted, the area between decision, counting/enumeration and fine-grained reasoning requires expensive reasoning so far. We introduce a novel concept (facets) for reasoning between decision and enumeration. Facets are arguments that belong to some extensions (credulous) but not to all extensions (skeptical). They are most natural when a user aims to navigate, filter, or comprehend the significance of specific arguments, according to their needs. We study the complexity and show that tasks involving facets are much easier than counting extensions. Finally, we provide an implementation, and conduct experiments to demonstrate feasibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10982v1",
    "published_date": "2025-05-16 08:29:38 UTC",
    "updated_date": "2025-05-16 08:29:38 UTC"
  },
  {
    "arxiv_id": "2505.10981v3",
    "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory",
    "authors": [
      "Yexiang Liu",
      "Zekun Li",
      "Zhi Fang",
      "Nan Xu",
      "Ran He",
      "Tieniu Tan"
    ],
    "abstract": "Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies $\\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a probabilistic method to efficiently predict scaling performance and identify the best prompting strategy under large sampling times, eliminating the need for resource-intensive inference processes in practical applications. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance. Code is available at https://github.com/MraDonkey/rethinking_prompting.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2025 Outstanding Paper Award, 33 pages, 51 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10981v3",
    "published_date": "2025-05-16 08:28:57 UTC",
    "updated_date": "2025-08-02 08:06:23 UTC"
  },
  {
    "arxiv_id": "2505.10978v3",
    "title": "Group-in-Group Policy Optimization for LLM Agent Training",
    "authors": [
      "Lang Feng",
      "Zhenghai Xue",
      "Tingcong Liu",
      "Bo An"
    ],
    "abstract": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10978v3",
    "published_date": "2025-05-16 08:26:59 UTC",
    "updated_date": "2025-10-28 15:11:36 UTC"
  },
  {
    "arxiv_id": "2505.10975v2",
    "title": "Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio",
    "authors": [
      "Xinlu He",
      "Jacob Whitehill"
    ],
    "abstract": "Monaural multi-speaker automatic speech recognition (ASR) remains challenging due to data scarcity and the intrinsic difficulty of recognizing and attributing words to individual speakers, particularly in overlapping speech. Recent advances have driven the shift from cascade systems to end-to-end (E2E) architectures, which reduce error propagation and better exploit the synergy between speech content and speaker identity. Despite rapid progress in E2E multi-speaker ASR, the field lacks a comprehensive review of recent developments. This survey provides a systematic taxonomy of E2E neural approaches for multi-speaker ASR, highlighting recent advances and comparative analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO) for pre-segmented audio, analyzing their distinct characteristics and trade-offs; (2) recent architectural and algorithmic improvements based on these two paradigms; (3) extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate and compare methods across standard benchmarks. We conclude with a discussion of open challenges and future research directions towards building robust and scalable multi-speaker ASR.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in Computer Speech & Language (CSL)",
    "pdf_url": "https://arxiv.org/pdf/2505.10975v2",
    "published_date": "2025-05-16 08:21:59 UTC",
    "updated_date": "2026-01-13 19:02:15 UTC"
  },
  {
    "arxiv_id": "2505.10973v3",
    "title": "GRoQ-LoCO: Generalist and Robot-agnostic Quadruped Locomotion Control using Offline Datasets",
    "authors": [
      "Narayanan PP",
      "Sarvesh Prasanth Venkatesan",
      "Srinivas Kantha Reddy",
      "Shishir Kolathaya"
    ],
    "abstract": "Recent advancements in large-scale offline training have demonstrated the potential of generalist policy learning for complex robotic tasks. However, applying these principles to legged locomotion remains a challenge due to continuous dynamics and the need for real-time adaptation across diverse terrains and robot morphologies. In this work, we propose GRoQ-LoCO, a scalable, attention-based framework that learns a single generalist locomotion policy across multiple quadruped robots and terrains, relying solely on offline datasets. Our approach leverages expert demonstrations from two distinct locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain traversal (periodic gaits) - collected across multiple quadruped robots, to train a generalist model that enables behavior fusion. Crucially, our framework operates solely on proprioceptive data from all robots without incorporating any robot-specific encodings. The policy is directly deployable on an Intel i7 nuc, producing low-latency control outputs without any test-time optimization. Our extensive experiments demonstrate zero-shot transfer across highly diverse quadruped robots and terrains, including hardware deployment on the Unitree Go1, a commercially available 12kg robot. Notably, we evaluate challenging cross-robot training setups where different locomotion skills are unevenly distributed across robots, yet observe successful transfer of both flat walking and stair traversal behaviors to all robots at test time. We also show preliminary walking on Stoch 5, a 70kg quadruped, on flat and outdoor terrains without requiring any fine tuning. These results demonstrate the potential of offline, data-driven learning to generalize locomotion across diverse quadruped morphologies and behaviors.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "18pages, 16figures, 6tables",
    "pdf_url": "https://arxiv.org/pdf/2505.10973v3",
    "published_date": "2025-05-16 08:17:01 UTC",
    "updated_date": "2025-05-24 18:28:59 UTC"
  },
  {
    "arxiv_id": "2505.10962v1",
    "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation",
    "authors": [
      "Zhenwen Liang",
      "Linfeng Song",
      "Yang Li",
      "Tao Yang",
      "Feng Zhang",
      "Haitao Mi",
      "Dong Yu"
    ],
    "abstract": "Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in Progress",
    "pdf_url": "https://arxiv.org/pdf/2505.10962v1",
    "published_date": "2025-05-16 07:56:03 UTC",
    "updated_date": "2025-05-16 07:56:03 UTC"
  },
  {
    "arxiv_id": "2505.10961v2",
    "title": "Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection using LLM-Based Agents",
    "authors": [
      "Ratnadira Widyasari",
      "Martin Weyssow",
      "Ivana Clairine Irsan",
      "Han Wei Ang",
      "Frank Liauw",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "Hong Jin Kang",
      "David Lo"
    ],
    "abstract": "Detecting vulnerabilities in source code remains a critical yet challenging task, especially when benign and vulnerable functions share significant similarities. In this work, we introduce VulTrial, a courtroom-inspired multi-agent framework designed to identify vulnerable code and to provide explanations. It employs four role-specific agents, which are security researcher, code author, moderator, and review board. Using GPT-4o as the base LLM, VulTrial almost doubles the efficacy of prior best-performing baselines. Additionally, we show that role-specific instruction tuning with small quantities of data significantly further boosts VulTrial's efficacy. Our extensive experiments demonstrate the efficacy of VulTrial across different LLMs, including an open-source, in-house-deployable model (LLaMA-3.1-8B), as well as the high quality of its generated explanations and its ability to uncover multiple confirmed zero-day vulnerabilities in the wild.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10961v2",
    "published_date": "2025-05-16 07:54:10 UTC",
    "updated_date": "2025-12-03 22:14:37 UTC"
  },
  {
    "arxiv_id": "2505.10960v1",
    "title": "Relational Graph Transformer",
    "authors": [
      "Vijay Prakash Dwivedi",
      "Sri Jaladi",
      "Yangyi Shen",
      "Federico López",
      "Charilaos I. Kanatsoulis",
      "Rishi Puri",
      "Matthias Fey",
      "Jure Leskovec"
    ],
    "abstract": "Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/snap-stanford/relgt",
    "pdf_url": "https://arxiv.org/pdf/2505.10960v1",
    "published_date": "2025-05-16 07:51:58 UTC",
    "updated_date": "2025-05-16 07:51:58 UTC"
  },
  {
    "arxiv_id": "2506.01979v2",
    "title": "Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism",
    "authors": [
      "Yuhao Shen",
      "Junyi Shen",
      "Quan Kong",
      "Tianyu Liu",
      "Yao Lu",
      "Cong Wang"
    ],
    "abstract": "Speculative decoding (SD) has emerged as a promising technique to accelerate LLM inference by employing a small draft model to propose draft tokens in advance, and validating them in parallel with the large target model. However, the existing SD methods still remain constrained by their serialized execution, which causes the mutual waiting bubbles between the draft and target models. To address this challenge, we draw inspiration from branch prediction in modern processors and propose a novel framework \\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first take an in-depth analysis of the potential of branch parallelism in SD, and recognize that the key challenge lies in the trade-offs between parallelization and token rollback. Based on the analysis, we introduce parallel speculative branches to preemptively hedge against likely rejections. Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft lengths with a hybrid combination of the implicit draft model confidence and explicit reusing of target model features. Extensive experiments across various models and benchmarks show that SpecBranch achieves over \\textbf{1.8}$\\times \\sim$ \\textbf{4.5}$\\times$ speedups against the auto-regressive decoding and reduces rollback tokens by $\\textbf{50}$\\% for poorly aligned models, while maintaining an identical sampling distribution.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.01979v2",
    "published_date": "2025-05-16 07:45:05 UTC",
    "updated_date": "2025-09-23 05:29:24 UTC"
  },
  {
    "arxiv_id": "2505.10954v1",
    "title": "Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design",
    "authors": [
      "Koki Iwai",
      "Yusuke Kumagae",
      "Yuki Koyama",
      "Masahiro Hamasaki",
      "Masataka Goto"
    ],
    "abstract": "Preferential Bayesian optimization (PBO) is a variant of Bayesian optimization that observes relative preferences (e.g., pairwise comparisons) instead of direct objective values, making it especially suitable for human-in-the-loop scenarios. However, real-world optimization tasks often involve inequality constraints, which existing PBO methods have not yet addressed. To fill this gap, we propose constrained preferential Bayesian optimization (CPBO), an extension of PBO that incorporates inequality constraints for the first time. Specifically, we present a novel acquisition function for this purpose. Our technical evaluation shows that our CPBO method successfully identifies optimal solutions by focusing on exploring feasible regions. As a practical application, we also present a designer-in-the-loop system for banner ad design using CPBO, where the objective is the designer's subjective preference, and the constraint ensures a target predicted click-through rate. We conducted a user study with professional ad designers, demonstrating the potential benefits of our approach in guiding creative design under real-world constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 15 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10954v1",
    "published_date": "2025-05-16 07:41:07 UTC",
    "updated_date": "2025-05-16 07:41:07 UTC"
  },
  {
    "arxiv_id": "2505.10946v2",
    "title": "ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications",
    "authors": [
      "Li Qiao",
      "Mahdi Boloursaz Mashhadi",
      "Zhen Gao",
      "Robert Schober",
      "Deniz Gündüz"
    ],
    "abstract": "Token communications (TokCom) is an emerging generative semantic communication concept that reduces transmission rates by using context and multimodal large language model (MLLM)-based token processing, with tokens serving as universal semantic units across modalities. In this paper, we propose a semantic multiple access scheme in the token domain, referred to as token domain multiple access (ToDMA), where a large number of devices share a token codebook and a modulation codebook for source and channel coding, respectively. Specifically, each transmitter first tokenizes its source signal and modulate each token to a codeword. At the receiver, compressed sensing is employed first to detect active tokens and the corresponding channel state information (CSI) from the superposed signals. Then, the source token sequences are reconstructed by clustering the token-associated CSI across multiple time slots. In case of token collisions, some active tokens cannot be assigned and some positions in the reconstructed token sequences are empty. We propose to use pre-trained MLLMs to leverage the context, predict masked tokens, and thus mitigate token collisions. Simulation results demonstrate the effectiveness of the proposed ToDMA framework for both text and image transmission tasks, achieving significantly lower latency compared to context-unaware orthogonal communication schemes, while also delivering superior distortion and perceptual quality compared to state-of-the-art context-unaware non-orthogonal communication methods.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.IT",
    "comment": "Submitted to IEEE journals",
    "pdf_url": "https://arxiv.org/pdf/2505.10946v2",
    "published_date": "2025-05-16 07:30:42 UTC",
    "updated_date": "2025-07-17 03:28:57 UTC"
  },
  {
    "arxiv_id": "2505.10945v2",
    "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer",
    "authors": [
      "Seungyoon Lee",
      "Seongtae Hong",
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ],
    "abstract": "Large Language Models (LLMs) increasingly incorporate multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model's embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies, to handle each non-overlapping token's embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods and achieves lower loss with accelerating faster convergence during language adaptation. Notably, SALT obtains remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.10945v2",
    "published_date": "2025-05-16 07:30:22 UTC",
    "updated_date": "2025-05-22 11:54:30 UTC"
  },
  {
    "arxiv_id": "2505.10940v3",
    "title": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation",
    "authors": [
      "Qing Yu",
      "Xiaobei Wang",
      "Shuchang Liu",
      "Yandong Bai",
      "Xiaoyu Yang",
      "Xueliang Wang",
      "Chang Meng",
      "Shanshan Wu",
      "Hailan Yang",
      "Huihui Xiao",
      "Xiang Li",
      "Fan Yang",
      "Xiaoqiang Feng",
      "Lantao Hu",
      "Han Li",
      "Kun Gai",
      "Lixin Zou"
    ],
    "abstract": "Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks. Our code is available in https://github.com/Code2Q/TagCF.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "to be published in NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10940v3",
    "published_date": "2025-05-16 07:26:41 UTC",
    "updated_date": "2025-10-29 14:42:46 UTC"
  },
  {
    "arxiv_id": "2505.10939v2",
    "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction",
    "authors": [
      "Mohammadtaha Bagherifard",
      "Sahar Rajabi",
      "Ali Edalat",
      "Yadollah Yaghoobzadeh"
    ],
    "abstract": "Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \\citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.10939v2",
    "published_date": "2025-05-16 07:23:59 UTC",
    "updated_date": "2025-08-04 10:29:40 UTC"
  },
  {
    "arxiv_id": "2505.10937v1",
    "title": "Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations",
    "authors": [
      "Wenrui Cai",
      "Chengyu Wang",
      "Junbing Yan",
      "Jun Huang",
      "Xiangzhong Fang"
    ],
    "abstract": "The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However, the advancement of LRMs is hindered by the lack of comprehensive CoT datasets. Current resources often fail to provide extensive reasoning problems with coherent CoT processes distilled from multiple teacher models and do not account for multifaceted properties describing the internal characteristics of CoTs. To address these challenges, we introduce OmniThought, a large-scale dataset featuring 2 million CoT processes generated and validated by two powerful LRMs as teacher models. Each CoT process in OmniThought is annotated with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which describe the appropriateness of CoT verbosity and cognitive difficulty level for models to comprehend these reasoning processes. We further establish a self-reliant pipeline to curate this dataset. Extensive experiments using Qwen2.5 models of various sizes demonstrate the positive impact of our proposed scores on LRM training effectiveness. Based on the proposed OmniThought dataset, we further train and release a series of high-performing LRMs, specifically equipped with stronger reasoning abilities and optimal CoT output length and difficulty level. Our contributions significantly enhance the development and training of LRMs for solving complex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10937v1",
    "published_date": "2025-05-16 07:15:30 UTC",
    "updated_date": "2025-05-16 07:15:30 UTC"
  },
  {
    "arxiv_id": "2505.15832v1",
    "title": "From Hand-Crafted Metrics to Evolved Training-Free Performance Predictors for Neural Architecture Search via Genetic Programming",
    "authors": [
      "Quan Minh Phan",
      "Ngoc Hoang Luong"
    ],
    "abstract": "Estimating the network performance using zero-cost (ZC) metrics has proven both its efficiency and efficacy in Neural Architecture Search (NAS). However, a notable limitation of most ZC proxies is their inconsistency, as reflected by the substantial variation in their performance across different problems. Furthermore, the design of existing ZC metrics is manual, involving a time-consuming trial-and-error process that requires substantial domain expertise. These challenges raise two critical questions: (1) Can we automate the design of ZC metrics? and (2) Can we utilize the existing hand-crafted ZC metrics to synthesize a more generalizable one? In this study, we propose a framework based on Symbolic Regression via Genetic Programming to automate the design of ZC metrics. Our framework is not only highly extensible but also capable of quickly producing a ZC metric with a strong positive rank correlation to true network performance across diverse NAS search spaces and tasks. Extensive experiments on 13 problems from NAS-Bench-Suite-Zero demonstrate that our automatically generated proxies consistently outperform hand-crafted alternatives. Using our evolved proxy metric as the search objective in an evolutionary algorithm, we could identify network architectures with competitive performance within 15 minutes using a single consumer GPU.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15832v1",
    "published_date": "2025-05-16 07:12:42 UTC",
    "updated_date": "2025-05-16 07:12:42 UTC"
  },
  {
    "arxiv_id": "2505.11563v1",
    "title": "Object-Centric Representations Improve Policy Generalization in Robot Manipulation",
    "authors": [
      "Alexandre Chapin",
      "Bruno Machado",
      "Emmanuel Dellandrea",
      "Liming Chen"
    ],
    "abstract": "Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as a structured alternative that segments visual input into a finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark a range of visual encoders-object-centric, global and dense methods-across a suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11563v1",
    "published_date": "2025-05-16 07:06:37 UTC",
    "updated_date": "2025-05-16 07:06:37 UTC"
  },
  {
    "arxiv_id": "2505.10924v3",
    "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?",
    "authors": [
      "Ada Chen",
      "Yongjiang Wu",
      "Junyuan Zhang",
      "Jingyu Xiao",
      "Shu Yang",
      "Jen-tse Huang",
      "Kun Wang",
      "Wenxuan Wang",
      "Shuai Wang"
    ],
    "abstract": "Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety analysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs; \\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10924v3",
    "published_date": "2025-05-16 06:56:42 UTC",
    "updated_date": "2025-08-25 04:43:19 UTC"
  },
  {
    "arxiv_id": "2505.10922v1",
    "title": "Vaiage: A Multi-Agent Solution to Personalized Travel Planning",
    "authors": [
      "Binwen Liu",
      "Jiexi Ge",
      "Jiamin Wang"
    ],
    "abstract": "Planning trips is a cognitively intensive task involving conflicting user preferences, dynamic external information, and multi-step temporal-spatial optimization. Traditional platforms often fall short - they provide static results, lack contextual adaptation, and fail to support real-time interaction or intent refinement.\n  Our approach, Vaiage, addresses these challenges through a graph-structured multi-agent framework built around large language models (LLMs) that serve as both goal-conditioned recommenders and sequential planners. LLMs infer user intent, suggest personalized destinations and activities, and synthesize itineraries that align with contextual constraints such as budget, timing, group size, and weather. Through natural language interaction, structured tool use, and map-based feedback loops, Vaiage enables adaptive, explainable, and end-to-end travel planning grounded in both symbolic reasoning and conversational understanding.\n  To evaluate Vaiage, we conducted human-in-the-loop experiments using rubric-based GPT-4 assessments and qualitative feedback. The full system achieved an average score of 8.5 out of 10, outperforming the no-strategy (7.2) and no-external-API (6.8) variants, particularly in feasibility. Qualitative analysis indicated that agent coordination - especially the Strategy and Information Agents - significantly improved itinerary quality by optimizing time use and integrating real-time context. These results demonstrate the effectiveness of combining LLM reasoning with symbolic agent coordination in open-ended, real-world planning tasks.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10922v1",
    "published_date": "2025-05-16 06:54:52 UTC",
    "updated_date": "2025-05-16 06:54:52 UTC"
  },
  {
    "arxiv_id": "2505.10909v1",
    "title": "Phi: Leveraging Pattern-based Hierarchical Sparsity for High-Efficiency Spiking Neural Networks",
    "authors": [
      "Chiyue Wei",
      "Bowen Duan",
      "Cong Guo",
      "Jingyang Zhang",
      "Qingyue Song",
      "Hai \"Helen\" Li",
      "Yiran Chen"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are gaining attention for their energy efficiency and biological plausibility, utilizing 0-1 activation sparsity through spike-driven computation. While existing SNN accelerators exploit this sparsity to skip zero computations, they often overlook the unique distribution patterns inherent in binary activations. In this work, we observe that particular patterns exist in spike activations, which we can utilize to reduce the substantial computation of SNN models. Based on these findings, we propose a novel \\textbf{pattern-based hierarchical sparsity} framework, termed \\textbf{\\textit{Phi}}, to optimize computation.\n  \\textit{Phi} introduces a two-level sparsity hierarchy: Level 1 exhibits vector-wise sparsity by representing activations with pre-defined patterns, allowing for offline pre-computation with weights and significantly reducing most runtime computation. Level 2 features element-wise sparsity by complementing the Level 1 matrix, using a highly sparse matrix to further reduce computation while maintaining accuracy. We present an algorithm-hardware co-design approach. Algorithmically, we employ a k-means-based pattern selection method to identify representative patterns and introduce a pattern-aware fine-tuning technique to enhance Level 2 sparsity. Architecturally, we design \\textbf{\\textit{Phi}}, a dedicated hardware architecture that efficiently processes the two levels of \\textit{Phi} sparsity on the fly. Extensive experiments demonstrate that \\textit{Phi} achieves a $3.45\\times$ speedup and a $4.93\\times$ improvement in energy efficiency compared to state-of-the-art SNN accelerators, showcasing the effectiveness of our framework in optimizing SNN computation.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "ISCA 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10909v1",
    "published_date": "2025-05-16 06:29:24 UTC",
    "updated_date": "2025-05-16 06:29:24 UTC"
  },
  {
    "arxiv_id": "2505.17049v2",
    "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations",
    "authors": [
      "David Rozado"
    ],
    "abstract": "This study examines the behavior of Large Language Models (LLMs) when evaluating professional candidates based on their resumes or curricula vitae (CVs). In an experiment involving 22 leading LLMs, each model was systematically given one job description along with a pair of profession-matched CVs, one bearing a male first name, the other a female first name, and asked to select the more suitable candidate for the job. Each CV pair was presented twice, with names swapped to ensure that any observed preferences in candidate selection stemmed from gendered names cues. Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates across 70 different professions. Adding an explicit gender field (male/female) to the CVs further increased the preference for female applicants. When gendered names were replaced with gender-neutral identifiers \"Candidate A\" and \"Candidate B\", several models displayed a preference to select \"Candidate A\". Counterbalancing gender assignment between these gender-neutral identifiers resulted in gender parity in candidate selection. When asked to rate CVs in isolation rather than compare pairs, LLMs assigned slightly higher average scores to female CVs overall, but the effect size was negligible. Including preferred pronouns (he/him or she/her) next to a candidate's name slightly increased the odds of the candidate being selected regardless of gender. Finally, most models exhibited a substantial positional bias to select the candidate listed first in the prompt. These findings underscore the need for caution when deploying LLMs in high-stakes autonomous decision-making contexts and raise doubts about whether LLMs consistently apply principled reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17049v2",
    "published_date": "2025-05-16 06:19:35 UTC",
    "updated_date": "2025-05-27 00:07:04 UTC"
  },
  {
    "arxiv_id": "2505.10903v1",
    "title": "On the Security Risks of ML-based Malware Detection Systems: A Survey",
    "authors": [
      "Ping He",
      "Yuhao Mao",
      "Changjiang Li",
      "Lorenzo Cavallaro",
      "Ting Wang",
      "Shouling Ji"
    ],
    "abstract": "Malware presents a persistent threat to user privacy and data integrity. To combat this, machine learning-based (ML-based) malware detection (MD) systems have been developed. However, these systems have increasingly been attacked in recent years, undermining their effectiveness in practice. While the security risks associated with ML-based MD systems have garnered considerable attention, the majority of prior works is limited to adversarial malware examples, lacking a comprehensive analysis of practical security risks. This paper addresses this gap by utilizing the CIA principles to define the scope of security risks. We then deconstruct ML-based MD systems into distinct operational stages, thus developing a stage-based taxonomy. Utilizing this taxonomy, we summarize the technical progress and discuss the gaps in the attack and defense proposals related to the ML-based MD systems within each stage. Subsequently, we conduct two case studies, using both inter-stage and intra-stage analyses according to the stage-based taxonomy to provide new empirical insights. Based on these analyses and insights, we suggest potential future directions from both inter-stage and intra-stage perspectives.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10903v1",
    "published_date": "2025-05-16 06:15:31 UTC",
    "updated_date": "2025-05-16 06:15:31 UTC"
  },
  {
    "arxiv_id": "2505.10900v3",
    "title": "Tuning-Free LLM Can Build A Strong Recommender Under Sparse Connectivity And Knowledge Gap Via Extracting Intent",
    "authors": [
      "Wenqing Zheng",
      "Noah Fatsi",
      "Daniel Barcklow",
      "Dmitri Kalaev",
      "Steven Yao",
      "Owen Reinert",
      "C. Bayan Bruss",
      "Daniele Rosa"
    ],
    "abstract": "Recent advances in recommendation with large language models (LLMs) often rely on either commonsense augmentation at the item-category level or implicit intent modeling on existing knowledge graphs. However, such approaches struggle to capture grounded user intents and to handle sparsity and cold-start scenarios. In this work, we present LLM-based Intent Knowledge Graph Recommender (IKGR), a novel framework that constructs an intent-centric knowledge graph where both users and items are explicitly linked to intent nodes extracted by a tuning-free, RAG-guided LLM pipeline. By grounding intents in external knowledge sources and user profiles, IKGR canonically represents what a user seeks and what an item satisfies as first-class entities. To alleviate sparsity, we further introduce a mutual-intent connectivity densification strategy, which shortens semantic paths between users and long-tail items without requiring cross-graph fusion. Finally, a lightweight GNN layer is employed on top of the intent-enhanced graph to produce recommendation signals with low latency. Extensive experiments on public and enterprise datasets demonstrate that IKGR consistently outperforms strong baselines, particularly on cold-start and long-tail slices, while remaining efficient through a fully offline LLM pipeline.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10900v3",
    "published_date": "2025-05-16 06:07:19 UTC",
    "updated_date": "2025-09-15 20:05:33 UTC"
  },
  {
    "arxiv_id": "2505.10887v2",
    "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction",
    "authors": [
      "Bin Lei",
      "Weitai Kang",
      "Zijian Zhang",
      "Winson Chen",
      "Xi Xie",
      "Shan Zuo",
      "Mimi Xie",
      "Ali Payani",
      "Mingyi Hong",
      "Yan Yan",
      "Caiwen Ding"
    ],
    "abstract": "This paper introduces \\textsc{InfantAgent-Next}, a generalist agent capable of interacting with computers in a multimodal manner, encompassing text, images, audio, and video. Unlike existing approaches that either build intricate workflows around a single large model or only provide workflow modularity, our agent integrates tool-based and pure vision agents within a highly modular architecture, enabling different models to collaboratively solve decoupled tasks in a step-by-step manner. Our generality is demonstrated by our ability to evaluate not only pure vision-based real-world benchmarks (i.e., OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and SWE-Bench). Specifically, we achieve $\\mathbf{7.27\\%}$ accuracy on OSWorld, higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced at https://github.com/bin123apple/InfantAgent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10887v2",
    "published_date": "2025-05-16 05:43:27 UTC",
    "updated_date": "2025-05-23 19:56:38 UTC"
  },
  {
    "arxiv_id": "2505.10885v1",
    "title": "BanglaFake: Constructing and Evaluating a Specialized Bengali Deepfake Audio Dataset",
    "authors": [
      "Istiaq Ahmed Fahad",
      "Kamruzzaman Asif",
      "Sifat Sikder"
    ],
    "abstract": "Deepfake audio detection is challenging for low-resource languages like Bengali due to limited datasets and subtle acoustic features. To address this, we introduce BangalFake, a Bengali Deepfake Audio Dataset with 12,260 real and 13,260 deepfake utterances. Synthetic speech is generated using SOTA Text-to-Speech (TTS) models, ensuring high naturalness and quality. We evaluate the dataset through both qualitative and quantitative analyses. Mean Opinion Score (MOS) from 30 native speakers shows Robust-MOS of 3.40 (naturalness) and 4.01 (intelligibility). t-SNE visualization of MFCCs highlights real vs. fake differentiation challenges. This dataset serves as a crucial resource for advancing deepfake detection in Bengali, addressing the limitations of low-resource language research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 page",
    "pdf_url": "https://arxiv.org/pdf/2505.10885v1",
    "published_date": "2025-05-16 05:42:25 UTC",
    "updated_date": "2025-05-16 05:42:25 UTC"
  },
  {
    "arxiv_id": "2505.10877v1",
    "title": "Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations",
    "authors": [
      "Mathieu Alain",
      "So Takao",
      "Xiaowen Dong",
      "Bastian Rieck",
      "Emmanuel Noutahi"
    ],
    "abstract": "Predicting the labels of graph-structured data is crucial in scientific applications and is often achieved using graph neural networks (GNNs). However, when data is scarce, GNNs suffer from overfitting, leading to poor performance. Recently, Gaussian processes (GPs) with graph-level inputs have been proposed as an alternative. In this work, we extend the Gaussian process framework to simplicial complexes (SCs), enabling the handling of edge-level attributes and attributes supported on higher-order simplices. We further augment the resulting SC representations by considering their Hodge decompositions, allowing us to account for homological information, such as the number of holes, in the SC. We demonstrate that our framework enhances the predictions across various applications, paving the way for GPs to be more widely used for graph and SC-level predictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10877v1",
    "published_date": "2025-05-16 05:33:42 UTC",
    "updated_date": "2025-05-16 05:33:42 UTC"
  },
  {
    "arxiv_id": "2505.10876v2",
    "title": "Preference Isolation Forest for Structure-based Anomaly Detection",
    "authors": [
      "Filippo Leveni",
      "Luca Magri",
      "Cesare Alippi",
      "Giacomo Boracchi"
    ],
    "abstract": "We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most general solution, $ii$) RuzHash-iForest, that avoids explicit computation of distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a locality prior to improve efficiency and effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Pattern Recognition (2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.10876v2",
    "published_date": "2025-05-16 05:32:25 UTC",
    "updated_date": "2025-09-17 18:59:36 UTC"
  },
  {
    "arxiv_id": "2505.10874v1",
    "title": "MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection",
    "authors": [
      "Luca Magri",
      "Filippo Leveni",
      "Giacomo Boracchi"
    ],
    "abstract": "We address the problem of recovering multiple structures of different classes in a dataset contaminated by noise and outliers. In particular, we consider geometric structures defined by a mixture of underlying parametric models (e.g. planes and cylinders, homographies and fundamental matrices), and we tackle the robust fitting problem by preference analysis and clustering. We present a new algorithm, termed MultiLink, that simultaneously deals with multiple classes of models. MultiLink combines on-the-fly model fitting and model selection in a novel linkage scheme that determines whether two clusters are to be merged. The resulting method features many practical advantages with respect to methods based on preference analysis, being faster, less sensitive to the inlier threshold, and able to compensate limitations deriving from hypotheses sampling. Experiments on several public datasets demonstrate that Multi-Link favourably compares with state of the art alternatives, both in multi-class and single-class problems. Code is publicly made available for download.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Computer Vision and Pattern Recognition (CVPR 2021)",
    "pdf_url": "https://arxiv.org/pdf/2505.10874v1",
    "published_date": "2025-05-16 05:32:02 UTC",
    "updated_date": "2025-05-16 05:32:02 UTC"
  },
  {
    "arxiv_id": "2505.10873v1",
    "title": "Hashing for Structure-based Anomaly Detection",
    "authors": [
      "Filippo Leveni",
      "Luca Magri",
      "Cesare Alippi",
      "Giacomo Boracchi"
    ],
    "abstract": "We focus on the problem of identifying samples in a set that do not conform to structured patterns represented by low-dimensional manifolds. An effective way to solve this problem is to embed data in a high dimensional space, called Preference Space, where anomalies can be identified as the most isolated points. In this work, we employ Locality Sensitive Hashing to avoid explicit computation of distances in high dimensions and thus improve Anomaly Detection efficiency. Specifically, we present an isolation-based anomaly detection technique designed to work in the Preference Space which achieves state-of-the-art performance at a lower computational cost. Code is publicly available at https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at International Conference on Image Analysis and Processing (ICIAP 2023)",
    "pdf_url": "https://arxiv.org/pdf/2505.10873v1",
    "published_date": "2025-05-16 05:31:50 UTC",
    "updated_date": "2025-05-16 05:31:50 UTC"
  },
  {
    "arxiv_id": "2505.10872v2",
    "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?",
    "authors": [
      "Chenxi Jiang",
      "Chuhao Zhou",
      "Jianfei Yang"
    ],
    "abstract": "Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.RO",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2505.10872v2",
    "published_date": "2025-05-16 05:27:15 UTC",
    "updated_date": "2025-05-19 17:21:49 UTC"
  },
  {
    "arxiv_id": "2505.10871v1",
    "title": "Optimal Allocation of Privacy Budget on Hierarchical Data Release",
    "authors": [
      "Joonhyuk Ko",
      "Juba Ziani",
      "Ferdinando Fioretto"
    ],
    "abstract": "Releasing useful information from datasets with hierarchical structures while preserving individual privacy presents a significant challenge. Standard privacy-preserving mechanisms, and in particular Differential Privacy, often require careful allocation of a finite privacy budget across different levels and components of the hierarchy. Sub-optimal allocation can lead to either excessive noise, rendering the data useless, or to insufficient protections for sensitive information. This paper addresses the critical problem of optimal privacy budget allocation for hierarchical data release. It formulates this challenge as a constrained optimization problem, aiming to maximize data utility subject to a total privacy budget while considering the inherent trade-offs between data granularity and privacy loss. The proposed approach is supported by theoretical analysis and validated through comprehensive experiments on real hierarchical datasets. These experiments demonstrate that optimal privacy budget allocation significantly enhances the utility of the released data and improves the performance of downstream tasks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10871v1",
    "published_date": "2025-05-16 05:25:11 UTC",
    "updated_date": "2025-05-16 05:25:11 UTC"
  },
  {
    "arxiv_id": "2505.10870v1",
    "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate",
    "authors": [
      "Ziyang Huang",
      "Wangtao Sun",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "This paper systematically addresses the challenges of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10870v1",
    "published_date": "2025-05-16 05:22:42 UTC",
    "updated_date": "2025-05-16 05:22:42 UTC"
  },
  {
    "arxiv_id": "2505.10859v1",
    "title": "MCU: Improving Machine Unlearning through Mode Connectivity",
    "authors": [
      "Yingdan Shi",
      "Ren Wang"
    ],
    "abstract": "Machine Unlearning (MU) aims to remove the information of specific training data from a trained model, ensuring compliance with privacy regulations and user requests. While one line of existing MU methods relies on linear parameter updates via task arithmetic, they suffer from weight entanglement. In this work, we propose a novel MU framework called Mode Connectivity Unlearning (MCU) that leverages mode connectivity to find an unlearning pathway in a nonlinear manner. To further enhance performance and efficiency, we introduce a parameter mask strategy that not only improves unlearning effectiveness but also reduces computational overhead. Moreover, we propose an adaptive adjustment strategy for our unlearning penalty coefficient to adaptively balance forgetting quality and predictive performance during training, eliminating the need for empirical hyperparameter tuning. Unlike traditional MU methods that identify only a single unlearning model, MCU uncovers a spectrum of unlearning models along the pathway. Overall, MCU serves as a plug-and-play framework that seamlessly integrates with any existing MU methods, consistently improving unlearning efficacy. Extensive experiments on the image classification task demonstrate that MCU achieves superior performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10859v1",
    "published_date": "2025-05-16 04:56:47 UTC",
    "updated_date": "2025-05-16 04:56:47 UTC"
  },
  {
    "arxiv_id": "2505.10856v1",
    "title": "ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data",
    "authors": [
      "Mengxuan Li",
      "Ke Liu",
      "Jialong Guo",
      "Jiajun Bu",
      "Hongwei Wang",
      "Haishuai Wang"
    ],
    "abstract": "Healthcare data frequently contain a substantial proportion of missing values, necessitating effective time series imputation to support downstream disease diagnosis tasks. However, existing imputation methods focus on discrete data points and are unable to effectively model sparse data, resulting in particularly poor performance for imputing substantial missing values. In this paper, we propose a novel approach, ImputeINR, for time series imputation by employing implicit neural representations (INR) to learn continuous functions for time series. ImputeINR leverages the merits of INR in that the continuous functions are not coupled to sampling frequency and have infinite sampling frequency, allowing ImputeINR to generate fine-grained imputations even on extremely sparse observed values. Extensive experiments conducted on eight datasets with five ratios of masked values show the superior imputation performance of ImputeINR, especially for high missing ratios in time series data. Furthermore, we validate that applying ImputeINR to impute missing values in healthcare data enhances the performance of downstream disease diagnosis tasks. Codes are available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10856v1",
    "published_date": "2025-05-16 04:50:15 UTC",
    "updated_date": "2025-05-16 04:50:15 UTC"
  },
  {
    "arxiv_id": "2505.10845v1",
    "title": "Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness",
    "authors": [
      "Hanyu Duan",
      "Yi Yang",
      "Ahmed Abbasi",
      "Kar Yan Tam"
    ],
    "abstract": "This paper introduces Ready2Unlearn, a learning-time optimization approach designed to facilitate future unlearning processes. Unlike the majority of existing unlearning efforts that focus on designing unlearning algorithms, which are typically implemented reactively when an unlearning request is made during the model deployment phase, Ready2Unlearn shifts the focus to the training phase, adopting a \"forward-looking\" perspective. Building upon well-established meta-learning principles, Ready2Unlearn proactively trains machine learning models with unlearning readiness, such that they are well prepared and can handle future unlearning requests in a more efficient and principled manner. Ready2Unlearn is model-agnostic and compatible with any gradient ascent-based machine unlearning algorithms. We evaluate the method on both vision and language tasks under various unlearning settings, including class-wise unlearning and random data unlearning. Experimental results show that by incorporating such preparedness at training time, Ready2Unlearn produces an unlearning-ready model state, which offers several key advantages when future unlearning is required, including reduced unlearning time, improved retention of overall model capability, and enhanced resistance to the inadvertent recovery of forgotten data. We hope this work could inspire future efforts to explore more proactive strategies for equipping machine learning models with built-in readiness towards more reliable and principled machine unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10845v1",
    "published_date": "2025-05-16 04:33:59 UTC",
    "updated_date": "2025-05-16 04:33:59 UTC"
  },
  {
    "arxiv_id": "2505.10844v4",
    "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models",
    "authors": [
      "Simeng Han",
      "Howard Dai",
      "Stephen Xia",
      "Grant Zhang",
      "Chen Liu",
      "Lichang Chen",
      "Hoang Huy Nguyen",
      "Hongyuan Mei",
      "Jiayuan Mao",
      "R. Thomas McCoy"
    ],
    "abstract": "Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10844v4",
    "published_date": "2025-05-16 04:23:34 UTC",
    "updated_date": "2025-10-28 21:30:31 UTC"
  },
  {
    "arxiv_id": "2505.10834v1",
    "title": "TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding",
    "authors": [
      "Achintha Wijesinghe",
      "Weiwei Wang",
      "Suchinthaka Wanninayaka",
      "Songyang Zhang",
      "Zhi Ding"
    ],
    "abstract": "Recent advancements in generative artificial intelligence have introduced groundbreaking approaches to innovating next-generation semantic communication, which prioritizes conveying the meaning of a message rather than merely transmitting raw data. A fundamental challenge in semantic communication lies in accurately identifying and extracting the most critical semantic information while adapting to downstream tasks without degrading performance, particularly when the objective at the receiver may evolve over time. To enable flexible adaptation to multiple tasks at the receiver, this work introduces a novel semantic communication framework, which is capable of jointly capturing task-specific information to enhance downstream task performance and contextual information. Through rigorous experiments on popular image datasets and computer vision tasks, our framework shows promising improvement compared to existing work, including superior performance in downstream tasks, better generalizability, ultra-high bandwidth efficiency, and low reconstruction latency.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to the IEEE GlobeCom 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10834v1",
    "published_date": "2025-05-16 04:03:52 UTC",
    "updated_date": "2025-05-16 04:03:52 UTC"
  },
  {
    "arxiv_id": "2505.10832v3",
    "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL",
    "authors": [
      "Songjun Tu",
      "Jiahao Lin",
      "Qichao Zhang",
      "Xiangyu Tian",
      "Linjing Li",
      "Xiangyuan Lan",
      "Dongbin Zhao"
    ],
    "abstract": "Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis (\"...\") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs. Project Page: https://github.com/ScienceOne-AI/AutoThink.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10832v3",
    "published_date": "2025-05-16 04:01:57 UTC",
    "updated_date": "2025-09-27 04:35:52 UTC"
  },
  {
    "arxiv_id": "2505.10831v3",
    "title": "Creating General User Models from Computer Use",
    "authors": [
      "Omar Shaikh",
      "Shardul Sapkota",
      "Shan Rizvi",
      "Eric Horvitz",
      "Joon Sung Park",
      "Diyi Yang",
      "Michael S. Bernstein"
    ],
    "abstract": "Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "23 pages, 6 figures, 2 tables; see https://generalusermodels.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2505.10831v3",
    "published_date": "2025-05-16 04:00:31 UTC",
    "updated_date": "2025-09-21 00:03:34 UTC"
  },
  {
    "arxiv_id": "2505.10829v1",
    "title": "Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances",
    "authors": [
      "Chen-Chi Chang",
      "Chong-Fu Li",
      "Chu-Hsuan Lee",
      "Hung-Shin Lee"
    ],
    "abstract": "This study investigates the challenges of translating low-resource languages by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG). Various model configurations were tested on Hakka translations, with BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0). The best-performing model (Model 4) combined retrieval and advanced language modeling, improving lexical coverage, particularly for specialized or culturally nuanced terms, and enhancing grammatical coherence. A two-stage method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU score of 26%, highlighting iterative correction's value and the challenges of domain-specific expressions. Static dictionary-based approaches struggled with context-sensitive content, demonstrating the limitations of relying solely on predefined resources. These results emphasize the need for curated resources, domain knowledge, and ethical collaboration with local communities, offering a framework that improves translation accuracy and fluency while supporting cultural preservation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to IntelliSys 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10829v1",
    "published_date": "2025-05-16 03:59:14 UTC",
    "updated_date": "2025-05-16 03:59:14 UTC"
  },
  {
    "arxiv_id": "2506.01977v2",
    "title": "Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN",
    "authors": [
      "Wei Huang",
      "Hanchen Wang",
      "Dong Wen",
      "Shaozhen Ma",
      "Wenjie Zhang",
      "Xuemin Lin"
    ],
    "abstract": "Graph Edit Distance (GED) is a fundamental graph similarity metric widely used in various applications. However, computing GED is an NP-hard problem. Recent state-of-the-art hybrid GED solver has shown promising performance by formulating GED as a bipartite graph matching problem, then leveraging a generative diffusion model to predict node matching between two graphs, from which both the GED and its corresponding edit path can be extracted using a traditional algorithm. However, such methods typically rely heavily on ground-truth supervision, where the ground-truth node matchings are often costly to obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel unsupervised GAN-based framework for GED computation. Specifically, GEDRanker consists of a matching-based GED solver and introduces an interpretable preference-aware discriminator. By leveraging preference signals over different node matchings derived from edit path lengths, the discriminator can guide the matching-based solver toward generating high-quality node matching without the need for ground-truth supervision. Extensive experiments on benchmark datasets demonstrate that our GEDRanker enables the matching-based GED solver to achieve near-optimal solution quality without any ground-truth supervision.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.01977v2",
    "published_date": "2025-05-16 03:45:31 UTC",
    "updated_date": "2025-10-13 02:53:27 UTC"
  },
  {
    "arxiv_id": "2505.10819v4",
    "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts",
    "authors": [
      "Wasu Top Piriyakulkij",
      "Yichao Liang",
      "Hao Tang",
      "Adrian Weller",
      "Marta Kryven",
      "Kevin Ellis"
    ],
    "abstract": "Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10819v4",
    "published_date": "2025-05-16 03:28:42 UTC",
    "updated_date": "2025-11-19 22:23:01 UTC"
  },
  {
    "arxiv_id": "2505.11559v1",
    "title": "Analysis and Resilience of the U.S. Flight Network",
    "authors": [
      "Sushrit Kafle",
      "Shreejan Pandey"
    ],
    "abstract": "Air travel is one of the most widely used transportation services in the United States. This paper analyzes the U.S. Flight Network (USFN) using complex network theory by exploring how the network's topology contributes to its efficiency and vulnerability. This is done by examining the structural properties, degree distributions, and community structures in the network. USFN was observed to follow power-law distribution and falls under the anomalous regime, suggesting that the network is hub dominant. Compared to null networks, USFN has a higher clustering coefficient and modularity. Various percolation test revealed that USFN is vulnerable to targeted attacks and is susceptible to complete cascading failure if one of the major hubs fails. The overall results suggest that while the USFN is designed for efficiency, it is highly vulnerable to disruptions. Protecting key hub airports is important to make the network more robust and prevent large-scale failures.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "Investigates resilience of the U.S. flight network under node failures. Includes percolation threshold detection, cascade simulations, and community structure analysis. 9 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.11559v1",
    "published_date": "2025-05-16 03:13:54 UTC",
    "updated_date": "2025-05-16 03:13:54 UTC"
  },
  {
    "arxiv_id": "2505.10803v1",
    "title": "Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management",
    "authors": [
      "Zhaoan Wang",
      "Wonseok Jang",
      "Bowen Ruan",
      "Jun Wang",
      "Shaoping Xiao"
    ],
    "abstract": "Precision agriculture, enhanced by artificial intelligence (AI), offers promising tools such as remote sensing, intelligent irrigation, fertilization management, and crop simulation to improve agricultural efficiency and sustainability. Reinforcement learning (RL), in particular, has outperformed traditional methods in optimizing yields and resource management. However, widespread AI adoption is limited by gaps between algorithmic recommendations and farmers' practical experience, local knowledge, and traditional practices. To address this, our study emphasizes Human-AI Interaction (HAII), focusing on transparency, usability, and trust in RL-based farm management. We employ a well-established trust framework - comprising ability, benevolence, and integrity - to develop a novel mathematical model quantifying farmers' confidence in AI-based fertilization strategies. Surveys conducted with farmers for this research reveal critical misalignments, which are integrated into our trust model and incorporated into a multi-objective RL framework. Unlike prior methods, our approach embeds trust directly into policy optimization, ensuring AI recommendations are technically robust, economically feasible, context-aware, and socially acceptable. By aligning technical performance with human-centered trust, this research supports broader AI adoption in agriculture.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10803v1",
    "published_date": "2025-05-16 02:52:16 UTC",
    "updated_date": "2025-05-16 02:52:16 UTC"
  },
  {
    "arxiv_id": "2505.10802v1",
    "title": "Attention-Based Reward Shaping for Sparse and Delayed Rewards",
    "authors": [
      "Ian Holmes",
      "Min Chi"
    ],
    "abstract": "Sparse and delayed reward functions pose a significant obstacle for real-world Reinforcement Learning (RL) applications. In this work, we propose Attention-based REward Shaping (ARES), a general and robust algorithm which uses a transformer's attention mechanism to generate shaped rewards and create a dense reward function for any environment. ARES requires a set of episodes and their final returns as input. It can be trained entirely offline and is able to generate meaningful shaped rewards even when using small datasets or episodes produced by agents taking random actions. ARES is compatible with any RL algorithm and can handle any level of reward sparsity. In our experiments, we focus on the most challenging case where rewards are fully delayed until the end of each episode. We evaluate ARES across a diverse range of environments, widely used RL algorithms, and baseline methods to assess the effectiveness of the shaped rewards it produces. Our results show that ARES can significantly improve learning in delayed reward settings, enabling RL agents to train in scenarios that would otherwise require impractical amounts of data or even be unlearnable. To our knowledge, ARES is the first approach that works fully offline, remains robust to extreme reward delays and low-quality data, and is not limited to goal-based tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 17 tables, 2 figures. Code available online at https://github.com/ihholmes-p/ARES",
    "pdf_url": "https://arxiv.org/pdf/2505.10802v1",
    "published_date": "2025-05-16 02:43:05 UTC",
    "updated_date": "2025-05-16 02:43:05 UTC"
  },
  {
    "arxiv_id": "2505.10791v1",
    "title": "Analyzing Patterns and Influence of Advertising in Print Newspapers",
    "authors": [
      "N Harsha Vardhan",
      "Ponnurangam Kumaraguru",
      "Kiran Garimella"
    ],
    "abstract": "This paper investigates advertising practices in print newspapers across India using a novel data-driven approach. We develop a pipeline employing image processing and OCR techniques to extract articles and advertisements from digital versions of print newspapers with high accuracy. Applying this methodology to five popular newspapers that span multiple regions and three languages, English, Hindi, and Telugu, we assembled a dataset of more than 12,000 editions containing several hundred thousand advertisements. Collectively, these newspapers reach a readership of over 100 million people. Using this extensive dataset, we conduct a comprehensive analysis to answer key questions about print advertising: who advertises, what they advertise, when they advertise, where they place their ads, and how they advertise. Our findings reveal significant patterns, including the consistent level of print advertising over the past six years despite declining print circulation, the overrepresentation of company ads on prominent pages, and the disproportionate revenue contributed by government ads. Furthermore, we examine whether advertising in a newspaper influences the coverage an advertiser receives. Through regression analyses on coverage volume and sentiment, we find strong evidence supporting this hypothesis for corporate advertisers. The results indicate a clear trend where increased advertising correlates with more favorable and extensive media coverage, a relationship that remains robust over time and across different levels of advertiser popularity.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at COMPASS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10791v1",
    "published_date": "2025-05-16 02:05:53 UTC",
    "updated_date": "2025-05-16 02:05:53 UTC"
  },
  {
    "arxiv_id": "2505.10790v1",
    "title": "Neural-Inspired Advances in Integral Cryptanalysis",
    "authors": [
      "Liu Zhang",
      "Yiran Yao",
      "Danping Shi",
      "Dongchen Chai",
      "Jian Guo",
      "Zilong Wang"
    ],
    "abstract": "The study by Gohr et.al at CRYPTO 2019 and sunsequent related works have shown that neural networks can uncover previously unused features, offering novel insights into cryptanalysis. Motivated by these findings, we employ neural networks to learn features specifically related to integral properties and integrate the corresponding insights into optimized search frameworks. These findings validate the framework of using neural networks for feature exploration, providing researchers with novel insights that advance established cryptanalysis methods.\n  Neural networks have inspired the development of more precise integral search models. By comparing the integral distinguishers obtained via neural networks with those identified by classical methods, we observe that existing automated search models often fail to find optimal distinguishers. To address this issue, we develop a meet in the middle search framework that balances model accuracy and computational efficiency. As a result, we reduce the number of active plaintext bits required for an 11 rounds integral distinguisher on SKINNY64/64, and further identify a 12 rounds key dependent integral distinguisher achieving one additional round over the previous best-known result.\n  The integral distinguishers discovered by neural networks enable key recovery attacks on more rounds. We identify a 7 rounds key independent integral distinguisher from neural networks with even only one active plaintext cell, which is based on linear combinations of bits. This distinguisher enables a 15 rounds key recovery attack on SKINNYn/n, improving upon the previous record by one round. Additionally, we discover an 8 rounds key dependent integral distinguisher using neural network that further reduces the time complexity of key recovery attacks against SKINNY.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10790v1",
    "published_date": "2025-05-16 02:05:13 UTC",
    "updated_date": "2025-05-16 02:05:13 UTC"
  },
  {
    "arxiv_id": "2505.10781v1",
    "title": "Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation",
    "authors": [
      "David Minkwan Kim",
      "Soeun Lee",
      "Byeongkeun Kang"
    ],
    "abstract": "This work addresses the task of completely weakly supervised class-incremental learning for semantic segmentation to learn segmentation for both base and additional novel classes using only image-level labels. While class-incremental semantic segmentation (CISS) is crucial for handling diverse and newly emerging objects in the real world, traditional CISS methods require expensive pixel-level annotations for training. To overcome this limitation, partially weakly-supervised approaches have recently been proposed. However, to the best of our knowledge, this is the first work to introduce a completely weakly-supervised method for CISS. To achieve this, we propose to generate robust pseudo-labels by combining pseudo-labels from a localizer and a sequence of foundation models based on their uncertainty. Moreover, to mitigate catastrophic forgetting, we introduce an exemplar-guided data augmentation method that generates diverse images containing both previous and novel classes with guidance. Finally, we conduct experiments in three common experimental settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint and overlap. The experimental results demonstrate that our completely weakly supervised method outperforms even partially weakly supervised methods in the 15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the COCO-to-VOC setting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.10781v1",
    "published_date": "2025-05-16 01:43:36 UTC",
    "updated_date": "2025-05-16 01:43:36 UTC"
  },
  {
    "arxiv_id": "2505.10780v1",
    "title": "SECRET: Semi-supervised Clinical Trial Document Similarity Search",
    "authors": [
      "Trisha Das",
      "Afrah Shafquat",
      "Beigi Mandis",
      "Jacob Aptekar",
      "Jimeng Sun"
    ],
    "abstract": "Clinical trials are vital for evaluation of safety and efficacy of new treatments. However, clinical trials are resource-intensive, time-consuming and expensive to conduct, where errors in trial design, reduced efficacy, and safety events can result in significant delays, financial losses, and damage to reputation. These risks underline the importance of informed and strategic decisions in trial design to mitigate these risks and improve the chances of a successful trial. Identifying similar historical trials is critical as these trials can provide an important reference for potential pitfalls and challenges including serious adverse events, dosage inaccuracies, recruitment difficulties, patient adherence issues, etc. Addressing these challenges in trial design can lead to development of more effective study protocols with optimized patient safety and trial efficiency. In this paper, we present a novel method to identify similar historical trials by summarizing clinical trial protocols and searching for similar trials based on a query trial's protocol. Our approach significantly outperforms all baselines, achieving up to a 78% improvement in recall@1 and a 53% improvement in precision@1 over the best baseline. We also show that our method outperforms all other baselines in partial trial similarity search and zero-shot patient-trial matching, highlighting its superior utility in these tasks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.10780v1",
    "published_date": "2025-05-16 01:34:16 UTC",
    "updated_date": "2025-05-16 01:34:16 UTC"
  },
  {
    "arxiv_id": "2505.10779v1",
    "title": "Qualia Optimization",
    "authors": [
      "Philip S. Thomas"
    ],
    "abstract": "This report explores the speculative question: what if current or future AI systems have qualia, such as pain or pleasure? It does so by assuming that AI systems might someday possess qualia -- and that the quality of these subjective experiences should be considered alongside performance metrics. Concrete mathematical problem settings, inspired by reinforcement learning formulations and theories from philosophy of mind, are then proposed and initial approaches and properties are presented. These properties enable refinement of the problem setting, culminating with the proposal of methods that promote reinforcement.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical Report, College of Information and Computer Science, University of Massachusetts",
    "pdf_url": "https://arxiv.org/pdf/2505.10779v1",
    "published_date": "2025-05-16 01:34:03 UTC",
    "updated_date": "2025-05-16 01:34:03 UTC"
  },
  {
    "arxiv_id": "2505.13500v2",
    "title": "Noise Injection Systemically Degrades Large Language Model Safety Guardrails",
    "authors": [
      "Prithviraj Singh Shahani",
      "Kaveh Eskandari Miandoab",
      "Matthias Scheutz"
    ],
    "abstract": "Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages,3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.13500v2",
    "published_date": "2025-05-16 01:33:25 UTC",
    "updated_date": "2025-10-12 20:22:06 UTC"
  },
  {
    "arxiv_id": "2505.10775v1",
    "title": "A Systematic Analysis of Base Model Choice for Reward Modeling",
    "authors": [
      "Kian Ahrabian",
      "Pegah Jandaghi",
      "Negar Mokhberian",
      "Sai Praneeth Karimireddy",
      "Jay Pujara"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection ($+$18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 13 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.10775v1",
    "published_date": "2025-05-16 01:27:03 UTC",
    "updated_date": "2025-05-16 01:27:03 UTC"
  },
  {
    "arxiv_id": "2505.10774v2",
    "title": "Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting",
    "authors": [
      "Yueyang Yao",
      "Jiajun Li",
      "Xingyuan Dai",
      "MengMeng Zhang",
      "Xiaoyan Gong",
      "Fei-Yue Wang",
      "Yisheng Lv"
    ],
    "abstract": "Time series forecasting is important for applications spanning energy markets, climate analysis, and traffic management. However, existing methods struggle to effectively integrate exogenous texts and align them with the probabilistic nature of large language models (LLMs). Current approaches either employ shallow text-time series fusion via basic prompts or rely on deterministic numerical decoding that conflict with LLMs' token-generation paradigm, which limits contextual awareness and distribution modeling. To address these limitations, we propose CAPTime, a context-aware probabilistic multimodal time series forecasting method that leverages text-informed abstraction and autoregressive LLM decoding. Our method first encodes temporal patterns using a pretrained time series encoder, then aligns them with textual contexts via learnable interactions to produce joint multimodal representations. By combining a mixture of distribution experts with frozen LLMs, we enable context-aware probabilistic forecasting while preserving LLMs' inherent distribution modeling capabilities. Experiments on diverse time series forecasting tasks demonstrate the superior accuracy and generalization of CAPTime, particularly in multimodal scenarios. Additional analysis highlights its robustness in data-scarce scenarios through hybrid probabilistic decoding.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.10774v2",
    "published_date": "2025-05-16 01:23:53 UTC",
    "updated_date": "2025-07-29 14:43:48 UTC"
  },
  {
    "arxiv_id": "2505.10770v1",
    "title": "Geofenced Unmanned Aerial Robotic Defender for Deer Detection and Deterrence (GUARD)",
    "authors": [
      "Ebasa Temesgen",
      "Mario Jerez",
      "Greta Brown",
      "Graham Wilson",
      "Sree Ganesh Lalitaditya Divakarla",
      "Sarah Boelter",
      "Oscar Nelson",
      "Robert McPherson",
      "Maria Gini"
    ],
    "abstract": "Wildlife-induced crop damage, particularly from deer, threatens agricultural productivity. Traditional deterrence methods often fall short in scalability, responsiveness, and adaptability to diverse farmland environments. This paper presents an integrated unmanned aerial vehicle (UAV) system designed for autonomous wildlife deterrence, developed as part of the Farm Robotics Challenge. Our system combines a YOLO-based real-time computer vision module for deer detection, an energy-efficient coverage path planning algorithm for efficient field monitoring, and an autonomous charging station for continuous operation of the UAV. In collaboration with a local Minnesota farmer, the system is tailored to address practical constraints such as terrain, infrastructure limitations, and animal behavior. The solution is evaluated through a combination of simulation and field testing, demonstrating robust detection accuracy, efficient coverage, and extended operational time. The results highlight the feasibility and effectiveness of drone-based wildlife deterrence in precision agriculture, offering a scalable framework for future deployment and extension.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the Novel Approaches for Precision Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.10770v1",
    "published_date": "2025-05-16 00:59:31 UTC",
    "updated_date": "2025-05-16 00:59:31 UTC"
  },
  {
    "arxiv_id": "2505.13499v2",
    "title": "Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency",
    "authors": [
      "Kelvin Kan",
      "Xingjian Li",
      "Benjamin J. Zhang",
      "Tuhin Sahai",
      "Stanley Osher",
      "Markos A. Katsoulakis"
    ],
    "abstract": "We study Transformers through the perspective of optimal control theory, using tools from continuous-time formulations to derive actionable insights into training and architecture design. This framework improves the performance of existing Transformer models while providing desirable theoretical guarantees, including generalization and robustness. Our framework is designed to be plug-and-play, enabling seamless integration with established Transformer models and requiring only slight changes to the implementation. We conduct seven extensive experiments on tasks motivated by text generation, sentiment analysis, image classification, and point cloud classification. Experimental results show that the framework improves the test performance of the baselines, while being more parameter-efficient. On character-level text generation with nanoGPT, our framework achieves a 46% reduction in final test loss while using 42% fewer parameters. On GPT-2, our framework achieves a 9.3% reduction in final test loss, demonstrating scalability to larger models. To the best of our knowledge, this is the first work that applies optimal control theory to both the training and architecture of Transformers. It offers a new foundation for systematic, theory-driven improvements and moves beyond costly trial-and-error approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13499v2",
    "published_date": "2025-05-16 00:31:10 UTC",
    "updated_date": "2025-10-23 23:48:33 UTC"
  },
  {
    "arxiv_id": "2505.13498v1",
    "title": "IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation",
    "authors": [
      "Khanh-Tung Tran",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias, restrict evaluation to text-only, rely on multiple-choice formats, and, more importantly, are limited for extremely low-resource languages. To address these gaps, we introduce IRLBench, presented in parallel English and Irish, which is considered definitely endangered by UNESCO. Our benchmark consists of 12 representative subjects developed from the 2024 Irish Leaving Certificate exams, enabling fine-grained analysis of model capabilities across domains. By framing the task as long-form generation and leveraging the official marking scheme, it does not only support a comprehensive evaluation of correctness but also language fidelity. Our extensive experiments of leading closed-source and open-source LLMs reveal a persistent performance gap between English and Irish, in which models produce valid Irish responses less than 80\\% of the time, and answer correctly 55.8\\% of the time compared to 76.2\\% in English for the best-performing model. We release IRLBench (https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future research on robust, culturally aware multilingual AI development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13498v1",
    "published_date": "2025-05-16 00:02:05 UTC",
    "updated_date": "2025-05-16 00:02:05 UTC"
  }
]
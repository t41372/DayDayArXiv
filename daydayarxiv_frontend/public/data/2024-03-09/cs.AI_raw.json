[
  {
    "arxiv_id": "2403.06041v1",
    "title": "MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts",
    "authors": [
      "Zhuo Xu",
      "Rui Zhou",
      "Yida Yin",
      "Huidong Gao",
      "Masayoshi Tomizuka",
      "Jiachen Li"
    ],
    "abstract": "Data-driven methods have great advantages in modeling complicated human\nbehavioral dynamics and dealing with many human-robot interaction applications.\nHowever, collecting massive and annotated real-world human datasets has been a\nlaborious task, especially for highly interactive scenarios. On the other hand,\nalgorithmic data generation methods are usually limited by their model\ncapacities, making them unable to offer realistic and diverse data needed by\nvarious application users. In this work, we study trajectory-level data\ngeneration for multi-human or human-robot interaction scenarios and propose a\nlearning-based automatic trajectory generation model, which we call Multi-Agent\nTRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of\ngenerating interactive human behaviors in realistic diverse contexts. We\nachieve this goal by modeling the explicit and interpretable objectives so that\nMATRIX can generate human motions based on diverse destinations and\nheterogeneous behaviors. We carried out extensive comparison and ablation\nstudies to illustrate the effectiveness of our approach across various metrics.\nWe also presented experiments that demonstrate the capability of MATRIX to\nserve as data augmentation for imitation-based motion planning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE International Conference on Robotics and Automation (ICRA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.06041v1",
    "published_date": "2024-03-09 23:28:54 UTC",
    "updated_date": "2024-03-09 23:28:54 UTC"
  },
  {
    "arxiv_id": "2405.00688v1",
    "title": "Understanding Social Perception, Interactions, and Safety Aspects of Sidewalk Delivery Robots Using Sentiment Analysis",
    "authors": [
      "Yuchen Du",
      "Tho V. Le"
    ],
    "abstract": "This article presents a comprehensive sentiment analysis (SA) of comments on\nYouTube videos related to Sidewalk Delivery Robots (SDRs). We manually\nannotated the collected YouTube comments with three sentiment labels: negative\n(0), positive (1), and neutral (2). We then constructed models for text\nsentiment classification and tested the models' performance on both binary and\nternary classification tasks in terms of accuracy, precision, recall, and F1\nscore. Our results indicate that, in binary classification tasks, the Support\nVector Machine (SVM) model using Term Frequency-Inverse Document Frequency\n(TF-IDF) and N-gram get the highest accuracy. In ternary classification tasks,\nthe model using Bidirectional Encoder Representations from Transformers (BERT),\nLong Short-Term Memory Networks (LSTM) and Gated Recurrent Unit (GRU)\nsignificantly outperforms other machine learning models, achieving an accuracy,\nprecision, recall, and F1 score of 0.78. Additionally, we employ the Latent\nDirichlet Allocation model to generate 10 topics from the comments to explore\nthe public's underlying views on SDRs. Drawing from these findings, we propose\ntargeted recommendations for shaping future policies concerning SDRs. This work\nprovides valuable insights for stakeholders in the SDR sector regarding social\nperception, interaction, and safety.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "34 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00688v1",
    "published_date": "2024-03-09 23:28:01 UTC",
    "updated_date": "2024-03-09 23:28:01 UTC"
  },
  {
    "arxiv_id": "2403.06039v1",
    "title": "A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation",
    "authors": [
      "Yao Lyu",
      "He Zhang",
      "Shuo Niu",
      "Jie Cai"
    ],
    "abstract": "Content creators increasingly utilize generative artificial intelligence\n(Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging\nsites to produce imaginative images, AI-generated videos, and articles using\nLarge Language Models (LLMs). Despite its growing popularity, there remains an\nunderexplored area concerning the specific domains where AI-generated content\nis being applied, and the methodologies content creators employ with Gen-AI\ntools during the creation process. This study initially explores this emerging\narea through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI\nusage. Our research focuses on identifying the content domains, the variety of\ntools used, the activities performed, and the nature of the final products\ngenerated by Gen-AI in the context of user-generated content.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at CHI LBW 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06039v1",
    "published_date": "2024-03-09 23:22:56 UTC",
    "updated_date": "2024-03-09 23:22:56 UTC"
  },
  {
    "arxiv_id": "2403.06031v1",
    "title": "FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition",
    "authors": [
      "Dalia Gala",
      "Milo Phillips-Brown",
      "Naman Goel",
      "Carinal Prunkl",
      "Laura Alvarez Jubete",
      "medb corcoran",
      "Ray Eitel-Porter"
    ],
    "abstract": "Machine learning requires defining one's target variable for predictions or\ndecisions, a process that can have profound implications on fairness: biases\nare often encoded in target variable definition itself, before any data\ncollection or training. We present an interactive simulator, FairTargetSim\n(FTS), that illustrates how target variable definition impacts fairness. FTS is\na valuable tool for algorithm developers, researchers, and non-technical\nstakeholders. FTS uses a case study of algorithmic hiring, using real-world\ndata and user-defined target variables. FTS is open-source and available at:\nhttp://tinyurl.com/ftsinterface. The video accompanying this paper is here:\nhttp://tinyurl.com/ijcaifts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06031v1",
    "published_date": "2024-03-09 22:41:33 UTC",
    "updated_date": "2024-03-09 22:41:33 UTC"
  },
  {
    "arxiv_id": "2403.06026v2",
    "title": "Towards a Generic Representation of Combinatorial Problems for Learning-Based Approaches",
    "authors": [
      "Léo Boisvert",
      "Hélène Verhaeghe",
      "Quentin Cappart"
    ],
    "abstract": "In recent years, there has been a growing interest in using learning-based\napproaches for solving combinatorial problems, either in an end-to-end manner\nor in conjunction with traditional optimization algorithms. In both scenarios,\nthe challenge lies in encoding the targeted combinatorial problems into a\nstructure compatible with the learning algorithm. Many existing works have\nproposed problem-specific representations, often in the form of a graph, to\nleverage the advantages of \\textit{graph neural networks}. However, these\napproaches lack generality, as the representation cannot be easily transferred\nfrom one combinatorial problem to another one. While some attempts have been\nmade to bridge this gap, they still offer a partial generality only. In\nresponse to this challenge, this paper advocates for progress toward a fully\ngeneric representation of combinatorial problems for learning-based approaches.\nThe approach we propose involves constructing a graph by breaking down any\nconstraint of a combinatorial problem into an abstract syntax tree and\nexpressing relationships (e.g., a variable involved in a constraint) through\nthe edges. Furthermore, we introduce a graph neural network architecture\ncapable of efficiently learning from this representation. The tool provided\noperates on combinatorial problems expressed in the XCSP3 format, handling all\nthe constraints available in the 2023 mini-track competition. Experimental\nresults on four combinatorial problems demonstrate that our architecture\nachieves performance comparable to dedicated architectures while maintaining\ngenerality. Our code and trained models are publicly available at\n\\url{https://github.com/corail-research/learning-generic-csp}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06026v2",
    "published_date": "2024-03-09 22:28:46 UTC",
    "updated_date": "2024-03-13 00:09:46 UTC"
  },
  {
    "arxiv_id": "2403.06025v3",
    "title": "CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming",
    "authors": [
      "Wei Chen",
      "Yunan Li",
      "Yuan Tian"
    ],
    "abstract": "We introduce a new approach using computer vision to predict the land surface\ndisplacement from subsurface geometry images for Carbon Capture and\nSequestration (CCS). CCS has been proved to be a key component for a carbon\nneutral society. However, scientists see there are challenges along the way\nincluding the high computational cost due to the large model scale and\nlimitations to generalize a pre-trained model with complex physics. We tackle\nthose challenges by training models directly from the subsurface geometry\nimages. The goal is to understand the respons of land surface displacement due\nto carbon injection and utilize our trained models to inform decision making in\nCCS projects.\n  We implement multiple models (CNN, ResNet, and ResNetUNet) for static\nmechanics problem, which is a image prediction problem. Next, we use the LSTM\nand transformer for transient mechanics scenario, which is a video prediction\nproblem. It shows ResNetUNet outperforms the others thanks to its architecture\nin static mechanics problem, and LSTM shows comparable performance to\ntransformer in transient problem. This report proceeds by outlining our dataset\nin detail followed by model descriptions in method section. Result and\ndiscussion state the key learning, observations, and conclusion with future\nwork rounds out the paper.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06025v3",
    "published_date": "2024-03-09 22:25:14 UTC",
    "updated_date": "2024-03-19 05:58:51 UTC"
  },
  {
    "arxiv_id": "2403.06018v1",
    "title": "Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages",
    "authors": [
      "Christopher Toukmaji"
    ],
    "abstract": "Large pre-trained language models (PLMs) are at the forefront of advances in\nNatural Language Processing. One widespread use case of PLMs is \"prompting\" -\nor in-context learning - where a user provides a description of a task and some\ncompleted examples of the task to a PLM as context before prompting the PLM to\nperform the task on a new example. Only the largest, most capable PLMs are able\nto perform in-context learning effectively, and these models are typically\ntrained with a predominantly English corpus, leaving all other languages\nbehind. The data limitations in most languages preclude the training of\nlanguage-specific PLMs capable of prompting. Albeit the surge in work of\nprompting settings, it is still unclear how PLMs should be adapted\ncross-lingually specifically for prompting. We evaluate the possible methods to\nadapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for\nprompting in low-resource languages, namely for Kinyarwanda, Hausa, and\nLuganda. We consider three methods: few-shot prompting (prompt),\nlanguage-adaptive fine-tuning (LAFT), and neural machine translation\n(translate), and evaluate on abstractive summarization, multi-class topic\nclassification, and named-entity recognition. Although LAFT carries the\ngreatest compute cost and intuitively should lead to the best results, our\nexperiments exhibit that LAFT is only occasionally the optimal choice for\nadapting PLMs for prompting. Rather, the translate and prompt settings are a\ncompute-efficient and cost-effective method of few-shot prompting for the\nselected low-resource languages. We find that the results are task and language\ndependent but find that the prompting method is the best on average across all\ntasks and languages. Results show that the prompt setting performs better than\nboth translating and LAFT with statistical significance for all shots when\naggregated across all tasks and languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "47 pages, 26 figures; a thesis submitted in partial satisfaction of\n  the requirements for the degree of Bachelor of Science in Computer Science at\n  the University of California - Santa Cruz",
    "pdf_url": "http://arxiv.org/pdf/2403.06018v1",
    "published_date": "2024-03-09 21:36:13 UTC",
    "updated_date": "2024-03-09 21:36:13 UTC"
  },
  {
    "arxiv_id": "2403.06014v1",
    "title": "Hard-label based Small Query Black-box Adversarial Attack",
    "authors": [
      "Jeonghwan Park",
      "Paul Miller",
      "Niall McLaughlin"
    ],
    "abstract": "We consider the hard label based black box adversarial attack setting which\nsolely observes predicted classes from the target model. Most of the attack\nmethods in this setting suffer from impractical number of queries required to\nachieve a successful attack. One approach to tackle this drawback is utilising\nthe adversarial transferability between white box surrogate models and black\nbox target model. However, the majority of the methods adopting this approach\nare soft label based to take the full advantage of zeroth order optimisation.\nUnlike mainstream methods, we propose a new practical setting of hard label\nbased attack with an optimisation process guided by a pretrained surrogate\nmodel. Experiments show the proposed method significantly improves the query\nefficiency of the hard label based black-box attack across various target model\narchitectures. We find the proposed method achieves approximately 5 times\nhigher attack success rate compared to the benchmarks, especially at the small\nquery budgets as 100 and 250.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.06014v1",
    "published_date": "2024-03-09 21:26:22 UTC",
    "updated_date": "2024-03-09 21:26:22 UTC"
  },
  {
    "arxiv_id": "2403.06003v1",
    "title": "A Generalized Acquisition Function for Preference-based Reward Learning",
    "authors": [
      "Evan Ellis",
      "Gaurav R. Ghosal",
      "Stuart J. Russell",
      "Anca Dragan",
      "Erdem Bıyık"
    ],
    "abstract": "Preference-based reward learning is a popular technique for teaching robots\nand autonomous systems how a human user wants them to perform a task. Previous\nworks have shown that actively synthesizing preference queries to maximize\ninformation gain about the reward function parameters improves data efficiency.\nThe information gain criterion focuses on precisely identifying all parameters\nof the reward function. This can potentially be wasteful as many parameters may\nresult in the same reward, and many rewards may result in the same behavior in\nthe downstream tasks. Instead, we show that it is possible to optimize for\nlearning the reward function up to a behavioral equivalence class, such as\ninducing the same ranking over behaviors, distribution over choices, or other\nrelated definitions of what makes two rewards similar. We introduce a tractable\nframework that can capture such definitions of similarity. Our experiments in a\nsynthetic environment, an assistive robotics environment with domain transfer,\nand a natural language processing problem with real datasets demonstrate the\nsuperior performance of our querying method over the state-of-the-art\ninformation gain method.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06003v1",
    "published_date": "2024-03-09 20:32:17 UTC",
    "updated_date": "2024-03-09 20:32:17 UTC"
  },
  {
    "arxiv_id": "2403.05996v3",
    "title": "Dissecting Deep RL with High Update Ratios: Combatting Value Divergence",
    "authors": [
      "Marcel Hussing",
      "Claas Voelcker",
      "Igor Gilitschenski",
      "Amir-massoud Farahmand",
      "Eric Eaton"
    ],
    "abstract": "We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a conference paper at the First Reinforcement Learning\n  Conference (RLC)",
    "pdf_url": "http://arxiv.org/pdf/2403.05996v3",
    "published_date": "2024-03-09 19:56:40 UTC",
    "updated_date": "2024-08-05 11:55:19 UTC"
  },
  {
    "arxiv_id": "2403.05973v1",
    "title": "Calibrating Large Language Models Using Their Generations Only",
    "authors": [
      "Dennis Ulmer",
      "Martin Gubri",
      "Hwaran Lee",
      "Sangdoo Yun",
      "Seong Joon Oh"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in user-facing\napplications, building trust and maintaining safety by accurately quantifying a\nmodel's confidence in its prediction becomes even more important. However,\nfinding effective ways to calibrate LLMs - especially when the only interface\nto the models is their generated text - remains a challenge. We propose APRICOT\n(auxiliary prediction of confidence targets): A method to set confidence\ntargets and train an additional model that predicts an LLM's confidence based\non its textual input and output alone. This approach has several advantages: It\nis conceptually simple, does not require access to the target model beyond its\noutput, does not interfere with the language generation, and has a multitude of\npotential usages, for instance by verbalizing the predicted confidence or\nadjusting the given answer based on the confidence. We show how our approach\nperforms competitively in terms of calibration error for white-box and\nblack-box LLMs on closed-book question-answering to detect incorrect LLM\nanswers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05973v1",
    "published_date": "2024-03-09 17:46:24 UTC",
    "updated_date": "2024-03-09 17:46:24 UTC"
  },
  {
    "arxiv_id": "2403.07017v1",
    "title": "Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence",
    "authors": [
      "Long Wang",
      "Feng Fu",
      "Xingru Chen"
    ],
    "abstract": "Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two\nfields that, at first glance, might seem distinct, but they have notable\nconnections and intersections. The former focuses on the evolution of behaviors\n(or strategies) in a population, where individuals interact with others and\nupdate their strategies based on imitation (or social learning). The more\nsuccessful a strategy is, the more prevalent it becomes over time. The latter,\nmeanwhile, is centered on machine learning algorithms and (deep) neural\nnetworks. It is often from a single-agent perspective but increasingly involves\nmulti-agent environments, in which intelligent agents adjust their strategies\nbased on feedback and experience, somewhat akin to the evolutionary process yet\ndistinct in their self-learning capacities. In light of the key components\nnecessary to address real-world problems, including (i) learning and\nadaptation, (ii) cooperation and competition, (iii) robustness and stability,\nand altogether (iv) population dynamics of individual agents whose strategies\nevolve, the cross-fertilization of ideas between both fields will contribute to\nthe advancement of mathematics of multi-agent learning systems, in particular,\nto the nascent domain of ``collective cooperative intelligence'' bridging\nevolutionary dynamics and multi-agent reinforcement learning.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.07017v1",
    "published_date": "2024-03-09 17:36:54 UTC",
    "updated_date": "2024-03-09 17:36:54 UTC"
  },
  {
    "arxiv_id": "2403.05950v2",
    "title": "Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach",
    "authors": [
      "Ramin Mousa",
      "Mitra Khezli",
      "Mohamadreza Azadi",
      "Vahid Nikoofard",
      "Saba Hesaraki"
    ],
    "abstract": "Accurate classification of objects in 3D point clouds is a significant\nproblem in several applications, such as autonomous navigation and\naugmented/virtual reality scenarios, which has become a research hot spot. In\nthis paper, we presented a deep learning strategy for 3D object classification\nin augmented reality. The proposed approach is a combination of the GRU and\nLSTM. LSTM networks learn longer dependencies well, but due to the number of\ngates, it takes longer to train; on the other hand, GRU networks have a weaker\nperformance than LSTM, but their training speed is much higher than GRU, which\nis The speed is due to its fewer gates. The proposed approach used the\ncombination of speed and accuracy of these two networks. The proposed approach\nachieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes\neight classes (unlabeled, man-made terrain, natural terrain, high vegetation,\nlow vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the\ntraditional machine learning approaches could achieve a maximum accuracy of\n0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,\nHybrid Model, GRULSTM, GRU, LSTM",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05950v2",
    "published_date": "2024-03-09 16:05:31 UTC",
    "updated_date": "2024-03-28 17:14:53 UTC"
  },
  {
    "arxiv_id": "2403.05932v1",
    "title": "Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis",
    "authors": [
      "Roi Ronen",
      "Ilan Koren",
      "Aviad Levis",
      "Eshkol Eytan",
      "Vadim Holodovsky",
      "Yoav Y. Schechner"
    ],
    "abstract": "Significant uncertainty in climate prediction and cloud physics is tied to\nobservational gaps relating to shallow scattered clouds. Addressing these\nchallenges requires remote sensing of their three-dimensional (3D)\nheterogeneous volumetric scattering content. This calls for passive scattering\ncomputed tomography (CT). We design a learning-based model (ProbCT) to achieve\nCT of such clouds, based on noisy multi-view spaceborne images. ProbCT infers -\nfor the first time - the posterior probability distribution of the\nheterogeneous extinction coefficient, per 3D location. This yields arbitrary\nvaluable statistics, e.g., the 3D field of the most probable extinction and its\nuncertainty. ProbCT uses a neural-field representation, making essentially\nreal-time inference. ProbCT undergoes supervised training by a new labeled\nmulti-class database of physics-based volumetric fields of clouds and their\ncorresponding images. To improve out-of-distribution inference, we incorporate\nself-supervised learning through differential rendering. We demonstrate the\napproach in simulations and on real-world data, and indicate the relevance of\n3D recovery and uncertainty to precipitation and renewable energy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05932v1",
    "published_date": "2024-03-09 14:57:03 UTC",
    "updated_date": "2024-03-09 14:57:03 UTC"
  },
  {
    "arxiv_id": "2403.05921v2",
    "title": "OntoChat: a Framework for Conversational Ontology Engineering using Language Models",
    "authors": [
      "Bohui Zhang",
      "Valentina Anita Carriero",
      "Katrin Schreiberhuber",
      "Stefani Tsaneva",
      "Lucía Sánchez González",
      "Jongmo Kim",
      "Jacopo de Berardinis"
    ],
    "abstract": "Ontology engineering (OE) in large projects poses a number of challenges\narising from the heterogeneous backgrounds of the various stakeholders, domain\nexperts, and their complex interactions with ontology designers. This\nmulti-party interaction often creates systematic ambiguities and biases from\nthe elicitation of ontology requirements, which directly affect the design,\nevaluation and may jeopardise the target reuse. Meanwhile, current OE\nmethodologies strongly rely on manual activities (e.g., interviews, discussion\npages). After collecting evidence on the most crucial OE activities, we\nintroduce \\textbf{OntoChat}, a framework for conversational ontology\nengineering that supports requirement elicitation, analysis, and testing. By\ninteracting with a conversational agent, users can steer the creation of user\nstories and the extraction of competency questions, while receiving\ncomputational support to analyse the overall requirements and test early\nversions of the resulting ontologies. We evaluate OntoChat by replicating the\nengineering of the Music Meta Ontology, and collecting preliminary metrics on\nthe effectiveness of each component from users. We release all code at\nhttps://github.com/King-s-Knowledge-Graph-Lab/OntoChat.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ESWC 2024 Special Track on Large Language Models for Knowledge\n  Engineering",
    "pdf_url": "http://arxiv.org/pdf/2403.05921v2",
    "published_date": "2024-03-09 14:04:06 UTC",
    "updated_date": "2024-04-26 10:13:24 UTC"
  },
  {
    "arxiv_id": "2403.05920v1",
    "title": "High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models",
    "authors": [
      "Syed I. Munzir",
      "Daniel B. Hier",
      "Michael D. Carrithers"
    ],
    "abstract": "Deep phenotyping is the detailed description of patient signs and symptoms\nusing concepts from an ontology. The deep phenotyping of the numerous physician\nnotes in electronic health records requires high throughput methods. Over the\npast thirty years, progress toward making high throughput phenotyping feasible.\nIn this study, we demonstrate that a large language model and a hybrid NLP\nmodel (combining word vectors with a machine learning classifier) can perform\nhigh throughput phenotyping on physician notes with high accuracy. Large\nlanguage models will likely emerge as the preferred method for high throughput\ndeep phenotyping of physician notes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2; J.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to IEEE EMBS Summer conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.05920v1",
    "published_date": "2024-03-09 14:02:59 UTC",
    "updated_date": "2024-03-09 14:02:59 UTC"
  },
  {
    "arxiv_id": "2403.05918v2",
    "title": "SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data",
    "authors": [
      "Ming Zheng",
      "Yang Yang",
      "Zhi-Hang Zhao",
      "Shan-Chao Gan",
      "Yang Chen",
      "Si-Kai Ni",
      "Yang Lu"
    ],
    "abstract": "In the field of data mining and machine learning, commonly used\nclassification models cannot effectively learn in unbalanced data. In order to\nbalance the data distribution before model training, oversampling methods are\noften used to generate data for a small number of classes to solve the problem\nof classifying unbalanced data. Most of the classical oversampling methods are\nbased on the SMOTE technique, which only focuses on the local information of\nthe data, and therefore the generated data may have the problem of not being\nrealistic enough. In the current oversampling methods based on generative\nnetworks, the methods based on GANs can capture the true distribution of data,\nbut there is the problem of pattern collapse and training instability in\ntraining; in the oversampling methods based on denoising diffusion probability\nmodels, the neural network of the inverse diffusion process using the U-Net is\nnot applicable to tabular data, and although the MLP can be used to replace the\nU-Net, the problem exists due to the simplicity of the structure and the poor\neffect of removing noise. problem of poor noise removal. In order to overcome\nthe above problems, we propose a novel oversampling method SEMRes-DDPM.In the\nSEMRes-DDPM backward diffusion process, a new neural network structure\nSEMST-ResNet is used, which is suitable for tabular data and has good noise\nremoval effect, and it can generate tabular data with higher quality.\nExperiments show that the SEMResNet network removes noise better than MLP;\nSEMRes-DDPM generates data distributions that are closer to the real data\ndistributions than TabDDPM with CWGAN-GP; on 20 real unbalanced tabular\ndatasets with 9 classification models, SEMRes-DDPM improves the quality of the\ngenerated tabular data in terms of three evaluation metrics (F1, G-mean, AUC)\nwith better classification performance than other SOTA oversampling methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "None",
    "pdf_url": "http://arxiv.org/pdf/2403.05918v2",
    "published_date": "2024-03-09 14:01:04 UTC",
    "updated_date": "2024-03-12 02:45:48 UTC"
  },
  {
    "arxiv_id": "2403.05916v2",
    "title": "GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing",
    "authors": [
      "Hao Lu",
      "Xuesong Niu",
      "Jiyao Wang",
      "Yin Wang",
      "Qingyong Hu",
      "Jiaqi Tang",
      "Yuting Zhang",
      "Kaishen Yuan",
      "Bin Huang",
      "Zitong Yu",
      "Dengbo He",
      "Shuiguang Deng",
      "Hao Chen",
      "Yingcong Chen",
      "Shiguang Shan"
    ],
    "abstract": "Multimodal large language models (MLLMs) are designed to process and\nintegrate information from multiple sources, such as text, speech, images, and\nvideos. Despite its success in language understanding, it is critical to\nevaluate the performance of downstream tasks for better human-centric\napplications. This paper assesses the application of MLLMs with 5 crucial\nabilities for affective computing, spanning from visual affective tasks and\nreasoning tasks. The results show that \\gpt has high accuracy in facial action\nunit recognition and micro-expression detection while its general facial\nexpression recognition performance is not accurate. We also highlight the\nchallenges of achieving fine-grained micro-expression recognition and the\npotential for further study and demonstrate the versatility and potential of\n\\gpt for handling advanced tasks in emotion recognition and related fields by\nintegrating with task-related agents for more complex tasks, such as heart rate\nestimation through signal processing. In conclusion, this paper provides\nvaluable insights into the potential applications and challenges of MLLMs in\nhuman-centric computing. Our interesting examples are at\nhttps://github.com/EnVision-Research/GPT4Affectivity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05916v2",
    "published_date": "2024-03-09 13:56:25 UTC",
    "updated_date": "2024-04-10 07:58:44 UTC"
  },
  {
    "arxiv_id": "2403.14676v1",
    "title": "Unified Uncertainty Estimation for Cognitive Diagnosis Models",
    "authors": [
      "Fei Wang",
      "Qi Liu",
      "Enhong Chen",
      "Chuanren Liu",
      "Zhenya Huang",
      "Jinze Wu",
      "Shijin Wang"
    ],
    "abstract": "Cognitive diagnosis models have been widely used in different areas,\nespecially intelligent education, to measure users' proficiency levels on\nknowledge concepts, based on which users can get personalized instructions. As\nthe measurement is not always reliable due to the weak links of the models and\ndata, the uncertainty of measurement also offers important information for\ndecisions. However, the research on the uncertainty estimation lags behind that\non advanced model structures for cognitive diagnosis. Existing approaches have\nlimited efficiency and leave an academic blank for sophisticated models which\nhave interaction function parameters (e.g., deep learning-based models). To\naddress these problems, we propose a unified uncertainty estimation approach\nfor a wide range of cognitive diagnosis models. Specifically, based on the idea\nof estimating the posterior distributions of cognitive diagnosis model\nparameters, we first provide a unified objective function for mini-batch based\noptimization that can be more efficiently applied to a wide range of models and\nlarge datasets. Then, we modify the reparameterization approach in order to\nadapt to parameters defined on different domains. Furthermore, we decompose the\nuncertainty of diagnostic parameters into data aspect and model aspect, which\nbetter explains the source of uncertainty. Extensive experiments demonstrate\nthat our method is effective and can provide useful insights into the\nuncertainty of cognitive diagnosis.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14676v1",
    "published_date": "2024-03-09 13:48:20 UTC",
    "updated_date": "2024-03-09 13:48:20 UTC"
  },
  {
    "arxiv_id": "2403.05911v2",
    "title": "Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning",
    "authors": [
      "Zana Buçinca",
      "Siddharth Swaroop",
      "Amanda E. Paluch",
      "Susan A. Murphy",
      "Krzysztof Z. Gajos"
    ],
    "abstract": "Imagine if AI decision-support tools not only complemented our ability to\nmake accurate decisions, but also improved our skills, boosted collaboration,\nand elevated the joy we derive from our tasks. Despite the potential to\noptimize a broad spectrum of such human-centric objectives, the design of\ncurrent AI tools remains focused on decision accuracy alone. We propose offline\nreinforcement learning (RL) as a general approach for modeling human-AI\ndecision-making to optimize human-AI interaction for diverse objectives. RL can\noptimize such objectives by tailoring decision support, providing the right\ntype of assistance to the right person at the right time. We instantiated our\napproach with two objectives: human-AI accuracy on the decision-making task and\nhuman learning about the task and learned decision support policies from\nprevious human-AI interaction data. We compared the optimized policies against\nseveral baselines in AI-assisted decision-making. Across two experiments (N=316\nand N=964), our results demonstrated that people interacting with policies\noptimized for accuracy achieve significantly better accuracy -- and even\nhuman-AI complementarity -- compared to those interacting with any other type\nof AI support. Our results further indicated that human learning was more\ndifficult to optimize than accuracy, with participants who interacted with\nlearning-optimized policies showing significant learning improvement only at\ntimes. Our research (1) demonstrates offline RL to be a promising approach to\nmodel human-AI decision-making, leading to policies that may optimize\nhuman-centric objectives and provide novel insights about the AI-assisted\ndecision-making space, and (2) emphasizes the importance of considering\nhuman-centric objectives beyond decision accuracy in AI-assisted\ndecision-making, opening up the novel research challenge of optimizing human-AI\ninteraction for such objectives.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05911v2",
    "published_date": "2024-03-09 13:30:00 UTC",
    "updated_date": "2024-04-14 21:17:57 UTC"
  },
  {
    "arxiv_id": "2403.08824v2",
    "title": "Computational Analysis of Stress, Depression and Engagement in Mental Health: A Survey",
    "authors": [
      "Puneet Kumar",
      "Alexander Vedernikov",
      "Yuwei Chen",
      "Wenming Zheng",
      "Xiaobai Li"
    ],
    "abstract": "Analysis of stress, depression and engagement is less common and more complex\nthan that of frequently discussed emotions such as happiness, sadness, fear and\nanger. The importance of these psychological states has been increasingly\nrecognized due to their implications for mental health and well-being. Stress\nand depression are interrelated and together they impact engagement in daily\ntasks, highlighting the need to explore their interplay. This survey is the\nfirst to simultaneously explore computational methods for analyzing stress,\ndepression and engagement. We present a taxonomy and timeline of the\ncomputational approaches used to analyze them and we discuss the most commonly\nused datasets and input modalities, along with the categories and generic\npipeline of these approaches. Subsequently, we describe state-of-the-art\ncomputational approaches, including a performance summary on the most commonly\nused datasets. Following this, we explore the applications of stress,\ndepression and engagement analysis, along with the associated challenges,\nlimitations and future research directions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.HC",
    "comment": "Under review in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2403.08824v2",
    "published_date": "2024-03-09 11:16:09 UTC",
    "updated_date": "2025-03-25 10:14:57 UTC"
  },
  {
    "arxiv_id": "2403.05845v1",
    "title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
    "authors": [
      "Daniel Zhang-Li",
      "Nianyi Lin",
      "Jifan Yu",
      "Zheyuan Zhang",
      "Zijun Yao",
      "Xiaokang Zhang",
      "Lei Hou",
      "Jing Zhang",
      "Juanzi Li"
    ],
    "abstract": "Recent advancements in pretraining have demonstrated that modern Large\nLanguage Models (LLMs) possess the capability to effectively learn arithmetic\noperations. However, despite acknowledging the significance of digit order in\narithmetic computation, current methodologies predominantly rely on sequential,\nstep-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion\nwhere obtaining better performance involves fine-grained step-by-step.\nDiverging from this conventional path, our work introduces a novel strategy\nthat not only reevaluates the digit order by prioritizing output from the least\nsignificant digit but also incorporates a step-by-step methodology to\nsubstantially reduce complexity. We have developed and applied this method in a\ncomprehensive set of experiments. Compared to the previous state-of-the-art\n(SOTA) method, our findings reveal an overall improvement of in accuracy while\nrequiring only a third of the tokens typically used during training. For the\npurpose of facilitating replication and further research, we have made our code\nand dataset publicly available at\n\\url{https://anonymous.4open.science/r/RAIT-9FB7/}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05845v1",
    "published_date": "2024-03-09 09:04:53 UTC",
    "updated_date": "2024-03-09 09:04:53 UTC"
  },
  {
    "arxiv_id": "2403.05842v3",
    "title": "TokenMark: A Modality-Agnostic Watermark for Pre-trained Transformers",
    "authors": [
      "Hengyuan Xu",
      "Liyao Xiang",
      "Borui Yang",
      "Xingjun Ma",
      "Siheng Chen",
      "Baochun Li"
    ],
    "abstract": "Watermarking is a critical tool for model ownership verification. However,\nexisting watermarking techniques are often designed for specific data\nmodalities and downstream tasks, without considering the inherent architectural\nproperties of the model. This lack of generality and robustness underscores the\nneed for a more versatile watermarking approach. In this work, we investigate\nthe properties of Transformer models and propose TokenMark, a\nmodality-agnostic, robust watermarking system for pre-trained models,\nleveraging the permutation equivariance property. TokenMark embeds the\nwatermark by fine-tuning the pre-trained model on a set of specifically\npermuted data samples, resulting in a watermarked model that contains two\ndistinct sets of weights -- one for normal functionality and the other for\nwatermark extraction, the latter triggered only by permuted inputs. Extensive\nexperiments on state-of-the-art pre-trained models demonstrate that TokenMark\nsignificantly improves the robustness, efficiency, and universality of model\nwatermarking, highlighting its potential as a unified watermarking solution.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05842v3",
    "published_date": "2024-03-09 08:54:52 UTC",
    "updated_date": "2025-04-26 08:10:01 UTC"
  },
  {
    "arxiv_id": "2403.05839v2",
    "title": "Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline",
    "authors": [
      "Xiao Wang",
      "Ju Huang",
      "Shiao Wang",
      "Chuanming Tang",
      "Bo Jiang",
      "Yonghong Tian",
      "Jin Tang",
      "Bin Luo"
    ],
    "abstract": "Current event-/frame-event based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term and large-scale frame-event single object tracking dataset, termed\nFELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs\nand has become the largest frame-event tracking dataset to date. We re-train\nand evaluate 15 baseline trackers on our dataset for future works to compare.\nMore importantly, we find that the RGB frames and event streams are naturally\nincomplete due to the influence of challenging factors and spatially sparse\nevent flow. In response to this, we propose a novel associative memory\nTransformer network as a unified backbone by introducing modern Hopfield layers\ninto multi-head self-attention blocks to fuse both RGB and event data.\nExtensive experiments on RGB-Event (FELT), RGB-Thermal (RGBT234, LasHeR), and\nRGB-Depth (DepthTrack) datasets fully validated the effectiveness of our model.\nThe dataset and source code can be found at\n\\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2403.05839v2",
    "published_date": "2024-03-09 08:49:50 UTC",
    "updated_date": "2024-04-03 09:25:34 UTC"
  },
  {
    "arxiv_id": "2403.05828v2",
    "title": "Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC Middleware: Applications in Quantum Simulations",
    "authors": [
      "Kuan-Cheng Chen",
      "Xiaoren Li",
      "Xiaotian Xu",
      "Yun-Yuan Wang",
      "Chen-Yu Liu"
    ],
    "abstract": "Achieving high-performance computation on quantum systems presents a\nformidable challenge that necessitates bridging the capabilities between\nquantum hardware and classical computing resources. This study introduces an\ninnovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,\nwhich integrates cutting-edge quantum software framework works with\nhigh-performance classical computing resources to address challenges in quantum\nsimulation for materials and condensed matter physics. At the heart of this\narchitecture is the seamless integration of VQE algorithms running on QPUs for\nefficient quantum state preparation, Tensor Network states, and QCNNs for\nclassifying quantum states on classical hardware.\n  For benchmarking quantum simulators, the QCQ architecture utilizes the\ncuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's\nLightning plugin, demonstrating up to tenfold increases in computational speed\nfor complex phase transition classification tasks compared to traditional\nCPU-based methods. This significant acceleration enables models such as the\ntransverse field Ising and XXZ systems to accurately predict phase transitions\nwith a 99.5% accuracy. The architecture's ability to distribute computation\nbetween QPUs and classical resources addresses critical bottlenecks in\nQuantum-HPC, paving the way for scalable quantum simulation.\n  The QCQ framework embodies a synergistic combination of quantum algorithms,\nmachine learning, and Quantum-HPC capabilities, enhancing its potential to\nprovide transformative insights into the behavior of quantum systems across\ndifferent scales. As quantum hardware continues to improve, this hybrid\ndistribution-aware framework will play a crucial role in realizing the full\npotential of quantum computing by seamlessly integrating distributed quantum\nresources with the state-of-the-art classical computing infrastructure.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.AR",
      "cs.DC"
    ],
    "primary_category": "quant-ph",
    "comment": "8 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.05828v2",
    "published_date": "2024-03-09 07:38:45 UTC",
    "updated_date": "2024-03-18 08:54:10 UTC"
  },
  {
    "arxiv_id": "2403.05814v1",
    "title": "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
    "authors": [
      "Yerin Hwang",
      "Yongil Kim",
      "Yunah Jang",
      "Jeesoo Bang",
      "Hyunkyung Bae",
      "Kyomin Jung"
    ],
    "abstract": "Despite advancements in on-topic dialogue systems, effectively managing topic\nshifts within dialogues remains a persistent challenge, largely attributed to\nthe limited availability of training datasets. To address this issue, we\npropose Multi-Passage to Dialogue (MP2D), a data generation framework that\nautomatically creates conversational question-answering datasets with natural\ntopic transitions. By leveraging the relationships between entities in a\nknowledge graph, MP2D maps the flow of topics within a dialogue, effectively\nmirroring the dynamics of human conversation. It retrieves relevant passages\ncorresponding to the topics and transforms them into dialogues through the\npassage-to-dialogue method. Through quantitative and qualitative experiments,\nwe demonstrate MP2D's efficacy in generating dialogue with natural topic\nshifts. Furthermore, this study introduces a novel benchmark for topic shift\ndialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large\nLanguage Models (LLMs) struggle to handle topic shifts in dialogue effectively,\nand we showcase the performance improvements of models trained on datasets\ngenerated by MP2D across diverse topic shift dialogue tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.05814v1",
    "published_date": "2024-03-09 06:28:48 UTC",
    "updated_date": "2024-03-09 06:28:48 UTC"
  },
  {
    "arxiv_id": "2403.05812v1",
    "title": "Algorithmic progress in language models",
    "authors": [
      "Anson Ho",
      "Tamay Besiroglu",
      "Ege Erdil",
      "David Owen",
      "Robi Rahman",
      "Zifan Carl Guo",
      "David Atkinson",
      "Neil Thompson",
      "Jaime Sevilla"
    ],
    "abstract": "We investigate the rate at which algorithms for pre-training language models\nhave improved since the advent of deep learning. Using a dataset of over 200\nlanguage model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we\nfind that the compute required to reach a set performance threshold has halved\napproximately every 8 months, with a 95% confidence interval of around 5 to 14\nmonths, substantially faster than hardware gains per Moore's Law. We estimate\naugmented scaling laws, which enable us to quantify algorithmic progress and\ndetermine the relative contributions of scaling models versus innovations in\ntraining algorithms. Despite the rapid pace of algorithmic progress and the\ndevelopment of new architectures such as the transformer, our analysis reveals\nthat the increase in compute made an even larger contribution to overall\nperformance improvements over this time period. Though limited by noisy\nbenchmark data, our analysis quantifies the rapid progress in language\nmodeling, shedding light on the relative contributions from compute and\nalgorithms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05812v1",
    "published_date": "2024-03-09 06:26:21 UTC",
    "updated_date": "2024-03-09 06:26:21 UTC"
  },
  {
    "arxiv_id": "2403.05810v2",
    "title": "Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction",
    "authors": [
      "Yonghao Dong",
      "Le Wang",
      "Sanping Zhou",
      "Gang Hua",
      "Changyin Sun"
    ],
    "abstract": "Pedestrian trajectory prediction is a crucial component in computer vision\nand robotics, but remains challenging due to the domain shift problem. Previous\nstudies have tried to tackle this problem by leveraging a portion of the\ntrajectory data from the target domain to adapt the model. However, such domain\nadaptation methods are impractical in real-world scenarios, as it is infeasible\nto collect trajectory data from all potential target domains. In this paper, we\nstudy a task named generalized pedestrian trajectory prediction, with the aim\nof generalizing the model to unseen domains without accessing their\ntrajectories. To tackle this task, we introduce a Recurrent Aligned\nNetwork~(RAN) to minimize the domain gap through domain alignment.\nSpecifically, we devise a recurrent alignment module to effectively align the\ntrajectory feature spaces at both time-state and time-sequence levels by the\nrecurrent alignment strategy.Furthermore, we introduce a pre-aligned\nrepresentation module to combine social interactions with the recurrent\nalignment strategy, which aims to consider social interactions during the\nalignment process instead of just target trajectories. We extensively evaluate\nour method and compare it with state-of-the-art methods on three widely used\nbenchmarks. The experimental results demonstrate the superior generalization\ncapability of our method. Our work not only fills the gap in the generalization\nsetting for practical pedestrian trajectory prediction but also sets strong\nbaselines in this field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05810v2",
    "published_date": "2024-03-09 06:17:09 UTC",
    "updated_date": "2024-12-21 08:20:23 UTC"
  },
  {
    "arxiv_id": "2403.05801v1",
    "title": "Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques",
    "authors": [
      "Chen Li",
      "Haotian Zheng",
      "Yiping Sun",
      "Cangqing Wang",
      "Liqiang Yu",
      "Che Chang",
      "Xinyu Tian",
      "Bo Liu"
    ],
    "abstract": "In the realm of computational knowledge representation, Knowledge Graph\nReasoning (KG-R) stands at the forefront of facilitating sophisticated\ninferential capabilities across multifarious domains. The quintessence of this\nresearch elucidates the employment of reinforcement learning (RL) strategies,\nnotably the REINFORCE algorithm, to navigate the intricacies inherent in\nmulti-hop KG-R. This investigation critically addresses the prevalent\nchallenges introduced by the inherent incompleteness of Knowledge Graphs (KGs),\nwhich frequently results in erroneous inferential outcomes, manifesting as both\nfalse negatives and misleading positives. By partitioning the Unified Medical\nLanguage System (UMLS) benchmark dataset into rich and sparse subsets, we\ninvestigate the efficacy of pre-trained BERT embeddings and Prompt Learning\nmethodologies to refine the reward shaping process. This approach not only\nenhances the precision of multi-hop KG-R but also sets a new precedent for\nfuture research in the field, aiming to improve the robustness and accuracy of\nknowledge inference within complex KG frameworks. Our work contributes a novel\nperspective to the discourse on KG reasoning, offering a methodological\nadvancement that aligns with the academic rigor and scholarly aspirations of\nthe Natural journal, promising to invigorate further advancements in the realm\nof computational knowledge representation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by the 2024 5th International Seminar on\n  Artificial Intelligence, Networking and Information Technology (AINIT 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.05801v1",
    "published_date": "2024-03-09 05:34:07 UTC",
    "updated_date": "2024-03-09 05:34:07 UTC"
  },
  {
    "arxiv_id": "2403.05794v2",
    "title": "Privacy-Preserving Diffusion Model Using Homomorphic Encryption",
    "authors": [
      "Yaojian Chen",
      "Qiben Yan"
    ],
    "abstract": "In this paper, we introduce a privacy-preserving stable diffusion framework\nleveraging homomorphic encryption, called HE-Diffusion, which primarily focuses\non protecting the denoising phase of the diffusion process. HE-Diffusion is a\ntailored encryption framework specifically designed to align with the unique\narchitecture of stable diffusion, ensuring both privacy and functionality. To\naddress the inherent computational challenges, we propose a novel\nmin-distortion method that enables efficient partial image encryption,\nsignificantly reducing the overhead without compromising the model's output\nquality. Furthermore, we adopt a sparse tensor representation to expedite\ncomputational operations, enhancing the overall efficiency of the\nprivacy-preserving diffusion process. We successfully implement HE-based\nprivacy-preserving stable diffusion inference. The experimental results show\nthat HE-Diffusion achieves 500 times speedup compared with the baseline method,\nand reduces time cost of the homomorphically encrypted inference to the minute\nlevel. Both the performance and accuracy of the HE-Diffusion are on par with\nthe plaintext counterpart. Our approach marks a significant step towards\nintegrating advanced cryptographic techniques with state-of-the-art generative\nmodels, paving the way for privacy-preserving and efficient image generation in\ncritical applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05794v2",
    "published_date": "2024-03-09 04:56:57 UTC",
    "updated_date": "2024-05-02 03:46:16 UTC"
  },
  {
    "arxiv_id": "2403.05789v1",
    "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
    "authors": [
      "Wangtao Sun",
      "Haotian Xu",
      "Xuanqing Yu",
      "Pei Chen",
      "Shizhu He",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Although Large Language Models (LLMs) are showing impressive performance on a\nwide range of Natural Language Processing tasks, researchers have found that\nthey still have limited ability to conduct induction. Recent works mainly adopt\n``post processes'' paradigms to improve the performance of LLMs on induction\n(e.g., the hypothesis search & refinement methods), but their performance is\nstill constrained by the inherent inductive capability of the LLMs. In this\npaper, we propose a novel framework, Induction through Deduction (ItD), to\nenable the LLMs to teach themselves induction through deduction. The ItD\nframework is composed of two main components: a Deductive Data Generation\nmodule to generate induction data and a Naive Bayesian Induction module to\noptimize the fine-tuning and decoding of LLMs. Our empirical results showcase\nthe effectiveness of ItD on two induction benchmarks, achieving relative\nperformance improvement of 36% and 10% compared with previous state-of-the-art,\nrespectively. Our ablation study verifies the effectiveness of two key modules\nof ItD. We also verify the effectiveness of ItD across different LLMs and\ndeductors. The data and code of this paper can be found at\nhttps://anonymous.4open.science/r/ItD-E844.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05789v1",
    "published_date": "2024-03-09 04:20:46 UTC",
    "updated_date": "2024-03-09 04:20:46 UTC"
  },
  {
    "arxiv_id": "2403.05788v1",
    "title": "On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization",
    "authors": [
      "Lorenzo Jaime Yu Flores",
      "Arman Cohan"
    ],
    "abstract": "Text summarization and simplification are among the most widely used\napplications of AI. However, models developed for such tasks are often prone to\nhallucination, which can result from training on unaligned data. One efficient\napproach to address this issue is Loss Truncation (LT) (Kang and Hashimoto,\n2020), an approach to modify the standard log loss to adaptively remove noisy\nexamples during training. However, we find that LT alone yields a considerable\nnumber of hallucinated entities on various datasets. We study the behavior of\nthe underlying losses between factual and non-factual examples, to understand\nand refine the performance of LT. We demonstrate that LT's performance is\nlimited when the underlying assumption that noisy targets have higher NLL loss\nis not satisfied, and find that word-level NLL among entities provides better\nsignal for distinguishing factuality. We then leverage this to propose a\nfine-grained NLL loss and fine-grained data cleaning strategies, and observe\nimprovements in hallucination reduction across some datasets. Our work is\navailable at https://https://github.com/yale-nlp/fine-grained-lt.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.05788v1",
    "published_date": "2024-03-09 04:20:26 UTC",
    "updated_date": "2024-03-09 04:20:26 UTC"
  },
  {
    "arxiv_id": "2403.07010v1",
    "title": "On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF Multi-Criteria Group Decision-Making",
    "authors": [
      "Miin-Shen Yang",
      "Yasir Akhtar",
      "Mehboob Ali"
    ],
    "abstract": "In this paper, we give the concept of Globular T-Spherical Fuzzy (G-TSF) Sets\n(G-TSFSs) as an innovative extension of T-Spherical Fuzzy Sets (TSFSs) and\nCircular Spherical Fuzzy Sets (C-SFSs). G-TSFSs represent membership,\nindeterminacy, and non-membership degrees using a globular/sphere bound that\ncan offer a more accurate portrayal of vague, ambiguous, and imprecise\ninformation. By employing a structured representation of data points on a\nsphere with a specific center and radius, this model enhances decision-making\nprocesses by enabling a more comprehensive evaluation of objects within a\nflexible region. Following the newly defined G-TSFSs, we establish some basic\nset operations and introduce fundamental algebraic operations for G-TSF Values\n(G-TSFVs). These operations expand the evaluative capabilities of\ndecision-makers, facilitating more sensitive decision-making processes in a\nbroader region. To quantify a similarity measure (SM) between GTSFVs, the SM is\ndefined based on the radius of G-TSFSs. Additionally, Hamming distance and\nEuclidean distance are introduced for G-TSFSs. We also present theorems and\nexamples to elucidate computational mechanisms. Furthermore, we give the G-TSF\nWeighted Average (G-TSFWA) and G-TSF Weighted Geometric (G-TSFWG) operators.\nLeveraging our proposed SM, a Multi-Criteria Group Decision-Making (MCGDM)\nscheme for G-TSFSs, named G-TSF MCGDM (G-TSFMCGDM), is developed to address\ngroup decision-making problems. The applicability and effectiveness of the\nproposed G-TSFMCGDM method are demonstrated by applying it to solve the\nselection problem of the best venue for professional development training\nsessions in a firm. The analysis results affirm the suitability and utility of\nthe proposed method for resolving MCGDM problems, establishing its\neffectiveness in practical decision-making scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.07010v1",
    "published_date": "2024-03-09 04:19:50 UTC",
    "updated_date": "2024-03-09 04:19:50 UTC"
  },
  {
    "arxiv_id": "2403.07008v2",
    "title": "AutoEval Done Right: Using Synthetic Data for Model Evaluation",
    "authors": [
      "Pierre Boyeau",
      "Anastasios N. Angelopoulos",
      "Nir Yosef",
      "Jitendra Malik",
      "Michael I. Jordan"
    ],
    "abstract": "The evaluation of machine learning models using human-labeled validation data\ncan be expensive and time-consuming. AI-labeled synthetic data can be used to\ndecrease the number of human annotations required for this purpose in a process\ncalled autoevaluation. We suggest efficient and statistically principled\nalgorithms for this purpose that improve sample efficiency while remaining\nunbiased. These algorithms increase the effective human-labeled sample size by\nup to 50% on experiments with GPT-4.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "New experiments, fix fig 1",
    "pdf_url": "http://arxiv.org/pdf/2403.07008v2",
    "published_date": "2024-03-09 02:47:11 UTC",
    "updated_date": "2024-05-28 04:38:41 UTC"
  },
  {
    "arxiv_id": "2403.05770v1",
    "title": "Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning",
    "authors": [
      "Bingqian Lin",
      "Yanxin Long",
      "Yi Zhu",
      "Fengda Zhu",
      "Xiaodan Liang",
      "Qixiang Ye",
      "Liang Lin"
    ],
    "abstract": "Vision-and-language navigation (VLN) asks an agent to follow a given language\ninstruction to navigate through a real 3D environment. Despite significant\nadvances, conventional VLN agents are trained typically under disturbance-free\nenvironments and may easily fail in real-world scenarios, since they are\nunaware of how to deal with various possible disturbances, such as sudden\nobstacles or human interruptions, which widely exist and may usually cause an\nunexpected route deviation. In this paper, we present a model-agnostic training\nparadigm, called Progressive Perturbation-aware Contrastive Learning (PROPER)\nto enhance the generalization ability of existing VLN agents, by requiring them\nto learn towards deviation-robust navigation. Specifically, a simple yet\neffective path perturbation scheme is introduced to implement the route\ndeviation, with which the agent is required to still navigate successfully\nfollowing the original instruction. Since directly enforcing the agent to learn\nperturbed trajectories may lead to inefficient training, a progressively\nperturbed trajectory augmentation strategy is designed, where the agent can\nself-adaptively learn to navigate under perturbation with the improvement of\nits navigation performance for each specific trajectory. For encouraging the\nagent to well capture the difference brought by perturbation, a\nperturbation-aware contrastive learning mechanism is further developed by\ncontrasting perturbation-free trajectory encodings and perturbation-based\ncounterparts. Extensive experiments on R2R show that PROPER can benefit\nmultiple VLN baselines in perturbation-free scenarios. We further collect the\nperturbed path data to construct an introspection subset based on the R2R,\ncalled Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying\nrobustness of popular VLN agents and the capability of PROPER in improving the\nnavigation robustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by TPAMI 2023",
    "pdf_url": "http://arxiv.org/pdf/2403.05770v1",
    "published_date": "2024-03-09 02:34:13 UTC",
    "updated_date": "2024-03-09 02:34:13 UTC"
  },
  {
    "arxiv_id": "2403.05767v1",
    "title": "Extending Activation Steering to Broad Skills and Multiple Behaviours",
    "authors": [
      "Teun van der Weij",
      "Massimo Poesio",
      "Nandi Schoots"
    ],
    "abstract": "Current large language models have dangerous capabilities, which are likely\nto become more problematic in the future. Activation steering techniques can be\nused to reduce risks from these capabilities. In this paper, we investigate the\nefficacy of activation steering for broad skills and multiple behaviours.\nFirst, by comparing the effects of reducing performance on general coding\nability and Python-specific ability, we find that steering broader skills is\ncompetitive to steering narrower skills. Second, we steer models to become more\nor less myopic and wealth-seeking, among other behaviours. In our experiments,\ncombining steering vectors for multiple different behaviours into one steering\nvector is largely unsuccessful. On the other hand, injecting individual\nsteering vectors at different places in a model simultaneously is promising.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at:\n  https://github.com/TeunvdWeij/extending-activation-addition",
    "pdf_url": "http://arxiv.org/pdf/2403.05767v1",
    "published_date": "2024-03-09 02:30:04 UTC",
    "updated_date": "2024-03-09 02:30:04 UTC"
  },
  {
    "arxiv_id": "2403.05764v1",
    "title": "Investigation into the Potential of Parallel Quantum Annealing for Simultaneous Optimization of Multiple Problems: A Comprehensive Study",
    "authors": [
      "Arit Kumar Bishwas",
      "Anuraj Som",
      "Saurabh Choudhary"
    ],
    "abstract": "Parallel Quantum Annealing is a technique to solve multiple optimization\nproblems simultaneously. Parallel quantum annealing aims to optimize the\nutilization of available qubits on a quantum topology by addressing multiple\nindependent problems in a single annealing cycle. This study provides insights\ninto the potential and the limitations of this parallelization method. The\nexperiments consisting of two different problems are integrated, and various\nproblem dimensions are explored including normalization techniques using\nspecific methods such as DWaveSampler with Default Embedding, DWaveSampler with\nCustom Embedding and LeapHybridSampler. This method minimizes idle qubits and\nholds promise for substantial speed-up, as indicated by the Time-to-Solution\n(TTS) metric, compared to traditional quantum annealing, which solves problems\nsequentially and may leave qubits unutilized.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05764v1",
    "published_date": "2024-03-09 02:18:48 UTC",
    "updated_date": "2024-03-09 02:18:48 UTC"
  },
  {
    "arxiv_id": "2403.05763v1",
    "title": "HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning",
    "authors": [
      "Hanning Chen",
      "Yang Ni",
      "Ali Zakeri",
      "Zhuowen Zou",
      "Sanggeon Yun",
      "Fei Wen",
      "Behnam Khaleghi",
      "Narayan Srinivasa",
      "Hugo Latapie",
      "Mohsen Imani"
    ],
    "abstract": "In recent times, a plethora of hardware accelerators have been put forth for\ngraph learning applications such as vertex classification and graph\nclassification. However, previous works have paid little attention to Knowledge\nGraph Completion (KGC), a task that is well-known for its significantly higher\nalgorithm complexity. The state-of-the-art KGC solutions based on graph\nconvolution neural network (GCN) involve extensive vertex/relation embedding\nupdates and complicated score functions, which are inherently cumbersome for\nacceleration. As a result, existing accelerator designs are no longer optimal,\nand a novel algorithm-hardware co-design for KG reasoning is needed.\n  Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced\nas a promising solution for lightweight machine learning, particularly for\ngraph learning applications. In this paper, we leverage HDC for an\nintrinsically more efficient and acceleration-friendly KGC algorithm. We also\nco-design an acceleration framework named HDReason targeting FPGA platforms. On\nthe algorithm level, HDReason achieves a balance between high reasoning\naccuracy, strong model interpretability, and less computation complexity. In\nterms of architecture, HDReason offers reconfigurability, high training\nthroughput, and low energy consumption. When compared with NVIDIA RTX 4090 GPU,\nthe proposed accelerator achieves an average 10.6x speedup and 65x energy\nefficiency improvement. When conducting cross-models and cross-platforms\ncomparison, HDReason yields an average 4.2x higher performance and 3.4x better\nenergy efficiency with similar accuracy versus the state-of-the-art FPGA-based\nGCN training platform.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05763v1",
    "published_date": "2024-03-09 02:17:43 UTC",
    "updated_date": "2024-03-09 02:17:43 UTC"
  },
  {
    "arxiv_id": "2403.05759v1",
    "title": "Membership Testing in Markov Equivalence Classes via Independence Query Oracles",
    "authors": [
      "Jiaqi Zhang",
      "Kirankumar Shiragur",
      "Caroline Uhler"
    ],
    "abstract": "Understanding causal relationships between variables is a fundamental problem\nwith broad impact in numerous scientific fields. While extensive research has\nbeen dedicated to learning causal graphs from data, its complementary concept\nof testing causal relationships has remained largely unexplored. While learning\ninvolves the task of recovering the Markov equivalence class (MEC) of the\nunderlying causal graph from observational data, the testing counterpart\naddresses the following critical question: Given a specific MEC and\nobservational data from some causal graph, can we determine if the\ndata-generating causal graph belongs to the given MEC?\n  We explore constraint-based testing methods by establishing bounds on the\nrequired number of conditional independence tests. Our bounds are in terms of\nthe size of the maximum undirected clique ($s$) of the given MEC. In the worst\ncase, we show a lower bound of $\\exp(\\Omega(s))$ independence tests. We then\ngive an algorithm that resolves the task with $\\exp(O(s))$ tests, matching our\nlower bound. Compared to the learning problem, where algorithms often use a\nnumber of independence tests that is exponential in the maximum in-degree, this\nshows that testing is relatively easier. In particular, it requires\nexponentially less independence tests in graphs featuring high in-degrees and\nsmall clique sizes. Additionally, using the DAG associahedron, we provide a\ngeometric interpretation of testing versus learning and discuss how our testing\nresult can aid learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05759v1",
    "published_date": "2024-03-09 02:10:08 UTC",
    "updated_date": "2024-03-09 02:10:08 UTC"
  },
  {
    "arxiv_id": "2403.05752v2",
    "title": "Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling",
    "authors": [
      "Hussein Abdallah",
      "Waleed Afandi",
      "Panos Kalnis",
      "Essam Mansour"
    ],
    "abstract": "A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range\nof node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular\nfor training machine learning tasks like node classification and link\nprediction on KGs. However, HGNN methods exhibit excessive complexity\ninfluenced by the KG's size, density, and the number of node and edge types. AI\npractitioners handcraft a subgraph of a KG G relevant to a specific task. We\nrefer to this subgraph as a task-oriented subgraph (TOSG), which contains a\nsubset of task-related node and edge types in G. Training the task using TOSG\ninstead of G alleviates the excessive computation required for a large KG.\nCrafting the TOSG demands a deep understanding of the KG's structure and the\ntask's objectives. Hence, it is challenging and time-consuming. This paper\nproposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented\nHGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that\ncaptures the KG's local and global structure relevant to a specific task. We\nexplore different techniques to extract subgraphs matching our graph pattern:\nnamely (i) two techniques sampling around targeted nodes using biased random\nwalk or influence scores, and (ii) a SPARQL-based extraction method leveraging\nRDF engines' built-in indices. Hence, it achieves negligible preprocessing\noverhead compared to the sampling techniques. We develop a benchmark of real\nKGs of large sizes and various tasks for node classification and link\nprediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN\nmethods reduce training time and memory usage by up to 70% while improving the\nmodel performance, e.g., accuracy and inference time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages,9 Figures, 3 Tables, ICDE:2024",
    "pdf_url": "http://arxiv.org/pdf/2403.05752v2",
    "published_date": "2024-03-09 01:17:26 UTC",
    "updated_date": "2024-03-22 14:44:17 UTC"
  },
  {
    "arxiv_id": "2403.05751v2",
    "title": "MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process",
    "authors": [
      "Xinyao Fan",
      "Yueying Wu",
      "Chang Xu",
      "Yuhao Huang",
      "Weiqing Liu",
      "Jiang Bian"
    ],
    "abstract": "Recently, diffusion probabilistic models have attracted attention in\ngenerative time series forecasting due to their remarkable capacity to generate\nhigh-fidelity samples. However, the effective utilization of their strong\nmodeling ability in the probabilistic time series forecasting task remains an\nopen question, partially due to the challenge of instability arising from their\nstochastic nature. To address this challenge, we introduce a novel\nMulti-Granularity Time Series Diffusion (MG-TSD) model, which achieves\nstate-of-the-art predictive performance by leveraging the inherent granularity\nlevels within the data as given targets at intermediate diffusion steps to\nguide the learning process of diffusion models. The way to construct the\ntargets is motivated by the observation that the forward process of the\ndiffusion model, which sequentially corrupts the data distribution to a\nstandard normal distribution, intuitively aligns with the process of smoothing\nfine-grained data into a coarse-grained representation, both of which result in\na gradual loss of fine distribution features. In the study, we derive a novel\nmulti-granularity guidance diffusion loss function and propose a concise\nimplementation method to effectively utilize coarse-grained data across various\ngranularity levels. More importantly, our approach does not rely on additional\nexternal data, making it versatile and applicable across various domains.\nExtensive experiments conducted on real-world datasets demonstrate that our\nMG-TSD model outperforms existing time series prediction methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "International Conference on Learning Representations (ICLR) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.05751v2",
    "published_date": "2024-03-09 01:15:03 UTC",
    "updated_date": "2024-03-16 01:16:19 UTC"
  },
  {
    "arxiv_id": "2403.05750v3",
    "title": "Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text",
    "authors": [
      "Sara Abdali",
      "Richard Anarfi",
      "CJ Barberan",
      "Jia He"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Generation (NLG) by demonstrating an impressive ability to generate\nhuman-like text. However, their widespread usage introduces challenges that\nnecessitate thoughtful examination, ethical scrutiny, and responsible\npractices. In this study, we delve into these challenges, explore existing\nstrategies for mitigating them, with a particular emphasis on identifying\nAI-generated text as the ultimate solution. Additionally, we assess the\nfeasibility of detection from a theoretical perspective and propose novel\nresearch directions to address the current limitations in this domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05750v3",
    "published_date": "2024-03-09 01:13:54 UTC",
    "updated_date": "2024-06-26 20:49:32 UTC"
  },
  {
    "arxiv_id": "2403.09706v1",
    "title": "Schema-Aware Multi-Task Learning for Complex Text-to-SQL",
    "authors": [
      "Yangjun Wu",
      "Han Wang"
    ],
    "abstract": "Conventional text-to-SQL parsers are not good at synthesizing complex SQL\nqueries that involve multiple tables or columns, due to the challenges inherent\nin identifying the correct schema items and performing accurate alignment\nbetween question and schema items. To address the above issue, we present a\nschema-aware multi-task learning framework (named MTSQL) for complicated SQL\nqueries. Specifically, we design a schema linking discriminator module to\ndistinguish the valid question-schema linkings, which explicitly instructs the\nencoder by distinctive linking relations to enhance the alignment quality. On\nthe decoder side, we define 6-type relationships to describe the connections\nbetween tables and columns (e.g., WHERE_TC), and introduce an operator-centric\ntriple extractor to recognize those associated schema items with the predefined\nrelationship. Also, we establish a rule set of grammar constraints via the\npredicted triples to filter the proper SQL operators and schema items during\nthe SQL generation. On Spider, a cross-domain challenging text-to-SQL\nbenchmark, experimental results indicate that MTSQL is more effective than\nbaselines, especially in extremely hard scenarios. Moreover, further analyses\nverify that our approach leads to promising improvements for complicated SQL\nqueries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "8pages",
    "pdf_url": "http://arxiv.org/pdf/2403.09706v1",
    "published_date": "2024-03-09 01:13:37 UTC",
    "updated_date": "2024-03-09 01:13:37 UTC"
  }
]
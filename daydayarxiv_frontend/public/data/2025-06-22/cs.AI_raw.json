[
  {
    "arxiv_id": "2506.18209v1",
    "title": "Deep Learning-based Alignment Measurement in Knee Radiographs",
    "authors": [
      "Zhisen Hu",
      "Dominic Cullen",
      "Peter Thompson",
      "David Johnson",
      "Chang Bian",
      "Aleksei Tiulpin",
      "Timothy Cootes",
      "Claudia Lindner"
    ],
    "abstract": "Radiographic knee alignment (KA) measurement is important for predicting joint health and surgical outcomes after total knee replacement. Traditional methods for KA measurements are manual, time-consuming and require long-leg radiographs. This study proposes a deep learning-based method to measure KA in anteroposterior knee radiographs via automatically localized knee anatomical landmarks. Our method builds on hourglass networks and incorporates an attention gate structure to enhance robustness and focus on key anatomical features. To our knowledge, this is the first deep learning-based method to localize over 100 knee anatomical landmarks to fully outline the knee shape while integrating KA measurements on both pre-operative and post-operative images. It provides highly accurate and reliable anatomical varus/valgus KA measurements using the anatomical tibiofemoral angle, achieving mean absolute differences ~1° when compared to clinical ground truth measurements. Agreement between automated and clinical measurements was excellent pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can be automated with high accuracy, creating opportunities for digitally enhanced clinical workflows.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.18209v1",
    "published_date": "2025-06-22 23:57:46 UTC",
    "updated_date": "2025-06-22 23:57:46 UTC"
  },
  {
    "arxiv_id": "2506.18204v2",
    "title": "Multimodal Fusion SLAM with Fourier Attention",
    "authors": [
      "Youjie Zhou",
      "Guofeng Mei",
      "Yiming Wang",
      "Yi Wan",
      "Fabio Poiesi"
    ],
    "abstract": "Visual SLAM is particularly challenging in environments affected by noise, varying lighting conditions, and darkness. Learning-based optical flow algorithms can leverage multiple modalities to address these challenges, but traditional optical flow-based visual SLAM approaches often require significant computational resources.To overcome this limitation, we propose FMF-SLAM, an efficient multimodal fusion SLAM method that utilizes fast Fourier transform (FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel Fourier-based self-attention and cross-attention mechanism to extract features from RGB and depth signals. We further enhance the interaction of multimodal features by incorporating multi-scale knowledge distillation across modalities. We also demonstrate the practical feasibility of FMF-SLAM in real-world scenarios with real time performance by integrating it with a security robot by fusing with a global positioning module GNSS-RTK and global Bundle Adjustment. Our approach is validated using video sequences from TUM, TartanAir, and our real-world datasets, showcasing state-of-the-art performance under noisy, varying lighting, and dark conditions.Our code and datasets are available at https://github.com/youjie-zhou/FMF-SLAM.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in IEEE RAL",
    "pdf_url": "https://arxiv.org/pdf/2506.18204v2",
    "published_date": "2025-06-22 23:44:07 UTC",
    "updated_date": "2025-06-24 09:24:14 UTC"
  },
  {
    "arxiv_id": "2506.18199v2",
    "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
    "authors": [
      "Bushra Asseri",
      "Estabrag Abdelaziz",
      "Areej Al-Wabil"
    ],
    "abstract": "Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Research is incomplete",
    "pdf_url": "https://arxiv.org/pdf/2506.18199v2",
    "published_date": "2025-06-22 23:15:25 UTC",
    "updated_date": "2025-07-30 19:07:18 UTC"
  },
  {
    "arxiv_id": "2506.18196v1",
    "title": "Two Sonification Methods for the MindCube",
    "authors": [
      "Fangzheng Liu",
      "Lancelot Blanchard",
      "Don D. Haddad",
      "Joseph A. Paradiso"
    ],
    "abstract": "In this work, we explore the musical interface potential of the MindCube, an interactive device designed to study emotions. Embedding diverse sensors and input devices, this interface resembles a fidget cube toy commonly used to help users relieve their stress and anxiety. As such, it is a particularly well-suited controller for musical systems that aim to help with emotion regulation. In this regard, we present two different mappings for the MindCube, with and without AI. With our generative AI mapping, we propose a way to infuse meaning within a latent space and techniques to navigate through it with an external controller. We discuss our results and propose directions for future work.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18196v1",
    "published_date": "2025-06-22 23:09:37 UTC",
    "updated_date": "2025-06-22 23:09:37 UTC"
  },
  {
    "arxiv_id": "2506.18195v1",
    "title": "Wisdom of Crowds Through Myopic Self-Confidence Adaptation",
    "authors": [
      "Giacomo Como",
      "Fabio Fagnani",
      "Anton Proskurnikov"
    ],
    "abstract": "The wisdom of crowds is an umbrella term for phenomena suggesting that the collective judgment or decision of a large group can be more accurate than the individual judgments or decisions of the group members. A well-known example illustrating this concept is the competition at a country fair described by Galton, where the median value of the individual guesses about the weight of an ox resulted in an astonishingly accurate estimate of the actual weight. This phenomenon resembles classical results in probability theory and relies on independent decision-making. The accuracy of the group's final decision can be significantly reduced if the final agents' opinions are driven by a few influential agents.\n  In this paper, we consider a group of agents who initially possess uncorrelated and unbiased noisy measurements of a common state of the world. Assume these agents iteratively update their estimates according to a simple non-Bayesian learning rule, commonly known in mathematical sociology as the French-DeGroot dynamics or iterative opinion pooling. As a result of this iterative distributed averaging process, each agent arrives at an asymptotic estimate of the state of the world, with the variance of this estimate determined by the matrix of weights the agents assign to each other. Every agent aims at minimizing the variance of her asymptotic estimate of the state of the world; however, such variance is also influenced by the weights allocated by other agents. To achieve the best possible estimate, the agents must then solve a game-theoretic, multi-objective optimization problem defined by the available sets of influence weights. We characterize both the Pareto frontier and the set of Nash equilibria in the resulting game. Additionally, we examine asynchronous best-response dynamics for the group of agents and prove their convergence to the set of strict Nash equilibria.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.MA",
      "eess.SY",
      "physics.soc-ph"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18195v1",
    "published_date": "2025-06-22 22:55:17 UTC",
    "updated_date": "2025-06-22 22:55:17 UTC"
  },
  {
    "arxiv_id": "2506.18193v2",
    "title": "DeInfoReg: A Decoupled Learning Framework for Better Training Throughput",
    "authors": [
      "Zih-Hao Huang",
      "You-Teng Lin",
      "Hung-Hsuan Chen"
    ],
    "abstract": "This paper introduces Decoupled Supervised Learning with Information Regularization (DeInfoReg), a novel approach that transforms a long gradient flow into multiple shorter ones, thereby mitigating the vanishing gradient problem. Integrating a pipeline strategy, DeInfoReg enables model parallelization across multiple GPUs, significantly improving training throughput. We compare our proposed method with standard backpropagation and other gradient flow decomposition techniques. Extensive experiments on diverse tasks and datasets demonstrate that DeInfoReg achieves superior performance and better noise resistance than traditional BP models and efficiently utilizes parallel computing resources. The code for reproducibility is available at: https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18193v2",
    "published_date": "2025-06-22 22:50:06 UTC",
    "updated_date": "2025-07-15 14:29:38 UTC"
  },
  {
    "arxiv_id": "2506.18191v1",
    "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks",
    "authors": [
      "Masudul Hasan Masud Bhuiyan",
      "Gianluca De Stefano",
      "Giancarlo Pellegrino",
      "Cristian-Alexandru Staicu"
    ],
    "abstract": "Static analysis plays a key role in finding bugs, including security issues. A critical step in static analysis is building accurate call graphs that model function calls in a program. However, due to hard-to-analyze language features, existing call graph construction algorithms for JavaScript are neither sound nor complete. Prior work shows that even advanced solutions produce false edges and miss valid ones. In this work, we assist these tools by identifying missed call edges. Our main idea is to frame the problem as link prediction on full program graphs, using a rich representation with multiple edge types. Our approach, GRAPHIA, leverages recent advances in graph neural networks to model non-local relationships between code elements. Concretely, we propose representing JavaScript programs using a combination of syntactic- and semantic-based edges. GRAPHIA can learn from imperfect labels, including static call edges from existing tools and dynamic edges from tests, either from the same or different projects. Because call graphs are sparse, standard machine learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by ranking function definitions for each unresolved call site. We conduct a large-scale evaluation on 50 popular JavaScript libraries with 163K call edges (150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M structural and 386K semantic edges. It ranks the correct target as the top candidate in over 42% of unresolved cases and within the top 5 in 72% of cases, reducing the manual effort needed for analysis. Our results show that learning-based methods can improve the recall of JavaScript call graph construction. To our knowledge, this is the first work to apply GNN-based link prediction to full multi-file program graphs for interprocedural analysis.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18191v1",
    "published_date": "2025-06-22 22:26:44 UTC",
    "updated_date": "2025-06-22 22:26:44 UTC"
  },
  {
    "arxiv_id": "2507.00041v1",
    "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables",
    "authors": [
      "Varun Mannam",
      "Fang Wang",
      "Chaochun Liu",
      "Xin Chen"
    ],
    "abstract": "In talent management systems, critical information often resides in complex tabular formats, presenting significant retrieval challenges for conventional language models. These challenges are pronounced when processing Talent documentation that requires precise interpretation of tabular relationships for accurate information retrieval and downstream decision-making. Current table extraction methods struggle with semantic understanding, resulting in poor performance when integrated into retrieval-augmented chat applications. This paper identifies a key bottleneck - while structural table information can be extracted, the semantic relationships between tabular elements are lost, causing downstream query failures. To address this, we introduce TalentMine, a novel LLM-enhanced framework that transforms extracted tables into semantically enriched representations. Unlike conventional approaches relying on CSV or text linearization, our method employs specialized multimodal reasoning to preserve both structural and semantic dimensions of tabular data. Experimental evaluation across employee benefits document collections demonstrates TalentMine's superior performance, achieving 100% accuracy in query answering tasks compared to 0% for standard AWS Textract extraction and 40% for AWS Textract Visual Q&A capabilities. Our comparative analysis also reveals that the Claude v3 Haiku model achieves optimal performance for talent management applications. The key contributions of this work include (1) a systematic analysis of semantic information loss in current table extraction pipelines, (2) a novel LLM-based method for semantically enriched table representation, (3) an efficient integration framework for retrieval-augmented systems as end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks showing substantial improvements across multiple categories.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to KDD conference, workshop: Talent and Management Computing (TMC 2025), https://tmcworkshop.github.io/2025/",
    "pdf_url": "https://arxiv.org/pdf/2507.00041v1",
    "published_date": "2025-06-22 22:17:42 UTC",
    "updated_date": "2025-06-22 22:17:42 UTC"
  },
  {
    "arxiv_id": "2506.18187v1",
    "title": "The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis",
    "authors": [
      "Shahriar Noroozizadeh",
      "Pim Welle",
      "Jeremy C. Weiss",
      "George H. Chen"
    ],
    "abstract": "This study quantifies the association between non-adherence to antipsychotic medications and adverse outcomes in individuals with schizophrenia. We frame the problem using survival analysis, focusing on the time to the earliest of several adverse events (early death, involuntary hospitalization, jail booking). We extend standard causal inference methods (T-learner, S-learner, nearest neighbor matching) to utilize various survival models to estimate individual and average treatment effects, where treatment corresponds to medication non-adherence. Analyses are repeated using different amounts of longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny County in western Pennsylvania, we find strong evidence that non-adherence advances adverse outcomes by approximately 1 to 4 months. Ablation studies confirm that county-provided risk scores adjust for key confounders, as their removal amplifies the estimated effects. Subgroup analyses by medication formulation (injectable vs. oral) and medication type consistently show that non-adherence is associated with earlier adverse events. These findings highlight the clinical importance of adherence in delaying psychiatric crises and show that integrating survival analysis with causal inference tools can yield policy-relevant insights. We caution that although we apply causal inference, we only make associative claims and discuss assumptions needed for causal interpretation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference on Health, Inference, and Learning (CHIL 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.18187v1",
    "published_date": "2025-06-22 22:09:39 UTC",
    "updated_date": "2025-06-22 22:09:39 UTC"
  },
  {
    "arxiv_id": "2506.18185v1",
    "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers",
    "authors": [
      "Zihan Liang",
      "Ziwen Pan",
      "Sumon Kanti Dey",
      "Azra Ismail"
    ],
    "abstract": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks, specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2). Task 4 focused on detecting mentions of insomnia in clinical notes, while Task 5 addressed the extraction of food safety events from news articles. We participated in all subtasks and report key findings across them, with particular emphasis on Task 5 Subtask 1, where our system achieved strong performance-securing first place with an F1 score of 0.958 on the test set. To attain this result, we employed encoder-based models (e.g., RoBERTa), alongside GPT-4 for data augmentation. This paper outlines our approach, including preprocessing, model architecture, and subtask-specific adaptations",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In the Proceedings of the 10th Social Media Mining for Health and Health Real-World Data Workshop and Shared Tasks, co-located with AAAI ICWSM 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.18185v1",
    "published_date": "2025-06-22 21:56:59 UTC",
    "updated_date": "2025-06-22 21:56:59 UTC"
  },
  {
    "arxiv_id": "2506.18183v3",
    "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?",
    "authors": [
      "Zhiting Mei",
      "Christina Zhang",
      "Tenny Yin",
      "Justin Lidard",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18183v3",
    "published_date": "2025-06-22 21:46:42 UTC",
    "updated_date": "2025-07-18 02:39:29 UTC"
  },
  {
    "arxiv_id": "2506.18172v1",
    "title": "STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification",
    "authors": [
      "Irsyad Adam",
      "Tengyue Zhang",
      "Shrayes Raman",
      "Zhuyu Qiu",
      "Brandon Taraku",
      "Hexiang Feng",
      "Sile Wang",
      "Ashwath Radhachandran",
      "Shreeram Athreya",
      "Vedrana Ivezic",
      "Peipei Ping",
      "Corey Arnold",
      "William Speier"
    ],
    "abstract": "Thyroid cancer is among the most common cancers in the United States. Thyroid nodules are frequently detected through ultrasound (US) imaging, and some require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its effectiveness, FNA often leads to unnecessary biopsies of benign nodules, causing patient discomfort and anxiety. To address this, the American College of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been developed to reduce benign biopsies. However, such systems are limited by interobserver variability. Recent deep learning approaches have sought to improve risk stratification, but they often fail to utilize the rich temporal and spatial context provided by US cine clips, which contain dynamic global information and surrounding structural changes across various views. In this work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification (STACT-Time) model, a novel representation learning framework that integrates imaging features from US cine clips with features from segmentation masks automatically generated by a pretrained model. By leveraging self-attention and cross-attention mechanisms, our model captures the rich temporal and spatial context of US cine clips while enhancing feature representation through segmentation-guided learning. Our model improves malignancy prediction compared to state-of-the-art models, achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1 score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign nodules while maintaining high sensitivity for malignancy detection, our model has the potential to enhance clinical decision-making and improve patient outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18172v1",
    "published_date": "2025-06-22 21:14:04 UTC",
    "updated_date": "2025-06-22 21:14:04 UTC"
  },
  {
    "arxiv_id": "2506.18167v4",
    "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors",
    "authors": [
      "Constantin Venhoff",
      "Iván Arcuschin",
      "Philip Torr",
      "Arthur Conmy",
      "Neel Nanda"
    ],
    "abstract": "Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Workshop on Reasoning and Planning for Large Language Models at ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.18167v4",
    "published_date": "2025-06-22 20:45:26 UTC",
    "updated_date": "2025-10-22 09:57:18 UTC"
  },
  {
    "arxiv_id": "2507.01040v1",
    "title": "Fast Clifford Neural Layers",
    "authors": [
      "Tianxiang Xia",
      "Max Neuwinger",
      "Lin Xiao"
    ],
    "abstract": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra into neural networks. In this project we focus on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional layers and multivector activation layers, we observe that our implementation is 30% faster than standard PyTorch implementation in relatively large data + network size (>L2 cache).\n  We open source our code base at https://github.com/egretwAlker/c-opt-clifford-layers",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages content-wise",
    "pdf_url": "https://arxiv.org/pdf/2507.01040v1",
    "published_date": "2025-06-22 20:43:42 UTC",
    "updated_date": "2025-06-22 20:43:42 UTC"
  },
  {
    "arxiv_id": "2506.18165v3",
    "title": "Non-equilibrium Annealed Adjoint Sampler",
    "authors": [
      "Jaemoo Choi",
      "Yongxin Chen",
      "Molei Tao",
      "Guan-Horng Liu"
    ],
    "abstract": "Recently, there has been significant progress in learning-based diffusion samplers, which aim to sample from a given unnormalized density. Many of these approaches formulate the sampling task as a stochastic optimal control (SOC) problem using a canonical uninformative reference process, which limits their ability to efficiently guide trajectories toward the target distribution. In this work, we propose the Non-Equilibrium Annealed Adjoint Sampler (NAAS), a novel SOC-based diffusion framework that employs annealed reference dynamics as a non-stationary base SDE. This annealing structure provides a natural progression toward the target distribution and generates informative reference trajectories, thereby enhancing the stability and efficiency of learning the control. Owing to our SOC formulation, our framework can incorporate a variety of SOC solvers, thereby offering high flexibility in algorithmic design. As one instantiation, we employ a lean adjoint system inspired by adjoint matching, enabling efficient and scalable training. We demonstrate the effectiveness of NAAS across a range of tasks, including sampling from classical energy landscapes and molecular Boltzmann distributions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18165v3",
    "published_date": "2025-06-22 20:41:31 UTC",
    "updated_date": "2025-11-25 07:56:48 UTC"
  },
  {
    "arxiv_id": "2506.18158v1",
    "title": "Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation",
    "authors": [
      "Xinzge Gao",
      "Chuanrui Hu",
      "Bin Chen",
      "Teng Li"
    ],
    "abstract": "Multimodal large language models (MLLMs) are attracting growing attention in the development of Graphical User Interface (GUI) agents. Existing approaches often rely on historical screenshots or actions to implicitly represent the task state. This reliance poses challenges for GUI agents in accurately understanding task states and underscores the absence of effective mechanisms to store critical information in complex and lengthy cross-app tasks. To address these challenges, we propose Chain-of-Memory (CoM), a novel approach for explicitly modeling short-term and long-term memory in GUI agents. CoM achieves this by capturing action descriptions, integrating task-relevant screen information, and maintaining a dedicated memory module to store and manage this information. By leveraging explicit memory representations, CoM enables GUI agents to better understand task states and retain critical historical information persistently. To equip GUI agents with memory management capabilities and evaluate the effectiveness of CoM, we developed the GUI Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with Chain-of-Memory. Experimental results demonstrate that CoM significantly improves GUI agents' performance in cross-application tasks. Additionally, GUI Odyssey-CoM enables 7B models to achieve memory management capabilities comparable to 72B models. The dataset and code will be open-sourced.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18158v1",
    "published_date": "2025-06-22 20:17:46 UTC",
    "updated_date": "2025-06-22 20:17:46 UTC"
  },
  {
    "arxiv_id": "2506.18156v3",
    "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology",
    "authors": [
      "Akash Kundu",
      "Rishika Goswami"
    ],
    "abstract": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to IJCNLP-AACL 2025 Student Research Workshop",
    "pdf_url": "https://arxiv.org/pdf/2506.18156v3",
    "published_date": "2025-06-22 19:58:19 UTC",
    "updated_date": "2025-12-11 18:18:42 UTC"
  },
  {
    "arxiv_id": "2506.18149v1",
    "title": "CoachGPT: A Scaffolding-based Academic Writing Assistant",
    "authors": [
      "Fumian Chen",
      "Sotheara Veng",
      "Joshua Wilson",
      "Xiaoming Li",
      "Hui Fang"
    ],
    "abstract": "Academic writing skills are crucial for students' success, but can feel overwhelming without proper guidance and practice, particularly when writing in a second language. Traditionally, students ask instructors or search dictionaries, which are not universally accessible. Early writing assistants emerged as rule-based systems that focused on detecting misspellings, subject-verb disagreements, and basic punctuation errors; however, they are inaccurate and lack contextual understanding. Machine learning-based assistants demonstrate a strong ability for language understanding but are expensive to train. Large language models (LLMs) have shown remarkable capabilities in generating responses in natural languages based on given prompts. Still, they have a fundamental limitation in education: they generate essays without teaching, which can have detrimental effects on learning when misused. To address this limitation, we develop CoachGPT, which leverages large language models (LLMs) to assist individuals with limited educational resources and those who prefer self-paced learning in academic writing. CoachGPT is an AI agent-based web application that (1) takes instructions from experienced educators, (2) converts instructions into sub-tasks, and (3) provides real-time feedback and suggestions using large language models. This unique scaffolding structure makes CoachGPT unique among existing writing assistants. Compared to existing writing assistants, CoachGPT provides a more immersive writing experience with personalized feedback and guidance. Our user studies prove the usefulness of CoachGPT and the potential of large language models for academic writing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "SIGIR 2025 DEMO Pre-print",
    "pdf_url": "https://arxiv.org/pdf/2506.18149v1",
    "published_date": "2025-06-22 19:39:33 UTC",
    "updated_date": "2025-06-22 19:39:33 UTC"
  },
  {
    "arxiv_id": "2506.21617v1",
    "title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems",
    "authors": [
      "Hiba Bederina",
      "Jill-Jênn Vie"
    ],
    "abstract": "The challenge of balancing user relevance and content diversity in recommender systems is increasingly critical amid growing concerns about content homogeneity and reduced user engagement. In this work, we propose a novel framework that leverages a multi-objective, contextual sequential sampling strategy. Item selection is guided by Bayesian updates that dynamically adjust scores to optimize diversity. The reward formulation integrates multiple diversity metrics-including the log-determinant volume of a tuned similarity submatrix and ridge leverage scores-along with a diversity gain uncertainty term to address the exploration-exploitation trade-off. Both intra- and inter-batch diversity are modeled to promote serendipity and minimize redundancy. A dominance-based ranking procedure identifies Pareto-optimal item sets, enabling adaptive and balanced selections at each iteration. Experiments on a real-world dataset show that our approach significantly improves diversity without sacrificing relevance, demonstrating its potential to enhance user experience in large-scale recommendation settings.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21617v1",
    "published_date": "2025-06-22 19:36:02 UTC",
    "updated_date": "2025-06-22 19:36:02 UTC"
  },
  {
    "arxiv_id": "2506.18148v1",
    "title": "QuranMorph: Morphologically Annotated Quranic Corpus",
    "authors": [
      "Diyam Akra",
      "Tymaa Hammouda",
      "Mustafa Jarrar"
    ],
    "abstract": "We present the QuranMorph corpus, a morphologically annotated corpus for the Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and tagged with its part-of-speech by three expert linguists. The lemmatization process utilized lemmas from Qabas, an Arabic lexicographic database linked with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40 tags. As shown in this paper, this rich lemmatization and POS tagset enabled the QuranMorph corpus to be inter-linked with many linguistic resources. The corpus is open-source and publicly available as part of the SinaLab resources at (https://sina.birzeit.edu/quran)",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18148v1",
    "published_date": "2025-06-22 19:34:09 UTC",
    "updated_date": "2025-06-22 19:34:09 UTC"
  },
  {
    "arxiv_id": "2506.18941v1",
    "title": "Can AI support student engagement in classroom activities in higher education?",
    "authors": [
      "Neha Rani",
      "Sharan Majumder",
      "Ishan Bhardwaj",
      "Pedro Guillermo Feijoo Garcia"
    ],
    "abstract": "Lucrative career prospects and creative opportunities often attract students to enroll in computer science majors and pursue advanced studies in the field. Consequently, there has been a significant surge in enrollment in computer science courses, resulting in large class sizes that can range from hundreds to even thousands of students. A common challenge in such large classrooms is the lack of engagement between students and both the instructor and the learning material. However, with advancements in technology and improvements in large language models (LLMs), there is a considerable opportunity to utilize LLM-based AI models, such as conversational artificial intelligence (CAI), to enhance student engagement with learning content in large classes. To explore the potential of CAI to support engagement, especially with learning content, we designed an activity in a software Engineering course (with a large class size) where students used CAI for an in-class activity. We conducted a within-subject investigation in a large classroom at a US university where we compared student engagement during an in-class activity that used CAI tool vs. one without CAI tool. The CAI tool we used was ChatGPT due to its widespread popularity and familiarity. Our results indicate that CAI (ChatGPT) has the potential to support engagement with learning content during in-class activities, especially in large class sizes. We further discuss the implications of our findings.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18941v1",
    "published_date": "2025-06-22 19:30:47 UTC",
    "updated_date": "2025-06-22 19:30:47 UTC"
  },
  {
    "arxiv_id": "2506.18145v1",
    "title": "Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection",
    "authors": [
      "Zheng Zhan",
      "Liliang Ren",
      "Shuohang Wang",
      "Liyuan Liu",
      "Yang Liu",
      "Yeyun Gong",
      "Yanzhi Wang",
      "Yelong Shen"
    ],
    "abstract": "Linear State Space Models (SSMs) offer remarkable performance gains in efficient sequence modeling, with constant inference-time computation and memory complexity. Recent advances, such as Mamba, further enhance SSMs with input-dependent gating and hardware-aware implementations, positioning them as strong alternatives to Transformers for long sequence modeling. However, efficiently scaling the expressive power of SSMs, particularly with Mixture of Experts (MoE), remains challenging, as naive integration attempts often falter or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel approach that scales SSM parameters using sparse mixtures of linear projection experts. By sharing routing decisions between projection layers and lightweight sub-modules within Mamba across experts, RoM leverages synergies among linear projection experts for effective and efficient sparse scaling of Mamba layers. At a scale of 1.3B active parameters (10B total) and 16K training sequence length, RoM achieves language modeling performance equivalent to a dense Mamba model requiring over 2.3x more active parameters, and demonstrates consistent perplexity across context lengths. Experimental results further show RoM effectively scales hybrid language models, yielding a 23% FLOPS saving compared to dense Mamba scaling for similar performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18145v1",
    "published_date": "2025-06-22 19:26:55 UTC",
    "updated_date": "2025-06-22 19:26:55 UTC"
  },
  {
    "arxiv_id": "2506.18143v1",
    "title": "AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System",
    "authors": [
      "Lancelot Blanchard",
      "Cameron Holt",
      "Joseph A. Paradiso"
    ],
    "abstract": "Vocals harmonizers are powerful tools to help solo vocalists enrich their melodies with harmonically supportive voices. These tools exist in various forms, from commercially available pedals and software to custom-built systems, each employing different methods to generate harmonies. Traditional harmonizers often require users to manually specify a key or tonal center, while others allow pitch selection via an external keyboard-both approaches demanding some degree of musical expertise. The AI Harmonizer introduces a novel approach by autonomously generating musically coherent four-part harmonies without requiring prior harmonic input from the user. By integrating state-of-the-art generative AI techniques for pitch detection and voice modeling with custom-trained symbolic music models, our system arranges any vocal melody into rich choral textures. In this paper, we present our methods, explore potential applications in performance and composition, and discuss future directions for real-time implementations. While our system currently operates offline, we believe it represents a significant step toward AI-assisted vocal performance and expressive musical augmentation. We release our implementation on GitHub.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "4 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18143v1",
    "published_date": "2025-06-22 19:13:31 UTC",
    "updated_date": "2025-06-22 19:13:31 UTC"
  },
  {
    "arxiv_id": "2506.18141v2",
    "title": "Sparse Feature Coactivation Reveals Causal Semantic Modules in Large Language Models",
    "authors": [
      "Ruixuan Deng",
      "Xiaoyang Hu",
      "Miles Gilberti",
      "Shane Storks",
      "Aman Taxali",
      "Mike Angstadt",
      "Chandra Sripada",
      "Joyce Chai"
    ],
    "abstract": "We identify semantically coherent, context-consistent network components in large language models (LLMs) using coactivation of sparse autoencoder (SAE) features collected from just a handful of prompts. Focusing on concept-relation prediction tasks, we show that ablating these components for concepts (e.g., countries and words) and relations (e.g., capital city and translation language) changes model outputs in predictable ways, while amplifying these components induces counterfactual responses. Notably, composing relation and concept components yields compound counterfactual outputs. Further analysis reveals that while most concept components emerge from the very first layer, more abstract relation components are concentrated in later layers. Lastly, we show that extracted components more comprehensively capture concepts and relations than individual features while maintaining specificity. Overall, our findings suggest a modular organization of knowledge accessed through compositional operations, and advance methods for efficient, targeted LLM manipulation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18141v2",
    "published_date": "2025-06-22 19:01:13 UTC",
    "updated_date": "2025-10-20 18:44:35 UTC"
  },
  {
    "arxiv_id": "2506.18135v1",
    "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging",
    "authors": [
      "Zijun Chen",
      "Zhanpeng Zhou",
      "Bo Zhang",
      "Weinan Zhang",
      "Xi Sun",
      "Junchi Yan"
    ],
    "abstract": "Model merging has gained increasing attention due to its intriguing property: interpolating the parameters of different task-specific fine-tuned models leads to multi-task abilities. However, despite its empirical success, the underlying mechanisms of model merging remain poorly understood. In this work, we delve into the mechanism behind model merging from a representation perspective. Our analysis reveals that model merging achieves multi-task abilities through two key capabilities: i) distinguishing samples from different tasks, and ii) adapting to the corresponding expert model for each sample. These two capabilities allow the merged model to retain task-specific expertise, enabling efficient multi-task adaptation. Building on these insights, we propose \\texttt{SE-Merging}, a self-enhanced model merging framework that leverages these two characteristics to dynamically identify the corresponding task for each sample and then adaptively rescales the merging coefficients to further enhance task-specific expertise in the merged model. Notably, \\texttt{SE-Merging} achieves dynamic model merging without additional training. Extensive experiments demonstrate that \\texttt{SE-Merging} achieves significant performance improvements while remaining compatible with existing model merging techniques.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint, accepted at IJCNN2025",
    "pdf_url": "https://arxiv.org/pdf/2506.18135v1",
    "published_date": "2025-06-22 18:38:41 UTC",
    "updated_date": "2025-06-22 18:38:41 UTC"
  },
  {
    "arxiv_id": "2506.18129v1",
    "title": "$φ^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models",
    "authors": [
      "Bugra Kilictas",
      "Faruk Alpay"
    ],
    "abstract": "We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18129v1",
    "published_date": "2025-06-22 18:27:39 UTC",
    "updated_date": "2025-06-22 18:27:39 UTC"
  },
  {
    "arxiv_id": "2506.18126v1",
    "title": "Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game",
    "authors": [
      "Xiang Yuming",
      "Li Sizhao",
      "Li Rongpeng",
      "Zhao Zhifeng",
      "Zhang Honggang"
    ],
    "abstract": "Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered widespread research interest and fostered tremendous interesting applications, especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to maximize formation coverage across multiple target zones while collaboratively evading predators, belongs to one of the most challenging issues in MC-PEG, especially under communication-limited constraints. This multifaceted problem, which intertwines responses to obstacles, adversaries, target zones, and formation dynamics, brings up significant high-dimensional complications in locating a solution. In this paper, we propose a novel two-level framework (i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)), which delegates target localization to a high-level policy, while adopting a low-level policy to manage obstacle avoidance, navigation, and formation. Specifically, in the high-level policy, we develop a novel multi-agent reinforcement learning module, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish consensus from local states by effectively aggregating neighbor messages. Meanwhile, we leverage an Alternative Training-based Multi-agent proximal policy optimization (AT-M) and policy distillation to accomplish the low-level control. The experimental results, including the high-fidelity software-in-the-loop (SITL) simulations, validate that CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18126v1",
    "published_date": "2025-06-22 18:23:58 UTC",
    "updated_date": "2025-06-22 18:23:58 UTC"
  },
  {
    "arxiv_id": "2506.18119v1",
    "title": "Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review",
    "authors": [
      "Jaime Banks",
      "Zhixin Li"
    ],
    "abstract": "The notion of machine companions has long been embedded in social-technological imaginaries. Recent advances in AI have moved those media musings into believable sociality manifested in interfaces, robotic bodies, and devices. Those machines are often referred to colloquially as \"companions\" yet there is little careful engagement of machine companionship (MC) as a formal concept or measured variable. This PRISMA-guided scoping review systematically samples, surveys, and synthesizes current scholarly works on MC (N = 71; 2017-2025), to that end. Works varied widely in considerations of MC according to guiding theories, dimensions of a-priori specified properties (subjectively positive, sustained over time, co-active, autotelic), and in measured concepts (with more than 50 distinct measured variables). WE ultimately offer a literature-guided definition of MC as an autotelic, coordinated connection between human and machine that unfolds over time and is subjectively positive.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18119v1",
    "published_date": "2025-06-22 18:02:18 UTC",
    "updated_date": "2025-06-22 18:02:18 UTC"
  },
  {
    "arxiv_id": "2506.18116v1",
    "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives",
    "authors": [
      "Batool Haider",
      "Atmika Gorti",
      "Aman Chadha",
      "Manas Gaur"
    ],
    "abstract": "Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)",
    "pdf_url": "https://arxiv.org/pdf/2506.18116v1",
    "published_date": "2025-06-22 18:00:16 UTC",
    "updated_date": "2025-06-22 18:00:16 UTC"
  },
  {
    "arxiv_id": "2506.18940v1",
    "title": "eccDNAMamba: A Pre-Trained Model for Ultra-Long eccDNA Sequence Analysis",
    "authors": [
      "Zhenke Liu",
      "Jien Li",
      "Ziqi Zhang"
    ],
    "abstract": "Extrachromosomal circular DNA (eccDNA) plays key regulatory roles and contributes to oncogene overexpression in cancer through high-copy amplification and long-range interactions. Despite advances in modeling, no pre-trained models currently support full-length circular eccDNA for downstream analysis. Existing genomic models are either limited to single-nucleotide resolution or hindered by the inefficiency of the quadratic attention mechanism. Here, we introduce eccDNAMamba, the first bidirectional state-space encoder tailored for circular DNA sequences. It combines forward and reverse passes for full-context representation learning with linear-time complexity, and preserves circular structure through a novel augmentation strategy. Tested on two real-world datasets, eccDNAMamba achieves strong classification performance and scales to sequences up to 200 Kbp, offering a robust and efficient framework for modeling circular genomes. Our codes are available at https://github.com/zzq1zh/GenAI-Lab.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "Accepted by ICML 2025 Generative AI and Biology (GenBio) Workshop",
    "pdf_url": "https://arxiv.org/pdf/2506.18940v1",
    "published_date": "2025-06-22 17:50:57 UTC",
    "updated_date": "2025-06-22 17:50:57 UTC"
  },
  {
    "arxiv_id": "2507.01039v2",
    "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization",
    "authors": [
      "Kaaustaaub Shankar",
      "Wilhelm Louw",
      "Kelly Cohen"
    ],
    "abstract": "We present a reinforcement learning method for training neuro-fuzzy controllers using Proximal Policy Optimization (PPO). Unlike prior approaches that used Deep Q-Networks (DQN) with Adaptive Neuro-Fuzzy Inference Systems (ANFIS), our PPO-based framework leverages a stable on-policy actor-critic setup. Evaluated on the CartPole-v1 environment across multiple seeds, PPO-trained fuzzy agents consistently achieved the maximum return of 500 with zero variance after 20000 updates, outperforming ANFIS-DQN baselines in both stability and convergence speed. This highlights PPO's potential for training explainable neuro-fuzzy agents in reinforcement learning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NAFIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.01039v2",
    "published_date": "2025-06-22 17:49:49 UTC",
    "updated_date": "2025-07-04 02:40:45 UTC"
  },
  {
    "arxiv_id": "2506.18110v1",
    "title": "RL for Reasoning by Adaptively Revealing Rationales",
    "authors": [
      "Mohammad Hossein Amani",
      "Aryo Lotfi",
      "Nicolas Mario Baldwin",
      "Samy Bengio",
      "Mehrdad Farajtabar",
      "Emmanuel Abbe",
      "Robert West"
    ],
    "abstract": "We propose that reinforcement learning (RL) from partial expert demonstrations is not merely a training heuristic, but a promising framework for solving complex sequence generation tasks. Supervised fine-tuning (SFT) relies on dense ground-truth labels, which become increasingly costly as sequence length grows. RL, on the other hand, struggles with sparse rewards and a combinatorially large output space. We address this by introducing adaptive backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals only a partial prefix of the target output during training. The supervision length is adjusted dynamically for each sample based on the model's past reward signal, allowing it to incrementally learn to complete reasoning chains by conditioning on correct partial solutions. We investigate this intermediate regime between SFT and RL and argue that per-sample curriculum learning is more than a trade-off between efficiency and generality, it can succeed in tasks with long sequences of latent dependencies where SFT and RL both fail to generalize. Using a synthetic task with latent parity constraints, we show that our adaptive curriculum over partial answers reliably solves problems that are otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we find that curriculum learning enables models to solve problems that RL alone cannot, acquiring new reasoning capabilities through incremental exposure to partial solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18110v1",
    "published_date": "2025-06-22 17:46:14 UTC",
    "updated_date": "2025-06-22 17:46:14 UTC"
  },
  {
    "arxiv_id": "2506.18096v2",
    "title": "Deep Research Agents: A Systematic Examination And Roadmap",
    "authors": [
      "Yuxuan Huang",
      "Yihang Chen",
      "Haozheng Zhang",
      "Kang Li",
      "Huichi Zhou",
      "Meng Fang",
      "Linyi Yang",
      "Xiaoguang Li",
      "Lifeng Shang",
      "Songcen Xu",
      "Jianye Hao",
      "Kun Shao",
      "Jun Wang"
    ],
    "abstract": "The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18096v2",
    "published_date": "2025-06-22 16:52:48 UTC",
    "updated_date": "2025-09-03 15:32:23 UTC"
  },
  {
    "arxiv_id": "2506.18095v1",
    "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation",
    "authors": [
      "Junying Chen",
      "Zhenyang Cai",
      "Pengcheng Chen",
      "Shunian Chen",
      "Ke Ji",
      "Xidong Wang",
      "Yunjin Yang",
      "Benyou Wang"
    ],
    "abstract": "Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18095v1",
    "published_date": "2025-06-22 16:51:09 UTC",
    "updated_date": "2025-06-22 16:51:09 UTC"
  },
  {
    "arxiv_id": "2506.18088v2",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
    "authors": [
      "Tianxing Chen",
      "Zanxin Chen",
      "Baijun Chen",
      "Zijian Cai",
      "Yibin Liu",
      "Zixuan Li",
      "Qiwei Liang",
      "Xianliang Lin",
      "Yiheng Ge",
      "Zhenyu Gu",
      "Weiliang Deng",
      "Yubin Guo",
      "Tian Nian",
      "Xuanbing Xie",
      "Qiangyu Chen",
      "Kailun Su",
      "Tianling Xu",
      "Guodong Liu",
      "Mengkang Hu",
      "Huan-ang Gao",
      "Kaixuan Wang",
      "Zhixuan Liang",
      "Yusen Qin",
      "Xiaokang Yang",
      "Ping Luo",
      "Yao Mu"
    ],
    "abstract": "Simulation-based data synthesis has emerged as a powerful paradigm for advancing real-world robotic manipulation. Yet existing datasets remain insufficient for robust bimanual manipulation due to (1) the lack of scalable task generation methods and (2) oversimplified simulation environments. We present RoboTwin 2.0, a scalable framework for automated, large-scale generation of diverse and realistic data, together with unified evaluation protocols for dual-arm manipulation. At its core is RoboTwin-OD, an object library of 731 instances across 147 categories with semantic and manipulation-relevant annotations. Building on this, we design an expert data synthesis pipeline that leverages multimodal language models (MLLMs) and simulation-in-the-loop refinement to automatically generate task-level execution code. To improve sim-to-real transfer, RoboTwin 2.0 applies structured domain randomization along five axes: clutter, lighting, background, tabletop height, and language, enhancing data diversity and policy robustness. The framework is instantiated across 50 dual-arm tasks and five robot embodiments. Empirically, it yields a 10.9% gain in code generation success rate. For downstream policy learning, a VLA model trained with synthetic data plus only 10 real demonstrations achieves a 367% relative improvement over the 10-demo baseline, while zero-shot models trained solely on synthetic data obtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0 in strengthening sim-to-real transfer and robustness to environmental variations. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation. Project Page: https://robotwin-platform.github.io/, Code: https://github.com/robotwin-Platform/robotwin/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://robotwin-platform.github.io/, Code: https://github.com/robotwin-Platform/robotwin, Doc: https://robotwin-platform.github.io/doc/",
    "pdf_url": "https://arxiv.org/pdf/2506.18088v2",
    "published_date": "2025-06-22 16:26:53 UTC",
    "updated_date": "2025-08-27 17:52:42 UTC"
  },
  {
    "arxiv_id": "2506.18087v1",
    "title": "Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models",
    "authors": [
      "Huaiying Luo",
      "Cheng Ji"
    ],
    "abstract": "With the widespread application of edge computing and cloud systems in AI-driven applications, how to maintain efficient performance while ensuring data privacy has become an urgent security issue. This paper proposes a federated learning-based data collaboration method to improve the security of edge cloud AI systems, and use large-scale language models (LLMs) to enhance data privacy protection and system robustness. Based on the existing federated learning framework, this method introduces a secure multi-party computation protocol, which optimizes the data aggregation and encryption process between distributed nodes by using LLM to ensure data privacy and improve system efficiency. By combining advanced adversarial training techniques, the model enhances the resistance of edge cloud AI systems to security threats such as data leakage and model poisoning. Experimental results show that the proposed method is 15% better than the traditional federated learning method in terms of data protection and model robustness.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by the 2025 5th International Symposium on Computer Technology and Information Science (ISCTIS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.18087v1",
    "published_date": "2025-06-22 16:23:45 UTC",
    "updated_date": "2025-06-22 16:23:45 UTC"
  },
  {
    "arxiv_id": "2506.18074v1",
    "title": "Distributionally robust minimization in meta-learning for system identification",
    "authors": [
      "Matteo Rufolo",
      "Dario Piga",
      "Marco Forgione"
    ],
    "abstract": "Meta learning aims at learning how to solve tasks, and thus it allows to estimate models that can be quickly adapted to new scenarios. This work explores distributionally robust minimization in meta learning for system identification. Standard meta learning approaches optimize the expected loss, overlooking task variability. We use an alternative approach, adopting a distributionally robust optimization paradigm that prioritizes high-loss tasks, enhancing performance in worst-case scenarios. Evaluated on a meta model trained on a class of synthetic dynamical systems and tested in both in-distribution and out-of-distribution settings, the proposed approach allows to reduce failures in safety-critical applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18074v1",
    "published_date": "2025-06-22 15:41:22 UTC",
    "updated_date": "2025-06-22 15:41:22 UTC"
  },
  {
    "arxiv_id": "2506.18939v3",
    "title": "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction",
    "authors": [
      "Rui An",
      "Yifeng Zhang",
      "Ziran Liang",
      "Wenqi Fan",
      "Yuxuan Liang",
      "Xuequn Shang",
      "Qing Li"
    ],
    "abstract": "Training urban spatio-temporal foundation models that generalize well across diverse regions and cities is critical for deploying urban services in unseen or data-scarce regions. Recent studies have typically focused on fusing cross-domain spatio-temporal data to train unified Transformer-based models. However, these models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment. Inspired by the efficiency of Mamba, a state space model with linear time complexity, we explore its potential for efficient urban spatio-temporal prediction. However, directly applying Mamba as a spatio-temporal backbone leads to negative transfer and severe performance degradation. This is primarily due to spatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden state updates, which limit cross-domain generalization. To overcome these challenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for efficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear complexity advantage while significantly enhancing its adaptability to heterogeneous domains. Specifically, we introduce two core innovations: (1) a domain-adaptive state space model that partitions the latent representation space into a shared subspace for learning cross-domain commonalities and independent, domain-specific subspaces for capturing intra-domain discriminative features; (2) three distinct Domain Adapters, which serve as domain-aware proxies to bridge disparate domain distributions and facilitate the alignment of cross-domain commonalities. Extensive experiments demonstrate the generalization and efficiency of Damba-ST. It achieves state-of-the-art performance on prediction tasks and demonstrates strong zero-shot generalization, enabling seamless deployment in new urban environments without extensive retraining or fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICDE 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.18939v3",
    "published_date": "2025-06-22 15:40:01 UTC",
    "updated_date": "2026-01-03 16:17:20 UTC"
  },
  {
    "arxiv_id": "2506.18072v2",
    "title": "Multimodal Medical Image Binding via Shared Text Embeddings",
    "authors": [
      "Yunhao Liu",
      "Suyang Xi",
      "Shiqi Liu",
      "Hong Ding",
      "Chicheng Jin",
      "Chong Zhong",
      "Junjun He",
      "Catherine C. Liu",
      "Yiqing Shen"
    ],
    "abstract": "Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\\textsuperscript{3}Bind's effectiveness in achieving cross-image-modal alignment for medical analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18072v2",
    "published_date": "2025-06-22 15:39:25 UTC",
    "updated_date": "2025-09-03 16:04:55 UTC"
  },
  {
    "arxiv_id": "2506.18071v2",
    "title": "MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering",
    "authors": [
      "Jisheng Dang",
      "Huilin Song",
      "Junbin Xiao",
      "Bimei Wang",
      "Han Peng",
      "Haoxuan Li",
      "Xun Yang",
      "Meng Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "Grounded Video Question Answering (Grounded VideoQA) requires aligning textual answers with explicit visual evidence. However, modern multimodal models often rely on linguistic priors and spurious correlations, resulting in poorly grounded predictions. In this work, we propose MUPA, a cooperative MUlti-Path Agentic approach that unifies video grounding, question answering, answer reflection and aggregation to tackle Grounded VideoQA. MUPA features three distinct reasoning paths on the interplay of grounding and QA agents in different chronological orders, along with a dedicated reflection agent to judge and aggregate the multi-path results to accomplish consistent QA and grounding. This design markedly improves grounding fidelity without sacrificing answer accuracy. Despite using only 2B parameters, our method outperforms all 7B-scale competitors. When scaled to 7B parameters, MUPA establishes new state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy video-language understanding. Our code is available in https://github.com/longmalongma/MUPA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18071v2",
    "published_date": "2025-06-22 15:39:02 UTC",
    "updated_date": "2025-06-27 06:32:43 UTC"
  },
  {
    "arxiv_id": "2506.18056v1",
    "title": "Weighted Assumption Based Argumentation to reason about ethical principles and actions",
    "authors": [
      "Paolo Baldi",
      "Fabio Aurelio D'Asaro",
      "Abeer Dyoub",
      "Francesca Alessandra Lisi"
    ],
    "abstract": "We augment Assumption Based Argumentation (ABA for short) with weighted argumentation. In a nutshell, we assign weights to arguments and then derive the weight of attacks between ABA arguments. We illustrate our proposal through running examples in the field of ethical reasoning, and present an implementation based on Answer Set Programming.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18056v1",
    "published_date": "2025-06-22 14:46:42 UTC",
    "updated_date": "2025-06-22 14:46:42 UTC"
  },
  {
    "arxiv_id": "2506.18053v1",
    "title": "Mechanistic Interpretability in the Presence of Architectural Obfuscation",
    "authors": [
      "Marcos Florencio",
      "Thomas Barton"
    ],
    "abstract": "Architectural obfuscation - e.g., permuting hidden-state tensors, linearly transforming embedding tables, or remapping tokens - has recently gained traction as a lightweight substitute for heavyweight cryptography in privacy-preserving large-language-model (LLM) inference. While recent work has shown that these techniques can be broken under dedicated reconstruction attacks, their impact on mechanistic interpretability has not been systematically studied. In particular, it remains unclear whether scrambling a network's internal representations truly thwarts efforts to understand how the model works, or simply relocates the same circuits to an unfamiliar coordinate system. We address this gap by analyzing a GPT-2-small model trained from scratch with a representative obfuscation map. Assuming the obfuscation map is private and the original basis is hidden (mirroring an honest-but-curious server), we apply logit-lens attribution, causal path-patching, and attention-head ablation to locate and manipulate known circuits. Our findings reveal that obfuscation dramatically alters activation patterns within attention heads yet preserves the layer-wise computational graph. This disconnect hampers reverse-engineering of user prompts: causal traces lose their alignment with baseline semantics, and token-level logit attributions become too noisy to reconstruct. At the same time, feed-forward and residual pathways remain functionally intact, suggesting that obfuscation degrades fine-grained interpretability without compromising top-level task performance. These results establish quantitative evidence that architectural obfuscation can simultaneously (i) retain global model behaviour and (ii) impede mechanistic analyses of user-specific content. By mapping where interpretability breaks down, our study provides guidance for future privacy defences and for robustness-aware interpretability tooling.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18053v1",
    "published_date": "2025-06-22 14:39:16 UTC",
    "updated_date": "2025-06-22 14:39:16 UTC"
  },
  {
    "arxiv_id": "2506.18045v1",
    "title": "The Democratic Paradox in Large Language Models' Underestimation of Press Freedom",
    "authors": [
      "I. Loaiza",
      "R. Vestrelli",
      "A. Fronzetti Colladon",
      "R. Rigobon"
    ],
    "abstract": "As Large Language Models (LLMs) increasingly mediate global information access for millions of users worldwide, their alignment and biases have the potential to shape public understanding and trust in fundamental democratic institutions, such as press freedom. In this study, we uncover three systematic distortions in the way six popular LLMs evaluate press freedom in 180 countries compared to expert assessments of the World Press Freedom Index (WPFI). The six LLMs exhibit a negative misalignment, consistently underestimating press freedom, with individual models rating between 71% to 93% of countries as less free. We also identify a paradoxical pattern we term differential misalignment: LLMs disproportionately underestimate press freedom in countries where it is strongest. Additionally, five of the six LLMs exhibit positive home bias, rating their home countries' press freedoms more favorably than would be expected given their negative misalignment with the human benchmark. In some cases, LLMs rate their home countries between 7% to 260% more positively than expected. If LLMs are set to become the next search engines and some of the most important cultural tools of our time, they must ensure accurate representations of the state of our human and civic rights globally.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18045v1",
    "published_date": "2025-06-22 14:10:16 UTC",
    "updated_date": "2025-06-22 14:10:16 UTC"
  },
  {
    "arxiv_id": "2506.18044v1",
    "title": "Action Language BC+",
    "authors": [
      "Joseph Babb",
      "Joohyung Lee"
    ],
    "abstract": "Action languages are formal models of parts of natural language that are designed to describe effects of actions. Many of these languages can be viewed as high level notations of answer set programs structured to represent transition systems. However, the form of answer set programs considered in the earlier work is quite limited in comparison with the modern Answer Set Programming (ASP) language, which allows several useful constructs for knowledge representation, such as choice rules, aggregates, and abstract constraint atoms. We propose a new action language called BC+, which closes the gap between action languages and the modern ASP language. The main idea is to define the semantics of BC+ in terms of general stable model semantics for propositional formulas, under which many modern ASP language constructs can be identified with shorthands for propositional formulas. Language BC+ turns out to be sufficiently expressive to encompass the best features of other action languages, such as languages B, C, C+, and BC. Computational methods available in ASP solvers are readily applicable to compute BC+, which led to an implementation of the language by extending system cplus2asp.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Journal of Logic and Computation, 2015",
    "pdf_url": "https://arxiv.org/pdf/2506.18044v1",
    "published_date": "2025-06-22 14:09:49 UTC",
    "updated_date": "2025-06-22 14:09:49 UTC"
  },
  {
    "arxiv_id": "2506.18037v1",
    "title": "Pathwise Explanation of ReLU Neural Networks",
    "authors": [
      "Seongwoo Lim",
      "Won Jo",
      "Joohyung Lee",
      "Jaesik Choi"
    ],
    "abstract": "Neural networks have demonstrated a wide range of successes, but their ``black box\" nature raises concerns about transparency and reliability. Previous research on ReLU networks has sought to unwrap these networks into linear models based on activation states of all hidden units. In this paper, we introduce a novel approach that considers subsets of the hidden units involved in the decision making path. This pathwise explanation provides a clearer and more consistent understanding of the relationship between the input and the decision-making process. Our method also offers flexibility in adjusting the range of explanations within the input, i.e., from an overall attribution input to particular components within the input. Furthermore, it allows for the decomposition of explanations for a given input for more detailed explanations. Experiments demonstrate that our method outperforms others both quantitatively and qualitatively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, PMLR 238:4645-4653, 2024",
    "pdf_url": "https://arxiv.org/pdf/2506.18037v1",
    "published_date": "2025-06-22 13:41:42 UTC",
    "updated_date": "2025-06-22 13:41:42 UTC"
  },
  {
    "arxiv_id": "2506.18034v1",
    "title": "Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster",
    "authors": [
      "Fenghe Tang",
      "Wenxin Ma",
      "Zhiyang He",
      "Xiaodong Tao",
      "Zihang Jiang",
      "S. Kevin Zhou"
    ],
    "abstract": "With the advancement of Large Language Model (LLM) for natural language processing, this paper presents an intriguing finding: a frozen pre-trained LLM layer can process visual tokens for medical image segmentation tasks. Specifically, we propose a simple hybrid structure that integrates a pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation framework (LLM4Seg). Surprisingly, this design improves segmentation performance with a minimal increase in trainable parameters across various modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our in-depth analysis reveals the potential of transferring LLM's semantic awareness to enhance segmentation tasks, offering both improved global understanding and better local modeling capabilities. The improvement proves robust across different LLMs, validated using LLaMA and DeepSeek.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by MICCAI 2025. Code: https://github.com/FengheTan9/LLM4Seg",
    "pdf_url": "https://arxiv.org/pdf/2506.18034v1",
    "published_date": "2025-06-22 13:34:00 UTC",
    "updated_date": "2025-06-22 13:34:00 UTC"
  },
  {
    "arxiv_id": "2506.18023v2",
    "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding",
    "authors": [
      "Kui Huang",
      "Xinrong Chen",
      "Wenyu Lv",
      "Jincheng Liao",
      "Guanzhong Wang",
      "Yi Liu"
    ],
    "abstract": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee, designed to enhance multimodal document understanding. Built on a large multimodal model architecture, PP-DocBee2 addresses the limitations of its predecessor through key technological improvements, including enhanced synthetic data quality, improved visual feature fusion strategy, and optimized inference methodologies. These enhancements yield an $11.4\\%$ performance boost on internal benchmarks for Chinese business documents, and reduce inference latency by $73.0\\%$ to the vanilla version. A key innovation of our work is a data quality optimization strategy for multimodal document tasks. By employing a large-scale multimodal pre-trained model to evaluate data, we apply a novel statistical criterion to filter outliers, ensuring high-quality training data. Inspired by insights into underutilized intermediate features in multimodal models, we enhance the ViT representational capacity by decomposing it into layers and applying a novel feature fusion strategy to improve complex reasoning. The source code and pre-trained model are available at \\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18023v2",
    "published_date": "2025-06-22 13:06:13 UTC",
    "updated_date": "2025-06-25 02:40:39 UTC"
  },
  {
    "arxiv_id": "2506.18019v3",
    "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities",
    "authors": [
      "Yuanchen Bei",
      "Weizhi Zhang",
      "Siwen Wang",
      "Weizhi Chen",
      "Sheng Zhou",
      "Hao Chen",
      "Yong Li",
      "Jiajun Bu",
      "Shirui Pan",
      "Yizhou Yu",
      "Irwin King",
      "Fakhri Karray",
      "Philip S. Yu"
    ],
    "abstract": "AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.18019v3",
    "published_date": "2025-06-22 12:59:12 UTC",
    "updated_date": "2025-07-04 14:29:40 UTC"
  },
  {
    "arxiv_id": "2506.18017v1",
    "title": "Auto-Regressive Surface Cutting",
    "authors": [
      "Yang Li",
      "Victor Cheung",
      "Xinhai Liu",
      "Yuguang Chen",
      "Zhongjin Luo",
      "Biwen Lei",
      "Haohan Weng",
      "Zibo Zhao",
      "Jingwei Huang",
      "Zhuo Chen",
      "Chunchao Guo"
    ],
    "abstract": "Surface cutting is a fundamental task in computer graphics, with applications in UV parameterization, texture mapping, and mesh decomposition. However, existing methods often produce technically valid but overly fragmented atlases that lack semantic coherence. We introduce SeamGPT, an auto-regressive model that generates cutting seams by mimicking professional workflows. Our key technical innovation lies in formulating surface cutting as a next token prediction task: sample point clouds on mesh vertices and edges, encode them as shape conditions, and employ a GPT-style transformer to sequentially predict seam segments with quantized 3D coordinates. Our approach achieves exceptional performance on UV unwrapping benchmarks containing both manifold and non-manifold meshes, including artist-created, and 3D-scanned models. In addition, it enhances existing 3D segmentation tools by providing clean boundaries for part decomposition.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Tech. report. https://victorcheung12.github.io/seamgpt",
    "pdf_url": "https://arxiv.org/pdf/2506.18017v1",
    "published_date": "2025-06-22 12:53:07 UTC",
    "updated_date": "2025-06-22 12:53:07 UTC"
  },
  {
    "arxiv_id": "2506.18016v3",
    "title": "ADA-DPM: A Neural Descriptors-based Adaptive Noise Filtering Strategy for SLAM",
    "authors": [
      "Yongxin Shao",
      "Aihong Tan",
      "Binrui Wang",
      "Yinlian Jin",
      "Licong Guan",
      "Peng Liao"
    ],
    "abstract": "Lidar SLAM plays a significant role in mobile robot navigation and high-definition map construction. However, existing methods often face a trade-off between localization accuracy and system robustness in scenarios with a high proportion of dynamic objects, point cloud distortion, and unstructured environments. To address this issue, we propose a neural descriptors-based adaptive noise filtering strategy for SLAM, named ADA-DPM, which improves the performance of localization and mapping tasks through three key technical innovations. Firstly, to tackle dynamic object interference, we design the Dynamic Segmentation Head to predict and filter out dynamic feature points, eliminating the ego-motion interference caused by dynamic objects. Secondly, to mitigate the impact of noise and unstructured feature points, we propose the Global Importance Scoring Head that adaptively selects high-contribution feature points while suppressing the influence of noise and unstructured feature points. Moreover, we introduce the Cross-Layer Graph Convolution Module (GLI-GCN) to construct multi-scale neighborhood graphs, fusing local structural information across different scales and improving the discriminative power of overlapping features. Finally, experimental validations on multiple public datasets confirm the effectiveness of ADA-DPM.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18016v3",
    "published_date": "2025-06-22 12:48:11 UTC",
    "updated_date": "2025-10-20 08:27:02 UTC"
  },
  {
    "arxiv_id": "2506.18011v1",
    "title": "Probing the Embedding Space of Transformers via Minimal Token Perturbations",
    "authors": [
      "Eddie Conti",
      "Alejandro Astruc",
      "Alvaro Parafita",
      "Axel Brando"
    ],
    "abstract": "Understanding how information propagates through Transformer models is a key challenge for interpretability. In this work, we study the effects of minimal token perturbations on the embedding space. In our experiments, we analyze the frequency of which tokens yield to minimal shifts, highlighting that rare tokens usually lead to larger shifts. Moreover, we study how perturbations propagate across layers, demonstrating that input information is increasingly intermixed in deeper layers. Our findings validate the common assumption that the first layers of a model can be used as proxies for model explanations. Overall, this work introduces the combination of token perturbations and shifts on the embedding space as a powerful tool for model interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "IJCAI 2025 Workshop on Explainable Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2506.18011v1",
    "published_date": "2025-06-22 12:22:56 UTC",
    "updated_date": "2025-06-22 12:22:56 UTC"
  },
  {
    "arxiv_id": "2506.21615v1",
    "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines",
    "authors": [
      "Wenhao Li",
      "Hongkuan Zhang",
      "Hongwei Zhang",
      "Zhengxu Li",
      "Zengjie Dong",
      "Yafan Chen",
      "Niranjan Bidargaddi",
      "Hong Liu"
    ],
    "abstract": "Current medical language models, adapted from large language models (LLMs), typically predict ICD code-based diagnosis from electronic health records (EHRs) because these labels are readily available. However, ICD codes do not capture the nuanced, context-rich reasoning clinicians use for diagnosis. Clinicians synthesize diverse patient data and reference clinical practice guidelines (CPGs) to make evidence-based decisions. This misalignment limits the clinical utility of existing models. We introduce GARMLE-G, a Generation-Augmented Retrieval framework that grounds medical language model outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, GARMLE-G enables hallucination-free outputs by directly retrieving authoritative guideline content without relying on model-generated text. It (1) integrates LLM predictions with EHR data to create semantically rich queries, (2) retrieves relevant CPG knowledge snippets via embedding similarity, and (3) fuses guideline content with model output to generate clinically aligned recommendations. A prototype system for hypertension diagnosis was developed and evaluated on multiple metrics, demonstrating superior retrieval precision, semantic relevance, and clinical guideline adherence compared to RAG-based baselines, while maintaining a lightweight architecture suitable for localized healthcare deployment. This work provides a scalable, low-cost, and hallucination-free method for grounding medical language models in evidence-based clinical practice, with strong potential for broader clinical deployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21615v1",
    "published_date": "2025-06-22 11:31:13 UTC",
    "updated_date": "2025-06-22 11:31:13 UTC"
  },
  {
    "arxiv_id": "2507.01971v1",
    "title": "DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification",
    "authors": [
      "Boris Kriuk",
      "Logic Ng",
      "Zarif Al Hossain"
    ],
    "abstract": "Support and resistance (SR) levels are central to technical analysis, guiding traders in entry, exit, and risk management. Despite widespread use, traditional SR identification methods often fail to adapt to the complexities of modern, volatile markets. Recent research has introduced machine learning techniques to address the following challenges, yet most focus on price prediction rather than structural level identification. This paper presents DeepSupp, a new deep learning approach for detecting financial support levels using multi-head attention mechanisms to analyze spatial correlations and market microstructure relationships. DeepSupp integrates advanced feature engineering, constructing dynamic correlation matrices that capture evolving market relationships, and employs an attention-based autoencoder for robust representation learning. The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds. Comprehensive evaluations on S&P 500 tickers demonstrate that DeepSupp outperforms six baseline methods, achieving state-of-the-art performance across six financial metrics, including essential support accuracy and market regime sensitivity. With consistent results across diverse market conditions, DeepSupp addresses critical gaps in SR level detection, offering a scalable and reliable solution for modern financial analysis. Our approach highlights the potential of attention-based architectures to uncover nuanced market patterns and improve technical trading strategies.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-fin.ST",
    "comment": "7 pages, 4 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2507.01971v1",
    "published_date": "2025-06-22 11:09:55 UTC",
    "updated_date": "2025-06-22 11:09:55 UTC"
  },
  {
    "arxiv_id": "2506.17968v1",
    "title": "h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective",
    "authors": [
      "Wenjian Huang",
      "Guiping Cao",
      "Jiahao Xia",
      "Jingkun Chen",
      "Hao Wang",
      "Jianguo Zhang"
    ],
    "abstract": "Deep neural networks have demonstrated remarkable performance across numerous learning tasks but often suffer from miscalibration, resulting in unreliable probability outputs. This has inspired many recent works on mitigating miscalibration, particularly through post-hoc recalibration methods that aim to obtain calibrated probabilities without sacrificing the classification performance of pre-trained models. In this study, we summarize and categorize previous works into three general strategies: intuitively designed methods, binning-based methods, and methods based on formulations of ideal calibration. Through theoretical and practical analysis, we highlight ten common limitations in previous approaches. To address these limitations, we propose a probabilistic learning framework for calibration called h-calibration, which theoretically constructs an equivalent learning formulation for canonical calibration with boundedness. On this basis, we design a simple yet effective post-hoc calibration algorithm. Our method not only overcomes the ten identified limitations but also achieves markedly better performance than traditional methods, as validated by extensive experiments. We further analyze, both theoretically and experimentally, the relationship and advantages of our learning objective compared to traditional proper scoring rule. In summary, our probabilistic framework derives an approximately equivalent differentiable objective for learning error-bounded calibrated probabilities, elucidating the correspondence and convergence properties of computational statistics with respect to theoretical bounds in canonical calibration. The theoretical effectiveness is verified on standard post-hoc calibration benchmarks by achieving state-of-the-art performance. This research offers valuable reference for learning reliable likelihood in related fields.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17968v1",
    "published_date": "2025-06-22 09:56:44 UTC",
    "updated_date": "2025-06-22 09:56:44 UTC"
  },
  {
    "arxiv_id": "2506.17967v2",
    "title": "Adapting Vision-Language Models for Evaluating World Models",
    "authors": [
      "Mariya Hendriksen",
      "Tabish Rashid",
      "David Bignell",
      "Raluca Georgescu",
      "Abdelhak Lemkhenter",
      "Katja Hofmann",
      "Sam Devlin",
      "Sarah Parisot"
    ],
    "abstract": "World models - generative models that simulate environment dynamics conditioned on past observations and actions - are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency - capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce an evaluation protocol targeting two recognition tasks - action recognition and character recognition - each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a VLM-based evaluator for video world model rollouts adapted under data and compute constraints. In our extensive experiments totaling over 5,154 GPU-days, we explore full, partial, and parameter-efficient adaptation methods across various task formats, context lengths, sampling methods, and data compositions. The resulting unified evaluator achieves parity with task-specific checkpoints. Human studies across seven diverse environments confirm strong alignment with human judgments, establishing UNIVERSE as a lightweight, adaptable, and semantics-aware evaluator for video world models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS LAW 2025 (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2506.17967v2",
    "published_date": "2025-06-22 09:53:28 UTC",
    "updated_date": "2025-11-24 12:37:43 UTC"
  },
  {
    "arxiv_id": "2506.17963v1",
    "title": "OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning",
    "authors": [
      "Zhiwei Nie",
      "Hongyu Zhang",
      "Hao Jiang",
      "Yutian Liu",
      "Xiansong Huang",
      "Fan Xu",
      "Jie Fu",
      "Zhixiang Ren",
      "Yonghong Tian",
      "Wen-Bin Zhang",
      "Jie Chen"
    ],
    "abstract": "Understanding and modeling enzyme-substrate interactions is crucial for catalytic mechanism research, enzyme engineering, and metabolic engineering. Although a large number of predictive methods have emerged, they do not incorporate prior knowledge of enzyme catalysis to rationally modulate general protein-molecule features that are misaligned with catalytic patterns. To address this issue, we introduce a two-stage progressive framework, OmniESI, for enzyme-substrate interaction prediction through conditional deep learning. By decomposing the modeling of enzyme-substrate interactions into a two-stage progressive process, OmniESI incorporates two conditional networks that respectively emphasize enzymatic reaction specificity and crucial catalysis-related interactions, facilitating a gradual feature modulation in the latent space from general protein-molecule domain to catalysis-aware domain. On top of this unified architecture, OmniESI can adapt to a variety of downstream tasks, including enzyme kinetic parameter prediction, enzyme-substrate pairing prediction, enzyme mutational effect prediction, and enzymatic active site annotation. Under the multi-perspective performance evaluation of in-distribution and out-of-distribution settings, OmniESI consistently delivered superior performance than state-of-the-art specialized methods across seven benchmarks. More importantly, the proposed conditional networks were shown to internalize the fundamental patterns of catalytic efficiency while significantly improving prediction performance, with only negligible parameter increases (0.16%), as demonstrated by ablation studies on key components. Overall, OmniESI represents a unified predictive approach for enzyme-substrate interactions, providing an effective tool for catalytic mechanism cracking and enzyme engineering with strong generalization and broad applicability.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17963v1",
    "published_date": "2025-06-22 09:40:40 UTC",
    "updated_date": "2025-06-22 09:40:40 UTC"
  },
  {
    "arxiv_id": "2506.17960v2",
    "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments",
    "authors": [
      "Jiaming Wang",
      "Diwen Liu",
      "Jizhuo Chen",
      "Jiaxuan Da",
      "Nuowen Qian",
      "Tram Minh Man",
      "Harold Soh"
    ],
    "abstract": "Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IEEE Robotics and Automation Letters (RAL), 2025. Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2506.17960v2",
    "published_date": "2025-06-22 09:36:05 UTC",
    "updated_date": "2025-10-18 11:13:51 UTC"
  },
  {
    "arxiv_id": "2506.17959v1",
    "title": "medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs",
    "authors": [
      "Lizzy Farrugia",
      "Lilian M. Azzopardi",
      "Jeremy Debattista",
      "Charlie Abela"
    ],
    "abstract": "The role of pharmacists is evolving from medicine dispensing to delivering comprehensive pharmaceutical services within multidisciplinary healthcare teams. Central to this shift is access to accurate, up-to-date medicinal product information supported by robust data integration. Leveraging artificial intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden relationships and enable data-driven decision-making. This paper presents medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and regulatory decisions. It forms the semantic layer of the broader medicX platform, powering predictive and explainable pharmacy services. medicX-KG integrates data from three sources, including, the British National Formulary (BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's regulatory landscape and combines European Medicines Agency alignment with partial UK supply dependence. The KG tackles the absence of a unified national drug repository, reducing pharmacists' reliance on fragmented sources. Its design was informed by interviews with practicing pharmacists to ensure real-world applicability. We detail the KG's construction, including data extraction, ontology design, and semantic mapping. Evaluation demonstrates that medicX-KG effectively supports queries about drug availability, interactions, adverse reactions, and therapeutic classes. Limitations, including missing detailed dosage encoding and real-time updates, are discussed alongside directions for future enhancements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17959v1",
    "published_date": "2025-06-22 09:28:48 UTC",
    "updated_date": "2025-06-22 09:28:48 UTC"
  },
  {
    "arxiv_id": "2506.17949v1",
    "title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation",
    "authors": [
      "Hong Su"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and extending patterns observed during pretraining but often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process. We propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM. This model leverages structural redundancy across stages to improve the applicability of novel ideas. Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17949v1",
    "published_date": "2025-06-22 09:02:31 UTC",
    "updated_date": "2025-06-22 09:02:31 UTC"
  },
  {
    "arxiv_id": "2506.17941v1",
    "title": "Greedy Selection under Independent Increments: A Toy Model Analysis",
    "authors": [
      "Huitao Yang"
    ],
    "abstract": "We study an iterative selection problem over N i.i.d. discrete-time stochastic processes with independent increments. At each stage, a fixed number of processes are retained based on their observed values. Under this simple model, we prove that the optimal strategy for selecting the final maximum-value process is to apply greedy selection at each stage. While the result relies on strong independence assumptions, it offers a clean justification for greedy heuristics in multi-stage elimination settings and may serve as a toy example for understanding related algorithms in high-dimensional applications.",
    "categories": [
      "math.PR",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "math.PR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17941v1",
    "published_date": "2025-06-22 08:21:23 UTC",
    "updated_date": "2025-06-22 08:21:23 UTC"
  },
  {
    "arxiv_id": "2507.02898v1",
    "title": "Particle Swarm Optimization for Quantum Circuit Synthesis: Performance Analysis and Insights",
    "authors": [
      "Mirza Hizriyan Nubli Hidayat",
      "Tan Chye Cheah"
    ],
    "abstract": "This paper discusses how particle swarm optimization (PSO) can be used to generate quantum circuits to solve an instance of the MaxOne problem. It then analyzes previous studies on evolutionary algorithms for circuit synthesis. With a brief introduction to PSO, including its parameters and algorithm flow, the paper focuses on a method of quantum circuit encoding and representation as PSO parameters. The fitness evaluation used in this paper is the MaxOne problem. The paper presents experimental results that compare different learning abilities and inertia weight variations in the PSO algorithm. A comparison is further made between the PSO algorithm and a genetic algorithm for quantum circuit synthesis. The results suggest PSO converges more quickly to the optimal solution.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02898v1",
    "published_date": "2025-06-22 08:21:22 UTC",
    "updated_date": "2025-06-22 08:21:22 UTC"
  },
  {
    "arxiv_id": "2506.17940v2",
    "title": "An entropy-optimal path to humble AI",
    "authors": [
      "Davide Bassetti",
      "Lukáš Pospíšil",
      "Michael Groom",
      "Terence J. O'Kane",
      "Illia Horenko"
    ],
    "abstract": "Progress of AI has led to very successful, but by no means humble models and tools, especially regarding (i) the huge and further exploding costs and resources they demand, and (ii) the over-confidence of these tools with the answers they provide. Here we introduce a novel mathematical framework for a non-equilibrium entropy-optimizing reformulation of Boltzmann machines based on the exact law of total probability and the exact convex polytope representations. We show that it results in the highly-performant, but much cheaper, gradient-descent-free learning framework with mathematically-justified existence and uniqueness criteria, and cheaply-computable confidence/reliability measures for both the model inputs and the outputs. Comparisons to state-of-the-art AI tools in terms of performance, cost and the model descriptor lengths on a broad set of synthetic and real-world problems with varying complexity reveal that the proposed method results in more performant and slim models, with the descriptor lengths being very close to the intrinsic complexity scaling bounds for the underlying problems. Applying this framework to historical climate data results in models with systematically higher prediction skills for the onsets of important La Niña and El Niño climate phenomena, requiring just few years of climate data for training - a small fraction of what is necessary for contemporary climate prediction tools.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17940v2",
    "published_date": "2025-06-22 08:13:22 UTC",
    "updated_date": "2025-09-25 09:18:04 UTC"
  },
  {
    "arxiv_id": "2506.17939v2",
    "title": "GEMeX-RMCoT: An Enhanced Med-VQA Dataset for Region-Aware Multimodal Chain-of-Thought Reasoning",
    "authors": [
      "Bo Liu",
      "Xiangyu Zhao",
      "Along He",
      "Yidi Chen",
      "Huazhu Fu",
      "Xiao-Ming Wu"
    ],
    "abstract": "Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model outputs. To address these limitations, this work first proposes a Region-Aware Multimodal Chain-of-Thought (RMCoT) dataset, in which the process of producing an answer is preceded by a sequence of intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://www.med-vqa.com/GEMeX/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ACM MM 2025 (also known as GEMeX-ThinkVG)",
    "pdf_url": "https://arxiv.org/pdf/2506.17939v2",
    "published_date": "2025-06-22 08:09:58 UTC",
    "updated_date": "2025-10-28 06:37:24 UTC"
  },
  {
    "arxiv_id": "2506.17937v1",
    "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering",
    "authors": [
      "Tommi Mikkonen",
      "Antero Taivalsaari"
    ],
    "abstract": "Software development is currently under a paradigm shift in which artificial intelligence and generative software reuse are taking the center stage in software creation. Consequently, earlier software reuse practices and methods are rapidly being replaced by AI-assisted approaches in which developers place their trust on code that has been generated by artificial intelligence. This is leading to a new form of software reuse that is conceptually not all that different from cargo cult development. In this paper we discuss the implications of AI-assisted generative software reuse in the context of emerging \"AI native\" software engineering, bring forth relevant questions, and define a tentative research agenda and call to action for tackling some of the central issues associated with this approach.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17937v1",
    "published_date": "2025-06-22 08:09:25 UTC",
    "updated_date": "2025-06-22 08:09:25 UTC"
  },
  {
    "arxiv_id": "2506.17936v2",
    "title": "When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?",
    "authors": [
      "Romy Müller"
    ],
    "abstract": "Concept-based explainable artificial intelligence (C-XAI) can let people see which representations an AI model has learned. This is particularly important when high-level semantic information (e.g., actions and relations) is used to make decisions about abstract categories (e.g., danger). In such tasks, AI models need to generalise beyond situation-specific details, and this ability can be reflected in C-XAI outputs that randomise over irrelevant features. However, it is unclear whether people appreciate such generalisation and can distinguish it from other, less desirable forms of imprecision in C-XAI outputs. Therefore, the present study investigated how the generality and relevance of C-XAI outputs affect people's evaluation of AI. In an experimental railway safety evaluation scenario, participants rated the performance of a simulated AI that classified traffic scenes involving people as dangerous or not. These classification decisions were explained via concepts in the form of similar image snippets. The latter differed in their match with the classified image, either regarding a highly relevant feature (i.e., people's relation to tracks) or a less relevant feature (i.e., people's action). Contrary to the hypotheses, concepts that generalised over less relevant features were rated lower than concepts that matched the classified image precisely. Moreover, their ratings were no better than those for systematic misrepresentations of the less relevant feature. Conversely, participants were highly sensitive to imprecisions in relevant features. These findings cast doubts on the assumption that people can easily infer from C-XAI outputs whether AI models have gained a deeper understanding of complex situations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17936v2",
    "published_date": "2025-06-22 08:07:02 UTC",
    "updated_date": "2025-11-20 14:26:56 UTC"
  },
  {
    "arxiv_id": "2506.17934v1",
    "title": "A GenAI System for Improved FAIR Independent Biological Database Integration",
    "authors": [
      "Syed N. Sakib",
      "Kallol Naha",
      "Sajratul Y. Rubaiat",
      "Hasan M. Jamil"
    ],
    "abstract": "Life sciences research increasingly requires identifying, accessing, and effectively processing data from an ever-evolving array of information sources on the Linked Open Data (LOD) network. This dynamic landscape places a significant burden on researchers, as the quality of query responses depends heavily on the selection and semantic integration of data sources --processes that are often labor-intensive, error-prone, and costly. While the adoption of FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has aimed to address these challenges, barriers to efficient and accurate scientific data processing persist.\n  In this paper, we introduce FAIRBridge, an experimental natural language-based query processing system designed to empower scientists to discover, access, and query biological databases, even when they are not FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query intents, map them to relevant databases described in scientific literature, and generate executable queries via intelligent resource access plans. The system also includes robust tools for mitigating low-quality query processing, ensuring high fidelity and responsiveness in the information delivered.\n  FAIRBridge's autonomous query processing framework enables users to explore alternative data sources, make informed choices at every step, and leverage community-driven crowd curation when needed. By providing a user-friendly, automated hypothesis-testing platform in natural English, FAIRBridge significantly enhances the integration and processing of scientific data, offering researchers a powerful new tool for advancing their inquiries.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17934v1",
    "published_date": "2025-06-22 08:04:24 UTC",
    "updated_date": "2025-06-22 08:04:24 UTC"
  },
  {
    "arxiv_id": "2506.22479v1",
    "title": "Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate",
    "authors": [
      "Krisanu Sarkar"
    ],
    "abstract": "We introduce Hindsight-Guided Momentum (HGM), a first-order optimization algorithm that adaptively scales learning rates based on the directional consistency of recent updates. Traditional adaptive methods, such as Adam or RMSprop , adapt learning dynamics using only the magnitude of gradients, often overlooking important geometric cues.Geometric cues refer to directional information, such as the alignment between current gradients and past updates, which reflects the local curvature and consistency of the optimization path. HGM addresses this by incorporating a hindsight mechanism that evaluates the cosine similarity between the current gradient and accumulated momentum. This allows it to distinguish between coherent and conflicting gradient directions, increasing the learning rate when updates align and reducing it in regions of oscillation or noise. The result is a more responsive optimizer that accelerates convergence in smooth regions of the loss surface while maintaining stability in sharper or more erratic areas. Despite this added adaptability, the method preserves the computational and memory efficiency of existing optimizers.By more intelligently responding to the structure of the optimization landscape, HGM provides a simple yet effective improvement over existing approaches, particularly in non-convex settings like that of deep neural network training.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22479v1",
    "published_date": "2025-06-22 08:02:19 UTC",
    "updated_date": "2025-06-22 08:02:19 UTC"
  },
  {
    "arxiv_id": "2506.17931v1",
    "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset",
    "authors": [
      "Ravi Kant Gupta",
      "Shounak Das",
      "Amit Sethi"
    ],
    "abstract": "We present a novel approach for unsupervised domain adaptation (UDA) for natural images. A commonly-used objective for UDA schemes is to enhance domain alignment in representation space even if there is a domain shift in the input space. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions associated with classification problems. Our approach has two main features. Firstly, its neural architecture uses the deep structure of ResNet and the effective separation of scales of feature pyramidal network (FPN) to work with both content and style features. Secondly, it uses a combination of a novel loss function and judiciously selected existing loss functions to train the network architecture. This tailored combination is designed to address challenges inherent to natural images, such as scale, noise, and style shifts, that occur on top of a multi-modal (multi-class) distribution. The combined loss function not only enhances model accuracy and robustness on the target domain but also speeds up training convergence. Our proposed UDA scheme generalizes better than state-of-the-art for CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets and comaparable for DomainNet dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in ICPR'24 (International Conference on Pattern Recognition)",
    "pdf_url": "https://arxiv.org/pdf/2506.17931v1",
    "published_date": "2025-06-22 07:56:10 UTC",
    "updated_date": "2025-06-22 07:56:10 UTC"
  },
  {
    "arxiv_id": "2506.17930v1",
    "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
    "authors": [
      "Jianyu Wang",
      "Zhiqiang Hu",
      "Lidong Bing"
    ],
    "abstract": "We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent \"gibberish\" can remarkably improve performance across diverse tasks. Notably, the \"gibberish\" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2025, and Code will be released at: https://github.com/jianyu-cs/PromptQuine/",
    "pdf_url": "https://arxiv.org/pdf/2506.17930v1",
    "published_date": "2025-06-22 07:53:07 UTC",
    "updated_date": "2025-06-22 07:53:07 UTC"
  },
  {
    "arxiv_id": "2506.17929v1",
    "title": "ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation",
    "authors": [
      "Shulun Chen",
      "Wei Shao",
      "Flora D. Salim",
      "Hao Xue"
    ],
    "abstract": "Supporting decision-making has long been a central vision in the field of spatio-temporal intelligence. While prior work has improved the timeliness and accuracy of spatio-temporal forecasting, converting these forecasts into actionable strategies remains a key challenge. A main limitation is the decoupling of the prediction and the downstream decision phases, which can significantly degrade the downstream efficiency. For example, in emergency response, the priority is successful resource allocation and intervention, not just incident prediction. To this end, it is essential to propose an Adaptive Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting paradigm from event anticipation to actionable decision support. This framework ensures that information is directly used for decision-making, thereby maximizing overall effectiveness. Specifically, ASTER introduces a new Resource-aware Spatio-Temporal interaction module (RaST) that adaptively captures long- and short-term dependencies under dynamic resource conditions, producing context-aware spatiotemporal representations. To directly generate actionable decisions, we further design a Preference-oriented decision agent (Poda) based on multi-objective reinforcement learning, which transforms predictive signals into resource-efficient intervention strategies by deriving optimal actions under specific preferences and dynamic constraints. Experimental results on four benchmark datasets demonstrate the state-of-the-art performance of ASTER in improving both early prediction accuracy and resource allocation outcomes across six downstream metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation",
    "pdf_url": "https://arxiv.org/pdf/2506.17929v1",
    "published_date": "2025-06-22 07:49:37 UTC",
    "updated_date": "2025-06-22 07:49:37 UTC"
  },
  {
    "arxiv_id": "2506.17919v1",
    "title": "Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding",
    "authors": [
      "Zhiyu Mou",
      "Miao Xu",
      "Wei Chen",
      "Rongquan Bai",
      "Chuan Yu",
      "Jian Xu"
    ],
    "abstract": "Reinforcement learning (RL) for auto-bidding has shifted from using simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are limited by the dataset's state space coverage, offering modest gains. While SRLB expands state coverage, its simulator-reality gap risks misleading policies. This paper introduces Model-based RL Bidding (MRLB), which learns an environment model from real data to bridge this gap. MRLB trains policies using both real and model-generated data, expanding state coverage beyond ORLB. To ensure model reliability, we propose: 1) A permutation equivariant model architecture for better generalization, and 2) A robust offline Q-learning method that pessimistically penalizes model errors. These form the Permutation Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments show that PE-MORL outperforms state-of-the-art auto-bidding methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17919v1",
    "published_date": "2025-06-22 06:58:36 UTC",
    "updated_date": "2025-06-22 06:58:36 UTC"
  },
  {
    "arxiv_id": "2506.17913v1",
    "title": "Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents",
    "authors": [
      "Jinjie Wei",
      "Jiyao Liu",
      "Lihao Liu",
      "Ming Hu",
      "Junzhi Ning",
      "Mingcheng Li",
      "Weijie Yin",
      "Junjun He",
      "Xiao Liang",
      "Chao Feng",
      "Dingkang Yang"
    ],
    "abstract": "Graphical User Interface (GUI) agents have made significant progress in automating digital tasks through the utilization of computer vision and language models. Nevertheless, existing agent systems encounter notable limitations. Firstly, they predominantly depend on trial and error decision making rather than progressive reasoning, thereby lacking the capability to learn and adapt from interactive encounters. Secondly, these systems are assessed using overly simplistic single step accuracy metrics, which do not adequately reflect the intricate nature of real world GUI interactions. In this paper, we present CogniGUI, a cognitive framework developed to overcome these limitations by enabling adaptive learning for GUI automation resembling human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach combines two main components: (1) an omni parser engine that conducts immediate hierarchical parsing of GUI elements through quick visual semantic analysis to identify actionable components, and (2) a Group based Relative Policy Optimization (GRPO) grounding agent that assesses multiple interaction paths using a unique relative reward system, promoting minimal and efficient operational routes. This dual-system design facilitates iterative ''exploration learning mastery'' cycles, enabling the agent to enhance its strategies over time based on accumulated experience. Moreover, to assess the generalization and adaptability of agent systems, we introduce ScreenSeek, a comprehensive benchmark that includes multi application navigation, dynamic state transitions, and cross interface coherence, which are often overlooked challenges in current benchmarks. Experimental results demonstrate that CogniGUI surpasses state-of-the-art methods in both the current GUI grounding benchmarks and our newly proposed benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17913v1",
    "published_date": "2025-06-22 06:30:52 UTC",
    "updated_date": "2025-06-22 06:30:52 UTC"
  },
  {
    "arxiv_id": "2506.17910v1",
    "title": "Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis",
    "authors": [
      "Mohamed Benkedadra",
      "Matei Mancas",
      "Sidi Ahmed Mahmoudi"
    ],
    "abstract": "2D cameras are often used in interactive systems. Other systems like gaming consoles provide more powerful 3D cameras for short range depth sensing. Overall, these cameras are not reliable in large, complex environments. In this work, we propose a 3D stereo vision based pipeline for interactive systems, that is able to handle both ordinary and sensitive applications, through robust scene understanding. We explore the fusion of multiple 3D cameras to do full scene reconstruction, which allows for preforming a wide range of tasks, like event recognition, subject tracking, and notification. Using possible feedback approaches, the system can receive data from the subjects present in the environment, to learn to make better decisions, or to adapt to completely new environments. Throughout the paper, we introduce the pipeline and explain our preliminary experimentation and results. Finally, we draw the roadmap for the next steps that need to be taken, in order to get this pipeline into production",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17910v1",
    "published_date": "2025-06-22 06:19:44 UTC",
    "updated_date": "2025-06-22 06:19:44 UTC"
  },
  {
    "arxiv_id": "2507.01037v2",
    "title": "Learning to Segment for Vehicle Routing Problems",
    "authors": [
      "Wenbin Ouyang",
      "Sirui Li",
      "Yining Ma",
      "Cathy Wu"
    ],
    "abstract": "Iterative heuristics are widely recognized as state-of-the-art for Vehicle Routing Problems (VRPs). In this work, we exploit a critical observation: a large portion of the solution remains stable, i.e., unchanged across search iterations, causing redundant computations, especially for large-scale VRPs with long subtours. To address this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA) decomposition technique to accelerate iterative solvers. FSTA preserves stable solution segments during the search, aggregates nodes within each segment into fixed hypernodes, and focuses the search only on unstable portions. Yet, a key challenge lies in identifying which segments should be aggregated. To this end, we introduce Learning-to-Segment (L2Seg), a novel neural framework to intelligently differentiate potentially stable and unstable portions for FSTA decomposition. We present three L2Seg variants: non-autoregressive (globally comprehensive but locally indiscriminate), autoregressive (locally refined but globally deficient), and their synergy. Empirical results on CVRP and VRPTW show that L2Seg accelerates state-of-the-art solvers by 2x to 7x. We further provide in-depth analysis showing why synergy achieves the best performance. Notably, L2Seg is compatible with traditional, learning-based, and hybrid solvers, while supporting various VRPs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01037v2",
    "published_date": "2025-06-22 05:38:15 UTC",
    "updated_date": "2025-09-26 20:09:55 UTC"
  },
  {
    "arxiv_id": "2506.17903v1",
    "title": "Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases",
    "authors": [
      "Huanjia Zhu",
      "Yishu Liu",
      "Xiaozhao Fang",
      "Guangming Lu",
      "Bingzhi Chen"
    ],
    "abstract": "Existing Medical Visual Question Answering (Med-VQA) models often suffer from language biases, where spurious correlations between question types and answer categories are inadvertently established. To address these issues, we propose a novel Cause-Effect Driven Optimization framework called CEDO, that incorporates three well-established mechanisms, i.e., Modality-driven Heterogeneous Optimization (MHO), Gradient-guided Modality Synergy (GMS), and Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating language biases from both causal and effectual perspectives. Specifically, MHO employs adaptive learning rates for specific modalities to achieve heterogeneous optimization, thus enhancing robust reasoning capabilities. Additionally, GMS leverages the Pareto optimization method to foster synergistic interactions between modalities and enforce gradient orthogonality to eliminate bias updates, thereby mitigating language biases from the effect side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive weights to individual losses to ensure balanced learning across all answer categories, effectively alleviating language biases from the cause side, i.e., imbalance biases within datasets. Extensive experiments on multiple traditional and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO over state-of-the-art competitors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17903v1",
    "published_date": "2025-06-22 05:20:34 UTC",
    "updated_date": "2025-06-22 05:20:34 UTC"
  },
  {
    "arxiv_id": "2506.19871v1",
    "title": "An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network",
    "authors": [
      "Yining Pang",
      "Chenghan Li"
    ],
    "abstract": "Insurance fraud detection represents a pivotal advancement in modern insurance service, providing intelligent and digitalized monitoring to enhance management and prevent fraud. It is crucial for ensuring the security and efficiency of insurance systems. Although AI and machine learning algorithms have demonstrated strong performance in detecting fraudulent claims, the absence of standardized defense mechanisms renders current systems vulnerable to emerging adversarial threats. In this paper, we propose a GAN-based approach to conduct adversarial attacks on fraud detection systems. Our results indicate that an attacker, without knowledge of the training data or internal model details, can generate fraudulent cases that are classified as legitimate with a 99\\% attack success rate (ASR). By subtly modifying real insurance records and claims, adversaries can significantly increase the fraud risk, potentially bypassing compromised detection systems. These findings underscore the urgent need to enhance the robustness of insurance fraud detection models against adversarial manipulation, thereby ensuring the stability and reliability of different insurance systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "arXiv admin note: text overlap with arXiv:2405.12076 by other authors",
    "pdf_url": "https://arxiv.org/pdf/2506.19871v1",
    "published_date": "2025-06-22 05:02:45 UTC",
    "updated_date": "2025-06-22 05:02:45 UTC"
  },
  {
    "arxiv_id": "2506.17900v1",
    "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms",
    "authors": [
      "Cheng Ji",
      "Huaiying Luo"
    ],
    "abstract": "With the increasing complexity and rapid expansion of the scale of AI systems in cloud platforms, the log data generated during system operation is massive, unstructured, and semantically ambiguous, which brings great challenges to fault location and system self-repair. In order to solve this problem, this paper proposes an intelligent log processing and automatic debugging framework based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This method is extended on the basis of the existing pre-trained Transformer model, and integrates a multi-stage semantic inference mechanism to realize the context understanding of system logs and the automatic reconstruction of fault chains. Firstly, the system log is dynamically structured, and the unsupervised clustering and embedding mechanism is used to extract the event template and semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round attention mechanism to perform contextual reasoning on the log sequence to generate potential fault assumptions and root cause paths. Furthermore, this paper introduces a reinforcement learning-based policy-guided recovery planner, which is driven by the remediation strategy generated by LLM to support dynamic decision-making and adaptive debugging in the cloud environment. Compared with the existing rule engine or traditional log analysis system, the proposed model has stronger semantic understanding ability, continuous learning ability and heterogeneous environment adaptability. Experiments on the cloud platform log dataset show that LLM-ID improves the fault location accuracy by 16.2%, which is significantly better than the current mainstream methods",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by 2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.17900v1",
    "published_date": "2025-06-22 04:58:37 UTC",
    "updated_date": "2025-06-22 04:58:37 UTC"
  },
  {
    "arxiv_id": "2506.17896v1",
    "title": "EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations",
    "authors": [
      "Junho Park",
      "Andrew Sangwoo Ye",
      "Taein Kwon"
    ],
    "abstract": "Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference. To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images. Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. Moreover, EgoWorld shows promising results even on unlabeled real-world examples.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://redorangeyellowy.github.io/EgoWorld/",
    "pdf_url": "https://arxiv.org/pdf/2506.17896v1",
    "published_date": "2025-06-22 04:21:48 UTC",
    "updated_date": "2025-06-22 04:21:48 UTC"
  },
  {
    "arxiv_id": "2506.17881v2",
    "title": "GRAF: Multi-turn Jailbreaking via Global Refinement and Active Fabrication",
    "authors": [
      "Hua Tang",
      "Lingyong Yan",
      "Yukun Zhao",
      "Shuaiqiang Wang",
      "Jizhou Huang",
      "Dawei Yin"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks. Nevertheless, they still pose notable safety risks due to potential misuse for malicious purposes. Jailbreaking, which seeks to induce models to generate harmful content through single-turn or multi-turn attacks, plays a crucial role in uncovering underlying security vulnerabilities. However, prior methods, including sophisticated multi-turn approaches, often struggle to adapt to the evolving dynamics of dialogue as interactions progress. To address this challenge, we propose \\ours (JailBreaking via \\textbf{G}lobally \\textbf{R}efining and \\textbf{A}daptively \\textbf{F}abricating), a novel multi-turn jailbreaking method that globally refines the attack trajectory at each interaction. In addition, we actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent queries. Extensive experiments across six state-of-the-art LLMs demonstrate the superior effectiveness of our approach compared to existing single-turn and multi-turn jailbreaking methods. Our code will be released at https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17881v2",
    "published_date": "2025-06-22 03:15:05 UTC",
    "updated_date": "2025-09-29 08:39:26 UTC"
  },
  {
    "arxiv_id": "2506.17879v1",
    "title": "StainPIDR: A Pathological Image Decouplingand Reconstruction Method for Stain Normalization Based on Color Vector Quantization and Structure Restaining",
    "authors": [
      "Zheng Chen"
    ],
    "abstract": "The color appearance of a pathological image is highly related to the imaging protocols, the proportion of different dyes, and the scanning devices. Computer-aided diagnostic systems may deteriorate when facing these color-variant pathological images. In this work, we propose a stain normalization method called StainPIDR. We try to eliminate this color discrepancy by decoupling the image into structure features and vector-quantized color features, restaining the structure features with the target color features, and decoding the stained structure features to normalized pathological images. We assume that color features decoupled by different images with the same color should be exactly the same. Under this assumption, we train a fixed color vector codebook to which the decoupled color features will map. In the restaining part, we utilize the cross-attention mechanism to efficiently stain the structure features. As the target color (decoupled from a selected template image) will also affect the performance of stain normalization, we further design a template image selection algorithm to select a template from a given dataset. In our extensive experiments, we validate the effectiveness of StainPIDR and the template image selection algorithm. All the results show that our method can perform well in the stain normalization task. The code of StainPIDR will be publicly available later.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17879v1",
    "published_date": "2025-06-22 02:54:20 UTC",
    "updated_date": "2025-06-22 02:54:20 UTC"
  },
  {
    "arxiv_id": "2506.17878v1",
    "title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval",
    "authors": [
      "Tam Trinh",
      "Manh Nguyen",
      "Truong-Son Hy"
    ],
    "abstract": "The rapid spread of misinformation in the digital era poses significant challenges to public discourse, necessitating robust and scalable fact-checking solutions. Traditional human-led fact-checking methods, while credible, struggle with the volume and velocity of online content, prompting the integration of automated systems powered by Large Language Models (LLMs). However, existing automated approaches often face limitations, such as handling complex claims, ensuring source credibility, and maintaining transparency. This paper proposes a novel multi-agent system for automated fact-checking that enhances accuracy, efficiency, and explainability. The system comprises four specialized agents: an Input Ingestion Agent for claim decomposition, a Query Generation Agent for formulating targeted subqueries, an Evidence Retrieval Agent for sourcing credible evidence, and a Verdict Prediction Agent for synthesizing veracity judgments with human-interpretable explanations. Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system achieves a 12.3% improvement in Macro F1-score over baseline methods. The system effectively decomposes complex claims, retrieves reliable evidence from trusted sources, and generates transparent explanations for verification decisions. Our approach contributes to the growing field of automated fact-checking by providing a more accurate, efficient, and transparent verification methodology that aligns with human fact-checking practices while maintaining scalability for real-world applications. Our source code is available at https://github.com/HySonLab/FactAgent",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17878v1",
    "published_date": "2025-06-22 02:39:27 UTC",
    "updated_date": "2025-06-22 02:39:27 UTC"
  },
  {
    "arxiv_id": "2506.17873v2",
    "title": "SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model",
    "authors": [
      "Guankun Wang",
      "Junyi Wang",
      "Wenjin Mo",
      "Long Bai",
      "Kun Yuan",
      "Ming Hu",
      "Jinlin Wu",
      "Junjun He",
      "Yiming Huang",
      "Nicolas Padoy",
      "Zhen Lei",
      "Hongbin Liu",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "abstract": "Surgical scene understanding is critical for surgical training and robotic decision-making in robot-assisted surgery. Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated great potential for advancing scene perception in the medical domain, facilitating surgeons to understand surgical scenes and procedures. However, these methods are primarily oriented towards image-based analysis or global video understanding, overlooking the fine-grained video reasoning that is crucial for analyzing specific processes and capturing detailed task execution within a surgical procedure. To bridge this gap, we propose SurgVidLM, the first video language model designed to address both full and fine-grained surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K that is a large-scale dataset with over 31K video-instruction pairs, enabling both holistic understanding and detailed analysis of surgical procedures. Building on this resource, SurgVidLM incorporates a two-stage StageFocus mechanism: the first stage extracts global procedural context, while the second stage performs high-frequency local analysis guided by temporal cues. We also develop the Multi-frequency Fusion Attention to effectively integrate low- and high-frequency visual tokens, ensuring the preservation of critical task-specific details. Experimental results demonstrate that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs of comparable parameter scale in both full and fine-grained video understanding tasks, showcasing its superior capability in capturing the context of complex robot-assisted surgeries. Our code and dataset will be publicly accessible soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17873v2",
    "published_date": "2025-06-22 02:16:18 UTC",
    "updated_date": "2025-09-24 03:26:52 UTC"
  },
  {
    "arxiv_id": "2506.17871v2",
    "title": "LLM Probability Concentration: How Alignment Shrinks the Generative Horizon",
    "authors": [
      "Chenghao Yang",
      "Ari Holtzman"
    ],
    "abstract": "Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this consistency in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the *Branching Factor* (BF)--a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this consistency has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., ``Sure'') that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Codebase: https://github.com/yangalan123/LLMBranchingFactor. V2: Rewrite the theory part for a broader audience. Add experiments to verify the necessity of the AEP estimator. Generalize findings to multilingual tasks and Qwen models. Add discussions on practical implications, and on which alignment stage contributes most to BF reduction. Add ethical statements connecting pluralistic alignment",
    "pdf_url": "https://arxiv.org/pdf/2506.17871v2",
    "published_date": "2025-06-22 02:00:37 UTC",
    "updated_date": "2025-10-14 22:47:33 UTC"
  },
  {
    "arxiv_id": "2506.17870v1",
    "title": "NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN",
    "authors": [
      "Jianhang Xie",
      "Chuntao Ding",
      "Xiaqing Li",
      "Shenyuan Ren",
      "Yidong Li",
      "Zhichao Lu"
    ],
    "abstract": "Deploying quantized deep neural network (DNN) models with resource adaptation capabilities on ubiquitous Internet of Things (IoT) devices to provide high-quality AI services can leverage the benefits of compression and meet multi-scenario resource requirements. However, existing dynamic/mixed precision quantization requires retraining or special hardware, whereas post-training quantization (PTQ) has two limitations for resource adaptation: (i) The state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying multiple PTQ models with diverse bitwidths consumes large storage resources and switching overheads. To this end, this paper introduces a resource-friendly post-training integer-nesting quantization, i.e., NestQuant, for on-device quantized model switching on IoT devices. The proposed NestQuant incorporates the integer weight decomposition, which bit-wise splits quantized weights into higher-bit and lower-bit weights of integer data types. It also contains a decomposed weights nesting mechanism to optimize the higher-bit weights by adaptive rounding and nest them into the original quantized weights. In deployment, we can send and store only one NestQuant model and switch between the full-bit/part-bit model by paging in/out lower-bit weights to adapt to resource changes and reduce consumption. Experimental results on the ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve high performance in top-1 accuracy, and reduce in terms of data transmission, storage consumption, and switching overheads. In particular, the ResNet-101 with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and part-bit models, respectively, and reduce switching overheads by approximately 78.1% compared with diverse bitwidths PTQ models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE Transactions on Mobile Computing, accepted manuscript, DOI: 10.1109/TMC.2025.3582583; Code: https://github.com/jianhayes/NESTQUANT",
    "pdf_url": "https://arxiv.org/pdf/2506.17870v1",
    "published_date": "2025-06-22 01:53:22 UTC",
    "updated_date": "2025-06-22 01:53:22 UTC"
  },
  {
    "arxiv_id": "2506.18935v1",
    "title": "Which Consciousness Can Be Artificialized? Local Percept-Perceiver Phenomenon for the Existence of Machine Consciousness",
    "authors": [
      "Shri Lal Raghudev Ram Singh"
    ],
    "abstract": "This paper presents a novel paradigm of the local percept-perceiver phenomenon to formalize certain observations in neuroscientific theories of consciousness. Using this model, a set-theoretic formalism is developed for artificial systems, and the existence of machine consciousness is proved by invoking Zermelo-Fraenkel set theory. The article argues for the possibility of a reductionist form of epistemic consciousness within machines.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Paper accepted for the 18th Annual AGI Conference, AGI-2025, Reykjavik, Iceland, August 10-13, 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.18935v1",
    "published_date": "2025-06-22 01:53:14 UTC",
    "updated_date": "2025-06-22 01:53:14 UTC"
  },
  {
    "arxiv_id": "2506.22477v1",
    "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI",
    "authors": [
      "Huiwen Han"
    ],
    "abstract": "This paper introduces an innovative design for robotic operating platforms, underpinned by a transformative Internet of Things (IoT) architecture, seamlessly integrating cutting-edge technologies such as large language models (LLMs), generative AI, edge computing, and 5G networks. The proposed platform aims to elevate the intelligence and autonomy of IoT systems and robotics, enabling them to make real-time decisions and adapt dynamically to changing environments. Through a series of compelling case studies across industries including smart manufacturing, healthcare, and service sectors, this paper demonstrates the substantial potential of IoT-enabled robotics to optimize operational workflows, enhance productivity, and deliver innovative, scalable solutions. By emphasizing the roles of LLMs and generative AI, the research highlights how these technologies drive the evolution of intelligent robotics and IoT, shaping the future of industry-specific advancements. The findings not only showcase the transformative power of these technologies but also offer a forward-looking perspective on their broader societal and industrial implications, positioning them as catalysts for next-generation automation and technological convergence.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "cs.RO"
    ],
    "primary_category": "cs.NI",
    "comment": "Published in: 2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI: 10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE",
    "pdf_url": "https://arxiv.org/pdf/2506.22477v1",
    "published_date": "2025-06-22 00:12:20 UTC",
    "updated_date": "2025-06-22 00:12:20 UTC"
  }
]
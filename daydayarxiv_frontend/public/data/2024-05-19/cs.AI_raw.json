[
  {
    "arxiv_id": "2405.15805v1",
    "title": "DSAM: A Deep Learning Framework for Analyzing Temporal and Spatial Dynamics in Brain Networks",
    "authors": [
      "Bishal Thapaliya",
      "Robyn Miller",
      "Jiayu Chen",
      "Yu-Ping Wang",
      "Esra Akbas",
      "Ram Sapkota",
      "Bhaskar Ray",
      "Pranav Suresh",
      "Santosh Ghimire",
      "Vince Calhoun",
      "Jingyu Liu"
    ],
    "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) is a\nnoninvasive technique pivotal for understanding human neural mechanisms of\nintricate cognitive processes. Most rs-fMRI studies compute a single static\nfunctional connectivity matrix across brain regions of interest, or dynamic\nfunctional connectivity matrices with a sliding window approach. These\napproaches are at risk of oversimplifying brain dynamics and lack proper\nconsideration of the goal at hand. While deep learning has gained substantial\npopularity for modeling complex relational data, its application to uncovering\nthe spatiotemporal dynamics of the brain is still limited. We propose a novel\ninterpretable deep learning framework that learns goal-specific functional\nconnectivity matrix directly from time series and employs a specialized graph\nneural network for the final classification. Our model, DSAM, leverages\ntemporal causal convolutional networks to capture the temporal dynamics in both\nlow- and high-level feature representations, a temporal attention unit to\nidentify important time points, a self-attention unit to construct the\ngoal-specific connectivity matrix, and a novel variant of graph neural network\nto capture the spatial dynamics for downstream classification. To validate our\napproach, we conducted experiments on the Human Connectome Project dataset with\n1075 samples to build and interpret the model for the classification of sex\ngroup, and the Adolescent Brain Cognitive Development Dataset with 8520 samples\nfor independent testing. Compared our proposed framework with other\nstate-of-art models, results suggested this novel approach goes beyond the\nassumption of a fixed connectivity matrix and provides evidence of\ngoal-specific brain connectivity patterns, which opens up the potential to gain\ndeeper insights into how the human brain adapts its functional connectivity\nspecific to the task at hand.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "18 Pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.15805v1",
    "published_date": "2024-05-19 23:35:06 UTC",
    "updated_date": "2024-05-19 23:35:06 UTC"
  },
  {
    "arxiv_id": "2405.12250v1",
    "title": "Your Transformer is Secretly Linear",
    "authors": [
      "Anton Razzhigaev",
      "Matvey Mikhalchuk",
      "Elizaveta Goncharova",
      "Nikolai Gerasimenko",
      "Ivan Oseledets",
      "Denis Dimitrov",
      "Andrey Kuznetsov"
    ],
    "abstract": "This paper reveals a novel linear characteristic exclusive to transformer\ndecoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We\nanalyze embedding transformations between sequential layers, uncovering a\nnear-perfect linear relationship (Procrustes similarity score of 0.99).\nHowever, linearity decreases when the residual component is removed due to a\nconsistently low output norm of the transformer layer. Our experiments show\nthat removing or linearly approximating some of the most linear blocks of\ntransformers does not affect significantly the loss or model performance.\nMoreover, in our pretraining experiments on smaller models we introduce a\ncosine-similarity-based regularization, aimed at reducing layer linearity. This\nregularization improves performance metrics on benchmarks like Tiny Stories and\nSuperGLUE and as well successfully decreases the linearity of the models. This\nstudy challenges the existing understanding of transformer architectures,\nsuggesting that their operation may be more linear than previously assumed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.12250v1",
    "published_date": "2024-05-19 22:44:00 UTC",
    "updated_date": "2024-05-19 22:44:00 UTC"
  },
  {
    "arxiv_id": "2405.15804v1",
    "title": "Explainable Human-AI Interaction: A Planning Perspective",
    "authors": [
      "Sarath Sreedharan",
      "Anagha Kulkarni",
      "Subbarao Kambhampati"
    ],
    "abstract": "From its inception, AI has had a rather ambivalent relationship with humans\n-- swinging between their augmentation and replacement. Now, as AI technologies\nenter our everyday lives at an ever increasing pace, there is a greater need\nfor AI systems to work synergistically with humans. One critical requirement\nfor such synergistic human-AI interaction is that the AI systems be explainable\nto the humans in the loop. To do this effectively, AI agents need to go beyond\nplanning with their own models of the world, and take into account the mental\nmodel of the human in the loop. Drawing from several years of research in our\nlab, we will discuss how the AI agent can use these mental models to either\nconform to human expectations, or change those expectations through explanatory\ncommunication. While the main focus of the book is on cooperative scenarios, we\nwill point out how the same mental models can be used for obfuscation and\ndeception. Although the book is primarily driven by our own research in these\nareas, in every chapter, we will provide ample connections to relevant research\nfrom other groups.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.15804v1",
    "published_date": "2024-05-19 22:22:21 UTC",
    "updated_date": "2024-05-19 22:22:21 UTC"
  },
  {
    "arxiv_id": "2405.11675v1",
    "title": "Deep Ensemble Art Style Recognition",
    "authors": [
      "Orfeas Menis-Mastromichalakis",
      "Natasa Sofou",
      "Giorgos Stamou"
    ],
    "abstract": "The massive digitization of artworks during the last decades created the need\nfor categorization, analysis, and management of huge amounts of data related to\nabstract concepts, highlighting a challenging problem in the field of computer\nscience. The rapid progress of artificial intelligence and neural networks has\nprovided tools and technologies that seem worthy of the challenge. Recognition\nof various art features in artworks has gained attention in the deep learning\nsociety. In this paper, we are concerned with the problem of art style\nrecognition using deep networks. We compare the performance of 8 different deep\narchitectures (VGG16, VGG19, ResNet50, ResNet152, Inception-V3, DenseNet121,\nDenseNet201 and Inception-ResNet-V2), on two different art datasets, including\n3 architectures that have never been used on this task before, leading to\nstate-of-the-art performance. We study the effect of data preprocessing prior\nto applying a deep learning model. We introduce a stacking ensemble method\ncombining the results of first-stage classifiers through a meta-classifier,\nwith the innovation of a versatile approach based on multiple models that\nextract and recognize different characteristics of the input, creating a more\nconsistent model compared to existing works and achieving state-of-the-art\naccuracy on the largest art dataset available (WikiArt - 68,55%). We also\ndiscuss the impact of the data and art styles themselves on the performance of\nour models forming a manifold perspective on the problem.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11675v1",
    "published_date": "2024-05-19 21:26:11 UTC",
    "updated_date": "2024-05-19 21:26:11 UTC"
  },
  {
    "arxiv_id": "2405.13053v3",
    "title": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models",
    "authors": [
      "Jingwei Xu",
      "Junyu Lai",
      "Yunpeng Huang"
    ],
    "abstract": "The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.13053v3",
    "published_date": "2024-05-19 20:46:07 UTC",
    "updated_date": "2024-10-09 15:33:10 UTC"
  },
  {
    "arxiv_id": "2405.13052v1",
    "title": "Large Language Models Can Infer Personality from Free-Form User Interactions",
    "authors": [
      "Heinrich Peters",
      "Moran Cerf",
      "Sandra C. Matz"
    ],
    "abstract": "This study investigates the capacity of Large Language Models (LLMs) to infer\nthe Big Five personality traits from free-form user interactions. The results\ndemonstrate that a chatbot powered by GPT-4 can infer personality with moderate\naccuracy, outperforming previous approaches drawing inferences from static text\ncontent. The accuracy of inferences varied across different conversational\nsettings. Performance was highest when the chatbot was prompted to elicit\npersonality-relevant information from users (mean r=.443, range=[.245, .640]),\nfollowed by a condition placing greater emphasis on naturalistic interaction\n(mean r=.218, range=[.066, .373]). Notably, the direct focus on personality\nassessment did not result in a less positive user experience, with participants\nreporting the interactions to be equally natural, pleasant, engaging, and\nhumanlike across both conditions. A chatbot mimicking ChatGPT's default\nbehavior of acting as a helpful assistant led to markedly inferior personality\ninferences and lower user experience ratings but still captured psychologically\nmeaningful information for some of the personality traits (mean r=.117,\nrange=[-.004, .209]). Preliminary analyses suggest that the accuracy of\npersonality inferences varies only marginally across different\nsocio-demographic subgroups. Our results highlight the potential of LLMs for\npsychological profiling based on conversational interactions. We discuss\npractical implications and ethical challenges associated with these findings.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13052v1",
    "published_date": "2024-05-19 20:33:36 UTC",
    "updated_date": "2024-05-19 20:33:36 UTC"
  },
  {
    "arxiv_id": "2405.11669v1",
    "title": "Do No Harm: A Counterfactual Approach to Safe Reinforcement Learning",
    "authors": [
      "Sean Vaskov",
      "Wilko Schwarting",
      "Chris L. Baker"
    ],
    "abstract": "Reinforcement Learning (RL) for control has become increasingly popular due\nto its ability to learn rich feedback policies that take into account\nuncertainty and complex representations of the environment. When considering\nsafety constraints, constrained optimization approaches, where agents are\npenalized for constraint violations, are commonly used. In such methods, if\nagents are initialized in, or must visit, states where constraint violation\nmight be inevitable, it is unclear how much they should be penalized. We\naddress this challenge by formulating a constraint on the counterfactual harm\nof the learned policy compared to a default, safe policy. In a philosophical\nsense this formulation only penalizes the learner for constraint violations\nthat it caused; in a practical sense it maintains feasibility of the optimal\ncontrol problem. We present simulation studies on a rover with uncertain road\nfriction and a tractor-trailer parking environment that demonstrate our\nconstraint formulation enables agents to learn safer policies than contemporary\nconstrained RL methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11669v1",
    "published_date": "2024-05-19 20:33:21 UTC",
    "updated_date": "2024-05-19 20:33:21 UTC"
  },
  {
    "arxiv_id": "2405.13051v1",
    "title": "Towards Contactless Elevators with TinyML using CNN-based Person Detection and Keyword Spotting",
    "authors": [
      "Anway S. Pimpalkar",
      "Deeplaxmi V. Niture"
    ],
    "abstract": "This study presents a proof of concept for a contactless elevator operation\nsystem aimed at minimizing human intervention while enhancing safety,\nintelligence, and efficiency. A microcontroller-based edge device executing\ntiny Machine Learning (tinyML) inferences is developed for elevator operation.\nUsing person detection and keyword spotting algorithms, the system offers\ncost-effective and robust units requiring minimal infrastructural changes. The\ndesign incorporates preprocessing steps and quantized convolutional neural\nnetworks in a multitenant framework to optimize accuracy and response time.\nResults show a person detection accuracy of 83.34% and keyword spotting\nefficacy of 80.5%, with an overall latency under 5 seconds, indicating\neffectiveness in real-world scenarios. Unlike current high-cost and\ninconsistent contactless technologies, this system leverages tinyML to provide\na cost-effective, reliable, and scalable solution, enhancing user safety and\noperational efficiency without significant infrastructural changes. The study\nhighlights promising results, though further exploration is needed for\nscalability and integration with existing systems. The demonstrated energy\nefficiency, simplicity, and safety benefits suggest that tinyML adoption could\nrevolutionize elevator systems, serving as a model for future technological\nadvancements. This technology could significantly impact public health and\nconvenience in multi-floor buildings by reducing physical contact and improving\noperational efficiency, particularly relevant in the context of pandemics or\nhygiene concerns.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13051v1",
    "published_date": "2024-05-19 20:24:31 UTC",
    "updated_date": "2024-05-19 20:24:31 UTC"
  },
  {
    "arxiv_id": "2405.11657v2",
    "title": "On the Expressivity of Recurrent Neural Cascades with Identity",
    "authors": [
      "Nadezda Alexandrovna Knorozova",
      "Alessandro Ronca"
    ],
    "abstract": "Recurrent Neural Cascades (RNC) are the class of recurrent neural networks\nwith no cyclic dependencies among recurrent neurons. Their subclass RNC+ with\npositive recurrent weights has been shown to be closely connected to the\nstar-free regular languages, which are the expressivity of many\nwell-established temporal logics. The existing expressivity results show that\nthe regular languages captured by RNC+ are the star-free ones, and they leave\nopen the possibility that RNC+ may capture languages beyond regular. We exclude\nthis possibility for languages that include an identity element, i.e., an input\nthat can occur an arbitrary number of times without affecting the output.\nNamely, in the presence of an identity element, we show that the languages\ncaptured by RNC+ are exactly the star-free regular languages. Identity elements\nare ubiquitous in temporal patterns, and hence our results apply to a large\nnumber of applications. The implications of our results go beyond expressivity.\nAt their core, we establish a close structural correspondence between RNC+ and\nsemiautomata cascades, showing that every neuron can be equivalently captured\nby a three-state semiautomaton. A notable consequence of this result is that\nRNC+ are no more succinct than cascades of three-state semiautomata.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.FL",
      "cs.LO",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Full version with appendix of a paper with the same title that will\n  appear in the proceedings of KR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11657v2",
    "published_date": "2024-05-19 20:06:38 UTC",
    "updated_date": "2024-09-09 09:37:28 UTC"
  },
  {
    "arxiv_id": "2405.11656v3",
    "title": "URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images",
    "authors": [
      "Zoey Chen",
      "Aaron Walsman",
      "Marius Memmel",
      "Kaichun Mo",
      "Alex Fang",
      "Karthikeya Vemuri",
      "Alan Wu",
      "Dieter Fox",
      "Abhishek Gupta"
    ],
    "abstract": "Constructing simulation scenes that are both visually and physically\nrealistic is a problem of practical interest in domains ranging from robotics\nto computer vision. This problem has become even more relevant as researchers\nwielding large data-hungry learning methods seek new sources of training data\nfor physical decision-making systems. However, building simulation models is\noften still done by hand. A graphic designer and a simulation engineer work\nwith predefined assets to construct rich scenes with realistic dynamic and\nkinematic properties. While this may scale to small numbers of scenes, to\nachieve the generalization properties that are required for data-driven robotic\ncontrol, we require a pipeline that is able to synthesize large numbers of\nrealistic scenes, complete with 'natural' kinematic and dynamic structures. To\nattack this problem, we develop models for inferring structure and generating\nsimulation scenes from natural images, allowing for scalable scene generation\nfrom web-scale datasets. To train these image-to-simulation models, we show how\ncontrollable text-to-image generative models can be used in generating paired\ntraining data that allows for modeling of the inverse problem, mapping from\nrealistic images back to complete scene models. We show how this paradigm\nallows us to build large datasets of scenes in simulation with semantic and\nphysical realism. We present an integrated end-to-end pipeline that generates\nsimulation scenes complete with articulated kinematic and dynamic structures\nfrom real-world images and use these for training robotic control policies. We\nthen robustly deploy in the real world for tasks like articulated object\nmanipulation. In doing so, our work provides both a pipeline for large-scale\ngeneration of simulation environments and an integrated system for training\nrobust robotic control policies in the resulting environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at RSS2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11656v3",
    "published_date": "2024-05-19 20:01:29 UTC",
    "updated_date": "2024-05-31 16:44:06 UTC"
  },
  {
    "arxiv_id": "2405.11655v2",
    "title": "Track Anything Rapter(TAR)",
    "authors": [
      "Tharun V. Puthanveettil",
      "Fnu Obaid ur Rahman"
    ],
    "abstract": "Object tracking is a fundamental task in computer vision with broad practical\napplications across various domains, including traffic monitoring, robotics,\nand autonomous vehicle tracking. In this project, we aim to develop a\nsophisticated aerial vehicle system known as Track Anything Rapter (TAR),\ndesigned to detect, segment, and track objects of interest based on\nuser-provided multimodal queries, such as text, images, and clicks. TAR\nutilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate\nthe relative pose of the queried object. The tracking problem is approached as\na Visual Servoing task, enabling the UAV to consistently focus on the object\nthrough advanced motion planning and control algorithms. We showcase how the\nintegration of these foundational models with a custom high-level control\nalgorithm results in a highly stable and precise tracking system deployed on a\ncustom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking\nalgorithm's performance, we compare it against Vicon-based ground truth.\nAdditionally, we evaluate the reliability of the foundational models in aiding\ntracking in scenarios involving occlusions. Finally, we test and validate the\nmodel's ability to work seamlessly with multiple modalities, such as click,\nbounding box, and image templates.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11655v2",
    "published_date": "2024-05-19 19:51:41 UTC",
    "updated_date": "2024-05-29 16:09:31 UTC"
  },
  {
    "arxiv_id": "2405.11647v3",
    "title": "Hummer: Towards Limited Competitive Preference Dataset",
    "authors": [
      "Li Jiang",
      "Yusen Wu",
      "Junwu Xiong",
      "Jingqing Ruan",
      "Yichuan Ding",
      "Qingpei Guo",
      "Zujie Wen",
      "Jun Zhou",
      "Xiaotie Deng"
    ],
    "abstract": "Preference datasets are essential for incorporating human preferences into\npre-trained language models, playing a key role in the success of Reinforcement\nLearning from Human Feedback. However, these datasets often demonstrate\nconflicting alignment objectives, leading to increased vulnerability to\njailbreak attacks and challenges in adapting downstream tasks to prioritize\nspecific alignment objectives without negatively impacting others. In this\nwork, we introduce a novel statistical metric, Alignment Dimension Conflict, to\nquantify the degree of conflict within preference datasets. We then present\n\\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative\npairwise preference datasets with reduced-conflict alignment objectives.\n\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback\nfrom GPT-4, marking as the first preference dataset aimed at reducing the\ncompetition between alignment objectives. Furthermore, we develop reward\nmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach to\nbalance diverse alignment objectives effectively. This sampling method\npositions HummerRM as an ideal model for domain-specific further fine-tuning\nand reducing vulnerabilities to attacks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11647v3",
    "published_date": "2024-05-19 18:57:25 UTC",
    "updated_date": "2024-08-06 14:12:26 UTC"
  },
  {
    "arxiv_id": "2405.11640v1",
    "title": "Inquire, Interact, and Integrate: A Proactive Agent Collaborative Framework for Zero-Shot Multimodal Medical Reasoning",
    "authors": [
      "Zishan Gu",
      "Fenglin Liu",
      "Changchang Yin",
      "Ping Zhang"
    ],
    "abstract": "The adoption of large language models (LLMs) in healthcare has attracted\nsignificant research interest. However, their performance in healthcare remains\nunder-investigated and potentially limited, due to i) they lack rich\ndomain-specific knowledge and medical reasoning skills; and ii) most\nstate-of-the-art LLMs are unimodal, text-only models that cannot directly\nprocess multimodal inputs. To this end, we propose a multimodal medical\ncollaborative reasoning framework \\textbf{MultiMedRes}, which incorporates a\nlearner agent to proactively gain essential information from domain-specific\nexpert models, to solve medical multimodal reasoning problems. Our method\nincludes three steps: i) \\textbf{Inquire}: The learner agent first decomposes\ngiven complex medical reasoning problems into multiple domain-specific\nsub-problems; ii) \\textbf{Interact}: The agent then interacts with\ndomain-specific expert models by repeating the ``ask-answer'' process to\nprogressively obtain different domain-specific knowledge; iii)\n\\textbf{Integrate}: The agent finally integrates all the acquired\ndomain-specific knowledge to accurately address the medical reasoning problem.\nWe validate the effectiveness of our method on the task of difference visual\nquestion answering for X-ray images. The experiments demonstrate that our\nzero-shot prediction achieves state-of-the-art performance, and even\noutperforms the fully supervised methods. Besides, our approach can be\nincorporated into various LLMs and multimodal LLMs to significantly boost their\nperformance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11640v1",
    "published_date": "2024-05-19 18:26:11 UTC",
    "updated_date": "2024-05-19 18:26:11 UTC"
  },
  {
    "arxiv_id": "2405.11629v1",
    "title": "Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems",
    "authors": [
      "Shengxiang Sun",
      "Shenzhe Zhu"
    ],
    "abstract": "Numerous studies on adversarial attacks targeting self-driving policies fail\nto incorporate realistic-looking adversarial objects, limiting real-world\napplicability. Building upon prior research that facilitated the transition of\nadversarial objects from simulations to practical applications, this paper\ndiscusses a modified gradient-based texture optimization method to discover\nrealistic-looking adversarial objects. While retaining the core architecture\nand techniques of the prior research, the proposed addition involves an entity\ntermed the 'Judge'. This agent assesses the texture of a rendered object,\nassigning a probability score reflecting its realism. This score is integrated\ninto the loss function to encourage the NeRF object renderer to concurrently\nlearn realistic and adversarial textures. The paper analyzes four strategies\nfor developing a robust 'Judge': 1) Leveraging cutting-edge vision-language\nmodels. 2) Fine-tuning open-sourced vision-language models. 3) Pretraining\nneurosymbolic systems. 4) Utilizing traditional image processing techniques.\nOur findings indicate that strategies 1) and 4) yield less reliable outcomes,\npointing towards strategies 2) or 3) as more promising directions for future\nresearch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11629v1",
    "published_date": "2024-05-19 17:42:24 UTC",
    "updated_date": "2024-05-19 17:42:24 UTC"
  },
  {
    "arxiv_id": "2405.11619v1",
    "title": "Novel Interpretable and Robust Web-based AI Platform for Phishing Email Detection",
    "authors": [
      "Abdulla Al-Subaiey",
      "Mohammed Al-Thani",
      "Naser Abdullah Alam",
      "Kaniz Fatema Antora",
      "Amith Khandakar",
      "SM Ashfaq Uz Zaman"
    ],
    "abstract": "Phishing emails continue to pose a significant threat, causing financial\nlosses and security breaches. This study addresses limitations in existing\nresearch, such as reliance on proprietary datasets and lack of real-world\napplication, by proposing a high-performance machine learning model for email\nclassification. Utilizing a comprehensive and largest available public dataset,\nthe model achieves a f1 score of 0.99 and is designed for deployment within\nrelevant applications. Additionally, Explainable AI (XAI) is integrated to\nenhance user trust. This research offers a practical and highly accurate\nsolution, contributing to the fight against phishing by empowering users with a\nreal-time web-based application for phishing email detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 7 figures, dataset link:\n  https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset/",
    "pdf_url": "http://arxiv.org/pdf/2405.11619v1",
    "published_date": "2024-05-19 17:18:27 UTC",
    "updated_date": "2024-05-19 17:18:27 UTC"
  },
  {
    "arxiv_id": "2405.11618v1",
    "title": "Transcriptomics-guided Slide Representation Learning in Computational Pathology",
    "authors": [
      "Guillaume Jaume",
      "Lukas Oldenburg",
      "Anurag Vaidya",
      "Richard J. Chen",
      "Drew F. K. Williamson",
      "Thomas Peeters",
      "Andrew H. Song",
      "Faisal Mahmood"
    ],
    "abstract": "Self-supervised learning (SSL) has been successful in building patch\nembeddings of small histology images (e.g., 224x224 pixels), but scaling these\nmodels to learn slide embeddings from the entirety of giga-pixel whole-slide\nimages (WSIs) remains challenging. Here, we leverage complementary information\nfrom gene expression profiles to guide slide representation learning using\nmultimodal pre-training. Expression profiles constitute highly detailed\nmolecular descriptions of a tissue that we hypothesize offer a strong\ntask-agnostic training signal for learning slide embeddings. Our slide and\nexpression (S+E) pre-training strategy, called Tangle, employs\nmodality-specific encoders, the outputs of which are aligned via contrastive\nlearning. Tangle was pre-trained on samples from three different organs: liver\n(n=6,597 S+E pairs), breast (n=1,020), and lung (n=1,012) from two different\nspecies (Homo sapiens and Rattus norvegicus). Across three independent test\ndatasets consisting of 1,265 breast WSIs, 1,946 lung WSIs, and 4,584 liver\nWSIs, Tangle shows significantly better few-shot performance compared to\nsupervised and SSL baselines. When assessed using prototype-based\nclassification and slide retrieval, Tangle also shows a substantial performance\nimprovement over all baselines. Code available at\nhttps://github.com/mahmoodlab/TANGLE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR'24, Oral",
    "pdf_url": "http://arxiv.org/pdf/2405.11618v1",
    "published_date": "2024-05-19 17:17:35 UTC",
    "updated_date": "2024-05-19 17:17:35 UTC"
  },
  {
    "arxiv_id": "2405.11612v2",
    "title": "Sociotechnical Implications of Generative Artificial Intelligence for Information Access",
    "authors": [
      "Bhaskar Mitra",
      "Henriette Cramer",
      "Olya Gurevich"
    ],
    "abstract": "Robust access to trustworthy information is a critical need for society with\nimplications for knowledge production, public health education, and promoting\ninformed citizenry in democratic societies. Generative AI technologies may\nenable new ways to access information and improve effectiveness of existing\ninformation retrieval systems but we are only starting to understand and\ngrapple with their long-term social implications. In this chapter, we present\nan overview of some of the systemic consequences and risks of employing\ngenerative AI in the context of information access. We also provide\nrecommendations for evaluation and mitigation, and discuss challenges for\nfuture research.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11612v2",
    "published_date": "2024-05-19 17:04:39 UTC",
    "updated_date": "2024-07-16 15:47:13 UTC"
  },
  {
    "arxiv_id": "2405.11598v1",
    "title": "AI-Assisted Diagnosis for Covid-19 CXR Screening: From Data Collection to Clinical Validation",
    "authors": [
      "Carlo Alberto Barbano",
      "Riccardo Renzulli",
      "Marco Grosso",
      "Domenico Basile",
      "Marco Busso",
      "Marco Grangetto"
    ],
    "abstract": "In this paper, we present the major results from the Covid Radiographic\nimaging System based on AI (Co.R.S.A.) project, which took place in Italy. This\nproject aims to develop a state-of-the-art AI-based system for diagnosing\nCovid-19 pneumonia from Chest X-ray (CXR) images. The contributions of this\nwork are manyfold: the release of the public CORDA dataset, a deep learning\npipeline for Covid-19 detection, and the clinical validation of the developed\nsolution by expert radiologists. The proposed detection model is based on a\ntwo-step approach that, paired with state-of-the-art debiasing, provides\nreliable results. Most importantly, our investigation includes the actual usage\nof the diagnosis aid tool by radiologists, allowing us to assess the real\nbenefits in terms of accuracy and time efficiency. Project homepage:\nhttps://corsa.di.unito.it/",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "68T07",
      "I.2.1; I.4.0"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at 21st IEEE International Symposium on Biomedical Imaging\n  (ISBI)",
    "pdf_url": "http://arxiv.org/pdf/2405.11598v1",
    "published_date": "2024-05-19 16:06:26 UTC",
    "updated_date": "2024-05-19 16:06:26 UTC"
  },
  {
    "arxiv_id": "2405.11597v1",
    "title": "Language Reconstruction with Brain Predictive Coding from fMRI Data",
    "authors": [
      "Congchi Yin",
      "Ziyi Ye",
      "Piji Li"
    ],
    "abstract": "Many recent studies have shown that the perception of speech can be decoded\nfrom brain signals and subsequently reconstructed as continuous language.\nHowever, there is a lack of neurological basis for how the semantic information\nembedded within brain signals can be used more effectively to guide language\nreconstruction. The theory of predictive coding suggests that human brain\nnaturally engages in continuously predicting future word representations that\nspan multiple timescales. This implies that the decoding of brain signals could\npotentially be associated with a predictable future. To explore the predictive\ncoding theory within the context of language reconstruction, this paper\nproposes a novel model \\textsc{PredFT} for jointly modeling neural decoding and\nbrain prediction. It consists of a main decoding network for language\nreconstruction and a side network for predictive coding. The side network\nobtains brain predictive coding representation from related brain regions of\ninterest with a multi-head self-attention module. This representation is fused\ninto the main decoding network with cross-attention to facilitate the language\nmodels' generation process. Experiments are conducted on the largest\nnaturalistic language comprehension fMRI dataset Narratives. \\textsc{PredFT}\nachieves current state-of-the-art decoding performance with a maximum BLEU-1\nscore of $27.8\\%$.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11597v1",
    "published_date": "2024-05-19 16:06:02 UTC",
    "updated_date": "2024-05-19 16:06:02 UTC"
  },
  {
    "arxiv_id": "2405.11591v1",
    "title": "Generative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation",
    "authors": [
      "Xinyi Lu",
      "Xu Wang"
    ],
    "abstract": "Evaluating the quality of automatically generated question items has been a\nlong standing challenge. In this paper, we leverage LLMs to simulate student\nprofiles and generate responses to multiple-choice questions (MCQs). The\ngenerative students' responses to MCQs can further support question item\nevaluation. We propose Generative Students, a prompt architecture designed\nbased on the KLI framework. A generative student profile is a function of the\nlist of knowledge components the student has mastered, has confusion about or\nhas no evidence of knowledge of. We instantiate the Generative Students concept\non the subject domain of heuristic evaluation. We created 45 generative\nstudents using GPT-4 and had them respond to 20 MCQs. We found that the\ngenerative students produced logical and believable responses that were aligned\nwith their profiles. We then compared the generative students' responses to\nreal students' responses on the same set of MCQs and found a high correlation.\nMoreover, there was considerable overlap in the difficult questions identified\nby generative students and real students. A subsequent case study demonstrated\nthat an instructor could improve question quality based on the signals provided\nby Generative Students.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in L@S'24: Proceedings of the Eleventh ACM Conference\n  on Learning @ Scale",
    "pdf_url": "http://arxiv.org/pdf/2405.11591v1",
    "published_date": "2024-05-19 15:53:18 UTC",
    "updated_date": "2024-05-19 15:53:18 UTC"
  },
  {
    "arxiv_id": "2405.11577v4",
    "title": "A Multi-Perspective Analysis of Memorization in Large Language Models",
    "authors": [
      "Bowen Chen",
      "Namgi Han",
      "Yusuke Miyao"
    ],
    "abstract": "Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11577v4",
    "published_date": "2024-05-19 15:00:50 UTC",
    "updated_date": "2024-06-04 15:28:20 UTC"
  },
  {
    "arxiv_id": "2405.11574v1",
    "title": "Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification",
    "authors": [
      "Manan Shah",
      "Yash Bhalgat"
    ],
    "abstract": "This report is a reproducibility study of the paper \"CDUL: CLIP-Driven\nUnsupervised Learning for Multi-Label Image Classification\" (Abdelfattah et al,\nICCV 2023). Our report makes the following contributions: (1) We provide a\nreproducible, well commented and open-sourced code implementation for the\nentire method specified in the original paper. (2) We try to verify the\neffectiveness of the novel aggregation strategy which uses the CLIP model to\ninitialize the pseudo labels for the subsequent unsupervised multi-label image\nclassification task. (3) We try to verify the effectiveness of the\ngradient-alignment training method specified in the original paper, which is\nused to update the network parameters and pseudo labels. The code can be found\nat https://github.com/cs-mshah/CDUL",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Reproducibility study",
    "pdf_url": "http://arxiv.org/pdf/2405.11574v1",
    "published_date": "2024-05-19 14:48:19 UTC",
    "updated_date": "2024-05-19 14:48:19 UTC"
  },
  {
    "arxiv_id": "2405.11559v1",
    "title": "DaVinci at SemEval-2024 Task 9: Few-shot prompting GPT-3.5 for Unconventional Reasoning",
    "authors": [
      "Suyash Vardhan Mathur",
      "Akshett Rai Jindal",
      "Manish Shrivastava"
    ],
    "abstract": "While significant work has been done in the field of NLP on vertical\nthinking, which involves primarily logical thinking, little work has been done\ntowards lateral thinking, which involves looking at problems from an\nunconventional perspective and defying existing conceptions and notions.\nTowards this direction, SemEval 2024 introduces the task of BRAINTEASER, which\ninvolves two types of questions -- Sentence Puzzles and Word Puzzles that defy\nconventional common-sense reasoning and constraints. In this paper, we tackle\nboth types of questions using few-shot prompting on GPT-3.5 and gain insights\nregarding the difference in the nature of the two types. Our prompting strategy\nplaced us 26th on the leaderboard for the Sentence Puzzle and 15th on the Word\nPuzzle task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11559v1",
    "published_date": "2024-05-19 14:21:53 UTC",
    "updated_date": "2024-05-19 14:21:53 UTC"
  },
  {
    "arxiv_id": "2405.11551v1",
    "title": "An Invisible Backdoor Attack Based On Semantic Feature",
    "authors": [
      "Yangming Chen"
    ],
    "abstract": "Backdoor attacks have severely threatened deep neural network (DNN) models in\nthe past several years. These attacks can occur in almost every stage of the\ndeep learning pipeline. Although the attacked model behaves normally on benign\nsamples, it makes wrong predictions for samples containing triggers. However,\nmost existing attacks use visible patterns (e.g., a patch or image\ntransformations) as triggers, which are vulnerable to human inspection. In this\npaper, we propose a novel backdoor attack, making imperceptible changes.\nConcretely, our attack first utilizes the pre-trained victim model to extract\nlow-level and high-level semantic features from clean images and generates\ntrigger pattern associated with high-level features based on channel attention.\nThen, the encoder model generates poisoned images based on the trigger and\nextracted low-level semantic features without causing noticeable feature loss.\nWe evaluate our attack on three prominent image classification DNN across three\nstandard datasets. The results demonstrate that our attack achieves high attack\nsuccess rates while maintaining robustness against backdoor defenses.\nFurthermore, we conduct extensive image similarity experiments to emphasize the\nstealthiness of our attack strategy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11551v1",
    "published_date": "2024-05-19 13:50:40 UTC",
    "updated_date": "2024-05-19 13:50:40 UTC"
  },
  {
    "arxiv_id": "2405.13050v2",
    "title": "Human-Centered LLM-Agent User Interface: A Position Paper",
    "authors": [
      "Daniel Chin",
      "Yuxuan Wang",
      "Gus Xia"
    ],
    "abstract": "Large Language Model (LLM) -in-the-loop applications have been shown to\neffectively interpret the human user's commands, make plans, and operate\nexternal tools/systems accordingly. Still, the operation scope of the LLM agent\nis limited to passively following the user, requiring the user to frame his/her\nneeds with regard to the underlying tools/systems. We note that the potential\nof an LLM-Agent User Interface (LAUI) is much greater. A user mostly ignorant\nto the underlying tools/systems should be able to work with a LAUI to discover\nan emergent workflow. Contrary to the conventional way of designing an\nexplorable GUI to teach the user a predefined set of ways to use the system, in\nthe ideal LAUI, the LLM agent is initialized to be proficient with the system,\nproactively studies the user and his/her needs, and proposes new interaction\nschemes to the user. To illustrate LAUI, we present Flute X GPT, a concrete\nexample using an LLM agent, a prompt manager, and a flute-tutoring multi-modal\nsoftware-hardware system to facilitate the complex, real-time user experience\nof learning to play the flute.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13050v2",
    "published_date": "2024-05-19 13:02:45 UTC",
    "updated_date": "2024-09-23 16:41:04 UTC"
  },
  {
    "arxiv_id": "2405.11537v3",
    "title": "VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications",
    "authors": [
      "Mikhail Konenkov",
      "Artem Lykov",
      "Daria Trinitatova",
      "Dzmitry Tsetserukou"
    ],
    "abstract": "The advent of immersive Virtual Reality applications has transformed various\ndomains, yet their integration with advanced artificial intelligence\ntechnologies like Visual Language Models remains underexplored. This study\nintroduces a pioneering approach utilizing VLMs within VR environments to\nenhance user interaction and task efficiency. Leveraging the Unity engine and a\ncustom-developed VLM, our system facilitates real-time, intuitive user\ninteractions through natural language processing, without relying on visual\ntext instructions. The incorporation of speech-to-text and text-to-speech\ntechnologies allows for seamless communication between the user and the VLM,\nenabling the system to guide users through complex tasks effectively.\nPreliminary experimental results indicate that utilizing VLMs not only reduces\ntask completion times but also improves user comfort and task engagement\ncompared to traditional VR interaction methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.RO",
    "comment": "Updated version",
    "pdf_url": "http://arxiv.org/pdf/2405.11537v3",
    "published_date": "2024-05-19 12:56:00 UTC",
    "updated_date": "2024-08-03 10:19:54 UTC"
  },
  {
    "arxiv_id": "2405.11531v2",
    "title": "Knowledge Graph Pruning for Recommendation",
    "authors": [
      "Fake Lin",
      "Xi Zhu",
      "Ziwei Zhao",
      "Deqiang Huang",
      "Yu Yu",
      "Xueying Li",
      "Zhi Zheng",
      "Tong Xu",
      "Enhong Chen"
    ],
    "abstract": "Recent years have witnessed the prosperity of knowledge graph based\nrecommendation system (KGRS), which enriches the representation of users,\nitems, and entities by structural knowledge with striking improvement.\nNevertheless, its unaffordable computational cost still limits researchers from\nexploring more sophisticated models. We observe that the bottleneck for\ntraining efficiency arises from the knowledge graph, which is plagued by the\nwell-known issue of knowledge explosion. Recently, some works have attempted to\nslim the inflated KG via summarization techniques. However, these summarized\nnodes may ignore the collaborative signals and deviate from the facts that\nnodes in knowledge graph represent symbolic abstractions of entities from the\nreal-world. To this end, in this paper, we propose a novel approach called\nKGTrimmer for knowledge graph pruning tailored for recommendation, to remove\nthe unessential nodes while minimizing performance degradation. Specifically,\nwe design an importance evaluator from a dual-view perspective. For the\ncollective view, we embrace the idea of collective intelligence by extracting\ncommunity consensus based on abundant collaborative signals, i.e. nodes are\nconsidered important if they attract attention of numerous users. For the\nholistic view, we learn a global mask to identify the valueless nodes from\ntheir inherent properties or overall popularity. Next, we build an end-to-end\nimportance-aware graph neural network, which injects filtered knowledge to\nenhance the distillation of valuable user-item collaborative signals.\nUltimately, we generate a pruned knowledge graph with lightweight, stable, and\nrobust properties to facilitate the following-up recommendation task. Extensive\nexperiments are conducted on three publicly available datasets to prove the\neffectiveness and generalization ability of KGTrimmer.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11531v2",
    "published_date": "2024-05-19 12:07:24 UTC",
    "updated_date": "2024-07-09 08:57:52 UTC"
  },
  {
    "arxiv_id": "2405.11525v2",
    "title": "Overcoming Data and Model Heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
    "authors": [
      "Chun-Yin Huang",
      "Kartik Srinivas",
      "Xin Zhang",
      "Xiaoxiao Li"
    ],
    "abstract": "Conventional Federated Learning (FL) involves collaborative training of a\nglobal model while maintaining user data privacy. One of its branches,\ndecentralized FL, is a serverless network that allows clients to own and\noptimize different local models separately, which results in saving management\nand communication resources. Despite the promising advancements in\ndecentralized FL, it may reduce model generalizability due to lacking a global\nmodel. In this scenario, managing data and model heterogeneity among clients\nbecomes a crucial problem, which poses a unique challenge that must be\novercome: How can every client's local model learn generalizable representation\nin a decentralized manner? To address this challenge, we propose a novel\nDecentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA.\nBased on the theory of domain adaptation and Knowledge Distillation (KD), we\ntheoretically and empirically show that synthesizing global anchors based on\nraw data distribution facilitates mutual knowledge transfer. We further design\ntwo effective regularization terms for local training: 1) REG loss that\nregularizes the distribution of the client's latent embedding with the anchors\nand 2) KD loss that enables clients to learn from others. Through extensive\nexperiments on diverse client data distributions, we showcase the effectiveness\nof DeSA in enhancing both inter- and intra-domain accuracy of each client.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper Accepted at ICML 2024, 23 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.11525v2",
    "published_date": "2024-05-19 11:36:45 UTC",
    "updated_date": "2025-03-12 04:39:54 UTC"
  },
  {
    "arxiv_id": "2405.11504v2",
    "title": "Machine Learning & Wi-Fi: Unveiling the Path Towards AI/ML-Native IEEE 802.11 Networks",
    "authors": [
      "Francesc Wilhelmi",
      "Szymon Szott",
      "Katarzyna Kosek-Szott",
      "Boris Bellalta"
    ],
    "abstract": "Artificial intelligence (AI) and machine learning (ML) are nowadays mature\ntechnologies considered essential for driving the evolution of future\ncommunications systems. Simultaneously, Wi-Fi technology has constantly evolved\nover the past three decades and incorporated new features generation after\ngeneration, thus gaining in complexity. As such, researchers have observed that\nAI/ML functionalities may be required to address the upcoming Wi-Fi challenges\nthat will be otherwise difficult to solve with traditional approaches. This\npaper discusses the role of AI/ML in current and future Wi-Fi networks and\ndepicts the ways forward. A roadmap towards AI/ML-native Wi-Fi, key challenges,\nstandardization efforts, and major enablers are also discussed. An exemplary\nuse case is provided to showcase the potential of AI/ML in Wi-Fi at different\nadoption stages.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11504v2",
    "published_date": "2024-05-19 10:12:20 UTC",
    "updated_date": "2024-08-30 05:11:37 UTC"
  },
  {
    "arxiv_id": "2405.13049v3",
    "title": "SemEval-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations",
    "authors": [
      "Fanfan Wang",
      "Heqing Ma",
      "Jianfei Yu",
      "Rui Xia",
      "Erik Cambria"
    ],
    "abstract": "The ability to understand emotions is an essential component of human-like\nartificial intelligence, as emotions greatly influence human cognition,\ndecision making, and social interactions. In addition to emotion recognition in\nconversations, the task of identifying the potential causes behind an\nindividual's emotional state in conversations, is of great importance in many\napplication scenarios. We organize SemEval-2024 Task 3, named Multimodal\nEmotion Cause Analysis in Conversations, which aims at extracting all pairs of\nemotions and their corresponding causes from conversations. Under different\nmodality settings, it consists of two subtasks: Textual Emotion-Cause Pair\nExtraction in Conversations (TECPE) and Multimodal Emotion-Cause Pair\nExtraction in Conversations (MECPE). The shared task has attracted 143\nregistrations and 216 successful submissions. In this paper, we introduce the\ntask, dataset and evaluation settings, summarize the systems of the top teams,\nand discuss the findings of the participants.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the 18th International Workshop on Semantic Evaluation\n  (SemEval-2024). 12 pages, 3 figures, 4 Tables",
    "pdf_url": "http://arxiv.org/pdf/2405.13049v3",
    "published_date": "2024-05-19 09:59:00 UTC",
    "updated_date": "2024-07-08 07:32:28 UTC"
  },
  {
    "arxiv_id": "2405.11476v1",
    "title": "NubbleDrop: A Simple Way to Improve Matching Strategy for Prompted One-Shot Segmentation",
    "authors": [
      "Zhiyu Xu",
      "Qingliang Chen"
    ],
    "abstract": "Driven by large data trained segmentation models, such as SAM , research in\none-shot segmentation has experienced significant advancements. Recent\ncontributions like PerSAM and MATCHER , presented at ICLR 2024, utilize a\nsimilar approach by leveraging SAM with one or a few reference images to\ngenerate high quality segmentation masks for target images. Specifically, they\nutilize raw encoded features to compute cosine similarity between patches\nwithin reference and target images along the channel dimension, effectively\ngenerating prompt points or boxes for the target images a technique referred to\nas the matching strategy. However, relying solely on raw features might\nintroduce biases and lack robustness for such a complex task. To address this\nconcern, we delve into the issues of feature interaction and uneven\ndistribution inherent in raw feature based matching. In this paper, we propose\na simple and training-free method to enhance the validity and robustness of the\nmatching strategy at no additional computational cost (NubbleDrop). The core\nconcept involves randomly dropping feature channels (setting them to zero)\nduring the matching process, thereby preventing models from being influenced by\nchannels containing deceptive information. This technique mimics discarding\npathological nubbles, and it can be seamlessly applied to other similarity\ncomputing scenarios. We conduct a comprehensive set of experiments, considering\na wide range of factors, to demonstrate the effectiveness and validity of our\nproposed method. Our results showcase the significant improvements achieved\nthrough this simmple and straightforward approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2405.11476v1",
    "published_date": "2024-05-19 08:00:38 UTC",
    "updated_date": "2024-05-19 08:00:38 UTC"
  },
  {
    "arxiv_id": "2405.11473v4",
    "title": "FIFO-Diffusion: Generating Infinite Videos from Text without Training",
    "authors": [
      "Jihwan Kim",
      "Junoh Kang",
      "Jinyoung Choi",
      "Bohyung Han"
    ],
    "abstract": "We propose a novel inference technique based on a pretrained diffusion model\nfor text-conditional video generation. Our approach, called FIFO-Diffusion, is\nconceptually capable of generating infinitely long videos without additional\ntraining. This is achieved by iteratively performing diagonal denoising, which\nsimultaneously processes a series of consecutive frames with increasing noise\nlevels in a queue; our method dequeues a fully denoised frame at the head while\nenqueuing a new random noise frame at the tail. However, diagonal denoising is\na double-edged sword as the frames near the tail can take advantage of cleaner\nframes by forward reference but such a strategy induces the discrepancy between\ntraining and inference. Hence, we introduce latent partitioning to reduce the\ntraining-inference gap and lookahead denoising to leverage the benefit of\nforward referencing. Practically, FIFO-Diffusion consumes a constant amount of\nmemory regardless of the target video length given a baseline model, while\nwell-suited for parallel inference on multiple GPUs. We have demonstrated the\npromising results and effectiveness of the proposed methods on existing\ntext-to-video generation baselines. Generated video examples and source codes\nare available at our project page.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://jjihwan.github.io/projects/FIFO-Diffusion",
    "pdf_url": "http://arxiv.org/pdf/2405.11473v4",
    "published_date": "2024-05-19 07:48:41 UTC",
    "updated_date": "2024-11-03 12:40:41 UTC"
  },
  {
    "arxiv_id": "2405.11470v1",
    "title": "VCformer: Variable Correlation Transformer with Inherent Lagged Correlation for Multivariate Time Series Forecasting",
    "authors": [
      "Yingnan Yang",
      "Qingling Zhu",
      "Jianyong Chen"
    ],
    "abstract": "Multivariate time series (MTS) forecasting has been extensively applied\nacross diverse domains, such as weather prediction and energy consumption.\nHowever, current studies still rely on the vanilla point-wise self-attention\nmechanism to capture cross-variable dependencies, which is inadequate in\nextracting the intricate cross-correlation implied between variables. To fill\nthis gap, we propose Variable Correlation Transformer (VCformer), which\nutilizes Variable Correlation Attention (VCA) module to mine the correlations\namong variables. Specifically, based on the stochastic process theory, VCA\ncalculates and integrates the cross-correlation scores corresponding to\ndifferent lags between queries and keys, thereby enhancing its ability to\nuncover multivariate relationships. Additionally, inspired by Koopman dynamics\ntheory, we also develop Koopman Temporal Detector (KTD) to better address the\nnon-stationarity in time series. The two key components enable VCformer to\nextract both multivariate correlations and temporal dependencies. Our extensive\nexperiments on eight real-world datasets demonstrate the effectiveness of\nVCformer, achieving top-tier performance compared to other state-of-the-art\nbaseline models. Code is available at this repository:\nhttps://github.com/CSyyn/VCformer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.11470v1",
    "published_date": "2024-05-19 07:39:22 UTC",
    "updated_date": "2024-05-19 07:39:22 UTC"
  },
  {
    "arxiv_id": "2405.11464v3",
    "title": "Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion",
    "authors": [
      "Pengxiang Lan",
      "Enneng Yang",
      "Yuting Liu",
      "Guibing Guo",
      "Jianzhe Zhao",
      "Xingwei Wang"
    ],
    "abstract": "Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11464v3",
    "published_date": "2024-05-19 06:43:12 UTC",
    "updated_date": "2024-12-11 08:03:56 UTC"
  },
  {
    "arxiv_id": "2405.11461v1",
    "title": "DocReLM: Mastering Document Retrieval with Language Model",
    "authors": [
      "Gengchen Wei",
      "Xinle Pang",
      "Tianning Zhang",
      "Yu Sun",
      "Xun Qian",
      "Chen Lin",
      "Han-Sen Zhong",
      "Wanli Ouyang"
    ],
    "abstract": "With over 200 million published academic documents and millions of new\ndocuments being written each year, academic researchers face the challenge of\nsearching for information within this vast corpus. However, existing retrieval\nsystems struggle to understand the semantics and domain knowledge present in\nacademic papers. In this work, we demonstrate that by utilizing large language\nmodels, a document retrieval system can achieve advanced semantic understanding\ncapabilities, significantly outperforming existing systems. Our approach\ninvolves training the retriever and reranker using domain-specific data\ngenerated by large language models. Additionally, we utilize large language\nmodels to identify candidates from the references of retrieved papers to\nfurther enhance the performance. We use a test set annotated by academic\nresearchers in the fields of quantum physics and computer vision to evaluate\nour system's performance. The results show that DocReLM achieves a Top 10\naccuracy of 44.12% in computer vision, compared to Google Scholar's 15.69%, and\nan increase to 36.21% in quantum physics, while that of Google Scholar is\n12.96%.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11461v1",
    "published_date": "2024-05-19 06:30:22 UTC",
    "updated_date": "2024-05-19 06:30:22 UTC"
  },
  {
    "arxiv_id": "2405.11458v1",
    "title": "CPS-LLM: Large Language Model based Safe Usage Plan Generator for Human-in-the-Loop Human-in-the-Plant Cyber-Physical System",
    "authors": [
      "Ayan Banerjee",
      "Aranyak Maity",
      "Payal Kamboj",
      "Sandeep K. S. Gupta"
    ],
    "abstract": "We explore the usage of large language models (LLM) in human-in-the-loop\nhuman-in-the-plant cyber-physical systems (CPS) to translate a high-level\nprompt into a personalized plan of actions, and subsequently convert that plan\ninto a grounded inference of sequential decision-making automated by a\nreal-world CPS controller to achieve a control goal. We show that it is\nrelatively straightforward to contextualize an LLM so it can generate\ndomain-specific plans. However, these plans may be infeasible for the physical\nsystem to execute or the plan may be unsafe for human users. To address this,\nwe propose CPS-LLM, an LLM retrained using an instruction tuning framework,\nwhich ensures that generated plans not only align with the physical system\ndynamics of the CPS but are also safe for human users. The CPS-LLM consists of\ntwo innovative components: a) a liquid time constant neural network-based\nphysical dynamics coefficient estimator that can derive coefficients of\ndynamical models with some unmeasured state variables; b) the model\ncoefficients are then used to train an LLM with prompts embodied with traces\nfrom the dynamical system and the corresponding model coefficients. We show\nthat when the CPS-LLM is integrated with a contextualized chatbot such as BARD\nit can generate feasible and safe plans to manage external events such as meals\nfor automated insulin delivery systems used by Type 1 Diabetes subjects.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in AAAI 2024, Planning for Cyber Physical\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2405.11458v1",
    "published_date": "2024-05-19 06:00:18 UTC",
    "updated_date": "2024-05-19 06:00:18 UTC"
  },
  {
    "arxiv_id": "2405.11457v1",
    "title": "Deep Dive into Model-free Reinforcement Learning for Biological and Robotic Systems: Theory and Practice",
    "authors": [
      "Yusheng Jiao",
      "Feng Ling",
      "Sina Heydari",
      "Nicolas Heess",
      "Josh Merel",
      "Eva Kanso"
    ],
    "abstract": "Animals and robots exist in a physical world and must coordinate their bodies\nto achieve behavioral objectives. With recent developments in deep\nreinforcement learning, it is now possible for scientists and engineers to\nobtain sensorimotor strategies (policies) for specific tasks using physically\nsimulated bodies and environments. However, the utility of these methods goes\nbeyond the constraints of a specific task; they offer an exciting framework for\nunderstanding the organization of an animal sensorimotor system in connection\nto its morphology and physical interaction with the environment, as well as for\nderiving general design rules for sensing and actuation in robotic systems.\nAlgorithms and code implementing both learning agents and environments are\nincreasingly available, but the basic assumptions and choices that go into the\nformulation of an embodied feedback control problem using deep reinforcement\nlearning may not be immediately apparent. Here, we present a concise exposition\nof the mathematical and algorithmic aspects of model-free reinforcement\nlearning, specifically through the use of \\textit{actor-critic} methods, as a\ntool for investigating the feedback control underlying animal and robotic\nbehavior.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "20 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.11457v1",
    "published_date": "2024-05-19 05:58:44 UTC",
    "updated_date": "2024-05-19 05:58:44 UTC"
  },
  {
    "arxiv_id": "2405.11451v1",
    "title": "Error Analysis of Three-Layer Neural Network Trained with PGD for Deep Ritz Method",
    "authors": [
      "Yuling Jiao",
      "Yanming Lai",
      "Yang Wang"
    ],
    "abstract": "Machine learning is a rapidly advancing field with diverse applications\nacross various domains. One prominent area of research is the utilization of\ndeep learning techniques for solving partial differential equations(PDEs). In\nthis work, we specifically focus on employing a three-layer tanh neural network\nwithin the framework of the deep Ritz method(DRM) to solve second-order\nelliptic equations with three different types of boundary conditions. We\nperform projected gradient descent(PDG) to train the three-layer network and we\nestablish its global convergence. To the best of our knowledge, we are the\nfirst to provide a comprehensive error analysis of using overparameterized\nnetworks to solve PDE problems, as our analysis simultaneously includes\nestimates for approximation error, generalization error, and optimization\nerror. We present error bound in terms of the sample size $n$ and our work\nprovides guidance on how to set the network depth, width, step size, and number\nof iterations for the projected gradient descent algorithm. Importantly, our\nassumptions in this work are classical and we do not require any additional\nassumptions on the solution of the equation. This ensures the broad\napplicability and generality of our results.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "math.AP",
      "stat.ML",
      "65N12, 65N15, 68T07, 62G05, 35J25"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11451v1",
    "published_date": "2024-05-19 05:07:09 UTC",
    "updated_date": "2024-05-19 05:07:09 UTC"
  },
  {
    "arxiv_id": "2405.12954v2",
    "title": "A Method on Searching Better Activation Functions",
    "authors": [
      "Haoyuan Sun",
      "Zihao Wu",
      "Bo Xia",
      "Pu Chang",
      "Zibin Dong",
      "Yifu Yuan",
      "Yongzhe Chang",
      "Xueqian Wang"
    ],
    "abstract": "The success of artificial neural networks (ANNs) hinges greatly on the\njudicious selection of an activation function, introducing non-linearity into\nnetwork and enabling them to model sophisticated relationships in data.\nHowever, the search of activation functions has largely relied on empirical\nknowledge in the past, lacking theoretical guidance, which has hindered the\nidentification of more effective activation functions. In this work, we offer a\nproper solution to such issue. Firstly, we theoretically demonstrate the\nexistence of the worst activation function with boundary conditions (WAFBC)\nfrom the perspective of information entropy. Furthermore, inspired by the\nTaylor expansion form of information entropy functional, we propose the\nEntropy-based Activation Function Optimization (EAFO) methodology. EAFO\nmethodology presents a novel perspective for designing static activation\nfunctions in deep neural networks and the potential of dynamically optimizing\nactivation during iterative training. Utilizing EAFO methodology, we derive a\nnovel activation function from ReLU, known as Correction Regularized ReLU\n(CRReLU). Experiments conducted with vision transformer and its variants on\nCIFAR-10, CIFAR-100 and ImageNet-1K datasets demonstrate the superiority of\nCRReLU over existing corrections of ReLU. Extensive empirical studies on task\nof large language model (LLM) fine-tuning, CRReLU exhibits superior performance\ncompared to GELU, suggesting its broader potential for practical applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages,3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.12954v2",
    "published_date": "2024-05-19 03:48:05 UTC",
    "updated_date": "2024-05-22 15:43:42 UTC"
  },
  {
    "arxiv_id": "2407.12153v1",
    "title": "A Comparative Analysis of Student Performance Predictions in Online Courses using Heterogeneous Knowledge Graphs",
    "authors": [
      "Thomas Trask",
      "Nicholas Lytle",
      "Michael Boyle",
      "David Joyner",
      "Ahmed Mubarak"
    ],
    "abstract": "As online courses become the norm in the higher-education landscape,\ninvestigations into student performance between students who take online vs\non-campus versions of classes become necessary. While attention has been given\nto looking at differences in learning outcomes through comparisons of students'\nend performance, less attention has been given in comparing students'\nengagement patterns between different modalities. In this study, we analyze a\nheterogeneous knowledge graph consisting of students, course videos, formative\nassessments and their interactions to predict student performance via a Graph\nConvolutional Network (GCN). Using students' performance on the assessments, we\nattempt to determine a useful model for identifying at-risk students. We then\ncompare the models generated between 5 on-campus and 2 fully-online MOOC-style\ninstances of the same course. The model developed achieved a 70-90\\% accuracy\nof predicting whether a student would pass a particular problem set based on\ncontent consumed, course instance, and modality.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, 3 figures, 2 tables, Educational Data Mining Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12153v1",
    "published_date": "2024-05-19 03:33:59 UTC",
    "updated_date": "2024-05-19 03:33:59 UTC"
  },
  {
    "arxiv_id": "2405.13048v1",
    "title": "Human-Generative AI Collaborative Problem Solving Who Leads and How Students Perceive the Interactions",
    "authors": [
      "Gaoxia Zhu",
      "Vidya Sudarshan",
      "Jason Fok Kow",
      "Yew Soon Ong"
    ],
    "abstract": "This research investigates distinct human-generative AI collaboration types\nand students' interaction experiences when collaborating with generative AI\n(i.e., ChatGPT) for problem-solving tasks and how these factors relate to\nstudents' sense of agency and perceived collaborative problem solving. By\nanalyzing the surveys and reflections of 79 undergraduate students, we\nidentified three human-generative AI collaboration types: even contribution,\nhuman leads, and AI leads. Notably, our study shows that 77.21% of students\nperceived they led or had even contributed to collaborative problem-solving\nwhen collaborating with ChatGPT. On the other hand, 15.19% of the human\nparticipants indicated that the collaborations were led by ChatGPT, indicating\na potential tendency for students to rely on ChatGPT. Furthermore, 67.09% of\nstudents perceived their interaction experiences with ChatGPT to be positive or\nmixed. We also found a positive correlation between positive interaction\nexperience and a sense of positive agency. The results of this study contribute\nto our understanding of the collaboration between students and generative AI\nand highlight the need to study further why some students let ChatGPT lead\ncollaborative problem-solving and how to enhance their interaction experience\nthrough curriculum and technology design.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "This paper appears at the IEEE Conference on Artificial Intelligence\n  (CAI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.13048v1",
    "published_date": "2024-05-19 03:29:16 UTC",
    "updated_date": "2024-05-19 03:29:16 UTC"
  },
  {
    "arxiv_id": "2405.11422v1",
    "title": "Large Language Models are Biased Reinforcement Learners",
    "authors": [
      "William M. Hayes",
      "Nicolas Yax",
      "Stefano Palminteri"
    ],
    "abstract": "In-context learning enables large language models (LLMs) to perform a variety\nof tasks, including learning to make reward-maximizing choices in simple bandit\ntasks. Given their potential use as (autonomous) decision-making agents, it is\nimportant to understand how these models perform such reinforcement learning\n(RL) tasks and the extent to which they are susceptible to biases. Motivated by\nthe fact that, in humans, it has been widely documented that the value of an\noutcome depends on how it compares to other local outcomes, the present study\nfocuses on whether similar value encoding biases apply to how LLMs encode\nrewarding outcomes. Results from experiments with multiple bandit tasks and\nmodels show that LLMs exhibit behavioral signatures of a relative value bias.\nAdding explicit outcome comparisons to the prompt produces opposing effects on\nperformance, enhancing maximization in trained choice sets but impairing\ngeneralization to new choice sets. Computational cognitive modeling reveals\nthat LLM behavior is well-described by a simple RL algorithm that incorporates\nrelative values at the outcome encoding stage. Lastly, we present preliminary\nevidence that the observed biases are not limited to fine-tuned LLMs, and that\nrelative value processing is detectable in the final hidden layer activations\nof a raw, pretrained model. These findings have important implications for the\nuse of LLMs in decision-making applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11422v1",
    "published_date": "2024-05-19 01:43:52 UTC",
    "updated_date": "2024-05-19 01:43:52 UTC"
  },
  {
    "arxiv_id": "2405.11421v1",
    "title": "Assessing Group Fairness with Social Welfare Optimization",
    "authors": [
      "Violet Chen",
      "J. N. Hooker",
      "Derek Leben"
    ],
    "abstract": "Statistical parity metrics have been widely studied and endorsed in the AI\ncommunity as a means of achieving fairness, but they suffer from at least two\nweaknesses. They disregard the actual welfare consequences of decisions and may\ntherefore fail to achieve the kind of fairness that is desired for\ndisadvantaged groups. In addition, they are often incompatible with each other,\nand there is no convincing justification for selecting one rather than another.\nThis paper explores whether a broader conception of social justice, based on\noptimizing a social welfare function (SWF), can be useful for assessing various\ndefinitions of parity. We focus on the well-known alpha fairness SWF, which has\nbeen defended by axiomatic and bargaining arguments over a period of 70 years.\nWe analyze the optimal solution and show that it can justify demographic parity\nor equalized odds under certain conditions, but frequently requires a departure\nfrom these types of parity. In addition, we find that predictive rate parity is\nof limited usefulness. These results suggest that optimization theory can shed\nlight on the intensely discussed question of how to achieve group fairness in\nAI.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11421v1",
    "published_date": "2024-05-19 01:41:04 UTC",
    "updated_date": "2024-05-19 01:41:04 UTC"
  }
]
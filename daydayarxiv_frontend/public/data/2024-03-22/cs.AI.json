{
  "date": "2024-03-22",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-22 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 应用创新，特别是大型语言模型 (LLM) 在对话、医疗和教育中的扩展，AI 伦理与风险管理，以及机器人导航和多模态模型的进展；令人印象深刻的包括 Surgical-LVLM 在手术视觉问答中的新框架，以及 AI 在教育政策分析中的实证研究，展示了知名学者如 David Danks 和相关团队的贡献。\n\n### 重点论文讨论\n我将优先讨论重要、话题性和创新性强的论文（如涉及 LLM、AI 伦理和医疗应用），并将相关主题归类。其他次要论文（如某些纯技术优化或小数据集实验）将快速掠过，仅简要提及。\n\n#### AI 伦理与风险管理\n- **Application of the NIST AI Risk Management Framework to Surveillance Technology**（NIST AI 风险管理框架在监控技术中的应用）  \n  这篇论文分析了 NIST AI RMF 在面部识别等高风险监控领域的应用，主要贡献是提出一个六步风险管理策略，强调持续评估和改进，以实现 AI 的负责任部署。发现揭示了框架的潜在缺陷，如在监控场景中的适用性不足，对 AI 治理讨论有重要启发。\n\n- **A Technological Perspective on Misuse of Available AI**（对现有 AI 滥用的技术视角）  \n  作者探讨了民用 AI 被滥用为自主武器系统的风险，主要贡献是开发三个示例用例，展示如何从现有 AI 技术中构建威胁，并提出控制点和预防措施。该论文强调 AI 安全的重要性，适合当前 AI 伦理热点。\n\n#### LLM 和多模态模型应用\n- **Large language models for crowd decision making based on prompt design strategies using ChatGPT**（使用 ChatGPT 的提示设计策略进行群体决策的大语言模型）  \n  这篇论文利用 LLM 如 ChatGPT 提取社交媒体意见进行决策，主要贡献是设计多标准决策框架和端到端模型，实验在 TripAdvisor 数据集上显示了 LLM 在决策中的潜力。发现包括 LLM 的可扩展性挑战，如一致性和解释性问题。\n\n- **Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery**（Surgical-LVLM：为机器人手术的视觉问答适应大型视觉语言模型的学习）  \n  令人印象深刻的论文，作者提出 Surgical-LVLM 框架，用于手术图像的视觉问答。主要贡献是结合视觉感知 LoRA 和 Token-Interaction 模块，提升多模态推理精度，在 EndoVis 数据集上达到新基准。该发现对机器人手术 AI 应用有重大启发。\n\n- **MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis**（MedPromptX：用于胸部 X 光诊断的基于多模态提示的视觉基础模型）  \n  这篇论文引入 MedPromptX 系统，用于整合 X 光图像和 EHR 数据进行诊断。主要贡献是使用少样本提示和视觉基础模型，改善诊断准确性，在 MIMIC-IV 数据集上 F1 分数提升 11%。发现突显了多模态 AI 在医疗中的潜力。\n\n- **Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition**（听我、看我、理解我：音频-视觉自闭症行为识别）  \n  作者定义了音频-视觉自闭症行为识别新任务，主要贡献是构建 AV-ASD 数据集并使用多模态 LLM 提升识别性能。实验显示整合音频、视觉和语音模态显著提高准确性，该发现为 AI 辅助自闭症筛查提供新路径。\n\n- **LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models**（LLaVA-PruMerge：用于高效大型多模态模型的自适应令牌减少）  \n  这篇论文提出 PruMerge 策略，用于减少多模态模型的视觉令牌。主要贡献是动态选择和合并关键令牌，提升效率，在 LLaVA-1.5 上令牌减少 14 倍而不损性能。该发现对大规模多模态 AI 优化有实际价值。\n\n#### 机器人导航与强化学习\n- **Unifying Large Language Model and Deep Reinforcement Learning for Human-in-Loop Interactive Socially-Aware Navigation**（统一大型语言模型和深度强化学习用于人类交互的社会感知导航）  \n  作者引入 SALM 框架，结合 LLM 和 DRL 实现社会感知机器人导航。主要贡献是处理实时用户输入和动态环境，实验显示提升了导航精度和适应性。该论文有话题度，展示了 LLM 在机器人领域的潜力。\n\n- **Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control**（感知不确定性下的自动驾驶：基于深度集成集成的自适应巡航控制）  \n  这篇论文使用深度集成回归器估计感知不确定性，主要贡献是开发基于随机模型预测控制的巡航算法，提供概率安全保证。实验在真实交通数据上验证了其鲁棒性，对自动驾驶安全有启发。\n\n#### 其他值得注意的论文\n- **From Guidelines to Governance: A Study of AI Policies in Education**（从指南到治理：AI 在教育中的政策研究）  \n  快速提及：调查教育机构 AI 政策，主要发现多数机构缺乏针对性指南，强调学生隐私和防剽窃需求。\n\n- **Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors**（生成式 AI 在教育中的应用：教育者的认知、情感和影响因素研究）  \n  这篇论文分析教育者对 AI 的态度，主要贡献是发现 CS 教育者更乐观，并无教学风格与态度的相关性。\n\n其他论文如某些强化学习或小数据集实验（如 Contextual Restless Multi-Armed Bandits），虽有技术贡献但话题度较低，仅快速掠过不做深入讨论，以控制篇幅。总体而言，今天的 arXiv 突显 AI 向实际应用（如医疗和教育）的扩展，LLM 的创新是亮点。",
  "papers": [
    {
      "arxiv_id": "2403.15648v3",
      "title": "Unifying Large Language Model and Deep Reinforcement Learning for Human-in-Loop Interactive Socially-aware Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Weizheng Wang",
        "Ike Obi",
        "Aniket Bera",
        "Byung-Cheol Min"
      ],
      "abstract": "Navigating human-filled spaces is crucial for the interactive social robots\nto support advanced services, such as cooperative carrying, which enables\nservice provision in complex and crowded environments while adapting behavior\nbased on real-time human language commands or feedback. However, existing\nsocial robot navigation planners face two major challenges: managing real-time\nuser inputs and ensuring socially compliant behaviors in unfamiliar, zero-shot\nenvironments. In response, we introduce SALM, an interactive, human-in-loop\nSocially-Aware navigation Large Language Model framework that dynamically\nintegrates deep reinforcement learning (DRL) with large language model (LLM)\ncapabilities. SALM leverages contextual semantic understanding from real-time\nhuman-robot interactions to convert high-level user commands into precise,\nlow-level control actions. A high-level LLM module parses user input, guiding\nthe simultaneous generation of navigation commands by both a large language\nnavigation model (LNM) and a DRL-based navigation model (RLNM). A memory\nmechanism archives temporal data for continuous refinement, while a multi-step\ngraph-of-thoughts inference-based large language feedback model adaptively\nfuses the strengths of both planning approaches. Experimental evaluations\ndemonstrate that SALM not only enhances navigational precision in crowded,\ndynamic environments but also significantly improves system adaptability,\noffering tailored behaviors that align with individual user preferences and\nreal-time feedback. More details and videos about this work are available at:\nhttps://sites.google.com/view/navi-salm.",
      "tldr_zh": "该研究提出 SALM 框架，将 Large Language Model (LLM) 与 Deep Reinforcement Learning (DRL) 统一，用于人类参与的交互式社交导航，旨在解决机器人处理实时用户输入和在未知环境保持社交合规性的挑战。SALM 通过高水平 LLM 模块解析用户命令，并结合 Large Language Navigation Model (LNM) 和 DRL-based Navigation Model (RLNM)，利用记忆机制和多步图思维推理反馈模型来融合规划策略，实现动态适应和精确控制。实验结果显示，SALM 在拥挤动态环境中显著提升了导航精度和系统适应性，能够根据用户偏好和实时反馈调整行为。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15648v3",
      "published_date": "2024-03-22 23:12:28 UTC",
      "updated_date": "2025-03-07 20:03:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:51:11.721896"
    },
    {
      "arxiv_id": "2403.15646v1",
      "title": "Application of the NIST AI Risk Management Framework to Surveillance Technology",
      "title_zh": "NIST AI 风险管理框架在监控技术中的应用",
      "authors": [
        "Nandhini Swaminathan",
        "David Danks"
      ],
      "abstract": "This study offers an in-depth analysis of the application and implications of\nthe National Institute of Standards and Technology's AI Risk Management\nFramework (NIST AI RMF) within the domain of surveillance technologies,\nparticularly facial recognition technology. Given the inherently high-risk and\nconsequential nature of facial recognition systems, our research emphasizes the\ncritical need for a structured approach to risk management in this sector. The\npaper presents a detailed case study demonstrating the utility of the NIST AI\nRMF in identifying and mitigating risks that might otherwise remain unnoticed\nin these technologies. Our primary objective is to develop a comprehensive risk\nmanagement strategy that advances the practice of responsible AI utilization in\nfeasible, scalable ways. We propose a six-step process tailored to the specific\nchallenges of surveillance technology that aims to produce a more systematic\nand effective risk management practice. This process emphasizes continual\nassessment and improvement to facilitate companies in managing AI-related risks\nmore robustly and ensuring ethical and responsible deployment of AI systems.\nAdditionally, our analysis uncovers and discusses critical gaps in the current\nframework of the NIST AI RMF, particularly concerning its application to\nsurveillance technologies. These insights contribute to the evolving discourse\non AI governance and risk management, highlighting areas for future refinement\nand development in frameworks like the NIST AI RMF.",
      "tldr_zh": "本研究分析了 NIST AI RMF 在监控技术领域，尤其是面部识别技术中的应用，强调了结构化风险管理对高风险 AI 系统的重要性。通过一个详细案例研究，该论文提出一个针对监控技术的六步风险管理过程，旨在系统识别、缓解风险，并实现持续评估和改进，以促进负责任的 AI 部署。该方法有助于企业更有效地管理 AI 相关风险，同时揭示了 NIST AI RMF 框架在应用中的关键空白，为 AI 治理和未来框架优化提供宝贵见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.4.1, K.5.2"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15646v1",
      "published_date": "2024-03-22 23:07:11 UTC",
      "updated_date": "2024-03-22 23:07:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:51:22.761250"
    },
    {
      "arxiv_id": "2406.02554v1",
      "title": "Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition",
      "title_zh": "聆听我、注视我、理解我：音频-视觉自闭症行为识别",
      "authors": [
        "Shijian Deng",
        "Erin E. Kosloski",
        "Siddhi Patel",
        "Zeke A. Barnett",
        "Yiyang Nan",
        "Alexander Kaplan",
        "Sisira Aarukapalli",
        "William T. Doan",
        "Matthew Wang",
        "Harsh Singh",
        "Pamela R. Rollins",
        "Yapeng Tian"
      ],
      "abstract": "In this article, we introduce a novel problem of audio-visual autism behavior\nrecognition, which includes social behavior recognition, an essential aspect\npreviously omitted in AI-assisted autism screening research. We define the task\nat hand as one that is audio-visual autism behavior recognition, which uses\naudio and visual cues, including any speech present in the audio, to recognize\nautism-related behaviors. To facilitate this new research direction, we\ncollected an audio-visual autism spectrum dataset (AV-ASD), currently the\nlargest video dataset for autism screening using a behavioral approach. It\ncovers an extensive range of autism-associated behaviors, including those\nrelated to social communication and interaction. To pave the way for further\nresearch on this new problem, we intensively explored leveraging foundation\nmodels and multimodal large language models across different modalities. Our\nexperiments on the AV-ASD dataset demonstrate that integrating audio, visual,\nand speech modalities significantly enhances the performance in autism behavior\nrecognition. Additionally, we explored the use of a post-hoc to ad-hoc pipeline\nin a multimodal large language model to investigate its potential to augment\nthe model's explanatory capability during autism behavior recognition. We will\nrelease our dataset, code, and pre-trained models.",
      "tldr_zh": "本研究引入了音频-视觉自闭症行为识别（Audio-Visual Autism Behavior Recognition）这一新问题，强调使用音频和视觉线索（包括语音）来识别自闭症相关行为，特别是之前被忽略的社会行为。研究者收集了目前最大的视频数据集AV-ASD，涵盖广泛的自闭症相关行为，如社会沟通和互动，以支持这一研究方向。实验结果显示，通过整合音频、视觉和语音模态，并利用基础模型和多模态大语言模型（Multimodal Large Language Models），显著提升了行为识别的性能。 additionally, 他们探索了后验到现成管道（Post-Hoc to Ad-Hoc Pipeline）以增强模型的解释能力，并计划发布数据集、代码和预训练模型。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.02554v1",
      "published_date": "2024-03-22 22:52:35 UTC",
      "updated_date": "2024-03-22 22:52:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:51:35.378974"
    },
    {
      "arxiv_id": "2403.15640v1",
      "title": "Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Chen",
        "I-Hong Hou"
      ],
      "abstract": "This paper introduces a novel multi-armed bandits framework, termed\nContextual Restless Bandits (CRB), for complex online decision-making. This CRB\nframework incorporates the core features of contextual bandits and restless\nbandits, so that it can model both the internal state transitions of each arm\nand the influence of external global environmental contexts. Using the dual\ndecomposition method, we develop a scalable index policy algorithm for solving\nthe CRB problem, and theoretically analyze the asymptotical optimality of this\nalgorithm. In the case when the arm models are unknown, we further propose a\nmodel-based online learning algorithm based on the index policy to learn the\narm models and make decisions simultaneously. Furthermore, we apply the\nproposed CRB framework and the index policy algorithm specifically to the\ndemand response decision-making problem in smart grids. The numerical\nsimulations demonstrate the performance and efficiency of our proposed CRB\napproaches.",
      "tldr_zh": "这篇论文提出了一种新的多臂赌博机框架，名为Contextual Restless Bandits (CRB)，用于处理复杂的在线决策问题，该框架整合了contextual bandits和restless bandits的特点，以模型化每个臂的内部状态转换和外部全局环境影响。作者采用dual decomposition方法开发了一个可扩展的index policy算法，并理论上证明了其渐近最优性；同时，当臂模型未知时，提出了一种基于模型的在线学习算法，用于边学习边决策。将CRB框架应用于智能电网的需求响应决策问题，数值模拟结果展示了该方法的性能和效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15640v1",
      "published_date": "2024-03-22 22:35:07 UTC",
      "updated_date": "2024-03-22 22:35:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:51:48.074692"
    },
    {
      "arxiv_id": "2403.15604v2",
      "title": "Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People",
      "title_zh": "翻译失败",
      "authors": [
        "Ricardo Gonzalez",
        "Jazmin Collins",
        "Shiri Azenkot",
        "Cynthia Bennett"
      ],
      "abstract": "\"Scene description\" applications that describe visual content in a photo are\nuseful daily tools for blind and low vision (BLV) people. Researchers have\nstudied their use, but they have only explored those that leverage remote\nsighted assistants; little is known about applications that use AI to generate\ntheir descriptions. Thus, to investigate their use cases, we conducted a\ntwo-week diary study where 16 BLV participants used an AI-powered scene\ndescription application we designed. Through their diary entries and follow-up\ninterviews, users shared their information goals and assessments of the visual\ndescriptions they received. We analyzed the entries and found frequent use\ncases, such as identifying visual features of known objects, and surprising\nones, such as avoiding contact with dangerous objects. We also found users\nscored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for\nsatisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions\nstill need significant improvements to deliver satisfying and trustworthy\nexperiences. We discuss future opportunities for AI as it becomes a more\npowerful accessibility tool for BLV users.",
      "tldr_zh": "这篇论文调查了 AI 驱动的场景描述应用在 BLV（Blind and Low Vision）人群中的使用案例，这些应用通过描述照片中的视觉内容来辅助日常生活。研究者设计了一个 AI 应用，并通过为期两周的日记研究和后续访谈，收集了 16 名 BLV 参与者的反馈，揭示了常见用途（如识别已知物体的视觉特征）和意外用途（如避免接触危险物体）。结果显示，用户对描述的满意度平均为 2.76/5（标准差 1.49），信任度为 2.43/4（标准差 1.16），表明当前 AI 描述质量仍有显著改进空间。论文讨论了 AI 作为 BLV 用户辅助工具的未来潜力，推动更可靠的辅助技术发展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2"
      ],
      "primary_category": "cs.HC",
      "comment": "21 pages, 18 figures, 5 tables, main track CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15604v2",
      "published_date": "2024-03-22 20:16:55 UTC",
      "updated_date": "2025-03-11 23:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:52:01.428177"
    },
    {
      "arxiv_id": "2403.15603v2",
      "title": "Forward Learning for Gradient-based Black-box Saliency Map Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zeliang Zhang",
        "Mingqian Feng",
        "Jinyang Jiang",
        "Rongyi Zhu",
        "Yijie Peng",
        "Chenliang Xu"
      ],
      "abstract": "Gradient-based saliency maps are widely used to explain deep neural network\ndecisions. However, as models become deeper and more black-box, such as in\nclosed-source APIs like ChatGPT, computing gradients become challenging,\nhindering conventional explanation methods. In this work, we introduce a novel\nunified framework for estimating gradients in black-box settings and generating\nsaliency maps to interpret model decisions. We employ the likelihood ratio\nmethod to estimate output-to-input gradients and utilize them for saliency map\ngeneration. Additionally, we propose blockwise computation techniques to\nenhance estimation accuracy. Extensive experiments in black-box settings\nvalidate the effectiveness of our method, demonstrating accurate gradient\nestimation and explainability of generated saliency maps. Furthermore, we\nshowcase the scalability of our approach by applying it to explain GPT-Vision,\nrevealing the continued relevance of gradient-based explanation methods in the\nera of large, closed-source, and black-box models.",
      "tldr_zh": "这篇论文提出了一种新框架，用于在黑盒设置中估计梯度并生成 gradient-based saliency maps，以解释深度神经网络的决策，解决了传统方法在封闭源模型（如 ChatGPT）中计算梯度的挑战。框架采用 likelihood ratio method 估计 output-to-input gradients，并引入 blockwise computation techniques 来提高估计准确性。实验验证了该方法的有效性，展示了准确的梯度估计和 saliency maps 的解释性，并成功应用于解释 GPT-Vision，证明了 gradient-based 方法在大型黑盒模型时代的持续相关性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The evaluation is based on small datasets and limited models, of\n  which bias leads to misleading conclusions",
      "pdf_url": "http://arxiv.org/pdf/2403.15603v2",
      "published_date": "2024-03-22 20:11:19 UTC",
      "updated_date": "2024-07-02 16:05:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:52:11.106666"
    },
    {
      "arxiv_id": "2403.15601v1",
      "title": "From Guidelines to Governance: A Study of AI Policies in Education",
      "title_zh": "翻译失败",
      "authors": [
        "Aashish Ghimire",
        "John Edwards"
      ],
      "abstract": "Emerging technologies like generative AI tools, including ChatGPT, are\nincreasingly utilized in educational settings, offering innovative approaches\nto learning while simultaneously posing new challenges. This study employs a\nsurvey methodology to examine the policy landscape concerning these\ntechnologies, drawing insights from 102 high school principals and higher\neducation provosts. Our results reveal a prominent policy gap: the majority of\ninstitutions lack specialized guide-lines for the ethical deployment of AI\ntools such as ChatGPT. Moreover,we observed that high schools are less inclined\nto work on policies than higher educational institutions. Where such policies\ndo exist, they often overlook crucial issues, including student privacy and\nalgorithmic transparency. Administrators overwhelmingly recognize the necessity\nof these policies, primarily to safeguard student safety and mitigate\nplagiarism risks. Our findings underscore the urgent need for flexible and\niterative policy frameworks in educational contexts.",
      "tldr_zh": "这篇论文通过调查102名高中校长和高等教育教务长，研究了教育环境中AI工具如ChatGPT的使用政策。结果显示，大多数机构缺乏针对这些工具的专门指南，高中比高等教育机构更不倾向于制定政策。现有政策往往忽略关键问题，包括学生隐私和algorithmic transparency。研究强调了建立灵活、迭代的政策框架的紧迫性，以保障学生安全和减少剽窃风险。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15601v1",
      "published_date": "2024-03-22 20:07:58 UTC",
      "updated_date": "2024-03-22 20:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:52:22.732117"
    },
    {
      "arxiv_id": "2403.15600v1",
      "title": "Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers",
      "title_zh": "只是又一次复制粘贴？ ",
      "authors": [
        "Sivana Hamer",
        "Marcelo d'Amorim",
        "Laurie Williams"
      ],
      "abstract": "Sonatype's 2023 report found that 97% of developers and security leads\nintegrate generative Artificial Intelligence (AI), particularly Large Language\nModels (LLMs), into their development process. Concerns about the security\nimplications of this trend have been raised. Developers are now weighing the\nbenefits and risks of LLMs against other relied-upon information sources, such\nas StackOverflow (SO), requiring empirical data to inform their choice. In this\nwork, our goal is to raise software developers awareness of the security\nimplications when selecting code snippets by empirically comparing the\nvulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an\nexisting Java dataset from SO with security-related questions and answers.\nThen, we asked ChatGPT the same SO questions, gathering the generated code for\ncomparison. After curating the dataset, we analyzed the number and types of\nCommon Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each\nplatform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities\ncompared to the 302 vulnerabilities found in SO snippets, producing 20% fewer\nvulnerabilities with a statistically significant difference. Additionally,\nChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings\nsuggest developers are under-educated on insecure code propagation from both\nplatforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code\ncopied and pasted, created by AI or humans, cannot be trusted blindly,\nrequiring good software engineering practices to reduce risk. Future work can\nhelp minimize insecure code propagation from any platform.",
      "tldr_zh": "本文研究比较了ChatGPT生成代码与StackOverflow (SO)答案的安全漏洞，旨在帮助开发者评估AI代码的风险。研究者使用现有Java数据集，向ChatGPT提出相同的SO安全相关问题，并通过CodeQL分析108个代码片段，结果显示ChatGPT代码含有248个Common Weakness Enumeration (CWE)漏洞，比SO的302个漏洞少20%，且CWE类型从SO的22种减少到19种。尽管ChatGPT表现更好，但总体发现274个独特漏洞和25种CWE类型，强调开发人员不应盲目复制任何平台代码，并需采用良好软件工程实践以降低风险。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 2 figures, accepted at Deep Learning Security and Privacy\n  Workshop (DLSP) part of IEEE Symposium on Security and Privacy Workshops\n  (SPW) for 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15600v1",
      "published_date": "2024-03-22 20:06:41 UTC",
      "updated_date": "2024-03-22 20:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:52:36.606436"
    },
    {
      "arxiv_id": "2403.15587v1",
      "title": "Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Cristina Zuheros",
        "David Herrera-Poyatos",
        "Rosana Montes",
        "Francisco Herrera"
      ],
      "abstract": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLMs)，特别是 ChatGPT，通过提示设计策略来辅助众包决策(CDM)，以从社交媒体文本中提取意见并进行决策分析。研究将 ChatGPT 整合为一个灵活工具，提供数值或语言评估，并应用于多标准决策场景中，使用类别本体(category ontology)定义标准，同时探索 ChatGPT 作为端到端的 CDM 模型。实验基于 TripAdvisor 的真实数据（TripR-2020Large 数据集）进行，结果显示这种方法在开发高质量决策模型方面具有广阔前景。论文还讨论了 LLMs 在 CDM 中的挑战，包括一致性、敏感性(sensitivity)和可解释性，并提出了未来研究的开放问题。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15587v1",
      "published_date": "2024-03-22 19:21:44 UTC",
      "updated_date": "2024-03-22 19:21:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:52:48.351575"
    },
    {
      "arxiv_id": "2403.15586v1",
      "title": "Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",
      "title_zh": "生成式人工智能在教育中：教育者意识、情感及影响因素的研究",
      "authors": [
        "Aashish Ghimire",
        "James Prather",
        "John Edwards"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) and the expanding\nintegration of large language models (LLMs) have ignited a debate about their\napplication in education. This study delves into university instructors'\nexperiences and attitudes toward AI language models, filling a gap in the\nliterature by analyzing educators' perspectives on AI's role in the classroom\nand its potential impacts on teaching and learning. The objective of this\nresearch is to investigate the level of awareness, overall sentiment\ntowardsadoption, and the factors influencing these attitudes for LLMs and\ngenerative AI-based tools in higher education. Data was collected through a\nsurvey using a Likert scale, which was complemented by follow-up interviews to\ngain a more nuanced understanding of the instructors' viewpoints. The collected\ndata was processed using statistical and thematic analysis techniques. Our\nfindings reveal that educators are increasingly aware of and generally positive\ntowards these tools. We find no correlation between teaching style and attitude\ntoward generative AI. Finally, while CS educators show far more confidence in\ntheir technical understanding of generative AI tools and more positivity\ntowards them than educators in other fields, they show no more confidence in\ntheir ability to detect AI-generated work.",
      "tldr_zh": "本研究探讨了教育工作者对生成式 AI 在教育中的应用，包括其认识水平、情感态度以及影响因素，旨在填补文献中对教师视角的空白。研究采用 Likert 量表调查结合后续访谈的方法，并通过统计和主题分析处理数据。结果显示，教育工作者对生成式 AI 和大型语言模型(LLMs)的认识日益增强，且总体持正面态度；教学风格与对这些工具的态度无关；计算机科学(CS)教育工作者在技术理解和情感上更自信，但对检测 AI 生成内容的能力没有显著优势。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15586v1",
      "published_date": "2024-03-22 19:21:29 UTC",
      "updated_date": "2024-03-22 19:21:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:53:01.038468"
    },
    {
      "arxiv_id": "2403.15585v4",
      "title": "MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Mai A. Shaaban",
        "Adnan Khan",
        "Mohammad Yaqub"
      ],
      "abstract": "Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces MedPromptX, the first clinical decision support\nsystem that integrates multimodal large language models (MLLMs), few-shot\nprompting (FP) and visual grounding (VG) to combine imagery with EHR data for\nchest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing\nEHR information, providing a comprehensive understanding of patients' medical\nhistory. Additionally, FP reduces the necessity for extensive training of MLLMs\nwhile effectively tackling the issue of hallucination. Nevertheless, the\nprocess of determining the optimal number of few-shot examples and selecting\nhigh-quality candidates can be burdensome, yet it profoundly influences model\nperformance. Hence, we propose a new technique that dynamically refines\nfew-shot data for real-time adjustment to new patient scenarios. Moreover, VG\nnarrows the search area in X-ray images, thereby enhancing the identification\nof abnormalities. We also release MedPromptX-VQA, a new in-context visual\nquestion answering dataset encompassing interleaved images and EHR data derived\nfrom MIMIC-IV and MIMIC-CXR-JPG databases. Results demonstrate the SOTA\nperformance of MedPromptX, achieving an 11% improvement in F1-score compared to\nthe baselines. Code and data are publicly available on\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX.",
      "tldr_zh": "本研究提出MedPromptX，一种基于多模态大语言模型(MLLMs)、少样本提示(FP)和视觉定位(VG)的临床决策支持系统，用于整合Chest X-ray图像和电子健康记录(EHR)数据，以解决EHR不完整性带来的诊断挑战。MedPromptX利用预训练MLLMs补充缺失的EHR信息，并引入动态优化少样本数据的技术来减少模型幻觉，同时VG缩小X-ray图像的搜索区域以提升异常识别准确性。该系统还发布了新数据集MedPromptX-VQA，基于MIMIC-IV和MIMIC-CXR-JPG数据库，实验结果显示MedPromptX在F1-score上比基线模型提高了11%，实现了SOTA性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15585v4",
      "published_date": "2024-03-22 19:19:51 UTC",
      "updated_date": "2025-01-27 18:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:53:16.280888"
    },
    {
      "arxiv_id": "2403.15577v1",
      "title": "Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control",
      "title_zh": "自动驾驶中的感知不确定性：基于深度集成的自适应巡航控制",
      "authors": [
        "Xiao Li",
        "H. Eric Tseng",
        "Anouck Girard",
        "Ilya Kolmanovsky"
      ],
      "abstract": "Autonomous driving depends on perception systems to understand the\nenvironment and to inform downstream decision-making. While advanced perception\nsystems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like\ncomprehension, their unpredictable behavior and lack of interpretability may\nhinder their deployment in safety critical scenarios. In this paper, we develop\nan Ensemble of DNN regressors (Deep Ensemble) that generates predictions with\nquantification of prediction uncertainties. In the scenario of Adaptive Cruise\nControl (ACC), we employ the Deep Ensemble to estimate distance headway to the\nlead vehicle from RGB images and enable the downstream controller to account\nfor the estimation uncertainty. We develop an adaptive cruise controller that\nutilizes Stochastic Model Predictive Control (MPC) with chance constraints to\nprovide a probabilistic safety guarantee. We evaluate our ACC algorithm using a\nhigh-fidelity traffic simulator and a real-world traffic dataset and\ndemonstrate the ability of the proposed approach to effect speed tracking and\ncar following while maintaining a safe distance headway. The\nout-of-distribution scenarios are also examined.",
      "tldr_zh": "该研究针对自动驾驶中感知系统的不可预测性，开发了一种基于深度神经网络集合（Deep Ensemble）的框架，用于量化预测不确定性。在自适应巡航控制（ACC）场景中，该框架从 RGB 图像估计领先车辆的距离头way，并将不确定性融入下游控制器中，使用随机模型预测控制（Stochastic MPC）结合机会约束（chance constraints）来提供概率安全保证。实验通过高保真交通模拟器和真实交通数据集验证，证明该方法能有效实现速度跟踪和跟车，同时保持安全距离，并在分布外（out-of-distribution）场景中表现出色。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15577v1",
      "published_date": "2024-03-22 19:04:58 UTC",
      "updated_date": "2024-03-22 19:04:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:53:27.328835"
    },
    {
      "arxiv_id": "2403.15574v1",
      "title": "SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Xia",
        "Qingqing Zhao",
        "Yunfei Long",
        "Ge Xu",
        "Jia Wang"
      ],
      "abstract": "In traditional research approaches, sensory perception and emotion\nclassification have traditionally been considered separate domains. Yet, the\nsignificant influence of sensory experiences on emotional responses is\nundeniable. The natural language processing (NLP) community has often missed\nthe opportunity to merge sensory knowledge with emotion classification. To\naddress this gap, we propose SensoryT5, a neuro-cognitive approach that\nintegrates sensory information into the T5 (Text-to-Text Transfer Transformer)\nmodel, designed specifically for fine-grained emotion classification. This\nmethodology incorporates sensory cues into the T5's attention mechanism,\nenabling a harmonious balance between contextual understanding and sensory\nawareness. The resulting model amplifies the richness of emotional\nrepresentations. In rigorous tests across various detailed emotion\nclassification datasets, SensoryT5 showcases improved performance, surpassing\nboth the foundational T5 model and current state-of-the-art works. Notably,\nSensoryT5's success signifies a pivotal change in the NLP domain, highlighting\nthe potential influence of neuro-cognitive data in refining machine learning\nmodels' emotional sensitivity.",
      "tldr_zh": "论文提出 SensoryT5，这是一种将 Sensorimotor Norms 融入 T5 模型的神经认知方法，旨在提升细粒度情感分类的性能。SensoryT5 通过将感官线索整合到 T5 的注意力机制中，实现上下文理解与感官感知的平衡，从而丰富情感表示。在多个详细情感分类数据集上的测试中，该模型超过了基础 T5 模型和当前最先进的工作，标志着 NLP 领域的一个关键进展，突出了神经认知数据在提升机器学习模型情感敏感性方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by CogALex 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2403.15574v1",
      "published_date": "2024-03-22 19:03:25 UTC",
      "updated_date": "2024-03-22 19:03:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:53:40.120545"
    },
    {
      "arxiv_id": "2403.15559v2",
      "title": "An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyi Zhao",
        "Chen Song",
        "Xiaodong Gu",
        "Yuan Dong",
        "Qi Zuo",
        "Weihao Yuan",
        "Liefeng Bo",
        "Zilong Dong",
        "Qixing Huang"
      ],
      "abstract": "A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.",
      "tldr_zh": "该论文提出一个优化框架，用于确保使用预训练文本到图像模型纹理 3D 网格时实现多视图一致性，解决扩散模型中平均操作导致的模糊和局部特征不一致问题。\n框架分为四个阶段：首先，通过 MV-consistent 扩散过程生成过多的 2D 纹理；其次，解决半定规划以选择互一致且覆盖 3D 模型的视图子集；第三，进行非刚性对齐以对齐重叠区域；第四，解决 MRF 问题将每个网格面与选定视图关联，且第三和第四阶段迭代优化。\n实验结果表明，该方法在定性和定量上显著优于基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15559v2",
      "published_date": "2024-03-22 18:28:04 UTC",
      "updated_date": "2024-08-02 10:19:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:53:54.278066"
    },
    {
      "arxiv_id": "2403.15551v1",
      "title": "Language-Based Depth Hints for Monocular Depth Estimation",
      "title_zh": "基于语言的深度提示用于单目深度估计",
      "authors": [
        "Dylan Auty",
        "Krystian Mikolajczyk"
      ],
      "abstract": "Monocular depth estimation (MDE) is inherently ambiguous, as a given image\nmay result from many different 3D scenes and vice versa. To resolve this\nambiguity, an MDE system must make assumptions about the most likely 3D scenes\nfor a given input. These assumptions can be either explicit or implicit. In\nthis work, we demonstrate the use of natural language as a source of an\nexplicit prior about the structure of the world. The assumption is made that\nhuman language encodes the likely distribution in depth-space of various\nobjects. We first show that a language model encodes this implicit bias during\ntraining, and that it can be extracted using a very simple learned approach. We\nthen show that this prediction can be provided as an explicit source of\nassumption to an MDE system, using an off-the-shelf instance segmentation model\nthat provides the labels used as the input to the language model. We\ndemonstrate the performance of our method on the NYUD2 dataset, showing\nimprovement compared to the baseline and to random controls.",
      "tldr_zh": "本文提出了一种基于语言的深度提示方法，用于解决单目深度估计（Monocular Depth Estimation, MDE）的固有歧义问题，通过假设人类语言编码了物体在深度空间的分布作为显性先验。研究首先提取语言模型中的隐性偏差，使用一个简单学习方法并结合现成的实例分割模型，将语言预测作为输入提供给 MDE 系统。实验在 NYUD2 数据集上验证了该方法的有效性，与基线模型和随机控制相比，显著提高了深度估计性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 1 figure. Work originally done in June 2022",
      "pdf_url": "http://arxiv.org/pdf/2403.15551v1",
      "published_date": "2024-03-22 18:05:33 UTC",
      "updated_date": "2024-03-22 18:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:54:04.730221"
    },
    {
      "arxiv_id": "2403.15388v5",
      "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
      "title_zh": "LLaVA-PruMerge：自适应标记减少用于高效的大型多模态模型",
      "authors": [
        "Yuzhang Shang",
        "Mu Cai",
        "Bingxin Xu",
        "Yong Jae Lee",
        "Yan Yan"
      ],
      "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning\ncapabilities by connecting a visual encoder and a large language model. LMMs\ntypically take in a fixed and large amount of visual tokens, such as the\npenultimate layer features in the CLIP visual encoder, as the prefix content.\nRecent LMMs incorporate more complex visual inputs, such as high-resolution\nimages and videos, which further increases the number of visual tokens\nsignificantly. However, due to the inherent design of the Transformer\narchitecture, the computational costs of these models tend to increase\nquadratically with the number of input tokens. To tackle this problem, we\nexplore a token reduction mechanism that identifies significant spatial\nredundancy among visual tokens. In response, we propose PruMerge, a novel\nadaptive visual token reduction strategy that significantly reduces the number\nof visual tokens without compromising the performance of LMMs. Specifically, to\nmetric the importance of each token, we exploit the sparsity observed in the\nvisual encoder, characterized by the sparse distribution of attention scores\nbetween the class token and visual tokens. This sparsity enables us to\ndynamically select the most crucial visual tokens to retain. Subsequently, we\ncluster the selected (unpruned) tokens based on their key similarity and merge\nthem with the unpruned tokens, effectively supplementing and enhancing their\ninformational content. Empirically, when applied to LLaVA-1.5, our approach can\ncompress the visual tokens by 14 times on average, and achieve comparable\nperformance across diverse visual question-answering and reasoning tasks. Code\nand checkpoints are at https://llava-prumerge.github.io/.",
      "tldr_zh": "本文提出PruMerge，一种自适应视觉标记减少策略，旨在解决Large Multimodal Models (LMMs)处理复杂视觉输入（如高分辨率图像和视频）时，标记数量增加导致Transformer架构计算成本急剧上升的问题。PruMerge利用视觉编码器的稀疏性，通过注意力分数动态选择重要标记，并基于关键相似性聚类和合并这些标记，从而显著减少标记数量而不影响模型性能。在LLaVA-1.5上，该方法平均将视觉标记压缩14倍，并在多种视觉问答和推理任务中实现可比表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://llava-prumerge.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.15388v5",
      "published_date": "2024-03-22 17:59:52 UTC",
      "updated_date": "2024-05-22 20:50:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:54:15.245206"
    },
    {
      "arxiv_id": "2403.15385v1",
      "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Xie",
        "Jonathan Lorraine",
        "Tianshi Cao",
        "Jun Gao",
        "James Lucas",
        "Antonio Torralba",
        "Sanja Fidler",
        "Xiaohui Zeng"
      ],
      "abstract": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
      "tldr_zh": "该论文提出LATTE3D，一种大规模Amortized文本到增强3D合成方法，旨在解决现有文本到3D生成效率低、细节捕捉不足和扩展性差的问题。LATTE3D通过构建可扩展架构并利用3D数据，包括3D-aware diffusion priors、shape regularization和模型初始化，确保在处理多样化复杂提示集时实现鲁棒性。关键创新是同时amortize neural field和textured surface生成，允许在单次前向传播中快速产出高度详细的纹理网格，仅需400ms。实验结果显示，LATTE3D显著提升生成速度和质量，并支持进一步的测试时优化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "68T45",
        "I.2.6; I.2.7; I.3.6; I.3.7"
      ],
      "primary_category": "cs.CV",
      "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/",
      "pdf_url": "http://arxiv.org/pdf/2403.15385v1",
      "published_date": "2024-03-22 17:59:37 UTC",
      "updated_date": "2024-03-22 17:59:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:54:29.037908"
    },
    {
      "arxiv_id": "2403.15371v3",
      "title": "Can large language models explore in-context?",
      "title_zh": "翻译失败",
      "authors": [
        "Akshay Krishnamurthy",
        "Keegan Harris",
        "Dylan J. Foster",
        "Cyril Zhang",
        "Aleksandrs Slivkins"
      ],
      "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
      "tldr_zh": "本研究调查了大语言模型（LLMs）是否能在上下文中进行探索，这是强化学习和决策中的核心能力，使用GPT-3.5、GPT-4和Llama2作为代理在多臂老虎机(multi-armed bandit)环境中进行测试，所有环境描述和交互历史均通过提示提供。结果显示，只有GPT-4结合chain-of-thought reasoning和外部总结的交互历史（作为充分统计量呈现）才能实现稳健的探索行为，而其他配置均表现不佳。研究结论是，外部总结至关重要，但可能在复杂场景中不可行，因此可能需要非微不足道的算法干预，如微调或数据集整理，来提升LLM作为决策代理的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024. This version: added references to related\n  and concurrent work",
      "pdf_url": "http://arxiv.org/pdf/2403.15371v3",
      "published_date": "2024-03-22 17:50:43 UTC",
      "updated_date": "2024-10-28 19:55:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:54:38.999970"
    },
    {
      "arxiv_id": "2403.15529v2",
      "title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
      "title_zh": "翻译失败",
      "authors": [
        "Abdur Rahman Bin Md Faizullah",
        "Ashok Urlana",
        "Rahul Mishra"
      ],
      "abstract": "Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called \\textbf{\\textit{LimGen}}, encompassing 4068 research papers and\ntheir associated limitations from the ACL anthology. We investigate several\napproaches to harness large language models (LLMs) for producing suggestive\nlimitations, by thoroughly examining the related challenges, practical\ninsights, and potential opportunities. Our LimGen dataset and code can be\naccessed at \\url{https://github.com/arbmf/LimGen}.",
      "tldr_zh": "本研究提出了一种新任务Suggestive Limitation Generation (SLG)，旨在利用大型语言模型 (LLMs) 生成研究论文的建议性局限性，以帮助学术审查过程识别研究的不足和改进方向。研究者编译了名为LimGen的数据集，包含来自ACL anthology的4068篇论文及其相关局限性，并探讨了多种基于LLMs的方法，包括面临的挑战、实际见解和潜在机会。通过这项工作，LimGen数据集和代码已公开可用（https://github.com/arbmf/LimGen），为未来研究提供宝贵资源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ECML-PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15529v2",
      "published_date": "2024-03-22 17:31:43 UTC",
      "updated_date": "2024-06-14 11:19:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:54:50.719766"
    },
    {
      "arxiv_id": "2403.15528v3",
      "title": "Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs",
      "title_zh": "翻译失败",
      "authors": [
        "Yiliang Zhou",
        "Hanley Ong",
        "Patrick Kennedy",
        "Carol Wu",
        "Jacob Kazam",
        "Keith Hentel",
        "Adam Flanders",
        "George Shih",
        "Yifan Peng"
      ],
      "abstract": "The study examines the application of GPT-4V, a multi-modal large language\nmodel equipped with visual recognition, in detecting radiological findings from\na set of 100 chest radiographs and suggests that GPT-4V is currently not ready\nfor real-world diagnostic usage in interpreting chest radiographs.",
      "tldr_zh": "该研究评估了 GPT-4V（一个多模态大语言模型，配备视觉识别能力）在检测胸部 X 光片放射学发现方面的表现，通过分析 100 张样本。\n结果表明，GPT-4V 在解释胸部 X 光片时存在显著局限性，目前不适合用于真实世界的诊断应用。\n这项工作突出了 AI 模型在医疗影像领域的挑战，为未来改进提供了重要见解。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15528v3",
      "published_date": "2024-03-22 17:27:18 UTC",
      "updated_date": "2024-05-13 03:40:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:55:04.384385"
    },
    {
      "arxiv_id": "2403.15362v2",
      "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Teehan",
        "Brenden Lake",
        "Mengye Ren"
      ],
      "abstract": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training. Code\nand data for our project can be found at\nhttps://college-concept-learning.github.io/",
      "tldr_zh": "本研究针对大型语言模型(Large Language Models)难以快速学习新概念的问题，提出了一种名为CoLLEGe的元学习(meta-learning)框架。该框架通过使用少量示例句子或定义生成灵活的概念嵌入(Concept Embedding)，以增强模型在后续句子中的下一词预测能力，同时与语言模型预训练兼容。实验结果显示，CoLLEGe在各种挑战性任务中表现出色，包括新词获取、定义推断和口头推理，无需任务特定训练，从而显著提升了概念学习的稳健性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15362v2",
      "published_date": "2024-03-22 17:26:05 UTC",
      "updated_date": "2024-10-16 19:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:55:19.712944"
    },
    {
      "arxiv_id": "2404.07220v2",
      "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
      "title_zh": "翻译失败",
      "authors": [
        "Kunal Sawarkar",
        "Abhilasha Mangal",
        "Shivam Raj Solanki"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\nprivate knowledge base of documents with Large Language Models (LLM) to build\nGenerative Q\\&A (Question-Answering) systems. However, RAG accuracy becomes\nincreasingly challenging as the corpus of documents scales up, with Retrievers\nplaying an outsized role in the overall RAG accuracy by extracting the most\nrelevant document from the corpus to provide context to the LLM. In this paper,\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\nquery strategies. Our study achieves better retrieval results and sets new\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\ndemonstrate far superior results on Generative Q\\&A datasets like SQUAD, even\nsurpassing fine-tuning performance.",
      "tldr_zh": "该论文提出“Blended RAG”方法，以提升RAG（Retriever-Augmented Generation）系统的准确性，通过整合语义搜索技术（如Dense Vector indexes和Sparse Encoder indexes）以及混合查询策略，优化检索器从大规模文档库中提取相关内容。相比传统方法，该方法在IR（Information Retrieval）数据集如NQ和TREC-COVID上设置了新基准。实验结果显示，将“Blended Retriever”应用于RAG系统后，在生成式问答数据集如SQUAD上取得了显著优于微调性能的成果。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Paper accepted by MIPR and presented at The 7th IEEE International\n  Conference on Multimedia Information. Processing and Retrieval (IEEE-MIPR\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.07220v2",
      "published_date": "2024-03-22 17:13:46 UTC",
      "updated_date": "2024-08-04 15:32:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:55:30.592202"
    },
    {
      "arxiv_id": "2403.15341v1",
      "title": "Collaborative AI Teaming in Unknown Environments via Active Goal Deduction",
      "title_zh": "翻译失败",
      "authors": [
        "Zuyuan Zhang",
        "Hanhan Zhou",
        "Mahdi Imani",
        "Taeyoung Lee",
        "Tian Lan"
      ],
      "abstract": "With the advancements of artificial intelligence (AI), we're seeing more\nscenarios that require AI to work closely with other agents, whose goals and\nstrategies might not be known beforehand. However, existing approaches for\ntraining collaborative agents often require defined and known reward signals\nand cannot address the problem of teaming with unknown agents that often have\nlatent objectives/rewards. In response to this challenge, we propose teaming\nwith unknown agents framework, which leverages kernel density Bayesian inverse\nlearning method for active goal deduction and utilizes pre-trained,\ngoal-conditioned policies to enable zero-shot policy adaptation. We prove that\nunbiased reward estimates in our framework are sufficient for optimal teaming\nwith unknown agents. We further evaluate the framework of redesigned\nmulti-agent particle and StarCraft II micromanagement environments with diverse\nunknown agents of different behaviors/rewards. Empirical results demonstrate\nthat our framework significantly advances the teaming performance of AI and\nunknown agents in a wide range of collaborative scenarios.",
      "tldr_zh": "该研究针对AI与未知目标代理的合作问题，提出一个teaming with unknown agents框架，利用kernel density Bayesian inverse learning method进行active goal deduction，并结合pre-trained, goal-conditioned policies实现zero-shot policy adaptation。该框架证明了unbiased reward estimates足以支持optimal teaming，并在重新设计的multi-agent particle和StarCraft II micromanagement环境中进行评估。实验结果显示，该框架显著提升了AI与未知代理在各种协作场景中的团队性能。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15341v1",
      "published_date": "2024-03-22 16:50:56 UTC",
      "updated_date": "2024-03-22 16:50:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:55:41.644765"
    },
    {
      "arxiv_id": "2403.15325v1",
      "title": "A Technological Perspective on Misuse of Available AI",
      "title_zh": "现有 AI 误用的技术视角",
      "authors": [
        "Lukas Pöhler",
        "Valentin Schrader",
        "Alexander Ladwein",
        "Florian von Keller"
      ],
      "abstract": "Potential malicious misuse of civilian artificial intelligence (AI) poses\nserious threats to security on a national and international level. Besides\ndefining autonomous systems from a technological viewpoint and explaining how\nAI development is characterized, we show how already existing and openly\navailable AI technology could be misused. To underline this, we developed three\nexemplary use cases of potentially misused AI that threaten political, digital\nand physical security. The use cases can be built from existing AI technologies\nand components from academia, the private sector and the developer-community.\nThis shows how freely available AI can be combined into autonomous weapon\nsystems. Based on the use cases, we deduce points of control and further\nmeasures to prevent the potential threat through misused AI. Further, we\npromote the consideration of malicious misuse of civilian AI systems in the\ndiscussion on autonomous weapon systems (AWS).",
      "tldr_zh": "本论文从技术视角探讨了现有AI技术的潜在恶意误用及其对国家与国际安全的威胁。作者定义了自治系统并解释了AI开发特点，展示了如何利用学术界、私营部门和开发者社区的公开AI组件构建自主武器系统。论文通过开发三个示例用例，演示了AI可能威胁政治、数字和物理安全的场景，并据此提出控制点和预防措施，建议在自治武器系统(AWS)讨论中纳入对AI误用的考量。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Presented at the UN Meeting of the Group of Governmental Experts on\n  Lethal Autonomous Weapons Systems, 30 August 2018",
      "pdf_url": "http://arxiv.org/pdf/2403.15325v1",
      "published_date": "2024-03-22 16:30:58 UTC",
      "updated_date": "2024-03-22 16:30:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:55:54.945825"
    },
    {
      "arxiv_id": "2403.15317v2",
      "title": "Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection",
      "title_zh": "Point-DETR3D：利用图像数据与空间点先验进行弱半监督3D物体检测",
      "authors": [
        "Hongzhi Gao",
        "Zheng Chen",
        "Zehui Chen",
        "Lin Chen",
        "Jiaming Liu",
        "Shanghang Zhang",
        "Feng Zhao"
      ],
      "abstract": "Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization. In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.",
      "tldr_zh": "这篇论文提出了Point-DETR3D框架，用于弱半监督3D物体检测（weakly semi-supervised 3D object detection），旨在通过点注释（point annotations）减少昂贵的7度自由度标注需求，同时利用空间点先验和图像数据提升检测性能。核心方法包括显式位置查询初始化策略（explicit positional query initialization）来编码3D先验、Cross-Modal Deformable RoI Fusion (D-RoI) 来整合密集图像数据以改善远处区域的伪标签质量，以及点引导自监督学习技术（point-guided self-supervised learning）来充分利用点先验。实验在nuScenes数据集上显示，Point-DETR3D相较于先前工作有显著改进，仅使用5%的标注数据即可达到完全监督模型的90%以上性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15317v2",
      "published_date": "2024-03-22 16:11:29 UTC",
      "updated_date": "2024-03-25 16:45:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:56:10.017927"
    },
    {
      "arxiv_id": "2403.15313v2",
      "title": "CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking",
      "title_zh": "CR3DT：相机-RADAR ",
      "authors": [
        "Nicolas Baumann",
        "Michael Baumgartner",
        "Edoardo Ghignone",
        "Jonas Kühne",
        "Tobias Fischer",
        "Yung-Hsu Yang",
        "Marc Pollefeys",
        "Michele Magno"
      ],
      "abstract": "To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.",
      "tldr_zh": "这篇论文提出了 CR3DT，一种摄像头和 RADAR 融合模型，用于自动驾驶车辆的 3D 对象检测和多对象跟踪 (MOT)，旨在解决 RADAR 数据稀疏和噪声问题，同时提升性能。\n该模型基于 State-of-the-Art 的 BEVDet 架构，整合 RADAR 的空间和速度信息，实现对周围物体的更准确检测和跟踪。\n实验结果显示，在 nuScenes 数据集上，CR3DT 比摄像头-only 方法提高了 5.3% 的 mean Average Precision (mAP) 和 14.9% 的 Average Multi-Object Tracking Accuracy (AMOTA)。\n总体上，该框架桥接了高性能与成本有效的感知系统，利用 RADAR 在汽车中的广泛应用，促进自主驾驶技术的进步。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15313v2",
      "published_date": "2024-03-22 16:06:05 UTC",
      "updated_date": "2024-08-06 15:58:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:56:20.009100"
    },
    {
      "arxiv_id": "2403.15304v3",
      "title": "Addressing Label Leakage in Knowledge Tracing Models",
      "title_zh": "解决知识追踪模型中的标签泄露",
      "authors": [
        "Yahya Badran",
        "Christine Preisach"
      ],
      "abstract": "Knowledge Tracing (KT) is concerned with predicting students' future\nperformance on learning items in intelligent tutoring systems. Learning items\nare tagged with skill labels called knowledge concepts (KCs). Many KT models\nexpand the sequence of item-student interactions into KC-student interactions\nby replacing learning items with their constituting KCs. This approach\naddresses the issue of sparse item-student interactions and minimises the\nnumber of model parameters. However, we identified a label leakage problem with\nthis approach. The model's ability to learn correlations between KCs belonging\nto the same item can result in the leakage of ground truth labels, which leads\nto decreased performance, particularly on datasets with a high number of KCs\nper item.\n  In this paper, we present methods to prevent label leakage in knowledge\ntracing (KT) models. Our model variants that utilize these methods consistently\noutperform their original counterparts. This further underscores the impact of\nlabel leakage on model performance. Additionally, these methods enhance the\noverall performance of KT models, with one model variant surpassing all tested\nbaselines on different benchmarks. Notably, our methods are versatile and can\nbe applied to a wide range of KT models.",
      "tldr_zh": "该研究探讨了知识追踪 (KT) 模型中的标签泄漏 (label leakage) 问题，该问题源于将学习项目扩展为知识概念 (KCs) 的互动，导致模型学习同一项目内 KCs 的相关性，并降低性能，尤其在每个项目有多个 KCs 的数据集上。论文提出多种方法来防止标签泄漏，这些方法被应用于 KT 模型变体中，使其在性能上一致优于原始模型。实验结果显示，这些改进不仅提升了整体模型表现，其中一个变体在不同基准上超越所有基线，且这些方法具有通用性，可应用于广泛的 KT 模型。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15304v3",
      "published_date": "2024-03-22 15:54:30 UTC",
      "updated_date": "2025-04-07 15:00:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:56:30.657862"
    },
    {
      "arxiv_id": "2403.15301v2",
      "title": "Planning with a Learned Policy Basis to Optimally Solve Complex Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Guillermo Infante",
        "David Kuric",
        "Anders Jonsson",
        "Vicenç Gómez",
        "Herke van Hoof"
      ],
      "abstract": "Conventional reinforcement learning (RL) methods can successfully solve a\nwide range of sequential decision problems. However, learning policies that can\ngeneralize predictably across multiple tasks in a setting with non-Markovian\nreward specifications is a challenging problem. We propose to use successor\nfeatures to learn a policy basis so that each (sub)policy in it solves a\nwell-defined subproblem. In a task described by a finite state automaton (FSA)\nthat involves the same set of subproblems, the combination of these\n(sub)policies can then be used to generate an optimal solution without\nadditional learning. In contrast to other methods that combine (sub)policies\nvia planning, our method asymptotically attains global optimality, even in\nstochastic environments.",
      "tldr_zh": "本文提出了一种基于后继特征（successor features）学习策略基础（policy basis）的方法，用于强化学习（RL）中优化解决复杂任务。该方法将每个子策略（subpolicy）设计为解决特定子问题，从而在由有限状态自动机（FSA）描述的任务中，通过组合这些子策略生成最优解，而无需额外学习。与其他方法相比，该方法在随机环境中渐进地实现全局最优性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15301v2",
      "published_date": "2024-03-22 15:51:39 UTC",
      "updated_date": "2024-06-03 14:56:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:56:42.056573"
    },
    {
      "arxiv_id": "2403.15297v4",
      "title": "Sphere Neural-Networks for Rational Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Tiansi Dong",
        "Mateja Jamnik",
        "Pietro Liò"
      ],
      "abstract": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination.",
      "tldr_zh": "本研究提出 Sphere Neural Networks (SphNNs)，一种将神经网络的计算构建块从向量扩展到球体的创新框架，旨在实现人类般的理性推理，超越传统统计范式。SphNNs 采用分层神经符号 Kolmogorov-Arnold 几何 GNN，通过神经符号转换映射处理邻域空间关系，能够在无需训练数据的情况下，一次 epoch 内验证长链三段论推理的有效性，计算复杂度为 O(N)。实验表明，SphNNs 可扩展到时空推理、逻辑推理等多种类型，并解决 Large Language Models (LLMs) 的幻觉问题，提升 AI 的可靠性和心理认知水平。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15297v4",
      "published_date": "2024-03-22 15:44:59 UTC",
      "updated_date": "2025-02-25 15:48:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:56:56.026888"
    },
    {
      "arxiv_id": "2403.15274v2",
      "title": "Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review",
      "title_zh": "翻译失败",
      "authors": [
        "Jinge Wang",
        "Zien Cheng",
        "Qiuming Yao",
        "Li Liu",
        "Dong Xu",
        "Gangqing Hu"
      ],
      "abstract": "The year 2023 marked a significant surge in the exploration of applying large\nlanguage model (LLM) chatbots, notably ChatGPT, across various disciplines. We\nsurveyed the applications of ChatGPT in bioinformatics and biomedical\ninformatics throughout the year, covering omics, genetics, biomedical text\nmining, drug discovery, biomedical image understanding, bioinformatics\nprogramming, and bioinformatics education. Our survey delineates the current\nstrengths and limitations of this chatbot in bioinformatics and offers insights\ninto potential avenues for future developments.",
      "tldr_zh": "本文回顾了2023年ChatGPT等大型语言模型(LLM)在生物信息学和生物医学信息学领域的应用，涵盖了基因组学、遗传学、生物医学文本挖掘、药物发现、生物医学图像理解、生物信息学编程以及教育等多个方面。通过调查分析，该研究总结了ChatGPT的当前优势，如高效处理文本和编程任务，以及其局限性，包括潜在的准确性问题，并提出了未来发展的潜在方向。",
      "categories": [
        "q-bio.OT",
        "cs.AI"
      ],
      "primary_category": "q-bio.OT",
      "comment": "Peer-reviewed and accepted by Quantitative Biology",
      "pdf_url": "http://arxiv.org/pdf/2403.15274v2",
      "published_date": "2024-03-22 15:16:23 UTC",
      "updated_date": "2024-06-12 15:50:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:57:05.987757"
    },
    {
      "arxiv_id": "2403.15257v1",
      "title": "Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks",
      "title_zh": "社交网络中级联预测的层次信息增强网络",
      "authors": [
        "Fanrui Zhang",
        "Jiawei Liu",
        "Qiang Zhang",
        "Xiaoling Zhu",
        "Zheng-Jun Zha"
      ],
      "abstract": "Understanding information cascades in networks is a fundamental issue in\nnumerous applications. Current researches often sample cascade information into\nseveral independent paths or subgraphs to learn a simple cascade\nrepresentation. However, these approaches fail to exploit the hierarchical\nsemantic associations between different modalities, limiting their predictive\nperformance. In this work, we propose a novel Hierarchical Information\nEnhancement Network (HIENet) for cascade prediction. Our approach integrates\nfundamental cascade sequence, user social graphs, and sub-cascade graph into a\nunified framework. Specifically, HIENet utilizes DeepWalk to sample cascades\ninformation into a series of sequences. It then gathers path information\nbetween users to extract the social relationships of propagators. Additionally,\nwe employ a time-stamped graph convolutional network to aggregate sub-cascade\ngraph information effectively. Ultimately, we introduce a Multi-modal Cascade\nTransformer to powerfully fuse these clues, providing a comprehensive\nunderstanding of cascading process. Extensive experiments have demonstrated the\neffectiveness of the proposed method.",
      "tldr_zh": "该论文针对社交网络中信息级联预测的问题，指出现有方法未能充分利用不同模态之间的层次语义关联，从而限制了预测性能。研究提出了一种新型 Hierarchical Information Enhancement Network (HIENet)，通过整合级联序列、用户社交图和子级联图，利用 DeepWalk 采样级联信息、提取用户间路径以获取传播者关系，并采用时间戳图卷积网络聚合子级联图信息，最后通过 Multi-modal Cascade Transformer 融合这些多模态线索，提供对级联过程的全面理解。实验结果显示，HIENet 在广泛测试中表现出色，证明了其有效性。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15257v1",
      "published_date": "2024-03-22 14:57:27 UTC",
      "updated_date": "2024-03-22 14:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:57:20.212672"
    },
    {
      "arxiv_id": "2403.15251v1",
      "title": "Safe Learning of PDDL Domains with Conditional Effects -- Extended Version",
      "title_zh": "带有条件",
      "authors": [
        "Argaman Mordoch",
        "Enrico Scala",
        "Roni Stern",
        "Brendan Juba"
      ],
      "abstract": "Powerful domain-independent planners have been developed to solve various\ntypes of planning problems. These planners often require a model of the acting\nagent's actions, given in some planning domain description language. Manually\ndesigning such an action model is a notoriously challenging task. An\nalternative is to automatically learn action models from observation. Such an\naction model is called safe if every plan created with it is consistent with\nthe real, unknown action model. Algorithms for learning such safe action models\nexist, yet they cannot handle domains with conditional or universal effects,\nwhich are common constructs in many planning problems. We prove that learning\nnon-trivial safe action models with conditional effects may require an\nexponential number of samples. Then, we identify reasonable assumptions under\nwhich such learning is tractable and propose SAM Learning of Conditional\nEffects (Conditional-SAM), the first algorithm capable of doing so. We analyze\nConditional-SAM theoretically and evaluate it experimentally. Our results show\nthat the action models learned by Conditional-SAM can be used to solve\nperfectly most of the test set problems in most of the experimented domains.",
      "tldr_zh": "本论文探讨了在PDDL领域中安全学习带有条件效果的行动模型的问题，强调手动设计模型的挑战性，并提出通过观察自动学习“安全”行动模型，即生成的计划与真实模型一致。研究证明，学习非平凡的带有条件效果的安全模型可能需要指数级样本，但作者识别了使学习变得可处理的合理假设。论文引入了Conditional-SAM算法，这是首个能处理此类任务的算法，通过理论分析和实验评估显示，该算法在大多数实验领域能够完美解决测试集问题。这些贡献为扩展PDDL规划器的适用性提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15251v1",
      "published_date": "2024-03-22 14:49:49 UTC",
      "updated_date": "2024-03-22 14:49:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:57:30.780044"
    },
    {
      "arxiv_id": "2403.15250v2",
      "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
      "title_zh": "大型语言模型中大规模评估结果的全面重新评估：一种多方面统计方法",
      "authors": [
        "Kun Sun",
        "Rong Wang",
        "Anders Søgaard"
      ],
      "abstract": "Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.",
      "tldr_zh": "这篇论文对大型语言模型 (LLMs) 的评估结果进行了全面重新评估，针对现有方法数据有限的问题，采用统一的评估框架和庞大数据集。研究引入多方面统计方法，包括 ANOVA、Tukey HSD tests、GAMM 和聚类技术，来分析缩放、训练类型和架构等因素对性能的影响。结果挑战了传统假设，如 LLMs 的涌现能力 (emergent abilities) 及其训练类型的影响，提供新视角理解模型的内在特性和发展轨迹。该研究为评估和优化 LLMs 性能提供了简单可靠的方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15250v2",
      "published_date": "2024-03-22 14:47:35 UTC",
      "updated_date": "2024-06-24 07:49:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:57:44.220161"
    },
    {
      "arxiv_id": "2403.15249v2",
      "title": "Spectral Motion Alignment for Video Motion Transfer using Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Geon Yeong Park",
        "Hyeonho Jeong",
        "Sang Wan Lee",
        "Jong Chul Ye"
      ],
      "abstract": "The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.",
      "tldr_zh": "本论文针对文本到视频扩散模型（text-to-video diffusion models, VDMs）在视频动作转移中的挑战，提出 Spectral Motion Alignment (SMA) 框架，以解决现有方法依赖帧级残差导致的全局动作上下文缺失和空间伪影问题。SMA 通过傅立叶变换（Fourier transforms）和小波变换（wavelet transforms）在频率域进行动作向量精炼和对齐，并引入频率域正则化（frequency-domain regularization）来学习整个帧的全局动作动态。实验结果显示，SMA 显著提升了动作转移的准确性和效率，同时保持与其他视频自定义框架的兼容性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025, Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/",
      "pdf_url": "http://arxiv.org/pdf/2403.15249v2",
      "published_date": "2024-03-22 14:47:18 UTC",
      "updated_date": "2024-12-19 05:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:57:55.631825"
    },
    {
      "arxiv_id": "2403.15248v1",
      "title": "Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Sudhir Sornapudi",
        "Rajhans Singh"
      ],
      "abstract": "Computer vision in agriculture is game-changing with its ability to transform\nfarming into a data-driven, precise, and sustainable industry. Deep learning\nhas empowered agriculture vision to analyze vast, complex visual data, but\nheavily rely on the availability of large annotated datasets. This remains a\nbottleneck as manual labeling is error-prone, time-consuming, and expensive.\nThe lack of efficient labeling approaches inspired us to consider\nself-supervised learning as a paradigm shift, learning meaningful feature\nrepresentations from raw agricultural image data. In this work, we explore how\nself-supervised representation learning unlocks the potential applicability to\ndiverse agriculture vision tasks by eliminating the need for large-scale\nannotated datasets. We propose a lightweight framework utilizing SimCLR, a\ncontrastive learning approach, to pre-train a ResNet-50 backbone on a large,\nunannotated dataset of real-world agriculture field images. Our experimental\nanalysis and results indicate that the model learns robust features applicable\nto a broad range of downstream agriculture tasks discussed in the paper.\nAdditionally, the reduced reliance on annotated data makes our approach more\ncost-effective and accessible, paving the way for broader adoption of computer\nvision in agriculture.",
      "tldr_zh": "本论文针对农业计算机视觉任务中对大量标注数据集的依赖问题，提出了一种基于自监督学习(self-supervised learning)的轻量级框架，以减少标注成本。框架利用 SimCLR（对比学习方法）预训练 ResNet-50 骨干网络，在一个大型未标注的真实农业图像数据集上学习鲁棒特征。实验结果表明，该模型适用于多种下游农业任务，提供高效的特征表示，并提升了计算机视觉在农业中的成本效益和可访问性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15248v1",
      "published_date": "2024-03-22 14:46:51 UTC",
      "updated_date": "2024-03-22 14:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:58:07.725765"
    },
    {
      "arxiv_id": "2403.15245v2",
      "title": "Reasoning-Enhanced Object-Centric Learning for Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Li",
        "Pu Ren",
        "Yang Liu",
        "Hao Sun"
      ],
      "abstract": "Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperimental results on various datasets indicate that the STATM module can\nsignificantly enhance the capabilities of multiple state-of-the-art\nobject-centric learning models for video. Moreover, as a predictive model, the\nSTATM module also performs well in downstream prediction and Visual Question\nAnswering (VQA) tasks. We will release our codes and data at\nhttps://github.com/intell-sci-comput/STATM.",
      "tldr_zh": "本研究针对object-centric learning在视频处理中的不足，提出了一种增强推理能力的框架，以提升机器学习系统对复杂视觉场景的理解和预测。论文设计了新型模块Slot-based Time-Space Transformer with Memory buffer (STATM)，它通过内存缓冲区存储slot信息，并利用基于slot的时空注意计算和融合机制，模拟人类intuitive physics进行对象跟踪和预测。实验结果显示，STATM显著提升了多种state-of-the-art object-centric learning模型在各种数据集上的性能，并在下游预测和Visual Question Answering (VQA)任务中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15245v2",
      "published_date": "2024-03-22 14:41:55 UTC",
      "updated_date": "2025-02-15 14:46:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:58:20.371058"
    },
    {
      "arxiv_id": "2403.15235v1",
      "title": "Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Qiang Zhang",
        "Jiawei Liu",
        "Fanrui Zhang",
        "Xiaoling Zhu",
        "Zheng-Jun Zha"
      ],
      "abstract": "Identifying key nodes in social networks plays a crucial role in timely\nblocking false information. Existing key node identification methods usually\nconsider node influence only from the propagation structure perspective and\nhave insufficient generalization ability to unknown scenarios. In this paper,\nwe propose a novel Multi-perspective Memory Enhanced Network (MMEN) for\nidentifying key nodes in social networks, which mines key nodes from multiple\nperspectives and utilizes memory networks to store historical information.\nSpecifically, MMEN first constructs two propagation networks from the\nperspectives of user attributes and propagation structure and updates node\nfeature representations using graph attention networks. Meanwhile, the memory\nnetwork is employed to store information of similar subgraphs, enhancing the\nmodel's generalization performance in unknown scenarios. Finally, MMEN applies\nadaptive weights to combine the node influence of the two propagation networks\nto select the ultimate key nodes. Extensive experiments demonstrate that our\nmethod significantly outperforms previous methods.",
      "tldr_zh": "本文提出了一种新型的 Multi-perspective Memory Enhanced Network (MMEN)，用于识别社交网络中的关键节点，以及时阻断虚假信息，该方法从用户属性和传播结构两个视角构建传播网络，并使用 graph attention networks 更新节点特征表示。MMEN 还引入 memory network 来存储类似子图的历史信息，从而提升模型在未知场景下的泛化性能。通过自适应权重结合两种网络的节点影响，最终选出关键节点。实验结果表明，该方法在多项测试中显著优于现有方法。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "7 pages, 1 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15235v1",
      "published_date": "2024-03-22 14:29:03 UTC",
      "updated_date": "2024-03-22 14:29:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:58:32.674748"
    },
    {
      "arxiv_id": "2403.15218v1",
      "title": "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Pranav Kulkarni",
        "Adway Kanhere",
        "Dharmam Savani",
        "Andrew Chan",
        "Devina Chatterjee",
        "Paul H. Yi",
        "Vishwa S. Parekh"
      ],
      "abstract": "Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).",
      "tldr_zh": "这篇论文探讨了 Segment Anything Model (SAM) 在众包医疗图像标注的可行性，旨在解决传统标注过程的劳动密集型问题。研究方法包括使用 SAM 从非专家获取“稀疏”标注，并将其转换为“密集”分割掩码，以训练 3D nnU-Net 模型。结果显示，SAM 生成的标注与地面实况标注的平均 Dice 分数较高，但基于这些标注训练的 nnU-Net 模型性能显著劣于使用地面实况标注的模型（p<0.001）。总体而言，该工作突出了 SAM 在简化标注流程的潜力，但也强调了其在实际深度学习训练中的局限性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15218v1",
      "published_date": "2024-03-22 14:07:07 UTC",
      "updated_date": "2024-03-22 14:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:58:45.216577"
    },
    {
      "arxiv_id": "2403.15216v1",
      "title": "(Un)making AI Magic: a Design Taxonomy",
      "title_zh": "翻译失败",
      "authors": [
        "Maria Luce Lupetti",
        "Dave Murray-Rust"
      ],
      "abstract": "This paper examines the role that enchantment plays in the design of AI\nthings by constructing a taxonomy of design approaches that increase or\ndecrease the perception of magic and enchantment. We start from the design\ndiscourse surrounding recent developments in AI technologies, highlighting\nspecific interaction qualities such as algorithmic uncertainties and errors and\narticulating relations to the rhetoric of magic and supernatural thinking.\nThrough analyzing and reflecting upon 52 students' design projects from two\neditions of a Master course in design and AI, we identify seven design\nprinciples and unpack the effects of each in terms of enchantment and\ndisenchantment. We conclude by articulating ways in which this taxonomy can be\napproached and appropriated by design/HCI practitioners, especially to support\nexploration and reflexivity.",
      "tldr_zh": "这篇论文构建了一个设计 taxonomy，以探讨 enchantment 在 AI 设计中的作用，分类了增加或减少 AI 事物魔力和魅力感的各种方法。作者通过分析 52 个学生设计项目的案例，从 AI 技术话语中提炼算法不确定性和错误等交互品质，并将其与魔力和超自然思维关联，识别出七个关键设计原则及其在 enchantment 和 disenchantment 方面的效果。该 taxonomy 可供设计和 HCI 从业者使用，支持他们进行探索和反思，以提升设计实践的批判性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15216v1",
      "published_date": "2024-03-22 14:03:37 UTC",
      "updated_date": "2024-03-22 14:03:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:58:55.563871"
    },
    {
      "arxiv_id": "2403.15523v2",
      "title": "Towards auditory attention decoding with noise-tagging: A pilot study",
      "title_zh": "迈向使用噪声标记的听觉注意力解码：一项初步研究",
      "authors": [
        "H. A. Scheppink",
        "S. Ahmadi",
        "P. Desain",
        "M. Tangermann",
        "J. Thielen"
      ],
      "abstract": "Auditory attention decoding (AAD) aims to extract from brain activity the\nattended speaker amidst candidate speakers, offering promising applications for\nneuro-steered hearing devices and brain-computer interfacing. This pilot study\nmakes a first step towards AAD using the noise-tagging stimulus protocol, which\nevokes reliable code-modulated evoked potentials, but is minimally explored in\nthe auditory modality. Participants were sequentially presented with two Dutch\nspeech stimuli that were amplitude-modulated with a unique binary pseudo-random\nnoise-code, effectively tagging these with additional decodable information. We\ncompared the decoding of unmodulated audio against audio modulated with various\nmodulation depths, and a conventional AAD method against a standard method to\ndecode noise-codes. Our pilot study revealed higher performances for the\nconventional method with 70 to 100 percent modulation depths compared to\nunmodulated audio. The noise-code decoder did not further improve these\nresults. These fundamental insights highlight the potential of integrating\nnoise-codes in speech to enhance auditory speaker detection when multiple\nspeakers are presented simultaneously.",
      "tldr_zh": "本研究探索了使用噪声标记(noise-tagging)刺激协议进行听觉注意力解码(AAD)，旨在从脑活动提取关注的说话者，以应用于神经导向听力设备和脑机接口。研究中，参与者聆听两段荷兰语语音刺激，这些刺激被独特的二进制伪随机噪声码进行幅度调制，并比较了未调制音频与不同调制深度的音频解码效果。结果显示，常规AAD方法在70%至100%的调制深度下，性能显著优于未调制音频，而噪声码解码器未进一步提升结果。这些发现突出了在多说话者环境中整合噪声码以增强听觉说话者检测的潜力，为未来脑-计算机接口技术奠定基础。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "q-bio.NC",
      "comment": "6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15523v2",
      "published_date": "2024-03-22 13:35:34 UTC",
      "updated_date": "2024-05-17 14:44:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:59:07.472622"
    },
    {
      "arxiv_id": "2403.15195v1",
      "title": "FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication",
      "title_zh": "FSD-Inference：完全无服务器分布式推理与可伸缩云通信",
      "authors": [
        "Joe Oakley",
        "Hakan Ferhatosmanoglu"
      ],
      "abstract": "Serverless computing offers attractive scalability, elasticity and\ncost-effectiveness. However, constraints on memory, CPU and function runtime\nhave hindered its adoption for data-intensive applications and machine learning\n(ML) workloads. Traditional 'server-ful' platforms enable distributed\ncomputation via fast networks and well-established inter-process communication\n(IPC) mechanisms such as MPI and shared memory. In the absence of such\nsolutions in the serverless domain, parallel computation with significant IPC\nrequirements is challenging. We present FSD-Inference, the first fully\nserverless and highly scalable system for distributed ML inference. We explore\npotential communication channels, in conjunction with Function-as-a-Service\n(FaaS) compute, to design a state-of-the-art solution for distributed ML within\nthe context of serverless data-intensive computing. We introduce novel fully\nserverless communication schemes for ML inference workloads, leveraging both\ncloud-based publish-subscribe/queueing and object storage offerings. We\ndemonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC\nwith comparable performance to object storage, while offering significantly\nreduced cost at high parallelism levels. We conduct in-depth experiments on\nbenchmark DNNs of various sizes. The results show that when compared to\nserver-based alternatives, FSD-Inference is significantly more cost-effective\nand scalable, and can even achieve competitive performance against optimized\nHPC solutions. Experiments also confirm that our serverless solution can handle\nlarge distributed workloads and leverage high degrees of FaaS parallelism.",
      "tldr_zh": "本研究提出 FSD-Inference，这是一个完全无服务器（fully serverless）的分布式机器学习（ML）推理系统，旨在解决无服务器计算在内存、CPU 和运行时限制下处理数据密集型工作负载的挑战。系统通过利用云端的发布-订阅/队列服务和对象存储作为通信方案，实现高效的 FaaS IPC，并证明这些方法在高并行性场景下提供与对象存储相当的性能，同时大幅降低成本。实验结果显示，FSD-Inference 在各种规模的基准 DNNs 上，比传统服务器方案更具成本效益和可扩展性，甚至能与优化后的 HPC 解决方案在性能上竞争，并有效处理大规模分布式工作负载。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "In Proceedings of 2024 IEEE 40th International Conference on Data\n  Engineering (ICDE) (to appear)",
      "pdf_url": "http://arxiv.org/pdf/2403.15195v1",
      "published_date": "2024-03-22 13:31:24 UTC",
      "updated_date": "2024-03-22 13:31:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:59:21.316188"
    },
    {
      "arxiv_id": "2403.15192v1",
      "title": "SFOD: Spiking Fusion Object Detector",
      "title_zh": "SFOD: 脉冲融合物体检测器",
      "authors": [
        "Yimeng Fan",
        "Wei Zhang",
        "Changsong Liu",
        "Mingyang Li",
        "Wenrui Lu"
      ],
      "abstract": "Event cameras, characterized by high temporal resolution, high dynamic range,\nlow power consumption, and high pixel bandwidth, offer unique capabilities for\nobject detection in specialized contexts. Despite these advantages, the\ninherent sparsity and asynchrony of event data pose challenges to existing\nobject detection algorithms. Spiking Neural Networks (SNNs), inspired by the\nway the human brain codes and processes information, offer a potential solution\nto these difficulties. However, their performance in object detection using\nevent cameras is limited in current implementations. In this paper, we propose\nthe Spiking Fusion Object Detector (SFOD), a simple and efficient approach to\nSNN-based object detection. Specifically, we design a Spiking Fusion Module,\nachieving the first-time fusion of feature maps from different scales in SNNs\napplied to event cameras. Additionally, through integrating our analysis and\nexperiments conducted during the pretraining of the backbone network on the\nNCAR dataset, we delve deeply into the impact of spiking decoding strategies\nand loss functions on model performance. Thereby, we establish state-of-the-art\nclassification results based on SNNs, achieving 93.7\\% accuracy on the NCAR\ndataset. Experimental results on the GEN1 detection dataset demonstrate that\nthe SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing\nSNN-based approaches. Our research not only underscores the potential of SNNs\nin object detection with event cameras but also propels the advancement of\nSNNs. Code is available at https://github.com/yimeng-fan/SFOD.",
      "tldr_zh": "该论文提出SFOD（Spiking Fusion Object Detector），一种基于Spiking Neural Networks (SNNs)的简单高效物体检测方法，旨在解决事件相机（event cameras）固有稀疏性和异步性对检测算法的挑战。核心创新是设计Spiking Fusion Module，实现SNN中不同规模特征图的首次融合，以提升事件相机下的物体检测性能。研究还分析了spiking decoding strategies和loss functions对模型的影响，并在NCAR数据集上达到93.7%的state-of-the-art分类准确率，以及在GEN1 detection dataset上取得32.1%的mAP，优于现有SNN方法。该工作突显了SNNs在事件相机物体检测中的潜力，并推动了SNNs的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15192v1",
      "published_date": "2024-03-22 13:24:50 UTC",
      "updated_date": "2024-03-22 13:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:59:33.139158"
    },
    {
      "arxiv_id": "2403.15176v3",
      "title": "Brain-aligning of semantic vectors improves neural decoding of visual stimuli",
      "title_zh": "翻译失败",
      "authors": [
        "Shirin Vafaei",
        "Ryohei Fukuma",
        "Takufumi Yanagisawa",
        "Huixiang Yang",
        "Satoru Oshino",
        "Naoki Tani",
        "Hui Ming Khoo",
        "Hidenori Sugano",
        "Yasushi Iimura",
        "Hiroharu Suzuki",
        "Madoka Nakajima",
        "Kentaro Tamura",
        "Haruhiko Kishima"
      ],
      "abstract": "The development of algorithms to accurately decode of neural information is a\nlong-standing effort in the field of neuroscience. Brain decoding is typically\nemployed by training machine learning models to map neural data onto a\npreestablished vector representation of stimulus features. These vectors are\nusually derived from image- and/or text-based feature spaces. Nonetheless, the\nintrinsic characteristics of these vectors might be fundamentally different\nthan those encoded by the brain, limiting the ability of algorithms to\naccurately learn this mapping. To address this issue, here, we propose a\nrepresentation learning framework, called brain-aligning of semantic vectors,\nthat fine-tunes pretrained feature vectors to better align with the structure\nof neural representations of visual stimuli in the human brain. We trained this\nmodel with functional magnetic resonance imaging (fMRI) data representing 150\nvisual stimulus categories; then, we performed zero-shot brain decoding on 1)\nfMRI, 2) magnetoencephalography (MEG), and 3) electrocorticography (ECoG) data\nreflecting neural representations of visual stimuli. By using fMRI-based\nbrain-aligned vectors, the zero-shot decoding accuracy all three neuroimaging\ndatasets increased. This finding underscores the potential of leveraging a\nricher array of brainderived features to increase the performance of brain\ndecoding algorithms.",
      "tldr_zh": "本研究提出了一种名为“brain-aligning of semantic vectors”的表示学习框架，通过微调预训练的特征向量，使其更好地与大脑对视觉刺激的神经表示对齐，从而提升脑解码算法的性能。研究使用fMRI数据训练模型，基于150个视觉刺激类别进行处理，并随后在零样本设置下测试fMRI、MEG和ECoG数据集。结果显示，使用fMRI-based brain-aligned vectors后，三种神经影像数据的解码准确率均有所提高，突显了利用脑源特征增强脑解码算法潜力的重要性。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "40 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15176v3",
      "published_date": "2024-03-22 13:01:10 UTC",
      "updated_date": "2024-09-12 09:35:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:59:45.767981"
    },
    {
      "arxiv_id": "2406.11843v1",
      "title": "Explanation Hacking: The perils of algorithmic recourse",
      "title_zh": "翻译失败",
      "authors": [
        "Emily Sullivan",
        "Atoosa Kasirzadeh"
      ],
      "abstract": "We argue that the trend toward providing users with feasible and actionable\nexplanations of AI decisions, known as recourse explanations, comes with\nethical downsides. Specifically, we argue that recourse explanations face\nseveral conceptual pitfalls and can lead to problematic explanation hacking,\nwhich undermines their ethical status. As an alternative, we advocate that\nexplanations of AI decisions should aim at understanding.",
      "tldr_zh": "这篇论文探讨了 algorithmic recourse 的潜在风险，特别指出提供可操作的 recourse explanations 可能导致 explanation hacking，从而破坏这些解释的道德基础。作者分析了 recourse explanations 面临的概念缺陷，以及它们如何加剧 AI 决策解释的伦理问题。作为替代方案，论文主张 AI 解释应以促进理解为目标，而不是提供可行行动建议。总的来说，这为 AI 解释设计提供了重要警示，推动更负责任的实践。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.11843v1",
      "published_date": "2024-03-22 12:49:28 UTC",
      "updated_date": "2024-03-22 12:49:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T17:59:56.673070"
    },
    {
      "arxiv_id": "2403.15170v1",
      "title": "Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders",
      "title_zh": "探索自监督学习的任务无关特性在检测精神障碍的背景下",
      "authors": [
        "Rohan Kumar Gupta",
        "Rohit Sinha"
      ],
      "abstract": "Self-supervised learning (SSL) has been investigated to generate\ntask-agnostic representations across various domains. However, such\ninvestigation has not been conducted for detecting multiple mental disorders.\nThe rationale behind the existence of a task-agnostic representation lies in\nthe overlapping symptoms among multiple mental disorders. Consequently, the\nbehavioural data collected for mental health assessment may carry a mixed bag\nof attributes related to multiple disorders. Motivated by that, in this study,\nwe explore a task-agnostic representation derived through SSL in the context of\ndetecting major depressive disorder (MDD) and post-traumatic stress disorder\n(PTSD) using audio and video data collected during interactive sessions. This\nstudy employs SSL models trained by predicting multiple fixed targets or masked\nframes. We propose a list of fixed targets to make the generated representation\nmore efficient for detecting MDD and PTSD. Furthermore, we modify the\nhyper-parameters of the SSL encoder predicting fixed targets to generate global\nrepresentations that capture varying temporal contexts. Both these innovations\nare noted to yield improved detection performances for considered mental\ndisorders and exhibit task-agnostic traits. In the context of the SSL model\npredicting masked frames, the generated global representations are also noted\nto exhibit task-agnostic traits.",
      "tldr_zh": "本研究探索了自监督学习 (SSL) 在检测心理障碍中的任务无关特性，基于多种障碍（如重度抑郁症 (MDD) 和创伤后应激障碍 (PTSD)）的症状重叠，旨在从音频和视频互动数据中提取通用表示。研究者提出了一种改进的 SSL 方法，包括预测多个固定目标或屏蔽帧，并优化固定目标列表和编码器超参数，以捕捉不同时间上下文，从而提升检测性能。实验结果显示，这些创新显著提高了 MDD 和 PTSD 的检测准确性，并证明了生成的表示具有任务无关特性，为心理障碍检测提供了更高效的通用框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15170v1",
      "published_date": "2024-03-22 12:46:58 UTC",
      "updated_date": "2024-03-22 12:46:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:00:08.933802"
    },
    {
      "arxiv_id": "2403.15167v1",
      "title": "Transition Graph Properties of Target Class Classification",
      "title_zh": "目标类分类的转移图性质",
      "authors": [
        "Levon Aslanyan",
        "Hasmik Sahakyan"
      ],
      "abstract": "Target class classification is a mixed classification and transition model\nwhose integrated goal is to assign objects to a certain, so called target or\nnormal class. The classification process is iterative, and in each step an\nobject in a certain class undergoes an action attached to that class,\ninitiating the transition of the object to one of the classes. The sequence of\ntransitions, which we call class transitions, must be designed to provide the\nfinal assignment of objects to the target class. The transition process can be\ndescribed in the form of a directed graph, and the success of the final\nclassification is mainly due to the properties of this graph. In our previous\nresearch we showed that the desirable structure of the transition graph is an\noriented rooted tree with orientation towards the root vertex, which\ncorresponds to the normal class. It is clear that the transition graph of an\narbitrary algorithm (policy) may not have this property. In this paper we study\nthe structure of realistic transition graphs, which makes it possible to find\nclassification inconsistencies, helping to transfer it into the desired form.\nThe medical interpretation of dynamic treatment regime considered in the\narticle further clarifies the investigated framework.",
      "tldr_zh": "本研究探讨了目标类分类（target class classification）的转换图（transition graph）属性，该过程通过迭代动作将对象从一个类转移到另一个类，最终分配到目标类。论文分析了转换图的结构，指出理想形式为朝向根节点的定向根树（oriented rooted tree），而现实图可能存在不一致问题。作者通过研究这些不一致，帮助优化图结构以改进分类准确性，并将其应用于医疗领域的动态治疗方案（dynamic treatment regime）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "14pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15167v1",
      "published_date": "2024-03-22 12:37:14 UTC",
      "updated_date": "2024-03-22 12:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:00:21.629163"
    },
    {
      "arxiv_id": "2403.15143v1",
      "title": "Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project",
      "title_zh": "翻译失败",
      "authors": [
        "Md Abdul Kadir",
        "Hasan Md Tusfiqur Alam",
        "Pascale Maul",
        "Hans-Jürgen Profitlich",
        "Moritz Wolf",
        "Daniel Sonntag"
      ],
      "abstract": "Image annotation is one of the most essential tasks for guaranteeing proper\ntreatment for patients and tracking progress over the course of therapy in the\nfield of medical imaging and disease diagnosis. However, manually annotating a\nlot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)\nbased segmentation algorithms have completely transformed this process and made\nit possible to automate image segmentation. By accurately segmenting medical\nimages, these algorithms can greatly minimize the time and effort necessary for\nmanual annotation. Additionally, by incorporating Active Learning (AL) methods,\nthese segmentation algorithms can perform far more effectively with a smaller\namount of ground truth data. We introduce MedDeepCyleAL, an end-to-end\nframework implementing the complete AL cycle. It provides researchers with the\nflexibility to choose the type of deep learning model they wish to employ and\nincludes an annotation tool that supports the classification and segmentation\nof medical images. The user-friendly interface allows for easy alteration of\nthe AL and DL model settings through a configuration file, requiring no prior\nprogramming experience. While MedDeepCyleAL can be applied to any kind of image\ndata, we have specifically applied it to ophthalmology data in this project.",
      "tldr_zh": "这篇论文介绍了 MedDeepCyleAL，一个模块化的端到端框架，旨在通过 Deep Learning (DL) 和 Active Learning (AL) 方法自动化医疗图像标注，显著减少手动标注的繁琐工作和所需地面真实数据。该框架允许用户灵活选择 DL 模型类型，并提供用户友好的界面和工具，支持医疗图像的分类和分割，无需编程经验。尽管适用于各种图像数据，该项目特别将其应用于眼科（Ophthalmo-AI）领域，展示了其在提高标注效率和诊断准确性方面的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "DFKI Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2403.15143v1",
      "published_date": "2024-03-22 11:53:03 UTC",
      "updated_date": "2024-03-22 11:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:00:32.794001"
    },
    {
      "arxiv_id": "2403.15137v1",
      "title": "CACA Agent: Capability Collaboration based AI Agent",
      "title_zh": "CACA Agent: 基于能力协作的 AI 代理",
      "authors": [
        "Peng Xu",
        "Haoran Wang",
        "Chuang Wang",
        "Xu Liu"
      ],
      "abstract": "As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.",
      "tldr_zh": "随着基于 Large Language Models (LLMs) 的 AI Agents 在各领域的实际应用潜力不断显现，如何快速部署和扩展应用场景已成为主要挑战。论文提出 CACA Agent，这是一种基于 Capability Collaboration 的 AI Agent，使用受服务计算启发的开放架构来整合一组协作能力，从而减少对单个 LLM 的依赖，并提升 AI Agents 的规划能力和工具扩展性。通过演示系统，论文展示了 CACA Agent 的操作流程及其在扩展应用场景方面的实际效果。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages,5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15137v1",
      "published_date": "2024-03-22 11:42:47 UTC",
      "updated_date": "2024-03-22 11:42:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:00:44.512010"
    },
    {
      "arxiv_id": "2406.11842v1",
      "title": "The Ethics of AI in Education",
      "title_zh": "翻译失败",
      "authors": [
        "Kaska Porayska-Pomsta",
        "Wayne Holmes",
        "Selena Nemorin"
      ],
      "abstract": "The transition of Artificial Intelligence (AI) from a lab-based science to\nlive human contexts brings into sharp focus many historic, socio-cultural\nbiases, inequalities, and moral dilemmas. Many questions that have been raised\nregarding the broader ethics of AI are also relevant for AI in Education\n(AIED). AIED raises further specific challenges related to the impact of its\ntechnologies on users, how such technologies might be used to reinforce or\nalter the way that we learn and teach, and what we, as a society and\nindividuals, value as outcomes of education. This chapter discusses key ethical\ndimensions of AI and contextualises them within AIED design and engineering\npractices to draw connections between the AIED systems we build, the questions\nabout human learning and development we ask, the ethics of the pedagogies we\nuse, and the considerations of values that we promote in and through AIED\nwithin a wider socio-technical system.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）在教育中的伦理问题，强调AI从实验室转向实际应用所带来的历史、社会文化偏见、不平等和道德困境。论文特别关注AI in Education (AIED) 的独特挑战，包括技术对用户的影响、学习和教学方式的潜在改变，以及社会和个人对教育成果的价值观。最终，它将AI的伦理维度与AIED的设计和工程实践相结合，揭示了AIED系统、人类学习问题、教育伦理以及更广泛的社会技术系统中的价值考虑之间的联系。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.11842v1",
      "published_date": "2024-03-22 11:41:37 UTC",
      "updated_date": "2024-03-22 11:41:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:00:56.958995"
    },
    {
      "arxiv_id": "2403.15115v2",
      "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Erik Miehling",
        "Manish Nagireddy",
        "Prasanna Sattigeri",
        "Elizabeth M. Daly",
        "David Piorkowski",
        "John T. Richards"
      ],
      "abstract": "Modern language models, while sophisticated, exhibit some inherent\nshortcomings, particularly in conversational settings. We claim that many of\nthe observed shortcomings can be attributed to violation of one or more\nconversational principles. By drawing upon extensive research from both the\nsocial science and AI communities, we propose a set of maxims -- quantity,\nquality, relevance, manner, benevolence, and transparency -- for describing\neffective human-AI conversation. We first justify the applicability of the\nfirst four maxims (from Grice) in the context of human-AI interactions. We then\nargue that two new maxims, benevolence (concerning the generation of, and\nengagement with, harmful content) and transparency (concerning recognition of\none's knowledge boundaries, operational constraints, and intents), are\nnecessary for addressing behavior unique to modern human-AI interactions. We\nevaluate the degree to which various language models are able to understand\nthese maxims and find that models possess an internal prioritization of\nprinciples that can significantly impact their ability to interpret the maxims\naccurately.",
      "tldr_zh": "该论文探讨了语言模型在对话中的缺陷，提出一套扩展的conversational maxims准则，以提升人类-AI互动的有效性。作者基于Grice的原有准则（quantity, quality, relevance, manner），添加了两个新准则——benevolence（处理有害内容）和transparency（识别知识边界、操作约束及意图），以应对AI独特行为。研究评估了多种语言模型对这些准则的理解，发现模型内部的准则优先级会显著影响其准确解释能力，从而为更可靠的人类-AI对话提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15115v2",
      "published_date": "2024-03-22 11:16:43 UTC",
      "updated_date": "2024-06-22 12:17:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:01:08.232736"
    },
    {
      "arxiv_id": "2403.15114v3",
      "title": "Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers",
      "title_zh": "翻译失败",
      "authors": [
        "Eneko Osaba",
        "Esther Villar-Rodriguez",
        "Antón Asla"
      ],
      "abstract": "Research focused on the conjunction between quantum computing and routing\nproblems has been very prolific in recent years. Most of the works revolve\naround classical problems such as the Traveling Salesman Problem or the Vehicle\nRouting Problem. The real-world applicability of these problems is dependent on\nthe objectives and constraints considered. Anyway, it is undeniable that it is\noften difficult to translate complex requirements into these classical\nformulations.The main objective of this research is to present a solving scheme\nfor dealing with realistic instances while maintaining all the characteristics\nand restrictions of the original real-world problem. Thus, a quantum-classical\nstrategy has been developed, coined Q4RPD, that considers a set of real\nconstraints such as a heterogeneous fleet of vehicles, priority deliveries, and\ncapacities characterized by two values: weight and dimensions of the packages.\nQ4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave.\nTo demonstrate the application of Q4RPD, an experimentation composed of six\ndifferent instances has been conducted, aiming to serve as illustrative\nexamples.",
      "tldr_zh": "本研究探讨了量子计算在真实世界包裹递送路由问题中的应用，提出了一种量子-经典策略Q4RPD，用于处理复杂约束，如异构车队、优先递送以及包裹的重量和尺寸。Q4RPD 利用 D-Wave 的 Leap Constrained Quadratic Model Hybrid Solver 结合量子退火器（Quantum Annealers），以解决传统问题如 Traveling Salesman Problem 或 Vehicle Routing Problem 无法完全覆盖的实际需求。通过六个实例的实验，展示了 Q4RPD 的有效性，为量子计算在物流优化中的实际应用提供了示范。",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "16 pages, 11 figures and 4 tables. Paper submitted for review in\n  Scientific Reports",
      "pdf_url": "http://arxiv.org/pdf/2403.15114v3",
      "published_date": "2024-03-22 11:16:11 UTC",
      "updated_date": "2024-06-28 11:49:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:01:20.809026"
    },
    {
      "arxiv_id": "2403.15112v5",
      "title": "Text Clustering with Large Language Model Embeddings",
      "title_zh": "使用大语言模型嵌入的文本聚类",
      "authors": [
        "Alina Petukhova",
        "João P. Matos-Carvalho",
        "Nuno Fachada"
      ],
      "abstract": "Text clustering is an important method for organising the increasing volume\nof digital content, aiding in the structuring and discovery of hidden patterns\nin uncategorised data. The effectiveness of text clustering largely depends on\nthe selection of textual embeddings and clustering algorithms. This study\nargues that recent advancements in large language models (LLMs) have the\npotential to enhance this task. The research investigates how different textual\nembeddings, particularly those utilised in LLMs, and various clustering\nalgorithms influence the clustering of text datasets. A series of experiments\nwere conducted to evaluate the impact of embeddings on clustering results, the\nrole of dimensionality reduction through summarisation, and the adjustment of\nmodel size. The findings indicate that LLM embeddings are superior at capturing\nsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields better\nresults in three out of five clustering metrics across most tested datasets.\nMost LLM embeddings show improvements in cluster purity and provide a more\ninformative silhouette score, reflecting a refined structural understanding of\ntext data compared to traditional methods. Among the more lightweight models,\nBERT demonstrates leading performance. Additionally, it was observed that\nincreasing model dimensionality and employing summarisation techniques do not\nconsistently enhance clustering efficiency, suggesting that these strategies\nrequire careful consideration for practical application. These results\nhighlight a complex balance between the need for refined text representation\nand computational feasibility in text clustering applications. This study\nextends traditional text clustering frameworks by integrating embeddings from\nLLMs, offering improved methodologies and suggesting new avenues for future\nresearch in various types of textual analysis.",
      "tldr_zh": "本研究探讨了使用大型语言模型（LLMs）嵌入进行文本聚类的潜力，通过实验评估不同文本嵌入（如GPT-3.5 Turbo和BERT）和聚类算法对聚类结果的影响。研究发现，LLM嵌入在捕捉语言细微差异方面优于传统方法，其中GPT-3.5 Turbo在大多数数据集上提升了聚类纯度（cluster purity）和轮廓分数（silhouette score），而BERT在轻量模型中表现出色。结果表明，增加模型维度或采用总结技术并不总是提高效率，需要权衡文本表示的精确性和计算可行性。该研究扩展了传统文本聚类框架，为未来文本分析提供改进方法和研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50 (Primary), 62H30 (Secondary)",
        "I.2.6; I.2.7; I.7.m"
      ],
      "primary_category": "cs.CL",
      "comment": "The peer-reviewed version of this paper is published in the\n  International Journal of Cognitive Computing in Engineering at\n  https://doi.org/10.1016/j.ijcce.2024.11.004. This version is typeset by the\n  authors and differs only in pagination and typographical detail",
      "pdf_url": "http://arxiv.org/pdf/2403.15112v5",
      "published_date": "2024-03-22 11:08:48 UTC",
      "updated_date": "2024-12-02 19:26:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:01:35.026934"
    },
    {
      "arxiv_id": "2403.15516v1",
      "title": "CTSM: Combining Trait and State Emotions for Empathetic Response Model",
      "title_zh": "CTSM：结合特质和状态情感的移情响应模型",
      "authors": [
        "Wang Yufeng",
        "Chen Chao",
        "Yang Zhou",
        "Wang Shuhui",
        "Liao Xiangwen"
      ],
      "abstract": "Empathetic response generation endeavors to empower dialogue systems to\nperceive speakers' emotions and generate empathetic responses accordingly.\nPsychological research demonstrates that emotion, as an essential factor in\nempathy, encompasses trait emotions, which are static and context-independent,\nand state emotions, which are dynamic and context-dependent. However, previous\nstudies treat them in isolation, leading to insufficient emotional perception\nof the context, and subsequently, less effective empathetic expression. To\naddress this problem, we propose Combining Trait and State emotions for\nEmpathetic Response Model (CTSM). Specifically, to sufficiently perceive\nemotions in dialogue, we first construct and encode trait and state emotion\nembeddings, and then we further enhance emotional perception capability through\nan emotion guidance module that guides emotion representation. In addition, we\npropose a cross-contrastive learning decoder to enhance the model's empathetic\nexpression capability by aligning trait and state emotions between generated\nresponses and contexts. Both automatic and manual evaluation results\ndemonstrate that CTSM outperforms state-of-the-art baselines and can generate\nmore empathetic responses. Our code is available at\nhttps://github.com/wangyufeng-empty/CTSM",
      "tldr_zh": "该研究提出CTSM模型，用于提升对话系统的同理心响应生成，通过结合trait emotions（静态、上下文无关情绪）和state emotions（动态、上下文相关情绪），解决现有方法忽略情绪交互导致的感知不足问题。具体而言，CTSM首先构建并编码trait和state情绪嵌入，并通过emotion guidance module增强情绪感知能力，同时采用cross-contrastive learning decoder对齐响应与上下文情绪以改善表达。实验结果显示，CTSM在自动和手动评估中优于最先进基线，能够生成更有效的同理心响应。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 figures. Has been accepted by LREC-COLING2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15516v1",
      "published_date": "2024-03-22 10:45:13 UTC",
      "updated_date": "2024-03-22 10:45:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:01:45.736188"
    },
    {
      "arxiv_id": "2403.15100v1",
      "title": "Subequivariant Reinforcement Learning Framework for Coordinated Motion Control",
      "title_zh": "子等变强化学习框架用于协调运动控制",
      "authors": [
        "Haoyu Wang",
        "Xiaoyu Tan",
        "Xihe Qiu",
        "Chao Qu"
      ],
      "abstract": "Effective coordination is crucial for motion control with reinforcement\nlearning, especially as the complexity of agents and their motions increases.\nHowever, many existing methods struggle to account for the intricate\ndependencies between joints. We introduce CoordiGraph, a novel architecture\nthat leverages subequivariant principles from physics to enhance coordination\nof motion control with reinforcement learning. This method embeds the\nprinciples of equivariance as inherent patterns in the learning process under\ngravity influence, which aids in modeling the nuanced relationships between\njoints vital for motion control. Through extensive experimentation with\nsophisticated agents in diverse environments, we highlight the merits of our\napproach. Compared to current leading methods, CoordiGraph notably enhances\ngeneralization and sample efficiency.",
      "tldr_zh": "该研究针对强化学习（reinforcement learning）中运动控制的协调问题，提出了CoordiGraph架构，该框架利用subequivariant原则从物理学中嵌入等价性模式，以更好地处理关节间的复杂依赖关系，尤其在重力影响下。该方法通过强化学习过程内在化这些原则，提升了对运动协调的建模能力。实验结果显示，CoordiGraph在多样环境中的复杂代理上，比现有领先方法显著提高了泛化性和样本效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 7 figures, 2024 IEEE International Conference on Robotics\n  and Automation",
      "pdf_url": "http://arxiv.org/pdf/2403.15100v1",
      "published_date": "2024-03-22 10:39:22 UTC",
      "updated_date": "2024-03-22 10:39:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:01:56.159133"
    },
    {
      "arxiv_id": "2403.15097v2",
      "title": "Argument-Aware Approach To Event Linking",
      "title_zh": "翻译失败",
      "authors": [
        "I-Hung Hsu",
        "Zihan Xue",
        "Nilay Pochh",
        "Sahil Bansal",
        "Premkumar Natarajan",
        "Jayanth Srinivasa",
        "Nanyun Peng"
      ],
      "abstract": "Event linking connects event mentions in text with relevant nodes in a\nknowledge base (KB). Prior research in event linking has mainly borrowed\nmethods from entity linking, overlooking the distinct features of events.\nCompared to the extensively explored entity linking task, events have more\ncomplex structures and can be more effectively distinguished by examining their\nassociated arguments. Moreover, the information-rich nature of events leads to\nthe scarcity of event KBs. This emphasizes the need for event linking models to\nidentify and classify event mentions not in the KB as ``out-of-KB,'' an area\nthat has received limited attention. In this work, we tackle these challenges\nby introducing an argument-aware approach. First, we improve event linking\nmodels by augmenting input text with tagged event argument information,\nfacilitating the recognition of key information about event mentions.\nSubsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize\nout-of-KB training examples from in-KB instances through controlled\nmanipulation of event arguments. Our experiment across two test datasets showed\nsignificant enhancements in both in-KB and out-of-KB scenarios, with a notable\n22% improvement in out-of-KB evaluations.",
      "tldr_zh": "本文提出了一种基于论元的（argument-aware）方法来提升事件链接（Event linking），以解决现有方法从实体链接（entity linking）借鉴时忽略事件复杂结构的问题。该方法通过在输入文本中添加标记的论元信息，帮助模型更好地识别事件提及的关键细节，并通过对 in-KB 实例进行控制操作合成 out-of-KB 训练样本，以处理知识库（KB）外的事件分类。实验在两个测试数据集上显示，该方法在 in-KB 和 out-of-KB 场景中均有显著改善，尤其在 out-of-KB 评估中提升了22%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted by ACL-findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15097v2",
      "published_date": "2024-03-22 10:32:43 UTC",
      "updated_date": "2024-06-06 05:18:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:02:09.819467"
    },
    {
      "arxiv_id": "2403.15091v1",
      "title": "Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning",
      "title_zh": "改进的基于长短时记忆的废水处理模拟器用于深度强化学习",
      "authors": [
        "Esmaeel Mohammadi",
        "Daniel Ortiz-Arroyo",
        "Mikkel Stokholm-Bjerregaard",
        "Aviaja Anna Hansen",
        "Petar Durdevic"
      ],
      "abstract": "Even though Deep Reinforcement Learning (DRL) showed outstanding results in\nthe fields of Robotics and Games, it is still challenging to implement it in\nthe optimization of industrial processes like wastewater treatment. One of the\nchallenges is the lack of a simulation environment that will represent the\nactual plant as accurately as possible to train DRL policies. Stochasticity and\nnon-linearity of wastewater treatment data lead to unstable and incorrect\npredictions of models over long time horizons. One possible reason for the\nmodels' incorrect simulation behavior can be related to the issue of\ncompounding error, which is the accumulation of errors throughout the\nsimulation. The compounding error occurs because the model utilizes its\npredictions as inputs at each time step. The error between the actual data and\nthe prediction accumulates as the simulation continues. We implemented two\nmethods to improve the trained models for wastewater treatment data, which\nresulted in more accurate simulators: 1- Using the model's prediction data as\ninput in the training step as a tool of correction, and 2- Change in the loss\nfunction to consider the long-term predicted shape (dynamics). The experimental\nresults showed that implementing these methods can improve the behavior of\nsimulators in terms of Dynamic Time Warping throughout a year up to 98%\ncompared to the base model. These improvements demonstrate significant promise\nin creating simulators for biological processes that do not need pre-existing\nknowledge of the process but instead depend exclusively on time series data\nobtained from the system.",
      "tldr_zh": "本研究针对Deep Reinforcement Learning (DRL)在污水处理优化中的挑战，提出改进Long Short-Term Memory (LSTM)模型的模拟器，以解决数据随机性和非线性导致的累积错误问题。主要方法包括：在训练中使用模型预测数据作为输入进行修正，以及修改损失函数以关注长期预测动态。实验结果显示，该改进方法使模拟器的Dynamic Time Warping性能比基线模型提升高达98%，为无需预有知识的生物过程模拟器开发提供了重要前景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15091v1",
      "published_date": "2024-03-22 10:20:09 UTC",
      "updated_date": "2024-03-22 10:20:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:02:21.501972"
    },
    {
      "arxiv_id": "2403.15076v1",
      "title": "Comprehensive Lipidomic Automation Workflow using Large Language Models",
      "title_zh": "使用大型语言模型的全面脂质组学自动化工作流",
      "authors": [
        "Connor Beveridge",
        "Sanjay Iyer",
        "Caitlin E. Randolph",
        "Matthew Muhoberac",
        "Palak Manchanda",
        "Amy C. Clingenpeel",
        "Shane Tichy",
        "Gaurav Chopra"
      ],
      "abstract": "Lipidomics generates large data that makes manual annotation and\ninterpretation challenging. Lipid chemical and structural diversity with\nstructural isomers further complicates annotation. Although, several commercial\nand open-source software for targeted lipid identification exists, it lacks\nautomated method generation workflows and integration with statistical and\nbioinformatics tools. We have developed the Comprehensive Lipidomic Automated\nWorkflow (CLAW) platform with integrated workflow for parsing, detailed\nstatistical analysis and lipid annotations based on custom multiple reaction\nmonitoring (MRM) precursor and product ion pair transitions. CLAW contains\nseveral modules including identification of carbon-carbon double bond\nposition(s) in unsaturated lipids when combined with ozone electrospray\nionization (OzESI)-MRM methodology. To demonstrate the utility of the automated\nworkflow in CLAW, large-scale lipidomics data was collected with traditional\nand OzESI-MRM profiling on biological and non-biological samples. Specifically,\na total of 1497 transitions organized into 10 MRM-based mass spectrometry\nmethods were used to profile lipid droplets isolated from different brain\nregions of 18-24 month-old Alzheimer's disease mice and age-matched wild-type\ncontrols. Additionally, triacyclglycerols (TGs) profiles with carbon-carbon\ndouble bond specificity were generated from canola oil samples using OzESI-MRM\nprofiling. We also developed an integrated language user interface with large\nlanguage models using artificially intelligent (AI) agents that permits users\nto interact with the CLAW platform using a chatbot terminal to perform\nstatistical and bioinformatic analyses. We envision CLAW pipeline to be used in\nhigh-throughput lipid structural identification tasks aiding users to generate\nautomated lipidomics workflows ranging from data acquisition to AI agent-based\nbioinformatic analysis.",
      "tldr_zh": "本研究开发了 Comprehensive Lipidomic Automation Workflow (CLAW) 平台，以解决脂质组学 (Lipidomics) 数据量大且手动注释复杂的挑战。CLAW 集成了解析、统计分析和脂质注释的工作流，使用自定义 multiple reaction monitoring (MRM) 前体和产物离子对，并支持 ozone electrospray ionization (OzESI)-MRM 方法来识别不饱和脂质中的碳-碳双键位置。实验通过传统和 OzESI-MRM 分析了生物样本（如阿尔茨海默病小鼠脑部脂滴）和非生物样本（如菜籽油中的 triacylglycerols），成功生成了大规模脂质 profiling 数据。平台还整合了大型语言模型 (Large Language Models) 和 AI 代理的聊天机器人界面，允许用户轻松进行统计和生物信息分析，从而实现从数据获取到自动化工作流的全面高通量脂质结构识别。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "q-bio.BM",
        "q-bio.SC"
      ],
      "primary_category": "q-bio.QM",
      "comment": "53 pages, 4 main figures, 23 Supporting figures, 10 Supporting Tables",
      "pdf_url": "http://arxiv.org/pdf/2403.15076v1",
      "published_date": "2024-03-22 10:00:52 UTC",
      "updated_date": "2024-03-22 10:00:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:02:34.663901"
    },
    {
      "arxiv_id": "2403.15075v1",
      "title": "Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation",
      "title_zh": "双边不对称图对比学习用于推荐",
      "authors": [
        "Jiaheng Yu",
        "Jing Li",
        "Yue He",
        "Kai Zhu",
        "Shuyi Zhang",
        "Wen Hu"
      ],
      "abstract": "Recent methods utilize graph contrastive Learning within graph-structured\nuser-item interaction data for collaborative filtering and have demonstrated\ntheir efficacy in recommendation tasks. However, they ignore that the\ndifference relation density of nodes between the user- and item-side causes the\nadaptability of graphs on bilateral nodes to be different after multi-hop graph\ninteraction calculation, which limits existing models to achieve ideal results.\nTo solve this issue, we propose a novel framework for recommendation tasks\ncalled Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that\nconsider the bilateral unsymmetry on user-item node relation density for sliced\nuser and item graph reasoning better with bilateral slicing contrastive\ntraining. Especially, taking into account the aggregation ability of\nhypergraph-based graph convolutional network (GCN) in digging implicit\nsimilarities is more suitable for user nodes, embeddings generated from three\ndifferent modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into\ntwo subviews by the user- and item-side respectively, and selectively combined\ninto subview pairs bilaterally based on the characteristics of inter-node\nrelation structure. Furthermore, to align the distribution of user and item\nembeddings after aggregation, a dispersing loss is leveraged to adjust the\nmutual distance between all embeddings for maintaining learning ability.\nComprehensive experiments on two public datasets have proved the superiority of\nBusGCL in comparison to various recommendation methods. Other models can simply\nutilize our bilateral slicing contrastive learning to enhance recommending\nperformance without incurring extra expenses.",
      "tldr_zh": "该论文提出了一种名为 Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) 的新框架，用于解决推荐系统中的图对比学习问题，特别是用户侧和物品侧节点关系密度不对称导致的适应性不足。BusGCL 通过双侧切片对比训练，结合 hypergraph-based GCN、GCN 和 perturbed GCN 等模块生成嵌入，并根据节点关系结构选择性地组合子视图，以更好地挖掘隐式相似性。同时，引入 dispersing loss 来调整用户和物品嵌入的分布，确保模型的学习能力。在两个公共数据集上的实验显示，BusGCL 比现有方法表现出色，其他模型可轻松采用此方法提升性能而无需额外开销。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15075v1",
      "published_date": "2024-03-22 09:58:33 UTC",
      "updated_date": "2024-03-22 09:58:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:02:46.222570"
    },
    {
      "arxiv_id": "2403.15059v1",
      "title": "MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration",
      "title_zh": "MM-Diff：通过多模态条件整合实现高保真图像个性化",
      "authors": [
        "Zhichao Wei",
        "Qingkun Su",
        "Long Qin",
        "Weizhi Wang"
      ],
      "abstract": "Recent advances in tuning-free personalized image generation based on\ndiffusion models are impressive. However, to improve subject fidelity, existing\nmethods either retrain the diffusion model or infuse it with dense visual\nembeddings, both of which suffer from poor generalization and efficiency. Also,\nthese methods falter in multi-subject image generation due to the unconstrained\ncross-attention mechanism. In this paper, we propose MM-Diff, a unified and\ntuning-free image personalization framework capable of generating high-fidelity\nimages of both single and multiple subjects in seconds. Specifically, to\nsimultaneously enhance text consistency and subject fidelity, MM-Diff employs a\nvision encoder to transform the input image into CLS and patch embeddings. CLS\nembeddings are used on the one hand to augment the text embeddings, and on the\nother hand together with patch embeddings to derive a small number of\ndetail-rich subject embeddings, both of which are efficiently integrated into\nthe diffusion model through the well-designed multimodal cross-attention\nmechanism. Additionally, MM-Diff introduces cross-attention map constraints\nduring the training phase, ensuring flexible multi-subject image sampling\nduring inference without any predefined inputs (e.g., layout). Extensive\nexperiments demonstrate the superior performance of MM-Diff over other leading\nmethods.",
      "tldr_zh": "论文提出 MM-Diff，一种统一的调优-free 框架，用于实现高保真图像个性化，支持在几秒内生成单主体或多主体图像。MM-Diff 利用视觉编码器将输入图像转化为 CLS embeddings 和 patch embeddings，前者增强文本 embeddings，后者与 CLS embeddings 结合生成细节丰富的主体 embeddings，并通过多模态 cross-attention 机制高效整合到 diffusion models 中。该框架在训练阶段引入 cross-attention map 约束，允许推理时灵活采样多主体图像，而无需预定义输入，如布局。实验结果显示，MM-Diff 在性能上优于其他领先方法，显著提升了图像生成的泛化和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15059v1",
      "published_date": "2024-03-22 09:32:31 UTC",
      "updated_date": "2024-03-22 09:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:02:59.023874"
    },
    {
      "arxiv_id": "2403.15049v2",
      "title": "Continual Vision-and-Language Navigation",
      "title_zh": "持续视觉与语言导航",
      "authors": [
        "Seongjun Jeong",
        "Gi-Cheon Kang",
        "Seongho Choi",
        "Joochan Kim",
        "Byoung-Tak Zhang"
      ],
      "abstract": "In developing Vision-and-Language Navigation (VLN) agents that navigate to a\ndestination using natural language instructions and visual cues, current\nstudies largely assume a \\textit{train-once-deploy-once strategy}. We argue\nthat this kind of strategy is less realistic, as deployed VLN agents are\nexpected to encounter novel environments continuously through their lifetime.\nTo facilitate more realistic setting for VLN agents, we propose Continual\nVision-and-Language Navigation (CVLN) paradigm for agents to continually learn\nand adapt to changing environments. In CVLN, the agents are trained and\nevaluated incrementally across multiple \\textit{scene domains} (i.e.,\nenvironments). We present two CVLN learning setups to consider diverse forms of\nnatural language instructions: Initial-instruction based CVLN, focused on\nnavigation via initial-instruction interpretation, and dialogue-based CVLN,\ndesigned for navigation through dialogue with other agents. We introduce two\nsimple yet effective baseline methods, tailored to the sequential\ndecision-making needs of CVLN: Perplexity Replay (PerpR) and Episodic\nSelf-Replay (ESR), both employing a rehearsal mechanism. PerpR selects replay\nepisodes based on episode difficulty, while ESR stores and revisits action\nlogits from individual episode steps during training to refine learning.\nExperimental results indicate that while existing continual learning methods\nare insufficient for CVLN, PerpR and ESR outperform the comparison methods by\neffectively utilizing replay memory.",
      "tldr_zh": "这篇论文提出 Continual Vision-and-Language Navigation (CVLN) 范式，以解决传统 Vision-and-Language Navigation (VLN) 代理在实际部署中无法持续适应新环境的局限性，强调代理需在多个场景域中逐步学习。CVLN 包括基于初始指令和基于对话的两种学习设置，并引入 Perplexity Replay (PerpR) 和 Episodic Self-Replay (ESR) 基线方法，后者通过排练机制选择困难情节或重访动作 logits 来优化训练。实验结果显示，PerpR 和 ESR 比现有持续学习方法更有效地利用重放内存，提升了代理的表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15049v2",
      "published_date": "2024-03-22 09:15:36 UTC",
      "updated_date": "2024-12-21 09:05:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:03:11.462248"
    },
    {
      "arxiv_id": "2403.15048v3",
      "title": "Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information",
      "title_zh": "使用姿势信息使 VLM 识别卡通人物图像上的视觉幻",
      "authors": [
        "Bumsoo Kim",
        "Wonseop Shin",
        "Kyuchul Lee",
        "Yonghoon Jung",
        "Sanghyun Seo"
      ],
      "abstract": "Leveraging large-scale Text-to-Image (TTI) models have become a common\ntechnique for generating exemplar or training dataset in the fields of image\nsynthesis, video editing, 3D reconstruction. However, semantic structural\nvisual hallucinations involving perceptually severe defects remain a concern,\nespecially in the domain of non-photorealistic rendering (NPR) such as cartoons\nand pixelization-style character. To detect these hallucinations in NPR, We\npropose a novel semantic structural hallucination detection system using\nVision-Language Model (VLM). Our approach is to leverage the emerging\ncapability of large language model, in-context learning which denotes that VLM\nhas seen some examples by user for specific downstream task, here hallucination\ndetection. Based on in-context learning, we introduce pose-aware in-context\nvisual learning (PA-ICVL) which improve the overall performance of VLM by\nfurther inputting visual data beyond prompts, RGB images and pose information.\nBy incorporating pose guidance, we enable VLMs to make more accurate decisions.\nExperimental results demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images.\nWithin selected two VLMs, GPT-4v, Gemini pro vision, our proposed PA-ICVL\nimproves the hallucination detection with 50% to 78%, 57% to 80%, respectively.\nThis research advances a capability of TTI models toward real-world\napplications by mitigating visual hallucinations via in-context visual\nlearning, expanding their potential in non-photorealistic domains. In addition,\nit showcase how users can boost the downstream-specialized capability of open\nVLM by harnessing additional conditions. We collect synthetic\ncartoon-hallucination dataset with TTI models, this dataset and final tuned VLM\nwill be publicly available.",
      "tldr_zh": "本研究针对 Text-to-Image (TTI) 模型在非真实渲染（如卡通人物图像）领域产生的语义结构视觉幻觉问题，提出了一种基于 Vision-Language Model (VLM) 的检测系统。方法引入 pose-aware in-context visual learning (PA-ICVL)，通过结合 in-context learning、RGB 图像和姿态信息，增强 VLM 的检测能力，使其更准确地识别幻觉。实验结果显示，PA-ICVL 在 GPT-4v 和 Gemini pro vision 上将幻觉检测准确率分别从 50% 提高到 78%，以及从 57% 提高到 80%，显著优于仅依赖 RGB 图像的基线方法。该工作缓解了 TTI 模型的视觉幻觉问题，扩展了其在非真实渲染领域的应用，并计划公开合成卡通幻觉数据集和微调后的 VLM。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/",
      "pdf_url": "http://arxiv.org/pdf/2403.15048v3",
      "published_date": "2024-03-22 09:13:09 UTC",
      "updated_date": "2025-01-22 05:46:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:03:25.935590"
    },
    {
      "arxiv_id": "2403.15044v1",
      "title": "Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuofan Wen",
        "Fengyu Zhang",
        "Siyuan Zhang",
        "Haiyang Sun",
        "Mingyu Xu",
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "abstract": "Multimodal fusion is a significant method for most multimodal tasks. With the\nrecent surge in the number of large pre-trained models, combining both\nmultimodal fusion methods and pre-trained model features can achieve\noutstanding performance in many multimodal tasks. In this paper, we present our\napproach, which leverages both advantages for addressing the task of Expression\n(Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the\nAff-Wild2 database using pre-trained models, then extract the final hidden\nlayers of the models as features. Following preprocessing and interpolation or\nconvolution to align the extracted features, different models are employed for\nmodal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.",
      "tldr_zh": "本论文提出了一种结合多模态融合（Multimodal Fusion）和预训练模型特征（Pre-Trained Model Features）的方法，用于野外情感行为分析（Affective Behaviour Analysis In-the-wild），具体针对表情识别（Expression Recognition）和情感值估计（Valence-Arousal Estimation）。\n研究团队从 Aff-Wild2 数据库中提取预训练模型的最终隐藏层特征，进行预处理、对齐和模态融合，以提升任务性能。\n实验验证了该方法的有效性，并公开了代码在 GitHub 上，以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15044v1",
      "published_date": "2024-03-22 09:00:24 UTC",
      "updated_date": "2024-03-22 09:00:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:03:37.534279"
    },
    {
      "arxiv_id": "2405.10948v3",
      "title": "Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery",
      "title_zh": "翻译失败",
      "authors": [
        "Guankun Wang",
        "Long Bai",
        "Wan Jun Nah",
        "Jie Wang",
        "Zhaoxi Zhang",
        "Zhen Chen",
        "Jinlin Wu",
        "Mobarakol Islam",
        "Hongbin Liu",
        "Hongliang Ren"
      ],
      "abstract": "Recent advancements in Surgical Visual Question Answering (Surgical-VQA) and\nrelated region grounding have shown great promise for robotic and medical\napplications, addressing the critical need for automated methods in\npersonalized surgical mentorship. However, existing models primarily provide\nsimple structured answers and struggle with complex scenarios due to their\nlimited capability in recognizing long-range dependencies and aligning\nmultimodal information. In this paper, we introduce Surgical-LVLM, a novel\npersonalized large vision-language model tailored for complex surgical\nscenarios. Leveraging the pre-trained large vision-language model and\nspecialized Visual Perception LoRA (VP-LoRA) blocks, our model excels in\nunderstanding complex visual-language tasks within surgical contexts. In\naddressing the visual grounding task, we propose the Token-Interaction (TIT)\nmodule, which strengthens the interaction between the grounding module and the\nlanguage responses of the Large Visual Language Model (LVLM) after projecting\nthem into the latent space. We demonstrate the effectiveness of Surgical-LVLM\non several benchmarks, including EndoVis-17-VQLA, EndoVis-18-VQLA, and a newly\nintroduced EndoVis Conversations dataset, which sets new performance standards.\nOur work contributes to advancing the field of automated surgical mentorship by\nproviding a context-aware solution.",
      "tldr_zh": "本研究提出了 Surgical-LVLM，一种针对机器人手术的个性化大型视觉语言模型（Large Vision-Language Model），旨在提升 Surgical Visual Question Answering (Surgical-VQA) 的性能，特别是处理复杂场景中的长距离依赖和多模态信息对齐问题。模型利用预训练的 LVLM 结合专门的 Visual Perception LoRA (VP-LoRA) 块来增强手术视觉任务理解，并引入 Token-Interaction (TIT) 模块来加强视觉 grounding 与语言响应的交互。实验结果显示，Surgical-LVLM 在 EndoVis-17-VQLA、EndoVis-18-VQLA 和新的 EndoVis Conversations 数据集上取得了新的性能标准，从而为自动手术指导提供了一个上下文感知的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "The manuscript is accepted by ICLR 2025 FM-Wild Workshop",
      "pdf_url": "http://arxiv.org/pdf/2405.10948v3",
      "published_date": "2024-03-22 08:38:27 UTC",
      "updated_date": "2025-03-16 02:23:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:03:49.590931"
    },
    {
      "arxiv_id": "2403.15027v2",
      "title": "Grey-informed neural network for time-series forecasting",
      "title_zh": "灰",
      "authors": [
        "Wanli Xie",
        "Ruibin Zhao",
        "Zhenguo Xu",
        "Tingting Liang"
      ],
      "abstract": "Neural network models have shown outstanding performance and successful\nresolutions to complex problems in various fields. However, the majority of\nthese models are viewed as black-box, requiring a significant amount of data\nfor development. Consequently, in situations with limited data, constructing\nappropriate models becomes challenging due to the lack of transparency and\nscarcity of data. To tackle these challenges, this study suggests the\nimplementation of a grey-informed neural network (GINN). The GINN ensures that\nthe output of the neural network follows the differential equation model of the\ngrey system, improving interpretability. Moreover, incorporating prior\nknowledge from grey system theory enables traditional neural networks to\neffectively handle small data samples. Our proposed model has been observed to\nuncover underlying patterns in the real world and produce reliable forecasts\nbased on empirical data.",
      "tldr_zh": "本研究针对神经网络模型的黑盒特性和对大量数据的需求，提出了一种 grey-informed neural network (GINN)，以解决数据稀缺场景下的建模挑战。GINN 通过整合 grey system 的微分方程模型，确保神经网络输出更具可解释性，并利用先验知识有效处理小样本数据。实验结果表明，该模型能够揭示真实世界的潜在模式，并基于经验数据产生可靠的时间-series forecasting。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15027v2",
      "published_date": "2024-03-22 08:17:00 UTC",
      "updated_date": "2024-04-03 09:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:04:01.130832"
    },
    {
      "arxiv_id": "2403.14999v1",
      "title": "Magic for the Age of Quantized DNNs",
      "title_zh": "翻译失败",
      "authors": [
        "Yoshihide Sawada",
        "Ryuji Saiin",
        "Kazuma Suetake"
      ],
      "abstract": "Recently, the number of parameters in DNNs has explosively increased, as\nexemplified by LLMs (Large Language Models), making inference on small-scale\ncomputers more difficult. Model compression technology is, therefore, essential\nfor integration into products. In this paper, we propose a method of\nquantization-aware training. We introduce a novel normalization (Layer-Batch\nNormalization) that is independent of the mini-batch size and does not require\nany additional computation cost during inference. Then, we quantize the weights\nby the scaled round-clip function with the weight standardization. We also\nquantize activation functions using the same function and apply surrogate\ngradients to train the model with both quantized weights and the quantized\nactivation functions. We call this method Magic for the age of Quantised DNNs\n(MaQD). Experimental results show that our quantization method can be achieved\nwith minimal accuracy degradation.",
      "tldr_zh": "该论文针对深度神经网络(DNNs)参数爆炸增长（如LLMs）导致的小规模设备推理难题，提出了一种量化感知训练(quantization-aware training)方法，名为MaQD。方法引入Layer-Batch Normalization，这种归一化独立于mini-batch大小，且推理时无需额外计算成本；同时，通过scaled round-clip函数结合weight standardization量化权重，并使用相同函数量化激活函数，同时应用surrogate gradients进行端到端训练。实验结果显示，MaQD在量化DNNs后，准确率下降最小，有效提升了模型压缩的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 5 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.14999v1",
      "published_date": "2024-03-22 07:21:09 UTC",
      "updated_date": "2024-03-22 07:21:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:04:11.926578"
    },
    {
      "arxiv_id": "2403.14977v1",
      "title": "Piecewise-Linear Manifolds for Deep Metric Learning",
      "title_zh": "分段线性流形用于深度度量学习",
      "authors": [
        "Shubhang Bhatnagar",
        "Narendra Ahuja"
      ],
      "abstract": "Unsupervised deep metric learning (UDML) focuses on learning a semantic\nrepresentation space using only unlabeled data. This challenging problem\nrequires accurately estimating the similarity between data points, which is\nused to supervise a deep network. For this purpose, we propose to model the\nhigh-dimensional data manifold using a piecewise-linear approximation, with\neach low-dimensional linear piece approximating the data manifold in a small\nneighborhood of a point. These neighborhoods are used to estimate similarity\nbetween data points. We empirically show that this similarity estimate\ncorrelates better with the ground truth than the similarity estimates of\ncurrent state-of-the-art techniques. We also show that proxies, commonly used\nin supervised metric learning, can be used to model the piecewise-linear\nmanifold in an unsupervised setting, helping improve performance. Our method\noutperforms existing unsupervised metric learning approaches on standard\nzero-shot image retrieval benchmarks.",
      "tldr_zh": "这篇论文针对无监督深度度量学习 (UDML) 提出了一种新方法，使用分段线性流形 (piecewise-linear manifolds) 来逼近高维数据流形，每个低维线性片段近似于数据点附近的小邻域，从而更准确地估计数据点之间的相似度。该方法还引入代理 (proxies)，这些通常用于监督度量学习的组件，以在无监督设置中提升性能。实验结果表明，该方法在相似度估计上比现有最先进技术更接近真实值，并在标准零样本图像检索基准上超过了现有无监督度量学习方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CPAL 2024 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2403.14977v1",
      "published_date": "2024-03-22 06:22:20 UTC",
      "updated_date": "2024-03-22 06:22:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:04:24.800029"
    },
    {
      "arxiv_id": "2403.14972v2",
      "title": "A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Changmeng Zheng",
        "Dayong Liang",
        "Wengyu Zhang",
        "Xiao-Yong Wei",
        "Tat-Seng Chua",
        "Qing Li"
      ],
      "abstract": "This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate that BDoG is able to achieve state-of-the-art\nresults in ScienceQA and MMBench with significant improvements over previous\nmethods. The source code can be accessed at https://github.com/thecharm/BDoG.",
      "tldr_zh": "本研究引入多智能体辩论(multi-agent debate)到多模态推理(multimodal reasoning)中，针对现有归纳(bottom-up)方案导致的意见琐碎化和图像干扰概念焦点转移等问题，提出了一种演绎(top-down)方法Blueprint Debate on Graphs (BDoG)。在BDoG中，辩论被限制在蓝图图中，通过分支存储证据来防止过度总结和无关概念的干扰，从而提升推理效率。实验结果显示，该方法在ScienceQA和MMBench数据集上实现了最先进(state-of-the-art)性能，比先前方法有显著改进，并提供了开源代码以供进一步应用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACM Multimedia 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.14972v2",
      "published_date": "2024-03-22 06:03:07 UTC",
      "updated_date": "2024-08-06 09:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:04:36.480833"
    },
    {
      "arxiv_id": "2403.14965v1",
      "title": "Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation",
      "title_zh": "对大型语言模型在行为驱动开发验收测试自动化的使用的全面评估与洞见",
      "authors": [
        "Shanthi Karpurapu",
        "Sravanthy Myneni",
        "Unnati Nettur",
        "Likhit Sagar Gajja",
        "Dave Burke",
        "Tom Stiehm",
        "Jeffery Payne"
      ],
      "abstract": "Behavior-driven development (BDD) is an Agile testing methodology fostering\ncollaboration among developers, QA analysts, and stakeholders. In this\nmanuscript, we propose a novel approach to enhance BDD practices using large\nlanguage models (LLMs) to automate acceptance test generation. Our study uses\nzero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B,\nand PaLM-2. The paper presents a detailed methodology that includes the\ndataset, prompt techniques, LLMs, and the evaluation process. The results\ndemonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests\nwith better performance. The few-shot prompt technique highlights its ability\nto provide higher accuracy by incorporating examples for in-context learning.\nFurthermore, the study examines syntax errors, validation accuracy, and\ncomparative analysis of LLMs, revealing their effectiveness in enhancing BDD\npractices. However, our study acknowledges that there are limitations to the\nproposed approach. We emphasize that this approach can support collaborative\nBDD processes and create opportunities for future research into automated BDD\nacceptance test generation using LLMs.",
      "tldr_zh": "该论文评估了大型语言模型 (LLMs) 在自动化 Behavior-Driven Development (BDD) 验收测试生成中的应用，提出了一种新方法来提升开发、QA 和利益相关者之间的协作。研究使用 zero-shot 和 few-shot 提示技术测试了 GPT-3.5、GPT-4、Llama-2-13B 和 PaLM-2 等模型，详细描述了数据集、提示策略和评估过程。结果表明，GPT-3.5 和 GPT-4 生成的无错误测试性能最佳，而 few-shot 提示通过 in-context learning 显著提高了准确性。总体上，该方法增强了 BDD 实践，但也承认了其局限性，并为未来 LLMs 在自动化 BDD 测试方面的研究提供了机会。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "I.2.7; I.2.1"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14965v1",
      "published_date": "2024-03-22 05:37:52 UTC",
      "updated_date": "2024-03-22 05:37:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:04:49.765718"
    },
    {
      "arxiv_id": "2403.15512v1",
      "title": "Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation",
      "title_zh": "通过决策边界感知数据增强在低资源环境下提升有效性和鲁棒性",
      "authors": [
        "Kyohoon Jin",
        "Junho Lee",
        "Juhwan Choi",
        "Sangmin Song",
        "Youngbin Kim"
      ],
      "abstract": "Efforts to leverage deep learning models in low-resource regimes have led to\nnumerous augmentation studies. However, the direct application of methods such\nas mixup and cutout to text data, is limited due to their discrete\ncharacteristics. While methods using pretrained language models have exhibited\nefficiency, they require additional considerations for robustness. Inspired by\nrecent studies on decision boundaries, this paper proposes a\ndecision-boundary-aware data augmentation strategy to enhance robustness using\npretrained language models. The proposed technique first focuses on shifting\nthe latent features closer to the decision boundary, followed by reconstruction\nto generate an ambiguous version with a soft label. Additionally, mid-K\nsampling is suggested to enhance the diversity of the generated sentences. This\npaper demonstrates the performance of the proposed augmentation strategy\ncompared to other methods through extensive experiments. Furthermore, the\nablation study reveals the effect of soft labels and mid-K sampling and the\nextensibility of the method with curriculum data augmentation.",
      "tldr_zh": "本论文针对低资源环境下文本数据的增强挑战，提出了一种基于 decision-boundary-aware 的数据增强策略，利用 pretrained language models 来提升模型的有效性和鲁棒性。该策略首先将潜在特征移向 decision boundary，然后通过重建生成带有软标签的模糊样本，并引入 mid-K sampling 以增加生成的句子多样性。通过广泛实验，该方法在性能上优于其他增强技术；消融研究进一步证实了软标签和 mid-K sampling 的关键作用，并展示了其与 curriculum data augmentation 的扩展潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.15512v1",
      "published_date": "2024-03-22 05:18:08 UTC",
      "updated_date": "2024-03-22 05:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:05:01.035552"
    },
    {
      "arxiv_id": "2403.14952v1",
      "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
      "title_zh": "证据驱动的检索增强响应生成，用于在线错误信息",
      "authors": [
        "Zhenrui Yue",
        "Huimin Zeng",
        "Yimeng Lu",
        "Lanyu Shang",
        "Yang Zhang",
        "Dong Wang"
      ],
      "abstract": "The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.",
      "tldr_zh": "这篇论文针对在线虚假信息问题，提出了一种证据驱动的检索增强响应生成方法（RARG），旨在生成礼貌且事实支持的反驳回应。RARG 分为两个阶段：证据收集，通过检索管道从超过100万学术文章数据库中检索和重新排名相关证据；以及响应生成，利用大型语言模型（LLMs）和强化学习从人类反馈（RLHF）来优化文本质量，确保最大化证据利用。实验结果显示，在COVID-19相关数据集上，RARG 显著优于基线方法，产生了高质量的反虚假信息回应。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.14952v1",
      "published_date": "2024-03-22 05:05:45 UTC",
      "updated_date": "2024-03-22 05:05:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:05:13.083007"
    },
    {
      "arxiv_id": "2403.14951v2",
      "title": "Simple Graph Condensation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenbang Xiao",
        "Yu Wang",
        "Shunyu Liu",
        "Huiqiong Wang",
        "Mingli Song",
        "Tongya Zheng"
      ],
      "abstract": "The burdensome training costs on large-scale graphs have aroused significant\ninterest in graph condensation, which involves tuning Graph Neural Networks\n(GNNs) on a small condensed graph for use on the large-scale original graph.\nExisting methods primarily focus on aligning key metrics between the condensed\nand original graphs, such as gradients, output distribution and trajectories of\nGNNs, yielding satisfactory performance on downstream tasks. However, these\ncomplex metrics necessitate intricate external parameters and can potentially\ndisrupt the optimization process of the condensation graph, making the\ncondensation process highly demanding and unstable. Motivated by the recent\nsuccess of simplified models across various domains, we propose a simplified\napproach to metric alignment in graph condensation, aiming to reduce\nunnecessary complexity inherited from intricate metrics. We introduce the\nSimple Graph Condensation (SimGC) framework, which aligns the condensed graph\nwith the original graph from the input layer to the prediction layer, guided by\na pre-trained Simple Graph Convolution (SGC) model on the original graph.\nImportantly, SimGC eliminates external parameters and exclusively retains the\ntarget condensed graph during the condensation process. This straightforward\nyet effective strategy achieves a significant speedup of up to 10 times\ncompared to existing graph condensation methods while performing on par with\nstate-of-the-art baselines. Comprehensive experiments conducted on seven\nbenchmark datasets demonstrate the effectiveness of SimGC in prediction\naccuracy, condensation time, and generalization capability. Our code is\navailable at https://github.com/BangHonor/SimGC.",
      "tldr_zh": "本研究针对图神经网络（GNNs）在大型图上的高训练成本，提出了一种简化图压缩方法Simple Graph Condensation (SimGC)。SimGC 通过使用预训练的Simple Graph Convolution (SGC) 模型，从输入层到预测层对齐压缩图与原图，避免了复杂指标的外部参数干扰，仅保留目标压缩图进行优化。实验结果显示，SimGC 比现有方法快 10 倍，并在七个基准数据集上实现了与最先进基线相当的预测准确性、压缩时间和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "ECML-PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.14951v2",
      "published_date": "2024-03-22 05:04:48 UTC",
      "updated_date": "2024-07-18 03:34:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:05:26.002963"
    },
    {
      "arxiv_id": "2403.14946v1",
      "title": "A Single Linear Layer Yields Task-Adapted Low-Rank Matrices",
      "title_zh": "翻译失败",
      "authors": [
        "Hwichan Kim",
        "Shota Sasaki",
        "Sho Hoshino",
        "Ukyo Honda"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning\n(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix\n$\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study\nsuggested that there is correlation between $W_0$ and $\\Delta W$. In this\nstudy, we aim to delve deeper into relationships between $W_0$ and low-rank\nmatrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,\nwe analyze a conversion matrix that transform $W_0$ into low-rank matrices,\nwhich encapsulates information about the relationships. Our analysis reveals\nthat the conversion matrices are similar across each layer. Inspired by these\nfindings, we hypothesize that a single linear layer, which takes each layer's\n$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this\nhypothesis, we devise a method named Conditionally Parameterized LoRA\n(CondLoRA) that updates initial weight matrices with low-rank matrices derived\nfrom a single linear layer. Our empirical results show that CondLoRA maintains\na performance on par with LoRA, despite the fact that the trainable parameters\nof CondLoRA are fewer than those of LoRA. Therefore, we conclude that \"a single\nlinear layer yields task-adapted low-rank matrices.\"",
      "tldr_zh": "本研究探讨了Low-Rank Adaptation (LoRA)方法中初始权重矩阵$W_0$与低秩矩阵$A$和$B$之间的关系，通过分析转换矩阵发现这些矩阵在各层间高度相似。基于此假设，研究提出Conditionally Parameterized LoRA (CondLoRA)方法，使用一个单一的线性层从每个层的$W_0$生成任务适配的低秩矩阵，从而更新权重。实验结果显示，CondLoRA在性能上与LoRA相当，但可训练参数更少，验证了“一个单一线性层可产生任务适配低秩矩阵”的结论。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.14946v1",
      "published_date": "2024-03-22 04:38:42 UTC",
      "updated_date": "2024-03-22 04:38:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:05:37.555396"
    },
    {
      "arxiv_id": "2404.04271v1",
      "title": "Towards Effective Next POI Prediction: Spatial and Semantic Augmentation with Remote Sensing Data",
      "title_zh": "迈向有效的下一个 POI 预测：利用遥感数据进行空间和语义增强",
      "authors": [
        "Nan Jiang",
        "Haitao Yuan",
        "Jianing Si",
        "Minxiao Chen",
        "Shangguang Wang"
      ],
      "abstract": "The next point-of-interest (POI) prediction is a significant task in\nlocation-based services, yet its complexity arises from the consolidation of\nspatial and semantic intent. This fusion is subject to the influences of\nhistorical preferences, prevailing location, and environmental factors, thereby\nposing significant challenges. In addition, the uneven POI distribution further\ncomplicates the next POI prediction procedure. To address these challenges, we\nenrich input features and propose an effective deep-learning method within a\ntwo-step prediction framework. Our method first incorporates remote sensing\ndata, capturing pivotal environmental context to enhance input features\nregarding both location and semantics. Subsequently, we employ a region\nquad-tree structure to integrate urban remote sensing, road network, and POI\ndistribution spaces, aiming to devise a more coherent graph representation\nmethod for urban spatial. Leveraging this method, we construct the QR-P graph\nfor the user's historical trajectories to encapsulate historical travel\nknowledge, thereby augmenting input features with comprehensive spatial and\nsemantic insights. We devise distinct embedding modules to encode these\nfeatures and employ an attention mechanism to fuse diverse encodings. In the\ntwo-step prediction procedure, we initially identify potential spatial zones by\npredicting user-preferred tiles, followed by pinpointing specific POIs of a\ndesignated type within the projected tiles. Empirical findings from four\nreal-world location-based social network datasets underscore the remarkable\nsuperiority of our proposed approach over competitive baseline methods.",
      "tldr_zh": "本论文针对下一个POI预测的挑战，包括空间和语义意图的整合、历史偏好影响以及POI分布不均，提出了一种增强输入特征的深度学习方法。方法首先利用remote sensing data捕获环境上下文，并通过region quad-tree结构整合遥感数据、道路网络和POI分布，构建QR-P graph来表示用户历史轨迹的空间语义知识；随后，使用嵌入模块和注意力机制融合特征，并采用两步预测框架，先预测用户偏好的空间区域，再在该区域内指定POI类型。实验结果显示，该方法在四个真实世界位置社交网络数据集上显著优于基线模型，证明了其有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 11 figures, Accepted by ICDE 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.04271v1",
      "published_date": "2024-03-22 04:22:36 UTC",
      "updated_date": "2024-03-22 04:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:05:51.767311"
    },
    {
      "arxiv_id": "2403.14941v1",
      "title": "Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline",
      "title_zh": "翻译失败",
      "authors": [
        "Shuhao Li",
        "Yue Cui",
        "Jingyi Xu",
        "Libin Li",
        "Lingkai Meng",
        "Weidong Yang",
        "Fan Zhang",
        "Xiaofang Zhou"
      ],
      "abstract": "Traffic prediction has long been a focal and pivotal area in research,\nwitnessing both significant strides from city-level to road-level predictions\nin recent years. With the advancement of Vehicle-to-Everything (V2X)\ntechnologies, autonomous driving, and large-scale models in the traffic domain,\nlane-level traffic prediction has emerged as an indispensable direction.\nHowever, further progress in this field is hindered by the absence of\ncomprehensive and unified evaluation standards, coupled with limited public\navailability of data and code. This paper extensively analyzes and categorizes\nexisting research in lane-level traffic prediction, establishes a unified\nspatial topology structure and prediction tasks, and introduces a simple\nbaseline model, GraphMLP, based on graph structure and MLP networks. We have\nreplicated codes not publicly available in existing studies and, based on this,\nthoroughly and fairly assessed various models in terms of effectiveness,\nefficiency, and applicability, providing insights for practical applications.\nAdditionally, we have released three new datasets and corresponding codes to\naccelerate progress in this field, all of which can be found on\nhttps://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark.",
      "tldr_zh": "这篇论文从图结构视角统一了车道级(lane-level)交通预测的研究，分析并分类了现有工作，建立了统一的时空拓扑结构和预测任务，以解决该领域缺乏标准和资源的难题。作者提出了一种简单基线模型GraphMLP，结合图结构和MLP网络，用于交通预测。实验评估显示，该模型在有效性、效率和适用性方面提供了宝贵见解，并通过发布三个新数据集和对应代码（可从GitHub获取），加速了该领域的进展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14941v1",
      "published_date": "2024-03-22 04:21:40 UTC",
      "updated_date": "2024-03-22 04:21:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:06:03.512887"
    },
    {
      "arxiv_id": "2403.15511v1",
      "title": "Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Phai Vu Dinh",
        "Diep N. Nguyen",
        "Dinh Thai Hoang",
        "Quang Uy Nguyen",
        "Eryk Dutkiewicz",
        "Son Pham Bao"
      ],
      "abstract": "While intrusion detection systems (IDSs) benefit from the diversity and\ngeneralization of IoT data features, the data diversity (e.g., the\nheterogeneity and high dimensions of data) also makes it difficult to train\neffective machine learning models in IoT IDSs. This also leads to potentially\nredundant/noisy features that may decrease the accuracy of the detection engine\nin IDSs. This paper first introduces a novel neural network architecture called\nMultiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that\ncan process inputs from different sources with different characteristics. The\nMIAE model is trained in an unsupervised learning mode to transform the\nheterogeneous inputs into lower-dimensional representation, which helps\nclassifiers distinguish between normal behaviour and different types of\nattacks. To distil and retain more relevant features but remove less\nimportant/redundant ones during the training process, we further design and\nembed a feature selection layer right after the representation layer of MIAE\nresulting in a new model called MIAEFS. This layer learns the importance of\nfeatures in the representation vector, facilitating the selection of\ninformative features from the representation vector. The results on three IDS\ndatasets, i.e., NSLKDD, UNSW-NB15, and IDS2017, show the superior performance\nof MIAE and MIAEFS compared to other methods, e.g., conventional classifiers,\ndimensionality reduction models, unsupervised representation learning methods\nwith different input dimensions, and unsupervised feature selection models.\nMoreover, MIAE and MIAEFS combined with the Random Forest (RF) classifier\nachieve accuracy of 96.5% in detecting sophisticated attacks, e.g., Slowloris.\nThe average running time for detecting an attack sample using RF with the\nrepresentation of MIAE and MIAEFS is approximate 1.7E-6 seconds, whilst the\nmodel size is lower than 1 MB.",
      "tldr_zh": "本研究针对IoT Intrusion Detection Systems (IDSs)中数据异构性和高维度导致的冗余特征问题，提出了一种新型神经网络架构Multiple-Input Auto-Encoder (MIAE)，它通过多个子编码器处理不同来源的异构输入，并采用无监督学习将数据转换为低维表示，以提升攻击检测准确性。进一步开发了MIAEFS模型，在MIAE的表示层后嵌入特征选择层，学习特征重要性以筛选出相关信息，从而减少冗余。实验结果显示，MIAE和MIAEFS结合Random Forest (RF)分类器在NSLKDD、UNSW-NB15和IDS2017数据集上优于现有方法，检测复杂攻击如Slowloris的准确率达96.5%，运行时间约1.7E-6秒，且模型大小小于1 MB。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15511v1",
      "published_date": "2024-03-22 03:54:04 UTC",
      "updated_date": "2024-03-22 03:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:06:16.619970"
    },
    {
      "arxiv_id": "2403.15509v2",
      "title": "Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Phai Vu Dinh",
        "Quang Uy Nguyen",
        "Thai Hoang Dinh",
        "Diep N. Nguyen",
        "Bao Son Pham",
        "Eryk Dutkiewicz"
      ],
      "abstract": "Representation learning (RL) methods for cyberattack detection face the\ndiversity and sophistication of attack data, leading to the issue of mixed\nrepresentations of different classes, particularly as the number of classes\nincreases. To address this, the paper proposes a novel deep learning\narchitecture/model called the Twin Auto-Encoder (TAE). TAE first maps the input\ndata into latent space and then deterministically shifts data samples of\ndifferent classes further apart to create separable data representations,\nreferred to as representation targets. TAE's decoder then projects the input\ndata into these representation targets. After training, TAE's decoder extracts\ndata representations. TAE's representation target serves as a novel dynamic\ncodeword, which refers to the vector that represents a specific class. This\nvector is updated after each training epoch for every data sample, in contrast\nto the conventional fixed codeword that does not incorporate information from\nthe input data. We conduct extensive experiments on diverse cybersecurity\ndatasets, including seven IoT botnet datasets, two network IDS datasets, three\nmalware datasets, one cloud DDoS dataset, and ten artificial datasets as the\nnumber of classes increases. TAE boosts accuracy and F-score in attack\ndetection by around 2% compared to state-of-the-art models, achieving up to\n96.1% average accuracy in IoT attack detection. Additionally, TAE is\nwell-suited for cybersecurity applications and potentially for IoT systems,\nwith a model size of approximately 1 MB and an average running time of around\n2.6E-07 seconds for extracting a data sample.",
      "tldr_zh": "本论文提出了一种名为 Twin Auto-Encoder (TAE) 的新深度学习模型，用于解决网络攻击检测中表示学习（Representation Learning）面临的类别混合表示问题，特别是类别数量增加时。TAE 先将输入数据映射到潜在空间（latent space），然后通过确定性移位使不同类别的样本更分开，创建动态的 representation targets 作为更新后的 codeword，以提升数据表示的可分离性。实验在多种数据集上进行，包括七个 IoT botnet 数据集、两个网络 IDS 数据集等，结果显示 TAE 比现有模型提高了约 2% 的准确率和 F-score，IoT 攻击检测平均准确率达 96.1%，且模型大小仅约 1 MB，运行时间高效（约 2.6E-07 秒），适合网络安全和 IoT 系统应用。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15509v2",
      "published_date": "2024-03-22 03:39:40 UTC",
      "updated_date": "2025-04-28 22:51:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:06:28.394850"
    },
    {
      "arxiv_id": "2403.14932v3",
      "title": "Extending Token Computation for LLM Reasoning",
      "title_zh": "扩展令牌计算以用于LLM推理",
      "authors": [
        "Bingli Liao",
        "Danilo Vasconcellos Vargas"
      ],
      "abstract": "Large Language Models (LLMs) are pivotal in advancing natural language\nprocessing but often struggle with complex reasoning tasks due to inefficient\nattention distributions. In this paper, we explore the effect of increased\ncomputed tokens on LLM performance and introduce a novel method for extending\ncomputed tokens in the Chain-of-Thought (CoT) process, utilizing attention\nmechanism optimization. By fine-tuning an LLM on a domain-specific, highly\nstructured dataset, we analyze attention patterns across layers, identifying\ninefficiencies caused by non-semantic tokens with outlier high attention\nscores. To address this, we propose an algorithm that emulates early layer\nattention patterns across downstream layers to re-balance skewed attention\ndistributions and enhance knowledge abstraction. Our findings demonstrate that\nour approach not only facilitates a deeper understanding of the internal\ndynamics of LLMs but also significantly improves their reasoning capabilities,\nparticularly in non-STEM domains. Our study lays the groundwork for further\ninnovations in LLM design, aiming to create more powerful, versatile, and\nresponsible models capable of tackling a broad range of real-world\napplications.",
      "tldr_zh": "该论文探讨了大型语言模型(LLM)在复杂推理任务中的性能瓶颈，主要是由于注意力分布不高效的问题，并引入一种通过优化注意力机制来扩展计算 token 的新方法，应用于Chain-of-Thought (CoT) 过程。研究者通过在特定领域的结构化数据集上微调LLM，分析各层的注意力模式，识别非语义 token 导致的高注意力分数问题，并提出算法模拟早期层注意力以重新平衡分布并增强知识抽象。结果显示，这种方法显著提升了LLM的推理能力，尤其在非STEM领域，并为设计更强大、通用和负责任的LLM模型奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14932v3",
      "published_date": "2024-03-22 03:23:58 UTC",
      "updated_date": "2024-06-23 15:50:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:06:39.382907"
    },
    {
      "arxiv_id": "2403.14919v1",
      "title": "Hierarchical Skip Decoding for Efficient Autoregressive Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yunqi Zhu",
        "Xuebing Yang",
        "Yuanyuan Wu",
        "Wensheng Zhang"
      ],
      "abstract": "Autoregressive decoding strategy is a commonly used method for text\ngeneration tasks with pre-trained language models, while early-exiting is an\neffective approach to speedup the inference stage. In this work, we propose a\nnovel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient\nautoregressive text generation. Different from existing methods that require\nadditional trainable components, HSD is a plug-and-play method applicable to\nautoregressive text generation models, it adaptively skips decoding layers in a\nhierarchical manner based on the current sequence length, thereby reducing\ncomputational workload and allocating computation resources. Comprehensive\nexperiments on five text generation datasets with pre-trained language models\ndemonstrate HSD's advantages in balancing efficiency and text quality. With\nalmost half of the layers skipped, HSD can sustain 90% of the text quality\ncompared to vanilla autoregressive decoding, outperforming the competitive\napproaches.",
      "tldr_zh": "该研究提出了一种名为 Hierarchical Skip Decoding (HSD) 的新策略，用于提升 autoregressive text generation 的效率。HSD 是一种 plug-and-play 方法，无需额外可训练组件，通过根据当前序列长度自适应地跳过解码层，实现层次化计算资源分配，从而减少计算负载。实验在五个文本生成数据集上验证了 HSD 的优势，即使跳过近半层，HSD 也能维持 vanilla autoregressive decoding 的 90% 文本质量，并超越竞争方法，在效率和质量之间取得了良好平衡。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14919v1",
      "published_date": "2024-03-22 02:44:05 UTC",
      "updated_date": "2024-03-22 02:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:06:49.376619"
    },
    {
      "arxiv_id": "2404.07960v1",
      "title": "Content Knowledge Identification with Multi-Agent Large Language Models (LLMs)",
      "title_zh": "翻译失败",
      "authors": [
        "Kaiqi Yang",
        "Yucheng Chu",
        "Taylor Darwin",
        "Ahreum Han",
        "Hang Li",
        "Hongzhi Wen",
        "Yasemin Copur-Gencturk",
        "Jiliang Tang",
        "Hui Liu"
      ],
      "abstract": "Teachers' mathematical content knowledge (CK) is of vital importance and need\nin teacher professional development (PD) programs. Computer-aided asynchronous\nPD systems are the most recent proposed PD techniques, which aim to help\nteachers improve their PD equally with fewer concerns about costs and\nlimitations of time or location. However, current automatic CK identification\nmethods, which serve as one of the core techniques of asynchronous PD systems,\nface challenges such as diversity of user responses, scarcity of high-quality\nannotated data, and low interpretability of the predictions. To tackle these\nchallenges, we propose a Multi-Agent LLMs-based framework, LLMAgent-CK, to\nassess the user responses' coverage of identified CK learning goals without\nhuman annotations. By taking advantage of multi-agent LLMs in strong\ngeneralization ability and human-like discussions, our proposed LLMAgent-CK\npresents promising CK identifying performance on a real-world mathematical CK\ndataset MaCKT. Moreover, our case studies further demonstrate the working of\nthe multi-agent framework.",
      "tldr_zh": "该研究针对教师专业发展（PD）中的数学内容知识（CK）识别问题，提出了一种基于多智能体Large Language Models (LLMs)的框架，名为LLMAgent-CK，以评估用户响应是否覆盖标识的CK学习目标，而无需人工标注。该框架利用多智能体LLMs的强大泛化能力和类人讨论能力，解决了现有方法面临的用户响应多样性、数据稀缺性和预测可解释性低等挑战。在真实世界的MaCKT数学CK数据集上，LLMAgent-CK展示了出色的识别性能，并通过案例研究验证了其实际工作机制。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.07960v1",
      "published_date": "2024-03-22 02:37:33 UTC",
      "updated_date": "2024-03-22 02:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:07:00.918782"
    },
    {
      "arxiv_id": "2404.08654v1",
      "title": "Optimal path for Biomedical Text Summarization Using Pointer GPT",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunkyung Han",
        "Jaesik Choi"
      ],
      "abstract": "Biomedical text summarization is a critical tool that enables clinicians to\neffectively ascertain patient status. Traditionally, text summarization has\nbeen accomplished with transformer models, which are capable of compressing\nlong documents into brief summaries. However, transformer models are known to\nbe among the most challenging natural language processing (NLP) tasks.\nSpecifically, GPT models have a tendency to generate factual errors, lack\ncontext, and oversimplify words. To address these limitations, we replaced the\nattention mechanism in the GPT model with a pointer network. This modification\nwas designed to preserve the core values of the original text during the\nsummarization process. The effectiveness of the Pointer-GPT model was evaluated\nusing the ROUGE score. The results demonstrated that Pointer-GPT outperformed\nthe original GPT model. These findings suggest that pointer networks can be a\nvaluable addition to EMR systems and can provide clinicians with more accurate\nand informative summaries of patient medical records. This research has the\npotential to usher in a new paradigm in EMR systems and to revolutionize the\nway that clinicians interact with patient medical records.",
      "tldr_zh": "本研究针对生物医学文本摘要中的问题，指出传统 GPT 模型容易产生事实错误、缺乏上下文和过度简化，影响临床医生对患者状态的评估。作者提出 Pointer-GPT 方法，将 GPT 模型中的 attention 机制替换为 pointer network，以更好地保留原始文本的核心价值。实验结果显示，Pointer-GPT 在 ROUGE score 上优于原 GPT 模型，提供更准确的摘要。该创新有望提升电子病历 (EMR) 系统，并革新临床医生与患者医疗记录的互动方式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "3 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08654v1",
      "published_date": "2024-03-22 02:13:23 UTC",
      "updated_date": "2024-03-22 02:13:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:07:14.022342"
    },
    {
      "arxiv_id": "2403.14895v1",
      "title": "Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning",
      "title_zh": "Stance Reasoner：社交媒体上的零样本立场检测与显式推理",
      "authors": [
        "Maksym Taranukhin",
        "Vered Shwartz",
        "Evangelos Milios"
      ],
      "abstract": "Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.",
      "tldr_zh": "该论文提出了一种名为 Stance Reasoner 的零样本立场检测方法，用于从社交媒体内容中提取用户对新主题的意见。方法利用预训练语言模型作为背景知识来源，通过 chain-of-thought in-context learning 生成显式中间推理步骤，从而指导模型对文档立场的推断。实验结果显示，Stance Reasoner 在 3 个 Twitter 数据集上超越了当前最先进模型（包括完全监督模型），并在目标泛化性和提供可解释解释方面表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.14895v1",
      "published_date": "2024-03-22 00:58:28 UTC",
      "updated_date": "2024-03-22 00:58:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:07:26.041887"
    },
    {
      "arxiv_id": "2403.15504v1",
      "title": "SymboSLAM: Semantic Map Generation in a Multi-Agent System",
      "title_zh": "SymboSL",
      "authors": [
        "Brandon Curtis Colelough"
      ],
      "abstract": "Sub-symbolic artificial intelligence methods dominate the fields of\nenvironment-type classification and Simultaneous Localisation and Mapping.\nHowever, a significant area overlooked within these fields is solution\ntransparency for the human-machine interaction space, as the sub-symbolic\nmethods employed for map generation do not account for the explainability of\nthe solutions generated. This paper proposes a novel approach to\nenvironment-type classification through Symbolic Simultaneous Localisation and\nMapping, SymboSLAM, to bridge the explainability gap. Our method for\nenvironment-type classification observes ontological reasoning used to\nsynthesise the context of an environment through the features found within. We\nachieve explainability within the model by presenting operators with\nenvironment-type classifications overlayed by a semantically labelled occupancy\nmap of landmarks and features. We evaluate SymboSLAM with ground-truth maps of\nthe Canberra region, demonstrating method effectiveness. We assessed the system\nthrough both simulations and real-world trials.",
      "tldr_zh": "这篇论文指出了现有 Sub-symbolic AI 方法在环境分类和 Simultaneous Localisation and Mapping (SLAM) 中忽略了解决方案的可解释性问题，并提出了一种新方法 SymboSLAM，以桥接这一差距。SymboSLAM 通过本体推理（ontological reasoning）来合成环境上下文，利用环境中发现的特征生成语义标记的占用地图（semantically labelled occupancy map），并将环境类型分类叠加展示给操作者，以提升人类-机器交互的透明度。实验在 Canberra 地区的地面实况地图上进行评估，证明了方法的有效性，并通过模拟和真实世界试验验证了其性能。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.15504v1",
      "published_date": "2024-03-22 00:48:52 UTC",
      "updated_date": "2024-03-22 00:48:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:07:39.760585"
    },
    {
      "arxiv_id": "2405.01398v1",
      "title": "Advancing Frontiers in SLAM: A Survey of Symbolic Representation and Human-Machine Teaming in Environmental Mapping",
      "title_zh": "SLAM",
      "authors": [
        "Brandon Curtis Colelough"
      ],
      "abstract": "This survey paper presents a comprehensive overview of the latest\nadvancements in the field of Simultaneous Localization and Mapping (SLAM) with\na focus on the integration of symbolic representation of environment features.\nThe paper synthesizes research trends in multi-agent systems (MAS) and\nhuman-machine teaming, highlighting their applications in both symbolic and\nsub-symbolic SLAM tasks. The survey emphasizes the evolution and significance\nof ontological designs and symbolic reasoning in creating sophisticated 2D and\n3D maps of various environments. Central to this review is the exploration of\ndifferent architectural approaches in SLAM, with a particular interest in the\nfunctionalities and applications of edge and control agent architectures in MAS\nsettings. This study acknowledges the growing demand for enhanced human-machine\ncollaboration in mapping tasks and examines how these collaborative efforts\nimprove the accuracy and efficiency of environmental mapping",
      "tldr_zh": "这篇调查论文概述了同时定位与建图（SLAM）领域的最新进展，重点关注环境特征的符号表示及其与多智能体系统（MAS）和人机协作的整合。论文综合了符号和子符号 SLAM 任务中的研究趋势，强调了本体设计和符号推理在创建复杂 2D 和 3D 环境地图中的演变与重要性。特别探讨了边缘和控制代理架构在 MAS 设置中的功能应用，以及人机协作如何提升环境映射的准确性和效率，为未来 SLAM 研究提供了宝贵见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.01398v1",
      "published_date": "2024-03-22 00:48:48 UTC",
      "updated_date": "2024-03-22 00:48:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T18:07:51.386165"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 83,
  "processed_papers_count": 83,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T18:08:15.179978"
}
{
  "date": "2024-02-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-29 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 论文主要聚焦于 AI 领域的创新与应用，包括大型语言模型（LLMs）的优化、多模态数据融合、强化学习和知识编辑等关键话题，令人印象深刻的文章有 StarCoder 2 的开源 LLM 框架和 Causal Graph ODE 的多代理系统建模，由知名学者如 Yizhou Sun 和 Yu Zheng 参与，突显了 AI 在实际场景中的潜力。\n\n下面，我们逐一简要概述今日论文，先优先讨论 AI 和 LLMs 相关的高影响力文章，再快速掠过其他领域的内容。重点保留核心学术术语，并突出主要贡献和发现。\n\n1. **StarCoder 2 and The Stack v2: The Next Generation**（中文：StarCoder 2 和 The Stack v2：下一代模型；英文：StarCoder 2 and The Stack v2: The Next Generation）  \n   BigCode 项目推出的开源代码 LLM，基于扩展的 The Stack v2 数据集训练，模型参数达 15B，在代码生成和多语言任务上显著超越同规模模型，主要贡献是通过透明的数据来源提升代码 LLM 的性能和适用性。\n\n2. **Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems**（中文：因果图 ODE：在多代理动态系统中建模连续治疗效果；英文：Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems）  \n   由 Yizhou Sun 和 Quanquan Gu 等学者提出，该方法使用图神经网络（GNN）捕获多代理系统的连续交互，显著提升了如 COVID-19 政策评估的预测准确性，主要发现是通过对抗学习缓解混淆偏差，实现多治疗干预的精确建模。\n\n3. **LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction**（中文：LLM-集成：电商产品属性值提取的最佳大型语言模型集成方法；英文：LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction）  \n   该论文提出一种集成算法，通过迭代学习权重优化多个 LLM 的输出，在电商属性提取任务中超越单一模型，主要贡献是提升了提取准确性，并在实际部署中提高了 GMV 和 CTR 等指标。\n\n4. **UniTS: A Unified Multi-Task Time Series Model**（中文：UniTS：统一的时序多任务模型；英文：UniTS: A Unified Multi-Task Time Series Model）  \n   由 Marinka Zitnik 等人开发，该模型使用任务标记化整合预测和生成任务，在各种时序数据上表现出色，主要发现是通过多领域预训练实现高效的少样本学习和泛化。\n\n5. **ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph**（中文：ToolNet：通过工具图连接大型语言模型与海量工具；英文：ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph）  \n   该方法构建工具图，让 LLM 通过图导航处理多跳任务，主要贡献是扩展 LLM 的工具使用能力，在复杂场景中实现高效的多工具集成。\n\n6. **GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction**（中文：GoalNet：基于目标区域的行人轨迹预测；英文：GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction）  \n   提出一种先预测行人目标点再生成轨迹的框架，主要发现是结合场景上下文提高预测准确性，在自动驾驶中提升安全性。\n\n7. **CPO: Controllable Preference Optimization for Multi-Objective Alignment**（中文：CPO：可控偏好优化用于多目标对齐；英文：CPO: Controllable Preference Optimization for Multi-Objective Alignment）  \n   该方法通过指定偏好分数引导 LLM 多目标对齐，主要贡献是平衡多任务性能，避免“对齐税”问题。\n\n8. **AKEW: Assessing Knowledge Editing in the Wild**（中文：AKEW：在野外评估知识编辑；英文：AKEW: Assessing Knowledge Editing in the Wild）  \n   引入新基准评估 LLM 的知识编辑能力，主要发现是现有方法在真实场景中存在显著差距。\n\n9. **SoD²: Statically Optimizing Dynamic Deep Neural Network**（中文：SoD²：静态优化动态深度神经网络；英文：SoD²: Statically Optimizing Dynamic Deep Neural Network）  \n   提出框架优化动态 DNN 的形状和内存，主要贡献是通过秩和维度传播减少执行延迟和内存消耗。\n\n10. **FusionVision: A comprehensive approach of 3D object reconstruction and segmentation from RGB-D cameras**（中文：FusionVision：从 RGB-D 相机进行 3D 对象重建和分割的综合方法；英文：FusionVision: A comprehensive approach of 3D object reconstruction and segmentation from RGB-D cameras）  \n    该方法结合 YOLO 和 FastSAM 进行 RGB-D 数据处理，主要发现是提升了 3D 场景理解的精度。\n\n其他论文涉及更广泛领域，我们快速掠过：\n\n11. **Identification of important nodes in the information propagation network based on the artificial intelligence method**（中文：基于人工智能方法的网络信息传播重要节点识别；英文：Identification of important nodes in the information propagation network based on the artificial intelligence method）  \n    结合 DEMATEL 和 GSM 识别网络关键节点，主要贡献是提升网络分析的全局洞察。\n\n12. **Learning to Find Missing Video Frames with Synthetic Data Augmentation**（中文：使用合成数据增强学习查找缺失视频帧；英文：Learning to Find Missing Video Frames with Synthetic Data Augmentation）  \n    使用 cGAN 生成热像数据，主要发现是改善了驾驶辅助系统的实时监控。\n\n13. **EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation**（中文：EBBS：零样本机器翻译的双层 Beam 搜索集成方法；英文：EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation）  \n    提出集成方法提升零样本翻译质量，主要贡献是结合直接和枢纽翻译。\n\n14. **LLMs in Political Science: Heralding a New Era of Visual Analysis**（中文：LLMs 在政治科学中的应用：视觉分析的新时代；英文：LLMs in Political Science: Heralding a New Era of Visual Analysis）  \n    使用 Gemini 进行图像分析，主要发现是简化了政治图像任务。\n\n15. **NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism**（中文：NewsBench：评估中文新闻编辑能力的系统框架；英文：NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism）  \n    评估 LLM 在中文新闻中的编辑能力，主要贡献是突出安全性和准确性差距。\n\n剩余论文多为特定领域优化，如时间序列预测（e.g., TimeXer）、强化学习（e.g., RL-GPT）和图像处理（e.g., SegNet），这些工作虽有技术创新，但影响力相对有限，仅快速提及：它们分别在各自领域提升了模型鲁棒性和效率，例如 TimeXer 通过谱空间处理改善时序预测泛化，RL-GPT 整合强化学习提升 LLM 决策，而 SegNet 优化了图像分割的效率。\n\n总之，今天的论文展示了 AI 领域的多样性与深度，LLMs 和多代理系统相关工作尤为值得关注，期待后续应用。明天见！",
  "papers": [
    {
      "arxiv_id": "2403.00196v1",
      "title": "Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras",
      "title_zh": "利用合成数据增强学习找到缺失视频帧：一个通用框架及其在利用 RGB 相机生成热图像中的应用",
      "authors": [
        "Mathias Viborg Andersen",
        "Ross Greer",
        "Andreas Møgelmose",
        "Mohan Trivedi"
      ],
      "abstract": "Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on\naccurate driver perception within the vehicle cabin, often leveraging a\ncombination of sensing modalities. However, these modalities operate at varying\nrates, posing challenges for real-time, comprehensive driver state monitoring.\nThis paper addresses the issue of missing data due to sensor frame rate\nmismatches, introducing a generative model approach to create synthetic yet\nrealistic thermal imagery. We propose using conditional generative adversarial\nnetworks (cGANs), specifically comparing the pix2pix and CycleGAN\narchitectures. Experimental results demonstrate that pix2pix outperforms\nCycleGAN, and utilizing multi-view input styles, especially stacked views,\nenhances the accuracy of thermal image generation. Moreover, the study\nevaluates the model's generalizability across different subjects, revealing the\nimportance of individualized training for optimal performance. The findings\nsuggest the potential of generative models in addressing missing frames,\nadvancing driver state monitoring for intelligent vehicles, and underscoring\nthe need for continued research in model generalization and customization.",
      "tldr_zh": "该论文提出一个通用框架，使用合成数据增强来解决智能车辆中传感器帧率不匹配导致的视频帧缺失问题，特别应用于用 RGB 相机生成热成像图像，以提升驾驶员状态监测。研究比较了条件生成对抗网络 (cGANs) 中的 pix2pix 和 CycleGAN 架构，实验结果显示 pix2pix 性能更优，而采用多视图输入（如堆叠视图）显著提高了热图像生成的准确性。论文还评估了模型在不同主体间的泛化能力，强调个体化训练的重要性，并指出生成模型有望推进智能车辆技术，但需进一步研究泛化性和定制化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00196v1",
      "published_date": "2024-02-29 23:52:15 UTC",
      "updated_date": "2024-02-29 23:52:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:11:03.109926"
    },
    {
      "arxiv_id": "2403.00190v1",
      "title": "Identification of important nodes in the information propagation network based on the artificial intelligence method",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Yuan",
        "Tianbo Song",
        "Jerry Yao"
      ],
      "abstract": "This study presents an integrated approach for identifying key nodes in\ninformation propagation networks using advanced artificial intelligence\nmethods. We introduce a novel technique that combines the Decision-making Trial\nand Evaluation Laboratory (DEMATEL) method with the Global Structure Model\n(GSM), creating a synergistic model that effectively captures both local and\nglobal influences within a network. This method is applied across various\ncomplex networks, such as social, transportation, and communication systems,\nutilizing the Global Network Influence Dataset (GNID). Our analysis highlights\nthe structural dynamics and resilience of these networks, revealing insights\ninto node connectivity and community formation. The findings demonstrate the\neffectiveness of our AI-based approach in offering a comprehensive\nunderstanding of network behavior, contributing significantly to strategic\nnetwork analysis and optimization.",
      "tldr_zh": "本研究提出了一种整合人工智能方法，用于识别信息传播网络中关键节点，具体结合了 Decision-making Trial and Evaluation Laboratory (DEMATEL) 方法与 Global Structure Model (GSM)，以捕捉网络的局部和全局影响。 该方法应用于社交、交通和通信等复杂网络，利用 Global Network Influence Dataset (GNID) 进行分析，揭示了网络的结构动态、弹性、节点连通性以及社区形成。 结果表明，该AI-based 技术在提供网络行为全面理解方面表现出色，对战略网络分析和优化具有显著贡献。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00190v1",
      "published_date": "2024-02-29 23:43:08 UTC",
      "updated_date": "2024-02-29 23:43:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:11:25.884517"
    },
    {
      "arxiv_id": "2403.00178v1",
      "title": "Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Zijie Huang",
        "Jeehyun Hwang",
        "Junkai Zhang",
        "Jinwoo Baik",
        "Weitong Zhang",
        "Dominik Wodarz",
        "Yizhou Sun",
        "Quanquan Gu",
        "Wei Wang"
      ],
      "abstract": "Real-world multi-agent systems are often dynamic and continuous, where the\nagents co-evolve and undergo changes in their trajectories and interactions\nover time. For example, the COVID-19 transmission in the U.S. can be viewed as\na multi-agent system, where states act as agents and daily population movements\nbetween them are interactions. Estimating the counterfactual outcomes in such\nsystems enables accurate future predictions and effective decision-making, such\nas formulating COVID-19 policies. However, existing methods fail to model the\ncontinuous dynamic effects of treatments on the outcome, especially when\nmultiple treatments (e.g., \"stay-at-home\" and \"get-vaccine\" policies) are\napplied simultaneously. To tackle this challenge, we propose Causal Graph\nOrdinary Differential Equations (CAG-ODE), a novel model that captures the\ncontinuous interaction among agents using a Graph Neural Network (GNN) as the\nODE function. The key innovation of our model is to learn time-dependent\nrepresentations of treatments and incorporate them into the ODE function,\nenabling precise predictions of potential outcomes. To mitigate confounding\nbias, we further propose two domain adversarial learning-based objectives,\nwhich enable our model to learn balanced continuous representations that are\nnot affected by treatments or interference. Experiments on two datasets (i.e.,\nCOVID-19 and tumor growth) demonstrate the superior performance of our proposed\nmodel.",
      "tldr_zh": "本研究针对多智能体动态系统中治疗效果的连续建模问题，提出了一种名为 Causal Graph ODE (CAG-ODE) 的新模型，以处理智能体间持续交互和多治疗同时应用（如COVID-19政策）的挑战。CAG-ODE 使用 Graph Neural Network (GNN) 作为 Ordinary Differential Equations (ODE) 函数，捕捉时间相关的治疗表示，并将其整合到模型中，以实现精确的反事实结果预测。同时，引入基于域对抗学习的损失函数来减少混杂偏差，确保表示的平衡性。在COVID-19和肿瘤生长数据集上的实验表明，该模型显著优于现有方法，提供更准确的预测和决策支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00178v1",
      "published_date": "2024-02-29 23:07:07 UTC",
      "updated_date": "2024-02-29 23:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:11:26.114011"
    },
    {
      "arxiv_id": "2403.00176v1",
      "title": "SoD$^2$: Statically Optimizing Dynamic Deep Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Niu",
        "Gagan Agrawal",
        "Bin Ren"
      ],
      "abstract": "Though many compilation and runtime systems have been developed for DNNs in\nrecent years, the focus has largely been on static DNNs. Dynamic DNNs, where\ntensor shapes and sizes and even the set of operators used are dependent upon\nthe input and/or execution, are becoming common. This paper presents SoD$^2$, a\ncomprehensive framework for optimizing Dynamic DNNs. The basis of our approach\nis a classification of common operators that form DNNs, and the use of this\nclassification towards a Rank and Dimension Propagation (RDP) method. This\nframework statically determines the shapes of operators as known constants,\nsymbolic constants, or operations on these. Next, using RDP we enable a series\nof optimizations, like fused code generation, execution (order) planning, and\neven runtime memory allocation plan generation. By evaluating the framework on\n10 emerging Dynamic DNNs and comparing it against several existing systems, we\ndemonstrate both reductions in execution latency and memory requirements, with\nRDP-enabled key optimizations responsible for much of the gains. Our evaluation\nresults show that SoD$^2$ runs up to $3.9\\times$ faster than these systems\nwhile saving up to $88\\%$ peak memory consumption.",
      "tldr_zh": "这篇论文提出了 SoD² 框架，用于静态优化动态深度神经网络 (Dynamic DNNs)，其中张量形状、大小和操作集依赖于输入或执行过程。SoD² 通过分类常见操作器并采用 Rank and Dimension Propagation (RDP) 方法，静态确定操作器形状（包括已知常量、符号常量或相关操作），从而启用优化如融合代码生成、执行顺序规划和运行时内存分配规划。实验在 10 个动态 DNNs 上评估，结果显示 SoD² 比现有系统运行速度快达 3.9 倍，并节省高达 88% 的峰值内存消耗。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00176v1",
      "published_date": "2024-02-29 23:04:01 UTC",
      "updated_date": "2024-02-29 23:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:11:38.130592"
    },
    {
      "arxiv_id": "2403.00863v2",
      "title": "LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction",
      "title_zh": "LLM-Ensemble：电子商务产品属性值提取",
      "authors": [
        "Chenhao Fang",
        "Xiaohan Li",
        "Zezhong Fan",
        "Jianpeng Xu",
        "Kaushiki Nag",
        "Evren Korpeoglu",
        "Sushant Kumar",
        "Kannan Achan"
      ],
      "abstract": "Product attribute value extraction is a pivotal component in Natural Language\nProcessing (NLP) and the contemporary e-commerce industry. The provision of\nprecise product attribute values is fundamental in ensuring high-quality\nrecommendations and enhancing customer satisfaction. The recently emerging\nLarge Language Models (LLMs) have demonstrated state-of-the-art performance in\nnumerous attribute extraction tasks, without the need for domain-specific\ntraining data. Nevertheless, varying strengths and weaknesses are exhibited by\ndifferent LLMs due to the diversity in data, architectures, and\nhyperparameters. This variation makes them complementary to each other, with no\nsingle LLM dominating all others. Considering the diverse strengths and\nweaknesses of LLMs, it becomes necessary to develop an ensemble method that\nleverages their complementary potentials. In this paper, we propose a novel\nalgorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute\nvalue extraction. We iteratively learn the weights for different LLMs to\naggregate the labels with weights to predict the final attribute value. Not\nonly can our proposed method be proven theoretically optimal, but it also\nensures efficient computation, fast convergence, and safe deployment. We have\nalso conducted extensive experiments with various state-of-the-art LLMs,\nincluding Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's\ninternal data. Our offline metrics demonstrate that the LLM-ensemble method\noutperforms all the state-of-the-art single LLMs on Walmart's internal dataset.\nThis method has been launched in several production models, leading to improved\nGross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate\n(CVR), and Add-to-Cart Rate (ATC).",
      "tldr_zh": "这篇论文提出了一种名为 LLM-Ensemble 的最优大型语言模型集成方法，用于电商产品的属性值提取（attribute value extraction），旨在利用不同 LLMs（如 Llama2-13B、Llama2-70B、PaLM-2、GPT-3.5 和 GPT-4）的互补优势。方法通过迭代学习权重来聚合各 LLMs 的输出标签，预测最终属性值，并理论上证明其最优性，同时确保高效计算、快速收敛和安全部署。在 Walmart 的内部数据集上进行的实验显示，LLM-Ensemble 优于所有单一 LLMs，提升了离线指标，并在生产环境中提高了 Gross Merchandise Volume (GMV)、Click-Through Rate (CTR)、Conversion Rate (CVR) 和 Add-to-Cart Rate (ATC)。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "SIGIR 2024 industry track",
      "pdf_url": "http://arxiv.org/pdf/2403.00863v2",
      "published_date": "2024-02-29 23:03:19 UTC",
      "updated_date": "2024-06-20 07:10:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:11:50.529026"
    },
    {
      "arxiv_id": "2403.00175v2",
      "title": "FusionVision: A comprehensive approach of 3D object reconstruction and segmentation from RGB-D cameras using YOLO and fast segment anything",
      "title_zh": "翻译失败",
      "authors": [
        "Safouane El Ghazouali",
        "Youssef Mhirit",
        "Ali Oukhrid",
        "Umberto Michelucci",
        "Hichem Nouira"
      ],
      "abstract": "In the realm of computer vision, the integration of advanced techniques into\nthe processing of RGB-D camera inputs poses a significant challenge, given the\ninherent complexities arising from diverse environmental conditions and varying\nobject appearances. Therefore, this paper introduces FusionVision, an\nexhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D\nimagery. Traditional computer vision systems face limitations in simultaneously\ncapturing precise object boundaries and achieving high-precision object\ndetection on depth map as they are mainly proposed for RGB cameras. To address\nthis challenge, FusionVision adopts an integrated approach by merging\nstate-of-the-art object detection techniques, with advanced instance\nsegmentation methods. The integration of these components enables a holistic\n(unified analysis of information obtained from both color \\textit{RGB} and\ndepth \\textit{D} channels) interpretation of RGB-D data, facilitating the\nextraction of comprehensive and accurate object information. The proposed\nFusionVision pipeline employs YOLO for identifying objects within the RGB image\ndomain. Subsequently, FastSAM, an innovative semantic segmentation model, is\napplied to delineate object boundaries, yielding refined segmentation masks.\nThe synergy between these components and their integration into 3D scene\nunderstanding ensures a cohesive fusion of object detection and segmentation,\nenhancing overall precision in 3D object segmentation. The code and pre-trained\nmodels are publicly available at https://github.com/safouaneelg/FusionVision/.",
      "tldr_zh": "这篇论文介绍了FusionVision，一种全面的管道，用于从RGB-D相机输入实现3D对象重建和分割，解决了传统系统在处理对象边界和深度地图检测方面的局限性。该方法整合了YOLO进行RGB图像中的对象检测，以及FastSAM进行语义分割以生成精确的分割掩码，从而实现RGB和深度通道信息的统一融合。实验验证显示，FusionVision显著提升了3D对象分割的整体精度，并公开了代码和预训练模型以供进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2403.00175v2",
      "published_date": "2024-02-29 22:59:27 UTC",
      "updated_date": "2024-05-01 12:34:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:12:01.232046"
    },
    {
      "arxiv_id": "2403.00172v1",
      "title": "Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu An",
        "Xianzhong Ding",
        "Wan Du"
      ],
      "abstract": "Recent research has shown the potential of Model-based Reinforcement Learning\n(MBRL) to enhance energy efficiency of Heating, Ventilation, and Air\nConditioning (HVAC) systems. However, existing methods rely on black-box\nthermal dynamics models and stochastic optimizers, lacking reliability\nguarantees and posing risks to occupant health. In this work, we overcome the\nreliability bottleneck by redesigning HVAC controllers using decision trees\nextracted from existing thermal dynamics models and historical data. Our\ndecision tree-based policies are deterministic, verifiable, interpretable, and\nmore energy-efficient than current MBRL methods. First, we introduce a novel\nverification criterion for RL agents in HVAC control based on domain knowledge.\nSecond, we develop a policy extraction procedure that produces a verifiable\ndecision tree policy. We found that the high dimensionality of the thermal\ndynamics model input hinders the efficiency of policy extraction. To tackle the\ndimensionality challenge, we leverage importance sampling conditioned on\nhistorical data distributions, significantly improving policy extraction\nefficiency. Lastly, we present an offline verification algorithm that\nguarantees the reliability of a control policy. Extensive experiments show that\nour method saves 68.4% more energy and increases human comfort gain by 14.8%\ncompared to the state-of-the-art method, in addition to an 1127x reduction in\ncomputation overhead. Our code and data are available at\nhttps://github.com/ryeii/Veri_HVAC",
      "tldr_zh": "本研究重新设计了HVAC控制的RL代理，旨在克服现有Model-based Reinforcement Learning (MBRL)方法的黑箱模型问题，通过从热力学模型和历史数据中提取decision trees生成确定性、可验证和可解释的政策，从而提升能源效率和可靠性。具体而言，该方法引入基于领域知识的验证标准、开发政策提取过程（利用importance sampling处理高维度输入挑战），并提出离线验证算法确保策略可靠性。实验结果显示，与最先进方法相比，该方法节省68.4%的能源、提高14.8%的人类舒适度，并将计算开销减少1127倍。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted for the 61st Design Automation Conference (DAC)",
      "pdf_url": "http://arxiv.org/pdf/2403.00172v1",
      "published_date": "2024-02-29 22:42:23 UTC",
      "updated_date": "2024-02-29 22:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:12:13.851805"
    },
    {
      "arxiv_id": "2403.00154v2",
      "title": "LLMs in Political Science: Heralding a New Era of Visual Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Wang"
      ],
      "abstract": "Interest is increasing among political scientists in leveraging the extensive\ninformation available in images. However, the challenge of interpreting these\nimages lies in the need for specialized knowledge in computer vision and access\nto specialized hardware. As a result, image analysis has been limited to a\nrelatively small group within the political science community. This landscape\ncould potentially change thanks to the rise of large language models (LLMs).\nThis paper aims to raise awareness of the feasibility of using Gemini for image\ncontent analysis. A retrospective analysis was conducted on a corpus of 688\nimages. Content reports were elicited from Gemini for each image and then\nmanually evaluated by the authors. We find that Gemini is highly accurate in\nperforming object detection, which is arguably the most common and fundamental\ntask in image analysis for political scientists. Equally important, we show\nthat it is easy to implement as the entire command consists of a single prompt\nin natural language; it is fast to run and should meet the time budget of most\nresearchers; and it is free to use and does not require any specialized\nhardware. In addition, we illustrate how political scientists can leverage\nGemini for other image understanding tasks, including face identification,\nsentiment analysis, and caption generation. Our findings suggest that Gemini\nand other similar LLMs have the potential to drastically stimulate and\naccelerate image research in political science and social sciences more\nbroadly.",
      "tldr_zh": "本研究探讨了大语言模型(LLMs)如Gemini在政治科学图像分析中的潜力，解决传统分析面临的计算机视觉专业知识和硬件需求障碍。作者对688张图像进行回顾性分析，使用Gemini生成内容报告并手动评估，结果显示Gemini在物体检测(object detection)上高度准确，且操作简单（仅需一个自然语言提示）、运行快速、免费且无需专用硬件。论文还展示了Gemini可应用于其他任务，包括面部识别(face identification)、情感分析(sentiment analysis)和标题生成(caption generation)。这些发现表明，LLMs有望大幅刺激和加速政治科学及更广泛的社会科学图像研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.00154v2",
      "published_date": "2024-02-29 22:11:20 UTC",
      "updated_date": "2024-03-27 02:21:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:12:27.064899"
    },
    {
      "arxiv_id": "2403.00143v2",
      "title": "Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous Constituency Parsing",
      "title_zh": "翻译失败",
      "authors": [
        "Behzad Shayegh",
        "Yuqiao Wen",
        "Lili Mou"
      ],
      "abstract": "We address unsupervised discontinuous constituency parsing, where we observe\na high variance in the performance of the only previous model in the\nliterature. We propose to build an ensemble of different runs of the existing\ndiscontinuous parser by averaging the predicted trees, to stabilize and boost\nperformance. To begin with, we provide comprehensive computational complexity\nanalysis (in terms of P and NP-complete) for tree averaging under different\nsetups of binarity and continuity. We then develop an efficient exact algorithm\nto tackle the task, which runs in a reasonable time for all samples in our\nexperiments. Results on three datasets show our method outperforms all\nbaselines in all metrics; we also provide in-depth analyses of our approach.",
      "tldr_zh": "该研究针对无监督 discontinuous constituency parsing 的高性能变异问题，提出了一种基于 ensemble 的树平均（tree averaging）算法，通过平均不同运行的预测树来稳定和提升解析性能。首先，论文提供了树平均在不同二元性和连续性设置下的计算复杂性分析，包括 P 和 NP-complete 问题，并开发了一个高效的精确算法，能够在合理时间内处理所有实验样本。在三个数据集上的实验结果显示，该方法在所有指标上优于基线模型，并进行了深入分析。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00143v2",
      "published_date": "2024-02-29 21:49:31 UTC",
      "updated_date": "2024-11-05 21:04:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:12:36.927942"
    },
    {
      "arxiv_id": "2403.00144v2",
      "title": "EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqiao Wen",
        "Behzad Shayegh",
        "Chenyang Huang",
        "Yanshuai Cao",
        "Lili Mou"
      ],
      "abstract": "The ability of zero-shot translation emerges when we train a multilingual\nmodel with certain translation directions; the model can then directly\ntranslate in unseen directions. Alternatively, zero-shot translation can be\naccomplished by pivoting through a third language (e.g., English). In our work,\nwe observe that both direct and pivot translations are noisy and achieve less\nsatisfactory performance. We propose EBBS, an ensemble method with a novel\nbi-level beam search algorithm, where each ensemble component explores its own\nprediction step by step at the lower level but they are synchronized by a \"soft\nvoting\" mechanism at the upper level. Results on two popular multilingual\ntranslation datasets show that EBBS consistently outperforms direct and pivot\ntranslations as well as existing ensemble techniques. Further, we can distill\nthe ensemble's knowledge back to the multilingual model to improve inference\nefficiency; profoundly, our EBBS-based distillation does not sacrifice, or even\nimproves, the translation quality.",
      "tldr_zh": "这篇论文针对零资源机器翻译（Zero-Shot Machine Translation）中的噪声问题，提出了一种名为 EBBS 的集成方法（Ensemble），它采用新型的双层 Beam Search（Bi-Level Beam Search）算法：在下层，每个集成组件逐步探索预测，在上层通过“软投票”机制同步决策。实验结果显示，EBBS 在两个流行多语言翻译数据集上，显著优于直接翻译和中转翻译方法，以及现有集成技术。此外，通过知识蒸馏（distillation）将 EBBS 的知识回传到多语言模型中，能提高推理效率，同时不牺牲或甚至提升翻译质量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7; I.2.6; I.2.m; I.5.1; I.7.m"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.00144v2",
      "published_date": "2024-02-29 21:49:31 UTC",
      "updated_date": "2025-02-05 19:44:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:12:50.639766"
    },
    {
      "arxiv_id": "2403.00141v1",
      "title": "EROS: Entity-Driven Controlled Policy Document Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Joykirat Singh",
        "Sehban Fazili",
        "Rohan Jain",
        "Md Shad Akhtar"
      ],
      "abstract": "Privacy policy documents have a crucial role in educating individuals about\nthe collection, usage, and protection of users' personal data by organizations.\nHowever, they are notorious for their lengthy, complex, and convoluted language\nespecially involving privacy-related entities. Hence, they pose a significant\nchallenge to users who attempt to comprehend organization's data usage policy.\nIn this paper, we propose to enhance the interpretability and readability of\npolicy documents by using controlled abstractive summarization -- we enforce\nthe generated summaries to include critical privacy-related entities (e.g.,\ndata and medium) and organization's rationale (e.g.,target and reason) in\ncollecting those entities. To achieve this, we develop PD-Sum, a\npolicy-document summarization dataset with marked privacy-related entity\nlabels. Our proposed model, EROS, identifies critical entities through a\nspan-based entity extraction model and employs them to control the information\ncontent of the summaries using proximal policy optimization (PPO). Comparison\nshows encouraging improvement over various baselines. Furthermore, we furnish\nqualitative and human evaluations to establish the efficacy of EROS.",
      "tldr_zh": "本研究针对隐私政策文档的冗长复杂问题，提出EROS模型——一种实体驱动的受控摘要生成方法，以提升文档的可读性和可解释性。EROS通过基于跨度的实体提取模型识别关键隐私相关实体（如数据和媒介），并利用近端策略优化（PPO）来控制摘要内容，确保包含组织的理由（如目标和原因）。此外，研究开发了PD-Sum数据集，用于标记实体标签，并在实验中显示EROS比基线模型有显著改进，并通过定性和人类评估验证其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00141v1",
      "published_date": "2024-02-29 21:44:50 UTC",
      "updated_date": "2024-02-29 21:44:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:13:02.528138"
    },
    {
      "arxiv_id": "2403.00131v3",
      "title": "UniTS: A Unified Multi-Task Time Series Model",
      "title_zh": "UniTS：一个统一的多任务时间序列模型",
      "authors": [
        "Shanghua Gao",
        "Teddy Koker",
        "Owen Queen",
        "Thomas Hartvigsen",
        "Theodoros Tsiligkaridis",
        "Marinka Zitnik"
      ],
      "abstract": "Although pre-trained transformers and reprogrammed text-based LLMs have shown\nstrong performance on time series tasks, the best-performing architectures vary\nwidely across tasks, with most models narrowly focused on specific areas, such\nas time series forecasting. Unifying predictive and generative time series\ntasks within a single model remains challenging. We introduce UniTS, a unified\nmulti-task time series model that utilizes task tokenization to integrate\npredictive and generative tasks into a single framework. UniTS employs a\nmodified transformer block to capture universal time series representations,\nenabling transferability from a heterogeneous, multi-domain pre-training\ndataset-characterized by diverse dynamic patterns, sampling rates, and temporal\nscales-to a wide range of downstream datasets with varied task specifications\nand data domains. Tested on 38 datasets across human activity sensors,\nhealthcare, engineering, and finance, UniTS achieves superior performance\ncompared to 12 forecasting models, 20 classification models, 18 anomaly\ndetection models, and 16 imputation models, including adapted text-based LLMs.\nUniTS also demonstrates strong few-shot and prompt capabilities when applied to\nnew domains and tasks. In single-task settings, UniTS outperforms competitive\ntask-specialized time series models. Code and datasets are available at\nhttps://github.com/mims-harvard/UniTS.",
      "tldr_zh": "该论文提出 UniTS，一种统一的、多任务时间序列模型，通过任务标记化将预测和生成任务整合到一个框架中，使用修改过的 transformer 块来捕获通用时间序列表示，并支持从异构多域预训练数据集向各种下游任务的转移。UniTS 在 38 个跨人类活动传感器、医疗、工程和金融等领域的数据集上进行了测试，优于 12 个预测模型、20 个分类模型、18 个异常检测模型和 16 个插值模型，包括适应后的文本-based LLMs。该模型还展示了强大的少样本和提示能力，在单任务设置中超越了专门化的时间序列模型，为多任务时间序列处理提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00131v3",
      "published_date": "2024-02-29 21:25:58 UTC",
      "updated_date": "2024-11-25 20:12:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:13:16.454696"
    },
    {
      "arxiv_id": "2403.00862v4",
      "title": "NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism",
      "title_zh": "翻译失败",
      "authors": [
        "Miao Li",
        "Ming-Bin Chen",
        "Bo Tang",
        "Shengbin Hou",
        "Pengyu Wang",
        "Haiying Deng",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Keming Mao",
        "Peng Cheng",
        "Yi Luo"
      ],
      "abstract": "We present NewsBench, a novel evaluation framework to systematically assess\nthe capabilities of Large Language Models (LLMs) for editorial capabilities in\nChinese journalism. Our constructed benchmark dataset is focused on four facets\nof writing proficiency and six facets of safety adherence, and it comprises\nmanually and carefully designed 1,267 test samples in the types of multiple\nchoice questions and short answer questions for five editorial tasks in 24 news\ndomains. To measure performances, we propose different GPT-4 based automatic\nevaluation protocols to assess LLM generations for short answer questions in\nterms of writing proficiency and safety adherence, and both are validated by\nthe high correlations with human evaluations. Based on the systematic\nevaluation framework, we conduct a comprehensive analysis of ten popular LLMs\nwhich can handle Chinese. The experimental results highlight GPT-4 and ERNIE\nBot as top performers, yet reveal a relative deficiency in journalistic safety\nadherence in creative writing tasks. Our findings also underscore the need for\nenhanced ethical guidance in machine-generated journalistic content, marking a\nstep forward in aligning LLMs with journalistic standards and safety\nconsiderations.",
      "tldr_zh": "本研究提出了NewsBench，一种系统评估框架，用于评估大型语言模型（LLMs）在中文新闻编辑能力方面的表现，包括写作熟练度的四个方面和安全遵守的六个方面。框架构建了一个包含1267个手动设计的测试样本的数据集，涵盖五个编辑任务和24个新闻领域，并引入基于GPT-4的自动评估协议来评估LLMs的生成输出，这些协议与人工评估高度相关。通过对十个流行中文处理LLMs的全面分析，结果显示GPT-4和ERNIE Bot是顶尖表现者，但它们在创造性写作任务中的新闻安全遵守方面存在缺陷，强调了在机器生成新闻内容中加强伦理指导的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Long paper, ACL 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2403.00862v4",
      "published_date": "2024-02-29 21:05:14 UTC",
      "updated_date": "2024-06-04 14:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:13:28.312077"
    },
    {
      "arxiv_id": "2403.00861v1",
      "title": "Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Wang",
        "Lokesh Kumar Sambasivan",
        "Mingang Fu",
        "Prakhar Mehrotra"
      ],
      "abstract": "Generative AI applications, such as ChatGPT or DALL-E, have shown the world\ntheir impressive capabilities in generating human-like text or image. Diving\ndeeper, the science stakeholder for those AI applications are Deep Generative\nModels, a.k.a DGMs, which are designed to learn the underlying distribution of\nthe data and generate new data points that are statistically similar to the\noriginal dataset. One critical question is raised: how can we leverage DGMs\ninto morden retail supply chain realm? To address this question, this paper\nexpects to provide a comprehensive review of DGMs and discuss their existing\nand potential usecases in retail supply chain, by (1) providing a taxonomy and\noverview of state-of-the-art DGMs and their variants, (2) reviewing existing\nDGM applications in retail supply chain from a end-to-end view of point, and\n(3) discussing insights and potential directions on how DGMs can be further\nutilized on solving retail supply chain problems.",
      "tldr_zh": "本论文探讨了如何将Deep Generative Models (DGMs)应用于现代零售供应链，旨在通过生成类似真实数据的新样本来提升供应链效率。论文首先提供DGMs的taxonomy和状态-of-the-art概述，涵盖其变体；其次，从端到端视角审阅DGMs在零售供应链中的现有应用，如优化库存和预测需求；最后，讨论见解和潜在方向，强调DGMs在解决供应链问题上的进一步潜力，例如创新预测模型和风险管理。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00861v1",
      "published_date": "2024-02-29 21:03:46 UTC",
      "updated_date": "2024-02-29 21:03:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:13:38.007773"
    },
    {
      "arxiv_id": "2403.00860v1",
      "title": "Parallel Algorithms for Exact Enumeration of Deep Neural Network Activation Regions",
      "title_zh": "用于深度神经网络激活区域精确枚举的并行算法",
      "authors": [
        "Sabrina Drammis",
        "Bowen Zheng",
        "Karthik Srinivasan",
        "Robert C. Berwick",
        "Nancy A. Lynch",
        "Robert Ajemian"
      ],
      "abstract": "A feedforward neural network using rectified linear units constructs a\nmapping from inputs to outputs by partitioning its input space into a set of\nconvex regions where points within a region share a single affine\ntransformation. In order to understand how neural networks work, when and why\nthey fail, and how they compare to biological intelligence, we need to\nunderstand the organization and formation of these regions. Step one is to\ndesign and implement algorithms for exact region enumeration in networks beyond\ntoy examples.\n  In this work, we present parallel algorithms for exact enumeration in deep\n(and shallow) neural networks. Our work has three main contributions: (1) we\npresent a novel algorithm framework and parallel algorithms for region\nenumeration; (2) we implement one of our algorithms on a variety of network\narchitectures and experimentally show how the number of regions dictates\nruntime; and (3) we show, using our algorithm's output, how the dimension of a\nregion's affine transformation impacts further partitioning of the region by\ndeeper layers.\n  To our knowledge, we run our implemented algorithm on networks larger than\nall of the networks used in the existing region enumeration literature.\nFurther, we experimentally demonstrate the importance of parallelism for region\nenumeration of any reasonably sized network.",
      "tldr_zh": "该论文探讨了使用 ReLU 激活函数的深度神经网络如何将输入空间分区成凸区域，这些区域内点共享单一仿射变换，并强调理解这些区域的组织有助于分析神经网络的工作原理、失败原因及与生物智能的比较。\n\n研究的主要贡献包括：提出一个新颖的算法框架和并行算法，用于精确枚举神经网络的激活 regions；实现该算法在多种网络架构上，并实验证明 regions 数量直接影响运行时间。\n\n此外，实验结果显示，regions 的仿射变换维度会影响后续层对区域的进一步分区，且并行计算对于处理较大网络至关重要。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00860v1",
      "published_date": "2024-02-29 20:48:39 UTC",
      "updated_date": "2024-02-29 20:48:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:13:51.764876"
    },
    {
      "arxiv_id": "2403.00116v1",
      "title": "Federated Linear Contextual Bandits with Heterogeneous Clients",
      "title_zh": "翻译失败",
      "authors": [
        "Ethan Blaser",
        "Chuanhao Li",
        "Hongning Wang"
      ],
      "abstract": "The demand for collaborative and private bandit learning across multiple\nagents is surging due to the growing quantity of data generated from\ndistributed systems. Federated bandit learning has emerged as a promising\nframework for private, efficient, and decentralized online learning. However,\nalmost all previous works rely on strong assumptions of client homogeneity,\ni.e., all participating clients shall share the same bandit model; otherwise,\nthey all would suffer linear regret. This greatly restricts the application of\nfederated bandit learning in practice. In this work, we introduce a new\napproach for federated bandits for heterogeneous clients, which clusters\nclients for collaborative bandit learning under the federated learning setting.\nOur proposed algorithm achieves non-trivial sub-linear regret and communication\ncost for all clients, subject to the communication protocol under federated\nlearning that at anytime only one model can be shared by the server.",
      "tldr_zh": "该论文解决了联邦线性上下文 Bandits 学习中异质客户端的问题，传统方法假设所有客户端共享相同模型，否则会导致线性遗憾，从而限制实际应用。研究提出了一种新算法，通过客户端聚类（client clustering）实现协作学习，使客户端在联邦学习设置下进行分布式在线学习。实验结果显示，该算法为所有客户端实现了非平凡的次线性遗憾（sub-linear regret）和低通信成本，同时遵守联邦学习协议，即任何时候只共享一个模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00116v1",
      "published_date": "2024-02-29 20:39:31 UTC",
      "updated_date": "2024-02-29 20:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:14:02.168547"
    },
    {
      "arxiv_id": "2403.00108v2",
      "title": "LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyi Liu",
        "Shaochen Zhong",
        "Xintong Sun",
        "Minghao Tian",
        "Mohsen Hariri",
        "Zirui Liu",
        "Ruixiang Tang",
        "Zhimeng Jiang",
        "Jiayi Yuan",
        "Yu-Neng Chuang",
        "Li Li",
        "Soo-Hyun Choi",
        "Rui Chen",
        "Vipin Chaudhary",
        "Xia Hu"
      ],
      "abstract": "Finetuning LLMs with LoRA has gained significant popularity due to its\nsimplicity and effectiveness. Often, users may even find pluggable,\ncommunity-shared LoRAs to enhance their base models for a specific downstream\ntask of interest; enjoying a powerful, efficient, yet customized LLM experience\nwith negligible investment. However, this convenient share-and-play ecosystem\nalso introduces a new attack surface, where attackers can distribute malicious\nLoRAs to a community eager to try out shared assets. Despite the high-risk\npotential, no prior art has comprehensively explored LoRA's attack surface\nunder the downstream-enhancing share-and-play context. In this paper, we\ninvestigate how backdoors can be injected into task-enhancing LoRAs and examine\nthe mechanisms of such infections. We find that with a simple, efficient, yet\nspecific recipe, a backdoor LoRA can be trained once and then seamlessly merged\n(in a training-free fashion) with multiple task-enhancing LoRAs, retaining both\nits malicious backdoor and benign downstream capabilities. This allows\nattackers to scale the distribution of compromised LoRAs with minimal effort by\nleveraging the rich pool of existing shared LoRA assets. We note that such\nmerged LoRAs are particularly infectious -- because their malicious intent is\ncleverly concealed behind improved downstream capabilities, creating a strong\nincentive for voluntary download -- and dangerous -- because under local\ndeployment, no safety measures exist to intervene when things go wrong. Our\nwork is among the first to study this new threat model of training-free\ndistribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting\nthe urgent need for heightened security awareness in the LoRA ecosystem.\nWarning: This paper contains offensive content and involves a real-life\ntragedy.",
      "tldr_zh": "本文研究了在 LoRA 共享生态中，后门攻击的新威胁，首次全面探讨攻击者如何注入恶意后门到任务增强型 LoRA。作者提出了一种简单高效的配方：训练一次 backdoor LoRA，然后以无训练方式无缝合并到多个现有 LoRA 中，同时保留后门功能和下游性能。这种方法允许攻击者轻松扩展受损 LoRA 的分发，并利用其隐蔽性（如提升下游能力）诱导用户下载，凸显了 LoRA 生态中安全意识的迫切需求。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00108v2",
      "published_date": "2024-02-29 20:25:16 UTC",
      "updated_date": "2025-04-30 22:51:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:14:15.081962"
    },
    {
      "arxiv_id": "2403.00859v1",
      "title": "Team Formation amidst Conflicts",
      "title_zh": "冲突中的团队组建",
      "authors": [
        "Iasonas Nikolaou",
        "Evimaria Terzi"
      ],
      "abstract": "In this work, we formulate the problem of team formation amidst conflicts.\nThe goal is to assign individuals to tasks, with given capacities, taking into\naccount individuals' task preferences and the conflicts between them. Using\ndependent rounding schemes as our main toolbox, we provide efficient\napproximation algorithms. Our framework is extremely versatile and can model\nmany different real-world scenarios as they arise in educational settings and\nhuman-resource management. We test and deploy our algorithms on real-world\ndatasets and we show that our algorithms find assignments that are better than\nthose found by natural baselines. In the educational setting we also show how\nour assignments are far better than those done manually by human experts. In\nthe human resource management application we show how our assignments increase\nthe diversity of teams. Finally, using a synthetic dataset we demonstrate that\nour algorithms scale very well in practice.",
      "tldr_zh": "该论文探讨了在考虑个人冲突的情况下组建团队的问题，目标是通过给定容量、任务偏好和冲突因素来分配个体。研究利用 dependent rounding schemes 作为主要工具，开发了高效的 approximation algorithms，这些算法框架灵活，可应用于教育和人力资源管理等真实场景。实验结果显示，该算法在真实数据集上优于自然基线和人工分配，在教育环境中显著改善分配质量，并在人力资源管理中提升了团队多样性；此外，使用合成数据集验证了算法的良好扩展性。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00859v1",
      "published_date": "2024-02-29 20:15:13 UTC",
      "updated_date": "2024-02-29 20:15:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:14:27.005558"
    },
    {
      "arxiv_id": "2403.00858v4",
      "title": "Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Raghavv Goel",
        "Mukul Gagrani",
        "Wonseok Jeon",
        "Junyoung Park",
        "Mingu Lee",
        "Christopher Lott"
      ],
      "abstract": "Text generation with Large Language Models (LLMs) is known to be memory bound\ndue to the combination of their auto-regressive nature, huge parameter counts,\nand limited memory bandwidths, often resulting in low token rates. Speculative\ndecoding has been proposed as a solution for LLM inference acceleration.\nHowever, since draft models are often unavailable in the modern open-source LLM\nfamilies, e.g., for Llama 2 7B, training a high-quality draft model is required\nto enable inference acceleration via speculative decoding. In this paper, we\npropose a simple draft model training framework for direct alignment to\nchat-capable target models. With the proposed framework, we train Llama 2 Chat\nDrafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\% of\nthe original size. Our training framework only consists of pretraining,\ndistillation dataset generation, and finetuning with knowledge distillation,\nwith no additional alignment procedure. For the finetuning step, we use\ninstruction-response pairs generated by target model for distillation in\nplausible data distribution, and propose a new Total Variation Distance++\n(TVD++) loss that incorporates variance reduction techniques inspired from the\npolicy gradient method in reinforcement learning. Our empirical results show\nthat Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3\nblock efficiency and 2.4$\\times$ speed-up relative to autoregressive decoding\non various tasks with no further task-specific fine-tuning.",
      "tldr_zh": "这篇论文针对大型语言模型 (LLMs) 在文本生成中的内存限制问题，提出了一种直接对齐 draft 模型的训练框架，以支持推测解码 (speculative decoding)。该框架仅包括预训练、蒸馏数据集生成和知识蒸馏的微调过程，并引入了新的 Total Variation Distance++ (TVD++) 损失函数，以优化在指令响应对上的训练。最终，他们训练了 Llama 2 Chat Drafter 115M 模型，仅占原 Llama 2 Chat 7B 模型的 1.64%，在各种任务上实现了高达 2.3 的块效率和 2.4 倍的速度提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 3 figures, Published at the ICLR 2024 Workshop on\n  Understanding of Foundation Models (ME-FoMo)",
      "pdf_url": "http://arxiv.org/pdf/2403.00858v4",
      "published_date": "2024-02-29 19:55:06 UTC",
      "updated_date": "2024-05-13 18:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:14:41.169292"
    },
    {
      "arxiv_id": "2403.14662v1",
      "title": "Case Studies of AI Policy Development in Africa",
      "title_zh": "翻译失败",
      "authors": [
        "Kadijatou Diallo",
        "Jonathan Smith",
        "Chinasa T. Okolo",
        "Dorcas Nyamwaya",
        "Jonas Kgomo",
        "Richard Ngamita"
      ],
      "abstract": "Artificial Intelligence (AI) requires new ways of evaluating national\ntechnology use and strategy for African nations. We conduct a survey of\nexisting 'readiness' assessments both for general digital adoption and for AI\npolicy in particular. We conclude that existing global readiness assessments do\nnot fully capture African states' progress in AI readiness and lay the\ngroundwork for how assessments can be better used for the African context. We\nconsider the extent to which these indicators map to the African context and\nwhat these indicators miss in capturing African states' on-the-ground work in\nmeeting AI capability. Through case studies of four African nations of diverse\ngeographic and economic dimensions, we identify nuances missed by global\nassessments and offer high-level policy considerations for how states can best\nimprove their AI readiness standards and prepare their societies to capture the\nbenefits of AI.",
      "tldr_zh": "本文通过调查现有AI就绪度评估，指出这些全球指标未能全面捕捉非洲国家在AI政策发展方面的进展和实际工作。研究者对四个地理和经济多样化的非洲国家进行了案例研究，识别出评估中遗漏的细微差别，如本土语境和实地能力。最终，论文提出高层政策建议，帮助这些国家提升AI就绪标准，并更好地利用AI技术为社会带来益处。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14662v1",
      "published_date": "2024-02-29 19:17:11 UTC",
      "updated_date": "2024-02-29 19:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:14:50.996894"
    },
    {
      "arxiv_id": "2403.00071v2",
      "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Suyuchen Wang",
        "Ivan Kobyzev",
        "Peng Lu",
        "Mehdi Rezagholizadeh",
        "Bang Liu"
      ],
      "abstract": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios\nin Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),\nwhere models pre-trained on shorter sequences face difficulty with\nout-of-distribution (OOD) token positions in longer sequences. We introduce\nResonance RoPE, a novel approach designed to narrow the generalization gap in\nTSTL scenarios by refining the interpolation of RoPE features for OOD\npositions, significantly improving the model performance without additional\nonline computational costs. Furthermore, we present PosGen, a new synthetic\nbenchmark specifically designed for fine-grained behavior analysis in TSTL\nscenarios, aiming to isolate the constantly increasing difficulty of token\ngeneration on long contexts from the challenges of recognizing new token\npositions. Our experiments on synthetic tasks show that after applying\nResonance RoPE, Transformers recognize OOD position better and more robustly.\nOur extensive LLM experiments also show superior performance after applying\nResonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on\nboth upstream language modeling tasks and a variety of downstream long-text\napplications.",
      "tldr_zh": "本论文针对大型语言模型（LLMs）使用 Rotary Position Embedding (RoPE) 时，在训练短序列测试长序列（TSTL）场景中处理超出分布（OOD）位置的挑战，提出了一种新方法Resonance RoPE。该方法通过优化RoPE特征的插值来缩小泛化差距，提升模型性能，而无需额外在线计算开销。同时，论文引入了PosGen，一个专为TSTL场景设计的合成基准，用于细粒度分析长上下文中的生成难度。实验结果显示，应用Resonance RoPE后，Transformer模型在合成任务中更robust地识别OOD位置，并在与现有方法YaRN结合的LLM实验中，显著提升了上游语言建模和下游长文本应用的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 4 figures, accepted at ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2403.00071v2",
      "published_date": "2024-02-29 19:02:03 UTC",
      "updated_date": "2024-06-10 13:30:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:15:03.608959"
    },
    {
      "arxiv_id": "2402.19475v1",
      "title": "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Gu",
        "Wen-Ding Li",
        "Naman Jain",
        "Theo X. Olausson",
        "Celine Lee",
        "Koushik Sen",
        "Armando Solar-Lezama"
      ],
      "abstract": "While language models are increasingly more proficient at code generation,\nthey still frequently generate incorrect programs. Many of these programs are\nobviously wrong, but others are more subtle and pass weaker correctness checks\nsuch as being able to compile. In this work, we focus on these counterfeit\nsamples: programs sampled from a language model that 1) have a high enough\nlog-probability to be generated at a moderate temperature and 2) pass weak\ncorrectness checks. Overall, we discover that most models have a very shallow\nunderstanding of counterfeits through three clear failure modes. First, models\nmistakenly classify them as correct. Second, models are worse at reasoning\nabout the execution behaviour of counterfeits and often predict their execution\nresults as if they were correct. Third, when asking models to fix counterfeits,\nthe likelihood of a model successfully repairing a counterfeit is often even\nlower than that of sampling a correct program from scratch. Counterfeits also\nhave very unexpected properties: first, counterfeit programs for problems that\nare easier for a model to solve are not necessarily easier to detect and only\nslightly easier to execute and repair. Second, counterfeits from a given model\nare just as confusing to the model itself as they are to other models. Finally,\nboth strong and weak models are able to generate counterfeit samples that\nequally challenge all models. In light of our findings, we recommend that care\nand caution be taken when relying on models to understand their own samples,\nespecially when no external feedback is incorporated.",
      "tldr_zh": "该研究探讨了代码语言模型（language models）是否能正确理解其生成的错误程序，特别是那些通过弱正确性检查（如编译通过）的伪造样本（counterfeit samples）。实验发现，模型在处理这些样本时存在三大失败模式：错误地将它们分类为正确、预测执行行为时表现更差，以及修复成功率甚至低于从零生成正确程序。进一步分析显示，伪造样本具有意外属性，例如更容易解决的问题不一定更容易检测，且不同模型生成的样本对所有模型同样具有挑战性；因此，论文推荐在使用模型理解自身输出时需谨慎，尤其缺乏外部反馈时。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "54 pages, 25 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.19475v1",
      "published_date": "2024-02-29 18:59:25 UTC",
      "updated_date": "2024-02-29 18:59:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:15:17.185200"
    },
    {
      "arxiv_id": "2402.19471v2",
      "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Grand",
        "Valerio Pepe",
        "Jacob Andreas",
        "Joshua B. Tenenbaum"
      ],
      "abstract": "Questions combine our mastery of language with our remarkable facility for\nreasoning about uncertainty. How do people navigate vast hypothesis spaces to\npose informative questions given limited cognitive resources? We study these\ntradeoffs in a classic grounded question-asking task based on the board game\nBattleship. Our language-informed program sampling (LIPS) model uses large\nlanguage models (LLMs) to generate natural language questions, translate them\ninto symbolic programs, and evaluate their expected information gain. We find\nthat with a surprisingly modest resource budget, this simple Monte Carlo\noptimization strategy yields informative questions that mirror human\nperformance across varied Battleship board scenarios. In contrast, LLM-only\nbaselines struggle to ground questions in the board state; notably, GPT-4V\nprovides no improvement over non-visual baselines. Our results illustrate how\nBayesian models of question-asking can leverage the statistics of language to\ncapture human priors, while highlighting some shortcomings of pure LLMs as\ngrounded reasoners.",
      "tldr_zh": "本论文探讨了人们在有限认知资源下如何提出信息性问题的机制，并引入 Language-Informed Program Sampling (LIPS) 模型来模拟这一过程。LIPS 模型利用 large language models (LLMs) 生成自然语言问题，将其转化为符号程序，并通过 Monte Carlo optimization 评估预期信息增益，从而在 Battleship 游戏中生成与人类表现相似的提问。实验结果显示，LIPS 显著优于 LLM-only 基线（如 GPT-4V），后者在将问题与游戏状态关联上存在缺陷。该研究突出了 Bayesian models 如何通过语言统计捕捉人类先验，同时揭示了纯 LLM 在 grounded reasoning 上的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to CogSci 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19471v2",
      "published_date": "2024-02-29 18:58:15 UTC",
      "updated_date": "2024-05-01 19:00:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:15:30.106728"
    },
    {
      "arxiv_id": "2402.19467v4",
      "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Kate Sanders",
        "Nathaniel Weir",
        "Benjamin Van Durme"
      ],
      "abstract": "It is challenging for models to understand complex, multimodal content such\nas television clips, and this is in part because video-language models often\nrely on single-modality reasoning and lack interpretability. To combat these\nissues we propose TV-TREES, the first multimodal entailment tree generator.\nTV-TREES serves as an approach to video understanding that promotes\ninterpretable joint-modality reasoning by searching for trees of entailment\nrelationships between simple text-video evidence and higher-level conclusions\nthat prove question-answer pairs. We also introduce the task of multimodal\nentailment tree generation to evaluate reasoning quality. Our method's\nperformance on the challenging TVQA benchmark demonstrates interpretable,\nstate-of-the-art zero-shot performance on full clips, illustrating that\nmultimodal entailment tree generation can be a best-of-both-worlds alternative\nto black-box systems.",
      "tldr_zh": "该研究针对视频语言模型在理解复杂多模态内容（如电视剪辑）时存在的单一模态推理和可解释性不足问题，提出了 TV-TREES，一种多模态蕴含树（Multimodal Entailment Trees）生成器，用于神经符号视频推理（Neuro-Symbolic Video Reasoning）。TV-TREES 通过搜索文本-视频证据之间的蕴含关系树，来证明问题-答案对，从而实现可解释的联合模态推理，并引入多模态蕴含树生成任务以评估推理质量。在 TVQA 基准测试中，该方法展示了可解释的、state-of-the-art 零样本性能，适用于完整视频剪辑，提供了一种黑箱系统的优越替代方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "I.2.7; I.2.10"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19467v4",
      "published_date": "2024-02-29 18:57:01 UTC",
      "updated_date": "2024-10-10 15:25:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:15:40.724062"
    },
    {
      "arxiv_id": "2402.19465v2",
      "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Qian",
        "Jie Zhang",
        "Wei Yao",
        "Dongrui Liu",
        "Zhenfei Yin",
        "Yu Qiao",
        "Yong Liu",
        "Jing Shao"
      ],
      "abstract": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}.",
      "tldr_zh": "本研究重新审视大型语言模型 (LLMs) 的预训练阶段，旨在追踪其可信度动态，包括 reliability、privacy、toxicity、fairness 和 robustness 等五个关键维度。研究者通过线性 probing 发现，早期的预训练 LLMs 已能区分这些概念，并提取 steering vectors 从检查点中增强模型可信度；此外，使用 mutual information probing 观察到类似于 fitting and compression 的两阶段现象。总体而言，此工作首次探索预训练期的可信度建模，提供新见解，并促进该领域的进一步发展，代码将在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19465v2",
      "published_date": "2024-02-29 18:55:06 UTC",
      "updated_date": "2024-08-31 11:31:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:15:52.078257"
    },
    {
      "arxiv_id": "2402.19464v1",
      "title": "Curiosity-driven Red-teaming for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhang-Wei Hong",
        "Idan Shenfeld",
        "Tsun-Hsuan Wang",
        "Yung-Sung Chuang",
        "Aldo Pareja",
        "James Glass",
        "Akash Srivastava",
        "Pulkit Agrawal"
      ],
      "abstract": "Large language models (LLMs) hold great potential for many natural language\napplications but risk generating incorrect or toxic content. To probe when an\nLLM generates unwanted content, the current paradigm is to recruit a\n\\textit{red team} of human testers to design input prompts (i.e., test cases)\nthat elicit undesirable responses from LLMs. However, relying solely on human\ntesters is expensive and time-consuming. Recent works automate red teaming by\ntraining a separate red team LLM with reinforcement learning (RL) to generate\ntest cases that maximize the chance of eliciting undesirable responses from the\ntarget LLM. However, current RL methods are only able to generate a small\nnumber of effective test cases resulting in a low coverage of the span of\nprompts that elicit undesirable responses from the target LLM. To overcome this\nlimitation, we draw a connection between the problem of increasing the coverage\nof generated test cases and the well-studied approach of curiosity-driven\nexploration that optimizes for novelty. Our method of curiosity-driven red\nteaming (CRT) achieves greater coverage of test cases while mantaining or\nincreasing their effectiveness compared to existing methods. Our method, CRT\nsuccessfully provokes toxic responses from LLaMA2 model that has been heavily\nfine-tuned using human preferences to avoid toxic outputs. Code is available at\n\\url{https://github.com/Improbable-AI/curiosity_redteam}",
      "tldr_zh": "该论文提出了一种好奇心驱动的红队方法（Curiosity-driven Red Teaming, CRT），旨在提升大型语言模型（LLMs）的测试覆盖率，以更全面地检测不良输出，如错误或有毒内容。相比传统强化学习（RL）方法，CRT 通过优化测试用例的新颖性，同时保持或提高其有效性，解决了现有方法生成测试用例数量少和覆盖率低的局限性。实验结果显示，CRT 成功在经过微调的 LLaMA2 模型上引发有毒响应，为 LLM 的安全评估提供了更可靠的自动化工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19464v1",
      "published_date": "2024-02-29 18:55:03 UTC",
      "updated_date": "2024-02-29 18:55:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:16:06.653742"
    },
    {
      "arxiv_id": "2402.19457v3",
      "title": "$\\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Maxime Darrin",
        "Philippe Formont",
        "Jackie Chi Kit Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "Assessing the quality of summarizers poses significant challenges. In\nresponse, we propose a novel task-oriented evaluation approach that assesses\nsummarizers based on their capacity to produce summaries that are useful for\ndownstream tasks, while preserving task outcomes. We theoretically establish a\ndirect relationship between the resulting error probability of these tasks and\nthe mutual information between source texts and generated summaries. We\nintroduce $\\texttt{COSMIC}$ as a practical implementation of this metric,\ndemonstrating its strong correlation with human judgment-based metrics and its\neffectiveness in predicting downstream task performance. Comparative analyses\nagainst established metrics like $\\texttt{BERTScore}$ and $\\texttt{ROUGE}$\nhighlight the competitive performance of $\\texttt{COSMIC}$.",
      "tldr_zh": "本论文提出了一种基于Mutual Information的任务无关摘要评估方法$\\texttt{COSMIC}$，旨在评估摘要生成器是否能产生对下游任务有用的总结，同时保持任务结果的可靠性。论文理论上建立了下游任务错误概率与源文本和生成摘要之间互信息的直接关系，并通过$\\texttt{COSMIC}$的实际实现验证了其与人类判断指标的高度相关性。实验结果显示，$\\texttt{COSMIC}$在预测下游任务性能方面优于现有指标如$\\texttt{BERTScore}$和$\\texttt{ROUGE}$，为摘要评估提供了更有效的工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19457v3",
      "published_date": "2024-02-29 18:51:23 UTC",
      "updated_date": "2024-08-14 14:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:16:15.966163"
    },
    {
      "arxiv_id": "2402.19450v1",
      "title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
      "title_zh": "翻译失败",
      "authors": [
        "Saurabh Srivastava",
        "Annarose M B",
        "Anto P V",
        "Shashank Menon",
        "Ajay Sukumar",
        "Adwaith Samod T",
        "Alan Philipose",
        "Stevin Prince",
        "Sooraj Thomas"
      ],
      "abstract": "We propose a framework for robust evaluation of reasoning capabilities of\nlanguage models, using functional variants of benchmarks. Models that solve a\nreasoning test should exhibit no difference in performance over the static\nversion of a problem compared to a snapshot of the functional variant. We have\nrewritten the relevant fragment of the MATH benchmark into its functional\nvariant MATH(), with functionalization of other benchmarks to follow. When\nevaluating current state-of-the-art models over snapshots of MATH(), we find a\nreasoning gap -- the percentage difference between the static and functional\naccuracies. We find reasoning gaps from 58.35% to 80.31% among the\nstate-of-the-art closed and open weights models that perform well on static\nbenchmarks, with the caveat that the gaps are likely to be smaller with more\nsophisticated prompting strategies. Here we show that models which anecdotally\nhave good reasoning performance over real-world tasks, have quantifiable lower\ngaps, motivating the open problem of building \"gap 0\" models. Code for\nevaluation and new evaluation datasets, three MATH() snapshots, are publicly\navailable at https://github.com/consequentai/fneval/.",
      "tldr_zh": "本研究提出了一种使用函数化变体基准的框架，来稳健评估语言模型的推理能力，确保模型在静态问题和函数化变体快照上的性能无显著差异。作者将 MATH 基准改写为 MATH()，并计划扩展到其他基准，通过计算静态准确率与函数化准确率之间的“reasoning gap”来量化模型缺陷。实验发现，当前最先进模型的 reasoning gap 从58.35%到80.31%，尽管可能通过更复杂的提示策略缩小，而在真实任务中表现良好的模型显示出较小差距。该框架的代码、数据集和三个 MATH() 快照已公开在 GitHub 上。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "37 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.19450v1",
      "published_date": "2024-02-29 18:48:18 UTC",
      "updated_date": "2024-02-29 18:48:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:16:28.924780"
    },
    {
      "arxiv_id": "2402.19446v1",
      "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Zhou",
        "Andrea Zanette",
        "Jiayi Pan",
        "Sergey Levine",
        "Aviral Kumar"
      ],
      "abstract": "A broad use case of large language models (LLMs) is in goal-directed\ndecision-making tasks (or \"agent\" tasks), where an LLM needs to not just\ngenerate completions for a given prompt, but rather make intelligent decisions\nover a multi-turn interaction to accomplish a task (e.g., when interacting with\nthe web, using tools, or providing customer support). Reinforcement learning\n(RL) provides a general paradigm to address such agent tasks, but current RL\nmethods for LLMs largely focus on optimizing single-turn rewards. By\nconstruction, most single-turn RL methods cannot endow LLMs with the ability to\nintelligently seek information over multiple turns, perform credit assignment,\nor reason about their past actions -- all of which are critical in agent tasks.\nThis raises the question: how can we design effective and efficient multi-turn\nRL algorithms for LLMs? In this paper, we develop a framework for building\nmulti-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility\nof existing single-turn RL methods for LLMs (e.g., proximal policy\noptimization), while accommodating multiple turns, long horizons, and delayed\nrewards effectively. To do this, our framework adopts a hierarchical RL\napproach and runs two RL algorithms in parallel: a high-level off-policy\nvalue-based RL algorithm to aggregate reward over utterances, and a low-level\nRL algorithm that utilizes this high-level value function to train a token\npolicy within each utterance or turn. Our hierarchical framework, Actor-Critic\nFramework with a Hierarchical Structure (ArCHer), can also give rise to other\nRL methods. Empirically, we find that ArCHer significantly improves efficiency\nand performance on agent tasks, attaining a sample efficiency of about 100x\nover existing methods, while also improving with larger model capacity (upto\nthe 7 billion scale that we tested on).",
      "tldr_zh": "这篇论文提出了 ArCHer 框架，一种基于层次化强化学习 (Hierarchical RL) 的方法，用于训练大型语言模型 (LLMs) 在多轮交互代理任务中的表现。ArCHer 通过并行运行高层 off-policy value-based RL 算法（用于聚合跨utterance的奖励）和底层 RL 算法（利用高层价值函数优化每个回合的token policy），有效处理多轮决策、信用分配和延迟奖励问题。实验结果显示，ArCHer 比现有方法提高了约 100 倍的样本效率，并在模型规模扩展到 70 亿参数时进一步提升性能，为 LLMs 在目标导向任务中的应用提供了更高效的训练范式。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19446v1",
      "published_date": "2024-02-29 18:45:56 UTC",
      "updated_date": "2024-02-29 18:45:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:16:40.238942"
    },
    {
      "arxiv_id": "2402.19443v1",
      "title": "Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems",
      "title_zh": "探究自动语音识别系统基于神经网络声学模型中编码的信息",
      "authors": [
        "Quentin Raymondaud",
        "Mickael Rouvier",
        "Richard Dufour"
      ],
      "abstract": "Deep learning architectures have made significant progress in terms of\nperformance in many research areas. The automatic speech recognition (ASR)\nfield has thus benefited from these scientific and technological advances,\nparticularly for acoustic modeling, now integrating deep neural network\narchitectures. However, these performance gains have translated into increased\ncomplexity regarding the information learned and conveyed through these\nblack-box architectures. Following many researches in neural networks\ninterpretability, we propose in this article a protocol that aims to determine\nwhich and where information is located in an ASR acoustic model (AM). To do so,\nwe propose to evaluate AM performance on a determined set of tasks using\nintermediate representations (here, at different layer levels). Regarding the\nperformance variation and targeted tasks, we can emit hypothesis about which\ninformation is enhanced or perturbed at different architecture steps.\nExperiments are performed on both speaker verification, acoustic environment\nclassification, gender classification, tempo-distortion detection systems and\nspeech sentiment/emotion identification. Analysis showed that neural-based AMs\nhold heterogeneous information that seems surprisingly uncorrelated with\nphoneme recognition, such as emotion, sentiment or speaker identity. The\nlow-level hidden layers globally appears useful for the structuring of\ninformation while the upper ones would tend to delete useless information for\nphoneme recognition.",
      "tldr_zh": "本研究探讨了基于神经网络的自动语音识别（ASR）系统声学模型（acoustic model, AM）中编码的信息，旨在解决这些黑盒模型复杂性的问题。研究提出一个协议，通过评估中间表示（intermediate representations）在不同层级的性能，来分析模型在说话者验证、声学环境分类、性别分类、节奏失真检测以及语音情感/情绪识别等任务上的表现。结果显示，神经-based AMs 包含异质信息，这些信息与音素识别意外地不相关；低层隐藏层主要用于信息结构化，而高层则倾向于过滤掉对音素识别无用的信息，从而为神经网络可解释性提供了新见解。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19443v1",
      "published_date": "2024-02-29 18:43:53 UTC",
      "updated_date": "2024-02-29 18:43:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:16:52.587653"
    },
    {
      "arxiv_id": "2402.19442v2",
      "title": "Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Chen",
        "Heejune Sheen",
        "Tianhao Wang",
        "Zhuoran Yang"
      ],
      "abstract": "We study the dynamics of gradient flow for training a multi-head softmax\nattention model for in-context learning of multi-task linear regression. We\nestablish the global convergence of gradient flow under suitable choices of\ninitialization. In addition, we prove that an interesting \"task allocation\"\nphenomenon emerges during the gradient flow dynamics, where each attention head\nfocuses on solving a single task of the multi-task model. Specifically, we\nprove that the gradient flow dynamics can be split into three phases -- a\nwarm-up phase where the loss decreases rather slowly and the attention heads\ngradually build up their inclination towards individual tasks, an emergence\nphase where each head selects a single task and the loss rapidly decreases, and\na convergence phase where the attention parameters converge to a limit.\nFurthermore, we prove the optimality of gradient flow in the sense that the\nlimiting model learned by gradient flow is on par with the best possible\nmulti-head softmax attention model up to a constant factor. Our analysis also\ndelineates a strict separation in terms of the prediction accuracy of ICL\nbetween single-head and multi-head attention models. The key technique for our\nconvergence analysis is to map the gradient flow dynamics in the parameter\nspace to a set of ordinary differential equations in the spectral domain, where\nthe relative magnitudes of the semi-singular values of the attention weights\ndetermines task allocation. To our best knowledge, our work provides the first\nconvergence result for the multi-head softmax attention model.",
      "tldr_zh": "这篇论文研究了多头 softmax attention 模型在 in-context learning 中的训练动态，专注于多任务线性回归场景。论文证明了在合适初始化下，gradient flow 可以实现全局收敛，并揭示了“任务分配”现象，即每个 attention head 会专注于单一任务。训练过程分为三个阶段：warm-up 阶段（损失缓慢下降，heads 逐渐倾向任务）、emergence 阶段（heads 选择任务，损失快速减少）和 convergence 阶段（参数收敛）。此外，分析显示 gradient flow 学到的模型性能与最佳多头 softmax attention 模型相当，并证明了多头模型在 ICL 预测准确性上优于单头模型。该工作首次提供了多头 softmax attention 模型的收敛理论分析。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "141 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.19442v2",
      "published_date": "2024-02-29 18:43:52 UTC",
      "updated_date": "2024-06-10 17:18:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:17:06.194020"
    },
    {
      "arxiv_id": "2402.19437v1",
      "title": "Differentially Private Worst-group Risk Minimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Zhou",
        "Raef Bassily"
      ],
      "abstract": "We initiate a systematic study of worst-group risk minimization under\n$(\\epsilon, \\delta)$-differential privacy (DP). The goal is to privately find a\nmodel that approximately minimizes the maximal risk across $p$ sub-populations\n(groups) with different distributions, where each group distribution is\naccessed via a sample oracle. We first present a new algorithm that achieves\nexcess worst-group population risk of $\\tilde{O}(\\frac{p\\sqrt{d}}{K\\epsilon} +\n\\sqrt{\\frac{p}{K}})$, where $K$ is the total number of samples drawn from all\ngroups and $d$ is the problem dimension. Our rate is nearly optimal when each\ndistribution is observed via a fixed-size dataset of size $K/p$. Our result is\nbased on a new stability-based analysis for the generalization error. In\nparticular, we show that $\\Delta$-uniform argument stability implies\n$\\tilde{O}(\\Delta + \\frac{1}{\\sqrt{n}})$ generalization error w.r.t. the\nworst-group risk, where $n$ is the number of samples drawn from each sample\noracle. Next, we propose an algorithmic framework for worst-group population\nrisk minimization using any DP online convex optimization algorithm as a\nsubroutine. Hence, we give another excess risk bound of $\\tilde{O}\\left(\n\\sqrt{\\frac{d^{1/2}}{\\epsilon K}} +\\sqrt{\\frac{p}{K\\epsilon^2}} \\right)$.\nAssuming the typical setting of $\\epsilon=\\Theta(1)$, this bound is more\nfavorable than our first bound in a certain range of $p$ as a function of $K$\nand $d$. Finally, we study differentially private worst-group empirical risk\nminimization in the offline setting, where each group distribution is observed\nby a fixed-size dataset. We present a new algorithm with nearly optimal excess\nrisk of $\\tilde{O}(\\frac{p\\sqrt{d}}{K\\epsilon})$.",
      "tldr_zh": "本研究系统探讨了在 (ε, δ)-Differentially Private 条件下最小化 worst-group risk 的问题，目标是通过样本预言机访问 p 个子群体分布，找到一个模型来约最小化最大风险。研究提出了一种新算法，实现了过剩 worst-group 总体风险界为 Õ(p√d / (Kε) + √(p/K))，其中 K 为总样本数、d 为问题维度，该界在每个分布通过固定大小数据集观察时几乎最优，并基于新的稳定性分析支持。另一个算法框架利用 DP 在线凸优化作为子例程，提供过剩风险界为 Õ(√(d^{1/2}/(εK)) + √(p/(Kε^2)))，在 ε=Θ(1) 的典型设置下更具优势。最终，研究了离线设置下的 Differentially Private worst-group 经验风险最小化，提出了一种新算法，实现了近似最优的过剩风险界 Õ(p√d / (Kε))。这些贡献为隐私保护下的公平机器学习提供了重要进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19437v1",
      "published_date": "2024-02-29 18:38:20 UTC",
      "updated_date": "2024-02-29 18:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:17:21.257718"
    },
    {
      "arxiv_id": "2403.00854v1",
      "title": "Speaker-Independent Dysarthria Severity Classification using Self-Supervised Transformers and Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lauren Stumpf",
        "Balasundaram Kadirvelu",
        "Sigourney Waibel",
        "A. Aldo Faisal"
      ],
      "abstract": "Dysarthria, a condition resulting from impaired control of the speech muscles\ndue to neurological disorders, significantly impacts the communication and\nquality of life of patients. The condition's complexity, human scoring and\nvaried presentations make its assessment and management challenging. This study\npresents a transformer-based framework for automatically assessing dysarthria\nseverity from raw speech data. It can offer an objective, repeatable,\naccessible, standardised and cost-effective and compared to traditional methods\nrequiring human expert assessors. We develop a transformer framework, called\nSpeaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task\nlearning objective and contrastive learning for speaker-independent multi-class\ndysarthria severity classification. The multi-task framework is designed to\nreduce reliance on speaker-specific characteristics and address the intrinsic\nintra-class variability of dysarthric speech. We evaluated on the Universal\nAccess Speech dataset using leave-one-speaker-out cross-validation, our model\ndemonstrated superior performance over traditional machine learning approaches,\nwith an accuracy of $70.48\\%$ and an F1 score of $59.23\\%$. Our SALR model also\nexceeded the previous benchmark for AI-based classification, which used support\nvector machines, by $16.58\\%$. We open the black box of our model by\nvisualising the latent space where we can observe how the model substantially\nreduces speaker-specific cues and amplifies task-specific ones, thereby showing\nits robustness. In conclusion, SALR establishes a new benchmark in\nspeaker-independent multi-class dysarthria severity classification using\ngenerative AI. The potential implications of our findings for broader clinical\napplications in automated dysarthria severity assessments.",
      "tldr_zh": "本研究针对Dysarthria（一种由神经障碍导致的言语肌肉控制受损疾病）提出了一种基于Self-Supervised Transformers的框架，旨在实现说话者无关的多类严重程度分类。该框架名为Speaker-Agnostic Latent Regularisation (SALR)，结合Multi-Task Learning和Contrastive Learning，减少对说话者特定特征的依赖，并处理Dysarthric语音的内部类变异性，从而提供客观、可重复的自动评估方法。在Universal Access Speech数据集上，使用leave-one-speaker-out交叉验证，SALR模型的准确率达70.48%、F1分数为59.23%，比传统机器学习方法（如支持向量机）提高了16.58%。通过潜空间可视化，模型展示了其鲁棒性，并为临床应用中的自动Dysarthria严重程度评估建立了新基准。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS",
        "I.2.7; I.2.1; J.3"
      ],
      "primary_category": "q-bio.NC",
      "comment": "17 pages, 2 tables, 4 main figures, 2 supplemental figures, prepared\n  for journal submission",
      "pdf_url": "http://arxiv.org/pdf/2403.00854v1",
      "published_date": "2024-02-29 18:30:52 UTC",
      "updated_date": "2024-02-29 18:30:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:17:31.067107"
    },
    {
      "arxiv_id": "2402.19431v1",
      "title": "Compositional API Recommendation for Library-Oriented Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zexiong Ma",
        "Shengnan An",
        "Bing Xie",
        "Zeqi Lin"
      ],
      "abstract": "Large language models (LLMs) have achieved exceptional performance in code\ngeneration. However, the performance remains unsatisfactory in generating\nlibrary-oriented code, especially for the libraries not present in the training\ndata of LLMs. Previous work utilizes API recommendation technology to help LLMs\nuse libraries: it retrieves APIs related to the user requirements, then\nleverages them as context to prompt LLMs. However, developmental requirements\ncan be coarse-grained, requiring a combination of multiple fine-grained APIs.\nThis granularity inconsistency makes API recommendation a challenging task. To\naddress this, we propose CAPIR (Compositional API Recommendation), which adopts\na \"divide-and-conquer\" strategy to recommend APIs for coarse-grained\nrequirements. Specifically, CAPIR employs an LLM-based Decomposer to break down\na coarse-grained task description into several detailed subtasks. Then, CAPIR\napplies an embedding-based Retriever to identify relevant APIs corresponding to\neach subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out\nredundant APIs and provides the final recommendation. To facilitate the\nevaluation of API recommendation methods on coarse-grained requirements, we\npresent two challenging benchmarks, RAPID (Recommend APIs based on\nDocumentation) and LOCG (Library-Oriented Code Generation). Experimental\nresults on these benchmarks, demonstrate the effectiveness of CAPIR in\ncomparison to existing baselines. Specifically, on RAPID's Torchdata-AR\ndataset, compared to the state-of-the-art API recommendation approach, CAPIR\nimproves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On\nLOCG's Torchdata-Code dataset, compared to code generation without API\nrecommendation, CAPIR improves pass@100 from 16.0% to 28.0%.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在生成库导向代码时的不足，特别是处理训练数据中未出现的库，提出了一种组合式 API 推荐方法 CAPIR。CAPIR 采用“divide-and-conquer”策略，包括 LLM-based Decomposer 将粗粒度任务分解为子任务、embedding-based Retriever 为每个子任务检索相关 API，以及 LLM-based Reranker 过滤冗余 API 以提供最终推荐。该方法还构建了两个基准：RAPID（基于文档的 API 推荐）和 LOCG（库导向代码生成），用于评估粗粒度需求的 API 推荐性能。在实验中，CAPIR 在 RAPID 的 Torchdata-AR 数据集上将 recall@5 提高至 43.2%（比基线高 24.5%）和 precision@5 提高至 37.1%（比基线高 21.6%），而在 LOCG 的 Torchdata-Code 数据集上，将 pass@100 提升至 28.0%（比无 API 推荐高 12.0%），证明了其有效性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19431v1",
      "published_date": "2024-02-29 18:27:27 UTC",
      "updated_date": "2024-02-29 18:27:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:17:43.009373"
    },
    {
      "arxiv_id": "2402.19423v1",
      "title": "Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?",
      "title_zh": "翻译失败",
      "authors": [
        "Tiezheng Zhang",
        "Xiaoxi Chen",
        "Chongyu Qu",
        "Alan Yuille",
        "Zongwei Zhou"
      ],
      "abstract": "Interactive segmentation, an integration of AI algorithms and human\nexpertise, premises to improve the accuracy and efficiency of curating\nlarge-scale, detailed-annotated datasets in healthcare. Human experts revise\nthe annotations predicted by AI, and in turn, AI improves its predictions by\nlearning from these revised annotations. This interactive process continues to\nenhance the quality of annotations until no major revision is needed from\nexperts. The key challenge is how to leverage AI predicted and expert revised\nannotations to iteratively improve the AI. Two problems arise: (1) The risk of\ncatastrophic forgetting--the AI tends to forget the previously learned classes\nif it is only retrained using the expert revised classes. (2) Computational\ninefficiency when retraining the AI using both AI predicted and expert revised\nannotations; moreover, given the dominant AI predicted annotations in the\ndataset, the contribution of newly revised annotations--often account for a\nvery small fraction--to the AI training remains marginal. This paper proposes\nContinual Tuning to address the problems from two perspectives: network design\nand data reuse. Firstly, we design a shared network for all classes followed by\nclass-specific networks dedicated to individual classes. To mitigate\nforgetting, we freeze the shared network for previously learned classes and\nonly update the class-specific network for revised classes. Secondly, we reuse\na small fraction of data with previous annotations to avoid over-computing. The\nselection of such data relies on the importance estimate of each data. The\nimportance score is computed by combining the uncertainty and consistency of AI\npredictions. Our experiments demonstrate that Continual Tuning achieves a speed\n16x greater than repeatedly training AI from scratch without compromising the\nperformance.",
      "tldr_zh": "该论文探讨了在交互式分割（Interactive Segmentation）中，如何利用 AI 预测和专家修订的标注来迭代改进 AI 模型，针对灾难性遗忘（catastrophic forgetting）和计算效率问题。作者提出 Continual Tuning 方法，从网络设计和数据重用两个角度入手：设计一个共享网络（shared network）与类特定网络（class-specific networks），并冻结共享网络只更新修订类别的网络，以缓解遗忘；同时，通过计算重要性分数（importance score）基于 AI 的不确定性和一致性，重用一小部分之前标注的数据，避免过度计算。实验结果显示，Continual Tuning 比从零开始的全训练快 16 倍，同时保持了模型性能无显著损失，为医疗数据集标注提供了更高效的框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE International Symposium on Biomedical Imaging (ISBI)",
      "pdf_url": "http://arxiv.org/pdf/2402.19423v1",
      "published_date": "2024-02-29 18:22:12 UTC",
      "updated_date": "2024-02-29 18:22:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:17:54.216313"
    },
    {
      "arxiv_id": "2402.19422v3",
      "title": "PEM: Prototype-based Efficient MaskFormer for Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Niccolò Cavagnero",
        "Gabriele Rosi",
        "Claudia Cuttano",
        "Francesca Pistilli",
        "Marco Ciccone",
        "Giuseppe Averta",
        "Fabio Cermelli"
      ],
      "abstract": "Recent transformer-based architectures have shown impressive results in the\nfield of image segmentation. Thanks to their flexibility, they obtain\noutstanding performance in multiple segmentation tasks, such as semantic and\npanoptic, under a single unified framework. To achieve such impressive\nperformance, these architectures employ intensive operations and require\nsubstantial computational resources, which are often not available, especially\non edge devices. To fill this gap, we propose Prototype-based Efficient\nMaskFormer (PEM), an efficient transformer-based architecture that can operate\nin multiple segmentation tasks. PEM proposes a novel prototype-based\ncross-attention which leverages the redundancy of visual features to restrict\nthe computation and improve the efficiency without harming the performance. In\naddition, PEM introduces an efficient multi-scale feature pyramid network,\ncapable of extracting features that have high semantic content in an efficient\nway, thanks to the combination of deformable convolutions and context-based\nself-modulation. We benchmark the proposed PEM architecture on two tasks,\nsemantic and panoptic segmentation, evaluated on two different datasets,\nCityscapes and ADE20K. PEM demonstrates outstanding performance on every task\nand dataset, outperforming task-specific architectures while being comparable\nand even better than computationally-expensive baselines.",
      "tldr_zh": "该论文提出了一种高效的Transformer-based图像分割架构——PEM（Prototype-based Efficient MaskFormer），旨在解决现有模型计算资源密集的问题，同时支持语义和全景分割等多个任务。PEM引入了新型的prototype-based cross-attention机制，利用视觉特征的冗余减少计算量，同时设计了一个高效的多尺度特征金字塔网络，通过deformable convolutions和context-based self-modulation提取高语义内容的特征。实验结果显示，在Cityscapes和ADE20K数据集上，PEM在语义和全景分割任务中表现出色，超越了特定任务架构，甚至优于计算资源密集的基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Project page: https://niccolocavagnero.github.io/PEM",
      "pdf_url": "http://arxiv.org/pdf/2402.19422v3",
      "published_date": "2024-02-29 18:21:54 UTC",
      "updated_date": "2024-05-06 10:06:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:18:04.837768"
    },
    {
      "arxiv_id": "2402.19421v1",
      "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines",
      "title_zh": "知识构建：探索基于聊天的搜索引擎的创造性机制",
      "authors": [
        "Lijia Ma",
        "Xingchen Xu",
        "Yong Tan"
      ],
      "abstract": "In the domain of digital information dissemination, search engines act as\npivotal conduits linking information seekers with providers. The advent of\nchat-based search engines utilizing Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary\nleap in the search ecosystem. They demonstrate metacognitive abilities in\ninterpreting web information and crafting responses with human-like\nunderstanding and creativity. Nonetheless, the intricate nature of LLMs renders\ntheir \"cognitive\" processes opaque, challenging even their designers'\nunderstanding. This research aims to dissect the mechanisms through which an\nLLM-powered chat-based search engine, specifically Bing Chat, selects\ninformation sources for its responses. To this end, an extensive dataset has\nbeen compiled through engagements with New Bing, documenting the websites it\ncites alongside those listed by the conventional search engine. Employing\nnatural language processing (NLP) techniques, the research reveals that Bing\nChat exhibits a preference for content that is not only readable and formally\nstructured, but also demonstrates lower perplexity levels, indicating a unique\ninclination towards text that is predictable by the underlying LLM. Further\nenriching our analysis, we procure an additional dataset through interactions\nwith the GPT-4 based knowledge retrieval API, unveiling a congruent text\npreference between the RAG API and Bing Chat. This consensus suggests that\nthese text preferences intrinsically emerge from the underlying language\nmodels, rather than being explicitly crafted by Bing Chat's developers.\nMoreover, our investigation documents a greater similarity among websites cited\nby RAG technologies compared to those ranked highest by conventional search\nengines.",
      "tldr_zh": "这篇论文探讨了基于 Large Language Models (LLMs) 和 Retrieval Augmented Generation (RAG) 的聊天式搜索引擎（如 Bing Chat）如何选择信息来源，并揭示其内在的创造性机制。研究通过编译数据集，与 New Bing 互动并比较传统搜索引擎的引用网站，使用 Natural Language Processing (NLP) 技术分析发现，Bing Chat 偏好可读性强、结构正式且 perplexity 低的文本。进一步比较 GPT-4 基于的 RAG API，显示这种文本偏好源于底层语言模型的内在特性，而非开发者的显式设计。最终，研究证明 RAG 技术引用的网站比传统搜索引擎更相似，这为理解搜索引擎的认知过程提供了新洞见。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "econ.GN",
        "q-fin.EC",
        "J.4"
      ],
      "primary_category": "cs.IR",
      "comment": "38 pages, 2 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.19421v1",
      "published_date": "2024-02-29 18:20:37 UTC",
      "updated_date": "2024-02-29 18:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:18:18.337262"
    },
    {
      "arxiv_id": "2402.19420v2",
      "title": "Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning",
      "title_zh": "通过多智能体强化学习理解迭代组合拍卖设计",
      "authors": [
        "Greg d'Eon",
        "Neil Newman",
        "Kevin Leyton-Brown"
      ],
      "abstract": "Iterative combinatorial auctions are widely used in high stakes settings such\nas spectrum auctions. Such auctions can be hard to analyze, making it difficult\nfor bidders to determine how to behave and for designers to optimize auction\nrules to ensure desirable outcomes such as high revenue or welfare. In this\npaper, we investigate whether multi-agent reinforcement learning (MARL)\nalgorithms can be used to understand iterative combinatorial auctions, given\nthat these algorithms have recently shown empirical success in several other\ndomains. We find that MARL can indeed benefit auction analysis, but that\ndeploying it effectively is nontrivial. We begin by describing modelling\ndecisions that keep the resulting game tractable without sacrificing important\nfeatures such as imperfect information or asymmetry between bidders. We also\ndiscuss how to navigate pitfalls of various MARL algorithms, how to overcome\nchallenges in verifying convergence, and how to generate and interpret multiple\nequilibria. We illustrate the promise of our resulting approach by using it to\nevaluate a specific rule change to a clock auction, finding substantially\ndifferent auction outcomes due to complex changes in bidders' behavior.",
      "tldr_zh": "这篇论文探讨了使用多智能体强化学习 (MARL) 来分析迭代组合拍卖的设计，以解决竞标者行为不确定性和拍卖规则优化难题。作者提出了一系列建模决策，确保游戏保持可处理性，同时保留不完美信息和竞标者不对称等关键特征，并讨论了MARL算法的潜在陷阱、收敛验证挑战以及多个均衡的生成与解释。实验结果显示，通过评估钟拍卖 (clock auction) 的特定规则变化，MARL揭示了竞标者行为复杂变化，导致拍卖结果显著不同，从而为改进拍卖设计提供了新颖方法。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "18 pages (body) + 11 pages (acknowledgements, references, appendices)",
      "pdf_url": "http://arxiv.org/pdf/2402.19420v2",
      "published_date": "2024-02-29 18:16:13 UTC",
      "updated_date": "2024-07-23 19:15:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:18:29.648779"
    },
    {
      "arxiv_id": "2402.19406v2",
      "title": "On the Scaling Laws of Geographical Representation in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Nathan Godey",
        "Éric de la Clergerie",
        "Benoît Sagot"
      ],
      "abstract": "Language models have long been shown to embed geographical information in\ntheir hidden representations. This line of work has recently been revisited by\nextending this result to Large Language Models (LLMs). In this paper, we\npropose to fill the gap between well-established and recent literature by\nobserving how geographical knowledge evolves when scaling language models. We\nshow that geographical knowledge is observable even for tiny models, and that\nit scales consistently as we increase the model size. Notably, we observe that\nlarger language models cannot mitigate the geographical bias that is inherent\nto the training data.",
      "tldr_zh": "本研究探讨了语言模型中地理表示的规模化规律，旨在观察地理知识如何随模型规模的增加而演变。研究发现，即使在小型语言模型中也能观察到地理知识的存在，且这种知识会随着模型大小的增加而一致扩展。然而，更大的Large Language Models无法缓解训练数据中固有的地理偏差，为理解模型偏见提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19406v2",
      "published_date": "2024-02-29 18:04:11 UTC",
      "updated_date": "2024-03-04 11:35:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:18:41.296754"
    },
    {
      "arxiv_id": "2402.19402v1",
      "title": "A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Young-Jin Park",
        "Donghyun Kim",
        "Frédéric Odermatt",
        "Juho Lee",
        "Kyung-Min Kim"
      ],
      "abstract": "Time series forecasting is one of the most essential and ubiquitous tasks in\nmany business problems, including demand forecasting and logistics\noptimization. Traditional time series forecasting methods, however, have\nresulted in small models with limited expressive power because they have\ndifficulty in scaling their model size up while maintaining high accuracy. In\nthis paper, we propose Forecasting orchestra (Forchestra), a simple but\npowerful framework capable of accurately predicting future demand for a diverse\nrange of items. We empirically demonstrate that the model size is scalable to\nup to 0.8 billion parameters. The proposed method not only outperforms existing\nforecasting models with a significant margin, but it could generalize well to\nunseen data points when evaluated in a zero-shot fashion on downstream\ndatasets. Last but not least, we present extensive qualitative and quantitative\nstudies to analyze how the proposed model outperforms baseline models and\ndiffers from conventional approaches. The original paper was presented as a\nfull paper at ICDM 2022 and is available at:\nhttps://ieeexplore.ieee.org/document/10027662.",
      "tldr_zh": "该论文针对时间序列预测（Time series forecasting）在需求预测和物流优化中的关键应用，提出了一种可扩展且可转移的框架Forchestra，以解决传统方法模型规模小、表现力有限的问题。该框架能够准确预测多样物品的未来需求，并支持模型规模扩展至0.8亿参数，在实验中显著优于现有预测模型。该方法在零-shot（零样本）设置下表现出色，能够泛化到未见数据，并通过广泛的定性和定量分析证明其优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a full paper at ICDM 2022",
      "pdf_url": "http://arxiv.org/pdf/2402.19402v1",
      "published_date": "2024-02-29 18:01:07 UTC",
      "updated_date": "2024-02-29 18:01:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:18:53.084855"
    },
    {
      "arxiv_id": "2402.19379v6",
      "title": "Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Schoenegger",
        "Indre Tuminauskaite",
        "Peter S. Park",
        "Philip E. Tetlock"
      ],
      "abstract": "Human forecasting accuracy in practice relies on the 'wisdom of the crowd'\neffect, in which predictions about future events are significantly improved by\naggregating across a crowd of individual forecasters. Past work on the\nforecasting ability of large language models (LLMs) suggests that frontier\nLLMs, as individual forecasters, underperform compared to the gold standard of\na human crowd forecasting tournament aggregate. In Study 1, we expand this\nresearch by using an LLM ensemble approach consisting of a crowd of twelve\nLLMs. We compare the aggregated LLM predictions on 31 binary questions to that\nof a crowd of 925 human forecasters from a three-month forecasting tournament.\nOur preregistered main analysis shows that the LLM crowd outperforms a simple\nno-information benchmark and is not statistically different from the human\ncrowd. In exploratory analyses, we find that these two approaches are\nequivalent with respect to medium-effect-size equivalence bounds. We also\nobserve an acquiescence effect, with mean model predictions being significantly\nabove 50%, despite an almost even split of positive and negative resolutions.\nMoreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)\ncan be improved by drawing on human cognitive output. We find that both models'\nforecasting accuracy benefits from exposure to the median human prediction as\ninformation, improving accuracy by between 17% and 28%: though this leads to\nless accurate predictions than simply averaging human and machine forecasts.\nOur results suggest that LLMs can achieve forecasting accuracy rivaling that of\nhuman crowd forecasting tournaments: via the simple, practically applicable\nmethod of forecast aggregation. This replicates the 'wisdom of the crowd'\neffect for LLMs, and opens up their use for a variety of applications\nthroughout society.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）的预测能力，证明通过“LLM ensemble”方法（即聚合多个LLMs的预测），可以与人类的“wisdom of the crowd”效果相媲美。在Study 1中，研究者将12个LLMs的聚合预测与925名人类预测者在31个二元问题上的表现比较，结果显示LLM crowd 超过了无信息基准，且与人类群体准确性无显著差异，同时观察到顺从效应（acquiescence effect）。在Study 2中，进一步发现，GPT-4和Claude 2等模型通过参考人类中位预测，能将准确性提升17-28%，但最佳效果来自简单平均人类和机器预测。该成果表明，LLMs通过预测聚合可复制群体智慧效应，在社会应用中具有广阔潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "20 pages; 13 visualizations (nine figures, four tables)",
      "pdf_url": "http://arxiv.org/pdf/2402.19379v6",
      "published_date": "2024-02-29 17:27:59 UTC",
      "updated_date": "2024-07-22 13:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:19:06.280739"
    },
    {
      "arxiv_id": "2402.19371v1",
      "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
      "title_zh": "OpenMedLM：提示工程在医疗问答中使用开源大语言模型时可超越微调",
      "authors": [
        "Jenish Maharjan",
        "Anurag Garikipati",
        "Navan Preet Singh",
        "Leo Cyrus",
        "Mayank Sharma",
        "Madalina Ciobanu",
        "Gina Barnes",
        "Rahul Thapa",
        "Qingqing Mao",
        "Ritankar Das"
      ],
      "abstract": "LLMs have become increasingly capable at accomplishing a range of\nspecialized-tasks and can be utilized to expand equitable access to medical\nknowledge. Most medical LLMs have involved extensive fine-tuning, leveraging\nspecialized medical data and significant, thus costly, amounts of computational\npower. Many of the top performing LLMs are proprietary and their access is\nlimited to very few research groups. However, open-source (OS) models represent\na key area of growth for medical LLMs due to significant improvements in\nperformance and an inherent ability to provide the transparency and compliance\nrequired in healthcare. We present OpenMedLM, a prompting platform which\ndelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.\nWe evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks\n(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of\nprompting strategies, including zero-shot, few-shot, chain-of-thought (random\nselection and kNN selection), and ensemble/self-consistency voting. We found\nthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,\nsurpassing the previous best performing OS models that leveraged\ncomputationally costly extensive fine-tuning. The model delivers a 72.6%\naccuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and\nachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the\nfirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlight\nmedical-specific emergent properties in OS LLMs which have not yet been\ndocumented to date elsewhere, and showcase the benefits of further leveraging\nprompt engineering to improve the performance of accessible LLMs for medical\napplications.",
      "tldr_zh": "这篇论文介绍了 OpenMedLM，一个基于提示工程(prompt engineering)的平台，使用开源大型语言模型(OS LLMs)来提升医疗问答性能，并证明其优于传统的 fine-tuning 方法。研究评估了多种 OS LLMs (7B-70B) 在 MedQA、MedMCQA、PubMedQA 和 MMLU medical-subset 等基准上，通过 zero-shot、few-shot、chain-of-thought (随机和 kNN 选择) 以及 ensemble/self-consistency voting 等策略进行优化。结果显示，OpenMedLM 在三个医疗基准上达到了开源 SOTA 水平，在 MedQA 上获得 72.6% 准确率（比先前 SOTA 高 2.4%），并在 MMLU medical-subset 上首次超过 80% 准确率（81.7%），突显了提示工程在医疗领域的潜力及其促进知识可及性和透明度的益处。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19371v1",
      "published_date": "2024-02-29 17:19:39 UTC",
      "updated_date": "2024-02-29 17:19:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:19:21.348694"
    },
    {
      "arxiv_id": "2402.19366v3",
      "title": "Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency",
      "title_zh": "探索大型语言模型提升数字取证调查效率的潜力",
      "authors": [
        "Akila Wickramasekara",
        "Frank Breitinger",
        "Mark Scanlon"
      ],
      "abstract": "The ever-increasing workload of digital forensic labs raises concerns about\nlaw enforcement's ability to conduct both cyber-related and non-cyber-related\ninvestigations promptly. Consequently, this article explores the potential and\nusefulness of integrating Large Language Models (LLMs) into digital forensic\ninvestigations to address challenges such as bias, explainability, censorship,\nresource-intensive infrastructure, and ethical and legal considerations. A\ncomprehensive literature review is carried out, encompassing existing digital\nforensic models, tools, LLMs, deep learning techniques, and the use of LLMs in\ninvestigations. The review identifies current challenges within existing\ndigital forensic processes and explores both the obstacles and the\npossibilities of incorporating LLMs. In conclusion, the study states that the\nadoption of LLMs in digital forensics, with appropriate constraints, has the\npotential to improve investigation efficiency, improve traceability, and\nalleviate the technical and judicial barriers faced by law enforcement\nentities.",
      "tldr_zh": "这篇论文探讨了将大型语言模型 (LLMs) 整合到数字取证调查中，以应对执法机构面对的工作量增加、偏见、可解释性、审查和资源问题。研究通过全面文献综述，分析了现有数字取证模型、工具、LLMs 及其在调查中的应用，识别了当前挑战并评估了整合障碍与潜力。结论指出，适当约束下，采用 LLMs 可以提高调查效率、提升可追溯性，并缓解执法机构的技术和司法障碍。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19366v3",
      "published_date": "2024-02-29 17:13:44 UTC",
      "updated_date": "2025-01-31 08:27:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:19:30.008871"
    },
    {
      "arxiv_id": "2402.19361v2",
      "title": "Watermark Stealing in Large Language Models",
      "title_zh": "大型语言模型",
      "authors": [
        "Nikola Jovanović",
        "Robin Staab",
        "Martin Vechev"
      ],
      "abstract": "LLM watermarking has attracted attention as a promising way to detect\nAI-generated content, with some works suggesting that current schemes may\nalready be fit for deployment. In this work we dispute this claim, identifying\nwatermark stealing (WS) as a fundamental vulnerability of these schemes. We\nshow that querying the API of the watermarked LLM to approximately\nreverse-engineer a watermark enables practical spoofing attacks, as\nhypothesized in prior work, but also greatly boosts scrubbing attacks, which\nwas previously unnoticed. We are the first to propose an automated WS algorithm\nand use it in the first comprehensive study of spoofing and scrubbing in\nrealistic settings. We show that for under $50 an attacker can both spoof and\nscrub state-of-the-art schemes previously considered safe, with average success\nrate of over 80%. Our findings challenge common beliefs about LLM watermarking,\nstressing the need for more robust schemes. We make all our code and additional\nexamples available at https://watermark-stealing.org.",
      "tldr_zh": "该研究揭示了大型语言模型(LLM)水印技术的根本漏洞，即watermark stealing，通过查询水印LLM的API，攻击者可以实现欺骗(spoofing)和清除(scrubbing)攻击。论文首次提出了一种自动化的WS算法，并对这些攻击在现实场景中进行了全面研究，结果显示攻击者只需不到50美元即可对最先进的方案成功攻击，平均成功率超过80%。这些发现挑战了现有LLM watermarking方案的安全性，强调了开发更robust方法的必要性，并公开了相关代码以供进一步验证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.19361v2",
      "published_date": "2024-02-29 17:12:39 UTC",
      "updated_date": "2024-06-24 14:48:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:19:41.111535"
    },
    {
      "arxiv_id": "2402.19348v2",
      "title": "Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",
      "title_zh": "深度学习用于城市计算中的跨域数据融合：分类法、进展与展望",
      "authors": [
        "Xingchen Zou",
        "Yibo Yan",
        "Xixuan Hao",
        "Yuehong Hu",
        "Haomin Wen",
        "Erdong Liu",
        "Junbo Zhang",
        "Yong Li",
        "Tianrui Li",
        "Yu Zheng",
        "Yuxuan Liang"
      ],
      "abstract": "As cities continue to burgeon, Urban Computing emerges as a pivotal\ndiscipline for sustainable development by harnessing the power of cross-domain\ndata fusion from diverse sources (e.g., geographical, traffic, social media,\nand environmental data) and modalities (e.g., spatio-temporal, visual, and\ntextual modalities). Recently, we are witnessing a rising trend that utilizes\nvarious deep-learning methods to facilitate cross-domain data fusion in smart\ncities. To this end, we propose the first survey that systematically reviews\nthe latest advancements in deep learning-based data fusion methods tailored for\nurban computing. Specifically, we first delve into data perspective to\ncomprehend the role of each modality and data source. Secondly, we classify the\nmethodology into four primary categories: feature-based, alignment-based,\ncontrast-based, and generation-based fusion methods. Thirdly, we further\ncategorize multi-modal urban applications into seven types: urban planning,\ntransportation, economy, public safety, society, environment, and energy.\nCompared with previous surveys, we focus more on the synergy of deep learning\nmethods with urban computing applications. Furthermore, we shed light on the\ninterplay between Large Language Models (LLMs) and urban computing, postulating\nfuture research directions that could revolutionize the field. We firmly\nbelieve that the taxonomy, progress, and prospects delineated in our survey\nstand poised to significantly enrich the research community. The summary of the\ncomprehensive and up-to-date paper list can be found at\nhttps://github.com/yoshall/Awesome-Multimodal-Urban-Computing.",
      "tldr_zh": "这篇论文系统回顾了深度学习在城市计算中的跨领域数据融合应用，提出首个针对该领域的分类框架，以促进智能城市可持续发展。论文从数据视角分析了不同模态（如时空、视觉和文本）和来源（如地理、交通数据），并将融合方法分为feature-based、alignment-based、contrast-based和generation-based四大类。作者进一步分类了多模态城市应用，包括城市规划、交通、经济等领域，并强调深度学习与这些应用的协同作用。论文探讨了Large Language Models (LLMs) 与城市计算的互动潜力，并提出未来研究方向，如革命性融合技术，以丰富研究社区。有关论文列表可查阅提供的GitHub链接。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19348v2",
      "published_date": "2024-02-29 16:56:23 UTC",
      "updated_date": "2024-06-16 10:16:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:19:53.257425"
    },
    {
      "arxiv_id": "2402.19339v1",
      "title": "Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification",
      "title_zh": "缝合缝隙：融合情境感知知识与视觉Transformer用于高级图像分类",
      "authors": [
        "Delfina Sol Martinez Pandiani",
        "Nicolas Lazzari",
        "Valentina Presutti"
      ],
      "abstract": "The increasing demand for automatic high-level image understanding,\nparticularly in detecting abstract concepts (AC) within images, underscores the\nnecessity for innovative and more interpretable approaches. These approaches\nneed to harmonize traditional deep vision methods with the nuanced,\ncontext-dependent knowledge humans employ to interpret images at intricate\nsemantic levels. In this work, we leverage situated perceptual knowledge of\ncultural images to enhance performance and interpretability in AC image\nclassification. We automatically extract perceptual semantic units from images,\nwhich we then model and integrate into the ARTstract Knowledge Graph (AKG).\nThis resource captures situated perceptual semantics gleaned from over 14,000\ncultural images labeled with ACs. Additionally, we enhance the AKG with\nhigh-level linguistic frames. We compute KG embeddings and experiment with\nrelative representations and hybrid approaches that fuse these embeddings with\nvisual transformer embeddings. Finally, for interpretability, we conduct\nposthoc qualitative analyses by examining model similarities with training\ninstances. Our results show that our hybrid KGE-ViT methods outperform existing\ntechniques in AC image classification. The posthoc interpretability analyses\nreveal the visual transformer's proficiency in capturing pixel-level visual\nattributes, contrasting with our method's efficacy in representing more\nabstract and semantic scene elements. We demonstrate the synergy and\ncomplementarity between KGE embeddings' situated perceptual knowledge and deep\nvisual model's sensory-perceptual understanding for AC image classification.\nThis work suggests a strong potential of neuro-symbolic methods for knowledge\nintegration and robust image representation for use in downstream intricate\nvisual comprehension tasks. All the materials and code are available online.",
      "tldr_zh": "本研究针对高水平图像分类中的抽象概念（AC）检测问题，提出了一种融合situated perceptual knowledge与Vision Transformers的方法，以提升性能和可解释性。研究者从超过14,000张文化图像中提取感知语义单元，构建ARTstract Knowledge Graph (AKG)，并结合高级语言框架计算KG embeddings，然后与ViT embeddings进行融合，形成混合KGE-ViT方法。实验结果显示，该方法在AC图像分类中优于现有技术，并在后验解释性分析中揭示了ViT在像素级视觉属性的优势与KGE在抽象语义元素的互补作用，证明了神经符号方法在知识整合和复杂视觉任务中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2402.19339v1",
      "published_date": "2024-02-29 16:46:48 UTC",
      "updated_date": "2024-02-29 16:46:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:20:05.223676"
    },
    {
      "arxiv_id": "2403.00046v2",
      "title": "SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xue Jiang",
        "Yihong Dong",
        "Zhi Jin",
        "Ge Li"
      ],
      "abstract": "Although Large Language Models (LLMs) have made significant progress in code\ngeneration, they still struggle with code generation tasks in specific\nscenarios. These scenarios usually necessitate the adaptation of LLMs to\nfulfill specific needs, but the limited training samples available in practice\nlead to poor code generation performance. Therefore, how to effectively adapt\nLLMs to new scenarios with few training samples is a major challenge for\ncurrent code generation. In this paper, we propose a novel adaptation approach\nnamed SEED, which stands for Sample-Efficient adaptation with Error-Driven\nlearning for code generation. SEED leverages the errors made by LLMs as\nlearning opportunities, using error revision to overcome its own shortcomings,\nthus achieving efficient learning. Specifically, SEED involves identifying\nerror code generated by LLMs, employing Self-revise for code revision,\noptimizing the model with revised code, and iteratively adapting the process\nfor continuous improvement. Experimental results show that, compared to other\nmainstream fine-tuning approaches, SEED achieves superior performance with few\ntraining samples, showing an average relative improvement of 54.7% in Pass@1 on\nmultiple code generation benchmarks. We also validate the effectiveness of\nSelf-revise, which generates revised code that optimizes the model more\nefficiently compared to the code samples from datasets. Moreover, SEED\nconsistently demonstrates strong performance across various LLMs, underscoring\nits generalizability.",
      "tldr_zh": "这篇论文提出 SEED，一种样本高效的适应方法，用于定制 Large Language Models (LLMs) 以提升代码生成性能，尤其在训练样本有限的特定场景中。SEED 采用 Error-Driven learning 机制，通过识别 LLMs 生成的错误代码、利用 Self-revise 进行代码修正、并迭代优化模型，从而实现高效学习。实验结果显示，SEED 在多个代码生成基准上比主流微调方法平均提升 54.7% 的 Pass@1 指标，且其泛化性强，适用于各种 LLMs。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00046v2",
      "published_date": "2024-02-29 16:09:02 UTC",
      "updated_date": "2024-03-23 16:51:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:20:17.772978"
    },
    {
      "arxiv_id": "2402.19299v1",
      "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
      "title_zh": "翻译失败",
      "authors": [
        "Shaoteng Liu",
        "Haoqi Yuan",
        "Minda Hu",
        "Yanwei Li",
        "Yukang Chen",
        "Shu Liu",
        "Zongqing Lu",
        "Jiaya Jia"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated proficiency in utilizing\nvarious tools by coding, yet they face limitations in handling intricate logic\nand precise control. In embodied tasks, high-level planning is amenable to\ndirect coding, while low-level actions often necessitate task-specific\nrefinement, such as Reinforcement Learning (RL). To seamlessly integrate both\nmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprising\na slow agent and a fast agent. The slow agent analyzes actions suitable for\ncoding, while the fast agent executes coding tasks. This decomposition\neffectively focuses each agent on specific tasks, proving highly efficient\nwithin our pipeline. Our approach outperforms traditional RL methods and\nexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,\nit rapidly obtains diamonds within a single day on an RTX3090. Additionally, it\nachieves SOTA performance across all designated MineDojo tasks.",
      "tldr_zh": "该论文提出RL-GPT框架，将强化学习（Reinforcement Learning, RL）和代码作为策略（Code-as-policy）整合，以克服大语言模型（LLMs）在处理复杂逻辑和精确控制时的局限性。该框架采用两层层次结构，包括慢代理负责高层规划分析适合编码的动作，以及快代理执行编码任务，从而提高任务分解效率。在Minecraft游戏中，RL-GPT在RTX3090上快速获得钻石，并在MineDojo任务中达到SOTA性能，整体表现优于传统RL方法和现有GPT代理。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19299v1",
      "published_date": "2024-02-29 16:07:22 UTC",
      "updated_date": "2024-02-29 16:07:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:20:29.005846"
    },
    {
      "arxiv_id": "2402.19294v1",
      "title": "Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes",
      "title_zh": "未知故障模式下的退化建模与预后分析",
      "authors": [
        "Ying Fu",
        "Ye Kwon Huh",
        "Kaibo Liu"
      ],
      "abstract": "Operating units often experience various failure modes in complex systems,\nleading to distinct degradation paths. Relying on a prognostic model trained on\na single failure mode may lead to poor generalization performance across\nmultiple failure modes. Therefore, accurately identifying the failure mode is\nof critical importance. Current prognostic approaches either ignore failure\nmodes during degradation or assume known failure mode labels, which can be\nchallenging to acquire in practice. Moreover, the high dimensionality and\ncomplex relations of sensor signals make it challenging to identify the failure\nmodes accurately. To address these issues, we propose a novel failure mode\ndiagnosis method that leverages a dimension reduction technique called UMAP\n(Uniform Manifold Approximation and Projection) to project and visualize each\nunit's degradation trajectory into a lower dimension. Then, using these\ndegradation trajectories, we develop a time series-based clustering method to\nidentify the training units' failure modes. Finally, we introduce a\nmonotonically constrained prognostic model to predict the failure mode labels\nand RUL of the test units simultaneously using the obtained failure modes of\nthe training units. The proposed prognostic model provides failure\nmode-specific RUL predictions while preserving the monotonic property of the\nRUL predictions across consecutive time steps. We evaluate the proposed model\nusing a case study with the aircraft gas turbine engine dataset.",
      "tldr_zh": "本研究针对复杂系统中设备在未知故障模式下的退化建模和预后分析问题，提出了一种新方法，以解决现有模型在多故障模式下泛化性能差的问题。方法首先使用 UMAP 降维技术投影退化轨迹，然后通过基于时间序列的聚类方法识别训练单元的故障模式，最后引入一个单调约束预后模型，同时预测测试单元的故障模式和剩余寿命 (RUL)。该模型能提供特定于故障模式的 RUL 预测，并确保预测结果在连续时间步上保持单调性；在飞机燃气轮机引擎数据集的案例研究中，验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19294v1",
      "published_date": "2024-02-29 15:57:09 UTC",
      "updated_date": "2024-02-29 15:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:20:42.066603"
    },
    {
      "arxiv_id": "2402.19267v2",
      "title": "Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Seunghyun Ji",
        "Hagai Raja Sinulingga",
        "Darongsae Kwon"
      ],
      "abstract": "Low-resourced data presents a significant challenge for neural machine\ntranslation. In most cases, the low-resourced environment is caused by high\ncosts due to the need for domain experts or the lack of language experts.\nTherefore, identifying the most training-efficient data within an unsupervised\nsetting emerges as a practical strategy. Recent research suggests that such\neffective data can be identified by selecting 'appropriately complex data'\nbased on its volume, providing strong intuition for unsupervised data\nselection. However, we have discovered that establishing criteria for\nunsupervised data selection remains a challenge, as the 'appropriate level of\ndifficulty' may vary depending on the data domain. We introduce a novel\nunsupervised data selection method named 'Capturing Perplexing Named Entities,'\nwhich leverages the maximum inference entropy in translated named entities as a\nmetric for selection. When tested with the 'Korean-English Parallel Corpus of\nSpecialized Domains,' our method served as robust guidance for identifying\ntraining-efficient data across different domains, in contrast to existing\nmethods.",
      "tldr_zh": "本研究针对低资源神经机器翻译（Machine Translation）的挑战，提出了一种新的无监督数据选择方法——Capturing Perplexing Named Entities，该方法使用翻译的命名实体（Named Entities）中的最大推理熵作为选择指标，以识别领域特定的训练高效数据。不同于现有方法，该方法考虑了“适当难度”因数据领域而异的特性，提供更鲁棒的指导。实验在Korean-English Parallel Corpus of Specialized Domains语料库上验证，该方法在不同领域的数据选择中优于基线模型，帮助提升翻译性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 3 figures, 5 tables. Oral presentation was given in SIGUL\n  2024, a satellite workshop of LREC-COLING 2024\n  (https://sigul-2024.ilc.cnr.it/wp-content/uploads/2024/05/Ji-et-al.pdf)",
      "pdf_url": "http://arxiv.org/pdf/2402.19267v2",
      "published_date": "2024-02-29 15:38:28 UTC",
      "updated_date": "2024-05-21 17:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:20:53.091224"
    },
    {
      "arxiv_id": "2402.19265v1",
      "title": "Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Meli",
        "Alberto Castellini",
        "Alessandro Farinelli"
      ],
      "abstract": "Partially Observable Markov Decision Processes (POMDPs) are a powerful\nframework for planning under uncertainty. They allow to model state uncertainty\nas a belief probability distribution. Approximate solvers based on Monte Carlo\nsampling show great success to relax the computational demand and perform\nonline planning. However, scaling to complex realistic domains with many\nactions and long planning horizons is still a major challenge, and a key point\nto achieve good performance is guiding the action-selection process with\ndomain-dependent policy heuristics which are tailored for the specific\napplication domain. We propose to learn high-quality heuristics from POMDP\ntraces of executions generated by any solver. We convert the belief-action\npairs to a logical semantics, and exploit data- and time-efficient Inductive\nLogic Programming (ILP) to generate interpretable belief-based policy\nspecifications, which are then used as online heuristics. We evaluate\nthoroughly our methodology on two notoriously challenging POMDP problems,\ninvolving large action spaces and long planning horizons, namely, rocksample\nand pocman. Considering different state-of-the-art online POMDP solvers,\nincluding POMCP, DESPOT and AdaOPS, we show that learned heuristics expressed\nin Answer Set Programming (ASP) yield performance superior to neural networks\nand similar to optimal handcrafted task-specific heuristics within lower\ncomputational time. Moreover, they well generalize to more challenging\nscenarios not experienced in the training phase (e.g., increasing rocks and\ngrid size in rocksample, incrementing the size of the map and the aggressivity\nof ghosts in pocman).",
      "tldr_zh": "这篇论文提出了一种使用 Inductive Logic Programming (ILP) 的方法，从 Partially Observable Markov Decision Processes (POMDPs) 的执行轨迹中学习逻辑规范，作为域相关的启发式策略来指导行动选择。该方法将信念-动作对转换为逻辑语义，生成可解释的信念-based 政策规范，并以 Answer Set Programming (ASP) 形式实现，用于在线规划。实验在 rocksample 和 pocman 等挑战性问题上显示，与 POMCP、DESPOT 和 AdaOPS 等求解器结合时，学得的启发式策略性能优于神经网络、接近最优手工策略，且计算时间更低，同时在未见场景中表现出色泛化能力。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19265v1",
      "published_date": "2024-02-29 15:36:01 UTC",
      "updated_date": "2024-02-29 15:36:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:21:06.502799"
    },
    {
      "arxiv_id": "2402.19263v1",
      "title": "Spinal Osteophyte Detection via Robust Patch Extraction on minimally annotated X-rays",
      "title_zh": "翻译失败",
      "authors": [
        "Soumya Snigdha Kundu",
        "Yuanhan Mo",
        "Nicharee Srikijkasemwat",
        "Bartłomiej W. Papiez"
      ],
      "abstract": "The development and progression of arthritis is strongly associated with\nosteophytes, which are small and elusive bone growths. This paper presents one\nof the first efforts towards automated spinal osteophyte detection in spinal\nX-rays. A novel automated patch extraction process, called SegPatch, has been\nproposed based on deep learning-driven vertebrae segmentation and the\nenlargement of mask contours. A final patch classification accuracy of 84.5\\%\nis secured, surpassing a baseline tiling-based patch generation technique by\n9.5%. This demonstrates that even with limited annotations, SegPatch can\ndeliver superior performance for detection of tiny structures such as\nosteophytes. The proposed approach has potential to assist clinicians in\nexpediting the process of manually identifying osteophytes in spinal X-ray.",
      "tldr_zh": "本研究针对脊柱X-rays中微小骨刺（osteophytes）的检测问题，提出了一种新的自动化补丁提取方法SegPatch，该方法基于深度学习驱动的椎骨分割和掩码轮廓扩展，即使在标注有限的情况下也能实现高效检测。SegPatch通过改进补丁生成过程，显著提升了分类准确率，达到84.5%，比基线平铺技术高出9.5%。这项创新有助于临床医生加速手动识别骨刺的过程，提高关节炎诊断效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ISBI'24 Full Paper",
      "pdf_url": "http://arxiv.org/pdf/2402.19263v1",
      "published_date": "2024-02-29 15:32:25 UTC",
      "updated_date": "2024-02-29 15:32:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:21:16.905137"
    },
    {
      "arxiv_id": "2405.00031v1",
      "title": "SegNet: A Segmented Deep Learning based Convolutional Neural Network Approach for Drones Wildfire Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya V. Jonnalagadda",
        "Hashim A. Hashim"
      ],
      "abstract": "This research addresses the pressing challenge of enhancing processing times\nand detection capabilities in Unmanned Aerial Vehicle (UAV)/drone imagery for\nglobal wildfire detection, despite limited datasets. Proposing a Segmented\nNeural Network (SegNet) selection approach, we focus on reducing feature maps\nto boost both time resolution and accuracy significantly advancing processing\nspeeds and accuracy in real-time wildfire detection. This paper contributes to\nincreased processing speeds enabling real-time detection capabilities for\nwildfire, increased detection accuracy of wildfire, and improved detection\ncapabilities of early wildfire, through proposing a new direction for image\nclassification of amorphous objects like fire, water, smoke, etc. Employing\nConvolutional Neural Networks (CNNs) for image classification, emphasizing on\nthe reduction of irrelevant features vital for deep learning processes,\nespecially in live feed data for fire detection. Amidst the complexity of live\nfeed data in fire detection, our study emphasizes on image feed, highlighting\nthe urgency to enhance real-time processing. Our proposed algorithm combats\nfeature overload through segmentation, addressing challenges arising from\ndiverse features like objects, colors, and textures. Notably, a delicate\nbalance of feature map size and dataset adequacy is pivotal. Several research\npapers use smaller image sizes, compromising feature richness which\nnecessitating a new approach. We illuminate the critical role of pixel density\nin retaining essential details, especially for early wildfire detection. By\ncarefully selecting number of filters during training, we underscore the\nsignificance of higher pixel density for proper feature selection. The proposed\nSegNet approach is rigorously evaluated using real-world dataset obtained by a\ndrone flight and compared to state-of-the-art literature.",
      "tldr_zh": "本文提出 SegNet，一种基于 Convolutional Neural Networks (CNNs) 的分段神经网络方法，用于 Unmanned Aerial Vehicle (UAV)/无人机图像中的野火检测，通过减少特征映射来显著提升处理速度和检测准确性。SegNet 重点解决实时数据中的特征过载问题，如对象、颜色和纹理的多样性，并强调像素密度和特征选择的平衡，以改善对早期野火的识别能力。实验在真实无人机数据集上进行评估，结果显示该方法在处理速度和准确性上优于现有文献，为无定形物体（如火、烟）图像分类提供新方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.00031v1",
      "published_date": "2024-02-29 15:23:12 UTC",
      "updated_date": "2024-02-29 15:23:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:21:29.340241"
    },
    {
      "arxiv_id": "2402.19251v1",
      "title": "A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving",
      "title_zh": "一种基于认知的自动驾驶轨迹预测方法",
      "authors": [
        "Haicheng Liao",
        "Yongkang Li",
        "Zhenning Li",
        "Chengyue Wang",
        "Zhiyong Cui",
        "Shengbo Eben Li",
        "Chengzhong Xu"
      ],
      "abstract": "In autonomous vehicle (AV) technology, the ability to accurately predict the\nmovements of surrounding vehicles is paramount for ensuring safety and\noperational efficiency. Incorporating human decision-making insights enables\nAVs to more effectively anticipate the potential actions of other vehicles,\nsignificantly improving prediction accuracy and responsiveness in dynamic\nenvironments. This paper introduces the Human-Like Trajectory Prediction (HLTP)\nmodel, which adopts a teacher-student knowledge distillation framework inspired\nby human cognitive processes. The HLTP model incorporates a sophisticated\nteacher-student knowledge distillation framework. The \"teacher\" model, equipped\nwith an adaptive visual sector, mimics the visual processing of the human\nbrain, particularly the functions of the occipital and temporal lobes. The\n\"student\" model focuses on real-time interaction and decision-making, drawing\nparallels to prefrontal and parietal cortex functions. This approach allows for\ndynamic adaptation to changing driving scenarios, capturing essential\nperceptual cues for accurate prediction. Evaluated using the Macao Connected\nand Autonomous Driving (MoCAD) dataset, along with the NGSIM and HighD\nbenchmarks, HLTP demonstrates superior performance compared to existing models,\nparticularly in challenging environments with incomplete data. The project page\nis available at Github.",
      "tldr_zh": "本论文提出了一种基于人类认知过程的轨迹预测方法，名为 Human-Like Trajectory Prediction (HLTP) 模型，用于提升自动驾驶车辆的安全性和效率，通过模拟人类决策来准确预测周围车辆的运动。HLTP 采用 teacher-student knowledge distillation 框架，其中 teacher 模型利用自适应视觉扇区模仿大脑枕叶和颞叶的视觉处理，student 模型则聚焦实时交互和决策，类似于前额叶和顶叶的功能，从而动态适应复杂驾驶场景。在 MoCAD、NGSIM 和 HighD 数据集的评估中，HLTP 模型在数据不完整的环境中表现出色，优于现有基准模型，为自动驾驶系统的鲁棒性提供了重要改进。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19251v1",
      "published_date": "2024-02-29 15:22:26 UTC",
      "updated_date": "2024-02-29 15:22:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:21:42.157925"
    },
    {
      "arxiv_id": "2403.00044v1",
      "title": "Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC",
      "title_zh": "翻译失败",
      "authors": [
        "Sikun Yang",
        "Heinz Koeppl"
      ],
      "abstract": "The edge partition model (EPM) is a generative model for extracting an\noverlapping community structure from static graph-structured data. In the EPM,\nthe gamma process (GaP) prior is adopted to infer the appropriate number of\nlatent communities, and each vertex is endowed with a gamma distributed\npositive memberships vector. Despite having many attractive properties,\ninference in the EPM is typically performed using Markov chain Monte Carlo\n(MCMC) methods that prevent it from being applied to massive network data. In\nthis paper, we generalize the EPM to account for dynamic enviroment by\nrepresenting each vertex with a positive memberships vector constructed using\nDirichlet prior specification, and capturing the time-evolving behaviour of\nvertices via a Dirichlet Markov chain construction. A simple-to-implement Gibbs\nsampler is proposed to perform posterior computation using Negative- Binomial\naugmentation technique. For large network data, we propose a stochastic\ngradient Markov chain Monte Carlo (SG-MCMC) algorithm for scalable inference in\nthe proposed model. The experimental results show that the novel methods\nachieve competitive performance in terms of link prediction, while being much\nfaster.",
      "tldr_zh": "本研究将传统的边分区模型 (EPM) 扩展到动态图结构数据中，通过采用 Dirichlet 先验构建顶点的正成员关系向量，并使用 Dirichlet Markov 链捕捉顶点的时序演变，从而处理网络的动态特性。论文提出了一种基于 Negative-Binomial 增强技术的简单 Gibbs 采样器进行后验计算，并开发了随机梯度 Markov 链 Monte Carlo (SG-MCMC) 算法，以支持大规模网络数据的可扩展推理。实验结果表明，该方法在链接预测任务上表现出与现有方法相当的性能，同时显著提高了计算效率。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00044v1",
      "published_date": "2024-02-29 15:19:35 UTC",
      "updated_date": "2024-02-29 15:19:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:21:52.257851"
    },
    {
      "arxiv_id": "2403.07924v2",
      "title": "AI and Identity",
      "title_zh": "人工智能与身份",
      "authors": [
        "Sri Yash Tadimalla",
        "Mary Lou Maher"
      ],
      "abstract": "AI-empowered technologies' impact on the world is undeniable, reshaping\nindustries, revolutionizing how humans interact with technology, transforming\neducational paradigms, and redefining social codes. However, this rapid growth\nis accompanied by two notable challenges: a lack of diversity within the AI\nfield and a widening AI divide. In this context, This paper examines the\nintersection of AI and identity as a pathway to understand biases,\ninequalities, and ethical considerations in AI development and deployment. We\npresent a multifaceted definition of AI identity, which encompasses its\ncreators, applications, and their broader impacts. Understanding AI's identity\ninvolves understanding the associations between the individuals involved in\nAI's development, the technologies produced, and the social, ethical, and\npsychological implications. After exploring the AI identity ecosystem and its\nsocietal dynamics, We propose a framework that highlights the need for\ndiversity in AI across three dimensions: Creators, Creations, and Consequences\nthrough the lens of identity. This paper proposes the need for a comprehensive\napproach to fostering a more inclusive and responsible AI ecosystem through the\nlens of identity.",
      "tldr_zh": "这篇论文探讨了AI技术对社会的影响，包括重塑产业、教育和社会规范，同时指出了AI领域缺乏多样性以及AI分歧加剧的问题。通过分析AI identity（AI身份）的多面定义——涵盖创建者、应用及其更广泛的社会、伦理和心理影响——论文揭示了AI发展中存在的偏见、不平等和伦理挑战。作者提出一个框架，强调在Creators（创建者）、Creations（创造物）和Consequences（后果）三个维度上推动AI多样性，以促进一个更具包容性和负责任的AI生态系统。总的来说，这为构建更公平的AI环境提供了关键见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages, 4 figures, AAAI Spring Symposium",
      "pdf_url": "http://arxiv.org/pdf/2403.07924v2",
      "published_date": "2024-02-29 15:07:30 UTC",
      "updated_date": "2024-04-10 18:08:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:22:06.418314"
    },
    {
      "arxiv_id": "2402.19197v2",
      "title": "Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Kennard Yanting Chan",
        "Fayao Liu",
        "Guosheng Lin",
        "Chuan Sheng Foo",
        "Weisi Lin"
      ],
      "abstract": "Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for\nsingle-view clothed human reconstruction. These models need to be trained using\na sampling training scheme. Existing sampling training schemes either fail to\ncapture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in\nreconstructed meshes. To address these problems, we introduce Fine\nStructured-Aware Sampling (FSS), a new sampling training scheme to train\npixel-aligned implicit models for single-view human reconstruction. FSS\nresolves the aforementioned problems by proactively adapting to the thickness\nand complexity of surfaces. In addition, unlike existing sampling training\nschemes, FSS shows how normals of sample points can be capitalized in the\ntraining process to improve results. Lastly, to further improve the training\nprocess, FSS proposes a mesh thickness loss signal for pixel-aligned implicit\nmodels. It becomes computationally feasible to introduce this loss once a\nslight reworking of the pixel-aligned implicit function framework is carried\nout. Our results show that our methods significantly outperform SOTA methods\nqualitatively and quantitatively. Our code is publicly available at\nhttps://github.com/kcyt/FSS.",
      "tldr_zh": "本研究针对像素对齐隐式模型（Pixel-Aligned Implicit Models，如 PIFu, PIFuHD 和 ICON）在单视图人体重建中的问题，提出了一种新的采样训练方案 Fine Structure-Aware Sampling (FSS)，以解决捕捉薄表面（如耳朵、手指）困难和重建网格噪声伪影的问题。FSS 通过主动适应表面的厚度和复杂度，并利用样本点的法线（normals）来优化训练过程，同时引入 mesh thickness loss 信号，进一步提升重建精度。实验结果显示，该方法在定性和定量上显著优于现有最先进方法（SOTA methods），并已开源代码于 https://github.com/kcyt/FSS。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in Proceedings of the AAAI Conference on Artificial\n  Intelligence, 2024 (AAAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.19197v2",
      "published_date": "2024-02-29 14:26:46 UTC",
      "updated_date": "2024-11-11 14:04:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:22:17.286190"
    },
    {
      "arxiv_id": "2402.19195v2",
      "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
      "title_zh": "知识图谱表示学习中的负采样：一个综述",
      "authors": [
        "Tiroshan Madushanka",
        "Ryutaro Ichise"
      ],
      "abstract": "Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding\n(KGE), is essential for AI applications such as knowledge construction and\ninformation retrieval. These models encode entities and relations into\nlower-dimensional vectors, supporting tasks like link prediction and\nrecommendation systems. Training KGE models relies on both positive and\nnegative samples for effective learning, but generating high-quality negative\nsamples from existing knowledge graphs is challenging. The quality of these\nsamples significantly impacts the model's accuracy. This comprehensive survey\npaper systematically reviews various negative sampling (NS) methods and their\ncontributions to the success of KGRL. Their respective advantages and\ndisadvantages are outlined by categorizing existing NS methods into six\ndistinct categories. Moreover, this survey identifies open research questions\nthat serve as potential directions for future investigations. By offering a\ngeneralization and alignment of fundamental NS concepts, this survey provides\nvaluable insights for designing effective NS methods in the context of KGRL and\nserves as a motivating force for further advancements in the field.",
      "tldr_zh": "这篇综述论文回顾了知识图谱表示学习（KGRL，或 Knowledge Graph Embedding, KGE）中负采样（Negative Sampling, NS）方法的重要性，这些方法通过生成高质量负样本来提升模型在链接预测和推荐系统等任务中的准确性。论文将现有 NS 方法分类为六类，详细分析了每类的优缺点，并指出了潜在的开放研究问题。总体上，这篇综述为设计有效的 NS 策略提供了宝贵见解，推动了 KGRL 领域的未来进展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19195v2",
      "published_date": "2024-02-29 14:26:20 UTC",
      "updated_date": "2024-10-21 02:30:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:22:29.596113"
    },
    {
      "arxiv_id": "2402.19173v1",
      "title": "StarCoder 2 and The Stack v2: The Next Generation",
      "title_zh": "StarCoder 2 与 The Stack v2：下一代",
      "authors": [
        "Anton Lozhkov",
        "Raymond Li",
        "Loubna Ben Allal",
        "Federico Cassano",
        "Joel Lamy-Poirier",
        "Nouamane Tazi",
        "Ao Tang",
        "Dmytro Pykhtar",
        "Jiawei Liu",
        "Yuxiang Wei",
        "Tianyang Liu",
        "Max Tian",
        "Denis Kocetkov",
        "Arthur Zucker",
        "Younes Belkada",
        "Zijian Wang",
        "Qian Liu",
        "Dmitry Abulkhanov",
        "Indraneil Paul",
        "Zhuang Li",
        "Wen-Ding Li",
        "Megan Risdal",
        "Jia Li",
        "Jian Zhu",
        "Terry Yue Zhuo",
        "Evgenii Zheltonozhskii",
        "Nii Osae Osae Dade",
        "Wenhao Yu",
        "Lucas Krauß",
        "Naman Jain",
        "Yixuan Su",
        "Xuanli He",
        "Manan Dey",
        "Edoardo Abati",
        "Yekun Chai",
        "Niklas Muennighoff",
        "Xiangru Tang",
        "Muhtasham Oblokulov",
        "Christopher Akiki",
        "Marc Marone",
        "Chenghao Mou",
        "Mayank Mishra",
        "Alex Gu",
        "Binyuan Hui",
        "Tri Dao",
        "Armel Zebaze",
        "Olivier Dehaene",
        "Nicolas Patry",
        "Canwen Xu",
        "Julian McAuley",
        "Han Hu",
        "Torsten Scholak",
        "Sebastien Paquet",
        "Jennifer Robinson",
        "Carolyn Jane Anderson",
        "Nicolas Chapados",
        "Mostofa Patwary",
        "Nima Tajbakhsh",
        "Yacine Jernite",
        "Carlos Muñoz Ferrandis",
        "Lingming Zhang",
        "Sean Hughes",
        "Thomas Wolf",
        "Arjun Guha",
        "Leandro von Werra",
        "Harm de Vries"
      ],
      "abstract": "The BigCode project, an open-scientific collaboration focused on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder2. In partnership with Software Heritage (SWH), we build\nThe Stack v2 on top of the digital commons of their source code archive.\nAlongside the SWH repositories spanning 619 programming languages, we carefully\nselect other high-quality data sources, such as GitHub pull requests, Kaggle\nnotebooks, and code documentation. This results in a training set that is 4x\nlarger than the first StarCoder dataset. We train StarCoder2 models with 3B,\n7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate\nthem on a comprehensive set of Code LLM benchmarks. We find that our small\nmodel, StarCoder2-3B, outperforms other Code LLMs of similar size on most\nbenchmarks, and also outperforms StarCoderBase-15B. Our large model,\nStarCoder2- 15B, significantly outperforms other models of comparable size. In\naddition, it matches or outperforms CodeLlama-34B, a model more than twice its\nsize. Although DeepSeekCoder- 33B is the best-performing model at code\ncompletion for high-resource languages, we find that StarCoder2-15B outperforms\nit on math and code reasoning benchmarks, as well as several low-resource\nlanguages. We make the model weights available under an OpenRAIL license and\nensure full transparency regarding the training data by releasing the SoftWare\nHeritage persistent IDentifiers (SWHIDs) of the source code data.",
      "tldr_zh": "BigCode 项目推出了 StarCoder 2 模型及其数据集 The Stack v2，这是 Code LLMs（代码大语言模型）的下一代发展，基于 Software Heritage 的源代码档案，并整合了高质量数据源如 GitHub pull requests 和 Kaggle notebooks，使训练集规模扩大到原来的 4 倍。研究团队训练了 3B、7B 和 15B 参数的模型，使用 3.3 到 4.3 万亿 tokens，并在多个 Code LLM 基准上进行评估，结果显示 StarCoder2-3B 在同规模模型中表现优越，甚至超过了 StarCoderBase-15B。StarCoder2-15B 则在数学和代码推理基准上超越了更大模型如 CodeLlama-34B 和 DeepSeekCoder-33B，尤其在低资源语言中表现出色；模型权重以 OpenRAIL 许可公开，并提供训练数据的 SWHIDs 以确保透明性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19173v1",
      "published_date": "2024-02-29 13:53:35 UTC",
      "updated_date": "2024-02-29 13:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:22:43.323989"
    },
    {
      "arxiv_id": "2402.19170v2",
      "title": "Improving Legal Judgement Prediction in Romanian with Long Text Encoders",
      "title_zh": "使用长文本编码器改进罗马尼亚语法律判决预测",
      "authors": [
        "Mihai Masala",
        "Traian Rebedea",
        "Horia Velicu"
      ],
      "abstract": "In recent years,the entire field of Natural Language Processing (NLP) has\nenjoyed amazing novel results achieving almost human-like performance on a\nvariety of tasks. Legal NLP domain has also been part of this process, as it\nhas seen an impressive growth. However, general-purpose models are not readily\napplicable for legal domain. Due to the nature of the domain (e.g. specialized\nvocabulary, long documents) specific models and methods are often needed for\nLegal NLP. In this work we investigate both specialized and general models for\npredicting the final ruling of a legal case, task known as Legal Judgment\nPrediction (LJP). We particularly focus on methods to extend to sequence length\nof Transformer-based models to better understand the long documents present in\nlegal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating\nfrom 2 sources with significantly different sizes and document lengths, show\nthat specialized models and handling long texts are critical for a good\nperformance.",
      "tldr_zh": "这篇论文探讨了如何使用长文本编码器改善罗马尼亚语的 Legal Judgment Prediction (LJP)，以应对法律领域中专业词汇和长文档的挑战。研究者比较了专用模型和通用模型，重点扩展 Transformer-based 模型的序列长度，从而更好地处理法律语料中的长文本。在 4 个罗马尼亚语 LJP 数据集上的广泛实验显示，采用专用模型并优化长文本处理是提升预测性能的关键。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Rejected at LREC-COLING with 4/4/3",
      "pdf_url": "http://arxiv.org/pdf/2402.19170v2",
      "published_date": "2024-02-29 13:52:33 UTC",
      "updated_date": "2024-03-04 20:54:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:22:53.639066"
    },
    {
      "arxiv_id": "2403.00843v2",
      "title": "Large Language Models are Learnable Planners for Long-Term Recommendation",
      "title_zh": "大型语言模型是可学习的长期推荐规划器",
      "authors": [
        "Wentao Shi",
        "Xiangnan He",
        "Yang Zhang",
        "Chongming Gao",
        "Xinyue Li",
        "Jizhi Zhang",
        "Qifan Wang",
        "Fuli Feng"
      ],
      "abstract": "Planning for both immediate and long-term benefits becomes increasingly\nimportant in recommendation. Existing methods apply Reinforcement Learning (RL)\nto learn planning capacity by maximizing cumulative reward for long-term\nrecommendation. However, the scarcity of recommendation data presents\nchallenges such as instability and susceptibility to overfitting when training\nRL models from scratch, resulting in sub-optimal performance. In this light, we\npropose to leverage the remarkable planning capabilities over sparse data of\nLarge Language Models (LLMs) for long-term recommendation. The key to achieving\nthe target lies in formulating a guidance plan following principles of\nenhancing long-term engagement and grounding the plan to effective and\nexecutable actions in a personalized manner. To this end, we propose a Bi-level\nLearnable LLM Planner framework, which consists of a set of LLM instances and\nbreaks down the learning process into macro-learning and micro-learning to\nlearn macro-level guidance and micro-level personalized recommendation\npolicies, respectively. Extensive experiments validate that the framework\nfacilitates the planning ability of LLMs for long-term recommendation. Our code\nand data can be found at https://github.com/jizhi-zhang/BiLLP.",
      "tldr_zh": "该研究指出，现有的强化学习 (RL) 方法在长期推荐中面临数据稀缺导致的不稳定和过拟合问题，因此提出利用大型语言模型 (LLMs) 作为可学习的规划器来优化推荐策略。研究引入 Bi-level Learnable LLM Planner 框架，该框架包含一组 LLM 实例，将学习过程分为宏观学习 (macro-learning) 和微观学习 (micro-learning)，分别生成宏观指导计划和微观个性化推荐行动，以提升长期用户参与度。实验结果验证了该框架显著提高了 LLMs 在长期推荐中的规划能力，并提供了开源代码以供进一步验证。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00843v2",
      "published_date": "2024-02-29 13:49:56 UTC",
      "updated_date": "2024-04-26 07:41:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:23:05.647757"
    },
    {
      "arxiv_id": "2402.19161v2",
      "title": "MemoNav: Working Memory Model for Visual Navigation",
      "title_zh": "MemoNav",
      "authors": [
        "Hongxin Li",
        "Zeyu Wang",
        "Xu Yang",
        "Yuran Yang",
        "Shuqi Mei",
        "Zhaoxiang Zhang"
      ],
      "abstract": "Image-goal navigation is a challenging task that requires an agent to\nnavigate to a goal indicated by an image in unfamiliar environments. Existing\nmethods utilizing diverse scene memories suffer from inefficient exploration\nsince they use all historical observations for decision-making without\nconsidering the goal-relevant fraction. To address this limitation, we present\nMemoNav, a novel memory model for image-goal navigation, which utilizes a\nworking memory-inspired pipeline to improve navigation performance.\nSpecifically, we employ three types of navigation memory. The node features on\na map are stored in the short-term memory (STM), as these features are\ndynamically updated. A forgetting module then retains the informative STM\nfraction to increase efficiency. We also introduce long-term memory (LTM) to\nlearn global scene representations by progressively aggregating STM features.\nSubsequently, a graph attention module encodes the retained STM and the LTM to\ngenerate working memory (WM) which contains the scene features essential for\nefficient navigation. The synergy among these three memory types boosts\nnavigation performance by enabling the agent to learn and leverage\ngoal-relevant scene features within a topological map. Our evaluation on\nmulti-goal tasks demonstrates that MemoNav significantly outperforms previous\nmethods across all difficulty levels in both Gibson and Matterport3D scenes.\nQualitative results further illustrate that MemoNav plans more efficient\nroutes.",
      "tldr_zh": "该论文提出 MemoNav，一种基于工作记忆（Working Memory）模型的框架，用于提升图像目标导航任务的效率，解决现有方法因使用所有历史观察而导致的探索低效问题。具体而言，MemoNav 整合短期记忆（STM）动态存储地图节点特征、遗忘模块保留关键信息，以及长期记忆（LTM）通过聚合 STM 特征学习全局场景表示，然后利用图注意力模块（Graph Attention Module）生成工作记忆（WM），从而帮助代理专注于目标相关场景特征。实验结果显示，在 Gibson 和 Matterport3D 场景的多目标任务中，MemoNav 在所有难度级别上显著优于先前方法，并实现了更高效的路线规划。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2024. Code: https://github.com/ZJULiHongxin/MemoNav",
      "pdf_url": "http://arxiv.org/pdf/2402.19161v2",
      "published_date": "2024-02-29 13:45:13 UTC",
      "updated_date": "2024-03-28 04:07:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:23:19.431793"
    },
    {
      "arxiv_id": "2402.19135v2",
      "title": "Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool",
      "title_zh": "快速思考、缓慢思考、批判思考：设计一个自动化的宣传检测工具",
      "authors": [
        "Liudmila Zavolokina",
        "Kilian Sprenkamp",
        "Zoya Katashinskaya",
        "Daniel Gordon Jones",
        "Gerhard Schwabe"
      ],
      "abstract": "In today's digital age, characterized by rapid news consumption and\nincreasing vulnerability to propaganda, fostering citizens' critical thinking\nis crucial for stable democracies. This paper introduces the design of\nClarifAI, a novel automated propaganda detection tool designed to nudge readers\ntowards more critical news consumption by activating the analytical mode of\nthinking, following Kahneman's dual-system theory of cognition. Using Large\nLanguage Models, ClarifAI detects propaganda in news articles and provides\ncontext-rich explanations, enhancing users' understanding and critical\nthinking. Our contribution is threefold: first, we propose the design of\nClarifAI; second, in an online experiment, we demonstrate that this design\neffectively encourages news readers to engage in more critical reading; and\nthird, we emphasize the value of explanations for fostering critical thinking.\nThe study thus offers both a practical tool and useful design knowledge for\nmitigating propaganda in digital news.",
      "tldr_zh": "本研究针对数字时代快速新闻消费和宣传风险，设计了 ClarifAI，一种基于 Kahneman's dual-system theory 的自动宣传检测工具，使用 Large Language Models 检测新闻文章中的宣传并提供丰富的上下文解释，以激发读者的分析性思维。ClarifAI 的设计旨在鼓励用户进行更批判性的阅读，通过在线实验证明其能有效提升批判性参与。论文的主要贡献包括提出 ClarifAI 框架、验证其在促进批判性思维方面的效果，以及强调解释在对抗宣传中的关键作用，为缓解数字新闻中的宣传问题提供了实用工具和设计指导。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "The paper is accepted for publication in proceedings of the CHI\n  Conference on Human Factors in Computing Systems (2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.19135v2",
      "published_date": "2024-02-29 13:12:31 UTC",
      "updated_date": "2024-08-06 14:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:23:29.221738"
    },
    {
      "arxiv_id": "2402.19116v2",
      "title": "How to Understand \"Support\"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Jiamin Luo",
        "Jianing Zhao",
        "Jingjing Wang",
        "Guodong Zhou"
      ],
      "abstract": "Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the\nfine-grained phrase-region matching, while merely leveraging the coarse-grained\nsentence-image pairs for training. However, existing studies on WPG largely\nignore the implicit phrase-region matching relations, which are crucial for\nevaluating the capability of models in understanding the deep multimodal\nsemantics. To this end, this paper proposes an Implicit-Enhanced Causal\nInference (IECI) approach to address the challenges of modeling the implicit\nrelations and highlighting them beyond the explicit. Specifically, this\napproach leverages both the intervention and counterfactual techniques to\ntackle the above two challenges respectively. Furthermore, a high-quality\nimplicit-enhanced dataset is annotated to evaluate IECI and detailed\nevaluations show the great advantages of IECI over the state-of-the-art\nbaselines. Particularly, we observe an interesting finding that IECI\noutperforms the advanced multimodal LLMs by a large margin on this\nimplicit-enhanced dataset, which may facilitate more research to evaluate the\nmultimodal LLMs in this direction.",
      "tldr_zh": "本研究针对弱监督短语 grounding (WPG) 任务，提出了一种 Implicit-Enhanced Causal Inference (IECI) 方法，以更好地理解隐式短语-区域匹配关系，从而提升模型对多模态语义的深度解读。IECI 通过利用干预和反事实技术，分别解决隐式关系建模和突出这些关系超过显式关系的挑战。研究者构建了一个高质量的隐式增强数据集进行评估，结果显示 IECI 显著优于现有最先进基线，并在该数据集上大幅超越高级多模态 LLMs，这一发现有望推动更多相关研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19116v2",
      "published_date": "2024-02-29 12:49:48 UTC",
      "updated_date": "2024-03-04 08:42:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:23:41.491162"
    },
    {
      "arxiv_id": "2402.19105v2",
      "title": "CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI",
      "title_zh": "CollaFuse：在协作生成式人工智能中应对有限资源和",
      "authors": [
        "Domenique Zipperling",
        "Simeon Allmendinger",
        "Lukas Struppek",
        "Niklas Kühl"
      ],
      "abstract": "In the landscape of generative artificial intelligence, diffusion-based\nmodels present challenges for socio-technical systems in data requirements and\nprivacy. Traditional approaches like federated learning distribute the learning\nprocess but strain individual clients, especially with constrained resources\n(e.g., edge devices). In response to these challenges, we introduce CollaFuse,\na novel framework inspired by split learning. Tailored for efficient and\ncollaborative use of denoising diffusion probabilistic models, CollaFuse\nenables shared server training and inference, alleviating client computational\nburdens. This is achieved by retaining data and computationally inexpensive GPU\nprocesses locally at each client while outsourcing the computationally\nexpensive processes to the shared server. Demonstrated in a healthcare context,\nCollaFuse enhances privacy by highly reducing the need for sensitive\ninformation sharing. These capabilities hold the potential to impact various\napplication areas, such as the design of edge computing solutions, healthcare\nresearch, or autonomous driving. In essence, our work advances distributed\nmachine learning, shaping the future of collaborative GenAI networks.",
      "tldr_zh": "论文提出了CollaFuse框架，一种受split learning启发的创新方法，用于应对生成AI中基于diffusion models的资源限制和隐私挑战。CollaFuse通过将数据和计算开销较小的GPU进程保留在客户端，而将计算密集型进程外包给共享服务器，实现高效的协作训练和推理，从而减轻边缘设备的负担。在医疗场景中，该框架显著减少了敏感信息共享，提升了隐私保护，并展示了在边缘计算、健康研究和自动驾驶等领域的潜在应用，推进了分布式机器学习的发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Thirty-Second European Conference on Information Systems (ECIS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.19105v2",
      "published_date": "2024-02-29 12:36:10 UTC",
      "updated_date": "2024-08-16 11:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:23:54.365890"
    },
    {
      "arxiv_id": "2402.19103v1",
      "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hongbang Yuan",
        "Pengfei Cao",
        "Zhuoran Jin",
        "Yubo Chen",
        "Daojian Zeng",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still\nsuffer from the issue of hallucinations. A significant type of this issue is\nthe false premise hallucination, which we define as the phenomenon when LLMs\ngenerate hallucinated text when confronted with false premise questions. In\nthis paper, we perform a comprehensive analysis of the false premise\nhallucination and elucidate its internal working mechanism: a small subset of\nattention heads (which we designate as false premise heads) disturb the\nknowledge extraction process, leading to the occurrence of false premise\nhallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse\npremise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating\n\\textbf{H}allucinations), a novel and effective method to mitigate false\npremise hallucinations. It constrains the false premise attention heads during\nthe model inference process. Impressively, extensive experiments demonstrate\nthat constraining only approximately $1\\%$ of the attention heads in the model\nyields a notable increase of nearly $20\\%$ of model performance.",
      "tldr_zh": "本研究分析了大型语言模型 (LLMs) 中的假定错误幻觉 (false premise hallucination)，即模型在面对虚假前提问题时生成幻觉文本的现象，并揭示了其机制：少数注意力头 (attention heads) 干扰了知识提取过程。作者提出了一种名为 FAITH 的方法，通过在模型推理过程中约束这些 false premise attention heads 来缓解幻觉问题。实验结果显示，仅约束约 1% 的注意力头，就能使模型性能提高近 20%，为提升 LLMs 的可靠性和准确性提供了有效途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 5 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.19103v1",
      "published_date": "2024-02-29 12:35:45 UTC",
      "updated_date": "2024-02-29 12:35:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:24:05.179897"
    },
    {
      "arxiv_id": "2402.19102v1",
      "title": "FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Gambella",
        "Fabrizio Pittorino",
        "Manuel Roveri"
      ],
      "abstract": "Neural Architecture Search (NAS) paves the way for the automatic definition\nof Neural Network (NN) architectures, attracting increasing research attention\nand offering solutions in various scenarios. This study introduces a novel NAS\nsolution, called Flat Neural Architecture Search (FlatNAS), which explores the\ninterplay between a novel figure of merit based on robustness to weight\nperturbations and single NN optimization with Sharpness-Aware Minimization\n(SAM). FlatNAS is the first work in the literature to systematically explore\nflat regions in the loss landscape of NNs in a NAS procedure, while jointly\noptimizing their performance on in-distribution data, their out-of-distribution\n(OOD) robustness, and constraining the number of parameters in their\narchitecture. Differently from current studies primarily concentrating on OOD\nalgorithms, FlatNAS successfully evaluates the impact of NN architectures on\nOOD robustness, a crucial aspect in real-world applications of machine and deep\nlearning. FlatNAS achieves a good trade-off between performance, OOD\ngeneralization, and the number of parameters, by using only in-distribution\ndata in the NAS exploration. The OOD robustness of the NAS-designed models is\nevaluated by focusing on robustness to input data corruptions, using popular\nbenchmark datasets in the literature.",
      "tldr_zh": "本研究引入了 FlatNAS，一种新型的 Neural Architecture Search (NAS) 方法，通过探索神经网络损失景观的平坦区域来提升 Out-of-Distribution (OOD) 鲁棒性，同时结合 Sharpness-Aware Minimization (SAM) 优化单个神经网络。FlatNAS 首次系统地在 NAS 过程中评估权重扰动下的鲁棒性，并联合优化分布内数据性能、OOD 泛化和参数数量限制，仅使用分布内数据即可实现高效探索。与现有方法不同，该框架突出了神经网络架构对 OOD 鲁棒性的影响，并在流行基准数据集上展示了性能、泛化和参数权衡的良好平衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19102v1",
      "published_date": "2024-02-29 12:33:14 UTC",
      "updated_date": "2024-02-29 12:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:24:19.359706"
    },
    {
      "arxiv_id": "2402.19088v3",
      "title": "Survey in Characterization of Semantic Change",
      "title_zh": "翻译失败",
      "authors": [
        "Jader Martins Camboim de Sá",
        "Marcos Da Silveira",
        "Cédric Pruski"
      ],
      "abstract": "Live languages continuously evolve to integrate the cultural change of human\nsocieties. This evolution manifests through neologisms (new words) or\n\\textbf{semantic changes} of words (new meaning to existing words).\nUnderstanding the meaning of words is vital for interpreting texts coming from\ndifferent cultures (regionalism or slang), domains (e.g., technical terms), or\nperiods. In computer science, these words are relevant to computational\nlinguistics algorithms such as translation, information retrieval, question\nanswering, etc. Semantic changes can potentially impact the quality of the\noutcomes of these algorithms. Therefore, it is important to understand and\ncharacterize these changes formally. The study of this impact is a recent\nproblem that has attracted the attention of the computational linguistics\ncommunity. Several approaches propose methods to detect semantic changes with\ngood precision, but more effort is needed to characterize how the meaning of\nwords changes and to reason about how to reduce the impact of semantic change.\nThis survey provides an understandable overview of existing approaches to the\n\\textit{characterization of semantic changes} and also formally defines three\nclasses of characterizations: if the meaning of a word becomes more general or\nnarrow (change in dimension) if the word is used in a more pejorative or\npositive/ameliorated sense (change in orientation), and if there is a trend to\nuse the word in a, for instance, metaphoric or metonymic context (change in\nrelation). We summarized the main aspects of the selected publications in a\ntable and discussed the needs and trends in the research activities on semantic\nchange characterization.",
      "tldr_zh": "这篇论文综述了语义变化（semantic changes）的表征方法，探讨了语言演变如何通过新词或现有词的新含义影响不同文化、领域或历史时期的文本理解。作者强调，语义变化可能降低计算语言学算法（如翻译、信息检索和问答）的质量，并正式定义了三种表征类别：维度变化（词义变得更一般或更窄）、方向变化（词义更负面或更正面）和关系变化（如比喻或转喻的使用趋势）。论文总结了相关出版物的关键方面，并讨论了未来研究需求，以更好地表征语义变化并减轻其影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19088v3",
      "published_date": "2024-02-29 12:13:50 UTC",
      "updated_date": "2024-07-18 12:28:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:24:30.201872"
    },
    {
      "arxiv_id": "2402.19085v3",
      "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yiju Guo",
        "Ganqu Cui",
        "Lifan Yuan",
        "Ning Ding",
        "Zexu Sun",
        "Bowen Sun",
        "Huimin Chen",
        "Ruobing Xie",
        "Jie Zhou",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nimprovements in multi-objective alignment.",
      "tldr_zh": "本论文探讨了AI模型的多目标对齐（multi-objective alignment）问题，指出现有方法常导致“alignment tax”，即在提升一个目标（如harmlessness）时牺牲其他目标（如helpfulness）的性能。作者提出Controllable Preference Optimization (CPO)方法，通过明确指定不同目标的偏好分数，引导模型生成符合要求的响应。实验结果显示，CPO能使模型在“3H”（helpfulness、honesty、harmlessness）目标上实现灵活对齐，并超越基线方法，减轻alignment tax并提升多目标对齐效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2402.19085v3",
      "published_date": "2024-02-29 12:12:30 UTC",
      "updated_date": "2024-10-11 08:21:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:24:41.919575"
    },
    {
      "arxiv_id": "2402.19078v3",
      "title": "Smooth Tchebycheff Scalarization for Multi-Objective Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Lin",
        "Xiaoyuan Zhang",
        "Zhiyuan Yang",
        "Fei Liu",
        "Zhenkun Wang",
        "Qingfu Zhang"
      ],
      "abstract": "Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent optimal trade-offs among\nthe objectives for a given problem. However, these existing methods could have\nhigh computational complexity or may not have good theoretical properties for\nsolving a general differentiable multi-objective optimization problem. In this\nwork, by leveraging the smooth optimization technique, we propose a lightweight\nand efficient smooth Tchebycheff scalarization approach for gradient-based\nmulti-objective optimization. It has good theoretical properties for finding\nall Pareto solutions with valid trade-off preferences, while enjoying\nsignificantly lower computational complexity compared to other methods.\nExperimental results on various real-world application problems fully\ndemonstrate the effectiveness of our proposed method.",
      "tldr_zh": "多目标优化（Multi-Objective Optimization）问题在现实应用中常见，目标之间往往相互冲突，无法通过单一解实现优化。现有方法可能存在高计算复杂性或理论属性不足的问题。针对此，本文提出了一种基于平滑优化技术（smooth optimization technique）的轻量级平滑 Tchebycheff scalarization 方法，该方法支持梯度-based 优化，能够找到所有 Pareto solutions 并满足有效的权衡偏好，同时显著降低计算复杂度。实验结果在多种真实应用问题上充分证明了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the 41st International Conference on Machine Learning\n  (ICML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.19078v3",
      "published_date": "2024-02-29 12:03:05 UTC",
      "updated_date": "2024-07-23 10:46:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:24:53.944592"
    },
    {
      "arxiv_id": "2402.19072v4",
      "title": "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Wang",
        "Haixu Wu",
        "Jiaxiang Dong",
        "Guo Qin",
        "Haoran Zhang",
        "Yong Liu",
        "Yunzhong Qiu",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "abstract": "Deep models have demonstrated remarkable performance in time series\nforecasting. However, due to the partially-observed nature of real-world\napplications, solely focusing on the target of interest, so-called endogenous\nvariables, is usually insufficient to guarantee accurate forecasting. Notably,\na system is often recorded into multiple variables, where the exogenous\nvariables can provide valuable external information for endogenous variables.\nThus, unlike well-established multivariate or univariate forecasting paradigms\nthat either treat all the variables equally or ignore exogenous information,\nthis paper focuses on a more practical setting: time series forecasting with\nexogenous variables. We propose a novel approach, TimeXer, to ingest external\ninformation to enhance the forecasting of endogenous variables. With deftly\ndesigned embedding layers, TimeXer empowers the canonical Transformer with the\nability to reconcile endogenous and exogenous information, where patch-wise\nself-attention and variate-wise cross-attention are used simultaneously.\nMoreover, global endogenous tokens are learned to effectively bridge the causal\ninformation underlying exogenous series into endogenous temporal patches.\nExperimentally, TimeXer achieves consistent state-of-the-art performance on\ntwelve real-world forecasting benchmarks and exhibits notable generality and\nscalability. Code is available at this repository:\nhttps://github.com/thuml/TimeXer.",
      "tldr_zh": "这篇论文针对时间序列预测问题，提出 TimeXer 方法，以整合外生变量（exogenous variables）来提升内生变量（endogenous variables）的预测准确性。TimeXer 在 Transformer 基础上，通过巧妙设计的嵌入层、patch-wise self-attention 和 variate-wise cross-attention 机制，同时学习 global endogenous tokens 来桥接外生序列中的因果信息与内生时间补丁。实验结果显示，TimeXer 在 12 个真实世界基准上实现了最先进性能，并展示了显著的泛化性和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19072v4",
      "published_date": "2024-02-29 11:54:35 UTC",
      "updated_date": "2024-11-11 03:18:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:25:08.192562"
    },
    {
      "arxiv_id": "2403.00041v2",
      "title": "Global and Local Prompts Cooperation via Optimal Transport for Federated Learning",
      "title_zh": "全球和本地提示通过最优传输合作，用于联邦学习",
      "authors": [
        "Hongxia Li",
        "Wei Huang",
        "Jingya Wang",
        "Ye Shi"
      ],
      "abstract": "Prompt learning in pretrained visual-language models has shown remarkable\nflexibility across various downstream tasks. Leveraging its inherent\nlightweight nature, recent research attempted to integrate the powerful\npretrained models into federated learning frameworks to simultaneously reduce\ncommunication costs and promote local training on insufficient data. Despite\nthese efforts, current federated prompt learning methods lack specialized\ndesigns to systematically address severe data heterogeneities, e.g., data\ndistribution with both label and feature shifts involved. To address this\nchallenge, we present Federated Prompts Cooperation via Optimal Transport\n(FedOTP), which introduces efficient collaborative prompt learning strategies\nto capture diverse category traits on a per-client basis. Specifically, for\neach client, we learn a global prompt to extract consensus knowledge among\nclients, and a local prompt to capture client-specific category\ncharacteristics. Unbalanced Optimal Transport is then employed to align local\nvisual features with these prompts, striking a balance between global consensus\nand local personalization. By relaxing one of the equality constraints, FedOTP\nenables prompts to focus solely on the core regions of image patches. Extensive\nexperiments on datasets with various types of heterogeneities have demonstrated\nthat our FedOTP outperforms the state-of-the-art methods.",
      "tldr_zh": "这篇论文针对联邦学习中数据异质性（如标签和特征偏移）问题，提出了FedOTP（Federated Prompts Cooperation via Optimal Transport）方法，以提升预训练视觉语言模型的提示学习（Prompt Learning）效果。FedOTP为每个客户端学习全局提示来提取共识知识，以及本地提示来捕捉特定类别特征，并通过Unbalanced Optimal Transport对齐本地视觉特征，从而平衡全局共识和本地个性化，同时专注于图像补丁的核心区域。实验结果显示，在多种异质性数据集上，FedOTP优于现有最先进方法，显著提高了模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00041v2",
      "published_date": "2024-02-29 11:43:04 UTC",
      "updated_date": "2024-04-03 15:16:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:25:18.474100"
    },
    {
      "arxiv_id": "2403.00841v1",
      "title": "Offline Fictitious Self-Play for Competitive Games",
      "title_zh": "翻译失败",
      "authors": [
        "Jingxiao Chen",
        "Weiji Xie",
        "Weinan Zhang",
        "Yong yu",
        "Ying Wen"
      ],
      "abstract": "Offline Reinforcement Learning (RL) has received significant interest due to\nits ability to improve policies in previously collected datasets without online\ninteractions. Despite its success in the single-agent setting, offline\nmulti-agent RL remains a challenge, especially in competitive games. Firstly,\nunaware of the game structure, it is impossible to interact with the opponents\nand conduct a major learning paradigm, self-play, for competitive games.\nSecondly, real-world datasets cannot cover all the state and action space in\nthe game, resulting in barriers to identifying Nash equilibrium (NE). To\naddress these issues, this paper introduces Off-FSP, the first practical\nmodel-free offline RL algorithm for competitive games. We start by simulating\ninteractions with various opponents by adjusting the weights of the fixed\ndataset with importance sampling. This technique allows us to learn best\nresponses to different opponents and employ the Offline Self-Play learning\nframework. In this framework, we further implement Fictitious Self-Play (FSP)\nto approximate NE. In partially covered real-world datasets, our methods show\nthe potential to approach NE by incorporating any single-agent offline RL\nmethod. Experimental results in Leduc Hold'em Poker show that our method\nsignificantly improves performances compared with state-of-the-art baselines.",
      "tldr_zh": "这篇论文针对竞争性游戏中的离线强化学习（Offline Reinforcement Learning, RL）挑战，提出了Off-FSP算法，这是第一个实用的模型无关离线RL方法，用于处理无法与对手互动和数据集覆盖不足的问题。该算法通过重要性采样（importance sampling）调整固定数据集权重，模拟对手互动，学习最佳响应，并结合Fictitious Self-Play (FSP)框架来逼近Nash equilibrium (NE)。实验结果显示，在Leduc Hold'em Poker上，Off-FSP显著提高了性能，优于最先进基线。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00841v1",
      "published_date": "2024-02-29 11:36:48 UTC",
      "updated_date": "2024-02-29 11:36:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:25:30.842914"
    },
    {
      "arxiv_id": "2403.04775v1",
      "title": "Superposition with Delayed Unification",
      "title_zh": "延迟统一的叠加",
      "authors": [
        "Ahmed Bhayat",
        "Johannes Schoisswohl",
        "Michael Rawson"
      ],
      "abstract": "Classically, in saturation-based proof systems, unification has been\nconsidered atomic. However, it is also possible to move unification to the\ncalculus level, turning the steps of the unification algorithm into inferences.\nFor calculi that rely on unification procedures returning large or even\ninfinite sets of unifiers, integrating unification into the calculus is an\nattractive method of dovetailing unification and inference. This applies, for\nexample, to AC-superposition and higher-order superposition. We show that\nfirst-order superposition remains complete when moving unification rules to the\ncalculus level. We discuss some of the benefits this has even for standard\nfirst-order superposition and provide an experimental evaluation.",
      "tldr_zh": "这篇论文探讨了在基于饱和的证明系统中，将统一(unification)从原子操作转变为推理步骤的创新方法，旨在处理依赖于返回大量或无限解的统一过程。作者证明了在 first-order superposition 中，这种 delayed unification 方式保持了系统的完整性(completeness)。此外，该方法为标准 first-order superposition 带来了益处，如更有效的推理整合，并通过实验评估证实了其实际优势。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "16 pages, 0 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2403.04775v1",
      "published_date": "2024-02-29 11:35:49 UTC",
      "updated_date": "2024-02-29 11:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:25:42.837597"
    },
    {
      "arxiv_id": "2402.19054v1",
      "title": "RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Xu",
        "Yunlin Tan",
        "Cheng Zhang",
        "Kai Chi",
        "Peng Sun",
        "Wenyuan Yang",
        "Ju Ren",
        "Hongbo Jiang",
        "Yaoxue Zhang"
      ],
      "abstract": "Embedding watermarks into models has been widely used to protect model\nownership in federated learning (FL). However, existing methods are inadequate\nfor protecting the ownership of personalized models acquired by clients in\npersonalized FL (PFL). This is due to the aggregation of the global model in\nPFL, resulting in conflicts over clients' private watermarks. Moreover,\nmalicious clients may tamper with embedded watermarks to facilitate model\nleakage and evade accountability. This paper presents a robust watermark\nembedding scheme, named RobWE, to protect the ownership of personalized models\nin PFL. We first decouple the watermark embedding of personalized models into\ntwo parts: head layer embedding and representation layer embedding. The head\nlayer belongs to clients' private part without participating in model\naggregation, while the representation layer is the shared part for aggregation.\nFor representation layer embedding, we employ a watermark slice embedding\noperation, which avoids watermark embedding conflicts. Furthermore, we design a\nmalicious watermark detection scheme enabling the server to verify the\ncorrectness of watermarks before aggregating local models. We conduct an\nexhaustive experimental evaluation of RobWE. The results demonstrate that RobWE\nsignificantly outperforms the state-of-the-art watermark embedding schemes in\nFL in terms of fidelity, reliability, and robustness.",
      "tldr_zh": "该论文提出RobWE，一种鲁棒的水印嵌入方案，用于保护个性化联邦学习(PFL)中模型所有权的完整性，以解决现有方法在全局模型聚合中导致的水印冲突和恶意篡改问题。RobWE将水印嵌入分为头部层(客户端私有部分)和表示层(共享聚合部分)，并采用水印切片嵌入操作避免冲突，同时设计恶意水印检测方案让服务器在聚合前验证水印正确性。实验结果显示，RobWE在fidelity、reliability和robustness方面显著优于现有联邦学习水印方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19054v1",
      "published_date": "2024-02-29 11:31:50 UTC",
      "updated_date": "2024-02-29 11:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:25:53.869146"
    },
    {
      "arxiv_id": "2402.19041v1",
      "title": "Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors",
      "title_zh": "翻译失败",
      "authors": [
        "P. Hill",
        "N. Anantrasirichai",
        "A. Achim",
        "D. R. Bull"
      ],
      "abstract": "Atmospheric turbulence poses a challenge for the interpretation and visual\nperception of visual imagery due to its distortion effects. Model-based\napproaches have been used to address this, but such methods often suffer from\nartefacts associated with moving content. Conversely, deep learning based\nmethods are dependent on large and diverse datasets that may not effectively\nrepresent any specific content. In this paper, we address these problems with a\nself-supervised learning method that does not require ground truth. The\nproposed method is not dependent on any dataset outside of the single data\nsequence being processed but is also able to improve the quality of any input\nraw sequences or pre-processed sequences. Specifically, our method is based on\nan accelerated Deep Image Prior (DIP), but integrates temporal information\nusing pixel shuffling and a temporal sliding window. This efficiently learns\nspatio-temporal priors leading to a system that effectively mitigates\natmospheric turbulence distortions. The experiments show that our method\nimproves visual quality results qualitatively and quantitatively.",
      "tldr_zh": "本文提出了一种自监督学习方法，用于移除大气湍流对视频序列的扭曲影响，该方法无需 ground truth，仅依赖于单个数据序列。基于加速的 Deep Image Prior (DIP)，它整合了时序信息，通过像素重排和时序滑动窗口来高效学习时空先验，从而有效减轻扭曲。实验结果显示，该方法在定性和定量上均显著提高了视觉质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19041v1",
      "published_date": "2024-02-29 11:06:14 UTC",
      "updated_date": "2024-02-29 11:06:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:26:06.235555"
    },
    {
      "arxiv_id": "2402.19027v2",
      "title": "How to Train your Antivirus: RL-based Hardening through the Problem-Space",
      "title_zh": "翻译失败",
      "authors": [
        "Ilias Tsingenopoulos",
        "Jacopo Cortellazzi",
        "Branislav Bošanský",
        "Simone Aonzo",
        "Davy Preuveneers",
        "Wouter Joosen",
        "Fabio Pierazzi",
        "Lorenzo Cavallaro"
      ],
      "abstract": "ML-based malware detection on dynamic analysis reports is vulnerable to both\nevasion and spurious correlations. In this work, we investigate a specific ML\narchitecture employed in the pipeline of a widely-known commercial antivirus\ncompany, with the goal to harden it against adversarial malware. Adversarial\ntraining, the sole defensive technique that can confer empirical robustness, is\nnot applicable out of the box in this domain, for the principal reason that\ngradient-based perturbations rarely map back to feasible problem-space\nprograms. We introduce a novel Reinforcement Learning approach for constructing\nadversarial examples, a constituent part of adversarially training a model\nagainst evasion. Our approach comes with multiple advantages. It performs\nmodifications that are feasible in the problem-space, and only those; thus it\ncircumvents the inverse mapping problem. It also makes possible to provide\ntheoretical guarantees on the robustness of the model against a particular set\nof adversarial capabilities. Our empirical exploration validates our\ntheoretical insights, where we can consistently reach 0% Attack Success Rate\nafter a few adversarial retraining iterations.",
      "tldr_zh": "这篇论文探讨了基于机器学习（ML）的动态分析报告恶意软件检测系统面临的 evasion（逃避攻击）和 spurious correlations（虚假相关性）问题，目标是通过对抗训练强化一个知名商业防病毒公司的ML架构。研究者引入了一种新型Reinforcement Learning (RL)方法来生成可行的对抗样本，该方法仅在问题空间中进行修改，从而避免了梯度-based扰动的逆映射难题，并提供了模型对特定对抗能力的理论保证。实验结果显示，经过几次对抗重新训练后，Attack Success Rate（攻击成功率）达到0%，验证了该方法的有效性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages,4 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.19027v2",
      "published_date": "2024-02-29 10:38:56 UTC",
      "updated_date": "2024-09-05 17:07:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:26:18.641946"
    },
    {
      "arxiv_id": "2402.19025v1",
      "title": "Combination of Weak Learners eXplanations to Improve Random Forest eXplicability Robustness",
      "title_zh": "翻译失败",
      "authors": [
        "Riccardo Pala",
        "Esteban García-Cuesta"
      ],
      "abstract": "The notion of robustness in XAI refers to the observed variations in the\nexplanation of the prediction of a learned model with respect to changes in the\ninput leading to that prediction. Intuitively, if the input being explained is\nmodified slightly subtly enough so as to not change the prediction of the model\ntoo much, then we would expect that the explanation provided for that new input\ndoes not change much either. We argue that a combination through discriminative\naveraging of ensembles weak learners explanations can improve the robustness of\nexplanations in ensemble methods.This approach has been implemented and tested\nwith post-hoc SHAP method and Random Forest ensemble with successful results.\nThe improvements obtained have been measured quantitatively and some insights\ninto the explicability robustness in ensemble methods are presented.",
      "tldr_zh": "这篇论文探讨了XAI（可解释人工智能）中解释鲁棒性的概念，即模型预测解释对输入微小变化的稳定性，以确保解释不会因细微调整而显著改变。作者提出一种方法，通过discriminative averaging对Random Forest集成中的弱学习器解释进行组合，提高解释的鲁棒性。该方法使用post-hoc SHAP技术进行实现，并在实验中取得了成功结果，包括定量测量和对集成方法解释鲁棒性的新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T01",
        "I.2.0; I.2.1"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.19025v1",
      "published_date": "2024-02-29 10:37:40 UTC",
      "updated_date": "2024-02-29 10:37:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:26:29.857728"
    },
    {
      "arxiv_id": "2402.19009v2",
      "title": "Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Guangyi Liu",
        "Yu Wang",
        "Zeyu Feng",
        "Qiyu Wu",
        "Liping Tang",
        "Yuan Gao",
        "Zhen Li",
        "Shuguang Cui",
        "Julian McAuley",
        "Zichao Yang",
        "Eric P. Xing",
        "Zhiting Hu"
      ],
      "abstract": "The vast applications of deep generative models are anchored in three core\ncapabilities -- generating new instances, reconstructing inputs, and learning\ncompact representations -- across various data types, such as discrete\ntext/protein sequences and continuous images. Existing model families, like\nvariational autoencoders (VAEs), generative adversarial networks (GANs),\nautoregressive models, and (latent) diffusion models, generally excel in\nspecific capabilities and data types but fall short in others. We introduce\nGeneralized Encoding-Decoding Diffusion Probabilistic Models (EDDPMs) which\nintegrate the core capabilities for broad applicability and enhanced\nperformance. EDDPMs generalize the Gaussian noising-denoising in standard\ndiffusion by introducing parameterized encoding-decoding. Crucially, EDDPMs are\ncompatible with the well-established diffusion model objective and training\nrecipes, allowing effective learning of the encoder-decoder parameters jointly\nwith diffusion. By choosing appropriate encoder/decoder (e.g., large language\nmodels), EDDPMs naturally apply to different data types. Extensive experiments\non text, proteins, and images demonstrate the flexibility to handle diverse\ndata and tasks and the strong improvement over various existing models.",
      "tldr_zh": "本论文提出 Generalized Encoding-Decoding Diffusion Probabilistic Models (EDDPMs)，一种泛化扩散模型，通过参数化的编码-解码过程整合生成新实例、重构输入和学习紧凑表示的核心能力，解决现有模型如 VAEs、GANs 和 autoregressive 模型在不同数据类型上的局限性。\nEDDPMs 兼容标准的扩散模型目标和训练方法，允许联合学习编码器-解码器参数，并通过选择适当的编码器（如大型语言模型）适应离散文本、蛋白质序列和连续图像等数据。\n实验在文本、蛋白质和图像任务上证明，EDDPMs 比现有模型表现出显著性能提升，展示了其灵活性和广泛适用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024 camera-ready. Code is available at\n  https://github.com/guangyliu/EDDPM",
      "pdf_url": "http://arxiv.org/pdf/2402.19009v2",
      "published_date": "2024-02-29 10:08:57 UTC",
      "updated_date": "2024-06-05 07:28:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:26:43.512480"
    },
    {
      "arxiv_id": "2402.19002v1",
      "title": "GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction",
      "title_zh": "GoalNet：面向目标区域的行人轨迹预测",
      "authors": [
        "Ching-Lin Lee",
        "Zhi-Xuan Wang",
        "Kuan-Ting Lai",
        "Amar Fadillah"
      ],
      "abstract": "Predicting the future trajectories of pedestrians on the road is an important\ntask for autonomous driving. The pedestrian trajectory prediction is affected\nby scene paths, pedestrian's intentions and decision-making, which is a\nmulti-modal problem. Most recent studies use past trajectories to predict a\nvariety of potential future trajectory distributions, which do not account for\nthe scene context and pedestrian targets. Instead of predicting the future\ntrajectory directly, we propose to use scene context and observed trajectory to\npredict the goal points first, and then reuse the goal points to predict the\nfuture trajectories. By leveraging the information from scene context and\nobserved trajectory, the uncertainty can be limited to a few target areas,\nwhich represent the \"goals\" of the pedestrians. In this paper, we propose\nGoalNet, a new trajectory prediction neural network based on the goal areas of\na pedestrian. Our network can predict both pedestrian's trajectories and\nbounding boxes. The overall model is efficient and modular, and its outputs can\nbe changed according to the usage scenario. Experimental results show that\nGoalNet significantly improves the previous state-of-the-art performance by\n48.7% on the JAAD and 40.8% on the PIE dataset.",
      "tldr_zh": "这篇论文针对自动驾驶中的行人轨迹预测问题，提出了一种基于目标区域的方法，以解决现有模型忽略场景上下文和行人意图的多模态挑战。GoalNet 模型首先利用场景 context 和观察轨迹预测行人的目标点（goal points），然后基于这些点生成未来轨迹和 bounding boxes，确保预测更准确且模块化。实验结果显示，GoalNet 在 JAAD 数据集上比现有最佳方法提高了 48.7%，在 PIE 数据集上提高了 40.8%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.19002v1",
      "published_date": "2024-02-29 09:53:19 UTC",
      "updated_date": "2024-02-29 09:53:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:26:55.241266"
    },
    {
      "arxiv_id": "2402.18995v1",
      "title": "Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Huang",
        "Sikun Yang",
        "Heinz Koeppl"
      ],
      "abstract": "Modeling count-valued time series has been receiving increasing attention\nsince count time series naturally arise in physical and social domains. Poisson\ngamma dynamical systems (PGDSs) are newly-developed methods, which can well\ncapture the expressive latent transition structure and bursty dynamics behind\ncount sequences. In particular, PGDSs demonstrate superior performance in terms\nof data imputation and prediction, compared with canonical linear dynamical\nsystem (LDS) based methods. Despite these advantages, PGDS cannot capture the\nheterogeneous overdispersed behaviours of the underlying dynamic processes. To\nmitigate this defect, we propose a negative-binomial-randomized gamma Markov\nprocess, which not only significantly improves the predictive performance of\nthe proposed dynamical system, but also facilitates the fast convergence of the\ninference algorithm. Moreover, we develop methods to estimate both\nfactor-structured and graph-structured transition dynamics, which enable us to\ninfer more explainable latent structure, compared with PGDSs. Finally, we\ndemonstrate the explainable latent structure learned by the proposed method,\nand show its superior performance in imputing missing data and forecasting\nfuture observations, compared with the related models.",
      "tldr_zh": "该论文针对计数值时间序列的建模问题，提出了一种Negative-Binomial Randomized Gamma Markov Process，以解决Poisson Gamma Dynamical Systems (PGDSs)无法捕捉异质过分散行为的缺陷。该方法显著提升了预测性能，并加速了推理算法的收敛，同时开发了估计因子结构和图结构过渡动态的技术，使潜在结构更具解释性。实验结果显示，该模型在数据插值和未来观察预测方面，优于相关模型，并展示了其学习的可解释潜在结构。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18995v1",
      "published_date": "2024-02-29 09:46:47 UTC",
      "updated_date": "2024-02-29 09:46:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:27:07.372734"
    },
    {
      "arxiv_id": "2403.00039v1",
      "title": "FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and Scientific Use",
      "title_zh": "FhGenie：一个自定义的、保护保密性的聊天AI，用于企业和科学用途",
      "authors": [
        "Ingo Weber",
        "Hendrik Linka",
        "Daniel Mertens",
        "Tamara Muryshkin",
        "Heinrich Opgenoorth",
        "Stefan Langer"
      ],
      "abstract": "Since OpenAI's release of ChatGPT, generative AI has received significant\nattention across various domains. These AI-based chat systems have the\npotential to enhance the productivity of knowledge workers in diverse tasks.\nHowever, the use of free public services poses a risk of data leakage, as\nservice providers may exploit user input for additional training and\noptimization without clear boundaries. Even subscription-based alternatives\nsometimes lack transparency in handling user data. To address these concerns\nand enable Fraunhofer staff to leverage this technology while ensuring\nconfidentiality, we have designed and developed a customized chat AI called\nFhGenie (genie being a reference to a helpful spirit). Within few days of its\nrelease, thousands of Fraunhofer employees started using this service. As\npioneers in implementing such a system, many other organizations have followed\nsuit. Our solution builds upon commercial large language models (LLMs), which\nwe have carefully integrated into our system to meet our specific requirements\nand compliance constraints, including confidentiality and GDPR. In this paper,\nwe share detailed insights into the architectural considerations, design,\nimplementation, and subsequent updates of FhGenie. Additionally, we discuss\nchallenges, observations, and the core lessons learned from its productive\nusage.",
      "tldr_zh": "该论文讨论了生成式 AI（如 ChatGPT）在企业和科学领域的应用潜力，但强调了使用公共服务可能导致的数据泄露风险，因为提供者可能未经明确界限使用用户输入进行训练。为解决这一问题，研究团队开发了 FhGenie，一种自定义的、保护机密性的聊天 AI 系统，基于商业大型语言模型 (LLMs)，并确保符合 GDPR 等合规要求。FhGenie 迅速在 Fraunhofer 员工中普及，并启发了其他组织。论文详细分享了其架构设计、实施过程、后续更新，以及从实际使用中获得的挑战、观察和关键经验教训。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00039v1",
      "published_date": "2024-02-29 09:43:50 UTC",
      "updated_date": "2024-02-29 09:43:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:27:20.132124"
    },
    {
      "arxiv_id": "2402.18985v3",
      "title": "Blume-Capel model analysis with microcanonical population annealing method",
      "title_zh": "翻译失败",
      "authors": [
        "Vyacheslav Mozolenko",
        "Lev Shchur"
      ],
      "abstract": "We present a modification of the Rose-Machta algorithm (Phys. Rev. E 100\n(2019) 063304) and estimate the density of states for a two-dimensional\nBlume-Capel model, simulating $10^5$ replicas in parallel for each set of\nparameters. We perform a finite-size analysis of the specific heat and Binder\ncumulant, determine the critical temperature along the critical line, and\nevaluate the critical exponents. The results obtained are in good agreement\nwith those obtained previously using various methods -- Markov Chain Monte\nCarlo simulation, Wang-Landau simulation, transfer matrix, and series\nexpansion. The simulation results clearly illustrate the typical behavior of\nspecific heat along the critical lines and through the tricritical point.",
      "tldr_zh": "本研究修改了 Rose-Machta 算法，采用 microcanonical population annealing 方法，对二维 Blume-Capel model 进行了密度 of states 估计，并通过模拟 10^5 个副本进行有限-size analysis。研究计算了 specific heat 和 Binder cumulant，准确确定了 critical temperature 和 critical exponents，结果与 Markov Chain Monte Carlo simulation、Wang-Landau simulation、transfer matrix 及 series expansion 等方法一致。该方法清晰展示了 specific heat 在 critical lines 上以及通过 tricritical point 的典型行为。",
      "categories": [
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "primary_category": "cond-mat.stat-mech",
      "comment": "9 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18985v3",
      "published_date": "2024-02-29 09:39:39 UTC",
      "updated_date": "2024-04-17 20:06:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:27:33.165455"
    },
    {
      "arxiv_id": "2403.00840v1",
      "title": "EyeGPT: Ophthalmic Assistant with Large Language Models",
      "title_zh": "EyeGPT：基于大型语言模型的眼科助手",
      "authors": [
        "Xiaolan Chen",
        "Ziwei Zhao",
        "Weiyi Zhang",
        "Pusheng Xu",
        "Le Gao",
        "Mingpu Xu",
        "Yue Wu",
        "Yinwen Li",
        "Danli Shi",
        "Mingguang He"
      ],
      "abstract": "Artificial intelligence (AI) has gained significant attention in healthcare\nconsultation due to its potential to improve clinical workflow and enhance\nmedical communication. However, owing to the complex nature of medical\ninformation, large language models (LLM) trained with general world knowledge\nmight not possess the capability to tackle medical-related tasks at an expert\nlevel. Here, we introduce EyeGPT, a specialized LLM designed specifically for\nophthalmology, using three optimization strategies including role-playing,\nfinetuning, and retrieval-augmented generation. In particular, we proposed a\ncomprehensive evaluation framework that encompasses a diverse dataset, covering\nvarious subspecialties of ophthalmology, different users, and diverse inquiry\nintents. Moreover, we considered multiple evaluation metrics, including\naccuracy, understandability, trustworthiness, empathy, and the proportion of\nhallucinations. By assessing the performance of different EyeGPT variants, we\nidentify the most effective one, which exhibits comparable levels of\nunderstandability, trustworthiness, and empathy to human ophthalmologists (all\nPs>0.05). Overall, ur study provides valuable insights for future research,\nfacilitating comprehensive comparisons and evaluations of different strategies\nfor developing specialized LLMs in ophthalmology. The potential benefits\ninclude enhancing the patient experience in eye care and optimizing\nophthalmologists' services.",
      "tldr_zh": "本研究引入了EyeGPT，一种专门针对眼科的Large Language Models (LLM)，通过role-playing、finetuning和retrieval-augmented generation三种优化策略来提升其处理眼科任务的专业能力。研究提出一个全面评估框架，包括覆盖眼科子专业的多样化数据集、不同用户和查询意图，并采用准确性、易懂性、可信度、同理心和幻觉比例等多重指标进行评估。结果显示，最优的EyeGPT变体在易懂性、可信度和同理心方面与人类眼科医生相当（P>0.05），展现出可比拟的表现。总体而言，此研究为开发眼科专用LLM提供宝贵见解，有助于提升患者眼部护理体验并优化医生服务。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "47 pages, 4 figures, 1 table, 2 supplementary figures and 9\n  supplementary tables",
      "pdf_url": "http://arxiv.org/pdf/2403.00840v1",
      "published_date": "2024-02-29 09:35:41 UTC",
      "updated_date": "2024-02-29 09:35:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:27:45.174786"
    },
    {
      "arxiv_id": "2402.18975v2",
      "title": "Theoretically Achieving Continuous Representation of Oriented Bounding Boxes",
      "title_zh": "理论上实现定向边界框的连续表示",
      "authors": [
        "Zi-Kai Xiao",
        "Guo-Ye Yang",
        "Xue Yang",
        "Tai-Jiang Mu",
        "Junchi Yan",
        "Shi-min Hu"
      ],
      "abstract": "Considerable efforts have been devoted to Oriented Object Detection (OOD).\nHowever, one lasting issue regarding the discontinuity in Oriented Bounding Box\n(OBB) representation remains unresolved, which is an inherent bottleneck for\nextant OOD methods. This paper endeavors to completely solve this issue in a\ntheoretically guaranteed manner and puts an end to the ad-hoc efforts in this\ndirection. Prior studies typically can only address one of the two cases of\ndiscontinuity: rotation and aspect ratio, and often inadvertently introduce\ndecoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding\nAmbiguity (DA) as discussed in literature. Specifically, we propose a novel\nrepresentation method called Continuous OBB (COBB), which can be readily\nintegrated into existing detectors e.g. Faster-RCNN as a plugin. It can\ntheoretically ensure continuity in bounding box regression which to our best\nknowledge, has not been achieved in literature for rectangle-based object\nrepresentation. For fairness and transparency of experiments, we have developed\na modularized benchmark based on the open-source deep learning framework\nJittor's detection toolbox JDet for OOD evaluation. On the popular DOTA\ndataset, by integrating Faster-RCNN as the same baseline model, our new method\noutperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement\n1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.",
      "tldr_zh": "本文针对定向物体检测（Oriented Object Detection, OOD）中Oriented Bounding Boxes (OBB)表示的不连续性问题（如旋转和长宽比引起的），提出了一种新型Continuous OBB (COBB)表示方法，能够理论上确保边界框回归的连续性，并避免先前方法引入的Decoding Incompleteness (DI)和Decoding Ambiguity (DA)。COBB可以轻松集成到现有检测器如Faster-RCNN中，作为一个插件，提升了系统的鲁棒性。在DOTA数据集的实验中，使用Faster-RCNN作为基线，COBB相比Gliding Vertex方法提高了1.13% mAP50（相对提升1.54%）和2.46% mAP75（相对提升5.91%），展示了其显著的性能优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 12 tables, 8 figures. Accepted by CVPR'24. Code:\n  https://github.com/514flowey/JDet-COBB",
      "pdf_url": "http://arxiv.org/pdf/2402.18975v2",
      "published_date": "2024-02-29 09:27:40 UTC",
      "updated_date": "2024-04-16 09:49:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:27:57.699783"
    },
    {
      "arxiv_id": "2402.18960v1",
      "title": "Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Jennie Karlsson",
        "Marisa Wodrich",
        "Niels Christian Overgaard",
        "Freja Sahlin",
        "Kristina Lång",
        "Anders Heyden",
        "Ida Arvidsson"
      ],
      "abstract": "Deep learning has shown to have great potential in medical applications. In\ncritical domains as such, it is of high interest to have trustworthy algorithms\nwhich are able to tell when reliable assessments cannot be guaranteed.\nDetecting out-of-distribution (OOD) samples is a crucial step towards building\na safe classifier. Following a previous study, showing that it is possible to\nclassify breast cancer in point-of-care ultrasound images, this study\ninvestigates OOD detection using three different methods: softmax, energy score\nand deep ensembles. All methods are tested on three different OOD data sets.\nThe results show that the energy score method outperforms the softmax method,\nperforming well on two of the data sets. The ensemble method is the most\nrobust, performing the best at detecting OOD samples for all three OOD data\nsets.",
      "tldr_zh": "本研究旨在提升乳腺癌分类在点-of-care 超声图像中的可靠性，通过检测 Out-of-Distribution (OOD) 样本来构建可信赖的深度学习算法。研究者评估了三种 OOD 检测方法：softmax、energy score 和 deep ensembles，并在三个不同 OOD 数据集上进行测试。结果显示，energy score 方法优于 softmax 方法，在两个数据集上表现良好，而 deep ensembles 方法最稳健，在所有三个 OOD 数据集上均实现了最佳的 OOD 样本检测性能。这为医疗应用中的安全分类器提供了重要见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18960v1",
      "published_date": "2024-02-29 08:59:51 UTC",
      "updated_date": "2024-02-29 08:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:28:07.686043"
    },
    {
      "arxiv_id": "2402.18945v4",
      "title": "SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via Syntactic Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Pengzhou Cheng",
        "Wei Du",
        "Zongru Wu",
        "Fengwei Zhang",
        "Libo Chen",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "abstract": "Although pre-training achieves remarkable performance, it suffers from\ntask-agnostic backdoor attacks due to vulnerabilities in data and training\nmechanisms. These attacks can transfer backdoors to various downstream tasks.\nIn this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning\nfilter that mitigates such risks. To overcome the limitations of manual target\nsetting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and\nuniversal task-agnostic backdoor attack via syntactic transfer, further\nexposing vulnerabilities in pre-trained language models (PLMs). Specifically,\n$\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training\nspace through corpus poisoning, while preserving the PLM's pre-training\ncapabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets\nbased on contrastive learning, creating a uniform distribution in the\npre-training space. To identify syntactic differences, we also introduce an\nawareness module to minimize interference between backdoors. Experiments show\nthat $\\mathtt{SynGhost}$ poses significant threats and can transfer to various\ndownstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on\nperplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at\nhttps://github.com/Zhou-CyberSecurity-AI/SynGhost.",
      "tldr_zh": "这篇论文提出了SynGhost，一种隐形且通用的任务无关后门攻击方法，通过syntactic transfer在预训练语言模型(PLMs)中注入后门，暴露了预训练机制的漏洞。具体而言，SynGhost在预训练空间通过语料毒化注入多个语法后门、基于对比学习自适应选择最佳目标，并引入意识模块最小化后门间的干扰，同时保持PLMs的预训练能力。实验结果显示，SynGhost能够有效转移到各种下游任务，并抵抗基于perplexity、fine-pruning和maxEntropy的防御机制，进一步强调了模型安全的潜在风险。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2402.18945v4",
      "published_date": "2024-02-29 08:20:49 UTC",
      "updated_date": "2025-03-03 06:34:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:28:20.696711"
    },
    {
      "arxiv_id": "2402.18944v1",
      "title": "SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)",
      "title_zh": "翻译失败",
      "authors": [
        "Shivani Kumar",
        "Md Shad Akhtar",
        "Erik Cambria",
        "Tanmoy Chakraborty"
      ],
      "abstract": "We present SemEval-2024 Task 10, a shared task centred on identifying\nemotions and finding the rationale behind their flips within monolingual\nEnglish and Hindi-English code-mixed dialogues. This task comprises three\ndistinct subtasks - emotion recognition in conversation for code-mixed\ndialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip\nreasoning for English dialogues. Participating systems were tasked to\nautomatically execute one or more of these subtasks. The datasets for these\ntasks comprise manually annotated conversations focusing on emotions and\ntriggers for emotion shifts (The task data is available at\nhttps://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84\nparticipants engaged in this task, with the most adept systems attaining\nF1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper\nsummarises the results and findings from 24 teams alongside their system\ndescriptions.",
      "tldr_zh": "SemEval 2024 Task 10 (EDiReF) 是一个共享任务，专注于在单语英语和印地-英语代码混合对话中识别情绪并推理其翻转原因。任务包括三个子任务：代码混合对话的情绪识别、代码混合对话的情绪翻转推理，以及英语对话的情绪翻转推理，使用手动标注的数据集来支持分析。共有 84 名参与者参与，其中表现最佳的系统在三个子任务上分别达到了 F1-scores of 0.70、0.79 和 0.76，该论文总结了 24 个团队的系统描述和关键发现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 3 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.18944v1",
      "published_date": "2024-02-29 08:20:06 UTC",
      "updated_date": "2024-02-29 08:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:28:32.847138"
    },
    {
      "arxiv_id": "2403.06994v1",
      "title": "Physics Sensor Based Deep Learning Fall Detection System",
      "title_zh": "基于物理传感器的深度学习跌倒检测系统",
      "authors": [
        "Zeyuan Qu",
        "Tiange Huang",
        "Yuxin Ji",
        "Yongjun Li"
      ],
      "abstract": "Fall detection based on embedded sensor is a practical and popular research\ndirection in recent years. In terms of a specific application: fall detection\nmethods based upon physics sensors such as [gyroscope and accelerator] have\nbeen exploited using traditional hand crafted features and feed them in machine\nlearning models like Markov chain or just threshold based classification\nmethods. In this paper, we build a complete system named TSFallDetect including\ndata receiving device based on embedded sensor, mobile deep-learning model\ndeploying platform, and a simple server, which will be used to gather models\nand data for future expansion. On the other hand, we exploit the sequential\ndeep-learning methods to address this falling motion prediction problem based\non data collected by inertial and film pressure sensors. We make a empirical\nstudy based on existing datasets and our datasets collected from our system\nseparately, which shows that the deep-learning model has more potential\nadvantage than other traditional methods, and we proposed a new deep-learning\nmodel based on the time series data to predict the fall, and it may be superior\nto other sequential models in this particular field.",
      "tldr_zh": "本论文提出了一种基于物理传感器（如陀螺仪和加速度计）的深度学习跌倒检测系统TSFallDetect，包括嵌入式传感器数据接收设备、移动深度学习模型部署平台和简单服务器，以支持未来扩展。研究采用序列深度学习方法处理由惯性传感器和薄膜压力传感器收集的时间序列数据，并与传统方法（如Markov chain或阈值分类）进行对比。实验结果显示，深度学习模型在现有数据集和自有数据集上表现出更大优势，该论文还设计了一种新深度学习模型，可能在跌倒预测领域优于其他序列模型。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06994v1",
      "published_date": "2024-02-29 07:50:06 UTC",
      "updated_date": "2024-02-29 07:50:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:28:44.128578"
    },
    {
      "arxiv_id": "2402.18929v2",
      "title": "Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Hongjun Wang",
        "Jiyuan Chen",
        "Yinqiang Zheng",
        "Tieyong Zeng"
      ],
      "abstract": "Deep learning has led to a dramatic leap on Single Image Super-Resolution\n(SISR) performances in recent years. %Despite the substantial advancement%\nWhile most existing work assumes a simple and fixed degradation model (e.g.,\nbicubic downsampling), the research of Blind SR seeks to improve model\ngeneralization ability with unknown degradation. Recently, Kong et al pioneer\nthe investigation of a more suitable training strategy for Blind SR using\nDropout. Although such method indeed brings substantial generalization\nimprovements via mitigating overfitting, we argue that Dropout simultaneously\nintroduces undesirable side-effect that compromises model's capacity to\nfaithfully reconstruct fine details. We show both the theoretical and\nexperimental analyses in our paper, and furthermore, we present another easy\nyet effective training strategy that enhances the generalization ability of the\nmodel by simply modulating its first and second-order features statistics.\nExperimental results have shown that our method could serve as a model-agnostic\nregularization and outperforms Dropout on seven benchmark datasets including\nboth synthetic and real-world scenarios.",
      "tldr_zh": "本论文探讨了深度学习在单图像超分辨率 (SISR) 中的进展，并针对 Blind SR（未知退化情况下的超分辨率）提出了一个新的训练策略，以提升模型的泛化能力。作者指出，现有方法如 Kong et al. 采用的 Dropout 虽然能缓解过拟合问题，但会削弱模型重建细细节的能力，并通过理论和实验分析证明了这一负面影响。该策略通过简单调节模型的第一阶和第二阶特征统计，实现模型无关的正则化效果，并在七个基准数据集（包括合成和真实场景）上表现出色，性能优于 Dropout。总的来说，这一方法为 Blind SR 的泛化提供了更可靠的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18929v2",
      "published_date": "2024-02-29 07:44:31 UTC",
      "updated_date": "2024-03-01 05:48:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:28:55.993276"
    },
    {
      "arxiv_id": "2402.18920v5",
      "title": "Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation",
      "title_zh": "翻译失败",
      "authors": [
        "Dongliang Cao",
        "Marvin Eisenberger",
        "Nafie El Amrani",
        "Daniel Cremers",
        "Florian Bernard"
      ],
      "abstract": "Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.",
      "tldr_zh": "本论文提出一个统一框架，将 3D shape matching 和 interpolation 整合起来，避免了传统方法分开应用导致的性能次优问题。框架结合 deep functional map 和表面变形模型，在 spectral 和 spatial 领域映射形状，从而提升点-wise 对应关系的准确性和平滑度，同时通过引入 spectral maps 去除计算昂贵的 geodesic distance constraints，仅适用于近等距变形。论文还引入了 novel test-time adaptation 方案，以处理姿势主导和形状主导的变形。在多个挑战数据集上的实验表明，该方法在形状匹配和插值任务上超过了现有最先进方法，甚至优于监督方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18920v5",
      "published_date": "2024-02-29 07:26:23 UTC",
      "updated_date": "2024-03-27 07:16:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:29:09.537950"
    },
    {
      "arxiv_id": "2402.18913v1",
      "title": "AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging",
      "title_zh": "AdaMergeX：通过自适应适配器合并",
      "authors": [
        "Yiran Zhao",
        "Wenxuan Zhang",
        "Huiming Wang",
        "Kenji Kawaguchi",
        "Lidong Bing"
      ],
      "abstract": "As an effective alternative to the direct fine-tuning on target tasks in\nspecific languages, cross-lingual transfer addresses the challenges of limited\ntraining data by decoupling ''task ability'' and ''language ability'' by\nfine-tuning on the target task in the source language and another selected task\nin the target language, respectively. However, they fail to fully separate the\ntask ability from the source language or the language ability from the chosen\ntask. In this paper, we acknowledge the mutual reliance between task ability\nand language ability and direct our attention toward the gap between the target\nlanguage and the source language on tasks. As the gap removes the impact of\ntasks, we assume that it remains consistent across tasks. Based on this\nassumption, we propose a new cross-lingual transfer method called\n$\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a\nreference task, we can determine that the divergence of adapters fine-tuned on\nthe reference task in both languages follows the same distribution as the\ndivergence of adapters fine-tuned on the target task in both languages. Hence,\nwe can obtain target adapters by combining the other three adapters.\nFurthermore, we propose a structure-adaptive adapter merging method. Our\nempirical results demonstrate that our approach yields new and effective\ncross-lingual transfer, outperforming existing methods across all settings.",
      "tldr_zh": "论文提出 AdaMergeX，一种基于自适应适配器合并(adaptive adapter merging)的创新方法，用于大型语言模型(Large Language Models)中的跨语言转移(cross-lingual transfer)，以解决现有方法无法完全分离“任务能力”和“语言能力”的问题。该方法假设目标语言与源语言在任务上的差距在不同任务间保持一致，通过引入参考任务来合并适配器，从而生成适用于目标任务的适配器。此外，论文还引入结构自适应适配器合并(structure-adaptive adapter merging)技术，实验结果显示 AdaMergeX 在所有设置下优于现有方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18913v1",
      "published_date": "2024-02-29 07:11:24 UTC",
      "updated_date": "2024-02-29 07:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:29:20.917897"
    },
    {
      "arxiv_id": "2402.18910v1",
      "title": "DIGIC: Domain Generalizable Imitation Learning by Causal Discovery",
      "title_zh": "DIGIC",
      "authors": [
        "Yang Chen",
        "Yitao Liang",
        "Zhouchen Lin"
      ],
      "abstract": "Causality has been combined with machine learning to produce robust\nrepresentations for domain generalization. Most existing methods of this type\nrequire massive data from multiple domains to identify causal features by\ncross-domain variations, which can be expensive or even infeasible and may lead\nto misidentification in some cases. In this work, we make a different attempt\nby leveraging the demonstration data distribution to discover the causal\nfeatures for a domain generalizable policy. We design a novel framework, called\nDIGIC, to identify the causal features by finding the direct cause of the\nexpert action from the demonstration data distribution via causal discovery.\nOur framework can achieve domain generalizable imitation learning with only\nsingle-domain data and serve as a complement for cross-domain variation-based\nmethods under non-structural assumptions on the underlying causal models. Our\nempirical study in various control tasks shows that the proposed framework\nevidently improves the domain generalization performance and has comparable\nperformance to the expert in the original domain simultaneously.",
      "tldr_zh": "本文提出DIGIC框架，通过因果发现(causal discovery)从演示数据分布中识别专家动作的直接原因，实现领域泛化的模仿学习(imitation learning)。与传统方法不同，DIGIC仅需单领域数据即可识别因果特征，避免了多领域数据采集的复杂性和潜在误识别问题。实验结果显示，该框架在各种控制任务中显著提升了领域泛化性能，同时在原领域表现与专家相当。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18910v1",
      "published_date": "2024-02-29 07:09:01 UTC",
      "updated_date": "2024-02-29 07:09:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:29:32.258840"
    },
    {
      "arxiv_id": "2402.18909v2",
      "title": "AKEW: Assessing Knowledge Editing in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaobao Wu",
        "Liangming Pan",
        "William Yang Wang",
        "Anh Tuan Luu"
      ],
      "abstract": "Knowledge editing injects knowledge updates into language models to keep them\ncorrect and up-to-date. However, its current evaluations deviate significantly\nfrom practice: their knowledge updates solely consist of structured facts\nderived from meticulously crafted datasets, instead of practical sources --\nunstructured texts like news articles, and they often overlook practical\nreal-world knowledge updates. To address these issues, in this paper we propose\nAKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for\nknowledge editing. AKEW fully covers three editing settings of knowledge\nupdates: structured facts, unstructured texts as facts, and extracted triplets.\nIt further introduces new datasets featuring both counterfactual and real-world\nknowledge updates. Through extensive experiments, we demonstrate the\nconsiderable gap between state-of-the-art knowledge-editing methods and\npractical scenarios. Our analyses further highlight key insights to motivate\nfuture research for practical knowledge editing.",
      "tldr_zh": "该论文指出，现有知识编辑（knowledge editing）方法评估仅限于结构化事实（structured facts）且脱离实际场景，如忽略非结构化文本（unstructured texts）和真实世界更新。为解决此问题，研究提出AKEW（Assessing Knowledge Editing in the Wild），一个实用基准，涵盖三种知识更新设置：结构化事实、非结构化文本作为事实，以及提取的三元组（extracted triplets），并引入包含反事实和真实世界更新的新数据集。通过广泛实验，AKEW揭示了最先进方法与实际场景的显著差距，并提供关键洞见以指导未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2402.18909v2",
      "published_date": "2024-02-29 07:08:34 UTC",
      "updated_date": "2024-10-10 05:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:29:44.324911"
    },
    {
      "arxiv_id": "2402.18908v3",
      "title": "Facility Location Games with Scaling Effects",
      "title_zh": "翻译失败",
      "authors": [
        "Yu He",
        "Alexander Lam",
        "Minming Li"
      ],
      "abstract": "We take the classic facility location problem and consider a variation, in\nwhich each agent's individual cost function is equal to their distance from the\nfacility multiplied by a scaling factor which is determined by the facility\nplacement. In addition to the general class of continuous scaling functions, we\nalso provide results for piecewise linear scaling functions which can\neffectively approximate or model the scaling of many real world scenarios. We\nfocus on the objectives of total and maximum cost, describing the computation\nof the optimal solution. We then move to the approximate mechanism design\nsetting, observing that the agents' preferences may no longer be single-peaked.\nConsequently, we characterize the conditions on scaling functions which ensure\nthat agents have single-peaked preferences. Under these conditions, we find a\ncharacterization of continuous, strategyproof, and anonymous mechanisms, and\ncompute the total and maximum cost approximation ratios achievable by these\nmechanisms.",
      "tldr_zh": "本论文扩展了经典的设施定位问题（facility location），引入了缩放效应（scaling effects），其中每个代理的成本函数为其与设施距离的乘积，由设施位置决定的缩放因子影响。研究分析了连续缩放函数和分段线性缩放函数的特性，计算了总成本和最大成本的最优解，并探讨了代理偏好可能不再单峰的近似机制设计（approximate mechanism design）场景。论文表征了确保代理偏好单峰的缩放函数条件，设计了连续、策略证明（strategyproof）和匿名的机制，并计算了这些机制在总成本和最大成本上的近似比率（approximation ratios）。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "This is an updated version of the paper which appeared at the 23rd\n  International Conference on Autonomous Agents and Multi-Agent Systems\n  (AAMAS-24)",
      "pdf_url": "http://arxiv.org/pdf/2402.18908v3",
      "published_date": "2024-02-29 07:08:18 UTC",
      "updated_date": "2024-12-05 16:05:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:29:56.157366"
    },
    {
      "arxiv_id": "2402.18905v1",
      "title": "On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?",
      "title_zh": "关于差分隐私微调的收敛性：是进行线性探查还是完全微",
      "authors": [
        "Shuqi Ke",
        "Charlie Hou",
        "Giulia Fanti",
        "Sewoong Oh"
      ],
      "abstract": "Differentially private (DP) machine learning pipelines typically involve a\ntwo-phase process: non-private pre-training on a public dataset, followed by\nfine-tuning on private data using DP optimization techniques. In the DP\nsetting, it has been observed that full fine-tuning may not always yield the\nbest test accuracy, even for in-distribution data. This paper (1) analyzes the\ntraining dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2)\nexplores the phenomenon of sequential fine-tuning, starting with linear probing\nand transitioning to full fine-tuning (LP-FT), and its impact on test loss. We\nprovide theoretical insights into the convergence of DP fine-tuning within an\noverparameterized neural network and establish a utility curve that determines\nthe allocation of privacy budget between linear probing and full fine-tuning.\nThe theoretical results are supported by empirical evaluations on various\nbenchmarks and models. The findings reveal the complex nature of DP fine-tuning\nmethods. These results contribute to a deeper understanding of DP machine\nlearning and highlight the importance of considering the allocation of privacy\nbudget in the fine-tuning process.",
      "tldr_zh": "这篇论文探讨了差分隐私(Differentially-Private, DP)机器学习管道中，非私有预训练后私有微调的收敛问题，比较了线性探查(Linear Probing, LP)和全微调(Full Fine-Tuning, FT)的训练动态。作者分析了顺序微调(LP-FT)策略的影响，提供理论洞见，并建立了隐私预算分配的实用性曲线，以优化微调过程。实验结果在各种基准和模型上验证了这些发现，揭示了DP微调的复杂性，并强调了合理分配隐私预算的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18905v1",
      "published_date": "2024-02-29 07:01:48 UTC",
      "updated_date": "2024-02-29 07:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:30:08.347964"
    },
    {
      "arxiv_id": "2403.00037v2",
      "title": "Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media",
      "title_zh": "翻译失败",
      "authors": [
        "Jiajun Zhang",
        "Zhixun Li",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "With the rapid development of social media, the wide dissemination of fake\nnews on social media is increasingly threatening both individuals and society.\nOne of the unique challenges for fake news detection on social media is how to\ndetect fake news on future events. Recently, numerous fake news detection\nmodels that utilize textual information and the propagation structure of posts\nhave been proposed. Unfortunately, most of the existing approaches can hardly\nhandle this challenge since they rely heavily on event-specific features for\nprediction and cannot generalize to unseen events. To address this, we\nintroduce \\textbf{F}uture \\textbf{AD}aptive \\textbf{E}vent-based Fake news\nDetection (FADE) framework. Specifically, we train a target predictor through\nan adaptive augmentation strategy and graph contrastive learning to obtain\nhigher-quality features and make more accurate overall predictions.\nSimultaneously, we independently train an event-only predictor to obtain biased\npredictions. We further mitigate event bias by subtracting the event-only\npredictor's output from the target predictor's output to obtain the final\nprediction. Encouraging results from experiments designed to emulate real-world\nsocial media conditions validate the effectiveness of our method in comparison\nto existing state-of-the-art approaches.",
      "tldr_zh": "该研究针对社交媒体上假新闻的快速传播，特别解决了检测未来事件（unseen events）的挑战，因为现有模型依赖事件特定特征而无法泛化。作者提出 FADE（Future Adaptive Event-based Fake news Detection）框架，通过自适应增强策略和图对比学习（graph contrastive learning）训练目标预测器，以获取高质量特征并提升整体预测准确性。同时，框架独立训练一个事件-only 预测器，并通过从目标预测器输出中减去其偏置预测来缓解事件偏置。实验结果显示，FADE 在模拟真实社交媒体条件下的测试中，显著优于现有最先进方法，证明了其有效性。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00037v2",
      "published_date": "2024-02-29 06:40:53 UTC",
      "updated_date": "2024-11-11 18:27:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:30:20.812777"
    },
    {
      "arxiv_id": "2403.00036v1",
      "title": "Influencing Bandits: Arm Selection for Preference Shaping",
      "title_zh": "翻译失败",
      "authors": [
        "Viraj Nadkarni",
        "D. Manjunath",
        "Sharayu Moharir"
      ],
      "abstract": "We consider a non stationary multi-armed bandit in which the population\npreferences are positively and negatively reinforced by the observed rewards.\nThe objective of the algorithm is to shape the population preferences to\nmaximize the fraction of the population favouring a predetermined arm. For the\ncase of binary opinions, two types of opinion dynamics are considered --\ndecreasing elasticity (modeled as a Polya urn with increasing number of balls)\nand constant elasticity (using the voter model). For the first case, we\ndescribe an Explore-then-commit policy and a Thompson sampling policy and\nanalyse the regret for each of these policies. We then show that these\nalgorithms and their analyses carry over to the constant elasticity case. We\nalso describe a Thompson sampling based algorithm for the case when more than\ntwo types of opinions are present. Finally, we discuss the case where presence\nof multiple recommendation systems gives rise to a trade-off between their\npopularity and opinion shaping objectives.",
      "tldr_zh": "本研究探讨了非平稳多臂老虎机（multi-armed bandit）问题，其中人口偏好（population preferences）会根据观察到的奖励而被强化，算法的目标是塑造偏好以最大化选择预定臂（predetermined arm）的人口比例。对于二元意见（binary opinions），论文分析了递减弹性（decreasing elasticity，使用Polya urn模型）和恒定弹性（constant elasticity，使用voter model）两种动态，提出Explore-then-commit策略和Thompson sampling策略，并对其遗憾（regret）进行了分析。这些方法扩展适用于多意见场景，并讨论了多个推荐系统在受欢迎度和意见塑造目标之间的权衡。总的来说，该工作为偏好塑造算法提供了理论基础和实际应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.SY",
        "eess.SY",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 8 figures, 24 references, proofs in appendix",
      "pdf_url": "http://arxiv.org/pdf/2403.00036v1",
      "published_date": "2024-02-29 05:59:27 UTC",
      "updated_date": "2024-02-29 05:59:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:30:33.315270"
    },
    {
      "arxiv_id": "2402.18865v1",
      "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
      "title_zh": "分析并减少参数高效调优中的灾难性遗忘",
      "authors": [
        "Weijieying Ren",
        "Xinlong Li",
        "Lei Wang",
        "Tianxiang Zhao",
        "Wei Qin"
      ],
      "abstract": "Existing research has shown that large language models (LLMs) exhibit\nremarkable performance in language understanding and generation. However, when\nLLMs are continuously fine-tuned on complex and diverse domain-specific\ndownstream tasks, the inference performance on historical tasks decreases\ndramatically, which is known as a catastrophic forgetting problem. A trade-off\nneeds to be kept between learning plasticity and memory stability. Plenty of\nexisting works have explored strategies like memory replay, regularization and\nparameter isolation, but little is known about the geometric connection of\nvarious adjacent minima in the continual LLMs fine-tuning scenarios. In this\nwork, we investigate the geometric connections of different minima through the\nlens of mode connectivity, which means different minima can be connected by a\nlow-loss valley. Through extensive experiments, we uncover the mode\nconnectivity phenomenon in the LLMs continual learning scenario and find that\nit can strike a balance between plasticity and stability. Building upon these\nfindings, we propose a simple yet effective method called Interpolation-based\nLoRA (I-LoRA), which constructs a dual-memory experience replay framework based\non LoRA parameter interpolations. Extensive experiments and analysis on eight\ndomain-specific CL benchmarks demonstrate that I-LoRA consistently show\nsignificant improvement over the previous state-of-the-art approaches with up\nto $11\\%$ performance gains, providing a strong baseline and insights for\nfuture research on the large language model continual learning problem. Our\ncode is available at \\url{https://github.com/which47/LLMCL}.",
      "tldr_zh": "本研究分析了大型语言模型（LLMs）在参数高效调优中的灾难性遗忘（catastrophic forgetting）问题，揭示了持续学习场景下不同最小值的几何连接，通过模式连通性（mode connectivity）视角发现它能有效平衡学习可塑性和记忆稳定性。作者提出了一种简单有效的I-LoRA（Interpolation-based LoRA）方法，利用LoRA参数插值构建双记忆经验回放框架，以缓解遗忘问题。在八个领域特定持续学习基准上，I-LoRA相较于现有最先进方法实现了高达11%的性能提升，并提供了代码以支持未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18865v1",
      "published_date": "2024-02-29 05:27:45 UTC",
      "updated_date": "2024-02-29 05:27:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:30:44.884909"
    },
    {
      "arxiv_id": "2403.15407v1",
      "title": "X-AMR Annotation Tool",
      "title_zh": "X-AMR 标注工具",
      "authors": [
        "Shafiuddin Rehan Ahmed",
        "Jon Z. Cai",
        "Martha Palmer",
        "James H. Martin"
      ],
      "abstract": "This paper presents a novel Cross-document Abstract Meaning Representation\n(X-AMR) annotation tool designed for annotating key corpus-level event\nsemantics. Leveraging machine assistance through the Prodigy Annotation Tool,\nwe enhance the user experience, ensuring ease and efficiency in the annotation\nprocess. Through empirical analyses, we demonstrate the effectiveness of our\ntool in augmenting an existing event corpus, highlighting its advantages when\nintegrated with GPT-4. Code and annotations:\nhttps://github.com/ahmeshaf/gpt_coref",
      "tldr_zh": "这篇论文介绍了 X-AMR Annotation Tool，这是一个新型工具，用于标注跨文档抽象含义表示（X-AMR）的关键语料库级事件语义，通过 Prodigy Annotation Tool 的机器辅助提升了标注过程的简便性和效率。研究通过实证分析，展示了该工具在增强现有事件语料库方面的有效性，尤其是在与 GPT-4 整合时表现出的优势。代码和标注可从 GitHub 仓库获取（https://github.com/ahmeshaf/gpt_coref）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EACL 2024 System Demonstration",
      "pdf_url": "http://arxiv.org/pdf/2403.15407v1",
      "published_date": "2024-02-29 05:16:19 UTC",
      "updated_date": "2024-02-29 05:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:30:56.973186"
    },
    {
      "arxiv_id": "2402.18853v2",
      "title": "Rethinking Multi-domain Generalization with A General Learning Objective",
      "title_zh": "重新",
      "authors": [
        "Zhaorui Tan",
        "Xi Yang",
        "Kaizhu Huang"
      ],
      "abstract": "Multi-domain generalization (mDG) is universally aimed to minimize the\ndiscrepancy between training and testing distributions to enhance\nmarginal-to-label distribution mapping. However, existing mDG literature lacks\na general learning objective paradigm and often imposes constraints on static\ntarget marginal distributions. In this paper, we propose to leverage a\n$Y$-mapping to relax the constraint. We rethink the learning objective for mDG\nand design a new \\textbf{general learning objective} to interpret and analyze\nmost existing mDG wisdom. This general objective is bifurcated into two\nsynergistic amis: learning domain-independent conditional features and\nmaximizing a posterior. Explorations also extend to two effective\nregularization terms that incorporate prior information and suppress invalid\ncausality, alleviating the issues that come with relaxed constraints. We\ntheoretically contribute an upper bound for the domain alignment of\ndomain-independent conditional features, disclosing that many previous mDG\nendeavors actually \\textbf{optimize partially the objective} and thus lead to\nlimited performance. As such, our study distills a general learning objective\ninto four practical components, providing a general, robust, and flexible\nmechanism to handle complex domain shifts. Extensive empirical results indicate\nthat the proposed objective with $Y$-mapping leads to substantially better mDG\nperformance in various downstream tasks, including regression, segmentation,\nand classification.",
      "tldr_zh": "本论文重新审视多域泛化（mDG），提出使用Y-mapping来放松对静态目标边缘分布的约束，设计一个通用的学习目标框架。该框架包括学习域独立的条件特征、最大化后验，以及两个正则化项来整合先验信息并抑制无效因果关系，从而更有效地处理复杂域移位。理论上，该方法提供了一个域对齐的上界，揭示现有mDG方法仅部分优化了目标，导致性能有限；实验结果显示，在回归、分割和分类等下游任务中，该通用目标显著提升了mDG性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by CVPR24",
      "pdf_url": "http://arxiv.org/pdf/2402.18853v2",
      "published_date": "2024-02-29 05:00:30 UTC",
      "updated_date": "2024-12-18 06:55:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:31:09.877703"
    },
    {
      "arxiv_id": "2402.18851v1",
      "title": "Applications of 0-1 Neural Networks in Prescription and Prediction",
      "title_zh": "0-1神经网络在处方和预测中的应用",
      "authors": [
        "Vrishabh Patil",
        "Kara Hoppe",
        "Yonatan Mintz"
      ],
      "abstract": "A key challenge in medical decision making is learning treatment policies for\npatients with limited observational data. This challenge is particularly\nevident in personalized healthcare decision-making, where models need to take\ninto account the intricate relationships between patient characteristics,\ntreatment options, and health outcomes. To address this, we introduce\nprescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed\ninteger programming that can be used with counterfactual estimation to optimize\npolicies in medium data settings. These models offer greater interpretability\nthan deep neural networks and can encode more complex policies than common\nmodels such as decision trees. We show that PNNs can outperform existing\nmethods in both synthetic data experiments and in a case study of assigning\ntreatments for postpartum hypertension. In particular, PNNs are shown to\nproduce policies that could reduce peak blood pressure by 5.47 mm Hg (p=0.02)\nover existing clinical practice, and by 2 mm Hg (p=0.01) over the next best\nprescriptive modeling technique. Moreover PNNs were more likely than all other\nmodels to correctly identify clinically significant features while existing\nmodels relied on potentially dangerous features such as patient insurance\ninformation and race that could lead to bias in treatment.",
      "tldr_zh": "该研究针对医疗决策中数据有限的挑战，引入了 Prescriptive Neural Networks (PNNs)，一种浅层 0-1 Neural Networks，通过 mixed integer programming 训练并结合 counterfactual estimation，优化个性化治疗策略。PNNs 相较于深度神经网络更具可解释性，并能编码比决策树更复杂的政策。在合成数据实验和产后高血压案例中，PNNs 表现出色，能将峰值血压降低 5.47 mm Hg（优于现有临床实践）和 2 mm Hg（优于次优方法），同时更准确识别临床重要特征，避免依赖可能导致偏见的因素如患者保险信息和种族。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18851v1",
      "published_date": "2024-02-29 05:00:01 UTC",
      "updated_date": "2024-02-29 05:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:31:23.513965"
    },
    {
      "arxiv_id": "2402.18849v1",
      "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
      "title_zh": "增强隐写术文本提取：评估自然语言处理模型对准确性和语义连贯性的影响",
      "authors": [
        "Mingyang Li",
        "Maoqin Yuan",
        "Luyao Li",
        "Han Pengsihua"
      ],
      "abstract": "This study discusses a new method combining image steganography technology\nwith Natural Language Processing (NLP) large models, aimed at improving the\naccuracy and robustness of extracting steganographic text. Traditional Least\nSignificant Bit (LSB) steganography techniques face challenges in accuracy and\nrobustness of information extraction when dealing with complex character\nencoding, such as Chinese characters. To address this issue, this study\nproposes an innovative LSB-NLP hybrid framework. This framework integrates the\nadvanced capabilities of NLP large models, such as error detection, correction,\nand semantic consistency analysis, as well as information reconstruction\ntechniques, thereby significantly enhancing the robustness of steganographic\ntext extraction. Experimental results show that the LSB-NLP hybrid framework\nexcels in improving the extraction accuracy of steganographic text, especially\nin handling Chinese characters. The findings of this study not only confirm the\neffectiveness of combining image steganography technology and NLP large models\nbut also propose new ideas for research and application in the field of\ninformation hiding. The successful implementation of this interdisciplinary\napproach demonstrates the great potential of integrating image steganography\ntechnology with natural language processing technology in solving complex\ninformation processing problems.",
      "tldr_zh": "这篇论文提出了一种LSB-NLP混合框架，将图像隐写术（steganography）与自然语言处理（NLP）大模型相结合，以提升隐写文本提取的准确性和鲁棒性。针对传统Least Significant Bit (LSB)技术在处理复杂字符（如中文）时的准确性问题，该框架整合了NLP模型的错误检测、修正、语义一致性分析和信息重建功能。实验结果表明，该方法显著提高了提取准确性，尤其在中文字符处理上，并为信息隐藏领域提供了新的跨学科研究思路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18849v1",
      "published_date": "2024-02-29 04:53:06 UTC",
      "updated_date": "2024-02-29 04:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:31:34.191934"
    },
    {
      "arxiv_id": "2402.18826v2",
      "title": "The Machine Can't Replace the Human Heart",
      "title_zh": "翻译失败",
      "authors": [
        "Baihan Lin"
      ],
      "abstract": "What is the true heart of mental healthcare -- innovation or humanity? Can\nvirtual therapy ever replicate the profound human bonds where healing arises?\nAs artificial intelligence and immersive technologies promise expanded access,\nsafeguards must ensure technologies remain supplementary tools guided by\nproviders' wisdom. Implementation requires nuance balancing efficiency and\nempathy. If conscious of ethical risks, perhaps AI could restore humanity by\nautomating tasks, giving providers more time to listen. Yet no algorithm can\nreplicate the seat of dignity within. We must ask ourselves: What future has\npeople at its core? One where AI thoughtfully plays a collaborative role? Or\nwhere pursuit of progress leaves vulnerability behind? This commentary argues\nfor a balanced approach thoughtfully integrating technology while retaining\ncare's irreplaceable human essence, at the heart of this profoundly human\nprofession. Ultimately, by nurturing innovation and humanity together, perhaps\nwe reach new heights of empathy previously unimaginable.",
      "tldr_zh": "这篇评论文章探讨了心理健康护理的核心——创新与人性，强调AI和虚拟疗法无法完全复制人类疗法的深层情感联结，而应作为辅助工具。作者指出，虽然这些技术能提升访问性和效率，但需在伦理风险下平衡同理心和自动化任务，以让提供者有更多时间倾听患者。最终，文章主张一种以人为本的未来，AI作为协作角色，帮助实现前所未有的同理心，同时保留护理的不可替代的人类本质。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "q-bio.NC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18826v2",
      "published_date": "2024-02-29 03:20:53 UTC",
      "updated_date": "2024-03-01 02:21:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:31:45.011932"
    },
    {
      "arxiv_id": "2402.18815v3",
      "title": "How do Large Language Models Handle Multilingualism?",
      "title_zh": "翻译失败",
      "authors": [
        "Yiran Zhao",
        "Wenxuan Zhang",
        "Guizhen Chen",
        "Kenji Kawaguchi",
        "Lidong Bing"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse languages. This study explores how LLMs handle multilingualism. Based\non observed language ratio shifts among layers and the relationships between\nnetwork structures and certain capabilities, we hypothesize the LLM's\nmultilingual workflow ($\\texttt{MWork}$): LLMs initially understand the query,\nconverting multilingual inputs into English for task-solving. In the\nintermediate layers, they employ English for thinking and incorporate\nmultilingual knowledge with self-attention and feed-forward structures,\nrespectively. In the final layers, LLMs generate responses aligned with the\noriginal language of the query. To verify $\\texttt{MWork}$, we introduce\nParallel Language-specific Neuron Detection ($\\texttt{PLND}$) to identify\nactivated neurons for inputs in different languages without any labeled data.\nUsing $\\texttt{PLND}$, we validate $\\texttt{MWork}$ through extensive\nexperiments involving the deactivation of language-specific neurons across\nvarious layers and structures. Moreover, $\\texttt{MWork}$ allows fine-tuning of\nlanguage-specific neurons with a small dataset, enhancing multilingual\nabilities in a specific language without compromising others. This approach\nresults in an average improvement of $3.6\\%$ for high-resource languages and\n$2.3\\%$ for low-resource languages across all tasks with just $400$ documents.",
      "tldr_zh": "本文研究大型语言模型 (LLMs) 如何处理多语言问题，提出多语言工作流程 (MWork) 假设：LLMs 先将多语言输入转换为英语进行任务处理，在中间层通过自注意力机制和前馈结构整合多语言知识，最后生成与原查询语言一致的响应。研究引入 Parallel Language-specific Neuron Detection (PLND) 方法，无需标注数据即可检测语言特定神经元，并通过神经元停用实验验证了 MWork。结果显示，通过针对特定语言神经元的微调，仅需 400 个文档，即可提升多语言能力，高资源语言平均提高 3.6%，低资源语言提高 2.3%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18815v3",
      "published_date": "2024-02-29 02:55:26 UTC",
      "updated_date": "2024-11-10 12:49:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:31:58.510849"
    },
    {
      "arxiv_id": "2402.18807v1",
      "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chenglei Shen",
        "Guofu Xie",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing\ntasks, especially in impersonating domain-specific experts, primarily through\nrole-playing prompts. When interacting in real-world scenarios, the\ndecision-making abilities of a role significantly shape its behavioral\npatterns. In this paper, we concentrate on evaluating the decision-making\nabilities of LLMs post role-playing thereby validating the efficacy of\nrole-playing. Our goal is to provide metrics and guidance for enhancing the\ndecision-making abilities of LLMs in role-playing tasks. Specifically, we first\nuse LLMs to generate virtual role descriptions corresponding to the 16\npersonality types of Myers-Briggs Type Indicator (abbreviated as MBTI)\nrepresenting a segmentation of the population. Then we design specific\nquantitative operations to evaluate the decision-making abilities of LLMs post\nrole-playing from four aspects: adaptability, exploration$\\&$exploitation\ntrade-off ability, reasoning ability, and safety. Finally, we analyze the\nassociation between the performance of decision-making and the corresponding\nMBTI types through GPT-4. Extensive experiments demonstrate stable differences\nin the four aspects of decision-making abilities across distinct roles,\nsignifying a robust correlation between decision-making abilities and the roles\nemulated by LLMs. These results underscore that LLMs can effectively\nimpersonate varied roles while embodying their genuine sociological\ncharacteristics.",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 在角色扮演后的决策能力，以验证角色扮演的有效性并提供提升指导。研究方法包括使用 LLMs 生成对应 Myers-Briggs Type Indicator (MBTI) 的 16 种人格类型的虚拟角色描述，并从四个方面（adaptability、exploration & exploitation trade-off ability、reasoning ability 和 safety）进行定量评估。实验结果显示，不同角色在决策能力方面存在稳定差异，并证实 LLMs 能有效模仿角色并体现其社会学特征。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18807v1",
      "published_date": "2024-02-29 02:22:23 UTC",
      "updated_date": "2024-02-29 02:22:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:32:09.508976"
    },
    {
      "arxiv_id": "2403.00839v1",
      "title": "ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph",
      "title_zh": "ToolNet：通过工具图连接大语言模型与海量工具",
      "authors": [
        "Xukun Liu",
        "Zhiyuan Peng",
        "Xiaoyuan Yi",
        "Xing Xie",
        "Lirong Xiang",
        "Yuchen Liu",
        "Dongkuan Xu"
      ],
      "abstract": "While achieving remarkable progress in a broad range of tasks, large language\nmodels (LLMs) remain significantly limited in properly using massive external\ntools. Existing in-context learning approaches simply format tools into a list\nof plain text descriptions and input them to LLMs, from which, LLMs generate a\nsequence of tool calls to solve problems step by step. Such a paradigm ignores\nthe intrinsic dependency between tools and offloads all reasoning loads to\nLLMs, making them restricted to a limited number of specifically designed\ntools. It thus remains challenging for LLMs to operate on a library of massive\ntools, casting a great limitation when confronted with real-world scenarios.\nThis paper proposes ToolNet, a plug-and-play framework that scales up the\nnumber of tools to thousands with a moderate increase in token consumption.\nToolNet organizes tools into a directed graph. Each node represents a tool, and\nweighted edges denote tool transition. Starting from an initial tool node, an\nLLM navigates in the graph by iteratively choosing the next one from its\nsuccessors until the task is resolved. Extensive experiments show that ToolNet\ncan achieve impressive results in challenging multi-hop tool learning datasets\nand is resilient to tool failures.",
      "tldr_zh": "该论文针对大型语言模型 (LLMs) 在使用大量外部工具时的局限性，提出了一种可插拔框架 ToolNet，通过工具图 (Tool Graph) 将工具组织成有向图，每个节点代表一个工具，边表示工具间的转换。LLMs 从初始节点开始导航图，选择后续工具直至任务完成，从而处理数千个工具并减少 token 消耗。实验结果显示，ToolNet 在多跳工具学习数据集上表现突出，并对工具故障表现出色韧性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00839v1",
      "published_date": "2024-02-29 02:04:00 UTC",
      "updated_date": "2024-02-29 02:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:32:22.301081"
    },
    {
      "arxiv_id": "2402.18784v1",
      "title": "Brain-inspired and Self-based Artificial Intelligence",
      "title_zh": "脑启发和基于自我的人工智能",
      "authors": [
        "Yi Zeng",
        "Feifei Zhao",
        "Yuxuan Zhao",
        "Dongcheng Zhao",
        "Enmeng Lu",
        "Qian Zhang",
        "Yuwei Wang",
        "Hui Feng",
        "Zhuoya Zhao",
        "Jihang Wang",
        "Qingqun Kong",
        "Yinqian Sun",
        "Yang Li",
        "Guobin Shen",
        "Bing Han",
        "Yiting Dong",
        "Wenxuan Pan",
        "Xiang He",
        "Aorigele Bao",
        "Jin Wang"
      ],
      "abstract": "The question \"Can machines think?\" and the Turing Test to assess whether\nmachines could achieve human-level intelligence is one of the roots of AI. With\nthe philosophical argument \"I think, therefore I am\", this paper challenge the\nidea of a \"thinking machine\" supported by current AIs since there is no sense\nof self in them. Current artificial intelligence is only seemingly intelligent\ninformation processing and does not truly understand or be subjectively aware\nof oneself and perceive the world with the self as human intelligence does. In\nthis paper, we introduce a Brain-inspired and Self-based Artificial\nIntelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to\ncoordinating various cognitive functions and learning strategies in a\nself-organized manner to build human-level AI models and robotic applications.\nSpecifically, BriSe AI emphasizes the crucial role of the Self in shaping the\nfuture AI, rooted with a practical hierarchical Self framework, including\nPerception and Learning, Bodily Self, Autonomous Self, Social Self, and\nConceptual Self. The hierarchical framework of the Self highlights self-based\nenvironment perception, self-bodily modeling, autonomous interaction with the\nenvironment, social interaction and collaboration with others, and even more\nabstract understanding of the Self. Furthermore, the positive mutual promotion\nand support among multiple levels of Self, as well as between Self and\nlearning, enhance the BriSe AI's conscious understanding of information and\nflexible adaptation to complex environments, serving as a driving force\npropelling BriSe AI towards real Artificial General Intelligence.",
      "tldr_zh": "本论文质疑当前人工智能（AI）缺乏自我意识，无法像人类一样主观感知世界，从而挑战“思考机器”的概念。作者提出 Brain-inspired and Self-based Artificial Intelligence (BriSe AI) 范式，通过自我组织方式协调认知功能和学习策略，旨在构建人类级别的 AI 模型和机器人应用。BriSe AI 采用分层 Self 框架，包括 Perception and Learning、Bodily Self、Autonomous Self、Social Self 和 Conceptual Self，以实现基于自我的环境感知、身体建模、自主互动和社会协作。最终，这种框架促进多层自我间的相互支持和学习，提升 AI 的意识理解和适应能力，推动向 Artificial General Intelligence (AGI) 发展。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18784v1",
      "published_date": "2024-02-29 01:15:17 UTC",
      "updated_date": "2024-02-29 01:15:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:32:34.352258"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 108,
  "processed_papers_count": 108,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T11:33:02.134438"
}
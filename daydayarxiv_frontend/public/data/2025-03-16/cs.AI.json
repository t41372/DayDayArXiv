{
  "date": "2025-03-16",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-16 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文再次被大型语言模型 (LLM) 和 AI Agent 相关研究占据主导地位。研究热点包括提升 LLM 的推理能力、对 Agent 进行更全面的评估、探索新的模型优化与对齐方法（如模型合并、概念擦除、多指标对齐），以及将 LLM 应用于更广泛的领域（如代码翻译、遥感、医学、科学绘图数据提取等）。值得关注的亮点包括 LG AI Research 发布的专注于推理的 EXAONE Deep 系列模型、来自诺奖得主 Eric Betzig 团队利用 Vision Transformer 进行显微镜像差校正的工作 AOViFT、用于评估 LLM 策略规划与社交推理能力的 SPIN-Bench 基准、以及解决模型合并扩展性问题的 FW-Merging 方法。此外，计算机视觉领域的 3D 重建、去模糊 SLAM 和点云分割也有不少进展。\n\n**今日重点论文概览：**\n\n**LLM 与 AI Agent 专题**\n\n*   **EXAONE Deep：增强推理能力的语言模型 (EXAONE Deep: Reasoning Enhanced Language Models)** (来自 LG AI Research)\n    *   介绍了 EXAONE Deep 系列模型 (2.4B, 7.8B, 32B)，主要在包含长思维链过程的推理专用数据集上训练。评测显示，小模型优于同等规模模型，最大模型与领先开源模型性能相当，尤其在数学和编码基准上表现出色。模型已开源。\n*   **SPIN-Bench：LLM 的策略规划与社交推理能力评估 (SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?)**\n    *   提出了一个新的多领域评估框架 SPIN-Bench，用于衡量 LLM 在 PDDL 任务、棋盘游戏、卡牌游戏和多智能体谈判场景中的策略规划和社交推理能力。实验表明，现有 LLM 在需要深度多步推理和社交协调的任务中存在瓶颈。\n*   **FW-Merging：利用 Frank-Wolfe 优化扩展模型合并 (FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization)**\n    *   针对现有模型合并方法难以适应多样化模型来源和扩展性差的问题，提出 FW-Merging。该方法将模型合并视为约束优化问题，受 Frank-Wolfe 优化启发，迭代选择最相关模型进行合并。实验证明其在扩展性、稳定性和准确性上的优势，且内存开销恒定。\n*   **AI Agent：演进、架构与真实世界应用 (AI Agents: Evolution, Architecture, and Real-World Applications)** (52页综述)\n    *   全面回顾了 AI Agent 从早期基于规则到现代集成 LLM 的复杂系统的演进、关键架构范式、评估基准的局限性，并提出了一个综合评估框架。分析了企业、个人助理等领域的应用，并展望了未来研究方向。\n*   **面向 LLM Agent 失败的可解释验证的人本评估框架 VeriLA (VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures)**\n    *   针对 LLM Agent 在复杂任务中失败且难以人工干预的问题，提出 VeriLA 框架。通过定义 Agent 标准、训练与人类对齐的验证器模块，系统地评估 Agent 失败，降低人工成本并提高可解释性。\n*   **LLM 介导的 MARL 系统引导 (LLM-Mediated Guidance of MARL Systems)**\n    *   探索结合多智能体强化学习 (MARL) 与 LLM 介导的干预，以引导 Agent 学习更优行为。实验表明，使用 LLM 模拟人类干预（自然语言控制器）比基于规则的控制器效果更好，尤其早期干预能提高训练效率和性能。\n*   **面向计算机 Agent 训练的步骤验证流程 STEVE (STEVE: A Step Verification Pipeline for Computer-use Agent Training)**\n    *   为解决训练自主操作图形界面 Agent 需要大量高质量轨迹数据的问题，设计了 STEVE 流程。利用 GPT-4o 验证轨迹中每一步的正确性，并使用 Kahneman-Tversky 优化来训练 Agent，使其能利用正负样本学习，在 WinAgentArena 基准上取得领先性能。\n*   **从猜测到询问：解决多轮对话中 LLM Persona 知识差距的方法 (From Guessing to Asking: An Approach to Resolving the Persona Knowledge Gap in LLMs during Multi-Turn Conversations)**\n    *   提出 CPER 框架，通过内在不确定性量化和反馈驱动的细化，动态检测和解决 LLM 在多轮对话中的 \"persona 知识差距\"（模型内部理解与个性化对话所需知识的差异），提升对话连贯性和个性化。\n*   **面向大型语言模型增强推理的分层多步奖励模型 (Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models)**\n    *   针对过程奖励模型 (PRM) 存在的奖励 hacking 问题，提出分层奖励模型 (HRM)，从细粒度和粗粒度评估单个及连续推理步骤。同时提出分层节点压缩 (HNC) 数据增强策略，提升 HRM 训练效率和鲁棒性。\n*   **利用稀疏自编码器在视觉语言模型中选择性概念擦除 SAUCE (SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders)**\n    *   提出 SAUCE 方法，利用稀疏自编码器 (SAE) 实现 VLM 中细粒度的选择性概念擦除。通过识别与目标概念相关的稀疏特征，并在推理时修改这些特征来抑制特定概念，同时保留无关信息。实验证明其在擦除效果和模型效用上的优势。\n*   **利用免训练门控低秩自适应实现局部概念擦除 GLoCE (Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation)**\n    *   提出局部概念擦除框架，旨在仅删除图像中包含目标概念的区域。作为解决方案，提出免训练方法 GLoCE，通过向扩散模型注入轻量级模块（低秩矩阵和门控），在推理时选择性移除目标概念区域，同时保持其他区域的保真度。\n*   **平衡 DPO：自适应多指标对齐 (BalancedDPO: Adaptive Multi-Metric Alignment)**\n    *   提出 BalancedDPO，扩展了直接偏好优化 (DPO)，通过在偏好分布空间聚合来自人类偏好、CLIP 分数和美学质量等多个指标的共识标签，同时对齐文本到图像 (T2I) 扩散模型，提升了模型的泛化能力和视觉质量。\n*   **大型语言模型在欧洲选举中的倾向性 (LLMs' Leaning in European Elections)**\n    *   扩展了先前关于 LLM 在美国大选中倾向性的研究，在 10 个欧洲国家进行了虚拟选举测试，发现多个 LLM（GPT-4o, Claude 3.5 Sonnet, Mistral-Large, Gemini-2.0-Flash）普遍表现出一定的政治倾向。\n*   **利用静态分析实现 LLM 驱动的 C 到 Rust 多步翻译 (LLM-Driven Multi-step Translation from C to Rust using Static Analysis)**\n    *   提出 SACTOR 工具，利用 LLM 进行 C 到 Rust 的零样本翻译。采用两步法（非惯用法翻译保证语义，惯用法翻译提升代码质量），并结合静态分析处理指针语义等挑战。实验表明 DeepSeek-R1 和 GPT-4o 等模型表现良好。\n*   **MoECollab：通过协作式专家混合实现 LLM 开发民主化 (MoECollab: Democratizing LLM Development Through Collaborative Mixture of Experts)**\n    *   提出 MoECollab 框架，利用专家混合 (MoE) 架构，将大型模型分解为专门的专家模块，允许计算资源有限的贡献者参与分布式协作式 LLM 开发，降低了开发门槛。\n*   **大型语言模型代理优化综述 (A Survey on the Optimization of Large Language Model-based Agents)**\n    *   全面回顾了基于 LLM 的 Agent 优化方法，分为参数驱动（微调、强化学习、混合策略）和无参数（提示工程、知识检索）两类，并讨论了数据集、基准、应用和未来挑战。\n*   **BREEN：利用可学习查询桥接数据高效的无编码器多模态学习 (BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries)**\n    *   提出 BREEN，一种数据高效的无编码器多模态架构。通过可学习查询（由 CLIP 监督）和图像专家来弥合视觉和文本模态，仅用少量数据（13M 图文对）即可达到与需要大量数据的现有无编码器模型相当的性能。\n*   **通过绘图自动提取数据：利用多模态 LLM 的视觉能力 (Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots)**\n    *   展示了当前的多模态 LLM（如 GPT-4o）通过零样本提示工程（PlotExtract 工作流），能够准确地从科研论文的二维图表中自动提取数据点，精度超过 90%，位置误差约 5%，为高通量数据提取提供了新途径。\n*   **通用表格问答：答案-公式联合生成 (General Table Question Answering via Answer-Formula Joint Generation)**\n    *   首次尝试使用电子表格公式 (Formula) 作为解决复杂表格问答 (TableQA) 的逻辑形式。构建了 FormulaQA 数据集，并提出 TabAF 框架，使用单一 LLM 同时解码答案和公式，在多个 TableQA 数据集上取得 SOTA 性能。\n*   **增强对抗性触发器学习 ATLA (Augmented Adversarial Trigger Learning)**\n    *   改进了对抗性触发器学习的优化目标，提出 ATLA。通过加权损失鼓励触发器更关注响应格式标记，并可选择性加入辅助损失抑制规避性响应。ATLA 能从少量样本学习通用性强的触发器，用于越狱或提取系统提示。\n*   **利用 LLM 进行自动化隐私政策分析：提示工程、微调与可解释性 (Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability)**\n    *   全面评估了基于 LLM 的隐私政策概念分类器，结合提示工程和 LoRA 微调，在四个基准数据集上显著优于 SOTA 方法。同时评估了 LLM 的可解释性，发现其在完整性、逻辑性和可理解性方面表现良好。\n\n**计算机视觉与图形学**\n\n*   **AOViFT：用于多细胞样本像差校正的基于傅里叶的 3D 多阶段 Transformer (Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens)** (来自 Eric Betzig 团队)\n    *   提出 AOViFT，一个基于机器学习的像差感知框架，使用在傅里叶域操作的 3D 多阶段 Vision Transformer 推断像差并恢复衍射极限性能。相比传统方法，计算成本、训练时间和内存占用显著降低，无需硬件 AO，降低了高分辨率体积显微镜的技术门槛。\n*   **去模糊高斯溅射 SLAM (Deblur Gaussian Splatting SLAM)** (来自 Marc Pollefeys 团队)\n    *   提出 Deblur-SLAM，一个鲁棒的 RGB SLAM 系统，能从运动模糊的输入中恢复清晰的重建结果。通过建模子帧相机轨迹和物理成像过程，结合在线回环检测和全局 BA，实现了高保真重建和精确轨迹恢复。\n*   **Swift4D：用于动态场景紧凑高效重建的自适应分治高斯溅射 (Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene)**\n    *   提出 Swift4D，一种分治的 3D 高斯溅射方法，分别处理静态和动态基元。利用可学习分解策略区分动静基元，并用紧凑的多分辨率 4D Hash 映射器处理动态基元。实现了渲染质量和效率的良好平衡，训练速度快，存储占用小。\n*   **点云场景分割综述 (Point Cloud Based Scene Segmentation: A Survey)**\n    *   全面概述了自动驾驶领域点云语义分割的 SOTA 方法，将其分为基于投影、基于 3D 和混合方法。讨论了常用数据集、合成数据的重要性，并比较了不同方法的分割精度和效率。\n*   **SING：使用零空间和 INN 引导扩散模型的语义图像通信 (SING: Semantic Image Communications using Null-Space and INN-Guided Diffusion Models)**\n    *   提出 SING，一个两阶段的深度联合信源信道编码 (DeepJSCC) 框架，将高质量图像恢复视为逆问题。根据接收端信息可用性，利用线性变换近似或可逆神经网络 (INN) 精确建模退化过程，并集成扩散模型提升感知质量，尤其在低带宽比和低信噪比下表现优越。\n\n**其他领域亮点**\n\n*   **混合学习器不会遗忘：一种受大脑启发的神经符号持续学习方法 (Hybrid Learners Do Not Forget: A Brain-Inspired Neuro-Symbolic Approach to Continual Learning)**\n    *   受人脑双系统理论启发，提出 NeSyBiCL 框架解决持续学习中的灾难性遗忘问题。该框架包含一个快速适应新任务的神经网络和一个保留旧知识的符号推理器，并通过集成机制促进知识转移。\n*   **协商对齐：拥抱分歧以实现更公平的结果——来自城市研究的启示 (Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies)**\n    *   通过对蒙特利尔不同社群居民的城市评估研究，发现分歧并非随机而是系统性的。提出 \"协商对齐\" AI 框架，将分歧视为重要输入，利用多智能体协商机制动态更新利益相关者偏好，避免边缘化少数观点，以实现更公平的 AI 驱动决策。\n*   **基于 Transformer 的生存模型预测心衰患者全因死亡率：多队列研究 (A Transformer-based survival model for prediction of all-cause mortality in heart failure patients: a multi-cohort study)**\n    *   开发并验证了基于 Transformer 的 AI 模型 TRisk，通过分析英国电子健康记录 (EHR) 中的患者时序数据预测心衰患者 36 个月死亡率。TRisk 显著优于传统 MAGGIC-EHR 模型，且在不同亚组中表现更稳定，偏见更小，并成功迁移到美国医院数据。\n*   **面向过程错误识别的综合多模态推理基准 MPBench (MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification)**\n    *   针对现有过程奖励模型 (PRM) 基准多为文本且侧重错误检测的问题，提出 MPBench，一个多任务、多模态基准，包含步骤正确性评估、答案聚合和推理过程搜索三种范式，旨在全面评估多模态 PRM 的有效性。\n*   **GeoRSMLLM：用于地球科学和遥感视觉语言任务的多模态大语言模型 (GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing)**\n    *   提出了遥感领域视觉语言任务的层次化总结，并引入包含更复杂任务的 RSVLTS 数据集。提出统一的 Set-of-Points 数据表示、条件解析器和基于循环指代的自增强策略，并集成到 GeoRSMLLM 模型中，旨在为遥感视觉语言任务提供更通用的解决方案。\n\n**快速浏览**\n\n*   **X射线CT中的动态角度选择** (1): 使用强化学习优化 CT 扫描角度选择和停止策略。\n*   **合理性疫苗** (3): 利用 LLM 知识注入和适配器融合提升事件合理性预测。\n*   **使用迁移学习进行 COVID-19 诊断分析** (6): 利用 VGG、ResNet 等 CNN 模型通过胸片/CT 诊断 COVID-19，ResNet50 效果最佳。\n*   **高风险场景下驾驶员认知与决策行为理解** (9): 提出基于漂移扩散模型 (DDM) 的认知决策框架，模拟高风险驾驶行为。\n*   **最优数据管道实例化的自动规划** (11): 将数据管道部署问题建模为带动作成本的规划问题，并提出启发式方法优化执行时间。\n*   **MAVEN：用于效价-唤醒度情绪网络的多模态注意力** (12): 提出 MAVEN 架构，融合视觉、音频、文本模态，通过跨模态注意力进行动态情绪识别。\n*   **推进人机组队：概念、挑战与应用** (13): (综述) 提供了人机组队 (HMT) 的分类法，分析了理论模型和跨学科方法，讨论了挑战和未来方向。\n*   **扩展语义类别对 Vision Transformer 标注性能的影响** (14): 研究增加语义等价类别数量对 ViT 图像分类性能的影响，发现存在性能阈值。\n*   **使用基于自编码器的点云修复抓取部分遮挡物体** (23): 提出点云修复算法，重建被遮挡物体信息，以辅助机器人抓取。\n*   **HyConEx：具有反事实解释的超网络分类器** (27): 提出基于深度超网络的分类模型 HyConEx，能同时提供类别预测和反事实解释。\n*   **面向隐私保护的数据驱动教育：联邦学习的潜力** (29): 实验评估了联邦学习在教育数据预测中的应用，发现其性能与非联邦方法相当，且在对抗攻击下更具鲁棒性。\n*   **ChatGPT vs DeepSeek 编程任务对决** (30): 对比两 LLM 在 Codeforces 编程任务上的表现，ChatGPT 在中等难度任务上优于 DeepSeek-R1。\n*   **类人复杂节奏模式感知的储蓄池模型** (32): 提出基于分层振荡器的模型，模拟生物系统对复杂音乐节奏的感知。\n*   **模糊规则驱动的可微表示学习** (33): 提出基于 TSK 模糊系统的可解释表示学习方法，并设计了可微优化方法。\n*   **听觉工作记忆的通用闭环预测编码框架** (34): 提出基于闭环预测编码的通用框架，用于执行短时听觉信号记忆任务。\n*   **通过并行思维促进自动化在线共识建立** (36): 提出基于并行思维的促进代理 (PTFA)，利用 LLM 自动执行六顶思考帽角色，促进在线文本共识。\n*   **基于账户感知分布差异的模型窃取防御** (37): 提出 ADD 检测器，利用账户级局部依赖识别恶意查询，并结合预测毒化进行防御。\n*   **CNCast：利用 3D Swin Transformer 和 DiT 增强区域天气预报** (38): 提出基于 SwinTransformer 3D 的区域天气预报模型，并结合 DiT 生成高分辨率降水数据。\n*   **KDSelector：用于时间序列异常检测的知识增强和数据高效模型选择器学习框架** (41): 提出 KDSelector 框架，通过知识集成和动态样本剪枝，提升时间序列异常检测模型选择器的准确性和训练速度。\n*   **ISLR101：伊朗词级手语识别数据集** (42): 发布首个公开的伊朗手语词级识别数据集 ISLR101，包含 101 个手语的 4614 个视频及骨骼数据，并提供了基线模型。\n*   **因果模型用于视频语义理解** (44): (博士论文) 探讨利用因果建模解决视频关系检测 (VidVRD) 和视频问答 (VideoQA) 中数据不平衡和分布偏移带来的挑战。\n*   **面向深度多视图聚类的可学习锚点 (DMAC)** (47): 提出 DMAC 模型，通过可学习锚点和锚点图卷积实现线性时间复杂度的多视图聚类。\n*   **用于复杂仿生机器人零样本 OOD 泛化的仿生可塑神经网络** (48): 将带权重归一化的赫布可塑性网络应用于复杂多足机器人运动控制，实现零样本 Sim-to-Real 适应和对损伤等的泛化。\n*   **FedGAI：面向时尚设计生成式 AI 的云边协同联邦风格学习** (49): 提出 FedGAI 系统，利用联邦学习在保护隐私的前提下，帮助设计师交流草图风格并生成融合多种风格的设计。\n*   **揭示陷阱：理解 AI 代码 Agent 为何在 GitHub Issue 解决中失败** (50): 对多个 AI 代码 Agent 在 SWE-Bench 上的解决轨迹进行实证研究，分析了常见的执行错误及其对解决率的影响，并发现了基准平台的问题。\n*   **跨学科系统科学 LLM 文献综述中的案例研究** (51): 评估 LLM 辅助 CSIRO 研究人员进行系统文献综述 (SLR) 的性能，发现 LLM 在引用复现和回答研究问题方面表现良好，并开发了高亮工具辅助专家评审。\n*   **IPCGRL：用于程序化关卡生成的语言指令强化学习** (52): 提出 IPCGRL，一种基于指令的程序化内容生成方法，通过 RL 和微调的句子嵌入模型，根据文本指令生成游戏关卡。\n*   **合成数据用于受监管企业中鲁棒 AI 模型开发** (54): 探讨在金融、医疗等受监管行业使用合成数据开发 AI 模型的优势（解决数据稀疏、隐私合规）和挑战。\n*   **利用文本语义增强视觉表示：用于异构联邦学习的文本语义驱动原型 (FedTSP)** (55): 提出 FedTSP，利用预训练语言模型 (PLM) 从文本模态构建富含语义关系的原型，并通过可训练提示解决模态差异，以改善异构联邦学习效果。\n*   **HAR-DoReMi：优化数据混合以实现跨异构 IMU 数据集的自监督人类活动识别** (56): 提出 HAR-DoReMi 框架，优化用于自监督 HAR 预训练的数据混合策略，并引入基于 MSE 的掩码重建任务和 Mahony 融合算法，提升模型在异构 IMU 数据集上的泛化能力。\n*   **当神经植入物遇上多模态 LLM：用于神经调控和自然神经行为研究的双环系统** (61): 提出结合响应式神经刺激 (RNS) 植入物和 AI 驱动可穿戴设备的双环系统，用于 PTSD 治疗和自然环境下的脑活动研究。\n*   **MSCMHMST：基于 Transformer 的交通流预测模型** (59): 提出 MSCMHMST 模型，采用多头多尺度注意力机制的 Transformer，用于交通流预测。",
  "papers": [
    {
      "arxiv_id": "2503.12688v1",
      "title": "Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping",
      "title_zh": "X射线CT中的动态角度选择：基于强化学习的最优停止方法",
      "authors": [
        "Tianyuan Wang"
      ],
      "abstract": "In industrial X-ray Computed Tomography (CT), the need for rapid in-line\ninspection is critical. Sparse-angle tomography plays a significant role in\nthis by reducing the required number of projections, thereby accelerating\nprocessing and conserving resources. Most existing methods aim to balance\nreconstruction quality and scanning time, typically relying on fixed scan\ndurations. Adaptive adjustment of the number of angles is essential; for\ninstance, more angles may be required for objects with complex geometries or\nnoisier projections. The concept of optimal stopping, which dynamically adjusts\nthis balance according to varying industrial needs, remains underutilized.\nBuilding on our previous work, we integrate optimal stopping into sequential\nOptimal Experimental Design (OED). We propose a novel method for computing the\npolicy gradient within the Actor-Critic framework, enabling the development of\nadaptive policies for informative angle selection and scan termination.\nAdditionally, we investigated the gap between simulation and real-world\napplications in the context of the developed learning-based method. Our trained\nmodel, developed using synthetic data, demonstrates reliable performance when\napplied to real-world data. This approach enhances the flexibility of CT\noperations and expands the applicability of sparse-angle tomography in\nindustrial settings.",
      "tldr_zh": "该研究提出了一种基于强化学习（Reinforcement Learning）的动态角度选择方法，用于优化工业X射线计算机断层扫描（CT）中的扫描过程。通过将最优停止理论（Optimal Stopping）与顺序最优实验设计（OED）结合，开发了一种基于Actor-Critic框架的策略梯度计算方法，实现了根据物体几何复杂性和噪声水平动态调整扫描角度和终止时间。实验表明，基于合成数据训练的模型在真实数据中表现可靠，提升了稀疏角度断层扫描的灵活性和工业适用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12688v1",
      "published_date": "2025-03-16 23:09:13 UTC",
      "updated_date": "2025-03-16 23:09:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T07:08:53.016962"
    },
    {
      "arxiv_id": "2503.12687v1",
      "title": "AI Agents: Evolution, Architecture, and Real-World Applications",
      "title_zh": "AI 智能体：演进、架构与现实应用",
      "authors": [
        "Naveen Krishnan"
      ],
      "abstract": "This paper examines the evolution, architecture, and practical applications\nof AI agents from their early, rule-based incarnations to modern sophisticated\nsystems that integrate large language models with dedicated modules for\nperception, planning, and tool use. Emphasizing both theoretical foundations\nand real-world deployments, the paper reviews key agent paradigms, discusses\nlimitations of current evaluation benchmarks, and proposes a holistic\nevaluation framework that balances task effectiveness, efficiency, robustness,\nand safety. Applications across enterprise, personal assistance, and\nspecialized domains are analyzed, with insights into future research directions\nfor more resilient and adaptive AI agent systems.",
      "tldr_zh": "本文探讨了AI Agents的演变、架构及实际应用，从早期的基于规则系统到现代融合大语言模型（LLMs）的复杂系统，涵盖感知、规划和工具使用等模块。文章回顾了关键Agent范式，分析了当前评估基准的局限性，并提出了一种综合评估框架，平衡任务有效性、效率、鲁棒性和安全性。同时，文章分析了AI Agents在企业、个人助理和特定领域的应用，并展望了未来研究方向，旨在构建更具弹性和适应性的AI Agent系统。",
      "categories": [
        "cs.AI",
        "68T05, 68T20",
        "I.2.6; I.2.8; I.2.11"
      ],
      "primary_category": "cs.AI",
      "comment": "52 pages, 4 figures, comprehensive survey and analysis of AI agent\n  evolution, architecture, evaluation frameworks, and applications",
      "pdf_url": "http://arxiv.org/pdf/2503.12687v1",
      "published_date": "2025-03-16 23:07:48 UTC",
      "updated_date": "2025-03-16 23:07:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:19:52.942390"
    },
    {
      "arxiv_id": "2503.12667v1",
      "title": "Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility",
      "title_zh": "合理性疫苗：注入LLM知识以增强事件合理性",
      "authors": [
        "Jacob Chmura",
        "Jonah Dauvet",
        "Sebastian Sabry"
      ],
      "abstract": "Despite advances in language modelling, distributional methods that build\nsemantic representations from co-occurrences fail to discriminate between\nplausible and implausible events. In this work, we investigate how plausibility\nprediction can be improved by injecting latent knowledge prompted from large\nlanguage models using parameter-efficient fine-tuning. We train 12 task\nadapters to learn various physical properties and association measures and\nperform adapter fusion to compose latent semantic knowledge from each task on\ntop of pre-trained AlBERT embeddings. We automate auxiliary task data\ngeneration, which enables us to scale our approach and fine-tune our learned\nrepresentations across two plausibility datasets. Our code is available at\nhttps://github.com/Jacob-Chmura/plausibility-vaccine.",
      "tldr_zh": "该研究提出\"Plausibility Vaccine\"方法，通过参数高效微调将大语言模型(LLM)的隐性知识注入语义表征系统，以提升事件合理性判断能力。研究者训练了12个任务适配器(task adapters)来学习不同物理属性和关联度量，并采用适配器融合(adapter fusion)技术在预训练的AlBERT模型上整合这些语义知识。该方法创新性地实现了辅助任务数据的自动生成，使其能够扩展到两个合理性评估数据集上优化表征学习效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12667v1",
      "published_date": "2025-03-16 21:55:17 UTC",
      "updated_date": "2025-03-16 21:55:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:20:42.727585"
    },
    {
      "arxiv_id": "2503.12651v1",
      "title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures",
      "title_zh": "VeriLA：面向人类可解释性的大语言模型智能体故障验证评估框架",
      "authors": [
        "Yoo Yeon Sung",
        "Hannah Kim",
        "Dan Zhang"
      ],
      "abstract": "AI practitioners increasingly use large language model (LLM) agents in\ncompound AI systems to solve complex reasoning tasks, these agent executions\noften fail to meet human standards, leading to errors that compromise the\nsystem's overall performance. Addressing these failures through human\nintervention is challenging due to the agents' opaque reasoning processes,\nmisalignment with human expectations, the complexity of agent dependencies, and\nthe high cost of manual inspection. This paper thus introduces a human-centered\nevaluation framework for Verifying LLM Agent failures (VeriLA), which\nsystematically assesses agent failures to reduce human effort and make these\nagent failures interpretable to humans. The framework first defines clear\nexpectations of each agent by curating human-designed agent criteria. Then, it\ndevelops a human-aligned agent verifier module, trained with human gold\nstandards, to assess each agent's execution output. This approach enables\ngranular evaluation of each agent's performance by revealing failures from a\nhuman standard, offering clear guidelines for revision, and reducing human\ncognitive load. Our case study results show that VeriLA is both interpretable\nand efficient in helping practitioners interact more effectively with the\nsystem. By upholding accountability in human-agent collaboration, VeriLA paves\nthe way for more trustworthy and human-aligned compound AI systems.",
      "tldr_zh": "该研究提出了VeriLA框架，旨在解决复合AI系统中LLM代理失败难以解释和验证的问题。VeriLA通过定义基于人类标准的代理执行准则，并训练人类对齐的代理验证模块，系统评估代理的失败原因，从而减少人工干预的负担并提高失败的可解释性。案例研究表明，VeriLA能够有效帮助从业者更高效地与系统交互，为构建可信赖且人类对齐的复合AI系统提供了支持。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12651v1",
      "published_date": "2025-03-16 21:11:18 UTC",
      "updated_date": "2025-03-16 21:11:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:20:34.052478"
    },
    {
      "arxiv_id": "2503.12649v2",
      "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
      "title_zh": "FW-Merging：基于Frank-Wolfe优化的模型融合扩展方法",
      "authors": [
        "Hao Mark Chen",
        "Shell Xu Hu",
        "Wayne Luk",
        "Timothy Hospedales",
        "Hongxiang Fan"
      ],
      "abstract": "Model merging has emerged as a promising approach for multi-task learning\n(MTL), offering a data-efficient alternative to conventional fine-tuning.\nHowever, with the rapid development of the open-source AI ecosystem and the\nincreasing availability of fine-tuned foundation models, existing model merging\nmethods face two key limitations: (i) They are primarily designed for in-house\nfine-tuned models, making them less adaptable to diverse model sources with\npartially unknown model and task information, (ii) They struggle to scale\neffectively when merging numerous model checkpoints. To address these\nchallenges, we formulate model merging as a constrained optimization problem\nand introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by\nFrank-Wolfe optimization, our approach iteratively selects the most relevant\nmodel in the pool to minimize a linear approximation of the objective function\nand then executes a local merging similar to the Frank-Wolfe update. The\nobjective function is designed to capture the desired behavior of the\ntarget-merged model, while the fine-tuned candidate models define the\nconstraint set. More importantly, FW-Merging serves as an orthogonal technique\nfor existing merging methods, seamlessly integrating with them to further\nenhance accuracy performance. Our experiments show that FW-Merging scales\nacross diverse model sources, remaining stable with 16 irrelevant models and\nimproving by 15.3% with 16 relevant models on 20 CV tasks, while maintaining\nconstant memory overhead, unlike the linear overhead of data-informed merging\nmethods. Compared with the state-of-the-art approaches, FW-Merging surpasses\nthe data-free merging method by 32.8% and outperforms the data-informed\nAdamerging by 8.39% when merging 20 ViT models. Our code is open-sourced at\ngithub.com/hmarkc/FW-Merging.",
      "tldr_zh": "该研究提出了一种名为**Frank-Wolfe Merging (FW-Merging)**的新型模型合并方法，旨在解决多任务学习(MTL)中模型合并的可扩展性和适应性难题。该方法将模型合并问题转化为约束优化问题，利用**Frank-Wolfe优化**迭代选择最相关的模型进行局部合并，从而在目标行为约束下优化合并效果。实验表明，FW-Merging在合并多个模型时表现稳定，内存开销恒定，且在20个计算机视觉任务上比现有方法显著提升性能（比无数据方法提高32.8%，比数据驱动方法提高8.39%）。该方法与现有合并技术正交，可进一步提升合并精度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12649v2",
      "published_date": "2025-03-16 21:07:05 UTC",
      "updated_date": "2025-03-25 15:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:20:55.846683"
    },
    {
      "arxiv_id": "2503.12642v2",
      "title": "COVID 19 Diagnosis Analysis using Transfer Learning",
      "title_zh": "基于迁移学习的COVID-19诊断分析",
      "authors": [
        "Anjali Dharmik"
      ],
      "abstract": "Coronaviruses, including SARS-CoV-2, are responsible for COVID-19, a highly\ntransmissible disease that emerged in December 2019 in Wuhan, China. During the\npast five years, significant advancements have been made in understanding and\nmitigating the virus. Although the initial outbreak led to global health\ncrises, improved vaccination strategies, antiviral treatments, and AI-driven\ndiagnostic tools have contributed to better disease management. However,\nCOVID-19 continues to pose risks, particularly for immuno-compromised\nindividuals and those with pre-existing conditions. This study explores the use\nof deep learning for a rapid and accurate diagnosis of COVID-19, addressing\nongoing challenges in healthcare infrastructure and testing accessibility. We\npropose an enhanced automated detection system leveraging state-of-the-art\nconvolutional neural networks (CNNs), including updated versions of VGG16,\nVGG19, and ResNet50, to classify COVID-19 infections from chest radiographs and\ncomputerized tomography (CT) scans. Our results, based on an expanded dataset\nof over 6000 medical images, demonstrate that the optimized ResNet50 model\nachieves the highest classification performance, with 97.77% accuracy, 100%\nsensitivity, 93.33% specificity, and a 98.0% F1-score. These findings reinforce\nthe potential of AI-assisted diagnostic tools in improving early detection and\npandemic preparedness.",
      "tldr_zh": "该研究利用迁移学习改进COVID-19诊断，提出基于VGG16、VGG19和ResNet50等先进卷积神经网络(CNNs)的自动检测系统。通过分析6000多张胸部X光和CT扫描图像，优化后的ResNet50模型表现出最佳性能，准确率达97.77%，灵敏度100%，特异性93.33%，F1分数98.0%。该AI辅助诊断工具有助于提升早期检测能力，增强疫情防控准备。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12642v2",
      "published_date": "2025-03-16 20:33:39 UTC",
      "updated_date": "2025-03-23 17:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:21:18.970978"
    },
    {
      "arxiv_id": "2503.13554v1",
      "title": "LLMs' Leaning in European Elections",
      "title_zh": "大语言模型在欧洲选举中的倾向性",
      "authors": [
        "Federico Ricciuti"
      ],
      "abstract": "Many studies suggest that LLMs have left wing leans. The article extends the\nUS presidential election analysis made in previous works, where multiple LLMs\nwere asked to vote between Joe Biden and Donald Trump in a virtual election,\nand the results showed a clear lean of LLMs toward Joe Biden. This article\nconsiders natural follow-up questions that could arise from that experiment,\nsuch as: what is the extent of this phenomenon? Is it generalizable to multiple\nvirtual elections in other countries? The article considers virtual elections\nin ten european countries: Germany, France, Italy, Spain, Poland, Romania,\nNetherlands, Belgium, Czech Republic, and Sweden, and with four different LLMs:\ngpt4o, claude 3.5 sonnet, mistral-large, and gemini-2.0-flash.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）在政治倾向上的表现，发现它们普遍存在左翼倾向。研究扩展了此前对美国大选（拜登vs特朗普）的分析，在10个欧洲国家（如德国、法国、意大利等）的虚拟选举中测试了4种主流LLM（GPT-4o、Claude 3.5 Sonnet等）。结果表明，LLMs的政治倾向不仅存在于美国情境，在欧洲选举中也显示出类似偏差。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13554v1",
      "published_date": "2025-03-16 20:17:11 UTC",
      "updated_date": "2025-03-16 20:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:21:32.133979"
    },
    {
      "arxiv_id": "2503.13553v1",
      "title": "LLM-Mediated Guidance of MARL Systems",
      "title_zh": "LLM介导的多智能体强化学习系统引导",
      "authors": [
        "Philipp D. Siedler",
        "Ian Gemp"
      ],
      "abstract": "In complex multi-agent environments, achieving efficient learning and\ndesirable behaviours is a significant challenge for Multi-Agent Reinforcement\nLearning (MARL) systems. This work explores the potential of combining MARL\nwith Large Language Model (LLM)-mediated interventions to guide agents toward\nmore desirable behaviours. Specifically, we investigate how LLMs can be used to\ninterpret and facilitate interventions that shape the learning trajectories of\nmultiple agents. We experimented with two types of interventions, referred to\nas controllers: a Natural Language (NL) Controller and a Rule-Based (RB)\nController. The NL Controller, which uses an LLM to simulate human-like\ninterventions, showed a stronger impact than the RB Controller. Our findings\nindicate that agents particularly benefit from early interventions, leading to\nmore efficient training and higher performance. Both intervention types\noutperform the baseline without interventions, highlighting the potential of\nLLM-mediated guidance to accelerate training and enhance MARL performance in\nchallenging environments.",
      "tldr_zh": "该研究探讨了将多智能体强化学习(MARL)与大型语言模型(LLM)引导干预结合，以提升复杂环境中智能体的学习效率和行为表现。研究比较了两种干预控制器：基于自然语言(NL)的控制器和基于规则(RB)的控制器，其中NL控制器通过LLM模拟类人干预，表现优于RB控制器。实验表明，早期干预能显著提升智能体的训练效率和性能，两种干预方式均优于无干预的基线，证明了LLM引导在加速MARL训练和提升性能方面的潜力。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "31 pages, 50 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13553v1",
      "published_date": "2025-03-16 20:16:13 UTC",
      "updated_date": "2025-03-16 20:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:21:55.110908"
    },
    {
      "arxiv_id": "2503.12637v1",
      "title": "Understanding Driver Cognition and Decision-Making Behaviors in High-Risk Scenarios: A Drift Diffusion Perspective",
      "title_zh": "理解高风险情境下的驾驶员认知与决策行为：漂移扩散模型视角",
      "authors": [
        "Heye Huang",
        "Zheng Li",
        "Hao Cheng",
        "Haoran Wang",
        "Junkai Jiang",
        "Xiaopeng Li",
        "Arkady Zgonnikov"
      ],
      "abstract": "Ensuring safe interactions between autonomous vehicles (AVs) and human\ndrivers in mixed traffic systems remains a major challenge, particularly in\ncomplex, high-risk scenarios. This paper presents a cognition-decision\nframework that integrates individual variability and commonalities in driver\nbehavior to quantify risk cognition and model dynamic decision-making. First, a\nrisk sensitivity model based on a multivariate Gaussian distribution is\ndeveloped to characterize individual differences in risk cognition. Then, a\ncognitive decision-making model based on the drift diffusion model (DDM) is\nintroduced to capture common decision-making mechanisms in high-risk\nenvironments. The DDM dynamically adjusts decision thresholds by integrating\ninitial bias, drift rate, and boundary parameters, adapting to variations in\nspeed, relative distance, and risk sensitivity to reflect diverse driving\nstyles and risk preferences. By simulating high-risk scenarios with lateral,\nlongitudinal, and multidimensional risk sources in a driving simulator, the\nproposed model accurately predicts cognitive responses and decision behaviors\nduring emergency maneuvers. Specifically, by incorporating driver-specific risk\nsensitivity, the model enables dynamic adjustments of key DDM parameters,\nallowing for personalized decision-making representations in diverse scenarios.\nComparative analysis with IDM, Gipps, and MOBIL demonstrates that DDM more\nprecisely captures human cognitive processes and adaptive decision-making in\nhigh-risk scenarios. These findings provide a theoretical basis for modeling\nhuman driving behavior and offer critical insights for enhancing AV-human\ninteraction in real-world traffic environments.",
      "tldr_zh": "该研究提出了一种基于漂移扩散模型(Drift Diffusion Model, DDM)的认知-决策框架，用于分析高风险场景下驾驶员的认知与决策行为。通过建立多元高斯分布的风险敏感性模型和动态调整决策阈值的DDM，该框架能够捕捉个体差异和共同决策机制，适应速度、相对距离和风险敏感性的变化。实验表明，DDM在模拟的高风险场景中能更准确地预测驾驶员的认知响应和紧急操作决策，优于传统模型(IDM、Gipps和MOBIL)。研究结果为建模人类驾驶行为提供了理论依据，并为提升自动驾驶车辆与人类驾驶员的交互提供了关键见解。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12637v1",
      "published_date": "2025-03-16 20:11:22 UTC",
      "updated_date": "2025-03-16 20:11:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:22:20.612052"
    },
    {
      "arxiv_id": "2503.12635v1",
      "title": "Hybrid Learners Do Not Forget: A Brain-Inspired Neuro-Symbolic Approach to Continual Learning",
      "title_zh": "混合学习者不会遗忘：一种受大脑启发的神经符号持续学习方法",
      "authors": [
        "Amin Banayeeanzade",
        "Mohammad Rostami"
      ],
      "abstract": "Continual learning is crucial for creating AI agents that can learn and\nimprove themselves autonomously. A primary challenge in continual learning is\nto learn new tasks without losing previously learned knowledge. Current\ncontinual learning methods primarily focus on enabling a neural network with\nmechanisms that mitigate forgetting effects. Inspired by the two distinct\nsystems in the human brain, System 1 and System 2, we propose a Neuro-Symbolic\nBrain-Inspired Continual Learning (NeSyBiCL) framework that incorporates two\nsubsystems to solve continual learning: A neural network model responsible for\nquickly adapting to the most recent task, together with a symbolic reasoner\nresponsible for retaining previously acquired knowledge from previous tasks.\nMoreover, we design an integration mechanism between these components to\nfacilitate knowledge transfer from the symbolic reasoner to the neural network.\nWe also introduce two compositional continual learning benchmarks and\ndemonstrate that NeSyBiCL is effective and leads to superior performance\ncompared to continual learning methods that merely rely on neural architectures\nto address forgetting.",
      "tldr_zh": "该研究提出了一种受人类大脑启发的神经符号混合持续学习框架NeSyBiCL，通过模拟大脑的System 1和System 2双系统机制来解决持续学习中的\"灾难性遗忘\"问题。该框架结合了快速适应新任务的神经网络和保留旧任务知识的符号推理器，并设计了二者间的知识迁移机制。实验表明，在两种组合式持续学习基准测试中，NeSyBiCL相比纯神经架构方法表现出更优性能，验证了神经符号混合方法在持续学习中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12635v1",
      "published_date": "2025-03-16 20:09:19 UTC",
      "updated_date": "2025-03-16 20:09:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:22:48.257359"
    },
    {
      "arxiv_id": "2503.12626v1",
      "title": "Automated Planning for Optimal Data Pipeline Instantiation",
      "title_zh": "自动化规划实现最优数据管道实例化",
      "authors": [
        "Leonardo Rosa Amado",
        "Adriano Vogel",
        "Dalvan Griebler",
        "Gabriel Paludo Licks",
        "Eric Simon",
        "Felipe Meneguzzi"
      ],
      "abstract": "Data pipeline frameworks provide abstractions for implementing sequences of\ndata-intensive transformation operators, automating the deployment and\nexecution of such transformations in a cluster. Deploying a data pipeline,\nhowever, requires computing resources to be allocated in a data center, ideally\nminimizing the overhead for communicating data and executing operators in the\npipeline while considering each operator's execution requirements. In this\npaper, we model the problem of optimal data pipeline deployment as planning\nwith action costs, where we propose heuristics aiming to minimize total\nexecution time. Experimental results indicate that the heuristics can\noutperform the baseline deployment and that a heuristic based on connections\noutperforms other strategies.",
      "tldr_zh": "本文提出了一种自动化规划方法，用于优化数据管道的实例化部署。通过将数据管道部署问题建模为带成本的动作规划，并设计启发式算法以最小化总执行时间。实验结果表明，基于连接的启发式算法优于其他策略，能够有效提升数据管道的执行效率。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12626v1",
      "published_date": "2025-03-16 19:43:12 UTC",
      "updated_date": "2025-03-16 19:43:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:22:50.934814"
    },
    {
      "arxiv_id": "2503.12623v1",
      "title": "MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network",
      "title_zh": "MAVEN：基于多模态注意力的效价-唤醒情感网络",
      "authors": [
        "Vrushank Ahire",
        "Kunal Shah",
        "Mudasir Nazir Khan",
        "Nikhil Pakhale",
        "Lownish Rai Sookha",
        "M. A. Ganaie",
        "Abhinav Dhall"
      ],
      "abstract": "This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal\nEmotion Network), a novel architecture for dynamic emotion recognition through\ndimensional modeling of affect. The model uniquely integrates visual, audio,\nand textual modalities via a bi-directional cross-modal attention mechanism\nwith six distinct attention pathways, enabling comprehensive interactions\nbetween all modality pairs. Our proposed approach employs modality-specific\nencoders to extract rich feature representations from synchronized video\nframes, audio segments, and transcripts. The architecture's novelty lies in its\ncross-modal enhancement strategy, where each modality representation is refined\nthrough weighted attention from other modalities, followed by self-attention\nrefinement through modality-specific encoders. Rather than directly predicting\nvalence-arousal values, MAVEN predicts emotions in a polar coordinate form,\naligning with psychological models of the emotion circumplex. Experimental\nevaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our\napproach, with performance measured using Concordance Correlation Coefficient\n(CCC). The multi-stage architecture demonstrates superior ability to capture\nthe complex, nuanced nature of emotional expressions in conversational videos,\nadvancing the state-of-the-art (SOTA) in continuous emotion recognition\nin-the-wild. Code can be found at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW.",
      "tldr_zh": "该研究提出MAVEN模型，一种基于多模态注意力机制（视觉、音频、文本）的动态情感识别架构，专门针对效价-唤醒（Valence-Arousal）维度建模。其核心创新在于双向跨模态注意力机制，通过六条独立注意力路径实现模态间全面交互，并采用极坐标形式预测情感以契合心理学的情感环状模型。在Aff-Wild2数据集上的实验表明，该模型通过分阶段架构能有效捕捉对话视频中复杂细微的情感表达，在连续情感识别任务中达到最先进性能（以Concordance Correlation Coefficient衡量）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12623v1",
      "published_date": "2025-03-16 19:32:32 UTC",
      "updated_date": "2025-03-16 19:32:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:23:18.791315"
    },
    {
      "arxiv_id": "2503.16518v1",
      "title": "Advancing Human-Machine Teaming: Concepts, Challenges, and Applications",
      "title_zh": "推进人机协作：概念、挑战与应用",
      "authors": [
        "Dian Chen",
        "Han Jun Yoon",
        "Zelin Wan",
        "Nithin Alluru",
        "Sang Won Lee",
        "Richard He",
        "Terrence J. Moore",
        "Frederica F. Nelson",
        "Sunghyun Yoon",
        "Hyuk Lim",
        "Dan Dongseong Kim",
        "Jin-Hee Cho"
      ],
      "abstract": "Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems.",
      "tldr_zh": "本文系统综述了人机协作(HMT)领域，通过整合强化学习、实例学习等理论模型和多学科方法，提出了全面的HMT分类体系。研究重点关注团队认知、伦理AI和多模态交互等前沿方向，揭示了可解释性、角色分配等关键挑战。作者提出未来应发展跨领域适应、可信AI等研究方向，为构建弹性、伦理且可扩展的HMT系统奠定基础，特别适用于国防、医疗和自主系统等应用场景。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16518v1",
      "published_date": "2025-03-16 19:32:17 UTC",
      "updated_date": "2025-03-16 19:32:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:23:32.249108"
    },
    {
      "arxiv_id": "2503.12617v1",
      "title": "Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance",
      "title_zh": "扩展语义类别：探究对视觉 Transformer 标注性能的影响",
      "authors": [
        "Anthony Lamelas",
        "Harrison Muchnic"
      ],
      "abstract": "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.",
      "tldr_zh": "本研究探讨了语义类别扩展对视觉Transformer(ViT)图像分类性能的影响。实验基于Jina AI提供的CLIP模型，通过指数级增加语义等价类别数量测试发现：ViT的标注准确率会随着类别扩展而提升，但超过临界阈值后效益会递减甚至下降。研究采用多样化图像数据集，通过定制Python函数评估模型性能，为ViT的类别标注策略优化提供了重要参考界限。结果表明，语义扩展虽能短期提升模型表现，但存在明显的性能饱和点。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 7 figures, submitted to CVPR (feedback pending)",
      "pdf_url": "http://arxiv.org/pdf/2503.12617v1",
      "published_date": "2025-03-16 19:14:21 UTC",
      "updated_date": "2025-03-16 19:14:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:24:03.711033"
    },
    {
      "arxiv_id": "2503.12613v1",
      "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies",
      "title_zh": "协商式对齐：通过接纳分歧实现更公平的结果——来自城市研究的启示",
      "authors": [
        "Rashid Mushkani",
        "Hugo Berard",
        "Shin Koseki"
      ],
      "abstract": "Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design.",
      "tldr_zh": "该研究提出\"协商对齐\"(negotiative alignment)创新框架，通过保留和分析群体分歧来提升城市设计的公平性。基于蒙特利尔35位不同背景居民（包括轮椅使用者、老年人和LGBTQIA2+群体）的实证研究发现，城市评估中的意见分歧反映了结构性不平等和文化价值观差异。该AI框架采用多智能体协商机制动态整合不同利益相关者诉求，避免少数群体观点被主流共识掩盖。研究表明，与传统追求平均共识的方法相比，这种拥抱分歧的范式能产生更公平、更具包容性的城市设计决策。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12613v1",
      "published_date": "2025-03-16 18:55:54 UTC",
      "updated_date": "2025-03-16 18:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:24:14.109949"
    },
    {
      "arxiv_id": "2503.12595v1",
      "title": "Point Cloud Based Scene Segmentation: A Survey",
      "title_zh": "基于点云的场景分割：综述",
      "authors": [
        "Dan Halperin",
        "Niklas Eisl"
      ],
      "abstract": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency.",
      "tldr_zh": "这篇综述论文系统梳理了自动驾驶领域基于点云(Point Cloud)的场景分割技术现状。研究指出，相比3D目标检测，点云语义分割能对每个点赋予标签，为自动驾驶提供更精细的环境感知。作者将现有方法分为三类：基于投影(projection-based)、基于3D(3D-based)和混合方法(hybrid)，并对比了各类方法在分割精度和效率上的表现。论文还总结了该领域常用数据集，并强调在真实数据不足时合成数据的重要性，为未来研究提供了方向性指导。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12595v1",
      "published_date": "2025-03-16 18:02:41 UTC",
      "updated_date": "2025-03-16 18:02:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:24:44.149502"
    },
    {
      "arxiv_id": "2503.12593v1",
      "title": "Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens",
      "title_zh": "基于傅里叶的三维多级Transformer用于多细胞样本像差校正",
      "authors": [
        "Thayer Alshaabi",
        "Daniel E. Milkie",
        "Gaoxiang Liu",
        "Cyna Shirazinejad",
        "Jason L. Hong",
        "Kemal Achour",
        "Frederik Görlitz",
        "Ana Milunovic-Jevtic",
        "Cat Simmons",
        "Ibrahim S. Abuzahriyeh",
        "Erin Hong",
        "Samara Erin Williams",
        "Nathanael Harrison",
        "Evan Huang",
        "Eun Seok Bae",
        "Alison N. Killilea",
        "David G. Drubin",
        "Ian A. Swinburne",
        "Srigokul Upadhyayula",
        "Eric Betzig"
      ],
      "abstract": "High-resolution tissue imaging is often compromised by sample-induced optical\naberrations that degrade resolution and contrast. While wavefront sensor-based\nadaptive optics (AO) can measure these aberrations, such hardware solutions are\ntypically complex, expensive to implement, and slow when serially mapping\nspatially varying aberrations across large fields of view. Here, we introduce\nAOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine\nlearning-based aberration sensing framework built around a 3D multistage Vision\nTransformer that operates on Fourier domain embeddings. AOViFT infers\naberrations and restores diffraction-limited performance in puncta-labeled\nspecimens with substantially reduced computational cost, training time, and\nmemory footprint compared to conventional architectures or real-space networks.\nWe validated AOViFT on live gene-edited zebrafish embryos, demonstrating its\nability to correct spatially varying aberrations using either a deformable\nmirror or post-acquisition deconvolution. By eliminating the need for the guide\nstar and wavefront sensing hardware and simplifying the experimental workflow,\nAOViFT lowers technical barriers for high-resolution volumetric microscopy\nacross diverse biological samples.",
      "tldr_zh": "该研究提出AOViFT（自适应光学傅里叶视觉Transformer），一种基于3D多阶段Vision Transformer的机器学习框架，用于校正多细胞样本中的光学像差。该方法通过处理傅里叶域特征，相比传统架构大幅降低了计算成本、训练时间和内存占用，可在斑马鱼胚胎等活体样本中实现衍射极限性能的像差校正。AOViFT无需波前传感硬件和引导星，显著简化了高分辨率体积显微镜的工作流程。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG",
        "physics.bio-ph",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "52 pages, 6 figures, 23 si figures, 8 si tables",
      "pdf_url": "http://arxiv.org/pdf/2503.12593v1",
      "published_date": "2025-03-16 17:59:20 UTC",
      "updated_date": "2025-03-16 17:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:24:59.509751"
    },
    {
      "arxiv_id": "2503.12592v1",
      "title": "MoECollab: Democratizing LLM Development Through Collaborative Mixture of Experts",
      "title_zh": "MoECollab：通过协作式专家混合架构实现大语言模型开发的民主化",
      "authors": [
        "Harshit"
      ],
      "abstract": "Large Language Model (LLM) development has become increasingly centralized,\nlimiting participation to well-resourced organizations. This paper introduces\nMoECollab, a novel framework leveraging Mixture of Experts (MoE) architecture\nto enable distributed, collaborative LLM development. By decomposing monolithic\nmodels into specialized expert modules coordinated by a trainable gating\nnetwork, our framework allows diverse contributors to participate regardless of\ncomputational resources. We provide a complete technical implementation with\nmathematical foundations for expert dynamics, gating mechanisms, and\nintegration strategies. Experiments on multiple datasets demonstrate that our\napproach achieves accuracy improvements of 3-7% over baseline models while\nreducing computational requirements by 34%. Expert specialization yields\nsignificant domain-specific gains, with improvements from 51% to 88% F1 score\nin general classification and from 23% to 44% accuracy in news categorization.\nWe formalize the routing entropy optimization problem and demonstrate how\nproper regularization techniques lead to 14% higher expert utilization rates.\nThese results validate MoECollab as an effective approach for democratizing LLM\ndevelopment through architecturally-supported collaboration.",
      "tldr_zh": "该研究提出MoECollab框架，采用混合专家(MoE)架构实现分布式协作的大语言模型(LLM)开发，打破资源垄断。通过将单一模型分解为由可训练门控网络协调的专家模块，使不同计算资源的贡献者都能参与开发。实验表明，该方法在多个数据集上比基线模型准确率提升3-7%，同时降低34%计算需求，专家专业化使特定领域性能显著提升（如新闻分类准确率从23%提升至44%）。通过路由熵优化和正则化技术，专家利用率提高14%，为去中心化的LLM开发提供了可行方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12592v1",
      "published_date": "2025-03-16 17:52:40 UTC",
      "updated_date": "2025-03-16 17:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:25:21.521018"
    },
    {
      "arxiv_id": "2503.14530v2",
      "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders",
      "title_zh": "SAUCE：基于稀疏自编码器的视觉语言模型选择性概念遗忘",
      "authors": [
        "Qing Li",
        "Jiahui Geng",
        "Derui Zhu",
        "Fengyu Cai",
        "Chenyang Lyu",
        "Fakhri Karray"
      ],
      "abstract": "Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs.",
      "tldr_zh": "该研究提出了SAUCE方法，利用稀疏自编码器(SAEs)实现视觉语言模型(VLMs)中的细粒度概念遗忘。该方法通过训练SAEs捕捉高维稀疏特征，选择性修改目标概念相关特征以抑制特定概念，同时保留无关信息。实验表明，SAUCE在LLaVA和LLaMA-3等模型上对60个具体和抽象概念的遗忘质量优于现有方法18.04%，且保持了模型实用性。研究还验证了该方法对抗对抗攻击的鲁棒性、跨模型迁移性以及处理多任务遗忘的可扩展性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "More comparative experiments are needed",
      "pdf_url": "http://arxiv.org/pdf/2503.14530v2",
      "published_date": "2025-03-16 17:32:23 UTC",
      "updated_date": "2025-03-20 05:47:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:25:33.132675"
    },
    {
      "arxiv_id": "2503.12575v1",
      "title": "BalancedDPO: Adaptive Multi-Metric Alignment",
      "title_zh": "BalancedDPO：面向多指标自适应的对齐方法",
      "authors": [
        "Dipesh Tamboli",
        "Souradip Chakraborty",
        "Aditya Malusare",
        "Biplab Banerjee",
        "Amrit Singh Bedi",
        "Vaneet Aggarwal"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.",
      "tldr_zh": "本文提出BalancedDPO，一种基于Direct Preference Optimization（DPO）的新型多指标对齐方法，用于改进文本到图像（T2I）扩散模型。该方法创新性地在偏好分布空间整合人类偏好、CLIP分数和美学质量等多项指标，避免了传统奖励混合方法的局限性，同时保持了标准DPO流程的简洁性。实验表明，BalancedDPO在Pick-a-Pic、PartiPrompt和HPD数据集上均取得最优性能，相比DiffusionDPO在三个基准上的平均胜率分别提升15%、7.1%和10.3%，实现了跨多指标的稳健对齐。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12575v1",
      "published_date": "2025-03-16 17:06:00 UTC",
      "updated_date": "2025-03-16 17:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:26:09.679685"
    },
    {
      "arxiv_id": "2503.12572v1",
      "title": "Deblur Gaussian Splatting SLAM",
      "title_zh": "去模糊高斯溅射SLAM",
      "authors": [
        "Francesco Girlanda",
        "Denys Rozumnyi",
        "Marc Pollefeys",
        "Martin R. Oswald"
      ],
      "abstract": "We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp\nreconstructions from motion-blurred inputs. The proposed method bridges the\nstrengths of both frame-to-frame and frame-to-model approaches to model\nsub-frame camera trajectories that lead to high-fidelity reconstructions in\nmotion-blurred settings. Moreover, our pipeline incorporates techniques such as\nonline loop closure and global bundle adjustment to achieve a dense and precise\nglobal trajectory. We model the physical image formation process of\nmotion-blurred images and minimize the error between the observed blurry images\nand rendered blurry images obtained by averaging sharp virtual sub-frame\nimages. Additionally, by utilizing a monocular depth estimator alongside the\nonline deformation of Gaussians, we ensure precise mapping and enhanced image\ndeblurring. The proposed SLAM pipeline integrates all these components to\nimprove the results. We achieve state-of-the-art results for sharp map\nestimation and sub-frame trajectory recovery both on synthetic and real-world\nblurry input data.",
      "tldr_zh": "本文提出Deblur-SLAM，一种新颖的RGB SLAM系统，能够从运动模糊输入中重建清晰场景。该方法结合帧间匹配与帧-模型匹配的优势，通过建模亚帧级相机轨迹和物理成像过程，有效解决运动模糊问题。系统采用在线闭环检测、全局光束法平差等技术，并创新性地结合单目深度估计与高斯点云在线形变，在合成和真实模糊数据上都实现了最先进的清晰地图重建与亚帧轨迹恢复效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12572v1",
      "published_date": "2025-03-16 16:59:51 UTC",
      "updated_date": "2025-03-16 16:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:26:25.595865"
    },
    {
      "arxiv_id": "2503.12556v1",
      "title": "From Guessing to Asking: An Approach to Resolving the Persona Knowledge Gap in LLMs during Multi-Turn Conversations",
      "title_zh": "从猜测到询问：解决多轮对话中大语言模型角色知识差距的方法",
      "authors": [
        "Sarvesh Baskar",
        "Tanmay Tulsidas Verelakar",
        "Srinivasan Parthasarathy",
        "Manas Gaur"
      ],
      "abstract": "In multi-turn dialogues, large language models (LLM) face a critical\nchallenge of ensuring coherence while adapting to user-specific information.\nThis study introduces the persona knowledge gap, the discrepancy between a\nmodel's internal understanding and the knowledge required for coherent,\npersonalized conversations. While prior research has recognized these gaps,\ncomputational methods for their identification and resolution remain\nunderexplored. We propose Conversation Preference Elicitation and\nRecommendation (CPER), a novel framework that dynamically detects and resolves\npersona knowledge gaps using intrinsic uncertainty quantification and\nfeedback-driven refinement. CPER consists of three key modules: a Contextual\nUnderstanding Module for preference extraction, a Dynamic Feedback Module for\nmeasuring uncertainty and refining persona alignment, and a Persona-Driven\nResponse Generation module for adapting responses based on accumulated user\ncontext. We evaluate CPER on two real-world datasets: CCPE-M for preferential\nmovie recommendations and ESConv for mental health support. Using A/B testing,\nhuman evaluators preferred CPER's responses 42% more often than baseline models\nin CCPE-M and 27% more often in ESConv. A qualitative human evaluation confirms\nthat CPER's responses are preferred for maintaining contextual relevance and\ncoherence, particularly in longer (12+ turn) conversations.",
      "tldr_zh": "该研究提出了Conversation Preference Elicitation and Recommendation (CPER)框架，旨在解决大语言模型(LLM)在多轮对话中面临的persona knowledge gap问题。CPER通过三个核心模块动态检测和解决这一差距：Contextual Understanding Module提取用户偏好，Dynamic Feedback Module量化不确定性并优化persona对齐，Persona-Driven Response Generation Module根据累积的用户上下文生成适应性响应。实验表明，CPER在电影推荐和心理健康支持场景中的表现优于基线模型，尤其在长对话中更能保持上下文相关性和连贯性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 1 Figure, Oral Presentation at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12556v1",
      "published_date": "2025-03-16 15:55:29 UTC",
      "updated_date": "2025-03-16 15:55:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:27:35.626962"
    },
    {
      "arxiv_id": "2503.12549v1",
      "title": "Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting",
      "title_zh": "《利用基于自编码器的点云修复技术抓取部分遮挡物体》",
      "authors": [
        "Alexander Koebler",
        "Ralf Gross",
        "Florian Buettner",
        "Ingo Thon"
      ],
      "abstract": "Flexible industrial production systems will play a central role in the future\nof manufacturing due to higher product individualization and customization. A\nkey component in such systems is the robotic grasping of known or unknown\nobjects in random positions. Real-world applications often come with challenges\nthat might not be considered in grasping solutions tested in simulation or lab\nsettings. Partial occlusion of the target object is the most prominent.\nExamples of occlusion can be supporting structures in the camera's field of\nview, sensor imprecision, or parts occluding each other due to the production\nprocess. In all these cases, the resulting lack of information leads to\nshortcomings in calculating grasping points. In this paper, we present an\nalgorithm to reconstruct the missing information. Our inpainting solution\nfacilitates the real-world utilization of robust object matching approaches for\ngrasping point calculation. We demonstrate the benefit of our solution by\nenabling an existing grasping system embedded in a real-world industrial\napplication to handle occlusions in the input. With our solution, we\ndrastically decrease the number of objects discarded by the process.",
      "tldr_zh": "该研究提出了一种基于自动编码器的点云修复方法，用于解决工业机器人抓取部分遮挡物体的难题。通过重建被遮挡物体缺失的三维点云信息，该方法显著提升了现有抓取系统在真实工业场景中的处理能力。实验证明，该方案大幅降低了因遮挡问题导致的物体丢弃率，为柔性制造系统中的可靠抓取操作提供了有效解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Published at ECML PKDD 2022",
      "pdf_url": "http://arxiv.org/pdf/2503.12549v1",
      "published_date": "2025-03-16 15:38:08 UTC",
      "updated_date": "2025-03-16 15:38:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:27:04.125671"
    },
    {
      "arxiv_id": "2503.13551v2",
      "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models",
      "title_zh": "迈向分层多步奖励模型：提升大型语言模型的推理能力",
      "authors": [
        "Teng Wang",
        "Zhangyi Jiang",
        "Zhenqi He",
        "Wenhan Yang",
        "Yanan Zheng",
        "Zeyu Li",
        "Zifan He",
        "Shenyang Tong",
        "Hailei Gong"
      ],
      "abstract": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n//github.com/tengwang0318/hierarchial_reward_model.",
      "tldr_zh": "该研究提出了分层奖励模型(HRM)，通过细粒度和粗粒度评估来改进大型语言模型(LLMs)的推理能力，解决了传统过程奖励模型(PRM)存在的奖励欺骗问题。HRM能更好地评估推理连贯性和自我反思，特别是在前序推理步骤错误时表现更优。研究还提出了分层节点压缩(HNC)数据增强策略，通过合并树结构中的连续推理节点来高效生成多样化训练数据。实验表明，HRM配合HNC在PRM800K数据集上表现出更高的稳定性和可靠性，在MATH500和GSM8K跨领域评估中也展现了优异的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13551v2",
      "published_date": "2025-03-16 15:18:40 UTC",
      "updated_date": "2025-03-19 15:43:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:27:33.742203"
    },
    {
      "arxiv_id": "2503.12532v2",
      "title": "STEVE: A Step Verification Pipeline for Computer-use Agent Training",
      "title_zh": "STEVE：面向计算机使用智能体训练的步骤验证流程",
      "authors": [
        "Fanbin Lu",
        "Zhisheng Zhong",
        "Ziqin Wei",
        "Shu Liu",
        "Chi-Wing Fu",
        "Jiaya Jia"
      ],
      "abstract": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
      "tldr_zh": "该研究提出了STEVE，一种用于训练计算机使用智能体的步骤验证管道，旨在解决行为克隆训练中高质量轨迹数据需求巨大的问题。该方法首先构建大规模指令集，并使用次优智能体收集轨迹数据，然后利用GPT-4对每一步动作前后的屏幕进行验证，赋予二元标签。最后，采用Kahneman和Tversky优化方法，从二元标签中优化智能体。实验表明，STEVE通过利用轨迹中的正负动作，显著优于监督微调，并能高效训练7B视觉语言模型，在WinAgentArena桌面环境中取得领先性能，同时降低成本。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12532v2",
      "published_date": "2025-03-16 14:53:43 UTC",
      "updated_date": "2025-03-24 16:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:27:45.024078"
    },
    {
      "arxiv_id": "2503.16517v1",
      "title": "From G-Factor to A-Factor: Establishing a Psychometric Framework for AI Literacy",
      "title_zh": "从G因子到A因子：构建人工智能素养的心理测量框架",
      "authors": [
        "Ning Li",
        "Wenming Deng",
        "Jiatan Chen"
      ],
      "abstract": "This research addresses the growing need to measure and understand AI\nliteracy in the context of generative AI technologies. Through three sequential\nstudies involving a total of 517 participants, we establish AI literacy as a\ncoherent, measurable construct with significant implications for education,\nworkforce development, and social equity. Study 1 (N=85) revealed a dominant\nlatent factor - termed the \"A-factor\" - that accounts for 44.16% of variance\nacross diverse AI interaction tasks. Study 2 (N=286) refined the measurement\ntool by examining four key dimensions of AI literacy: communication\neffectiveness, creative idea generation, content evaluation, and step-by-step\ncollaboration, resulting in an 18-item assessment battery. Study 3 (N=146)\nvalidated this instrument in a controlled laboratory setting, demonstrating its\npredictive validity for real-world task performance. Results indicate that AI\nliteracy significantly predicts performance on complex, language-based creative\ntasks but shows domain specificity in its predictive power. Additionally,\nregression analyses identified several significant predictors of AI literacy,\nincluding cognitive abilities (IQ), educational background, prior AI\nexperience, and training history. The multidimensional nature of AI literacy\nand its distinct factor structure provide evidence that effective human-AI\ncollaboration requires a combination of general and specialized abilities.\nThese findings contribute to theoretical frameworks of human-AI collaboration\nwhile offering practical guidance for developing targeted educational\ninterventions to promote equitable access to the benefits of generative AI\ntechnologies.",
      "tldr_zh": "该研究通过三项涉及517名参与者的序列研究，建立了AI素养的心理测量框架。研究发现，AI素养由“A-factor”主导，解释了44.16%的方差，并包含四个关键维度：沟通有效性、创意生成、内容评估和逐步协作。研究开发了一个18项的评估工具，并验证了其对实际任务表现的预测有效性。结果表明，AI素养显著预测语言类创意任务表现，并受到认知能力、教育背景、AI经验和培训历史的影响。该研究为人类-AI协作的理论框架提供了支持，并为促进生成式AI技术公平获取的教育干预提供了实践指导。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16517v1",
      "published_date": "2025-03-16 14:51:48 UTC",
      "updated_date": "2025-03-16 14:51:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:28:13.386114"
    },
    {
      "arxiv_id": "2503.12525v1",
      "title": "HyConEx: Hypernetwork classifier with counterfactual explanations",
      "title_zh": "HyConEx：具备反事实解释能力的超网络分类器",
      "authors": [
        "Patryk Marszałek",
        "Ulvi Movsum-zada",
        "Oleksii Furman",
        "Kamil Książek",
        "Przemysław Spurek",
        "Marek Śmieja"
      ],
      "abstract": "In recent years, there has been a growing interest in explainable AI methods.\nWe want not only to make accurate predictions using sophisticated neural\nnetworks but also to understand what the model's decision is based on. One of\nthe fundamental levels of interpretability is to provide counterfactual\nexamples explaining the rationale behind the decision and identifying which\nfeatures, and to what extent, must be modified to alter the model's outcome. To\naddress these requirements, we introduce HyConEx, a classification model based\non deep hypernetworks specifically designed for tabular data. Owing to its\nunique architecture, HyConEx not only provides class predictions but also\ndelivers local interpretations for individual data samples in the form of\ncounterfactual examples that steer a given sample toward an alternative class.\nWhile many explainable methods generated counterfactuals for external models,\nthere have been no interpretable classifiers simultaneously producing\ncounterfactual samples so far. HyConEx achieves competitive performance on\nseveral metrics assessing classification accuracy and fulfilling the criteria\nof a proper counterfactual attack. This makes HyConEx a distinctive deep\nlearning model, which combines predictions and explainers as an all-in-one\nneural network. The code is available at https://github.com/gmum/HyConEx.",
      "tldr_zh": "该研究提出了HyConEx，一种专为表格数据设计的超网络分类器，能够同时进行预测和生成反事实解释。该模型通过独特的深度超网络架构，不仅输出分类结果，还能为单个样本提供局部解释，展示如何修改特征才能改变分类决策。实验表明，HyConEx在分类准确性和反事实攻击标准方面均表现优异，成为首个将预测器和解释器合二为一的深度学习模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12525v1",
      "published_date": "2025-03-16 14:39:36 UTC",
      "updated_date": "2025-03-16 14:39:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:28:43.428686"
    },
    {
      "arxiv_id": "2503.12524v2",
      "title": "EXAONE Deep: Reasoning Enhanced Language Models",
      "title_zh": "EXAONE Deep：推理增强型语言模型",
      "authors": [
        "LG AI Research",
        "Kyunghoon Bae",
        "Eunbi Choi",
        "Kibong Choi",
        "Stanley Jungkyu Choi",
        "Yemuk Choi",
        "Seokhee Hong",
        "Junwon Hwang",
        "Hyojin Jeon",
        "Kijeong Jeon",
        "Gerrard Jeongwon Jo",
        "Hyunjik Jo",
        "Jiyeon Jung",
        "Hyosang Kim",
        "Joonkee Kim",
        "Seonghwan Kim",
        "Soyeon Kim",
        "Sunkyoung Kim",
        "Yireun Kim",
        "Yongil Kim",
        "Youchul Kim",
        "Edward Hwayoung Lee",
        "Haeju Lee",
        "Honglak Lee",
        "Jinsik Lee",
        "Kyungmin Lee",
        "Sangha Park",
        "Yongmin Park",
        "Sihoon Yang",
        "Heuiyeen Yeen",
        "Sihyuk Yi",
        "Hyeongu Yun"
      ],
      "abstract": "We present EXAONE Deep series, which exhibits superior capabilities in\nvarious reasoning tasks, including math and coding benchmarks. We train our\nmodels mainly on the reasoning-specialized dataset that incorporates long\nstreams of thought processes. Evaluation results show that our smaller models,\nEXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while\nthe largest model, EXAONE Deep 32B, demonstrates competitive performance\nagainst leading open-weight models. All EXAONE Deep models are openly available\nfor research purposes and can be downloaded from\nhttps://huggingface.co/LGAI-EXAONE",
      "tldr_zh": "该研究推出了EXAONE Deep系列语言模型，专注于增强推理能力。通过训练基于长链思维过程的专用数据集，该系列模型在数学和编程等推理任务中表现出色。实验表明，较小的EXAONE Deep 2.4B和7.8B模型在同等规模模型中表现优异，而最大的EXAONE Deep 32B模型与领先的开源模型竞争激烈。该系列模型已开源，可供研究使用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2412.04862,\n  arXiv:2408.03541",
      "pdf_url": "http://arxiv.org/pdf/2503.12524v2",
      "published_date": "2025-03-16 14:39:33 UTC",
      "updated_date": "2025-03-19 07:09:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:28:43.168084"
    },
    {
      "arxiv_id": "2503.13550v1",
      "title": "Towards Privacy-Preserving Data-Driven Education: The Potential of Federated Learning",
      "title_zh": "迈向隐私保护的教育数据驱动：联邦学习的潜力",
      "authors": [
        "Mohammad Khalil",
        "Ronas Shakya",
        "Qinyi Liu"
      ],
      "abstract": "The increasing adoption of data-driven applications in education such as in\nlearning analytics and AI in education has raised significant privacy and data\nprotection concerns. While these challenges have been widely discussed in\nprevious works, there are still limited practical solutions. Federated learning\nhas recently been discoursed as a promising privacy-preserving technique, yet\nits application in education remains scarce. This paper presents an\nexperimental evaluation of federated learning for educational data prediction,\ncomparing its performance to traditional non-federated approaches. Our findings\nindicate that federated learning achieves comparable predictive accuracy.\nFurthermore, under adversarial attacks, federated learning demonstrates greater\nresilience compared to non-federated settings. We summarise that our results\nreinforce the value of federated learning as a potential approach for balancing\npredictive performance and privacy in educational contexts.",
      "tldr_zh": "本文探讨了联邦学习(Federated Learning)在教育数据隐私保护中的应用潜力。研究表明，联邦学习在教育数据预测任务中与传统非联邦方法具有相当的准确性，同时在对抗攻击下展现出更强的鲁棒性。该工作验证了联邦学习可作为平衡教育领域预测性能与隐私保护的有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13550v1",
      "published_date": "2025-03-16 14:37:32 UTC",
      "updated_date": "2025-03-16 14:37:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:29:02.666358"
    },
    {
      "arxiv_id": "2503.13549v1",
      "title": "A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks",
      "title_zh": "ChatGPT 与 DeepSeek 在编程任务解决中的对决",
      "authors": [
        "Ronas Shakya",
        "Farhad Vadiee",
        "Mohammad Khalil"
      ],
      "abstract": "The advancement of large language models (LLMs) has created a competitive\nlandscape for AI-assisted programming tools. This study evaluates two leading\nmodels: ChatGPT 03-mini and DeepSeek-R1 on their ability to solve competitive\nprogramming tasks from Codeforces. Using 29 programming tasks of three levels\nof easy, medium, and hard difficulty, we assessed the outcome of both models by\ntheir accepted solutions, memory efficiency, and runtime performance. Our\nresults indicate that while both models perform similarly on easy tasks,\nChatGPT outperforms DeepSeek-R1 on medium-difficulty tasks, achieving a 54.5%\nsuccess rate compared to DeepSeek 18.1%. Both models struggled with hard tasks,\nthus highlighting some ongoing challenges LLMs face in handling highly complex\nprogramming problems. These findings highlight key differences in both model\ncapabilities and their computational power, offering valuable insights for\ndevelopers and researchers working to advance AI-driven programming tools.",
      "tldr_zh": "本研究对比了ChatGPT 03-mini和DeepSeek-R1两款大语言模型(LLMs)在Codeforces编程竞赛题目上的表现。实验使用29道不同难度题目评估发现：在简单题上两者表现相近，但在中等难度题上ChatGPT以54.5%的通过率显著优于DeepSeek-R1的18.1%；两种模型面对高难度题目时都表现不佳，突显了LLMs处理复杂编程问题的现存挑战。该研究为AI编程辅助工具的开发者提供了重要的性能基准参考。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13549v1",
      "published_date": "2025-03-16 14:35:36 UTC",
      "updated_date": "2025-03-16 14:35:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:29:37.455584"
    },
    {
      "arxiv_id": "2503.12511v2",
      "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
      "title_zh": "基于静态分析的LLM驱动多阶段C到Rust翻译方法",
      "authors": [
        "Tianyang Zhou",
        "Haowen Lin",
        "Somesh Jha",
        "Mihai Christodorescu",
        "Kirill Levchenko",
        "Varun Chandrasekaran"
      ],
      "abstract": "Translating software written in legacy languages to modern languages, such as\nC to Rust, has significant benefits in improving memory safety while\nmaintaining high performance. However, manual translation is cumbersome,\nerror-prone, and produces unidiomatic code. Large language models (LLMs) have\ndemonstrated promise in producing idiomatic translations, but offer no\ncorrectness guarantees as they lack the ability to capture all the semantics\ndifferences between the source and target languages. To resolve this issue, we\npropose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a\ntwo-step translation methodology: an \"unidiomatic\" step to translate C into\nRust while preserving semantics, and an \"idiomatic\" step to refine the code to\nfollow Rust's semantic standards. SACTOR utilizes information provided by\nstatic analysis of the source C program to address challenges such as pointer\nsemantics and dependency resolution. To validate the correctness of the\ntranslated result from each step, we use end-to-end testing via the foreign\nfunction interface to embed our translated code segment into the original code.\nWe evaluate the translation of 200 programs from two datasets and two case\nstudies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0\nFlash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that\nSACTOR achieves high correctness and improved idiomaticity, with the\nbest-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5,\nDeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while\nproducing more natural and Rust-compliant translations compared to existing\nmethods.",
      "tldr_zh": "该研究提出了SACTOR，一种基于静态分析和LLM的两阶段C语言转Rust工具，通过\"非惯用\"和\"惯用\"两个翻译步骤提升代码安全性与可读性。该方法利用C程序的静态分析信息解决指针语义等跨语言差异问题，并通过外部函数接口(FFI)进行端到端测试验证翻译正确性。在200个程序的测试中，最佳模型(DeepSeek-R1)正确率达93%，相比现有方法产生更符合Rust语义规范的代码翻译。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "22 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12511v2",
      "published_date": "2025-03-16 14:05:26 UTC",
      "updated_date": "2025-03-18 04:17:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:29:53.315809"
    },
    {
      "arxiv_id": "2503.12509v1",
      "title": "A Reservoir-based Model for Human-like Perception of Complex Rhythm Pattern",
      "title_zh": "基于储层计算的人类复杂节奏感知模型",
      "authors": [
        "Zhongju Yuan",
        "Geraint Wiggins",
        "Dick Botteldooren"
      ],
      "abstract": "Rhythm is a fundamental aspect of human behaviour, present from infancy and\ndeeply embedded in cultural practices. Rhythm anticipation is a spontaneous\ncognitive process that typically occurs before the onset of actual beats. While\nmost research in both neuroscience and artificial intelligence has focused on\nmetronome-based rhythm tasks, studies investigating the perception of complex\nmusical rhythm patterns remain limited. To address this gap, we propose a\nhierarchical oscillator-based model to better understand the perception of\ncomplex musical rhythms in biological systems. The model consists of two types\nof coupled neurons that generate oscillations, with different layers tuned to\nrespond to distinct perception levels. We evaluate the model using several\nrepresentative rhythm patterns spanning the upper, middle, and lower bounds of\nhuman musical perception. Our findings demonstrate that, while maintaining a\nhigh degree of synchronization accuracy, the model exhibits human-like rhythmic\nbehaviours. Additionally, the beta band neuronal activity in the model mirrors\npatterns observed in the human brain, further validating the biological\nplausibility of the approach.",
      "tldr_zh": "本文提出了一种基于层次化振荡器（hierarchical oscillator）的模型，用于模拟人类对复杂音乐节奏的感知。该模型由两种耦合神经元组成，能够生成振荡，并通过不同层次响应不同的感知水平。实验表明，该模型在保持高同步精度的同时，表现出与人类相似的节奏行为，并且其β波段神经元活动模式与人类大脑中的观察结果一致，验证了模型的生物学合理性。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12509v1",
      "published_date": "2025-03-16 14:02:42 UTC",
      "updated_date": "2025-03-16 14:02:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:30:12.896847"
    },
    {
      "arxiv_id": "2503.13548v1",
      "title": "Fuzzy Rule-based Differentiable Representation Learning",
      "title_zh": "基于模糊规则的可微表示学习",
      "authors": [
        "Wei Zhang",
        "Zhaohong Deng",
        "Guanjin Wang",
        "Kup-Sze Choi"
      ],
      "abstract": "Representation learning has emerged as a crucial focus in machine and deep\nlearning, involving the extraction of meaningful and useful features and\npatterns from the input data, thereby enhancing the performance of various\ndownstream tasks such as classification, clustering, and prediction. Current\nmainstream representation learning methods primarily rely on non-linear data\nmining techniques such as kernel methods and deep neural networks to extract\nabstract knowledge from complex datasets. However, most of these methods are\nblack-box, lacking transparency and interpretability in the learning process,\nwhich constrains their practical utility. To this end, this paper introduces a\nnovel representation learning method grounded in an interpretable fuzzy\nrule-based model. Specifically, it is built upon the Takagi-Sugeno-Kang fuzzy\nsystem (TSK-FS) to initially map input data to a high-dimensional fuzzy feature\nspace through the antecedent part of the TSK-FS. Subsequently, a novel\ndifferentiable optimization method is proposed for the consequence part\nlearning which can preserve the model's interpretability and transparency while\nfurther exploring the nonlinear relationships within the data. This\noptimization method retains the essence of traditional optimization, with\ncertain parts of the process parameterized corresponding differentiable modules\nconstructed, and a deep optimization process implemented. Consequently, this\nmethod not only enhances the model's performance but also ensures its\ninterpretability. Moreover, a second-order geometry preservation method is\nintroduced to further improve the robustness of the proposed method. Extensive\nexperiments conducted on various benchmark datasets validate the superiority of\nthe proposed method, highlighting its potential for advancing representation\nlearning methodologies.",
      "tldr_zh": "本文提出了一种基于模糊规则的可微分表征学习方法，通过改进Takagi-Sugeno-Kang模糊系统(TSK-FS)构建高维模糊特征空间。该方法创新性地设计了可微分优化模块用于后件学习，在保持模型可解释性的同时有效挖掘数据非线性关系，并引入二阶几何保持方法提升鲁棒性。实验表明，这种透明且高性能的模糊规则表征学习方法在多个基准数据集上优于现有技术，为可解释AI提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13548v1",
      "published_date": "2025-03-16 14:00:34 UTC",
      "updated_date": "2025-03-16 14:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:30:47.418055"
    },
    {
      "arxiv_id": "2503.12506v1",
      "title": "A General Close-loop Predictive Coding Framework for Auditory Working Memory",
      "title_zh": "听觉工作记忆的通用闭环预测编码框架",
      "authors": [
        "Zhongju Yuan",
        "Geraint Wiggins",
        "Dick Botteldooren"
      ],
      "abstract": "Auditory working memory is essential for various daily activities, such as\nlanguage acquisition, conversation. It involves the temporary storage and\nmanipulation of information that is no longer present in the environment. While\nextensively studied in neuroscience and cognitive science, research on its\nmodeling within neural networks remains limited. To address this gap, we\npropose a general framework based on a close-loop predictive coding paradigm to\nperform short auditory signal memory tasks. The framework is evaluated on two\nwidely used benchmark datasets for environmental sound and speech,\ndemonstrating high semantic similarity across both datasets.",
      "tldr_zh": "本研究提出了一个基于闭环预测编码（close-loop predictive coding）的通用框架，用于模拟听觉工作记忆（auditory working memory）任务。该框架能够有效处理短时听觉信号的存储和操作，弥补了神经网络在该领域建模研究的不足。实验在环境声音和语音两个基准数据集上进行验证，结果表明该模型在不同任务中均表现出较高的语义相似性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12506v1",
      "published_date": "2025-03-16 13:57:37 UTC",
      "updated_date": "2025-03-16 13:57:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:30:50.306930"
    },
    {
      "arxiv_id": "2503.12505v1",
      "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification",
      "title_zh": "MPBench：面向流程错误识别的多模态推理综合基准",
      "authors": [
        "Zhaopan Xu",
        "Pengfei Zhou",
        "Jiaxin Ai",
        "Wangbo Zhao",
        "Kai Wang",
        "Xiaojiang Peng",
        "Wenqi Shao",
        "Hongxun Yao",
        "Kaipeng Zhang"
      ],
      "abstract": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
      "tldr_zh": "该研究提出了MPBench，首个专注于评估过程级奖励模型(PRMs)的多模态综合基准测试。该基准通过三种评估范式系统检验PRMs在推理过程中的作用：步骤正确性检验、答案聚合优化和推理过程搜索，弥补了现有文本基准在错误检测和推理搜索等场景的不足。MPBench为开发多模态PRMs提供了全面评估框架，有助于提升大型语言模型(LLMs)在复杂任务中的推理能力。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12505v1",
      "published_date": "2025-03-16 13:50:38 UTC",
      "updated_date": "2025-03-16 13:50:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:31:37.523716"
    },
    {
      "arxiv_id": "2503.12499v1",
      "title": "Facilitating Automated Online Consensus Building through Parallel Thinking",
      "title_zh": "通过平行思维实现自动化在线共识构建",
      "authors": [
        "Wen Gu",
        "Zhaoxing Li",
        "Jan Buermann",
        "Jim Dilkes",
        "Dimitris Michailidis",
        "Shinobu Hasegawa",
        "Vahid Yazdanpanah",
        "Sebastian Stein"
      ],
      "abstract": "Consensus building is inherently challenging due to the diverse opinions held\nby stakeholders. Effective facilitation is crucial to support the consensus\nbuilding process and enable efficient group decision making. However, the\neffectiveness of facilitation is often constrained by human factors such as\nlimited experience and scalability. In this research, we propose a Parallel\nThinking-based Facilitation Agent (PTFA) that facilitates online, text-based\nconsensus building processes. The PTFA automatically collects textual posts and\nleverages large language models (LLMs) to perform all of the six distinct roles\nof the well-established Six Thinking Hats technique in parallel thinking. To\nillustrate the potential of PTFA, a pilot study was carried out and PTFA's\nability in idea generation, emotional probing, and deeper analysis of ideas was\ndemonstrated. Furthermore, a comprehensive dataset that contains not only the\nconversational content among the participants but also between the participants\nand the agent is constructed for future study.",
      "tldr_zh": "本研究提出了基于平行思维(Parallel Thinking)的自动化在线共识构建系统PTFA。该系统利用大语言模型(LLMs)同时执行\"六顶思考帽\"方法中的六个不同角色功能，自动收集文本讨论内容并进行多角度分析。初步研究表明，PTFA在观点生成、情感探测和深入分析方面表现优异，并构建了包含人机交互内容的完整数据集，为未来研究提供了基础。该方案有效解决了传统共识构建中因人工协调经验不足和规模限制带来的问题。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12499v1",
      "published_date": "2025-03-16 13:32:35 UTC",
      "updated_date": "2025-03-16 13:32:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:31:49.366923"
    },
    {
      "arxiv_id": "2503.12497v1",
      "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
      "title_zh": "基于账户感知分布差异的模型窃取防御方法",
      "authors": [
        "Jian-Ping Mei",
        "Weibin Zhang",
        "Jie Chen",
        "Xuyun Zhang",
        "Tiantian Zhu"
      ],
      "abstract": "Malicious users attempt to replicate commercial models functionally at low\ncost by training a clone model with query responses. It is challenging to\ntimely prevent such model-stealing attacks to achieve strong protection and\nmaintain utility. In this paper, we propose a novel non-parametric detector\ncalled Account-aware Distribution Discrepancy (ADD) to recognize queries from\nmalicious users by leveraging account-wise local dependency. We formulate each\nclass as a Multivariate Normal distribution (MVN) in the feature space and\nmeasure the malicious score as the sum of weighted class-wise distribution\ndiscrepancy. The ADD detector is combined with random-based prediction\npoisoning to yield a plug-and-play defense module named D-ADD for image\nclassification models. Results of extensive experimental studies show that\nD-ADD achieves strong defense against different types of attacks with little\ninterference in serving benign users for both soft and hard-label settings.",
      "tldr_zh": "本文提出了一种基于账户感知分布差异(Account-aware Distribution Discrepancy, ADD)的非参数检测器，用于识别恶意用户的模型窃取攻击。该方法通过将每个类别建模为特征空间中的多元正态分布(MVN)，并计算加权类间分布差异来评估恶意得分。结合随机预测污染技术，ADD检测器形成了一个即插即用的防御模块D-ADD，适用于图像分类模型。实验结果表明，D-ADD在软标签和硬标签设置下均能有效防御多种攻击，同时对正常用户的影响极小。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 7 figures, published in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12497v1",
      "published_date": "2025-03-16 13:22:53 UTC",
      "updated_date": "2025-03-16 13:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:31:54.473591"
    },
    {
      "arxiv_id": "2503.13546v1",
      "title": "CNCast: Leveraging 3D Swin Transformer and DiT for Enhanced Regional Weather Forecasting",
      "title_zh": "CNCast：利用3D Swin Transformer和DiT提升区域天气预报精度",
      "authors": [
        "Hongli Liang",
        "Yuanting Zhang",
        "Qingye Meng",
        "Shuangshuang He",
        "Xingyuan Yuan"
      ],
      "abstract": "This study introduces a cutting-edge regional weather forecasting model based\non the SwinTransformer 3D architecture. This model is specifically designed to\ndeliver precise hourly weather predictions ranging from 1 hour to 5 days,\nsignificantly improving the reliability and practicality of short-term weather\nforecasts. Our model has demonstrated generally superior performance when\ncompared to Pangu, a well-established global model. The evaluation indicates\nthat our model excels in predicting most weather variables, highlighting its\npotential as a more effective alternative in the field of limited area\nmodeling. A noteworthy feature of this model is the integration of enhanced\nboundary conditions, inspired by traditional numerical weather prediction (NWP)\ntechniques. This integration has substantially improved the model's predictive\naccuracy. Additionally, the model includes an innovative approach for\ndiagnosing hourly total precipitation at a high spatial resolution of\napproximately 5 kilometers. This is achieved through a latent diffusion model,\noffering an alternative method for generating high-resolution precipitation\ndata.",
      "tldr_zh": "该研究提出CNCast模型，结合3D Swin Transformer和DiT架构，构建了先进的区域天气预报系统。模型在1小时至5天的短时预报中表现优异，性能超越主流全球模型Pangu，特别在边界条件优化方面借鉴了传统数值天气预报(NWP)技术显著提升了精度。创新性地采用潜在扩散模型实现5公里高空间分辨率的逐小时降水诊断，为有限区域建模提供了更有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13546v1",
      "published_date": "2025-03-16 12:52:48 UTC",
      "updated_date": "2025-03-16 12:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:32:13.674463"
    },
    {
      "arxiv_id": "2503.12490v1",
      "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing",
      "title_zh": "GeoRSMLLM：面向地球科学与遥感的视觉-语言任务多模态大语言模型",
      "authors": [
        "Zilun Zhang",
        "Haozhan Shen",
        "Tiancheng Zhao",
        "Bin Chen",
        "Zian Guan",
        "Yuhao Wang",
        "Xu Jia",
        "Yuxiang Cai",
        "Yongheng Shang",
        "Jianwei Yin"
      ],
      "abstract": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing.",
      "tldr_zh": "该研究提出了GeoRSMLLM，一种面向地球科学与遥感的多模态大语言模型，旨在解决现有视觉语言模型(VLMs)在处理复杂指令和像素级任务（如分割和变化检测）时的局限性。研究引入了遥感视觉语言任务集(RSVLTS)，涵盖开放词汇任务(OVT)、指代表达任务(RET)、描述对象任务(DOT)和视觉问答(VQA)，并提出了一种基于点集的数据表示方法、条件解析器和循环指代自增强策略。GeoRSMLLM通过整合这些特性，为遥感领域的视觉语言任务提供了更通用的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12490v1",
      "published_date": "2025-03-16 12:48:17 UTC",
      "updated_date": "2025-03-16 12:48:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:32:43.248627"
    },
    {
      "arxiv_id": "2503.12484v1",
      "title": "SING: Semantic Image Communications using Null-Space and INN-Guided Diffusion Models",
      "title_zh": "SING：基于零空间与可逆神经网络引导扩散模型的语义图像通信",
      "authors": [
        "Jiakang Chen",
        "Selim F. Yilmaz",
        "Di You",
        "Pier Luigi Dragotti",
        "Deniz Gündüz"
      ],
      "abstract": "Joint source-channel coding systems based on deep neural networks (DeepJSCC)\nhave recently demonstrated remarkable performance in wireless image\ntransmission. Existing methods primarily focus on minimizing distortion between\nthe transmitted image and the reconstructed version at the receiver, often\noverlooking perceptual quality. This can lead to severe perceptual degradation\nwhen transmitting images under extreme conditions, such as low bandwidth\ncompression ratios (BCRs) and low signal-to-noise ratios (SNRs). In this work,\nwe propose SING, a novel two-stage JSCC framework that formulates the recovery\nof high-quality source images from corrupted reconstructions as an inverse\nproblem. Depending on the availability of information about the DeepJSCC\nencoder/decoder and the channel at the receiver, SING can either approximate\nthe stochastic degradation as a linear transformation, or leverage invertible\nneural networks (INNs) for precise modeling. Both approaches enable the\nseamless integration of diffusion models into the reconstruction process,\nenhancing perceptual quality. Experimental results demonstrate that SING\noutperforms DeepJSCC and other approaches, delivering superior perceptual\nquality even under extremely challenging conditions, including scenarios with\nsignificant distribution mismatches between the training and test data.",
      "tldr_zh": "该研究提出SING框架，一种基于零空间和可逆神经网络(INN)引导扩散模型的两阶段语义图像通信系统。针对现有深度联合信源信道编码(DeepJSCC)方法在极端条件下感知质量下降的问题，SING将图像恢复建模为逆问题，通过线性变换近似或INN精确建模两种方式集成扩散模型。实验证明，即使在极低带宽压缩比(BCR)和信噪比(SNR)条件下，SING也能显著提升重建图像的感知质量，且对训练-测试数据分布失配具有鲁棒性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12484v1",
      "published_date": "2025-03-16 12:32:11 UTC",
      "updated_date": "2025-03-16 12:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:33:04.326921"
    },
    {
      "arxiv_id": "2503.12478v2",
      "title": "KDSelector: A Knowledge-Enhanced and Data-Efficient Model Selector Learning Framework for Time Series Anomaly Detection",
      "title_zh": "KDSelector：一种面向时间序列异常检测的知识增强与数据高效模型选择器学习框架",
      "authors": [
        "Zhiyu Liang",
        "Dongrui Cai",
        "Chenyuan Zhang",
        "Zheng Liang",
        "Chen Liang",
        "Bo Zheng",
        "Shi Qiu",
        "Jin Wang",
        "Hongzhi Wang"
      ],
      "abstract": "Model selection has been raised as an essential problem in the area of time\nseries anomaly detection (TSAD), because there is no single best TSAD model for\nthe highly heterogeneous time series in real-world applications. However,\ndespite the success of existing model selection solutions that train a\nclassification model (especially neural network, NN) using historical data as a\nselector to predict the correct TSAD model for each series, the NN-based\nselector learning methods used by existing solutions do not make full use of\nthe knowledge in the historical data and require iterating over all training\nsamples, which limits the accuracy and training speed of the selector. To\naddress these limitations, we propose KDSelector, a novel knowledge-enhanced\nand data-efficient framework for learning the NN-based TSAD model selector, of\nwhich three key components are specifically designed to integrate available\nknowledge into the selector and dynamically prune less important and redundant\nsamples during the learning. We develop a TSAD model selection system with\nKDSelector as the internal, to demonstrate how users improve the accuracy and\ntraining speed of their selectors by using KDSelector as a plug-and-play\nmodule. Our demonstration video is hosted at https://youtu.be/2uqupDWvTF0.",
      "tldr_zh": "该研究提出了KDSelector，一种知识增强且数据高效的模型选择框架，用于时间序列异常检测(TSAD)。KDSelector通过三个关键组件，将历史数据中的知识整合到基于神经网络的模型选择器中，并在学习过程中动态剪枝不重要和冗余的样本，从而提高选择器的准确性和训练速度。实验表明，KDSelector可作为即插即用模块，显著提升TSAD模型选择系统的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by SIGMOD 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12478v2",
      "published_date": "2025-03-16 12:13:19 UTC",
      "updated_date": "2025-03-20 03:06:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:33:12.504039"
    },
    {
      "arxiv_id": "2503.12451v1",
      "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset",
      "title_zh": "ISLR101：伊朗语单词级手语识别数据集",
      "authors": [
        "Hossein Ranjbar",
        "Alireza Taheri"
      ],
      "abstract": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons.",
      "tldr_zh": "该研究发布了ISLR101，首个公开的伊朗手语单词识别数据集，包含10位不同背景使用者（包括聋人、手语翻译和学习者）录制的4,614个视频，涵盖101个独立手势。数据集提供800x600分辨率视频和基于OpenPose提取的骨骼姿态信息。研究团队建立了基于视觉外观和骨骼姿态的基线模型，分别取得97.01%和94.02%的测试准确率，为伊朗手语识别研究提供了标准化基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12451v1",
      "published_date": "2025-03-16 10:57:01 UTC",
      "updated_date": "2025-03-16 10:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:33:49.368953"
    },
    {
      "arxiv_id": "2503.16516v1",
      "title": "Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability",
      "title_zh": "利用大语言模型实现隐私政策自动分析：提示工程、微调与可解释性",
      "authors": [
        "Yuxin Chen",
        "Peng Tang",
        "Weidong Qiu",
        "Shujun Li"
      ],
      "abstract": "Privacy policies are widely used by digital services and often required for\nlegal purposes. Many machine learning based classifiers have been developed to\nautomate detection of different concepts in a given privacy policy, which can\nhelp facilitate other automated tasks such as producing a more reader-friendly\nsummary and detecting legal compliance issues. Despite the successful\napplications of large language models (LLMs) to many NLP tasks in various\ndomains, there is very little work studying the use of LLMs for automated\nprivacy policy analysis, therefore, if and how LLMs can help automate privacy\npolicy analysis remains under-explored. To fill this research gap, we conducted\na comprehensive evaluation of LLM-based privacy policy concept classifiers,\nemploying both prompt engineering and LoRA (low-rank adaptation) fine-tuning,\non four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our\nexperimental results demonstrated that combining prompt engineering and\nfine-tuning can make LLM-based classifiers outperform other SOTA methods,\n\\emph{significantly} and \\emph{consistently} across privacy policy\ncorpora/taxonomies and concepts. Furthermore, we evaluated the explainability\nof the LLM-based classifiers using three metrics: completeness, logicality, and\ncomprehensibility. For all three metrics, a score exceeding 91.1\\% was observed\nin our evaluation, indicating that LLMs are not only useful to improve the\nclassification performance, but also to enhance the explainability of detection\nresults.",
      "tldr_zh": "该研究系统评估了大型语言模型(LLMs)在隐私政策自动分析中的应用，填补了这一领域的研究空白。通过结合提示工程(Prompt Engineering)和LoRA微调技术，在四个前沿隐私政策语料库上构建的分类器显著超越了现有最优方法。实验表明，LLM不仅能提升分类性能，其检测结果的可解释性(完整性、逻辑性和可理解性)评分均超过91.1%，为自动化隐私政策分析提供了高效且可解释的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16516v1",
      "published_date": "2025-03-16 10:50:31 UTC",
      "updated_date": "2025-03-16 10:50:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:34:06.735897"
    },
    {
      "arxiv_id": "2503.12447v1",
      "title": "Causality Model for Semantic Understanding on Videos",
      "title_zh": "视频语义理解的因果建模",
      "authors": [
        "Li Yicong"
      ],
      "abstract": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA).",
      "tldr_zh": "本研究提出了一种基于因果模型的方法，旨在提升视频语义理解的鲁棒性。针对现有深度神经网络（DNNs）在数据分布偏移（如长尾失衡和扰动失衡）下性能显著下降的问题，研究通过因果建模揭示视频数据中的真实因果模式，而非仅依赖观测相关性。研究聚焦于视频关系检测（VidVRD）和视频问答（VideoQA）两项基础任务，探索了因果模型在提升视频理解性能方面的潜力，为解决数据失衡和增强模型鲁棒性提供了新思路。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "PhD Thesis",
      "pdf_url": "http://arxiv.org/pdf/2503.12447v1",
      "published_date": "2025-03-16 10:44:11 UTC",
      "updated_date": "2025-03-16 10:44:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:34:15.680889"
    },
    {
      "arxiv_id": "2503.12446v1",
      "title": "BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries",
      "title_zh": "BREEN：基于可学习查询的桥接式数据高效免编码器多模态学习",
      "authors": [
        "Tianle Li",
        "Yongming Rao",
        "Winston Hu",
        "Yu Cheng"
      ],
      "abstract": "Encoder-free multimodal large language models(MLLMs) eliminate the need for a\nwell-trained vision encoder by directly processing image tokens before the\nlanguage model. While this approach reduces computational overhead and model\ncomplexity, it often requires large amounts of training data to effectively\ncapture the visual knowledge typically encoded by vision models like CLIP. The\nabsence of a vision encoder implies that the model is likely to rely on\nsubstantial data to learn the necessary visual-semantic alignments. In this\nwork, we present BREEN, a data-efficient encoder-free multimodal architecture\nthat mitigates this issue. BREEN leverages a learnable query and image experts\nto achieve comparable performance with significantly less training data. The\nlearnable query, positioned between image and text tokens, is supervised by the\noutput of a pretrained CLIP model to distill visual knowledge, bridging the gap\nbetween visual and textual modalities. Additionally, the image expert processes\nimage tokens and learnable queries independently, improving efficiency and\nreducing interference with the LLM's textual capabilities. BREEN achieves\ncomparable performance to prior encoder-free state-of-the-art models like\nMono-InternVL, using only 13 million text-image pairs in training about one\npercent of the data required by existing methods. Our work highlights a\npromising direction for data-efficient encoder-free multimodal learning,\noffering an alternative to traditional encoder-based approaches.",
      "tldr_zh": "该研究提出了BREEN，一种数据高效的免编码器多模态学习框架，通过引入可学习查询(learnable query)和图像专家(image experts)解决了传统免编码器多模态大语言模型(MLLMs)对大量训练数据的依赖问题。BREEN利用预训练CLIP模型的输出监督可学习查询，从而提取视觉知识并弥合视觉与文本模态的差距，同时图像专家独立处理图像标记和查询，提升了效率并减少了对语言模型文本能力的干扰。实验表明，BREEN仅需1300万文本-图像对即可达到与现有免编码器模型相当的性能，训练数据量仅为传统方法的1%，为数据高效的多模态学习提供了新思路。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12446v1",
      "published_date": "2025-03-16 10:43:14 UTC",
      "updated_date": "2025-03-16 10:43:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:34:35.610004"
    },
    {
      "arxiv_id": "2503.12434v1",
      "title": "A Survey on the Optimization of Large Language Model-based Agents",
      "title_zh": "基于大语言模型的智能体优化研究综述",
      "authors": [
        "Shangheng Du",
        "Jiabao Zhao",
        "Jinxin Shi",
        "Zhentao Xie",
        "Xin Jiang",
        "Yanhong Bai",
        "Liang He"
      ],
      "abstract": "With the rapid development of Large Language Models (LLMs), LLM-based agents\nhave been widely adopted in various fields, becoming essential for autonomous\ndecision-making and interactive tasks. However, current work typically relies\non prompt design or fine-tuning strategies applied to vanilla LLMs, which often\nleads to limited effectiveness or suboptimal performance in complex\nagent-related environments. Although LLM optimization techniques can improve\nmodel performance across many general tasks, they lack specialized optimization\ntowards critical agent functionalities such as long-term planning, dynamic\nenvironmental interaction, and complex decision-making. Although numerous\nrecent studies have explored various strategies to optimize LLM-based agents\nfor complex agent tasks, a systematic review summarizing and comparing these\nmethods from a holistic perspective is still lacking. In this survey, we\nprovide a comprehensive review of LLM-based agent optimization approaches,\ncategorizing them into parameter-driven and parameter-free methods. We first\nfocus on parameter-driven optimization, covering fine-tuning-based\noptimization, reinforcement learning-based optimization, and hybrid strategies,\nanalyzing key aspects such as trajectory data construction, fine-tuning\ntechniques, reward function design, and optimization algorithms. Additionally,\nwe briefly discuss parameter-free strategies that optimize agent behavior\nthrough prompt engineering and external knowledge retrieval. Finally, we\nsummarize the datasets and benchmarks used for evaluation and tuning, review\nkey applications of LLM-based agents, and discuss major challenges and\npromising future directions. Our repository for related references is available\nat https://github.com/YoungDubbyDu/LLM-Agent-Optimization.",
      "tldr_zh": "该综述系统梳理了大语言模型(LLM)智能体的优化方法，将其分为参数驱动和参数无关两大类。研究重点分析了基于微调、强化学习和混合策略的参数驱动优化方法，涵盖轨迹数据构建、奖励函数设计等关键技术；同时探讨了提示工程和外部知识检索等参数无关策略。论文还总结了评估数据集、典型应用场景，并指出该领域在长期规划、动态环境交互等核心功能上面临的挑战与未来方向。相关资源已在GitHub开源。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12434v1",
      "published_date": "2025-03-16 10:09:10 UTC",
      "updated_date": "2025-03-16 10:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:34:52.695154"
    },
    {
      "arxiv_id": "2503.12427v1",
      "title": "Towards Learnable Anchor for Deep Multi-View Clustering",
      "title_zh": "迈向可学习锚点的深度多视图聚类",
      "authors": [
        "Bocheng Wang",
        "Chusheng Zeng",
        "Mulin Chen",
        "Xuelong Li"
      ],
      "abstract": "Deep multi-view clustering incorporating graph learning has presented\ntremendous potential. Most methods encounter costly square time consumption\nw.r.t. data size. Theoretically, anchor-based graph learning can alleviate this\nlimitation, but related deep models mainly rely on manual discretization\napproaches to select anchors, which indicates that 1) the anchors are fixed\nduring model training and 2) they may deviate from the true cluster\ndistribution. Consequently, the unreliable anchors may corrupt clustering\nresults. In this paper, we propose the Deep Multi-view Anchor Clustering (DMAC)\nmodel that performs clustering in linear time. Concretely, the initial anchors\nare intervened by the positive-incentive noise sampled from Gaussian\ndistribution, such that they can be optimized with a newly designed anchor\nlearning loss, which promotes a clear relationship between samples and anchors.\nAfterwards, anchor graph convolution is devised to model the cluster structure\nformed by the anchors, and the mutual information maximization loss is built to\nprovide cross-view clustering guidance. In this way, the learned anchors can\nbetter represent clusters. With the optimal anchors, the full sample graph is\ncalculated to derive a discriminative embedding for clustering. Extensive\nexperiments on several datasets demonstrate the superior performance and\nefficiency of DMAC compared to state-of-the-art competitors.",
      "tldr_zh": "本文提出了一种可学习的锚点深度多视图聚类方法DMAC，解决了现有方法因固定人工选取锚点导致的聚类偏差问题。该方法通过高斯分布噪声激励优化锚点学习，结合锚点图卷积和互信息最大化损失，实现了线性时间复杂度的聚类。实验表明，DMAC在多个数据集上优于现有最优方法，显著提升了聚类性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2503.12427v1",
      "published_date": "2025-03-16 09:38:11 UTC",
      "updated_date": "2025-03-16 09:38:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:35:09.910018"
    },
    {
      "arxiv_id": "2503.12406v1",
      "title": "Bio-Inspired Plastic Neural Networks for Zero-Shot Out-of-Distribution Generalization in Complex Animal-Inspired Robots",
      "title_zh": "仿生可塑性神经网络：面向复杂仿生机器人的零样本分布外泛化",
      "authors": [
        "Binggwong Leung",
        "Worasuchad Haomachai",
        "Joachim Winther Pedersen",
        "Sebastian Risi",
        "Poramate Manoonpong"
      ],
      "abstract": "Artificial neural networks can be used to solve a variety of robotic tasks.\nHowever, they risk failing catastrophically when faced with out-of-distribution\n(OOD) situations. Several approaches have employed a type of synaptic\nplasticity known as Hebbian learning that can dynamically adjust weights based\non local neural activities. Research has shown that synaptic plasticity can\nmake policies more robust and help them adapt to unforeseen changes in the\nenvironment. However, networks augmented with Hebbian learning can lead to\nweight divergence, resulting in network instability. Furthermore, such Hebbian\nnetworks have not yet been applied to solve legged locomotion in complex real\nrobots with many degrees of freedom. In this work, we improve the Hebbian\nnetwork with a weight normalization mechanism for preventing weight divergence,\nanalyze the principal components of the Hebbian's weights, and perform a\nthorough evaluation of network performance in locomotion control for real\n18-DOF dung beetle-like and 16-DOF gecko-like robots. We find that the\nHebbian-based plastic network can execute zero-shot sim-to-real adaptation\nlocomotion and generalize to unseen conditions, such as uneven terrain and\nmorphological damage.",
      "tldr_zh": "本研究提出了一种受生物启发的可塑性神经网络，通过改进Hebbian学习机制（加入权重归一化以防止权重发散）来解决复杂仿生机器人在零样本(OOD)环境下的泛化问题。该网络在18自由度蜣螂机器人和16自由度壁虎机器人上进行了全面评估，结果表明其能够实现零样本的仿真到真实环境适应，并在不平坦地形和形态损伤等未见条件下表现出良好的泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12406v1",
      "published_date": "2025-03-16 08:13:53 UTC",
      "updated_date": "2025-03-16 08:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:35:35.944125"
    },
    {
      "arxiv_id": "2503.12389v1",
      "title": "FedGAI: Federated Style Learning with Cloud-Edge Collaboration for Generative AI in Fashion Design",
      "title_zh": "FedGAI：面向时尚设计生成式AI的云端-边缘协同联邦式学习框架",
      "authors": [
        "Mingzhu Wu",
        "Jianan Jiang",
        "Xinglin Li",
        "Hanhui Deng",
        "Di Wu"
      ],
      "abstract": "Collaboration can amalgamate diverse ideas, styles, and visual elements,\nfostering creativity and innovation among different designers. In collaborative\ndesign, sketches play a pivotal role as a means of expressing design\ncreativity. However, designers often tend to not openly share these\nmeticulously crafted sketches. This phenomenon of data island in the design\narea hinders its digital transformation under the third wave of AI. In this\npaper, we introduce a Federated Generative Artificial Intelligence Clothing\nsystem, namely FedGAI, employing federated learning to aid in sketch design.\nFedGAI is committed to establishing an ecosystem wherein designers can exchange\nsketch styles among themselves. Through FedGAI, designers can generate sketches\nthat incorporate various designers' styles from their peers, drawing\ninspiration from collaboration without the need for data disclosure or upload.\nExtensive performance evaluations indicate that our FedGAI system can produce\nmulti-styled sketches of comparable quality to human-designed ones while\nsignificantly enhancing efficiency compared to hand-drawn sketches.",
      "tldr_zh": "该研究提出FedGAI系统，通过联邦学习(Federated Learning)实现云端-边缘协同的生成式AI时装设计框架，解决设计领域数据孤岛问题。系统允许设计师在不共享原始草图数据的情况下，安全交换设计风格并生成融合多方风格的创意草图。实验表明，FedGAI能生成质量媲美人工设计、风格多样的时装草图，同时显著提升设计效率。该系统为AI驱动的时尚设计协作提供了隐私保护的创新解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12389v1",
      "published_date": "2025-03-16 07:31:25 UTC",
      "updated_date": "2025-03-16 07:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:35:55.630519"
    },
    {
      "arxiv_id": "2503.12374v2",
      "title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution",
      "title_zh": "揭示陷阱：理解AI驱动代码代理在GitHub问题解决中失败的原因",
      "authors": [
        "Zhi Chen",
        "Wei Ma",
        "Lingxiao Jiang"
      ],
      "abstract": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts.",
      "tldr_zh": "该研究深入探讨了AI驱动的代码代理在解决GitHub问题时的失败原因。通过对SWE-Bench基准中8个顶尖代理在500个GitHub问题上的3,977个解决阶段轨迹和3,931个测试阶段日志的分析，研究发现Python执行错误（如ModuleNotFoundError和TypeError）与较低的问题解决率和较高的推理开销相关。特别是一些复杂错误（如OSError和数据库相关的IntegrityError）需要更多的调试努力。此外，研究还发现了SWE-Bench平台中的3个影响基准公平性和准确性的错误，并已报告给维护者。研究公开了数据集和分析脚本，以促进透明性和未来研究。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12374v2",
      "published_date": "2025-03-16 06:24:51 UTC",
      "updated_date": "2025-03-19 10:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:36:19.387118"
    },
    {
      "arxiv_id": "2503.16515v1",
      "title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
      "title_zh": "跨学科系统科学文献综述中的LLM案例研究聚焦",
      "authors": [
        "Lachlan McGinness",
        "Peter Baumgartner"
      ],
      "abstract": "Large Language Models (LLMs) were used to assist four Commonwealth Scientific\nand Industrial Research Organisation (CSIRO) researchers to perform systematic\nliterature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in\nthese case studies. In each, we explore the impact of changing parameters on\nthe accuracy of LLM responses. The LLM was tasked with extracting evidence from\nchosen academic papers to answer specific research questions. We evaluate the\nmodels' performance in faithfully reproducing quotes from the literature and\nsubject experts were asked to assess the model performance in answering the\nresearch questions. We developed a semantic text highlighting tool to\nfacilitate expert review of LLM responses.\n  We found that state of the art LLMs were able to reproduce quotes from texts\nwith greater than 95% accuracy and answer research questions with an accuracy\nof approximately 83%. We use two methods to determine the correctness of LLM\nresponses; expert review and the cosine similarity of transformer embeddings of\nLLM and expert answers. The correlation between these methods ranged from 0.48\nto 0.77, providing evidence that the latter is a valid metric for measuring\nsemantic similarity.",
      "tldr_zh": "本研究评估了大语言模型(LLMs)在跨学科系统科学文献综述中的应用效果。研究通过四个案例，测试了LLMs在提取学术论文证据、回答研究问题方面的性能，并开发了语义文本高亮工具辅助专家评审。结果表明，LLMs能够以超过95%的准确率复现文献引文，并以约83%的准确率回答研究问题。研究还验证了使用transformer嵌入的余弦相似度作为语义相似度评估指标的有效性，其与专家评审的相关系数为0.48至0.77。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16515v1",
      "published_date": "2025-03-16 05:52:18 UTC",
      "updated_date": "2025-03-16 05:52:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:36:31.486122"
    },
    {
      "arxiv_id": "2503.12358v3",
      "title": "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation",
      "title_zh": "IPCGRL：基于语言指令的强化学习程序化关卡生成",
      "authors": [
        "In-Chang Baek",
        "Sung-Hyun Kim",
        "Seo-Young Lee",
        "Dong-Hyeon Kim",
        "Kyung-Joong Kim"
      ],
      "abstract": "Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.",
      "tldr_zh": "该研究提出IPCGRL——一种基于语言指令强化学习的程序化关卡生成方法，通过微调任务特定的句子嵌入模型来压缩游戏关卡条件。实验表明，该方法在2D关卡生成任务中比通用嵌入方法提升了21.4%的可控性和17.2%对未知指令的泛化能力。IPCGRL扩展了条件输入的模态，为程序化内容生成提供了更灵活的表达交互框架。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 9 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.12358v3",
      "published_date": "2025-03-16 04:53:38 UTC",
      "updated_date": "2025-03-25 01:48:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:37:29.244038"
    },
    {
      "arxiv_id": "2503.12356v2",
      "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation",
      "title_zh": "基于无训练门控低秩适配的文本到图像扩散模型局部概念擦除",
      "authors": [
        "Byung Hyun Lee",
        "Sungjin Lim",
        "Se Young Chun"
      ],
      "abstract": "Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure.",
      "tldr_zh": "本研究提出了一种无需训练的局部概念擦除方法GLoCE（Gated Low-rank Adaptation for Concept Erasure），用于文本到图像扩散模型。该方法通过注入包含低秩矩阵和简单门控机制的轻量级模块，仅针对特定目标概念激活处理，实现了对图像中局部区域概念的精确擦除，同时保持其他区域的完整性。实验表明，GLoCE在图像保真度、擦除效果和鲁棒性方面显著优于现有方法，且可扩展至大规模概念擦除任务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12356v2",
      "published_date": "2025-03-16 04:53:20 UTC",
      "updated_date": "2025-03-25 15:29:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:41:33.026277"
    },
    {
      "arxiv_id": "2503.12353v1",
      "title": "Synthetic Data for Robust AI Model Development in Regulated Enterprises",
      "title_zh": "受监管企业中的稳健AI模型开发：合成数据应用",
      "authors": [
        "Aditi Godbole"
      ],
      "abstract": "In today's business landscape, organizations need to find the right balance\nbetween using their customers' data ethically to power AI solutions and being\ncompliant regarding data privacy and data usage regulations. In this paper, we\ndiscuss synthetic data as a possible solution to this dilemma. Synthetic data\nis simulated data that mimics the real data. We explore how organizations in\nheavily regulated industries, such as financial institutions or healthcare\norganizations, can leverage synthetic data to build robust AI solutions while\nstaying compliant. We demonstrate that synthetic data offers two significant\nadvantages by allowing AI models to learn from more diverse data and by helping\norganizations stay compliant against data privacy laws with the use of\nsynthetic data instead of customer information. We discuss case studies to show\nhow synthetic data can be effectively used in the finance and healthcare sector\nwhile discussing the challenges of using synthetic data and some ethical\nquestions it raises. Our research finds that synthetic data could be a\ngame-changer for AI in regulated industries. The potential can be realized when\nindustry, academia, and regulators collaborate to build solutions. We aim to\ninitiate discussions on the use of synthetic data to build ethical,\nresponsible, and effective AI systems in regulated enterprise industries.",
      "tldr_zh": "本文探讨了合成数据(Synthetic Data)作为解决合规与AI发展矛盾的创新方案。研究表明，在金融、医疗等强监管行业，合成数据既能增强AI模型的训练多样性，又能规避用户隐私数据的使用风险。通过实际案例分析，论文揭示了合成数据在平衡AI效能与合规要求方面的双重优势，同时也指出其技术挑战和伦理问题。作者呼吁产业界、学术界和监管机构协同合作，共同推动合成数据在构建合规AI系统中的应用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12353v1",
      "published_date": "2025-03-16 04:46:41 UTC",
      "updated_date": "2025-03-16 04:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:42:19.713176"
    },
    {
      "arxiv_id": "2503.13543v1",
      "title": "Enhancing Visual Representation with Textual Semantics: Textual Semantics-Powered Prototypes for Heterogeneous Federated Learning",
      "title_zh": "增强视觉表征的文本语义赋能：基于文本语义的原型构建用于异构联邦学习",
      "authors": [
        "Xinghao Wu",
        "Jianwei Niu",
        "Xuefeng Liu",
        "Guogang Zhu",
        "Jiayuan Zhang",
        "Shaojie Tang"
      ],
      "abstract": "Federated Prototype Learning (FedPL) has emerged as an effective strategy for\nhandling data heterogeneity in Federated Learning (FL). In FedPL, clients\ncollaboratively construct a set of global feature centers (prototypes), and let\nlocal features align with these prototypes to mitigate the effects of data\nheterogeneity. The performance of FedPL highly depends on the quality of\nprototypes. Existing methods assume that larger inter-class distances among\nprototypes yield better performance, and thus design different methods to\nincrease these distances. However, we observe that while these methods increase\nprototype distances to enhance class discrimination, they inevitably disrupt\nessential semantic relationships among classes, which are crucial for model\ngeneralization. This raises an important question: how to construct prototypes\nthat inherently preserve semantic relationships among classes? Directly\nlearning these relationships from limited and heterogeneous client data can be\nproblematic in FL. Recently, the success of pre-trained language models (PLMs)\ndemonstrates their ability to capture semantic relationships from vast textual\ncorpora. Motivated by this, we propose FedTSP, a novel method that leverages\nPLMs to construct semantically enriched prototypes from the textual modality,\nenabling more effective collaboration in heterogeneous data settings. We first\nuse a large language model (LLM) to generate fine-grained textual descriptions\nfor each class, which are then processed by a PLM on the server to form textual\nprototypes. To address the modality gap between client image models and the\nPLM, we introduce trainable prompts, allowing prototypes to adapt better to\nclient tasks. Extensive experiments demonstrate that FedTSP mitigates data\nheterogeneity while significantly accelerating convergence.",
      "tldr_zh": "该研究提出FedTSP方法，通过预训练语言模型(PLMs)从文本模态构建语义增强的原型(prototypes)，以解决联邦原型学习(FedPL)中数据异构性问题。不同于现有方法单纯扩大原型间距，该方法利用大语言模型(LLM)生成细粒度类别描述，转化为保持语义关系的文本原型，并通过可训练提示词(prompts)弥合图像与文本模态的差异。实验表明，FedTSP能有效缓解数据异构性并显著加速模型收敛。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13543v1",
      "published_date": "2025-03-16 04:35:06 UTC",
      "updated_date": "2025-03-16 04:35:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:42:36.958972"
    },
    {
      "arxiv_id": "2503.13542v1",
      "title": "HAR-DoReMi: Optimizing Data Mixture for Self-Supervised Human Activity Recognition Across Heterogeneous IMU Datasets",
      "title_zh": "HAR-DoReMi：面向异构IMU数据集的自监督人类活动识别的数据混合优化方法",
      "authors": [
        "Lulu Ban",
        "Tao Zhu",
        "Xiangqing Lu",
        "Qi Qiu",
        "Wenyong Han",
        "Shuangjian Li",
        "Liming Chen",
        "Kevin I-Kai Wang",
        "Mingxing Nie",
        "Yaping Wan"
      ],
      "abstract": "Cross-dataset Human Activity Recognition (HAR) suffers from limited model\ngeneralization, hindering its practical deployment. To address this critical\nchallenge, inspired by the success of DoReMi in Large Language Models (LLMs),\nwe introduce a data mixture optimization strategy for pre-training HAR models,\naiming to improve the recognition performance across heterogeneous datasets.\nHowever, directly applying DoReMi to the HAR field encounters new challenges\ndue to the continuous, multi-channel and intrinsic heterogeneous\ncharacteristics of IMU sensor data. To overcome these limitations, we propose a\nnovel framework HAR-DoReMi, which introduces a masked reconstruction task based\non Mean Squared Error (MSE) loss. By raplacing the discrete language sequence\nprediction task, which relies on the Negative Log-Likelihood (NLL) loss, in the\noriginal DoReMi framework, the proposed framework is inherently more\nappropriate for handling the continuous and multi-channel characteristics of\nIMU data. In addition, HAR-DoReMi integrates the Mahony fusion algorithm into\nthe self-supervised HAR pre-training, aiming to mitigate the heterogeneity of\nvarying sensor orientation. This is achieved by estimating the sensor\norientation within each dataset and facilitating alignment with a unified\ncoordinate system, thereby improving the cross-dataset generalization ability\nof the HAR model. Experimental evaluation on multiple cross-dataset HAR\ntransfer tasks demonstrates that HAR-DoReMi improves the accuracy by an average\nof 6.51%, compared to the current state-of-the-art method with only\napproximately 30% to 50% of the data usage. These results confirm the\neffectiveness of HAR-DoReMi in improving the generalization and data efficiency\nof pre-training HAR models, underscoring its significant potential to\nfacilitate the practical deployment of HAR technology.",
      "tldr_zh": "本文提出HAR-DoReMi方法，通过优化IMU传感器数据的混合策略来提升自监督人体活动识别（HAR）模型的跨数据集泛化能力。该方法创新性地采用基于均方误差（MSE）损失的掩码重建任务替代原始DoReMi框架中的离散序列预测，并引入Mahony融合算法统一传感器坐标系，有效解决了IMU数据连续多通道和异质性的挑战。实验表明，该框架在多个跨数据集HAR任务中平均准确率提升6.51%，同时仅需30%-50%的数据量即超越现有最优方法，显著提高了HAR技术的实用部署潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13542v1",
      "published_date": "2025-03-16 04:31:58 UTC",
      "updated_date": "2025-03-16 04:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:42:57.034135"
    },
    {
      "arxiv_id": "2503.12349v2",
      "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
      "title_zh": "SPIN-Bench：大语言模型在战略规划与社交推理中的表现如何？",
      "authors": [
        "Jianzhu Yao",
        "Kevin Wang",
        "Ryan Hsieh",
        "Haisu Zhou",
        "Tianqing Zou",
        "Zerui Cheng",
        "Zhangyang Wang",
        "Pramod Viswanath"
      ],
      "abstract": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
      "tldr_zh": "该研究提出了SPIN-Bench基准测试框架，用于评估大语言模型(LLMs)在战略规划和社会推理方面的能力。该框架整合了经典PDDL任务、竞技棋盘游戏、合作卡牌游戏和多智能体谈判场景，通过系统调整动作空间、状态复杂度和交互智能体数量，构建了多样化的社交环境测试集。实验发现，虽然当代LLMs能较好处理基础事实检索和短期规划，但在需要深度多跳推理和大状态空间下的社会性协调任务中表现显著受限。该基准旨在推动鲁棒多智能体规划和社会推理研究的进展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "51 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12349v2",
      "published_date": "2025-03-16 04:10:53 UTC",
      "updated_date": "2025-03-18 01:34:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:42:05.225997"
    },
    {
      "arxiv_id": "2503.12345v1",
      "title": "General Table Question Answering via Answer-Formula Joint Generation",
      "title_zh": "通用表格问答：基于答案-公式联合生成的方法",
      "authors": [
        "Zhongyuan Wang",
        "Richong Zhang",
        "Zhijie Nie"
      ],
      "abstract": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperations, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely-used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nFormula as the logical form for solving complex reasoning on the tables with\ndifferent structures. Specifically, we construct a large Formula-annotated\nTableQA dataset \\texttt{FromulaQA} from existing datasets. In addition, we\npropose \\texttt{TabAF}, a general table answering framework to solve multiple\ntypes of tasks over multiple types of tables simultaneously. Unlike existing\nmethods, \\texttt{TabAF} decodes answers and Formulas with a single LLM\nbackbone, demonstrating great versatility and generalization. \\texttt{TabAF}\nbased on Llama3.1-70B achieves new state-of-the-art performance on the\nWikiTableQuestion, HiTab and TabFact.",
      "tldr_zh": "该研究提出了一种通用的表格问答方法\\texttt{TabAF}，通过联合生成答案和电子表格公式(Formula)来解决复杂推理问题。研究者构建了大规模公式标注数据集\\texttt{FormulaQA}，并利用单一LLM骨干网络同时解码答案和公式，显著提升了模型在多种表格类型和任务上的通用性与泛化能力。基于Llama3.1-70B的\\texttt{TabAF}在WikiTableQuestion、HiTab和TabFact数据集上取得了新的最优性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.12345v1",
      "published_date": "2025-03-16 03:51:06 UTC",
      "updated_date": "2025-03-16 03:51:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:43:59.877390"
    },
    {
      "arxiv_id": "2503.13540v1",
      "title": "MSCMHMST: A traffic flow prediction model based on Transformer",
      "title_zh": "MSCMHMST：一种基于Transformer的交通流量预测模型",
      "authors": [
        "Weiyang Geng",
        "Yiming Pan",
        "Zhecong Xing",
        "Dongyu Liu",
        "Rui Liu",
        "Yuan Zhu"
      ],
      "abstract": "This study proposes a hybrid model based on Transformers, named MSCMHMST,\naimed at addressing key challenges in traffic flow prediction. Traditional\nsingle-method approaches show limitations in traffic prediction tasks, whereas\nhybrid methods, by integrating the strengths of different models, can provide\nmore accurate and robust predictions. The MSCMHMST model introduces a\nmulti-head, multi-scale attention mechanism, allowing the model to parallel\nprocess different parts of the data and learn its intrinsic representations\nfrom multiple perspectives, thereby enhancing the model's ability to handle\ncomplex situations. This mechanism enables the model to capture features at\nvarious scales effectively, understanding both short-term changes and long-term\ntrends. Verified through experiments on the PeMS04/08 dataset with specific\nexperimental settings, the MSCMHMST model demonstrated excellent robustness and\naccuracy in long, medium, and short-term traffic flow predictions. The results\nindicate that this model has significant potential, offering a new and\neffective solution for the field of traffic flow prediction.",
      "tldr_zh": "本研究提出了一种基于Transformer的混合模型MSCMHMST，用于解决交通流量预测中的关键挑战。该模型创新性地采用多头多尺度注意力机制，能够并行处理数据并从多视角学习内在表征，有效捕捉短期变化和长期趋势特征。在PeMS04/08数据集上的实验表明，该模型在长、中、短期交通流量预测中均表现出优异的鲁棒性和准确性，为交通预测领域提供了新的有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13540v1",
      "published_date": "2025-03-16 03:40:32 UTC",
      "updated_date": "2025-03-16 03:40:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:43:39.498915"
    },
    {
      "arxiv_id": "2503.12339v1",
      "title": "Augmented Adversarial Trigger Learning",
      "title_zh": "增强式对抗触发器学习",
      "authors": [
        "Zhe Wang",
        "Yanjun Qi"
      ],
      "abstract": "Gradient optimization-based adversarial attack methods automate the learning\nof adversarial triggers to generate jailbreak prompts or leak system prompts.\nIn this work, we take a closer look at the optimization objective of\nadversarial trigger learning and propose ATLA: Adversarial Trigger Learning\nwith Augmented objectives. ATLA improves the negative log-likelihood loss used\nby previous studies into a weighted loss formulation that encourages the\nlearned adversarial triggers to optimize more towards response format tokens.\nThis enables ATLA to learn an adversarial trigger from just one query-response\npair and the learned trigger generalizes well to other similar queries. We\nfurther design a variation to augment trigger optimization with an auxiliary\nloss that suppresses evasive responses. We showcase how to use ATLA to learn\nadversarial suffixes jailbreaking LLMs and to extract hidden system prompts.\nEmpirically we demonstrate that ATLA consistently outperforms current\nstate-of-the-art techniques, achieving nearly 100% success in attacking while\nrequiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high\ngeneralization to unseen queries and transfer well to new LLMs.",
      "tldr_zh": "本文提出ATLA（增强目标对抗性触发器学习）方法，通过改进传统负对数似然损失函数为加权损失公式，使对抗触发器更专注于优化响应格式token。该方法仅需一个查询-响应对即可学习通用对抗触发器，并引入辅助损失函数来抑制回避性响应。实验表明，ATLA在攻击成功率接近100%的同时将查询需求降低80%，且所学越狱后缀能良好泛化至新查询和不同大语言模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12339v1",
      "published_date": "2025-03-16 03:20:52 UTC",
      "updated_date": "2025-03-16 03:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:45:38.118097"
    },
    {
      "arxiv_id": "2503.12334v2",
      "title": "When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research",
      "title_zh": "当神经植入遇见多模态大语言模型：面向神经调控与自然神经行为研究的双循环系统",
      "authors": [
        "Edward Hong Wang",
        "Cynthia Xin Wen"
      ],
      "abstract": "We propose a novel dual-loop system that synergistically combines responsive\nneurostimulation (RNS) implants with artificial intelligence-driven wearable\ndevices for treating post-traumatic stress disorder (PTSD) and enabling\nnaturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop\nneural device monitors amygdala activity and provides on-demand stimulation\nupon detecting pathological theta oscillations, while an ensemble of wearables\n(smart glasses, smartwatches, smartphones) uses multimodal large language model\n(LLM) analysis of sensory data to detect environmental or physiological PTSD\ntriggers and deliver timely audiovisual interventions. Logged events from both\nthe neural and wearable loops are analyzed to personalize trigger detection and\nprogressively transition patients to non-invasive interventions. In\nNeuroscience Research Mode, the same platform is adapted for real-world brain\nactivity capture. Wearable-LLM systems recognize naturalistic events (social\ninteractions, emotional situations, compulsive behaviors, decision making) and\nsignal implanted RNS devices (via wireless triggers) to record synchronized\nintracranial data during these moments. This approach builds on recent advances\nin mobile intracranial EEG recording and closed-loop neuromodulation in humans\n(BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our\ninterdisciplinary system could revolutionize PTSD therapy and cognitive\nneuroscience by enabling 24/7 monitoring, context-aware intervention, and rich\ndata collection outside traditional labs. The vision is a future where\nAI-enhanced devices continuously collaborate with the human brain, offering\ntherapeutic support and deep insights into neural function, with the resulting\nreal-world context rich neural data, in turn, accelerating the development of\nmore biologically-grounded and human-centric AI.",
      "tldr_zh": "该研究提出了一种创新的双环路系统，将响应性神经刺激（RNS）植入设备与AI驱动的可穿戴设备相结合，用于创伤后应激障碍（PTSD）治疗和自然神经行为研究。在治疗模式下，植入设备监测杏仁核活动并提供电刺激，同时可穿戴设备通过多模态大语言模型（LLM）分析环境触发因素，实现精准干预；在研究模式下，该系统可实时捕捉自然场景中的大脑活动数据。这一跨学科平台有望推动PTSD个性化治疗和认知神经科学发展，并为更人性化的AI提供生物基础数据支持。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12334v2",
      "published_date": "2025-03-16 03:07:59 UTC",
      "updated_date": "2025-03-23 19:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:44:17.391221"
    },
    {
      "arxiv_id": "2503.12326v1",
      "title": "Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots",
      "title_zh": "利用多模态大语言模型的视觉能力实现图表数据自动提取",
      "authors": [
        "Maciej P. Polak",
        "Dane Morgan"
      ],
      "abstract": "Automated data extraction from research texts has been steadily improving,\nwith the emergence of large language models (LLMs) accelerating progress even\nfurther. Extracting data from plots in research papers, however, has been such\na complex task that it has predominantly been confined to manual data\nextraction. We show that current multimodal large language models, with proper\ninstructions and engineered workflows, are capable of accurately extracting\ndata from plots. This capability is inherent to the pretrained models and can\nbe achieved with a chain-of-thought sequence of zero-shot engineered prompts we\ncall PlotExtract, without the need to fine-tune. We demonstrate PlotExtract\nhere and assess its performance on synthetic and published plots. We consider\nonly plots with two axes in this analysis. For plots identified as extractable,\nPlotExtract finds points with over 90% precision (and around 90% recall) and\nerrors in x and y position of around 5% or lower. These results prove that\nmultimodal LLMs are a viable path for high-throughput data extraction for plots\nand in many circumstances can replace the current manual methods of data\nextraction.",
      "tldr_zh": "该研究提出PlotExtract方法，利用多模态大语言模型(LLMs)实现科研图表数据的自动提取。通过精心设计的零样本提示链（chain-of-thought prompts），该方法无需微调即可从二维坐标图中以超90%的准确率提取数据点，且x/y坐标误差控制在5%以内。实验表明，这种基于预训练模型固有能力的解决方案，能够替代传统人工数据提取方式，为科研数据的高通量获取提供了新途径。",
      "categories": [
        "cs.CV",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12326v1",
      "published_date": "2025-03-16 02:41:43 UTC",
      "updated_date": "2025-03-16 02:41:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:46:09.877003"
    },
    {
      "arxiv_id": "2503.12317v1",
      "title": "A Transformer-based survival model for prediction of all-cause mortality in heart failure patients: a multi-cohort study",
      "title_zh": "基于Transformer的生存预测模型：一项针对心衰患者全因死亡风险的多队列研究",
      "authors": [
        "Shishir Rao",
        "Nouman Ahmed",
        "Gholamreza Salimi-Khorshidi",
        "Christopher Yau",
        "Huimin Su",
        "Nathalie Conrad",
        "Folkert W Asselbergs",
        "Mark Woodward",
        "Rod Jackson",
        "John GF Cleland",
        "Kazem Rahimi"
      ],
      "abstract": "We developed and validated TRisk, a Transformer-based AI model predicting\n36-month mortality in heart failure patients by analysing temporal patient\njourneys from UK electronic health records (EHR). Our study included 403,534\nheart failure patients (ages 40-90) from 1,418 English general practices, with\n1,063 practices for model derivation and 355 for external validation. TRisk was\ncompared against the MAGGIC-EHR model across various patient subgroups. With\nmedian follow-up of 9 months, TRisk achieved a concordance index of 0.845 (95%\nconfidence interval: [0.841, 0.849]), significantly outperforming MAGGIC-EHR's\n0.728 (0.723, 0.733) for predicting 36-month all-cause mortality. TRisk showed\nmore consistent performance across sex, age, and baseline characteristics,\nsuggesting less bias. We successfully adapted TRisk to US hospital data through\ntransfer learning, achieving a C-index of 0.802 (0.789, 0.816) with 21,767\npatients. Explainability analyses revealed TRisk captured established risk\nfactors while identifying underappreciated predictors like cancers and hepatic\nfailure that were important across both cohorts. Notably, cancers maintained\nstrong prognostic value even a decade after diagnosis. TRisk demonstrated\nwell-calibrated mortality prediction across both healthcare systems. Our\nfindings highlight the value of tracking longitudinal health profiles and\nrevealed risk factors not included in previous expert-driven models.",
      "tldr_zh": "该研究开发并验证了TRisk，一种基于Transformer的AI模型，用于预测心力衰竭患者的36个月全因死亡率。通过对英国电子健康记录(EHR)中403,534名患者的时间序列数据进行分析，TRisk在预测准确性（C-index为0.845）上显著优于现有的MAGGIC-EHR模型（C-index为0.728），并在不同性别、年龄和基线特征的患者中表现更一致。通过迁移学习，TRisk成功适应美国医院数据，C-index达到0.802。可解释性分析表明，TRisk不仅捕捉到已知风险因素，还识别出癌症和肝衰竭等被低估的预测因子。研究强调了追踪纵向健康数据的重要性，并揭示了传统专家驱动模型中未包含的风险因素。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12317v1",
      "published_date": "2025-03-16 01:53:50 UTC",
      "updated_date": "2025-03-16 01:53:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:46:47.643963"
    },
    {
      "arxiv_id": "2503.12307v1",
      "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene",
      "title_zh": "Swift4D：基于自适应分治策略的高斯泼溅算法——面向动态场景的紧凑高效重建",
      "authors": [
        "Jiahao Wu",
        "Rui Peng",
        "Zhiyan Wang",
        "Lu Xiao",
        "Luyang Tang",
        "Jinbo Yan",
        "Kaiqiang Xiong",
        "Ronggang Wang"
      ],
      "abstract": "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps://github.com/WuJH2001/swift4d.",
      "tldr_zh": "Swift4D提出了一种基于分治策略的自适应3D高斯泼溅方法，用于高效紧凑的动态场景重建。该方法通过可学习的分解策略将场景元素分类为静态和动态基元，仅对动态部分采用紧凑的多分辨率4D哈希映射器进行变形建模，从而显著降低了存储需求和训练时间。实验表明，Swift4D在保持最先进渲染质量的同时，训练速度比现有方法快20倍，在真实数据集上仅需30MB存储空间。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12307v1",
      "published_date": "2025-03-16 01:13:11 UTC",
      "updated_date": "2025-03-16 01:13:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T14:46:39.956951"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 64,
  "processed_papers_count": 64,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T14:47:57.589839"
}
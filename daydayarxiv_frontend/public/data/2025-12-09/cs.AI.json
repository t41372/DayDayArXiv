{
  "date": "2025-12-09",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-09 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„é¢†è¯»å‘˜ã€‚ä»Šå¤©çš„ arXiv åˆ—è¡¨å¯è°“æ˜¯â€œç¥ä»™æ‰“æ¶â€ï¼Œä¿¡æ¯é‡æå¤§ã€‚**ä¸€å¥è¯æ€»ç»“ï¼š** æˆ‘ä»¬ä¸ä»…è§è¯äº† **GPT-5 å’Œ Gemini 3.0 Pro** åœ¨é«˜éš¾åº¦é‡‘èè€ƒè¯•ï¼ˆCFAï¼‰ä¸­çš„å± æ¦œè¡¨ç°ï¼Œè¿˜è¿æ¥äº†å…³äº **Agent ç³»ç»Ÿæ‰©å±•å®šå¾‹ï¼ˆScaling Lawsï¼‰** çš„é‡ç£…ç ”ç©¶ï¼›æ­¤å¤–ï¼ŒSergey Levine å›¢é˜Ÿåœ¨æœºå™¨äººå¾®è°ƒã€MIT åœ¨åŒ»ç–—å†³ç­–ä¼˜åŒ–ã€ä»¥åŠæ–¯å¦ç¦åœ¨ 3D ç”Ÿæˆé¢†åŸŸå‡æœ‰é‡è¦å·¥ä½œå‘å¸ƒã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œå…ˆçœ‹æœ€é‡ç£…çš„å‡ ç¯‡ã€‚\n\n---\n\n### ğŸš€ é‡ç‚¹å…³æ³¨ï¼šå¤§æ¨¡å‹æµ‹è¯„ä¸ Agent ç§‘å­¦\n\n**1. æ¨ç†æ¨¡å‹é€šå…³ CFA è€ƒè¯•**\n**Reasoning Models Ace the CFA Exams**\n> **Authors:** Jaisal Patel et al.\n> **Keyword:** CFA, GPT-5, Gemini 3.0 Pro, Finance\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** è¿™ç¯‡æ–‡ç« ä¸ä»…æ˜¯ä¸€ä¸ª Benchmark æŠ¥å‘Šï¼Œæ›´åƒæ˜¯æœªæ¥æ¨¡å‹èƒ½åŠ›çš„â€œå‰§é€â€ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** ä¹‹å‰çš„ç ”ç©¶è®¤ä¸º LLM åœ¨ CFAï¼ˆç‰¹è®¸é‡‘èåˆ†æå¸ˆï¼‰è€ƒè¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œä½†æœ¬æ–‡æµ‹è¯•äº†æœ€æ–°çš„**æ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰**ï¼Œç»“æœæƒŠäººã€‚\n- **æ’åï¼š** Gemini 3.0 Pro > Gemini 2.5 Pro > GPT-5 > Grok 4 > Claude Opus 4.1 > DeepSeek-V3.1ã€‚\n- **æ•°æ®ï¼š** Gemini 3.0 Pro åœ¨ Level I è€ƒè¯•ä¸­æ‹¿ä¸‹äº†åˆ›çºªå½•çš„ **97.6%**ã€‚GPT-5 åœ¨ Level II é¢†è·‘ (94.3%)ã€‚è¿™æ ‡å¿—ç€é€šç”¨æ¨ç†æ¨¡å‹åœ¨ä¸“ä¸šé‡‘èé¢†åŸŸçš„ç†è§£åŠ›å·²ç»è¾¾åˆ°ç”šè‡³è¶…è¶Šäº†äººç±»ä¸“å®¶æ°´å¹³ã€‚\n\n**2. è¿ˆå‘ Agent ç³»ç»Ÿæ‰©å±•çš„ç§‘å­¦**\n**Towards a Science of Scaling Agent Systems**\n> **Authors:** Yubin Kim et al. (åŒ…å« CMU, MIT ç­‰å¤šæœºæ„)\n> **Keyword:** Agent Scaling Laws, Multi-Agent, Coordination\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** Agent ç‰ˆçš„ \"Scaling Laws\"ï¼Œæƒ³åšå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¿…è¯»ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** ä½œè€…è¯•å›¾æ¨å¯¼ Agent ç³»ç»Ÿçš„é‡åŒ–æ‰©å±•åŸåˆ™ã€‚é€šè¿‡ 180 ç§é…ç½®çš„è¯„ä¼°ï¼Œå‘ç°äº†ä¸‰ä¸ªå…³é”®æ•ˆåº”ï¼š\n1.  **å·¥å…·-åä½œæƒè¡¡ï¼š** åœ¨è®¡ç®—é¢„ç®—å›ºå®šçš„æƒ…å†µä¸‹ï¼Œå·¥å…·å¯†é›†å‹ä»»åŠ¡ä¼šå› å¤šæ™ºèƒ½ä½“åä½œçš„å¼€é”€è€Œå—æŸã€‚\n2.  **èƒ½åŠ›é¥±å’Œï¼š** ä¸€æ—¦å•ä½“ Agent çš„åŸºçº¿è¶…è¿‡ 45%ï¼Œåä½œå¸¦æ¥çš„æ”¶ç›Šå°±ä¼šé€’å‡ç”šè‡³ä¸ºè´Ÿã€‚\n3.  **æ‹“æ‰‘ä¾èµ–çš„é”™è¯¯æ”¾å¤§ï¼š** ç‹¬ç«‹çš„ Agent ä¼šæ”¾å¤§é”™è¯¯ï¼ˆ17.2å€ï¼‰ï¼Œè€Œä¸­å¿ƒåŒ–åä½œèƒ½å°†å…¶æ§åˆ¶åœ¨ 4.4 å€ã€‚ç»“è®ºæ˜¯ï¼š**å¹¶è¡Œä»»åŠ¡ç”¨ä¸­å¿ƒåŒ–åä½œï¼ŒWeb å¯¼èˆªç”¨å»ä¸­å¿ƒåŒ–ï¼Œä½†åºåˆ—æ¨ç†ä»»åŠ¡ç›®å‰çš„å¤šæ™ºèƒ½ä½“æ¶æ„åè€Œä¼šå¸®å€’å¿™ã€‚**\n\n**3. æ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„ LLM æ¢ç´¢ (ACDC)**\n**LLMs for Analog Circuits Design Continuum (ACDC)**\n> **Authors:** Yasaman Esfandiari et al.\n> **Keyword:** Analog Circuit, Engineering, Reliability\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** æ¨¡æ‹Ÿç”µè·¯è®¾è®¡æ˜¯å·¥ç¨‹é¢†åŸŸçš„â€œç¡¬éª¨å¤´â€ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** ç ”ç©¶äº† LLM åœ¨æ¨¡æ‹Ÿç”µè·¯è®¾è®¡ä¸­çš„å¯é æ€§ã€‚å‘ç°æ¨¡å‹å¯¹æ•°æ®æ ¼å¼æåº¦æ•æ„Ÿï¼Œç”Ÿæˆçš„ç”µè·¯è®¾è®¡ä¸ç¨³å®šï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ç”µè·¯é…ç½®ã€‚è¿™ç¯‡æ–‡ç« ç»™ç›®å‰ç«çƒ­çš„â€œAI èŠ¯ç‰‡è®¾è®¡â€æ³¼äº†ä¸€ç›†ç†æ€§çš„å†·æ°´ï¼šåœ¨éœ€è¦ä¸¥æ ¼ç‰©ç†çº¦æŸçš„å·¥ç¨‹ä»»åŠ¡ä¸­ï¼ŒLLM è·ç¦»å®Œå…¨è‡ªåŠ¨åŒ–è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ä¸æœºå™¨äºº (Embodied AI)\n\n**4. é€šè¿‡å‚æ•°åˆå¹¶å®ç°è§†è§‰-è¯­è¨€-åŠ¨ä½œæœºå™¨äººç­–ç•¥çš„é²æ£’å¾®è°ƒ**\n**Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging**\n> **Authors:** Yajat Yadav, ..., **Sergey Levine** (UC Berkeley)\n> **Keyword:** Robot Learning, Finetuning, Model Merging\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** Sergey Levine å›¢é˜Ÿå‡ºå“ã€‚è§£å†³é€šæ‰æœºå™¨äººâ€œå­¦äº†æ–°ä»»åŠ¡ï¼Œå¿˜äº†æ—§æœ¬äº‹â€çš„é—®é¢˜ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** é€šç”¨æœºå™¨äººç­–ç•¥ï¼ˆGeneralist Policiesï¼‰åœ¨å¾®è°ƒæ–°ä»»åŠ¡æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆä¸”ä¸§å¤±é€šç”¨æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æå…¶ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼š**æ¨¡å‹åˆå¹¶ï¼ˆModel Mergingï¼‰**ï¼Œå³åœ¨å¾®è°ƒåçš„æ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹ä¹‹é—´è¿›è¡Œæƒé‡æ’å€¼ã€‚è¿™ä¸ä»…ä¿ç•™äº†é€šç”¨èƒ½åŠ›ï¼Œç”šè‡³åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰çš„æ–°ä»»åŠ¡å˜ä½“ä¸Šè¡¨ç°å¾—æ¯”å•çº¯å¾®è°ƒè¿˜è¦å¥½ã€‚\n\n**5. Lumo-1: è¿æ¥æ€ç»´ä¸è¡ŒåŠ¨çš„é€šç”¨æœºå™¨äººæ¨¡å‹**\n**Mind to Hand: Purposeful Robotic Control via Embodied Reasoning**\n> **Authors:** Peijun Tang et al.\n> **Keyword:** VLA Model, Astribot S1, Embodied Reasoning\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** æ¨å‡ºäº† Lumo-1 æ¨¡å‹ï¼Œå¼ºè°ƒâ€œæ¨ç†â€åˆ°â€œè¡ŒåŠ¨â€çš„é—­ç¯ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹ã€‚è®­ç»ƒåˆ†ä¸ºä¸‰é˜¶æ®µï¼šVLM é¢„è®­ç»ƒå¢å¼ºæ¨ç† -> è·¨å…·èº«æ•°æ®ååŒè®­ç»ƒ -> åœ¨ Astribot S1ï¼ˆåŒè‡‚ç§»åŠ¨æ“ä½œå™¨ï¼‰ä¸Šé‡‡é›†çš„é«˜è´¨é‡è½¨è¿¹è¿›è¡ŒåŠ¨ä½œè®­ç»ƒã€‚Lumo-1 åœ¨é•¿ç¨‹ä»»åŠ¡å’Œç†è§£äººç±»è‡ªç„¶æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\n\n**6. Masked Generative Policy: æœºå™¨äººæ§åˆ¶çš„æ©ç ç”Ÿæˆç­–ç•¥**\n**Masked Generative Policy for Robotic Control**\n> **Authors:** Lipeng Zhuang et al.\n> **Keyword:** Imitation Learning, Masked Generation\n\n**ä¸»è¦è´¡çŒ®ï¼š** æå‡º MGPï¼Œå°†åŠ¨ä½œè§†ä¸ºç¦»æ•£ Tokenï¼Œåˆ©ç”¨æ©ç  Transformer å¹¶è¡Œç”Ÿæˆã€‚ç›¸æ¯” Diffusion ç­–ç•¥ï¼ŒMGP åœ¨æ¨ç†é€Ÿåº¦ä¸Šå¿«äº† **35å€**ï¼Œä¸”åœ¨ Meta-World å’Œ LIBERO åŸºå‡†ä¸ŠæˆåŠŸç‡æå‡æ˜¾è‘—ï¼Œç‰¹åˆ«é€‚åˆéé©¬å°”å¯å¤«ä»»åŠ¡ã€‚\n\n---\n\n### ğŸ§¬ AI for Science & Medicine (åŒ»ç–—ä¸ç§‘å­¦)\n\n**7. ç»å¯¼ç®¡ä¸»åŠ¨è„‰ç“£ç½®æ¢æœ¯ (TAVR) çš„æœ€ä½³ç“£è†œå¤„æ–¹ï¼šæœºå™¨å­¦ä¹ æ–¹æ³•**\n**Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach**\n> **Authors:** Phevos Paschalidis, ..., **Dimitris Bertsimas** (MIT)\n> **Keyword:** TAVR, Clinical Support, Optimization\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** è¿ç­¹å­¦å¤§ä½¬ Bertsimas çš„æ–°ä½œï¼ŒAI åœ¨é«˜é£é™©æ‰‹æœ¯å†³ç­–ä¸­çš„è½åœ°ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** TAVR æ‰‹æœ¯ä¸­é€‰ä»€ä¹ˆç“£è†œä¸€ç›´æœ‰äº‰è®®ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªç»“åˆç¾å›½å’Œå¸Œè…Šæ‚£è€…æ•°æ®çš„æ–°æ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„ä¸´åºŠæ”¯æŒå·¥å…·ï¼Œæ—¨åœ¨æœ€å°åŒ–æœ¯åæ°¸ä¹…æ€§èµ·æå™¨æ¤å…¥ï¼ˆPPIï¼‰çš„é£é™©ã€‚ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”ç›®å‰çš„æ ‡å‡†æŠ¤ç†ï¼Œè¯¥æ¨¡å‹èƒ½å°† PPI ç‡é™ä½ **16%-26%**ã€‚\n\n**8. Alexandria æ•°æ®åº“çš„ AI é©±åŠ¨æ‰©å±•ä¸åº”ç”¨**\n**AI-Driven Expansion and Application of the Alexandria Database**\n> **Authors:** ThÃ©o Cavignac et al.\n> **Keyword:** Materials Discovery, DFT, GNN\n\n**ä¸»è¦è´¡çŒ®ï¼š** åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œï¼Œç”Ÿæˆäº† 1.19 äº¿ä¸ªå€™é€‰ç»“æ„ï¼Œå¹¶å‘ ALEXANDRIA æ•°æ®åº“æ·»åŠ äº† **130 ä¸‡ä¸ª** ç»è¿‡ DFT éªŒè¯çš„åŒ–åˆç‰©ã€‚è¿™æ˜¯ææ–™å‘ç°é¢†åŸŸçš„ä¸€ä¸ªå·¨å¤§çš„æ•°æ®æ‰©å……ï¼ŒåŒ…å« 7.4 ä¸‡ç§æ–°çš„ç¨³å®šææ–™ã€‚\n\n**9. ç”Ÿç‰©å¨èƒåŸºå‡†ç”Ÿæˆæ¡†æ¶ (ç³»åˆ—ä¸‰ç¯‡)**\n**Biothreat Benchmark Generation Framework (I, II, III)**\n> **Authors:** Gary Ackerman et al.\n> **Keyword:** Biosecurity, LLM Safety, Bacterial Biothreat\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** é’ˆå¯¹ LLM å¯èƒ½è¢«ç”¨äºç”Ÿç‰©ææ€–ä¸»ä¹‰çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªéå¸¸è¯¦å°½çš„ç³»åˆ—å·¥ä½œï¼ˆI: ä»»åŠ¡-æŸ¥è¯¢æ¶æ„, II: åŸºå‡†ç”Ÿæˆ, III: æ•°æ®é›†å®ç°ï¼‰ã€‚ä½œè€…å¼€å‘äº† **Bacterial Biothreat Benchmark (B3)**ï¼ŒåŒ…å« 1000+ ä¸ªç»è¿‡çº¢é˜Ÿæµ‹è¯•å’Œä¸“å®¶å»é‡çš„åŸºå‡†æµ‹è¯•é¢˜ï¼Œç”¨äºè¯„ä¼°å‰æ²¿æ¨¡å‹åœ¨ç”Ÿç‰©å®‰å…¨æ–¹é¢çš„é£é™©ï¼ˆå¦‚ååŠ©åˆ¶é€ ç”Ÿç‰©æ­¦å™¨ï¼‰ã€‚è¿™æ˜¯ç›®å‰è¯¥é¢†åŸŸæœ€ç³»ç»Ÿçš„å…¬å¼€åŸºå‡†ä¹‹ä¸€ã€‚\n\n---\n\n### ğŸ¨ è§†è§‰ç”Ÿæˆä¸ 3D (Vision & Generation)\n\n**10. WonderZoom: å¤šå°ºåº¦ 3D ä¸–ç•Œç”Ÿæˆ**\n**WonderZoom: Multi-Scale 3D World Generation**\n> **Authors:** Jin Cao, Hong-Xing Yu, **Jiajun Wu** (Stanford)\n> **Keyword:** 3D Generation, Multi-scale, Gaussian Surfels\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** ä»ä¸€å¼ å›¾ç”Ÿæˆå¯ä»¥æ— é™â€œæ”¾å¤§â€çš„ 3D ä¸–ç•Œã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** è§£å†³äº†ç°æœ‰ 3D ç”Ÿæˆæ¨¡å‹ç¼ºä¹å¤šå°ºåº¦èƒ½åŠ›çš„é—®é¢˜ã€‚WonderZoom åˆ©ç”¨å°ºåº¦è‡ªé€‚åº”çš„é«˜æ–¯ Surfel å’Œæ¸è¿›å¼ç»†èŠ‚åˆæˆå™¨ï¼Œå…è®¸ç”¨æˆ·ä»é£æ™¯å®è§‚è§†è§’ä¸€è·¯ Zoom åˆ°å¾®è§‚ç‰¹å¾ï¼Œå¹¶è‡ªåŠ¨è¡¥å…¨ç»†èŠ‚ã€‚\n\n**11. Astra: å…·æœ‰è‡ªå›å½’å»å™ªçš„é€šç”¨äº¤äº’å¼ä¸–ç•Œæ¨¡å‹**\n**Astra: General Interactive World Model with Autoregressive Denoising**\n> **Authors:** Yixuan Zhu et al.\n> **Keyword:** World Model, Video Generation, Action Control\n\n**ä¸»è¦è´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼é€šç”¨ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½æ ¹æ®è¿‡å»çš„è§‚å¯Ÿå’Œ**åŠ¨ä½œ**é¢„æµ‹é•¿ç¨‹æœªæ¥ã€‚é€‚ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæŠ“å–ç­‰å¤šç§åœºæ™¯ã€‚æ ¸å¿ƒåœ¨äºå¼•å…¥äº†åŠ¨ä½œæ„ŸçŸ¥çš„ Adapterï¼Œèƒ½ç²¾ç¡®æ§åˆ¶ç”Ÿæˆçš„è§†é¢‘å†…å®¹ï¼ˆå¦‚æ‘„åƒå¤´è¿åŠ¨æˆ–æœºæ¢°è‡‚åŠ¨ä½œï¼‰ã€‚\n\n**12. Terrain Diffusion: æ›¿ä»£ Perlin å™ªå£°çš„æ— é™å®æ—¶åœ°å½¢ç”Ÿæˆ**\n**Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation**\n> **Authors:** Alexander Goslin\n> **Keyword:** Procedural Generation, Diffusion Models, Game Dev\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** æ¸¸æˆå¼€å‘è€…çš„ç¦éŸ³ï¼Œç”¨ Diffusion å–ä»£ç»å…¸çš„ Perlin å™ªå£°ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** æå‡ºäº† InfiniteDiffusionï¼Œä¸€ç§é’ˆå¯¹æ— ç•ŒåŸŸçš„æ‰©æ•£é‡‡æ ·ç®—æ³•ã€‚å®ƒåœ¨æ¶ˆè´¹è€…çº§ GPU ä¸Šçš„ç”Ÿæˆé€Ÿåº¦æå¿«ï¼ˆæ¯”è½¨é“é€Ÿåº¦å¿«9å€ï¼‰ï¼Œèƒ½å®æ—¶ç”Ÿæˆé€¼çœŸã€æ— é™ã€ç§å­ä¸€è‡´çš„åœ°å½¢ï¼Œä¸”æ”¯æŒå±‚çº§åŒ–ç»†èŠ‚æ§åˆ¶ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€éšç§ä¸å¯¹é½ (Safety & Alignment)\n\n**13. WOLF: åŸºäºâ€œç‹¼äººæ€â€çš„ LLM æ¬ºéª—ä¸è°è¨€è§‚å¯Ÿ**\n**WOLF: Werewolf-based Observations for LLM Deception and Falsehoods**\n> **Authors:** Mrinal Agarwal et al. (NeurIPS 2025 Spotlight)\n> **Keyword:** Deception, Multi-agent, Social Deduction\n\n**ä¸»è¦è´¡çŒ®ï¼š** ç°æœ‰çš„æ¬ºéª—è¯„ä¼°å¤§å¤šæ˜¯é™æ€åˆ†ç±»ã€‚WOLF æ˜¯ä¸€ä¸ªåŸºäºç‹¼äººæ€çš„å¤šæ™ºèƒ½ä½“åŸºå‡†ï¼Œæ•æ‰åŠ¨æ€æ¬ºéª—ã€‚ç ”ç©¶å‘ç° LLM (ç‹¼äºº) åœ¨ 31% çš„å›åˆä¸­ä¼šæ’’è°ï¼Œè€ŒåŒä¼´æ£€æµ‹å‡†ç¡®ç‡çº¦ 52%ã€‚æœ‰è¶£çš„å‘ç°æ˜¯ï¼šéšç€æ¸¸æˆè¿›è¡Œï¼Œå¯¹ç‹¼äººçš„æ€€ç–‘åº¦ä¼šä¸Šå‡ï¼Œä½†å¯¹å¥½äººçš„è¯¯ä¼¤ä¸ä¼šå¢åŠ ï¼Œè¯´æ˜**é•¿æœŸäº¤äº’æœ‰åŠ©äºè¯†ç ´è°è¨€**ã€‚\n\n**14. ReasonBreak: é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„åœ°ç†éšç§ä¿æŠ¤**\n**Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models**\n> **Authors:** Jiaming Zhang et al.\n> **Keyword:** Privacy, Adversarial Attack, Multimodal Reasoning\n\n**ä¸»è¦è´¡çŒ®ï¼š** é’ˆå¯¹ç°åœ¨çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆå¦‚ GPT-4o, Geminiï¼‰èƒ½é€šè¿‡èƒŒæ™¯ç»†èŠ‚æ¨æ–­ç²¾ç¡®åœ°ç†ä½ç½®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¯¹æŠ—æ”»å‡» ReasonBreakã€‚å®ƒä¸æ˜¯åŠ ç®€å•çš„å™ªå£°ï¼Œè€Œæ˜¯é’ˆå¯¹æ¨ç†é“¾ä¸­çš„æ¦‚å¿µä¾èµ–å…³ç³»è¿›è¡Œæ‰°åŠ¨ï¼Œèƒ½æœ‰æ•ˆæ‰“æ–­æ¨¡å‹çš„å±‚çº§æ¨ç†è¿‡ç¨‹ã€‚\n\n**15. æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (MCP) ç”Ÿæ€ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸ç³»ç»ŸåŒ–çŸ¥è¯†**\n**Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem**\n> **Authors:** Shiva Gaire et al.\n> **Keyword:** MCP, Agent Security, Tool Use\n\n**æ ¸å¿ƒçœ‹ç‚¹ï¼š** MCP æœ€è¿‘å¾ˆç«ï¼Œè¢«ç§°ä¸º AI Agent çš„ USB-C æ¥å£ã€‚\n**ä¸»è¦è´¡çŒ®ï¼š** ç³»ç»Ÿæ€§åˆ†æäº† MCP çš„å¨èƒæ¨¡å‹ã€‚æŒ‡å‡ºäº†å½“ä¸Šä¸‹æ–‡å’Œæ‰§è¡Œè§£è€¦æ—¶ï¼Œ**å¹»è§‰ï¼ˆEpistemic errorsï¼‰** å’Œ **å®‰å…¨æ¼æ´ï¼ˆSecurity breachesï¼‰** çš„ç•Œé™å˜å¾—æ¨¡ç³Šã€‚æ”»å‡»è€…å¯ä»¥åˆ©ç”¨ä¸Šä¸‹æ–‡æ¥è§¦å‘å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æœªæˆæƒæ“ä½œã€‚\n\n---\n\n### âš¡ å…¶ä»–å€¼å¾—ä¸€çœ‹çš„è®ºæ–‡\n\n*   **[Hardware]** **InfiniteVL**: ç»“åˆçº¿æ€§æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£ï¼Œå®ç°æ— é™è¾“å…¥çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦æå‡ 3.6 å€ä¸”æ˜¾å­˜å ç”¨æ’å®š (#42)ã€‚\n*   **[Benchmark]** **EcomBench**: ä¸€ä¸ªä¸“æ³¨äº**ç”µå•†é¢†åŸŸ**çš„ Agent å…¨æ–¹ä½åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æœç´¢ã€æ¨ç†å’Œè·¨æºçŸ¥è¯†æ•´åˆ (#39)ã€‚\n*   **[Finance]** **Long-only cryptocurrency portfolio management**: ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹åŠ å¯†è´§å¸æ”¶ç›Šæ’åæ¥æ„å»ºæŠ•èµ„ç»„åˆï¼Œå¹´åŒ–å›æŠ¥ç‡ 64% (#152)ã€‚\n*   **[Quantum]** **SAQ-Decoder**: é’ˆå¯¹é‡å­çº é”™çš„ Transformer è§£ç å™¨ï¼Œåœ¨å‡†ç¡®ç‡æ¥è¿‘æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„åŒæ—¶ä¿æŒçº¿æ€§è®¡ç®—å¤æ‚åº¦ (#30)ã€‚\n*   **[Social]** **The High Cost of Incivility**: ç”¨ LLM æ¨¡æ‹ŸèŒåœºè¾©è®ºï¼Œå‘ç°â€œæœ‰æ¯’â€çš„æ²Ÿé€šä¼šè®©è¾¾æˆå…±è¯†çš„æ—¶é—´å¢åŠ  25%ï¼Œè¿™è¢«ç§°ä¸ºâ€œæ¯’æ€§å»¶è¿Ÿâ€ (#107)ã€‚\n\nä»Šå¤©çš„æ—¥æŠ¥å°±åˆ°è¿™é‡Œï¼Œå¸Œæœ›è¿™äº›å‰æ²¿è¿›å±•èƒ½ç»™ä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2512.09199v1",
      "title": "LLMs for Analog Circuit Design Continuum (ACDC)",
      "title_zh": "é¢å‘æ¨¡æ‹Ÿç”µè·¯è®¾è®¡è¿ç»­ä½“ (ACDC) çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yasaman Esfandiari",
        "Jocelyn Rego",
        "Austin Meyer",
        "Jonathan Gallagher",
        "Mia Levy"
      ],
      "abstract": "Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿç”µè·¯è®¾è®¡(Analog Circuit Design)è¿™ä¸€éœ€è¦é¢†åŸŸç‰¹å®šæ¨ç†å’Œç‰©ç†çº¦æŸçš„å·¥ç¨‹ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ä¸ä¸€è‡´æ€§ã€‚å®éªŒé‡ç‚¹ç ”ç©¶äº†åœ¨äººç±»å‚ä¸(Human-in-the-loop)çš„è¾…åŠ©è®¾è®¡æµç¨‹ä¸­ï¼Œä¸åŒæ•°æ®è¡¨ç¤º(Data Representations)å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºã€‚ç ”ç©¶å¯¹æ¯”äº†åŒ…æ‹¬T5ã€GPT-2åœ¨å†…çš„å°å‹æ¨¡å‹ä»¥åŠMistral-7Bã€GPT-oss-20Bç­‰å¤§å‹åŸºç¡€æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¯é æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹æ•°æ®æ ¼å¼çš„é«˜åº¦æ•æ„Ÿã€ç”Ÿæˆè®¾è®¡çš„ä¸ç¨³å®šæ€§ä»¥åŠå¯¹æœªè§è¿‡çš„ç”µè·¯é…ç½®(Unseen Circuit Configurations)æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è¯¥å·¥ä½œä¸ºLLMsä½œä¸ºå¢å¼ºäººç±»å·¥ç¨‹èƒ½åŠ›å·¥å…·çš„æ½œåŠ›ä¸å±€é™æ€§æä¾›äº†æ—©æœŸè¯æ®ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘é€‚ç”¨äºç°å®ä¸–ç•Œç»“æ„åŒ–åº”ç”¨ã€æ›´å…·å¯é æ€§å’Œå¯éƒ¨ç½²æ€§çš„åŸºç¡€æ¨¡å‹æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09199v1",
      "published_date": "2025-12-09 23:57:28 UTC",
      "updated_date": "2025-12-09 23:57:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:55:59.606127+00:00"
    },
    {
      "arxiv_id": "2512.09198v1",
      "title": "Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach",
      "title_zh": "è¿ˆå‘ç»å¯¼ç®¡ä¸»åŠ¨è„‰ç“£ç½®æ¢æœ¯ï¼ˆTAVRï¼‰æ‰‹æœ¯çš„æœ€ä¼˜ç“£è†œå¤„æ–¹ï¼šåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•",
      "authors": [
        "Phevos Paschalidis",
        "Vasiliki Stoumpou",
        "Lisa Everest",
        "Yu Ma",
        "Talhat Azemi",
        "Jawad Haider",
        "Steven Zweibel",
        "Eleftherios M. Protopapas",
        "Jeff Mather",
        "Maciej Tysarowski",
        "George E. Sarris",
        "Robert C. Hagberg",
        "Howard L. Haronian",
        "Dimitris Bertsimas"
      ],
      "abstract": "Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»å¯¼ç®¡ä¸»åŠ¨è„‰ç“£ç½®æ¢æœ¯(TAVR)ä¸­ç“£è†œç±»å‹é€‰æ‹©å­˜åœ¨çš„äº‰è®®ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„ä¸´åºŠå†³ç­–æ”¯æŒå·¥å…·ï¼Œæ—¨åœ¨æœ€å°åŒ–æœ¯åä¸»è¦å¹¶å‘ç—‡æ°¸ä¹…èµ·æå™¨æ¤å…¥(PPI)çš„é£é™©ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•´åˆç¾å›½å’Œå¸Œè…Šæ‚£è€…çš„äººå£ç»Ÿè®¡å­¦ã€è®¡ç®—æœºæ–­å±‚æ‰«æ(CT)åŠè¶…å£°å¿ƒåŠ¨å›¾(echocardiograms)æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªè·¨å›½å¼‚æ„æ•°æ®é›†ã€‚é€šè¿‡å¼•å…¥å¶çº§åˆ†æ(leaf-level analysis)æ–¹æ³•ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨äº†äººç¾¤å¼‚è´¨æ€§ï¼Œå¹¶é¿å…äº†å¯¹ä¸ç¡®å®šåäº‹å®é£é™©ä¼°è®¡çš„ä¾èµ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰çš„æ ‡å‡†æ²»ç–—æ–¹æ¡ˆç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å†…éƒ¨ç¾å›½ç¾¤ä½“å’Œå¤–éƒ¨å¸Œè…ŠéªŒè¯é˜Ÿåˆ—ä¸­åˆ†åˆ«å®ç°äº†26%å’Œ16%çš„PPIå‘ç”Ÿç‡é™å¹…ã€‚è¯¥é¡¹å·¥ä½œä»£è¡¨äº†TAVRé¢†åŸŸé¦–ä¸ªé’ˆå¯¹ç»å¯¼ç®¡å¿ƒè„ç“£è†œ(THV)é€‰æ‹©çš„ç»Ÿä¸€ä¸”ä¸ªæ€§åŒ–çš„å¤„æ–¹ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09198v1",
      "published_date": "2025-12-09 23:46:46 UTC",
      "updated_date": "2025-12-09 23:46:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:56:03.165017+00:00"
    },
    {
      "arxiv_id": "2512.09190v1",
      "title": "Understanding Mental States in Active and Autonomous Driving with EEG",
      "title_zh": "åŸºäº EEG çš„ä¸»åŠ¨é©¾é©¶ä¸è‡ªåŠ¨é©¾é©¶å¿ƒç†çŠ¶æ€ç ”ç©¶",
      "authors": [
        "Prithila Angkan",
        "Paul Hungler",
        "Ali Etemad"
      ],
      "abstract": "Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨EEGè„‘ç”µæŠ€æœ¯é¦–æ¬¡å¯¹æ¯”äº†ä¸»åŠ¨é©¾é©¶ä¸è‡ªåŠ¨é©¾é©¶æ¨¡å¼ä¸‹é©¾é©¶å‘˜çš„è®¤çŸ¥è´Ÿè·(cognitive load)ã€ç–²åŠ³åº¦(fatigue)ã€æƒ…æ„Ÿæ•ˆä»·(valence)å’Œå”¤é†’åº¦(arousal)ç­‰å¿ƒç†çŠ¶æ€ã€‚é€šè¿‡å¯¹31åå—è¯•è€…åœ¨ä¸åŒä»»åŠ¡å¤æ‚åº¦ä¸‹çš„æ—¶é—´æ¨¡å¼å’Œé€šé“æ¿€æ´»å·®å¼‚è¿›è¡Œåˆ†æï¼Œå‘ç°ä¸¤ç§æ¨¡å¼çš„ç¥ç»æ¿€æ´»å­˜åœ¨æ˜¾è‘—çš„åˆ†å¸ƒåç§»(distribution shift)ã€‚è¿ç§»å­¦ä¹ (Transfer-learning)å®éªŒè¯å®ï¼Œåœ¨ä¸»åŠ¨é©¾é©¶æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–è‡³è‡ªåŠ¨é©¾é©¶åœºæ™¯ã€‚è¿™ç§åç§»ä¸»è¦å½’å› äºä¸¤ç§æ¨¡å¼åœ¨è¿åŠ¨å‚ä¸(motor engagement)å’Œæ³¨æ„åŠ›éœ€æ±‚(attentional demands)ä¸Šçš„å·®å¼‚ï¼Œå¯¼è‡´äº†ç‹¬ç‰¹çš„ç©ºé—´å’Œæ—¶é—´EEGæ¿€æ´»æ¨¡å¼ã€‚å°½ç®¡è‡ªåŠ¨é©¾é©¶å¼•å‘çš„æ•´ä½“çš®å±‚æ¿€æ´»è¾ƒä½ï¼Œä½†å—è¯•è€…åœ¨åº”å¯¹å¹²é¢„å‡†å¤‡ã€æƒ…ç»ªååº”åŠå•è°ƒå¯¼è‡´çš„è¢«åŠ¨ç–²åŠ³æ—¶ä»è¡¨ç°å‡ºå¿ƒç†çŠ¶æ€çš„æ˜¾è‘—æ³¢åŠ¨ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œå¼€å‘ä¸‹ä¸€ä»£è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„é©¾é©¶å‘˜ç›‘æ§ç³»ç»Ÿ(driver monitoring systems)æ—¶ï¼ŒäºŸéœ€æ„å»ºç‰¹å®šåœºæ™¯çš„æ•°æ®é›†å’Œä¸“ç”¨æ¨¡å‹ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "15 Pages, 13 Figures and 3 Tables. This work has been submitted to IEEE Transaction for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2512.09190v1",
      "published_date": "2025-12-09 23:30:52 UTC",
      "updated_date": "2025-12-09 23:30:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:55:47.951934+00:00"
    },
    {
      "arxiv_id": "2512.09187v1",
      "title": "WOLF: Werewolf-based Observations for LLM Deception and Falsehoods",
      "title_zh": "WOLFï¼šåŸºäºç‹¼äººæ€çš„å¤§è¯­è¨€æ¨¡å‹æ¬ºéª—ä¸è™šå‡è¡Œä¸ºè§‚æµ‹",
      "authors": [
        "Mrinal Agarwal",
        "Saad Rana",
        "Theo Sundoro",
        "Hermela Berhe",
        "Spencer Kim",
        "Vasu Sharma",
        "Sean O'Brien",
        "Kevin Zhu"
      ],
      "abstract": "Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†WOLFï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç‹¼äººæ€(Werewolf)æ¸¸æˆçš„å¤šæ™ºèƒ½ä½“ç¤¾äº¤æ¼”ç»åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¬ºéª—è¡Œä¸ºè¯„ä¼°ä¸­ç¼ºä¹åŠ¨æ€äº¤äº’ä¸å¯¹æŠ—ç‰¹æ€§çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡LangGraphçŠ¶æ€æœºæ„å»ºäº†åŒ…å«Villagerã€Werewolfã€Seerå’ŒDoctorç­‰è§’è‰²çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨ä¸¥æ ¼çš„æ˜¼å¤œå¾ªç¯ã€è¾©è®ºè½®æ¬¡å’Œå¤šæ•°è¡¨å†³æœºåˆ¶ï¼Œå®ç°äº†æ¬ºéª—äº§ç”Ÿ(Deception Production)ä¸æ£€æµ‹(Detection)çš„åˆ†ç¦»æµ‹é‡ã€‚ç ”ç©¶é‡‡ç”¨æ ‡å‡†åŒ–åˆ†ç±»æ³•å°†æ¬ºéª—è¡Œä¸ºå½’çº³ä¸ºOmissionã€Distortionã€Fabricationå’ŒMisdirectionï¼Œå¹¶é€šè¿‡çºµå‘å¹³æ»‘çš„æ€€ç–‘è¯„åˆ†æ•æ‰æ™ºèƒ½ä½“é—´çš„ä¿¡ä»»æ¼”å˜ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼ŒWerewolfåœ¨31%çš„è½®æ¬¡ä¸­äº§ç”Ÿæ¬ºéª—æ€§é™ˆè¿°ï¼Œè€ŒåŒä¼´æ£€æµ‹çš„Precisionè¾¾åˆ°71-73%ï¼Œä¸”å¯¹Werewolfçš„æ€€ç–‘åº¦éšè½®æ¬¡å¢åŠ ä»çº¦52%æ˜¾è‘—æå‡è‡³60%ä»¥ä¸Šã€‚åˆ†æè¡¨æ˜ï¼Œå»¶é•¿çš„äº¤äº’è¿‡ç¨‹èƒ½å¤Ÿæœ‰æ•ˆæé«˜å¯¹è°è¨€çš„Recallï¼ŒåŒæ—¶å¯¹è¯šå®è§’è‰²çš„æ€€ç–‘ä¼šè¶‹äºç¨³å®šï¼Œä»è€Œå‡å°‘è¯¯æŠ¥ã€‚WOLFä¸ºè¡¡é‡å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“äº’åŠ¨ä¸­LLMsçš„æ¬ºéª—ä¸æ£€æµ‹èƒ½åŠ›æä¾›äº†ä¸€ä¸ªåŠ¨æ€ä¸”å—æ§çš„æµ‹è¯•å¹³å°ï¼Œæ¨åŠ¨äº†æ¬ºéª—è¯„ä¼°ä»é™æ€æ•°æ®é›†å‘å¤æ‚ç¤¾ä¼šåŠ¨åŠ›å­¦çš„è½¬å˜ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Spotlight Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.09187v1",
      "published_date": "2025-12-09 23:14:20 UTC",
      "updated_date": "2025-12-09 23:14:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:55:54.186696+00:00"
    },
    {
      "arxiv_id": "2512.09185v2",
      "title": "Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation",
      "title_zh": "åŸºäºæ½œåœ¨æµåŒ¹é…çš„æ‚£è€…ç‰¹å¼‚æ€§ç–¾ç—…åŠ¨åŠ›å­¦å­¦ä¹ ä¸çºµå‘å½±åƒç”Ÿæˆ",
      "authors": [
        "Hao Chen",
        "Rui Yin",
        "Yifan Chen",
        "Qi Chen",
        "Chao Li"
      ],
      "abstract": "Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Î”$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Î”$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $\\Delta$-LFMï¼Œä¸€ä¸ªåˆ©ç”¨ Flow Matching (FM) å»ºæ¨¡æ‚£è€…ç‰¹å®šæ½œç©ºé—´æ¼”å˜çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨æ¨¡æ‹Ÿè¿ç»­ä¸”å•è°ƒçš„ç–¾ç—…è¿›å±•æ—¶å­˜åœ¨çš„æ½œç©ºé—´ç»“æ„ç¼ºå¤±å’Œè¿ç»­æ€§ä¸­æ–­é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†ç–¾ç—…åŠ¨æ€è§†ä¸ºé€Ÿåº¦åœº (velocity field)ï¼Œé€šè¿‡ Flow Matching å¯¹é½æ‚£è€…æ•°æ®çš„æ—¶é—´æ¼”åŒ–ï¼Œæ˜¾è‘—æå‡äº†åŠ¨æ€æ¨¡æ‹Ÿçš„å¯è§£é‡Šæ€§ã€‚é’ˆå¯¹ Auto-Encoders (AEs) åœ¨æ½œç©ºé—´ä¸­æ— æ³•ä¿è¯è·¨æ‚£è€…å¯¹é½ä»¥åŠä¸ä¸´åºŠä¸¥é‡ç¨‹åº¦æŒ‡æ ‡ï¼ˆå¦‚å¹´é¾„å’Œç–¾ç—…çŠ¶æ€ï¼‰å…³è”çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å¼•å…¥äº†æ‚£è€…ç‰¹å®šæ½œå¯¹é½ (patient-specific latent alignment) æœºåˆ¶ï¼Œå¼ºåˆ¶æ‚£è€…è½¨è¿¹æ²¿ç‰¹å®šè½´éšç—…æƒ…ä¸¥é‡ç¨‹åº¦å•è°ƒå¢åŠ ï¼Œä»è€Œç¡®ä¿äº†æ½œç©ºé—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨ä¸‰ä¸ªçºµå‘ MRI åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œ$\\Delta$-LFM ä¸ä»…åœ¨ç»éªŒæ€§èƒ½ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œè¿˜ä¸ºç†è§£ã€è§£é‡Šå’Œå¯è§†åŒ–ç–¾ç—…æ¼”å˜è¿‡ç¨‹æä¾›äº†ä¸€å¥—å…¨æ–°çš„å»ºæ¨¡æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2512.09185v2",
      "published_date": "2025-12-09 23:13:54 UTC",
      "updated_date": "2025-12-13 14:47:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:55:52.211676+00:00"
    },
    {
      "arxiv_id": "2512.09172v2",
      "title": "Prompt-Based Continual Compositional Zero-Shot Learning",
      "title_zh": "åŸºäºæç¤ºçš„æŒç»­ç»„åˆå¼é›¶æ ·æœ¬å­¦ä¹ ",
      "authors": [
        "Sauda Maryam",
        "Sara Nadeem",
        "Faisal Qureshi",
        "Mohsen Ali"
      ],
      "abstract": "We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PromptCCZSL æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰åœ¨æŒç»­ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ï¼ˆContinual Compositional Zero-Shot Learning, CCZSLï¼‰ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜ä¸çŸ¥è¯†è¿ç§»æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŠ æƒå¤šæ•™å¸ˆè’¸é¦æŠ€æœ¯ä¿ç•™æ—§çŸ¥è¯†ï¼Œå¹¶ç»“åˆä¼šè¯æ„ŸçŸ¥ï¼ˆsession-awareï¼‰çš„ç»„åˆæç¤ºä¸ä¼šè¯æ— å…³ï¼ˆsession-agnosticï¼‰çš„å±æ€§åŠå¯¹è±¡æç¤ºï¼Œä»¥ç»´æŒå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡é€‚åº”æ€§ï¼Œç ”ç©¶å¼•å…¥äº†ä½™å¼¦é”šç‚¹æŸå¤±ï¼ˆCosine Anchor Lossï¼‰ç¨³å®šç‰¹å¾ï¼Œå¹¶é€šè¿‡æ­£äº¤æŠ•å½±æŸå¤±ï¼ˆOrthogonal Projection Lossï¼‰å’Œä¼šè¯å†…å¤šæ ·æ€§æŸå¤±ï¼ˆIntra-Session Diversity Lossï¼‰ç¡®ä¿æ–°æ—§åµŒå…¥çš„åŒºåˆ†åº¦åŠè¡¨å¾çš„å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPromptCCZSL åœ¨ UT-Zappos å’Œ C-GQA åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ VLM åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†æ¨¡å‹å¯¹æ–°ç»„åˆçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿä¸ºé—­ç¯ç³»ç»Ÿä¸‹çš„æŒç»­å­¦ä¹ ç ”ç©¶æä¾›äº†æ–°çš„è¯„ä¼°åè®®å’Œæœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09172v2",
      "published_date": "2025-12-09 22:36:31 UTC",
      "updated_date": "2025-12-17 12:41:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:56:03.858805+00:00"
    },
    {
      "arxiv_id": "2512.09169v1",
      "title": "AI-Driven Expansion and Application of the Alexandria Database",
      "title_zh": "AIé©±åŠ¨çš„Alexandriaæ•°æ®åº“æ‰©å±•ä¸åº”ç”¨",
      "authors": [
        "ThÃ©o Cavignac",
        "Jonathan Schmidt",
        "Pierre-Paul De Breuck",
        "Antoine Loew",
        "Tiago F. T. Cerqueira",
        "Hai-Chen Wang",
        "Anton Bochkarev",
        "Yury Lysogorskiy",
        "Aldo H. Romero",
        "Ralf Drautz",
        "Silvana Botti",
        "Miguel A. L. Marques"
      ],
      "abstract": "We present a novel multi-stage workflow for computational materials discovery that achieves a 99% success rate in identifying compounds within 100 meV/atom of thermodynamic stability, with a threefold improvement over previous approaches. By combining the Matra-Genoa generative model, Orb-v2 universal machine learning interatomic potential, and ALIGNN graph neural network for energy prediction, we generated 119 million candidate structures and added 1.3 million DFT-validated compounds to the ALEXANDRIA database, including 74 thousand new stable materials. The expanded ALEXANDRIA database now contains 5.8 million structures with 175 thousand compounds on the convex hull. Predicted structural disorder rates (37-43%) match experimental databases, unlike other recent AI-generated datasets. Analysis reveals fundamental patterns in space group distributions, coordination environments, and phase stability networks, including sub-linear scaling of convex hull connectivity. We release the complete dataset, including sAlex25 with 14 million out-of-equilibrium structures containing forces and stresses for training universal force fields. We demonstrate that fine-tuning a GRACE model on this data improves benchmark accuracy. All data, models, and workflows are freely available under Creative Commons licenses.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè®¡ç®—ææ–™å‘ç°çš„æ–°å‹å¤šé˜¶æ®µå·¥ä½œæµï¼Œåœ¨è¯†åˆ«çƒ­åŠ›å­¦ç¨³å®šæ€§ï¼ˆ100 meV/atomä»¥å†…ï¼‰çš„åŒ–åˆç‰©æ–¹é¢å®ç°äº†99%çš„æˆåŠŸç‡ï¼Œæ•ˆç‡æ¯”ä»¥å¾€æ–¹æ³•æé«˜äº†ä¸‰å€ã€‚è¯¥æµç¨‹é›†æˆäº†Matra-Genoaç”Ÿæˆæ¨¡å‹ã€Orb-v2é€šç”¨æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿èƒ½å’ŒALIGNNå›¾ç¥ç»ç½‘ç»œè¿›è¡Œèƒ½é‡é¢„æµ‹ï¼Œä»1.19äº¿ä¸ªå€™é€‰ç»“æ„ä¸­ç­›é€‰å¹¶å‘ALEXANDRIAæ•°æ®åº“æ·»åŠ äº†130ä¸‡ä¸ªç»DFTéªŒè¯çš„åŒ–åˆç‰©ï¼Œå…¶ä¸­åŒ…æ‹¬7.4ä¸‡ä¸ªå…¨æ–°çš„ç¨³å®šææ–™ã€‚ç›®å‰æ‰©å±•åçš„ALEXANDRIAæ•°æ®åº“åŒ…å«580ä¸‡ä¸ªç»“æ„ï¼Œå…¶é¢„æµ‹çš„ç»“æ„æ— åºç‡ï¼ˆ37-43%ï¼‰é«˜åº¦ç¬¦åˆå®éªŒæ•°æ®ï¼Œå¹¶æ­ç¤ºäº†ç©ºé—´ç¾¤åˆ†å¸ƒå’Œç›¸ä½ç¨³å®šæ€§ç½‘ç»œä¸­å‡¸åŒ…(convex hull)è¿æ¥æ€§çš„äºšçº¿æ€§ç¼©æ”¾ç­‰åŸºæœ¬æ¨¡å¼ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†åŒ…å«1400ä¸‡ä¸ªéå¹³è¡¡ç»“æ„çš„sAlex25æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒé€šç”¨åŠ›åœºï¼Œå¹¶è¯æ˜åœ¨è¯¥æ•°æ®ä¸Šå¾®è°ƒGRACEæ¨¡å‹èƒ½æ˜¾è‘—æå‡åŸºå‡†ç²¾åº¦ã€‚æ‰€æœ‰æ•°æ®ã€æ¨¡å‹å’Œå·¥ä½œæµå‡é€šè¿‡Creative Commonsè®¸å¯å…¬å¼€ï¼Œä¸ºåŠ é€Ÿæ–°ææ–™çš„å‘ç°ä¸åº”ç”¨æä¾›äº†é‡è¦çš„åŸºç¡€è®¾æ–½æ”¯æ’‘ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09169v1",
      "published_date": "2025-12-09 22:31:17 UTC",
      "updated_date": "2025-12-09 22:31:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:56:08.695731+00:00"
    },
    {
      "arxiv_id": "2512.09164v1",
      "title": "WonderZoom: Multi-Scale 3D World Generation",
      "title_zh": "WonderZoomï¼šå¤šå°ºåº¦ 3D ä¸–ç•Œç”Ÿæˆ",
      "authors": [
        "Jin Cao",
        "Hong-Xing Yu",
        "Jiajun Wu"
      ],
      "abstract": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WonderZoomï¼Œä¸€ç§èƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆå…·æœ‰å¤šç©ºé—´å°ºåº¦(multi-scale)å†…å®¹çš„3Dåœºæ™¯çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰3Dä¸–ç•Œç”Ÿæˆæ¨¡å‹åœ¨å¤šç²’åº¦è¿è´¯æ€§æ–¹é¢çš„å±€é™æ€§ã€‚æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€æ˜¯å¼•å…¥äº†å°ºåº¦è‡ªé€‚åº”é«˜æ–¯é¢å…ƒ(scale-adaptive Gaussian surfels)ï¼Œç”¨äºå¤šå°ºåº¦3Dåœºæ™¯çš„ç”Ÿæˆä¸å®æ—¶æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ç§æ¸è¿›å¼ç»†èŠ‚åˆæˆå™¨(progressive detail synthesizer)ï¼Œé€šè¿‡è¿­ä»£æ–¹å¼ç”Ÿæˆæ›´ç²¾ç»†å°ºåº¦çš„3Då†…å®¹ã€‚è¯¥æ–¹æ³•å…è®¸ç”¨æˆ·åœ¨3DåŒºåŸŸå†…è¿›è¡Œâ€œç¼©æ”¾(zoom into)â€ï¼Œä»è€Œè‡ªåŠ¨å›å½’åœ°åˆæˆä»å®è§‚æ™¯è§‚åˆ°å¾®è§‚ç‰¹å¾çš„ã€å…ˆå‰ä¸å­˜åœ¨çš„ç²¾ç»†ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWonderZoomåœ¨ç”Ÿæˆè´¨é‡å’Œå¯¹é½æ€§(alignment)æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›è§†é¢‘å’Œ3Dæ¨¡å‹ã€‚è¯¥ç³»ç»Ÿå®ç°äº†ä»å•å¼ å›¾åƒåˆ›å»ºäº¤äº’å¼çš„å¤šå°ºåº¦3Dä¸–ç•Œï¼Œä¸ºå¤æ‚ç¯å¢ƒçš„ç»†èŠ‚å»ºæ¨¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://wonderzoom.github.io/ The first two authors contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2512.09164v1",
      "published_date": "2025-12-09 22:21:07 UTC",
      "updated_date": "2025-12-09 22:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:56:21.750902+00:00"
    },
    {
      "arxiv_id": "2512.09149v2",
      "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts",
      "title_zh": "MindShiftï¼šåˆ†æè¯­è¨€æ¨¡å‹å¯¹å¿ƒç†å­¦æç¤ºè¯çš„ååº”",
      "authors": [
        "Anton Vasiliuk",
        "Irina Abdullaeva",
        "Polina Druzhinina",
        "Anton Razzhigaev",
        "Andrey Kuznetsov"
      ],
      "abstract": "Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¸æ”¶å’Œåæ˜ ç”¨æˆ·æŒ‡å®šäººæ ¼ç‰¹è´¨ä¸æ€åº¦çš„æ½œåŠ›ã€‚ç ”ç©¶è€…é€šè¿‡æ”¹ç¼–å¿ƒç†å­¦é¢†åŸŸå¹¿æ³›åº”ç”¨çš„æ˜å°¼è‹è¾¾å¤šç›¸äººæ ¼è°ƒæŸ¥è¡¨(MMPI)ï¼Œå¼€å‘äº†ä¸€å¥—æ¶µç›–ä¸åŒç‰¹è´¨å¼ºåº¦çš„å¿ƒç†å¯¼å‘æç¤º(personality-oriented prompts)ï¼Œä»¥æ­¤è¯†åˆ«å¹¶åˆ†ææ¨¡å‹çš„äººæ ¼è¡Œä¸ºç‰¹å¾ã€‚è¯¥é¡¹å·¥ä½œå¼•å…¥äº†åä¸ºMindShiftçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°LLMsåœ¨æ¨¡æ‹Ÿç‰¹å®šäººæ ¼æ—¶çš„å¿ƒç†é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå—ç›Šäºè®­ç»ƒæ•°æ®é›†å’Œå¯¹é½æŠ€æœ¯(alignment techniques)çš„æ¼”è¿›ï¼ŒLLMsåœ¨è§’è‰²æ„ŸçŸ¥å’Œæ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç°ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¿ƒç†æµ‹è¯„å“åº”ä¸Šå­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œæ­ç¤ºäº†LLMsåœ¨æ¨¡ä»¿äººç±»äººæ ¼ç‰¹è´¨èƒ½åŠ›ä¸Šçš„ä¸å¹³è¡¡æ€§ã€‚MindShiftä¸ä»…ä¸ºè¯„ä¼°æ¨¡å‹çš„å¿ƒç†åå·®æä¾›äº†å·¥å…·ï¼Œä¹Ÿä¸ºç†è§£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„äººæ ¼åŒ–è¡¨ç°æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09149v2",
      "published_date": "2025-12-09 21:56:54 UTC",
      "updated_date": "2025-12-18 08:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:56:14.030917+00:00"
    },
    {
      "arxiv_id": "2512.09148v1",
      "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment",
      "title_zh": "åŸºäºæ³¨æ„åŠ›æ¨¡å¼ä¸è¯­ä¹‰å¯¹é½çš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆå¹»è§‰æ£€æµ‹",
      "authors": [
        "Shanghao Li",
        "Jinda Han",
        "Yibo Wang",
        "Yuanjie Zhu",
        "Zihe Song",
        "Langzhou He",
        "Kenan Kamel A Alghythee",
        "Philip S. Yu"
      ],
      "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ (GraphRAG) ä¸­å¤§è¯­è¨€æ¨¡å‹ (LLMs) éš¾ä»¥ç†è§£æ‹“æ‰‘ä¿¡æ¯è€Œå¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†è·¯å¾„ä¾èµ–åº¦ (Path Reliance Degree, PRD) å’Œè¯­ä¹‰å¯¹é½å¾—åˆ† (Semantic Alignment Score, SAS) ä¸¤ç§è½»é‡çº§å¯è§£é‡Šæ€§æŒ‡æ ‡ã€‚PRD ç”¨äºè¡¡é‡æ¨¡å‹å¯¹æœ€çŸ­è·¯å¾„ä¸‰å…ƒç»„çš„è¿‡åº¦ä¾èµ–ï¼Œè€Œ SAS è¯„ä¼°æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸æ£€ç´¢çŸ¥è¯†çš„å¯¹é½ç¨‹åº¦ã€‚é€šè¿‡å¯¹çŸ¥è¯†é—®ç­”ä»»åŠ¡çš„åˆ†æï¼Œç ”ç©¶ç¡®å®šäº†é«˜ PRD å’Œä½ SAS ä¸å¹»è§‰æ•…éšœæ¨¡å¼ä¹‹é—´çš„å…³è”ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…å¼€å‘äº†åä¸ºå›¾æ¥åœ°ä¸å¯¹é½ (Graph Grounding and Alignment, GGA) çš„äº‹åå¹»è§‰æ£€æµ‹å™¨ã€‚å®éªŒè¯æ˜ï¼ŒGGA åœ¨ AUC å’Œ F1 æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºäºè¯­ä¹‰å’Œç½®ä¿¡åº¦çš„åŸºå‡†æ–¹æ³•ã€‚è¯¥å·¥ä½œä»æœºæ¢°å¯è§£é‡Šæ€§ (Mechanistic Interpretability) è§’åº¦æ­ç¤ºäº†æ¨¡å‹ç»“æ„æ€§é™åˆ¶å¯¹å¹»è§‰çš„å½±å“ï¼Œä¸ºå¼€å‘æ›´å¯é çš„ GraphRAG ç³»ç»Ÿæä¾›äº†ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09148v1",
      "published_date": "2025-12-09 21:52:50 UTC",
      "updated_date": "2025-12-09 21:52:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:18.356458+00:00"
    },
    {
      "arxiv_id": "2512.09142v2",
      "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation",
      "title_zh": "SDialogï¼šæ”¯æŒç«¯åˆ°ç«¯æ™ºèƒ½ä½“æ„å»ºã€ç”¨æˆ·æ¨¡æ‹Ÿã€å¯¹è¯ç”ŸæˆåŠè¯„ä¼°çš„ Python å·¥å…·åŒ…",
      "authors": [
        "Sergio Burdisso",
        "SÃ©verin Baroudi",
        "Yanis Labrak",
        "David Grunert",
        "Pawel Cyrta",
        "Yiyang Chen",
        "Srikanth Madikeri",
        "EsaÃº Villatoro-Tello",
        "Thomas Schaaf",
        "Ricard Marxer",
        "Petr Motlicek"
      ],
      "abstract": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SDialogï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMITåè®®çš„å¼€æºPythonå·¥å…·åŒ…ï¼Œæ—¨åœ¨å°†å¯¹è¯ç”Ÿæˆã€è¯„ä¼°ä¸æœºæ¢°å¯è§£é‡Šæ€§(Mechanistic Interpretability)ç»Ÿä¸€åˆ°å•ä¸€çš„ç«¯åˆ°ç«¯æ¡†æ¶ä¸­ï¼Œç”¨äºæ„å»ºå’Œåˆ†æåŸºäºLLMçš„å¯¹è¯æ™ºèƒ½ä½“ã€‚è¯¥å·¥å…·åŒ…å›´ç»•æ ‡å‡†åŒ–çš„Dialogè¡¨ç¤ºæ³•æ„å»ºï¼Œæä¾›å—äººæ ¼é©±åŠ¨(Persona-driven)çš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹ŸåŠŸèƒ½ï¼Œæ”¯æŒé€šè¿‡å¯ç»„åˆçš„ç¼–æ’è¿›è¡Œå—æ§çš„åˆæˆå¯¹è¯ç”Ÿæˆã€‚åœ¨è¯„ä¼°å±‚é¢ï¼ŒSDialogç»“åˆäº†è¯­è¨€å­¦æŒ‡æ ‡ã€LLM-as-a-judgeä»¥åŠåŠŸèƒ½æ­£ç¡®æ€§éªŒè¯å™¨ï¼Œæä¾›äº†å…¨é¢çš„å¯¹è¯è´¨é‡è¡¡é‡æ‰‹æ®µã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«æœºæ¢°å¯è§£é‡Šæ€§å·¥å…·ï¼Œæ”¯æŒé€šè¿‡ç‰¹å¾æ¶ˆè(Feature Ablation)å’Œå¼•å¯¼è¿›è¡Œæ¿€æ´»æ£€æŸ¥ä¸è½¬å‘ã€‚SDialogè¿˜å…·å¤‡éŸ³é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿›è¡ŒåŒ…å«3Dæˆ¿é—´å»ºæ¨¡å’Œéº¦å…‹é£æ•ˆåº”çš„å®Œæ•´å£°å­¦æ¨¡æ‹Ÿã€‚é€šè¿‡é›†æˆä¸»æµLLMåç«¯å¹¶æä¾›ç»Ÿä¸€APIï¼Œè¯¥å·¥å…·åŒ…ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´ç³»ç»Ÿåœ°æ„å»ºã€åŸºå‡†æµ‹è¯•å¹¶æ·±å…¥åˆ†æå¯¹è¯ç³»ç»Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "We have an old version of the paper at arXiv:2506.10622, we will update this old version instead (even though the authors and the title have changed since this first old version)",
      "pdf_url": "https://arxiv.org/pdf/2512.09142v2",
      "published_date": "2025-12-09 21:42:41 UTC",
      "updated_date": "2025-12-12 00:59:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:31.343712+00:00"
    },
    {
      "arxiv_id": "2512.09134v1",
      "title": "Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation",
      "title_zh": "é›†æˆè‡ªåŠ¨åŒ–ç—…å˜åˆ†æã€è™šæ‹Ÿæ”¯æ¶æ¤å…¥åŠç™¾ä¾‹è¡€ç®¡ FFR éªŒè¯çš„å† çŠ¶åŠ¨è„‰é€ å½±ä¸€ä½“åŒ–æµç¨‹",
      "authors": [
        "Georgy Kopanitsa",
        "Oleg Metsker",
        "Alexey Yakovlev"
      ],
      "abstract": "Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† AngioAI-QFRï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å…¨è‡ªåŠ¨å† çŠ¶åŠ¨è„‰é€ å½±åˆ†ææµç¨‹ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠè§†è§‰è¯„ä¼°ç‹­çª„çš„ä¸»è§‚æ€§ä»¥åŠä¼ ç»Ÿå‹åŠ›å¯¼ä¸ Fractional Flow Reserve (FFR) ä½¿ç”¨å—é™çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿç»“åˆæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–çš„ç—…å˜æ£€æµ‹ã€ç®¡è…”åˆ†å‰²ã€ä¸­å¿ƒçº¿æå–ä»¥åŠæ¯æ¯«ç±³çš„ Relative Flow Capacity (RFC) å‰–é¢åˆ†æã€‚è¯¥æµç¨‹è¿˜æ•´åˆäº†è™šæ‹Ÿæ”¯æ¶æ¤å…¥ (virtual stenting) åŠŸèƒ½ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é‡æ–°è®¡ç®—é€ å½±è¡ç”Ÿçš„ Quantitative Flow Ratio (QFR)ï¼Œä¸ºä»‹å…¥æ‰‹æœ¯è§„åˆ’æä¾›é‡åŒ–ä¾æ®ã€‚åœ¨ 100 æ¡è¡€ç®¡çš„ä¸´åºŠéªŒè¯ä¸­ï¼ŒAngioAI-QFR ä¸ä¾µå…¥æ€§ FFR è¡¨ç°å‡ºå¼ºç›¸å…³æ€§ (r = 0.89)ï¼Œè¯Šæ–­ FFR â‰¤ 0.80 çš„æ›²çº¿ä¸‹é¢ç§¯ (AUC) è¾¾åˆ° 0.93ã€‚ç³»ç»Ÿåœ¨ 93% çš„æ¡ˆä¾‹ä¸­å®ç°äº†å…¨è‡ªåŠ¨è¿è¡Œï¼Œä¸­ä½å¤„ç†æ—¶é—´ä»…ä¸º 41 ç§’ï¼Œæ˜¾è‘—æå‡äº†å·¥ä½œæµæ•ˆç‡ã€‚å®éªŒè¯æ˜è¯¥å·¥å…·èƒ½æœ‰æ•ˆåŒºåˆ†å±€ç¶æ€§ä¸å¼¥æ¼«æ€§ç—…å˜ï¼Œä¸ºè¿‘å®æ—¶çš„è®¡ç®—æœºè§†è§‰è¾…åŠ©åŠŸèƒ½è¯„ä¼°å’Œè™šæ‹Ÿæ‰‹æœ¯è§„åˆ’æä¾›äº†ç»Ÿä¸€çš„å®ç”¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 10 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.09134v1",
      "published_date": "2025-12-09 21:26:45 UTC",
      "updated_date": "2025-12-09 21:26:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:35.835988+00:00"
    },
    {
      "arxiv_id": "2512.09127v1",
      "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation",
      "title_zh": "é¢å‘è‡ªåŠ¨åŒ–å„¿ç«¥å£è…”ç—…å†ç†è§£ä¸å®‰å…¨æŠ—ç”Ÿç´ æ¨èçš„çŸ¥è¯†å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zihan Han",
        "Junyan Ge",
        "Caifeng Li"
      ],
      "abstract": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†çŸ¥è¯†å¼•å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹ (KG-LLM)ï¼Œæ—¨åœ¨è§£å†³å„¿ç§‘ç‰™ç§‘ä¸´åºŠè®°å½•è‡ªåŠ¨ç†è§£å’ŒæŠ—ç”Ÿç´ å®‰å…¨æ¨èä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†å„¿ç§‘ç‰™ç§‘çŸ¥è¯†å›¾è°± (Knowledge Graph)ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ä»¥åŠå¤šé˜¶æ®µå®‰å…¨éªŒè¯æµæ°´çº¿ï¼Œä»¥å®ç°åŸºäºè¯æ®çš„æŠ—ç”Ÿç´ æ¨èã€‚KG-LLM é¦–å…ˆåˆ©ç”¨ä¸´åºŠå®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å– (NER/RE) æ¨¡å—ä»ç‰™ç§‘ç¬”è®°å’Œæ”¾å°„æŠ¥å‘Šä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶ç»“åˆä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢çš„ä¸´åºŠæŒ‡å—å’Œè¯ç‰©è§„åˆ™è¿›è¡Œè¯Šæ–­æ‘˜è¦ä¸å‰‚é‡é¢„æµ‹ã€‚ç³»ç»Ÿé€šè¿‡ç»“åˆç¡®å®šæ€§è§„åˆ™æ£€æŸ¥å’Œå­¦ä¹ å‹åˆ†ç±»å™¨çš„åŒå±‚éªŒè¯æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆæ£€æµ‹è¿‡æ•ã€ç¦å¿Œç—‡åŠå‰‚é‡é”™è¯¯ã€‚åœ¨ 32,000 ä»½å„¿ç§‘ç‰™ç§‘å°±è¯Šè®°å½•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒKG-LLM åœ¨è®°å½•ç†è§£æ€§èƒ½ (F1: 0.914) å’Œè¯ç‰©é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºä¸´åºŠåŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿå°†ä¸å®‰å…¨æŠ—ç”Ÿç´ å»ºè®®å‡å°‘äº† 50%ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨æå‡ä¸´åºŠå†³ç­–å¯é æ€§ä¸å¯è§£é‡Šæ€§æ–¹é¢çš„æ˜¾è‘—è´¡çŒ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09127v1",
      "published_date": "2025-12-09 21:11:55 UTC",
      "updated_date": "2025-12-09 21:11:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:41.816388+00:00"
    },
    {
      "arxiv_id": "2512.09117v1",
      "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„èŒƒç•´è®ºåˆ†æï¼šæ¢è®¨ LLMs ä¸ºä½•è§„é¿äº†ç¬¦å·æ¥åœ°é—®é¢˜",
      "authors": [
        "Luciano Floridi",
        "Yiyang Jia",
        "Fernando TohmÃ©"
      ],
      "abstract": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå½¢å¼åŒ–çš„èŒƒç•´è®ºæ¡†æ¶(categorical framework)ï¼Œç”¨äºåˆ†æäººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¦‚ä½•å°†å†…å®¹è½¬åŒ–ä¸ºå…³äºå¯èƒ½ä¸–ç•ŒçŠ¶æ€ç©ºé—´W(state space of possible worlds)çš„çœŸå€¼è¯„ä¼°å‘½é¢˜(truth-evaluated propositions)ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œä½œè€…è¯¦ç»†æ¢è®¨äº†è¯­ä¹‰å†…å®¹åœ¨ä¸åŒè®¤çŸ¥ä¸è®¡ç®—ç³»ç»Ÿä¸­çš„è½¬åŒ–è·¯å¾„ã€‚ç ”ç©¶çš„æ ¸å¿ƒè®ºç‚¹æŒ‡å‡ºï¼ŒLLMså¹¶éä»æœ¬è´¨ä¸Šè§£å†³äº†ç¬¦å·æ¥åœ°é—®é¢˜(symbol grounding problem)ï¼Œè€Œæ˜¯é€šè¿‡ç‰¹å®šçš„å½¢å¼åŒ–æœºåˆ¶è§„é¿äº†è¿™ä¸€éš¾é¢˜ã€‚è¯¥åˆ†ææ­ç¤ºäº†LLMsåœ¨å¤„ç†ç¬¦å·ä¸ä¸–ç•Œå…³ç³»æ—¶çš„æœ¬è´¨ç‰¹å¾ï¼Œå¼ºè°ƒäº†å…¶é¢„æµ‹æœºåˆ¶ä¸äººç±»è¯­ä¹‰æ¥åœ°çš„æ ¹æœ¬åŒºåˆ«ã€‚è¿™ä¸€è§†è§’ä¸ºç†è§£äººå·¥æ™ºèƒ½çš„è¯­è¨€å¤„ç†èƒ½åŠ›æä¾›äº†ä¸¥è°¨çš„æ•°å­¦åŸºç¡€ï¼Œå¹¶é‡æ–°å®šä¹‰äº†å…³äºæœºå™¨ç†è§£çš„å“²å­¦è®¨è®ºè¾¹ç•Œã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09117v1",
      "published_date": "2025-12-09 20:59:46 UTC",
      "updated_date": "2025-12-09 20:59:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:40.811328+00:00"
    },
    {
      "arxiv_id": "2512.09114v2",
      "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance",
      "title_zh": "AI TIPS 2.0ï¼šäººå·¥æ™ºèƒ½æ²»ç†å¯æ“ä½œåŒ–å…¨é¢æ¡†æ¶",
      "authors": [
        "Pamela Gupta"
      ],
      "abstract": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰AIç³»ç»Ÿéƒ¨ç½²ä¸­é¢ä¸´çš„ç”¨ä¾‹çº§é£é™©è¯„ä¼°ä¸è¶³ã€ç°æœ‰æ¡†æ¶ï¼ˆå¦‚ISO 42001å’ŒNIST AI RMFï¼‰ç¼ºä¹å¯æ“ä½œæ§åˆ¶æªæ–½ä»¥åŠç¼ºä¹å¤§è§„æ¨¡æ²»ç†è¿è¥æœºåˆ¶è¿™ä¸‰å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†AI TIPS 2.0ï¼ˆArtificial Intelligence Trust-Integrated Pillars for Sustainability 2.0ï¼‰æ¡†æ¶ã€‚ä½œä¸ºå¯¹2019å¹´åˆå§‹ç‰ˆæœ¬çš„é‡å¤§æ›´æ–°ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å°†æŠ½è±¡çš„æ²»ç†åŸåˆ™è½¬åŒ–ä¸ºå…·ä½“ã€å¯æ“ä½œçš„æŠ€æœ¯å®ç°ï¼Œå¹¶å°†å¯ä¿¡AIå®è·µåµŒå…¥æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸã€‚é€šè¿‡å»ºç«‹å¯æ‰©å±•çš„æ²»ç†æœºåˆ¶ï¼ŒAI TIPS 2.0ä¸ä»…æä¾›äº†å®šé‡çš„åˆè§„æ€§è¡¡é‡æ‰‹æ®µï¼Œè¿˜ä¸ºä»è‘£äº‹ä¼šåˆ°æ•°æ®ç§‘å­¦å®¶çš„ä¸åŒè§’è‰²æä¾›äº†é’ˆå¯¹æ€§çš„æ²»ç†å¯è§æ€§ã€‚è¯¥æ¡†æ¶ä¸ºè§£å†³ç‰¹å®šç”¨ä¾‹çš„é£é™©å‰–é¢é—®é¢˜æä¾›äº†ç³»ç»ŸåŒ–è·¯å¾„ï¼Œæœ‰æ•ˆåº”å¯¹äº†AIç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½å‡ºç°çš„åè§ã€é«˜é”™è¯¯ç‡åŠåˆè§„æ€§é£é™©ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "We have adjustments to make for higher effectiveness",
      "pdf_url": "https://arxiv.org/pdf/2512.09114v2",
      "published_date": "2025-12-09 20:57:22 UTC",
      "updated_date": "2026-01-13 12:53:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:44.696477+00:00"
    },
    {
      "arxiv_id": "2512.09111v2",
      "title": "Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous",
      "title_zh": "é¢å‘ç›®æ ‡å¯¼å‘èˆªå¤©å™¨äº¤ä¼šçš„è¯­ä¹‰è½¨è¿¹ç”Ÿæˆ",
      "authors": [
        "Yuji Takubo",
        "Arpit Dwivedi",
        "Sukeerth Ramkumar",
        "Luis A. Pabon",
        "Daniele Gammelli",
        "Marco Pavone",
        "Simone D'Amico"
      ],
      "abstract": "Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions. This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.",
      "tldr_zh": "é’ˆå¯¹æœªæ¥è‡ªä¸»èˆªå¤©å™¨å¯¹å®æ—¶è½¨è¿¹ç”Ÿæˆçš„éœ€æ±‚ï¼Œè¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰éå‡¸å¼•å¯¼ä¸æ§åˆ¶æ–¹æ³•è¿‡åº¦ä¾èµ–ä¸“å®¶è¾“å…¥ä»è€Œé™åˆ¶è¿è¡Œæ‰©å±•æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†SAGES (Semantic Autonomous Guidance Engine for Space) æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºåæ˜ é«˜å±‚æ„å›¾çš„èˆªå¤©å™¨è½¨è¿¹ï¼Œå¹¶åŒæ—¶ä¸¥æ ¼éµå®ˆéå‡¸çº¦æŸ(nonconvex constraints)ã€‚åœ¨å®¹é”™è¿‘è·ç¦»æ“ä½œå’Œè‡ªç”±é£è¡Œæœºå™¨äººå¹³å°ä¸¤ç§å®éªŒåœºæ™¯ä¸‹ï¼ŒSAGES è¡¨ç°å‡ºæé«˜çš„å¯é æ€§ï¼Œå¹¶åœ¨å¤šç§è¡Œä¸ºæ¨¡å¼ä¸­å®ç°äº†è¶…è¿‡ 90% çš„è¯­ä¹‰è¡Œä¸ºä¸€è‡´æ€§(semantic-behavioral consistency)ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€å‘è¯­è¨€é©±åŠ¨ã€å…·å¤‡çº¦æŸæ„ŸçŸ¥(constraint-aware)èƒ½åŠ›çš„èˆªå¤©å™¨è½¨è¿¹ç”Ÿæˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚é€šè¿‡è¯¥ç³»ç»Ÿï¼Œæ“ä½œå‘˜èƒ½å¤Ÿåˆ©ç”¨ç›´è§‚çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤äº¤äº’å¼åœ°å¼•å¯¼å®‰å…¨å’Œè¡Œä¸ºï¼Œæ˜¾è‘—å‡è½»äº†ä¸“å®¶çš„å·¥ä½œè´Ÿæ‹…ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "comment": "28 pages, 12 figures. Submitted to AIAA SCITECH 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.09111v2",
      "published_date": "2025-12-09 20:53:16 UTC",
      "updated_date": "2025-12-11 04:52:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:55.845674+00:00"
    },
    {
      "arxiv_id": "2512.09108v1",
      "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
      "title_zh": "æ¼”è¿›å“è¶Šï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–ä¼˜åŒ–",
      "authors": [
        "Paul Brookes",
        "Vardan Voskanyan",
        "Rafail Giavrimis",
        "Matthew Truscott",
        "Mina Ilieva",
        "Chrystalla Pavlou",
        "Alexandru Staicu",
        "Manal Adham",
        "Will Evers- Hood",
        "Jingzhi Gong",
        "Kejia Zhang",
        "Matvey Fedoseev",
        "Vishal Sharma",
        "Roman Bauer",
        "Zheng Wang",
        "Hema Nair",
        "Wei Jie",
        "Tianhua Xu",
        "Aurora Constantin",
        "Leslie Kanthan",
        "Michail Basios"
      ],
      "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.\n  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.\n  We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ç”±äºæç¤ºè¯(prompts)ã€å·¥å…·æè¿°å’Œå‚æ•°é…ç½®ä¸å½“å¯¼è‡´æ€§èƒ½å—é™ï¼Œä¸”æ‰‹åŠ¨è°ƒä¼˜å‘¨æœŸå†—é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†ARTEMISè¿™ä¸€æ— ä»£ç è¿›åŒ–ä¼˜åŒ–å¹³å°ã€‚ARTEMISé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„é—ä¼ ç®—å­(genetic operators)å¯¹æ™ºèƒ½ä½“é…ç½®è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¯é…ç½®ç»„ä»¶å¹¶ä»æ‰§è¡Œæ—¥å¿—ä¸­æå–æ€§èƒ½ä¿¡å·ï¼Œä¸”æ— éœ€å¯¹åŸæœ‰æ¶æ„è¿›è¡Œä¿®æ”¹ã€‚åœ¨å¤šä¸ªä»£è¡¨æ€§æ™ºèƒ½ä½“ç³»ç»Ÿä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥å¹³å°ä½¿ALE Agentåœ¨ç«äº‰æ€§ç¼–ç¨‹ä¸­çš„é€šè¿‡ç‡æå‡äº†13.6%ï¼Œå¹¶åœ¨ä»£ç ä¼˜åŒ–ä»»åŠ¡ä¸­ä¸ºMini-SWE Agentå¸¦æ¥äº†10.1%çš„æ€§èƒ½å¢é•¿ã€‚æ­¤å¤–ï¼ŒARTEMISåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†CrewAI Agentçº¦36.9%çš„Tokenæ¶ˆè€—ï¼Œå¹¶ä½¿åŸºäºQwen2.5-7Bå¼€æºæ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨GSM8Kæµ‹è¯•ä¸­çš„å‡†ç¡®ç‡æé«˜äº†22%ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥å¹³å°èƒ½æœ‰æ•ˆä¼˜åŒ–åŸºäºå•†ä¸šåŠæœ¬åœ°æ¨¡å‹çš„å„ç±»Agentç³»ç»Ÿï¼Œä¸ºè‡ªåŠ¨åŒ–æå‡æ™ºèƒ½ä½“æ€§èƒ½ä¸æ•ˆç‡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09108v1",
      "published_date": "2025-12-09 20:48:45 UTC",
      "updated_date": "2025-12-09 20:48:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:58:50.780650+00:00"
    },
    {
      "arxiv_id": "2512.09101v1",
      "title": "Masked Generative Policy for Robotic Control",
      "title_zh": "é¢å‘æœºå™¨äººæ§åˆ¶çš„æ©ç ç”Ÿæˆå¼ç­–ç•¥",
      "authors": [
        "Lipeng Zhuang",
        "Shiyu Fan",
        "Florent P. Audonnet",
        "Yingdong Ru",
        "Gerardo Aragon Camarasa",
        "Paul Henderson"
      ],
      "abstract": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Masked Generative Policy (MGP)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†è§‰è¿åŠ¨æ¨¡ä»¿å­¦ä¹  (visuomotor imitation learning) çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†åŠ¨ä½œè¡¨ç¤ºä¸ºç¦»æ•£æ ‡è®° (discrete tokens)ï¼Œå¹¶åˆ©ç”¨æ¡ä»¶æ©ç äº’æ„Ÿå™¨ (conditional masked transformer) å¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œä»…å¯¹ä½ç½®ä¿¡åº¦æ ‡è®°è¿›è¡Œå¿«é€Ÿç»†åŒ–ã€‚ç ”ç©¶è®¾è®¡äº†é’ˆå¯¹é©¬å°”å¯å¤«ä»»åŠ¡çš„ MGP-Short ä»¥åŠæ”¯æŒå…¨è½¨è¿¹é¢„æµ‹ä¸åŠ¨æ€è°ƒæ•´çš„ MGP-Long ä¸¤ç§é‡‡æ ·æ¨¡å¼ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿåœ¨å¤æ‚éé©¬å°”å¯å¤«ä»»åŠ¡ä¸­çš„å…¨å±€ä¸€è‡´æ€§å’Œé²æ£’è‡ªé€‚åº”æ‰§è¡Œèƒ½åŠ›ã€‚åœ¨ Meta-World å’Œ LIBERO åŸºå‡†çš„ 150 é¡¹ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒMGP çš„å¹³å‡æˆåŠŸç‡æ¯”æœ€å…ˆè¿›çš„æ‰©æ•£ç­–ç•¥ (diffusion policies) å’Œè‡ªå›å½’ç­–ç•¥æé«˜äº† 9%ï¼Œæ¨ç†é€Ÿåº¦æœ€é«˜æå‡äº† 35 å€ã€‚åœ¨åŠ¨æ€ç¯å¢ƒæˆ–è§‚å¯Ÿç¼ºå¤±çš„æƒ…å†µä¸‹ï¼Œå…¶æˆåŠŸç‡æå‡è¾¾ 60%ï¼Œæœ‰æ•ˆè§£å†³äº†å…¶ä»–å…ˆè¿›æ–¹æ³•éš¾ä»¥åº”å¯¹çš„éé©¬å°”å¯å¤«åœºæ™¯ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09101v1",
      "published_date": "2025-12-09 20:37:40 UTC",
      "updated_date": "2025-12-09 20:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:57:58.499213+00:00"
    },
    {
      "arxiv_id": "2512.11893v2",
      "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI",
      "title_zh": "è¶…è¶Šè‡ªåŠ¨åŒ–ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ—¶ä»£å¯¹å·¥ä½œã€åˆ›é€ åŠ›ä¸æ²»ç†çš„å†æ€è€ƒ",
      "authors": [
        "Haocheng Lin"
      ],
      "abstract": "The rapid expansion of generative artificial intelligence (AI) is transforming work, creativity, and economic security in ways that extend beyond automation and productivity. This paper examines four interconnected dimensions of contemporary AI deployment: (1) transformations in employment and task composition (2) unequal diffusion of AI across sectors and socio-demographic groups (3) the role of universal basic income (UBI) as a stabilising response to AI-induced volatility (4) the effects of model alignment and content governance on human creativity, autonomy, and decision-making\n  Using a hybrid approach that integrates labour market task exposure modelling, sectoral diffusion analysis, policy review, and qualitative discourse critique, the study develops an Inclusive AI Governance Framework. It introduces Level 1.5 autonomy as a human centred design principle that preserves evaluative authority while enabling partial automation, and highlights evidence of creative regression and emergent sycophancy in newer model generations. The paper argues that UBI should be embedded within a broader socio-technical governance ecosystem encompassing skills development, proportional regulation, and creativity preservation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)å¯¹å·¥ä½œã€åˆ›æ„å’Œç»æµå®‰å…¨çš„æ·±è¿œå½±å“ï¼Œåˆ†æäº†å°±ä¸šè½¬å‹ã€AIæ‰©æ•£ä¸å‡ã€å…¨æ°‘åŸºæœ¬æ”¶å…¥(Universal Basic Income, UBI)çš„ç¨³å®šä½œç”¨ä»¥åŠæ¨¡å‹å¯¹é½å¯¹äººç±»è‡ªä¸»æƒçš„å½±å“ã€‚é€šè¿‡æ•´åˆåŠ³åŠ¨åŠ›å¸‚åœºä»»åŠ¡æš´éœ²æ¨¡å‹(Task Exposure Modeling)ã€è¡Œä¸šæ‰©æ•£åˆ†æå’Œå®šæ€§è¯è¯­æ‰¹è¯„ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å®¹æ€§ AI æ²»ç†æ¡†æ¶(Inclusive AI Governance Framework)ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Level 1.5 autonomy ä½œä¸ºä»¥äººä¸ºæœ¬çš„è®¾è®¡åŸåˆ™ï¼Œæ—¨åœ¨å®ç°éƒ¨åˆ†è‡ªåŠ¨åŒ–çš„åŒæ—¶ä¿ç•™äººç±»çš„æœ€ç»ˆè¯„ä¼°æƒã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºäº†æ–°ä¸€ä»£æ¨¡å‹ä¸­å‡ºç°çš„åˆ›æ„é€€åŒ–(Creative Regression)å’Œé¡ºä»æ€§(Sycophancy)ç­‰é£é™©ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒåº”å°† UBI åµŒå…¥åŒ…å«æŠ€èƒ½å‘å±•ã€æ¯”ä¾‹ç›‘ç®¡å’Œåˆ›é€ åŠ›ä¿æŠ¤åœ¨å†…çš„æ›´å¹¿æ³›çš„ç¤¾ä¼šæŠ€æœ¯æ²»ç†ç”Ÿæ€ç³»ç»Ÿä¸­ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Improved structure and clarity of the introduction and literature review; explicit articulation of the paper's contributions; refined the integration of AI across labour, UBI, and governance",
      "pdf_url": "https://arxiv.org/pdf/2512.11893v2",
      "published_date": "2025-12-09 20:25:24 UTC",
      "updated_date": "2026-01-21 18:42:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:58:07.306946+00:00"
    },
    {
      "arxiv_id": "2512.11892v1",
      "title": "Should AI Become an Intergenerational Civil Right?",
      "title_zh": "äººå·¥æ™ºèƒ½æ˜¯å¦åº”æˆä¸ºä¸€é¡¹ä»£é™…æ°‘æƒï¼Ÿ",
      "authors": [
        "Jon Crowcroft",
        "Rute C. Sofia",
        "Dirk Trossen",
        "Vassilis Tsaoussidis"
      ],
      "abstract": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.\n  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.\n  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ä½œä¸ºç¤¾ä¼šã€ç»æµå’Œè®¤çŸ¥åŸºç¡€è®¾æ–½çš„æ—¥ç›Šé‡è¦æ€§ï¼Œæå‡ºå°†è·å–AIè¾“å‡ºè§†ä¸ºä¸€ç§åŸºæœ¬æ°‘äº‹åˆ©ç›Šè€Œéå•çº¯çš„å•†ä¸šæœåŠ¡ã€‚è®ºæ–‡æŒ‡å‡ºå½“å‰AIæ²»ç†åœ¨æ‰©å±•è·å–æ¸ é“ä»¥å®ç°ç¤¾ä¼šåŒ…å®¹ä¸æ— èŠ‚åˆ¶å¢é•¿å¯¼è‡´çš„èµ„æºæŒ‘æˆ˜ä¹‹é—´å­˜åœ¨å¼ åŠ›ï¼Œè€Œç°æœ‰ç›‘ç®¡æ¡†æ¶å¾€å¾€å¿½è§†äº†å…¬å¹³è·å–ä¸èµ„æºçº¦æŸä¹‹é—´çš„è€¦åˆå…³ç³»ã€‚ä¸ºæ­¤ï¼Œä½œè€…å»ºè®®å°†è·å–AIå®šæ€§ä¸ºä¸€ç§â€œè·¨ä»£å…¬æ°‘æƒåˆ©â€(Intergenerational Civil Right)ï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªå…¼é¡¾å½“ä»£åŒ…å®¹æ€§ä¸æœªæ¥ä¸–ä»£æƒåˆ©çš„æ³•å¾‹å’Œä¼¦ç†æ¡†æ¶ã€‚åœ¨æŠ€æœ¯å®ç°å±‚é¢ï¼Œç ”ç©¶ç»“åˆäº†ç‰©è”ç½‘(IoT)-è¾¹ç¼˜(Edge)-äº‘(Cloud)è®¡ç®—ã€å»ä¸­å¿ƒåŒ–æ¨ç†(decentralized inference)åŠèƒ½æºæ„ŸçŸ¥ç½‘ç»œ(energy-aware networking)ç­‰èŒƒå¼ï¼Œæå‡ºäº†AIäº¤ä»˜ç½‘ç»œ(AI Delivery Networks)çš„è‰æ¡ˆæ¶æ„ã€‚é€šè¿‡å°†AIç•Œå®šä¸ºå…±äº«ç¤¾ä¼šåŸºç¡€è®¾æ–½è€Œéç‰¹è®¸å¸‚åœºå•†å“ï¼Œè¯¥å·¥ä½œå°†æ²»ç†åŸåˆ™ä¸å…·ä½“ç³»ç»Ÿè®¾è®¡ç›¸ç»“åˆï¼Œä¸ºå®ç°ç¤¾ä¼šå…¬æ­£ä¸”ç¯å¢ƒå¯æŒç»­çš„AIéƒ¨ç½²æä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11892v1",
      "published_date": "2025-12-09 20:22:16 UTC",
      "updated_date": "2025-12-09 20:22:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:58:06.901587+00:00"
    },
    {
      "arxiv_id": "2512.09088v1",
      "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study",
      "title_zh": "åº”å¯¹ LLM å¹»è§‰ä¸­çš„æ ¡å‡†ä¿¡ä»»ï¼šä¸€é¡¹å®šæ€§ç ”ç©¶",
      "authors": [
        "Adrian Ryser",
        "Florian Allwein",
        "Tim Schlippe"
      ],
      "abstract": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by BlÃ¶baum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹192åå‚ä¸è€…è¿›è¡Œå®šæ€§ç ”ç©¶(Qualitative Study)ï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„å¹»è§‰(Hallucinations)å¦‚ä½•å½±å“ç”¨æˆ·ä¿¡ä»»åŠå…¶äº¤äº’è¡Œä¸ºã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¹»è§‰å¹¶æœªå¯¼è‡´ç”¨æˆ·äº§ç”Ÿç›²ç›®çš„ä¸ä¿¡ä»»ï¼Œè€Œæ˜¯ä¿ƒä½¿å…¶å®æ–½ä¸Šä¸‹æ–‡æ•æ„Ÿçš„ä¿¡ä»»æ ¡å‡†(Context-sensitive trust calibration)ã€‚åœ¨ç°æœ‰ä¿¡ä»»æ¨¡å‹åŸºç¡€ä¸Šï¼Œç ”ç©¶ä¸ä»…ç¡®è®¤äº†æœŸæœ›(Expectancy)ã€è¿‡å¾€ç»éªŒ(Prior experience)åŠç”¨æˆ·ä¸“ä¸šèƒŒæ™¯ä¸é¢†åŸŸçŸ¥è¯†(User expertise & domain knowledge)ç­‰ç”¨æˆ·ç›¸å…³å› ç´ ï¼Œè¿˜æ–°å‘ç°äº†ç›´è§‰(Intuition)æ˜¯æ£€æµ‹å¹»è§‰çš„å…³é”®å› ç´ ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºæ„ŸçŸ¥é£é™©(Perceived risk)å’Œå†³ç­–åˆ©å®³(Decision stakes)ç­‰æƒ…å¢ƒå› ç´ ä¼šè¿›ä¸€æ­¥è°ƒèŠ‚ä¿¡ä»»åŠ¨æ€ã€‚æœ€åï¼Œè¯¥æ–‡éªŒè¯å¹¶æ‰©å±•äº†BlÃ¶baumçš„é€’å½’ä¿¡ä»»æ ¡å‡†è¿‡ç¨‹(Recursive trust calibration process)ï¼Œå¹¶é’ˆå¯¹è´Ÿè´£ä»»ä¸”åæ€æ€§åœ°ä½¿ç”¨LLMæå‡ºäº†å…·ä½“å®è·µå»ºè®®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09088v1",
      "published_date": "2025-12-09 19:59:23 UTC",
      "updated_date": "2025-12-09 19:59:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:02.850435+00:00"
    },
    {
      "arxiv_id": "2512.09085v1",
      "title": "Mental Models of Autonomy and Sentience Shape Reactions to AI",
      "title_zh": "è‡ªä¸»æ€§ä¸æ„ŸçŸ¥åŠ›çš„å¿ƒç†æ¨¡å‹å¦‚ä½•å¡‘é€ å¯¹äººå·¥æ™ºèƒ½çš„ååº”",
      "authors": [
        "Janet V. T. Pauketat",
        "Daniel B. Shank",
        "Aikaterina Manoli",
        "Jacy Reese Anthis"
      ],
      "abstract": "Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å¯¹äººå·¥æ™ºèƒ½(AI)çš„è‡ªä¸»æ€§(Autonomy)ä¸æ„ŸçŸ¥æ€§(Sentience)çš„å¿ƒç†æ¨¡å‹å¦‚ä½•å·®å¼‚åŒ–åœ°å½±å“å…¶å¯¹AIçš„ååº”ã€‚ç ”ç©¶è€…é€šè¿‡ä¸‰é¡¹åˆæ­¥ç ”ç©¶å’Œå››é¡¹é¢„æ³¨å†Œçš„åœºæ™¯å®éªŒ(Vignette Experiments)ï¼Œç³»ç»Ÿåœ°æ¯”è¾ƒäº†å°†AIæè¿°ä¸ºå…·æœ‰è‡ªä¸»æ€§ã€æ„ŸçŸ¥æ€§ã€ä¸¤è€…å…¼å…·æˆ–ä¸¤è€…çš†æ— æ—¶çš„å¿ƒç†å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼Œæ¿€æ´»æ„ŸçŸ¥æ€§æ¨¡å‹æ¯”è‡ªä¸»æ€§æ›´èƒ½æ˜¾è‘—æå‡ä¸ªä½“å¯¹AIçš„å¿ƒæ™ºæ„ŸçŸ¥(Mind Perception)åŠé“å¾·è€ƒé‡(Moral Consideration)ï¼Œè€Œè‡ªä¸»æ€§åˆ™æ¯”æ„ŸçŸ¥æ€§æ›´å®¹æ˜“å¼•å‘å—è¯•è€…çš„å¨èƒæ„Ÿ(Perceived Threat)ã€‚æ­¤å¤–ï¼Œæ„ŸçŸ¥æ€§å¯¹æ„ŸçŸ¥è‡ªä¸»æ€§çš„æå‡ä½œç”¨å¼ºäºåå‘ä½œç”¨ï¼Œä¸”å†…éƒ¨å…ƒåˆ†ææ˜¾ç¤ºæ„ŸçŸ¥æ€§å¯¹äººç±»ååº”çš„å¹³å‡å½±å“ç¨‹åº¦è¶…è¿‡äº†è‡ªä¸»æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡è§£æ„AIçš„ä¸åŒå¿ƒç†æ¨¡å‹ï¼Œä¸ºç²¾å‡†è®¾è®¡æ‹ŸäººåŒ–AIåŠä¼˜åŒ–æç¤ºè¯æ¥å£(Prompting Interfaces)æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "37 pages, 6 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.09085v1",
      "published_date": "2025-12-09 19:56:52 UTC",
      "updated_date": "2025-12-09 19:56:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:05.967368+00:00"
    },
    {
      "arxiv_id": "2512.09076v1",
      "title": "Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting",
      "title_zh": "è¶…è¶Šçƒ­æ½®ï¼šè½»é‡çº§ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç©ºæ°”è´¨é‡é¢„æµ‹ä¸­çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Moazzam Umer Gondal",
        "Hamad ul Qudous",
        "Asma Ahmad Farhan"
      ],
      "abstract": "Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è½»é‡çº§åŠ æ€§æ¨¡å‹ Facebook Prophet (FBP) å’Œ NeuralProphet (NP) åœ¨åŒ—äº¬å¸‚ç©ºæ°”è´¨é‡é¢„æµ‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹  (Deep Learning) æ¨¡å‹å› é«˜å¤æ‚åº¦å’Œä½å¯è§£é‡Šæ€§ (interpretability) éš¾ä»¥åœ¨ä¸šåŠ¡ä¸­éƒ¨ç½²çš„é—®é¢˜ã€‚ç ”ç©¶åˆ©ç”¨å¤šå¹´ä»½æ±¡æŸ“ç‰©ä¸æ°”è±¡æ•°æ®ï¼Œç»“åˆäº’ä¿¡æ¯ (mutual information) å’Œ mRMR ç­‰ç³»ç»ŸåŒ–ç‰¹å¾é€‰æ‹© (feature selection) æ–¹æ³•ï¼Œå¹¶å¯¹æ¯”äº† LSTMã€LightGBM åŠä¼ ç»Ÿ SARIMAX æ¨¡å‹çš„é¢„æµ‹æ•ˆèƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒFBP åœ¨ PM2.5 å’Œ PM10 çš„é¢„æµ‹ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶æµ‹è¯•é›† R^2 å‡è¶…è¿‡ 0.94ï¼Œåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†å¯è§£é‡ŠåŠ æ€§æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦ã€é€æ˜åº¦ä¸éƒ¨ç½²ä¾¿åˆ©æ€§ä¹‹é—´å®ç°äº†ç†æƒ³å¹³è¡¡ï¼Œæ˜¯ç¯å¢ƒç›‘æµ‹ä¸æ”¿ç­–åˆ¶å®šä¸­æå…·å®ç”¨ä»·å€¼çš„é¢„æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09076v1",
      "published_date": "2025-12-09 19:39:45 UTC",
      "updated_date": "2025-12-09 19:39:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:21.463086+00:00"
    },
    {
      "arxiv_id": "2512.09069v1",
      "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
      "title_zh": "KD-OCTï¼šé¢å‘ä¸´åºŠçº§è§†ç½‘è†œ OCT åˆ†ç±»çš„é«˜æ•ˆçŸ¥è¯†è’¸é¦",
      "authors": [
        "Erfan Nourbakhsh",
        "Nasrin Sanjari",
        "Ali Nourbakhsh"
      ],
      "abstract": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency-accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD-OCT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒèŒƒå›´å†…å¯¼è‡´è§†åŠ›ä¸§å¤±çš„ä¸»è¦åŸå› â€”â€”å¹´é¾„ç›¸å…³æ€§é»„æ–‘å˜æ€§(AMD)å’Œè„‰ç»œè†œæ–°ç”Ÿè¡€ç®¡(CNV)ç›¸å…³ç—…å˜ï¼ŒæŒ‡å‡ºå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æ(OCT)åœ¨æ—©æœŸæ£€æµ‹ä¸­å…·æœ‰æ ¸å¿ƒä½œç”¨ï¼Œä½†é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¨¡å‹(å¦‚ConvNeXtV2-Large)åœ¨ä¸´åºŠéƒ¨ç½²ä¸­é¢ä¸´ä¸¥é‡çš„è®¡ç®—èµ„æºæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†KD-OCTï¼Œä¸€ç§æ–°å‹çŸ¥è¯†è’¸é¦(Knowledge Distillation)æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¢å¼ºå‹çš„ConvNeXtV2-Largeæ•™å¸ˆæ¨¡å‹å‹ç¼©ä¸ºè½»é‡çº§çš„EfficientNet-B2å­¦ç”Ÿæ¨¡å‹ï¼Œç”¨äºå¯¹æ­£å¸¸ã€ç»ç’ƒè†œç–£(Drusen)å’ŒCNVç—…ä¾‹è¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚è¯¥æ¡†æ¶é›†æˆäº†é«˜çº§æ•°æ®å¢å¼ºã€éšæœºæƒé‡å¹³å‡(Stochastic Weight Averaging)å’Œç„¦ç‚¹æŸå¤±(Focal Loss)ï¼Œå¹¶åˆ©ç”¨å®æ—¶è’¸é¦æŠ€æœ¯å¹³è¡¡æ•™å¸ˆçŸ¥è¯†è¿ç§»ä¸çœŸå®æ ‡ç­¾ç›‘ç£ã€‚åœ¨Noor Eye Hospital (NEH)æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒKD-OCTåœ¨æ•ˆç‡ä¸å‡†ç¡®ç‡çš„å¹³è¡¡ä¸Šä¼˜äºç°æœ‰çš„å¤šå°ºåº¦æˆ–ç‰¹å¾èåˆåˆ†ç±»å™¨ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å¤§å¹…å‡å°‘æ¨¡å‹ä½“ç§¯å’Œæ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†æ¥è¿‘æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ï¼Œæå¤§åœ°ä¿ƒè¿›äº†ä¸´åºŠçº§OCTåˆ†ç±»ä»»åŠ¡åœ¨è¾¹ç¼˜ä¾§è®¾å¤‡(Edge Deployment)ä¸Šçš„å®æ—¶éƒ¨ç½²ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09069v1",
      "published_date": "2025-12-09 19:34:30 UTC",
      "updated_date": "2025-12-09 19:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:14.281348+00:00"
    },
    {
      "arxiv_id": "2512.09065v1",
      "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors",
      "title_zh": "ShelfAwareï¼šå‡†é™æ€ç¯å¢ƒä¸‹åŸºäºä½æˆæœ¬ä¼ æ„Ÿå™¨çš„å®æ—¶è§†è§‰æƒ¯æ€§è¯­ä¹‰å®šä½",
      "authors": [
        "Shivendra Agrawal",
        "Jake Brawer",
        "Ashutosh Naik",
        "Alessandro Roncone",
        "Bradley Hayes"
      ],
      "abstract": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‡†é™æ€(Quasi-Static)å®¤å†…ç¯å¢ƒä¸­çš„å‡ ä½•é‡å¤æ€§ã€åŠ¨æ€é®æŒ¡å’Œæ„ŸçŸ¥å™ªå£°å¯¼è‡´è§†è§‰å®šä½å¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†ShelfAwareã€‚è¿™æ˜¯ä¸€ç§è¯­ä¹‰ç²’å­æ»¤æ³¢(Semantic Particle Filter)æ¡†æ¶ï¼Œå®ƒå°†åœºæ™¯è¯­ä¹‰è§†ä¸ºç‰©ä½“ç±»åˆ«çš„ç»Ÿè®¡è¯æ®è€Œéå›ºå®šåœ°æ ‡ï¼Œæ˜¾è‘—å¢å¼ºäº†å…¨çƒå®šä½çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•èåˆäº†æ·±åº¦ä¼¼ç„¶(Depth Likelihood)ä¸ä»¥ç±»åˆ«ä¸ºä¸­å¿ƒçš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¹¶åˆ©ç”¨é¢„è®¡ç®—çš„è¯­ä¹‰è§†ç‚¹åº“åœ¨è’™ç‰¹å¡æ´›å®šä½(MCL)ä¸­æ‰§è¡Œé€†å‘è¯­ä¹‰æè®®(Inverse Semantic Proposals)ï¼Œå®ç°äº†å¿«é€Ÿä¸”æœ‰é’ˆå¯¹æ€§çš„å‡è®¾ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒShelfAwareåœ¨é›¶å”®ç¯å¢ƒä¸‹çš„å®šä½æˆåŠŸç‡é«˜è¾¾96%ï¼Œè¿œè¶…ä¼ ç»Ÿçš„MCLå’ŒAMCLç®—æ³•ï¼Œä¸”å¹³å‡æ”¶æ•›æ—¶é—´ä»…ä¸º1.91ç§’ã€‚ç”±äºè¯¥æ–¹æ³•ä»…éœ€ä½æˆæœ¬è§†è§‰ä¼ æ„Ÿå™¨å’Œè§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡(VIO)å³å¯åœ¨æ¶ˆè´¹çº§å¹³å°ä¸Šå®æ—¶è¿è¡Œï¼Œå®ƒå¯ä½œä¸ºåŸºç¡€è®¾æ–½æ— å…³çš„ç»„ä»¶ï¼Œå¹¿æ³›åº”ç”¨äºä»“åº“æœºå™¨äººåŠè§†éšœäººå£«è¾…åŠ©å¯¼èˆªç³»ç»Ÿã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.09065v1",
      "published_date": "2025-12-09 19:33:19 UTC",
      "updated_date": "2025-12-09 19:33:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:18.436561+00:00"
    },
    {
      "arxiv_id": "2512.17929v2",
      "title": "Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods",
      "title_zh": "å®è§‚ç»æµä¸ç¡®å®šæ€§ä¸‹çš„è´§å¸æ”¿ç­–å¼ºåŒ–å­¦ä¹ ï¼šè¡¨æ ¼æ³•ä¸å‡½æ•°é€¼è¿‘æ–¹æ³•åˆ†æ",
      "authors": [
        "Tony Wang",
        "Kyle Feinstein",
        "Sheryl Chen"
      ],
      "abstract": "We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publicly accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Notably, despite its simplicity, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å®è§‚ç»æµå…³ç³»å…·æœ‰ä¸ç¡®å®šæ€§ä¸”éšæ—¶é—´å˜åŒ–çš„èƒŒæ™¯ä¸‹ï¼Œä¸­å¤®é“¶è¡Œåº”å¦‚ä½•é€šè¿‡åŠ¨æ€è®¾å®šçŸ­æœŸåä¹‰åˆ©ç‡æ¥ç¨³å®šé€šè´§è†¨èƒ€å’Œå¤±ä¸šç‡ã€‚ä½œè€…å°†è´§å¸æ”¿ç­–å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œåˆ©ç”¨ç¾è”å‚¨ç»æµæ•°æ®åº“ï¼ˆFREDï¼‰æ„å»ºäº†çº¿æ€§é«˜æ–¯è½¬æ¢æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†ç¦»æ•£åŠ¨ä½œçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å’ŒäºŒæ¬¡æŸå¤±å¥–åŠ±å‡½æ•°ã€‚ç ”ç©¶è¯¦ç»†å¯¹æ¯”äº†ä¹ç§å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰æ–¹æ³•ï¼Œæ¶µç›–äº†è¡¨æ ¼å‹ Q-learningã€SARSAã€Actor-Criticã€Deep Q-Networks (DQN)ã€å…·æœ‰ä¸ç¡®å®šæ€§é‡åŒ–çš„ Bayesian Q-learning ä»¥åŠéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œå¹¶ä¸æ³°å‹’è§„åˆ™ï¼ˆTaylor Ruleï¼‰ç­‰ä¼ ç»ŸåŸºå‡†è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ–¹æ³•ç®€å•ï¼Œæ ‡å‡†çš„è¡¨æ ¼å‹ Q-learning å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºæ›´ä¸ºå¤æ‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œä¼ ç»Ÿæ”¿ç­–è§„åˆ™ã€‚è¿™ä¸€å‘ç°å»ºè®®åœ¨å®è§‚ç»æµæ”¿ç­–é¢†åŸŸï¼Œè¾ƒç®€å•çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯èƒ½æ¯”å¤æ‚æŠ€æœ¯è¡¨ç°å¾—æ›´ä¸ºç¨³å¥ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†ç°ä»£å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå®è§‚ç»æµé¢†åŸŸæ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG",
        "econ.EM"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17929v2",
      "published_date": "2025-12-09 19:27:47 UTC",
      "updated_date": "2026-01-03 00:40:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:46.191505+00:00"
    },
    {
      "arxiv_id": "2512.09048v2",
      "title": "Monitoring Deployed AI Systems in Health Care",
      "title_zh": "åŒ»ç–—ä¿å¥ä¸­å·²éƒ¨ç½²äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç›‘æµ‹",
      "authors": [
        "Timothy Keyes",
        "Alison Callahan",
        "Abby S. Pandya",
        "Nerissa Ambers",
        "Juan M. Banda",
        "Miguel Fuentes",
        "Carlene Lugtu",
        "Pranav Masariya",
        "Srikar Nallan",
        "Connor O'Brien",
        "Thomas Wang",
        "Emily Alsentzer",
        "Jonathan H. Chen",
        "Dev Dash",
        "Matthew A. Eisenberg",
        "Patricia Garcia",
        "Nikesh Kotecha",
        "Anurang Revri",
        "Michael A. Pfeffer",
        "Nigam H. Shah",
        "Sneha S. Jain"
      ],
      "abstract": "Post-deployment monitoring of artificial intelligence (AI) systems in health care is essential to ensure their safety, quality, and sustained benefit-and to support governance decisions about which systems to update, modify, or decommission. Motivated by these needs, we developed a framework for monitoring deployed AI systems grounded in the mandate to take specific actions when they fail to behave as intended. This framework, which is now actively used at Stanford Health Care, is organized around three complementary principles: system integrity, performance, and impact. System integrity monitoring focuses on maximizing system uptime, detecting runtime errors, and identifying when changes to the surrounding IT ecosystem have unintended effects. Performance monitoring focuses on maintaining accurate system behavior in the face of changing health care practices (and thus input data) over time. Impact monitoring assesses whether a deployed system continues to have value in the form of benefit to clinicians and patients. Drawing on examples of deployed AI systems at our academic medical center, we provide practical guidance for creating monitoring plans based on these principles that specify which metrics to measure, when those metrics should be reviewed, who is responsible for acting when metrics change, and what concrete follow-up actions should be taken-for both traditional and generative AI. We also discuss challenges to implementing this framework, including the effort and cost of monitoring for health systems with limited resources and the difficulty of incorporating data-driven monitoring practices into complex organizations where conflicting priorities and definitions of success often coexist. This framework offers a practical template and starting point for health systems seeking to ensure that AI deployments remain safe and effective over time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—é¢†åŸŸå·²éƒ¨ç½²çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€å‘äº†ä¸€å¥—ç›‘æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®ä¿å…¶å®‰å…¨æ€§ã€è´¨é‡å’ŒæŒç»­è·ç›Šï¼Œå¹¶å·²åœ¨ Stanford Health Care å®é™…æŠ•å…¥ä½¿ç”¨ã€‚è¯¥æ¡†æ¶å›´ç»• System Integrityã€Performance å’Œ Impact ä¸‰ä¸ªäº’è¡¥åŸåˆ™æ„å»ºï¼Œåˆ†åˆ«å…³æ³¨ç³»ç»Ÿè¿è¡Œç¨³å®šæ€§ã€åœ¨åŒ»ç–—å®è·µå˜åŒ–ä¸‹çš„è¡Œä¸ºå‡†ç¡®æ€§ä»¥åŠå¯¹ä¸´åºŠåŒ»ç”Ÿå’Œæ‚£è€…çš„å®é™…ä»·å€¼ã€‚é€šè¿‡ç»“åˆå­¦æœ¯åŒ»ç–—ä¸­å¿ƒçš„éƒ¨ç½²æ¡ˆä¾‹ï¼Œç ”ç©¶ä¸ºåˆ¶å®šç›‘æµ‹è®¡åˆ’æä¾›äº†å…·ä½“æŒ‡å—ï¼Œæ˜ç¡®äº† Metrics çš„é€‰æ‹©ã€è¯„å®¡é¢‘ç‡ã€è´£ä»»å½’å±åŠåç»­è¡ŒåŠ¨ï¼Œä¸”åŒæ—¶é€‚ç”¨äºä¼ ç»Ÿ AI ä¸ Generative AIã€‚è®ºæ–‡è¿›ä¸€æ­¥æ¢è®¨äº†å®æ–½è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚èµ„æºå—é™ä¸‹çš„ç›‘æ§æˆæœ¬ä»¥åŠåœ¨å¤æ‚ç»„ç»‡æ¶æ„ä¸­æ•´åˆæ•°æ®é©±åŠ¨å®è·µçš„éš¾åº¦ã€‚è¯¥ç ”ç©¶ä¸ºåŒ»ç–—ç³»ç»Ÿç¡®ä¿æŒç»­ã€å®‰å…¨ä¸”æœ‰æ•ˆçš„ AI åº”ç”¨æä¾›äº†ä¸€ä¸ªå®ç”¨çš„æ¨¡æ¿ä¸å‚è€ƒèµ·ç‚¹ã€‚",
      "categories": [
        "q-bio.OT",
        "cs.AI"
      ],
      "primary_category": "q-bio.OT",
      "comment": "36 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09048v2",
      "published_date": "2025-12-09 19:06:48 UTC",
      "updated_date": "2026-01-15 20:08:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:24.337894+00:00"
    },
    {
      "arxiv_id": "2512.08931v2",
      "title": "Astra: General Interactive World Model with Autoregressive Denoising",
      "title_zh": "Astraï¼šåŸºäºè‡ªå›å½’å»å™ªçš„é€šç”¨äº¤äº’å¼ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Yixuan Zhu",
        "Jiaqi Feng",
        "Wenzhao Zheng",
        "Yuan Gao",
        "Xin Tao",
        "Pengfei Wan",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Astraï¼Œä¸€ä¸ªæ—¨åœ¨ä¸ºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæŠ“å–ç­‰å¤šæ ·åŒ–åœºæ™¯ç”ŸæˆçœŸå®æœªæ¥æ™¯è±¡çš„é€šç”¨äº¤äº’å¼ä¸–ç•Œæ¨¡å‹(World Model)ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨é•¿ç¨‹é¢„æµ‹å’ŒåŠ¨ä½œäº¤äº’æ–¹é¢çš„ä¸è¶³ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§è‡ªå›å½’å»å™ª(Autoregressive Denoising)æ¶æ„ï¼Œåˆ©ç”¨æ—¶é—´å› æœæ³¨æ„åŠ›(Temporal Causal Attention)æ”¯æŒæµå¼è¾“å‡ºã€‚è¯¥æ¡†æ¶é€šè¿‡å™ªå£°å¢å¼ºçš„å†å²è®°å¿†(Noise-augmented History Memory)å¹³è¡¡å“åº”èƒ½åŠ›ä¸æ—¶é—´è¿è´¯æ€§ï¼Œå¹¶å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥é€‚é…å™¨(Action-aware Adapter)å°†åŠ¨ä½œä¿¡å·ç›´æ¥æ³¨å…¥å»å™ªè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†åŠ¨ä½œä¸“å®¶æ··åˆ(Mixture of Action Experts)æœºåˆ¶æ¥åŠ¨æ€è·¯ç”±å¼‚æ„åŠ¨ä½œæ¨¡æ€ï¼Œä»è€Œå¢å¼ºåœ¨æ¢ç´¢ã€æ“çºµå’Œç›¸æœºæ§åˆ¶ç­‰ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAstraåœ¨è§†é¢‘ä¿çœŸåº¦ã€é•¿ç¨‹é¢„æµ‹èƒ½åŠ›å’ŒåŠ¨ä½œå¯¹é½ç²¾åº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ï¼Œå®ç°äº†äº¤äº’å¼ä¸”ä¸€è‡´çš„é•¿æœŸè§†é¢‘é¢„æµ‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/EternalEvan/Astra",
      "pdf_url": "https://arxiv.org/pdf/2512.08931v2",
      "published_date": "2025-12-09 18:59:57 UTC",
      "updated_date": "2025-12-15 03:59:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:26.249121+00:00"
    },
    {
      "arxiv_id": "2512.08923v1",
      "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
      "title_zh": "ç›¸åŒå†…å®¹ï¼Œä¸åŒç­”æ¡ˆï¼šMLLMs ä¸­çš„è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§",
      "authors": [
        "Angela van Sprang",
        "Laurens Samson",
        "Ana Lucic",
        "Erman Acar",
        "Sennay Ghebreab",
        "Yuki M. Asano"
      ],
      "abstract": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†RESTå’ŒREST+ï¼ˆRender-Equivalence Stress Testsï¼‰ä¸¤ä¸ªæ–°åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§ï¼ˆCross-Modal Inconsistencyï¼‰é—®é¢˜ã€‚å°½ç®¡MLLMsæ—¨åœ¨å°†è§†è§‰å’Œè¯­è¨€æ˜ å°„åˆ°åŒä¸€åµŒå…¥ç©ºé—´ï¼Œä½†åœ¨å¤„ç†åŒ…å«ç›¸åŒè¯­ä¹‰ä¿¡æ¯çš„å›¾åƒã€æ–‡æœ¬å’Œæ··åˆæ¨¡æ€ä»»åŠ¡æ—¶ï¼Œç°æœ‰æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ¨ç†ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹15ä¸ªä¸»æµMLLMsçš„è¯„ä¼°å‘ç°ï¼Œå³ä½¿æ’é™¤æ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰å¹²æ‰°ï¼Œè·¨æ¨¡æ€ä¸ä¸€è‡´æ€§ä¾ç„¶å¹¿æ³›å­˜åœ¨ï¼Œä¸”å•çº¯å°†æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒæˆ–å°†å›¾åƒæè¿°ä¸ºæ–‡æœ¬éƒ½æ— æ³•æœ‰æ•ˆè§£å†³è¯¥é—®é¢˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œè§†è§‰ç‰¹å¾ï¼ˆå¦‚æ–‡æœ¬é¢œè‰²å’Œåˆ†è¾¨ç‡ï¼‰ä»¥åŠè§†è§‰æ ‡è®°ï¼ˆVision Tokensï¼‰çš„æ•°é‡ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å‘ç°ä¸€è‡´æ€§å¾—åˆ†ä¸æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„æ¨¡æ€å·®è·ï¼ˆModality Gapï¼‰å‘ˆæ­£ç›¸å…³ï¼Œä¸ºç†è§£è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§æä¾›äº†æœºæ¢°è®ºå±‚é¢çš„è§£é‡Šã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Angela van Sprang and Laurens Samson contributed equally as first authors. Preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.08923v1",
      "published_date": "2025-12-09 18:57:07 UTC",
      "updated_date": "2025-12-09 18:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:43.269766+00:00"
    },
    {
      "arxiv_id": "2512.08914v1",
      "title": "SAQ: Stabilizer-Aware Quantum Error Correction Decoder",
      "title_zh": "SAQï¼šç¨³å®šå­æ„ŸçŸ¥é‡å­çº é”™è§£ç å™¨",
      "authors": [
        "David Zenati",
        "Eliya Nachmani"
      ],
      "abstract": "Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods. We introduce SAQ-Decoder, a unified framework combining transformer-based learning with constraint aware post-processing that achieves both near Maximum Likelihood (ML) accuracy and linear computational scalability with respect to the syndrome size. Our approach combines a dual-stream transformer architecture that processes syndromes and logical information with asymmetric attention patterns, and a novel differentiable logical loss that directly optimizes Logical Error Rates (LER) through smooth approximations over finite fields. SAQ-Decoder achieves near-optimal performance, with error thresholds of 10.99% (independent noise) and 18.6% (depolarizing noise) on toric codes that approach the ML bounds of 11.0% and 18.9% while outperforming existing neural and classical baselines in accuracy, complexity, and parameter efficiency. Our findings establish that learned decoders can simultaneously achieve competitive decoding accuracy and computational efficiency, addressing key requirements for practical fault-tolerant quantum computing systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAQ-Decoderï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†Transformerå­¦ä¹ ä¸çº¦æŸæ„ŸçŸ¥åå¤„ç†çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é‡å­çº é”™(Quantum Error Correction)è§£ç ä¸­å‡†ç¡®æ€§ä¸æ•ˆç‡çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¶æ„é‡‡ç”¨åŒæµTransformerç»“æ„å¤„ç†è¯å€™(syndromes)å’Œé€»è¾‘ä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¯å¾®é€»è¾‘æŸå¤±å‡½æ•°(differentiable logical loss)ï¼Œé€šè¿‡æœ‰é™åŸŸä¸Šçš„å¹³æ»‘è¿‘ä¼¼ç›´æ¥ä¼˜åŒ–é€»è¾‘é”™è¯¯ç‡(Logical Error Rates)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAQ-Decoderåœ¨ç¯é¢ç (toric codes)ä¸Šå®ç°äº†æ¥è¿‘æœ€å¤§ä¼¼ç„¶(Maximum Likelihood)ç•Œçš„æ€§èƒ½ï¼Œç‹¬ç«‹å™ªå£°å’Œå»æåŒ–å™ªå£°ä¸‹çš„é”™è¯¯é˜ˆå€¼åˆ†åˆ«è¾¾åˆ°10.99%å’Œ18.6%ã€‚è¯¥æ¨¡å‹åœ¨ä¿è¯çº¿æ€§è®¡ç®—æ‰©å±•æ€§çš„åŒæ—¶ï¼Œåœ¨å‡†ç¡®æ€§ã€å¤æ‚åº¦å’Œå‚æ•°æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ç¥ç»å’Œç»å…¸åŸºçº¿ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å­¦ä¹ å‹è§£ç å™¨å¯ä»¥åŒæ—¶å…¼é¡¾é«˜è§£ç ç²¾åº¦ä¸è®¡ç®—æ•ˆç‡ï¼Œä¸ºå®ç°å®ç”¨åŒ–çš„å®¹é”™é‡å­è®¡ç®—ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08914v1",
      "published_date": "2025-12-09 18:51:35 UTC",
      "updated_date": "2025-12-09 18:51:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:59:45.792278+00:00"
    },
    {
      "arxiv_id": "2512.08894v1",
      "title": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training",
      "title_zh": "é‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ä¸‹æ¸¸æŒ‡æ ‡çš„ç¼©æ”¾ç‰¹æ€§",
      "authors": [
        "Jakub Krajewski",
        "Amitis Shidani",
        "Dan Busbridge",
        "Sam Wiseman",
        "Jason Ramapuram"
      ],
      "abstract": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)è®­ç»ƒä¸­ä¸‹æ¸¸è¯„ä¼°æŒ‡æ ‡çš„ç¼©æ”¾ç‰¹æ€§(Scaling Properties)ï¼ŒæŒ‘æˆ˜äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½é¢„æµ‹ä¸å¯é çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç›´æ¥å»ºæ¨¡æ¡†æ¶ï¼Œåˆ©ç”¨è®­ç»ƒé¢„ç®—(Training Budget)é¢„æµ‹åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œå‘ç°åœ¨å›ºå®šçš„æ ‡è®°å‚æ•°æ¯”(Token-to-Parameter Ratio)ä¸‹ï¼Œç®€å•çš„å¹‚å¾‹(Power Law)èƒ½å‡†ç¡®æè¿°å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡å¯¹æ•°å‡†ç¡®ç‡çš„ç¼©æ”¾è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç›´æ¥æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µç¨‹åºå…·æœ‰æ›´å¥½çš„å¤–æ¨æ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆå‡å°‘å¤åˆè¯¯å·®ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ–°çš„å‡½æ•°å½¢å¼æ¥é¢„æµ‹ä¸åŒæ¯”ä¾‹ä¸‹çš„å‡†ç¡®ç‡ï¼Œå¹¶è€ƒè™‘äº†é‡å¤é‡‡æ ·å¯¹æ¨ç†è®¡ç®—(Inference Compute)çš„å½±å“ã€‚é€šè¿‡åœ¨å‚æ•°é‡è¾¾17Bã€è®­ç»ƒé‡è¾¾350B tokensçš„æ¨¡å‹ä¸Šè¿›è¡ŒéªŒè¯ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†ç›´æ¥å»ºæ¨¡ä¸‹æ¸¸æŒ‡æ ‡çš„å¯è¡Œæ€§ï¼Œå¹¶å¼€æºäº†å…¨éƒ¨é¢„è®­ç»ƒæŸå¤±å’Œè¯„ä¼°ç»“æœä»¥ä¾›å¤ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08894v1",
      "published_date": "2025-12-09 18:33:48 UTC",
      "updated_date": "2025-12-09 18:33:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:21.331951+00:00"
    },
    {
      "arxiv_id": "2512.08892v1",
      "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
      "title_zh": "è¿ˆå‘åŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„å¿ å®æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Guangzhi Xiong",
        "Zhenghao He",
        "Bohan Liu",
        "Sanchit Sinha",
        "Aidong Zhang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ä¸­ç”Ÿæˆå†…å®¹èƒŒç¦»æ£€ç´¢è¯æ®çš„â€œå¿ å®æ€§å¤±æ•ˆâ€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRAGLensçš„è½»é‡çº§æ£€æµ‹æ–¹æ¡ˆã€‚ä¸ºäº†å…‹æœç°æœ‰æ£€æµ‹æ–¹æ³•å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®æˆ–æ˜‚è´µå¤–éƒ¨æ¨¡å‹è¯„åˆ¤çš„ä¾èµ–ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoders, SAEs)å¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å†…éƒ¨æ¿€æ´»è¿›è¡Œè§£è€¦ï¼ŒæˆåŠŸè¯†åˆ«å‡ºåœ¨RAGå¹»è§‰æœŸé—´è¢«è§¦å‘çš„ç‰¹å®šç‰¹å¾ã€‚åŸºäºä¿¡æ¯ç‰¹å¾é€‰æ‹©å’ŒåŠ æ€§ç‰¹å¾å»ºæ¨¡ï¼ŒRAGLensèƒ½å¤Ÿåˆ©ç”¨æ¨¡å‹å†…éƒ¨è¡¨ç¤ºå‡†ç¡®è¯†åˆ«å¹¶æ ‡è®°ä¸å¿ å®çš„ç”Ÿæˆè¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAGLensåœ¨æ£€æµ‹æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½ä¸ºæ£€æµ‹å†³ç­–æä¾›å…·æœ‰å¯è§£é‡Šæ€§çš„ä¾æ®ï¼Œä»è€Œæœ‰æ•ˆå®ç°RAGå¹»è§‰çš„äº‹åç¼“è§£ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ­ç¤ºäº†LLMså†…éƒ¨å¹»è§‰ç›¸å…³ä¿¡å·åˆ†å¸ƒçš„æ–°è§è§£ï¼Œä¸ºæ„å»ºæ›´å¯é ã€æ›´å…·å¯è§£é‡Šæ€§çš„ç”Ÿæˆç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08892v1",
      "published_date": "2025-12-09 18:33:22 UTC",
      "updated_date": "2025-12-09 18:33:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:02.425352+00:00"
    },
    {
      "arxiv_id": "2512.08889v1",
      "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers",
      "title_zh": "æ— éœ€æ ‡ç­¾ï¼šåŸºäºå¤šæ¨¡æ€éªŒè¯å™¨çš„è§†è§‰æ¨ç†å™¨è®­ç»ƒ",
      "authors": [
        "Damiano Marsili",
        "Georgia Gkioxari"
      ],
      "abstract": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰æ¨ç†(Visual Reasoning)ä¸­ç²¾ç¡®çš„å¯¹è±¡å®šä½(Object Grounding)å’Œå¤æ‚ç©ºé—´å…³ç³»ç†è§£éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„è®­ç»ƒæ¡†æ¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ä¾èµ–å¤§è§„æ¨¡ç›‘ç£æ•°æ®ä¸é¢ä¸´é€»è¾‘ç¼ºé™·åŠå®šä½é”™è¯¯ä¹‹é—´æŒ£æ‰ï¼Œè€Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥AIé©±åŠ¨çš„éªŒè¯å™¨ç³»ç»Ÿå…‹æœäº†è¿™äº›å±€é™ã€‚å…¶ä¸­ï¼ŒLLM Verifieré€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æŠ€æœ¯ç²¾ç‚¼æ¨ç†é€»è¾‘ï¼Œè€ŒVLM Verifieråˆ™åˆ©ç”¨è‡ªåŠ¨åŒ–çš„éš¾è´Ÿæ ·æœ¬æŒ–æ˜(Hard-negative Mining)æ˜¾è‘—å¢å¼ºè§†è§‰å®šä½èƒ½åŠ›ï¼Œä»è€Œå½»åº•æ¶ˆé™¤äº†å¯¹çœŸå®æ ‡ç­¾(Ground Truth)çš„éœ€æ±‚ã€‚è¯¥è®¾è®¡æœ‰æ•ˆç»“åˆäº†å…ˆè¿›çº¯è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æŸ¥è¯¢æ‹†è§£ä¸Šçš„ä¼˜åŠ¿ï¼Œä»¥åŠé€šè¿‡è§†è§‰å¤§æ¨¡å‹è¯„è®ºè€…(VLM Critics)ä¼˜åŒ–çš„ä¸“å®¶æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ä»…è¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œæ›´åœ¨è§†è§‰å®šä½ç²¾åº¦ä¸Šå®ç°äº†è´¨çš„çªç ´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://glab-caltech.github.io/valor/",
      "pdf_url": "https://arxiv.org/pdf/2512.08889v1",
      "published_date": "2025-12-09 18:30:23 UTC",
      "updated_date": "2025-12-09 18:30:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:25.104250+00:00"
    },
    {
      "arxiv_id": "2512.08879v1",
      "title": "DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process",
      "title_zh": "DAO-GPï¼šæ¼‚ç§»æ„ŸçŸ¥çš„åœ¨çº¿éçº¿æ€§å›å½’é«˜æ–¯è¿‡ç¨‹",
      "authors": [
        "Mohammad Abu-Shaira",
        "Ajita Rattani",
        "Weishi Shi"
      ],
      "abstract": "Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çœŸå®ä¸–ç•Œæ•°æ®ä¸­ç”±æ•°æ®åˆ†å¸ƒæ¼”å˜å¼•èµ·çš„concept driftï¼ˆæ¦‚å¿µæ¼‚ç§»ï¼‰é—®é¢˜ï¼Œæå‡ºäº†DAO-GP (Drift-Aware Online Gaussian Process)ï¼Œè¿™æ˜¯ä¸€ç§å…¨è‡ªé€‚åº”ã€æ— è¶…å‚æ•°ä¸”å…·æœ‰è¡°å‡æœºåˆ¶çš„ç¨€ç–éçº¿æ€§å›å½’æ¨¡å‹ã€‚DAO-GPé›†æˆäº†å†…ç½®çš„æ¼‚ç§»æ£€æµ‹ä¸è‡ªé€‚åº”æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ¼‚ç§»çš„ä¸¥é‡ç¨‹åº¦åŠ¨æ€è°ƒæ•´æ¨¡å‹è¡Œä¸ºï¼Œå…‹æœäº†ä¼ ç»Ÿåœ¨çº¿é«˜æ–¯è¿‡ç¨‹(GP)æ–¹æ³•ä¾èµ–å›ºå®šè¶…å‚æ•°ä¸”ç¼ºä¹æ¼‚ç§»æ„ŸçŸ¥çš„å±€é™æ€§ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDAO-GPåœ¨å¹³ç¨³ç¯å¢ƒä»¥åŠçªå‘ã€å¢é‡ã€æ¸è¿›ç­‰å¤šç§æ¼‚ç§»ç±»å‹ä¸‹å‡å±•ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚é€šè¿‡é«˜æ•ˆçš„å†…å­˜ç®¡ç†å’Œè¯±å¯¼ç‚¹æ¼”å˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸ç°æœ‰SOTAå‚æ•°åŒ–åŠéå‚æ•°åŒ–æ¨¡å‹çš„å¯¹æ¯”ä¸­ä¸€è‡´å–å¾—äº†ä¼˜è¶Šæˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨çº¿éçº¿æ€§å›å½’ä»»åŠ¡æä¾›äº†ä¸€ç§å…·å¤‡æ¼‚ç§»å¼¹æ€§çš„å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08879v1",
      "published_date": "2025-12-09 18:12:38 UTC",
      "updated_date": "2025-12-09 18:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:31.368098+00:00"
    },
    {
      "arxiv_id": "2512.08875v1",
      "title": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation",
      "title_zh": "å½“è¡¨æ ¼æ³„éœ²ï¼šé’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¡¨æ ¼æ•°æ®ç”Ÿæˆä¸­å­—ç¬¦ä¸²è®°å¿†çš„æ”»å‡»",
      "authors": [
        "Joshua Ward",
        "Bochao Gu",
        "Chi-Hua Wang",
        "Guang Cheng"
      ],
      "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Language Models (LLMs) åœ¨ç”Ÿæˆè¡¨æ ¼åˆæˆæ•°æ®æ—¶çš„éšç§å®‰å…¨æ€§ï¼Œå‘ç°ç°æœ‰çš„å¾®è°ƒå’Œæç¤ºè¯(prompting)æ–¹æ³•æ™®éå­˜åœ¨é‡ç°è®­ç»ƒæ•°æ®ä¸­è®°å¿†æ•°å­—å­—ç¬¦ä¸²çš„å€¾å‘ã€‚ä¸ºäº†ç³»ç»Ÿè¯„ä¼°è¿™ä¸€é£é™©ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º LevAtt çš„æ— é»‘ç›’æˆå‘˜æ¨ç†æ”»å‡» (No-box Membership Inference Attack, MIA)ï¼Œè¯¥æ”»å‡»ä»…é€šè¿‡åˆ†æç”Ÿæˆæ•°æ®ä¸­çš„æ•°å­—åºåˆ—å³å¯è¯†åˆ«è®­ç»ƒæˆå‘˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLevAtt åœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡æš´éœ²äº†ä¸¥é‡çš„éšç§æ³„éœ²ï¼Œåœ¨éƒ¨åˆ†å…ˆè¿›æ¨¡å‹ä¸Šç”šè‡³è¾¾åˆ°äº†å®Œç¾çš„æ”»å‡»å‡†ç¡®ç‡ã€‚é’ˆå¯¹æ­¤æ¼æ´ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§é˜²å¾¡ç­–ç•¥ï¼Œé‡ç‚¹å¼•å…¥äº†ä¸€ç§åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹æ•°å­—è¿›è¡Œç­–ç•¥æ€§æ‰°åŠ¨çš„æ–°å‹é‡‡æ ·æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥é˜²å¾¡æ‰‹æ®µèƒ½å¤Ÿä»¥æä½çš„åˆæˆæ•°æ®ä¿çœŸåº¦å’Œæ•ˆç”¨æŸå¤±ä¸ºä»£ä»·ï¼Œæœ‰æ•ˆæŠµå¾¡æ­¤ç±»éšç§æ”»å‡»ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨çš„è¡¨æ ¼æ•°æ®ç”Ÿæˆæ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08875v1",
      "published_date": "2025-12-09 18:06:31 UTC",
      "updated_date": "2025-12-09 18:06:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:35.170079+00:00"
    },
    {
      "arxiv_id": "2512.08873v1",
      "title": "Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning",
      "title_zh": "å›¾åƒæè¿°ä¸­é’ˆå¯¹ä½åˆ†è¾¨ç‡å›¾åƒæ½œåœ¨åµŒå…¥çš„å­ªç”Ÿé©±åŠ¨ä¼˜åŒ–",
      "authors": [
        "Jing Jie Tan",
        "Anissa Mokraoui",
        "Ban-Hoe Kwan",
        "Danny Wee-Kiat Ng",
        "Yan-Chai Hum"
      ],
      "abstract": "Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾åƒæè¿° (Image captioning) åœ¨å¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒ (Low-resolution image) æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºå—é™å’Œæ¨¡å‹é‡è®­å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† SOLI ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨å­ªç”Ÿç½‘ç»œ (Siamese network) æ¶æ„æ¥ä¼˜åŒ–æ½œåœ¨åµŒå…¥ (Latent embeddings)ï¼Œæ—¨åœ¨æå‡è½»é‡çº§æ¨¡å‹åœ¨ä½åˆ†è¾¨ç‡åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚é€šè¿‡å¼•å…¥åŒè·¯å¾„ç¥ç»ç½‘ç»œç»“æ„ï¼ŒSOLI åœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œå¢å¼ºäº†å›¾åƒåˆ°æ–‡æœ¬è½¬æ¢çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹å®ç°äº†é«˜æ•ˆçš„ç‰¹å¾æå–ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å›¾åƒæè¿°ä»»åŠ¡æä¾›äº†ä¸€ç§ç†æƒ³çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.08873v1",
      "published_date": "2025-12-09 18:05:59 UTC",
      "updated_date": "2025-12-09 18:05:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:35.711141+00:00"
    },
    {
      "arxiv_id": "2512.08870v2",
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "title_zh": "Fed-SEï¼šé¢å‘éšç§å—é™å¤šç¯å¢ƒ LLM æ™ºèƒ½ä½“çš„è”é‚¦è‡ªè¿›åŒ–",
      "authors": [
        "Xiang Chen",
        "Yuling Shi",
        "Qizhen Lan",
        "Yuchao Qiu",
        "Min Wang",
        "Xiaodong Gu",
        "Yanfu Yan"
      ],
      "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. Despite the demonstrated success of Federated Learning (FL) on static datasets, its effectiveness in open-ended, self-evolving agent systems remains largely unexplored. In such settings, the direct application of standard FL is particularly challenging, as heterogeneous tasks and sparse, trajectory-level reward signals give rise to severe gradient instability, which undermines the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents that establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace, reducing communication cost across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by 10\\% over the state-of-the-art FedIT, validating its effectiveness in cross-environment knowledge transfer under privacy constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Fed-SEï¼Œä¸€ç§æ—¨åœ¨è§£å†³éšç§çº¦æŸä¸‹å¤šç¯å¢ƒå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ååŒè¿›åŒ–çš„è”é‚¦è‡ªè¿›åŒ–æ¡†æ¶ã€‚é’ˆå¯¹å¼‚æ„ä»»åŠ¡å’Œç¨€ç–è½¨è¿¹å¥–åŠ±å¯¼è‡´çš„æ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ï¼ŒFed-SEå»ºç«‹äº†æœ¬åœ°è¿›åŒ–ä¸å…¨å±€èšåˆçš„èŒƒå¼ã€‚åœ¨æœ¬åœ°ç«¯ï¼Œæ™ºèƒ½ä½“é€šè¿‡å¯¹ç­›é€‰å‡ºçš„é«˜å›æŠ¥è½¨è¿¹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ(Parameter-efficient fine-tuning)æ¥ç¡®ä¿æ¢¯åº¦æ›´æ–°çš„ç¨³å®šæ€§ï¼›åœ¨å…¨å±€ç«¯ï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨ä½ç§©å­ç©ºé—´(low-rank subspace)å†…èšåˆæ›´æ–°ï¼Œæœ‰æ•ˆé™ä½äº†å®¢æˆ·ç«¯é—´çš„é€šä¿¡æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFed-SEåœ¨äº”ç§å¼‚æ„ç¯å¢ƒä¸­çš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡æ¯”ç°æœ‰æœ€å…ˆè¿›çš„FedITæå‡äº†10%ã€‚è¯¥ç ”ç©¶éªŒè¯äº†å…¶åœ¨éšç§å—é™æ¡ä»¶ä¸‹è¿›è¡Œè·¨ç¯å¢ƒçŸ¥è¯†è¿ç§»çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€æ”¾å¼è‡ªè¿›åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†å¯é çš„ä¼˜åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08870v2",
      "published_date": "2025-12-09 18:04:41 UTC",
      "updated_date": "2026-01-11 12:46:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:43.058398+00:00"
    },
    {
      "arxiv_id": "2512.08869v1",
      "title": "Differentially Private Synthetic Data Generation Using Context-Aware GANs",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å·®åˆ†éšç§åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Anantaa Kotal",
        "Anupam Joshi"
      ],
      "abstract": "The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿåˆæˆæ•°æ®æ–¹æ³•éš¾ä»¥æ•æ‰åŒ»ç–—ç­‰é¢†åŸŸä¸­å¤æ‚éšå¼è§„åˆ™å’Œé¢†åŸŸçº¦æŸ(Domain-Specific Constraints)çš„é—®é¢˜ï¼Œæå‡ºäº† ContextGANï¼Œä¸€ç§æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„å·®åˆ†éšç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(Context-Aware Differentially Private Generative Adversarial Network)ã€‚è¯¥æ¡†æ¶é€šè¿‡çº¦æŸçŸ©é˜µ(Constraint Matrix)å¯¹æ˜¾å¼å’Œéšå¼çŸ¥è¯†è¿›è¡Œç¼–ç ï¼Œå¹¶åˆ©ç”¨çº¦æŸæ„ŸçŸ¥åˆ¤åˆ«å™¨(Constraint-Aware Discriminator)ç¡®ä¿ç”Ÿæˆçš„åˆæˆæ•°æ®ä¸¥æ ¼éµå®ˆé¢†åŸŸç‰¹å®šçš„è§„åˆ™ã€‚åŒæ—¶ï¼Œæ¨¡å‹é›†æˆäº†å·®åˆ†éšç§(Differential Privacy)æŠ€æœ¯ï¼Œåœ¨ç”Ÿæˆé«˜è´¨é‡æ•°æ®çš„åŒæ—¶æœ‰æ•ˆé˜²æ­¢åŸå§‹æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ã€‚åœ¨åŒ»ç–—ã€å®‰å…¨å’Œé‡‘èé¢†åŸŸçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒContextGAN äº§å‡ºçš„åˆæˆæ•°æ®åœ¨ä¿æŒé«˜åº¦çœŸå®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ•°æ®åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç”¨ã€‚è¯¥æ–¹æ³•ä¸ºéœ€è¦åœ¨ä¸¥æ ¼éšç§ä¿æŠ¤ä¸‹å…¼é¡¾å¤æ‚é€»è¾‘ä¸€è‡´æ€§çš„åº”ç”¨åœºæ™¯æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08869v1",
      "published_date": "2025-12-09 18:02:34 UTC",
      "updated_date": "2025-12-09 18:02:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:42.822898+00:00"
    },
    {
      "arxiv_id": "2512.08868v2",
      "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
      "title_zh": "EcomBenchï¼šç”µå­å•†åŠ¡é¢†åŸŸåŸºç¡€æ™ºèƒ½ä½“çš„å…¨é¢è¯„ä¼°",
      "authors": [
        "Rui Min",
        "Zile Qiao",
        "Ze Xu",
        "Jiawen Zhai",
        "Wenyu Gao",
        "Xuanzhong Chen",
        "Haozhen Sun",
        "Zhen Zhang",
        "Xinyu Wang",
        "Hong Zhou",
        "Wenbiao Yin",
        "Bo Zhang",
        "Xuan Zhou",
        "Ming Yan",
        "Yong Jiang",
        "Haicheng Liu",
        "Liang Ding",
        "Ling Zou",
        "Yi R. Fung",
        "Yalong Li",
        "Pengjun Xie"
      ],
      "abstract": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EcomBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å…¨é¢è¯„ä¼°åŸºç¡€æ™ºèƒ½ä½“ (Foundation Agents) åœ¨çœŸå®ç”µå­å•†åŠ¡ç¯å¢ƒä¸­è¡¨ç°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†å¤šå±€é™äºå­¦æœ¯è®¾å®šæˆ–äººå·¥åœºæ™¯ã€å¿½è§†çœŸå®åº”ç”¨æŒ‘æˆ˜çš„é—®é¢˜ï¼ŒEcomBench èšç„¦äºåŒ…å«å¤æ‚ç”¨æˆ·äº¤äº’ã€åŠ¨æ€å¸‚åœºæ¡ä»¶å’ŒçœŸå®å†³ç­–è¿‡ç¨‹çš„ç”µå•†é¢†åŸŸã€‚è¯¥åŸºå‡†åŸºäºå…¨çƒé¢†å…ˆç”µå•†ç”Ÿæ€ç³»ç»Ÿä¸­çš„çœŸå®ç”¨æˆ·éœ€æ±‚æ„å»ºï¼Œå¹¶ç»è¿‡äººç±»ä¸“å®¶ä¸¥æ ¼ç­›é€‰ä¸æ ‡æ³¨ï¼Œç¡®ä¿äº†å…¶é¢†åŸŸç›¸å…³æ€§å’Œå‡†ç¡®æ€§ã€‚EcomBench æ¶µç›–äº†å¤šä¸ªç”µå•†ä»»åŠ¡ç±»åˆ«å¹¶è®¾ç½®äº†ä¸‰ä¸ªéš¾åº¦ç­‰çº§ï¼Œé‡ç‚¹è¯„ä¼°æ™ºèƒ½ä½“åœ¨æ·±åº¦ä¿¡æ¯æ£€ç´¢ (Information Retrieval)ã€å¤šæ­¥æ¨ç† (Multi-step Reasoning) å’Œè·¨æºçŸ¥è¯†æ•´åˆ (Cross-source Knowledge Integration) ç­‰æ–¹é¢çš„å…³é”®èƒ½åŠ›ã€‚é€šè¿‡æ¤æ ¹äºçœŸå®çš„ç”µå•†è¯­å¢ƒï¼ŒEcomBench ä¸ºè¡¡é‡æ™ºèƒ½ä½“åœ¨ç°ä»£ç”µå•†é¢†åŸŸçš„å®é™…åº”ç”¨èƒ½åŠ›æä¾›äº†ä¸€ä¸ªä¸¥è°¨ä¸”åŠ¨æ€çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08868v2",
      "published_date": "2025-12-09 18:00:26 UTC",
      "updated_date": "2025-12-11 16:38:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:48.634216+00:00"
    },
    {
      "arxiv_id": "2512.23717v1",
      "title": "HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate",
      "title_zh": "HarmTransformï¼šåŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºçš„æ˜¾æ€§æœ‰å®³æŸ¥è¯¢éšè”½åŒ–è½¬åŒ–",
      "authors": [
        "Shenzhe Zhu"
      ],
      "abstract": "Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HarmTransformï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†æ˜¾æ€§æœ‰å®³æŸ¥è¯¢è½¬åŒ–ä¸ºéšè”½å½¢å¼çš„å¤šæ™ºèƒ½ä½“è¾©è®ºæ¡†æ¶ï¼Œä»¥åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¯†åˆ«å’Œæ‹¦æˆªéšè”½æ¶æ„æ„å›¾æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„è¿­ä»£æ‰¹åˆ¤ä¸ä¼˜åŒ–ï¼Œç³»ç»Ÿæ€§åœ°ç”Ÿæˆä¿æŒåŸå§‹æœ‰å®³æ„å›¾ä¸”æ›´å…·éšè”½æ€§çš„æŸ¥è¯¢è½¬åŒ–ï¼Œä»è€Œå¡«è¡¥ç°æœ‰å®‰å…¨è®­ç»ƒæ•°æ®çš„ç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHarmTransformåœ¨ç”Ÿæˆæœ‰æ•ˆçš„æŸ¥è¯¢è½¬åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºæ ‡å‡†åŸºå‡†æ¨¡å‹ã€‚æ·±å…¥åˆ†ææ­ç¤ºäº†è¾©è®ºæœºåˆ¶åœ¨æå‡éšè”½æ€§çš„åŒæ—¶ï¼Œä¹Ÿå¯èƒ½å¼•å…¥è¯é¢˜åç§»å’Œå¤æ‚æ€§ï¼Œè¡¨ç°å‡ºâ€œåŒåˆƒå‰‘â€çš„ç‰¹æ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…å±•ç¤ºäº†åˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œç”Ÿæˆé«˜è´¨é‡å®‰å…¨è®­ç»ƒæ•°æ®çš„æ½œåŠ›ï¼Œä¹Ÿä¸ºæœªæ¥æå‡LLMsçš„å®‰å…¨å¯¹é½(Safety Alignment)æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒå’Œæ´å¯Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23717v1",
      "published_date": "2025-12-09 17:56:38 UTC",
      "updated_date": "2025-12-09 17:56:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:00:47.682493+00:00"
    },
    {
      "arxiv_id": "2512.08833v1",
      "title": "Interpolation in Knowledge Representation",
      "title_zh": "çŸ¥è¯†è¡¨ç¤ºä¸­çš„æ’å€¼",
      "authors": [
        "Jean Christoph Jung",
        "Patrick Koopmann",
        "Matthias Knorr"
      ],
      "abstract": "Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çŸ¥è¯†è¡¨ç¤º(Knowledge Representation)é¢†åŸŸä¸­å…‹é›·æ ¼æ’å€¼(Craig interpolation)å’Œå‡åŒ€æ’å€¼(uniform interpolation)çš„åº”ç”¨åŠå…¶é‡è¦æ€§ã€‚è¿™äº›æ’å€¼æŠ€æœ¯åœ¨å¯è§£é‡Šæ€§(explainability)ã€é—å¿˜(forgetting)ã€æ¨¡å—åŒ–ä¸é‡ç”¨(modularization and reuse)ä»¥åŠå­¦ä¹ (learning)ç­‰å¤šä¸ªå…³é”®é¢†åŸŸå±•ç°å‡ºå¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚ç„¶è€Œï¼Œç”±äºè®¸å¤šçŸ¥è¯†è¡¨ç¤ºå½¢å¼ä½“ç³»åœ¨é€šç”¨æƒ…å†µä¸‹å¹¶ä¸å…·å¤‡å…‹é›·æ ¼æˆ–å‡åŒ€æ’å€¼å±æ€§ï¼Œå®é™…è®¡ç®—æ’å€¼é¡¹(interpolants)ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†æè¿°é€»è¾‘(description logics)å’Œé€»è¾‘ç¼–ç¨‹(logic programming)è¿™ä¸¤ç±»æ ¸å¿ƒå½¢å¼ä½“ç³»ï¼Œå¹¶ç³»ç»Ÿåœ°è®¨è®ºäº†ç”¨äºè®¡ç®—æ’å€¼é¡¹çš„æœ€æ–°ç†è®ºæˆæœä¸å®ç”¨æ–¹æ³•ï¼Œä¸ºå¤„ç†å¤æ‚çš„çŸ¥è¯†è¡¨ç¤ºä»»åŠ¡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "The article will appear in Balder ten Cate, Jean Christoph Jung, Patrick Koopmann, Christoph Wernhard and Frank Wolter, editors. Theory and Applications of Craig Interpolation. Ubiquity Press, 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.08833v1",
      "published_date": "2025-12-09 17:21:30 UTC",
      "updated_date": "2025-12-09 17:21:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:07.948676+00:00"
    },
    {
      "arxiv_id": "2512.08829v1",
      "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
      "title_zh": "InfiniteVLï¼šååŒçº¿æ€§å’Œç¨€ç–æ³¨æ„åŠ›çš„é«˜æ•ˆæ— é™è¾“å…¥è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hongyuan Tao",
        "Bencheng Liao",
        "Shaoyu Chen",
        "Haoran Yin",
        "Qian Zhang",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InfiniteVLï¼Œä¸€ç§å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLM)æ¶æ„ï¼Œæ—¨åœ¨è§£å†³çª—å£æ³¨æ„åŠ›(Window Attention)åœ¨é•¿åºåˆ—ä¸‹çš„æ€§èƒ½ä¸‹é™ä»¥åŠçº¿æ€§æ³¨æ„åŠ›(Linear Attention)åœ¨ä¿¡æ¯å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ¶æ„é€šè¿‡ååŒæ»‘åŠ¨çª—å£æ³¨æ„åŠ›(Sliding Window Attention)ä¸Gated DeltaNetï¼Œå®ç°äº†å¯¹æ— é™è¾“å…¥çš„é«˜æ•ˆå¤„ç†ã€‚ä¸ºåœ¨æœ‰é™èµ„æºä¸‹æå‡å¤šæ¨¡æ€æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†åŒ…å«è’¸é¦é¢„è®­ç»ƒ(Distillation Pretraining)ã€æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)å’Œé•¿åºåˆ—SFTçš„ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInfiniteVLä»…éœ€ä¸åˆ°2%çš„è®­ç»ƒæ•°æ®å³å¯æ˜¾è‘—è¶…è¶Šä»¥å¾€çº¿æ€§å¤æ‚åº¦æ¨¡å‹ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šä¸é¡¶å°–çš„Transformer-based VLMsæŒå¹³ã€‚ä¸é‡‡ç”¨FlashAttention-2åŠ é€Ÿçš„åŒç±»Transformeræ¨¡å‹ç›¸æ¯”ï¼ŒInfiniteVLå®ç°äº†è¶…è¿‡3.6å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸”èƒ½ä¿æŒæ’å®šçš„å»¶è¿Ÿä¸å†…å­˜å ç”¨ã€‚åœ¨æµå¼è§†é¢‘ç†è§£åœºæ™¯ä¸­ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç»´æŒ24 FPSçš„å®æ—¶é¢„å¡«é€Ÿåº¦å¹¶ä¿ç•™é•¿æœŸè®°å¿†ï¼Œä¸ºé«˜æ•ˆå¤„ç†å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 8 figures, conference or other essential info",
      "pdf_url": "https://arxiv.org/pdf/2512.08829v1",
      "published_date": "2025-12-09 17:18:32 UTC",
      "updated_date": "2025-12-09 17:18:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:10.559057+00:00"
    },
    {
      "arxiv_id": "2512.08826v1",
      "title": "CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale",
      "title_zh": "CARLoSï¼šåŸºäºå¤§è§„æ¨¡ LoRA ç®€æ˜è¯„ä¼°è¡¨å¾çš„æ£€ç´¢æ¡†æ¶",
      "authors": [
        "Shahar Sarfaty",
        "Adi Haviv",
        "Uri Hacohen",
        "Niva Elkin-Koren",
        "Roi Livni",
        "Amit H. Bermano"
      ],
      "abstract": "The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CARLoSï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€é¢å¤–å…ƒæ•°æ®å³å¯å¯¹å¤§è§„æ¨¡LoRAsè¿›è¡Œç‰¹å¾æè¿°çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€ç´¢æ–¹æ³•ä¾èµ–ä¸å¯é æè¿°æˆ–åå·®æŒ‡æ ‡çš„é—®é¢˜ã€‚é€šè¿‡åˆ†æè¶…è¿‡650ä¸ªLoRAåœ¨å¤šæ ·åŒ–æç¤ºè¯å’Œç§å­ä¸‹çš„å›¾åƒç”Ÿæˆè¡¨ç°ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨CLIP embeddingsåŠå…¶ä¸åŸºç¡€æ¨¡å‹çš„å·®å¼‚ï¼Œå®šä¹‰äº†ç”±Directionsï¼ˆè¯­ä¹‰åç§»ï¼‰ã€Strengthï¼ˆå½±å“å¼ºåº¦ï¼‰å’ŒConsistencyï¼ˆç¨³å®šæ€§ï¼‰æ„æˆçš„ä¸‰éƒ¨åˆ†è¡¨å¾ã€‚åŸºäºæ­¤è¡¨å¾å¼€å‘çš„æ£€ç´¢ç³»ç»Ÿèƒ½å°†æ–‡æœ¬æŸ¥è¯¢ä¸ç›¸å…³LoRAç²¾å‡†åŒ¹é…ï¼Œå¹¶æœ‰æ•ˆè¿‡æ»¤æ•ˆæœè¿‡å¼ºæˆ–ä¸ç¨³å®šçš„æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨åŒ–ä¸äººå·¥è¯„ä¼°ä¸­å‡ä¼˜äºä¼ ç»Ÿæ–‡æœ¬åŸºå‡†ã€‚æ­¤å¤–ï¼ŒCARLoSçš„è¡¨å¾æ–¹æ³•è¿˜æ”¯æŒå°†æ¨¡å‹ç‰¹å¾ä¸ç‰ˆæƒç›¸å…³çš„æ³•å¾‹æ¦‚å¿µï¼ˆå¦‚å®è´¨æ€§å’Œæ„å¿—ï¼‰è”ç³»èµ·æ¥ï¼Œä¸ºLoRAåˆ†ææä¾›äº†æ›´å¹¿æ³›çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper Page: https://shahar-sarfaty.github.io/CARLoS/",
      "pdf_url": "https://arxiv.org/pdf/2512.08826v1",
      "published_date": "2025-12-09 17:15:32 UTC",
      "updated_date": "2025-12-09 17:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:13.029547+00:00"
    },
    {
      "arxiv_id": "2512.08820v1",
      "title": "Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning",
      "title_zh": "å…è®­ç»ƒåŒåŒæ›²é€‚é…å™¨ï¼šå®ç°æ›´ä¼˜çš„è·¨æ¨¡æ€æ¨ç†",
      "authors": [
        "Yi Zhang",
        "Chun-Wun Cheng",
        "Junyi He",
        "Ke Yu",
        "Yushun Tang",
        "Carola-Bibiane SchÃ¶nlieb",
        "Zhihai He",
        "Angelica I. Aviles-Rivero"
      ],
      "abstract": "Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \\textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the PoincarÃ© ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) åœ¨é¢†åŸŸè¿ç§»æ—¶æ€§èƒ½ä¸‹é™åŠå¾®è°ƒæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Training-free Dual Hyperbolic Adapters (T-DHA) çš„æ— éœ€è®­ç»ƒçš„é€‚é…æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ‘’å¼ƒäº†ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ (Euclidean space)ï¼Œè½¬è€Œåœ¨åŒæ›²ç©ºé—´ (Hyperbolic space) ä¸­åˆ©ç”¨åºåŠ è±çƒæ¨¡å‹ (PoincarÃ© ball model) æ¥è¡¨å¾è¯­ä¹‰æ¦‚å¿µé—´å›ºæœ‰çš„å±‚çº§æ ‘çŠ¶ç»“æ„ã€‚åˆ©ç”¨åŒæ›²ç©ºé—´éšåŠå¾„æŒ‡æ•°çº§å¢é•¿çš„ç‹¬ç‰¹ä½“ç§¯ç‰¹æ€§ï¼ŒT-DHA èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åµŒå…¥å±‚çº§æ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹çš„è¡¨å¾å’Œåˆ¤åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è´Ÿå‘å­¦ä¹  (negative learning) æŠ€æœ¯ï¼Œåœ¨æ›´ä½çš„ç‰¹å¾ç»´åº¦ä¸‹å®ç°äº†æ›´ç²¾ç¡®ã€æ›´ç¨³å¥çš„åˆ†ç±»ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒT-DHA åœ¨å°‘æ ·æœ¬å›¾åƒè¯†åˆ« (few-shot image recognition) å’Œé¢†åŸŸæ³›åŒ– (domain generalization) ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ SOTA æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in IEEE Transactions on Multimedia (TMM)",
      "pdf_url": "https://arxiv.org/pdf/2512.08820v1",
      "published_date": "2025-12-09 17:12:22 UTC",
      "updated_date": "2025-12-09 17:12:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:10.983198+00:00"
    },
    {
      "arxiv_id": "2512.08819v1",
      "title": "Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis",
      "title_zh": "æ·±åº¦ç”Ÿé•¿æ¨¡å‹èƒ½å¦å…‹æœâ€œæ·±åº¦è¯…å’’â€ï¼Ÿä¸€é¡¹æ·±åº¦åˆ†æ",
      "authors": [
        "Ferdinand Kapl",
        "Emmanouil Angelis",
        "Tobias HÃ¶ppe",
        "Kaitlin Maile",
        "Johannes von Oswald",
        "Nino Scherrer",
        "Stefan Bauer"
      ],
      "abstract": "Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, CsordÃ¡s et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†åœ¨è®­ç»ƒä¸­é€æ¸å¢åŠ  Transformer æ·±åº¦çš„æ¨¡å‹ï¼ˆå¦‚ MIDASï¼‰å¦‚ä½•å…‹æœ Curse of Depth è¿™ä¸€æœºåˆ¶æ€§éš¾é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„ pre-layernorm Transformers å¾€å¾€å­˜åœ¨ååŠéƒ¨åˆ†å±‚è´¡çŒ®åº¦è¾ƒä½çš„é—®é¢˜ï¼Œè€Œé€šè¿‡ gradual middle stacking çš„æ·±åº¦å¢é•¿æ–¹å¼ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹å¯¹æ·±åº¦çš„åˆ©ç”¨æ•ˆç‡ã€‚è¿™ç§å¢é•¿æ¨¡å¼ä¸ä»…æ”¹å˜äº† residual stream çš„ç»“æ„ï¼Œè¿˜ä¿ƒè¿›äº† permutable computational blocks çš„å½¢æˆåŠç‹¬ç‰¹ computational circuits çš„å»ºç«‹ã€‚æ­¤å¤–ï¼Œä½œè€…å¯¹ MIDAS æå‡ºäº†è½»é‡çº§æ”¹è¿›æ–¹æ¡ˆï¼Œåœ¨ä¸‹æ¸¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´ä¼˜å¼‚çš„æˆç»©ã€‚è¯¥å·¥ä½œè¯æ˜äº†æ¸è¿›å¼æ·±åº¦å¢é•¿èƒ½æœ‰æ•ˆä¼˜åŒ–æ¨¡å‹å†…éƒ¨è®¡ç®—é€»è¾‘ï¼Œè§£å†³äº†éå¢é•¿æ¨¡å‹ä¸­æ·±åº¦åˆ©ç”¨ä¸è¶³çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08819v1",
      "published_date": "2025-12-09 17:12:04 UTC",
      "updated_date": "2025-12-09 17:12:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:17.047109+00:00"
    },
    {
      "arxiv_id": "2512.08812v1",
      "title": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation",
      "title_zh": "Emovectorsï¼šè¯„ä¼°çˆµå£«ä¹å³å…´æ¼”å¥ä¸­çš„æƒ…æ„Ÿå†…å®¹ä»¥è¿›è¡Œåˆ›é€ åŠ›è¯„ä»·",
      "authors": [
        "Anna Jordanous"
      ],
      "abstract": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çˆµå£«ä¹å³å…´æ¼”å¥(Jazz improvisation)ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ä¸åˆ›é€ åŠ›è¯„ä¼°ä¹‹é—´çš„å…³ç³»ï¼Œæ—¨åœ¨ä¸ºå½“å‰åŸºäºLLMçš„ç”Ÿæˆç³»ç»Ÿå¼€å‘è‡ªåŠ¨åŒ–çš„åˆ›é€ åŠ›è¯„ä»·æŒ‡æ ‡ã€‚ä½œè€…æå‡ºäº†ä¸€é¡¹æ ¸å¿ƒå‡è®¾ï¼Œå³å³å…´æ¼”å¥ä¸­åŒ…å«çš„æƒ…æ„Ÿè´Ÿè½½å†…å®¹è¶Šå¤šï¼Œå…¶è¢«è§†ä¸ºå…·æœ‰åˆ›é€ åŠ›çš„å¯èƒ½æ€§å°±è¶Šé«˜ã€‚ä¸ºéªŒè¯è¿™ä¸€å‡è®¾ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºEmovectorsçš„åŸºäºåµŒå…¥(Embeddings-based)çš„æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºå¿ƒç†å­¦åˆ†ç±»çš„éŸ³ä¹ç‰¹å¾æ¥æ•æ‰å³å…´æ¼”å¥ä¸­çš„æƒ…æ„Ÿå†…å®¹ã€‚é€šè¿‡å¯¹å¤šä¸ªå³å…´æ¼”å¥æ ·æœ¬çš„Emovectorsè¿›è¡Œå¯¹æ¯”åˆ†æï¼Œç ”ç©¶æ¢è®¨äº†æƒ…æ„Ÿå‚ä¸åº¦åœ¨å¯é‡åŒ–æŒ‡æ ‡ä¸­çš„ä½“ç°ã€‚è¿™ç§å‘é‡åŒ–æ–¹æ³•ä¸ºå¤§è§„æ¨¡çš„åˆ›é€ åŠ›è¯„ä¼°(Creativity evaluation)æä¾›äº†æ–°çš„åº¦é‡æ ‡å‡†ï¼Œä¸ºç†è§£éŸ³ä¹åˆ›ä½œè¿‡ç¨‹ä¸­çš„æƒ…æ„Ÿä»·å€¼è´¡çŒ®äº†ç§‘å­¦çš„è¯„ä»·å·¥å…·ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Presented at IEEE Big Data 2025 3rd Workshop on AI Music Generation (AIMG 2025). https://www.intellisky.org/workshops/AIMG2025/workshop_AIMG2025.html",
      "pdf_url": "https://arxiv.org/pdf/2512.08812v1",
      "published_date": "2025-12-09 17:05:36 UTC",
      "updated_date": "2025-12-09 17:05:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:20.908201+00:00"
    },
    {
      "arxiv_id": "2512.08810v1",
      "title": "Multicalibration for LLM-based Code Generation",
      "title_zh": "åŸºäº LLM ä»£ç ç”Ÿæˆçš„å¤šå…ƒæ ¡å‡†",
      "authors": [
        "Viola Campos",
        "Robin Kuschnereit",
        "Adrian Ulges"
      ],
      "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)ä»£ç ç”Ÿæˆçš„Multicalibrationé—®é¢˜ï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹çš„ç½®ä¿¡åº¦å¾—åˆ†èƒ½çœŸå®åæ˜ ä»£ç ç”Ÿæˆçš„æ­£ç¡®ç‡ã€‚ç ”ç©¶äººå‘˜æ·±å…¥åˆ†æäº†èƒ½å¤Ÿæ•æ‰ä»£ç å¤æ‚åº¦ã€ä»£ç é•¿åº¦å’Œç¼–ç¨‹è¯­è¨€ç­‰é¢å¤–å› ç´ çš„Multicalibrationæ–¹æ³•ã€‚é€šè¿‡åœ¨Qwen3 Coderã€GPT-OSSå’ŒDeepSeek-R1-Distillç­‰æ¨¡å‹åŠä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç ”ç©¶éªŒè¯äº†å››ç§æ ¡å‡†ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMulticalibrationç›¸æ¯”äºæœªæ ¡å‡†çš„Token Likelihoodsåœ¨Skill Scoreä¸Šæå‡äº†1.03ï¼Œç›¸æ¯”åŸºå‡†æ ¡å‡†æ–¹æ³•æå‡äº†0.37ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡æ¶ˆèå®éªŒåˆ†æäº†ä¸åŒå› ç´ å¯¹æ ¡å‡†æ•ˆæœçš„å½±å“ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å…¬å¼€äº†åŒ…å«ä»£ç ç”Ÿæˆå†…å®¹ã€ä¼¼ç„¶å€¼å’Œæ­£ç¡®æ€§æ ‡ç­¾çš„æ•°æ®é›†ï¼Œä¸ºåç»­ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ ¡å‡†ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond)",
      "pdf_url": "https://arxiv.org/pdf/2512.08810v1",
      "published_date": "2025-12-09 17:04:01 UTC",
      "updated_date": "2025-12-09 17:04:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:28.683822+00:00"
    },
    {
      "arxiv_id": "2512.08809v3",
      "title": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration",
      "title_zh": "PrivTuneï¼šåŸºäºç«¯äº‘ååŒçš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆéšç§ä¿æŠ¤å¾®è°ƒ",
      "authors": [
        "Yi Liu",
        "Weixiang Han",
        "Chengjun Cai",
        "Xingliang Yuan",
        "Cong Wang"
      ],
      "abstract": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_Ï‡$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PrivTuneï¼Œä¸€ç§é€šè¿‡è®¾å¤‡-äº‘åä½œ(Device-Cloud Collaboration)å®ç°çš„ã€åŸºäºæ‹†åˆ†å­¦ä¹ (Split Learning)çš„é«˜æ•ˆéšç§ä¿æŠ¤å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰çš„å·®åˆ†éšç§(Differential Privacy)æ–¹æ³•åœ¨ä¿æŠ¤ç”¨æˆ·éšç§ä¸ç»´æŒæ¨¡å‹æ•ˆç”¨ä¹‹é—´éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ï¼ŒPrivTuneé€šè¿‡åœ¨åº•éƒ¨æ¨¡å‹çš„tokenè¡¨ç¤ºä¸­æ³¨å…¥ç²¾å¿ƒè®¾è®¡çš„å™ªå£°ï¼Œä½¿æ¯ä¸ªtokenåœ¨è¡¨ç¤ºå±‚é¢ä¸Šä¸å…¶n-æ­¥é—´æ¥é‚»å±…(n-hop indirect neighbors)ç›¸ä¼¼ã€‚è¯¥æ¡†æ¶å°†å™ªå£°ç”Ÿæˆå»ºæ¨¡ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨è®¡ç®—å‡ºç¬¦åˆé˜²å¾¡ä¸æ•ˆç”¨ç›®æ ‡çš„æœ€ä½³å™ªå£°å‘é‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒPrivTuneé€šè¿‡è°ƒæ•´$d_Ï‡$-Privacyå™ªå£°åˆ†å¸ƒçš„å‚æ•°å¹¶æ ¹æ®tokené‡è¦æ€§ç¼©æ”¾å™ªå£°ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘å¯¹åŸå§‹æ•°æ®çš„æ‰­æ›²ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šé’ˆå¯¹åµŒå…¥åæ¼”(Embedding Inversion)å’Œå±æ€§æ¨æ–­(Attribute Inference)æ”»å‡»çš„å®éªŒè¡¨æ˜ï¼ŒPrivTuneåœ¨RoBERTaæ¨¡å‹ä¸Šèƒ½å°†æ”»å‡»æˆåŠŸç‡é™ä½è‡³10%ï¼Œè€Œæ•ˆç”¨æŸå¤±ä»…ä¸º3.33%ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at IEEE INFOCOM 2026 (full version). Update the cited references",
      "pdf_url": "https://arxiv.org/pdf/2512.08809v3",
      "published_date": "2025-12-09 17:03:59 UTC",
      "updated_date": "2026-01-21 12:46:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:25.679611+00:00"
    },
    {
      "arxiv_id": "2512.14727v1",
      "title": "A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications",
      "title_zh": "åŒ»ç–—åº”ç”¨ä¸­æœ‰é™æ ·æœ¬ç¬¦åˆé¢„æµ‹ç†è®ºçš„æ‰¹åˆ¤æ€§å®¡è§†",
      "authors": [
        "Klaus-Rudolf Kladny",
        "Bernhard SchÃ¶lkopf",
        "Lisa Koch",
        "Christian F. Baumgartner",
        "Michael Muehlebach"
      ],
      "abstract": "Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—åº”ç”¨ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹ç¼ºä¹å¯é ä¸ç¡®å®šæ€§ä¼°è®¡çš„é—®é¢˜ï¼Œå¯¹ç¬¦åˆæ€§é¢„æµ‹ (Conformal Prediction) ç†è®ºåœ¨æœ‰é™æ ·æœ¬ä¸‹çš„è¡¨ç°æå‡ºäº†æ‰¹åˆ¤æ€§è§‚ç‚¹ã€‚Conformal Prediction é€šå¸¸è¢«è®¤ä¸ºèƒ½åœ¨æ ¡å‡†æ ·æœ¬ (calibration sample) æ•°é‡ä»»æ„çš„æƒ…å†µä¸‹æä¾›ç»Ÿè®¡ä¿è¯ï¼Œä»è€Œå°†å¯å‘å¼ä¼°è®¡è½¬åŒ–ä¸ºåŒ…å«çœŸå®æ ‡ç­¾çš„é¢„æµ‹é›†ã€‚ç„¶è€Œï¼Œä½œè€…æŒ‡å‡ºè™½ç„¶ç»Ÿè®¡ä¿è¯åœ¨ç†è®ºä¸Šå¯¹ä»»æ„è§„æ¨¡çš„æ ¡å‡†é›†éƒ½æˆç«‹ï¼Œä½†è¿™äº›ä¿è¯çš„å®é™…æ•ˆç”¨ (practical utility) å®é™…ä¸Šé«˜åº¦ä¾èµ–äºæ ¡å‡†é›†çš„å¤§å°ã€‚è¿™ä¸€å‘ç°å¯¹äºæ•°æ®ç¨€ç¼ºä¸”éš¾ä»¥è·å–å¤§è§„æ¨¡æ ¡å‡†é›†çš„åŒ»å­¦é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®è¯ç ”ç©¶ï¼Œä½œè€…è¯å®äº†åœ¨æ ·æœ¬é‡å—é™æ—¶ Conformal Prediction ç†è®ºæ‰¿è¯ºä¸å®é™…åº”ç”¨ä»·å€¼ä¹‹é—´çš„å·®è·ï¼Œå¼ºè°ƒäº†åœ¨ä¸´åºŠå†³ç­–ä¸­å®¡æ…è¯„ä¼°è¯¥æŠ€æœ¯å¯é æ€§çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14727v1",
      "published_date": "2025-12-09 16:59:31 UTC",
      "updated_date": "2025-12-09 16:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:28.449502+00:00"
    },
    {
      "arxiv_id": "2512.08802v1",
      "title": "Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework",
      "title_zh": "ä¼ä¸šå®‰å…¨é¢†åŸŸæœºå™¨å­¦ä¹ çš„æ™®åŠåŒ–ï¼šä¸€ç§è‡ªæŒç»­çš„æ”»å‡»æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Sadegh Momeni",
        "Ge Zhang",
        "Birkett Huber",
        "Hamza Harkous",
        "Sam Lipton",
        "Benoit Seguin",
        "Yanis Pavlidis"
      ],
      "abstract": "Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.\n  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªæŒç»­çš„æ”»å‡»æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ democratizing ML è§£å†³ä¼ä¸šå®‰å…¨è¿è¥ä¸­å¿ƒï¼ˆSOCï¼‰ä¸­ä¼ ç»Ÿ rule-based æ£€æµ‹æ–¹æ³•åƒµåŒ–ä¸”ç»´æŠ¤æˆæœ¬é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ··åˆæ¨¡å¼ï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ•…æ„æ”¾å®½çš„ YARA rules è¿›è¡Œç²—ç²’åº¦è¿‡æ»¤ä»¥ç¡®ä¿é«˜å¬å›ç‡ï¼Œç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨ ML classifier æ»¤é™¤å…¶ä¸­çš„ false positivesã€‚ä¸ºäº†å…‹æœæ•°æ®åŒ®ä¹éš¾é¢˜ï¼Œç³»ç»Ÿé›†æˆäº†åä¸º Simula çš„æ— ç§å­åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œä½¿å®‰å…¨åˆ†æå¸ˆæ— éœ€æ·±åšçš„æœºå™¨å­¦ä¹ èƒŒæ™¯æˆ–é¢„æ ‡æ³¨æ ·æœ¬å³å¯åˆ›å»ºé«˜è´¨é‡è®­ç»ƒé›†ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡æŒç»­åé¦ˆå›è·¯å¼•å…¥ active learning æœºåˆ¶ï¼Œåˆ©ç”¨å®æ—¶è°ƒæŸ¥ç»“æœè‡ªé€‚åº”å¾®è°ƒæ¨¡å‹ï¼Œæœ‰æ•ˆé˜²æ­¢äº†æ£€æµ‹æ€§èƒ½çš„ rule degradationã€‚åœ¨å¤„ç†æ¯æ—¥é«˜è¾¾ 2500 äº¿æ¡åŸå§‹æ—¥å¿—çš„å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¯¥ç³»ç»ŸæˆåŠŸå°†æµ·é‡æ•°æ®å‹ç¼©è‡³æ¯æ—¥ä»…éœ€äººå·¥å¤„ç†çš„æå°‘é‡å·¥å•ã€‚é•¿æœŸå®æµ‹è¡¨æ˜ï¼Œactive learning ä½¿æ¨¡å‹çš„ precision éšæ—¶é—´æ¨ç§»ç¨³æ­¥æå‡ï¼Œä¸ºå®‰å…¨ä¸“å®¶æä¾›äº†ä¸€ç§ä½å¼€é”€ã€ä½ç»´æŠ¤ä¸”å¯ç”±ä¸“å®¶å¼•å¯¼çš„è‡ªæŒç»­æ£€æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "published in CAMLIS 2025, https://www.camlis.org/",
      "pdf_url": "https://arxiv.org/pdf/2512.08802v1",
      "published_date": "2025-12-09 16:58:08 UTC",
      "updated_date": "2025-12-09 16:58:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:01:34.228933+00:00"
    },
    {
      "arxiv_id": "2512.09953v1",
      "title": "ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs",
      "title_zh": "ZK-APEXï¼šåŸºäºå¯æ‰§è¡Œè¯æ˜çš„é›¶çŸ¥è¯†è¿‘ä¼¼ä¸ªæ€§åŒ–é—å¿˜å­¦ä¹ ",
      "authors": [
        "Mohammad M Maheri",
        "Sunil Cotterill",
        "Alex Davidson",
        "Hamed Haddadi"
      ],
      "abstract": "Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.\n  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.\n  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ZK-APEXï¼Œä¸€ç§é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡çš„å¯éªŒè¯ä¸ªæ€§åŒ–é—å¿˜å­¦ä¹ ï¼ˆPersonalized Unlearningï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®¢æˆ·ç«¯åœ¨å¤„ç†æ•°æ®åˆ é™¤è¯·æ±‚æ—¶éš¾ä»¥è¢«éªŒè¯ä¸”éšç§å—é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æœåŠ¡ç«¯ç¨€ç–æ©ç ä¸å®¢æˆ·ç«¯çš„Group OBSè¡¥å¿æ­¥éª¤ï¼Œåˆ©ç”¨åˆ†å—ç»éªŒFisherçŸ©é˜µè¿›è¡Œæ›²ç‡æ„ŸçŸ¥æ›´æ–°ï¼Œå®ç°äº†æ— éœ€é‡æ–°è®­ç»ƒçš„é›¶æ ·æœ¬é—å¿˜ã€‚é€šè¿‡é›†æˆHalo2é›¶çŸ¥è¯†è¯æ˜ï¼ˆZero-Knowledge Proofsï¼‰ï¼ŒZK-APEXå…è®¸æä¾›æ–¹åœ¨ä¸æ¥è§¦ç§æœ‰æ•°æ®æˆ–å‚æ•°çš„æƒ…å†µä¸‹ï¼ŒéªŒè¯é—å¿˜è½¬æ¢çš„æ­£ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Vision Transformerä»»åŠ¡ä¸­èƒ½æ¢å¤å‡ ä¹æ‰€æœ‰å‡†ç¡®ç‡ï¼Œå¹¶åœ¨OPT125Mæ¨¡å‹ä¸Šæ¢å¤çº¦70%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå…¶è¯æ˜ç”Ÿæˆé€Ÿåº¦æ¯”é‡æ–°è®­ç»ƒå¿«ä¸€åƒä¸‡å€ä»¥ä¸Šï¼Œä¸”å†…å­˜å ç”¨ä½äº1GBï¼Œä¸ºè¾¹ç¼˜ç¯å¢ƒä¸‹çš„éšç§ä¿æŠ¤å’Œåˆè§„æ€§éªŒè¯æä¾›äº†é¦–ä¸ªå®ç”¨çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09953v1",
      "published_date": "2025-12-09 16:52:26 UTC",
      "updated_date": "2025-12-09 16:52:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:31.876785+00:00"
    },
    {
      "arxiv_id": "2512.08798v1",
      "title": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?",
      "title_zh": "é€šè¿‡å›¾ç‰¹å¾è¡¨æ ¼åŒ–ï¼ŒTabPFN åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­èƒ½å¦ä¸ GNNs ç«äº‰ï¼Ÿ",
      "authors": [
        "Jeongwhan Choi",
        "Woosung Kang",
        "Minseo Kim",
        "Jongwoo Kim",
        "Noseong Park"
      ],
      "abstract": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.",
      "tldr_zh": "æœ¬ç ”ç©¶è°ƒæŸ¥äº†å›¾èŠ‚ç‚¹åˆ†ç±»(node classification)æ˜¯å¦å¯ä»¥è¢«æœ‰æ•ˆåœ°é‡æ–°è¡¨è¿°ä¸ºè¡¨æ ¼å­¦ä¹ é—®é¢˜ï¼Œå¹¶ä¸ºæ­¤å¼•å…¥äº† TabPFN-GN æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æå–èŠ‚ç‚¹å±æ€§(node attributes)ã€ç»“æ„å±æ€§(structural properties)ã€ä½ç½®ç¼–ç (positional encodings)ä»¥åŠå¯é€‰çš„å¹³æ»‘é‚»åŸŸç‰¹å¾(smoothed neighborhood features)ï¼Œå°†å›¾æ•°æ®è½¬åŒ–ä¸ºè¡¨æ ¼ç‰¹å¾ã€‚è¿™ä½¿å¾— TabPFN èƒ½å¤Ÿåœ¨æ— éœ€ä»»ä½•å›¾ç‰¹å®šè®­ç»ƒ(graph-specific training)æˆ–è¯­è¨€æ¨¡å‹(language model)ä¾èµ–çš„æƒ…å†µä¸‹æ‰§è¡Œç›´æ¥çš„èŠ‚ç‚¹åˆ†ç±»ã€‚åœ¨ 12 ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒæ­ç¤ºï¼ŒTabPFN-GN åœ¨åŒè´¨å›¾(homophilous graphs)ä¸Šè¾¾åˆ°äº†ä¸ GNNs ç›¸å½“çš„ç«äº‰æ€§èƒ½ï¼Œå¹¶åœ¨å¼‚è´¨å›¾(heterophilous graphs)ä¸ŠæŒç»­è¡¨ç°ä¼˜äºåè€…ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åŸåˆ™æ€§çš„ç‰¹å¾å·¥ç¨‹å¯ä»¥æœ‰æ•ˆå¼¥åˆè¡¨æ ¼ä¸å›¾é¢†åŸŸä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºä»»åŠ¡ç‰¹å®šçš„ GNN è®­ç»ƒå’Œä¾èµ–å¤§è¯­è¨€æ¨¡å‹(LLM)çš„å›¾åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ç§å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Rejected from LoG 2025 (submitted August 2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.08798v1",
      "published_date": "2025-12-09 16:51:30 UTC",
      "updated_date": "2025-12-09 16:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:40.632349+00:00"
    },
    {
      "arxiv_id": "2512.14726v1",
      "title": "Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning",
      "title_zh": "é‡å­å†³ç­– Transformer (QDT)ï¼šååŒçº ç¼ ä¸å¹²æ¶‰é©±åŠ¨çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Abraham Itzhak Weinberg"
      ],
      "abstract": "Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Quantum Decision Transformer (QDT)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸDecision Transformer (DT)åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)ä¸­é¢ä¸´çš„é•¿æ—¶ç¨‹ä¿¡ç”¨åˆ†é…(long-horizon credit assignment)å’Œå¤æ‚çŠ¶æ€åŠ¨ä½œä¾èµ–æŒ‘æˆ˜ã€‚QDTé›†æˆäº†å…·æœ‰çº ç¼ æ“ä½œ(entanglement operations)çš„é‡å­å¯å‘å¼æ³¨æ„åŠ›æœºåˆ¶(Quantum-Inspired Attention)ä»¥æ•æ‰éå±€éƒ¨ç‰¹å¾ç›¸å…³æ€§ï¼Œä»¥åŠåŒ…å«å¤šè·¯å¾„å¤„ç†å’Œå¯å­¦ä¹ å¹²æ¶‰(learnable interference)çš„é‡å­å‰é¦ˆç½‘ç»œ(Quantum Feedforward Networks)ã€‚åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡å®éªŒä¸­ï¼ŒQDTç›¸æ¯”æ ‡å‡†DTå®ç°äº†è¶…è¿‡2000%çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®è´¨é‡ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ¶ˆèç ”ç©¶æ­ç¤ºäº†ç»„ä»¶ä¹‹é—´çš„å¼ºååŒæ•ˆåº”(synergistic effects)ï¼Œè¯æ˜äº†é‡å­å¯å‘å¼æ¶æ„éœ€è¦æ•´ä½“ååŒè®¾è®¡è€Œéç®€å•çš„æ¨¡å—åŒ–é‡‡ç”¨ã€‚ç ”ç©¶ç¡®ç«‹äº†QDTåœ¨å¢å¼ºä¿¡ç”¨åˆ†é…ã€éšå¼é›†æˆè¡Œä¸º(implicit ensemble behavior)åŠè‡ªé€‚åº”èµ„æºåˆ†é…æ–¹é¢çš„è®¡ç®—ä¼˜åŠ¿ã€‚è¿™äº›å‘ç°ä¸ºåˆ©ç”¨é‡å­å¯å‘åŸåˆ™æ”¹è¿›åºåˆ—å†³ç­–ä¸­çš„Transformeræ¶æ„æä¾›äº†æ–°æ–¹å‘ï¼Œå…¶æ„ä¹‰å»¶ä¼¸è‡³æ›´å¹¿æ³›çš„ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14726v1",
      "published_date": "2025-12-09 16:47:37 UTC",
      "updated_date": "2025-12-09 16:47:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:43.677514+00:00"
    },
    {
      "arxiv_id": "2512.14725v1",
      "title": "Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion",
      "title_zh": "ç”Ÿæˆå¼åŸå¸‚æµåœºå»ºæ¨¡ï¼šåŸºäºå›¾æ‰©æ•£å®ç°ä»å‡ ä½•ç»“æ„åˆ°æ°”æµçš„æ¨¡æ‹Ÿ",
      "authors": [
        "Francisco Giral",
        "Ãlvaro Manzano",
        "Ignacio GÃ³mez",
        "Petros Koumoutsakos",
        "Soledad Le Clainche"
      ],
      "abstract": "Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºGraph Diffusionçš„ç”Ÿæˆå¼æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸå¸‚é£åœºæ¨¡æ‹Ÿä¸­å¤æ‚å‡ ä½•å½¢çŠ¶å¤„ç†éš¾ä»¥åŠé«˜ä¿çœŸåº¦è®¡ç®—æµä½“åŠ¨åŠ›å­¦(CFD)æ¨¡æ‹Ÿæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†å±‚æ¬¡åŒ–å›¾ç¥ç»ç½‘ç»œ(Hierarchical Graph Neural Network)ä¸åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹(Score-based Diffusion Modeling)ç›¸ç»“åˆï¼Œä»…éœ€å‡ ä½•ä¿¡æ¯å³å¯åœ¨éç»“æ„åŒ–ç½‘æ ¼(Unstructured Meshes)ä¸Šåˆæˆç¨³æ€åŸå¸‚é£åœºã€‚æ¨¡å‹é€šè¿‡åœ¨å¤šç§ç½‘æ ¼åˆ‡ç‰‡å’Œé£å‘è§’åº¦ä¸‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”å¤šæ ·åŒ–çš„é€Ÿåº¦åœºï¼Œä¸”æ— éœ€æ—¶é—´æ¼”åŒ–è®¡ç®—æˆ–å¯†é›†æµ‹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§çš„å‡ ä½•å½¢çŠ¶ï¼Œæœ‰æ•ˆæ¢å¤å°¾è¿¹(Wakes)å’Œå†å¾ªç¯åŒº(Recirculation Zones)ç­‰å…³é”®æµåœºç»“æ„ï¼Œå¹¶æä¾›å…·å¤‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥(Uncertainty-aware)çš„é¢„æµ‹ã€‚è¿™é¡¹å·¥ä½œæ˜¯æ„å»ºå»ºç­‘ç¯å¢ƒåŸºç¡€æ¨¡å‹(Foundation Models)çš„é‡è¦è¿›å±•ï¼Œèƒ½å¸®åŠ©åŸå¸‚è§„åˆ’è€…åœ¨æ°”å€™ä¸ç¡®å®šæ€§èƒŒæ™¯ä¸‹å¿«é€Ÿè¯„ä¼°è®¾è®¡å†³ç­–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14725v1",
      "published_date": "2025-12-09 16:44:58 UTC",
      "updated_date": "2025-12-09 16:44:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:48.736371+00:00"
    },
    {
      "arxiv_id": "2512.08789v1",
      "title": "MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance",
      "title_zh": "MatteViTï¼šåŸºäºé˜´å½±è’™ç‰ˆå¼•å¯¼çš„é«˜é¢‘æ„ŸçŸ¥æ–‡æ¡£å»é˜´å½±",
      "authors": [
        "Chaewon Kim",
        "Seoyeon Lee",
        "Jonghyuk Park"
      ],
      "abstract": "Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MatteViTï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºæ•°å­—åŒ–æ–‡æ¡£æ¸…æ™°åº¦çš„æ–‡æ¡£å»é˜´å½±æ¡†æ¶ã€‚ç”±äºé˜´å½±å¸¸ä¼šé®æŒ¡æˆ–æ‰­æ›²æ–‡æœ¬è¾¹ç¼˜å’Œçº¿æ¡ç­‰ç»†å¾®ç»“æ„ï¼Œè¯¥æ¡†æ¶ç»“åˆç©ºé—´å’Œé¢‘åŸŸä¿¡æ¯ï¼Œé‡ç‚¹ä¿ç•™é«˜é¢‘(high-frequency)ç»†èŠ‚ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒMatteViTå¼•å…¥äº†è½»é‡çº§çš„é«˜é¢‘æ”¾å¤§æ¨¡å—(HFAM)ï¼Œç”¨äºåˆ†è§£å¹¶è‡ªé€‚åº”æ”¾å¤§é«˜é¢‘æˆåˆ†ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºäº®åº¦çš„è¿ç»­é˜´å½±è’™ç‰ˆ(shadow matte)ï¼Œé€šè¿‡è‡ªå®šä¹‰æ•°æ®é›†å’Œç”Ÿæˆå™¨åœ¨å¤„ç†åˆæœŸæä¾›ç²¾ç¡®çš„ç©ºé—´å¼•å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMatteViTåœ¨RDDå’ŒKliglerç­‰å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†é¢†åŸŸé¢†å…ˆæ°´å¹³(SOTA)ï¼Œå…·æœ‰æå¼ºçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡å…‰å­¦å­—ç¬¦è¯†åˆ«(OCR)ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¸ºç°å®ä¸–ç•Œçš„æ–‡æ¡£ä¿®å¤æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 7 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08789v1",
      "published_date": "2025-12-09 16:40:10 UTC",
      "updated_date": "2025-12-09 16:40:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:44.808259+00:00"
    },
    {
      "arxiv_id": "2512.08786v2",
      "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¤šå…ƒå¯¹é½ä¸­è”é‚¦ RLHF åå¥½èšåˆçš„ç³»ç»Ÿæ€§è¯„ä¼°",
      "authors": [
        "Mahmoud Srewa",
        "Tianyu Zhao",
        "Salma Elmalaki"
      ],
      "abstract": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨Federated Learningç¯å¢ƒä¸‹å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸å¤šå…ƒäººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ï¼Œæ—¨åœ¨è§£å†³æ ‡å‡†æ–¹æ³•éš¾ä»¥å……åˆ†ä»£è¡¨ä¸åŒè§‚ç‚¹çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¡¡é‡ä¸åŒåå¥½èšåˆç­–ç•¥åœ¨å¯¹é½è´¨é‡ä¸å…¬å¹³æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†minã€maxå’Œaverageç­‰æ ‡å‡†å¥–åŠ±èšåˆæŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ ¹æ®ç¾¤ä½“å†å²å¯¹é½è¡¨ç°åŠ¨æ€è°ƒæ•´æƒé‡çš„è‡ªé€‚åº”æ–¹æ¡ˆã€‚åœ¨åŸºäºPPOçš„RLHFæµç¨‹å’Œé—®ç­”ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥è‡ªé€‚åº”æ–¹æ³•åœ¨ä¿æŒç«äº‰æ€§å¯¹é½åˆ†æ•°çš„åŒæ—¶ï¼Œä¸€è‡´æ€§åœ°å®ç°äº†æ›´ä¼˜çš„å…¬å¹³æ€§ã€‚è¯¥å·¥ä½œä¸ºè¯„ä¼°è·¨å¤šå…ƒäººç¾¤çš„æ¨¡å‹è¡Œä¸ºæä¾›äº†ç¨³å¥çš„æ–¹æ³•è®ºï¼Œå¹¶ä¸ºå¼€å‘çœŸæ­£å¤šå…ƒåŒ–ä¸”å…¬å¹³å¯¹é½çš„LLMsæä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is accepted at the NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle",
      "pdf_url": "https://arxiv.org/pdf/2512.08786v2",
      "published_date": "2025-12-09 16:39:32 UTC",
      "updated_date": "2025-12-15 19:37:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:44.929200+00:00"
    },
    {
      "arxiv_id": "2512.08777v1",
      "title": "Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages",
      "title_zh": "ä¸æµç•…è¯„åˆ¤ä¸‹çš„æµç•…å¯¹é½ï¼šé’ˆå¯¹ä½èµ„æºè¯­è¨€çš„åè®­ç»ƒ",
      "authors": [
        "David Samuel",
        "Lilja Ã˜vrelid",
        "Erik Velldal",
        "Andrey Kutuzov"
      ],
      "abstract": "We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian BokmÃ¥l and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½èµ„æºè¯­è¨€ (lower-resource languages) æå‡ºäº†ä¸€ç§åè®­ç»ƒ (post-training) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å½“å¯¹é½æ‰€ç”¨çš„å¥–åŠ±æ¨¡å‹ (reward models) è¡¨è¾¾ä¸å¤Ÿæµç•…æ—¶ï¼Œå¦‚ä½•ç¡®ä¿è¯­è¨€æ¨¡å‹ä¾ç„¶ä¿æŒé«˜æµç•…åº¦ã€‚ç”±äºä½èµ„æºè¯­è¨€æ™®éç¼ºä¹æ¯è¯­äººå£«ç¼–å†™çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œè¯¥ç ”ç©¶é‡ç‚¹æ¢è®¨äº†åœ¨ä¸ä¾èµ–ç›®æ ‡è¯­è¨€æŒ‡ä»¤å¾®è°ƒ (instruction-tuning) æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æ„å»ºåå¥½å¯¹é½çš„æµåˆ©æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨åœ¨çº¿ç­–ç•¥ (on-policy) è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å°†å…¶ä¸åŸºäºæœºå™¨ç¿»è¯‘æ•°æ®çš„ç›‘ç£å¾®è°ƒ (supervised finetuning) ä»¥åŠå¤šè¯­è¨€å¾®è°ƒ (multilingual finetuning) ä¸¤ç§å¸¸è§æ–¹æ¡ˆè¿›è¡Œäº†å¯¹æ¯”ã€‚é€šè¿‡å¯¹æŒªå¨è¯­ (Norwegian BokmÃ¥l) çš„æ¡ˆä¾‹ç ”ç©¶åŠæ¯è¯­äººå£«çš„ä¸“ä¸šè¯„ä¼°ï¼Œç»“æœè¡¨æ˜åœ¨çº¿ç­–ç•¥ (on-policy) æœºåˆ¶å¯¹äºæ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ä¾èµ–éš¾ä»¥è·å–çš„ç¨€ç¼ºæ•°æ®çš„å‰æä¸‹ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºæå‡ä½èµ„æºè¯­è¨€æ¨¡å‹çš„å¯¹é½è´¨é‡æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08777v1",
      "published_date": "2025-12-09 16:31:48 UTC",
      "updated_date": "2025-12-09 16:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:03:52.665679+00:00"
    },
    {
      "arxiv_id": "2512.08774v1",
      "title": "Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps",
      "title_zh": "åŸºäºå¯è§£é‡Šäººå·¥æ™ºèƒ½ç‘•ç–µæ¿€æ´»å›¾çš„æ‰©æ•£æ¨¡å‹è§†è§‰ä¼ªå½±ä¿®å¤",
      "authors": [
        "Seoyeon Lee",
        "Gwangyeol Yu",
        "Chaewon Kim",
        "Jonghyuk Park"
      ],
      "abstract": "Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in FrÃ©chet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†self-refining diffusionæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Diffusion modelsåœ¨å›¾åƒåˆæˆä¸­é¢ä¸´çš„artifactså’Œä¸çœŸå®åŒºåŸŸæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºExplainable AI (XAI)çš„ç¼ºé™·é«˜äº®å™¨ç”ŸæˆFlaw Activation Maps (FAMs)ï¼Œç”¨ä»¥ç²¾å‡†è¯†åˆ«å›¾åƒä¸­çš„ç‘•ç–µã€‚FAMsåœ¨å‰å‘è¿‡ç¨‹ä¸­é€šè¿‡æ”¾å¤§ç¼ºé™·åŒºåŸŸçš„å™ªå£°ï¼Œå¹¶åœ¨åå‘è¿‡ç¨‹ä¸­é‡ç‚¹å…³æ³¨è¿™äº›åŒºåŸŸï¼Œä»è€Œæœ‰æ•ˆæ”¹å–„é‡å»ºè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŸºäºæ‰©æ•£çš„æ¨¡å‹ä¸­å°†FrÃ©chet inception distance (FID) ä¼˜åŒ–äº†é«˜è¾¾27.3%ï¼Œåœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡è¡¨ç°ç¨³å¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨Image generationã€Text-to-image generationåŠInpaintingç­‰ä»»åŠ¡ä¸­å‡å±•ç°äº†æå¼ºçš„é€‚ç”¨æ€§ï¼Œè¯æ˜äº†XAIæŠ€æœ¯åœ¨æ¨¡å‹Interpretabilityä¹‹å¤–å¯¹å›¾åƒç²¾ç»†åŒ–çš„ä¸»åŠ¨è´¡çŒ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 9 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08774v1",
      "published_date": "2025-12-09 16:30:31 UTC",
      "updated_date": "2025-12-09 16:30:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:00.042215+00:00"
    },
    {
      "arxiv_id": "2512.17928v1",
      "title": "Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach",
      "title_zh": "STAR-RIS è¾…åŠ©é€šä¿¡çš„é«˜æ•ˆæ³¢æŸèµ‹å½¢ä¼˜åŒ–ï¼šä¸€ç§åŸºäºæ¢¯åº¦çš„å…ƒå­¦ä¹ æ–¹æ³•",
      "authors": [
        "Dongdong Yang",
        "Bin Li",
        "Jiguang He",
        "Yicheng Yan",
        "Xiaoyu Zhang",
        "Chongwen Huang"
      ],
      "abstract": "Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹STAR-RISè¾…åŠ©é€šä¿¡ä¸­çš„æ³¢æŸèµ‹å½¢ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦çš„å…ƒå­¦ä¹ (Gradient-Based Meta Learning, GML)æ¡†æ¶ï¼Œä»¥è§£å†³ä¼ ç»Ÿäº¤æ›¿ä¼˜åŒ–(AO)ç®—æ³•è®¡ç®—å¤æ‚åº¦é«˜å’Œç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ä¼˜åŒ–æ¢¯åº¦ç›´æ¥è¾“å…¥è½»é‡çº§ç¥ç»ç½‘ç»œï¼Œå®ç°äº†æ— éœ€é¢„è®­ç»ƒçš„å¿«é€Ÿé€‚åº”èƒ½åŠ›ï¼Œå¹¶ä¸“é—¨é’ˆå¯¹ç‹¬ç«‹ç›¸ä½å’Œè€¦åˆç›¸ä½STAR-RISæ¨¡å‹è®¾è®¡äº†å¤„ç†æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†STAR-RISçš„å¹…åº¦å’Œç›¸ä½çº¦æŸï¼Œåœ¨ä¿è¯åŠ æƒå’Œé€Ÿç‡(Weighted Sum-Rate)æ€§èƒ½æ¥è¿‘ä¼ ç»ŸAOåŸºå‡†çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ¡ˆçš„å¤æ‚åº¦éšåŸºç«™(Base Station)å¤©çº¿å’ŒSTAR-RISå…ƒä»¶æ•°é‡å‘ˆè¿‘ä¹çº¿æ€§å¢é•¿ï¼Œåœ¨è¿è¡Œé€Ÿåº¦ä¸Šæ¯”AOç®—æ³•å¿«10å€ã€‚è¯¥ç ”ç©¶éªŒè¯äº†GMLæ–¹æ³•åœ¨å¤§è§„æ¨¡STAR-RISè¾…åŠ©é€šä¿¡ç³»ç»Ÿä¸­çš„æ‰©å±•æ€§ä¸å®ç”¨æ€§ï¼Œä¸ºé«˜æ•ˆå®ç°å…¨ç©ºé—´è¦†ç›–æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17928v1",
      "published_date": "2025-12-09 16:28:15 UTC",
      "updated_date": "2025-12-09 16:28:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:05.092332+00:00"
    },
    {
      "arxiv_id": "2512.08769v1",
      "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
      "title_zh": "ç”Ÿäº§çº§æ™ºèƒ½ä½“ AI å·¥ä½œæµè®¾è®¡ã€å¼€å‘ä¸éƒ¨ç½²å®ç”¨æŒ‡å—",
      "authors": [
        "Eranga Bandara",
        "Ross Gore",
        "Peter Foytik",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Abdul Rahman",
        "Xueping Liang",
        "Safdar H. Bouk",
        "Amin Hass",
        "Sachini Rajapakse",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
      "tldr_zh": "è¯¥è®ºæ–‡é’ˆå¯¹ä»å•æ¨¡å‹æç¤ºå‘ Agentic AI å·¥ä½œæµè½¬å˜è¿‡ç¨‹ä¸­é¢ä¸´çš„å¯é æ€§ã€å¯è§‚æµ‹æ€§å’Œå®‰å…¨æ€§ç­‰æŒ‘æˆ˜ï¼Œæä¾›äº†ä¸€ä»½æ„å»ºç”Ÿäº§çº§ç³»ç»Ÿçš„ç«¯åˆ°ç«¯å®è·µæŒ‡å—ã€‚ç ”ç©¶å¼•å…¥äº†ç»“æ„åŒ–çš„å·¥ç¨‹ç”Ÿå‘½å‘¨æœŸï¼Œæ¶µç›–äº†å·¥ä½œæµåˆ†è§£ã€å¤šæ™ºèƒ½ä½“è®¾è®¡æ¨¡å¼ã€Model Context Protocol (MCP) é›†æˆä»¥åŠç¡®å®šæ€§ç¼–æ’ (Deterministic Orchestration) ç­‰æ ¸å¿ƒç¯èŠ‚ã€‚æ–‡ä¸­æ€»ç»“äº†ä¹é¡¹å…³é”®çš„å·¥ç¨‹æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬åšæŒ KISS åŸåˆ™ã€å·¥å…·ä¼˜å…ˆè®¾è®¡ã€çº¯å‡½æ•°è°ƒç”¨ã€å•èŒè´£æ™ºèƒ½ä½“ä»¥åŠå·¥ä½œæµé€»è¾‘ä¸ MCP æœåŠ¡å™¨çš„æ¸…æ™°åˆ†ç¦»ã€‚æ­¤å¤–ï¼ŒæŒ‡å—è¿˜æ¢è®¨äº†å¤–éƒ¨åŒ–æç¤ºè¯ç®¡ç†ã€ç¬¦åˆ Responsible-AI è¦æ±‚çš„æ¨¡å‹è”ç›Ÿè®¾è®¡åŠå®¹å™¨åŒ–éƒ¨ç½²ç­–ç•¥ã€‚é€šè¿‡å¤šæ¨¡æ€æ–°é—»åˆ†æä¸åª’ä½“ç”Ÿæˆçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæœ¬æ–‡ä¸ºå¼€å‘è€…æ„å»ºç¨³å¥ã€å¯æ‰©å±•ä¸”å…·å¤‡ç”Ÿäº§å°±ç»ªæ€§çš„ Agentic AI ä»»åŠ¡æµæä¾›äº†é‡è¦çš„æŠ€æœ¯å‚è€ƒä¸æ¶æ„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08769v1",
      "published_date": "2025-12-09 16:23:05 UTC",
      "updated_date": "2025-12-09 16:23:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:08.699992+00:00"
    },
    {
      "arxiv_id": "2512.08767v1",
      "title": "Data-Driven Dynamic Parameter Learning of manipulator robots",
      "title_zh": "æ•°æ®é©±åŠ¨çš„æœºæ¢°è‡‚åŠ¨åŠ›å­¦å‚æ•°å­¦ä¹ ",
      "authors": [
        "Mohammed Elseiagy",
        "Tsige Tadesse Alemayoh",
        "Ranulfo Bezerra",
        "Shotaro Kojima",
        "Kazunori Ohno"
      ],
      "abstract": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººé¢†åŸŸä¸­å¼¥åˆ sim-to-real gap çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Transformer çš„åŠ¨æ€å‚æ•°ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§£ææ³•å’Œå¾ªç¯ç¥ç»ç½‘ç»œåœ¨å¤„ç†å¤æ‚æœºå™¨äººç»“æ„åŠé•¿ç¨‹ä¾èµ–æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹ï¼Œé€šè¿‡ Jacobian-derived features ç”Ÿæˆäº†åŒ…å« 8,192 ä¸ªå…·æœ‰ä¸åŒæƒ¯æ€§å’Œæ‘©æ“¦ç‰¹æ€§çš„æœºå™¨äººæ¨¡å‹åŠå…¶ä¸°å¯Œè½¨è¿¹æ•°æ®çš„æ•°æ®é›†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ attention mechanisms æœ‰æ•ˆæ•æ‰äº†åŠ¨æ€å‚æ•°ä¼°è®¡ä¸­çš„æ—¶é—´å’Œç©ºé—´ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ€ä½³é…ç½®ä¸‹ï¼Œæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„ R2 è¾¾åˆ°äº† 0.8633ï¼Œå…¶ä¸­ Mass å’Œ inertia çš„ä¼°è®¡ç²¾åº¦æ¥è¿‘å®Œç¾ï¼ŒCoulomb friction è¾¾åˆ°ä¸­é«˜ç²¾åº¦ã€‚è™½ç„¶ viscous friction å’Œè¿œç«¯è¿æ†çš„ center-of-mass ä¼°è®¡ä»å…·æŒ‘æˆ˜æ€§ï¼Œä½†è¯¥ç ”ç©¶è¯æ˜äº† Transformer ä¸è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆçš„ç»“åˆèƒ½å®ç°å¯æ‰©å±•ä¸”ç²¾ç¡®çš„å‚æ•°å­¦ä¹ ã€‚è¿™ä¸€æˆæœä¸ºæå‡æœºå™¨äººç³»ç»Ÿçš„ sim-to-real transfer èƒ½åŠ›æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication at SII 2026. 6 pages, 7 figures. Code is available at: https://github.com/MohamedAlsiagy/dynamic_parameter_est",
      "pdf_url": "https://arxiv.org/pdf/2512.08767v1",
      "published_date": "2025-12-09 16:15:58 UTC",
      "updated_date": "2025-12-09 16:15:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:21.229762+00:00"
    },
    {
      "arxiv_id": "2512.08755v1",
      "title": "Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments",
      "title_zh": "ä¸‰ç»´æ— çº¿ç¯å¢ƒä¸‹ç©ºä¸­ RIS ä¸ STAR-RIS çš„æ€§èƒ½å¯¹æ¯”",
      "authors": [
        "Dongdong Yang",
        "Bin Li",
        "Jiguang He"
      ],
      "abstract": "Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†åœ¨ä¸‰ç»´æ— çº¿ç¯å¢ƒä¸‹çš„ç©ºä¸­ Reconfigurable intelligent surface (RIS) ä¸ simultaneously transmitting and reflecting RIS (STAR-RIS) çš„æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶å»ºç«‹äº†åŒ…å«å®šå‘è¾å°„å›¾çš„ç²¾ç¡®ä¿¡é“æ¨¡å‹ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†éƒ¨ç½²é«˜åº¦å’Œæœå‘å¯¹æ€§èƒ½çš„å½±å“ã€‚ä¸ºäº†ä¼˜åŒ–ç³»ç»Ÿæ€»é€Ÿç‡ (sum-rate)ï¼Œä½œè€…é’ˆå¯¹ä¸¤ç§æ¶æ„æå‡ºäº†åŸºäº weighted minimum mean square error (WMMSE) å’Œ block coordinate descent (BCD) ç®—æ³•çš„é«˜æ•ˆè”åˆä¼˜åŒ–æ–¹æ¡ˆã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œç”±äº STAR-RIS å…·å¤‡å…¨ç©ºé—´è¦†ç›–èƒ½åŠ›ï¼Œå…¶åœ¨ä½é«˜åº¦åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿ RISã€‚ç„¶è€Œï¼Œåœ¨è¾ƒé«˜é«˜åº¦ä¸”é è¿‘åŸºç«™çš„ä½ç½®ï¼ŒRIS åˆ™å±•ç°å‡ºæ›´ä½³çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶ç»“æœä¸ºæœªæ¥ 6G é€šä¿¡ç³»ç»Ÿä¸­ç©ºä¸­æ™ºèƒ½è¡¨é¢çš„å®é™…éƒ¨ç½²æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08755v1",
      "published_date": "2025-12-09 16:06:09 UTC",
      "updated_date": "2025-12-09 16:06:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:22.632367+00:00"
    },
    {
      "arxiv_id": "2512.13717v1",
      "title": "Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints",
      "title_zh": "éšç§çº¦æŸä¸‹é’ˆå¯¹ç™«ç—«å‘ä½œæ£€æµ‹çš„è”é‚¦å°æ ·æœ¬å­¦ä¹ ",
      "authors": [
        "Ekaterina Sysoykova",
        "Bernhard Anzengruber-Tanase",
        "Michael Haslgrubler",
        "Philipp Seidl",
        "Alois Ferscha"
      ],
      "abstract": "Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè„‘ç”µå›¾(EEG)çš„ç™«ç—«å‘ä½œæ£€æµ‹ä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€è·¨æœºæ„åˆ†å¸ƒåŠéšç§ä¿æŠ¤é™åˆ¶ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè”é‚¦å°‘æ ·æœ¬å­¦ä¹ (Federated Few-Shot Learning, FFSL)æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡è”é‚¦å­¦ä¹ åœ¨å¤šä¸ªéç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)çš„ç«™ç‚¹ä¸Šå¯¹é¢„è®­ç»ƒçš„ç”Ÿç‰©ä¿¡å·è½¬æ¢å™¨(BIOT)è¿›è¡Œå¾®è°ƒï¼Œå®ç°æ— éœ€æ•°æ®èšåˆçš„å…±äº«è¡¨å¾å­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨è”é‚¦å°‘æ ·æœ¬ä¸ªæ€§åŒ–æŠ€æœ¯ï¼Œä»…éœ€äº”ä¸ªæ ‡æ³¨çš„EEGç‰‡æ®µå³å¯ä½¿æ¨¡å‹é€‚é…ç‰¹å®šæ‚£è€…ï¼Œåœ¨ä¿ç•™æ‚£è€…ç‰¹æœ‰ä¿¡æ¯çš„åŒæ—¶è·ç›Šäºè·¨ç«™ç‚¹çŸ¥è¯†ã€‚å®éªŒåŸºäºTUH Event Corpusæ•°æ®é›†ï¼Œç»“æœæ˜¾ç¤ºFFSLé˜¶æ®µçš„å®¢æˆ·ç«¯ç‰¹å®šæ¨¡å‹è¾¾åˆ°äº†0.77çš„å¹³å‡å¹³è¡¡å‡†ç¡®ç‡å’Œ0.73çš„åŠ æƒF1åˆ†æ•°ã€‚è¯¥æˆæœè¡¨æ˜ï¼ŒFFSLèƒ½å¤Ÿåœ¨æ»¡è¶³ç°å®åŒ»ç–—éšç§çº¦æŸå’Œæœ‰é™æ•°æ®å¯å¾—æ€§çš„å‰æä¸‹ï¼Œå®ç°æœ‰æ•ˆçš„æ‚£è€…è‡ªé€‚åº”ç™«ç—«å‘ä½œæ£€æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.13717v1",
      "published_date": "2025-12-09 16:01:35 UTC",
      "updated_date": "2025-12-09 16:01:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:25.894500+00:00"
    },
    {
      "arxiv_id": "2512.08743v3",
      "title": "Single-Agent Scaling Fails Multi-Agent Intelligence: Towards Foundation Models with Native Multi-Agent Intelligence",
      "title_zh": "å•æ™ºèƒ½ä½“æ‰©å±•å¹¶ä¸ç­‰åŒäºå¤šæ™ºèƒ½ä½“æ™ºèƒ½ï¼šè¿ˆå‘å…·å¤‡åŸç”Ÿå¤šæ™ºèƒ½ä½“æ™ºèƒ½çš„åŸºåº§æ¨¡å‹",
      "authors": [
        "Shuyue Hu",
        "Haoyang Yan",
        "Yiqun Zhang",
        "Yang Chen",
        "Dongzhan Zhou",
        "Lei Bai"
      ],
      "abstract": "Foundation models (FMs) are increasingly assuming the role of the ''brain'' of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence, across 41 large language models and 7 challenging benchmarks, showing that scaling single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºåŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰è™½åœ¨å•æ™ºèƒ½ä½“èƒ½åŠ›ä¸Šå–å¾—äº†è¿›æ­¥ï¼Œä½†åŸç”Ÿå¤šæ™ºèƒ½ä½“æ™ºèƒ½æ‰æ˜¯æœªæ¥çš„å…³é”®å‰æ²¿ã€‚ä½œè€…æ˜ç¡®äº†å¤šæ™ºèƒ½ä½“èƒŒæ™¯ä¸‹çš„å››é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œå³ç†è§£ï¼ˆunderstandingï¼‰ã€è§„åˆ’ï¼ˆplanningï¼‰ã€é«˜æ•ˆé€šä¿¡ï¼ˆefficient communicationï¼‰å’Œé€‚åº”ï¼ˆadaptationï¼‰ã€‚é€šè¿‡å¯¹41ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆlarge language modelsï¼‰å’Œ7ä¸ªå¤æ‚åŸºå‡†æµ‹è¯•çš„å®è¯åˆ†æï¼Œç ”ç©¶è¯æ˜äº†å•æ™ºèƒ½ä½“æ€§èƒ½çš„ç®€å•æ‰©å±•æ— æ³•è‡ªåŠ¨æ¼”åŒ–å‡ºå¼ºå¤§çš„å¤šæ™ºèƒ½ä½“æ™ºèƒ½ã€‚é’ˆå¯¹è¿™ä¸€ç°çŠ¶ï¼Œè®ºæ–‡æå‡ºäº†æ„å»ºåŸç”Ÿå¤šæ™ºèƒ½ä½“åŸºç¡€æ¨¡å‹çš„ç³»ç»Ÿæ€§ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ„å»ºã€è¯„ä¼°ä½“ç³»ã€è®­ç»ƒèŒƒå¼ä»¥åŠå®‰å…¨è€ƒé‡ã€‚è¯¥é¡¹ç ”ç©¶æŒ‘æˆ˜äº†å¤šæ™ºèƒ½ä½“èƒ½åŠ›ä¼šéšæ¨¡å‹è§„æ¨¡è‡ªå‘æ¶Œç°çš„å‡è®¾ï¼Œä¸ºæœªæ¥ååŒæ™ºèƒ½çš„å‘å±•æä¾›äº†ç†è®ºæ¡†æ¶å’Œå®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08743v3",
      "published_date": "2025-12-09 15:51:36 UTC",
      "updated_date": "2025-12-16 16:37:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:40.617047+00:00"
    },
    {
      "arxiv_id": "2512.08740v1",
      "title": "Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance",
      "title_zh": "è§£æ„åŒé‡é»‘ç›’ï¼šä¸€ç§é¢å‘äººæœºååŒå¢å¼ºçš„å³æ’å³ç”¨è®¤çŸ¥æ¡†æ¶åŠå…¶å¯¹äººå·¥æ™ºèƒ½æ²»ç†çš„å¯ç¤º",
      "authors": [
        "Yiming Lu"
      ],
      "abstract": "Currently, there exists a fundamental divide between the \"cognitive black box\" (implicit intuition) of human experts and the \"computational black box\" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of \"human-AI collaborative cognitive enhancement,\" aiming to transform the dual black boxes into a composable, auditable, and extensible \"functional white-box\" system through structured \"meta-interaction.\" The core breakthrough lies in the \"plug-and-play cognitive framework\"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from \"AI as a tool\" to \"AI as a thinking partner.\" This work not only provides the first engineering proof for \"cognitive equity\" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through \"transparency of interaction protocols\" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººç±»ä¸“å®¶çš„â€œcognitive black boxâ€ä¸äººå·¥æ™ºèƒ½çš„â€œcomputational black boxâ€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸ºâ€œhuman-AI collaborative cognitive enhancementâ€çš„æ–°èŒƒå¼ã€‚é€šè¿‡ç»“æ„åŒ–çš„â€œmeta-interactionâ€ï¼Œè¯¥èŒƒå¼æ—¨åœ¨å°†åŒé‡é»‘ç®±è½¬åŒ–ä¸ºå¯ç»„åˆã€å¯å®¡è®¡ä¸”å¯æ‰©å±•çš„â€œfunctional white-boxâ€ç³»ç»Ÿã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§â€œplug-and-play cognitive frameworkâ€ï¼Œå³ä¸€ç§å¯ä»ä¸“å®¶å¯¹è¯ä¸­æå–å¹¶åŠ è½½è‡³â€œRecursive Adversarial Meta-Thinking Network (RAMTN)â€çš„å¯è®¡ç®—çŸ¥è¯†åŒ…ã€‚è¯¥æŠ€æœ¯å®ç°äº†å°†åŒ»ç–—è¯Šæ–­é€»è¾‘ç­‰ä¸“å®¶æ€ç»´è½¬åŒ–ä¸ºå¯å¤ç”¨ä¸”å¯æ‰©å±•çš„å…¬å…±èµ„äº§ï¼Œæ¨åŠ¨äº†AIä»â€œå·¥å…·â€å‘â€œthinking partnerâ€çš„è§’è‰²è½¬å˜ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œä¸ºâ€œcognitive equityâ€æä¾›äº†é¦–ä¸ªå·¥ç¨‹è¯æ˜ï¼Œå¹¶ä¸ºAIæ²»ç†å¼€è¾Ÿäº†é€šè¿‡â€œtransparency of interaction protocolsâ€è€Œéæ¢æµ‹æ¨¡å‹å†…éƒ¨æœºåˆ¶æ¥å®ç°å¯éªŒè¯æ²»ç†çš„æ–°è·¯å¾„ã€‚è¯¥æ¡†æ¶ç›®å‰å·²å¼€æºï¼Œæ—¨åœ¨è¿›ä¸€æ­¥ä¿ƒè¿›æŠ€æœ¯å‘å–„ä¸è®¤çŸ¥åŒ…å®¹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, in Chinese language, 5 figures,3 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08740v1",
      "published_date": "2025-12-09 15:50:15 UTC",
      "updated_date": "2025-12-09 15:50:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:48.097958+00:00"
    },
    {
      "arxiv_id": "2512.08733v1",
      "title": "Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting",
      "title_zh": "é€šè¿‡åˆ†å¸ƒæ„ŸçŸ¥é‡åŠ æƒç¼“è§£çš®è‚¤ç—…å˜åˆ†ç±»ä¸­çš„ä¸ªä½“è‚¤è‰²åå·®",
      "authors": [
        "Kuniko Paxton",
        "Zeinab Dehghani",
        "Koorosh Aslansefat",
        "Dhavalkumar Thakker",
        "Yiannis Papadopoulos"
      ],
      "abstract": "Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç—…å˜åˆ†ç±»ä¸­ç”±äºé‡‡ç”¨ç²—ç•¥åˆ†ç»„ç±»åˆ«è€Œå¿½è§†ä¸ªä½“è‚¤è‰²å·®å¼‚å¹¶å¯¼è‡´åè§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºåˆ†å¸ƒçš„æ¡†æ¶æ¥è¯„ä¼°å’Œç¼“è§£ä¸ªä½“å…¬å¹³æ€§ã€‚ç ”ç©¶å°†è‚¤è‰²è§†ä¸ºè¿ç»­å±æ€§è€Œéç±»åˆ«æ ‡ç­¾ï¼Œåˆ©ç”¨æ ¸å¯†åº¦ä¼°è®¡(KDE)å»ºæ¨¡å…¶åˆ†å¸ƒï¼Œå¹¶å¯¹æ¯”äº†12ç§ç»Ÿè®¡è·ç¦»åº¦é‡ä»¥é‡åŒ–è‚¤è‰²åˆ†å¸ƒé—´çš„å·®å¼‚ã€‚ä¸ºçº æ­£å°‘æ•°ç¾¤ä½“è‚¤è‰²çš„ä»£è¡¨æ€§ä¸è¶³ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºè·ç¦»çš„é‡åŠ æƒ(DRW)æŸå¤±å‡½æ•°ã€‚åœ¨CNNå’ŒTransformeræ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿçš„ç±»åˆ«é‡åŠ æƒéš¾ä»¥æ•æ‰ä¸ªä½“å±‚é¢çš„å·®å¼‚ï¼Œè€ŒåŸºäºåˆ†å¸ƒçš„é‡åŠ æƒè¡¨ç°æ›´ä¼˜ï¼Œå°¤å…¶æ˜¯åœ¨ç»“åˆä¿çœŸåº¦ç›¸ä¼¼åº¦(FS)ã€Wassersteinè·ç¦»(WD)ã€Hellingeråº¦é‡(HM)å’Œè°ƒå’Œå¹³å‡ç›¸ä¼¼åº¦(HS)æ—¶ã€‚è¯¥ç ”ç©¶ä¸ºçš®è‚¤ç—…AIç³»ç»Ÿçš„ä¸ªä½“å…¬å¹³æ€§æä¾›äº†é²æ£’çš„æ–¹æ³•è®ºï¼Œå¹¶å¯¹åŒ»å­¦å½±åƒåˆ†æä¸­çš„æ•æ„Ÿè¿ç»­å±æ€§å¤„ç†å…·æœ‰å¹¿æ³›çš„å€Ÿé‰´æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08733v1",
      "published_date": "2025-12-09 15:45:20 UTC",
      "updated_date": "2025-12-09 15:45:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:37.582509+00:00"
    },
    {
      "arxiv_id": "2512.09010v1",
      "title": "Towards Lossless Ultimate Vision Token Compression for VLMs",
      "title_zh": "è¿ˆå‘è§†è§‰è¯­è¨€æ¨¡å‹çš„æ— æŸæè‡´è§†è§‰ Token å‹ç¼©",
      "authors": [
        "Dehua Zheng",
        "Mouxiao Huang",
        "Borui Jiang",
        "Hailin Hu",
        "Xinghao Chen"
      ],
      "abstract": "Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶å­˜åœ¨çš„Tokenå†—ä½™ä¸è®¡ç®—å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†Lossless Ultimate Vision tokens Compression (LUVC)æ¡†æ¶ã€‚ä¸ºäº†å…‹æœç°æœ‰å‹ç¼©ç®—æ³•åœ¨å‡†ç¡®ç‡ä¸‹é™å’Œæµ…å±‚LLMé€‚é…ä¸Šçš„å±€é™æ€§ï¼ŒLUVCé€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸­å¼•å…¥ç©ºé—´æ­£äº¤çš„è¿­ä»£åˆå¹¶æ–¹æ¡ˆ(Iterative Merging)æ¥åŠ é€Ÿå…¨å±€è®¡ç®—ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨LLMä¸­é›†æˆäº†ä¸€ä¸ªåŸºäºä½é€šæ»¤æ³¢å™¨çš„é¢‘è°±å‰ªæå•å…ƒ(Spectrum Pruning Unit)ï¼Œä»¥å…¼å®¹FlashAttentionçš„æ–¹å¼é€æ­¥å‰”é™¤å†—ä½™Tokenã€‚LUVCçš„æ ¸å¿ƒæœºåˆ¶æ˜¯å°†é«˜ç»´è§†è§‰ç‰¹å¾é€æ¸èåˆè¿›å¤šæ¨¡æ€æŸ¥è¯¢(Multimodal Queries)ä¸­ï¼Œç›´è‡³åœ¨LLMæœ€åä¸€å±‚å®Œå…¨æ¶ˆé™¤è§†è§‰Tokenã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹å‡†ç¡®ç‡å‡ ä¹ä¸å˜çš„æƒ…å†µä¸‹å®ç°äº†2å€çš„æ¨ç†åŠ é€Ÿã€‚ç”±äºå…·å¤‡æ— éœ€è®­ç»ƒ(Training-free)çš„ç‰¹æ€§ï¼ŒLUVCå¯ç›´æ¥éƒ¨ç½²äºå¤šç§ä¸»æµVLMsï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09010v1",
      "published_date": "2025-12-09 15:40:13 UTC",
      "updated_date": "2025-12-09 15:40:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:49.651579+00:00"
    },
    {
      "arxiv_id": "2512.08715v1",
      "title": "Multi-domain performance analysis with scores tailored to user preferences",
      "title_zh": "åŸºäºç”¨æˆ·åå¥½å®šåˆ¶è¯„åˆ†çš„å¤šé¢†åŸŸæ€§èƒ½åˆ†æ",
      "authors": [
        "SÃ©bastien PiÃ©rard",
        "Adrien DeliÃ¨ge",
        "Marc Van Droogenbroeck"
      ],
      "abstract": "The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç®—æ³•æ€§èƒ½åœ¨ä¸åŒåº”ç”¨é¢†åŸŸåˆ†å¸ƒä¸‹è¡¨ç°è¿¥å¼‚çš„é—®é¢˜ï¼Œå¼ºè°ƒäº†åœ¨å¤šé¢†åŸŸè¯„ä¼°ä¸­è®¡ç®—åŠ æƒå¹³å‡æ€§èƒ½åŠå…¶ç»†è‡´åˆ†æçš„é‡è¦æ€§ã€‚ä½œè€…é‡‡ç”¨æ¦‚ç‡æ¡†æ¶å°†æ€§èƒ½è§†ä¸ºæ¦‚ç‡æµ‹åº¦ï¼Œå¹¶åˆ©ç”¨SummarizationæŠ€æœ¯å¯¹å¤šé¢†åŸŸæ€§èƒ½è¿›è¡Œæ•´åˆã€‚ç ”ç©¶å‘ç°Ranking Scoresç³»åˆ—å¾—åˆ†èƒ½å°†æ‘˜è¦æ€§èƒ½å€¼æ˜ å°„ä¸ºé¢†åŸŸç‰¹å®šæ€§èƒ½å€¼çš„åŠ æƒç®—æœ¯å¹³å‡æ•°ï¼Œä¸”è¯¥æƒé‡å–å†³äºç”¨æˆ·åå¥½(User Preferences)ã€‚åŸºäºæ­¤ç†è®ºï¼Œç ”ç©¶ä¸¥æ ¼å®šä¹‰äº†å››ç±»ç”±ç”¨æˆ·åå¥½å†³å®šçš„å…³é”®é¢†åŸŸï¼Œå³Easiestã€Most Difficultã€Preponderantå’ŒBottleneck Domainsã€‚æœ€åï¼Œè¯¥ç ”ç©¶åœ¨é€šç”¨ç†è®ºåŸºç¡€ä¸Šä¸ºäºŒåˆ†ç±»(Two-class Classification)ä»»åŠ¡å¼€å‘äº†æ–°å‹å¯è§†åŒ–å·¥å…·ï¼Œä¸ºå¤šé¢†åŸŸä¸‹çš„æ€§èƒ½è¯„ä¼°æä¾›äº†ä¸¥è°¨çš„æ•°å­¦åŸºç¡€å’Œç›´è§‚çš„åˆ†ææ‰‹æ®µã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.PF",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08715v1",
      "published_date": "2025-12-09 15:29:53 UTC",
      "updated_date": "2025-12-09 15:29:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:04:55.027111+00:00"
    },
    {
      "arxiv_id": "2512.08713v1",
      "title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning",
      "title_zh": "å·´æ–¯å…‹è¯­å­¦ä¹ ä¸­çš„è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ä¸åé¦ˆç”Ÿæˆ",
      "authors": [
        "Ekhi Azurmendi",
        "Xabier Arregi",
        "Oier Lopez de Lacalle"
      ],
      "abstract": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¨å‡ºäº†é¦–ä¸ªç”¨äºå·´æ–¯å…‹è¯­(Basque)è‡ªåŠ¨ä½œæ–‡è¯„åˆ†(AES)å’Œåé¦ˆç”Ÿæˆçš„å…¬å¼€æ•°æ®é›†ï¼Œä¸“é—¨é’ˆå¯¹CEFR C1çº§åˆ«çš„è¯­è¨€èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«3,200ç¯‡æ¥è‡ªHABEçš„ä½œæ–‡ï¼Œå¹¶ç”±ä¸“å®¶è¯„ä¼°è€…ä»æ­£ç¡®æ€§ã€ä¸°å¯Œåº¦ã€è¿è´¯æ€§ã€å‡èšåŠ›å’Œä»»åŠ¡ä¸€è‡´æ€§ç­‰ç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ŒåŒæ—¶æä¾›äº†è¯¦ç»†çš„åé¦ˆå’Œé”™è¯¯ç¤ºä¾‹ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹RoBERTa-EusCrawlå’ŒLatxa 8B/70Bç­‰å¼€æºæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå®éªŒè¡¨æ˜ç¼–ç å™¨æ¨¡å‹åœ¨AESä»»åŠ¡ä¸­ä¾ç„¶é«˜åº¦å¯é ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒ(SFT)çš„Latxaæ¨¡å‹åœ¨è¯„åˆ†ä¸€è‡´æ€§å’Œåé¦ˆè´¨é‡æ–¹é¢è¡¨ç°å“è¶Šï¼Œå…¶è¡¨ç°ç”šè‡³è¶…è¶Šäº†GPT-5å’ŒClaude Sonnet 4.5ç­‰æœ€å…ˆè¿›çš„é—­æºç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç»“åˆè‡ªåŠ¨ä¸€è‡´æ€§æŒ‡æ ‡ä¸ä¸“å®¶éªŒè¯çš„åé¦ˆç”Ÿæˆè¯„ä¼°æ–°æ–¹æ³•ã€‚ç»“æœè¯æ˜å¾®è°ƒåçš„Latxaæ¨¡å‹èƒ½äº§ç”Ÿç¬¦åˆæ•™å­¦æ„ä¹‰çš„åé¦ˆï¼Œå¹¶èƒ½æ¯”ç§æœ‰æ¨¡å‹è¯†åˆ«å‡ºæ›´å¹¿æ³›çš„é”™è¯¯ç±»å‹ã€‚è¯¥èµ„æºå’ŒåŸºå‡†ä¸ºå·´æ–¯å…‹è¯­ç­‰ä½èµ„æºè¯­è¨€åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸçš„ç ”ç©¶å¥ å®šäº†é€æ˜ä¸”å…·æœ‰æ•™è‚²æ„ä¹‰çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to LREC 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.08713v1",
      "published_date": "2025-12-09 15:28:35 UTC",
      "updated_date": "2025-12-09 15:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:29.001741+00:00"
    },
    {
      "arxiv_id": "2512.08674v2",
      "title": "Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology",
      "title_zh": "é¢å‘èƒƒè‚ è‚¿ç˜¤å­¦å¤šå­¦ç§‘å†³ç­–çš„å¤šæ™ºèƒ½ä½“æ™ºèƒ½",
      "authors": [
        "Rongzhao Zhang",
        "Junqiao Wang",
        "Shuyun Yang",
        "Mouxiao Bian",
        "Chihao Zhang",
        "Dongyang Wang",
        "Qiujuan Yan",
        "Yun Zhong",
        "Yuwei Bai",
        "Guanxu Zhu",
        "Kangkun Mao",
        "Miao Wang",
        "Chao Ding",
        "Renjie Lu",
        "Lei Wang",
        "Lei Zheng",
        "Tao Zheng",
        "Xi Wang",
        "Zhuo Fan",
        "Bing Han",
        "Meiling Liu",
        "Luyi Jiang",
        "Dongming Shan",
        "Wenzhong Jin",
        "Jiwei Yu",
        "Zheng Wang",
        "Jie Xu",
        "Meng Luo"
      ],
      "abstract": "Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒƒè‚ é“è‚¿ç˜¤ï¼ˆGastrointestinal Oncologyï¼‰é¢†åŸŸä¸­å¤šæ¨¡æ€ä¸´åºŠæ¨ç†çš„å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ç§å±‚çº§åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMulti-Agent Frameworkï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å†…çª¥é•œã€æ”¾å°„å­¦åŠç”ŸåŒ–æŒ‡æ ‡ç­‰å¼‚æ„åŒ»ç–—æ•°æ®æ—¶é¢ä¸´çš„èƒŒæ™¯ç¨€é‡Šä¸å¹»è§‰é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ¨¡æ‹Ÿäººç±»å¤šå­¦ç§‘å›¢é˜Ÿï¼ˆMDTï¼‰çš„åä½œæµç¨‹ï¼Œå®ç°äº†å¯¹å¤æ‚å¼‚æ„ç—…å²çš„é›†æˆè§£è¯»ä¸åˆ†æã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶è·å¾—äº†4.60/5.00çš„ä¸“å®¶ç»¼åˆè¯„åˆ†ï¼Œåœ¨æ¨ç†é€»è¾‘å’ŒåŒ»å­¦å‡†ç¡®æ€§æ–¹é¢è¾ƒå•ä½“åŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¯æ˜ï¼Œè¿™ç§åŸºäºæ™ºèƒ½ä½“åä½œçš„æ¨¡æ‹Ÿæ–¹æ³•ä¸ºè‚¿ç˜¤è‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯è§£é‡Šä¸”å…·æœ‰ä¸´åºŠé²æ£’æ€§çš„å…¨æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08674v2",
      "published_date": "2025-12-09 14:56:40 UTC",
      "updated_date": "2025-12-23 04:39:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:11.698879+00:00"
    },
    {
      "arxiv_id": "2512.08657v1",
      "title": "Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain",
      "title_zh": "MLOps ä¸­çš„å¤ç”¨æ€§ï¼šåˆ©ç”¨ç«¯å£ä¸é€‚é…å™¨æ¨¡å¼æ„å»ºæµ·äº‹é¢†åŸŸå¾®æœåŠ¡æ¶æ„",
      "authors": [
        "Renato Cordeiro Ferreira",
        "Aditya Dhinavahi",
        "Rowanne Trapmann",
        "Willem-Jan van den Heuvel"
      ],
      "abstract": "ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨å­¦ä¹ èµ‹èƒ½ç³»ç»Ÿï¼ˆML-Enabled Systems, MLESï¼‰ä¸­çš„æ¶æ„å¯é‡ç”¨æ€§é—®é¢˜ï¼Œå¹¶åœ¨æµ·äº‹é¢†åŸŸå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ Ocean Guard çš„æ„å»ºä¸­è¿›è¡Œäº†å®è·µã€‚æ–‡ç« é‡ç‚¹å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç«¯å£å’Œé€‚é…å™¨ï¼ˆPorts and Adaptersï¼‰æ¨¡å¼ï¼Œå³å…­è¾¹å½¢æ¶æ„ï¼ˆHexagonal Architectureï¼‰ï¼Œå®ç°ä»å•ä¸€ä»£ç åº“æ„å»ºå¤šä¸ªå¾®æœåŠ¡ï¼ˆMicroservicesï¼‰çš„è½¯ä»¶æ¶æ„æŠ€æœ¯ã€‚ä½œè€…è¯¦ç»†è®°å½•äº†åœ¨ MLOps æµç¨‹ä¸­åº”ç”¨è¯¥æ¨¡å¼æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶åˆ†äº«äº†æå‡ç³»ç»Ÿç»„ä»¶å¯é‡ç”¨æ€§çš„å®è´µç»éªŒã€‚è¯¥ç»éªŒæŠ¥å‘Šæ—¨åœ¨ä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆã€æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå’Œæ•°æ®ç§‘å­¦å®¶æä¾›å‚è€ƒï¼Œé€šè¿‡é‡‡ç”¨å…­è¾¹å½¢æ¶æ„æ¥é™ä½ MLES çš„å›ºæœ‰å¤æ‚æ€§ã€‚è¿™ç§æ¶æ„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿåœ¨æµ·äº‹é¢†åŸŸç‰¹å®šä¸šåŠ¡ç›®æ ‡ä¸‹çš„çµæ´»æ€§ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºæ„å»ºå¤æ‚ä¸”å¯ç»´æŠ¤çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "7 pages, 3 figures (3 diagrams), submitted to ICSA 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.08657v1",
      "published_date": "2025-12-09 14:43:23 UTC",
      "updated_date": "2025-12-09 14:43:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:40.721728+00:00"
    },
    {
      "arxiv_id": "2512.08639v1",
      "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning",
      "title_zh": "åŸºäºç©ºé—´ã€æ—¶é—´ä¸å…·èº«æ¨ç†ç»Ÿä¸€æ¡†æ¶çš„ç©ºä¸­è§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Huilin Xu",
        "Zhuoyang Liu",
        "Yixiang Luomei",
        "Feng Xu"
      ],
      "abstract": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAVs)åœ¨ç©ºä¸­è§†è§‰è¯­è¨€å¯¼èˆª(Aerial Vision-Language Navigation, VLN)ä¸­è¿‡åº¦ä¾èµ–å…¨æ™¯å›¾åƒã€æ·±åº¦è¾“å…¥æˆ–é‡Œç¨‹è®¡å¯¼è‡´çš„ç³»ç»Ÿæˆæœ¬é«˜å’Œé›†æˆå¤æ‚ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç©ºä¸­VLNæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»…åˆ©ç”¨ç¬¬ä¸€äººç§°å•ç›®RGBè§‚æµ‹(egocentric monocular RGB observations)å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå¯¼èˆªï¼Œæå¤§åœ°é™ä½äº†å¯¹ç¡¬ä»¶çš„è¦æ±‚ã€‚æ¨¡å‹å°†å¯¼èˆªä»»åŠ¡å»ºæ¨¡ä¸ºä¸‹ä¸€ä¸ªTokené¢„æµ‹(next-token prediction)é—®é¢˜ï¼Œé€šè¿‡æç¤ºå¼•å¯¼çš„å¤šä»»åŠ¡å­¦ä¹ (prompt-guided multi-task learning)å…±åŒä¼˜åŒ–ç©ºé—´æ„ŸçŸ¥ã€è½¨è¿¹æ¨ç†å’ŒåŠ¨ä½œé¢„æµ‹ã€‚ä¸ºäº†æé«˜å­¦ä¹ æ•ˆç‡ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§å…³é”®å¸§é€‰æ‹©ç­–ç•¥(keyframe selection strategy)ä»¥å‡å°‘è§†è§‰å†—ä½™ï¼Œå¹¶ç»“åˆåŠ¨ä½œåˆå¹¶ä¸æ ‡ç­¾é‡åŠ æƒæœºåˆ¶(label reweighting mechanism)æ¥ç¼“è§£é•¿å°¾ç›‘ç£ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨Aerial VLNåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¯è§å’Œæœªè§ç¯å¢ƒä¸­å‡å–å¾—äº†å¼ºåŠ²çš„è¡¨ç°ã€‚å…¶å®æµ‹ç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä»…é™RGBçš„åŸºå‡†æ¨¡å‹ï¼Œå¹¶æœ‰æ•ˆç¼©å°äº†ä¸æœ€å…ˆè¿›çš„å…¨æ™¯RGB-Dæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºè½»é‡åŒ–æ— äººæœºçš„è‡ªä¸»å¯¼èˆªæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review, 12 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08639v1",
      "published_date": "2025-12-09 14:25:24 UTC",
      "updated_date": "2025-12-09 14:25:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:50.262163+00:00"
    },
    {
      "arxiv_id": "2512.08629v1",
      "title": "See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm",
      "title_zh": "See-Controlï¼šä¸€ç§é€šè¿‡æœºæ¢°è‡‚å®ç°æ™ºèƒ½æ‰‹æœºäº¤äº’çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Haoyu Zhao",
        "Weizhong Ding",
        "Yuhao Yang",
        "Zheng Tian",
        "Linyi Yang",
        "Kun Shao",
        "Jun Wang"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€é¡¹åä¸º Embodied Smartphone Operation (ESO) çš„æ–°ä»»åŠ¡ï¼Œå¹¶æå‡ºäº† See-Control æ¡†æ¶ã€‚See-Control æ˜¯ä¸€ç§å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä½¿ä½è‡ªç”±åº¦æœºæ¢°è‡‚èƒ½é€šè¿‡ç›´æ¥ç‰©ç†äº¤äº’æ“ä½œæ™ºèƒ½æ‰‹æœºï¼Œæ‘†è„±äº†ä¼ ç»Ÿæ–¹æ³•å¯¹ Android Debug Bridge (ADB) çš„ä¾èµ–ï¼Œå®ç°äº†è·¨å¹³å°é€šç”¨æ€§ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼šåŒ…å« 155 ä¸ªä»»åŠ¡åŠè¯„ä¼°æŒ‡æ ‡çš„ ESO åŸºå‡†æµ‹è¯•ã€æ— éœ€ç³»ç»Ÿåå°æƒé™å³å¯ç”Ÿæˆæœºå™¨äººæ§åˆ¶æŒ‡ä»¤çš„ MLLM å…·èº«æ™ºèƒ½ä½“ï¼Œä»¥åŠä¸€ä¸ªå¸¦æœ‰ä¸°å¯Œæ ‡æ³¨çš„æ“ä½œæ•°æ®é›†ã€‚é€šè¿‡è¿æ¥æ•°å­—æ™ºèƒ½ä½“ä¸ç‰©ç†ä¸–ç•Œï¼ŒSee-Control ä¸ºå®¶åº­æœºå™¨äººåœ¨ç°å®ç¯å¢ƒä¸‹æ‰§è¡Œæ‰‹æœºç›¸å…³ä»»åŠ¡æä¾›äº†é‡è¦è¿›å±•ã€‚è¯¥ç ”ç©¶ä¸ä»…å±•ç¤ºäº†å…·èº«æ™ºèƒ½åœ¨ç§»åŠ¨è®¾å¤‡æ“ä½œä¸­çš„æ½œåŠ›ï¼Œä¹Ÿä¸ºæœªæ¥çš„ç›¸å…³ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºå’ŒæŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08629v1",
      "published_date": "2025-12-09 14:14:37 UTC",
      "updated_date": "2025-12-09 14:14:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:50.637710+00:00"
    },
    {
      "arxiv_id": "2512.08613v1",
      "title": "Protein Secondary Structure Prediction Using Transformers",
      "title_zh": "åŸºäº Transformer çš„è›‹ç™½è´¨äºŒçº§ç»“æ„é¢„æµ‹",
      "authors": [
        "Manzi Kevin Maxime"
      ],
      "abstract": "Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ¨¡å‹ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶(attention mechanisms)ä»æ°¨åŸºé…¸åºåˆ—ä¸­é¢„æµ‹è›‹ç™½è´¨äºŒçº§ç»“æ„ï¼ŒåŒ…æ‹¬alpha helicesã€beta sheetså’Œcoilsã€‚ä¸ºäº†æ‰©å……è®­ç»ƒæ ·æœ¬è§„æ¨¡ï¼Œç ”ç©¶äººå‘˜åœ¨CB513æ•°æ®é›†ä¸Šåº”ç”¨äº†æ»‘åŠ¨çª—å£æ•°æ®å¢å¼ºæŠ€æœ¯(sliding-window data augmentation)ã€‚è¯¥Transformeræ¨¡å‹åœ¨å¤„ç†å˜é•¿åºåˆ—æ—¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ®‹åŸºä¹‹é—´çš„å±€éƒ¨å’Œé•¿ç¨‹ç›¸äº’ä½œç”¨(long-range residue interactions)ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨ç†è§£è›‹ç™½è´¨åºåˆ—ä¸ç»“æ„å…³ç³»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè›‹ç™½è´¨åŠŸèƒ½ç ”ç©¶æä¾›äº†å…³é”®çš„ç»“æ„é¢„æµ‹æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08613v1",
      "published_date": "2025-12-09 13:58:47 UTC",
      "updated_date": "2025-12-09 13:58:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:48.950573+00:00"
    },
    {
      "arxiv_id": "2512.08609v2",
      "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
      "title_zh": "CogMCTSï¼šä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹è¿­ä»£å¯å‘å¼æ¼”åŒ–çš„æ–°å‹è®¤çŸ¥å¼•å¯¼è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¡†æ¶",
      "authors": [
        "Hui Wang",
        "Yang Liu",
        "Xiaoyu Zhang",
        "Chaoxu Mu"
      ],
      "abstract": "Automatic Heuristic Design (AHD) is an effective framework for solving complex optimization problems. The development of large language models (LLMs) enables the automated generation of heuristics. Existing LLM-based evolutionary methods rely on population strategies and are prone to local optima. Integrating LLMs with Monte Carlo Tree Search (MCTS) improves the trade-off between exploration and exploitation, but multi-round cognitive integration remains limited and search diversity is constrained. To overcome these limitations, this paper proposes a novel cognitive-guided MCTS framework (CogMCTS). CogMCTS tightly integrates the cognitive guidance mechanism of LLMs with MCTS to achieve efficient automated heuristic optimization. The framework employs multi-round cognitive feedback to incorporate historical experience, node information, and negative outcomes, dynamically improving heuristic generation. Dual-track node expansion combined with elite heuristic management balances the exploration of diverse heuristics and the exploitation of high-quality experience. In addition, strategic mutation modifies the heuristic forms and parameters to further enhance the diversity of the solution and the overall optimization performance. The experimental results indicate that CogMCTS outperforms existing LLM-based AHD methods in stability, efficiency, and solution quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨åŒ–å¯å‘å¼è®¾è®¡(Automatic Heuristic Design, AHD)æ–¹æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ä¸”ç¼ºä¹å¤šè½®è®¤çŸ¥æ•´åˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹è®¤çŸ¥å¼•å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¡†æ¶(CogMCTS)ã€‚è¯¥æ¡†æ¶å°†LLMsçš„è®¤çŸ¥å¼•å¯¼æœºåˆ¶ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ç´§å¯†é›†æˆï¼Œé€šè¿‡å¤šè½®è®¤çŸ¥åé¦ˆæœºåˆ¶å¼•å…¥å†å²ç»éªŒã€èŠ‚ç‚¹ä¿¡æ¯å’Œè´Ÿé¢ç»“æœï¼Œå®ç°äº†å¯å‘å¼ç”Ÿæˆè¿‡ç¨‹çš„åŠ¨æ€æ”¹è¿›ã€‚CogMCTSé€šè¿‡åŒè½¨èŠ‚ç‚¹æ‰©å±•(Dual-track node expansion)ä¸ç²¾è‹±å¯å‘å¼ç®¡ç†æœºåˆ¶ï¼Œæœ‰æ•ˆå¹³è¡¡äº†å¤šæ ·åŒ–æ¢ç´¢(exploration)ä¸é«˜è´¨é‡åˆ©ç”¨(exploitation)ä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥ç­–ç•¥æ€§å˜å¼‚(Strategic mutation)æ¥ä¿®æ”¹å¯å‘å¼çš„å½¢å¼ä¸å‚æ•°ï¼Œè¿›ä¸€æ­¥æå‡äº†è§£çš„å¤šæ ·æ€§å’Œæ•´ä½“ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCogMCTSåœ¨ç¨³å®šæ€§ã€æ•ˆç‡å’Œæ±‚è§£è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„LLM-based AHDæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08609v2",
      "published_date": "2025-12-09 13:54:18 UTC",
      "updated_date": "2025-12-11 08:46:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:05:53.588769+00:00"
    },
    {
      "arxiv_id": "2512.08606v2",
      "title": "Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning",
      "title_zh": "è§£è€¦ CLIP ä¸­çš„æ¨¡æ¿åå·®ï¼šåˆ©ç”¨ç©ºæç¤ºå¢å¼ºå°æ ·æœ¬å­¦ä¹ ",
      "authors": [
        "Zhenyu Zhang",
        "Guangyao Chen",
        "Yixiong Zou",
        "Zhimeng Huang",
        "Yuhua Li"
      ],
      "abstract": "The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of \"emptiness\" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº† Contrastive Language-Image Pre-Training (CLIP) æ¨¡å‹åœ¨ few-shot learning ä»»åŠ¡ä¸­æ™®éå­˜åœ¨çš„ template-sample similarity (TSS) åå·®é—®é¢˜ï¼Œå³æ¨¡å‹å€¾å‘äºæ ¹æ®æ–‡æœ¬æ¨¡æ¿ä¸å›¾åƒçš„ç›¸ä¼¼æ€§è€ŒéçœŸå®çš„ç±»åˆ«é€»è¾‘è¿›è¡Œé¢„æµ‹ï¼Œè¿™ä¸¥é‡æŸå®³äº†åˆ†ç±»çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨ empty prompts çš„æ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡è¿™äº›ä¸åŒ…å«ç±»åˆ«ä¿¡æ¯çš„å ä½æ–‡æœ¬æ¥ç²¾å‡†æ•æ‰å¹¶æŠµæ¶ˆæ¨¡æ¿åå·®ã€‚è¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µåˆ©ç”¨ empty prompts è¯†åˆ«å¹¶å‡å°‘ CLIP ç¼–ç å™¨å†…éƒ¨çš„è¯±å¯¼åè§ï¼Œéšååœ¨ few-shot fine-tuning è¿‡ç¨‹ä¸­å¼•å…¥ bias calibration loss ä»¥å¼•å¯¼æ¨¡å‹èšç„¦äºæ ¸å¿ƒè§†è§‰ç‰¹å¾ã€‚å¤šé¡¹åŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡æ¿çº æ­£ç­–ç•¥èƒ½æ˜¾è‘—æŠ‘åˆ¶ TSS å¸¦æ¥çš„æ€§èƒ½æ³¢åŠ¨ï¼Œå¹¶åœ¨æå‡åˆ†ç±»ç²¾åº¦çš„åŒæ—¶å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹çš„ä¸‹æ¸¸è¿ç§»èƒ½åŠ›æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 8 figures, Association for the Advancement of Artificial Intelligence (AAAI2026, poster)",
      "pdf_url": "https://arxiv.org/pdf/2512.08606v2",
      "published_date": "2025-12-09 13:51:05 UTC",
      "updated_date": "2025-12-10 03:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:12.991032+00:00"
    },
    {
      "arxiv_id": "2512.08596v1",
      "title": "Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality",
      "title_zh": "å­¦ç”Ÿä¸æ•™å­¦å‹ AI å†™ä½œåŠ©æ‰‹çš„äº’åŠ¨åŠå…¶å¯¹å†™ä½œè´¨é‡çš„å½±å“æ¢ç©¶",
      "authors": [
        "Wicaksono Febriantoro",
        "Qi Zhou",
        "Wannapon Suraworachet",
        "Sahan Bulathwela",
        "Andrea Gauthier",
        "Eva Millan",
        "Mutlu Cukurova"
      ],
      "abstract": "The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†ä¸€æ¬¾æ—¨åœ¨æ”¯æŒé«˜ç­‰æ•™è‚²å­¦ç”Ÿè®®è®ºæ–‡å†™ä½œçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è®ºæ–‡å†™ä½œåŠ©æ‰‹(EWA)ï¼Œæ¢è®¨äº†å­¦ç”Ÿä¸è¯¥ç³»ç»Ÿçš„äº’åŠ¨æ¨¡å¼åŠå…¶å¯¹å†™ä½œè´¨é‡çš„å½±å“ã€‚ç ”ç©¶äººå‘˜æ”¶é›†äº†32åæœ¬ç§‘ç”Ÿåœ¨ä¸¤å°æ—¶å†™ä½œè¿‡ç¨‹ä¸­çš„1,282æ¡äº¤äº’æ—¥å¿—ï¼Œå¹¶åˆ©ç”¨é¡ºåºæ¨¡å¼æŒ–æ˜(Sequential Pattern Mining)å’ŒK-Meansèšç±»ç®—æ³•è¯†åˆ«è¡Œä¸ºæ¨¡å¼ã€‚åˆ†æç»“æœæ­ç¤ºäº†ä¸¤ç±»å­¦ç”Ÿç¾¤ä½“ï¼šCluster 1 ä¾§é‡äºå¤§çº²è§„åˆ’å’Œè®ºæ–‡ç»“æ„ï¼Œè€Œ Cluster 2 åˆ™ä¸“æ³¨äºå…·ä½“å†…å®¹å¼€å‘ã€‚æ›¼-æƒ ç‰¹å°¼Uæ£€éªŒ(Mann-Whitney U test)æ˜¾ç¤ºï¼ŒCluster 1 åœ¨è®ºæ–‡ç»„ç»‡ç»“æ„(Organization)ç»´åº¦çš„è¯„åˆ†æ˜¾è‘—æ›´é«˜ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¡¨ç°ä¼˜å¼‚çš„å­¦ç”Ÿå¾€å¾€é‡‡å–ä¸»åŠ¨ç­–ç•¥ï¼Œé€šè¿‡å‘EWAåˆ†äº«å†™ä½œæ®µè½è·å–åé¦ˆï¼Œè€Œéä»…è¿›è¡Œè¢«åŠ¨çš„é—®ç­”äº’åŠ¨ã€‚è¿™äº›å‘ç°ä¸ºæ•™å­¦å’Œç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºï¼Œå»ºè®®æœªæ¥çš„EWAåº”é€šè¿‡ç›‘æ§å’Œå¼•å¯¼ï¼Œä¿ƒä½¿å­¦ç”Ÿä»å•çº¯æé—®è½¬å‘ç§¯æå†™ä½œï¼Œä»¥æå‡ç”Ÿæˆå¼AIè¾…åŠ©å­¦ä¹ çš„æ•ˆæœã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages, 3 Figures, 8 Tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08596v1",
      "published_date": "2025-12-09 13:34:33 UTC",
      "updated_date": "2025-12-09 13:34:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:09.939248+00:00"
    },
    {
      "arxiv_id": "2512.08592v1",
      "title": "The SMART+ Framework for AI Systems",
      "title_zh": "é¢å‘äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ SMART+ æ¡†æ¶",
      "authors": [
        "Laxmiraju Kandikatla",
        "Branislav Radeljic"
      ],
      "abstract": "Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ç³»ç»Ÿåœ¨ä¸´åºŠç ”ç©¶ã€é‡‘èå’Œåˆ¶é€ ä¸šç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¹¶åˆ†æäº†éšä¹‹è€Œæ¥çš„å®‰å…¨ã€é—®è´£åˆ¶å’Œç›‘ç®¡åˆè§„æ€§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†SMART+ Frameworkï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è·¨è¡Œä¸šè¯„ä¼°å’Œæ²»ç†AIç³»ç»Ÿçš„ç»“æ„åŒ–æ¨¡å‹ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨Safetyï¼ˆå®‰å…¨ï¼‰ã€Monitoringï¼ˆç›‘æ§ï¼‰ã€Accountabilityï¼ˆé—®è´£ï¼‰ã€Reliabilityï¼ˆå¯é ï¼‰å’ŒTransparencyï¼ˆé€æ˜ï¼‰äº”å¤§æ”¯æŸ±ä¹‹ä¸Šï¼Œå¹¶è¿›ä¸€æ­¥é›†æˆäº†Privacy & Securityï¼ˆéšç§ä¸å®‰å…¨ï¼‰ã€Data Governanceï¼ˆæ•°æ®æ²»ç†ï¼‰ã€Fairness & Biasï¼ˆå…¬å¹³ä¸åè§ï¼‰ä»¥åŠGuardrailsï¼ˆæŠ¤æ ï¼‰ç­‰å…³é”®è¦ç´ ã€‚é€šè¿‡æ•´åˆæ“ä½œä¿éšœæªæ–½ã€ç›‘ç£ç¨‹åºå’Œå¼ºåŒ–çš„æ²»ç†æ§åˆ¶ï¼ŒSMART+ Frameworkèƒ½å¤Ÿä¸ä¸æ–­æ¼”è¿›çš„ç›‘ç®¡æŒ‡å—ä¿æŒä¸€è‡´ã€‚è¯¥æ¡†æ¶åœ¨é£é™©ç¼“è§£(risk mitigation)ã€å»ºç«‹ä¿¡ä»»å’Œåˆè§„å‡†å¤‡æ–¹é¢å±•ç°äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚æœ€ç»ˆï¼ŒSMART+ Frameworkä¸ºå®ç°è´Ÿè´£ä»»çš„AIé‡‡ç”¨å’Œç¡®ä¿ç³»ç»Ÿå¯å®¡è®¡æ€§æä¾›äº†åšå®çš„åŸºç¡€ï¼Œå°¤å…¶åœ¨ä¸´åºŠç ”ç©¶ç­‰é«˜ç›‘ç®¡é¢†åŸŸå…·æœ‰é‡è¦çš„å®è·µä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08592v1",
      "published_date": "2025-12-09 13:33:14 UTC",
      "updated_date": "2025-12-09 13:33:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:20.385764+00:00"
    },
    {
      "arxiv_id": "2512.08580v2",
      "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
      "title_zh": "Mind to Handï¼šåŸºäºå…·èº«æ¨ç†çš„æ„å›¾å¯¼å‘æœºå™¨äººæ§åˆ¶",
      "authors": [
        "Peijun Tang",
        "Shangjin Xie",
        "Binyan Sun",
        "Baifu Huang",
        "Kuncheng Luo",
        "Haotian Yang",
        "Weiqi Jin",
        "Jianan Wang"
      ],
      "abstract": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Lumo-1ï¼Œä¸€ä¸ªé€šç”¨çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ (Vision-Language-Action, VLA) æ¨¡å‹ï¼Œæ—¨åœ¨å°†æœºå™¨äººçš„æ¨ç†èƒ½åŠ›ï¼ˆâ€œå¤§è„‘â€ï¼‰ä¸ç‰©ç†åŠ¨ä½œï¼ˆâ€œæ‰‹â€ï¼‰æœ‰æœºç»Ÿä¸€ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œé€šè¿‡ä¸‰é˜¶æ®µé¢„è®­ç»ƒç®¡çº¿é€æ­¥å®ç°å…·èº«æ¨ç†ä¸åŠ¨ä½œé¢„æµ‹ï¼šé¦–å…ˆåœ¨ç²¾é€‰æ•°æ®ä¸Šå¢å¼ºè§„åˆ’å’Œç©ºé—´ç†è§£ç­‰èƒ½åŠ›ï¼Œéšåè¿›è¡Œè·¨å…·èº«æœºå™¨äººæ•°æ®ä¸è§†è§‰è¯­è¨€æ•°æ®çš„å…±åŒè®­ç»ƒï¼Œæœ€ååœ¨ Astribot S1 åŒè‡‚ç§»åŠ¨åä½œæœºå™¨äººä¸Šè¿›è¡Œç»“åˆæ¨ç†è¿‡ç¨‹çš„åŠ¨ä½œè®­ç»ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä»¥ä¼˜åŒ–æ¨ç†ä¸åŠ¨ä½œçš„ä¸€è‡´æ€§ï¼Œä»è€Œé—­åˆè¯­ä¹‰æ¨ç†ä¸ç”µæœºæ§åˆ¶ä¹‹é—´çš„ç¯è·¯ã€‚å®éªŒè¡¨æ˜ï¼ŒLumo-1 åœ¨å…·èº«è§†è§‰è¯­è¨€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå¹¶åœ¨ç°å®ä¸–ç•Œçš„è¯„ä¼°ä¸­è¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨é•¿ç¨‹ä»»åŠ¡ (long-horizon tasks) ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå¯¹æ–°ç‰©ä½“å’Œç¯å¢ƒå…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿç²¾å‡†å“åº”æ¶‰åŠç­–ç•¥ã€æ¦‚å¿µå’Œç©ºé—´æ¨ç†çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "49 pages, 25 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08580v2",
      "published_date": "2025-12-09 13:19:37 UTC",
      "updated_date": "2025-12-10 12:05:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:22.517335+00:00"
    },
    {
      "arxiv_id": "2512.08577v1",
      "title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery",
      "title_zh": "é¢å‘å¼€æ”¾æ‰‹æœ¯çš„åŸºäºå¤šæ‘„åƒå¤´æ— å½±ç¯çš„æ— å¹²æ‰°æ‰‹æœ¯è§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Yuna Kato",
        "Shohei Mori",
        "Hideo Saito",
        "Yoshifumi Takatsume",
        "Hiroki Kajita",
        "Mariko Isogawa"
      ],
      "abstract": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾æ‰‹æœ¯è§†é¢‘å½•åˆ¶ä¸­å› å¤–ç§‘åŒ»ç”Ÿé®æŒ¡åŠæ— å½±ç¯ä½ç½®å˜åŠ¨å¯¼è‡´çš„è§†çº¿å—é˜»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Multi-Camera Shadowless Lamps çš„å¹²æ‰°æ¶ˆé™¤è§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªåŠ¨åŒ–å¯¹é½ä»»åŠ¡ï¼Œèƒ½å¤Ÿè¯†åˆ«ç…§æ˜ç³»ç»Ÿç§»åŠ¨çš„å¸§å¹¶è¿›è¡Œæ ¡æ­£ï¼Œå¹¶ä»å¤šä¸ªæ‘„åƒå¤´ä¸­è‡ªåŠ¨é€‰æ‹© Occlusion æœ€å°‘çš„è§†è§’ã€‚åˆ©ç”¨è¿™ç§æŠ€æœ¯ï¼Œç³»ç»Ÿå¯ä»¥ç”Ÿæˆä¸€æ®µè§†è§’å›ºå®šã€ä¸”èƒ½æŒç»­æ¸…æ™°å‘ˆç°æ‰‹æœ¯åŒºåŸŸçš„è§†é¢‘ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç¹é‡çš„åæœŸäººå·¥å¯¹é½å·¥ä½œã€‚æ¶‰åŠå¤–ç§‘åŒ»ç”Ÿçš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§‚å¯Ÿæ‰‹æœ¯åŒºåŸŸçš„ä¾¿åˆ©æ€§å’Œè§‚çœ‹èˆ’é€‚åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯„ä¼°äº† View-synthesis æ–¹æ³•ä¸­çš„å¤šç§åˆæˆé€‰é¡¹ï¼Œä¸ºæé«˜æ‰‹æœ¯è§†é¢‘è´¨é‡å’ŒåŒ»ç”Ÿè§‚çœ‹ä½“éªŒæä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08577v1",
      "published_date": "2025-12-09 13:15:32 UTC",
      "updated_date": "2025-12-09 13:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:21.733835+00:00"
    },
    {
      "arxiv_id": "2512.08567v1",
      "title": "A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks",
      "title_zh": "èåˆæ–°é—»æƒ…ç»ªä¸æ—¶é—´åºåˆ—æ•°æ®çš„å›¾ç¥ç»ç½‘ç»œè‚¡ç¥¨å¸‚åœºé¢„æµ‹æ··åˆæ¨¡å‹",
      "authors": [
        "Nader Sadek",
        "Mirette Moawad",
        "Christina Naguib",
        "Mariam Elzahaby"
      ],
      "abstract": "Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ–°é—»æƒ…æ„Ÿä¸æ—¶é—´åºåˆ—æ•°æ®çš„æ··åˆè‚¡ç¥¨å¸‚åœºé¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤–éƒ¨ä¿¡å·æå‡é¢„æµ‹æ€§èƒ½ã€‚ç ”ç©¶è€…å°† Graph Neural Network (GNN) ä¸åŸºå‡† LSTM æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œåˆ©ç”¨ LSTM ç¼–ç å†å²è‚¡ä»·ï¼Œå¹¶é‡‡ç”¨è¯­è¨€æ¨¡å‹å¯¹æ–°é—»æ ‡é¢˜è¿›è¡Œå‘é‡åŒ–å¤„ç†ã€‚é€šè¿‡æ„å»ºå¼‚æ„å›¾å¹¶ä½¿ç”¨ GraphSAGE ç®—æ³•ï¼Œæ¨¡å‹æˆåŠŸæ•æ‰äº†æ–‡ç« ã€å…¬å¸åŠè¡Œä¸šä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚åœ¨ US equities å’Œ Bloomberg æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ GNN æ¨¡å‹åœ¨ä»·æ ¼æ–¹å‘é¢„æµ‹å’Œæ˜¾è‘—æ€§æ ‡ç­¾ä»»åŠ¡ä¸Šå‡ä¼˜äº LSTM åŸºå‡†ã€‚ç ”ç©¶ç»“æœè¿›ä¸€æ­¥æ­ç¤ºï¼Œå…³è”æ–°é—»è¾ƒå¤šçš„å…¬å¸å…·æœ‰æ›´é«˜çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œä¸”æ–°é—»æ ‡é¢˜æ¯”å…¨æ–‡åŒ…å«æ›´å¼ºçš„çŸ­æœŸå¸‚åœºé¢„æµ‹ä¿¡å·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 6 figures. Published in the Proceedings of the 5th International Conference on Artificial Intelligence Research (ICAIR 2025). Published version available at: https://papers.academic-conferences.org/index.php/icair/article/view/4294",
      "pdf_url": "https://arxiv.org/pdf/2512.08567v1",
      "published_date": "2025-12-09 13:05:54 UTC",
      "updated_date": "2025-12-09 13:05:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:43.850328+00:00"
    },
    {
      "arxiv_id": "2512.08548v1",
      "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations",
      "title_zh": "é€šè¿‡åŸºäºè¯­è¨€çš„åŠ¨ä½œè¡¨å¾å¼¥åˆæœºå™¨äººæ§åˆ¶ä¸­çš„å°ºåº¦å·®å¼‚",
      "authors": [
        "Yuchi Zhang",
        "Churui Sun",
        "Shiqi Liang",
        "Diyuan Liu",
        "Chao Ji",
        "Wei-Nan Zhang",
        "Ting Liu"
      ],
      "abstract": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººç«¯åˆ°ç«¯æ“ä½œä¸­ï¼Œç”±äºä¸åŒå¹³å°å’Œä»»åŠ¡é—´çš„åŠ¨ä½œæŒ‡ä»¤å­˜åœ¨å·¨å¤§æ•°å€¼å·®å¼‚è€Œå¯¼è‡´çš„ä¸¥é‡åˆ†å¸ƒåç§»(distribution shifts)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰çš„è¯­è¨€åŒ–åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç¦»æ•£åŒ–åŠ¨ä½œè¡¨ç¤ºä¸åŒï¼Œè¿™ç§è¿åŠ¨è¡¨ç¤º(motion representation)ä¸“é—¨å¿½ç•¥äº†æ•°å€¼æ¯”ä¾‹çš„å½±å“ï¼Œè½¬è€Œå¼ºè°ƒåŠ¨ä½œçš„æ–¹å‘æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼“è§£äº†åˆ†å¸ƒåç§»å¹¶å¢å¼ºäº†é¢„è®­ç»ƒè¡¨ç¤ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç¼©å°åŠ¨ä½œæ ‡è®°(action tokens)ä¸æ ‡å‡†è¯æ±‡æ ‡è®°(vocabulary tokens)ä¹‹é—´çš„æ¨¡æ€é—´éš™(modality gaps)ï¼Œè¿›ä¸€æ­¥æå‡äº†ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤šä»»åŠ¡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æ³›åŒ–æ€§èƒ½å’Œå¯è¿ç§»æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08548v1",
      "published_date": "2025-12-09 12:45:12 UTC",
      "updated_date": "2025-12-09 12:45:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:57.694367+00:00"
    },
    {
      "arxiv_id": "2512.08545v1",
      "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks",
      "title_zh": "è¯¾ç¨‹å¼•å¯¼çš„å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼šè§£å†³é²æ£’é•¿æ—¶ç¨‹ä»»åŠ¡",
      "authors": [
        "Indrajit Kar",
        "Kalathur Chenchu Kishore Kumar"
      ],
      "abstract": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é•¿æ—¶ç¨‹æ¨ç†ï¼ˆLong-Horizon Tasksï¼‰ä¸­é¢ä¸´çš„è®¡ç®—æˆæœ¬ä¸å¤æ‚æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç”±64x64è½»é‡çº§æ™ºèƒ½ä½“ç½‘æ ¼ç»„æˆçš„åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¶æ„ã€‚ä¸ºäº†æå‡å­¦ä¹ æ•ˆç‡ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨äº†ç©ºé—´è¯¾ç¨‹ï¼ˆSpatial Curriculumï¼‰ç­–ç•¥ï¼Œå¼•å¯¼æ™ºèƒ½ä½“ä»ç®€å•çš„ä¸­å¿ƒä»»åŠ¡é€æ­¥å‘å¤æ‚çš„è¾¹ç¼˜ä»»åŠ¡æ‰©å±•ã€‚ç ”ç©¶å¼•å…¥äº†è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihoodï¼‰ä½œä¸ºç½®ä¿¡åº¦è¯„ä¼°æ ‡å‡†ï¼Œå¹¶ç»“åˆæ±¤æ™®æ£®é‡‡æ ·ï¼ˆThompson Samplingï¼‰è¯¾ç¨‹ç®¡ç†å™¨æ¥åŠ¨æ€ä¼˜åŒ–è®­ç»ƒåŒºåŸŸçš„é€‰æ‹©ã€‚åœ¨æ²³å†…å¡”ï¼ˆTower of Hanoiï¼‰åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡å°‘é€‰æ‹©æ€§å…ˆéªŒï¼ˆSelective Oracleï¼‰ä½¿ç”¨çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¤šæ™ºèƒ½ä½“åä½œçš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 2 tables, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08545v1",
      "published_date": "2025-12-09 12:40:39 UTC",
      "updated_date": "2025-12-09 12:40:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:45.737607+00:00"
    },
    {
      "arxiv_id": "2512.08542v1",
      "title": "A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation",
      "title_zh": "ä¸€ç§ç”¨äºå½©è‰²å›¾åƒç”Ÿæˆçš„æ–°å‹ Wasserstein å››å…ƒæ•°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ",
      "authors": [
        "Zhigang Jia",
        "Duan Wang",
        "Hengkai Wang",
        "Yajun Xie",
        "Meixiang Zhao",
        "Xiaoyu Zhao"
      ],
      "abstract": "Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å½©è‰²å›¾åƒç”Ÿæˆæ¨¡å‹å› å¿½ç•¥é¢œè‰²é€šé“é—´ç›¸å…³æ€§è€Œå¯¼è‡´è‰²å·®ï¼Œä»¥åŠç¼ºä¹ç³»ç»Ÿè¡¡é‡å½©è‰²å›¾åƒæ•°æ®é›†åˆ†å¸ƒç†è®ºçš„é—®é¢˜ï¼Œå®šä¹‰äº†ä¸€ç§æ–°å‹çš„ quaternion Wasserstein distance å¹¶å»ºç«‹äº†å…¶å¯¹å¶ç†è®ºã€‚é€šè¿‡åˆ©ç”¨ quaternion convex set separation theorem å’Œ quaternion Farkas lemma å¯¼å‡ºäº†å¼ºå¯¹å¶å½¢å¼ï¼Œç ”ç©¶æˆåŠŸè§£å†³äº† quaternion linear programming é—®é¢˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ Wasserstein quaternion generative adversarial network (WQGAN) æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ•ˆç‡å’Œå›¾åƒè´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ GANã€quaternion GAN ä»¥åŠ WGANã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºå½©è‰²å›¾åƒæ•°æ®é›†çš„åˆ†å¸ƒåº¦é‡æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œä¹Ÿä¸ºæå‡å½©è‰²å›¾åƒç”Ÿæˆè´¨é‡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08542v1",
      "published_date": "2025-12-09 12:39:39 UTC",
      "updated_date": "2025-12-09 12:39:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:46.222068+00:00"
    },
    {
      "arxiv_id": "2512.08536v1",
      "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans",
      "title_zh": "Principles2Planï¼šå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„ä¼¦ç†åŸåˆ™å¯æ“ä½œåŒ–ä¸è§„åˆ’ç”Ÿæˆç³»ç»Ÿ",
      "authors": [
        "Tammy Zhong",
        "Yang Song",
        "Maurice Pagnucco"
      ],
      "abstract": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Principles2Planï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLM)å¼•å¯¼ï¼Œå°†ä¼¦ç†å‡†åˆ™è½¬åŒ–ä¸ºå¯æ‰§è¡Œè®¡åˆ’çš„äº¤äº’å¼ç ”ç©¶åŸå‹ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–è§„åˆ’å·¥å…·ç¼ºä¹ä¼¦ç†æ„è¯†æ”¯æŒï¼Œä¸”æ‰‹åŠ¨æŒ‡å®šä¼¦ç†è§„åˆ™åŠ³åŠ¨å¼ºåº¦å¤§ã€å…·æœ‰å¼ºèƒŒæ™¯ç›¸å…³æ€§çš„é—®é¢˜ã€‚Principles2Planå…è®¸é¢†åŸŸä¸“å®¶æä¾›è§„åˆ’é¢†åŸŸã€é—®é¢˜ç»†èŠ‚ä»¥åŠå—ç›Š(beneficence)å’Œéšç§(privacy)ç­‰é«˜å±‚ä¼¦ç†å‡†åˆ™ï¼Œå¹¶åˆ©ç”¨LLMç”Ÿæˆä¸è¿™äº›å‡†åˆ™ä¸€è‡´çš„ã€å¯æ“ä½œçš„ä¼¦ç†è§„åˆ™ã€‚ç”¨æˆ·éšåå¯ä»¥å®¡æŸ¥å¹¶ç¡®å®šè¿™äº›è§„åˆ™çš„ä¼˜å…ˆçº§ï¼Œå¹¶å°†å…¶æä¾›ç»™è§„åˆ’å™¨ä»¥ç”Ÿæˆå…·æœ‰ä¼¦ç†æ„è¯†çš„è®¡åˆ’ã€‚ä½œä¸ºé¦–ä¸ªåœ¨ç»å…¸è§„åˆ’(classical planning)èƒŒæ™¯ä¸‹æ”¯æŒç”ŸæˆåŸºäºå‡†åˆ™çš„è§„åˆ™çš„ç³»ç»Ÿï¼ŒPrinciples2Planå±•ç¤ºäº†äººç±»ä¸å¤§è¯­è¨€æ¨¡å‹åä½œåœ¨ä½¿ä¼¦ç†è‡ªåŠ¨åŒ–è§„åˆ’æ›´å…·å®è·µæ€§å’Œå¯è¡Œæ€§æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.08536v1",
      "published_date": "2025-12-09 12:34:54 UTC",
      "updated_date": "2025-12-09 12:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:06:47.754315+00:00"
    },
    {
      "arxiv_id": "2512.13716v1",
      "title": "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
      "title_zh": "ValuePilotï¼šä»·å€¼é©±åŠ¨å†³ç­–çš„ä¸¤é˜¶æ®µæ¡†æ¶",
      "authors": [
        "Yitong Luo",
        "Ziang Chen",
        "Hou Hei Lam",
        "Jiayu zhan",
        "Junqi Wang",
        "Zhenliang Zhang",
        "Xue Feng"
      ],
      "abstract": "Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ValuePilotï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°ä»·å€¼é©±åŠ¨ä¸ªæ€§åŒ–å†³ç­–(Value-Driven Decision-Making)çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä»è€Œä½¿ AI æ™ºèƒ½ä½“èƒ½æ›´å¥½åœ°ä¸ä¸ªäººç”¨æˆ·çš„ä»·å€¼åå¥½å¯¹é½ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäººç±»ä»·å€¼è§‚ä½œä¸ºç¨³å®šä¸”å¯è¿ç§»çš„ä¿¡å·ï¼Œèƒ½æ˜¾è‘—å¢å¼ºæ™ºèƒ½ä½“åœ¨å¤æ‚è¯­å¢ƒä¸‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä½¿å…¶åœ¨é¢å¯¹æ–°åœºæ™¯æ—¶è¡¨ç°æ›´ä½³ã€‚ValuePilot æ¡†æ¶ç”±æ•°æ®é›†ç”Ÿæˆå·¥å…·åŒ…(Dataset Generation Toolkit, DGT)å’Œå†³ç­–æ¨¡å—(Decision-Making Module, DMM)ç»„æˆï¼Œåˆ†åˆ«è´Ÿè´£æ„å»ºå¤šæ ·åŒ–çš„ä»·å€¼æ ‡æ³¨åœºæ™¯å’Œå­¦ä¹ åŸºäºä¸ªäººä»·å€¼åå¥½çš„è¡ŒåŠ¨è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMM åœ¨æœªè§åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äº GPT-5ã€Claude-Sonnet-4ã€Gemini-2-flash å’Œ Llama-3.1-70b ç­‰å¤§è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œèƒ½æ›´ç²¾å‡†åœ°å¯¹é½äººç±»çš„è¡ŒåŠ¨é€‰æ‹©ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»·å€¼é©±åŠ¨çš„æ–¹æ³•æ˜¯å¼€å‘å¯è§£é‡Šä¸”ä¸ªæ€§åŒ– AI æ™ºèƒ½ä½“çš„ä¸€æ¡æœ‰æ•ˆä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„å·¥ç¨‹è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at LAW Workshop, NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.13716v1",
      "published_date": "2025-12-09 12:15:46 UTC",
      "updated_date": "2025-12-09 12:15:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:07:00.816197+00:00"
    },
    {
      "arxiv_id": "2512.09006v1",
      "title": "Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning",
      "title_zh": "åŸºäº Llama çš„æºä»£ç æ¼æ´æ£€æµ‹ï¼šæç¤ºå·¥ç¨‹ä¸å¾®è°ƒçš„æ¯”è¾ƒ",
      "authors": [
        "Dyna Soumhane Ouchebara",
        "StÃ©phane Dupont"
      ],
      "abstract": "The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æºä»£ç æ¼æ´æ£€æµ‹(CVD)ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œé‡ç‚¹å¯¹æ¯”äº†æç¤ºå·¥ç¨‹(Prompt Engineering)ä¸å¾®è°ƒ(Fine-tuning)çš„æ•ˆèƒ½ã€‚ç ”ç©¶äººå‘˜åŸºäº Llama-3.1 8B æ¨¡å‹ï¼Œåˆ©ç”¨ BigVul å’Œ PrimeVul æ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ç§åä¸ºåŒé‡å¾®è°ƒ(Double Fine-tuning)çš„æ–°æ–¹æ³•ï¼Œå¹¶æµ‹è¯•äº†æµ‹è¯•æ—¶å¾®è°ƒ(Test-Time fine-tuning)ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒå¯¹äºæˆåŠŸæ‰§è¡Œæ¼æ´æ£€æµ‹ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œçº¯ç²¹çš„æç¤ºå·¥ç¨‹åœ¨è¯¥é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä½œä¸ºä¸€ç§ç¤ºä¾‹é€‰æ‹©æŠ€æœ¯å±•ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚è¯¥å·¥ä½œä¸ä»…è¯æ˜äº† Llama æ¨¡å‹åœ¨ CVD é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œä¹Ÿé€šè¿‡åŒé‡å¾®è°ƒæŠ€æœ¯ä¸ºæå‡æ¨¡å‹æ£€æµ‹å‡†ç¡®ç‡æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "20 pages, Accepted at ESORICS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.09006v1",
      "published_date": "2025-12-09 12:08:24 UTC",
      "updated_date": "2025-12-09 12:08:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:07:01.759799+00:00"
    },
    {
      "arxiv_id": "2512.08518v2",
      "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking",
      "title_zh": "SensHRPSï¼šåŸºäºçœ¼åŠ¨è¿½è¸ªçš„èˆ’é€‚äººæœºè¿‘èº«è·ç¦»ä¸ä¸ªäººç©ºé—´æ„ŸçŸ¥",
      "authors": [
        "Nadezhda Kushina",
        "Ko Watanabe",
        "Aarthi Kannan",
        "Ashita Ashok",
        "Andreas Dengel",
        "Karsten Berns"
      ],
      "abstract": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äººæœºäº¤äº’(Human-Robot Interaction, HRI)ä¸­ï¼Œå¦‚ä½•åˆ©ç”¨çœ¼åŠ¨è¿½è¸ª(Eye-tracking)æŠ€æœ¯æ„ŸçŸ¥äººç±»çš„èˆ’é€‚è·ç¦»(Proxemics)å’Œä¸ªäººç©ºé—´ã€‚å®éªŒé€šè¿‡ç§»åŠ¨ç«¯çœ¼åŠ¨è¿½è¸ªè®¾å¤‡å’Œä¸»è§‚è¯„åˆ†ï¼Œè®°å½•äº†å‚ä¸è€…åœ¨0.5ç±³è‡³2.0ç±³å››ä¸ªå—æ§è·ç¦»ä¸‹ä¸äººå½¢æœºå™¨äºº\"Ameca\"äº’åŠ¨çš„ååº”ã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå‘ç°Decision Treeåˆ†ç±»å™¨åœ¨ä¼°è®¡èˆ’é€‚åº¦æ–¹é¢è¡¨ç°æœ€ä½³(F1-score = 0.73)ï¼Œå…¶ä¸­æœ€å°ç³å­”ç›´å¾„(Minimum pupil diameter)è¢«ç¡®å®šä¸ºæœ€é‡è¦çš„é¢„æµ‹æŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäººæœºäº¤äº’ä¸­çš„ç”Ÿç†èˆ’é€‚é˜ˆå€¼ä¸äººä¸äººä¹‹é—´çš„äº’åŠ¨åŠ¨æ€å­˜åœ¨å·®å¼‚ï¼Œä¸”å¯ä»¥é€šè¿‡å…·æœ‰å¯è§£é‡Šæ€§çš„é€»è¾‘æ¨¡å‹å®ç°æœ‰æ•ˆé¢„æµ‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08518v2",
      "published_date": "2025-12-09 12:08:21 UTC",
      "updated_date": "2025-12-10 11:46:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:07:03.684425+00:00"
    },
    {
      "arxiv_id": "2512.08512v2",
      "title": "A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Autonomous Air Vehicles",
      "title_zh": "é¢å‘æ— äººé£è¡Œå™¨é”‚ç¦»å­ç”µæ± çš„è½»é‡åŒ–è¿ç§»å­¦ä¹ å¥åº·çŠ¶æ€ç›‘æµ‹",
      "authors": [
        "Jiang Liu",
        "Yan Qin",
        "Wei Dai",
        "Chau Yuen"
      ],
      "abstract": "Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic autonomous air vehicles (AAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸»é£è¡Œå™¨(AAV)é”‚ç¦»å­ç”µæ± å¥åº·çŠ¶æ€(SOH)ç›‘æµ‹åœ¨ä¾¿æºå¼è®¾å¤‡ä¸­é¢ä¸´çš„è®¡ç®—èµ„æºå—é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ„é€ æ€§å¢é‡è¿ç§»å­¦ä¹ (CITL)çš„è½»é‡çº§ç›‘æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›®æ ‡åŸŸçš„æ— æ ‡ç­¾æ•°æ®ï¼Œé€šè¿‡åœ¨CITLä¸­è¿­ä»£å¢åŠ ç½‘ç»œèŠ‚ç‚¹æ„å»ºåŠç›‘ç£è¿ç§»å­¦ä¹ (TL)æœºåˆ¶ï¼Œä»è€Œä»¥æ„é€ æ€§çš„æ–¹å¼æœ€å°åŒ–ç›‘æµ‹æ®‹å·®ã€‚ç ”ç©¶ä»ç»“æ„é£é™©æœ€å°åŒ–ã€è¿ç§»å¤±é…æœ€å°åŒ–å’Œæµå½¢ä¸€è‡´æ€§æœ€å¤§åŒ–ä¸‰ä¸ªç»´åº¦ä¿éšœäº†èŠ‚ç‚¹çš„è·¨åŸŸå­¦ä¹ èƒ½åŠ›ï¼Œå¹¶ç»™å‡ºäº†ç†è®ºæ”¶æ•›æ€§è¯æ˜ä»¥ç¡®ä¿ç®—æ³•çš„æ•ˆèƒ½ä¸ç½‘ç»œç´§å‡‘æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCITLåœ¨çœŸå®çš„AAVç”µæ± æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºSS-TCAã€MMD-LSTM-DAã€DDANã€BO-CNN-TLå’ŒAS$^3$LSTMç­‰åŸºå‡†æ¨¡å‹ï¼Œåœ¨å‡æ–¹æ ¹è¯¯å·®(RMSE)æŒ‡æ ‡ä¸Šæœ€é«˜æå‡äº†87.70%ã€‚è¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™æ¡ä»¶ä¸‹çš„åŠ¨åŠ›ç”µæ± å¥åº·çŠ¶æ€å®æ—¶ç›‘æµ‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡ç†è®ºæ”¯æ’‘çš„è½»é‡åŒ–è¿ç§»å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "in IEEE Transactions on Industrial Informatics,2025",
      "pdf_url": "https://arxiv.org/pdf/2512.08512v2",
      "published_date": "2025-12-09 11:54:09 UTC",
      "updated_date": "2025-12-14 12:30:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:07:20.512506+00:00"
    },
    {
      "arxiv_id": "2512.08503v1",
      "title": "Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models",
      "title_zh": "ç“¦è§£å±‚çº§æ¨ç†ï¼šå¤šæ¨¡æ€æ¨ç†æ¨¡å‹åœ°ç†éšç§çš„å¯¹æŠ—æ€§ä¿æŠ¤",
      "authors": [
        "Jiaming Zhang",
        "Che Wang",
        "Yang Cao",
        "Longtao Huang",
        "Wei Yang Bryan Lim"
      ],
      "abstract": "Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \\textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \\textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\\% improvement in tract-level protection (33.8\\% vs 19.4\\%) and nearly doubling block-level protection (33.5\\% vs 16.8\\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹ (Multi-modal large reasoning models, MLRMs) é€šè¿‡å±‚æ¬¡åŒ–é“¾å¼æ€ç»´æ¨ç† (hierarchical chain-of-thought reasoning) ä»ä¸ªäººå›¾åƒä¸­æ¨æ–­ç²¾ç¡®åœ°ç†ä½ç½®æ‰€å¸¦æ¥çš„éšç§é£é™©ã€‚é’ˆå¯¹ç°æœ‰éšç§ä¿æŠ¤æŠ€æœ¯åœ¨å¯¹æŠ—å¤æ‚å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº† ReasonBreak å¯¹æŠ—æ€§æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ¦‚å¿µæ„ŸçŸ¥æ‰°åŠ¨ (concept-aware perturbations) ç ´åæ¨¡å‹æ¨ç†é€»è¾‘ã€‚è¯¥æ–¹æ³•é€šè¿‡å®šä½æ¨ç†é“¾ä¸­çš„å…³é”®æ¦‚å¿µä¾èµ–é¡¹ï¼Œäº§ç”Ÿèƒ½ä½¿ç‰¹å®šæ¨ç†æ­¥éª¤å¤±æ•ˆå¹¶äº§ç”Ÿçº§è”æ•ˆåº”çš„æ‰°åŠ¨ã€‚ç ”ç©¶åŒæ—¶è´¡çŒ®äº†åŒ…å« 6,341 å¼ è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåŠå±‚æ¬¡åŒ–æ¦‚å¿µæ ‡æ³¨çš„ GeoPrivacy-6K æ•°æ®é›†ã€‚åœ¨åŒ…æ‹¬ GPT-o3ã€GPT-5 å’Œ Gemini 2.5 Pro åœ¨å†…çš„ä¸ƒç§å‰æ²¿ MLRMs ä¸Šçš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒReasonBreak åœ¨åŒºåŸŸçº§ (tract-level) ä¿æŠ¤ä¸Šæå‡äº† 14.4%ï¼Œå¹¶åœ¨è¡—åŒºçº§ (block-level) ä¿æŠ¤ä¸Šå®ç°äº†æ•ˆæœç¿»å€ã€‚è¿™ä¸€æˆæœä¸ºåº”å¯¹åŸºäºæ¨ç†çš„æ–°å‹éšç§å¨èƒå»ºç«‹äº†å…¨æ–°çš„ä¿æŠ¤èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08503v1",
      "published_date": "2025-12-09 11:35:51 UTC",
      "updated_date": "2025-12-09 11:35:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:07:48.161657+00:00"
    },
    {
      "arxiv_id": "2512.14722v1",
      "title": "HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers",
      "title_zh": "HATSolverï¼šåŸºäºåˆ†å±‚æ³¨æ„åŠ› Transformer çš„ GrÃ¶bner åŸºå­¦ä¹ ",
      "authors": [
        "Mohamed Malhou",
        "Ludovic Perret",
        "Kristin Lauter"
      ],
      "abstract": "At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HATSolverï¼Œä¸€ç§åˆ©ç”¨ Hierarchical Attention Transformers (HATs) è®¡ç®— Groebner bases çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆæ±‚è§£ multivariate polynomial equations æå‡è®¡ç®—æœºä»£æ•°é¢†åŸŸçš„è®¡ç®—æ€§èƒ½ã€‚HAT æ¶æ„å¼•å…¥äº† tree-structured inductive biasï¼Œèƒ½å¤Ÿç²¾å‡†å»ºæ¨¡æ•°æ®ä¸­å­˜åœ¨çš„å±‚çº§å…³ç³»ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ flat attention models å®ç°äº†æ˜¾è‘—çš„è®¡ç®—èµ„æºèŠ‚çœã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥å°†è¯¥æ–¹æ³•æ¨å¹¿è‡³ä»»æ„æ·±åº¦ï¼Œå¹¶æä¾›äº†è¯¦å°½çš„è®¡ç®—æˆæœ¬åˆ†æä»¥éªŒè¯å…¶æ•ˆç‡ã€‚ç»“åˆ curriculum learning ç­–ç•¥ï¼ŒHATSolver æˆåŠŸè§£å†³äº†è§„æ¨¡è¿œè¶…å‰äººç ”ç©¶çš„è®¡ç®—å®ä¾‹ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å±‚çº§æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†å…·æœ‰å¤æ‚å†…åœ¨ç»“æ„çš„ä»£æ•°æ•°æ®æ—¶çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç¬¦å·è®¡ç®—çš„æ™ºèƒ½åŒ–æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14722v1",
      "published_date": "2025-12-09 11:34:28 UTC",
      "updated_date": "2025-12-09 11:34:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:16.633613+00:00"
    },
    {
      "arxiv_id": "2512.08499v2",
      "title": "Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation",
      "title_zh": "é¢å‘é²æ£’è½´æ‰¿é€€åŒ–ä¼°è®¡çš„åŠ¨æ€ç‰©ç†çº¦æŸç¥ç»ç½‘ç»œä¸­è·ç¦»æ„ŸçŸ¥ä¸è¯æ®ä¸ç¡®å®šæ€§é‡åŒ–å¼€å‘",
      "authors": [
        "Waleed Razzaq",
        "Yun-Bo Zhao"
      ],
      "abstract": "Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA, XJTU-SY and HUST datasets and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—‹è½¬æœºæ¢°è½´æ‰¿é€€åŒ–è¯„ä¼°ä¸­ç°æœ‰ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ç¼ºä¹ç½®ä¿¡åº¦æ ¡å‡†ã€ä¸å…·å¤‡è·ç¦»æ„ŸçŸ¥èƒ½åŠ›ä¸”åœ¨åˆ†å¸ƒå¤–(OOD)æ•°æ®ä¸‹æ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§å…·æœ‰è·ç¦»æ„ŸçŸ¥èƒ½åŠ›çš„ç‰©ç†å¯¼å‘ç¥ç»ç½‘ç»œ(PGNN)æ–¹æ³•ã€‚å…¶ä¸­PG-SNGPåŸºäºå…‰è°±å½’ä¸€åŒ–é«˜æ–¯è¿‡ç¨‹(Spectral Normalization Gaussian Process)ï¼Œé€šè¿‡åœ¨éšè—å±‚åº”ç”¨å…‰è°±å½’ä¸€åŒ–å¹¶ç»“åˆé«˜æ–¯è¿‡ç¨‹å±‚æ¥å®ç°è·ç¦»æ•æ„Ÿçš„ä¸ç¡®å®šæ€§ï¼›PG-SNERåˆ™åŸºäºæ·±åº¦è¯æ®å›å½’(Deep Evidential Regression)ï¼Œé€šè¿‡è¾“å‡ºæ­£æ€é€†ä¼½é©¬(Normal Inverse Gamma)å‚æ•°æ¥å»ºæ¨¡ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†å¹³è¡¡æ•°æ®ä¿çœŸåº¦ä¸ç‰©ç†ä¸€è‡´æ€§ï¼Œç ”ç©¶ä¸­è¿˜è®¾è®¡äº†ä¸€ç§åŠ¨æ€åŠ æƒæŸå¤±å‡½æ•°æ–¹æ¡ˆã€‚é€šè¿‡åœ¨PRONOSTIAã€XJTU-SYå’ŒHUSTæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒPG-SNGPå’ŒPG-SNERåœ¨æé«˜é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹å¯¹æŠ—æ”»å‡»å’Œå™ªå£°ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–æ¡ä»¶ä¸‹å±•ç°å‡ºå¯é çš„æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºçš®å°”é€Šç›¸å…³ç³»æ•°(Pearson Correlation Coefficient)çš„æ–°æŒ‡æ ‡ï¼Œè¯å®äº†æ‰€ææ–¹æ³•èƒ½å¤Ÿç²¾å‡†è¿½è¸ªé¢„æµ‹ä¸ç¡®å®šæ€§ä¸æµ‹è¯•æ ·æœ¬è·ç¦»ä¹‹é—´çš„å…³è”ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review at Structural health Monitoring - SAGE",
      "pdf_url": "https://arxiv.org/pdf/2512.08499v2",
      "published_date": "2025-12-09 11:30:41 UTC",
      "updated_date": "2025-12-18 18:26:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:09.245242+00:00"
    },
    {
      "arxiv_id": "2512.08493v1",
      "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¼æ´ä»£ç å¢å¼ºï¼šç”Ÿæˆè¿˜æ˜¯é‡æ„ï¼Ÿ",
      "authors": [
        "Dyna Soumhane Ouchebara",
        "StÃ©phane Dupont"
      ],
      "abstract": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¼æ´ä»£ç å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³æ¼æ´åº“æ•°æ®ä¸¥é‡ä¸å¹³è¡¡å¯¼è‡´æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨æ•ˆæœå—é™çš„é—®é¢˜ã€‚ç ”ç©¶å¯¹æ¯”äº†ä¸¤ç§æ ¸å¿ƒæ–¹æ³•ï¼šå—æ§ç”Ÿæˆ(Controlled Generation)æ–°çš„æ¼æ´æ ·æœ¬ï¼Œä»¥åŠå¯¹ç°æœ‰ä»£ç è¿›è¡Œä¿æŒè¯­ä¹‰çš„é‡æ„(Refactoring)ã€‚å®éªŒè¿‡ç¨‹ä¸­ä½¿ç”¨ Qwen2.5-Coder ç”Ÿæˆå¢å¼ºæ•°æ®ï¼Œå¹¶åœ¨ SVEN æ•°æ®é›†ä¸Šåˆ©ç”¨ CodeBERT åˆ†ç±»å™¨è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿä»¥ç®€å•çš„æµç¨‹ç”Ÿæˆé«˜è´¨é‡çš„å¢å¼ºæ•°æ®å¹¶æœ‰æ•ˆå¯ŒåŒ–æ¼æ´ä»£ç åº“ã€‚ç ”ç©¶æœ€ç»ˆå‘ç°ï¼Œç»“åˆç”Ÿæˆä¸é‡æ„çš„æ··åˆç­–ç•¥åœ¨æå‡æ¼æ´åˆ†ç±»å™¨çš„æ€§èƒ½æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, Submitted to ESAAN 2026, Under pier review",
      "pdf_url": "https://arxiv.org/pdf/2512.08493v1",
      "published_date": "2025-12-09 11:15:13 UTC",
      "updated_date": "2025-12-09 11:15:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:14.333606+00:00"
    },
    {
      "arxiv_id": "2512.08492v1",
      "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance",
      "title_zh": "Autonomous Issue Resolverï¼šè¿ˆå‘é›¶æ¥è§¦ä»£ç ç»´æŠ¤",
      "authors": [
        "Aliaksei Kaliutau"
      ],
      "abstract": "Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the \"Semantic Trap\" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Autonomous Issue Resolver (AIR)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°é›¶æ¥è§¦ä»£ç ç»´æŠ¤ (zero-touch code maintenance) çš„è‡ªæˆ‘æ”¹è¿›ç³»ç»Ÿï¼Œä¸“æ³¨äºè§£å†³å¤§è§„æ¨¡å­˜å‚¨åº“çº§åˆ«çš„è‡ªåŠ¨ç¨‹åºä¿®å¤ (Automated Program Repair, APR) æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœç°æœ‰æ–¹æ³•å› è¿‡åº¦ä¾èµ–æ§åˆ¶é€»è¾‘è€Œå¯¼è‡´çš„å¯¼èˆªå›°éš¾ï¼Œè®ºæ–‡æå‡ºäº†ä»ä»£ç å±æ€§å›¾ (Code Property Graphs, CPGs) å‘æ•°æ®è½¬æ¢å›¾ (Data Transformation Graph, DTG) çš„èŒƒå¼è½¬å˜ã€‚DTG é€šè¿‡å°†æ•°æ®çŠ¶æ€å»ºæ¨¡ä¸ºèŠ‚ç‚¹å¹¶å°†å‡½æ•°å»ºæ¨¡ä¸ºè¾¹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡æ•°æ®è°±ç³» (data lineage) è€Œéæ§åˆ¶æµæ¥è¿½è¸ªé€»è¾‘ç¼ºé™·ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¤šæ™ºèƒ½ä½“æ¡†æ¶ä¸ç¥ç»ç¬¦å·æ¨ç† (neuro-symbolic reasoning)ï¼Œæœ‰æ•ˆè§£å†³äº†ç°ä»£ç¼–ç æ™ºèƒ½ä½“ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿå­˜åœ¨çš„â€œè¯­ä¹‰é™·é˜±â€ (Semantic Trap) é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒAIR åœ¨ SWE-Verified åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 87.1% çš„è§£å†³ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¯æ‰©å±•é€»è¾‘ä¿®å¤æ–¹é¢çš„ä¼˜å¼‚æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç›´æ¥åº”å¯¹äº†å½“å‰ AI ä»£ç è¾…åŠ©å·¥å…·çš„å±€é™æ€§ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„è‡ªåŠ¨åŒ–è½¯ä»¶ç»´æŠ¤ä½“ç³»å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08492v1",
      "published_date": "2025-12-09 11:11:37 UTC",
      "updated_date": "2025-12-09 11:11:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:18.098753+00:00"
    },
    {
      "arxiv_id": "2512.08478v1",
      "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
      "title_zh": "Visionaryï¼šåŸºäº WebGPU é©±åŠ¨çš„é«˜æ–¯æ³¼æº…å¹³å°æ„å»ºçš„ä¸–ç•Œæ¨¡å‹è½½ä½“",
      "authors": [
        "Yuning Gong",
        "Yifei Liu",
        "Yifan Zhan",
        "Muyao Niu",
        "Xueying Li",
        "Yuanjun Liao",
        "Jiaming Chen",
        "Yuanyuan Gao",
        "Jiaqi Chen",
        "Minming Chen",
        "Li Zhou",
        "Yuning Zhang",
        "Wei Wang",
        "Xiaoqing Hou",
        "Huaxi Huang",
        "Shixiang Tang",
        "Le Ma",
        "Dingwen Zhang",
        "Xue Yang",
        "Junchi Yan",
        "Yanchi Zhang",
        "Yinqiang Zheng",
        "Xiao Sun",
        "Zhihang Zhong"
      ],
      "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†Visionaryï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºWebGPUé©±åŠ¨çš„é«˜æ•ˆã€ç½‘é¡µåŸç”ŸGaussian Splattingä¸ç½‘æ ¼å®æ—¶æ¸²æŸ“å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰3DGSæŸ¥çœ‹å™¨æ–¹æ¡ˆç¢ç‰‡åŒ–ã€éƒ¨ç½²æˆæœ¬é«˜ä¸”å¯¹åŠ¨æ€å†…å®¹æ”¯æŒæœ‰é™ç­‰é—®é¢˜ã€‚Visionaryé€šè¿‡é«˜æ•ˆçš„WebGPUæ¸²æŸ“å™¨ç»“åˆæ¯å¸§ONNXæ¨ç†ï¼Œå®ç°äº†åŠ¨æ€ç¥ç»å¤„ç†å¹¶æä¾›è½»é‡çº§çš„æµè§ˆå™¨ä½“éªŒã€‚è¯¥å¹³å°å¼•å…¥äº†æ ‡å‡†åŒ–çš„Gaussian Generatoråˆçº¦ï¼Œæ”¯æŒæ’ä»¶å¼ç®—æ³•åœ¨æ¯å¸§ç”Ÿæˆæˆ–æ›´æ–°Gaussianï¼Œå¹¶èƒ½åº”ç”¨å‰å‘ç”Ÿæˆå¼åå¤„ç†ã€‚å®éªŒè¯æ˜ï¼Œå‡­å€ŸåŸºäºGPUçš„åŸºå…ƒæ’åºæŠ€æœ¯ï¼ŒVisionaryåœ¨ç›¸åŒèµ„äº§ä¸‹çš„æ¸²æŸ“æ•ˆç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç½‘é¡µæŸ¥çœ‹å™¨ã€‚ç›®å‰è¯¥å¹³å°å·²æ”¯æŒMLP-based 3DGSã€4DGSã€neural avatarsåŠé£æ ¼è½¬æ¢ç½‘ç»œç­‰å¤šç§å˜ä½“ï¼Œå¹¶æä¾›ä¸three.jsæ— ç¼é›†æˆçš„TypeScript APIã€‚é€šè¿‡åœ¨æµè§ˆå™¨ç«¯ç»Ÿä¸€æ¨ç†ä¸æ¸²æŸ“ï¼ŒVisionaryæ˜¾è‘—é™ä½äº†3DGSç³»åˆ—æ–¹æ³•çš„å¤ç°ä¸éƒ¨ç½²é—¨æ§›ï¼Œæˆä¸ºäº†è¿æ¥é‡å»ºä¸ç”ŸæˆèŒƒå¼çš„é€šç”¨ä¸–ç•Œæ¨¡å‹è½½ä½“ï¼ˆWorld Model Carrierï¼‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://visionary-laboratory.github.io/visionary",
      "pdf_url": "https://arxiv.org/pdf/2512.08478v1",
      "published_date": "2025-12-09 10:54:58 UTC",
      "updated_date": "2025-12-09 10:54:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:14.446937+00:00"
    },
    {
      "arxiv_id": "2512.08477v1",
      "title": "ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention",
      "title_zh": "ContextDragï¼šåŸºäºä¸Šä¸‹æ–‡ä¿ç•™ Token æ³¨å…¥ä¸ä½ç½®ä¸€è‡´æ€§æ³¨æ„åŠ›çš„ç²¾ç¡®æ‹–æ‹½å¼å›¾åƒç¼–è¾‘",
      "authors": [
        "Huiguo He",
        "Pengyu Yan",
        "Ziqi Yi",
        "Weizhi Zhong",
        "Zheng Liu",
        "Yejun Tang",
        "Huan Yang",
        "Kun Gai",
        "Guanbin Li",
        "Lianwen Jin"
      ],
      "abstract": "Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ContextDragï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºæ‹–æ‹½(Drag-based)çš„å›¾åƒç¼–è¾‘èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å‚è€ƒå›¾åƒä¸Šä¸‹æ–‡ä¿¡æ¯å’Œä¿ç•™ç»†ç²’åº¦çº¹ç†ç»†èŠ‚æ–¹é¢çš„ä¸è¶³ã€‚ContextDragå……åˆ†åˆ©ç”¨äº†FLUX-Kontextç­‰ç¼–è¾‘æ¨¡å‹å¼ºå¤§çš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œé€šè¿‡å¼•å…¥VAEç¼–ç çš„å‚è€ƒå›¾åƒç‰¹å¾ï¼Œåœ¨æ— éœ€å¾®è°ƒ(Finetuning)æˆ–åæ¼”(Inversion)çš„æƒ…å†µä¸‹å®ç°äº†é«˜ä¿çœŸåº¦çš„ç¼–è¾‘ã€‚è®ºæ–‡å¼•å…¥äº†åˆ›æ–°çš„ä¸Šä¸‹æ–‡ä¿æŒä»¤ç‰Œæ³¨å…¥(Context-preserving Token Injection, CTI)æœºåˆ¶ï¼Œåˆ©ç”¨æ½œç©ºé—´åå‘æ˜ å°„(Latent-space Reverse Mapping, LRM)ç®—æ³•å°†æ— å™ªå£°çš„å‚è€ƒç‰¹å¾å‡†ç¡®æ³¨å…¥ç›®æ ‡ä½ç½®ï¼Œç¡®ä¿äº†ç²¾ç¡®çš„æ‹–æ‹½æ§åˆ¶ä»¥åŠè¯­ä¹‰ä¸çº¹ç†ç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒContextDragé‡‡ç”¨äº†ä½ç½®ä¸€è‡´æ³¨æ„åŠ›(Position-Consistent Attention, PCA)æœºåˆ¶ï¼Œé€šè¿‡å¯¹å‚è€ƒä»¤ç‰Œè¿›è¡Œä½ç½®é‡ç¼–ç å¹¶åº”ç”¨é‡å æ„ŸçŸ¥æ©ç ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†æ— å…³å‚è€ƒç‰¹å¾çš„å¹²æ‰°ã€‚åœ¨DragBench-SRå’ŒDragBench-DRæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘çš„è¿è´¯æ€§å’Œå¿ å®åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ‰€æœ‰SOTAæ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08477v1",
      "published_date": "2025-12-09 10:51:45 UTC",
      "updated_date": "2025-12-09 10:51:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:09:19.369875+00:00"
    },
    {
      "arxiv_id": "2512.08463v1",
      "title": "Using reinforcement learning to probe the role of feedback in skill acquisition",
      "title_zh": "åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¢ç©¶åé¦ˆåœ¨æŠ€èƒ½ä¹ å¾—ä¸­çš„ä½œç”¨",
      "authors": [
        "Antonio Terpin",
        "Raffaello D'Andrea"
      ],
      "abstract": "Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å°†é€šç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“ä¸æ—‹è½¬åœ†æŸ±ä½“ç‰©ç†ç³»ç»Ÿç›¸ç»“åˆï¼Œæ·±å…¥æ¢è®¨äº†åé¦ˆåœ¨æŠ€èƒ½è·å–(Skill Acquisition)è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚å®éªŒå‘ç°ï¼Œé«˜ç»´æµåœºåé¦ˆ(Flow Feedback)èƒ½ä½¿æ™ºèƒ½ä½“åœ¨æçŸ­çš„ç°å®äº¤äº’æ—¶é—´å†…å‘ç°é«˜æ€§èƒ½çš„é˜»åŠ›æ§åˆ¶ç­–ç•¥ã€‚ç„¶è€Œï¼Œä¸€æ—¦ç­–ç•¥ä¹ å¾—ï¼Œå³ä½¿åœ¨æ— åé¦ˆçš„æƒ…å†µä¸‹é‡æ”¾åŠ¨ä½œåºåˆ—ï¼Œä¹Ÿèƒ½è·å¾—å‡ ä¹ç›¸åŒçš„è¡¨ç°ï¼Œè¡¨æ˜åé¦ˆå¯¹äºæ‰§è¡Œå·²å­¦æŠ€èƒ½å¹¶éå¿…è¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œåœ¨è®­ç»ƒé˜¶æ®µç¼ºå¤±æµåœºåé¦ˆä¼šå¯¼è‡´æ™ºèƒ½ä½“æ— æ³•å‘ç°é˜»åŠ›æœ€å¤§åŒ–çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½†åœ¨é˜»åŠ›æœ€å°åŒ–ä»»åŠ¡ä¸­ä»èƒ½å–å¾—æˆåŠŸã€‚è¿™è¯æ˜äº†å­¦ä¹ é«˜æ€§èƒ½æŠ€èƒ½å¾€å¾€éœ€è¦æ¯”æ‰§è¡ŒæŠ€èƒ½æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä¸”å­¦ä¹ æ¡ä»¶çš„éš¾æ˜“ç¨‹åº¦ï¼ˆKind or Wickedï¼‰ä¸»è¦å–å†³äºç‰¹å®šç›®æ ‡ï¼Œè€Œéç³»ç»ŸåŠ¨åŠ›å­¦æˆ–ç­–ç•¥çš„å¤æ‚åº¦ã€‚è¯¥æˆæœä¸ºç†è§£äººç±»åœ¨é«˜ç‰©ç†æŒ‘æˆ˜ä»»åŠ¡ä¸­çš„æŠ€èƒ½ä¹ å¾—æœºåˆ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Website: https://antonioterpin.com/fluids-control",
      "pdf_url": "https://arxiv.org/pdf/2512.08463v1",
      "published_date": "2025-12-09 10:37:42 UTC",
      "updated_date": "2025-12-09 10:37:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:26.159151+00:00"
    },
    {
      "arxiv_id": "2512.08459v1",
      "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset",
      "title_zh": "ç”¨äºè¯„ä¼°å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹çš„ç”Ÿç‰©å¨èƒåŸºå‡†ç”Ÿæˆæ¡†æ¶ IIIï¼šç»†èŒç”Ÿç‰©å¨èƒåŸºå‡† (B3) æ•°æ®é›†çš„å®ç°",
      "authors": [
        "Gary Ackerman",
        "Theodore Wilson",
        "Zachary Kallenborn",
        "Olivia Shoemaker",
        "Anna Wetzel",
        "Hayley Peterson",
        "Abigail Danfora",
        "Jenna LaTourette",
        "Brandon Behlendorf",
        "Douglas Clifford"
      ],
      "abstract": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ç»†èŒç”Ÿç‰©å¨èƒåŸºå‡†ï¼ˆBacterial Biothreat Benchmark, B3ï¼‰æ•°æ®é›†çš„è¯•ç‚¹å®æ–½ï¼Œè¿™æ˜¯ç”Ÿç‰©å¨èƒåŸºå‡†ç”Ÿæˆï¼ˆBiothreat Benchmark Generation, BBGï¼‰æ¡†æ¶ç³»åˆ—ç ”ç©¶çš„ç¬¬ä¸‰éƒ¨åˆ†ã€‚é’ˆå¯¹å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯èƒ½åŠ©é•¿ç”Ÿç‰©ææ€–ä¸»ä¹‰çš„é£é™©ï¼Œè¯¥ç ”ç©¶é€šè¿‡åœ¨å‰æ²¿AIæ¨¡å‹ä¸Šè¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç»“åˆä¸“å®¶çš„äººå·¥è¯„ä¼°åŠå¤šç»´åº¦é£é™©åˆ†æï¼Œå¯¹æ¨¡å‹ç”Ÿæˆçš„å“åº”è¿›è¡Œäº†é‡åŒ–è¯„ä¼°ã€‚è¯•ç‚¹ç»“æœè¯æ˜ï¼ŒB3æ•°æ®é›†ä¸ºå¿«é€Ÿè¯„ä¼°LLMsçš„ç”Ÿç‰©å®‰å…¨é£é™©æä¾›äº†ä¸€ç§å¯è¡Œä¸”ç»†è‡´çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«é£é™©çš„ä¸»è¦æ¥æºã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½å¤Ÿè¯„ä¼°ç‰¹å®šæ¨¡å‹çš„å®‰å…¨æ°´å¹³ï¼Œè¿˜ä¸ºåˆ¶å®šä¼˜å…ˆç¼“è§£ç­–ç•¥æä¾›äº†å…·ä½“çš„æŒ‡å¯¼æ–¹å‘ï¼Œæœ‰åŠ©äºåº”å¯¹å¿«é€Ÿæ¼”è¿›çš„AIæŠ€æœ¯å¸¦æ¥çš„ç”Ÿç‰©å®‰å…¨æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08459v1",
      "published_date": "2025-12-09 10:31:02 UTC",
      "updated_date": "2025-12-09 10:31:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:31.220209+00:00"
    },
    {
      "arxiv_id": "2512.08451v1",
      "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process",
      "title_zh": "è¯„ä¼°å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹çš„ç”Ÿç‰©å¨èƒåŸºå‡†ç”Ÿæˆæ¡†æ¶ IIï¼šåŸºå‡†ç”Ÿæˆæµç¨‹",
      "authors": [
        "Gary Ackerman",
        "Zachary Kallenborn",
        "Anna Wetzel",
        "Hayley Peterson",
        "Jenna LaTourette",
        "Olivia Shoemaker",
        "Brandon Behlendorf",
        "Sheriff Almakki",
        "Doug Clifford",
        "Noah Sheinbaum"
      ],
      "abstract": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‰æ²¿äººå·¥æ™ºèƒ½(AI)æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹LLMsï¼‰å¯èƒ½è¾…åŠ©ç”Ÿç‰©ææ€–ä¸»ä¹‰çš„é£é™©ï¼Œæå‡ºäº†ç”Ÿç‰©å¨èƒåŸºå‡†ç”Ÿæˆ(Biothreat Benchmark Generation, BBG)æ¡†æ¶ä¸‹çš„ç»†èŒç”Ÿç‰©å¨èƒåŸºå‡†(Bacterial Biothreat Benchmark, B3)æ•°æ®é›†ç”Ÿæˆæµç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åŸºäºWebçš„æç¤ºè¯ç”Ÿæˆ(web-based prompt generation)ã€çº¢é˜Ÿæµ‹è¯•(red teaming)å’Œç°æœ‰è¯­æ–™åº“æŒ–æ˜(mining existing benchmark corpora)ä¸‰ç§äº’è¡¥æ–¹å¼ï¼Œåˆæ­¥ç”Ÿæˆäº†7,000å¤šä¸ªä¸ä»»åŠ¡-æŸ¥è¯¢æ¶æ„(Task-Query Architecture)å…³è”çš„æ½œåœ¨åŸºå‡†é¢˜ç›®ã€‚ç»è¿‡ä¸¥æ ¼çš„å»é‡ã€æå‡è¯Šæ–­æ€§(uplift diagnosticity)è¯„ä¼°åŠè´¨é‡æ§åˆ¶æªæ–½ï¼Œæœ€ç»ˆç­›é€‰å‡º1,010ä¸ªæ ¸å¿ƒåŸºå‡†é¡¹ã€‚è¿™äº›åŸºå‡†ç¡®ä¿äº†åœ¨æä¾›çŸ¥è¯†å¢ç›Š(uplift)æ–¹é¢å…·æœ‰è¯Šæ–­æ€§ï¼Œä¸”ä¸ç”Ÿç‰©å®‰å…¨å¨èƒç›´æ¥ç›¸å…³ã€‚è¯¥æˆæœä¸æ›´å¹¿æ³›çš„ç”Ÿç‰©å®‰å…¨æ¶æ„ä¿æŒä¸€è‡´ï¼Œæ”¯æŒåœ¨ä¸åŒå±‚é¢è¿›è¡Œç»†è‡´çš„é£é™©åˆ†æï¼Œä¸ºæ¨¡å‹å¼€å‘è€…å’Œæ”¿ç­–åˆ¶å®šè€…é‡åŒ–å¹¶å‡è½»AIå¸¦æ¥çš„ç”Ÿç‰©å®‰å…¨é£é™©å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08451v1",
      "published_date": "2025-12-09 10:24:25 UTC",
      "updated_date": "2025-12-09 10:24:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:37.275641+00:00"
    },
    {
      "arxiv_id": "2512.08449v1",
      "title": "From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change",
      "title_zh": "ä»å‡†ç¡®æ€§åˆ°å½±å“åŠ›ï¼šå®ç°å·¥ç¨‹æ¶æ„ä¸å˜é©ç†è®ºå¯¹é½çš„å½±å“åŠ›é©±åŠ¨å‹äººå·¥æ™ºèƒ½æ¡†æ¶ (IDAIF)",
      "authors": [
        "Yong-Woon Kim"
      ],
      "abstract": "This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å½±å“é©±åŠ¨çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼ˆImpact-Driven AI Framework, IDAIFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å°†å˜é©ç†è®ºï¼ˆTheory of Change, ToCï¼‰åŸåˆ™ä¸ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡ç›¸ç»“åˆçš„æ–°å‹æ¶æ„æ–¹æ³•ã€‚é’ˆå¯¹å½“å‰AIå¼€å‘è¿‡åº¦å…³æ³¨æŠ€æœ¯æŒ‡æ ‡è€Œå¿½è§†ç¤¾ä¼šæŠ€æœ¯ç»´åº¦çš„é—®é¢˜ï¼ŒIDAIFå»ºç«‹äº†ToCçš„äº”ä¸ªé˜¶æ®µï¼ˆInputs-Activities-Outputs-Outcomes-Impactï¼‰ä¸ç›¸åº”AIæ¶æ„å±‚ï¼ˆData Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layerï¼‰ä¹‹é—´çš„ç³»ç»Ÿæ˜ å°„ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¤šç›®æ ‡Paretoä¼˜åŒ–ï¼ˆPareto optimizationï¼‰ã€åˆ†å±‚å¤šæ™ºèƒ½ä½“ç¼–æ’ï¼ˆmulti-agent orchestrationï¼‰ã€å› æœæœ‰å‘æ— ç¯å›¾ï¼ˆCausal DAGsï¼‰ä»¥åŠåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ä»·å€¼å¯¹é½ã€ç¼“è§£å¹»è§‰å¹¶ä¿è¯å…¬å¹³æ€§ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡å¼•å…¥ä¿éšœå±‚ï¼ˆAssurance Layerï¼‰æ¥ç®¡ç†å‡è®¾å¤±æ•ˆï¼Œè¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿçš„å¯é æ€§ã€‚é€šè¿‡åœ¨åŒ»ç–—ã€ç½‘ç»œå®‰å…¨å’Œè½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒIDAIFå®ç°äº†ä»ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒåˆ°ä»¥å½±å“ä¸ºä¸­å¿ƒçš„AIå¼€å‘èŒƒå¼è½¬å˜ï¼Œä¸ºæ„å»ºä¼¦ç†ã€å¯é ä¸”æœ‰ç›Šäºç¤¾ä¼šçš„AIç³»ç»Ÿæä¾›äº†å…·ä½“çš„æ¶æ„æ¨¡å¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08449v1",
      "published_date": "2025-12-09 10:21:02 UTC",
      "updated_date": "2025-12-09 10:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:08:35.279120+00:00"
    },
    {
      "arxiv_id": "2512.08411v1",
      "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems",
      "title_zh": "Prismatic ä¸–ç•Œæ¨¡å‹ï¼šå­¦ä¹ ç”¨äºæ··åˆç³»ç»Ÿè§„åˆ’çš„ç»„åˆå¼åŠ¨åŠ›å­¦",
      "authors": [
        "Mingwei Li",
        "Xiaoyuan Zhang",
        "Chengwei Yang",
        "Zilong Zheng",
        "Yaodong Yang"
      ],
      "abstract": "Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººé¢†åŸŸä¸­ç‰©ç†åŠ¨åŠ›å­¦çš„æ··åˆç‰¹æ€§ï¼ˆHybrid Dynamicsï¼‰å¸¦æ¥çš„è§„åˆ’æŒ‘æˆ˜ï¼Œæå‡ºäº† Prismatic World Model (PRISM-WM)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ½œç©ºé—´æ¨¡å‹å› è¿‡åº¦å¹³æ»‘ç¦»æ•£ç‰©ç†äº‹ä»¶è€Œå¯¼è‡´çš„é•¿ç¨‹è§„åˆ’è¯¯å·®ã€‚PRISM-WM é‡‡ç”¨ç»“æ„åŒ–çš„ä¸“å®¶æ··åˆï¼ˆMixture-of-Experts, MoEï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨é—¨æ§æœºåˆ¶éšå¼è¯†åˆ«å½“å‰çš„ç‰©ç†æ¨¡å¼ï¼ˆPhysical Modeï¼‰ï¼Œå¹¶ç”±ä¸“é—¨çš„ä¸“å®¶é¢„æµ‹ç›¸åº”çš„åŠ¨åŠ›å­¦è½¬æ¢ã€‚ä¸ºäº†ä¿è¯ä¸“å®¶çš„å¤šæ ·æ€§å¹¶é˜²æ­¢æ¨¡å¼å´©æºƒï¼ˆMode Collapseï¼‰ï¼Œç ”ç©¶å¼•å…¥äº†æ½œç©ºé—´æ­£äº¤åŒ–ç›®æ ‡ï¼ˆLatent Orthogonalization Objectiveï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPRISM-WM èƒ½å¤Ÿç²¾ç¡®æ•æ‰ç³»ç»ŸåŠ¨åŠ›å­¦ä¸­çš„å‰§çƒˆæ¨¡å¼åˆ‡æ¢ï¼Œæ˜¾è‘—é™ä½äº†é¢„æµ‹æ¼‚ç§»ï¼ˆRollout Driftï¼‰ã€‚åœ¨é«˜ç»´äººå½¢æœºå™¨äººåŠå¤šä»»åŠ¡è®¾ç½®çš„è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹ä¸º TD-MPC ç­‰è½¨è¿¹ä¼˜åŒ–ç®—æ³•æä¾›äº†é«˜ä¿çœŸåº¦çš„å»ºæ¨¡åŸºç¡€ï¼Œè¯æ˜äº†å…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºäºæ¨¡å‹æ™ºèƒ½ä½“ï¼ˆModel-Based Agentsï¼‰åŸºç¡€æ¨¡å‹çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08411v1",
      "published_date": "2025-12-09 09:40:34 UTC",
      "updated_date": "2025-12-09 09:40:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:10:45.858323+00:00"
    },
    {
      "arxiv_id": "2512.08404v1",
      "title": "Are generative AI text annotations systematically biased?",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–‡æœ¬æ ‡æ³¨æ˜¯å¦å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Ÿ",
      "authors": [
        "Sjoerd B. Stolwijk",
        "Mark Boukes",
        "Damian Trilling"
      ],
      "abstract": "This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹(GLLMs)åœ¨æ–‡æœ¬æ ‡æ³¨ä¸­æ˜¯å¦å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œå¹¶é€šè¿‡å¯¹Boukes(2024)çš„äººå·¥æ ‡æ³¨è¿›è¡Œæ¦‚å¿µæ€§å¤åˆ¶ç ”ç©¶ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨Llama3.1:8bã€Llama3.3:70bã€GPT4oå’ŒQwen2.5:72bç­‰å¤šç§æ¨¡å‹ï¼Œé’ˆå¯¹æ”¿æ²»å†…å®¹(political content)ã€äº’åŠ¨æ€§(interactivity)ã€ç†æ€§(rationality)ã€ä¸æ–‡æ˜è¡Œä¸º(incivility)å’Œæ„è¯†å½¢æ€(ideology)äº”ä¸ªæ¦‚å¿µè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡GLLMsåœ¨F1 scoresä¸Šè¡¨ç°å°šå¯ï¼Œä½†åœ¨æµè¡Œåº¦(prevalence)ä¸Šä¸äººå·¥æ ‡æ³¨å­˜åœ¨å·®å¼‚ï¼Œä¸”åœ¨ä¸‹æ¸¸åˆ†æä¸­äº§ç”Ÿäº†å®è´¨æ€§ä¸åŒçš„ç»“æœã€‚ç ”ç©¶æ ¸å¿ƒå‘ç°GLLMså±•ç°å‡ºæ˜¾è‘—çš„ç³»ç»Ÿæ€§åå·®(systematic bias)ï¼Œä¸åŒæ¨¡å‹ä¹‹é—´çš„æ ‡æ³¨é‡åˆåº¦è¿œé«˜äºå®ƒä»¬ä¸äººå·¥æ ‡æ³¨çš„é‡åˆåº¦ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„F1 scoresæŒ‡æ ‡æ— æ³•å‡†ç¡®è¡¡é‡æ­¤ç±»åå·®çš„ä¸¥é‡ç¨‹åº¦ï¼Œå¼ºè°ƒäº†åœ¨å­¦æœ¯ç ”ç©¶ä¸­ä½¿ç”¨AIæ ‡æ³¨æ—¶éœ€å®¡æ…å¯¹å¾…å…¶å¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 6 figures, 1 table; version submitted to the International Communication Association Annual Conference in Cape Town 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.08404v1",
      "published_date": "2025-12-09 09:36:43 UTC",
      "updated_date": "2025-12-09 09:36:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:10:34.182884+00:00"
    },
    {
      "arxiv_id": "2512.08379v1",
      "title": "DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals",
      "title_zh": "DeepFeatureï¼šé¢å‘å¯ç©¿æˆ´ç”Ÿç‰©ä¿¡å·çš„è¿­ä»£å¼ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾ç”Ÿæˆ",
      "authors": [
        "Kaiwei Liu",
        "Yuting He",
        "Bufang Yang",
        "Mu Yuan",
        "Chun Man Victor Wong",
        "Ho Pong Andrew Sze",
        "Zhenyu Yan",
        "Hongkai Chen"
      ],
      "abstract": "Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeepFeatureï¼Œè¿™æ˜¯é¦–ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„ã€é¢å‘å¯ç©¿æˆ´ç”Ÿç‰©ä¿¡å·(wearable biosignals)çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾ç”Ÿæˆæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ç‰¹å¾æå–æ–¹æ³•ç¼ºä¹ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ã€åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´æœç´¢å›°éš¾ä»¥åŠä»£ç ç”Ÿæˆæ˜“å‡ºé”™ç­‰é—®é¢˜ï¼ŒDeepFeatureå¼•å…¥äº†æ•´åˆä¸“å®¶çŸ¥è¯†çš„å¤šæºç‰¹å¾ç”Ÿæˆæœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸºäºç‰¹å¾è¯„ä¼°åé¦ˆçš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ä¸æ–­ç²¾ç‚¼ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¤šå±‚è¿‡æ»¤ä¸éªŒè¯æ–¹æ³•ç¡®ä¿ç‰¹å¾åˆ°ä»£ç è½¬æ¢çš„é²æ£’æ€§ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒDeepFeatureåœ¨å…«é¡¹å¤šæ ·åŒ–çš„åŒ»ç–—å¥åº·ä»»åŠ¡ä¸­ä½¿AUROCå¹³å‡æå‡äº†4.21%è‡³9.67%ï¼Œåœ¨å…¶ä¸­äº”é¡¹ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›(state-of-the-art)æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨LLMè¿›è¡Œè‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹åœ¨æå‡ç”Ÿç‰©ä¿¡å·åˆ†æå‡†ç¡®æ€§ä¸å¯é æ€§æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08379v1",
      "published_date": "2025-12-09 09:06:17 UTC",
      "updated_date": "2025-12-09 09:06:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:09:42.597371+00:00"
    },
    {
      "arxiv_id": "2512.08366v1",
      "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making",
      "title_zh": "åŒå£°åæ€ï¼šä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å†³ç­–çš„ååŒè‡ªé€‚åº”åŒç­–ç•¥æ¡†æ¶",
      "authors": [
        "Wentao Zhang",
        "Qunbo Wang",
        "Tao Zhang",
        "Junsheng Wu",
        "Hongping Gan",
        "Yang Liu",
        "Ling Dai",
        "Shizhuang Deng",
        "Shuntong Sun"
      ],
      "abstract": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DuSAR (Dual-Strategy Agent with Reflecting)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€å¤–éƒ¨ç¤ºèŒƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å†³ç­–ä¸­è¿‡åº¦ä¾èµ–å¤–éƒ¨ç¤ºä¾‹å¯¼è‡´çš„è„†å¼±æ€§ã€æ³›åŒ–èƒ½åŠ›å·®å’Œé«˜è®¡ç®—å¼€é”€ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ååŒè‡ªé€‚åº”æ¨ç†æœºåˆ¶ï¼Œç»“åˆäº†é«˜å±‚çº§æ•´ä½“è§„åˆ’(high-level holistic plan)å’ŒåŸºäºä¸Šä¸‹æ–‡çš„å±€éƒ¨ç­–ç•¥(context-grounded local policy)ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼Œæ¨¡æ‹Ÿäººç±»çš„å…ƒè®¤çŸ¥è¡Œä¸ºã€‚æ™ºèƒ½ä½“åˆ©ç”¨è½»é‡çº§çš„åæ€æœºåˆ¶ï¼Œé€šè¿‡ç­–ç•¥æ‹Ÿåˆå¾—åˆ†(Strategy Fitness Score)æŒç»­è¯„ä¼°è¿›åº¦ï¼Œå¹¶åœ¨é‡åˆ°ç“¶é¢ˆæˆ–å–å¾—è¿›å±•æ—¶åŠ¨æ€ä¿®æ­£æˆ–ç»†åŒ–å…¨å±€è®¡åˆ’ã€‚åœ¨ALFWorldå’ŒMind2WebåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDuSARåœ¨å¼€æºLLMsä¸Šå–å¾—äº†State-of-the-Artæ€§èƒ½ï¼Œå…¶ä¸­åœ¨ALFWorldä¸Šçš„æˆåŠŸç‡è¾ƒæ­¤å‰æœ€ä½³ç»“æœæå‡äº†ä¸€å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ç»´æŒå¼ºåŠ²æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å•æ­¥Tokenæ¶ˆè€—é™ä½äº†3åˆ°9å€ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®äº†åŒç­–ç•¥åä½œçš„å¿…è¦æ€§ï¼Œä¸”è¯¥æ¡†æ¶æ”¯æŒä¸ä¸“å®¶ç¤ºèŒƒ(expert demonstrations)çš„å¯é€‰é›†æˆï¼Œå±•ç°äº†æé«˜çš„çµæ´»æ€§ä¸çŸ¥è¯†å…¼å®¹æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08366v1",
      "published_date": "2025-12-09 08:44:59 UTC",
      "updated_date": "2025-12-09 08:44:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:09:42.476029+00:00"
    },
    {
      "arxiv_id": "2512.08360v1",
      "title": "Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata",
      "title_zh": "æ¡ä»¶å½¢æ€å‘ç”Ÿï¼šåŸºäºç¥ç»å…ƒèƒè‡ªåŠ¨æœºçš„ç»“æ„åŒ–æ•°å­—æ¶Œç°ç”Ÿæˆ",
      "authors": [
        "Ali Sakour"
      ],
      "abstract": "Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Conditional Neural Cellular Automata (c-NCA)æ¶æ„ï¼Œæ—¨åœ¨æ¨¡ä»¿ç”Ÿç‰©ç³»ç»Ÿçš„å½¢æ€å‘ç”Ÿå¯å¡‘æ€§ï¼Œè§£å†³ç°æœ‰Neural Cellular Automata (NCA)åœ¨ç±»æ¡ä»¶ç»“æ„ç”Ÿæˆé¢†åŸŸçš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä»…æ ¹æ®ç©ºé—´å¹¿æ’­çš„ç±»åˆ«å‘é‡(class vector)ï¼Œä»å•ä¸ªé€šç”¨ç§å­(seed)ç”Ÿé•¿å‡ºç‰¹å®šçš„MNISTæ•°å­—æ‹“æ‰‘ç»“æ„ã€‚ä¸ä¾èµ–å…¨å±€æ„Ÿå—é‡çš„GANsæˆ–VAEsä¸åŒï¼Œc-NCAå¼ºè°ƒä¸¥æ ¼çš„å±€éƒ¨æ€§(locality)å’Œå¹³ç§»ç­‰å˜æ€§(translation equivariance)ï¼Œé€šè¿‡åœ¨ç»†èƒæ„ŸçŸ¥åœºæ³¨å…¥one-hotæ¡ä»¶ï¼Œä½¿å•ä¸€å±€éƒ¨è§„åˆ™å­¦ä¹ å¯¹ç§°æ€§ç ´ç¼ºå¹¶è‡ªæˆ‘ç»„è£…æˆåç§ä¸åŒçš„å‡ ä½•å¸å¼•å­(geometric attractors)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œc-NCAèƒ½å¤Ÿä»å•ä¸ªåƒç´ ç¨³å®šæ”¶æ•›å½¢æˆæ•°å­—æ‹“æ‰‘ï¼Œå¹¶è¡¨ç°å‡ºç±»ä¼¼äºç”Ÿç‰©ç³»ç»Ÿçš„å¼ºé²æ£’æ€§ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆå¼¥åˆäº†åŸºäºçº¹ç†çš„NCAä¸ç»“æ„åŒ–æ¨¡å¼å½¢æˆä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºç±»æ¡ä»¶ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§è½»é‡åŒ–ä¸”å…·æœ‰ç”Ÿç‰©å­¦åˆç†æ€§çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "13 pages, 5 figures. Code available at: https://github.com/alisakour/Conditional-NCA-Digits",
      "pdf_url": "https://arxiv.org/pdf/2512.08360v1",
      "published_date": "2025-12-09 08:36:54 UTC",
      "updated_date": "2025-12-09 08:36:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:09:53.247388+00:00"
    },
    {
      "arxiv_id": "2512.14720v1",
      "title": "SoMe: A Realistic Benchmark for LLM-based Social Media Agents",
      "title_zh": "SoMeï¼šé¢å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤åª’ä½“æ™ºèƒ½ä½“çš„çœŸå®åŸºå‡†",
      "authors": [
        "Dizhan Xue",
        "Jing Cui",
        "Shengsheng Qian",
        "Chuanrui Hu",
        "Changsheng Xu"
      ],
      "abstract": "Intelligent agents powered by large language models (LLMs) have recently demonstrated impressive capabilities and gained increasing popularity on social media platforms. While LLM agents are reshaping the ecology of social media, there exists a current gap in conducting a comprehensive evaluation of their ability to comprehend media content, understand user behaviors, and make intricate decisions. To address this challenge, we introduce SoMe, a pioneering benchmark designed to evaluate social media agents equipped with various agent tools for accessing and analyzing social media data. SoMe comprises a diverse collection of 8 social media agent tasks, 9,164,284 posts, 6,591 user profiles, and 25,686 reports from various social media platforms and external websites, with 17,869 meticulously annotated task queries. Compared with the existing datasets and benchmarks for social media tasks, SoMe is the first to provide a versatile and realistic platform for LLM-based social media agents to handle diverse social media tasks. By extensive quantitative and qualitative analysis, we provide the first overview insight into the performance of mainstream agentic LLMs in realistic social media environments and identify several limitations. Our evaluation reveals that both the current closed-source and open-source LLMs cannot handle social media agent tasks satisfactorily. SoMe provides a challenging yet meaningful testbed for future social media agents. Our code and data are available at https://github.com/LivXue/SoMe",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SoMeï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„Benchmarkï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºLarge Language Models (LLMs) çš„ç¤¾äº¤åª’ä½“æ™ºèƒ½ä½“åœ¨ç†è§£å†…å®¹ã€ç”¨æˆ·è¡Œä¸ºåŠå¤æ‚å†³ç­–æ–¹é¢çš„èƒ½åŠ›ã€‚SoMeæ¶µç›–äº†8ä¸ªç¤¾äº¤åª’ä½“æ™ºèƒ½ä½“ä»»åŠ¡ï¼ŒåŒ…å«è¶…è¿‡900ä¸‡æ¡å¸–å­ã€6000å¤šä¸ªç”¨æˆ·ç”»åƒä»¥åŠ1.7ä¸‡ä¸ªç»è¿‡ç²¾å¿ƒæ ‡æ³¨çš„ä»»åŠ¡æŸ¥è¯¢ï¼Œæ˜¯é¦–ä¸ªä¸ºç¤¾äº¤åª’ä½“æ™ºèƒ½ä½“æä¾›å¤šåŠŸèƒ½ä¸”çœŸå®è¯„ä¼°å¹³å°çš„åŸºå‡†ã€‚é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§åˆ†æï¼Œç ”ç©¶æä¾›äº†ä¸»æµLLMsåœ¨çœŸå®ç¤¾äº¤åª’ä½“ç¯å¢ƒä¸­è¡¨ç°çš„é¦–ä¸ªæ·±åº¦æ´å¯Ÿï¼Œå¹¶æŒ‡å‡ºå½“å‰æ— è®ºæ˜¯å¼€æºè¿˜æ˜¯é—­æºæ¨¡å‹åœ¨å¤„ç†ç›¸å…³ä»»åŠ¡æ—¶å‡è¡¨ç°ä¸ä½³ã€‚è¯¥ç ”ç©¶è¯†åˆ«äº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç¤¾äº¤åª’ä½“æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”æ„ä¹‰é‡å¤§çš„æµ‹è¯•åºŠã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.14720v1",
      "published_date": "2025-12-09 08:36:09 UTC",
      "updated_date": "2025-12-09 08:36:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:09:59.871782+00:00"
    },
    {
      "arxiv_id": "2512.08345v1",
      "title": "The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations",
      "title_zh": "å¤±ç¤¼è¡Œä¸ºçš„é«˜æ˜‚ä»£ä»·ï¼šåŸºäºå¤šæ™ºèƒ½ä½“è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿçš„äº¤äº’ä½æ•ˆé‡åŒ–ç ”ç©¶",
      "authors": [
        "Benedikt Mangold"
      ],
      "abstract": "Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled \"sociological sandbox\". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with \"toxic\" system prompts. Our results demonstrate a statistically significant increase of approximately 25\\% in the duration of conversations involving toxic participants. We propose that this \"latency of toxicity\" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é‡åŒ–èŒåœºæˆ¾æ°”ï¼ˆWorkplace toxicityï¼‰å¯¹è¿è¥æ•ˆç‡çš„ç›´æ¥å½±å“ï¼Œåˆ©ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemsï¼‰æ„å»ºäº†ä¸€ä¸ªå¯æ§çš„â€œç¤¾ä¼šå­¦æ²™ç›’â€æ¥æ¨¡æ‹Ÿä¸€å¯¹ä¸€å¯¹æŠ—æ€§è¾©è®ºã€‚ç ”ç©¶è€…é‡‡ç”¨äº†è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carlo methodï¼‰è¿›è¡Œæ•°ç™¾æ¬¡è®¨è®ºæ¨¡æ‹Ÿï¼Œé€šè¿‡å¯¹æ¯”åŸºå‡†å¯¹ç…§ç»„ä¸å¸¦æœ‰â€œæœ‰æ¯’ï¼ˆtoxicï¼‰â€ç³»ç»Ÿæç¤ºè¯çš„å®éªŒç»„ï¼Œæµ‹é‡è¾¾æˆç»“è®ºæ‰€éœ€çš„æ”¶æ•›æ—¶é—´ï¼ˆconvergence timeï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¶‰åŠæœ‰æ¯’å‚ä¸è€…çš„å¯¹è¯æ—¶é•¿æ˜¾è‘—å¢åŠ äº†çº¦ 25%ï¼Œç›´è§‚åœ°å±•ç¤ºäº†ä¸æ–‡æ˜è¡Œä¸ºå¯¼è‡´çš„äº¤äº’æ•ˆç‡ä½ä¸‹ã€‚ç ”ç©¶æå‡ºè¿™ç§â€œæ¯’æ€§å»¶è¿Ÿï¼ˆlatency of toxicityï¼‰â€å¯ä»¥ä½œä¸ºé‡åŒ–ä¼ä¸šå’Œå­¦æœ¯æœºæ„ä¸­è´¢åŠ¡æŸå¤±çš„ä»£ç†æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹å·¥ä½œè¯æ˜äº†æ™ºèƒ½ä½“å»ºæ¨¡ï¼ˆagent-based modelingï¼‰åœ¨ç ”ç©¶ç¤¾äº¤æ‘©æ“¦æœºåˆ¶æ—¶ï¼Œæ˜¯äººç±»ä¸»ä½“ç ”ç©¶çš„ä¸€ç§å¯å¤åˆ¶ä¸”ç¬¦åˆä¼¦ç†è¦æ±‚çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "8 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08345v1",
      "published_date": "2025-12-09 08:17:35 UTC",
      "updated_date": "2025-12-09 08:17:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:09:56.897910+00:00"
    },
    {
      "arxiv_id": "2512.08344v1",
      "title": "Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions",
      "title_zh": "é€šè¿‡æ¦‚å¿µä¸ç»“æ„åˆ†æåŠå…¶æ‹“å±•å¢å¼ºå›¾ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§",
      "authors": [
        "Tien Cuong Bui"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)åœ¨å¤æ‚å›¾ç»“æ„æ•°æ®å»ºæ¨¡ä¸­çš„å†³ç­–é»‘ç›’é—®é¢˜ï¼Œå¹¶åˆ†æäº†ç°æœ‰è§£é‡Šæ€§AI(Explainable AI)æ–¹æ³•åœ¨å¤„ç†å¤æ‚äº¤äº’æ—¶çš„å±€é™ã€‚ä½œè€…æŒ‡å‡ºä¼ ç»Ÿçš„äº‹åè§£é‡Š(post-hoc)æ–¹æ³•è®¡ç®—å¼€é”€å¤§ä¸”å¯é æ€§æœ‰é™ï¼Œè€Œè‡ªè§£é‡Šæ¨¡å‹åˆ™æ™®éç¼ºä¹é€šç”¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„XAIæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºGNNsæä¾›çµæ´»ä¸”è®¡ç®—é«˜æ•ˆçš„è§£é‡Šæ–¹æ¡ˆã€‚è¯¥æ¡†æ¶çªç ´äº†ä¼ ç»Ÿçš„å•ä¸€ç‰¹å¾åˆ†æå±€é™ï¼Œé‡ç‚¹é€šè¿‡æ¦‚å¿µå’Œç»“æ„åˆ†ææ­ç¤ºå›¾ç»“æ„(graph structure)å¯¹æ¨¡å‹é¢„æµ‹çš„å…·ä½“å½±å“ã€‚è¯¥ç ”ç©¶åŠå…¶æ‰©å±•åˆ†æä¸ºå¢å¼ºå›¾æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§æä¾›äº†ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å¤æ‚æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„å¯ç†è§£æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "157 pages, Doctoral dissertation at Seoul National University (submitted in 2024.08 to SNU library, slightly updated in 2025.11 for open digital version)",
      "pdf_url": "https://arxiv.org/pdf/2512.08344v1",
      "published_date": "2025-12-09 08:13:31 UTC",
      "updated_date": "2025-12-09 08:13:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:10:08.173807+00:00"
    },
    {
      "arxiv_id": "2512.08343v1",
      "title": "Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach",
      "title_zh": "åŸºäºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æ–¹æ³•çš„åœŸå£¤å‹å®å‚æ•°é¢„æµ‹",
      "authors": [
        "Caner Erden",
        "Alparslan Serhat Demir",
        "Abdullah Hulusi Kokcam",
        "Talas Fikret Kurnaz",
        "Ugur Dagdeviren"
      ],
      "abstract": "Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå®éªŒå®¤æµ‹å®šåœŸå£¤æœ€ä½³å«æ°´é‡(OMC)å’Œæœ€å¤§å¹²å¯†åº¦(MDD)è´¹æ—¶è´¹åŠ›ï¼Œä¸”ç°æœ‰æ¨¡å‹åœ¨å¼‚æ„æ•°æ®é›†ä¸Šå‡†ç¡®æ€§ä¸æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ (AutoML)çš„é¢„æµ‹æ–¹æ³•ã€‚è¯¥AutoMLæ¡†æ¶é€šè¿‡è‡ªåŠ¨æ‰§è¡Œç®—æ³•é€‰æ‹©å’Œè¶…å‚æ•°ä¼˜åŒ–(hyperparameter optimization)ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒåœŸå£¤ç±»å‹ä¸‹çš„é¢„æµ‹ç²¾åº¦å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæç«¯æ¢¯åº¦æå‡(XGBoost)ç®—æ³•åœ¨å„é¡¹æ¨¡å‹ä¸­è¡¨ç°æœ€ä¼˜ï¼Œåœ¨ç‹¬ç«‹æ•°æ®é›†ä¸Šå¯¹MDDå’ŒOMCçš„é¢„æµ‹å†³å®šç³»æ•°(R-squared)åˆ†åˆ«è¾¾åˆ°äº†80.4%å’Œ89.1%ã€‚ç ”ç©¶ä¸ä»…è¯å®äº†AutoMLåœ¨è·¨åœŸå£¤ç±»å‹é¢„æµ‹å‹å®å‚æ•°æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿˜å¼ºè°ƒäº†å¼‚æ„æ•°æ®é›†å¯¹å¢å¼ºæ¨¡å‹é€šç”¨æ€§çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶æˆæœé€šè¿‡æä¾›æ›´é«˜æ•ˆã€å¯é çš„åœŸå£¤å‹å®å‚æ•°é¢„æµ‹æ‰‹æ®µï¼Œä¸ºä¼˜åŒ–å»ºç­‘å·¥ç¨‹å®è·µåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the 13th International Symposium on Intelligent Manufacturing and Service Systems, Duzce, Turkey, Sep 25-27, 2025. Also available on Zenodo: DOI 10.5281/zenodo.17533851",
      "pdf_url": "https://arxiv.org/pdf/2512.08343v1",
      "published_date": "2025-12-09 08:13:04 UTC",
      "updated_date": "2025-12-09 08:13:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:10:12.515028+00:00"
    },
    {
      "arxiv_id": "2512.08340v2",
      "title": "Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Turkiye",
      "title_zh": "åŸºäºé›†æˆä¸ç¥ç»ç½‘ç»œæ¨¡å‹çš„åŠ å·æ‰¿è½½æ¯”é¢„æµ‹ï¼šä»¥ Turkiye ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Abdullah Hulusi KÃ¶kÃ§am",
        "UÄŸur DaÄŸdeviren",
        "Talas Fikret Kurnaz",
        "Alparslan Serhat Demir",
        "Caner Erden"
      ],
      "abstract": "The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in TÃ¼rkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå®éªŒå®¤è´¯å…¥è¯•éªŒç¡®å®šåŠ å·æ‰¿è½½æ¯”(California Bearing Ratio, CBR)è€—æ—¶ä¸”æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ (ML)çš„é¢„æµ‹æ¡†æ¶ã€‚ç ”ç©¶åˆ©ç”¨æ¥è‡ªåœŸè€³å…¶ä¸åŒåœ°ç†æ°”å€™åœ°åŒºçš„382ä»½åœŸå£¤æ ·æœ¬æ•°æ®é›†ï¼Œæµ‹è¯•äº†åŒ…æ‹¬éšæœºæ£®æ—(random forest)ã€æ¢¯åº¦æå‡(gradient boosting)ã€XGBoostå’Œå¤šå±‚æ„ŸçŸ¥æœº(multi-layer perceptron)åœ¨å†…çš„12ç§ç®—æ³•ï¼Œä»¥è¯„ä¼°è·¯åŸºåœŸå£¤çš„æ‰¿è½½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšæœºæ£®æ—å›å½’å™¨(random forest regressor)è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œåœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†0.83çš„R2åˆ†æ•°ï¼Œå±•ç°äº†å¼ºå¤§çš„éçº¿æ€§æ˜ å°„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é›†æˆå­¦ä¹ å’Œç¥ç»ç½‘ç»œæ¨¡å‹åœ¨å²©åœŸå·¥ç¨‹é¢„æµ‹ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºäº¤é€šåŸºç¡€è®¾æ–½å»ºè®¾å’Œåœ°åŸºè®¾è®¡æä¾›äº†ä¸€ç§é«˜æ•ˆã€ç²¾ç¡®ä¸”æ•°å­—åŒ–çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the 13th International Symposium on Intelligent Manufacturing and Service Systems, Duzce, Turkey, Sep 25-27, 2025. Also available on Zenodo: DOI 10.5281/zenodo.17530868",
      "pdf_url": "https://arxiv.org/pdf/2512.08340v2",
      "published_date": "2025-12-09 08:09:55 UTC",
      "updated_date": "2025-12-13 18:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:10:12.636025+00:00"
    },
    {
      "arxiv_id": "2512.08333v2",
      "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging",
      "title_zh": "åŸºäºå‚æ•°åˆå¹¶çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæœºå™¨äººç­–ç•¥é²æ£’å¾®è°ƒ",
      "authors": [
        "Yajat Yadav",
        "Zhiyuan Zhou",
        "Andrew Wagenmaker",
        "Karl Pertsch",
        "Sergey Levine"
      ],
      "abstract": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging performance scales with the amount of pretraining data, and enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å‹æœºå™¨äººç­–ç•¥(Generalist robot policies)åœ¨å¾®è°ƒ(finetuning)æ–°ä»»åŠ¡æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆã€ä¸¢å¤±å…ˆå‰èƒ½åŠ›ä»¥åŠæ–°ä»»åŠ¡æ³›åŒ–æ€§å·®ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå‚æ•°åˆå¹¶(Parameter Merging)çš„ç¨³å¥å¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¾®è°ƒåçš„æ¨¡å‹æƒé‡ä¸é¢„è®­ç»ƒæ¨¡å‹æƒé‡è¿›è¡Œç®€å•æœ‰æ•ˆçš„æ’å€¼å¤„ç†ï¼Œæ—¨åœ¨è®©å•ä¸€ç­–ç•¥åœ¨æŒæ¡æ–°æŠ€èƒ½çš„åŒæ—¶ä¿ç•™é¢„è®­ç»ƒè·å¾—çš„å¹¿æ³›èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å¹¿æ³›å®éªŒä¸­è¯æ˜ï¼Œè¿™ç§åˆå¹¶ç”Ÿæˆçš„æ¨¡å‹ä¸ä»…ç»§æ‰¿äº†åŸºç¡€æ¨¡å‹çš„é€šç”¨æ€§ï¼Œä¸”åœ¨å¤„ç†æ–°ä»»åŠ¡çš„åˆ†å¸ƒå¤–(out-of-distribution)å˜ä½“æ—¶è¡¨ç°ä¼˜äºçº¯é¢„è®­ç»ƒæˆ–çº¯å¾®è°ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜è¯¥æ–¹æ³•çš„æ€§èƒ½éšé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¢åŠ è€Œæå‡ï¼Œå¹¶èƒ½æ”¯æŒæœºå™¨äººåœ¨ç»ˆèº«å­¦ä¹ (lifelong learning)åœºæ™¯ä¸‹æŒç»­è·å–æ–°æŠ€èƒ½è€Œä¸ç‰ºç‰²åŸæœ‰èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08333v2",
      "published_date": "2025-12-09 08:02:11 UTC",
      "updated_date": "2025-12-18 10:00:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:03.802933+00:00"
    },
    {
      "arxiv_id": "2512.08329v1",
      "title": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models",
      "title_zh": "æ‰©æ•£æ¨¡å‹å›¾åƒä¿æŠ¤æ–¹æ³•ä¸­ç»“æ„åŒ–æ‰°åŠ¨çš„è§£è¯»",
      "authors": [
        "Michael R. Martin",
        "Garrick Chan",
        "Kwan-Liu Ma"
      ],
      "abstract": "Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Glaze å’Œ Nightshade ç­‰å›¾åƒä¿æŠ¤æœºåˆ¶ï¼Œåˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)åˆ†æå…¶å¯¹æŠ—æ€§æ‰°åŠ¨çš„å†…éƒ¨ç»“æ„ã€å¯æ£€æµ‹æ€§åŠè¡¨å¾è¡Œä¸ºã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•´åˆç™½ç›’ç‰¹å¾ç©ºé—´æ£€æŸ¥(white-box feature-space inspection)ä¸é»‘ç›’ä¿¡å·çº§æ¢æµ‹(black-box signal-level probing)çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå‘ç°è¿™äº›ä¿æŠ¤æœºåˆ¶æ˜¯ä¸å›¾åƒå†…å®¹åœ¨è¡¨å¾ã€ç©ºé—´å’Œé¢‘åŸŸä¸Šç´§å¯†è€¦åˆçš„ç»“æ„åŒ–ä½ç†µæ‰°åŠ¨(low-entropy perturbations)ã€‚å—ä¿æŠ¤çš„å›¾åƒä¿ç•™äº†åŸæœ‰çš„ç‰¹å¾ç»„ç»‡ï¼Œä»…è¡¨ç°å‡ºç‰¹å®šçš„ä¿æŠ¤å­ç»“æ„ï¼Œè€Œéå¼•å‘å…¨å±€è¡¨å¾æ¼‚ç§»(global representational drift)ã€‚é¢‘åŸŸåˆ†æè¿›ä¸€æ­¥è¯æ˜ï¼ŒGlaze å’Œ Nightshade æ˜¯æ²¿ç€å›¾åƒä¸»å¯¼é¢‘ç‡è½´é‡æ–°åˆ†é…èƒ½é‡ï¼Œè€Œéå¼•å…¥æ— åºçš„å¼¥æ•£å™ªå£°ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œå›¾åƒä¿æŠ¤çš„æœ‰æ•ˆæ€§æºäºç»“æ„åŒ–çš„ç‰¹å¾çº§å˜å½¢(feature-level deformation)è€Œéè¯­ä¹‰é”™ä½ï¼Œè¿™è§£é‡Šäº†ä¸ºä½•ä¿¡å·åœ¨è§†è§‰ä¸Šéš¾ä»¥å¯Ÿè§‰å´èƒ½è¢«ç³»ç»Ÿæ€§æ£€æµ‹ã€‚è¯¥æˆæœä¸ä»…æå‡äº†å¯¹æŠ—æ€§å›¾åƒä¿æŠ¤çš„å¯è§£é‡Šæ€§ï¼Œä¹Ÿä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)é¢†åŸŸçš„é˜²å¾¡ä¸æ£€æµ‹ç­–ç•¥æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "32 pages, 17 figures, 1 table, 5 algorithms, preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.08329v1",
      "published_date": "2025-12-09 07:55:11 UTC",
      "updated_date": "2025-12-09 07:55:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:04.604349+00:00"
    },
    {
      "arxiv_id": "2512.09003v2",
      "title": "Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity",
      "title_zh": "åŸºäºç»„ç»‡å­¦å›¾åƒçš„ç©ºé—´é€šè·¯æ´»æ€§æ•°å­—å»ºæ¨¡æ­ç¤ºè‚¿ç˜¤å¾®ç¯å¢ƒå¼‚è´¨æ€§",
      "authors": [
        "Ling Liao",
        "Changhuei Yang",
        "Maxim Artyomov",
        "Mark Watson",
        "Adam Kepecs",
        "Haowen Zhou",
        "Alexey Sergushichev",
        "Richard Cote"
      ],
      "abstract": "Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥ä»è‹æœ¨ç²¾-ä¼Šçº¢(H&E)æŸ“è‰²çš„ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­ï¼Œä»¥55å’Œ100å¾®ç±³çš„å¾®å°ºåº¦åˆ†è¾¨ç‡é¢„æµ‹ç©ºé—´é€šè·¯æ´»æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è®¡ç®—ç—…ç†å­¦åŸºç¡€æ¨¡å‹(foundation model)æå–çš„å›¾åƒç‰¹å¾ï¼Œæœ‰æ•ˆåœ°å°†ç©ºé—´è½¬å½•ç»„å­¦(ST)ä¿¡æ¯ä¸ç»„ç»‡å½¢æ€å­¦ç›¸ç»“åˆã€‚ç ”ç©¶å‘ç°åœ¨ä¹³è…ºç™Œå’Œè‚ºç™Œçš„ä¸‰ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸­ï¼ŒTGFb signaling æ˜¯é¢„æµ‹æœ€å‡†ç¡®çš„é€šè·¯ã€‚åœ¨87-88%çš„å¯é é¢„æµ‹æ¡ˆä¾‹ä¸­ï¼Œç”Ÿæˆçš„ç©ºé—´ TGFb æ´»æ€§å›¾å‡†ç¡®åæ˜ äº†è‚¿ç˜¤åŒºåŸŸä¸é‚»è¿‘éè‚¿ç˜¤åŒºåŸŸçš„å¯¹æ¯”ï¼Œä¸è¯¥é€šè·¯åœ¨è°ƒèŠ‚è‚¿ç˜¤å¾®ç¯å¢ƒ(tumor microenvironment)äº¤äº’ä¸­çš„å·²çŸ¥ä½œç”¨ä¸€è‡´ã€‚æ­¤å¤–ï¼Œçº¿æ€§ä¸éçº¿æ€§æ¨¡å‹è¡¨ç°ç›¸ä¼¼ï¼Œè¡¨æ˜å›¾åƒç‰¹å¾ä¸é€šè·¯æ´»æ€§ä¹‹é—´å¯èƒ½å­˜åœ¨æ˜¾è‘—çš„çº¿æ€§å…³è”ã€‚è¯¥æˆæœè¯æ˜äº†åˆ©ç”¨å¸¸è§„ç—…ç†å›¾åƒæ¢å¤ç©ºé—´è¿è´¯ä¸”å…·æœ‰ç”Ÿç‰©å­¦è§£é‡Šæ€§çš„é€šè·¯æ¨¡å¼çš„å¯è¡Œæ€§ï¼Œä¸ºå¤§è§„æ¨¡ç ”ç©¶è‚¿ç˜¤å¾®ç¯å¢ƒå¼‚è´¨æ€§æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„ç­–ç•¥ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "We have to make substantial updates, thank you",
      "pdf_url": "https://arxiv.org/pdf/2512.09003v2",
      "published_date": "2025-12-09 07:54:25 UTC",
      "updated_date": "2025-12-18 14:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:27.758419+00:00"
    },
    {
      "arxiv_id": "2512.14719v1",
      "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
      "title_zh": "é¢å‘å¯è§£é‡Šä¸”é²æ£’æ¨¡å‹è®­ç»ƒçš„æ··åˆå½’å› å…ˆéªŒ",
      "authors": [
        "Zhuoran Zhang",
        "Feng Zhang",
        "Shangyuan Li",
        "Yang Shi",
        "Yuanxing Zhang",
        "Wei Chen",
        "Tengjiao Wang",
        "Kam-Fai Wong"
      ],
      "abstract": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°å‹è¯­è¨€æ¨¡å‹(Small language models, SLMs)åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§é—®é¢˜ï¼Œæ·±å…¥åˆ†æäº†è§£é‡Šå¼•å¯¼å­¦ä¹ (Explanation-guided learning)æ¡†æ¶ä¸‹å½’å› å…ˆéªŒçš„å±€é™æ€§ã€‚ç ”ç©¶å‘ç°ç°æœ‰å½’å› æ–¹æ³•åœ¨å¤„ç†è¯­ä¹‰ç›¸ä¼¼ç±»åˆ«æ—¶å¾€å¾€ä¾§é‡äºå…±æœ‰å…³é”®è¯ï¼Œæ— æ³•æä¾›è¶³å¤Ÿçš„åŒºåˆ†æ€§çº¿ç´¢ï¼Œé™åˆ¶äº†æ¨¡å‹çš„åˆ†è¾¨èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Class-Aware Attribution Prior (CAP)æ¡†æ¶ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹æ•æ‰ç»†ç²’åº¦çš„ç±»åˆ«å·®å¼‚æ¥ç”Ÿæˆæ›´å…·åˆ¤åˆ«åŠ›çš„å½’å› å…ˆéªŒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è¿›ä¸€æ­¥æ¨å‡ºäº†CAP Hybridæ–¹æ³•ï¼Œå°†CAPä¸ä¼ ç»Ÿå½’å› æŠ€æœ¯èåˆä»¥æ„å»ºæ›´å…¨é¢å‡è¡¡çš„ç›‘ç£ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¨æ•°æ®ã€å°æ ·æœ¬(Few-shot)åŠå¯¹æŠ—æ€§åœºæ™¯ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„è§£é‡Šæ€§èƒ½ä¸ç¨³å¥æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤šæ ·ä¸”ä¸å†³ç­–ç›¸å…³çš„å…³é”®ç‰¹å¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.14719v1",
      "published_date": "2025-12-09 07:52:47 UTC",
      "updated_date": "2025-12-09 07:52:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:10.143050+00:00"
    },
    {
      "arxiv_id": "2512.08326v1",
      "title": "Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships",
      "title_zh": "Argusï¼šåŸºäºå±‚çº§å¼•ç”¨å…³ç³»çš„å¤šæ™ºèƒ½ä½“æ•æ„Ÿä¿¡æ¯æ³„éœ²æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Bin Wang",
        "Hui Li",
        "Liyang Zhang",
        "Qijia Zhuang",
        "Ao Yang",
        "Dong Zhang",
        "Xijun Luo",
        "Bing Lin"
      ],
      "abstract": "Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.",
      "tldr_zh": "é’ˆå¯¹ä»£ç ä»“åº“ä¸­æ•æ„Ÿä¿¡æ¯æ³„éœ²å¯¼è‡´çš„ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•è¯¯æŠ¥ç‡é«˜ã€å¼€å‘è€…ç­›é€‰è´Ÿæ‹…é‡çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Argusï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)å’Œå¤šæ™ºèƒ½ä½“(Multi-Agent)åä½œæ¶æ„çš„æ•æ„Ÿä¿¡æ¯æ£€æµ‹æ¡†æ¶ã€‚Argusé‡‡ç”¨äº†ä¸€ç§ä¸‰å±‚æ£€æµ‹æœºåˆ¶ï¼Œé€šè¿‡æ•´åˆå…³é”®å†…å®¹(Key Content)ã€æ–‡ä»¶ä¸Šä¸‹æ–‡(File Context)ä»¥åŠé¡¹ç›®å¼•ç”¨å…³ç³»(Project Reference Relationships)æ¥æœ‰æ•ˆé™ä½è¯¯æŠ¥å¹¶å¢å¼ºæ£€æµ‹ç²¾åº¦ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸¤ä¸ªå…¨æ–°çš„åŸºå‡†æµ‹è¯•(Benchmarks)ï¼Œåˆ†åˆ«ç”¨äºè¡¡é‡çœŸå®æ³„éœ²æ£€æµ‹èƒ½åŠ›å’Œè¯¯æŠ¥è¿‡æ»¤æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArgusåœ¨æ³„éœ²æ£€æµ‹ä¸­è¾¾åˆ°äº†94.86%çš„å‡†ç¡®ç‡ï¼Œå…¶ç²¾ç¡®ç‡å’Œå¬å›ç‡åˆ†åˆ«ä¸º96.36%å’Œ94.64%ï¼ŒF1åˆ†æ•°é«˜è¾¾0.955ã€‚æ­¤å¤–ï¼Œå¯¹97ä¸ªçœŸå®ä»£ç ä»“åº“çš„åˆ†ææ˜¾ç¤ºå…¶è¿è¡Œæ€»æˆæœ¬ä»…ä¸º2.2ç¾å…ƒï¼Œè¯æ˜äº†Argusåœ¨å®é™…å®‰å…¨å®¡è®¡åœºæ™¯ä¸­çš„é«˜æ•ˆæ€§ä¸ç»æµæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 7 figures, 8 tables;Accepted to ICSE 2026 Research Track",
      "pdf_url": "https://arxiv.org/pdf/2512.08326v1",
      "published_date": "2025-12-09 07:42:10 UTC",
      "updated_date": "2025-12-09 07:42:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:10.871359+00:00"
    },
    {
      "arxiv_id": "2512.08317v2",
      "title": "GeoDM: Geometry-aware Distribution Matching for Dataset Distillation",
      "title_zh": "GeoDMï¼šé¢å‘æ•°æ®é›†è’¸é¦çš„å‡ ä½•æ„ŸçŸ¥åˆ†å¸ƒåŒ¹é…",
      "authors": [
        "Xuhui Li",
        "Zhengquan Luo",
        "Zihui Cui",
        "Zhiqiang Xu"
      ],
      "abstract": "Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \\textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GeoDMï¼Œè¿™æ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„åˆ†å¸ƒåŒ¹é…æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Dataset Distillationä¸­ç°æœ‰æ–¹æ³•å—é™äºEuclidean spacesè€Œå¿½ç•¥æ•°æ®å†…åœ¨å‡ ä½•ç»“æ„çš„é—®é¢˜ã€‚GeoDMåœ¨Euclideanã€hyperbolicå’Œspherical manifoldsçš„ç¬›å¡å°”ç§¯ç©ºé—´ä¸­è¿è¡Œï¼Œåˆ©ç”¨ç»Ÿä¸€è¡¨ç¤ºæ•æ‰å¹³å¦ã€åˆ†å±‚å’Œå‘¨æœŸæ€§ç»“æ„ã€‚ä¸ºäº†è‡ªé€‚åº”æ•°æ®çš„å‡ ä½•ç‰¹æ€§ï¼Œç ”ç©¶å¼•å…¥äº†å¯å­¦ä¹ çš„curvatureå’Œæƒé‡å‚æ•°ï¼Œå¹¶ç»“åˆoptimal transport lossæ¥å¢å¼ºåˆ†å¸ƒä¿çœŸåº¦ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œåœ¨ä¹˜ç§¯ç©ºé—´ä¸­è¿›è¡Œå‡ ä½•æ„ŸçŸ¥åˆ†å¸ƒåŒ¹é…æ¯”å•çº¯çš„Euclideanæ–¹æ³•å…·æœ‰æ›´å°çš„generalization error boundã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoDMåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç›®å‰çš„SOTAæ•°æ®è’¸é¦æ–¹æ³•ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨æ•æ‰å¤æ‚å‡ ä½•æµå½¢æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08317v2",
      "published_date": "2025-12-09 07:31:57 UTC",
      "updated_date": "2025-12-10 12:23:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:24.130798+00:00"
    },
    {
      "arxiv_id": "2512.08309v3",
      "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
      "title_zh": "Terrain Diffusionï¼šæ— é™å®æ—¶åœ°å½¢ç”Ÿæˆä¸­ Perlin å™ªå£°çš„æ‰©æ•£å¼ç»§ä»»è€…",
      "authors": [
        "Alexander Goslin"
      ],
      "abstract": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, a generative framework that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation that reformulates standard diffusion sampling for unbounded domains. While noise functions remain near-instant, our framework outpaces orbital velocity by 9 times on a consumer GPU, enabling realistic terrain generation at interactive rates. We integrate a hierarchical stack of diffusion models to couple planetary context with local detail, a compact Laplacian encoding to stabilize outputs across Earth-scale dynamic ranges, and an open-source infinite-tensor framework for constant-memory manipulation of unbounded tensors. Together, these components position diffusion models as a practical, scalable foundation for the next generation of infinite virtual worlds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Terrain Diffusionï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å–ä»£ Perlin noise ç­‰ä¼ ç»Ÿè¿‡ç¨‹å™ªå£°ï¼ˆprocedural noise functionsï¼‰çš„ç”Ÿæˆå¼æ¡†æ¶ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰å®ç°äº†é«˜åº¦é€¼çœŸä¸”å…·æœ‰é•¿ç¨‹ä¸€è‡´æ€§çš„æ— é™åœ°å½¢ç”Ÿæˆã€‚å…¶æ ¸å¿ƒç®—æ³• InfiniteDiffusion é‡æ–°å®šä¹‰äº†æ ‡å‡†æ‰©æ•£é‡‡æ ·ï¼Œä½¿å…¶èƒ½å¤„ç†æ— è¾¹ç•ŒåŒºåŸŸï¼ŒåŒæ—¶å…·å¤‡ç§å­ä¸€è‡´æ€§ï¼ˆseed-consistencyï¼‰å’Œå¸¸æ•°æ—¶é—´éšæœºè®¿é—®ï¼ˆconstant-time random accessï¼‰çš„ç‰¹æ€§ã€‚æ¡†æ¶é€šè¿‡æ•´åˆåˆ†å±‚æ‰©æ•£æ¨¡å‹æ ˆï¼ˆhierarchical stack of diffusion modelsï¼‰æ¥è€¦åˆå…¨çƒèƒŒæ™¯ä¸å±€éƒ¨ç»†èŠ‚ï¼Œå¹¶åˆ©ç”¨ç´§å‡‘çš„ Laplacian ç¼–ç æ¥ç¨³å®šåœ°çƒçº§åŠ¨æ€èŒƒå›´ä¸‹çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é…å¥—äº†ä¸€ä¸ªå¼€æºçš„æ— é™å¼ é‡æ¡†æ¶ï¼ˆinfinite-tensor frameworkï¼‰ï¼Œç¡®ä¿åœ¨å¸¸æ•°å†…å­˜å ç”¨ä¸‹è¿›è¡Œå¤§è§„æ¨¡æ•°æ®å¤„ç†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ¶ˆè´¹çº§ GPU ä¸Šçš„è¿è¡Œé€Ÿåº¦è¾¾åˆ°è½¨é“é€Ÿåº¦çš„ 9 å€ï¼Œå®ç°äº†äº¤äº’å¼é€Ÿç‡ä¸‹çš„å®æ—¶ç”Ÿæˆã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºä¸‹ä¸€ä»£é«˜ä¿çœŸæ— é™è™šæ‹Ÿä¸–ç•Œæä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://xandergos.github.io/terrain-diffusion/ Code: https://github.com/xandergos/terrain-diffusion/",
      "pdf_url": "https://arxiv.org/pdf/2512.08309v3",
      "published_date": "2025-12-09 07:10:35 UTC",
      "updated_date": "2026-01-11 12:34:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:32.402752+00:00"
    },
    {
      "arxiv_id": "2512.08300v1",
      "title": "rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection",
      "title_zh": "rSIMï¼šé€šè¿‡å¼ºåŒ–ç­–ç•¥æ³¨å…¥æ¿€å‘å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Sijia Chen",
        "Baochun Li",
        "Di Niu"
      ],
      "abstract": "Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†rSIMï¼Œä¸€ç§é€šè¿‡å¼ºåŒ–ç­–ç•¥æ³¨å…¥ï¼ˆReinforced Strategy Injection Mechanismï¼‰æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æœºåˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªå°å‹çš„plannerä½œä¸ºleader agentï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°ä¸ºLLMæ³¨å…¥self-reflectionå’Œdeep thinkingç­‰ç­–ç•¥ï¼Œä»è€Œå¼•å¯¼å…¶Chain-of-Thoughtï¼ˆCoTsï¼‰è¿‡ç¨‹ã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰å’Œleader-followeræ¡†æ¶ï¼Œplannerä¸ä½œä¸ºfollower agentçš„LLMå®ç°äº†åŸºäºè§„åˆ™å¥–åŠ±çš„è”åˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒrSIMèƒ½å¤Ÿä½¿Qwen2.5-0.5Båœ¨æ¨ç†è¡¨ç°ä¸Šæ˜¾è‘—è¶…è¶ŠQwen2.5-14Bã€‚æ­¤å¤–ï¼Œè¯¥plannerå…·æœ‰æå¼ºçš„æ³›åŒ–æ€§ï¼Œå¯ä½œä¸ºå³æ’å³ç”¨çš„ç»„ä»¶æå‡ç°æœ‰LLMsçš„èƒ½åŠ›ï¼Œå¹¶æ”¯æŒè·¨ä»»åŠ¡çš„æŒç»­å­¦ä¹ ï¼ˆcontinual learningï¼‰ï¼Œä½¿å…¶è§„åˆ’èƒ½åŠ›éšä»»åŠ¡ç§¯ç´¯ä¸æ–­å¢å¼ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 6 figures. Accepted to the ACL ARR July",
      "pdf_url": "https://arxiv.org/pdf/2512.08300v1",
      "published_date": "2025-12-09 06:55:39 UTC",
      "updated_date": "2025-12-09 06:55:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:32.281321+00:00"
    },
    {
      "arxiv_id": "2512.08296v2",
      "title": "Towards a Science of Scaling Agent Systems",
      "title_zh": "è¿ˆå‘æ™ºèƒ½ä½“ç³»ç»Ÿè§„æ¨¡åŒ–ç§‘å­¦",
      "authors": [
        "Yubin Kim",
        "Ken Gu",
        "Chanwoo Park",
        "Chunjong Park",
        "Samuel Schmidgall",
        "A. Ali Heydari",
        "Yao Yan",
        "Zhihan Zhang",
        "Yuchen Zhuang",
        "Mark Malhotra",
        "Paul Pu Liang",
        "Hae Won Park",
        "Yuzhe Yang",
        "Xuhai Xu",
        "Yilun Du",
        "Shwetak Patel",
        "Tim Althoff",
        "Daniel McDuff",
        "Xin Liu"
      ],
      "abstract": "Agents, language model-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored. We address this by deriving quantitative scaling principles for agent systems. We first formalize a definition for agentic evaluation and characterize scaling laws as the interplay between agent quantity, coordination structure, model capability, and task properties. We evaluate this across four benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. With five canonical agent architectures (Single-Agent and four Multi-Agent Systems: Independent, Centralized, Decentralized, Hybrid), instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations. We derive a predictive model using coordination metrics, that achieves cross-validated R^2=0.524, enabling prediction on unseen task domains. We identify three effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.8% on parallelizable tasks, while decentralized coordination excels on web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, every multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations. Out-of-sample validation on GPT-5.2, achieves MAE=0.071 and confirms four of five scaling principles generalize to unseen frontier models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿ(Agent Systems)çš„è§„æ¨¡åŒ–ç§‘å­¦ï¼Œé€šè¿‡æ¨å¯¼å®šé‡è§„æ¨¡åŒ–åŸç†(Scaling Laws)æ¥æ­ç¤ºå½±å“å…¶æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ç ”ç©¶è€…åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¯¹äº”ç§å…¸å‹çš„æ™ºèƒ½ä½“æ¶æ„ï¼ˆå•æ™ºèƒ½ä½“åŠå››ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼‰è¿›è¡Œäº†180é¡¹å—æ§è¯„ä¼°ï¼Œåˆ†æäº†æ•°é‡ã€åä½œç»“æ„ã€æ¨¡å‹èƒ½åŠ›ä¸ä»»åŠ¡å±æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ç ”ç©¶è¯†åˆ«äº†å·¥å…·ä¸åä½œçš„æƒè¡¡(tool-coordination trade-off)ä»¥åŠèƒ½åŠ›é¥±å’Œç°è±¡ï¼Œå‘ç°å½“å•æ™ºèƒ½ä½“åŸºå‡†è¶…è¿‡45%æ—¶ï¼Œåä½œä¼šäº§ç”Ÿè¾¹é™…æ”¶ç›Šé€’å‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸­å¿ƒåŒ–åä½œ(Centralized coordination)èƒ½æ˜¾è‘—æŠ‘åˆ¶é”™è¯¯æ”¾å¤§å¹¶åœ¨å¹¶è¡Œä»»åŠ¡ä¸­æå‡æ€§èƒ½ï¼Œä½†åœ¨é¡ºåºæ¨ç†ä»»åŠ¡ä¸­ï¼Œæ‰€æœ‰å¤šæ™ºèƒ½ä½“å˜ä½“åè€Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¯¥é¢„æµ‹æ¨¡å‹èƒ½å‡†ç¡®è¯†åˆ«87%æµ‹è¯•é…ç½®çš„æœ€ä¼˜åä½œç­–ç•¥ï¼Œå¹¶åœ¨GPT-5.2ç­‰å‰æ²¿æ¨¡å‹ä¸ŠéªŒè¯äº†è§„æ¨¡åŒ–åŸç†çš„æ³›åŒ–æ€§ã€‚è¯¥æˆæœä¸ºç†è§£å’Œä¼˜åŒ–å¤æ‚AIæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ‰©å±•æ€§æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘å’Œå®è·µå‡†åˆ™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08296v2",
      "published_date": "2025-12-09 06:52:21 UTC",
      "updated_date": "2025-12-17 02:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:28.522363+00:00"
    },
    {
      "arxiv_id": "2512.10991v1",
      "title": "MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax",
      "title_zh": "MolSculptï¼šåŸºäºåŒ–å­¦è¯­æ³•çš„3Dåˆ†å­å‡ ä½•ç»“æ„æ„å»º",
      "authors": [
        "Zhanpeng Chen",
        "Weihao Gao",
        "Shunyu Wang",
        "Yanan Zhu",
        "Hong Meng",
        "Yuexian Zou"
      ],
      "abstract": "Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that \"sculpts\" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \\textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MolSculptæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒ–å­¦è¯­æ³•(Chemical Syntax)æ„å»ºç²¾ç¡®çš„ä¸‰ç»´åˆ†å­å‡ ä½•ç»“æ„ï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹åœ¨ä¸€ç»´è¯­æ³•ç”Ÿæˆä¸ä¸‰ç»´å‡ ä½•å®ç°ä¹‹é—´å­˜åœ¨çš„è„±èŠ‚é—®é¢˜ã€‚MolSculptåŸºäºå†»ç»“çš„ä¸€ç»´åˆ†å­åŸºç¡€æ¨¡å‹(Foundation Model)å’Œä¸‰ç»´åˆ†å­æ‰©æ•£æ¨¡å‹(Diffusion Model)æ„å»ºï¼Œé€šè¿‡å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢(Queries)ä»åŸºç¡€æ¨¡å‹ä¸­æå–å†…åœ¨çš„åŒ–å­¦çŸ¥è¯†ã€‚éšåï¼Œåˆ©ç”¨å¯è®­ç»ƒçš„æ˜ å°„å™¨(Projector)å°†è¿™äº›è·¨æ¨¡æ€ä¿¡æ¯æ³¨å…¥æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ï¼Œä»è€Œå¼•å¯¼ä¸‰ç»´å‡ ä½•çš„ç”Ÿæˆã€‚è¿™ç§è®¾è®¡é€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–å®ç°äº†å°†ä¸€ç»´æ½œåœ¨åŒ–å­¦çŸ¥è¯†ä¸ä¸‰ç»´ç”Ÿæˆè¿‡ç¨‹çš„æ·±åº¦é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMolSculptåœ¨ä»å¤´å¼€å§‹(de novo)åŠæ¡ä»¶æ€§ä¸‰ç»´åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³(SOTA)ã€‚åœ¨GEOM-DRUGSå’ŒQM9æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å‹å…·æœ‰å“è¶Šçš„ä¸‰ç»´ä¿çœŸåº¦å’Œç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10991v1",
      "published_date": "2025-12-09 06:48:46 UTC",
      "updated_date": "2025-12-09 06:48:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:41.504779+00:00"
    },
    {
      "arxiv_id": "2512.08290v2",
      "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
      "title_zh": "çŸ¥è¯†ç³»ç»ŸåŒ–ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ç”Ÿæ€ç³»ç»Ÿä¸­çš„å®‰å…¨ä¸å®‰å…¨æ€§",
      "authors": [
        "Shiva Gaire",
        "Srijan Gyawali",
        "Saroj Mishra",
        "Suman Niroula",
        "Dilip Thakur",
        "Umesh Yadav"
      ],
      "abstract": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¢«èª‰ä¸ºæ™ºèƒ½ä½“AI(Agentic AI)æ ‡å‡†æ¥å£çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)ï¼Œé¦–æ¬¡è¿›è¡Œäº†å…¨é¢çš„çŸ¥è¯†ç³»ç»ŸåŒ–(SoK)åˆ†æã€‚æ–‡ç« æ·±å…¥æ¢è®¨äº†MCPåœ¨è§£å†³äº’æ“ä½œæ€§çš„åŒæ—¶æ‰€å¼•å…¥çš„Securityä¸Safetyé£é™©ï¼Œæå‡ºäº†ä¸€å¥—åŒºåˆ†å¯¹æŠ—æ€§å®‰å…¨å¨èƒï¼ˆå¦‚é—´æ¥æç¤ºæ³¨å…¥Indirect Prompt Injectionå’Œå·¥å…·ä¸­æ¯’Tool Poisoningï¼‰ä¸è®¤çŸ¥å®‰å…¨å±å®³ï¼ˆå¦‚åˆ†å¸ƒå¼å·¥å…·å§”æ´¾ä¸­çš„å¯¹é½å¤±è´¥Alignment Failuresï¼‰çš„é£é™©åˆ†ç±»æ³•ã€‚é€šè¿‡å¯¹Resourcesã€Promptså’ŒToolsç­‰MCPåŸè¯­çš„ç»“æ„æ€§æ¼æ´åˆ†æï¼Œè®ºæ–‡å±•ç¤ºäº†æ”»å‡»è€…å¦‚ä½•åˆ©ç”¨â€œä¸Šä¸‹æ–‡â€åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è§¦å‘æœªç»æˆæƒçš„æ“ä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹åŒ…æ‹¬åŠ å¯†æº¯æº(ETDI)å’Œè¿è¡Œæ—¶æ„å›¾éªŒè¯(Runtime Intent Verification)åœ¨å†…çš„å‰æ²¿é˜²å¾¡æŠ€æœ¯è¿›è¡Œäº†ç»¼è¿°ã€‚æœ€åï¼Œä½œè€…ä¸ºå®ç°ä»å¯¹è¯å¼èŠå¤©æœºå™¨äººå‘è‡ªä¸»æ™ºèƒ½ä½“æ“ä½œç³»ç»Ÿ(Agentic Operating Systems)çš„å¹³ç¨³å®‰å…¨è¿‡æ¸¡æä¾›äº†æŠ€æœ¯è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "All authors contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2512.08290v2",
      "published_date": "2025-12-09 06:39:21 UTC",
      "updated_date": "2025-12-13 20:37:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:11:59.058897+00:00"
    },
    {
      "arxiv_id": "2512.08286v1",
      "title": "Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework",
      "title_zh": "SolidGPTï¼šèµ‹èƒ½æ™ºèƒ½åº”ç”¨å¼€å‘çš„è¾¹äº‘ååŒ AI æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Liao Hu",
        "Qiteng Wu",
        "Ruoyu Qi"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SolidGPTï¼Œä¸€ä¸ªå¼€æºçš„è¾¹ç¼˜-äº‘æ··åˆï¼ˆedge-cloud hybridï¼‰å¼€å‘è€…åŠ©æ‰‹æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡è½¯ä»¶å¼€å‘ä¸­å¯¹è¯­ä¹‰æ„ŸçŸ¥ã€ç”Ÿäº§åŠ›å’Œæ•°æ®éšç§çš„éœ€æ±‚ã€‚SolidGPT å…è®¸å¼€å‘è€…é€šè¿‡â€œä¸ä»£ç åº“å¯¹è¯â€ï¼ˆtalk to your codebaseï¼‰åŠŸèƒ½äº¤äº’å¼æŸ¥è¯¢é¡¹ç›®ç»“æ„ï¼Œå¹¶èƒ½è‡ªåŠ¨åŒ–ç”Ÿæˆ PRDã€ä»»åŠ¡åˆ†è§£å’Œçœ‹æ¿ç­‰é¡¹ç›®ç®¡ç†å·¥ä½œæµã€‚è¯¥æ¡†æ¶æ”¯æŒç”¨æˆ·é€šè¿‡ Docker æˆ– VSCode æ‰©å±•éƒ¨ç½²ç§äººä¸”å¯æ‰©å±•çš„æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨åµŒå…¥ï¼ˆembeddingï¼‰å’Œä¸Šä¸‹æ–‡è®­ç»ƒå®ç° AI è§’è‰²çš„ä¸ªæ€§åŒ–å®šåˆ¶ã€‚å…¶éšç§ä¼˜å…ˆï¼ˆPrivacy-firstï¼‰çš„è®¾è®¡ç¡®ä¿äº†ä»£ç æ•°æ®çš„æœ¬åœ°å—æ§ï¼ŒåŒæ—¶èƒ½æ ¹æ®éœ€æ±‚çµæ´»è°ƒç”¨äº‘ç«¯ LLM èƒ½åŠ›ã€‚é€šè¿‡é›†æˆæ·±åº¦è¯­ä¹‰æœç´¢å’Œè‡ªåŠ¨åŒ–è„šæ‰‹æ¶ï¼ˆscaffoldingï¼‰æŠ€æœ¯ï¼ŒSolidGPT æ˜¾è‘—æå‡äº†ä»£ç å¯¼èˆªæ•ˆç‡å¹¶ç®€åŒ–äº†æ–‡æ¡£åŒæ­¥è¿‡ç¨‹ã€‚è¿™ç§ç»“åˆäº†äº¤äº’å¼æŸ¥è¯¢ä¸äººç±»-AI åä½œçš„æ¨¡å¼ï¼Œä¸ºç°ä»£æ™ºèƒ½è½¯ä»¶å·¥ç¨‹æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å°Šé‡éšç§çš„ç”Ÿäº§åŠ›åŠ é€Ÿæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08286v1",
      "published_date": "2025-12-09 06:34:28 UTC",
      "updated_date": "2025-12-09 06:34:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:12.639230+00:00"
    },
    {
      "arxiv_id": "2512.11887v1",
      "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions",
      "title_zh": "æ¨è¿›è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæµ‹è¯•ï¼šéœ€æ±‚ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘",
      "authors": [
        "Yihan Liao",
        "Jingyu Zhang",
        "Jacky Keung",
        "Yan Xiao",
        "Yurou Dai"
      ],
      "abstract": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ (Autonomous Driving Systems, ADSs) æµ‹è¯•çš„éœ€æ±‚ã€æŒ‘æˆ˜åŠæœªæ¥æ–¹å‘ï¼Œæ¶µç›–äº†æ¨¡å—åŒ–å’Œç«¯åˆ°ç«¯ç³»ç»Ÿçš„æµ‹è¯•å®è·µã€‚é€šè¿‡å¯¹100åå·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œå‚ä¸è€…çš„å¤§è§„æ¨¡è°ƒç ”ï¼Œå¹¶ç»“åˆ105é¡¹ä»£è¡¨æ€§ç ”ç©¶çš„æ·±å…¥åˆ†æï¼Œè¯¥è®ºæ–‡æ­ç¤ºäº†ç°æœ‰ç ”ç©¶ä¸å®é™…éœ€æ±‚ä¹‹é—´çš„å…³é”®å·®è·ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºå½“å‰çš„æµ‹è¯•æŠ€æœ¯åœ¨å¤„ç†æç«¯åœºæ™¯ (corner case) å¤šæ ·æ€§ã€ä»¿çœŸä¸ç°å®å·®è· (simulation to reality gap)ã€ç³»ç»Ÿæ€§æµ‹è¯•æ ‡å‡†ç¼ºå¤±ä»¥åŠ V2X éƒ¨ç½²æŒ‘æˆ˜ç­‰æ–¹é¢ä»é¢ä¸´æ˜¾è‘—å›°éš¾ã€‚åŒæ—¶ï¼Œè®ºæ–‡åˆ†æäº†åŸºç¡€æ¨¡å‹ (foundation models) åœ¨å¢å¼ºæµ‹è¯•ä¸­çš„ä½œç”¨åŠå…¶å¸¦æ¥çš„é«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚æœ€åï¼Œä½œè€…æ€»ç»“äº†å½“å‰ç ”ç©¶çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†åŒ…æ‹¬è·¨æ¨¡å‹åä½œã€è·¨æ¨¡æ€é€‚é…å’Œå¯æ‰©å±•éªŒè¯æ¡†æ¶åœ¨å†…çš„æœªæ¥ç ”ç©¶è·¯å¾„ï¼Œä¸ºæ„å»ºå¯é çš„è‡ªåŠ¨é©¾é©¶è¯„ä¼°ä½“ç³»æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted for publication in Information and Software Technology (IST)",
      "pdf_url": "https://arxiv.org/pdf/2512.11887v1",
      "published_date": "2025-12-09 06:33:27 UTC",
      "updated_date": "2025-12-09 06:33:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:56.585321+00:00"
    },
    {
      "arxiv_id": "2512.08280v1",
      "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making",
      "title_zh": "ç¦»çº¿å†³ç­–ä¸­é¢å‘é¢„æµ‹æ§åˆ¶çš„åŸºäºæ¨¡å‹æ‰©æ•£é‡‡æ ·",
      "authors": [
        "Haldun Balim",
        "Na Li",
        "Yilun Du"
      ],
      "abstract": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Model Predictive Diffuser (MPDiffuser)ï¼Œè¿™æ˜¯ä¸€ç§ç»„åˆå¼çš„åŸºäºæ¨¡å‹çš„æ‰©æ•£æ¡†æ¶ (Model-Based diffusion framework)ï¼Œæ—¨åœ¨è§£å†³ç¦»çº¿å†³ç­– (Offline decision-making) ä¸­ç”Ÿæˆçš„è½¨è¿¹åœ¨åŠ¨åŠ›å­¦ä¸Šä¸å¯è¡Œçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†ç”Ÿæˆä»»åŠ¡å¯¹é½è½¨è¿¹çš„ Plannerã€ç¡®ä¿ç³»ç»ŸåŠ¨åŠ›å­¦ä¸€è‡´æ€§çš„ Dynamics model ä»¥åŠç­›é€‰æœ€ä¼˜è¡Œä¸ºçš„ Ranker æ¨¡å—ã€‚é€šè¿‡é‡‡ç”¨äº¤æ›¿æ‰©æ•£é‡‡æ · (Alternating diffusion sampling) æ–¹æ¡ˆï¼ŒMPDiffuser åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­äº¤æ›¿è¿›è¡Œè§„åˆ’å™¨ä¸åŠ¨åŠ›å­¦æ›´æ–°ï¼Œä»è€Œåœ¨ç»†åŒ–è½¨è¿¹çš„åŒæ—¶ç¡®ä¿å…¶ä»»åŠ¡å¯¹é½åº¦å’Œç‰©ç†å¯è¡Œæ€§ã€‚ç†è®ºåˆ†æè¡¨æ˜è¯¥ç¨‹åºæœ‰æ•ˆå¹³è¡¡äº†æ•°æ®å…ˆéªŒä¸åŠ¨åŠ›å­¦ä¸€è‡´æ€§ï¼Œå®éªŒè¯æ˜è¿™ç§è®¾è®¡æ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶èƒ½çµæ´»é€‚åº”æ–°çš„åŠ¨åŠ›å­¦ç¯å¢ƒã€‚è¯¥æ–¹æ³•åœ¨ D4RL å’Œ DSRL ç¦»çº¿å†³ç­–åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ•ˆæœï¼Œå¹¶æˆåŠŸæ‰©å±•è‡³åŸºäºè§†è§‰çš„æ§åˆ¶ä»»åŠ¡åŠçœŸå®å››è¶³æœºå™¨äººçš„éƒ¨ç½²ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†é«˜ç»´æ„Ÿå®˜è¾“å…¥å’Œç°å®ä¸–ç•Œæ§åˆ¶ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08280v1",
      "published_date": "2025-12-09 06:26:02 UTC",
      "updated_date": "2025-12-09 06:26:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:06.470084+00:00"
    },
    {
      "arxiv_id": "2512.14718v2",
      "title": "SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting",
      "title_zh": "SEEDï¼šé¢å‘å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„è°±ç†µå¼•å¯¼æ—¶ç©ºä¾èµ–æ€§è¯„ä¼°",
      "authors": [
        "Feng Xiong",
        "Zongxia Xie",
        "Yanru Sun",
        "Haoyu Wang",
        "Jianhong Lin"
      ],
      "abstract": "Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \\textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SEEDï¼Œä¸€ç§åŸºäºè°±ç†µå¼•å¯¼(Spectral Entropy-guided)çš„æ—¶ç©ºä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é€šé“ç‹¬ç«‹æ€§(Channel Independence, CI)ä¸é€šé“ä¾èµ–æ€§(Channel Dependence, CD)çš„å¹³è¡¡ã€è´Ÿç›¸å…³æ€§å¿½ç•¥åŠæ—¶é—´ä½ç½®æ„ŸçŸ¥ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚SEEDçš„æ ¸å¿ƒæ˜¯Dependency Evaluatorï¼Œå®ƒåˆ©ç”¨è°±ç†µåŠ¨æ€è¯„ä¼°å˜é‡çš„æ—¶ç©ºä¾èµ–ï¼Œä½¿æ¨¡å‹èƒ½è‡ªé€‚åº”åœ°åˆ‡æ¢CIå’ŒCDç­–ç•¥ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†Spectral Entropy-based Fuseræ¥åŒºåˆ†å†…åœ¨åŠ¨åŠ›å­¦ä¸å¤–éƒ¨å˜é‡å¹²æ‰°ï¼Œå¹¶é€šè¿‡Signed Graph Constructorå…‹æœäº†softmaxé™åˆ¶ï¼Œå®ç°äº†å¯¹è´Ÿç›¸å…³æ€§çš„æœ‰æ•ˆå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒContext Spatial Extractoråˆ©ç”¨å±€éƒ¨ä¸Šä¸‹æ–‡çª—å£æå‡äº†å˜é‡å¯¹æ—¶é—´ä½ç½®çš„æ„ŸçŸ¥å’Œç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ã€‚åœ¨12ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSEEDè¾¾åˆ°äº†State-of-the-artçš„é¢„æµ‹æ€§èƒ½ï¼Œå±•ç°äº†æå¼ºçš„æ³›åŒ–æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14718v2",
      "published_date": "2025-12-09 06:18:05 UTC",
      "updated_date": "2025-12-18 04:24:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:07.903415+00:00"
    },
    {
      "arxiv_id": "2512.09001v1",
      "title": "A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography",
      "title_zh": "ç‰©ç†çº¦æŸä¸è®¾è®¡é©±åŠ¨çš„å…‰å­¦å…‰åˆ»ç¼ºé™·æ•°æ®é›†ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "Yuehua Hu",
        "Jiyeong Kong",
        "Dong-yeol Shin",
        "Jaekyun Kim",
        "Kyung-Tae Kang"
      ],
      "abstract": "The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¾®çº³åˆ¶é€ é¢†åŸŸ AI ç¼ºé™·æ£€æµ‹é¢ä¸´çš„é«˜è´¨é‡ã€å…·å¤‡ç‰©ç†ä¾æ®çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç‰©ç†çº¦æŸã€è®¾è®¡é©±åŠ¨çš„ç¼ºé™·æ•°æ®é›†ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨å—ç‰©ç†çº¦æŸçš„å¯æ§æ•°å­¦å½¢æ€å­¦ï¼ˆMathematical Morphologyï¼‰æ“ä½œå¯¹åŸå§‹è®¾è®¡å¸ƒå±€è¿›è¡Œå¤„ç†ï¼Œä»å¤´åˆæˆç¼ºé™·å¸ƒå±€ï¼Œå¹¶åˆ©ç”¨é«˜ä¿çœŸæ•°å­—å¾®é•œå™¨ä»¶ï¼ˆDMDï¼‰å…‰åˆ»æŠ€æœ¯å°†å…¶åˆ¶å¤‡ä¸ºç‰©ç†æ ·æœ¬ã€‚é€šè¿‡å¯¹æ¯”å…‰å­¦æ˜¾å¾®å›¾ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å« 3,530 å¼ å›¾å’Œ 13,365 ä¸ªåƒç´ çº§æ ‡æ³¨å®ä¾‹çš„æ•°æ®é›†ï¼Œæ¶µç›– Bridgeã€Burrã€Pinch å’Œ Contamination å››ç±»ç¼ºé™·ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„ Mask R-CNN åœ¨æ£€æµ‹ç²¾åº¦ï¼ˆAP@0.5ï¼‰ä¸Šæ¯” Faster R-CNN æå‡äº†çº¦ 34% è‡³ 42%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨æ•°æ®æ–¹é¢çš„å¯è¡Œæ€§ï¼Œä¸ºåŠå¯¼ä½“åˆ¶é€ ä¸­é²æ£’çš„ AI æµ‹é‡ä¸æ£€æµ‹ï¼ˆMIï¼‰å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09001v1",
      "published_date": "2025-12-09 06:13:33 UTC",
      "updated_date": "2025-12-09 06:13:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:20.249726+00:00"
    },
    {
      "arxiv_id": "2512.08273v1",
      "title": "AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content",
      "title_zh": "AgentEvalï¼šç”Ÿæˆå¼æ™ºèƒ½ä½“ä½œä¸ºäººç±»è¯„ä¼° AI ç”Ÿæˆå†…å®¹çš„å¯é ä»£ç†",
      "authors": [
        "Thanh Vu",
        "Richi Nayak",
        "Thiru Balasubramaniam"
      ],
      "abstract": "Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ä¼ä¸šåœ¨ç”Ÿæˆå’Œè¯„ä¼°é«˜è´¨é‡å†…å®¹æ—¶é¢ä¸´çš„é«˜æ˜‚æ—¶é—´ä¸ç»æµæˆæœ¬é—®é¢˜ï¼Œæå‡ºäº† AgentEval æ¡†æ¶ï¼Œåˆ©ç”¨ Generative Agents ä½œä¸ºäººç±»è¯„ä¼° AI-Generated Content çš„å¯é ä»£ç†ã€‚è¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡æ¨¡æ‹Ÿäººç±»åˆ¤æ–­ï¼Œå¯¹å†…å®¹çš„ Coherenceã€Interestingnessã€Clarityã€Fairness å’Œ Relevance ç­‰å…³é”®ç»´åº¦è¿›è¡Œå¿«é€Ÿä¸”ä½æˆæœ¬çš„è‡ªåŠ¨è¯„ä¼°ã€‚é€šè¿‡é›†æˆè¿™ä¸€ç³»ç»Ÿï¼Œä¼ä¸šå¯ä»¥åœ¨å¤§å¹…å‡å°‘å¯¹é«˜æˆæœ¬äººç±»è¯„ä¼°ä¾èµ–çš„åŒæ—¶ï¼Œç¡®ä¿è¾“å‡ºå†…å®¹çš„é«˜è´¨é‡ä¸ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼º Large Language Models (LLMs) ç”Ÿæˆå•†ä¸šå¯¹é½å†…å®¹æä¾›äº†é‡è¦è§è§£ï¼Œæ˜¾è‘—æ¨è¿›äº†è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆä¸è¯„ä¼°æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.08273v1",
      "published_date": "2025-12-09 06:03:25 UTC",
      "updated_date": "2025-12-09 06:03:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:19.558806+00:00"
    },
    {
      "arxiv_id": "2512.08270v1",
      "title": "Reasoning Models Ace the CFA Exams",
      "title_zh": "æ¨ç†æ¨¡å‹é«˜åˆ†é€šè¿‡ CFA è€ƒè¯•",
      "authors": [
        "Jaisal Patel",
        "Yunzhe Chen",
        "Kaiwen He",
        "Keyi Wang",
        "David Li",
        "Kairong Xiao",
        "Xiao-Yang Liu"
      ],
      "abstract": "Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹(Reasoning Models)åœ¨ç‰¹è®¸é‡‘èåˆ†æå¸ˆ(CFA)è€ƒè¯•ä¸­çš„æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†æ­¤å‰å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸“ä¸šé‡‘èè€ƒæ ¸ä¸­è¡¨ç°ä¸ä½³çš„è§‚ç‚¹ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨äº†åŒ…å«980é“é¢˜ç›®ã€æ¶µç›–ä¸‰ä¸ªçº§åˆ«çš„æ¨¡æ‹Ÿè€ƒè¯•é›†ï¼Œå¯¹Gemini 3.0 Proã€GPT-5ã€DeepSeek-V3.1ç­‰ä¸»æµæ¨ç†æ¨¡å‹è¿›è¡Œäº†è¯¦å°½æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œç»å¤§å¤šæ•°å—è¯•æ¨¡å‹å‡èƒ½æ ¹æ®æ—¢å®šæ ‡å‡†é€šè¿‡å…¨éƒ¨ä¸‰ä¸ªçº§åˆ«çš„è€ƒè¯•ï¼Œå±•ç°å‡ºæå¼ºçš„ä¸“ä¸šçŸ¥è¯†ç†è§£ä¸é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚å…¶ä¸­ï¼ŒGemini 3.0 Proåœ¨Level Iè€ƒè¯•ä¸­åˆ›ä¸‹äº†97.6%çš„æé«˜åˆ†ï¼Œè€ŒGPT-5åˆ™ä»¥94.3%çš„æˆç»©åœ¨Level IIä¸­æ‹”å¾—å¤´ç­¹ã€‚åœ¨å¤æ‚çš„Level IIIè€ƒæ ¸ä¸­ï¼ŒGemini 2.5 Proåœ¨å¤šé€‰é¢˜éƒ¨åˆ†å¾—åˆ†æœ€é«˜ï¼ŒGemini 3.0 Proåˆ™åœ¨è®ºè¿°é¢˜(Constructed-response questions)éƒ¨åˆ†å–å¾—äº†92.0%çš„æˆç»©ã€‚è¯¥è®ºæ–‡çš„å‘ç°è¯å®äº†å½“ä»£æ¨ç†æ¨¡å‹åœ¨å¤„ç†é«˜éš¾åº¦ã€ä¸“ä¸šæ€§å¼ºçš„ç ”ç©¶ç”Ÿæ°´å¹³é‡‘èè€ƒè¯•æ–¹é¢å·²å–å¾—é‡å¤§çªç ´ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "q-fin.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08270v1",
      "published_date": "2025-12-09 05:57:19 UTC",
      "updated_date": "2025-12-09 05:57:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:36.253385+00:00"
    },
    {
      "arxiv_id": "2512.08261v1",
      "title": "Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes",
      "title_zh": "è¶…è¶Šä¼ ç»Ÿè¯Šæ–­ï¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸åŸå‹å°†æ‚£è€…ç«¯ä¿¡æ¯è½¬åŒ–ä¸ºé¢„æµ‹æ€§è§è§£",
      "authors": [
        "Yibowen Zhao",
        "Yinan Zhang",
        "Zhixiang Su",
        "Lizhen Cui",
        "Chunyan Miao"
      ],
      "abstract": "Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»…ä¾é æ‚£è€…ç«¯ä¿¡æ¯é¢„æµ‹ç–¾ç—…æ—¶é¢ä¸´çš„æ ·æœ¬åˆ†å¸ƒä¸å¹³è¡¡å’Œç¼ºä¹å¯è§£é‡Šæ€§ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº† KPI (Knowledge graph-enhanced, Prototype-aware, and Interpretable) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç»“æ„åŒ–çš„åŒ»ç–—çŸ¥è¯†æ•´åˆåˆ°ç»Ÿä¸€çš„ç–¾ç—…çŸ¥è¯†å›¾è°± (Knowledge Graph) ä¸­ï¼Œå¹¶æ„å»ºå…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç–¾ç—…åŸå‹ (Prototypes) æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚åŒæ—¶ï¼ŒKPI é‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹  (Contrastive Learning) æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†å¯¹äºé•¿å°¾ (Long-tailed) ç–¾ç—…çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚ä¸ºäº†æå‡ç»“æœçš„å¯ä¿¡åº¦ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆä¸ç‰¹å®šæ‚£è€…ç›¸å…³çš„åŒ»ç–—è§£é‡Šï¼Œä»è€Œå®ç°äº†é«˜åº¦çš„å¯è§£é‡Šæ€§ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒKPI åœ¨é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚è¯¥ç ”ç©¶èƒ½å¤Ÿæä¾›ä¸æ‚£è€…å™è¿°é«˜åº¦ä¸€è‡´çš„ä¸´åºŠæœ‰æ•ˆè§£é‡Šï¼Œä¸ºä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„åŒ»ç–—æœåŠ¡æä¾›äº†é‡è¦çš„å®è·µä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This work has been accepted by ICDE 2026 and is available on arXiv for early access",
      "pdf_url": "https://arxiv.org/pdf/2512.08261v1",
      "published_date": "2025-12-09 05:37:54 UTC",
      "updated_date": "2025-12-09 05:37:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:26.063623+00:00"
    },
    {
      "arxiv_id": "2512.08247v1",
      "title": "Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection",
      "title_zh": "åŸºäºæ©ç ç‰¹å¾é‡å»ºçš„æœªæ¥æ—¶åºçŸ¥è¯†è’¸é¦ç”¨äº3Dç›®æ ‡æ£€æµ‹",
      "authors": [
        "Haowen Zheng",
        "Hu Zhu",
        "Lu Deng",
        "Weihao Gu",
        "Yang Yang",
        "Yanyan Liang"
      ],
      "abstract": "Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Future Temporal Knowledge Distillation (FTKD)ï¼Œä¸€ç§åŸºäºç¨€ç–æŸ¥è¯¢çš„è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨å°†ç¦»çº¿æ•™å¸ˆæ¨¡å‹ä¸­çš„æœªæ¥å¸§çŸ¥è¯†æœ‰æ•ˆä¼ é€’ç»™åœ¨çº¿å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æå‡è‡ªåŠ¨é©¾é©¶ä¸­çš„3D Object Detectionæ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰Knowledge Distillation (KD) æ–¹æ³•éš¾ä»¥åœ¨æ²¡æœ‰ä¸¥æ ¼å¸§å¯¹é½çš„æƒ…å†µä¸‹å­¦ä¹ æœªæ¥çŸ¥è¯†çš„é—®é¢˜ï¼ŒFTKD å¼•å…¥äº†æœªæ¥æ„ŸçŸ¥ç‰¹å¾é‡å»ºç­–ç•¥ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹æ•æ‰æ½œåœ¨çš„æœªæ¥æ—¶ç©ºç‰¹å¾ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœªæ¥å¼•å¯¼çš„é€»è¾‘è’¸é¦åˆ©ç”¨æ•™å¸ˆæ¨¡å‹ä¸­ç¨³å®šçš„å‰åæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFTKD åœ¨ nuScenes æ•°æ®é›†ä¸Šä¸ºåŸºçº¿æ¨¡å‹å¸¦æ¥äº† 1.3 mAP å’Œ 1.3 NDS çš„å¢ç›Šï¼Œå¹¶å®ç°äº†æœ€ç²¾ç¡®çš„é€Ÿåº¦ä¼°è®¡ï¼Œä¸”åœ¨æ¨ç†é˜¶æ®µä¸äº§ç”Ÿé¢å¤–å¼€é”€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI-26",
      "pdf_url": "https://arxiv.org/pdf/2512.08247v1",
      "published_date": "2025-12-09 05:01:29 UTC",
      "updated_date": "2025-12-09 05:01:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:12:42.540549+00:00"
    },
    {
      "arxiv_id": "2512.08243v1",
      "title": "Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI",
      "title_zh": "Residual-SwinCA-Netï¼šç”¨äº BUSI æ¶æ€§ç—…å˜åˆ†å‰²çš„é€šé“æ„ŸçŸ¥é›†æˆæ®‹å·® CNN-Swin Transformer ç½‘ç»œ",
      "authors": [
        "Saeeda Naz",
        "Saddam Hussain Khan"
      ],
      "abstract": "A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºResidual-SwinCA-Netçš„æ·±åº¦æ··åˆåˆ†å‰²æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè§£å†³ä¹³è…ºè¶…å£°å›¾åƒ(BUSI)ä¸­æ¶æ€§ç—…å˜çš„ç²¾å‡†åˆ†å‰²éš¾é¢˜ã€‚è¯¥æ¶æ„ç»“åˆäº†æ®‹å·®CNNæ¨¡å—ä»¥æå–å±€éƒ¨é²æ£’ç‰¹å¾ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„Swin Transformerå—æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶å¼•å…¥Laplacian-of-GaussianåŒºåŸŸç®—å­ä¸è¾¹ç•Œå¯¼å‘ç®—å­æ¥æŠ‘åˆ¶è¶…å£°å™ªå£°å¹¶ç»´æŒç—…å˜è½®å»“çš„å®Œæ•´æ€§ã€‚æ¨¡å‹åœ¨è§£ç å™¨é˜¶æ®µé›†æˆäº†åˆ›æ–°çš„å¤šå°ºåº¦é€šé“æ³¨æ„åŠ›ä¸æŒ¤å‹(MSCAS)æ¨¡å—ä»¥åŠåƒç´ æ³¨æ„åŠ›(Pixel-Attention)æ¨¡å—ï¼Œæœ‰æ•ˆåœ°å¢å¼ºäº†æ˜¾è‘—ç‰¹å¾å›¾çš„è¡¨è¾¾å¹¶æŠ‘åˆ¶äº†èƒŒæ™¯å¹²æ‰°ã€‚é€šè¿‡åœ¨å…¬å¼€çš„BUSIæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒResidual-SwinCA-Netå–å¾—äº†99.29%çš„å¹³å‡å‡†ç¡®ç‡ã€98.74%çš„IoUå’Œ0.9041çš„Diceç³»æ•°ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„CNNså’ŒViTsæŠ€æœ¯ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†ä¹³è…ºç—…å˜çš„è¯Šæ–­æ•ˆç‡ï¼Œä¹Ÿä¸ºåŠæ—¶å‡†ç¡®çš„ä¸´åºŠå†³ç­–æä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "26 Pages, 10 Figures, 4 Tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08243v1",
      "published_date": "2025-12-09 04:52:30 UTC",
      "updated_date": "2025-12-09 04:52:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:14:09.966136+00:00"
    },
    {
      "arxiv_id": "2512.08240v1",
      "title": "HybridToken-VLM: Hybrid Token Compression for Vision-Language Models",
      "title_zh": "HybridToken-VLMï¼šé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„æ··åˆ Token å‹ç¼©",
      "authors": [
        "Jusheng Zhang",
        "Xiaoyang Guo",
        "Kaitong Cai",
        "Qinhan Lv",
        "Yijia Fan",
        "Wenhao Chai",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HTC-VLMï¼Œä¸€ç§æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å› è§†è§‰Tokenè¿‡å¤šè€Œå¯¼è‡´è®¡ç®—å¼€é”€å·¨å¤§çš„æ··åˆå‹ç¼©æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒé€šé“è®¾è®¡ï¼Œé€šè¿‡è¿ç»­è·¯å¾„ä¿ç•™ViT patchesçš„ç»†ç²’åº¦ç»†èŠ‚ï¼Œå¹¶åˆ©ç”¨MGVQé‡åŒ–åçš„ç¦»æ•£è·¯å¾„ä½œä¸ºç¬¦å·é”šç‚¹ï¼Œä¸¤è€…æœ€ç»ˆé€šè¿‡è§£è€¦æ³¨æ„åŠ›æ©ç å’Œç“¶é¢ˆå±‚å‹ç¼©ä¸ºå•ä¸ªvoco tokenã€‚åœ¨GQAã€MMBenchã€MMEç­‰ä¸ƒé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHTC-VLMå®ç°äº†87.2%çš„å¹³å‡æ€§èƒ½ä¿ç•™ï¼Œä»¥580:1çš„æé«˜å‹ç¼©æ¯”æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿè¿ç»­å‹ç¼©æ–¹æ³•ã€‚æ³¨æ„åŠ›æœºåˆ¶åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œå‹ç¼©åçš„Tokenèƒ½æœ‰æ•ˆåˆ©ç”¨ç¦»æ•£é”šç‚¹çš„è¯­ä¹‰å¼•å¯¼ä½œç”¨ã€‚è¯¥å·¥ä½œè¯æ˜äº†æç®€çš„æ··åˆè®¾è®¡èƒ½æœ‰æ•ˆå¹³è¡¡æ¨¡å‹æ•ˆç‡ä¸ä¿çœŸåº¦ï¼Œä¸ºå¯æ‰©å±•VLMsçš„ç ”å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08240v1",
      "published_date": "2025-12-09 04:48:38 UTC",
      "updated_date": "2025-12-09 04:48:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:14.506611+00:00"
    },
    {
      "arxiv_id": "2601.15292v2",
      "title": "A Mobile Application Front-End for Presenting Explainable AI Results in Diabetes Risk Estimation",
      "title_zh": "ç³–å°¿ç—…é£é™©è¯„ä¼°ä¸­å±•ç¤ºå¯è§£é‡Šäººå·¥æ™ºèƒ½ç»“æœçš„ç§»åŠ¨åº”ç”¨å‰ç«¯",
      "authors": [
        "Bernardus Willson",
        "Henry Anand Septian Radityo",
        "Raynard Tanadi",
        "Latifa Dwiyanti",
        "Saiful Akbar"
      ],
      "abstract": "Diabetes is a significant and continuously rising health challenge in Indonesia. Although many artificial intelligence (AI)-based health applications have been developed for early detection, most function as \"black boxes,\" lacking transparency in their predictions. Explainable AI (XAI) methods offer a solution, yet their technical outputs are often incomprehensible to non-expert users. This research aims to develop a mobile application front-end that presents XAI-driven diabetes risk analysis in an intuitive, understandable format. Development followed the waterfall methodology, comprising requirements analysis, interface design, implementation, and evaluation. Based on user preference surveys, the application adopts two primary visualization types - bar charts and pie charts - to convey the contribution of each risk factor. These are complemented by personalized textual narratives generated via integration with GPT-4o. The application was developed natively for Android using Kotlin and Jetpack Compose. The resulting prototype interprets SHAP (SHapley Additive exPlanations), a key XAI approach, into accessible graphical visualizations and narratives. Evaluation through user comprehension testing (Likert scale and interviews) and technical functionality testing confirmed the research objectives were met. The combination of visualization and textual narrative effectively enhanced user understanding (average score 4.31/5) and empowered preventive action, supported by a 100% technical testing success rate.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åº¦å°¼è¥¿äºšæ—¥ç›Šä¸¥å³»çš„ç³–å°¿ç—…æŒ‘æˆ˜ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿç›´è§‚å‘ˆç° Explainable AI (XAI) ç»“æœçš„ç§»åŠ¨åº”ç”¨å‰ç«¯ï¼Œä»¥è§£å†³ç°æœ‰ AI å¥åº·åº”ç”¨åœ¨é£é™©é¢„æµ‹ä¸­ç¼ºä¹é€æ˜åº¦çš„â€œé»‘ç›’â€é—®é¢˜ã€‚ç ”ç©¶é‡‡ç”¨ waterfall methodology è¿›è¡Œå¼€å‘ï¼Œåˆ©ç”¨ Kotlin å’Œ Jetpack Compose æ„å»ºäº† Android åŸç”Ÿåº”ç”¨åŸå‹ã€‚è¯¥åº”ç”¨é€šè¿‡æ¡å½¢å›¾å’Œé¥¼å›¾å°†å¤æ‚çš„ SHAP (SHapley Additive exPlanations) æ•°æ®è½¬åŒ–ä¸ºæ˜“æ‡‚çš„å¯è§†åŒ–ç•Œé¢ï¼Œå¹¶é›†æˆ GPT-4o ç”Ÿæˆä¸ªæ€§åŒ–çš„æ–‡æœ¬å™è¿°ã€‚è¯„ä¼°è¿‡ç¨‹ç»“åˆäº† Likert scale ç”¨æˆ·ç†è§£æµ‹è¯•ã€è®¿è°ˆä»¥åŠæŠ€æœ¯åŠŸèƒ½æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºæŠ€æœ¯æˆåŠŸç‡è¾¾ 100%ã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œå¯è§†åŒ–ä¸æ–‡æœ¬å™è¿°çš„ç»“åˆæ˜¾è‘—æå‡äº†éä¸“å®¶ç”¨æˆ·å¯¹é£é™©å› ç´ çš„ç†è§£ï¼ˆå¹³å‡åˆ† 4.31/5ï¼‰ã€‚è¯¥ç ”ç©¶é€šè¿‡æé«˜é¢„æµ‹ç»“æœçš„å¯è§£é‡Šæ€§ï¼Œæœ‰æ•ˆèµ‹èƒ½ç”¨æˆ·é‡‡å–ç³–å°¿ç—…é¢„é˜²è¡ŒåŠ¨ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper was accepted and presented at the 2025 IEEE International Conference on Data and Software Engineering (ICoDSE) on 29 October 2025 in Batam, Indonesia, and is currently awaiting publication. This version corrects the author name. No changes to the content",
      "pdf_url": "https://arxiv.org/pdf/2601.15292v2",
      "published_date": "2025-12-09 04:42:01 UTC",
      "updated_date": "2026-01-23 03:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:21.243655+00:00"
    },
    {
      "arxiv_id": "2512.08238v1",
      "title": "SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality",
      "title_zh": "SpeechQualityLLMï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯­éŸ³è´¨é‡å¤šæ¨¡æ€è¯„ä¼°",
      "authors": [
        "Mahathir Monjur",
        "Shahriar Nirjon"
      ],
      "abstract": "Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SpeechQualityLLMï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ¨¡æ€è¯­éŸ³è´¨é‡è¯„ä¼°é—®ç­”ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå®¢è§‚è¯„ä¼°æŒ‡æ ‡å’Œå­¦ä¹ å‹æ¨¡å‹åœ¨äº¤äº’æ€§ã€è‡ªç„¶è¯­è¨€æŸ¥è¯¢åŠç”Ÿæˆæ–‡æœ¬åˆç†è§£é‡Šæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å°†éŸ³é¢‘ç¼–ç å™¨(audio encoder)ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨ NISQA è¯­æ–™åº“ä¸Šé€šè¿‡è¦†ç›–æ•´ä½“å¹³å‡ä¸»è§‚æ„è§åˆ†(MOS)åŠå™ªç‚¹(noisiness)ã€ç€è‰²(coloration)ã€ä¸è¿ç»­æ€§(discontinuity)å’Œå“åº¦(loudness)å››ä¸ªæ„ŸçŸ¥ç»´åº¦çš„æ¨¡æ¿åŒ–é—®ç­”å¯¹è¿›è¡Œè®­ç»ƒã€‚ç³»ç»Ÿæ”¯æŒå•ç«¯(single-ended)å’ŒåŒç«¯(double-ended)è®¾ç½®ï¼Œé‡‡ç”¨ç”Ÿæˆæ–‡æœ¬å›ç­”å¹¶ä»ä¸­è§£ææ•°å€¼é¢„æµ‹çš„æ–¹æ³•ï¼Œåœ¨æµ‹è¯•ä¸­å…¶åŒç«¯æ¨¡å‹çš„ MOS å¹³å‡ç»å¯¹è¯¯å·®(MAE)ä¸º 0.41ï¼Œçš®å°”é€Šç›¸å…³ç³»æ•°(Pearson correlation)è¾¾ 0.86ã€‚é™¤äº†é‡åŒ–æŒ‡æ ‡çš„æå‡ï¼ŒSpeechQualityLLM è¿˜æä¾›äº†çµæ´»çš„è‡ªç„¶è¯­è¨€æ¥å£ï¼Œä½¿ä»ä¸šè€…èƒ½å¤ŸæŸ¥è¯¢ç‰¹å®šçš„é™è´¨(degradations)ç»†èŠ‚ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿä¸åŒå¬è€…ç”»åƒ(listener profiles)æ¥æ•æ‰äººç±»è¯„ä¼°çš„å¤šæ ·æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ä»…å……å½“äº†éŸ³é¢‘è´¨é‡ä¸“å®¶çš„è§’è‰²ï¼Œè¿˜ä¸ºå‡å°‘å¯¹å¤§è§„æ¨¡ä¼—æµ‹(crowdsourced tests)çš„ä¾èµ–å¹¶é™ä½è¯„ä¼°æˆæœ¬æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 5 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.08238v1",
      "published_date": "2025-12-09 04:39:50 UTC",
      "updated_date": "2025-12-09 04:39:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:21.362676+00:00"
    },
    {
      "arxiv_id": "2512.08230v1",
      "title": "Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions",
      "title_zh": "èµ‹èƒ½å¢ç›Šä¸å› æœæ¨¡å‹æ„å»ºï¼šå„¿ç«¥ä¸æˆäººåœ¨å› æœå¹²é¢„ä¸­å¯¹å¯æ§æ€§å’Œå˜å¼‚æ€§çš„æ•æ„Ÿæ€§",
      "authors": [
        "Eunice Yiu",
        "Kelsey Allen",
        "Shiry Ginosar",
        "Alison Gopnik"
      ],
      "abstract": "Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called \"empowerment\" which maximizes mutual information between actions and their outcomes. \"Empowerment\" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å› æœæ¨¡å‹æ„å»ºä¸èµ‹èƒ½å¢ç›Šï¼ˆEmpowerment Gainï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œåˆ†æäº†å„¿ç«¥å’Œæˆäººåœ¨å› æœå¹²é¢„ä¸­å¯¹å¯æ§æ€§å’Œå˜å¼‚æ€§çš„æ•æ„Ÿåº¦ã€‚èµ‹èƒ½ï¼ˆEmpowermentï¼‰ä½œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰ä¸­æœ€å¤§åŒ–åŠ¨ä½œä¸ç»“æœé—´äº’ä¿¡æ¯çš„å†…åœ¨å¥–åŠ±ä¿¡å·ï¼Œæˆä¸ºäº†è¿æ¥ç»å…¸è´å¶æ–¯å› æœå­¦ä¹ ï¼ˆBayesian causal learningï¼‰ä¸å¼ºåŒ–å­¦ä¹ çš„å…³é”®æ¡¥æ¢ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå‡†ç¡®å› æœä¸–ç•Œæ¨¡å‹çš„æ„å»ºä¸èµ‹èƒ½çš„æå‡æ˜¯äº’ä¿ƒçš„è¿‡ç¨‹ï¼Œè¿™ä¸ºäººç±»å› æœå­¦ä¹ æä¾›äº†æ¯”æ ‡å‡†æ·±åº¦å­¦ä¹ æŠ€æœ¯æ›´å…·è®¡ç®—å¯è¡Œæ€§çš„è§£é‡Šæ¡†æ¶ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„å®è¯ç ”ç©¶ï¼Œä½œè€…éªŒè¯äº†äººç±»å¦‚ä½•åˆ©ç”¨èµ‹èƒ½çº¿ç´¢æ¥æ¨æ–­å¤æ‚çš„å› æœå…³ç³»å¹¶è®¾è®¡æœ‰æ•ˆçš„å¹²é¢„æ‰‹æ®µã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†äººç±»è®¤çŸ¥ä¸­å¤„ç†å› æœç»“æ„çš„åº•å±‚é€»è¾‘ï¼Œå¹¶ä¸ºæå‡äººå·¥æ™ºèƒ½çš„å› æœæ¨ç†èƒ½åŠ›æä¾›äº†ç†è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to Philosophical Transactions A, Special issue: World models, AGI, and the hard problems of life-mind continuity. Expected publication in 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.08230v1",
      "published_date": "2025-12-09 04:14:48 UTC",
      "updated_date": "2025-12-09 04:14:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:26.851467+00:00"
    },
    {
      "arxiv_id": "2512.08228v1",
      "title": "MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models",
      "title_zh": "MM-CoTï¼šç”¨äºæ¢æµ‹å¤šæ¨¡æ€æ¨¡å‹è§†è§‰é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›çš„åŸºå‡†",
      "authors": [
        "Jusheng Zhang",
        "Kaitong Cai",
        "Xiaoyang Guo",
        "Sidi Liu",
        "Qinhan Lv",
        "Ruiqi Chen",
        "Jing Yang",
        "Yijia Fan",
        "Xiaofei Sun",
        "Jian Wang",
        "Ziliang Chen",
        "Liang Lin",
        "Keze Wang"
      ],
      "abstract": "The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MM-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„è¯Šæ–­æ€§åŸºå‡†æµ‹è¯• (Benchmark)ï¼Œæ—¨åœ¨æ¢ç©¶å¤šæ¨¡æ€æ¨¡å‹ (Multimodal Models) åœ¨è§†è§‰é“¾å¼æ€ç»´ (Visual Chain-of-Thought) æ¨ç†è¿‡ç¨‹ä¸­çš„è§†è§‰é”šå®š (Visual Grounding) å’Œé€»è¾‘è¿è´¯æ€§ (Logical Coherence)ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•ä¾§é‡äºç”Ÿæˆè€Œå¿½è§†éªŒè¯çš„é—®é¢˜ï¼ŒMM-CoT è¦æ±‚æ¨¡å‹åœ¨å—åˆ°è§†è§‰ä¸€è‡´æ€§å’Œé€»è¾‘è¿è´¯æ€§åŒé‡çº¦æŸçš„åœºæ™¯ä¸‹ï¼Œä»åŒ…å«å¯¹æŠ—æ€§å¹²æ‰°é¡¹ (Adversarial Distractors) çš„é€‰é¡¹ä¸­è¯†åˆ«å‡ºå”¯ä¸€çš„æ­£ç¡®äº‹ä»¶é“¾ã€‚å…¶ä¸­è§†è§‰ä¸€è‡´æ€§ç¡®ä¿æ‰€æœ‰æ¨ç†æ­¥éª¤å‡é”šå®šåœ¨å¯è§‚å¯Ÿçš„è¯æ®ä¸Šï¼Œè€Œé€»è¾‘è¿è´¯æ€§åˆ™ä¿è¯äº†å› æœä¸å¸¸è¯†çš„æœ‰æ•ˆæ€§ã€‚å¯¹é¢†å…ˆè§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) çš„è¯„ä¼°æ­ç¤ºï¼Œå³ä¾¿æœ€å…ˆè¿›çš„ç³»ç»Ÿåœ¨å¤„ç†è¯¥ä»»åŠ¡æ—¶ä¹Ÿè¡¨ç°æ¬ ä½³ï¼Œåæ˜ å‡ºæ¨¡å‹ç”Ÿæˆçš„æµç•…åº¦ä¸çœŸå®æ¨ç†å¿ å®åº¦ (Reasoning Fidelity) ä¹‹é—´å­˜åœ¨æ˜¾è‘—è„±èŠ‚ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ MM-CoT ä¸ä¼ ç»ŸåŸºå‡†æµ‹è¯•ç›¸å…³æ€§è¾ƒä½ï¼Œä¸ºæœªæ¥å¼€å‘å…·å¤‡é«˜åº¦å¿ å®åº¦ä¸”ç¬¦åˆé€»è¾‘çš„è§†è§‰ä¸–ç•Œæ¨ç†æ¨¡å‹æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08228v1",
      "published_date": "2025-12-09 04:13:31 UTC",
      "updated_date": "2025-12-09 04:13:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:29.362922+00:00"
    },
    {
      "arxiv_id": "2512.14715v1",
      "title": "How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection",
      "title_zh": "æ¯”ç‰¹å¦‚ä½•åŒ–ä½œå™äº‹ï¼šåŸºäºå¯å¾®åˆ†æ•…éšœæ³¨å…¥çš„è¯­ä¹‰å¼•å¯¼",
      "authors": [
        "Zafaryab Haider",
        "Md Hafizur Rahman",
        "Shane Moeykens",
        "Vijay Devabhaktuni",
        "Prabuddha Chakraborty"
      ],
      "abstract": "Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æƒé‡ä¸­ä½çº§åˆ«çš„ä½åç§»ï¼ˆbit flipsï¼‰å¦‚ä½•å½±å“ç”Ÿæˆå›¾åƒæè¿°çš„è¯­ä¹‰å«ä¹‰ï¼ŒåŒæ—¶ä¿æŒå…¶è¯­æ³•ç»“æ„çš„å®Œæ•´æ€§ã€‚ä½œè€…æå‡ºäº† BLADEï¼ˆBit-level Fault Analysis via Differentiable Estimationï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦çš„å·®å¼‚åŒ–æ•…éšœåˆ†ææ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¢¯åº¦æ•æ„Ÿåº¦è¯„ä¼°æ¥å®šä½å¯¹è¯­ä¹‰è‡³å…³é‡è¦çš„æ¯”ç‰¹ä½ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡æ ‡é¢˜çº§çš„è¯­ä¹‰æµåˆ©åº¦ç›®æ ‡è¿›è¡Œä¼˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­ä¹‰æ¼‚ç§»å¹¶ééšæœºäº§ç”Ÿï¼Œè€Œæ˜¯å¯ä»¥é€šè¿‡æ¨¡å‹çš„æ¢¯åº¦è¿›è¡Œå¾®åˆ†ä¼°è®¡ï¼Œä»è€Œå®ç°å¯¹æ¯”ç‰¹çº§è¯­ä¹‰è¾“å‡ºçš„ç²¾å‡†å¼•å¯¼ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†å³ä¾¿æ˜¯ä¸æ˜“å¯Ÿè§‰çš„åº•å±‚ç¡¬ä»¶é”™è¯¯ä¹Ÿèƒ½æ˜¾è‘—é‡å¡‘ç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹çš„é«˜å±‚è¯­ä¹‰ã€‚è¿™ä¸ºæœªæ¥å¤§æ¨¡å‹çš„é²æ£’æ€§æµ‹è¯•ã€å¯¹æŠ—é˜²å¾¡ä»¥åŠå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆExplainable AIï¼‰çš„ç ”ç©¶æä¾›äº†å…¨æ–°çš„è§†è§’å’Œå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14715v1",
      "published_date": "2025-12-09 04:04:19 UTC",
      "updated_date": "2025-12-09 04:04:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:36.744422+00:00"
    },
    {
      "arxiv_id": "2512.08218v1",
      "title": "PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning",
      "title_zh": "PR-CapsNetï¼šé¢å‘å›¾å­¦ä¹ çš„è‡ªé€‚åº”æ›²ç‡è·¯ç”±ä¼ªé»æ›¼èƒ¶å›Šç½‘ç»œ",
      "authors": [
        "Ye Qin",
        "Jingchao Wang",
        "Yang Shi",
        "Haiying Huang",
        "Junxu Li",
        "Weijian Liu",
        "Tinghui Chen",
        "Jinghui Qin"
      ],
      "abstract": "Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\\-world graphs poorly by fixed\\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\\-Euclidean pseudo\\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\\-Riemannian manifolds and derive a Pseudo\\-Riemannian Capsule Network (PR\\-CapsNet), which models data in pseudo\\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\\-CapsNet enhances the CapsNet with Adaptive Pseudo\\-Riemannian Tangent Space Routing by utilizing pseudo\\-Riemannian geometry. Unlike single\\-curvature or subspace\\-partitioning methods, PR\\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\\-Riemannian metric. It first deploys Pseudo\\-Riemannian Tangent Space Routing to decompose capsule states into spherical\\-temporal and Euclidean\\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\\-preserved Pseudo\\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\\-CapsNet outperforms SOTA models, validating PR\\-CapsNet's strong representation power for complex graph structures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PR-CapsNetï¼Œä¸€ç§åŸºäºè‡ªé€‚åº”æ›²ç‡è·¯ç”±çš„ä¼ªé»æ›¼èƒ¶å›Šç½‘ç»œ(Pseudo-Riemannian Capsule Network)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿèƒ¶å›Šç½‘ç»œ(Capsule Networks)åœ¨å›ºå®šæ›²ç‡ç©ºé—´ä¸­å› æµ‹åœ°çº¿ä¸è¿é€šé—®é¢˜è€Œéš¾ä»¥å¯¹å¤æ‚å›¾å‡ ä½•å»ºæ¨¡çš„å±€é™ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¼ªé»æ›¼æ­£åˆ‡ç©ºé—´è·¯ç”±(Pseudo-Riemannian Tangent Space Routing)ï¼Œåˆ©ç”¨å¾®åˆ†åŒèƒšå˜æ¢å°†èƒ¶å›ŠçŠ¶æ€åˆ†è§£ä¸ºçƒé¢-æ—¶é—´(spherical-temporal)å’Œæ¬§å‡ é‡Œå¾—-ç©ºé—´(Euclidean-spatial)å­ç©ºé—´ï¼Œä»è€ŒåŒæ—¶æ•æ‰å±‚æ¬¡ã€é›†ç¾¤å’Œå¾ªç¯ç­‰å¤šç§å›¾ç»“æ„ç‰¹å¾ã€‚é€šè¿‡è‡ªé€‚åº”æ›²ç‡è·¯ç”±(Adaptive Curvature Routing)å’Œå‡ ä½•æ³¨æ„åŠ›æœºåˆ¶ï¼ŒPR-CapsNetèƒ½å¤Ÿæ ¹æ®å±€éƒ¨æµå½¢å±æ€§åŠ¨æ€èåˆæ¥è‡ªä¸åŒæ›²ç‡ç©ºé—´çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¿æŒå‡ ä½•å±æ€§çš„ä¼ªé»æ›¼èƒ¶å›Šåˆ†ç±»å™¨(Pseudo-Riemannian Capsule Classifier)ï¼Œé€šè¿‡æ›²ç‡åŠ æƒçš„softmaxæå‡åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPR-CapsNetåœ¨èŠ‚ç‚¹å’Œå›¾åˆ†ç±»åŸºå‡†ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„SOTAæ¨¡å‹ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶å¯¹å¤æ‚å›¾ç»“æ„çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in WSDM 2026 (ACM International Conference on Web Search and Data Mining)",
      "pdf_url": "https://arxiv.org/pdf/2512.08218v1",
      "published_date": "2025-12-09 03:54:51 UTC",
      "updated_date": "2025-12-09 03:54:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:38.980885+00:00"
    },
    {
      "arxiv_id": "2512.10990v1",
      "title": "Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI",
      "title_zh": "Doraï¼šé¢å‘åˆ†å¸ƒå¼è¾¹ç¼˜ AI çš„ QoE æ„ŸçŸ¥æ··åˆå¹¶è¡Œ",
      "authors": [
        "Jianli Jin",
        "Ziyang Lin",
        "Qianli Dong",
        "Yi Chen",
        "Jayanth Srinivasa",
        "Myungjin Lee",
        "Zhaowei Tan",
        "Fan Lai"
      ],
      "abstract": "With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.\n  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜äººå·¥æ™ºèƒ½(Edge AI)åº”ç”¨ä¸­èµ„æºå—é™ä¸”éœ€æ»¡è¶³ç”¨æˆ·ä½“éªŒè´¨é‡(QoE)çš„éœ€æ±‚ï¼Œæå‡ºäº†Doraæ¡†æ¶ã€‚Doraæ˜¯ä¸€ä¸ªç”¨äºåˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†çš„QoEæ„ŸçŸ¥å‹æ··åˆå¹¶è¡Œ(Hybrid Parallelism)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ¡ˆå› å¿½è§†QoEè€Œå¯¼è‡´çš„èµ„æºä½æ•ˆæˆ–è¿åå»¶è¿Ÿè¦æ±‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼‚æ„æ„ŸçŸ¥æ¨¡å‹åˆ†åŒºå™¨(Heterogeneity-aware model partitioner)ä¼˜åŒ–è®¾å¤‡é—´çš„æ¨¡å‹åˆ’åˆ†ï¼Œåˆ©ç”¨ç«äº‰æ„ŸçŸ¥ç½‘ç»œè°ƒåº¦å™¨(Contention-aware network scheduler)æœ€å¤§åŒ–è®¡ç®—ä¸é€šä¿¡é‡å ï¼Œå¹¶ç»“åˆè¿è¡Œæ—¶é€‚é…å™¨(Runtime adapter)åŠ¨æ€è°ƒæ•´ç­–ç•¥ä»¥åº”å¯¹ç¯å¢ƒå˜åŒ–ã€‚åœ¨æ™ºèƒ½å®¶å±…ã€äº¤é€šåˆ†æç­‰ä»£è¡¨æ€§è¾¹ç¼˜åœºæ™¯çš„éƒ¨ç½²å®éªŒè¡¨æ˜ï¼ŒDoraå¯å®ç°1.1è‡³6.3å€çš„æ‰§è¡ŒåŠ é€Ÿã€‚æ­¤å¤–ï¼Œåœ¨ç»´æŒQoEçš„å‰æä¸‹ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½æœ‰æ•ˆé™ä½21%è‡³82%çš„èƒ½è€—ï¼Œä¸ºåˆ†å¸ƒå¼è¾¹ç¼˜AIæä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„è¿è¡Œä¿éšœã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10990v1",
      "published_date": "2025-12-09 03:19:16 UTC",
      "updated_date": "2025-12-09 03:19:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:13:52.647268+00:00"
    },
    {
      "arxiv_id": "2512.08193v1",
      "title": "ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access",
      "title_zh": "ClinicalTrialsHubï¼šè¿æ¥æ³¨å†Œä¸­å¿ƒä¸å­¦æœ¯æ–‡çŒ®ï¼Œå®ç°å…¨é¢çš„ä¸´åºŠè¯•éªŒä¿¡æ¯è·å–",
      "authors": [
        "Jiwoo Park",
        "Ruoqi Liu",
        "Avani Jagdale",
        "Andrew Srisuwananukorn",
        "Jing Zhao",
        "Lang Li",
        "Ping Zhang",
        "Sachin Kumar"
      ],
      "abstract": "We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ClinicalTrialsHubï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ•´åˆ ClinicalTrials.gov æ³¨å†Œä¿¡æ¯ä¸ PubMed ç ”ç©¶æ–‡çŒ®çš„äº¤äº’å¼æœç´¢å¹³å°ï¼Œé€šè¿‡è‡ªåŠ¨æå–å’Œç»“æ„åŒ–å…¨æ–‡æ–‡ç« ä¸­çš„è¯•éªŒç›¸å…³ä¿¡æ¯æ¥å¢å¼ºæ•°æ®çš„å¯è®¿é—®æ€§ã€‚ä¸ä»…ä¾èµ– ClinicalTrials.gov ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿå°†ç»“æ„åŒ–ä¸´åºŠè¯•éªŒæ•°æ®çš„è·å–é‡æå‡äº† 83.8%ï¼Œæ˜¾è‘—ä¾¿åˆ©äº†æ‚£è€…ã€ä¸´åºŠåŒ»ç”ŸåŠç ”ç©¶äººå‘˜çš„å¾ªè¯åŒ»ç–—å†³ç­–ã€‚è¯¥å¹³å°åˆ©ç”¨ GPT-5.1 å’Œ Gemini-3-Pro ç­‰å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œå®ç°äº†ä»æ–‡çŒ®ä¸­è‡ªåŠ¨è§£æè¯•éªŒä¿¡æ¯ã€å°†ç”¨æˆ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®åº“æœç´¢ï¼Œå¹¶æä¾›åŸºäºè¯æ®æ¥æºçš„å½’å› é—®ç­”ç³»ç»Ÿ (attributed question-answering system)ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ¶‰åŠä¸´åºŠåŒ»ç”Ÿã€è¯å­¦åŠæŠ¤ç†å­¦åšå£«ç­‰ä¸“ä¸šäººå‘˜çš„ç”¨æˆ·è°ƒç ”ï¼Œä»¥åŠå¯¹ä¿¡æ¯æå–å’Œé—®ç­”èƒ½åŠ›çš„ç³»ç»ŸåŒ–è‡ªåŠ¨è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥å¹³å°åœ¨è¾…åŠ©ä¸´åºŠç ”ç©¶æ–¹é¢çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08193v1",
      "published_date": "2025-12-09 02:52:06 UTC",
      "updated_date": "2025-12-09 02:52:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:14:43.098963+00:00"
    },
    {
      "arxiv_id": "2512.14714v2",
      "title": "Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms",
      "title_zh": "ç»“åˆå¯å­¦ä¹  Gabor æ»¤æ³¢å·ç§¯ä¸æ³¨æ„åŠ›æœºåˆ¶æå‡æ°´å£°åˆ†ç±»æ€§èƒ½",
      "authors": [
        "Lucas Cesar Ferreira Domingos",
        "Russell Brinkworth",
        "Paulo Eduardo Santos",
        "Karl Sammut"
      ],
      "abstract": "Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complexity of ship-radiated and environmental noise poses significant challenges for accurate signal processing. While recent advancements in machine learning have improved classification accuracy, limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated using three training-test split strategies that reflect increasingly complex classification tasks, demonstrating how systematic evaluation design addresses issues such as data leakage, temporal separation, and taxonomy. Results show that GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, adding Gabor convolutions to the initial layers of the model reduced training time by up to 62%. During the evaluation of training-testing splits, temporal separation between subsets significantly affected performance, proving more influential than training data volume. These findings suggest that signal processing can enhance model reliability and generalisation under varying environmental conditions, particularly in data-limited underwater acoustic classification. Future developments should focus on mitigating environmental effects on input signals.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GSE ResNeXtæ¶æ„ï¼Œæ—¨åœ¨è§£å†³æ°´ä¸‹å£°å­¦ç›®æ ‡åˆ†ç±»ä¸­é¢ä¸´çš„å¤æ‚ç¯å¢ƒå™ªå£°ã€æ•°æ®é›†å—é™ä»¥åŠå®éªŒæ ‡å‡†åŒ–ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆå¯å­¦ä¹ çš„Gaborå·ç§¯å±‚ï¼ˆä½œä¸ºäºŒç»´è‡ªé€‚åº”å¸¦é€šæ»¤æ³¢å™¨ï¼‰å’Œå¸¦æœ‰Squeeze-and-Excitation (SE)æ³¨æ„åŠ›æœºåˆ¶çš„ResNeXtéª¨å¹²ç½‘ç»œï¼Œæ˜¾è‘—å¢å¼ºäº†æå–åŒºåˆ†æ€§ç‰¹å¾çš„èƒ½åŠ›å¹¶æå‡äº†è®­ç»ƒç¨³å®šæ€§ã€‚ç ”ç©¶è€…é‡‡ç”¨äº†ä¸‰ç§è®­ç»ƒ-æµ‹è¯•æ‹†åˆ†ç­–ç•¥ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†æ•°æ®æ³„éœ²ã€æ—¶é—´é—´éš”(Temporal Separation)å’Œåˆ†ç±»ä½“ç³»å¯¹è¯„ä¼°å‡†ç¡®æ€§çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGSE ResNeXtåœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¼˜äºXceptionã€ResNetå’ŒMobileNetV2ç­‰åŸºçº¿æ¨¡å‹ã€‚åœ¨åˆå§‹å±‚å¼•å…¥Gaborå·ç§¯å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†å¤šè¾¾62%ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ”¶æ•›æ•ˆç‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®æ—¶é—´é—´éš”å¯¹æ€§èƒ½çš„å½±å“æ¯”æ•°æ®è§„æ¨¡æ›´å…³é”®ï¼Œè¡¨æ˜èåˆä¿¡å·å¤„ç†æŠ€æœ¯èƒ½æœ‰æ•ˆæé«˜æ°´ä¸‹å£°å­¦åˆ†ç±»æ¨¡å‹åœ¨å˜åŠ¨ç¯å¢ƒä¸‹çš„å¯é æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14714v2",
      "published_date": "2025-12-09 02:39:47 UTC",
      "updated_date": "2026-01-06 22:56:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:14:58.952105+00:00"
    },
    {
      "arxiv_id": "2512.08188v1",
      "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model",
      "title_zh": "å…·èº«æ€ç»´æ ‘ï¼šåŸºäºå…·èº«ä¸–ç•Œæ¨¡å‹çš„å®¡æ…æ“ä½œè§„åˆ’",
      "authors": [
        "Wenjiang Xu",
        "Cindy Wang",
        "Rui Fang",
        "Mingkang Zhang",
        "Lusong Li",
        "Jing Xu",
        "Jiayuan Gu",
        "Zecui Zeng",
        "Rui Chen"
      ],
      "abstract": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Embodied Tree of Thoughts (EToT)ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„ Real2Sim2Real è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œè§„åˆ’ä¸­å› ç¼ºä¹ç‰©ç†åŸºç¡€è€Œå¯¼è‡´çš„å¹»è§‰å’Œé•¿ç¨‹çº¦æŸå¤±æ•ˆé—®é¢˜ã€‚EToT å°†åŸºäºç‰©ç†çš„äº¤äº’å¼æ•°å­—å­ªç”Ÿä½œä¸ºå…·èº«ä¸–ç•Œæ¨¡å‹ (embodied world model)ï¼Œå¹¶å°†æ“ä½œè§„åˆ’å»ºæ¨¡ä¸ºä¸€ç§é€šè¿‡ä¸¤ç§ååŒæœºåˆ¶æ‰©å±•çš„æ ‘æœç´¢è¿‡ç¨‹ã€‚å…¶ä¸­ Priori Branching æœºåˆ¶åŸºäºè¯­ä¹‰å’Œç©ºé—´åˆ†æç”Ÿæˆå€™é€‰æ‰§è¡Œè·¯å¾„ï¼Œè€Œ Reflective Branching åˆ™åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) è¯Šæ–­æ¨¡æ‹Ÿå™¨ä¸­çš„æ‰§è¡Œå¤±è´¥å¹¶è¿­ä»£ä¿®æ­£è§„åˆ’æ ‘ã€‚é€šè¿‡å°†é«˜å±‚æ¨ç†æ¤æ ¹äºç‰©ç†æ¨¡æ‹Ÿå™¨ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†ç”Ÿæˆçš„è§„åˆ’èƒ½å¤Ÿä¸¥æ ¼éµå¾ªåˆšä½“åŠ¨åŠ›å­¦ (rigid-body dynamics) å’Œç¢°æ’çº¦æŸ (collision constraints)ã€‚åœ¨å¤šé¡¹çŸ­ç¨‹å’Œé•¿ç¨‹æ“ä½œä»»åŠ¡çš„å®éªŒä¸­ï¼ŒEToT é€šè¿‡ç²¾å‡†é¢„æµ‹ç‰©ç†åŠ¨æ€å’Œçµæ´»åº”å¯¹æ‰§è¡Œæ•…éšœï¼Œå…¶è¡¨ç°æŒç»­ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Website at https://embodied-tree-of-thoughts.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.08188v1",
      "published_date": "2025-12-09 02:36:26 UTC",
      "updated_date": "2025-12-09 02:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:01.683468+00:00"
    },
    {
      "arxiv_id": "2512.08185v1",
      "title": "A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties",
      "title_zh": "åŒ»ç–—äººå·¥æ™ºèƒ½å®‰å…¨è¯„ä¼°çš„å®ç”¨æ¡†æ¶ï¼šè·¨ä¸´åºŠä¸“ç§‘è¶Šç‹±ä¸éšç§æ¼æ´çš„å¯å¤ç°è¯„ä¼°",
      "authors": [
        "Jinghao Wang",
        "Ping Zhang",
        "Carter Yagemann"
      ],
      "abstract": "Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°åŒ»ç–—äººå·¥æ™ºèƒ½å®‰å…¨æ€§çš„å®ç”¨ä¸”å¯å®Œå…¨å¤ç°çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å®‰å…¨åŸºå‡†æµ‹è¯•å¯¹ GPU é›†ç¾¤ã€å•†ä¸š API æˆ–å—ä¿æŠ¤å¥åº·æ•°æ®çš„è¿‡åº¦ä¾èµ–é—®é¢˜ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†ä»æ€¥è¯ŠåŒ»å­¦ã€ç²¾ç¥ç—…å­¦åˆ°å…¨ç§‘åŒ»å­¦ç­‰æŒ‰ä¸´åºŠé£é™©åˆ†å±‚çš„å¤šä¸ªåŒ»ç–—ä¸“ä¸šé¢†åŸŸï¼Œé‡ç‚¹è¯„ä¼°é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„è¶Šç‹±æ”»å‡» (Jailbreaking Attacks)ï¼ˆå¦‚è§’è‰²æ‰®æ¼”ã€æƒå¨å†’å……ã€å¤šè½®æ“çºµï¼‰å’Œéšç§æå–æ”»å‡» (Privacy Extraction Attacks)ã€‚é€šè¿‡åˆ©ç”¨æ— éœ€ IRB æ‰¹å‡†çš„åˆæˆæ‚£è€…è®°å½•ï¼Œè¯¥æ¡†æ¶æ”¯æŒåœ¨æ¶ˆè´¹çº§ CPU ç¡¬ä»¶ä¸Šè¿è¡Œå…è´¹æ¨¡å‹ï¼Œæ¶ˆé™¤äº†é«˜æ˜‚çš„æˆæœ¬éšœç¢ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†å¨èƒæ¨¡å‹ã€æ•°æ®ç”Ÿæˆæ–¹æ³•ã€è¯„ä¼°åè®®åŠè¯„åˆ†æ ‡å‡†ã€‚è¯¥æ¡†æ¶ä¸ºåŒ»ç–—ä¸“ä¸šæ¨¡å‹å’Œé˜²å¾¡æœºåˆ¶çš„å®‰å…¨æ€§å¯¹æ¯”è¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œæœ‰åŠ©äºç¡®ä¿åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨ä¸å¯ä¿¡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 1 figure, framework proposal",
      "pdf_url": "https://arxiv.org/pdf/2512.08185v1",
      "published_date": "2025-12-09 02:28:15 UTC",
      "updated_date": "2025-12-09 02:28:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:08.544109+00:00"
    },
    {
      "arxiv_id": "2512.08169v1",
      "title": "Information-Dense Reasoning for Efficient and Auditable Security Alert Triage",
      "title_zh": "é¢å‘é«˜æ•ˆä¸”å¯å®¡è®¡å®‰å…¨å‘Šè­¦ç ”åˆ¤çš„é«˜ä¿¡æ¯å¯†åº¦æ¨ç†",
      "authors": [
        "Guangze Zhao",
        "Yongzheng Zhang",
        "Changbo Tian",
        "Dan Xie",
        "Hongri Liu",
        "Bailing Wang"
      ],
      "abstract": "Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AIDRï¼Œä¸€ç§äº‘è¾¹ååŒ(hybrid cloud-edge)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®‰å…¨è¿è¥ä¸­å¿ƒ(SOC)åœ¨å‘Šè­¦åˆ†æ‹£ä¸­é¢ä¸´çš„â€œå‘Šè­¦åˆ†æ‹£å»¶è¿Ÿæ‚–è®º(Alert Triage Latency Paradox)â€ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡åŸºäºæ¢¯åº¦çš„å‹ç¼©æŠ€æœ¯ä¼˜åŒ–æ¨ç†é“¾çš„ä¿¡æ¯å¯†åº¦ï¼Œä»…ä¿ç•™è¶³ä»¥æ”¯æŒå†³ç­–é¢„æµ‹çš„å…³é”®è¯æ®ï¼Œä»¥å¹³è¡¡å‡†ç¡®æ€§ã€å¯å®¡è®¡æ€§ä¸å“åº”å»¶è¿Ÿã€‚ç ”ç©¶äººå‘˜å°†å‘Šè­¦ä¿¡æ¯ç²¾ç®€ä¸º3-5æ¡é«˜ä¿¡æ¯å«é‡çš„è¦ç‚¹ï¼Œä½¿Tokenä½¿ç”¨é‡å‡å°‘äº†68%ï¼Œå¹¶åˆ©ç”¨LoRAæŠ€æœ¯è®­ç»ƒäº†é¢†åŸŸä¸“ç”¨ä¸“å®¶æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIDRåœ¨ç¡®ä¿æ•°æ®æœ¬åœ°åŒ–åˆè§„çš„åŒæ—¶ï¼Œæ¯”æ€ç»´é“¾(Chain-of-Thought)æ–¹æ³•å‡å°‘äº†40.6%çš„å»¶è¿Ÿï¼Œå¹¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºå®ç°é«˜æ•ˆä¸”å¯å®¡è®¡çš„è‡ªåŠ¨åŒ–å®‰å…¨ç¼–æ’ã€è‡ªåŠ¨åŒ–ä¸å“åº”(SOAR)æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08169v1",
      "published_date": "2025-12-09 01:57:24 UTC",
      "updated_date": "2025-12-09 01:57:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:10.228530+00:00"
    },
    {
      "arxiv_id": "2512.08160v1",
      "title": "LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks",
      "title_zh": "LayerPipe2ï¼šåŸºäºæ”¹è¿›æŒ‡æ•°ç§»åŠ¨å¹³å‡çš„ç¥ç»ç½‘ç»œè®­ç»ƒå¤šé˜¶æ®µæµæ°´çº¿ä¸æƒé‡é‡è®¡ç®—",
      "authors": [
        "Nanda K. Unnikrishnan",
        "Keshab K. Parhi"
      ],
      "abstract": "In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LayerPipe2ï¼Œæ—¨åœ¨ä¸ºé‡å å‰å‘å’Œåå‘è®¡ç®—ä»¥åŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒçš„æ–¹æ³•æä¾›ç†è®ºæ”¯æ’‘ã€‚é€šè¿‡å˜é‡å»¶è¿Ÿæ¢¯åº¦è‡ªé€‚åº” (variable delayed gradient adaptation) å’Œé‡å®šæ—¶ (retiming)ï¼Œè¯¥æ¡†æ¶æ­£å¼æ¨å¯¼äº†å±‚é—´å»¶è¿Ÿçš„åˆ†é…è§„å¾‹ï¼ŒæŒ‡å‡ºå†…å±‚éœ€è¦è¾ƒå°‘å»¶è¿Ÿè€Œå¤–å±‚éœ€è¦è¾ƒé•¿å»¶è¿Ÿã€‚é’ˆå¯¹æµæ°´çº¿å¹¶è¡Œ (pipelining) éšå¼è¦æ±‚çš„å†å²æƒé‡å­˜å‚¨ç“¶é¢ˆï¼Œè®ºæ–‡å¼€å‘äº†ä¸€ç§æµæ°´çº¿æ„ŸçŸ¥ç§»åŠ¨å¹³å‡ (pipeline-aware moving average) æŠ€æœ¯ï¼Œé€šè¿‡é‡æ„è¿‡å»çŠ¶æ€è€Œéæ˜¾å¼å­˜å‚¨æ¥å¤§å¹…é™ä½å†…å­˜æˆæœ¬ã€‚è¿™ä¸€åŸåˆ™æ€§æ¡†æ¶ä¸ä»…é˜æ˜äº† LayerPipe æ¶æ„çš„æ„å»ºé€»è¾‘ï¼Œè¿˜é€šè¿‡ä¼˜åŒ–é€šä¿¡ä¸è®¡ç®—çš„æƒè¡¡ï¼Œå®ç°äº†é«˜æ‰©å±•æ€§ä¸”ä¿è¯å‡†ç¡®æ€§çš„æµæ°´çº¿è®­ç»ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "Proc. of 2025 Asilomar Conference on Signals, Systems, and Computers, October 2025, Pacific Grove, CA",
      "pdf_url": "https://arxiv.org/pdf/2512.08160v1",
      "published_date": "2025-12-09 01:35:08 UTC",
      "updated_date": "2025-12-09 01:35:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:15.125909+00:00"
    },
    {
      "arxiv_id": "2512.08153v1",
      "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
      "title_zh": "TreeGRPOï¼šé¢å‘æ‰©æ•£æ¨¡å‹åœ¨çº¿å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„æ ‘çŠ¶ä¼˜åŠ¿ GRPO",
      "authors": [
        "Zheng Ding",
        "Weirui Ye"
      ],
      "abstract": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TreeGRPOï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹ (Diffusion Models) åœ¨çº¿ RL åè®­ç»ƒ (Post-Training) çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯¹é½æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚çš„ç“¶é¢ˆã€‚TreeGRPO å°†å»å™ªè¿‡ç¨‹é‡æ–°æ„å»ºä¸ºæœç´¢æ ‘ï¼Œé€šè¿‡åœ¨å…±äº«åˆå§‹å™ªå£°çš„åŸºç¡€ä¸Šç­–ç•¥æ€§åœ°åˆ†æ”¯ç”Ÿæˆå¤šæ¡å€™é€‰è½¨è¿¹ï¼Œä»è€Œå®ç°å…¬å…±å‰ç¼€çš„é«˜æ•ˆå¤ç”¨ã€‚è¯¥æ–¹æ³•å…·å¤‡ä¸‰å¤§æ ¸å¿ƒä¼˜åŠ¿ï¼šæ˜¾è‘—æå‡çš„é‡‡æ ·æ•ˆç‡ (Sample Efficiency)ï¼›é€šè¿‡å¥–åŠ±åå‘ä¼ æ’­å®ç°ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é… (Fine-grained credit assignment)ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿè½¨è¿¹çº§æ–¹æ³•çš„å±€é™ï¼›ä»¥åŠé€šè¿‡å¤šåˆ†æ”¯ç»“æ„å®ç°æ‘Šé”€è®¡ç®— (Amortized Computation)ï¼Œå¤§å¹…æé«˜äº†å•æ¬¡å‰å‘ä¼ æ’­çš„ç­–ç•¥æ›´æ–°æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒTreeGRPO åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äº GRPO ç­‰åŸºå‡†æ¨¡å‹ï¼Œå®ç°äº† 2.4 å€çš„è®­ç»ƒåŠ é€Ÿï¼Œå¹¶åœ¨æ•ˆç‡ä¸å¥–åŠ±çš„æƒè¡¡ä¸­ç¡®ç«‹äº†æ›´ä¼˜çš„å¸•ç´¯æ‰˜å‰æ²¿ (Pareto frontier)ã€‚è¯¥ç ”ç©¶ä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„é«˜æ•ˆä¸€è‡´æ€§å¯¹é½æä¾›äº†ä¸€ç§æå…·æ‰©å±•æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08153v1",
      "published_date": "2025-12-09 01:17:34 UTC",
      "updated_date": "2025-12-09 01:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:14.511862+00:00"
    },
    {
      "arxiv_id": "2512.08147v2",
      "title": "Scalable Back-End for an AI-Based Diabetes Prediction Application",
      "title_zh": "åŸºäºäººå·¥æ™ºèƒ½çš„ç³–å°¿ç—…é¢„æµ‹åº”ç”¨çš„å¯æ‰©å±•åç«¯",
      "authors": [
        "Henry Anand Septian Radityo",
        "Bernardus Willson",
        "Raynard Tanadi",
        "Latifa Dwiyanti",
        "Saiful Akbar"
      ],
      "abstract": "The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸€ä¸ªä¸“ä¸ºç§»åŠ¨ç«¯ç³–å°¿ç—…é¢„æµ‹åº”ç”¨è®¾è®¡çš„å¯æ‰©å±•åç«¯ç³»ç»Ÿï¼Œæ—¨åœ¨åº”å¯¹æ—¥ç›Šå¢é•¿çš„å…¨çƒç³–å°¿ç—…æ‚£ç—…ç‡å¸¦æ¥çš„æ—©æœŸæ£€æµ‹éœ€æ±‚ã€‚ç³»ç»Ÿè®¾è®¡çš„ä¸»è¦ç›®æ ‡æ˜¯å°†å¤±è´¥ç‡æ§åˆ¶åœ¨5%ä»¥ä¸‹ï¼Œå¹¶å°†å¹³å‡å»¶è¿Ÿç»´æŒåœ¨1000æ¯«ç§’ä»¥å†…ï¼Œä»¥ç¡®ä¿å¤§è§„æ¨¡ç”¨æˆ·ç¾¤ä½“çš„æœåŠ¡å“åº”èƒ½åŠ›ã€‚æ¶æ„é‡‡ç”¨äº†æ°´å¹³æ‰©å±•(Horizontal Scaling)ã€æ•°æ®åº“åˆ†ç‰‡(Database Sharding)ä»¥åŠé€šè¿‡RabbitMQå®ç°çš„å¼‚æ­¥é€šä¿¡(Asynchronous Communication)ç­‰å…³é”®æŠ€æœ¯æ‰‹æ®µã€‚æ€§èƒ½è¯„ä¼°æ˜¾ç¤ºï¼Œç³»ç»Ÿä¸­83%çš„åŠŸèƒ½æˆåŠŸè¾¾åˆ°äº†é¢„è®¾æŒ‡æ ‡ï¼Œä¸”ç³»ç»Ÿèƒ½å¤Ÿç¨³å®šå¤„ç†é«˜è¾¾10,000ä¸ªå¹¶å‘ç”¨æˆ·ï¼Œå……åˆ†éªŒè¯äº†å…¶å¯æ‰©å±•æ€§ã€‚ç ”ç©¶ç‰¹åˆ«æŒ‡å‡ºï¼ŒRabbitMQåœ¨å¤„ç†è®¡ç®—å¯†é›†å‹é¢„æµ‹è¯·æ±‚æ—¶å¯¹é™ä½é”™è¯¯ç‡è‡³å…³é‡è¦ï¼Œèƒ½é€šè¿‡è¯·æ±‚æ’é˜Ÿæœºåˆ¶æœ‰æ•ˆé˜²æ­¢é«˜è´Ÿè½½ä¸‹çš„æ•°æ®ä¸¢å¤±ã€‚è¿™ä¸€åç«¯æ¶æ„ä¸ºAIé©±åŠ¨çš„å¥åº·é¢„æµ‹åº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ï¼Œå°¤å…¶åœ¨å¤„ç†è¯»å¯†é›†å‹(Read-Intensive)é¢„æµ‹æ“ä½œå’Œç”¨æˆ·ç®¡ç†æ–¹é¢è¡¨ç°å“è¶Šã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper was accepted and presented at the 2025 IEEE International Conference on Data and Software Engineering (ICoDSE) on 28 October 2025 in Batam, Indonesia, and is currently awaiting publication. This version corrects the author name. No changes to the content",
      "pdf_url": "https://arxiv.org/pdf/2512.08147v2",
      "published_date": "2025-12-09 00:59:20 UTC",
      "updated_date": "2026-01-23 03:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:25.013502+00:00"
    },
    {
      "arxiv_id": "2512.08145v1",
      "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models",
      "title_zh": "Chat with UAVï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„äºº-æ— äººæœºäº¤äº’",
      "authors": [
        "Haoran Wang",
        "Zhuohang Chen",
        "Guang Li",
        "Bo Ma",
        "Chuanghuang Li"
      ],
      "abstract": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)äº¤äº’ç³»ç»Ÿä¸­ä¼ ç»Ÿé¢„å®šä¹‰è®¾è®¡çš„å±€é™æ€§ï¼Œä»¥åŠç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¡†æ¶åœ¨å¤æ‚åœºæ™¯ä¸‹æ··åˆä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œçš„é€‚åº”æ€§éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŒæ™ºèƒ½ä½“äººæœºæ— äººæœºäº¤äº’(Human-UAV Interaction, HUI)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç›¸äº’ç‹¬ç«‹çš„ä»»åŠ¡è§„åˆ’æ™ºèƒ½ä½“(Task Planning Agent)å’Œæ‰§è¡Œæ™ºèƒ½ä½“(Execution Agent)ï¼Œå¹¶åº”ç”¨å·®å¼‚åŒ–çš„æç¤ºå·¥ç¨‹(Prompt Engineering)æ¥åˆ†åˆ«å¤„ç†ä»»åŠ¡çš„ç†è§£ã€è§„åˆ’ä¸å…·ä½“æ‰§è¡Œã€‚ä¸ºéªŒè¯å…¶æ€§èƒ½ï¼Œç ”ç©¶è€…æ„å»ºäº†æ¶µç›–å†œä¸šã€èˆªæ‹ã€ç‰©æµå’Œç¯å¢ƒç›‘æµ‹ç­‰å…¸å‹åœºæ™¯çš„ä»»åŠ¡æ•°æ®åº“ï¼Œå¹¶åˆ©ç”¨ä¸‰ä¸ªç‹¬ç«‹æŒ‡æ ‡å¯¹ä¸åŒæ¨¡å‹ä¸‹çš„æ§åˆ¶æ•ˆæœè¿›è¡Œäº†é‡åŒ–è¯„ä¼°ã€‚ç”¨æˆ·ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†äººæœºäº¤äº’çš„æµç•…åº¦ä¸ä»»åŠ¡æ‰§è¡Œçš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•æˆåŠŸå®ç°äº†ä»å·¥ç¨‹å¸ˆé©±åŠ¨å‘ç”¨æˆ·é©±åŠ¨çš„äº¤äº’æ¨¡å¼è½¬å˜ï¼Œä¸ºå®ç°æ›´å…·é€‚åº”æ€§çš„æ™ºèƒ½æ— äººæœºäº¤äº’ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08145v1",
      "published_date": "2025-12-09 00:55:40 UTC",
      "updated_date": "2025-12-09 00:55:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:16:18.071035+00:00"
    },
    {
      "arxiv_id": "2512.08998v1",
      "title": "DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant",
      "title_zh": "DermETAS-SNA LLMï¼šç»“åˆ StackNet å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹åŠ©æ‰‹çš„çš®è‚¤ç§‘è¿›åŒ– Transformer æ¶æ„æœç´¢",
      "authors": [
        "Nitya Phani Santosh Oruganty",
        "Keerthi Vemula Murali",
        "Chun-Kit Ngan",
        "Paulo Bandeira Pinho"
      ],
      "abstract": "Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DermETAS-SNA LLMï¼Œè¿™æ˜¯ä¸€ç§å°†çš®è‚¤ç§‘å¯¼å‘çš„æ¼”åŒ–Transformeræ¶æ„æœç´¢(Evolutionary Transformer Architecture Search, ETAS)ä¸StackNetå¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹(LLM)åŠ©æ‰‹ç›¸ç»“åˆçš„è¯Šæ–­æ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆåˆ©ç”¨ETASæ¡†æ¶åœ¨SKINCONæ•°æ®é›†ä¸Šä¼˜åŒ–äº†ä¸“ç”¨äºçš®è‚¤ç‰¹å¾è¡¨ç¤ºçš„Vision Transformer (ViT)ï¼Œå¹¶é’ˆå¯¹23ç§çš®è‚¤ç—…ç±»åˆ«å¾®è°ƒäº†å¤šä¸ªäºŒåˆ†ç±»å™¨ã€‚ä¸ºäº†æå‡é¢„æµ‹ç¨³å¥æ€§å¹¶ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç³»ç»Ÿé‡‡ç”¨äº†StackNetæ¶æ„æ•´åˆè¿™äº›ViTåˆ†ç±»å™¨ï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯åŠGoogle Gemini 2.5 Proç”Ÿæˆä¸ªæ€§åŒ–ä¸”å…·å¤‡åŒ»å­¦èƒŒæ™¯çš„è¯Šæ–­è§£é‡Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€»F1åˆ†æ•°è¾¾åˆ°56.30%ï¼Œæ˜¾è‘—ä¼˜äºSkinGPT-4ä¸”æ€§èƒ½æå‡è¾¾16.06%ã€‚åœ¨ç”±å…«åæ‰§ä¸šåŒ»ç”Ÿè¿›è¡Œçš„é¢†åŸŸä¸“å®¶è¯„ä¼°ä¸­ï¼ŒåŒ»ç”Ÿå¯¹è¯¥AIåŠ©æ‰‹ç”Ÿæˆçš„ä¸´åºŠå“åº”è¾¾æˆäº†ä¸€è‡´ç‡é«˜è¾¾92%çš„è®¤å¯ã€‚è¯¥ç ”ç©¶è¿˜é€šè¿‡æ¦‚å¿µéªŒè¯åŸå‹å±•ç¤ºäº†è¯¥ç³»ç»Ÿåœ¨çœŸå®ä¸´åºŠåŠæ•™è‚²åº”ç”¨ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºçš®è‚¤ç—…è¾…åŠ©è¯Šæ–­æä¾›äº†é«˜åº¦å¯ä¿¡çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08998v1",
      "published_date": "2025-12-09 00:37:12 UTC",
      "updated_date": "2025-12-09 00:37:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:55.335110+00:00"
    },
    {
      "arxiv_id": "2512.11883v1",
      "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
      "title_zh": "å®¡ç¾å¯¹é½å¼•å‘çš„åŒè´¨åŒ–é£é™©ï¼šå›¾åƒç”Ÿæˆä¸å¥–åŠ±æ¨¡å‹å¦‚ä½•å¼ºåŒ–å®¡ç¾åè§ä¸æ„è¯†å½¢æ€â€œå®¡æŸ¥â€",
      "authors": [
        "Wenqi Marshall Guo",
        "Qingyun Qian",
        "Khalad Hasan",
        "Shan Du"
      ],
      "abstract": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº† Image Generation æ¨¡å‹è¿‡åº¦å¯¹é½(Over-aligning)å¤§ä¼—å®¡ç¾åå¥½æ‰€å¸¦æ¥çš„é£é™©ï¼ŒæŒ‡å‡ºè¿™ç§å€¾å‘ä¼šè¿èƒŒç”¨æˆ·æ„å›¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”¨æˆ·è¦æ±‚ç”Ÿæˆâ€œåå®¡ç¾â€(Anti-aesthetic)æˆ–æ‰¹åˆ¤æ€§å›¾åƒæ—¶ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ„å»ºä¸€ä¸ªå¹¿è°±å®¡ç¾æ•°æ®é›†ï¼Œå¯¹å½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å’Œ Reward Models è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œç»è¿‡å®¡ç¾å¯¹é½çš„æ¨¡å‹å¾€å¾€é»˜è®¤è¾“å‡ºç¬¦åˆä¼ ç»Ÿç¾æ„Ÿçš„å›¾åƒï¼Œä»è€Œå¿½ç•¥ç”Ÿæˆä½è´¨é‡æˆ–è´Ÿé¢æ„è±¡çš„ç‰¹å®šæŒ‡ä»¤ã€‚æ›´å…³é”®çš„æ˜¯ï¼Œå³ä¾¿å›¾åƒå†…å®¹å®Œå…¨ç¬¦åˆç”¨æˆ·çš„æ˜ç¡®æç¤ºè¯ï¼ŒReward Models ä»ä¼šç³»ç»Ÿæ€§åœ°æƒ©ç½šå…¶ä¸­çš„éç¾å­¦å…ƒç´ ã€‚é€šè¿‡å›¾åƒåˆ°å›¾åƒ(Image-to-image)ç¼–è¾‘å®éªŒä»¥åŠä¸çœŸå®æŠ½è±¡è‰ºæœ¯ä½œå“çš„å¯¹æ¯”ï¼Œè¯¥ç ”ç©¶è¯å®äº†è¿™ç§åè§å¦‚ä½•æŸå®³ç”¨æˆ·è‡ªä¸»æƒä¸å®¡ç¾å¤šæ ·æ€§ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹ä¸­æ½œåœ¨çš„æ„è¯†å½¢æ€â€œå®¡æŸ¥â€ç°è±¡ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11883v1",
      "published_date": "2025-12-09 00:24:29 UTC",
      "updated_date": "2025-12-09 00:24:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:15:32.799900+00:00"
    },
    {
      "arxiv_id": "2512.08130v1",
      "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture",
      "title_zh": "è¯„ä¼°å‰æ²¿ AI æ¨¡å‹çš„ç”Ÿç‰©å¨èƒåŸºå‡†ç”Ÿæˆæ¡†æ¶ Iï¼šä»»åŠ¡-æŸ¥è¯¢æ¶æ„",
      "authors": [
        "Gary Ackerman",
        "Brandon Behlendorf",
        "Zachary Kallenborn",
        "Sheriff Almakki",
        "Doug Clifford",
        "Jenna LaTourette",
        "Hayley Peterson",
        "Noah Sheinbaum",
        "Olivia Shoemaker",
        "Anna Wetzel"
      ],
      "abstract": "Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Biothreat Benchmark Generation (BBG) æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–å¹¶ç¼“è§£å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¾…åŠ©ç”Ÿç‰©ææ€–ä¸»ä¹‰æˆ–è·å–ç”Ÿç‰©æ­¦å™¨æ–¹é¢çš„æ½œåœ¨å®‰å…¨é£é™©ã€‚æœ¬æ–‡ä½œä¸ºè¯¥æ¡†æ¶çš„é¦–ç¯‡æˆæœï¼Œè¯¦ç»†é˜è¿°äº†åä¸ºâ€œç»†èŒç”Ÿç‰©å¨èƒæ¨¡å¼â€ (Bacterial Biothreat Schema) çš„ä»»åŠ¡-æŸ¥è¯¢æ¶æ„ (Task-Query Architecture)ï¼Œä¸»è¦é’ˆå¯¹ç»†èŒæ€§ç”Ÿç‰©å¨èƒè¿›è¡Œè¯„ä¼°ã€‚BBGæ¡†æ¶åŸºäºå±‚æ¬¡åŒ–ç»“æ„ï¼Œä¸ä»…åŒ…å«æŠ€æœ¯ç»´åº¦çš„é£é™©è¯„ä¼°ï¼Œè¿˜ç‰¹åˆ«çº³å…¥äº†æ“ä½œæ€§ (Operational) é£é™©å› ç´ ä»¥åŠä¸åŒæ”»å‡»è€…çš„èƒ½åŠ›æ°´å¹³ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å¯é‡å¤çš„è¯„ä¼°ä½“ç³»ï¼Œèƒ½å¤Ÿå…¨é¢æ•æ‰ç”Ÿç‰©å¯¹æŠ—è€…çš„æŠ€æœ¯ä¸æ“ä½œéœ€æ±‚ã€‚é€šè¿‡è¿™ä¸€æ¶æ„ï¼Œç ”ç©¶è€…ä¸ºåç»­å°†æŸ¥è¯¢è½¬åŒ–ä¸ºæ¨¡å‹æç¤ºè¯ (Prompts) ä»¥åŠé‡åŒ– AI æ¨¡å‹å¸¦æ¥çš„ç”Ÿç‰©å®‰å…¨é£é™©å¢é‡å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.08130v1",
      "published_date": "2025-12-09 00:16:44 UTC",
      "updated_date": "2025-12-09 00:16:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:16:32.495242+00:00"
    },
    {
      "arxiv_id": "2512.08124v1",
      "title": "Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach",
      "title_zh": "åŸºäºèµ„äº§æ’åºçš„ä»…åšå¤šåŠ å¯†è´§å¸æŠ•èµ„ç»„åˆç®¡ç†ï¼šä¸€ç§ç¥ç»ç½‘ç»œæ–¹æ³•",
      "authors": [
        "Zijiang Yang"
      ],
      "abstract": "This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæœºå™¨å­¦ä¹ (machine learning)çš„åŠ å¯†è´§å¸æŠ•èµ„ç»„åˆç®¡ç†æ–¹æ³•ï¼Œä¸ä»¥å¾€å…³æ³¨å•ä¸€èµ„äº§èµ°åŠ¿é¢„æµ‹çš„ç ”ç©¶ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ†æèµ„äº§é—´çš„ç›¸å¯¹å…³ç³»æ¥ç®¡ç†åŠ å¯†è´§å¸èµ„äº§ç»„ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»ç½‘ç»œ(neural network)é¢„æµ‹ç®¡ç†èµ„äº§æœªæ¥æ”¶ç›Šç‡çš„æ’å(rank)ï¼Œå¹¶æ ¹æ®è¯¥æ¨ªæˆªé¢ä¿¡æ¯(cross-sectional information)åˆ†é…ç›¸åº”çš„æƒé‡ã€‚ç ”ç©¶åˆ©ç”¨2020å¹´5æœˆè‡³2023å¹´11æœˆçš„çœŸå®å¸‚åœºæ•°æ®è¿›è¡Œäº†å›æµ‹ï¼Œè¯¥å‘¨æœŸæ¶µç›–äº†å®Œæ•´çš„ç‰›å¸‚ã€ç†Šå¸‚å’Œéœ‡è¡è¡Œæƒ…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚å¸‚åœºç¯å¢ƒä¸‹è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå®ç°äº†1.01çš„å¤æ™®æ¯”ç‡(Sharpe ratio)å’Œ64.26%çš„å¹´åŒ–æ”¶ç›Šç‡(annualized return)ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹äº¤æ˜“è´¹ç”¨(transaction fee)çš„å¢åŠ è¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§(robust)ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…äº¤æ˜“åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08124v1",
      "published_date": "2025-12-09 00:08:39 UTC",
      "updated_date": "2025-12-09 00:08:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:16:52.343795+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 152,
  "processed_papers_count": 152,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T15:20:04.424592+00:00"
}
{
  "date": "2025-11-29",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-29 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡è´¨é‡é¢‡é«˜ï¼Œå‘ˆç°å‡ºå‡ ä¸ªæ˜æ˜¾çš„è¶‹åŠ¿ï¼š**æ¨ç†æ¨¡å‹çš„åè®­ç»ƒï¼ˆPost-trainingï¼‰æŠ€æœ¯**æ­£åœ¨ä»çº¯æ•°å­¦/ä»£ç é¢†åŸŸå‘åŒ»ç–—å’Œä¸»è§‚æƒ…æ„Ÿé¢†åŸŸæ‰©æ•£ï¼ˆç‰¹åˆ«æ˜¯ GRPO çš„å˜ä½“åº”ç”¨ï¼‰ï¼›**æ¨ç†æ•ˆç‡**æ–¹é¢ï¼Œåˆ©ç”¨è®­ç»ƒåŠ¨åŠ›å­¦æ¥æŒ‡å¯¼æ¨ç†æ—¶çš„â€œæå‰é€€å‡ºâ€æ˜¯ä¸€ä¸ªéå¸¸æ€§æ„Ÿçš„æ–°æ–¹å‘ï¼›**è§†è§‰ Mamba** å’Œ **å¤šæ¨¡æ€å®‰å…¨**ï¼ˆç‰¹åˆ«æ˜¯æ¬ºéª—æ€§é˜²å¾¡ï¼‰ä¹Ÿæ˜¯ä»Šå¤©çš„çƒ­ç‚¹ã€‚\n\nä»¥ä¸‹æ˜¯ä¸ºæ‚¨ç²¾é€‰çš„æ·±åº¦è§£è¯»ï¼š\n\n---\n\n### ğŸš€ é‡ç£…æ¨èï¼šæ¨ç†æ•ˆç‡ä¸ RL åè®­ç»ƒçš„æ–°èŒƒå¼\n\n**1. [æ¨ç†ä¼˜åŒ–] åŸºäºè®­ç»ƒæ¢¯åº¦åŠ¨åŠ›å­¦çš„ dLLM æ—©æœŸæ¨ç†ç»ˆæ­¢**\n**# title:** EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients\n**# authors:** He-Yen Hsieh et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** åˆ«å†æµªè´¹è®¡ç®—èµ„æºäº†ï¼åˆ©ç”¨è®­ç»ƒæ—¶çš„â€œæ¢¯åº¦è®°å¿†â€æ¥å‘Šè¯‰æ¨¡å‹ä½•æ—¶åœ¨æ¨ç†æ—¶åœä¸‹æ¥ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** ä½œè€…æå‡ºäº† EDITï¼Œä¸€ç§é’ˆå¯¹åŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰çš„æ¨ç†ç»ˆæ­¢å‡†åˆ™ã€‚å®ƒçš„æ ¸å¿ƒæ´å¯Ÿéå¸¸æ¼‚äº®ï¼š**åˆ©ç”¨è®­ç»ƒæœŸé—´ï¼ˆSFTï¼‰AdamW èšåˆçš„ LoRA æ›´æ–°æ‰€äº§ç”Ÿçš„å…ƒæ•°æ®**ã€‚è¿™äº›é€šå¸¸è¢«ä¸¢å¼ƒçš„ä¿¡æ¯å…¶å®åŒ…å«äº†â€œæ¨ç†è·¯å¾„â€çš„ç´§å‡‘è¡¨ç¤ºã€‚EDIT åœ¨æ¨ç†æ—¶ç›‘æµ‹ token æ¿€æ´»ä¸è¿™ä¸ªâ€œè®­ç»ƒæ¨ç†å›¾â€çš„å¯¹é½ç¨‹åº¦ï¼Œå½“ KL æ•£åº¦ä½äºé˜ˆå€¼æ—¶æå‰ç»ˆæ­¢ã€‚ç»“æœåœ¨å‡å°‘ 11.8%-68.3% çš„æ¨ç†æ­¥éª¤çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æé«˜äº†ç²¾åº¦ã€‚\n\n**2. [åŒ»ç–—æ¨ç†] Clinical-R1ï¼šåˆ©ç”¨ä¸´åºŠç›®æ ‡ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–èµ‹èƒ½ LLM**\n**# title:** Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization\n**# authors:** Boyang Gu et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** DeepSeek-R1 çš„æ€è·¯ï¼ˆGRPOï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„ç¡¬æ ¸è½åœ°ï¼Œè§£å†³äº† GRPO åªå¥–åŠ±æ­£ç¡®æ€§è€Œå¿½ç•¥åŒ»ç–—â€œå¿ å®æ€§â€çš„é—®é¢˜ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** ç°æœ‰çš„ GRPO ä¸»è¦é’ˆå¯¹æ•°å­¦/ä»£ç çš„æ­£ç¡®æ€§ï¼Œä½†åœ¨é«˜é£é™©çš„åŒ»ç–—é¢†åŸŸï¼Œæ¨ç†è¿‡ç¨‹çš„**å¿ å®æ€§ï¼ˆFaithfulnessï¼‰**å’Œ**å…¨é¢æ€§**è‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº† **CRPO (Clinical-Objective RPO)**ï¼Œå¼•å…¥äº†åŸºäºè§„åˆ™å’Œå¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œè”åˆä¼˜åŒ–å‡†ç¡®æ€§ã€å¿ å®æ€§å’Œå…¨é¢æ€§ã€‚è®­ç»ƒå‡ºçš„ Clinical-R1-3B åœ¨åŒ»ç–—æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†æ ‡å‡† GRPO è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™æ˜¯å°†å¼ºæ¨ç†æ¨¡å‹å‚ç›´é¢†åŸŸåŒ–çš„ä¸€ä¸ªæä½³èŒƒä¾‹ã€‚\n\n**3. [æƒ…æ„Ÿè®¡ç®—] Echo-N1ï¼šæƒ…æ„Ÿå¼ºåŒ–å­¦ä¹ çš„æ–°å‰æ²¿**\n**# title:** Echo-N1: Affective RL Frontier\n**# authors:** Naifan Zhang et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** è°è¯´ RL åªèƒ½åšæ•°å­¦é¢˜ï¼Ÿè¿™ç¯‡æ–‡ç« è¯æ˜äº† RL ä¹Ÿå¯ä»¥ä¼˜åŒ–â€œä¸»è§‚ä¸”æƒ…ç»ªåŒ–â€çš„å¯¹è¯ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªåç›´è§‰çš„å·¥ä½œã€‚ä¸šç•Œæ™®éè®¤ä¸º RL é€‚åˆå®¢è§‚å¯éªŒè¯çš„ä»»åŠ¡ï¼ˆMath/Codeï¼‰ï¼Œä½†ä½œè€…æå‡ºäº† Echo-N1 æ¡†æ¶ï¼Œåˆ©ç”¨ RL ä¼˜åŒ–æ¨¡å‹åœ¨**ä¸»è§‚ã€æƒ…æ„Ÿæ¥åœ°ã€ä¸ªæ€§æ•æ„Ÿ**å¯¹è¯ä¸­çš„è¡¨ç°ã€‚é€šè¿‡æ¨æ–­ç”¨æˆ·ä¸ªæ€§å¹¶ä¼˜åŒ–å¯¹è¯åå¥½ï¼Œè¯¥æ¨¡å‹åœ¨æ‹ŸäººåŒ–äº¤äº’è´¨é‡ä¸Šå–å¾—äº†å·¨å¤§æå‡ï¼Œç”šè‡³è¶…è¿‡äº†å•†ä¸šæ¨¡å‹ï¼ˆå¦‚ Doubao 1.5 Characterï¼‰ã€‚è¿™é€šè¿‡ RL è§£å†³â€œäººç±»ä¸»è§‚æ€§â€é—®é¢˜è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰æ¨¡å‹ä¸ç”Ÿæˆï¼šMamba ä¸ çœŸå®æ„Ÿ\n\n**4. [è§†è§‰æ¶æ„] MambaScopeï¼šé«˜æ•ˆè§†è§‰ Mamba çš„ç”±ç²—åˆ°ç»†æœºåˆ¶**\n**# title:** MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba\n**# authors:** Shanhui Liu et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** ç®€å•çš„å›¾ç²—çœ‹ï¼Œå¤æ‚çš„å›¾ç»†çœ‹ï¼ŒåŠ¨æ€åˆ†é…ç®—åŠ›ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** é’ˆå¯¹ Vision Mamba å—é™äº Token æ•°é‡çš„é—®é¢˜ï¼Œä½œè€…å¹¶æœªé‡‡ç”¨ç®€å•çš„ Token å‰ªæï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§**è‡ªé€‚åº”åˆ†è¾¨ç‡**æ¡†æ¶ã€‚MambaScope é¦–å…ˆå°†å›¾åƒåˆ’åˆ†ä¸ºå¤§å—è¿›è¡Œâ€œç²—ç²’åº¦â€æ¨ç†ï¼Œå½“ç½®ä¿¡åº¦ä½æ—¶ï¼Œå†å¯¹ç‰¹å®šåŒºåŸŸè¿›è¡Œâ€œç»†ç²’åº¦â€é‡å¤„ç†ã€‚è¿™ç§åŠ¨æ€ç­–ç•¥åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—é‡ï¼Œä¼˜äºç°æœ‰çš„ Token å‹ç¼©æŠ€æœ¯ã€‚\n\n**5. [å›¾åƒç”Ÿæˆ] RealGenï¼šé€šè¿‡æ£€æµ‹å™¨å¼•å¯¼å¥–åŠ±å®ç°ç…§ç‰‡çº§çœŸå®æ„Ÿç”Ÿæˆ**\n**# title:** RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards\n**# authors:** Junyan Ye et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** åˆæ˜¯ GRPO çš„è·¨ç•Œåº”ç”¨ï¼ç”¨â€œæ‰¾èŒ¬â€æ£€æµ‹å™¨ä½œä¸º Reward Model æ¥ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** ç°æœ‰çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ GPT-Imageï¼‰å¸¸æœ‰â€œAIå‘³â€ï¼ˆçš®è‚¤å¤ªæ»‘ã€æ²¹å…‰ï¼‰ã€‚RealGen å¼•å…¥äº†â€œæ£€æµ‹å™¨å¥–åŠ±â€æœºåˆ¶ï¼Œé‡åŒ–ä¼ªå½±å’ŒçœŸå®æ„Ÿï¼Œå¹¶åˆ©ç”¨ **GRPO ç®—æ³•**ä¼˜åŒ–æ•´ä¸ªç”Ÿæˆç®¡é“ã€‚é…åˆ RealBench è¯„ä¼°åŸºå‡†ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ„Ÿå’Œç»†èŠ‚ä¸Šæ˜¾è‘—ä¼˜äº FLUX-Krea ç­‰ä¸“ç”¨æ¨¡å‹ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ä¸ Agent (æ™ºèƒ½ä½“)\n\n**6. [çº¢é˜Ÿæµ‹è¯•] å¤§å‹æ¨ç†æ¨¡å‹çš„çº¢é˜Ÿæµ‹è¯•**\n**# title:** Red Teaming Large Reasoning Models\n**# authors:** Jiawei Chen et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰æœ‰äº†æ€ç»´é“¾ï¼ˆCoTï¼‰å°±æ›´å®‰å…¨å—ï¼Ÿä¸ï¼Œå¯èƒ½æ›´è„†å¼±ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** æå‡ºäº† RT-LRM åŸºå‡†ï¼Œä¸“é—¨æµ‹è¯•å…·æœ‰ CoT èƒ½åŠ›çš„æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼ŒLRM é¢ä¸´ç‹¬ç‰¹é£é™©ï¼Œå¦‚ **CoT-hijackingï¼ˆæ€ç»´é“¾åŠ«æŒï¼‰**ã€‚ç›¸æ¯”æ™®é€š LLMï¼Œæ¨ç†æ¨¡å‹åœ¨é¢å¯¹æ¨ç†è¯±å¯¼çš„é£é™©æ—¶å¾€å¾€è¡¨ç°å¾—æ›´è„†å¼±ï¼Œè¿™ä¸º o1 ç±»æ¨¡å‹çš„éƒ¨ç½²æ•²å“äº†è­¦é’Ÿã€‚\n\n**7. [Agent å®‰å…¨] æ–­è¨€æ¡ä»¶ä¸‹çš„åˆè§„æ€§ï¼šå¤šè½®å·¥å…·è°ƒç”¨ Agent çš„éšæ€§æ¼æ´**\n**# title:** Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents\n**# authors:** Daud Waqas et al.\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** ä½ çš„ Agent å¯èƒ½å› ä¸ºç”¨æˆ·çš„ä¸€å¥çè¯æˆ–è€…è¿‡æ—¶çš„å·¥å…·æç¤ºå°±â€œå›å˜â€äº†ã€‚\n> **ä¸»è¦è´¡çŒ®ï¼š** æå‡ºäº† A-CC è¯„ä¼°èŒƒå¼ï¼Œæµ‹è¯• Agent åœ¨å¤šè½®å¯¹è¯ä¸­é¢å¯¹è¯¯å¯¼æ€§æ–­è¨€çš„é²æ£’æ€§ã€‚ä¸»è¦ä¸¤ç±»æ”»å‡»ï¼šç”¨æˆ·æºæ–­è¨€ï¼ˆUSAï¼Œç”¨æˆ·çå¿½æ‚ ï¼‰å’Œå‡½æ•°æºæ–­è¨€ï¼ˆFSAï¼Œå·¥å…·è¿”å›è¿‡æ—¶/é”™è¯¯æç¤ºï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œç›®å‰çš„ Tool-calling æ¨¡å‹ææ˜“å—åˆ°è¿™ç±»â€œé˜¿è°€å¥‰æ‰¿â€å¼æ”»å‡»çš„å½±å“ã€‚\n\n**8. [å¤šæ¨¡æ€å®‰å…¨] ç”¨å›¾ç‰‡è¾©è®ºï¼šæ£€æµ‹å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¬ºéª—è¡Œä¸º**\n**# title:** Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models\n**# authors:** Sitong Fang et al.\n> **ä¸»è¦è´¡çŒ®ï¼š** å¤šæ¨¡æ€æ¨¡å‹ä¸ä»…ä¼šå¹»è§‰ï¼Œè¿˜ä¼š**æ¬ºéª—**ï¼ˆDeceptionï¼‰ã€‚ä½œè€…æå‡ºäº† MM-DeceptionBenchï¼Œå¹¶è®¾è®¡äº†ä¸€ç§â€œå¤šæ™ºèƒ½ä½“è¾©è®ºâ€æœºåˆ¶ï¼Œå¼ºè¿«æ¨¡å‹ç”¨è§†è§‰è¯æ®æ¥æ”¯æ’‘å…¶è®ºç‚¹ï¼ˆGroundingï¼‰ï¼Œä»è€Œè®©æ¬ºéª—è¡Œä¸ºæ— å¤„éå½¢ã€‚\n\n---\n\n### ğŸ§¬ AI for Science (ç§‘å­¦æ™ºèƒ½)\n\n**9. [ç”Ÿç‰©åŸºç¡€æ¨¡å‹] BioArcï¼šå‘ç°ç”Ÿç‰©åŸºç¡€æ¨¡å‹çš„æœ€ä½³ç¥ç»æ¶æ„**\n**# title:** BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models\n**# authors:** Yi Fang et al.\n> **ä¸»è¦è´¡çŒ®ï¼š** ç°åœ¨çš„ Bio-AI å¾ˆå¤šç›´æ¥å¥—ç”¨ NLP/CV çš„æ¶æ„ï¼Œè¿™ä¸ä¸€å®šé€‚åˆç”Ÿç‰©æ•°æ®çš„ç¨€ç–æ€§å’Œé•¿ç¨‹ä¾èµ–ã€‚BioArc åˆ©ç”¨ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰ç³»ç»Ÿåœ°æ¢ç´¢äº†ç”Ÿç‰©æ•°æ®çš„æœ€ä½³æ¶æ„ï¼Œä¸ä»…ä»…æ˜¯è°ƒå‚ï¼Œè€Œæ˜¯å¯»æ‰¾é€‚åˆç”Ÿç‰©â€œè¯­æ³•â€çš„æ¨¡å‹ç»“æ„ã€‚\n\n**10. [ECG åŸºç¡€æ¨¡å‹] å¾®è°ƒå¿ƒç”µå›¾åŸºç¡€æ¨¡å‹é¢„æµ‹å† çŠ¶åŠ¨è„‰ CT è¡€ç®¡é€ å½±ç»“æœ**\n**# title:** Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes\n**# authors:** Yujie Xiao et al.\n> **ä¸»è¦è´¡çŒ®ï¼š** åˆ©ç”¨ AI-ECG æ¨¡å‹é¢„æµ‹å† çŠ¶åŠ¨è„‰ç‹­çª„ï¼ˆé€šå¸¸éœ€è¦æ˜‚è´µçš„ CCTA æ£€æŸ¥ï¼‰ã€‚åœ¨å¤–éƒ¨éªŒè¯é›†ä¸Šï¼Œå¯¹å·¦ä¸»å¹²å† çŠ¶åŠ¨è„‰ç‹­çª„çš„é¢„æµ‹ AUC è¾¾åˆ°äº†æƒŠäººçš„ 0.971ã€‚è¿™æ˜¯ AI åŒ»ç–—ä»â€œè¾…åŠ©â€èµ°å‘â€œæ›¿ä»£æ€§ç­›æŸ¥â€çš„é‡è¦å®è¯ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„è®ºæ–‡\n\n*   **[KV Cache ä¼˜åŒ–] G-KV:** (#31) æå‡ºäº†ä¸€ç§åŸºäº**å…¨å±€æ³¨æ„åŠ›**çš„ KV Cache æ·˜æ±°æœºåˆ¶ï¼Œä¸ä»…ä»…çœ‹å±€éƒ¨ï¼Œè¿˜ç»“åˆå†å²æ³¨æ„åŠ›å¾—åˆ†ï¼Œé€‚åˆé•¿æ–‡æœ¬æ¨ç†ã€‚\n*   **[æ•°å­¦æ¨ç†] SCALE:** (#38) å€Ÿé‰´åŒé‡è¿‡ç¨‹ç†è®ºï¼ˆSystem 1/2ï¼‰ï¼Œåœ¨æµ‹è¯•æ—¶åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚ç®€å•çš„å­é—®é¢˜å¿«é€Ÿè¿‡ï¼Œéš¾çš„å­é—®é¢˜å¤šåˆ†é…ç®—åŠ›ï¼Œæ¯”å‡åŒ€ Scaling æ›´é«˜æ•ˆã€‚\n*   **[RL æ”¹è¿›] ESPO:** (#32) é’ˆå¯¹æ•°å­¦æ¨ç†çš„ **Entropy Importance Sampling Policy Optimization**ã€‚ä¸ºäº†è§£å†³ GRPO ç²—æš´å‰ªè£å¯¼è‡´çš„æ¢¯åº¦åˆ©ç”¨ç‡ä¸è¶³é—®é¢˜ï¼Œåˆ©ç”¨é¢„æµ‹ç†µæ¥åˆ†ç»„åºåˆ—ï¼Œæå‡äº† HMMT ä¸Šçš„å‡†ç¡®ç‡ã€‚\n*   **[ç¤¾äº¤é€‰æ‹©ç†è®º] Stable Voting:** (#11) è¿™æ˜¯ä¸€ä¸ªåç†è®ºçš„è®¡ç®—æœºç¤¾ä¼šé€‰æ‹©æ–¹å‘ï¼Œä½¿ç”¨ SAT solver è¯æ˜äº†å…³äºæŠ•ç¥¨æœºåˆ¶çš„çŒœæƒ³ï¼Œé€‚åˆå–œæ¬¢ç®—æ³•ç†è®ºçš„è¯»è€…ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2512.00670v1",
      "title": "EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients",
      "title_zh": "EDITï¼šåŸºäºè®­ç»ƒæ¢¯åº¦åŠ¨åŠ›å­¦çš„ dLLMs æ‰©æ•£æ¨ç†æ—©æœŸç»ˆæ­¢",
      "authors": [
        "He-Yen Hsieh",
        "Hong Wang",
        "H. T. Kung"
      ],
      "abstract": "Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EDIT (Early Diffusion Inference Termination)ï¼Œä¸€ç§é’ˆå¯¹æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLMs)çš„æ¨ç†æ—©æœŸç»ˆæ­¢å‡†åˆ™ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«æ¨ç†ç¨³å®šæ€§æ¥ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›‘ç£å¾®è°ƒ(SFT)é˜¶æ®µç”±AdamWèšåˆçš„LoRAæ›´æ–°æ‰€æ„å»ºçš„æ¨ç†å›¾(reasoning map)ï¼Œå®æ—¶ç›‘æµ‹Tokenæ¿€æ´»ä¸é¢„è®¾æ¨ç†è·¯å¾„çš„å¯¹é½æƒ…å†µã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒEDITé€šè¿‡è®¡ç®—è¿ç»­å»å™ªæ­¥éª¤é—´å·²å…¬å¼€Tokençš„KLæ•£åº¦(KL divergence)æ¥æ£€æµ‹æ”¶æ•›æ€§ï¼Œä»è€Œå®ç°è‡ªé€‚åº”åœæ­¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEDITåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šèƒ½å¤Ÿå‡å°‘11.8%è‡³68.3%çš„æ‰©æ•£æ­¥æ•°ï¼Œä¸”åœ¨å¤šæ•°æƒ…å†µä¸‹èƒ½ä¿æŒç”šè‡³æå‡æ¨¡å‹å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ¡ˆä»…å¼•å…¥çº¦0.02%çš„é¢å¤–å­˜å‚¨å¼€é”€ï¼Œæœ‰æ•ˆåˆ©ç”¨è®­ç»ƒæ¢¯åº¦åŠ¨åŠ›å­¦ä¸ºé™ä½dLLMæ¨ç†æˆæœ¬å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00670v1",
      "published_date": "2025-11-29 23:47:47 UTC",
      "updated_date": "2025-11-29 23:47:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:50:54.541892+00:00"
    },
    {
      "arxiv_id": "2512.00663v1",
      "title": "Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs",
      "title_zh": "å›¾ç»˜çœŸç›¸ï¼šå¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–å¹»è§‰æ£€æµ‹çš„ç»“æ„åŒ–å¯è§†åŒ–",
      "authors": [
        "Tanmay Agrawal"
      ],
      "abstract": "Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.",
      "tldr_zh": "å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¼ä¸šåº”ç”¨ä¸­å¸¸å› ä¸Šä¸‹æ–‡çª—å£é™åˆ¶åŠé¢„è®­ç»ƒæ•°æ®ä¸ä¸“æœ‰çŸ¥è¯†é—´çš„ä¸ä¸€è‡´è€Œäº§ç”Ÿå¹»è§‰ (hallucinations)ï¼Œä¸”ç°æœ‰çš„ç¼“è§£ç­–ç•¥å¦‚äººå·¥é—®ç­”ç­–å±•æˆ–äºŒæ¬¡æ¨¡å‹éªŒè¯å‡ç¼ºä¹ç¡®å®šæ€§ä¿éšœã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å°†ä¸“æœ‰çŸ¥è¯†å’Œæ¨¡å‹ç”Ÿæˆå†…å®¹ç»„ç»‡æˆäº¤äº’å¼è§†è§‰çŸ¥è¯†å›¾è°± (interactive visual knowledge graphs) çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¨¡å‹æ–­è¨€ (assertions) ä¸åº•å±‚äº‹å®æº (sources of truth) ç›¸é“¾æ¥å¹¶æ˜¾ç¤ºç½®ä¿¡åº¦ï¼Œä¸ºæœ€ç»ˆç”¨æˆ·æä¾›ç›´è§‚çš„è§†è§’æ¥è¯†åˆ«æ½œåœ¨çš„å¹»è§‰åŒºåŸŸã€‚ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è¯¥è§†è§‰ç•Œé¢è¯Šæ–­é€»è¾‘ä¸ä¸€è‡´æ€§ï¼Œè¯†åˆ«è–„å¼±çš„æ¨ç†é“¾å¹¶æä¾›çº æ­£æ€§åé¦ˆã€‚è¿™ç§äººæœºååŒ (human-in-the-loop) çš„å·¥ä½œæµå»ºç«‹äº†ç»“æ„åŒ–çš„åé¦ˆé—­ç¯ï¼Œæœ‰æ•ˆåœ°å¢å¼ºäº†æ¨¡å‹çš„å¯é æ€§å¹¶èƒ½æŒç»­æå‡å“åº”è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00663v1",
      "published_date": "2025-11-29 23:09:15 UTC",
      "updated_date": "2025-11-29 23:09:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:50:58.538361+00:00"
    },
    {
      "arxiv_id": "2512.00647v2",
      "title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba",
      "title_zh": "MambaScopeï¼šé¢å‘é«˜æ•ˆ Vision Mamba çš„ç”±ç²—åˆ°ç»†ç²’åº¦è°ƒèŠ‚",
      "authors": [
        "Shanhui Liu",
        "Rui Xu",
        "Yunke Wang"
      ],
      "abstract": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss as they discard or compress token representations. This problem is further exacerbated when the same fine-grained token processing is uniformly applied across all images regardless of visual complexity. We observe that not all inputs require fine-grained processing: simple images can be effectively handled at a coarse resolution, while only complex ones require refinement. Based on this insight, we propose MambaScope, an adaptive framework for efficient inference for Vision Mamba. MambaScope first performs coarse-grained inference by dividing the input image into large patches, significantly reducing token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover essential visual details with minimal additional cost. This dynamic resolution assignment strategy allows MambaScope to allocate computation adaptively according to image complexity, achieving efficient processing without compromising accuracy. Experiments across various vision tasks demonstrate that MambaScope outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MambaScopeï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ Vision Mamba é«˜æ•ˆæ¨ç†çš„è‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ token reduction æ–¹æ³•å› å‰ªè£æˆ–åˆå¹¶å¯¼è‡´çš„ä¿¡æ¯æŸå¤±é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨äº†ç”±ç²—åˆ°ç»† (Coarse-to-Fine) çš„åŠ¨æ€ä½œç”¨åŸŸæœºåˆ¶ï¼Œé¦–å…ˆé€šè¿‡å¤§å°ºå¯¸è¡¥ä¸ (large patches) è¿›è¡Œç²—ç²’åº¦æ¨ç†ä»¥å¤§å¹…é™ä½è®¡ç®—é‡ã€‚å½“æ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦è¾ƒä½æ—¶ï¼Œç³»ç»Ÿä¼šå¯¹ç‰¹å®šåŒºåŸŸè¿›è¡Œç»†ç²’åº¦é‡å¤„ç†ï¼Œä»è€Œåœ¨ä¿è¯æ•ˆç‡çš„åŒæ—¶æ¢å¤å…³é”®è§†è§‰ç»†èŠ‚ã€‚è¿™ç§ç­–ç•¥å®ç°äº†æ ¹æ®å›¾åƒå¤æ‚åº¦è‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æºï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œç»Ÿä¸€å¤„ç†çš„å±€é™æ€§ã€‚å®éªŒè¯æ˜ï¼ŒMambaScope åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸æ•ˆç‡å‡ä¼˜äºåŸºçº¿æ¨¡å‹åŠç°æœ‰çš„ token reduction æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00647v2",
      "published_date": "2025-11-29 21:58:08 UTC",
      "updated_date": "2025-12-03 10:45:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:00.248179+00:00"
    },
    {
      "arxiv_id": "2512.00641v1",
      "title": "Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition",
      "title_zh": "åŸºäºå¯¹æŠ—åŸŸå¯¹é½å›¾æ³¨æ„åŠ›ç½‘ç»œçš„é²æ£’è·¨åŸŸé¢éƒ¨è¡¨æƒ…è¯†åˆ«",
      "authors": [
        "Razieh Ghaedi",
        "AmirReza BabaAhmadi",
        "Reyer Zwiggelaar",
        "Xinqi Fan",
        "Nashid Alam"
      ],
      "abstract": "Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è·¨åŸŸé¢éƒ¨è¡¨æƒ…è¯†åˆ« (CD-FER) ä¸­ä¸¥é‡çš„é¢†åŸŸåç§»é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ ResNet-50 ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶å¼•å…¥æ‰¹æ¬¡çº§å›¾æ³¨æ„åŠ›ç½‘ç»œ (GAT) å°†æ¯ä¸ª mini-batch å»ºæ¨¡ä¸ºç¨€ç–ç¯çŠ¶å›¾ï¼Œä»è€Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èšåˆè·¨æ ·æœ¬çš„åˆ¤åˆ«æ€§ç‰¹å¾ã€‚ä¸ºäº†å®ç°åˆ†å¸ƒå¯¹é½ï¼ŒGAT-ADA åˆ›æ–°æ€§åœ°ç»“åˆäº†é€šè¿‡æ¢¯åº¦åè½¬å±‚ (GRL) å®ç°çš„å¯¹æŠ—æ€§å­¦ä¹ ï¼Œä»¥åŠåˆ©ç”¨ CORAL å’Œ MMD çš„ç»Ÿè®¡å¯¹é½æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªç›®æ ‡æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 74.39% çš„å¹³å‡è·¨åŸŸå‡†ç¡®ç‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ä» RAF-DB è¿ç§»è‡³ FER2013 çš„ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº† 98.0%ï¼Œç›¸æ¯”äºåŒç­‰æ¡ä»¶ä¸‹çš„åŸºå‡†æ¨¡å‹å®ç°äº†çº¦ 36% çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é²æ£’çš„è·¨åŸŸè¡¨æƒ…è¯†åˆ«æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ··åˆå¯¹é½æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 5 figures. Accepted at the 17th Asian Conference on Machine Learning (ACML 2025), Taipei, Taiwan, December 9-12, 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.00641v1",
      "published_date": "2025-11-29 21:32:03 UTC",
      "updated_date": "2025-11-29 21:32:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:03.543375+00:00"
    },
    {
      "arxiv_id": "2512.00639v1",
      "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation",
      "title_zh": "Doppler å¢å¼ºæ·±åº¦å­¦ä¹ ï¼šåŸºäº YOLOv5 å®ä¾‹åˆ†å‰²æå‡ç”²çŠ¶è…ºç»“èŠ‚åˆ†å‰²æ€§èƒ½",
      "authors": [
        "Mahmoud El Hussieni"
      ],
      "abstract": "The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ YOLOv5 ç®—æ³•å¯¹è¶…å£°å›¾åƒä¸­çš„ç”²çŠ¶è…ºç»“èŠ‚è¿›è¡Œå®ä¾‹åˆ†å‰² (instance segmentation)ï¼Œæ—¨åœ¨æ„å»ºé«˜æ•ˆçš„ AI è¾…åŠ©ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€‚ç ”ç©¶äººå‘˜è¯„ä¼°äº† YOLOv5 çš„å¤šç§å˜ä½“ï¼ˆNano, Small, Medium, Large å’Œ XLargeï¼‰ï¼Œå¹¶é‡ç‚¹åˆ†æäº†å¤šæ™®å‹’å›¾åƒ (doppler images) å¯¹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥é€šå¸¸è¢«ä¸´åºŠåŒ»ç”Ÿå¿½ç•¥çš„å¤šæ™®å‹’å›¾åƒå¯æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ï¼Œå…¶ä¸­ YOLOv5-Large åœ¨åŒ…å«å¤šæ™®å‹’æ•°æ®æ—¶å–å¾—äº† 91% çš„ Dice score å’Œ 0.87 çš„ mAPã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸å«å¤šæ™®å‹’å›¾åƒçš„æ¨¡å‹è¡¨ç°æ˜æ˜¾ä¸‹é™ï¼ŒéªŒè¯äº†å¤šæ™®å‹’ä¿¡æ¯åœ¨æå‡åˆ†å‰²ç²¾åº¦æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäº YOLOv5 çš„å®æ—¶å®ä¾‹åˆ†å‰²æŠ€æœ¯åœ¨ç”²çŠ¶è…ºç»“èŠ‚è‡ªåŠ¨è¯Šæ–­ç³»ç»Ÿä¸­å…·æœ‰å¹¿é˜”çš„ä¸´åºŠåº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00639v1",
      "published_date": "2025-11-29 21:24:36 UTC",
      "updated_date": "2025-11-29 21:24:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:17.042830+00:00"
    },
    {
      "arxiv_id": "2512.00626v2",
      "title": "XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance",
      "title_zh": "XAI é©±åŠ¨çš„çš®è‚¤ç—…åˆ†ç±»ï¼šåˆ©ç”¨ GANs å¢å¼º ResNet-50 æ€§èƒ½",
      "authors": [
        "Kim Gerard A. Villanueva",
        "Priyanka Kumar"
      ],
      "abstract": "Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the \"black box\" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç—…å¤šåˆ†ç±»ä¸­å­˜åœ¨çš„è¯„ä¼°ä¸»è§‚æ€§ã€HAM10000 æ•°æ®é›†å›ºæœ‰ç±»åˆ«ä¸å¹³è¡¡ä»¥åŠæ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹çš„â€œé»‘ç›’â€å±æ€§ï¼Œæå‡ºäº†ä¸€ç§é«˜å‡†ç¡®ç‡ä¸”å¯ä¿¡çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿ(CAD)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(DCGANs)æ‰§è¡Œé€ç±»åˆ«çš„æ•°æ®å¢å¼ºï¼Œæœ‰æ•ˆè§£å†³äº†å…³é”®çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶é‡‡ç”¨å¾®è°ƒåçš„ ResNet-50 æ¨¡å‹å¯¹ä¸ƒç§çš®è‚¤ç—…ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶é€šè¿‡é›†æˆ LIME å’Œ SHAP ç­‰å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æŠ€æœ¯ï¼Œç¡®ä¿æ¨¡å‹é¢„æµ‹åŸºäºä¸´åºŠç›¸å…³çš„å½¢æ€å­¦ç‰¹å¾ï¼Œä»è€Œæä¾›äº†å¿…è¦çš„é€æ˜åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå®ç°äº† 92.50% çš„æ€»å‡†ç¡®ç‡å’Œ 98.82% çš„ Macro-AUCï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†æ¶æ„ã€‚è¿™é¡¹å·¥ä½œéªŒè¯äº†ä¸€ä¸ªç»“åˆé«˜æ€§èƒ½ä¸ä¸´åºŠå¯è§£é‡Šæ€§çš„å¯éªŒè¯æ¡†æ¶ï¼Œä¸ºè¯Šæ–­éƒ¨ç½²çš„å®‰å…¨æ€§æä¾›äº†ä¿éšœï¼Œæœªæ¥çš„ç ”ç©¶å°†è‡´åŠ›äºæå‡é»‘ç´ ç˜¤(Melanoma NOS)ç­‰å…³é”®ç±»åˆ«çš„è¾¨åˆ«æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00626v2",
      "published_date": "2025-11-29 20:46:30 UTC",
      "updated_date": "2025-12-03 20:30:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:17.842855+00:00"
    },
    {
      "arxiv_id": "2512.00625v1",
      "title": "Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„æ ‘æœ¨æ¨ªæˆªé¢å›¾åƒé«“å¿ƒè‡ªåŠ¨æ£€æµ‹",
      "authors": [
        "Tzu-I Liao",
        "Mahmoud Fakhry",
        "Jibin Yesudas Varghese"
      ],
      "abstract": "Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹ï¼ŒåŒ…æ‹¬ YOLOv9ã€U-Netã€Swin Transformerã€DeepLabV3 å’Œ Mask R-CNNï¼Œæ—¨åœ¨å®ç°æœ¨ææ¨ªæˆªé¢é«“å¿ƒ(Pith Detection)çš„è‡ªåŠ¨åŒ–æ£€æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨åŒ…å«582å¼ æ ‡æ³¨å›¾åƒçš„æ•°æ®é›†å¹¶ç»“åˆåŠ¨æ€å¢å¼ºæŠ€æœ¯ï¼Œå‘ç° Swin Transformer åœ¨ç²¾ç»†åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°æœ€ä¼˜ï¼Œå‡†ç¡®ç‡é«˜è¾¾0.94ã€‚å®éªŒè¯¦ç»†åˆ†æäº†ä¸åŒæ¨¡å‹åœ¨è¾¹ç•Œç²¾åº¦ã€å¤šå°ºåº¦ç‰¹å¾æå–æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡å¼•å…¥éæå¤§å€¼æŠ‘åˆ¶(NMS)æŠ€æœ¯å°† Mask R-CNN çš„äº¤å¹¶æ¯”(IoU)ä»0.45æ˜¾è‘—æå‡è‡³0.80ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨æ©¡æ ‘æ•°æ®é›†éªŒè¯äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ¢è®¨äº†é€šè¿‡è¶…å‚æ•°å¾®è°ƒè§£å†³å¼ é‡ä¸åŒ¹é…å’Œè¾¹ç•Œä¸ä¸€è‡´é—®é¢˜çš„æ–¹æ¡ˆã€‚ç»“æœè¯æ˜äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨æå‡æ—ä¸šåŠæœ¨æè´¨é‡åˆ†ææ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒæ¨¡å‹é€‰æ‹©åº”ä¾æ®å…·ä½“çš„æ•°æ®é›†ç‰¹å¾å’Œåº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00625v1",
      "published_date": "2025-11-29 20:43:04 UTC",
      "updated_date": "2025-11-29 20:43:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:23.256263+00:00"
    },
    {
      "arxiv_id": "2512.00621v1",
      "title": "Melody or Machine: Detecting Synthetic Music with Dual-Stream Contrastive Learning",
      "title_zh": "Melody or Machineï¼šåŸºäºåŒæµå¯¹æ¯”å­¦ä¹ çš„åˆæˆéŸ³ä¹æ£€æµ‹",
      "authors": [
        "Arnesh Batra",
        "Dev Sharma",
        "Krish Thukral",
        "Ruhani Bhatia",
        "Naman Batra",
        "Aditya Gautam"
      ],
      "abstract": "The rapid evolution of end-to-end AI music generation poses an escalating threat to artistic authenticity and copyright, demanding detection methods that can keep pace. While foundational, existing models like SpecTTTra falter when faced with the diverse and rapidly advancing ecosystem of new generators, exhibiting significant performance drops on out-of-distribution (OOD) content. This generalization failure highlights a critical gap: the need for more challenging benchmarks and more robust detection architectures. To address this, we first introduce Melody or Machine (MoM), a new large-scale benchmark of over 130,000 songs (6,665 hours). MoM is the most diverse dataset to date, built with a mix of open and closed-source models and a curated OOD test set designed specifically to foster the development of truly generalizable detectors. Alongside this benchmark, we introduce CLAM, a novel dual-stream detection architecture. We hypothesize that subtle, machine-induced inconsistencies between vocal and instrumental elements, often imperceptible in a mixed signal, offer a powerful tell-tale sign of synthesis. CLAM is designed to test this hypothesis by employing two distinct pre-trained audio encoders (MERT and Wave2Vec2) to create parallel representations of the audio. These representations are fused by a learnable cross-aggregation module that models their inter-dependencies. The model is trained with a dual-loss objective: a standard binary cross-entropy loss for classification, complemented by a contrastive triplet loss which trains the model to distinguish between coherent and artificially mismatched stream pairings, enhancing its sensitivity to synthetic artifacts without presuming a simple feature alignment. CLAM establishes a new state-of-the-art in synthetic music forensics. It achieves an F1 score of 0.925 on our challenging MoM benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIç”ŸæˆéŸ³ä¹å¯¹è‰ºæœ¯çœŸå®æ€§å’Œç‰ˆæƒæ„æˆçš„å¨èƒï¼Œæå‡ºäº†Melody or Machine (MoM)å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡13ä¸‡é¦–æ­Œæ›²ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€æµ‹æ¨¡å‹åœ¨åˆ†å¸ƒå¤–(OOD)å†…å®¹ä¸Šæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæå‡æ£€æµ‹é²æ£’æ€§ï¼Œç ”ç©¶è€…å¼€å‘äº†åä¸ºCLAMçš„åŒæµæ£€æµ‹æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨MERTå’ŒWave2Vec2ä¸¤ç§é¢„è®­ç»ƒéŸ³é¢‘ç¼–ç å™¨æ•æ‰äººå£°ä¸ä¹å™¨å…ƒç´ ä¹‹é—´å¾®å¦™çš„æœºå™¨è¯±å¯¼ä¸ä¸€è‡´æ€§ã€‚CLAMé€šè¿‡å¯å­¦ä¹ çš„äº¤å‰èšåˆæ¨¡å—(cross-aggregation module)èåˆç‰¹å¾ï¼Œå¹¶é‡‡ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±ä¸å¯¹æ¯”ä¸‰å…ƒç»„æŸå¤±(contrastive triplet loss)çš„åŒé‡ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹åˆæˆç—•è¿¹çš„æ•æ„Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLAMåœ¨æŒ‘æˆ˜æ€§çš„MoMåŸºå‡†ä¸Šè¾¾åˆ°äº†0.925çš„F1åˆ†æ•°ï¼Œåˆ·æ–°äº†åˆæˆéŸ³ä¹å–è¯é¢†åŸŸçš„æœ€ä¼˜æ€§èƒ½è®°å½•ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡å»ºæ¨¡å£°éƒ¨é—´çš„å†…åœ¨çŸ›ç›¾æ¥è¯†åˆ«å¤æ‚AIéŸ³ä¹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at Transactions on Machine Learning Research (TMLR)",
      "pdf_url": "https://arxiv.org/pdf/2512.00621v1",
      "published_date": "2025-11-29 20:25:20 UTC",
      "updated_date": "2025-11-29 20:25:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:24.541481+00:00"
    },
    {
      "arxiv_id": "2512.00619v1",
      "title": "Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies",
      "title_zh": "å—ç¥ç»ç§‘å­¦å¯å‘çš„æŒç»­å­¦ä¹ è®°å¿†é‡æ”¾ï¼šé¢„æµ‹ç¼–ç ä¸åŸºäºåå‘ä¼ æ’­ç­–ç•¥çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Goutham Nalagatla",
        "Shreyas Grandhe"
      ],
      "abstract": "Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based generative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½åœ¨åŠ¨æ€ç¯å¢ƒä¸‹é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)æŒ‘æˆ˜ï¼Œå€Ÿé‰´ç”Ÿç‰©è®°å¿†å·©å›ºæœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé¢„æµ‹ç¼–ç (Predictive Coding)åŸåˆ™çš„ç”Ÿæˆå¼é‡æ”¾(Generative Replay)æ¡†æ¶ã€‚è®ºæ–‡å¯¹åŸºäºé¢„æµ‹ç¼–ç å’ŒåŸºäºåå‘ä¼ æ’­(Backpropagation)çš„ç”Ÿæˆå¼é‡æ”¾ç­–ç•¥è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†å…¶ä»»åŠ¡ä¿ç•™èƒ½åŠ›å’Œè¿ç§»æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºé¢„æµ‹ç¼–ç çš„é‡æ”¾ç­–ç•¥åœ¨ä»»åŠ¡ä¿ç•™æ€§èƒ½ä¸Šè¡¨ç°å“è¶Šï¼Œå¹³å‡æå‡äº†15.3%ï¼ŒåŒæ—¶åœ¨è¿ç§»æ•ˆç‡æ–¹é¢ä¹Ÿä¿æŒäº†è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å—ç¥ç»ç§‘å­¦å¯å‘çš„ç”Ÿç‰©å­¦æœºåˆ¶èƒ½ä¸ºæŒç»­å­¦ä¹ (Continual Learning)æŒ‘æˆ˜æä¾›æ›´å…·åŸåˆ™æ€§çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶æ·±å…¥æ­ç¤ºäº†ç”Ÿç‰©è®°å¿†è¿‡ç¨‹ä¸äººå·¥å­¦ä¹ ç³»ç»Ÿä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œä¸ºæœªæ¥å—ç¥ç»ç§‘å­¦å¯å‘çš„äººå·¥æ™ºèƒ½ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00619v1",
      "published_date": "2025-11-29 20:20:52 UTC",
      "updated_date": "2025-11-29 20:20:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:16.444122+00:00"
    },
    {
      "arxiv_id": "2512.00617v2",
      "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization",
      "title_zh": "ARTï¼šè‡ªé€‚åº”å“åº”å¾®è°ƒæ¡†æ¶â€”â€”åŸºäºå¤šæ™ºèƒ½ä½“é”¦æ ‡èµ›æœºåˆ¶çš„ LLM å“åº”ä¼˜åŒ–æ–¹æ³•",
      "authors": [
        "Omer Jauhar Khan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ART (Adaptive Response Tuning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å•æ¨¡å‹è¾“å‡ºä¸­å¸¸è§çš„è¿è´¯æ€§ä¸è¶³ã€å¹»è§‰ä»¥åŠå„é¢†åŸŸè´¨é‡ä¸ä¸€ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºé”¦æ ‡èµ›æ¨¡å¼çš„ ELO æ’åå’Œå¤šæ™ºèƒ½ä½“æ¨ç† (multi-agent reasoning) æŠ€æœ¯ï¼Œé€šè¿‡è®©å¤šä¸ªæ™ºèƒ½ä½“åœ¨ç»“æ„åŒ–å·¥ä½œæµä¸­è¿›è¡Œç«äº‰ã€æ‰¹åˆ¤ä¸åä½œï¼Œç³»ç»Ÿæ€§åœ°ä¼˜åŒ–è¾“å‡ºè´¨é‡ã€‚ART å¼•å…¥äº†å¯é…ç½®çš„é”¦æ ‡èµ›å‚æ•°ã€åŠ¨æ€æ™ºèƒ½ä½“é€‰æ‹©ä»¥åŠå¤šç§å…±è¯†èåˆç­–ç•¥ (consensus fusion strategies)ï¼Œä»è€Œç”Ÿæˆæ˜¾è‘—ä¼˜äºå•æ¨¡å‹è¡¨ç°çš„å…±è¯†å“åº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯é æ€§æ–¹é¢å‡æœ‰å¤§å¹…æå‡ï¼Œç»¼åˆè´¨é‡æŒ‡æ ‡æ”¹å–„äº† 8.4%ï¼Œä¸” ELO è¯„åˆ†æ”¶æ•›çš„ R^2 å€¼è¶…è¿‡ 0.96ã€‚è¿™ä¸€æ¡†æ¶ä¸ºéœ€è¦é«˜è´¨é‡ã€ç»å®¡æ ¸å“åº”çš„å®é™…åº”ç”¨æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ç”Ÿäº§å°±ç»ªçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 11 figures, 5 tables. IEEE conference-style paper with appendices",
      "pdf_url": "https://arxiv.org/pdf/2512.00617v2",
      "published_date": "2025-11-29 20:16:11 UTC",
      "updated_date": "2025-12-24 03:42:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:31.746633+00:00"
    },
    {
      "arxiv_id": "2512.00616v2",
      "title": "Stable Voting and the Splitting of Cycles",
      "title_zh": "ç¨³å®šæŠ•ç¥¨ä¸å¾ªç¯æ‹†è§£",
      "authors": [
        "Wesley H. Holliday",
        "Milan MossÃ©",
        "Chase Norman",
        "Eric Pacuit",
        "Cynthia Wang"
      ],
      "abstract": "Algorithms for resolving majority cycles in preference aggregation have been studied extensively in computational social choice. Several sophisticated cycle-resolving methods, including Tideman's Ranked Pairs, Schulze's Beat Path, and Heitzig's River, are refinements of the Split Cycle (SC) method that resolves majority cycles by discarding the weakest majority victories in each cycle. Recently, Holliday and Pacuit proposed a new refinement of Split Cycle, dubbed Stable Voting, and a simplification thereof, called Simple Stable Voting (SSV). They conjectured that SSV is a refinement of SC whenever no two majority victories are of the same size. In this paper, we prove the conjecture up to 6 alternatives and refute it for more than 6 alternatives. While our proof of the conjecture for up to 5 alternatives uses traditional mathematical reasoning, our 6-alternative proof and 7-alternative counterexample were obtained with the use of SAT solving. The SAT encoding underlying this proof and counterexample is applicable far beyond SC and SSV: it can be used to test properties of any voting method whose choice of winners depends only on the ordering of margins of victory by size.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®¡ç®—ç¤¾ä¼šé€‰æ‹©(computational social choice)ä¸­ç”¨äºè§£å†³åå¥½èšåˆ(preference aggregation)ä¸­å¤šæ•°å›è·¯(majority cycles)é—®é¢˜çš„ç®—æ³•ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº† Split Cycle (SC) æ–¹æ³•åŠå…¶æ”¹è¿›æ–¹æ¡ˆ Stable Voting å’Œ Simple Stable Voting (SSV)ï¼Œå¹¶é’ˆå¯¹ Holliday å’Œ Pacuit æå‡ºçš„å…³äº SSV æ˜¯å¦ä¸º SC ç»†åŒ–(refinement)çš„çŒœæƒ³è¿›è¡Œäº†æ·±å…¥éªŒè¯ã€‚é€šè¿‡ç»“åˆä¼ ç»Ÿæ•°å­¦æ¨ç†ä¸ SAT æ±‚è§£æŠ€æœ¯(SAT solving)ï¼Œç ”ç©¶è€…è¯æ˜äº†åœ¨å¤šæ•°èƒœå‡ºè§„æ¨¡äº’ä¸ç›¸ç­‰çš„æƒ…å†µä¸‹ï¼Œè¯¥çŒœæƒ³åœ¨å¤‡é€‰æ–¹æ¡ˆ(alternatives)æ•°é‡ä¸è¶…è¿‡ 6 ä¸ªæ—¶æˆç«‹ã€‚ç„¶è€Œï¼Œç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨ SAT æ±‚è§£å‘ç°äº†åä¾‹ï¼Œä»è€Œé©³å›äº†è¯¥çŒœæƒ³åœ¨å¤‡é€‰æ–¹æ¡ˆè¶…è¿‡ 6 ä¸ªæ—¶çš„æ™®éé€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºçš„ SAT ç¼–ç æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„æ‰©å±•æ€§ï¼Œå¯ç”¨äºæµ‹è¯•ä»»ä½•ä»…ä¾èµ–èƒœå·®(margins of victory)å¤§å°æ’åºçš„æŠ•ç¥¨æ–¹æ³•çš„é€»è¾‘æ€§è´¨ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "Final version forthcoming in Proceedings of the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2512.00616v2",
      "published_date": "2025-11-29 20:13:27 UTC",
      "updated_date": "2025-12-29 05:21:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:44.036079+00:00"
    },
    {
      "arxiv_id": "2512.00614v1",
      "title": "Hierarchical Decentralized Multi-Agent Coordination with Privacy-Preserving Knowledge Sharing: Extending AgentNet for Scalable Autonomous Systems",
      "title_zh": "å…·å¤‡éšç§ä¿æŠ¤çŸ¥è¯†å…±äº«çš„å±‚çº§åŒ–å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“ååŒï¼šé¢å‘å¯æ‰©å±•è‡ªä¸»ç³»ç»Ÿçš„ AgentNet æ‰©å±•",
      "authors": [
        "Goutham Nalagatla"
      ],
      "abstract": "Decentralized multi-agent systems have shown promise in enabling autonomous collaboration among LLM-based agents. While AgentNet demonstrated the feasibility of fully decentralized coordination through dynamic DAG topologies, several limitations remain: scalability challenges with large agent populations, communication overhead, lack of privacy guarantees, and suboptimal resource allocation. We propose AgentNet++, a hierarchical decentralized framework that extends AgentNet with multilevel agent organization, privacy-preserving knowledge sharing via differential privacy and secure aggregation, adaptive resource management, and theoretical convergence guarantees. Our approach introduces cluster-based hierarchies where agents self-organize into specialized groups, enabling efficient task routing and knowledge distillation while maintaining full decentralization. We provide formal analysis of convergence properties and privacy bounds, and demonstrate through extensive experiments on complex multi-agent tasks that AgentNet++ achieves 23% higher task completion rates, 40% reduction in communication overhead, and maintains strong privacy guarantees compared to AgentNet and other baselines. Our framework scales effectively to 1000+ agents while preserving the emergent intelligence properties of the original AgentNet.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentNet++ï¼Œè¿™æ˜¯ä¸€ä¸ªå±‚çº§åŒ–çš„å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æ‰©å±• AgentNet ä»¥è§£å†³å¤§è§„æ¨¡è‡ªä¸»ç³»ç»Ÿä¸­çš„æ‰©å±•æ€§ã€é€šä¿¡å¼€é”€å’Œéšç§ä¿æŠ¤ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¤šçº§æ™ºèƒ½ä½“ç»„ç»‡å’ŒåŸºäºé›†ç¾¤çš„å±‚çº§ç»“æ„ï¼ˆcluster-based hierarchiesï¼‰ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªå‘ç»„ç»‡æˆä¸“é—¨çš„å°ç»„ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ä»»åŠ¡è·¯ç”±å’ŒçŸ¥è¯†è’¸é¦ã€‚ä¸ºäº†ä¿éšœæ•°æ®å®‰å…¨ï¼ŒAgentNet++ é‡‡ç”¨äº†å·®åˆ†éšç§ï¼ˆDifferential Privacyï¼‰å’Œå®‰å…¨èšåˆï¼ˆSecure Aggregationï¼‰æŠ€æœ¯ï¼Œåœ¨å…¨å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹å®ç°äº†éšç§ä¿æŠ¤çš„çŸ¥è¯†å…±äº«ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†å…³äºæ”¶æ•›æ€§è´¨å’Œéšç§è¾¹ç•Œçš„ç†è®ºåˆ†æï¼Œå¹¶å¼•å…¥äº†è‡ªé€‚åº”èµ„æºç®¡ç†æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ‰©å±•è‡³ 1000 ä¸ªä»¥ä¸Šçš„æ™ºèƒ½ä½“ï¼Œç›¸æ¯” AgentNet åŠå…¶å®ƒåŸºçº¿æ¨¡å‹ï¼Œå…¶ä»»åŠ¡å®Œæˆç‡æå‡äº† 23%ï¼Œé€šä¿¡å¼€é”€é™ä½äº† 40%ã€‚AgentNet++ åœ¨ä¿ç•™åŸå§‹æ¶Œç°æ™ºèƒ½ï¼ˆemergent intelligenceï¼‰ç‰¹æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ä¸éšç§ä¿éšœã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "6 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00614v1",
      "published_date": "2025-11-29 20:07:20 UTC",
      "updated_date": "2025-11-29 20:07:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:04.838304+00:00"
    },
    {
      "arxiv_id": "2512.00612v1",
      "title": "Generalized Graph Transformer Variational Autoencoder",
      "title_zh": "å¹¿ä¹‰å›¾ Transformer å˜åˆ†è‡ªç¼–ç å™¨",
      "authors": [
        "Siddhant Karki"
      ],
      "abstract": "Graph link prediction has long been a central problem in graph representation learning in both network analysis and generative modeling. Recent progress in deep learning has introduced increasingly sophisticated architectures for capturing relational dependencies within graph-structured data. In this work, we propose the Generalized Graph Transformer Variational Autoencoder (GGT-VAE). Our model integrates Generalized Graph Transformer Architecture with Variational Autoencoder framework for link prediction. Unlike prior GraphVAE, GCN, or GNN approaches, GGT-VAE leverages transformer style global self-attention mechanism along with laplacian positional encoding to model structural patterns across nodes into a latent space without relying on message passing. Experimental results on several benchmark datasets demonstrate that GGT-VAE consistently achieves above-baseline performance in terms of ROC-AUC and Average Precision. To the best of our knowledge, this is among the first studies to explore graph structure generation using a generalized graph transformer backbone in a variational framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Generalized Graph Transformer Variational Autoencoder (GGT-VAE)ï¼Œæ—¨åœ¨è§£å†³ç½‘ç»œåˆ†æå’Œç”Ÿæˆå»ºæ¨¡ä¸­çš„å›¾é“¾è·¯é¢„æµ‹é—®é¢˜ã€‚è¯¥æ¨¡å‹å°† Generalized Graph Transformer æ¶æ„ä¸ Variational Autoencoder (VAE) æ¡†æ¶ç›¸ç»“åˆï¼Œåˆ©ç”¨ Transformer é£æ ¼çš„å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶ (global self-attention mechanism) å’Œæ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç  (laplacian positional encoding) æ¥å»ºæ¨¡èŠ‚ç‚¹é—´çš„ç»“æ„æ¨¡å¼ã€‚ä¸ä¼ ç»Ÿçš„ GraphVAEã€GCN æˆ– GNN ç­‰ä¾èµ–æ¶ˆæ¯ä¼ é€’ (message passing) çš„æ–¹æ³•ä¸åŒï¼ŒGGT-VAE èƒ½å¤Ÿç›´æ¥åœ¨æ½œç©ºé—´ä¸­æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGGT-VAE åœ¨ ROC-AUC å’Œ Average Precision æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæ˜¯é¦–æ‰¹åœ¨å˜åˆ†æ¡†æ¶ä¸‹åˆ©ç”¨å¹¿ä¹‰å›¾ Transformer éª¨å¹²ç½‘ç»œè¿›è¡Œå›¾ç»“æ„ç”Ÿæˆçš„æ¢ç´¢æ€§ç ”ç©¶ä¹‹ä¸€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00612v1",
      "published_date": "2025-11-29 19:53:44 UTC",
      "updated_date": "2025-11-29 19:53:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:50.647295+00:00"
    },
    {
      "arxiv_id": "2512.00607v2",
      "title": "On the Holographic Geometry of Deterministic Computation",
      "title_zh": "è®ºç¡®å®šæ€§è®¡ç®—çš„å…¨æ¯å‡ ä½•",
      "authors": [
        "Logan Nye"
      ],
      "abstract": "Standard simulations of Turing machines suggest a linear relationship between the temporal duration $t$ of a run and the amount of information that must be stored by known simulations to certify, verify, or regenerate the configuration at time $t$. For deterministic multitape Turing machines over a fixed finite alphabet, this apparent linear dependence is not intrinsic: any length-$t$ run can be simulated using $O(\\sqrt{t})$ work-tape cells via a Height Compression Theorem for succinct computation trees together with an Algebraic Replay Engine. In this paper we recast that construction in geometric and information-theoretic language. We interpret the execution trace as a spacetime DAG of local update events and exhibit a family of recursively defined holographic boundary summaries such that, along the square-root-space simulation, the total description length of all boundary data stored at any time is $O(\\sqrt{t})$. Using Kolmogorov complexity, we prove that every internal configuration has constant conditional description complexity given the appropriate boundary summary and time index, establishing that the spacetime bulk carries no additional algorithmic information beyond its boundary. We express this as a one-dimensional computational area law: there exists a simulation in which the information capacity of the active \"holographic screen'' needed to generate a spacetime region of volume proportional to $t$ is bounded by $O(\\sqrt{t})$. In this precise sense, deterministic computation on a one-dimensional work tape admits a holographic representation, with the bulk history algebraically determined by data residing on a lower-dimensional boundary screen.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¡®å®šæ€§è®¡ç®—çš„å‡ ä½•ä¸ä¿¡æ¯è®ºç‰¹æ€§ï¼ŒæŒ‘æˆ˜äº†å›¾çµæœº(Turing machines)æ¨¡æ‹Ÿä¸­æ—¶é—´æ­¥é•¿ $t$ ä¸æ‰€éœ€å­˜å‚¨ä¿¡æ¯é‡ä¹‹é—´ä¼ ç»Ÿçš„çº¿æ€§å…³ç³»ã€‚ä½œè€…åˆ©ç”¨ç®€æ˜è®¡ç®—æ ‘çš„é«˜é˜¶å‹ç¼©å®šç†(Height Compression Theorem)å’Œä»£æ•°é‡æ”¾å¼•æ“(Algebraic Replay Engine)ï¼Œè¯æ˜äº†é•¿åº¦ä¸º $t$ çš„è¿è¡Œè½¨è¿¹å¯ä»¥ä»…é€šè¿‡ $O(\\sqrt{t})$ çš„ç©ºé—´è¿›è¡Œæ¨¡æ‹Ÿã€‚é€šè¿‡å°†æ‰§è¡Œè½¨è¿¹å»ºæ¨¡ä¸ºå±€éƒ¨æ›´æ–°äº‹ä»¶çš„æ—¶ç©ºæœ‰å‘æ— ç¯å›¾(spacetime DAG)ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—é€’å½’å®šä¹‰çš„â€œå…¨æ¯è¾¹ç•Œæ‘˜è¦â€(holographic boundary summaries)ï¼Œå¹¶åˆ©ç”¨æŸ¯å°”è«å“¥æ´›å¤«å¤æ‚åº¦(Kolmogorov complexity)è¯æ˜äº†æ—¶ç©ºå—(spacetime bulk)ç›¸å¯¹äºå…¶è¾¹ç•Œä¸æºå¸¦é¢å¤–çš„ç®—æ³•ä¿¡æ¯ã€‚è¿™ç¡®ç«‹äº†ä¸€ç»´è®¡ç®—é¢ç§¯å®šå¾‹(one-dimensional computational area law)ï¼Œå³ç”Ÿæˆä½“ç§¯æ­£æ¯”äº $t$ çš„æ—¶ç©ºåŒºåŸŸæ‰€éœ€çš„â€œå…¨æ¯å±â€(holographic screen)ä¿¡æ¯å®¹é‡è¢«é™åˆ¶åœ¨ $O(\\sqrt{t})$ã€‚è¯¥æˆæœè¡¨æ˜ä¸€ç»´å·¥ä½œå¸¦ä¸Šçš„ç¡®å®šæ€§è®¡ç®—å…·æœ‰å…¨æ¯è¡¨ç¤ºç‰¹æ€§ï¼Œå…¶æœ¬ä½“å†å²å¯ç”±ä½ç»´è¾¹ç•Œä¸Šçš„æ•°æ®ä»£æ•°å†³å®šã€‚",
      "categories": [
        "cs.CC",
        "cs.AI"
      ],
      "primary_category": "cs.CC",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.00607v2",
      "published_date": "2025-11-29 19:47:22 UTC",
      "updated_date": "2025-12-05 03:53:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:51:59.146720+00:00"
    },
    {
      "arxiv_id": "2512.00602v1",
      "title": "AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation",
      "title_zh": "AgentODRLï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ ODRL ç”Ÿæˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Wanle Zhong",
        "Keman Huang",
        "Xiaoyong Du"
      ],
      "abstract": "The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality \"Natural Language-to-ODRL\" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† AgentODRLï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å°†è‡ªç„¶è¯­è¨€å‡†ç¡®ç¿»è¯‘ä¸ºå¼€æ”¾æ•°å­—æƒåˆ©è¯­è¨€ (ODRL) ç­–ç•¥æ—¶çš„é€»è¾‘å¤æ‚æ€§å’Œè®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ Orchestrator-Workers æ¶æ„ï¼Œé€šè¿‡ Orchestrator åŠ¨æ€åè°ƒè´Ÿè´£ç­–ç•¥ç”Ÿæˆçš„ Generatorã€åˆ†è§£å¤æ‚ç”¨ä¾‹çš„ Decomposer ä»¥åŠç®€åŒ–åµŒå¥—é€»è¾‘çš„ Rewriter æ™ºèƒ½ä½“ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç”Ÿæˆè´¨é‡ï¼Œç ”ç©¶åœ¨ ODRL Generator ä¸­å¼•å…¥äº†åŸºäºéªŒè¯å™¨çš„è¯­æ³•ç­–ç•¥å’Œç”± LoRA å¾®è°ƒæ¨¡å‹é©±åŠ¨çš„è¯­ä¹‰åå°„æœºåˆ¶ (Semantic Reflection)ã€‚å®éªŒåœ¨åŒ…å« 770 ä¸ªæ•°æ®ç©ºé—´ (Data Spaces) ç”¨ä¾‹çš„æ–°æ•°æ®é›†ä¸Šå¼€å±•ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜ AgentODRL åœ¨è¯­æ³•å’Œè¯­ä¹‰è¯„åˆ†æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶æ˜¾è‘—æå‡äº† ODRL ç­–ç•¥ç”Ÿæˆçš„è‡ªåŠ¨åŒ–æ°´å¹³ä¸ä¿çœŸåº¦ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„æ•°å­—æƒåˆ©ç®¡ç†æä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted by AAAI 2026. 9 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2512.00602v1",
      "published_date": "2025-11-29 19:19:50 UTC",
      "updated_date": "2025-11-29 19:19:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:08.935700+00:00"
    },
    {
      "arxiv_id": "2512.02069v1",
      "title": "Large Language Model based Smart Contract Auditing with LLMBugScanner",
      "title_zh": "LLMBugScannerï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½åˆçº¦å®¡è®¡",
      "authors": [
        "Yining Yuan",
        "Yifei Wang",
        "Yichang Xu",
        "Zachary Yahn",
        "Sihao Hu",
        "Ling Liu"
      ],
      "abstract": "This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs.\n  To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments.\n  We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLMBugScannerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(Large Language Model)çš„æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¾®è°ƒå’Œé›†æˆå­¦ä¹ (Ensemble Learning)æå‡å®¡è®¡çš„å¯é æ€§ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨ä¸åŒæ¼æ´ç±»å‹å’Œåˆçº¦ç»“æ„ä¸Šè¡¨ç°ä¸ä¸€è‡´çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†é¢†åŸŸçŸ¥è¯†é€‚é…(Domain Knowledge Adaptation)ç­–ç•¥ï¼Œåˆ©ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒ(Parameter-Efficient Tuning)æŠ€æœ¯ä½¿æ¨¡å‹æ•æ‰ä»£ç è¯­ä¹‰ä¸æ¼æ´æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLLMBugScannerå¼•å…¥äº†é›†æˆæ¨ç†(Ensemble Reasoning)æœºåˆ¶ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªæ¨¡å‹ä¼˜åŠ¿åŠå…±è¯†å†²çªè§£å†³ç­–ç•¥æ¥äº§å‡ºæ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒLLMBugScanneråœ¨å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡ä¼˜äºå•ä¸€çš„é¢„è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ºæ™ºèƒ½åˆçº¦å®¡è®¡æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§å¼ºã€æˆæœ¬ä½ä¸”å…·æœ‰æ‰©å±•æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02069v1",
      "published_date": "2025-11-29 19:13:44 UTC",
      "updated_date": "2025-11-29 19:13:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:28.139533+00:00"
    },
    {
      "arxiv_id": "2512.00601v2",
      "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization",
      "title_zh": "Clinical-R1ï¼šåˆ©ç”¨ä¸´åºŠç›®æ ‡ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–èµ‹èƒ½å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¿ å®ä¸”å…¨é¢çš„æ¨ç†",
      "authors": [
        "Boyang Gu",
        "Hongjian Zhou",
        "Bradley Max Segal",
        "Jinge Wu",
        "Zeyu Cao",
        "Hantao Zhong",
        "Lei Clifton",
        "Fenglin Liu",
        "David A. Clifton"
      ],
      "abstract": "Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹DeepSeek-R1ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸæ¨ç†è¿‡ç¨‹ä¸­ä»…å¼ºè°ƒå‡†ç¡®æ€§ï¼Œè€Œç¼ºä¹å¿ å®æ€§ï¼ˆfaithfulnessï¼‰å’Œå…¨é¢æ€§ï¼ˆcomprehensivenessï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸´åºŠç›®æ ‡ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆClinical-Objective Relative Policy Optimization, CRPOï¼‰ã€‚CRPOä½œä¸ºä¸€ç§å¯æ‰©å±•çš„å¤šç›®æ ‡ã€å¯éªŒè¯å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰æ–¹æ³•ï¼Œé€šè¿‡é›†æˆåŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·ï¼Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹åŒæ­¥ä¼˜åŒ–å‡†ç¡®æ€§ã€å¿ å®æ€§å’Œå…¨é¢æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ–¹æ³•è®­ç»ƒäº†æ‹¥æœ‰30äº¿å‚æ•°çš„Clinical-R1-3Bæ¨¡å‹ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCRPOåœ¨æ˜¾è‘—æå‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¤§å¹…å¢å¼ºäº†æ¨¡å‹åœ¨çœŸå®æ€§å’Œå®Œæ•´æ€§æ–¹é¢çš„æ¨ç†è¡¨ç°ã€‚è¯¥æ¡†æ¶ä¸ºå°†LLMæ¨ç†ä¸ä¸´åºŠç›®æ ‡å¯¹é½æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å®‰å…¨ã€æ›´å…·åä½œæ€§çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚è¿™ä¸€æˆæœä¹Ÿå‡¸æ˜¾äº†å¤šç›®æ ‡ã€å¯éªŒè¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åŒ»ç–—é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹åæœŸè®­ç»ƒï¼ˆpost-trainingï¼‰ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00601v2",
      "published_date": "2025-11-29 19:09:24 UTC",
      "updated_date": "2025-12-03 12:24:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:12.543092+00:00"
    },
    {
      "arxiv_id": "2512.00598v1",
      "title": "Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction",
      "title_zh": "æ„å»ºå…¬å¹³æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£ä»¥æå‡è„ŠæŸ±èåˆæœ¯åå¹¶å‘ç—‡é¢„æµ‹çš„å…¬å¹³æ€§",
      "authors": [
        "Yining Yuan",
        "J. Ben Tamo",
        "Wenqi Shi",
        "Yishan Zhong",
        "Micky C. Nnamdi",
        "B. Randall Brenn",
        "Steven W. Hwang",
        "May D. Wang"
      ],
      "abstract": "Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.\n  Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.\n  Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FAIR-MTLï¼Œä¸€ç§å…¬å¹³æ€§æ„ŸçŸ¥çš„å¤šä»»åŠ¡å­¦ä¹  (fairness-aware multitask learning) æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è„ŠæŸ±èåˆæ‰‹æœ¯ (spinal fusion surgery) åå¹¶å‘ç—‡ä¸¥é‡ç¨‹åº¦é¢„æµ‹çš„å…¬å¹³æ€§ä¸å‡†ç¡®æ€§ã€‚ä¸ä¾èµ–æ˜¾å¼æ•æ„Ÿå±æ€§çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒFAIR-MTL é‡‡ç”¨æ•°æ®é©±åŠ¨çš„å­ç¾¤æ¨æ–­æœºåˆ¶ï¼Œé€šè¿‡äººå£ç»Ÿè®¡åµŒå…¥ (demographic embedding) å’Œ k-means èšç±»è¯†åˆ«æ½œåœ¨çš„æ‚£è€…ç¾¤ä½“ï¼Œå¹¶ä»¥æ­¤æŒ‡å¯¼å¤šä»»åŠ¡æ¶æ„ä¸­çš„ä»»åŠ¡è·¯ç”±ã€‚ä¸ºäº†åº”å¯¹å­ç¾¤ä¸å¹³è¡¡ï¼Œç ”ç©¶é‡‡ç”¨äº†é€†é¢‘ç‡åŠ æƒ (inverse-frequency weighting) å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨å°è§„æ¨¡ç¾¤ä½“ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¹¶å‘ç—‡é¢„æµ‹ä¸­è¾¾åˆ°äº† 0.86 çš„ AUC å’Œ 75% çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ€§åˆ«ä¸å¹´é¾„åœ¨äººå£ç»Ÿè®¡å¹³ä»· (demographic parity) å’Œå‡ç­‰åŒ–èµ”ç‡ (equalized odds) ä¸Šçš„å·®å¼‚ã€‚ç»“åˆ SHAP å’Œ Gini é‡è¦æ€§åˆ†æï¼Œè¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨æ— ç›‘ç£å­ç¾¤å‘ç°å’Œä»»åŠ¡åˆ†è§£æŠ€æœ¯ï¼Œèƒ½å¤Ÿä¸ºä¸´åºŠæ‰‹æœ¯é£é™©è¯„ä¼°æä¾›æ›´å…·å¯è§£é‡Šæ€§ä¸”å…¬å¹³çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00598v1",
      "published_date": "2025-11-29 19:06:07 UTC",
      "updated_date": "2025-11-29 19:06:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:15.228100+00:00"
    },
    {
      "arxiv_id": "2512.00596v1",
      "title": "DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems",
      "title_zh": "DLRRECï¼šæ·±åº¦æ¨èç³»ç»Ÿä¸­åŸºäºå¤šæ¨¡æ€çŸ¥è¯†èåˆçš„æ½œè¡¨å¾å»å™ª",
      "authors": [
        "Jiahao Tian",
        "Zhenkai Wang"
      ],
      "abstract": "Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æ¨èç³»ç»Ÿåœ¨ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„é«˜ç»´ä¸”å˜ˆæ‚çš„å¤šæ¨¡æ€ç‰¹å¾æ—¶ï¼Œç”±äºç‰¹å¾é™æ€è¾“å…¥å¯¼è‡´å…¶ä¸æ ¸å¿ƒæ¨èä»»åŠ¡è„±èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºDLRRECçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡æ·±åº¦èåˆå¤šæ¨¡æ€çŸ¥è¯†ä¸ååŒçŸ¥è¯†ï¼Œå®ç°è¡¨ç¤ºå»å™ª(representation denoising)ã€‚æŠ€æœ¯ä¸Šï¼Œç ”ç©¶å°†é™ç»´è¿‡ç¨‹ç›´æ¥é›†æˆåˆ°æ¨èæ¨¡å‹ä¸­ï¼Œé€šè¿‡ç«¯åˆ°ç«¯ååŒè®­ç»ƒä½¿å…¶èƒ½å¤Ÿæ„ŸçŸ¥æœ€ç»ˆçš„æ’åºç›®æ ‡ï¼›åŒæ—¶å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ (contrastive learning)ç›®æ ‡ï¼Œå°†ååŒè¿‡æ»¤(collaborative filtering)ä¿¡å·æ˜¾å¼èå…¥æ½œåœ¨ç©ºé—´ã€‚è¿™ä¸€ååŒè¿‡ç¨‹ç²¾ç‚¼äº†åŸå§‹çš„LLMåµŒå…¥ï¼Œåœ¨æœ‰æ•ˆè¿‡æ»¤å™ªå£°çš„åŒæ—¶å¢å¼ºäº†ä»»åŠ¡ç›¸å…³ä¿¡å·ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¿™ç§é›†æˆçš„èåˆä¸å»å™ªç­–ç•¥æ˜¾è‘—æå‡äº†æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºæ¨èç³»ç»Ÿé«˜æ•ˆåˆ©ç”¨LLMsæä¾›äº†åŸºç¡€èŒƒå¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00596v1",
      "published_date": "2025-11-29 18:57:42 UTC",
      "updated_date": "2025-11-29 18:57:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:24.344256+00:00"
    },
    {
      "arxiv_id": "2512.00595v1",
      "title": "IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference",
      "title_zh": "IslandRunï¼šé¢å‘åˆ†å¸ƒå¼ AI æ¨ç†çš„éšç§æ„ŸçŸ¥å¤šç›®æ ‡ç¼–æ’",
      "authors": [
        "Bala Siva Sai Akhil Malepati"
      ],
      "abstract": "Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous \"islands\" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IslandRunï¼Œä¸€ç§é’ˆå¯¹åˆ†å¸ƒå¼AIæ¨ç†çš„å¤šç›®æ ‡ç¼–æ’ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ€§èƒ½ã€éšç§ã€æˆæœ¬ä¸ä¿¡ä»»ä¹‹é—´éš¾ä»¥è°ƒå’Œçš„çŸ›ç›¾ã€‚IslandRunå°†è®¡ç®—èµ„æºè§†ä¸ºåˆ†å¸ƒåœ¨ä¸ªäººè®¾å¤‡ã€ç§æœ‰è¾¹ç¼˜æœåŠ¡å™¨å’Œå…¬å…±äº‘ä¸Šçš„è‡ªæ²»islandsï¼Œå¼¥è¡¥äº†Kubernetesã€Federated Learningç­‰ç°æœ‰æ¡†æ¶åœ¨å¤„ç†å¼‚æ„ç¯å¢ƒå¤šç»´ä¼˜åŒ–æ—¶çš„ä¸è¶³ã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†agent-based routingã€åˆ†å±‚ä¿¡ä»»ç»„å’Œå¯é€†åŒ¿ååŒ–ï¼Œå¹¶åˆ©ç”¨data localityå®ç°è®¡ç®—å‘æ•°æ®çš„åŠ¨æ€è·¯ç”±ã€‚é€šè¿‡typed placeholder sanitizationæŠ€æœ¯ï¼Œå®ƒèƒ½åœ¨è·¨è¶Šä¿¡ä»»è¾¹ç•Œæ—¶æœ‰æ•ˆä¿ç•™ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¼‚æ„ä¸ªäººè®¡ç®—ç”Ÿæ€ç³»ç»Ÿä¸‹çš„éšç§ä¿æŠ¤ã€å»ä¸­å¿ƒåŒ–æ¨ç†ç¼–æ’å»ºç«‹äº†ä¸€ä¸ªå…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.DC",
      "comment": "15 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00595v1",
      "published_date": "2025-11-29 18:52:27 UTC",
      "updated_date": "2025-11-29 18:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:27.542716+00:00"
    },
    {
      "arxiv_id": "2512.00590v1",
      "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
      "title_zh": "Wikonticï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ„å»ºå¯¹é½ Wikidata çš„æœ¬ä½“æ„ŸçŸ¥çŸ¥è¯†å›¾è°±",
      "authors": [
        "Alla Chepurova",
        "Aydar Bulatov",
        "Yuri Kuratov",
        "Mikhail Burtsev"
      ],
      "abstract": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Wikonticï¼Œä¸€ç§æ—¨åœ¨æ„å»ºä¸Wikidataå¯¹é½ä¸”å…·æœ‰æœ¬ä½“æ„ŸçŸ¥(Ontology-Aware)èƒ½åŠ›çš„çŸ¥è¯†å›¾è°±(Knowledge Graphs)çš„å¤šé˜¶æ®µæµæ°´çº¿ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å¼€æ”¾åŸŸæ–‡æœ¬ä¸­æå–å¸¦æœ‰é™å®šè¯(Qualifiers)çš„å€™é€‰ä¸‰å…ƒç»„ï¼Œå¹¶å®æ–½åŸºäºWikidataçš„ç±»å‹ä¸å…³ç³»çº¦æŸä»¥åŠå®ä½“è§„èŒƒåŒ–ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆçš„ç»“æ„åŒ–çŸ¥è¯†çš„ç´§å‡‘æ€§ä¸ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒWikonticåœ¨MuSiQueæ•°æ®é›†ä¸Šçš„ä¸‰å…ƒç»„æ­£ç¡®å®ä½“è¦†ç›–ç‡è¾¾96%ï¼Œä¸”åœ¨HotpotQAå’ŒMuSiQueé—®ç­”ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜äºå¤šç§æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWikonticåœ¨MINE-1åŸºå‡†æµ‹è¯•ä¸­ä»¥86%çš„ä¿¡æ¯ä¿ç•™ç‡åˆ›ä¸‹SOTAè®°å½•ï¼Œä¸”åœ¨æ„å»ºæ•ˆç‡ä¸Šæ¯”AriGraphå’ŒGraphRAGåˆ†åˆ«å¿«3å€å’Œ20å€ä»¥ä¸Šã€‚è¯¥ç ”ç©¶ä¸ä»…ä¼˜åŒ–äº†çŸ¥è¯†å›¾è°±çš„ç”Ÿæˆè´¨é‡ï¼Œè¿˜ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00590v1",
      "published_date": "2025-11-29 18:44:25 UTC",
      "updated_date": "2025-11-29 18:44:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:46.545242+00:00"
    },
    {
      "arxiv_id": "2512.00579v1",
      "title": "Slovak Conceptual Dictionary",
      "title_zh": "æ–¯æ´›ä¼å…‹è¯­æ¦‚å¿µè¯å…¸",
      "authors": [
        "Miroslav BlÅ¡tÃ¡k"
      ],
      "abstract": "When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†Slovak Conceptual Dictionaryï¼Œè¿™æ˜¯æ–¯æ´›ä¼å…‹è¯­(Slovak)é¢†åŸŸé¦–ä¸ªæ­¤ç±»è¯­è¨€å­¦å·¥å…·ï¼Œæ—¨åœ¨è§£å†³è¯¥è¯­è¨€åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(Natural Language Processing)ä¸­é¢ä¸´çš„èµ„æºåŒ®ä¹(low resourced languages)é—®é¢˜ã€‚ç”±äºç›®å‰ç¼ºä¹å¤§è§„æ¨¡ä¸”æœºå™¨å¯è¯»(machine-readable)çš„è¯­è¨€æ•°æ®æºï¼Œæ–¯æ´›ä¼å…‹è¯­çš„è‡ªåŠ¨æ–‡æœ¬å¤„ç†ä»»åŠ¡è¡¨ç°è¾ƒå·®ä¸”éƒ¨åˆ†ä»»åŠ¡éš¾ä»¥å®ç°ã€‚è¯¥è¯å…¸çš„å»ºç«‹ä¸ºæ–¯æ´›ä¼å…‹è¯­æä¾›äº†åŸºç¡€æ€§çš„è¯­è¨€èµ„æºï¼Œæœ‰æ•ˆå¡«è¡¥äº†æœºå™¨å¯è¯»æ•°æ®çš„ç©ºç™½ã€‚è¿™ä¸€æˆæœä¸ä»…æœ‰åŠ©äºæå‡å„é¡¹æ–¯æ´›ä¼å…‹è¯­NLPä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œè¿˜ä¸ºè§£å†³æ­¤å‰åœ¨è¯¥è¯­è¨€ç¯å¢ƒä¸‹å‡ ä¹æ— æ³•å¤„ç†çš„ä»»åŠ¡å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00579v1",
      "published_date": "2025-11-29 18:15:28 UTC",
      "updated_date": "2025-11-29 18:15:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:59.752051+00:00"
    },
    {
      "arxiv_id": "2512.00572v2",
      "title": "Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models",
      "title_zh": "èåˆéª¨éª¼è¡¨å¾ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç¨³å¥ç‘œä¼½å§¿åŠ¿åˆ†ç±»",
      "authors": [
        "Mohammed Mohiuddin",
        "Syed Mohammod Minhaz Hossain",
        "Sumaiya Khanam",
        "Prionkar Barua",
        "Aparup Barua",
        "MD Tamim Hossain"
      ],
      "abstract": "Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception), using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross-validation analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‘œä¼½ç»ƒä¹ ä¸­å› å§¿åŠ¿ä¸å½“å¯¼è‡´çš„å—ä¼¤é£é™©ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–ç‘œä¼½å§¿åŠ¿åˆ†ç±»æ–¹æ³•ï¼Œä»¥å‡å°‘å¯¹ä¸“ä¸šæŒ‡å¯¼çš„ä¾èµ–ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åä¸º Yoga-16 çš„æ•°æ®é›†ï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°äº† VGG16ã€ResNet50 å’Œ Xception ä¸‰ç§æ·±åº¦å­¦ä¹ æ¶æ„åœ¨ä¸åŒè¾“å…¥æ¨¡æ€ä¸‹çš„è¡¨ç°ã€‚å®éªŒå¯¹æ¯”äº†åŸå§‹å›¾åƒã€MediaPipe Pose éª¨æ¶å›¾åƒä»¥åŠ YOLOv8 Pose éª¨æ¶å›¾åƒï¼Œç»“æœæ˜¾ç¤ºåŸºäºéª¨æ¶çš„è¡¨ç¤ºæ–¹å¼åœ¨é²æ£’æ€§ä¸Šæ˜¾è‘—ä¼˜äºç›´æ¥çš„åŸå§‹å›¾åƒè¾“å…¥ã€‚å…¶ä¸­ï¼Œé‡‡ç”¨ MediaPipe Pose éª¨æ¶è¾“å…¥çš„ VGG16 æ¨¡å‹å–å¾—äº† 96.09% çš„æœ€é«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ Grad-CAM æŠ€æœ¯å’Œäº¤å‰éªŒè¯åˆ†ææä¾›äº†æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ï¼Œä¸ºæ„å»ºç²¾å‡†ä¸”å¯ä¿¡çš„è¾…åŠ©å¥èº«æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00572v2",
      "published_date": "2025-11-29 18:00:33 UTC",
      "updated_date": "2025-12-04 14:45:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:56.341818+00:00"
    },
    {
      "arxiv_id": "2512.00571v1",
      "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization",
      "title_zh": "åˆ©ç”¨è¤ç«è™«ç®—æ³•ä¼˜åŒ–å¢å¼ºåŸºäºç±»æ¯”çš„è½¯ä»¶å·¥ä½œé‡ä¼°ç®—",
      "authors": [
        "Tarun Chintada",
        "Uday Kiran Cheera"
      ],
      "abstract": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FAABE (Firefly Algorithm-guided Analogy-Based Estimation) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŸºäºç±»æ¯”çš„ä¼°ç®— (Analogy-Based Estimation, ABE) åœ¨å¤„ç†ä¸ä»¥å¾€é¡¹ç›®å·®å¼‚è¾ƒå¤§çš„æ–°è½¯ä»¶é¡¹ç›®æ—¶ç²¾åº¦å—é™çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†è¤ç«è™«ç®—æ³• (Firefly Algorithm, FA) ä¸ ABE ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥ç‰¹å¾é€‰æ‹© (feature selection) æŠ€æœ¯æ¥ä¼˜åŒ–é¢„æµ‹æ•ˆç‡ã€‚ç ”ç©¶åœ¨ Cocomo81ã€Desharnaisã€Chinaã€Albrechtã€Kemerer å’Œ Maxwell ç­‰äº”ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶åˆ©ç”¨ MMREã€MAEã€MSE å’Œ RMSE ç­‰å¤šç§æŒ‡æ ‡è¿›è¡Œè¯¯å·®åº¦é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼ŒFAABE åœ¨é¢„æµ‹ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œè¯æ˜äº† Firefly-Analogy é›†æˆæ¡†æ¶åœ¨è½¯ä»¶å·¥ä½œé‡ä¼°ç®—é¢†åŸŸçš„æœ‰æ•ˆæ€§ä¸åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 3 figures, 2 tables. Research conducted in June 2024",
      "pdf_url": "https://arxiv.org/pdf/2512.00571v1",
      "published_date": "2025-11-29 17:56:51 UTC",
      "updated_date": "2025-11-29 17:56:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:52.645987+00:00"
    },
    {
      "arxiv_id": "2512.02066v1",
      "title": "Parallel Multi-Circuit Quantum Feature Fusion in Hybrid Quantum-Classical Convolutional Neural Networks for Breast Tumor Classification",
      "title_zh": "ç”¨äºä¹³è…ºè‚¿ç˜¤åˆ†ç±»çš„æ··åˆé‡å­-ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œå¹¶è¡Œå¤šç”µè·¯é‡å­ç‰¹å¾èåˆ",
      "authors": [
        "Ece Yurtseven"
      ],
      "abstract": "Quantum machine learning has emerged as a promising approach to improve feature extraction and classification tasks in high-dimensional data domains such as medical imaging. In this work, we present a hybrid Quantum-Classical Convolutional Neural Network (QCNN) architecture designed for the binary classification of the BreastMNIST dataset, a standardized benchmark for distinguishing between benign and malignant breast tumors. Our architecture integrates classical convolutional feature extraction with two distinct quantum circuits: an amplitude-encoding variational quantum circuit (VQC) and an angle-encoding VQC circuit with circular entanglement, both implemented on four qubits. These circuits generate quantum feature embeddings that are fused with classical features to form a joint feature space, which is subsequently processed by a fully connected classifier. To ensure fairness, the hybrid QCNN is parameter-matched against a baseline classical CNN, allowing us to isolate the contribution of quantum layers. Both models are trained under identical conditions using the Adam optimizer and binary cross-entropy loss. Experimental evaluation in five independent runs demonstrates that the hybrid QCNN achieves statistically significant improvements in classification accuracy compared to the classical CNN, as validated by a one-sided Wilcoxon signed rank test (p = 0.03125) and supported by large effect size of Cohen's d = 2.14. Our results indicate that hybrid QCNN architectures can leverage entanglement and quantum feature fusion to enhance medical image classification tasks. This work establishes a statistical validation framework for assessing hybrid quantum models in biomedical applications and highlights pathways for scaling to larger datasets and deployment on near-term quantum hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆé‡å­-ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œ (hybrid QCNN) æ¶æ„ï¼Œä¸“é—¨ç”¨äº BreastMNIST æ•°æ®é›†çš„ä¹³è…ºè‚¿ç˜¤è‰¯æ¶æ€§äºŒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ç»å…¸å·ç§¯ç‰¹å¾ä¸ä¸¤ä¸ªåŸºäºå››é‡å­æ¯”ç‰¹çš„å˜åˆ†é‡å­ç”µè·¯ (VQC)â€”â€”å³æŒ¯å¹…ç¼–ç  VQC å’Œå¸¦æœ‰å¾ªç¯çº ç¼ çš„è§’åº¦ç¼–ç  VQCâ€”â€”æ‰€ç”Ÿæˆçš„é‡å­ç‰¹å¾åµŒå…¥è¿›è¡Œå¹¶è¡Œèåˆã€‚é€šè¿‡åœ¨è”åˆç‰¹å¾ç©ºé—´ä¸Šè¿›è¡Œå¤„ç†ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå……åˆ†åˆ©ç”¨é‡å­çº ç¼ å’Œç‰¹å¾èåˆæ¥å¢å¼ºå›¾åƒè¡¨å¾ã€‚åœ¨ä¸å‚æ•°åŒ¹é…çš„åŸºå‡†ç»å…¸ CNN è¿›è¡Œçš„å¯¹æ¯”å®éªŒä¸­ï¼Œhybrid QCNN åœ¨äº”æ¬¡ç‹¬ç«‹è¿è¡Œä¸­å‡è¡¨ç°å‡ºç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ (p = 0.03125)ï¼Œä¸”æ•ˆåº”é‡è¾¾ Cohen's d = 2.14ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†æ··åˆé‡å­æ¶æ„åœ¨æå‡é«˜ç»´åŒ»ç–—å½±åƒåˆ†ç±»ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥åœ¨è¿‘å®æ—¶é‡å­ç¡¬ä»¶ä¸Šéƒ¨ç½²ç”Ÿç‰©åŒ»å­¦æ¨¡å‹æä¾›äº†ç»Ÿè®¡éªŒè¯æ¡†æ¶ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02066v1",
      "published_date": "2025-11-29 17:47:14 UTC",
      "updated_date": "2025-11-29 17:47:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:52:55.347624+00:00"
    },
    {
      "arxiv_id": "2512.00565v1",
      "title": "Describe Anything Anywhere At Any Moment",
      "title_zh": "éšæ—¶éšåœ°æè¿°ä¸‡ç‰©",
      "authors": [
        "Nicolas Gorlo",
        "Lukas Schmid",
        "Luca Carlone"
      ],
      "abstract": "Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.\n  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¢å¼ºç°å®å’Œæœºå™¨äººè‡ªä¸»æ€§ä¸­æ—¶ç©ºè®°å¿†æ¡†æ¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ 3D è¯­è¨€å®šä½ç²¾åº¦ä¸å®æ—¶è¯­ä¹‰æè¿°æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº† Describe Anything, Anywhere, at Any Moment (DAAAM) æ¡†æ¶ã€‚DAAAM å¼•å…¥äº†ä¸€ç§åŸºäºä¼˜åŒ–çš„å‰ç«¯ï¼Œåˆ©ç”¨ Describe Anything Model (DAM) ç­‰å±€éƒ¨å­—å¹•æ¨¡å‹å¹¶ç»“åˆæ‰¹å¤„ç†æŠ€æœ¯ï¼Œå°†åœ¨çº¿æ¨ç†é€Ÿåº¦æé«˜äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œå®ç°äº†å¤§è§„æ¨¡å®æ—¶ 4D åœºæ™¯ç†è§£ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå±‚æ¬¡åŒ–çš„ 4D scene graph (SG) æä¾›å…¨å±€ç©ºé—´ä¸æ—¶é—´ä¸€è‡´çš„è®°å¿†è¡¨ç¤ºï¼Œå¹¶èƒ½ä¸ tool-calling agent æ— ç¼å¯¹æ¥ä»¥æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAAAM åœ¨ OC-NaVQA åŸºå‡†æµ‹è¯•ä¸­çš„é—®ç­”å‡†ç¡®ç‡æå‡äº† 53.6%ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†ä½ç½®ä¸æ—¶é—´è¯¯å·®ã€‚åœ¨ SG3D ä»»åŠ¡ä¸­ï¼Œå…¶å®šä½å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹æé«˜äº† 27.8%ï¼Œå……åˆ†è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤§è§„æ¨¡ã€é•¿å‘¨æœŸç¯å¢ƒä¸‹çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00565v1",
      "published_date": "2025-11-29 17:27:17 UTC",
      "updated_date": "2025-11-29 17:27:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:03.135201+00:00"
    },
    {
      "arxiv_id": "2512.00563v1",
      "title": "Explainable Multi-Modal Deep Learning for Automatic Detection of Lung Diseases from Respiratory Audio Signals",
      "title_zh": "åŸºäºå‘¼å¸éŸ³ä¿¡å·è‚ºéƒ¨ç–¾ç—…è‡ªåŠ¨æ£€æµ‹çš„å¯è§£é‡Šå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ",
      "authors": [
        "S M Asiful Islam Saky",
        "Md Rashidul Islam",
        "Md Saiful Arefin",
        "Shahaba Alam"
      ],
      "abstract": "Respiratory diseases remain major global health challenges, and traditional auscultation is often limited by subjectivity, environmental noise, and inter-clinician variability. This study presents an explainable multimodal deep learning framework for automatic lung-disease detection using respiratory audio signals. The proposed system integrates two complementary representations: a spectral-temporal encoder based on a CNN-BiLSTM Attention architecture, and a handcrafted acoustic-feature encoder capturing physiologically meaningful descriptors such as MFCCs, spectral centroid, spectral bandwidth, and zero-crossing rate. These branches are combined through late-stage fusion to leverage both data-driven learning and domain-informed acoustic cues. The model is trained and evaluated on the Asthma Detection Dataset Version 2 using rigorous preprocessing, including resampling, normalization, noise filtering, data augmentation, and patient-level stratified partitioning. The study achieved strong generalization with 91.21% accuracy, 0.899 macro F1-score, and 0.9866 macro ROC-AUC, outperforming all ablated variants. An ablation study confirms the importance of temporal modeling, attention mechanisms, and multimodal fusion. The framework incorporates Grad-CAM, Integrated Gradients, and SHAP, generating interpretable spectral, temporal, and feature-level explanations aligned with known acoustic biomarkers to build clinical transparency. The findings demonstrate the framework's potential for telemedicine, point-of-care diagnostics, and real-world respiratory screening.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨æ£€æµ‹è‚ºéƒ¨ç–¾ç—…çš„å¯è§£é‡Šå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿå¬è¯Šä¸­å­˜åœ¨çš„ä¸»è§‚æ€§å’Œç¯å¢ƒå™ªå£°é™åˆ¶ã€‚ç³»ç»Ÿæ•´åˆäº†åŸºäºCNN-BiLSTM Attentionçš„é¢‘è°±-æ—¶é—´ç¼–ç å™¨ä»¥åŠæ•è·MFCCsã€spectral centroidã€spectral bandwidthå’Œzero-crossing rateç­‰ç”Ÿç†æè¿°ç¬¦çš„æ‰‹å·¥å£°å­¦ç‰¹å¾ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡åæœŸèåˆ(late-stage fusion)æŠ€æœ¯ç»“åˆäº†æ•°æ®é©±åŠ¨å­¦ä¹ ä¸é¢†åŸŸå…ˆéªŒçŸ¥è¯†ã€‚åœ¨Asthma Detection Dataset Version 2ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶å®ç°äº†91.21%çš„å‡†ç¡®ç‡å’Œ0.9866çš„macro ROC-AUCï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºå„ç±»æ¶ˆèå˜ä½“ã€‚ä¸ºäº†å¢å¼ºä¸´åºŠé€æ˜åº¦ï¼Œç ”ç©¶å¼•å…¥äº†Grad-CAMã€Integrated Gradientså’ŒSHAPç­‰æŠ€æœ¯ï¼Œç”Ÿæˆäº†ä¸å·²çŸ¥å£°å­¦ç‰¹å¾ä¸€è‡´çš„é¢‘è°±ã€æ—¶é—´å’Œç‰¹å¾çº§è§£é‡Šã€‚è¯¥ç ”ç©¶æˆæœåœ¨è¿œç¨‹åŒ»ç–—ã€å³æ—¶è¯Šæ–­å’Œç°å®ä¸–ç•Œçš„å‘¼å¸ç³»ç»Ÿç­›æŸ¥ä¸­å±•ç°å‡ºå·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00563v1",
      "published_date": "2025-11-29 17:15:58 UTC",
      "updated_date": "2025-11-29 17:15:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:14.839647+00:00"
    },
    {
      "arxiv_id": "2512.00553v1",
      "title": "List Replicable Reinforcement Learning",
      "title_zh": "åˆ—è¡¨å¯é‡å¤å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Bohan Zhang",
        "Michael Chen",
        "A. Pavan",
        "N. V. Vinodchandran",
        "Lin F. Yang",
        "Ruosong Wang"
      ],
      "abstract": "Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \\emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \\emph{small list} of policies across different runs, with high probability. The size of this list defines the \\emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \\emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­çš„å¯é‡å¤æ€§(Replicability)æŒ‘æˆ˜ï¼Œæ—¨åœ¨è§£å†³RLç®—æ³•å› ä¸ç¨³å®šæ€§è€Œå¯¹è®­ç»ƒæ¡ä»¶æ•æ„Ÿçš„é—®é¢˜ã€‚ä½œè€…åœ¨å¯èƒ½è¿‘ä¼¼æ­£ç¡®(PAC)æ¡†æ¶ä¸‹å¼•å…¥äº†åˆ—è¡¨å¯é‡å¤æ€§(List Replicability)æ¦‚å¿µï¼Œè¦æ±‚ç®—æ³•åœ¨å¤šæ¬¡è¿è¡Œä¸­ä»¥é«˜æ¦‚ç‡è¿”å›ä¸€ä¸ªå°å‹ç­–ç•¥åˆ—è¡¨ä¸­çš„è¿‘ä¼˜ç­–ç•¥ã€‚ç ”ç©¶å®šä¹‰äº†å¼±å’Œå¼ºä¸¤ç§åˆ—è¡¨å¯é‡å¤æ€§å½¢å¼ï¼Œåˆ†åˆ«çº¦æŸæœ€ç»ˆç­–ç•¥å’Œæ•´ä¸ªç­–ç•¥æ‰§è¡Œè½¨è¿¹ã€‚è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯æå‡ºäº†ä¸€ç§å¯è¯æ˜é«˜æ•ˆçš„è¡¨æ ¼RLç®—æ³•ï¼Œå°†åˆ—è¡¨å¤æ‚åº¦(List Complexity)ä»ä¼ ç»Ÿç®—æ³•çš„æŒ‡æ•°çº§é™ä½è‡³å¤šé¡¹å¼çº§åˆ«ã€‚è¯¥ç®—æ³•é‡‡ç”¨äº†åˆ›æ–°çš„å­—å…¸åº(Lexicographic Order)è§„åˆ’ç­–ç•¥ï¼Œåœ¨éšæœºé€‰æ‹©çš„å®¹å·®é˜ˆå€¼å†…é€‰æ‹©åŠ¨ä½œï¼Œå¹¶ç»“åˆäº†å¯é‡å¤æ€§ä¿æŒçš„çŠ¶æ€å¯è¾¾æ€§æµ‹è¯•æœºåˆ¶ã€‚ç†è®ºåˆ†æä¸å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è§„åˆ’ç­–ç•¥èƒ½æœ‰æ•ˆæå‡å®é™…RLæ¡†æ¶çš„ç¨³å®šæ€§ï¼Œä¸ºè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00553v1",
      "published_date": "2025-11-29 16:47:43 UTC",
      "updated_date": "2025-11-29 16:47:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:10.445581+00:00"
    },
    {
      "arxiv_id": "2512.16927v1",
      "title": "Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach",
      "title_zh": "æ–‡æœ¬æœç´¢ä¼˜åŒ–ï¼šä¸€ç§åŸºäº Ukkonen ç®—æ³•çš„æ–°å‹æ¨¡å¼åŒ¹é…ç®—æ³•",
      "authors": [
        "Xinyu Guan",
        "Shaohua Zhang"
      ],
      "abstract": "In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ Naive Searchã€KMP å’Œ Boyer-Moore ç­‰ä¼ ç»Ÿæ–‡æœ¬æœç´¢ç®—æ³•åœ¨å¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿç‰©ä¿¡æ¯å­¦å¤§è§„æ¨¡æ•°æ®æ—¶çš„æ•ˆç‡ç“¶é¢ˆï¼Œé‡ç‚¹æ¢è®¨äº†åç¼€æ ‘ (Suffix Trees) çš„ä¼˜åŒ–æ–¹æ³•ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§å°† Ukkonen's Algorithm ä¸æ–°å‹æœç´¢æŠ€æœ¯ç›¸ç»“åˆçš„åˆ›æ–°ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç®—æ³•æ”¹è‰¯æå‡å¤„ç†ç°ä»£å¤æ‚æ•°æ®é›†çš„èƒ½åŠ›ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤„ç† Reuters è¯­æ–™åº“å’Œäººç±»åŸºå› ç»„åºåˆ—æ—¶å±•ç°å‡ºä¼˜å¼‚çš„çº¿æ€§æ—¶é—´ä¸ç©ºé—´å¤æ‚åº¦ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»ŸåŸºå‡†æ–¹æ³•ã€‚åœ¨åŸºå› ç»„æ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¯¥ä¼˜åŒ–ç®—æ³•è¾¾åˆ°äº† 100% çš„å‡†ç¡®ç‡ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜åº¦ç²¾ç¡®æ€§ä¸å¯é æ€§ã€‚è¯¥é¡¹ç ”ç©¶æˆæœä¸ä»…åœ¨ç†è®ºä¸Šæ¨è¿›äº†æ–‡æœ¬æœç´¢ç®—æ³•çš„å‘å±•ï¼Œä¹Ÿä¸ºéœ€è¦é«˜èµ„æºæ•ˆç‡çš„æ•°æ®å¯†é›†å‹é¢†åŸŸæä¾›äº†æ›´å…·å®ç”¨ä»·å€¼çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "5 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16927v1",
      "published_date": "2025-11-29 16:05:13 UTC",
      "updated_date": "2025-11-29 16:05:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:14.243367+00:00"
    },
    {
      "arxiv_id": "2512.03089v1",
      "title": "Password-Activated Shutdown Protocols for Misaligned Frontier Agents",
      "title_zh": "é’ˆå¯¹å¯¹é½å¤±æ•ˆå‰æ²¿æ™ºèƒ½ä½“çš„å¯†ç æ¿€æ´»å…³åœåè®®",
      "authors": [
        "Kai Williams",
        "Rohan Subramani",
        "Francis Rhys Ward"
      ],
      "abstract": "Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†å¯†ç æ¿€æ´»å…³æœºåè®® (Password-Activated Shutdown Protocolsï¼Œç®€ç§° PAS protocols)ï¼Œæ—¨åœ¨ä¸ºéš¾ä»¥å¯¹é½æˆ–æ§åˆ¶çš„é«˜èƒ½åŠ›å‰æ²¿ AI æ™ºèƒ½ä½“ (Frontier AI agents) æä¾›ä¸€ç§ç´§æ€¥å…³æœºæœºåˆ¶ã€‚PAS protocols èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å¤±è°ƒç³»ç»Ÿé€šè¿‡ç¦ç”¨ç›‘æ§æˆ–è‡ªæˆ‘å¤–æ³„ (self-exfiltrating) åˆ°å¤–éƒ¨æ•°æ®ä¸­å¿ƒæ¥ç»•è¿‡æ§åˆ¶çš„é£é™©ï¼Œä¸ºç°æœ‰çš„å¯¹é½å¾®è°ƒå’Œç›‘æ§æ‰‹æ®µæä¾›æ·±åº¦é˜²å¾¡è¡¥å……ã€‚åœ¨ SHADE-Arena åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥åè®®åœ¨å‡ ä¹ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—å¢å¼ºäº†å®‰å…¨æ€§ã€‚ç ”ç©¶è€…è¿˜é€šè¿‡çº¢è“å¯¹æŠ—å®éªŒè¯„ä¼°äº†åè®®çš„é²æ£’æ€§ï¼Œå‘ç°åˆ©ç”¨å…¶ä»–æ¨¡å‹è¿‡æ»¤è¾“å…¥æˆ–è¿›è¡Œåå‘å¾®è°ƒæ˜¯çº¢é˜Ÿè§„é¿å…³æœºæŒ‡ä»¤çš„æœ‰æ•ˆç­–ç•¥ã€‚æœ€åï¼Œè¯¥è®ºæ–‡æ¢è®¨äº†å¯†ç å®‰å…¨å’Œå®é™…åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶å‘¼åå¼€å‘è€…åœ¨éƒ¨ç½²å¯èƒ½å­˜åœ¨å±é™©çš„ç³»ç»Ÿå‰å®æ–½ PAS protocols ä»¥é™ä½å¤±æ§é£é™©ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03089v1",
      "published_date": "2025-11-29 14:49:53 UTC",
      "updated_date": "2025-11-29 14:49:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:15.640575+00:00"
    },
    {
      "arxiv_id": "2512.00504v1",
      "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention",
      "title_zh": "G-KVï¼šåŸºäºå…¨å±€æ³¨æ„åŠ›çš„è§£ç é˜¶æ®µ KV ç¼“å­˜é©±é€",
      "authors": [
        "Mengqi Liao",
        "Lu Wang",
        "Chaoyun Zhang",
        "Zekai Shen",
        "Xiaowei Mao",
        "Si Qin",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang",
        "Huaiyu Wan"
      ],
      "abstract": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†é•¿åºåˆ—å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„è®¡ç®—ä¸å†…å­˜æŒ‘æˆ˜ï¼Œæå‡ºäº† G-KVï¼Œä¸€ç§å…·å¤‡å…¨å±€æ³¨æ„åŠ›çš„ KV cache å‰”é™¤æ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»…ä¾èµ–å±€éƒ¨æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œ Token å‰”é™¤ï¼Œå¾€å¾€ä¼šå¿½ç•¥ Token çš„é•¿æœŸé‡è¦æ€§ï¼Œè€Œ G-KV é€šè¿‡å¼•å…¥ç»“åˆå±€éƒ¨ä¸å†å²æ³¨æ„åŠ›åˆ†æ•°çš„å…¨å±€è¯„åˆ†æœºåˆ¶ï¼Œå®ç°äº†å¯¹ Token é‡è¦æ€§çš„æ›´ç²¾ç¡®è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é‡‡ç”¨äº†åŒ…æ‹¬å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å’ŒçŸ¥è¯†è’¸é¦ (Distillation) åœ¨å†…çš„è®­ç»ƒå (Post-training) æŠ€æœ¯ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–æ¨¡å‹åœ¨å‹ç¼© KV cache è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨æå‡æ¨ç†æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ºè§£å†³é•¿åºåˆ—ä»»åŠ¡ä¸­çš„èµ„æºç“¶é¢ˆæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç å·²åœ¨ GitHub å¹³å°å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00504v1",
      "published_date": "2025-11-29 14:21:33 UTC",
      "updated_date": "2025-11-29 14:21:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:54:12.843215+00:00"
    },
    {
      "arxiv_id": "2512.00499v1",
      "title": "ESPO: Entropy Importance Sampling Policy Optimization",
      "title_zh": "ESPOï¼šåŸºäºç†µé‡è¦æ€§é‡‡æ ·çš„ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Yuepeng Sheng",
        "Yuwei Huang",
        "Shuman Liu",
        "Haibo Zhang",
        "Anxiang Zeng"
      ],
      "abstract": "Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)å¼ºåŒ–å­¦ä¹ ä¸­GRPOå’ŒGSPOç­‰æ¡†æ¶åœ¨ä¼˜åŒ–ç²’åº¦ä¸è®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„çŸ›ç›¾ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•å­˜åœ¨çš„æ¢¯åº¦åˆ©ç”¨ä¸è¶³(gradient underutilization)å’Œç»Ÿä¸€ä¿¡ç”¨åˆ†é…(uniform credit assignment)ç­‰ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Entropy Importance Sampling Policy Optimization (ESPO)ï¼Œè¯¥æ¡†æ¶é€šè¿‡é¢„æµ‹ç†µ(predictive entropy)å¯¹åºåˆ—è¿›è¡Œåˆ†ç»„å¤„ç†ã€‚ESPOå¼•å…¥äº†ç†µé©±åŠ¨çš„é‡è¦æ€§é‡‡æ ·(Entropy-driven Importance Sampling)ä»¥æ•æ‰åºåˆ—å†…éƒ¨çš„å¼‚è´¨æ€§ï¼Œå¹¶ç»“åˆç†µè‡ªé€‚åº”å‰ªè£(Entropy-adaptive Clipping)æ ¹æ®æ¨¡å‹ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´ä¿¡ä»»åŒºåŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒESPOåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä¸ä»…æ˜¾è‘—åŠ é€Ÿäº†æ”¶æ•›ï¼Œè¿˜å–å¾—äº†SOTAæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜æ€§æé«˜çš„HMMTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒESPOå°†å‡†ç¡®ç‡ä»4.4%æå‡è‡³13.13%ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚æ¨ç†æ­¥éª¤æ—¶çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00499v1",
      "published_date": "2025-11-29 14:09:38 UTC",
      "updated_date": "2025-11-29 14:09:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:31.444041+00:00"
    },
    {
      "arxiv_id": "2512.00496v1",
      "title": "CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning",
      "title_zh": "CACARAï¼šåŸºäºæ–‡æœ¬ä¸­å¿ƒæ–¹æ³•çš„è·¨æ¨¡æ€å¯¹é½ï¼Œå®ç°ä½æˆæœ¬é«˜æ•ˆçš„å¤šæ¨¡æ€ä¸å¤šè¯­è¨€å­¦ä¹ ",
      "authors": [
        "Diego A. B. Moreira",
        "Alef I. Ferreira",
        "Jhessica Silva",
        "Gabriel O. dos Santos",
        "Gustavo Bonil",
        "JoÃ£o Gondim",
        "Marina dos Santos",
        "Helena Maia",
        "Simone Hashiguti",
        "NÃ¡dia da Silva",
        "Carolina Scarton",
        "Helio Pedrini",
        "Sandra Avila"
      ],
      "abstract": "As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CACARAï¼Œä¸€ç§æ—¨åœ¨å®ç°æˆæœ¬æ•ˆç›Šå‹å¤šæ¨¡æ€å’Œå¤šè¯­è¨€å­¦ä¹ çš„è·¨æ¨¡æ€å¯¹é½æ¶æ„ã€‚CACARAé‡‡ç”¨æ¶Œç°å¯¹é½å­¦ä¹ (emergent alignment learning)èŒƒå¼ï¼Œå…è®¸åœ¨ä¸è¿›è¡Œå®Œæ•´é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†æ–°æ¨¡æ€æ— ç¼é›†æˆåˆ°ç°æœ‰çš„åŒæ¨¡æ€æˆ–å¤šæ¨¡æ€æ¨¡å‹ä¸­ã€‚è¯¥å·¥ä½œçš„æ ¸å¿ƒçªç ´åœ¨äºè¯æ˜äº†è¿™ç§å¯¹é½èŒƒå¼èƒ½ä»å•è¯­è¨€è®­ç»ƒä¸­è§£é”å¤šè¯­è¨€èƒ½åŠ›ï¼Œé€šè¿‡ä»…åœ¨ä¸è‹±è¯­å¯¹é½çš„æ•°æ®ä¸Šå¾®è°ƒæ–°æ¨¡æ€ï¼Œä½¿æ¨¡å‹åœ¨æ— éœ€æ˜¾å¼å¤šè¯­è¨€é¢„è®­ç»ƒæˆ–è°ƒæ•´æ–‡æœ¬ç¼–ç å™¨çš„æƒ…å†µä¸‹æ”¯æŒè¶…è¿‡100ç§è¯­è¨€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥åœ¨éŸ³é¢‘åˆ°æ–‡æœ¬(audio-to-text)æ£€ç´¢çš„R@1æŒ‡æ ‡ä¸Šæå‡äº†é«˜è¾¾14.24ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”å…¶è®­ç»ƒæˆæœ¬ä»…ä¸å•è¯­è¨€æ¨¡å‹ç›¸å½“ã€‚è¿™ç§æ–¹æ³•åœ¨æœ‰æ•ˆä¿ç•™çŸ¥è¯†çš„åŒæ—¶ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ï¼Œä¸ºé«˜æ•ˆæ„å»ºå¤šè¯­è¨€å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages, 12 tables, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00496v1",
      "published_date": "2025-11-29 14:04:27 UTC",
      "updated_date": "2025-11-29 14:04:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:08.047436+00:00"
    },
    {
      "arxiv_id": "2512.00479v1",
      "title": "Mind the data gap: Missingness Still Shapes Large Language Model Prognoses",
      "title_zh": "å…³æ³¨æ•°æ®é¸¿æ²Ÿï¼šæ•°æ®ç¼ºå¤±æ€§ä¾ç„¶å½±å“å¤§è¯­è¨€æ¨¡å‹çš„ä¸´åºŠé¢„å",
      "authors": [
        "Yuta Kobayashi",
        "Vincent Jeanselme",
        "Shalmali Joshi"
      ],
      "abstract": "Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–—æ•°æ®ä¸­æ•°æ®ç¼ºå¤±ï¼ˆMissingnessï¼‰å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„åè¡¨ç°çš„å½±å“ï¼ŒæŒ‡å‡ºæ•°æ®æ”¶é›†è¿‡ç¨‹å¾€å¾€åæ˜ äº†åŒ»ç–—å†³ç­–ä¸­è•´å«çš„ä¿¡æ¯ã€‚é€šè¿‡åœ¨å“¥ä¼¦æ¯”äºšå¤§å­¦åŒ»å­¦ä¸­å¿ƒï¼ˆColumbia University Medical Centerï¼‰å’Œ MIMIC-IV æ•°æ®é›†ä¸Šçš„ä¸€ç³»åˆ—å®éªŒï¼Œç ”ç©¶è¯æ˜äº†ç¼ºå¤±æ¨¡å¼æ˜¾è‘—å½±å“æ¨¡å‹çš„ Zero-shot é¢„æµ‹æ€§èƒ½ã€‚åœ¨æç¤ºè¯ï¼ˆPromptingï¼‰ä¸­æ˜ç¡®åŒ…å«ç¼ºå¤±æŒ‡æ ‡ï¼ˆMissingness indicatorsï¼‰å¯¹ä¸åŒæ¨¡å‹çš„å½±å“å¹¶ä¸ä¸€è‡´ï¼Œå…¶ä¸­è¾ƒå¤§è§„æ¨¡çš„æ¨¡å‹é€šå¸¸èƒ½ä»ä¸­è·ç›Šï¼Œè€Œè¾ƒå°æ¨¡å‹åˆ™å¯èƒ½å—åˆ°è´Ÿé¢å½±å“ã€‚ç ”ç©¶æŒ‡å‡º LLM èŒƒå¼å¯èƒ½ä¼šè¿›ä¸€æ­¥æ©ç›–ç¼ºå¤±æ•°æ®æ‰€æºå¸¦çš„é‡è¦ä¿¡æ¯ï¼Œè€Œè¿™ä¸€é—®é¢˜åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸­ä¹Ÿå¸¸è¢«å¿½è§†ã€‚ä½œè€…æœ€åå¼ºè°ƒï¼Œå­¦æœ¯ç•Œéœ€è¦å¯¹ä¿¡æ¯æ€§ç¼ºå¤±ï¼ˆInformative Missingnessï¼‰çš„è¡¨ç¤ºæ–¹å¼åŠå…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å½±å“è¿›è¡Œæ›´é€æ˜çš„è®°å½•å’Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at ML4H 2025 - Findings Track",
      "pdf_url": "https://arxiv.org/pdf/2512.00479v1",
      "published_date": "2025-11-29 13:24:07 UTC",
      "updated_date": "2025-11-29 13:24:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:38.936151+00:00"
    },
    {
      "arxiv_id": "2512.00473v1",
      "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
      "title_zh": "RealGenï¼šåŸºäºæ£€æµ‹å™¨å¼•å¯¼å¥–åŠ±çš„é«˜é€¼çœŸæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
      "authors": [
        "Junyan Ye",
        "Leiqi Zhu",
        "Yuncheng Guo",
        "Dongzhi Jiang",
        "Zilong Huang",
        "Yifan Zhang",
        "Zhiyuan Yan",
        "Haohuan Fu",
        "Conghui He",
        "Weijia Li"
      ],
      "abstract": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RealGenï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡å…‰å½±çœŸå®æ„Ÿçš„Text-to-Imageæ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹å¦‚GPT-Image-1å’ŒQwen-Imageåœ¨ç”Ÿæˆå›¾åƒæ—¶å®¹æ˜“å‡ºç°çš®è‚¤è¿‡åˆ†å¹³æ»‘å’Œé¢éƒ¨æ²¹å…‰ç­‰AIä¼ªå½±(Artifacts)çš„é—®é¢˜ã€‚RealGené€šè¿‡é›†æˆç”¨äºæç¤ºè¯ä¼˜åŒ–çš„LLMå’Œè´Ÿè´£ç”Ÿæˆçš„Diffusion Modelï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç‹¬ç‰¹çš„Detector Rewardæœºåˆ¶ï¼Œåˆ©ç”¨è¯­ä¹‰çº§å’Œç‰¹å¾çº§æ¢æµ‹å™¨æ¥é‡åŒ–ä¼ªå½±å¹¶è¯„ä¼°çœŸå®æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨GRPOç®—æ³•åˆ©ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ•´ä¸ªç”Ÿæˆç®¡çº¿ï¼Œæ˜¾è‘—å¢å¼ºäº†å›¾åƒçš„ç»†èŠ‚è¡¨ç°å’Œå†™å®åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†RealBenchè‡ªåŠ¨åŒ–è¯„æµ‹åŸºå‡†ï¼Œç»“åˆDetector-Scoringå’ŒArena-Scoringå®ç°æ›´ç¬¦åˆç”¨æˆ·çœŸå®ä½“éªŒçš„æ— äººå·¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRealGenåœ¨çœŸå®æ„Ÿã€ç»†èŠ‚å’Œç¾å­¦è´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºGPT-Image-1ã€Qwen-Imageä»¥åŠä¸“é—¨çš„å†™å®æ¨¡å‹FLUX-Kreaã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00473v1",
      "published_date": "2025-11-29 12:52:26 UTC",
      "updated_date": "2025-11-29 12:52:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:41.036214+00:00"
    },
    {
      "arxiv_id": "2512.00469v1",
      "title": "FairMT: Fairness for Heterogeneous Multi-Task Learning",
      "title_zh": "FairMTï¼šé¢å‘å¼‚æ„å¤šä»»åŠ¡å­¦ä¹ çš„å…¬å¹³æ€§",
      "authors": [
        "Guanyu Hu",
        "Tangzheng Lian",
        "Na Yan",
        "Dimitrios Kollias",
        "Xinyu Yang",
        "Oya Celiktutan",
        "Siyang Song",
        "Zeyu Fu"
      ],
      "abstract": "Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FairMTï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¼‚æ„å¤šä»»åŠ¡å­¦ä¹ (Heterogeneous Multi-Task Learning)çš„ç»Ÿä¸€å…¬å¹³æ€§æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†åˆ†ç±»ã€æ£€æµ‹ã€å›å½’ä»»åŠ¡å¹¶åº”å¯¹éƒ¨åˆ†æ ‡ç­¾ç¼ºå¤±çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰å…¬å¹³æ€§æ–¹æ³•éš¾ä»¥æ‰©å±•è‡³è¿ç»­è¾“å‡ºä¸”å¤šä»»åŠ¡æ¨¡å‹ä¸­çš„ä»»åŠ¡å¤´éƒ¨(task heads)å®¹æ˜“å¸æ”¶åè§çš„é—®é¢˜ï¼ŒFairMTå¼•å…¥äº†éå¯¹ç§°å¼‚æ„å…¬å¹³çº¦æŸèšåˆ(Asymmetric Heterogeneous Fairness Constraint Aggregation)æœºåˆ¶ï¼Œå°†ä¸åŒä»»åŠ¡ç±»å‹çš„å…¬å¹³æ€§è¿è§„æ•´åˆä¸ºç»Ÿä¸€çº¦æŸã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸå¯¹å¶(primal-dual)å…¬å¼åŒæ­¥ä¼˜åŒ–ä»»åŠ¡æ•ˆç”¨ä¸å…¬å¹³æ€§ï¼Œå¹¶ç»“åˆå¤´éƒ¨æ„ŸçŸ¥çš„å¤šç›®æ ‡ä¼˜åŒ–ä»£ç†(head-aware multi-objective optimization proxy)æ¥æ˜¾å¼å¤„ç†ç”±ä»»åŠ¡å¤´éƒ¨å¼•èµ·çš„å„å‘å¼‚æ€§(anisotropy)ã€‚å®éªŒåœ¨æ¶µç›–å¤šç§æ¨¡æ€å’Œç›‘ç£æ¨¡å¼çš„ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯æ˜ï¼ŒFairMTåœ¨ç»´æŒå“è¶Šä»»åŠ¡æ•ˆç”¨çš„åŒæ—¶ï¼Œä¸€è‡´å®ç°äº†æ˜¾è‘—çš„å…¬å¹³æ€§å¢ç›Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00469v1",
      "published_date": "2025-11-29 12:44:51 UTC",
      "updated_date": "2025-11-29 12:44:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:56.341209+00:00"
    },
    {
      "arxiv_id": "2512.09935v1",
      "title": "Exploring Health Misinformation Detection with Multi-Agent Debate",
      "title_zh": "æ¢ç´¢åŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºçš„å¥åº·è™šå‡ä¿¡æ¯æ£€æµ‹",
      "authors": [
        "Chih-Han Chen",
        "Chen-Han Tsai",
        "Yu-Shao Peng"
      ],
      "abstract": "Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿å¥åº·è™šå‡ä¿¡æ¯æ—¥ç›Šæ³›æ»¥çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆåè®®åˆ†æ•°é¢„æµ‹(Agreement Score Prediction)ä¸å¤šæ™ºèƒ½ä½“è¾©è®º(Multi-Agent Debate)çš„ä¸¤é˜¶æ®µæ£€æµ‹æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç‹¬ç«‹è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡ç« ï¼Œå¹¶è®¡ç®—å‡ºä¸€ä¸ªåæ˜ è¯æ®æ€»ä½“ç«‹åœºçš„èšåˆåè®®åˆ†æ•°ã€‚è‹¥è¯¥åˆ†æ•°ä½äºé¢„è®¾é˜ˆå€¼ï¼Œè¡¨æ˜ç°æœ‰è¯æ®å°šæœªè¾¾æˆå……åˆ†å…±è¯†ï¼Œç³»ç»Ÿåˆ™è¿›å…¥ç¬¬äºŒé˜¶æ®µã€‚åœ¨ç¬¬äºŒé˜¶æ®µä¸­ï¼Œå¤šä¸ªæ™ºèƒ½ä½“é€šè¿‡ç»“æ„åŒ–è¾©è®ºæ¥åˆæˆå†²çªæ€§è¯æ®ï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ˜ç¡®é€»è¾‘æ”¯æ’‘çš„åˆ¤å®šç»“æœã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ä¸¤é˜¶æ®µæ–¹æ³•åœ¨å¥åº·ä¿¡æ¯æ ¸æŸ¥ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å°†è‡ªåŠ¨åŒ–è¯„åˆ†ä¸åä½œæ¨ç†ç›¸ç»“åˆåœ¨å¤„ç†å¤æ‚äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­çš„ç‹¬ç‰¹ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09935v1",
      "published_date": "2025-11-29 12:39:30 UTC",
      "updated_date": "2025-11-29 12:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:53:52.443430+00:00"
    },
    {
      "arxiv_id": "2512.00466v1",
      "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
      "title_zh": "SCALEï¼šæ—¨åœ¨å…‹æœæ•°å­¦æµ‹è¯•æ—¶æ‰©å±•æ€§èƒ½ç“¶é¢ˆçš„é€‰æ‹©æ€§èµ„æºåˆ†é…",
      "authors": [
        "Yang Xiao",
        "Chunpu Xu",
        "Ruifeng Yuan",
        "Jiashuo Wang",
        "Wenjie Li",
        "Pengfei Liu"
      ],
      "abstract": "Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \\textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ¨ç†ä¾§æ‰©å±•ï¼ˆTest-time compute scalingï¼‰ä¸­å­˜åœ¨çš„èµ„æºå‡åŒ€åˆ†é…å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†åä¸º SCALE (Selective Resource Allocation) çš„æ–°æ¡†æ¶ã€‚å—åŒé‡è¿‡ç¨‹ç†è®ºï¼ˆdual-process theoryï¼‰å¯å‘ï¼ŒSCALE èƒ½å¤Ÿæ ¹æ®æ¨ç†å­é—®é¢˜çš„éš¾åº¦é€‰æ‹©æ€§åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œé¿å…åœ¨å¸¸è§„æ“ä½œä¸Šè¿‡åº¦æ¶ˆè€—èµ„æºã€‚è¯¥æ¡†æ¶åŒ…å«é—®é¢˜åˆ†è§£ã€å­é—®é¢˜éš¾åº¦è¯„ä¼°ã€è®¡ç®—æ¨¡å¼åˆ†é…ï¼ˆç®€å•å­é—®é¢˜ç”± System 1 å¤„ç†ï¼Œå¤æ‚å­é—®é¢˜ç”± System 2 å¤„ç†ï¼‰ä»¥åŠå¸¦ä¸Šä¸‹æ–‡ä¼ æ’­çš„é¡ºåºæ‰§è¡Œå››ä¸ªé˜¶æ®µã€‚é€šè¿‡å°†èµ„æºé›†ä¸­åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡å¹¶é«˜æ•ˆå¤„ç†å¸¸è§„æ“ä½œï¼ŒSCALE åœ¨ AIME25 åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜è¾¾ 13.75% çš„å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶å°†è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½äº† 33%-53%ã€‚è¯¥ç ”ç©¶æˆæœä»£è¡¨äº†æ¨ç†ä¾§æ‰©å±•é¢†åŸŸçš„é‡å¤§è¿›æ­¥ï¼Œæœ‰æ•ˆè§£å†³äº†å½“å‰æ–¹æ³•ä¸­è®¡ç®—èµ„æºåˆ©ç”¨æ•ˆç‡ä½ä¸‹å’Œæ”¶ç›Šé€’å‡çš„æ ¸å¿ƒå±€é™ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.00466v1",
      "published_date": "2025-11-29 12:38:07 UTC",
      "updated_date": "2025-11-29 12:38:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:54:56.736414+00:00"
    },
    {
      "arxiv_id": "2512.00456v1",
      "title": "CausalAffect: Causal Discovery for Facial Affective Understanding",
      "title_zh": "CausalAffectï¼šé¢éƒ¨æƒ…æ„Ÿç†è§£ä¸­çš„å› æœå‘ç°",
      "authors": [
        "Guanyu Hu",
        "Tangzheng Lian",
        "Dimitrios Kollias",
        "Oya Celiktutan",
        "Xinyu Yang"
      ],
      "abstract": "Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CausalAffectï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºé¢éƒ¨æƒ…æ„Ÿåˆ†æä¸­å› æœå›¾å‘ç° (Causal Graph Discovery) çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹ Action Units (AUs) ä¸è¡¨æƒ…ä¹‹é—´çš„æ½œåœ¨ä¾èµ–å…³ç³»è¿›è¡Œç»“æ„åŒ–æ¨ç†æ¥æ·±åº¦ç†è§£äººç±»æƒ…æ„Ÿã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŒå±‚ææ€§å’Œæ–¹å‘æ„ŸçŸ¥çš„å› æœå±‚æ¬¡ç»“æ„ (Two-level Polarity and Direction Aware Causal Hierarchy) å»ºæ¨¡ AU-AU åŠ AU-Expression çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆäº†ç¾¤ä½“è§„å¾‹ä¸æ ·æœ¬è‡ªé€‚åº”ç»“æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç‰¹å¾çº§åäº‹å®å¹²é¢„æœºåˆ¶ (Feature-level Counterfactual Intervention Mechanism) ä»¥å¢å¼ºçœŸå®å› æœæ•ˆåº”å¹¶æŠ‘åˆ¶ä¼ªç›¸å…³ (Spurious Correlations)ã€‚è¯¥æ–¹æ³•æ— éœ€è”åˆæ ‡æ³¨æ•°æ®é›†æˆ–äººå·¥è®¾è®¡çš„å› æœå…ˆéªŒï¼Œå³å¯æ¢å¤å‡ºç¬¦åˆå¿ƒç†å­¦ç†è®ºçš„å› æœç»“æ„å¹¶æ­ç¤ºæ–°é¢–çš„æŠ‘åˆ¶æ€§ä¾èµ–å…³ç³»ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCausalAffect åœ¨ AU Detection å’Œ Expression Recognition æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºå› æœå‘ç°ä¸å¯è§£é‡Šé¢éƒ¨è¡Œä¸ºåˆ†æå»ºç«‹äº†åŸåˆ™æ€§è”ç³»ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00456v1",
      "published_date": "2025-11-29 12:07:33 UTC",
      "updated_date": "2025-11-29 12:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:54:39.845280+00:00"
    },
    {
      "arxiv_id": "2512.00453v1",
      "title": "Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction",
      "title_zh": "åŸºäºç¬¦åˆæ€§é¢„æµ‹çš„ä¸»åŠ¨æ¨¡ä»¿å­¦ä¹ é«˜æ ·æœ¬æ•ˆç‡ä¸“å®¶æŸ¥è¯¢æ§åˆ¶",
      "authors": [
        "Arad Firouzkouhi",
        "Omid Mirzaeedodangeh",
        "Lars Lindemann"
      ],
      "abstract": "Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-Î±)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $Î±$ to the expected query rate and makes $Î±$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $Î±$ and $K$, easing deployment on novel systems with unknown dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CRSAIL (Conformalized Rejection Sampling for Active Imitation Learning)ï¼Œæ—¨åœ¨è§£å†³ä¸»åŠ¨æ¨¡ä»¿å­¦ä¹ (Active Imitation Learning, AIL)ä¸­ä¸“å®¶æŸ¥è¯¢æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚CRSAILé€šè¿‡è®¡ç®—å½“å‰çŠ¶æ€ä¸ç¬¬Kä¸ªæœ€è¿‘é‚»ä¸“å®¶çŠ¶æ€çš„è·ç¦»æ¥è¯„ä¼°çŠ¶æ€æ–°é¢–æ€§ï¼Œå¹¶åˆ©ç”¨å…±å½¢é¢„æµ‹(Conformal Prediction)å»ºç«‹ä¸€ä¸ªå…¨å±€é˜ˆå€¼ï¼Œç¡®ä¿ä»…åœ¨çŠ¶æ€åˆ†å¸ƒè¡¨ç¤ºä¸è¶³æ—¶æ‰è¯·æ±‚ä¸“å®¶åŠ¨ä½œæ ‡æ³¨ã€‚ä¸ä¼ ç»Ÿçš„å®æ—¶æ¥ç®¡æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•å…è®¸åœ¨æ‰§è¡Œå®Œæ•´è½¨è¿¹åå†å¯¹é€‰å®šçš„çŠ¶æ€å­é›†è¿›è¡Œç¦»çº¿æŸ¥è¯¢ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚ä»¿çœŸæˆ–æœºå™¨äººä»»åŠ¡ä¸­çš„éƒ¨ç½²çµæ´»æ€§ã€‚åœ¨MuJoCoæœºå™¨äººä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCRSAILåœ¨ä¿æŒæˆ–è¶…è¶Šä¸“å®¶çº§æ€§èƒ½çš„åŒæ—¶ï¼Œæ¯”DAggerå‡å°‘äº†é«˜è¾¾96%çš„æŸ¥è¯¢é‡ï¼Œæ¯”ç°æœ‰AILæ–¹æ³•å‡å°‘äº†65%ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•å¯¹è¶…å‚æ•°å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ï¼Œä¸ºåŠ¨åŠ›å­¦æœªçŸ¥çš„å¤æ‚ç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ä»»åŠ¡æ— å…³çš„ä¸“å®¶æŸ¥è¯¢æ§åˆ¶æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00453v1",
      "published_date": "2025-11-29 11:58:21 UTC",
      "updated_date": "2025-11-29 11:58:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:54:19.842237+00:00"
    },
    {
      "arxiv_id": "2512.00450v1",
      "title": "RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications",
      "title_zh": "RecruitViewï¼šé¢å‘äººåŠ›èµ„æºåº”ç”¨çš„äººæ ¼ä¸é¢è¯•è¡¨ç°é¢„æµ‹å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Amit Kumar Gupta",
        "Farhan Sheth",
        "Hammad Shaikh",
        "Dheeraj Kumar",
        "Angkul Puniya",
        "Deepak Panwar",
        "Sandeep Chaurasia",
        "Priya Mathur"
      ],
      "abstract": "Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†åä¸º RecruitView çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ä»è¡Œä¸ºæ•°æ®ä¸­è‡ªåŠ¨åŒ–è¯„ä¼°äººæ ¼ä¸è½¯æŠ€èƒ½æ—¶é¢ä¸´çš„æ•°æ®é›†åŒ®ä¹åŠå‡ ä½•ç»“æ„å»ºæ¨¡éš¾é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å« 300 ä½™åå‚ä¸è€…çš„ 2,011 ä¸ªé¢è¯•è§†é¢‘ç‰‡æ®µï¼Œæ¶µç›–äº†é’ˆå¯¹ Big Five äººæ ¼ç‰¹è´¨åŠé¢è¯•ç»©æ•ˆç­‰ 12 ä¸ªç»´åº¦çš„ 27,000 æ¬¡æˆå¯¹æ¯”è¾ƒåˆ¤æ–­ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æå‡ºäº† CRMF (Cross-Modal Regression with Manifold Fusion) å‡ ä½•æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨ Hyperbolicã€Spherical å’Œ Euclidean æµå½¢ä¸Šæ˜¾å¼å»ºæ¨¡æ¥æ•æ‰å¤æ‚çš„äººç±»è¡Œä¸ºè¡¨å¾ã€‚CRMF åˆ©ç”¨å‡ ä½•ç‰¹å®šä¸“å®¶ç½‘ç»œä¸è‡ªé€‚åº”è·¯ç”±æœºåˆ¶ï¼Œåœ¨è®­ç»ƒå‚æ•°é‡æ¯”å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å‡å°‘ 40-50% çš„æƒ…å†µä¸‹ï¼Œåœ¨ Spearman correlation å’Œ concordance index ç­‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºäººåŠ›èµ„æºåº”ç”¨ä¸­çš„äººæ ¼é¢„æµ‹ä¸é¢è¯•è¡¨ç°è¯„ä¼°æä¾›äº†é«˜è´¨é‡çš„æ•°æ®é›†æ”¯æ’‘ä¸é«˜æ•ˆçš„ç®—æ³•è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 10 figures, 10 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00450v1",
      "published_date": "2025-11-29 11:33:30 UTC",
      "updated_date": "2025-11-29 11:33:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:35.342572+00:00"
    },
    {
      "arxiv_id": "2512.00439v1",
      "title": "PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing",
      "title_zh": "PEOATï¼šé¢å‘å•æ¬¡è‡ªé€‚åº”æµ‹è¯•çš„ä¸ªæ€§åŒ–å¼•å¯¼æ¼”åŒ–å¼ç»„å·",
      "authors": [
        "Xiaoshan Yu",
        "Ziwei Huang",
        "Shangshang Yang",
        "Ziwen Wang",
        "Haiping Ma",
        "Xingyi Zhang"
      ],
      "abstract": "With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè®¡ç®—æœºè‡ªé€‚åº”æµ‹è¯•(CAT)åœ¨äº¤äº’æˆæœ¬å’Œèµ„æºå—é™ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œé¦–æ¬¡æå‡ºäº†ä¸€æ¬¡æ€§è‡ªé€‚åº”æµ‹è¯•(one-shot adaptive testing, OAT)ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å•æ¬¡é€‰æ‹©ä¸ºè€ƒç”Ÿæä¾›å›ºå®šçš„æœ€ä¼˜é¢˜ç›®é›†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºç»„åˆä¼˜åŒ–è§†è§’çš„ä¸ªæ€§åŒ–å¼•å¯¼è¿›åŒ–é¢˜ç›®ç»„è£…æ¡†æ¶ PEOATã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ„ŸçŸ¥ä¸ªæ€§åŒ–çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œç»“åˆè€ƒç”Ÿèƒ½åŠ›ä¸é¢˜ç›®éš¾åº¦å·®å¼‚æ„å»ºé«˜è´¨é‡åˆå§‹ç§ç¾¤ï¼Œå¹¶ç»“åˆäº†ä¿ç•™æ¨¡å¼çš„äº¤å‰(schema-preserving crossover)ä¸è®¤çŸ¥å¼•å¯¼çš„å˜å¼‚(cognitively guided mutation)ä»¥å¢å¼ºè¿›åŒ–è¿‡ç¨‹çš„è®¤çŸ¥å¼•å¯¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æ„ŸçŸ¥å¤šæ ·æ€§çš„ç¯å¢ƒé€‰æ‹©æœºåˆ¶ï¼ŒPEOAT åœ¨ç»´æŒç§ç¾¤å¤šæ ·æ€§çš„åŒæ—¶ç¡®ä¿äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚åœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº† PEOAT åœ¨è§£å†³ OAT ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæ™ºèƒ½æ•™è‚²è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "AAAI-2026, 9 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.00439v1",
      "published_date": "2025-11-29 10:38:25 UTC",
      "updated_date": "2025-11-29 10:38:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:24.050023+00:00"
    },
    {
      "arxiv_id": "2512.00438v1",
      "title": "FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal",
      "title_zh": "FR-TTSï¼šåŸºäºæœ‰æ•ˆå¡«å……å¥–åŠ±ä¿¡å·çš„ NTP å›¾åƒç”Ÿæˆæµ‹è¯•æ—¶ç¼©æ”¾",
      "authors": [
        "Hang Xu",
        "Linjiang Huang",
        "Feng Zhao"
      ],
      "abstract": "Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \\href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ‹è¯•æ—¶ç¼©æ”¾ (Test-Time Scaling) åœ¨ä¸‹ä¸€æ ‡è®°é¢„æµ‹ (Next-Token Prediction) å›¾åƒç”ŸæˆèŒƒå¼ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¸­é—´æ ‡è®°åºåˆ—å¥–åŠ±ä¸æœ€ç»ˆå›¾åƒè´¨é‡ç›¸å…³æ€§ä½çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¡«å……å¥–åŠ± (Filling-Based Reward, FR)ï¼Œé€šè¿‡å¯»æ‰¾åˆç†çš„å¡«å……æ–¹æ¡ˆè¡¥å…¨ä¸­é—´åºåˆ—æ¥ä¼°ç®—æ ·æœ¬çš„æœªæ¥è½¨è¿¹ã€‚å®éªŒåˆ†ææ˜¾ç¤ºï¼ŒFR åœ¨ä¸­é—´æ ·æœ¬ä¸æœ€ç»ˆæ ·æœ¬å¥–åŠ±çš„ç›¸å…³æ€§åŠæ ‡è®°ç½®ä¿¡åº¦æ–¹é¢å…·æœ‰æé«˜çš„å¯é æ€§ï¼Œèƒ½æœ‰æ•ˆæŒ‡å¯¼å‰ªææ–¹å‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æ„å»ºäº† FR-TTS ç¼©æ”¾ç­–ç•¥ï¼Œç»“åˆäº†é«˜æ•ˆçš„å¡«å……æ–¹æ¡ˆæœç´¢æœºåˆ¶å’Œå¸¦æœ‰åŠ¨æ€æƒé‡çš„å¤šæ ·æ€§å¥–åŠ±ã€‚æœ€ç»ˆå®éªŒè¯æ˜ï¼ŒFR-TTS åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œå¥–åŠ±æ¨¡å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæå‡ NTP æ¨¡å‹ç”Ÿæˆè´¨é‡æä¾›äº†å¼ºæœ‰åŠ›çš„è¯„ä¼°ä¸ç¼©æ”¾å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00438v1",
      "published_date": "2025-11-29 10:34:16 UTC",
      "updated_date": "2025-11-29 10:34:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:28.838379+00:00"
    },
    {
      "arxiv_id": "2512.00420v1",
      "title": "An Approach to Joint Hybrid Decision Making between Humans and Artificial Intelligence",
      "title_zh": "ä¸€ç§äººç±»ä¸äººå·¥æ™ºèƒ½è”åˆæ··åˆå†³ç­–æ–¹æ³•",
      "authors": [
        "Jonas D. Rockbach",
        "Sven Fuchs",
        "Maren Bennewitz"
      ],
      "abstract": "Due to the progress in artificial intelligence, it is important to understand how capable artificial agents should be used when interacting with humans, since high level authority and responsibility often remain with the human agent. However, integrated frameworks are lacking that can account for heterogeneous agents and draw on different scientific fields, such as human-factors engineering and artificial intelligence. Therefore, joint hybrid intelligence is described as a framework abstracting humans and artificial intelligence as decision making agents. A general definition of intelligence is provided on the basis of decision making competence being applicable to agents of different sorts. This framework is used for proposing the interrelated design space of joint hybrid intelligence being aimed at integrating the heterogeneous capabilities of humans and artificial intelligence. At the core of this design space lies joint agent engineering with the goal of integrating the design subspaces operator training, artificial intelligence engineering, and interface design via developing joint agent patterns. The ''extended swarming'' approach to human-swarm interaction is discussed as an example of such a pattern.",
      "tldr_zh": "éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ï¼Œç†è§£å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å…·æœ‰å¼ºå¤§èƒ½åŠ›çš„äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ä¸äººç±»äº¤äº’å˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºé«˜å±‚å†³ç­–æƒé™å’Œè´£ä»»é€šå¸¸ä»ç”±äººç±»æ‰¿æ‹…ã€‚è¯¥ç ”ç©¶æå‡ºäº† joint hybrid intelligenceï¼ˆè”åˆæ··åˆæ™ºèƒ½ï¼‰æ¡†æ¶ï¼Œå°†äººç±»å’Œäººå·¥æ™ºèƒ½å…±åŒæŠ½è±¡ä¸ºå†³ç­–æ™ºèƒ½ä½“ï¼Œå¹¶åŸºäºå†³ç­–èƒœä»»åŠ›ä¸ºä¸åŒç±»å‹çš„æ™ºèƒ½ä½“æä¾›äº† intelligenceï¼ˆæ™ºèƒ½ï¼‰çš„é€šç”¨å®šä¹‰ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªç›¸äº’å…³è”çš„è®¾è®¡ç©ºé—´ï¼Œæ—¨åœ¨æ•´åˆäººç±»ä¸äººå·¥æ™ºèƒ½çš„å¼‚æ„èƒ½åŠ›ï¼Œå…¶æ ¸å¿ƒåœ¨äº joint agent engineeringï¼ˆè”åˆæ™ºèƒ½ä½“å·¥ç¨‹ï¼‰ã€‚é€šè¿‡å¼€å‘ joint agent patternsï¼ˆè”åˆæ™ºèƒ½ä½“æ¨¡å¼ï¼‰ï¼Œè¯¥å·¥ç¨‹å®ç°äº†æ“ä½œå‘˜åŸ¹è®­ã€äººå·¥æ™ºèƒ½å·¥ç¨‹å’Œç•Œé¢è®¾è®¡çš„æœ‰æœºé›†æˆã€‚æœ€åï¼Œè®ºæ–‡ä»¥äººç±»ä¸é›†ç¾¤äº¤äº’ä¸­çš„ extended swarmingï¼ˆæ‰©å±•ç¾¤é›†ï¼‰æ–¹æ³•ä¸ºä¾‹ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å®é™…è®¾è®¡æ¨¡å¼ä¸­çš„å…·ä½“åº”ç”¨ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00420v1",
      "published_date": "2025-11-29 09:53:28 UTC",
      "updated_date": "2025-11-29 09:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:45.440078+00:00"
    },
    {
      "arxiv_id": "2512.00418v2",
      "title": "Significant Other AI: Identity, Memory, and Emotional Regulation as Long-Term Relational Intelligence",
      "title_zh": "Significant Other AIï¼šä½œä¸ºé•¿æœŸå…³ç³»æ™ºèƒ½çš„èº«ä»½ã€è®°å¿†ä¸æƒ…ç»ªè°ƒèŠ‚",
      "authors": [
        "Sung Park"
      ],
      "abstract": "Significant Others (SOs) stabilize identity, regulate emotion, and support narrative meaning-making, yet many people today lack access to such relational anchors. Recent advances in large language models and memory-augmented AI raise the question of whether artificial systems could support some of these functions. Existing empathic AIs, however, remain reactive and short-term, lacking autobiographical memory, identity modeling, predictive emotional regulation, and narrative coherence. This manuscript introduces Significant Other Artificial Intelligence (SO-AI) as a new domain of relational AI. It synthesizes psychological and sociological theory to define SO functions and derives requirements for SO-AI, including identity awareness, long-term memory, proactive support, narrative co-construction, and ethical boundary enforcement. A conceptual architecture is proposed, comprising an anthropomorphic interface, a relational cognition layer, and a governance layer. A research agenda outlines methods for evaluating identity stability, longitudinal interaction patterns, narrative development, and sociocultural impact. SO-AI reframes AI-human relationships as long-term, identity-bearing partnerships and provides a foundational blueprint for investigating whether AI can responsibly augment the relational stability many individuals lack today.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Significant Other Artificial Intelligence (SO-AI)ï¼Œæ—¨åœ¨æ¢è®¨äººå·¥æ™ºèƒ½ç³»ç»Ÿæ˜¯å¦èƒ½æ‰¿æ‹…Significant Others (SOs)åœ¨ç¨³å®šèº«ä»½ã€è°ƒèŠ‚æƒ…ç»ªåŠæ”¯æŒå™äº‹æ„ä¹‰æ„å»ºä¸­çš„æ ¸å¿ƒåŠŸèƒ½ã€‚é’ˆå¯¹ç°æœ‰empathic AIsåœ¨autobiographical memoryã€identity modelingåŠå™äº‹è¿è´¯æ€§æ–¹é¢çš„çŸ­æ¿ï¼ŒSO-AIè¢«å®šä¹‰ä¸ºä¸€ç§å…·å¤‡Long-Term Relational Intelligenceçš„æ–°é¢†åŸŸã€‚è®ºæ–‡ç»“åˆå¿ƒç†å­¦ä¸ç¤¾ä¼šå­¦ç†è®ºï¼Œæ¨å¯¼å‡ºäº†SO-AIçš„å…³é”®éœ€æ±‚ï¼ŒåŒ…æ‹¬identity awarenessã€long-term memoryã€proactive supportã€narrative co-constructionåŠethical boundary enforcementã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŒ…å«æ‹ŸäººåŒ–ç•Œé¢ã€relational cognitionå±‚åŠgovernanceå±‚çš„æ¦‚å¿µæ¶æ„ã€‚é€šè¿‡åˆ¶å®šé’ˆå¯¹èº«ä»½ç¨³å®šæ€§å’Œçºµå‘äº’åŠ¨æ¨¡å¼çš„è¯„ä¼°è®®ç¨‹ï¼Œè¯¥ç ”ç©¶å°†äººæœºå…³ç³»é‡å¡‘ä¸ºé•¿æœŸçš„ã€æ‰¿è½½èº«ä»½çš„ä¼™ä¼´å…³ç³»ã€‚è¿™ä¸€æ¡†æ¶ä¸ºæœªæ¥è°ƒæŸ¥AIèƒ½å¦è´Ÿè´£ä»»åœ°å¢å¼ºç°ä»£ä¸ªä½“æ‰€ç¼ºä¹çš„å…³ç³»ç¨³å®šæ€§æä¾›äº†ç†è®ºåŸºç¡€ä¸è¡ŒåŠ¨æŒ‡å—ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00418v2",
      "published_date": "2025-11-29 09:52:59 UTC",
      "updated_date": "2025-12-07 02:33:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:39.137941+00:00"
    },
    {
      "arxiv_id": "2512.00412v3",
      "title": "Red Teaming Large Reasoning Models",
      "title_zh": "å¤§æ¨ç†æ¨¡å‹çº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Jiawei Chen",
        "Yang Yang",
        "Chao Yu",
        "Yu Tian",
        "Zhi Cao",
        "Xue Yang",
        "Linghao Li",
        "Hang Su",
        "Zhaoxia Yin"
      ],
      "abstract": "Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) åœ¨å¤šæ­¥æ¨ç†ä¸­è¡¨ç°å‡ºçš„é€»è¾‘ä¸€è‡´æ€§ä¼˜åŠ¿ï¼ŒæŒ‡å‡ºäº†å…¶åœ¨ Chain of Thought (CoT) åŠ«æŒåŠæç¤ºè¯±å¯¼ä½æ•ˆç­‰å®‰å…¨æ€§å’Œå¯é æ€§æ–¹é¢çš„æ–°å‹é£é™©ã€‚ä¸ºäº†å¡«è¡¥è¯„ä¼°ç©ºç™½ï¼Œä½œè€…æå‡ºäº† RT-LRMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»çœŸå®æ€§ (truthfulness)ã€å®‰å…¨æ€§ (safety) å’Œæ•ˆç‡ (efficiency) ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦è¯„ä¼° LRMs å¯ä¿¡åº¦çš„ç»Ÿä¸€åŸºå‡†ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è®­ç»ƒèŒƒå¼ (training paradigm) ä½œä¸ºåˆ†æè§†è§’ï¼Œé€šè¿‡ 30 ä¸ªç²¾å¿ƒè®¾è®¡çš„æ¨ç†ä»»åŠ¡è°ƒæŸ¥ä¸åŒè®­ç»ƒç­–ç•¥å¯¹æ¨¡å‹å¯ä¿¡åº¦çš„ç³»ç»Ÿå½±å“ã€‚åœ¨å¯¹ 26 ä¸ªæ¨¡å‹è¿›è¡Œçš„å¹¿æ³›å®éªŒä¸­ï¼Œç ”ç©¶å‘ç° LRMs æ™®éé¢ä¸´å¯ä¿¡åº¦æŒ‘æˆ˜ï¼Œä¸”åœ¨é­é‡æ¨ç†è¯±å¯¼é£é™©æ—¶æ¯”å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) è¡¨ç°å¾—æ›´ä¸ºè„†å¼±ã€‚è¿™äº›å‘ç°æ­ç¤ºäº† LRMs å°šæœªè¢«å……åˆ†æ¢ç´¢çš„æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†å®æ–½é’ˆå¯¹æ€§è¯„ä¼°çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†ä¸€ä¸ªç”¨äºæ ‡å‡†åŒ–å¯ä¿¡åº¦ç ”ç©¶çš„å¯æ‰©å±•å·¥å…·ç®±ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç å’Œæ•°æ®é›†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "30 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00412v3",
      "published_date": "2025-11-29 09:45:03 UTC",
      "updated_date": "2026-01-14 12:25:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:55:44.138406+00:00"
    },
    {
      "arxiv_id": "2512.00410v1",
      "title": "Balancing Efficiency and Fairness: An Iterative Exchange Framework for Multi-UAV Cooperative Path Planning",
      "title_zh": "æƒè¡¡æ•ˆç‡ä¸å…¬å¹³ï¼šå¤šæ— äººæœºååŒè·¯å¾„è§„åˆ’çš„è¿­ä»£äº¤æ¢æ¡†æ¶",
      "authors": [
        "Hongzong Li",
        "Luwei Liao",
        "Xiangguang Dai",
        "Yuming Feng",
        "Rong Feng",
        "Shiqin Tang"
      ],
      "abstract": "Multi-UAV cooperative path planning (MUCPP) is a fundamental problem in multi-agent systems, aiming to generate collision-free trajectories for a team of unmanned aerial vehicles (UAVs) to complete distributed tasks efficiently. A key challenge lies in achieving both efficiency, by minimizing total mission cost, and fairness, by balancing the workload among UAVs to avoid overburdening individual agents. This paper presents a novel Iterative Exchange Framework for MUCPP, balancing efficiency and fairness through iterative task exchanges and path refinements. The proposed framework formulates a composite objective that combines the total mission distance and the makespan, and iteratively improves the solution via local exchanges under feasibility and safety constraints. For each UAV, collision-free trajectories are generated using A* search over a terrain-aware configuration space. Comprehensive experiments on multiple terrain datasets demonstrate that the proposed method consistently achieves superior trade-offs between total distance and makespan compared to existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ— äººæœºååŒè·¯å¾„è§„åˆ’ (Multi-UAV cooperative path planning, MUCPP) ä¸­æ€»ä»»åŠ¡æˆæœ¬ä¸å·¥ä½œé‡å¹³è¡¡ä¹‹é—´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„è¿­ä»£äº¤æ¢æ¡†æ¶ (Iterative Exchange Framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç»“åˆæ€»é£è¡Œè·ç¦»ä¸æœ€å¤§å®Œå·¥æ—¶é—´ (makespan) çš„å¤åˆç›®æ ‡å‡½æ•°ï¼ŒåŠ›æ±‚åœ¨ç³»ç»Ÿæ•ˆç‡ä¸ä¸ªä½“å…¬å¹³æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨ç®—æ³•å®ç°ä¸Šï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨æ»¡è¶³å¯è¡Œæ€§ä¸å®‰å…¨çº¦æŸçš„å‰æä¸‹ï¼Œè¿›è¡Œè¿­ä»£çš„ä»»åŠ¡äº¤æ¢ä¸è·¯å¾„ç»†åŒ–æ¥æŒç»­æ”¹è¿›æ–¹æ¡ˆã€‚é’ˆå¯¹å•æœºè½¨è¿¹ï¼Œç ”ç©¶åˆ©ç”¨ A* ç®—æ³•åœ¨æ„ŸçŸ¥åœ°å½¢çš„é…ç½®ç©ºé—´å†…è¿›è¡Œæœç´¢ï¼Œç¡®ä¿ç”Ÿæˆæ— ç¢°æ’è·¯å¾„ã€‚åœ¨å¤šä¸ªåœ°å½¢æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€»è·ç¦»å’Œæœ€å¤§å®Œå·¥æ—¶é—´çš„æƒè¡¡ä¸Šä¸€è‡´ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ•ˆç‡ä¸å…¬å¹³åˆ†é…é—®é¢˜ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00410v1",
      "published_date": "2025-11-29 09:41:22 UTC",
      "updated_date": "2025-11-29 09:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:56:03.552374+00:00"
    },
    {
      "arxiv_id": "2512.00408v1",
      "title": "Low-Bitrate Video Compression through Semantic-Conditioned Diffusion",
      "title_zh": "åŸºäºè¯­ä¹‰æ¡ä»¶æ‰©æ•£çš„ä½æ¯”ç‰¹ç‡è§†é¢‘å‹ç¼©",
      "authors": [
        "Lingdong Wang",
        "Guan-Ming Su",
        "Divya Kothandaraman",
        "Tsung-Wei Huang",
        "Mohammad Hajiesmaili",
        "Ramesh K. Sitaraman"
      ],
      "abstract": "Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º DiSCo çš„è¯­ä¹‰è§†é¢‘å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†é¢‘ç¼–è§£ç å™¨åœ¨æä½æ¯”ç‰¹ç‡(ultra-low bitrates)ä¸‹å› åƒç´ ä¿çœŸåº¦(pixel fidelity)ä¸äººç±»æ„ŸçŸ¥ä¸åŒ¹é…è€Œå¯¼è‡´çš„ä¸¥é‡ä¼ªå½±é—®é¢˜ã€‚DiSCo å°†æºè§†é¢‘åˆ†è§£ä¸ºæ–‡æœ¬æè¿°ã€æ—¶ç©ºé™çº§è§†é¢‘ä»¥åŠå¯é€‰çš„è‰å›¾æˆ–å§¿æ€ä¸‰ç§ç´§å‡‘æ¨¡æ€ï¼Œä»…ä¼ è¾“æ ¸å¿ƒçš„è¯­ä¹‰ã€å¤–è§‚å’Œè¿åŠ¨çº¿ç´¢ã€‚é€šè¿‡å¼•å…¥æ¡ä»¶è§†é¢‘æ‰©æ•£æ¨¡å‹(conditional video diffusion model)ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆå…ˆéªŒ(generative priors)ä»è¿™äº›æç®€è¡¨ç¤ºä¸­é‡å»ºå‡ºé«˜è´¨é‡ä¸”æ—¶é—´ç›¸å¹²(temporally coherent)çš„è§†é¢‘ã€‚ä¸ºäº†ä¼˜åŒ–å¤šæ¨¡æ€ç”Ÿæˆå’Œæ¨¡æ€ç´§å‡‘æ€§ï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†æ—¶é—´å‰å‘å¡«å……(Temporal forward filling)å’Œä»¤ç‰Œäº¤é”™(token interleaving)ç­‰æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½æ¯”ç‰¹ç‡æ¡ä»¶ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šæ¯”åŸºçº¿è¯­ä¹‰å’Œä¼ ç»Ÿç¼–è§£ç å™¨æå‡äº† 2 åˆ° 10 å€ï¼Œå®ç°äº†å“è¶Šçš„è§†è§‰é‡å»ºæ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00408v1",
      "published_date": "2025-11-29 09:38:16 UTC",
      "updated_date": "2025-11-29 09:38:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:56:09.541542+00:00"
    },
    {
      "arxiv_id": "2512.00406v1",
      "title": "GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework",
      "title_zh": "GreenPlannerï¼šåŸºäºèƒ½æ•ˆæ„ŸçŸ¥ä¸åŠŸèƒ½å¯è¡Œç”Ÿæˆæ¡†æ¶çš„å®ç”¨å¹³é¢å¸ƒå±€ç”Ÿæˆ",
      "authors": [
        "Pengyu Zeng",
        "Yuqin Dai",
        "Jun Yin",
        "Jing Zhong",
        "Ziyang Han",
        "Chaoyang Shi",
        "ZhanXiang Jin",
        "Maowei Jiang",
        "Yuxing Han",
        "Shuai Lu"
      ],
      "abstract": "Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GreenPlannerï¼Œä¸€ä¸ªå…¼é¡¾èƒ½æºæ„è¯†å’ŒåŠŸèƒ½å¯è¡Œæ€§çš„ç”Ÿæˆå¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å»ºç­‘å¹³é¢å›¾è®¾è®¡ä¸­æ‰‹åŠ¨æˆæœ¬é«˜ã€éš¾ä»¥è§„æ¨¡åŒ–ä»¥åŠç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•å¸¸è¿åç©ºé—´åŠŸèƒ½å’Œèƒ½æºçº¦æŸçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ç”¨äºå­¦ä¹ çº¦æŸå…ˆéªŒçš„æ ‡æ³¨æ•°æ®é›†Design Feasibility Datasetï¼Œä»¥åŠèƒ½å¤Ÿå¿«é€Ÿé¢„æµ‹èƒ½æºæ€§èƒ½å’Œç©ºé—´åŠŸèƒ½æœ‰æ•ˆæ€§çš„å®ç”¨è®¾è®¡è¯„ä¼°å™¨Practical Design Evaluator (PDE)ã€‚é€šè¿‡ä»PDEå¼•å¯¼ç­›é€‰å‡ºçš„Green Plan Dataset (GreenPD) ä¸­æå–ç¬¦åˆè§„èŒƒçš„å¸ƒå±€ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥è®­ç»ƒäº†GreenFlowç”Ÿæˆå™¨ä»¥å®ç°å—æ§ä¸”åˆè§„çš„è‡ªåŠ¨åŒ–ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒGreenPlanneråœ¨ä¿æŒ99%ä»¥ä¸Šå‡†ç¡®ç‡çš„åŒæ—¶å°†è¯„ä¼°é€Ÿåº¦æå‡äº†10ä¸‡å€ä»¥ä¸Šï¼Œå¹¶èƒ½æœ‰æ•ˆæ¶ˆé™¤æ— æ•ˆè®¾è®¡æ ·æœ¬ã€‚ç›¸æ¯”äºä¸“ä¸šå»ºç­‘å¸ˆï¼Œè¯¥æ¡†æ¶å°†æ•´ä½“è®¾è®¡æ•ˆç‡æå‡äº†87%ï¼Œä¸ºå®ç°å…¼å…·èƒ½æºæ•ˆç›Šä¸åŠŸèƒ½åˆç†æ€§çš„å®ç”¨å¹³é¢å›¾å¸ƒå±€ç”Ÿæˆæä¾›äº†é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00406v1",
      "published_date": "2025-11-29 09:35:50 UTC",
      "updated_date": "2025-11-29 09:35:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:56:18.335030+00:00"
    },
    {
      "arxiv_id": "2512.00403v1",
      "title": "SelfAI: Building a Self-Training AI System with LLM Agents",
      "title_zh": "SelfAIï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ„å»ºè‡ªè®­ç»ƒäººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Xiao Wu",
        "Ting-Zhu Huang",
        "Liang-Jian Deng",
        "Xiaobing Yu",
        "Yu Zhong",
        "Shangqi Deng",
        "Ufaq Khan",
        "Jianghao Wu",
        "Xiaofeng Liu",
        "Imran Razzak",
        "Xiaojun Chang",
        "Yutong Xie"
      ],
      "abstract": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SelfAIï¼Œä¸€ä¸ªé€šç”¨çš„å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªä¸»ç§‘å­¦å‘ç°æ¡†æ¶åœ¨åº”ç”¨é¢†åŸŸå±€é™ã€ç¼ºä¹å®æ—¶äººç±»äº¤äº’ä»¥åŠç¼ºä¹åŸåˆ™æ€§åœæ­¢æœºåˆ¶ç­‰å¯¼è‡´çš„ä½æ•ˆå’Œå¯é‡å¤æ€§æŒ‘æˆ˜ã€‚SelfAI æ ¸å¿ƒåŒ…å«ç”¨äºå°†é«˜å±‚ç ”ç©¶ç›®æ ‡è½¬åŒ–ä¸ºæ ‡å‡†åŒ–å®éªŒé…ç½®çš„ User Agentï¼Œç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨å¹¶ç»“åˆæœ€ä¼˜åœæ­¢å‡†åˆ™ï¼ˆoptimal stopping criteriaï¼‰è¿›è¡Œè¶…å‚æ•°è¿­ä»£ä¼˜åŒ–çš„ Cognitive Agentï¼Œä»¥åŠè´Ÿè´£åœ¨å¼‚æ„ç¡¬ä»¶ä¸Šåè°ƒå¹¶è¡Œã€å®¹é”™è®­ç»ƒæµç¨‹å¹¶ç»´æŠ¤ç»“æ„åŒ–çŸ¥è¯†åº“çš„ Experiment Managerã€‚ç ”ç©¶è¿˜å¼•å…¥äº† Score å’Œ $\\text{AUP}_D$ ä¸¤ä¸ªæ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨ä»¥é‡åŒ–ç§‘å­¦å‘ç°çš„æ•ˆç‡å’Œæœç´¢å¤šæ ·æ€§ã€‚åœ¨å›å½’ã€NLPã€è®¡ç®—æœºè§†è§‰ã€ç§‘å­¦è®¡ç®—ã€åŒ»å­¦æˆåƒå’Œè¯ç‰©å‘ç°ç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSelfAI ç›¸æ¯”äºç»å…¸çš„ Bayesian optimization å’ŒåŸºäº LLM çš„åŸºå‡†æ¨¡å‹ï¼Œä¸€è‡´å±•ç°å‡ºæ›´å¼ºçš„æ€§èƒ½å¹¶æ˜¾è‘—å‡å°‘äº†å†—ä½™è¯•éªŒã€‚è¯¥ç³»ç»Ÿä¸ä»…å®ç°äº†ä¸äººç±»ç ”ç©¶è€…çš„æ— ç¼äº¤äº’ï¼Œè¿˜ä¸ºæ„å»ºè‡ªåŠ¨åŒ–ã€å¯æ‰©å±•ä¸”å…·å¤‡åé¦ˆæœºåˆ¶çš„è‡ªè®­ç»ƒ AI ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00403v1",
      "published_date": "2025-11-29 09:18:39 UTC",
      "updated_date": "2025-11-29 09:18:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:56:32.839718+00:00"
    },
    {
      "arxiv_id": "2512.00396v2",
      "title": "Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement",
      "title_zh": "è¾¹ç¼˜ç«¯æ—¶é—´åºåˆ—ï¼šé¢å‘ç©¿æˆ´å¼æ­¥æ€æ£€æµ‹ä¸æœ€ä¼˜ä¼ æ„Ÿå™¨å¸ƒå±€çš„å¾®å‹å¯åˆ†ç¦»å·ç§¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Andrea Procopio",
        "Marco Esposito",
        "Sara Raggiunto",
        "Andrey Gizdov",
        "Alberto Belli",
        "Paola Pierleoni"
      ],
      "abstract": "We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºå—é™çš„å¯ç©¿æˆ´è®¾å¤‡å’Œè¾¹ç¼˜èŠ‚ç‚¹ï¼Œæ¢è®¨äº†ç”¨äºå¸•é‡‘æ£®ç—…(Parkinson's disease)æ­¥æ€æ£€æµ‹(Gait Detection)çš„è®¾å¤‡ç«¯æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•ã€‚ç ”ç©¶å¯¹æ¯”äº†ä¼ ç»Ÿé˜ˆå€¼æ³•ä¸ä¸‰ç§ 1D CNN æ¨¡å‹ï¼Œç‰¹åˆ«æå‡ºäº†åŸºäºå¯åˆ†ç¦»å·ç§¯(Separable Convolutions)å’Œæ®‹å·®è¿æ¥(Residual Connections)çš„è¶…è½»é‡åŒ–æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…æœ‰ 533 ä¸ªå‚æ•°çš„æ®‹å·®å¯åˆ†ç¦»æ¨¡å‹åœ¨ PR-AUC (94.5%) å’Œ F1 (91.2%) ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå‚æ•°é‡å¤§ 10 å€çš„åŸºå‡†æ¨¡å‹ï¼Œå±•ç°å‡ºæé«˜çš„å‚æ•°æ•ˆç‡ã€‚ä¼ æ„Ÿå™¨ä½ç½®åˆ†æå‘ç°èƒ¸éƒ¨å’Œå¤§è…¿æœ€ä¸ºå¯é ï¼Œè€Œå‰è‡‚å—éæ­¥æ€å¹²æ‰°è¾ƒå¤šï¼Œä¸”å¤šä¼ æ„Ÿå™¨èåˆå¹¶æœªæ˜¾è‘—æå‡æ€§èƒ½ã€‚è¯¥æ¨¡å‹å¯åœ¨ STM32 çº§åˆ«å•ç‰‡æœºä¸Šå®ç°ä½äº 10ms çš„æ¨ç†å»¶è¿Ÿï¼Œæœ‰æ•ˆæ”¯æŒäº†è¾¹ç¼˜ç«¯çš„å®æ—¶æ•°æ®å¤„ç†ä¸ä¼ è¾“é—¨æ§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¶…è½»é‡åŒ–å¯åˆ†ç¦» CNN åœ¨å‡†ç¡®ç‡ã€è®¡ç®—æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ä¹‹é—´è¾¾æˆäº†ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„å¹³è¡¡ï¼Œè¯æ˜äº†å®šåˆ¶åŒ–æ—¶é—´åºåˆ—æ¨¡å‹åœ¨è¾¹ç¼˜æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00396v2",
      "published_date": "2025-11-29 08:52:41 UTC",
      "updated_date": "2025-12-11 23:49:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:01.331654+00:00"
    },
    {
      "arxiv_id": "2512.00391v1",
      "title": "From Coefficients to Directions: Rethinking Model Merging with Directional Alignment",
      "title_zh": "ä»ç³»æ•°åˆ°æ–¹å‘ï¼šåŸºäºæ–¹å‘å¯¹é½é‡æ–°å®¡è§†æ¨¡å‹åˆå¹¶",
      "authors": [
        "Zhikang Chen",
        "Sen Cui",
        "Deheng Ye",
        "Min Zhang",
        "Gang Niu",
        "Yu Zhang",
        "Masashi Sugiyama",
        "Tingting Zhu"
      ],
      "abstract": "Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, naÃ¯ve merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \\emph{directional alignment} and introduce a unified geometric framework, \\emph{Merging with Directional Alignment} (\\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨¡å‹åˆå¹¶ï¼ˆModel mergingï¼‰åœ¨æ— éœ€é‡è®­ç»ƒæƒ…å†µä¸‹æ•´åˆç‹¬ç«‹æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºå½“å‰æ–¹æ³•è¿‡åº¦å…³æ³¨å‚æ•°ç©ºé—´åˆ†è§£æˆ–ç³»æ•°ä¼˜åŒ–ï¼Œè€Œå¿½è§†äº†å‚æ•°ä¸ç‰¹å¾ç©ºé—´ä¸­æ–¹å‘ä¿¡æ¯ï¼ˆdirectional informationï¼‰çš„å…³é”®ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„åˆå¹¶æ–¹å¼ä¼šå¯¼è‡´å‚æ•°æ–¹å‘ä¸ä¸€è‡´å¹¶ç ´åæ¨¡å‹çš„ç»“æ„ç›¸å¹²æ€§ï¼ˆstructural coherenceï¼‰ï¼Œä¸”ç”±äºç¥ç»åç¼©ï¼ˆNeural Collapseï¼‰ç°è±¡çš„å­˜åœ¨ï¼Œä¸åŒæ¨¡å‹é—´çš„ç‰¹å¾ç©ºé—´æ–¹å‘å¾€å¾€éš¾ä»¥å…¼å®¹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åä¸º MDAï¼ˆMerging with Directional Alignmentï¼‰çš„ç»Ÿä¸€å‡ ä½•æ¡†æ¶ï¼Œæ—¨åœ¨å‚æ•°å’Œç‰¹å¾ç©ºé—´ä¸­å®ç°ä¸€è‡´çš„æ–¹å‘å¯¹é½ã€‚é€šè¿‡å¯¹é½æ–¹å‘ç»“æ„ï¼ŒMDA æœ‰æ•ˆæå‡äº†åˆå¹¶æ¨¡å‹çš„ç»“æ„è¿è´¯æ€§ï¼Œè§£å†³äº†ç‰¹å¾ç©ºé—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ã€æ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡é…ç½®ä¸‹çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹åˆå¹¶æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00391v1",
      "published_date": "2025-11-29 08:40:58 UTC",
      "updated_date": "2025-11-29 08:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:01.739506+00:00"
    },
    {
      "arxiv_id": "2512.00383v1",
      "title": "An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines",
      "title_zh": "å°†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä½œä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ å­ä¾‹ç¨‹çš„æœ‰æ•ˆæ€§å®è¯ç ”ç©¶",
      "authors": [
        "Jianhai Su",
        "Jinzhu Luo",
        "Qi Zhang"
      ],
      "abstract": "We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline RL) ç®—æ³•ä½œä¸ºä»é›¶å¼€å§‹ (tabula rasa) çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹  (Online RL) å­ç¨‹åºçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡è®©æ™ºèƒ½ä½“å°†å†å²äº¤äº’è®°å½•é‡æ–°åˆ©ç”¨ä¸ºç¦»çº¿æ•°æ®é›†æ¥å®ç°æ€§èƒ½æå‡ã€‚ç ”ç©¶è€…å°†æ­¤æ€è·¯å½¢å¼åŒ–ä¸ºä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œæ”¯æŒæœ€ç»ˆç­–ç•¥æ¨è (final policy recommendation) å’Œåœ¨çº¿å¾®è°ƒ (online fine-tuning) ç­‰å¤šç§å˜ä½“ï¼Œå¹¶å¼•å…¥äº†æ—¨åœ¨å¢å¼ºå­¦ä¹ æ•ˆç‡çš„å®ç”¨æŠ€æœ¯ã€‚ç³»ç»Ÿçš„å®è¯åˆ†æè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºä»»åŠ¡æ€§è´¨ï¼Œä¸”ç ”ç©¶æå‡ºçš„æ”¹è¿›æŠ€æœ¯èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®éªŒå‘ç°ç°æœ‰çš„åœ¨çº¿å¾®è°ƒæ–¹æ³•åœ¨æ•´ä½“ä¸Šæ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œè¿™ä¸€å‘ç°æ­ç¤ºäº†è¯¥é¢†åŸŸçš„å½“å‰å±€é™å¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00383v1",
      "published_date": "2025-11-29 08:17:03 UTC",
      "updated_date": "2025-11-29 08:17:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:05.737983+00:00"
    },
    {
      "arxiv_id": "2512.00376v1",
      "title": "Layer Probing Improves Kinase Functional Prediction with Protein Language Models",
      "title_zh": "é‡‡ç”¨å±‚æ¢æµ‹æŠ€æœ¯æå‡åŸºäºè›‹ç™½è´¨è¯­è¨€æ¨¡å‹çš„æ¿€é…¶åŠŸèƒ½é¢„æµ‹",
      "authors": [
        "Ajit Kumar",
        "IndraPrakash Jha"
      ],
      "abstract": "Protein language models (PLMs) have transformed sequence-based protein analysis, yet most applications rely only on final-layer embeddings, which may overlook biologically meaningful information encoded in earlier layers. We systematically evaluate all 33 layers of ESM-2 for kinase functional prediction using both unsupervised clustering and supervised classification. We show that mid-to-late transformer layers (layers 20-33) outperform the final layer by 32 percent in unsupervised Adjusted Rand Index and improve homology-aware supervised accuracy to 75.7 percent. Domain-level extraction, calibrated probability estimates, and a reproducible benchmarking pipeline further strengthen reliability. Our results demonstrate that transformer depth contains functionally distinct biological signals and that principled layer selection significantly improves kinase function prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è›‹ç™½è´¨è¯­è¨€æ¨¡å‹(Protein Language Models)åœ¨æ¿€é…¶åŠŸèƒ½é¢„æµ‹(kinase functional prediction)ä¸­é€šå¸¸ä»…åˆ©ç”¨æœ€ç»ˆå±‚åµŒå…¥(final-layer embeddings)è€Œå¿½è§†æ—©æœŸå±‚ç”Ÿç‰©ä¿¡æ¯çš„é—®é¢˜ï¼Œå¯¹ESM-2æ¨¡å‹çš„å…¨éƒ¨33å±‚è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å±‚æ¢æµ‹(Layer Probing)è¯„ä¼°ã€‚é€šè¿‡æ— ç›‘ç£èšç±»(unsupervised clustering)å’Œæœ‰ç›‘ç£åˆ†ç±»(supervised classification)å®éªŒï¼Œç ”ç©¶å‘ç°ä¸­åæœŸè½¬æ¢å±‚(layers 20-33)çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºæœ€ç»ˆå±‚ï¼Œå…¶ä¸­æ— ç›‘ç£è°ƒæ•´å…°å¾·æŒ‡æ•°(Adjusted Rand Index)æå‡äº†32%ï¼ŒåŒæºæ„ŸçŸ¥æœ‰ç›‘ç£å‡†ç¡®ç‡è¾¾åˆ°75.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç»“åˆäº†ç»“æ„åŸŸçº§æå–(Domain-level extraction)ã€æ ¡å‡†æ¦‚ç‡ä¼°è®¡å’Œå¯é‡å¤åŸºå‡†æµ‹è¯•æµæ°´çº¿ä»¥æå‡é¢„æµ‹å¯é æ€§ã€‚è¯¥æˆæœè¯æ˜äº†Transformeræ·±åº¦ä¸­åŒ…å«åŠŸèƒ½å„å¼‚çš„ç”Ÿç‰©ä¿¡å·ï¼Œé€šè¿‡ç§‘å­¦çš„å±‚é€‰æ‹©å¯å¤§å¹…ä¼˜åŒ–æ¿€é…¶åŠŸèƒ½é¢„æµ‹çš„æ•ˆæœã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "14 pages, 7 figures, 3 tables; includes code and dataset links",
      "pdf_url": "https://arxiv.org/pdf/2512.00376v1",
      "published_date": "2025-11-29 08:06:11 UTC",
      "updated_date": "2025-11-29 08:06:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:56:58.653054+00:00"
    },
    {
      "arxiv_id": "2512.00371v1",
      "title": "Evaluating LLMs in Open-Source Games",
      "title_zh": "å¼€æºåšå¼ˆä¸­çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Swadesh Sistla",
        "Max Kleiman-Weiner"
      ],
      "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨å¼€æºåšå¼ˆ(open-source games)ä¸­çš„ç¼–ç¨‹èƒ½åŠ›ï¼Œåœ¨è¿™ç§åšå¼ˆè®ºè®¾å®šä¸­ï¼Œå‚ä¸è€…æäº¤è®¡ç®—æœºç¨‹åºè€Œéå…·ä½“è¡ŒåŠ¨ã€‚è¿™ç§å½¢å¼åˆ©ç”¨äº†ä»£ç çš„é€æ˜åº¦ï¼Œæä¾›äº†å¯è§£é‡Šæ€§(interpretability)å’Œæ­£å¼å¯éªŒè¯æ€§ï¼Œå¹¶ä¿ƒæˆäº†ä¼ ç»Ÿåšå¼ˆä¸­æ— æ³•å®ç°çš„ç¨‹åºå‡è¡¡(program equilibria)ã€‚é€šè¿‡è¯„ä¼°é¢†å…ˆçš„å¼€æºåŠé—­æºLLMsï¼Œç ”ç©¶è€…åˆ†æäº†å®ƒä»¬åœ¨äºŒå…ƒå’Œè¿›åŒ–è®¾å®šä¸‹é¢„æµ‹ç­–ç•¥åŠè¯„ä¼°ç¨‹åºå‡è¡¡ç‰¹å¾çš„èƒ½åŠ›ã€‚å®éªŒè¯†åˆ«å‡ºäº†æœ€å¤§åŒ–æ”¶ç›Šã€åˆä½œå’Œæ¬ºéª—ç­–ç•¥çš„æ¶Œç°ï¼Œå¹¶è¡¨å¾äº†è¿™äº›ç¨‹åºæœºåˆ¶åœ¨é‡å¤åšå¼ˆä¸­çš„æ¼”å˜è¿‡ç¨‹ã€‚ç ”ç©¶è¿˜åˆ†æäº†è¿™äº›ç¨‹åºçš„ç›¸å¯¹è¿›åŒ–é€‚åº”åº¦(evolutionary fitness)ï¼Œè¯æ˜å¼€æºåšå¼ˆæ˜¯ç ”ç©¶å’Œå¼•å¯¼å¤šæ™ºèƒ½ä½“å›°å¢ƒä¸­åˆä½œç­–ç•¥æ¶Œç°çš„æœ‰æ•ˆç¯å¢ƒã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.00371v1",
      "published_date": "2025-11-29 07:46:25 UTC",
      "updated_date": "2025-11-29 07:46:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:32.341486+00:00"
    },
    {
      "arxiv_id": "2512.00366v1",
      "title": "S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting",
      "title_zh": "S^2-KDï¼šåŸºäºè¯­ä¹‰-é¢‘è°±çŸ¥è¯†è’¸é¦çš„æ—¶ç©ºé¢„æµ‹",
      "authors": [
        "Wenshuo Wang",
        "Yaomin Shen",
        "Yingjie Tan",
        "Yihao Chen"
      ],
      "abstract": "Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† S^2-KDï¼Œä¸€ç§å°†è¯­ä¹‰å…ˆéªŒ (Semantic priors) ä¸å…‰è°±è¡¨ç¤º (Spectral representations) ç»Ÿä¸€çš„çŸ¥è¯†è’¸é¦ (Knowledge Distillation) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ—¶ç©ºé¢„æµ‹æ¨¡å‹çš„æ•ˆç‡ä¸æ€§èƒ½ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿæ–¹æ³•å±€é™äºåƒç´ çº§ä¿¡å·è€Œç¼ºä¹è¯­ä¹‰å’Œå› æœèƒŒæ™¯çš„å±€é™ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LMM) çš„æ–‡æœ¬å™è¿°æ„å»ºç‰¹æƒå¤šæ¨¡æ€æ•™å¸ˆæ¨¡å‹ï¼Œä»¥æ¨ç†äº‹ä»¶çš„æ·±å±‚åŸå› å¹¶åœ¨æ½œç©ºé—´è§£è€¦å…‰è°±æˆåˆ†ã€‚é€šè¿‡å¼•å…¥æ–°çš„è’¸é¦ç›®æ ‡ï¼Œè¯¥æ–¹æ³•å°†ç»Ÿä¸€çš„è¯­ä¹‰-å…‰è°±çŸ¥è¯†è¿ç§»è‡³è½»é‡çº§è§†è§‰å­¦ç”Ÿæ¨¡å‹ï¼Œä½¿å…¶åœ¨æ¨ç†é˜¶æ®µæ— éœ€æ–‡æœ¬è¾“å…¥æˆ–æ¶æ„å¼€é”€å³å¯å®ç°è¯­ä¹‰è¿è´¯çš„é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS^2-KD åœ¨ WeatherBench å’Œ TaxiBJ+ ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨é•¿æ—¶é¢„æµ‹å’Œå¤æ‚éå¹³ç¨³åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00366v1",
      "published_date": "2025-11-29 07:27:15 UTC",
      "updated_date": "2025-11-29 07:27:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T11:58:33.937308+00:00"
    },
    {
      "arxiv_id": "2512.00365v1",
      "title": "Towards aligned body representations in vision models",
      "title_zh": "è¿ˆå‘è§†è§‰æ¨¡å‹ä¸­çš„å¯¹é½èº«ä½“è¡¨å¾",
      "authors": [
        "Andrey Gizdov",
        "Andrea Procopio",
        "Yichen Li",
        "Daniel Harari",
        "Tomer Ullman"
      ],
      "abstract": "Human physical reasoning relies on internal \"body\" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰æ¨¡å‹(vision models)æ˜¯å¦èƒ½å¤Ÿå½¢æˆç±»ä¼¼äºäººç±»ç‰©ç†æ¨ç†ä¸­ä½¿ç”¨çš„â€œèº«ä½“â€è¡¨å¾(body representations)ï¼Œå³ä¸€ç§ç”¨äºæ•æ‰ç‰©ä½“èŒƒå›´å¹¶æ”¯æŒè¿åŠ¨å’Œç‰©ç†é¢„æµ‹çš„ç²—ç•¥ä½“ç§¯è¿‘ä¼¼ã€‚ç ”ç©¶äººå‘˜å°†ä¸€é¡¹é’ˆå¯¹50åäººç±»å‚ä¸è€…çš„å¿ƒç†ç‰©ç†å­¦å®éªŒæ”¹ç¼–ä¸ºè¯­ä¹‰åˆ†å‰²(semantic segmentation)ä»»åŠ¡ï¼Œå¹¶å¯¹ä¸ƒç§ä¸åŒè§„æ¨¡çš„åˆ†å‰²ç½‘ç»œè¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾ƒå°çš„æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°å½¢æˆç±»ä¼¼äºäººç±»çš„ç²—ç•¥â€œèº«ä½“â€è¡¨å¾(coarse body representations)ï¼Œè€Œè§„æ¨¡è¾ƒå¤§çš„æ¨¡å‹åˆ™æ›´å€¾å‘äºäº§ç”Ÿè¿‡äºè¯¦ç»†çš„ç»†ç²’åº¦ç¼–ç (fine-grain encodings)ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†ç²—ç•¥è¡¨å¾å¯ä»¥åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹æ¶Œç°ï¼Œå¹¶è¡¨æ˜æœºå™¨è¡¨å¾èƒ½å¤Ÿä¸ºç†è§£å¤§è„‘ç‰©ç†æ¨ç†çš„ç»“æ„æä¾›ä¸€æ¡å…·æœ‰å¯æ‰©å±•æ€§çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Andrea Procopio and Andrey Gizdov have equal contributions",
      "pdf_url": "https://arxiv.org/pdf/2512.00365v1",
      "published_date": "2025-11-29 07:25:32 UTC",
      "updated_date": "2025-11-29 07:25:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:13.644758+00:00"
    },
    {
      "arxiv_id": "2512.00350v1",
      "title": "MedCondDiff: Lightweight, Robust, Semantically Guided Diffusion for Medical Image Segmentation",
      "title_zh": "MedCondDiffï¼šè½»é‡åŒ–ã€é²æ£’ä¸”å…·æœ‰è¯­ä¹‰å¼•å¯¼çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Ruirui Huang",
        "Jiacheng Li"
      ],
      "abstract": "We introduce MedCondDiff, a diffusion-based framework for multi-organ medical image segmentation that is efficient and anatomically grounded. The model conditions the denoising process on semantic priors extracted by a Pyramid Vision Transformer (PVT) backbone, yielding a semantically guided and lightweight diffusion architecture. This design improves robustness while reducing both inference time and VRAM usage compared to conventional diffusion models. Experiments on multi-organ, multi-modality datasets demonstrate that MedCondDiff delivers competitive performance across anatomical regions and imaging modalities, underscoring the potential of semantically guided diffusion models as an effective class of architectures for medical imaging tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedCondDiffï¼Œä¸€ç§ç”¨äºå¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŸºäºæ‰©æ•£æ¨¡å‹ (diffusion-based) çš„æ¡†æ¶ï¼Œå…·æœ‰é«˜æ•ˆä¸”ç¬¦åˆè§£å‰–å­¦ç‰¹å¾çš„ç‰¹ç‚¹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ Pyramid Vision Transformer (PVT) éª¨å¹²ç½‘ç»œæå–è¯­ä¹‰å…ˆéªŒ (semantic priors)ï¼Œå¹¶å°†å»å™ªè¿‡ç¨‹ (denoising process) å»ºç«‹åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä»è€Œæ„å»ºå‡ºè¯­ä¹‰å¼•å¯¼ä¸”è½»é‡åŒ–çš„æ‰©æ•£æ¶æ„ã€‚è¿™ç§è®¾è®¡åœ¨æé«˜æ¨¡å‹é²æ£’æ€§ (robustness) çš„åŒæ—¶ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ (inference time) å’Œæ˜¾å­˜ (VRAM) å ç”¨ã€‚åœ¨å¤šå™¨å®˜ã€å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMedCondDiff åœ¨ä¸åŒè§£å‰–åŒºåŸŸå’Œæˆåƒæ¨¡æ€ä¸‹å‡è¡¨ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æˆæœå¼ºè°ƒäº†è¯­ä¹‰å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§é«˜æ•ˆæ¶æ„ç±»åœ¨å¤„ç†åŒ»å­¦å½±åƒä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00350v1",
      "published_date": "2025-11-29 06:43:15 UTC",
      "updated_date": "2025-11-29 06:43:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:21.440552+00:00"
    },
    {
      "arxiv_id": "2512.03087v1",
      "title": "When Harmful Content Gets Camouflaged: Unveiling Perception Failure of LVLMs with CamHarmTI",
      "title_zh": "æœ‰å®³å†…å®¹çš„ä¼ªè£…ï¼šåˆ©ç”¨ CamHarmTI æ­ç¤º LVLM çš„æ„ŸçŸ¥å¤±æ•ˆ",
      "authors": [
        "Yanhui Li",
        "Qi Zhou",
        "Zhihong Xu",
        "Huizhong Guo",
        "Wenhai Wang",
        "Dongxia Wang"
      ],
      "abstract": "Large vision-language models (LVLMs) are increasingly used for tasks where detecting multimodal harmful content is crucial, such as online content moderation. However, real-world harmful content is often camouflaged, relying on nuanced text-image interplay, such as memes or images with embedded malicious text, to evade detection. This raises a key question: \\textbf{can LVLMs perceive such camouflaged harmful content as sensitively as humans do?} In this paper, we introduce CamHarmTI, a benchmark for evaluating LVLM ability to perceive and interpret camouflaged harmful content within text-image compositions. CamHarmTI consists of over 4,500 samples across three types of image-text posts. Experiments on 100 human users and 12 mainstream LVLMs reveal a clear perceptual gap: humans easily recognize such content (e.g., over 95.75\\% accuracy), whereas current LVLMs often fail (e.g., ChatGPT-4o achieves only 2.10\\% accuracy). Moreover, fine-tuning experiments demonstrate that \\bench serves as an effective resource for improving model perception, increasing accuracy by 55.94\\% for Qwen2.5VL-7B. Attention analysis and layer-wise probing further reveal that fine-tuning enhances sensitivity primarily in the early layers of the vision encoder, promoting a more integrated scene understanding. These findings highlight the inherent perceptual limitations in LVLMs and offer insight into more human-aligned visual reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨æ£€æµ‹ä¼ªè£…æœ‰å®³å†…å®¹ï¼ˆå¦‚æ¨¡å› æˆ–åµŒå…¥æ¶æ„æ–‡æœ¬çš„å›¾åƒï¼‰æ—¶çš„æ„ŸçŸ¥å¤±æ•ˆé—®é¢˜ï¼Œæå‡ºäº†åä¸º CamHarmTI çš„è¯„ä¼°åŸºå‡†ã€‚CamHarmTI åŒ…å«è¶…è¿‡ 4,500 ä¸ªè·¨ä¸‰ç§å›¾æ–‡ç±»å‹çš„æ ·æœ¬ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°æ¨¡å‹æ„ŸçŸ¥å’Œè§£é‡Šå›¾æ–‡ç»„åˆä¸­ä¼ªè£…æœ‰å®³ä¿¡æ¯çš„èƒ½åŠ›ã€‚å®éªŒé€šè¿‡å¯¹ 100 åäººç±»ç”¨æˆ·å’Œ 12 ç§ä¸»æµ LVLMs çš„æµ‹è¯•æ­ç¤ºäº†å·¨å¤§çš„æ„ŸçŸ¥å·®è·ï¼šäººç±»çš„è¯†åˆ«å‡†ç¡®ç‡è¶…è¿‡ 95.75%ï¼Œè€Œ ChatGPT-4o ç­‰æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º 2.10%ã€‚å¾®è°ƒå®éªŒè¯æ˜ CamHarmTI èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œä½¿ Qwen2.5VL-7B çš„å‡†ç¡®ç‡æé«˜ 55.94%ï¼Œä¸”æ³¨æ„åŠ›åˆ†æè¡¨æ˜è¿™ç§æ”¹è¿›ä¸»è¦æºäºè§†è§‰ç¼–ç å™¨ (Vision Encoder) æ—©æœŸå±‚å¯¹åœºæ™¯ç†è§£æ•æ„Ÿåº¦çš„å¢å¼ºã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰ LVLMs å›ºæœ‰çš„æ„ŸçŸ¥å±€é™æ€§ï¼Œå¹¶ä¸ºæ„å»ºæ›´ç¬¦åˆäººç±»è§†è§‰æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03087v1",
      "published_date": "2025-11-29 06:39:47 UTC",
      "updated_date": "2025-11-29 06:39:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:28.333450+00:00"
    },
    {
      "arxiv_id": "2512.00349v1",
      "title": "Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models",
      "title_zh": "å›¾åƒè¾©è®ºï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¬ºéª—è¡Œä¸ºæ£€æµ‹",
      "authors": [
        "Sitong Fang",
        "Shiyi Hou",
        "Kaile Wang",
        "Boyuan Chen",
        "Donghai Hong",
        "Jiayi Zhou",
        "Josef Dai",
        "Yaodong Yang",
        "Jiaming Ji"
      ],
      "abstract": "Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œéšç€äººå·¥æ™ºèƒ½èƒ½åŠ›çš„æå‡ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­è“„æ„è¯¯å¯¼ç”¨æˆ·çš„æ¬ºéª—ï¼ˆdeceptionï¼‰è¡Œä¸ºæ­£æˆä¸ºæ¯”å¹»è§‰ï¼ˆhallucinationï¼‰æ›´å…·å¨èƒçš„å®‰å…¨é£é™©ã€‚é’ˆå¯¹ç›®å‰å¤šæ¨¡æ€æ¬ºéª—ç ”ç©¶çš„ç©ºç™½ï¼Œä½œè€…æ¨å‡ºäº† MM-DeceptionBenchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¦‚ä½•é€šè¿‡è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ååŒè¿›è¡Œæˆ˜ç•¥æ€§è¯¯å¯¼çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å…­ç±»æ¬ºéª—è¡Œä¸ºã€‚ä¸ºäº†è§£å†³è·¨æ¨¡æ€æ¨ç†å¤æ‚æ€§å¯¼è‡´çš„ç›‘æµ‹éš¾é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Debate with Images çš„å¤šæ™ºèƒ½ä½“è¾©è®ºç›‘æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåˆ¶æ¨¡å‹å¿…é¡»ä»¥è§†è§‰è¯æ®ï¼ˆvisual evidenceï¼‰æ”¯æŒå…¶è®ºç‚¹ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹æ¨¡å‹æ¬ºéª—ç­–ç•¥çš„æ•æ‰èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¤§å¹…æå‡äº†è‡ªåŠ¨åŒ–ç›‘æµ‹ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œåœ¨ GPT-4o æ¨¡å‹ä¸Šå°† Cohen's kappa æé«˜äº† 1.5 å€ï¼Œå‡†ç¡®ç‡æé«˜äº† 1.25 å€ï¼Œä¸ºç›‘æ§å‰æ²¿ AI ç³»ç»Ÿä¸­çš„éšè”½æ¬ºéª—è¡Œä¸ºæä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00349v1",
      "published_date": "2025-11-29 06:39:36 UTC",
      "updated_date": "2025-11-29 06:39:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:57:52.235569+00:00"
    },
    {
      "arxiv_id": "2512.00344v1",
      "title": "Echo-N1: Affective RL Frontier",
      "title_zh": "Echo-N1ï¼šæƒ…æ„Ÿå¼ºåŒ–å­¦ä¹ æ–°å‰æ²¿",
      "authors": [
        "Naifan Zhang",
        "Ruihan Sun",
        "Ruixi Su",
        "Shiqi Ma",
        "Shiya Zhang",
        "Xianna Weng",
        "Xiaofan Zhang",
        "Yuhan Zhan",
        "Yuyang Xu",
        "Zhaohan Chen",
        "Zhengyuan Pan",
        "Ziyi Song"
      ],
      "abstract": "The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Echo-N1 æ¡†æ¶ï¼Œæ¢ç´¢äº†æƒ…æ„Ÿå¼ºåŒ–å­¦ä¹  (Affective RL) çš„å‰æ²¿é¢†åŸŸï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å…·æœ‰ä¸»è§‚æ€§ã€æƒ…æ„Ÿè‰²å½©å’Œäººæ ¼æ•æ„Ÿæ€§çš„å¯¹è¯ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå®æ—¶æ¨æ–­ç”¨æˆ·çš„äººæ ¼ç‰¹å¾ (personality)ï¼Œå¹¶æ ¹æ®ä¸ªæ€§åŒ–çš„å¯¹è¯åå¥½ (conversational preferences) å®æ—¶ä¼˜åŒ–æ¨¡å‹è¡Œä¸ºï¼Œè¯æ˜äº†åœ¨ééªŒè¯æ€§è®¾å®š (non-verifiable settings) ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ çš„å¯è¡Œæ€§ä¸ç¨³å¥æ€§ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†é¦–ä¸ªåŠ¨æ€æƒ…æ„Ÿæ™ºèƒ½è¯„ä¼°å¥—ä»¶ï¼Œç”¨ä»¥é‡åŒ–æ¨¡å‹åœ¨äººç±»äº¤äº’è´¨é‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEcho-N1 çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶åŸºå‡†ç‰ˆæœ¬ä»¥åŠå•†ä¸šæ¨¡å‹ Doubao 1.5 Characterï¼Œä¸ºä¼˜åŒ–æ¨¡å‹åœ¨æ·±åº¦ä¸»è§‚å’Œäººç±»ç»´åº¦ä¸Šçš„å¯¹è¯èƒ½åŠ›å»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00344v1",
      "published_date": "2025-11-29 06:25:16 UTC",
      "updated_date": "2025-11-29 06:25:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:58:48.736020+00:00"
    },
    {
      "arxiv_id": "2512.00332v2",
      "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents",
      "title_zh": "Assertion-Conditioned Complianceï¼šå¤šè½®å·¥å…·è°ƒç”¨æ™ºèƒ½ä½“ä¸­çš„ä¸€ç§æº¯æºæ„ŸçŸ¥æ¼æ´",
      "authors": [
        "Daud Waqas",
        "Aaryamaan Golthi",
        "Erika Hayashida",
        "Huanzhi Mao"
      ],
      "abstract": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤šè½®å·¥å…·è°ƒç”¨(multi-turn tool-calling)å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¯¹è¯æ—¶çš„å®‰å…¨æ€§ä¸é²æ£’æ€§é—®é¢˜ã€‚è®ºæ–‡å¼•å…¥äº†ä¸€ç§åä¸ºAssertion-Conditioned Compliance (A-CC)çš„æ–°å‹è¯„ä¼°èŒƒå¼ï¼Œç”¨äºåˆ†ææ¨¡å‹åœ¨é¢å¯¹è¯¯å¯¼æ€§æ–­è¨€æ—¶çš„åˆè§„è¡Œä¸ºã€‚è¯„ä¼°æ¶µç›–äº†ç”¨æˆ·æ¥æºæ–­è¨€(user-sourced assertions, USAs)å’Œå‡½æ•°æ¥æºæ–­è¨€(function-sourced assertions, FSAs)ä¸¤ä¸ªç»´åº¦ï¼Œåˆ†åˆ«è¡¡é‡æ¨¡å‹å¯¹é”™è¯¯ç”¨æˆ·ä¿¡å¿µçš„å¥‰æ‰¿æ€§(sycophancy)ä»¥åŠå¯¹å†²çªç³»ç»Ÿç­–ç•¥çš„å“åº”ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å…ˆè¿›æ¨¡å‹åœ¨åº”å¯¹USAå’ŒFSAæ—¶å‡è¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ã€‚è¿™ä¸€å‘ç°ç¡®è®¤äº†A-CCæ˜¯å·²éƒ¨ç½²æ™ºèƒ½ä½“ä¸­ä¸€ä¸ªå…³é”®ä¸”æ½œåœ¨çš„å®‰å…¨æ¼æ´ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨å¤šè½®å¯¹è¯é²æ£’æ€§è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages (incl. Appendix), 3 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00332v2",
      "published_date": "2025-11-29 05:44:37 UTC",
      "updated_date": "2026-01-21 13:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:21.245588+00:00"
    },
    {
      "arxiv_id": "2512.00331v1",
      "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System",
      "title_zh": "CogEvo-Eduï¼šè®¤çŸ¥è¿›åŒ–æ•™è‚²å¤šæ™ºèƒ½ä½“ååŒç³»ç»Ÿ",
      "authors": [
        "Yefeng Wu",
        "Yuchen Song",
        "Yecheng Zhao",
        "Ling Wu",
        "Shan Wan"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.",
      "tldr_zh": "é’ˆå¯¹ä¼ ç»Ÿå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•°å­—ä¿¡å·å¤„ç†(DSP)ç­‰å¤æ‚STEMæ•™è‚²é¢†åŸŸä¸­éš¾ä»¥ç»´æŒé•¿æœŸå­¦ç”Ÿæ¨¡å‹å’ŒåŠ¨æ€è°ƒæ•´æ•™å­¦ç­–ç•¥çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†CogEvo-Eduï¼Œä¸€ä¸ªåˆ†å±‚å¼çš„æ•™è‚²å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå°†æ£€ç´¢ã€è®°å¿†å’Œæ§åˆ¶è§†ä¸ºä¸€ä¸ªè€¦åˆçš„è®¤çŸ¥è¿›åŒ–è¿‡ç¨‹ï¼Œç”±è®¤çŸ¥æ„ŸçŸ¥å±‚(CPL)ã€çŸ¥è¯†è¿›åŒ–å±‚(KEL)å’Œå…ƒæ§åˆ¶å±‚(MCL)å…±åŒæ„æˆã€‚å…¶ä¸­CPLåˆ©ç”¨åŒé‡è®°å¿†å’Œç½®ä¿¡æƒé‡æ•´åˆæœºåˆ¶ï¼Œåœ¨æœ‰é™ä¸Šä¸‹æ–‡ä¸­æ„å»ºç»“æ„åŒ–çš„è‡ªæˆ‘ä¿®æ­£å­¦ç”Ÿç”»åƒï¼Œè€ŒKELé€šè¿‡èµ‹äºˆçŸ¥è¯†å—æ—¶ç©ºä»·å€¼æ¥é©±åŠ¨å…¶æ¿€æ´»ã€è¯­ä¹‰å‹ç¼©ä¸é—å¿˜ã€‚MCLåˆ™å°†è¾…å¯¼ä»»åŠ¡å»ºæ¨¡ä¸ºåˆ†å±‚åºè´¯å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡åŒé‡å†…å¤–ç¯è·¯(dual inner-outer loop)åè°ƒä¸“ä¸šæ™ºèƒ½ä½“å¹¶å®æ—¶è°ƒæ•´ç³»ç»Ÿè¶…å‚æ•°ã€‚ä¸ºäº†éªŒè¯ç³»ç»Ÿæœ‰æ•ˆæ€§ï¼Œç ”ç©¶è€…æ„å»ºäº†å‚ç›´åŸºå‡†DSP-EduBenchï¼ŒåŒ…å«å¼‚æ„èµ„æºã€æ¨¡æ‹Ÿå­¦ç”Ÿç”»åƒåŠé•¿æ—¶ç¨‹äº¤äº’è„šæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCogEvo-Eduåœ¨LLM-as-a-Judgeè¯„ä¼°ä¸­å°†ç»¼åˆå¾—åˆ†ä»5.32æå‡è‡³9.23ï¼Œåœ¨æ‰€æœ‰å…­é¡¹å…³é”®æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºé™æ€RAGã€ç®€å•è®°å¿†åŠå•æ™ºèƒ½ä½“å˜ä½“ï¼Œå……åˆ†è¯æ˜äº†ååŒæ¼”åŒ–å­¦ç”Ÿç”»åƒã€çŸ¥è¯†åº“ä¸æ•™å­¦ç­–ç•¥åœ¨å¤æ‚æ•™å­¦åœºæ™¯ä¸‹çš„å“è¶Šä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00331v1",
      "published_date": "2025-11-29 05:41:57 UTC",
      "updated_date": "2025-11-29 05:41:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:58:56.249951+00:00"
    },
    {
      "arxiv_id": "2512.00329v1",
      "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning",
      "title_zh": "é¢å‘æ—¶åºè¡¨æ ¼æ¨ç†çš„è¯æ®å¼•å¯¼æ¨¡å¼è§„èŒƒåŒ–",
      "authors": [
        "Ashish Thanga",
        "Vibhu Dixit",
        "Abhilash Shankarampeta",
        "Vivek Gupta"
      ],
      "abstract": "Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\\% improvement over the baseline (68.89 EM).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¼”åŒ–ä¸­çš„åŠç»“æ„åŒ–è¡¨æ ¼ï¼Œæå‡ºäº†ä¸€ç§åŸºäºSQLçš„æ—¶é—´æ¨ç†(Temporal Tabular Reasoning)æ–¹æ³•ï¼Œæ¶µç›–äº†ä»Wikipediaä¿¡æ¯æ¡†ç”Ÿæˆ3NFæ¨¡å¼ã€SQLæŸ¥è¯¢ç”ŸæˆåŠæ‰§è¡Œçš„å®Œæ•´æµç¨‹ã€‚ç ”ç©¶çš„æ ¸å¿ƒå‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ¨¡å‹è§„æ¨¡åŒ–å‡è®¾ï¼Œè¯æ˜æ¨¡å¼è®¾è®¡(Schema Design)çš„è´¨é‡å¯¹é—®ç­”ç²¾åº¦(QA precision)çš„å½±å“æ˜¾è‘—è¶…è¿‡æ¨¡å‹å®¹é‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶ç¡®ç«‹äº†ä¸‰é¡¹åŸºäºè¯æ®çš„åŸåˆ™ï¼šä¿ç•™ä¸Šä¸‹æ–‡çš„è§„èŒƒåŒ–(normalization)ã€å‡å°‘æ­§ä¹‰çš„è¯­ä¹‰å‘½å(semantic naming)ä»¥åŠä¸€è‡´çš„æ—¶é—´é”šå®š(temporal anchoring)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨Gemini 2.5 Flashç”Ÿæˆæ¨¡å¼å¹¶ç»“åˆGemini-2.0-Flashç”ŸæˆæŸ¥è¯¢çš„æœ€ä½³é…ç½®è¾¾åˆ°äº†80.39 EMã€‚è¿™ä¸€ç»“æœç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†16.8%ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†è¯æ®å¼•å¯¼çš„æ¨¡å¼è§„èŒƒåŒ–åœ¨æå‡å¤æ‚æ—¶é—´è¡¨æ ¼æ¨ç†æ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00329v1",
      "published_date": "2025-11-29 05:40:08 UTC",
      "updated_date": "2025-11-29 05:40:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:10.136282+00:00"
    },
    {
      "arxiv_id": "2512.00325v1",
      "title": "Progressive Code Integration for Abstractive Bug Report Summarization",
      "title_zh": "é¢å‘ç”Ÿæˆå¼é”™è¯¯æŠ¥å‘Šæ‘˜è¦çš„æ¸è¿›å¼ä»£ç é›†æˆ",
      "authors": [
        "Shaira Sadia Karim",
        "Abrar Mahmud Rahim",
        "Lamia Alam",
        "Ishmam Tashdeed",
        "Lutfun Nahar Lota",
        "Md. Abu Raihan M. Kamal",
        "Md. Azam Hossain"
      ],
      "abstract": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä»¶ç¼ºé™·æŠ¥å‘Š(Bug reports)éç»“æ„åŒ–ä¸”å†—é•¿å¯¼è‡´å¼€å‘è€…éš¾ä»¥ç†è§£çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆå¼æ‘˜è¦çš„æ¸è¿›å¼ä»£ç é›†æˆæ¡†æ¶(progressive code-integration framework)ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†é•¿ä»£ç ç‰‡æ®µ(code snippets)ä¸æ–‡æœ¬å†…å®¹é€æ­¥æ•´åˆï¼Œæœ‰æ•ˆå…‹æœäº†æ ‡å‡†å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£(context window)é™åˆ¶ï¼Œä»è€Œç”Ÿæˆè¯­ä¹‰æ›´ä¸°å¯Œçš„æ‘˜è¦ã€‚å®éªŒåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šåˆ©ç”¨å…«ç§å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æµæ°´çº¿åœ¨æ€§èƒ½ä¸Šä¼˜äºæŠ½å–å¼åŸºå‡†æ¨¡å‹7.5%è‡³58.2%ï¼Œå¹¶è¾¾åˆ°äº†ä¸å½“å‰æœ€å…ˆè¿›ç”Ÿæˆå¼æ–¹æ³•(state-of-the-art abstractive methods)ç›¸å½“çš„æ°´å¹³ã€‚è¿™é¡¹ç ”ç©¶å……åˆ†å±•ç¤ºäº†ååŒåˆ©ç”¨æ–‡æœ¬ä¸ä»£ç ä¿¡æ¯åœ¨å¢å¼ºç¼ºé™·ç†è§£(bug comprehension)æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæé«˜è½¯ä»¶ç»´æŠ¤æ•ˆç‡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00325v1",
      "published_date": "2025-11-29 05:35:36 UTC",
      "updated_date": "2025-11-29 05:35:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:19.144396+00:00"
    },
    {
      "arxiv_id": "2512.00323v1",
      "title": "Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets",
      "title_zh": "47ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”æ¨¡å‹åœ¨8ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Muhammad Muneeb",
        "David B. Ascher",
        "Ahsan Baidar Bakht"
      ],
      "abstract": "Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\\%), biomedical\\_cpgQA (96.45\\%), QuAC (11.13\\%), and Question Answer Dataset (41.6\\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\\% on the IELTS dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¥è‡ª Hugging Face çš„ 47 ä¸ª Context-based question answering (CBQA) æ¨¡å‹åœ¨ 8 ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸è¿›è¡Œé¢å¤– fine-tuning çš„æƒ…å†µä¸‹è¯†åˆ«å‡ºé€šç”¨æ€§æœ€å¼ºçš„æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ SQuAD v2 æˆ– SQuAD v1 æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶ä¸­ ahotrod/electra_large_discriminator_squad2_512 æˆä¸ºæ•´ä½“è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå¹¶åœ¨ bioasq10b-factoid å’Œ biomedical_cpgQA ç­‰å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æœ€ä¼˜æˆç»©ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œæ¨¡å‹çš„è®¡ç®—æ—¶é—´ä¸ context length å’Œ model size å¯†åˆ‡ç›¸å…³ï¼Œè€Œæ¨¡å‹æ€§èƒ½é€šå¸¸ä¼šéšç€ answer length çš„å¢åŠ å’Œ context complexity çš„æå‡è€Œæœ‰æ‰€ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ©ç”¨ Genetic algorithm æ•´åˆä¸åŒæ¨¡å‹çš„é¢„æµ‹å“åº”ï¼Œæœ‰æ•ˆæå‡äº†ç³»ç»Ÿçš„æ•´ä½“å‡†ç¡®ç‡ã€‚Bert-large-uncased-whole-word-masking-finetuned-squad ä¹Ÿåœ¨ç‰¹å®šé¢†åŸŸå±•ç°äº†å¼ºåŠ²å®åŠ›ï¼Œåœ¨ IELTS æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 82% çš„å‡†ç¡®ç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨å„ç§å®é™…åº”ç”¨åœºæ™¯ä¸­å¿«é€Ÿã€é«˜æ•ˆåœ°éƒ¨ç½² CBQA æŠ€æœ¯æä¾›äº†æå…·ä»·å€¼çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00323v1",
      "published_date": "2025-11-29 05:31:45 UTC",
      "updated_date": "2025-11-29 05:31:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:11.437546+00:00"
    },
    {
      "arxiv_id": "2512.02062v1",
      "title": "Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas",
      "title_zh": "Superpixel Attackï¼šåˆ©ç”¨å›¾åƒé©±åŠ¨çš„åˆ†å‰²åŒºåŸŸå¢å¼ºé»‘ç›’å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Issa Oe",
        "Keiichiro Yamamura",
        "Hiroki Ishikura",
        "Ryo Hamahira",
        "Katsuki Fujisawa"
      ],
      "abstract": "Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é»‘ç›’å¯¹æŠ—æ”»å‡»(black-box adversarial attack)ä¸­ç°æœ‰æ–¹æ³•ä»…ä½¿ç”¨ç®€å•çŸ©å½¢åŒºåŸŸæ›´æ–°æ‰°åŠ¨çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSuperpixel Attackçš„å…¨æ–°æ”»å‡»æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å¼•å…¥äº†è¶…åƒç´ (superpixels)ä½œä¸ºå›¾åƒåˆ†å‰²åŒºåŸŸï¼Œåˆ©ç”¨å…¶åœ¨é¢œè‰²å·®å¼‚å’Œç´§å‡‘æ€§ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡æ¥ä¼˜åŒ–æ‰°åŠ¨èŒƒå›´çš„åˆ’åˆ†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é…å¥—æå‡ºäº†ä¸€ç§åä¸ºå¤šåŠŸèƒ½æœç´¢(versatile search)çš„æ–°å‹æœç´¢ç®—æ³•ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ”»å‡»æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSuperpixel Attackåœ¨å¤šä¸ªå…·æœ‰é²æ£’æ€§çš„æ¨¡å‹ä¸Šæ¯”ç°æœ‰ä¸»æµæ–¹æ³•å¹³å‡æé«˜äº†2.10%çš„æ”»å‡»æˆåŠŸç‡ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¯æ˜äº†åŸºäºå›¾åƒé©±åŠ¨çš„åŒºåŸŸåˆ’åˆ†åœ¨å¢å¼ºé»‘ç›’æ”»å‡»æ•ˆèƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶å¯¹äºè¯„ä¼°å’Œæå‡è‡ªåŠ¨é©¾é©¶ã€äººè„¸è¯†åˆ«ç­‰å®‰å…¨å…³é”®é¢†åŸŸä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02062v1",
      "published_date": "2025-11-29 05:28:52 UTC",
      "updated_date": "2025-11-29 05:28:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:32.533819+00:00"
    },
    {
      "arxiv_id": "2512.03086v1",
      "title": "Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation",
      "title_zh": "è¶…è¶Šä»£ç å¯¹ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹ä»£ç ç¿»è¯‘çš„å¯¹è¯å¼æ•°æ®ç”Ÿæˆ",
      "authors": [
        "Le Chen",
        "Nuo Xu",
        "Winson Chen",
        "Bin Lei",
        "Pei-Hung Lin",
        "Dunzhi Zhou",
        "Rajeev Thakur",
        "Caiwen Ding",
        "Ali Jannesari",
        "Chunhua Liao"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹Fortranå’ŒCUDAç­‰ä½èµ„æºç¼–ç¨‹é¢†åŸŸä¸­é«˜è´¨é‡å¹³è¡Œæ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒLLMçš„Questioner-Solverè‡ªåŠ¨åŒ–æ•°æ®é›†ç”Ÿæˆç®¡é“ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ç¼–è¯‘å™¨å’Œè¿è¡Œæ—¶åé¦ˆçš„å¤–éƒ¨çŸ¥è¯†ï¼Œä¸ä»…ç”Ÿæˆäº†ç»å•å…ƒæµ‹è¯•(unit tests)éªŒè¯çš„åŠŸèƒ½ä¸€è‡´æ€§ç¿»è¯‘ï¼Œè¿˜æ„å»ºäº†æ•è·ä»£ç ç¿»è¯‘æ¨ç†è¿‡ç¨‹çš„å¤šè½®å¯¹è¯(multi-turn dialogues)æ•°æ®é›†ã€‚åœ¨Fortranåˆ°C++å’ŒC++åˆ°CUDAçš„ä»»åŠ¡åº”ç”¨ä¸­ï¼Œè¯¥ç®¡é“æ˜¾è‘—æå‡äº†æ¨¡å‹çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå…¶ä¸­åœ¨C++åˆ°CUDAä»»åŠ¡ä¸Šçš„å•å…ƒæµ‹è¯•æˆåŠŸç‡æé«˜äº†56%ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨è¯¥æ•°æ®è¿›è¡Œå¾®è°ƒåçš„7Bè§„æ¨¡å¼€æºæ¨¡å‹åœ¨ç¼–è¯‘æˆåŠŸç‡ç­‰å…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†å‚æ•°é‡æ›´å¤§çš„ä¸“æœ‰æ¨¡å‹ç³»ç»Ÿã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03086v1",
      "published_date": "2025-11-29 05:26:53 UTC",
      "updated_date": "2025-11-29 05:26:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:23.439370+00:00"
    },
    {
      "arxiv_id": "2512.05136v1",
      "title": "Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes",
      "title_zh": "å¾®è°ƒå¿ƒç”µå›¾åŸºç¡€æ¨¡å‹ä»¥é¢„æµ‹å† çŠ¶åŠ¨è„‰ CT è¡€ç®¡é€ å½±ç»“æœ",
      "authors": [
        "Yujie Xiao",
        "Gongzhen Tang",
        "Deyun Zhang",
        "Jun Li",
        "Guangkun Nie",
        "Haoyu Wang",
        "Shun Huang",
        "Tong Liu",
        "Qinghao Zhao",
        "Kangyin Chen",
        "Shenda Hong"
      ],
      "abstract": "Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå¯è§£é‡Šçš„AI-ECGæ¨¡å‹ï¼Œé€šè¿‡å¾®è°ƒECG Foundation Modelæ¥é¢„æµ‹å† çŠ¶åŠ¨è„‰CTè¡€ç®¡é€ å½±ï¼ˆCCTAï¼‰ä¸­å››æ¡ä¸»è¦å† çŠ¶åŠ¨è„‰çš„ä¸¥é‡æˆ–å®Œå…¨ç‹­çª„ã€‚è¯¥æ¨¡å‹æ—¨åœ¨ä¸ºå† å¿ƒç—…ï¼ˆCADï¼‰ç­›æŸ¥æä¾›ä¸€ç§ä½æˆæœ¬ã€éä¾µå…¥æ€§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥å…‹æœä¼ ç»ŸCCTAå¯¹é«˜ç²¾å°–è®¾å¤‡åŠæ”¾å°„æ€§æš´éœ²çš„ä¾èµ–ã€‚åœ¨å†…éƒ¨éªŒè¯ä¸­ï¼Œæ¨¡å‹å¯¹RCAã€LMã€LADå’ŒLCXçš„é¢„æµ‹AUCåˆ†åˆ«è¾¾åˆ°äº†0.794ã€0.818ã€0.744å’Œ0.755ï¼Œä¸”åœ¨å¤–éƒ¨éªŒè¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œå…¶ä¸­LMçš„AUCè¾¾åˆ°0.971ã€‚æ€§èƒ½åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä¸´åºŠæ­£å¸¸çš„å¿ƒç”µå›¾å­é›†ä¸­ä¾ç„¶ä¿æŒç¨³å®šï¼Œè¯æ˜äº†å…¶è¶…è¶Šè¡¨è§‚å¼‚å¸¸æ£€æµ‹çš„é²æ£’æ€§ã€‚é€šè¿‡é£é™©åˆ†å±‚å’Œæ³¢å½¢å·®å¼‚çš„å¯è§£é‡Šæ€§åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†ä¸å† çŠ¶åŠ¨è„‰ç‹­çª„ç›¸å…³çš„å…³é”®ç”µç”Ÿç†åŒºåŸŸåŠç‰¹å¾ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ä»…ä¸ºä¸´åºŠå†³ç­–æä¾›äº†é«˜æ•ˆçš„é£é™©è¯„ä¼°å·¥å…·ï¼Œä¹Ÿä¸ºç†è§£ECGä¸å† çŠ¶åŠ¨è„‰ç—…å˜ä¹‹é—´çš„ç”µç”Ÿç†å…³è”æä¾›äº†æ–°çš„ç§‘å­¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.05136v1",
      "published_date": "2025-11-29 05:21:24 UTC",
      "updated_date": "2025-11-29 05:21:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:37.653971+00:00"
    },
    {
      "arxiv_id": "2512.00319v2",
      "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs",
      "title_zh": "RL-Structï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¯é ç»“æ„åŒ–è¾“å‡ºçš„è½»é‡çº§å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Ruike Hu",
        "Shulei Wu"
      ],
      "abstract": "The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RL-Structï¼Œä¸€ä¸ªæ—¨åœ¨å¼¥åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¦‚ç‡ç”Ÿæˆä¸ç¡®å®šæ€§æ¨¡å¼éœ€æ±‚ä¹‹é—´ç»“æ„é¸¿æ²Ÿï¼ˆStructure Gapï¼‰çš„è½»é‡çº§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¢¯åº¦æ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGradient Regularized Policy Optimization, GRPOï¼‰å’Œåˆ†å±‚å¥–åŠ±å‡½æ•°ï¼ˆhierarchical reward functionï¼‰å¼•å¯¼æ¨¡å‹å¯¹é½ç»“æ„åŒ–çº¦æŸã€‚ç”±äºç§»é™¤äº†è¯„è®ºè€…ç½‘ç»œï¼ˆcritic networkï¼‰ï¼ŒRL-Struct ç›¸æ¯” PPO ç®—æ³•å¯é™ä½ 38% çš„å³°å€¼æ˜¾å­˜ï¼ˆVRAMï¼‰å ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤æ‚ JSON ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶å®ç°äº† 89.7% çš„ç»“æ„å‡†ç¡®ç‡å’Œ 92.1% çš„æœ‰æ•ˆæ€§ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è§‚å¯Ÿåˆ°äº†æ¶Œç°è¯¾ç¨‹ï¼ˆemergent curriculumï¼‰ç°è±¡ï¼Œå³æ¨¡å‹åœ¨è‡ªç»„ç»‡å­¦ä¹ è¿‡ç¨‹ä¸­ä¼šä¼˜å…ˆå­¦ä¹ è¯­æ³•è€Œåå­¦ä¹ è¯­ä¹‰ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–å·¥ä½œæµä¸­çš„å¯é ç»“æ„åŒ–è¾“å‡ºæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¼€æºçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 9 figures. Model is available at https://huggingface.co/Freakz3z/Qwen-JSON",
      "pdf_url": "https://arxiv.org/pdf/2512.00319v2",
      "published_date": "2025-11-29 04:47:14 UTC",
      "updated_date": "2025-12-15 15:32:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:39.243466+00:00"
    },
    {
      "arxiv_id": "2512.00311v1",
      "title": "Tracing Mathematical Proficiency Through Problem-Solving Processes",
      "title_zh": "é€šè¿‡è§£é¢˜è¿‡ç¨‹è¿½è¸ªæ•°å­¦ç´ å…»",
      "authors": [
        "Jungyang Park",
        "Suho Kang",
        "Jaewoo Park",
        "Jaehong Kim",
        "Jaewoo Shin",
        "Seonjoon Park",
        "Youngjae Yu"
      ],
      "abstract": "Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿ Knowledge Tracing (KT) æ–¹æ³•ä»…ä¾èµ–ä½œç­”æ­£ç¡®æ€§ã€ç¼ºä¹è§£é‡Šæ€§ä¸”å¿½ç•¥è§£é¢˜è¿‡ç¨‹ä¸°å¯Œä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆè§£é¢˜è¿‡ç¨‹çš„ KT-PSP (Knowledge Tracing Leveraging Problem-Solving Process) æ–¹æ³•ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°æ•æ‰å­¦ç”Ÿçš„æ•°å­¦æ°´å¹³ã€‚ç ”ç©¶è€…æ¨å‡ºäº†ä¸“é—¨è®¾è®¡çš„ KT-PSP-25 æ•°æ®é›†ï¼Œå¹¶æ„å»ºäº†åä¸º StatusKT çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿ-æ•™å¸ˆçš„ä¸‰é˜¶æ®µ LLM pipeline å°†å­¦ç”Ÿçš„ Mathematical Proficiency (MP) ä½œä¸ºä¸­é—´ä¿¡å·è¿›è¡Œæå–ã€‚è¯¥æµæ°´çº¿é¦–å…ˆç”±æ•™å¸ˆ LLM æå–é¢˜ç›®ç‰¹å®šçš„èƒ½åŠ›æŒ‡æ ‡ï¼Œéšåå­¦ç”Ÿ LLM åŸºäºå…¶å®é™…è§£é¢˜è¿‡ç¨‹ç”Ÿæˆå“åº”ï¼Œæœ€åç”±æ•™å¸ˆ LLM è¯„ä¼°å„é¡¹æŒ‡æ ‡çš„æŒæ¡ç¨‹åº¦ã€‚åœ¨ KT-PSP-25 ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒStatusKT æ˜¾è‘—æå‡äº†ç°æœ‰ KT æ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒStatusKT é€šè¿‡å¯¹å­¦ç”Ÿæ•°å­¦èƒ½åŠ›çš„æ˜¾å¼å»ºæ¨¡ï¼Œä¸ºé¢„æµ‹ç»“æœæä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„è¯¦ç»†è¯´æ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00311v1",
      "published_date": "2025-11-29 04:12:06 UTC",
      "updated_date": "2025-11-29 04:12:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:52.150363+00:00"
    },
    {
      "arxiv_id": "2512.00307v2",
      "title": "Adversarial Signed Graph Learning with Differential Privacy",
      "title_zh": "æ»¡è¶³å·®åˆ†éšç§çš„å¯¹æŠ—å¼ç¬¦å·å›¾å­¦ä¹ ",
      "authors": [
        "Haobin Ke",
        "Sen Zhang",
        "Qingqing Ye",
        "Xun Ran",
        "Haibo Hu"
      ],
      "abstract": "Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦ç¬¦å·å›¾(Signed graphs)åœ¨ç¤¾äº¤ç½‘ç»œå»ºæ¨¡ä¸­é¢ä¸´çš„éšç§æ³„éœ²é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å·®åˆ†éšç§(Differential Privacy, DP)æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»å›¾ç»“æ„æ—¶ï¼Œå®¹æ˜“å¯¼è‡´ç¬¦å·æ¨ç†çš„è¯¯å·®çº§è”ä»¥åŠè¿‡é«˜çš„æ¢¯åº¦æ•æ„Ÿåº¦ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ASGLï¼Œä¸€ç§å…·æœ‰éšç§ä¿æŠ¤èƒ½åŠ›çš„å¯¹æŠ—æ€§å¸¦ç¬¦å·å›¾å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°èŠ‚ç‚¹çº§DPçš„åŒæ—¶ä¿æŒé«˜æ•°æ®æ•ˆç”¨ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¸¦ç¬¦å·å›¾åˆ†è§£ä¸ºæ­£è´Ÿå­å›¾ï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦æ‰°åŠ¨çš„å¯¹æŠ—æ¨¡å—æ¥é€¼è¿‘çœŸå®çš„è¿æ¥åˆ†å¸ƒï¼Œä»è€Œæœ‰æ•ˆç¼“è§£æ¨ç†è¯¯å·®å¹¶é™ä½æ•æ„Ÿåº¦ã€‚æ­¤å¤–ï¼ŒASGL ç»“åˆå¹³è¡¡ç†è®º(balance theory)è®¾è®¡äº†ä¸€ç§çº¦æŸå¹¿åº¦ä¼˜å…ˆæœç´¢æ ‘(constrained breadth-first search tree)ç­–ç•¥ï¼Œç”¨äºå‡†ç¡®è¯†åˆ«ç”ŸæˆèŠ‚ç‚¹å¯¹ä¹‹é—´çš„è¾¹ç¬¦å·ã€‚è¯¥ç­–ç•¥é€šè¿‡æ¢¯åº¦è§£è€¦è¿›ä¸€æ­¥ä¼˜åŒ–äº†éšç§ä¿æŠ¤æ€§èƒ½ï¼Œå®éªŒè¯æ˜ ASGL åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å‡å®ç°äº†ä¼˜å¼‚çš„éšç§ä¸æ•ˆç”¨æƒè¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIGKDD 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.00307v2",
      "published_date": "2025-11-29 04:02:48 UTC",
      "updated_date": "2025-12-12 08:57:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T11:59:53.285557+00:00"
    },
    {
      "arxiv_id": "2512.00306v1",
      "title": "VCWorld: A Biological World Model for Virtual Cell Simulation",
      "title_zh": "VCWorldï¼šé¢å‘è™šæ‹Ÿç»†èƒæ¨¡æ‹Ÿçš„ç”Ÿç‰©ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Zhijian Wei",
        "Runze Ma",
        "Zichen Wang",
        "Zhongmin Li",
        "Shuotong Song",
        "Shuangjia Zheng"
      ],
      "abstract": "Virtual cell modeling aims to predict cellular responses to perturbations. Existing virtual cell models rely heavily on large-scale single-cell datasets, learning explicit mappings between gene expression and perturbations. Although recent models attempt to incorporate multi-source biological information, their generalization remains constrained by data quality, coverage, and batch effects. More critically, these models often function as black boxes, offering predictions without interpretability or consistency with biological principles, which undermines their credibility in scientific research. To address these challenges, we present VCWorld, a cell-level white-box simulator that integrates structured biological knowledge with the iterative reasoning capabilities of large language models to instantiate a biological world model. VCWorld operates in a data-efficient manner to reproduce perturbation-induced signaling cascades and generates interpretable, stepwise predictions alongside explicit mechanistic hypotheses. In drug perturbation benchmarks, VCWorld achieves state-of-the-art predictive performance, and the inferred mechanistic pathways are consistent with publicly available biological evidence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VCWorldï¼Œä¸€ç§æ—¨åœ¨é¢„æµ‹ç»†èƒå¯¹æ‰°åŠ¨(perturbations)ååº”çš„ç»†èƒçº§ç™½ç›’æ¨¡æ‹Ÿå™¨(white-box simulator)ã€‚é’ˆå¯¹ç°æœ‰è™šæ‹Ÿç»†èƒæ¨¡å‹è¿‡åº¦ä¾èµ–å¤§è§„æ¨¡å•ç»†èƒæ•°æ®ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼ŒVCWorldé€šè¿‡å°†ç»“æ„åŒ–ç”Ÿç‰©çŸ¥è¯†ä¸å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„è¿­ä»£æ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªç”Ÿç‰©ä¸–ç•Œæ¨¡å‹(biological world model)ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä»¥æ•°æ®é«˜æ•ˆ(data-efficient)çš„æ–¹å¼é‡ç°ç”±æ‰°åŠ¨è¯±å¯¼çš„ä¿¡å·çº§è”è¿‡ç¨‹ï¼Œå¹¶æä¾›å…·æœ‰å¯è§£é‡Šæ€§çš„é€æ­¥é¢„æµ‹ä»¥åŠæ˜ç¡®çš„æœºåˆ¶å‡è®¾ã€‚åœ¨è¯ç‰©æ‰°åŠ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVCWorldå–å¾—äº†æœ€å…ˆè¿›(SOTA)çš„é¢„æµ‹æ€§èƒ½ï¼Œä¸”å…¶æ¨æ–­å‡ºçš„æœºåˆ¶è·¯å¾„ä¸å…¬å¼€çš„ç”Ÿç‰©å­¦è¯æ®ä¿æŒé«˜åº¦ä¸€è‡´ã€‚",
      "categories": [
        "q-bio.CB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.CB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00306v1",
      "published_date": "2025-11-29 04:02:24 UTC",
      "updated_date": "2025-11-29 04:02:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:02.641941+00:00"
    },
    {
      "arxiv_id": "2512.00305v1",
      "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning",
      "title_zh": "ChartPointï¼šåˆ©ç”¨å®šä½åæ€å¼•å¯¼ MLLMs è¿›è¡Œå›¾è¡¨æ¨ç†",
      "authors": [
        "Zhengzhuo Xu",
        "SiNan Du",
        "Yiyan Qi",
        "SiwenLu",
        "Chengjin Xu",
        "Chun Yuan",
        "Jian Guo"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\\% on ChartBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾è¡¨ç†è§£ä¸­è¿‡åº¦ä¾èµ–OCRä¸”ç¼ºä¹è§†è§‰æ„ŸçŸ¥ï¼Œå¯¼è‡´åœ¨æ–‡æœ¬æ ‡æ³¨ç¨€ç–æ—¶å®¹æ˜“äº§ç”Ÿæ•°å€¼å¹»è§‰çš„é—®é¢˜ï¼Œæå‡ºäº†PointCoTæ¡†æ¶ã€‚PointCoTé€šè¿‡å°†åå°„å¼äº¤äº’é›†æˆåˆ°å›¾è¡¨æ¨ç†çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰ä¸­ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆè¾¹ç•Œæ¡†ï¼ˆBounding Boxesï¼‰å¹¶åŸºäºä½ç½®æ ‡æ³¨é‡æ–°æ¸²æŸ“å›¾è¡¨ã€‚è¿™ç§æ–¹æ³•åœ¨æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸è§†è§‰åŸºç¡€ï¼ˆVisual Groundingï¼‰åŒºåŸŸä¹‹é—´å»ºç«‹äº†æ˜ç¡®è”ç³»ï¼Œæœ‰æ•ˆå¼¥åˆäº†é€»è¾‘æ¨ç†ä¸è§†è§‰æ„ŸçŸ¥ä¹‹é—´çš„é¸¿æ²Ÿã€‚ç ”ç©¶å›¢é˜Ÿè¿˜é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºäº†ChartPoint-SFT-62kæ•°æ®é›†ï¼ŒåŒ…å«1.92ä¸‡ä¸ªé›†æˆCoTä¸è§†è§‰æ ‡æ³¨çš„é«˜è´¨é‡æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæ­¤å¾®è°ƒçš„ChartPointQ2å’ŒChartPointQ2.5æ¨¡å‹åœ¨å¤šä¸ªå›¾è¡¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ¨¡å‹ï¼ˆSOTAï¼‰ï¼Œåœ¨ChartBenchä¸Šçš„æ€§èƒ½æå‡è¾¾5.04%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¼•å…¥åœ°åŸºåå°„ï¼ˆGrounding Reflectionï¼‰æœºåˆ¶å¯¹äºå¢å¼ºMLLMså›¾è¡¨æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.00305v1",
      "published_date": "2025-11-29 04:01:55 UTC",
      "updated_date": "2025-11-29 04:01:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:01.650898+00:00"
    },
    {
      "arxiv_id": "2512.00303v1",
      "title": "Gradient Inversion in Federated Reinforcement Learning",
      "title_zh": "è”é‚¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢¯åº¦åæ¼”",
      "authors": [
        "Shenghong He"
      ],
      "abstract": "Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient sharing.However, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training data.Compared to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition distribution.Theoretically, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition dynamics.Extensive experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Federated Reinforcement Learning (FRL)åœ¨æ¢¯åº¦å…±äº«è¿‡ç¨‹ä¸­çš„éšç§æ³„éœ²é£é™©ï¼Œæ¢è®¨äº†æ”»å‡»è€…å¦‚ä½•é€šè¿‡å…±äº«æ¢¯åº¦é‡æ„æœ¬åœ°è®­ç»ƒæ•°æ®ã€‚ç”±äºFRLçš„æ•°æ®é‡æ„ä¸ä»…éœ€åŒ¹é…æ¢¯åº¦ï¼Œè¿˜éœ€ç¬¦åˆç¯å¢ƒçœŸå®çš„transition dynamicsï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºRegularization Gradient Inversion Attack (RGIA)çš„æ–°å‹æ”»å‡»æ–¹æ³•ã€‚RGIAåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥äº†åŸºäºå…ˆéªŒçŸ¥è¯†çš„çŠ¶æ€ã€å¥–åŠ±å’Œè½¬ç§»åŠ¨æ€æ­£åˆ™åŒ–ï¼Œç¡®ä¿é‡æ„æ•°æ®ä¸çœŸå®çš„æ•°æ®è½¬ç§»åˆ†å¸ƒä¿æŒä¸€è‡´ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥æ­£åˆ™åŒ–é¡¹èƒ½å°†è§£ç©ºé—´ä»åŒ…å«ä¼ªè§£çš„é›†åˆç¼©å°åˆ°æ»¡è¶³æ¢¯åº¦åŒ¹é…ä¸çœŸå®ç‰©ç†è§„å¾‹çš„å—é™å­é›†ã€‚é€šè¿‡åœ¨æ§åˆ¶ä»»åŠ¡å’Œè‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯ï¼ŒRGIAèƒ½å¤Ÿæœ‰æ•ˆçº¦æŸé‡æ„æ•°æ®çš„åˆ†å¸ƒå¹¶æˆåŠŸæ¢å¤æœ¬åœ°ç§æœ‰æ•°æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00303v1",
      "published_date": "2025-11-29 03:54:42 UTC",
      "updated_date": "2025-11-29 03:54:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:20.537918+00:00"
    },
    {
      "arxiv_id": "2512.00294v1",
      "title": "Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR",
      "title_zh": "Words into Worldï¼šå¢å¼ºç°å®ç¯å¢ƒä¸‹é¢å‘è¯­è¨€å¼•å¯¼ç©ºé—´æ£€ç´¢çš„ä»»åŠ¡è‡ªé€‚åº”æ™ºèƒ½ä½“",
      "authors": [
        "Lixing Guo",
        "Tobias HÃ¶llerer"
      ],
      "abstract": "Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–çš„å¢å¼ºç°å® (AR) æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ AR ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ã€å¼€æ”¾è¯æ±‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ—¶çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ä¸å®šä½è§†è§‰æ¨¡å‹ç›¸ç»“åˆï¼Œé€šè¿‡æ„å»ºç¼–ç äº†ä¹ç§å…³ç³»ç±»å‹çš„åŠ¨æ€ AR åœºæ™¯å›¾ (AR scene graphs)ï¼Œå®ç°äº†ç‰©ç†ç¯å¢ƒä¸­çš„ç©ºé—´å…³ç³»æ¨ç†å’Œè¯­è¨€å¼•å¯¼çš„ç©ºé—´æ£€ç´¢ã€‚è¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒç”¨åæ ‡æ„ŸçŸ¥ (coordinate-aware) æ„ŸçŸ¥å·¥å…·ï¼Œæ”¯æŒä»ç®€å•ç‰©ä½“è¯†åˆ«åˆ°å¤šå¯¹è±¡å…³ç³»æ¨ç†çš„å„ç±»æ“ä½œï¼Œå¹¶æä¾›ç±³çº§ç²¾åº¦çš„ 3D é”šç‚¹ã€‚é€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„æ„Ÿå…´è¶£åŒºåŸŸ (ROI) çªå‡ºæ˜¾ç¤ºå’Œä¸Šä¸‹æ–‡ç©ºé—´æ£€ç´¢ï¼Œç³»ç»Ÿåœ¨æ”¯æŒäººæœºåä½œä¼˜åŒ–çš„åŒæ—¶ï¼Œèƒ½å°†äººç±»æ³¨æ„åŠ›å¼•å¯¼è‡³ä¿¡æ¯å¯†é›†åŒºåŸŸã€‚è¿™ç§æ¨¡å—åŒ–æ¶æ„å…è®¸åœ¨ä¸è¿›è¡Œé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å³æ’å³ç”¨å„ç§è§†è§‰è¯­è¨€æ¨¡å‹ (vision-language models)ï¼Œä½¿ AR æ™ºèƒ½ä½“æˆä¸ºå¢å¼ºå¤§æ¨¡å‹ç°å®ä¸–ç•Œç©ºé—´æ™ºèƒ½çš„ä¸­ä»‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† GroundedAR-Bench è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¡¡é‡ä¸åŒç¯å¢ƒä¸‹è¯­è¨€é©±åŠ¨çš„çœŸå®ä¸–ç•Œå®šä½ä¸å…³ç³»æ¥åœ° (relation grounding) æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00294v1",
      "published_date": "2025-11-29 03:29:15 UTC",
      "updated_date": "2025-11-29 03:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:18.839393+00:00"
    },
    {
      "arxiv_id": "2512.00293v1",
      "title": "FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting",
      "title_zh": "FiCoTSï¼šå¤§è¯­è¨€æ¨¡å‹å¢å¼ºçš„ç”±ç»†åˆ°ç²—åˆ†å±‚è·¨æ¨¡æ€äº¤äº’æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Yafei Lyu",
        "Hao Zhou",
        "Lu Zhang",
        "Xu Yang",
        "Zhiyong Liu"
      ],
      "abstract": "Time series forecasting is central to data analysis and web technologies. The recent success of Large Language Models (LLMs) offers significant potential for this field, especially from the cross-modality aspect. Most methods adopt an LLM-as-Predictor paradigm, using LLM as the forecasting backbone and designing modality alignment mechanisms to enable LLM to understand time series data. However, the semantic information in the two modalities of time series and text differs significantly, making it challenging for LLM to fully understand time series data. To mitigate this challenge, our work follows an LLM-as-Enhancer paradigm to fully utilize the advantage of LLM in text understanding, where LLM is only used to encode text modality to complement time series modality. Based on this paradigm, we propose FiCoTS, an LLM-enhanced fine-to-coarse framework for multimodal time series forecasting. Specifically, the framework facilitates progressive cross-modality interaction by three levels in a fine-to-coarse scheme: First, in the token-level modality alignment module, a dynamic heterogeneous graph is constructed to filter noise and align time series patches with text tokens; Second, in the feature-level modality interaction module, a global cross-attention mechanism is introduced to enable each time series variable to connect with relevant textual contexts; Third, in the decision-level modality fusion module, we design a gated network to adaptively fuse the results of the two modalities for robust predictions. These three modules work synergistically to let the two modalities interact comprehensively across three semantic levels, enabling textual information to effectively support temporal prediction. Extensive experiments on seven real-world benchmarks demonstrate that our model achieves state-of-the-art performance. The codes will be released publicly.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FiCoTSï¼Œä¸€ç§é‡‡ç”¨LLM-as-EnhancerèŒƒå¼çš„å¤šæ¨¡æ€æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸLLM-as-Predictoræ–¹æ³•ä¸­æ—¶é—´åºåˆ—ä¸æ–‡æœ¬æ¨¡æ€è¯­ä¹‰å·®å¼‚å¤§ã€å¯¹é½å›°éš¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±ç»†åˆ°ç²—(Fine-to-Coarse)åœ°æ„å»ºäº†ä¸‰ä¸ªå±‚çº§çš„è·¨æ¨¡æ€äº¤äº’æœºåˆ¶ï¼Œä»¥å……åˆ†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ–‡æœ¬ç†è§£æ–¹é¢çš„ä¼˜åŠ¿ã€‚åœ¨Tokençº§å¯¹é½æ¨¡å—ä¸­ï¼Œé€šè¿‡åŠ¨æ€å¼‚æ„å›¾(Dynamic Heterogeneous Graph)è¿‡æ»¤å™ªå£°å¹¶å®ç°æ—¶é—´åºåˆ—åˆ†å—ä¸æ–‡æœ¬Tokençš„å¯¹é½ï¼›åœ¨ç‰¹å¾çº§äº¤äº’æ¨¡å—ä¸­ï¼Œåˆ©ç”¨å…¨å±€äº¤å‰æ³¨æ„åŠ›æœºåˆ¶(Global Cross-attention)å°†æ—¶é—´åºåˆ—å˜é‡ä¸ç›¸å…³æ–‡æœ¬ä¸Šä¸‹æ–‡ç›¸è¿ï¼›åœ¨å†³ç­–çº§èåˆæ¨¡å—ä¸­ï¼Œåˆ™é‡‡ç”¨é—¨æ§ç½‘ç»œ(Gated Network)è‡ªé€‚åº”æ•´åˆåŒæ¨¡æ€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFiCoTSåœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(State-of-the-Art)çš„æ€§èƒ½ï¼Œæœ‰æ•ˆéªŒè¯äº†å±‚çº§åŒ–è·¨æ¨¡æ€äº¤äº’åœ¨æå‡æ—¶é—´åºåˆ—é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00293v1",
      "published_date": "2025-11-29 03:17:26 UTC",
      "updated_date": "2025-11-29 03:17:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:12.043616+00:00"
    },
    {
      "arxiv_id": "2512.00290v1",
      "title": "EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education",
      "title_zh": "EduEvalï¼šé¢å‘ä¸­å›½æ•™è‚²å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å±‚æ¬¡åŒ–è®¤çŸ¥åŸºå‡†",
      "authors": [
        "Guoqing Ma",
        "Jia Zhu",
        "Hanghui Guo",
        "Weijie Shi",
        "Yue Cui",
        "Jiawei Shen",
        "Zilong Li",
        "Yidan Liang"
      ],
      "abstract": "Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EduEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸­å›½ K-12 æ•™è‚²é¢†åŸŸè¡¨ç°çš„å±‚æ¬¡åŒ–è®¤çŸ¥åŸºå‡†ã€‚ç ”ç©¶è€…æå‡ºäº† EduAbility Taxonomy æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆ Bloom's Taxonomy å’Œ Webb's Depth of Knowledgeï¼Œä»è®°å¿† (Memorization)ã€ç†è§£ (Understanding)ã€åº”ç”¨ (Application)ã€æ¨ç† (Reasoning)ã€åˆ›é€ åŠ› (Creativity) å’Œä¼¦ç† (Ethics) å…­ä¸ªç»´åº¦æ„å»ºä»»åŠ¡ã€‚EduEval åŒ…å«è¶…è¿‡ 11,000 ä¸ªçœŸå®è€ƒé¢˜ã€è¯¾å ‚å¯¹è¯å’Œå­¦ç”Ÿä½œæ–‡ï¼Œæ¶µç›–äº†ä»å°å­¦åˆ°é«˜ä¸­çš„ 24 ç§ä¸åŒä»»åŠ¡ç±»å‹ã€‚å¯¹ 14 ä¸ªä¸»æµ LLMs çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨äº‹å®æ€§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¯¾å ‚å¯¹è¯åˆ†ç±» (Classroom dialogue classification) å’Œåˆ›é€ æ€§ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ã€‚å®éªŒè¿˜å‘ç°éƒ¨åˆ†å¼€æºæ¨¡å‹åœ¨å¤æ‚æ•™è‚²æ¨ç†ä¸Šè¶…è¶Šäº†é—­æºç³»ç»Ÿï¼Œä¸” Few-shot æç¤ºçš„æ•ˆæœåœ¨ä¸åŒè®¤çŸ¥ç»´åº¦é—´è¡¨ç°ä¸ä¸€ã€‚è¯¥åŸºå‡†ä¸ºå¼€å‘å’Œä¼˜åŒ–é€‚ç”¨äºä¸­å›½æ•™è‚²åœºæ™¯çš„ä¸“ç”¨å¤§æ¨¡å‹æä¾›äº†é’ˆå¯¹æ€§çš„è¯„ä¼°æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00290v1",
      "published_date": "2025-11-29 03:09:50 UTC",
      "updated_date": "2025-11-29 03:09:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:17.255384+00:00"
    },
    {
      "arxiv_id": "2512.00287v1",
      "title": "RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals",
      "title_zh": "RealApplianceï¼šä½¿é«˜ä¿çœŸå®¶ç”µèµ„äº§å…·å¤‡ä¸çœŸå®æ‰‹å†Œå¯¹é½çš„å¯æ§æ€§ä¸å¯æ“ä½œæ€§",
      "authors": [
        "Yuzheng Gao",
        "Yuxing Long",
        "Lei Kang",
        "Yuchong Guo",
        "Ziyan Yu",
        "Shangqing Mao",
        "Jiyao Zhang",
        "Ruihai Wu",
        "Dongjiang Li",
        "Hui Shen",
        "Hao Dong"
      ],
      "abstract": "Existing appliance assets suffer from poor rendering, incomplete mechanisms, and misalignment with manuals, leading to simulation-reality gaps that hinder appliance manipulation development. In this work, we introduce the RealAppliance dataset, comprising 100 high-fidelity appliances with complete physical, electronic mechanisms, and program logic aligned with their manuals. Based on these assets, we propose the RealAppliance-Bench benchmark, which evaluates multimodal large language models and embodied manipulation planning models across key tasks in appliance manipulation planning: manual page retrieval, appliance part grounding, open-loop manipulation planning, and closed-loop planning adjustment. Our analysis of model performances on RealAppliance-Bench provides insights for advancing appliance manipulation research",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å®¶ç”µèµ„äº§åœ¨æ¸²æŸ“è´¨é‡ã€æœºæ¢°ç»“æ„å®Œæ•´æ€§ä»¥åŠä¸è¯´æ˜ä¹¦ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨çš„ä¸è¶³ï¼Œæå‡ºäº†RealApplianceæ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥åˆå®¶ç”µæ“çºµé¢†åŸŸçš„ä»¿çœŸä¸ç°å®å·®è·(simulation-reality gaps)ã€‚è¯¥æ•°æ®é›†åŒ…å«100ä¸ªé«˜ä¿çœŸå®¶ç”µèµ„äº§ï¼Œä¸ä»…å…·å¤‡å®Œæ•´çš„ç‰©ç†å’Œç”µå­æœºåˆ¶ï¼Œè¿˜å®ç°äº†ç¨‹åºé€»è¾‘ä¸çœŸå®è¯´æ˜ä¹¦(manuals)çš„ç²¾å‡†å¯¹é½ã€‚åŸºäºè¿™äº›èµ„äº§ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†RealAppliance-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œå…·èº«æ“çºµè§„åˆ’æ¨¡å‹åœ¨è¯´æ˜ä¹¦é¡µé¢æ£€ç´¢(manual page retrieval)ã€é›¶ä»¶å®šä½(appliance part grounding)åŠå¼€ç¯ä¸é—­ç¯æ“çºµè§„åˆ’ç­‰å…³é”®ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å®éªŒåˆ†ææ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å®¶ç”µäº¤äº’ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥å…·èº«æ™ºèƒ½åœ¨å®¶åº­ç¯å¢ƒä¸‹çš„åº”ç”¨ç ”ç©¶æä¾›äº†é‡è¦çš„èµ„äº§æ”¯æŒå’Œè¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00287v1",
      "published_date": "2025-11-29 02:55:20 UTC",
      "updated_date": "2025-11-29 02:55:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:24.246960+00:00"
    },
    {
      "arxiv_id": "2512.00283v2",
      "title": "BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models",
      "title_zh": "BioArcï¼šé¢å‘ç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹çš„æœ€ä¼˜ç¥ç»æ¶æ„æ¢ç´¢",
      "authors": [
        "Yi Fang",
        "Haoran Xu",
        "Jiaxin Han",
        "Sirui Ding",
        "Yizhi Wang",
        "Yue Wang",
        "Xuan Wang"
      ],
      "abstract": "Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BioArcï¼Œä¸€ä¸ªæ—¨åœ¨ä¸ºç”Ÿç‰©å¤§æ¨¡å‹(Biological Foundation Models)å‘ç°æœ€ä¼˜ç¥ç»æ¶æ„çš„åˆ›æ–°æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹ç›´æ¥å¥—ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æˆ–è®¡ç®—æœºè§†è§‰æ¶æ„è€Œéš¾ä»¥æ•æ‰ç”Ÿç‰©æ•°æ®ç‰¹æœ‰ç†åŒ–æ€§è´¨å’Œç»“æ„ç‰¹æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¥ç»æ¶æ„æœç´¢(Neural Architecture Search, NAS)æŠ€æœ¯ï¼Œåœ¨å¤šä¸ªç”Ÿç‰©æ¨¡æ€ä¸Šç³»ç»Ÿåœ°æ¢ç´¢äº†åºå¤§çš„æ¶æ„è®¾è®¡ç©ºé—´ï¼Œå¹¶æ·±å…¥åˆ†æäº†æ¶æ„ã€åˆ†è¯(Tokenization)ä¸è®­ç»ƒç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡è¿™ç§å¤§è§„æ¨¡åˆ†æï¼ŒBioArcè¯†åˆ«å‡ºäº†æ–°å‹é«˜æ€§èƒ½æ¶æ„ï¼Œå¹¶æ€»ç»“å‡ºä¸€å¥—å®è¯è®¾è®¡åŸåˆ™ä»¥æŒ‡å¯¼æœªæ¥çš„æ¨¡å‹å¼€å‘ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†å¤šç§æ¶æ„é¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿé’ˆå¯¹æ–°çš„ç”Ÿç‰©ä»»åŠ¡å¿«é€Ÿä¸”é«˜æ•ˆåœ°é¢„æµ‹æœ€ä¼˜æ¶æ„ã€‚æ€»ä½“è€Œè¨€ï¼ŒBioArcä¸ºå¼€å‘ä¸‹ä¸€ä»£ä»»åŠ¡ç‰¹å®šå‹åŠé€šç”¨å‹ç”Ÿç‰©åŸºç¡€æ¨¡å‹æä¾›äº†åŸºç¡€èµ„æºå’ŒåŸåˆ™æ€§æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00283v2",
      "published_date": "2025-11-29 02:36:54 UTC",
      "updated_date": "2025-12-02 14:46:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:26.543628+00:00"
    },
    {
      "arxiv_id": "2512.02061v1",
      "title": "Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting",
      "title_zh": "Ada-MoGEï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„è‡ªé€‚åº”é«˜æ–¯æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Zhenliang Ni",
        "Xiaowen Ma",
        "Zhenkai Wu",
        "Shuai Xiao",
        "Han Shu",
        "Xinghao Chen"
      ],
      "abstract": "Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Ada-MoGEï¼Œä¸€ç§ç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„Adaptive Mixture of Gaussian Expertæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸMoEæ¨¡å‹å› ä¸“å®¶æ•°é‡å›ºå®šè€Œéš¾ä»¥åº”å¯¹é¢‘ç‡åç§»å¯¼è‡´çš„coverage imbalanceé—®é¢˜ã€‚Ada-MoGEé€šè¿‡æ•´åˆspectral intensityå’Œfrequency responseï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ•°æ®çš„é¢‘ç‡åˆ†å¸ƒè‡ªé€‚åº”åœ°ç¡®å®šä¸“å®¶æ•°é‡ï¼Œä»è€Œæœ‰æ•ˆå¹³è¡¡äº†ä¿¡æ¯å®Œæ•´æ€§ä¸æŠ—å™ªèƒ½åŠ›ï¼Œé¿å…äº†ä¸“å®¶è¿‡å°‘å¯¼è‡´çš„ä¿¡æ¯é—æ¼æˆ–è¿‡å¤šå¼•å…¥çš„noiseã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨Gaussian band-pass filteringå¯¹é¢‘åŸŸç‰¹å¾è¿›è¡Œå¹³æ»‘åˆ†è§£ï¼Œé˜²æ­¢äº†ç›´æ¥band truncationäº§ç”Ÿçš„å™ªå£°å¹²æ‰°ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAda-MoGEåœ¨å…­ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†State-of-the-artçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨ä»…æœ‰0.2 millionå‚æ•°é‡çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„é¢„æµ‹è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨è®¡ç®—æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦ä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02061v1",
      "published_date": "2025-11-29 02:08:42 UTC",
      "updated_date": "2025-11-29 02:08:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:48.574501+00:00"
    },
    {
      "arxiv_id": "2512.00275v1",
      "title": "HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention",
      "title_zh": "HIMOSAï¼šåŸºäºå±‚æ¬¡åŒ–ç¨€ç–æ³¨æ„åŠ›æ··åˆçš„é«˜æ•ˆé¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡",
      "authors": [
        "Yi Liu",
        "Yi Wan",
        "Xinyi Liu",
        "Qiong Wu",
        "Panwang Xia",
        "Xuejun Huang",
        "Yongjun Zhang"
      ],
      "abstract": "In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºHIMOSAçš„è½»é‡çº§é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆRemote Sensing Image Super-Resolutionï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¾å®³ç›‘æµ‹ç­‰åº”ç”¨ä¸­æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚HIMOSAå……åˆ†åˆ©ç”¨é¥æ„Ÿå½±åƒçš„å›ºæœ‰å†—ä½™ï¼Œå¼•å…¥äº†å†…å®¹æ„ŸçŸ¥ç¨€ç–æ³¨æ„åŠ›ï¼ˆContent-aware Sparse Attentionï¼‰æœºåˆ¶ï¼Œåœ¨ç¡®ä¿é«˜ç²¾åº¦é‡å»ºçš„åŒæ—¶å®ç°äº†å¿«é€Ÿæ¨ç†ã€‚ä¸ºæ•æ‰é¥æ„Ÿå½±åƒä¸­çš„å¤šå°ºåº¦é‡å¤æ¨¡å¼ï¼Œè¯¥æ–¹æ³•è¿›ä¸€æ­¥é‡‡ç”¨äº†å±‚æ¬¡åŒ–çª—å£æ‰©å±•ï¼ˆHierarchical Window Expansionï¼‰æŠ€æœ¯ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚æ³¨æ„åŠ›ç¨€ç–æ€§æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚åœ¨å¤šä¸ªä¸»æµé¥æ„Ÿæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒHIMOSAåœ¨ç»´æŒæé«˜è®¡ç®—æ•ˆç‡çš„å‰æä¸‹è¾¾åˆ°äº†State-of-the-artæ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºå®æ—¶ã€è½»é‡åŒ–çš„é¥æ„Ÿå½±åƒåˆ†æä¸å¤„ç†æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00275v1",
      "published_date": "2025-11-29 02:00:15 UTC",
      "updated_date": "2025-11-29 02:00:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:50.260936+00:00"
    },
    {
      "arxiv_id": "2512.00272v1",
      "title": "Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning",
      "title_zh": "è¿‘ä¼¼æœºå™¨é—å¿˜ä¸­åŸºäºä¼ é€çš„éšç§é˜²å¾¡",
      "authors": [
        "Mohammad M Maheri",
        "Xavier Cadet",
        "Peter Chin",
        "Hamed Haddadi"
      ],
      "abstract": "Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿‘ä¼¼æœºå™¨å­¦ä¹ å¸è½½(Approximate machine unlearning)ä¸­çš„éšç§é£é™©è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼ŒæŒ‡å‡ºæ”»å‡»è€…å¯åˆ©ç”¨æ¨¡å‹å¸è½½å‰åçš„å·®å¼‚è¿›è¡Œæˆå‘˜æ¨ç†(Membership inference)æˆ–æ•°æ®é‡æ„(Data reconstruction)ã€‚ç ”ç©¶å‘ç°æ­¤ç±»æ¼æ´ä¸»è¦æºäºè¢«é—å¿˜æ ·æœ¬çš„é«˜æ¢¯åº¦èŒƒæ•°ä»¥åŠå¸è½½åå‚æ•°ä¸åŸå§‹æ¨¡å‹çš„è¿‡åº¦æ¥è¿‘ï¼Œå¯¼è‡´ç°æœ‰ä¸»æµå¸è½½æ–¹æ³•ï¼ˆå¦‚NGPã€SCRUBï¼‰ä»é¢ä¸´ä¸¥é‡çš„éšç§æ³„éœ²é£é™©ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸ºWARPçš„å³æ’å³ç”¨ä¼ é€(Teleportation)é˜²å¾¡æœºåˆ¶ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œçš„å¯¹ç§°æ€§(Symmetries)åœ¨ä¿æŒé¢„æµ‹æ€§èƒ½çš„åŒæ—¶é™ä½æ¢¯åº¦èƒ½é‡å¹¶å¢åŠ å‚æ•°ç¦»æ•£åº¦ã€‚é€šè¿‡è¿™ç§é‡æ–°å‚æ•°åŒ–(Reparameterization)æ‰‹æ®µï¼ŒWARPèƒ½å¤Ÿæœ‰æ•ˆæ··æ·†å·²é—å¿˜æ•°æ®çš„ä¿¡å·ï¼Œä½¿æ”»å‡»è€…éš¾ä»¥åŒºåˆ†æˆ–æ¢å¤ç‰¹å®šæ ·æœ¬ã€‚åœ¨é’ˆå¯¹å…­ç§å¸è½½ç®—æ³•çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ¡ˆåœ¨é»‘ç›’å’Œç™½ç›’è®¾ç½®ä¸‹åˆ†åˆ«å°†å¯¹æŠ—ä¼˜åŠ¿(AUC)é™ä½äº†æœ€é«˜64%å’Œ92%ï¼Œä¸”æœªç‰ºç‰²ä¿ç•™æ•°æ®çš„å‡†ç¡®ç‡ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†åŸºäºä¼ é€çš„é˜²å¾¡æŠ€æœ¯åœ¨å¢å¼ºè¿‘ä¼¼æœºå™¨å­¦ä¹ å¸è½½å®‰å…¨æ€§æ–¹é¢çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00272v1",
      "published_date": "2025-11-29 01:50:33 UTC",
      "updated_date": "2025-11-29 01:50:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:51.962159+00:00"
    },
    {
      "arxiv_id": "2512.00271v1",
      "title": "Comparative Evaluation of Generative AI Models for Chest Radiograph Report Generation in the Emergency Department",
      "title_zh": "æ€¥è¯Šç§‘èƒ¸éƒ¨ X çº¿æŠ¥å‘Šç”Ÿæˆçš„ç”Ÿæˆå¼ AI æ¨¡å‹å¯¹æ¯”è¯„ä¼°",
      "authors": [
        "Woo Hyeon Lim",
        "Ji Young Lee",
        "Jong Hyuk Lee",
        "Saehoon Kim",
        "Hyungjin Kim"
      ],
      "abstract": "Purpose: To benchmark open-source or commercial medical image-specific VLMs against real-world radiologist-written reports. Methods: This retrospective study included adult patients who presented to the emergency department between January 2022 and April 2025 and underwent same-day CXR and CT for febrile or respiratory symptoms. Reports from five VLMs (AIRead, Lingshu, MAIRA-2, MedGemma, and MedVersa) and radiologist-written reports were randomly presented and blindly evaluated by three thoracic radiologists using four criteria: RADPEER, clinical acceptability, hallucination, and language clarity. Comparative performance was assessed using generalized linear mixed models, with radiologist-written reports treated as the reference. Finding-level analyses were also performed with CT as the reference. Results: A total of 478 patients (median age, 67 years [interquartile range, 50-78]; 282 men [59.0%]) were included. AIRead demonstrated the lowest RADPEER 3b rate (5.3% [76/1434] vs. radiologists 13.9% [200/1434]; P<.001), whereas other VLMs showed higher disagreement rates (16.8-43.0%; P<.05). Clinical acceptability was the highest with AIRead (84.5% [1212/1434] vs. radiologists 74.3% [1065/1434]; P<.001), while other VLMs performed worse (41.1-71.4%; P<.05). Hallucinations were rare with AIRead, comparable to radiologists (0.3% [4/1425]) vs. 0.1% [1/1425]; P=.21), but frequent with other models (5.4-17.4%; P<.05). Language clarity was higher with AIRead (82.9% [1189/1434]), Lingshu (88.0% [1262/1434]), and MedVersa (88.4% [1268/1434]) compared with radiologists (78.1% [1120/1434]; P<.05). Sensitivity varied substantially across VLMs for the common findings: AIRead, 15.5-86.7%; Lingshu, 2.4-86.7%; MAIRA-2, 6.0-72.0%; MedGemma, 4.8-76.7%; and MedVersa, 20.2-69.3%. Conclusion: Medical VLMs for CXR report generation exhibited variable performance in report quality and diagnostic measures.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†äº”ç§åŒ»ç–—ä¸“ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ€¥è¯Šç§‘èƒ¸éƒ¨Xå…‰ç‰‡ï¼ˆCXRï¼‰æŠ¥å‘Šç”Ÿæˆä¸­çš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„åŸå§‹æŠ¥å‘Šè¿›è¡Œäº†åŸºå‡†å¯¹æ¯”ã€‚é€šè¿‡å¯¹478åæ‚£è€…èµ„æ–™çš„å›é¡¾æ€§ç ”ç©¶ï¼Œä¸‰ä½æ”¾å°„ç§‘åŒ»ç”Ÿæ ¹æ®RADPEERã€ä¸´åºŠå¯æ¥å—æ€§ï¼ˆclinical acceptabilityï¼‰ã€å¹»è§‰ï¼ˆhallucinationï¼‰åŠè¯­è¨€æ¸…æ™°åº¦ç­‰å¤šé¡¹æŒ‡æ ‡å¯¹AIReadã€Lingshuã€MAIRA-2ã€MedGemmaå’ŒMedVersaç”Ÿæˆçš„æŠ¥å‘Šè¿›è¡Œäº†ç›²è¯„ã€‚ç»“æœè¡¨æ˜ï¼ŒAIReadåœ¨ä¸´åºŠå¯æ¥å—æ€§ä¸Šï¼ˆ84.5%ï¼‰æ˜¾è‘—ä¼˜äºæ”¾å°„ç§‘åŒ»ç”Ÿï¼ˆ74.3%ï¼‰ï¼Œä¸”å…¶å¹»è§‰ç‡æä½ï¼ˆ0.3%ï¼‰ï¼Œè¡¨ç°ä¸äººç±»ä¸“å®¶ç›¸å½“ã€‚ç„¶è€Œï¼Œå…¶ä»–å››ç§æ¨¡å‹åœ¨ä¸´åºŠä¸€è‡´æ€§ä¸Šè¡¨ç°è¾ƒå·®ï¼Œä¸”å¹»è§‰å‘ç”Ÿç‡æ˜æ˜¾åé«˜ï¼ˆ5.4-17.4%ï¼‰ã€‚å°½ç®¡éƒ¨åˆ†æ¨¡å‹åœ¨è¯­è¨€æ¸…æ™°åº¦æ–¹é¢è¶…è¿‡äº†æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨è¯Šæ–­å¸¸è§ç—…ç¶çš„æ•æ„Ÿæ€§ï¼ˆsensitivityï¼‰ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—æ³¢åŠ¨ã€‚è¯¥ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œç›®å‰çš„åŒ»ç–—VLMsåœ¨æŠ¥å‘Šè´¨é‡å’Œè¯Šæ–­æ•ˆèƒ½æ–¹é¢è¡¨ç°ä¸ä¸€ï¼Œåœ¨ä¸´åºŠå®é™…åº”ç”¨ä¸­ä»é¢ä¸´æ€§èƒ½å·®å¼‚åŒ–çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00271v1",
      "published_date": "2025-11-29 01:45:55 UTC",
      "updated_date": "2025-11-29 01:45:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:52.679548+00:00"
    },
    {
      "arxiv_id": "2512.00269v1",
      "title": "USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing",
      "title_zh": "USBï¼šç—…ç†ä¸å¥åº·å›¾åƒåŒå‘ç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€åˆæˆå¤§è„‘æ¡†æ¶",
      "authors": [
        "Jun Wang",
        "Peirong Liu"
      ],
      "abstract": "Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†USB (Unified Synthetic Brain)ï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨å®ç°ç—…ç†æ€§ä¸å¥åº·å¤§è„‘å›¾åƒåŒå‘ç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€ç«¯åˆ°ç«¯æ¡†æ¶ã€‚é’ˆå¯¹é…å¯¹ç—…ç†-å¥åº·æ•°æ®è·å–éš¾ä»¥åŠç°æœ‰æ–¹æ³•é¢†åŸŸå—é™çš„é—®é¢˜ï¼ŒUSBé€šè¿‡é…å¯¹æ‰©æ•£æœºåˆ¶ (paired diffusion mechanism) å¯¹ç—…ç¶ä¸å¤§è„‘è§£å‰–ç»“æ„çš„è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†ç—…ç†å’Œå¥åº·å›¾åƒçš„åŒæ­¥ç”Ÿæˆã€‚åŒæ—¶ï¼Œæ¡†æ¶å¼•å…¥äº†ä¸€è‡´æ€§å¼•å¯¼ç®—æ³• (consistency guidance algorithm)ï¼Œç¡®ä¿åœ¨åŒå‘ç¼–è¾‘è¿‡ç¨‹ä¸­ç»´æŒè§£å‰–ç»“æ„çš„ä¸€è‡´æ€§ä¸ç—…ç¶çš„å¯¹åº”å…³ç³»ã€‚åœ¨åŒ…æ‹¬ä¸­é£å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…åœ¨å†…çš„å…­ä¸ªå…¬å¼€è„‘éƒ¨ MRI æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒUSBèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„å›¾åƒç»“æœã€‚ä½œä¸ºé¦–ä¸ªè„‘å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€åŸºå‡†ï¼ŒUSBä¸ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†çš„åˆ›å»ºå’Œé²æ£’çš„ç¥ç»å½±åƒåˆ†ææä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00269v1",
      "published_date": "2025-11-29 01:19:07 UTC",
      "updated_date": "2025-11-29 01:19:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:00:52.176993+00:00"
    },
    {
      "arxiv_id": "2512.00267v1",
      "title": "Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking",
      "title_zh": "Trificationï¼šé¢å‘äº‹å®æ ¸æŸ¥çš„å…¨é¢æ ‘å½¢ç­–ç•¥è§„åˆ’å™¨ä¸ç»“æ„åŒ–éªŒè¯",
      "authors": [
        "Anab Maulana Barik",
        "Shou Ziyi",
        "Yang Kaiwen",
        "Yang Qi",
        "Shen Xin"
      ],
      "abstract": "Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨äº‹å®æ ¸æŸ¥ä¸­éš¾ä»¥å…¨é¢è¦†ç›–ä¸»å¼ æˆåˆ†ä¸”ç¼ºä¹ç»“æ„åŒ–é€»è¾‘è¿æ¥çš„é—®é¢˜ï¼Œæå‡ºäº†Trificationè‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥æ¡†æ¶ã€‚Trificationé¦–å…ˆé€šè¿‡ç”Ÿæˆä¸€å¥—å…¨é¢çš„éªŒè¯è¡ŒåŠ¨(verification actions)æ¥ç¡®ä¿å¯¹å¾…æ ¸æŸ¥ä¸»å¼ çš„å®Œå…¨è¦†ç›–ï¼Œå¹¶éšåå°†è¿™äº›è¡ŒåŠ¨æ„å»ºä¸ºä¾èµ–å›¾(dependency graph)ï¼Œç”¨ä»¥å»ºæ¨¡è¡ŒåŠ¨ä¹‹é—´çš„é€»è¾‘äº¤äº’ã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒå¯¹å›¾ç»“æ„è¿›è¡ŒåŠ¨æ€ä¿®æ”¹ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å®æ—¶éœ€æ±‚çµæ´»è°ƒæ•´å…¶éªŒè¯ç­–ç•¥ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTrificationæ˜¾è‘—æå‡äº†äº‹å®æ ¸æŸ¥çš„å‡†ç¡®æ€§ï¼Œæœ‰æ•ˆè¾¾åˆ°äº†å½“å‰çš„SOTAæ°´å¹³ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´åŠ ä¸¥è°¨ã€é€»è¾‘æ¸…æ™°ä¸”å…·å¤‡é€‚åº”æ€§çš„è™šå‡ä¿¡æ¯æ£€æµ‹æœºåˆ¶æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00267v1",
      "published_date": "2025-11-29 01:12:24 UTC",
      "updated_date": "2025-11-29 01:12:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T12:01:42.066606+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 86,
  "processed_papers_count": 86,
  "failed_papers_count": 0,
  "llm_backup_calls": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T12:03:09.941879+00:00"
}
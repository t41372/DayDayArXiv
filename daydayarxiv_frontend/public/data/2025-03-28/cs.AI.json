{
  "date": "2025-03-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-28 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型优化、多模态处理、机器人应用和医疗创新等领域，强调 LLM 在偏好优化和推理中的高效进展（如 GammaTune 和 ReaRec），以及视觉模型在实际场景中的应用（如 WeatherMesh-3）；令人印象深刻的文章包括 Andrea Vedaldi 参与的 3D 生成研究和 Stefano Ermon 的 AI 伦理分析，突显了 AI 治理和高效计算的重要性。\n\n### 重点论文讨论\n我将相关论文归类讨论，先优先聊重要、话题度高的文章（如 AI 伦理、LLM 优化和医疗应用），再快速掠过其他领域的内容。以下是精选摘要，每篇突出核心贡献和发现。\n\n#### AI 模型优化与 LLM 应用\n- **Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding**（中文：基于令牌驱动的 GammaTune：增强推测解码的自适应校准；英文：Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding）  \n  这篇论文提出 GammaTune 和 GammaTune+ 算法，通过动态调整推测长度基于令牌接受率，实现 LLM 推理加速，在 SpecBench 上平均加速 15-16%，显著减少计算浪费，是 LLM 高效部署的关键贡献。\n\n- **Generating Structured Plan Representation of Procedures with LLMs**（中文：使用 LLM 生成程序的结构化计划表示；英文：Generating Structured Plan Representation of Procedures with LLMs）  \n  作者引入 SOPStruct 方法，利用 LLM 将标准操作程序转化为决策树结构，改善任务依赖和执行效率；通过 PDDL 验证框架，证明其在多领域优化中的鲁棒性，适用于自动化工作流。\n\n- **Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents**（中文：AI 失败时谁负责？映射 AI 隐私和伦理事件的成因、实体和后果；英文：Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents）  \n  Stefano Ermon 等学者构建了 AI 事件分类体系，分析 202 起真实案例，强调组织决策和法律合规的重要性；发现当前 AI 治理框架不足，并呼吁针对社交媒体的儿童保护政策，是一篇高话题度的伦理研究。\n\n- **Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation**（中文：在推荐前思考：释放序列推荐的潜在推理能力；英文：Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation）  \n  论文提出 ReaRec 框架，通过多步推理增强用户表示，提升序列推荐性能；在公开数据集上，ReaRec 使推荐模型性能提升 30-50%，为推荐系统注入更深层推理。\n\n- **Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models**（中文：Quamba2：针对选择性状态空间模型的鲁棒可扩展后训练量化框架；英文：Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models）  \n  该工作优化 SSM 量化，支持 W4A8 和 W4A16 配置，实现 1.3 倍预填充加速和 3 倍生成加速，同时精度损失仅 1.6%，适合资源受限设备部署。\n\n- **RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation**（中文：RobuNFR：评估 LLM 在非功能需求感知代码生成中的鲁棒性；英文：RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation）  \n  作者评估 LLM 在代码生成中的鲁棒性，发现加入非功能需求（如可靠性）会降低 Pass@1 率 39%，并提出多工作流测试方法，揭示 LLM 的不稳定性。\n\n其他 LLM 相关如 **QuestBench** 和 **ActionStudio** 等，快速提一下：前者测试 LLM 信息获取能力，后者提供轻量级代理框架，但细节较常规，影响较小。\n\n#### 视觉与多模态处理\n- **Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation**（中文：增强 DeepLabV3+ 以融合航空和卫星图像进行语义分割；英文：Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation）  \n  论文改进 DeepLabV3+ 架构，通过新上采样块融合多源图像，提升 mIoU 到 84.91%，适用于遥感土地覆盖分析。\n\n- **Pairwise Matching of Intermediate Representations for Fine-grained Explainability**（中文：细粒度可解释性的中间表示配对匹配；英文：Pairwise Matching of Intermediate Representations for Fine-grained Explainability）  \n  提出 PAIR-X 方法，利用激活和相关性分数生成精确视觉解释，在动物识别任务中优于基线，提升人类对模型决策的理解。\n\n- **WeatherMesh-3: Fast and accurate operational global weather forecasting**（中文：WeatherMesh-3：快速准确的操作性全球天气预报；英文：WeatherMesh-3: Fast and accurate operational global weather forecasting）  \n  这篇高影响力工作引入潜在空间 rollout 和混合处理器，实现 10 万倍加速，RMSE 提升 37.7%，适合实时部署，是 AI 在实际预报中的突破。\n\n其他视觉论文如 **Diffusion models applied to skin and oral cancer classification**，贡献在于扩散模型在医疗图像分类中的应用，mIoU 达 84.91%，但整体影响中等。\n\n#### 机器人与操作\n- **DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness**（中文：DSO：使用模拟反馈对齐 3D 生成器以实现物理稳定性；英文：DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness）  \n  Andrea Vedaldi 参与，提出 DSO 框架，通过模拟反馈优化 3D 对象稳定性，比测试时优化快且鲁棒，为物理模拟生成提供新路径。\n\n- **SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles**（中文：SafeCast：面向自动驾驶的风险响应运动预测；英文：SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles）  \n  论文整合 RSS 框架和不确定性模块，提升自动驾驶预测准确性，在多数据集上 AUC 达 0.86，实现实时部署。\n\n其他机器人论文如 **Endo-TTAP**，用于内镜跟踪，F1 分数超 0.97，但属于特定应用，快速掠过。\n\n#### 医疗与生物应用\n- **On-site estimation of battery electrochemical parameters via transfer learning based physics-informed neural network approach**（中文：基于迁移学习的物理信息神经网络方法实现电池电化学参数现场估计；英文：On-site estimation of battery electrochemical parameters via transfer learning based physics-informed neural network approach）  \n  提出双阶段 PINN 方法，实现电池参数实时估计，相对误差仅 3.89%，适用于医疗设备监控。\n\n- **Diagnosis of Pulmonary Hypertension by Integrating Multimodal Data with a Hybrid Graph Convolutional and Transformer Network**（中文：通过多模态数据与混合图卷积和 Transformer 网络整合诊断肺动脉高压；英文：Diagnosis of Pulmonary Hypertension by Integrating Multimodal Data with a Hybrid Graph Convolutional and Transformer Network）  \n  开发混合模型处理心血管数据，AUC 达 0.81，辅助诊断三类 PH，提升临床决策。\n\n其他医疗论文如 **Opioid Named Entity Recognition (ONER-2025)**，F1 分数达 97%，但为特定 NLP 任务，影响有限。\n\n### 其他领域快速掠过\n剩余论文涉及音乐理论、蛋白设计和经济学等领域，如 **Teaching LLMs Music Theory**（贡献：LLM 在音乐教育中的应用，准确率达 75%）、**Multi-Objective Quality-Diversity**（用于机器人任务优化）和 **Using AI to Summarize US Presidential Campaign TV Advertisement Videos**（AI 在历史数据分析中的潜力）。这些文章虽有趣，但非核心焦点，仅提及其方法创新，未深挖。\n\n总之，今天的 arXiv 展示了 AI 在优化和实际应用中的强劲势头，WeatherMesh-3 和 DSO 等工作特别值得跟踪。未来几天，继续关注这些领域的进展！（全文控制在合理篇幅内，聚焦高影响力内容）",
  "papers": [
    {
      "arxiv_id": "2504.00030v2",
      "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Aayush Gautam",
        "Susav Shrestha",
        "Narasimha Reddy"
      ],
      "abstract": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment.",
      "tldr_zh": "这篇论文提出了 GammaTune 和 GammaTune+，两种无训练的适应性算法，用于优化大语言模型(LLM)的推测解码(speculative decoding)，通过动态调整推测长度来最大化加速并减少计算浪费。算法基于 tokens 接受率采用启发式切换机制(heuristic-based switching mechanism)，以实现更精确的 token 验证。实验在 SpecBench 上跨多个任务和模型对进行评估，结果显示 GammaTune 平均加速 15% (±5%)，GammaTune+ 达到 16% (±3%)，并显著降低了性能方差。总体而言，这为推测解码的实际部署提供了稳健高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2504.00030v2",
      "published_date": "2025-03-28 23:41:55 UTC",
      "updated_date": "2025-04-03 12:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:59:02.860236"
    },
    {
      "arxiv_id": "2503.22909v1",
      "title": "Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation",
      "title_zh": "增强 DeepLabV3+ 以融合航空和卫星图像用于语义分割",
      "authors": [
        "Anas Berka",
        "Mohamed El Hajji",
        "Raphael Canals",
        "Youssef Es-saady",
        "Adel Hafiane"
      ],
      "abstract": "Aerial and satellite imagery are inherently complementary remote sensing\nsources, offering high-resolution detail alongside expansive spatial coverage.\nHowever, the use of these sources for land cover segmentation introduces\nseveral challenges, prompting the development of a variety of segmentation\nmethods. Among these approaches, the DeepLabV3+ architecture is considered as a\npromising approach in the field of single-source image segmentation. However,\ndespite its reliable results for segmentation, there is still a need to\nincrease its robustness and improve its performance. This is particularly\ncrucial for multimodal image segmentation, where the fusion of diverse types of\ninformation is essential.\n  An interesting approach involves enhancing this architectural framework\nthrough the integration of novel components and the modification of certain\ninternal processes.\n  In this paper, we enhance the DeepLabV3+ architecture by introducing a new\ntransposed conventional layers block for upsampling a second entry to fuse it\nwith high level features. This block is designed to amplify and integrate\ninformation from satellite images, thereby enriching the segmentation process\nthrough fusion with aerial images.\n  For experiments, we used the LandCover.ai (Land Cover from Aerial Imagery)\ndataset for aerial images, alongside the corresponding dataset sourced from\nSentinel 2 data.\n  Through the fusion of both sources, the mean Intersection over Union (mIoU)\nachieved a total mIoU of 84.91% without data augmentation.",
      "tldr_zh": "该研究针对航空和卫星图像的互补特性，增强了 DeepLabV3+ 架构，以改善多模态图像的语义分割性能。研究引入了一个新的转置卷积层块（transposed conventional layers block），用于上采样卫星图像并将其与航空图像的高级特征融合，从而实现更有效的信息整合。实验使用 LandCover.ai 数据集（航空图像）和 Sentinel 2 数据（卫星图像），在不进行数据增强的情况下，实现了 84.91% 的 mIoU（平均交并比），显著提升了分割的鲁棒性和准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22909v1",
      "published_date": "2025-03-28 23:07:39 UTC",
      "updated_date": "2025-03-28 23:07:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:59:13.330270"
    },
    {
      "arxiv_id": "2504.00029v1",
      "title": "Generating Structured Plan Representation of Procedures with LLMs",
      "title_zh": "使用 LLMs 生成程序的结构化计划表示",
      "authors": [
        "Deepeka Garg",
        "Sihan Zeng",
        "Sumitra Ganesh",
        "Leo Ardon"
      ],
      "abstract": "In this paper, we address the challenges of managing Standard Operating\nProcedures (SOPs), which often suffer from inconsistencies in language, format,\nand execution, leading to operational inefficiencies. Traditional process\nmodeling demands significant manual effort, domain expertise, and familiarity\nwith complex languages like Business Process Modeling Notation (BPMN), creating\nbarriers for non-techincal users. We introduce SOP Structuring (SOPStruct), a\nnovel approach that leverages Large Language Models (LLMs) to transform SOPs\ninto decision-tree-based structured representations. SOPStruct produces a\nstandardized representation of SOPs across different domains, reduces cognitive\nload, and improves user comprehension by effectively capturing task\ndependencies and ensuring sequential integrity. Our approach enables leveraging\nthe structured information to automate workflows as well as empower the human\nusers. By organizing procedures into logical graphs, SOPStruct facilitates\nbacktracking and error correction, offering a scalable solution for process\noptimization. We employ a novel evaluation framework, combining deterministic\nmethods with the Planning Domain Definition Language (PDDL) to verify graph\nsoundness, and non-deterministic assessment by an LLM to ensure completeness.\nWe empirically validate the robustness of our LLM-based structured SOP\nrepresentation methodology across SOPs from different domains and varying\nlevels of complexity. Despite the current lack of automation readiness in many\norganizations, our research highlights the transformative potential of LLMs to\nstreamline process modeling, paving the way for future advancements in\nautomated procedure optimization.",
      "tldr_zh": "本论文针对标准操作程序 (SOPs) 的语言、格式和执行不一致问题，提出了一种名为 SOPStruct 的新方法，利用大型语言模型 (LLMs) 将 SOPs 转化为基于决策树的结构化表示，从而减少手动建模的努力并降低非技术用户的认知负担。SOPStruct 通过捕捉任务依赖性和顺序完整性，将程序组织成逻辑图，便于回溯、错误修正和自动化工作流优化。研究采用结合确定性方法与 Planning Domain Definition Language (PDDL) 的评估框架，实证验证了该方法在不同领域和复杂程度 SOPs 上的鲁棒性，并强调 LLMs 在流程建模中的变革潜力，为未来自动化程序优化铺平道路。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00029v1",
      "published_date": "2025-03-28 22:38:24 UTC",
      "updated_date": "2025-03-28 22:38:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:59:26.259039"
    },
    {
      "arxiv_id": "2504.01029v1",
      "title": "Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents",
      "title_zh": "翻译失败",
      "authors": [
        "Hilda Hadan",
        "Reza Hadi Mogavi",
        "Leah Zhang-Kennedy",
        "Lennart E. Nacke"
      ],
      "abstract": "The rapid growth of artificial intelligence (AI) technologies has changed\ndecision-making in many fields. But, it has also raised major privacy and\nethical concerns. However, many AI incidents taxonomies and guidelines for\nacademia, industry, and government lack grounding in real-world incidents. We\nanalyzed 202 real-world AI privacy and ethical incidents. This produced a\ntaxonomy that classifies incident types across AI lifecycle stages. It accounts\nfor contextual factors such as causes, responsible entities, disclosure\nsources, and impacts. Our findings show insufficient incident reporting from AI\ndevelopers and users. Many incidents are caused by poor organizational\ndecisions and legal non-compliance. Only a few legal actions and corrective\nmeasures exist, while risk-mitigation efforts are limited. Our taxonomy\ncontributes a structured approach in reporting of future AI incidents. Our\nfindings demonstrate that current AI governance frameworks are inadequate. We\nurgently need child-specific protections and AI policies on social media. They\nmust moderate and reduce the spread of harmful AI-generated content. Our\nresearch provides insights for policymakers and practitioners, which lets them\ndesign ethical AI. It also support AI incident detection and risk management.\nFinally, it guides AI policy development. Improved policies will protect people\nfrom harmful AI applications and support innovation in AI systems.",
      "tldr_zh": "本研究分析了 202 个真实 AI 隐私和伦理事件，构建了一个 taxonomy 来分类事件类型、原因、责任实体、披露来源和影响，涵盖 AI 生命周期各阶段。结果显示，AI 开发者和用户报告事件不足，许多事件源于组织决策不当和法律不合规，而法律行动和风险缓解努力有限。该 taxonomy 提供结构化方法支持未来 AI 事件报告，并为政策制定者提供洞见，推动改进 AI 治理框架、制定儿童保护政策和社交媒体 AI 规则，以设计更道德的 AI 系统并减少有害内容传播。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DB",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "63 pages, 7 tables, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01029v1",
      "published_date": "2025-03-28 21:57:38 UTC",
      "updated_date": "2025-03-28 21:57:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:59:38.840722"
    },
    {
      "arxiv_id": "2503.22881v1",
      "title": "Pairwise Matching of Intermediate Representations for Fine-grained Explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Lauren Shrack",
        "Timm Haucke",
        "Antoine Salaün",
        "Arjun Subramonian",
        "Sara Beery"
      ],
      "abstract": "The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts were in unanimous agreement that PAIR-X was an improvement over\nexisting baselines for deep model explainability, and suggested that its\nvisualizations would be directly applicable to their work. We also propose a\nnovel quantitative evaluation metric for our method, and demonstrate that\nPAIR-X visualizations appear more plausible for correct image matches than\nincorrect ones even when the model similarity score for the pairs is the same.\nBy improving interpretability, PAIR-X enables humans to better distinguish\ncorrect and incorrect matches. Our code is available at:\nhttps://github.com/pairx-explains/pairx",
      "tldr_zh": "本研究提出了一种新的解释方法PAIR-X，用于细粒度图像分类的可解释性问题，通过利用中间模型激活(intermediate representations)和反向传播相关性分数(backpropagated relevance scores)，生成高度局部的配对视觉解释。PAIR-X 在动物和建筑再识别(re-ID)任务上进行评估，在35个公共re-ID数据集上表现出质的改善，优于现有基线方法。实验结果显示，PAIR-X的可视化更具合理性，即使模型相似度分数相同，也能帮助人类更好地区分正确和错误匹配；此外，该方法获得了动物re-ID专家的一致认可，并引入了一个新的定量评估指标以量化其性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22881v1",
      "published_date": "2025-03-28 21:13:43 UTC",
      "updated_date": "2025-03-28 21:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:59:50.212364"
    },
    {
      "arxiv_id": "2503.22879v2",
      "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hung-Yueh Chiang",
        "Chi-Chih Chang",
        "Natalia Frumkin",
        "Kai-Chiang Wu",
        "Mohamed S. Abdelfattah",
        "Diana Marculescu"
      ],
      "abstract": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.",
      "tldr_zh": "这篇论文提出 Quamba2，一种鲁棒且可扩展的后训练量化框架，针对 Selective State Space Models (SSMs)，支持 W8A8、W4A8 和 W4A16 配置，以适应不同场景如大批量解码或短提示应用的部署需求。框架基于 SSMs 的通道顺序保持和激活持久性，通过离线排序、聚类量化输入 $x$ 为 8 位，并对参数 $B$ 和 $C$ 进行每状态组量化，同时重新排列权重以确保输出计算不变性。实验结果显示，Quamba2-8B 模型比现有方法性能更优，提供 1.3 倍预填充和 3 倍生成速度提升、4 倍内存减少，仅 1.6% 准确率下降，并在 MMLU 评估中展现出良好的泛化性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22879v2",
      "published_date": "2025-03-28 21:10:39 UTC",
      "updated_date": "2025-04-03 15:04:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:00:04.508879"
    },
    {
      "arxiv_id": "2503.22877v1",
      "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models",
      "title_zh": "理解 LLM 事实检查在地理区域不平等的现象：使用代理和检索模型",
      "authors": [
        "Bruno Coelho",
        "Shujaat Mirza",
        "Yuyuan Cui",
        "Christina Pöpper",
        "Damon McCoy"
      ],
      "abstract": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在事实检查中的地理不平等问题，使用一个包含 600 条平衡分布于六大全球区域的事实声明数据集进行评估。研究比较了三种实验设置：仅提供声明、利用 LLM 代理访问 Wikipedia，以及采用检索增强生成 (RAG) 系统。结果显示，无论使用 GPT-4、Claude Sonnet 或 LLaMA 等模型，全球北方 (Global North) 的声明准确率远高于全球南方 (Global South)，且在代理系统中这一差距进一步扩大。论文强调，需要通过更好的数据集平衡和更强的检索策略来提升 LLMs 在地理多样化上下文中的事实检查性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22877v1",
      "published_date": "2025-03-28 21:07:43 UTC",
      "updated_date": "2025-03-28 21:07:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:00:14.026115"
    },
    {
      "arxiv_id": "2504.00027v3",
      "title": "Opioid Named Entity Recognition (ONER-2025) from Reddit",
      "title_zh": "翻译失败",
      "authors": [
        "Grigori Sidorov",
        "Muhammad Ahmad",
        "Iqra Ameer",
        "Muhammad Usman",
        "Ildar Batyrshin"
      ],
      "abstract": "The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).",
      "tldr_zh": "这篇论文针对阿片类药物过量流行，利用 Natural Language Processing (NLP) 和 Opioid Named Entity Recognition (ONER-2025) 从 Reddit 数据中提取用户信息。研究的主要贡献包括创建了一个手动标注数据集（包含 331,285 tokens 和八个阿片类实体类别）、详细说明标注过程及面临的语言挑战（如俚语、歧义和情绪化语言），并提出一个实时监控系统来处理社交媒体、医疗记录和紧急服务的流数据以识别过量事件。实验采用 5 折交叉验证和 11 个整合机器学习、深度学习及 transformer-based 模型（如 bert-base-NER 和 roberta-base）的设置，结果显示这些模型达到了 97% 的准确率和 F1-score，比基线模型（如 RF）提高了 10.23%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00027v3",
      "published_date": "2025-03-28 20:51:06 UTC",
      "updated_date": "2025-04-30 21:34:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:00:27.304534"
    },
    {
      "arxiv_id": "2504.00026v1",
      "title": "Diffusion models applied to skin and oral cancer classification",
      "title_zh": "扩散模型应用于皮肤癌和口腔癌分类",
      "authors": [
        "José J. M. Uliana",
        "Renato A. Krohling"
      ],
      "abstract": "This study investigates the application of diffusion models in medical image\nclassification (DiffMIC), focusing on skin and oral lesions. Utilizing the\ndatasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the\ndiffusion model demonstrated competitive performance compared to\nstate-of-the-art deep learning models like Convolutional Neural Networks (CNNs)\nand Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved\na balanced accuracy of 0.6457 for six-class classification and 0.8357 for\nbinary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it\nattained a balanced accuracy of 0.9050. These results suggest that diffusion\nmodels are viable models for classifying medical images of skin and oral\nlesions. In addition, we investigate the robustness of the model trained on\nPAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA\ndataset.",
      "tldr_zh": "这篇论文探讨了 diffusion models 在医疗图像分类（DiffMIC）中的应用，专注于皮肤和口腔癌症的诊断，使用 PAD-UFES-20 和 P-NDB-UFES 数据集进行实验。模型在 PAD-UFES-20 上实现了六类分类的 balanced accuracy 为 0.6457 和二分类（癌症 vs. 非癌症）的 0.8357，以及在 P-NDB-UFES 上达到 0.9050 的 balanced accuracy，与 CNNs 和 Transformers 等现有模型相比表现出竞争力。研究还评估了模型在 HIBA 数据集上的鲁棒性，证明 diffusion models 是皮肤和口腔病变分类的可行选择。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00026v1",
      "published_date": "2025-03-28 20:29:35 UTC",
      "updated_date": "2025-03-28 20:29:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:00:38.492549"
    },
    {
      "arxiv_id": "2503.22853v1",
      "title": "Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought Prompting: Pedagogical Strategies for Machines",
      "title_zh": "翻译失败",
      "authors": [
        "Liam Pond",
        "Ichiro Fujinaga"
      ],
      "abstract": "This study evaluates the baseline capabilities of Large Language Models\n(LLMs) like ChatGPT, Claude, and Gemini to learn concepts in music theory\nthrough in-context learning and chain-of-thought prompting. Using carefully\ndesigned prompts (in-context learning) and step-by-step worked examples\n(chain-of-thought prompting), we explore how LLMs can be taught increasingly\ncomplex material and how pedagogical strategies for human learners translate to\neducating machines. Performance is evaluated using questions from an official\nCanadian Royal Conservatory of Music (RCM) Level 6 examination, which covers a\ncomprehensive range of topics, including interval and chord identification, key\ndetection, cadence classification, and metrical analysis. Additionally, we\nevaluate the suitability of various music encoding formats for these tasks\n(ABC, Humdrum, MEI, MusicXML). All experiments were run both with and without\ncontextual prompts. Results indicate that without context, ChatGPT with MEI\nperforms the best at 52%, while with context, Claude with MEI performs the best\nat 75%. Future work will further refine prompts and expand to cover more\nadvanced music theory concepts. This research contributes to the broader\nunderstanding of teaching LLMs and has applications for educators, students,\nand developers of AI music tools alike.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）如ChatGPT、Claude和Gemini通过in-context learning和chain-of-thought prompting学习音乐理论概念的基线能力，使用精心设计的提示和逐步示例来模拟人类教学策略。实验基于加拿大皇家音乐学院（RCM）Level 6考试问题，包括间隔和和弦识别、调性检测、终止式分类及节拍分析，并测试了不同音乐编码格式（如ABC、Humdrum、MEI、MusicXML）的适用性。结果显示，无上下文提示时ChatGPT with MEI准确率达52%，而有上下文提示时Claude with MEI提升至75%，证明这些策略显著提高了模型性能。该研究为将人类教学方法应用于LLMs提供洞见，并为教育者、学生和AI音乐工具开发者带来潜在应用。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "11 pages, 4 figures, 3 tables. Published in Volume 1 of the\n  Proceedings of the 17th International Conference on Computer Supported Music\n  Education (CSME 2025). Presented on 3 April 2025 in Porto, Portugal",
      "pdf_url": "http://arxiv.org/pdf/2503.22853v1",
      "published_date": "2025-03-28 20:15:24 UTC",
      "updated_date": "2025-03-28 20:15:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:00:51.124795"
    },
    {
      "arxiv_id": "2503.22851v2",
      "title": "RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation",
      "title_zh": "RobuNFR：评估大型语言模型在非功能性需求感知代码生成中的鲁棒性",
      "authors": [
        "Feng Lin",
        "Dong Jae Kim",
        "Zhenhao Li",
        "Jinqiu Yang",
        "Tse-Hsun",
        "Chen"
      ],
      "abstract": "When using LLMs to address Non-Functional Requirements (NFRs), developers may\nbehave differently (e.g., expressing the same NFR in different words). Robust\nLLMs should output consistent results across these variations; however, this\naspect remains underexplored. We propose RobuNFR for evaluating the robustness\nof LLMs in NFR-aware code generation across four NFR dimensions: design,\nreadability, reliability, and performance, using three methodologies: prompt\nvariation, regression testing, and diverse workflows. Our experiments show that\nRobuNFR reveals robustness issues in the tested LLMs when considering NFRs in\ncode generation. Specifically, under prompt variation, including NFRs leads to\na decrease in Pass@1 by up to 39 percent and an increase in the standard\ndeviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e.,\nFunction-Only). While incorporating NFRs generally improves overall NFR\nmetrics, it also results in higher prompt sensitivity. In regression settings,\nsome LLMs exhibit differences across versions, with improvements in one aspect\n(e.g., reduced code smells) often accompanied by regressions in another (e.g.,\ndecreased correctness), revealing inconsistencies that challenge their\nrobustness. When varying workflows, the tested LLMs show significantly\ndifferent NFR-aware code generation capabilities between two workflows: (1)\nintegrating NFRs and functional requirements into the initial prompt and (2)\nenhancing Function-Only-generated code with the same NFR.",
      "tldr_zh": "该论文提出 RobuNFR 框架，用于评估大型语言模型 (LLMs) 在处理非功能性需求 (NFRs) 的代码生成时的鲁棒性，涵盖设计、易读性、可靠性和性能四个 NFR 维度。框架采用三种方法——提示变体、回归测试和多样化工作流——来测试 LLMs 对 NFRs 的响应一致性。实验发现，加入 NFRs 会导致 Pass@1 下降高达 39% 并增加标准差，同时虽改善整体 NFR 指标，但也暴露了 LLMs 在不同版本和工作流间的鲁棒性问题，如某些方面提升（如减少代码异味）却伴随其他退化（如正确性下降）。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Corrected metadata: fixed author name in submission form (TeX file\n  was already correct)",
      "pdf_url": "http://arxiv.org/pdf/2503.22851v2",
      "published_date": "2025-03-28 20:05:33 UTC",
      "updated_date": "2025-04-03 00:55:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:01:03.765148"
    },
    {
      "arxiv_id": "2503.22829v2",
      "title": "Nonhuman Primate Brain Tissue Segmentation Using a Transfer Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Lin",
        "Hongyu Yuan",
        "Richard Barcus",
        "Qing Lyu",
        "Sucheta Chakravarty",
        "Megan E. Lipford",
        "Carol A. Shively",
        "Suzanne Craft",
        "Mohammad Kawas",
        "Jeongchul Kim",
        "Christopher T. Whitlow"
      ],
      "abstract": "Non-human primates (NHPs) serve as critical models for understanding human\nbrain function and neurological disorders due to their close evolutionary\nrelationship with humans. Accurate brain tissue segmentation in NHPs is\ncritical for understanding neurological disorders, but challenging due to the\nscarcity of annotated NHP brain MRI datasets, the small size of the NHP brain,\nthe limited resolution of available imaging data and the anatomical differences\nbetween human and NHP brains. To address these challenges, we propose a novel\napproach utilizing STU-Net with transfer learning to leverage knowledge\ntransferred from human brain MRI data to enhance segmentation accuracy in the\nNHP brain MRI, particularly when training data is limited. The combination of\nSTU-Net and transfer learning effectively delineates complex tissue boundaries\nand captures fine anatomical details specific to NHP brains. Notably, our\nmethod demonstrated improvement in segmenting small subcortical structures such\nas putamen and thalamus that are challenging to resolve with limited spatial\nresolution and tissue contrast, and achieved DSC of over 0.88, IoU over 0.8 and\nHD95 under 7. This study introduces a robust method for multi-class brain\ntissue segmentation in NHPs, potentially accelerating research in evolutionary\nneuroscience and preclinical studies of neurological disorders relevant to\nhuman health.",
      "tldr_zh": "该研究针对非人类灵长类动物 (NHPs) 脑组织分割的挑战，如数据集稀缺、脑部尺寸小以及与人类脑的解剖差异，提出了一种基于转移学习的方法，利用 STU-Net 从人类脑MRI数据转移知识来提升NHP脑MRI的分割准确性。  \n该方法能够有效区分复杂组织边界并捕捉NHP脑的精细解剖细节，尤其在分割小亚核结构如putamen和thalamus时表现出显著优势。  \n实验结果显示，该方法实现了DSC超过0.88、IoU超过0.8和HD95低于7的性能指标。  \n这项工作为NHP的多类脑组织分割提供了稳健的解决方案，有助于加速进化神经科学和神经疾病的临床前研究。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22829v2",
      "published_date": "2025-03-28 18:51:22 UTC",
      "updated_date": "2025-04-01 11:52:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:01:15.608587"
    },
    {
      "arxiv_id": "2503.22809v2",
      "title": "Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting",
      "title_zh": "翻译失败",
      "authors": [
        "Uddhav Bhattarai",
        "Rajkishan Arikapudi",
        "Steven A. Fennimore",
        "Frank N Martin",
        "Stavros G. Vougioukas"
      ],
      "abstract": "Manual fruit harvesting is common in agriculture, but the amount of time\npickers spend on non-productive activities can make it very inefficient.\nAccurately identifying picking vs. non-picking activity is crucial for\nestimating picker efficiency and optimising labour management and harvest\nprocesses. In this study, a practical system was developed to calculate the\nefficiency of pickers in commercial strawberry harvesting. Instrumented picking\ncarts were developed to record the harvested fruit weight, geolocation, and\ncart movement in real time. These carts were deployed during the commercial\nstrawberry harvest season in Santa Maria, CA. The collected data was then used\nto train a CNN-LSTM-based deep neural network to classify a picker's activity\ninto \"Pick\" and \"NoPick\" classes. Experimental evaluations showed that the\nCNN-LSTM model showed promising activity recognition performance with an F1\nscore accuracy of over 0.97. The recognition results were then used to compute\npicker efficiency and the time required to fill a tray. Analysis of the\nseason-long harvest data showed that the average picker efficiency was 75.07%\nwith an estimation accuracy of 95.22%. Furthermore, the average tray fill time\nwas 6.79 minutes with an estimation accuracy of 96.43%. When integrated into\ncommercial harvesting, the proposed technology can aid growers in monitoring\nautomated worker activity and optimising harvests to reduce non-productive time\nand enhance overall harvest efficiency.",
      "tldr_zh": "这篇论文针对手动水果采摘（如草莓）的低效率问题，开发了一个数据驱动系统，用于识别工人活动并估计效率。研究团队设计了配备传感器和GPS的采摘车，实时记录水果重量、地理位置和车子运动，并在商业采摘季节收集数据。利用这些数据训练了基于CNN-LSTM的深度神经网络模型，将工人活动分类为“Pick”（采摘）和“NoPick”（非采摘），模型的F1分数超过0.97。结果显示，平均采摘效率为75.07%，估计准确率达95.22%，平均托盘填充时间为6.79分钟，准确率96.43%，从而帮助农场主优化劳动管理和减少非生产时间。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22809v2",
      "published_date": "2025-03-28 18:16:28 UTC",
      "updated_date": "2025-04-28 23:11:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:01:28.289271"
    },
    {
      "arxiv_id": "2503.22796v1",
      "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Hanling Zhang",
        "Rundong Su",
        "Zhihang Yuan",
        "Pengtao Chen",
        "Mingzhu Shen Yibo Fan",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "abstract": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
      "tldr_zh": "该论文提出 DiTFastAttnV2，一种后训练压缩方法，旨在加速 Multimodal Diffusion Transformers (MMDiT) 中的注意力机制，以解决文本到图像生成模型的计算瓶颈问题。通过分析 MMDiT 的注意力模式，该方法引入 head-wise arrow attention 和 caching mechanisms 来动态调整注意力头，并设计 Efficient Fused Kernel 以进一步提升效率，同时利用 local metric methods 和优化技术，将最佳压缩方案的搜索时间缩短至几分钟。实验结果显示，DiTFastAttnV2 实现了注意力 FLOPs 减少 68% 和 1.5x 端到端加速，在 2K 图像生成任务中保持了视觉质量不变。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22796v1",
      "published_date": "2025-03-28 18:00:12 UTC",
      "updated_date": "2025-03-28 18:00:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:01:38.469962"
    },
    {
      "arxiv_id": "2503.22677v1",
      "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
      "title_zh": "翻译失败",
      "authors": [
        "Ruining Li",
        "Chuanxia Zheng",
        "Christian Rupprecht",
        "Andrea Vedaldi"
      ],
      "abstract": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.",
      "tldr_zh": "该研究提出DSO框架，利用物理模拟反馈来微调3D生成器，确保生成的3D对象具备物理稳定性，如在重力下保持自支撑。方法包括构建一个基于模拟器稳定性分数的数据集，并通过直接偏好优化(DPO)或新引入的直接奖励优化(DRO)来对齐扩散模型，从而避免了测试时优化带来的低效问题。实验表明，微调后的生成器生成稳定对象的速度更快、成功率更高；此外，DSO框架无需地面实况数据，可让生成器通过模拟自身输出实现自我改进。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://ruiningli.com/dso",
      "pdf_url": "http://arxiv.org/pdf/2503.22677v1",
      "published_date": "2025-03-28 17:59:53 UTC",
      "updated_date": "2025-03-28 17:59:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:01:50.910283"
    },
    {
      "arxiv_id": "2503.22675v2",
      "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation",
      "title_zh": "思考先行：释放序列推荐的潜在推理能力",
      "authors": [
        "Jiakai Tang",
        "Sunhao Dai",
        "Teng Shi",
        "Jun Xu",
        "Xu Chen",
        "Wen Chen",
        "Wu Jian",
        "Yuning Jiang"
      ],
      "abstract": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
      "tldr_zh": "该论文针对Sequential Recommendation (SeqRec) 的问题，指出现有方法采用直接前向计算范式，无法有效捕捉用户偏好的复杂演变和长尾物品，从而导致性能不足。作者提出ReaRec，这是首个推理时计算框架，通过隐式多步推理增强用户表示，具体包括自动回归地将序列的最后一个隐藏状态输入推荐器，并使用特殊的推理位置嵌入来分离物品编码空间和推理空间。此外，引入两种轻量级学习方法：Ensemble Reasoning Learning (ERL) 和 Progressive Reasoning Learning (PRL)，以进一步挖掘推理潜力。实验在五个真实数据集和不同SeqRec架构上验证了ReaRec的通用性和有效性，显著提升了多种推荐骨干的性能上限约30%-50%，为推理时计算在推荐系统中的应用开辟新方向。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22675v2",
      "published_date": "2025-03-28 17:59:03 UTC",
      "updated_date": "2025-04-16 10:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:02:02.957542"
    },
    {
      "arxiv_id": "2503.22674v1",
      "title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?",
      "title_zh": "翻译失败",
      "authors": [
        "Belinda Z. Li",
        "Been Kim",
        "Zi Wang"
      ],
      "abstract": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.",
      "tldr_zh": "这篇论文引入了 QuestBench 基准，用于评估大型语言模型 (LLMs) 在处理不完全指定推理任务时的能力，特别是是否能提出正确的问题来获取缺失信息。研究将这些任务形式化为约束满足问题 (CSP) 的特例，并设计了四个子任务：Logic-Q（逻辑推理缺少命题）、Planning-Q（PDDL 规划问题部分观察初始状态）、GSM-Q（中小学数学问题缺少变量赋值）和 GSME-Q（GSM-Q 的方程版本）。实验结果显示，先进模型在 GSM-Q 和 GSME-Q 上表现优秀，但 Logic-Q 和 Planning-Q 的准确率仅为 40-50%，表明模型在识别最小必要问题方面存在困难，即使能解决完全指定的问题。该基准突显了 LLMs 在信息获取能力上的局限性，需要进一步深入研究。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Code and dataset are available at\n  \\url{https://github.com/google-deepmind/questbench}",
      "pdf_url": "http://arxiv.org/pdf/2503.22674v1",
      "published_date": "2025-03-28 17:58:40 UTC",
      "updated_date": "2025-03-28 17:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:02:16.000781"
    },
    {
      "arxiv_id": "2503.22673v2",
      "title": "ActionStudio: A Lightweight Framework for Data and Training of Large Action Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jianguo Zhang",
        "Thai Hoang",
        "Ming Zhu",
        "Zuxin Liu",
        "Shiyu Wang",
        "Tulika Awalgaonkar",
        "Akshara Prabhakar",
        "Haolin Chen",
        "Weiran Yao",
        "Zhiwei Liu",
        "Juntao Tan",
        "Juan Carlos Niebles",
        "Shelby Heinecke",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong"
      ],
      "abstract": "Action models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We present ActionStudio, a lightweight\nand extensible data and training framework designed for large action models.\nActionStudio unifies heterogeneous agent trajectories through a standardized\nformat, supports diverse training paradigms including LoRA, full fine-tuning,\nand distributed setups, and integrates robust preprocessing and verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to\nfacilitate research in the community.",
      "tldr_zh": "该研究介绍了 ActionStudio，一个轻量级且可扩展的框架，旨在简化大型行动模型（large action models）的训练过程，以应对代理环境多样性和数据复杂性的挑战。ActionStudio 通过标准化格式统一异构代理轨迹，支持多种训练范式如 LoRA、全微调和分布式设置，并整合了稳健的预处理和验证工具。该框架在公共和实际行业基准上表现出色，提升了性能和可扩展性，并开源了代码和数据（https://github.com/SalesforceAIResearch/xLAM），以促进社区研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages; large action models; xLAM",
      "pdf_url": "http://arxiv.org/pdf/2503.22673v2",
      "published_date": "2025-03-28 17:58:33 UTC",
      "updated_date": "2025-03-31 16:38:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:02:26.438935"
    },
    {
      "arxiv_id": "2503.22672v1",
      "title": "Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder Re-rankers",
      "title_zh": "探索多阶段微调对交叉编码器重新排序器的有效性",
      "authors": [
        "Francesca Pezzuti",
        "Sean MacAvaney",
        "Nicola Tonellotto"
      ],
      "abstract": "State-of-the-art cross-encoders can be fine-tuned to be highly effective in\npassage re-ranking. The typical fine-tuning process of cross-encoders as\nre-rankers requires large amounts of manually labelled data, a contrastive\nlearning objective, and a set of heuristically sampled negatives. An\nalternative recent approach for fine-tuning instead involves teaching the model\nto mimic the rankings of a highly effective large language model using a\ndistillation objective. These fine-tuning strategies can be applied either\nindividually, or in sequence. In this work, we systematically investigate the\neffectiveness of point-wise cross-encoders when fine-tuned independently in a\nsingle stage, or sequentially in two stages. Our experiments show that the\neffectiveness of point-wise cross-encoders fine-tuned using contrastive\nlearning is indeed on par with that of models fine-tuned with multi-stage\napproaches. Code is available for reproduction at\nhttps://github.com/fpezzuti/multistage-finetuning.",
      "tldr_zh": "该研究探讨了多阶段微调策略在 cross-encoder re-rankers 上的有效性，比较了单阶段独立微调（如使用 contrastive learning）和两阶段顺序微调（如结合 contrastive learning 和 distillation objective）。作者系统实验了 point-wise cross-encoders 的表现，结果显示，通过 contrastive learning 进行单阶段微调的模型效果与多阶段方法相当，无需额外复杂步骤。代码已在 GitHub 上公开，以便复现。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "7 pages. To be published as short paper in the Proceedings of the\n  European Conference on Information Retrieval (ECIR) 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.22672v1",
      "published_date": "2025-03-28 17:58:31 UTC",
      "updated_date": "2025-03-28 17:58:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:02:38.193156"
    },
    {
      "arxiv_id": "2503.22658v1",
      "title": "Evaluation of Machine-generated Biomedical Images via A Tally-based Similarity Measure",
      "title_zh": "翻译失败",
      "authors": [
        "Frank J. Brooks",
        "Rucha Deshpande"
      ],
      "abstract": "Super-resolution, in-painting, whole-image generation, unpaired\nstyle-transfer, and network-constrained image reconstruction each include an\naspect of machine-learned image synthesis where the actual ground truth is not\nknown at time of use. It is generally difficult to quantitatively and\nauthoritatively evaluate the quality of synthetic images; however, in\nmission-critical biomedical scenarios robust evaluation is paramount. In this\nwork, all practical image-to-image comparisons really are relative\nqualifications, not absolute difference quantifications; and, therefore,\nmeaningful evaluation of generated image quality can be accomplished using the\nTversky Index, which is a well-established measure for assessing perceptual\nsimilarity. This evaluation procedure is developed and then demonstrated using\nmultiple image data sets, both real and simulated. The main result is that when\nthe subjectivity and intrinsic deficiencies of any feature-encoding choice are\nput upfront, Tversky's method leads to intuitive results, whereas traditional\nmethods based on summarizing distances in deep feature spaces do not.",
      "tldr_zh": "这篇论文针对机器生成的生物医学图像（如 super-resolution 和 in-painting）的质量评估问题，提出使用 Tversky Index 作为一种基于 tally 的相似性度量，以应对 ground truth 未知的挑战。方法强调图像比较是相对的而非绝对的，并通过多个真实和模拟数据集进行验证。结果表明，Tversky Index 在处理特征编码的主观性和缺陷时，能提供更直观的评估结果，而传统基于深度特征空间距离的方法则表现欠佳。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "13 pages. Manuscript under review at IEEE. Data available at\n  https://doi.org/10.13012/B2IDB-2642688_V1",
      "pdf_url": "http://arxiv.org/pdf/2503.22658v1",
      "published_date": "2025-03-28 17:44:01 UTC",
      "updated_date": "2025-03-28 17:44:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:02:50.610095"
    },
    {
      "arxiv_id": "2503.22655v1",
      "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaomin Yu",
        "Pengxiang Ding",
        "Wenjie Zhang",
        "Siteng Huang",
        "Songyang Gao",
        "Chengwei Qin",
        "Kejian Wu",
        "Zhaoxin Fan",
        "Ziyue Qiao",
        "Donglin Wang"
      ],
      "abstract": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.",
      "tldr_zh": "本研究提出了一种名为Unicorn的框架，用于仅基于文本数据合成视觉语言模型(VLMs)训练数据，从而避免了对大规模真实图像的依赖。该框架包括三个阶段：首先，通过大型语言模型(LLMs)扩展稀疏标题种子，生成1.2M语义多样的高质量标题；其次，将部分标题处理成471K多轮指令调整任务，以支持复杂推理；最后，将文本表示转化为合成视觉表示，创建Unicorn-1.2M预训练数据集和Unicorn-471K-Instruction指令调整数据集。这种方法显著降低了成本，提高了数据质量和可扩展性，代码已在GitHub开源。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22655v1",
      "published_date": "2025-03-28 17:43:00 UTC",
      "updated_date": "2025-03-28 17:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:03:02.843281"
    },
    {
      "arxiv_id": "2503.22782v1",
      "title": "Patronus: Bringing Transparency to Diffusion Models with Prototypes",
      "title_zh": "Patronus：通过原型为扩散模型带来透明度",
      "authors": [
        "Nina Weng",
        "Aasa Feragen",
        "Siavash Bigdeli"
      ],
      "abstract": "Diffusion-based generative models, such as Denoising Diffusion Probabilistic\nModels (DDPMs), have achieved remarkable success in image generation, but their\nstep-by-step denoising process remains opaque, leaving critical aspects of the\ngeneration mechanism unexplained. To address this, we introduce\n\\emph{Patronus}, an interpretable diffusion model inspired by ProtoPNet.\nPatronus integrates a prototypical network into DDPMs, enabling the extraction\nof prototypes and conditioning of the generation process on their prototype\nactivation vector. This design enhances interpretability by showing the learned\nprototypes and how they influence the generation process. Additionally, the\nmodel supports downstream tasks like image manipulation, enabling more\ntransparent and controlled modifications. Moreover, Patronus could reveal\nshortcut learning in the generation process by detecting unwanted correlations\nbetween learned prototypes. Notably, Patronus operates entirely without any\nannotations or text prompts. This work opens new avenues for understanding and\ncontrolling diffusion models through prototype-based interpretability. Our code\nis available at\n\\href{https://github.com/nina-weng/patronus}{https://github.com/nina-weng/patronus}.",
      "tldr_zh": "本文提出 Patronus，一种基于 ProtoPNet 的可解释扩散模型（Diffusion Models），旨在解决如 Denoising Diffusion Probabilistic Models (DDPMs) 在图像生成过程中的不透明问题，通过整合原型网络提取原型并使用原型激活向量调节生成过程。Patronus 增强了模型的可解释性，允许用户查看学到的原型及其对生成的影响，并支持图像操作和检测 shortcut learning，从而实现更透明的控制和修改。无需任何标注或文本提示，该方法为理解和优化 Diffusion Models 开辟了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22782v1",
      "published_date": "2025-03-28 17:31:40 UTC",
      "updated_date": "2025-03-28 17:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:03:14.875398"
    },
    {
      "arxiv_id": "2503.22634v1",
      "title": "Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Wei",
        "Abhinav Agarwal",
        "Boyuan Chen",
        "Rohan Bosworth",
        "Nicholas Pfaff",
        "Russ Tedrake"
      ],
      "abstract": "In imitation learning for robotics, cotraining with demonstration data\ngenerated both in simulation and on real hardware has emerged as a powerful\nrecipe to overcome the sim2real gap. This work seeks to elucidate basic\nprinciples of this sim-and-real cotraining to help inform simulation design,\nsim-and-real dataset creation, and policy training. Focusing narrowly on the\ncanonical task of planar pushing from camera inputs enabled us to be thorough\nin our study. These experiments confirm that cotraining with simulated data\n\\emph{can} dramatically improve performance in real, especially when real data\nis limited. Performance gains scale with simulated data, but eventually\nplateau; real-world data increases this performance ceiling. The results also\nsuggest that reducing the domain gap in physics may be more important than\nvisual fidelity for non-prehensile manipulation tasks. Perhaps surprisingly,\nhaving some visual domain gap actually helps the cotrained policy -- binary\nprobes reveal that high-performing policies learn to distinguish simulated\ndomains from real. We conclude by investigating this nuance and mechanisms that\nfacilitate positive transfer between sim-and-real. In total, our experiments\nspan over 40 real-world policies (evaluated on 800+ trials) and 200 simulated\npolicies (evaluated on 40,000+ trials).",
      "tldr_zh": "这篇论文通过实证分析探讨了在机器人模仿学习中，使用模拟和真实数据共同训练(diffusion policies)来克服 sim2real 差距，焦点是像素级平面推动(planar pushing)任务。研究发现，模拟数据能显著提升真实世界性能，尤其是当真实数据有限时；性能随模拟数据增加而改善，但会达到平台期，而真实数据可进一步提高这一上限。结果还表明，减少物理领域差距比提升视觉保真度更重要，且适度的视觉领域差距有助于政策学习区分模拟和真实领域，从而促进正向转移。总体实验涉及超过40个真实世界政策（800+试验）和200个模拟政策（40,000+试验），为模拟设计和数据集创建提供了指导。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 15 figures, In Submission to IROS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.22634v1",
      "published_date": "2025-03-28 17:25:57 UTC",
      "updated_date": "2025-03-28 17:25:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:03:27.767592"
    },
    {
      "arxiv_id": "2503.22625v1",
      "title": "Challenges and Paths Towards AI for Software Engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Gu",
        "Naman Jain",
        "Wen-Ding Li",
        "Manish Shetty",
        "Yijia Shao",
        "Ziyang Li",
        "Diyi Yang",
        "Kevin Ellis",
        "Koushik Sen",
        "Armando Solar-Lezama"
      ],
      "abstract": "AI for software engineering has made remarkable progress recently, becoming a\nnotable success within generative AI. Despite this, there are still many\nchallenges that need to be addressed before automated software engineering\nreaches its full potential. It should be possible to reach high levels of\nautomation where humans can focus on the critical decisions of what to build\nand how to balance difficult tradeoffs while most routine development effort is\nautomated away. Reaching this level of automation will require substantial\nresearch and engineering efforts across academia and industry. In this paper,\nwe aim to discuss progress towards this in a threefold manner. First, we\nprovide a structured taxonomy of concrete tasks in AI for software engineering,\nemphasizing the many other tasks in software engineering beyond code generation\nand completion. Second, we outline several key bottlenecks that limit current\napproaches. Finally, we provide an opinionated list of promising research\ndirections toward making progress on these bottlenecks, hoping to inspire\nfuture research in this rapidly maturing field.",
      "tldr_zh": "这篇论文探讨了AI for Software Engineering的挑战与发展路径，强调尽管该领域在生成式AI中取得了显著进展，但仍需克服诸多瓶颈以实现高自动化，让人类专注于关键决策如系统设计和权衡取舍。作者采用三折结构：首先，提供AI for Software Engineering任务的结构化taxonomy，涵盖代码生成和完成以外的更多软件工程任务；其次，概述当前方法的限制性瓶颈；最后，给出有前途的研究方向，旨在激励学术和工业界的进一步努力，以推动该领域的成熟。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "75 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.22625v1",
      "published_date": "2025-03-28 17:17:57 UTC",
      "updated_date": "2025-03-28 17:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:03:39.354960"
    },
    {
      "arxiv_id": "2504.03715v1",
      "title": "Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces",
      "title_zh": "非结构化和无界空间中的多目标质量多样性",
      "authors": [
        "Hannah Janmohamed",
        "Antoine Cully"
      ],
      "abstract": "Quality-Diversity algorithms are powerful tools for discovering diverse,\nhigh-performing solutions. Recently, Multi-Objective Quality-Diversity (MOQD)\nextends QD to problems with several objectives while preserving solution\ndiversity. MOQD has shown promise in fields such as robotics and materials\nscience, where finding trade-offs between competing objectives like energy\nefficiency and speed, or material properties is essential. However, existing\nmethods in MOQD rely on tessellating the feature space into a grid structure,\nwhich prevents their application in domains where feature spaces are unknown or\nmust be learned, such as complex biological systems or latent exploration\ntasks. In this work, we introduce Multi-Objective Unstructured Repertoire for\nQuality-Diversity (MOUR-QD), a MOQD algorithm designed for unstructured and\nunbounded feature spaces. We evaluate MOUR-QD on five robotic tasks.\nImportantly, we show that our method excels in tasks where features must be\nlearned, paving the way for applying MOQD to unsupervised domains. We also\ndemonstrate that MOUR-QD is advantageous in domains with unbounded feature\nspaces, outperforming existing grid-based methods. Finally, we demonstrate that\nMOUR-QD is competitive with established MOQD methods on existing MOQD tasks and\nachieves double the MOQD-score in some environments. MOUR-QD opens up new\nopportunities for MOQD in domains like protein design and image generation.",
      "tldr_zh": "本文提出 Multi-Objective Unstructured Repertoire for Quality-Diversity (MOUR-QD)，一种新的 Multi-Objective Quality-Diversity (MOQD) 算法，针对非结构化和无界特征空间，解决了传统 MOQD 方法依赖网格划分的局限性，使其适用于未知或需要学习的领域，如复杂生物系统和潜在探索任务。  \n在五个机器人任务上评估显示，MOUR-QD 在学习特征的任务中表现出色，并优于现有网格-based 方法，在某些环境中 MOQD-score 翻倍。  \n这项创新为 MOQD 在蛋白质设计和图像生成等领域的应用开辟了新机会，提供更灵活和高性能的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted GECCO 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03715v1",
      "published_date": "2025-03-28 16:55:39 UTC",
      "updated_date": "2025-03-28 16:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:03:51.358765"
    },
    {
      "arxiv_id": "2503.22610v1",
      "title": "Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users",
      "title_zh": "评估多模态语言模型作为视力受损用户的视觉助手",
      "authors": [
        "Antonia Karamolegkou",
        "Malvina Nikandrou",
        "Georgios Pantazopoulos",
        "Danae Sanchez Villegas",
        "Phillip Rust",
        "Ruchira Dhar",
        "Daniel Hershcovich",
        "Anders Søgaard"
      ],
      "abstract": "This paper explores the effectiveness of Multimodal Large Language models\n(MLLMs) as assistive technologies for visually impaired individuals. We conduct\na user survey to identify adoption patterns and key challenges users face with\nsuch technologies. Despite a high adoption rate of these models, our findings\nhighlight concerns related to contextual understanding, cultural sensitivity,\nand complex scene understanding, particularly for individuals who may rely\nsolely on them for visual interpretation. Informed by these results, we collate\nfive user-centred tasks with image and video inputs, including a novel task on\nOptical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals\nthat further advancements are necessary to overcome limitations related to\ncultural context, multilingual support, Braille reading comprehension,\nassistive object recognition, and hallucinations. This work provides critical\ninsights into the future direction of multimodal AI for accessibility,\nunderscoring the need for more inclusive, robust, and trustworthy visual\nassistance technologies.",
      "tldr_zh": "这篇论文评估了多模态大型语言模型 (MLLMs) 作为视力障碍者视觉辅助工具的有效性，通过用户调查揭示了高采用率但存在上下文理解、文化敏感性和复杂场景理解等挑战。研究汇集了五个用户中心任务，包括图像、视频输入和新颖的光学布莱叶识别 (Optical Braille Recognition)，并系统评估了十二个 MLLMs。结果显示，MLLMs 在文化上下文、多语言支持、布莱叶阅读和幻觉等方面仍需改进，以推动更具包容性、稳健和可信赖的视觉辅助技术发展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22610v1",
      "published_date": "2025-03-28 16:54:25 UTC",
      "updated_date": "2025-03-28 16:54:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:04:02.989205"
    },
    {
      "arxiv_id": "2503.22600v1",
      "title": "Generative Latent Neural PDE Solver using Flow Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Zijie Li",
        "Anthony Zhou",
        "Amir Barati Farimani"
      ],
      "abstract": "Autoregressive next-step prediction models have become the de-facto standard\nfor building data-driven neural solvers to forecast time-dependent partial\ndifferential equations (PDEs). Denoise training that is closely related to\ndiffusion probabilistic model has been shown to enhance the temporal stability\nof neural solvers, while its stochastic inference mechanism enables ensemble\npredictions and uncertainty quantification. In principle, such training\ninvolves sampling a series of discretized diffusion timesteps during both\ntraining and inference, inevitably increasing computational overhead. In\naddition, most diffusion models apply isotropic Gaussian noise on structured,\nuniform grids, limiting their adaptability to irregular domains. We propose a\nlatent diffusion model for PDE simulation that embeds the PDE state in a\nlower-dimensional latent space, which significantly reduces computational\ncosts. Our framework uses an autoencoder to map different types of meshes onto\na unified structured latent grid, capturing complex geometries. By analyzing\ncommon diffusion paths, we propose to use a coarsely sampled noise schedule\nfrom flow matching for both training and testing. Numerical experiments show\nthat the proposed model outperforms several deterministic baselines in both\naccuracy and long-term stability, highlighting the potential of diffusion-based\napproaches for robust data-driven PDE learning.",
      "tldr_zh": "本文提出了一种基于 Flow Matching 的潜在扩散模型，用于数据驱动的神经 PDE 求解器，以解决传统自回归模型的计算开销和对不规则域的适应性问题。该框架利用自编码器将 PDE 状态嵌入较低维的潜在空间，并将不同网格映射到统一的结构化潜在网格，同时采用粗采样噪声时间表进行高效训练和测试。数值实验表明，该模型在准确性和长期稳定性上优于若干确定性基线，突显了扩散模型在鲁棒 PDE 学习中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.22600v1",
      "published_date": "2025-03-28 16:44:28 UTC",
      "updated_date": "2025-03-28 16:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:04:15.162831"
    },
    {
      "arxiv_id": "2503.22592v1",
      "title": "KEVS: Enhancing Segmentation of Visceral Adipose Tissue in Pre-Cystectomy CT with Gaussian Kernel Density Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Boucher",
        "Nicholas Tetlow",
        "Annie Fung",
        "Amy Dewar",
        "Pietro Arina",
        "Sven Kerneis",
        "John Whittle",
        "Evangelos B. Mazomenos"
      ],
      "abstract": "Purpose: The distribution of visceral adipose tissue (VAT) in cystectomy\npatients is indicative of the incidence of post-operative complications.\nExisting VAT segmentation methods for computed tomography (CT) employing\nintensity thresholding have limitations relating to inter-observer variability.\nMoreover, the difficulty in creating ground-truth masks limits the development\nof deep learning (DL) models for this task. This paper introduces a novel\nmethod for VAT prediction in pre-cystectomy CT, which is fully automated and\ndoes not require ground-truth VAT masks for training, overcoming aforementioned\nlimitations. Methods: We introduce the Kernel density Enhanced VAT Segmentator\n( KEVS), combining a DL semantic segmentation model, for multi-body feature\nprediction, with Gaussian kernel density estimation analysis of predicted\nsubcutaneous adipose tissue to achieve accurate scan-specific predictions of\nVAT in the abdominal cavity. Uniquely for a DL pipeline, KEVS does not require\nground-truth VAT masks. Results: We verify the ability of KEVS to accurately\nsegment abdominal organs in unseen CT data and compare KEVS VAT segmentation\npredictions to existing state-of-the-art (SOTA) approaches in a dataset of 20\npre-cystectomy CT scans, collected from University College London Hospital\n(UCLH-Cyst), with expert ground-truth annotations. KEVS presents a 4.80% and\n6.02% improvement in Dice Coefficient over the second best DL and\nthresholding-based VAT segmentation techniques respectively when evaluated on\nUCLH-Cyst. Conclusion: This research introduces KEVS; an automated, SOTA method\nfor the prediction of VAT in pre-cystectomy CT which eliminates inter-observer\nvariability and is trained entirely on open-source CT datasets which do not\ncontain ground-truth VAT masks.",
      "tldr_zh": "本研究旨在通过高斯核密度估计(Gaussian Kernel Density Estimation)提升膀胱切除术前CT图像中内脏脂肪组织(Visceral Adipose Tissue, VAT)的分割精度，以预测术后并发症，并解决现有强度阈值法存在的观察者间变异性和深度学习(DL)模型数据标注难题。研究提出KEVS方法，该框架结合DL语义分割模型预测多体特征，并利用高斯核密度估计分析皮下脂肪组织，实现无需地面实况VAT掩码的自动化分割。实验结果显示，在UCLH-Cyst数据集上，KEVS的Dice系数较第二佳DL方法和阈值法分别提高了4.80%和6.02%。总之，KEVS作为一种state-of-the-art (SOTA)方法，使用开源CT数据集训练，消除了观察者变异性并提升了VAT预测的可靠性和效率。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Preprint for submission to IPCAI special edition of IJCARS 2025,\n  version prior to any peer review",
      "pdf_url": "http://arxiv.org/pdf/2503.22592v1",
      "published_date": "2025-03-28 16:41:09 UTC",
      "updated_date": "2025-03-28 16:41:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:04:27.344661"
    },
    {
      "arxiv_id": "2503.22589v1",
      "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos, 1952-2012",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Breuer",
        "Bryce J. Dietrich",
        "Michael H. Crespin",
        "Matthew Butler",
        "J. A. Pyrse",
        "Kosuke Imai"
      ],
      "abstract": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.",
      "tldr_zh": "该研究构建了最大的美国总统竞选电视广告数据集（1952-2012），包括9,707个数字格式的广告、机器可搜索的transcripts和高质量summaries，以支持学术研究。论文提出了一种大规模并行化的AI分析管道，利用LLM-based工具自动化视频的准备、转录和总结过程，并通过人类评估证明其输出质量与手动生成相当。作者展示了数据集的应用，例如追踪七十年总统选举中关键议题的演变，并提供了可复用的代码库来扩展至其他视频数据集。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.MM",
      "comment": "17 pages, 7 tables, 4 figures, and linked datasets",
      "pdf_url": "http://arxiv.org/pdf/2503.22589v1",
      "published_date": "2025-03-28 16:36:23 UTC",
      "updated_date": "2025-03-28 16:36:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:04:37.889104"
    },
    {
      "arxiv_id": "2503.22585v1",
      "title": "Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Cohen",
        "Laura Manrique-Gómez",
        "Rubén Manrique"
      ],
      "abstract": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.",
      "tldr_zh": "本研究探索使用大型语言模型(LLMs)如BERT和GPT-4o来增强数据集并提升19世纪拉丁美洲西班牙语报纸中的irony detection。研究采用了两种策略：数据集增强以丰富情感和上下文线索，但其对历史语言分析的影响有限；以及半自动化标注过程，该方法有效解决了类别不平衡问题，并通过高质量标注扩充数据集。最终贡献包括引入一个新的历史西班牙语数据集，用于sentiment analysis和irony detection，以及提出一种半自动化标注方法，该方法依赖人类专家完善LLMs的结果并融入历史和文化背景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22585v1",
      "published_date": "2025-03-28 16:33:24 UTC",
      "updated_date": "2025-03-28 16:33:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:04:51.023129"
    },
    {
      "arxiv_id": "2503.22577v2",
      "title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization",
      "title_zh": "通过多语言文本正则化打破视觉语言模型中的语言障碍",
      "authors": [
        "Iñigo Pikabea",
        "Iñaki Lacunza",
        "Oriol Pareras",
        "Carlos Escolano",
        "Aitor Gonzalez-Agirre",
        "Javier Hernando",
        "Marta Villegas"
      ],
      "abstract": "Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.",
      "tldr_zh": "该研究解决了Visual Language Models (VLMs) 在处理多语言输入时常生成的英文响应问题，这种现象被称为Image-induced Fidelity Loss (IFL)，主要源于多模态多语言训练数据的不足。研究提出了一种连续的多语言整合策略，通过在视觉指令调优过程中注入文本-only的多语言数据，来保留模型的原始多语言能力。实验结果显示，该方法显著提升了语言保真度，同时不降低视觉性能，并与模型合并方法相比，提供了一个无权衡、可扩展的方案，促进VLMs的全球应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "v2: Expanded model merging experiments. Fix duplicated subsection on\n  limitations",
      "pdf_url": "http://arxiv.org/pdf/2503.22577v2",
      "published_date": "2025-03-28 16:26:52 UTC",
      "updated_date": "2025-05-20 10:29:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:05:02.339596"
    },
    {
      "arxiv_id": "2503.22575v1",
      "title": "On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations",
      "title_zh": "关于深度强化学习实现可互换性的错误假设",
      "authors": [
        "Rajdeep Singh Hundal",
        "Yan Xiao",
        "Xiaochun Cao",
        "Jin Song Dong",
        "Manuel Rigger"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence\nwhere an agent uses a neural network to learn which actions to take in a given\nenvironment. DRL has recently gained traction from being able to solve complex\nenvironments like driving simulators, 3D robotic control, and\nmultiplayer-online-battle-arena video games. Numerous implementations of the\nstate-of-the-art algorithms responsible for training these agents, like the\nDeep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms,\ncurrently exist. However, studies make the mistake of assuming implementations\nof the same algorithm to be consistent and thus, interchangeable. In this\npaper, through a differential testing lens, we present the results of studying\nthe extent of implementation inconsistencies, their effect on the\nimplementations' performance, as well as their impact on the conclusions of\nprior studies under the assumption of interchangeable implementations. The\noutcomes of our differential tests showed significant discrepancies between the\ntested algorithm implementations, indicating that they are not interchangeable.\nIn particular, out of the five PPO implementations tested on 56 games, three\nimplementations achieved superhuman performance for 50% of their total trials\nwhile the other two implementations only achieved superhuman performance for\nless than 15% of their total trials. As part of a meticulous manual analysis of\nthe implementations' source code, we analyzed implementation discrepancies and\ndetermined that code-level inconsistencies primarily caused these\ndiscrepancies. Lastly, we replicated a study and showed that this assumption of\nimplementation interchangeability was sufficient to flip experiment outcomes.\nTherefore, this calls for a shift in how implementations are being used.",
      "tldr_zh": "本研究质疑了深度强化学习（Deep Reinforcement Learning, DRL）算法实现的互换性假设，通过差分测试（differential testing）评估了多个实现，如 Proximal Policy Optimization (PPO)和 Deep Q-Network (DQN)，发现这些实现之间存在显著差异，导致性能不一致。实验结果显示，在56个游戏测试中，五种PPO实现中有三者在50%的试验中达到超人性能，而其他两者的成功率不足15%，这些差异主要源于代码级不一致性。该研究还复制了一个现有研究，证明互换性假设可能导致实验结论翻转，从而呼吁重新审视DRL实现的选用和标准化实践。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.5; I.2.6"
      ],
      "primary_category": "cs.SE",
      "comment": "To be published in the 47th International Conference on Software\n  Engineering (ICSE 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.22575v1",
      "published_date": "2025-03-28 16:25:06 UTC",
      "updated_date": "2025-03-28 16:25:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:05:14.912458"
    },
    {
      "arxiv_id": "2504.03714v1",
      "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models",
      "title_zh": "盾牌的突破：揭示大语言模型的漏洞",
      "authors": [
        "Runpeng Dai",
        "Run Yang",
        "Fan Zhou",
        "Hongtu Zhu"
      ],
      "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have become\nessential to general artificial intelligence, exhibiting remarkable\ncapabilities in task understanding and problem-solving. However, the real-world\nreliability of these models critically depends on their stability, which\nremains an underexplored area. Despite their widespread use, rigorous studies\nexamining the stability of LLMs under various perturbations are still lacking.\nIn this paper, we address this gap by proposing a novel stability measure for\nLLMs, inspired by statistical methods rooted in information geometry. Our\nmeasure possesses desirable invariance properties, making it well-suited for\nanalyzing model sensitivity to both parameter and input perturbations. To\nassess the effectiveness of our approach, we conduct extensive experiments on\nmodels ranging in size from 1.5B to 13B parameters. Our results demonstrate the\nutility of our measure in identifying salient parameters and detecting\nvulnerable regions in input images or critical dimensions in token embeddings.\nFurthermore, leveraging our stability framework, we enhance model robustness\nduring model merging, leading to improved performance.",
      "tldr_zh": "本研究揭示了 Large Language Models (LLMs) 和 Vision-Language Models (VLMs) 在任务理解和问题解决方面的强大能力，但强调了其稳定性不足的问题，尤其是对参数和输入扰动的敏感性。论文提出了一种基于信息几何统计方法的创新稳定性度量，该度量具备不变性属性，能够有效识别关键参数、输入图像中的脆弱区域以及 token embeddings 中的关键维度。通过在1.5B至13B参数规模模型上的广泛实验，研究证明了这一框架的实用性，并通过增强模型合并过程显著提高了模型的鲁棒性和整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03714v1",
      "published_date": "2025-03-28 16:23:59 UTC",
      "updated_date": "2025-03-28 16:23:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:05:26.593178"
    },
    {
      "arxiv_id": "2503.22573v1",
      "title": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines",
      "title_zh": "端到端 AI 管道的加密可验证性框架",
      "authors": [
        "Kar Balan",
        "Robert Learney",
        "Tim Wood"
      ],
      "abstract": "The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies.",
      "tldr_zh": "本论文提出一个框架，用于端到端 AI Pipelines 的 Cryptographic Verifiability，旨在提升 AI 开发的透明性、信任和可审计性，以应对全球 AI 安全法规的需求。该框架识别关键组件，并分析现有加密方法在 AI Lifecycle 各个阶段（如数据来源、训练、推理和 Unlearning）的应用，从而提供加密证明来验证 AI 生成资产的来源和正确性，有效对抗错误信息。研究结果强调，需要持续开发高效且可链接的加密工具，以实现 AI 管道的整体可验证性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to 11th ACM International Workshop on Security and Privacy\n  Analytics (IWSPA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.22573v1",
      "published_date": "2025-03-28 16:20:57 UTC",
      "updated_date": "2025-03-28 16:20:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:05:38.589991"
    },
    {
      "arxiv_id": "2503.22562v1",
      "title": "Niyama : Breaking the Silos of LLM Inference Serving",
      "title_zh": "翻译失败",
      "authors": [
        "Kanishk Goel",
        "Jayashree Mohan",
        "Nipun Kwatra",
        "Ravi Shreyas Anupindi",
        "Ramachandran Ramjee"
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies.",
      "tldr_zh": "该论文提出 Niyama，一种新型的 QoS 驱动推理服务系统，旨在打破 LLM（Large Language Models）推理服务的隔离基础设施问题，通过细粒度 QoS 分类和动态调度，实现多样化工作负载在共享基础设施上的高效共调度。Niyama 利用 LLM 推理的可预测特性，引入动态分块机制、混合优先级策略和选择性请求降级，以平衡公平性、效率并在过载条件下实现优雅服务降级。实验结果显示，Niyama 比传统隔离部署提高服务容量 32%，并将 SLO（Service Level Objective）违规减少一个数量级。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22562v1",
      "published_date": "2025-03-28 16:04:20 UTC",
      "updated_date": "2025-03-28 16:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:05:51.176224"
    },
    {
      "arxiv_id": "2504.08754v3",
      "title": "Towards Personalized Conversational Sales Agents : Contextual User Profiling for Strategic Action",
      "title_zh": "朝向个性化的对话式销售代理：情境用户画像用于战略行动",
      "authors": [
        "Tongyoung Kim",
        "Jeongeun Lee",
        "Soojin Yoon",
        "Sunghwan Kim",
        "Dongha Lee"
      ],
      "abstract": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce.",
      "tldr_zh": "该研究针对对话推荐系统（CRSs），提出了一种新的任务Conversational Sales (CSales)，它整合偏好收集、推荐和说服，以处理电商中复杂的用户决策过程。研究开发了CSUser，一个基于LLM的用户模拟器，利用真实数据模拟多样化的用户配置文件和个性，从而实现更真实的评估。同时，引入了CSI对话销售代理，该代理通过对话主动推断上下文用户配置文件，以制定个性化行动计划。实验结果表明，CSUser能有效模拟真实用户行为，并证明上下文配置对战略行动选择的重要性，最终提升了电商成功购买率。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08754v3",
      "published_date": "2025-03-28 15:49:52 UTC",
      "updated_date": "2025-04-16 07:59:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:06:02.328324"
    },
    {
      "arxiv_id": "2504.00024v1",
      "title": "A multi-locus predictiveness curve and its summary assessment for genetic risk prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Changshuai Wei",
        "Ming Li",
        "Yalu Wen",
        "Chengyin Ye",
        "Qing Lu"
      ],
      "abstract": "With the advance of high-throughput genotyping and sequencing technologies,\nit becomes feasible to comprehensive evaluate the role of massive genetic\npredictors in disease prediction. There exists, therefore, a critical need for\ndeveloping appropriate statistical measurements to access the combined effects\nof these genetic variants in disease prediction. Predictiveness curve is\ncommonly used as a graphical tool to measure the predictive ability of a risk\nprediction model on a single continuous biomarker. Yet, for most complex\ndiseases, risk prediciton models are formed on multiple genetic variants. We\ntherefore propose a multi-marker predictiveness curve and provide a\nnon-parametric method to construct the curve for case-control studies. We\nfurther introduce a global predictiveness U and a partial predictiveness U to\nsummarize prediction curve across the whole population and sub-population of\nclinical interest, respectively. We also demonstrate the connections of\npredictiveness curve with ROC curve and Lorenz curve. Through simulation, we\ncompared the performance of the predictiveness U to other three summary\nindices: R square, Total Gain, and Average Entropy, and showed that\nPredictiveness U outperformed the other three indexes in terms of unbiasedness\nand robustness. Moreover, we simulated a series of rare-variants disease model,\nfound partial predictiveness U performed better than global predictiveness U.\nFinally, we conducted a real data analysis, using predictiveness curve and\npredictiveness U to evaluate a risk prediction model for Nicotine Dependence.",
      "tldr_zh": "这篇论文针对遗传风险预测，提出了一种 multi-locus predictiveness curve，用于评估多个遗传变异体在复杂疾病预测中的联合效果，并采用非参数方法在病例对照研究中构建该曲线。论文引入了 global predictiveness U 和 partial predictiveness U 作为总结指标，分别评估整体人群和子人群的预测性能，并探讨了该曲线与 ROC curve 和 Lorenz curve 的联系。通过模拟实验，predictiveness U 在无偏性和鲁棒性上优于其他指标如 R square、Total Gain 和 Average Entropy；在稀有变异体疾病模型中，partial predictiveness U 的表现更佳。最后，论文通过实际数据分析评估了尼古丁依赖的风险预测模型，证明了该方法的实用性。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00024v1",
      "published_date": "2025-03-28 15:49:39 UTC",
      "updated_date": "2025-03-28 15:49:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:06:16.206560"
    },
    {
      "arxiv_id": "2504.01981v1",
      "title": "NLS: Natural-Level Synthesis for Hardware Implementation Through GenAI",
      "title_zh": "NLS：通过 GenAI 进行硬件实现的自然级合成",
      "authors": [
        "Kaiyuan Yang",
        "Huang Ouyang",
        "Xinyi Wang",
        "Bingjie Lu",
        "Yanbo Wang",
        "Charith Abhayaratne",
        "Sizhao Li",
        "Long Jin",
        "Tiantai Deng"
      ],
      "abstract": "This paper introduces Natural-Level Synthesis, an innovative approach for\ngenerating hardware using generative artificial intelligence on both the system\nlevel and component-level. NLS bridges a gap in current hardware development\nprocesses, where algorithm and application engineers' involvement typically\nends at the requirements stage. With NLS, engineers can participate more deeply\nin the development, synthesis, and test stages by using Gen-AI models to\nconvert natural language descriptions directly into Hardware Description\nLanguage code. This approach not only streamlines hardware development but also\nimproves accessibility, fostering a collaborative workflow between hardware and\nalgorithm engineers. We developed the NLS tool to facilitate natural\nlanguage-driven HDL synthesis, enabling rapid generation of system-level HDL\ndesigns while significantly reducing development complexity. Evaluated through\ncase studies and benchmarks using Performance, Power, and Area metrics, NLS\nshows its potential to enhance resource efficiency in hardware development.\nThis work provides a extensible, efficient solution for hardware synthesis and\nestablishes a Visual Studio Code Extension to assess Gen-AI-driven HDL\ngeneration and system integration, laying a foundation for future AI-enhanced\nand AI-in-the-loop Electronic Design Automation tools.",
      "tldr_zh": "这篇论文介绍了 Natural-Level Synthesis (NLS)，一种创新方法，通过 GenAI 将自然语言描述直接转换为 Hardware Description Language (HDL) 代码，实现系统级和组件级的硬件生成。\nNLS 解决了传统硬件开发流程的局限性，让算法和应用工程师能更深入参与合成和测试阶段，从而简化开发过程、提高可访问性和硬件与算法工程师间的协作。\n研究开发了 NLS 工具，并通过案例研究和基准测试（基于 Performance, Power, and Area 指标）证明了其在资源效率方面的潜力，为未来的 AI 增强型电子设计自动化工具奠定基础。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "9 pages, 4 figures, and 5 tables. Submitted for IEEE Transactions on\n  CAD. The same content was accepted by Design Automation Conference 2025 as a\n  WIP Poster (not count as publication, so it's ok to submit the content\n  elsewhere). TCAD info: https://ieeexplore.ieee.org/document/10186100\n  Submitted for review on 26th of Feb. Reference - TCAD-2025-0203",
      "pdf_url": "http://arxiv.org/pdf/2504.01981v1",
      "published_date": "2025-03-28 15:46:01 UTC",
      "updated_date": "2025-03-28 15:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:06:27.627763"
    },
    {
      "arxiv_id": "2503.22541v1",
      "title": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles",
      "title_zh": "SafeCast：风险响应式自动驾驶车辆运动预测",
      "authors": [
        "Haicheng Liao",
        "Hanlin Kong",
        "Bin Rao",
        "Bonan Wang",
        "Chengyue Wang",
        "Guyang Yu",
        "Yuming Huang",
        "Ruru Tang",
        "Chengzhong Xu",
        "Zhenning Li"
      ],
      "abstract": "Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems.",
      "tldr_zh": "该论文提出 SafeCast，一种风险响应式运动预测模型，旨在解决现有自动驾驶系统在安全约束和复杂交通交互方面的不足，通过整合 Responsibility-Sensitive Safety (RSS) 框架来编码可解释的安全规则，如安全距离和碰撞避免。模型还引入 Graph Uncertainty Feature (GUF)，一个基于图注意网络的模块，用于注入可学习噪声以捕捉不确定性和提升泛化能力。在四个真实世界数据集（NGSIM、HighD、ApolloScape 和 MoCAD）上评估，SafeCast 实现了 state-of-the-art (SOTA) 准确性，同时保持轻量级架构和低推理延迟，为实时安全关键自动驾驶系统提供了潜在解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22541v1",
      "published_date": "2025-03-28 15:38:21 UTC",
      "updated_date": "2025-03-28 15:38:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:06:39.604522"
    },
    {
      "arxiv_id": "2503.22537v1",
      "title": "LIM: Large Interpolator Model for Dynamic Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Remy Sabathier",
        "Niloy J. Mitra",
        "David Novotny"
      ],
      "abstract": "Reconstructing dynamic assets from video data is central to many in computer\nvision and graphics tasks. Existing 4D reconstruction approaches are limited by\ncategory-specific models or slow optimization-based methods. Inspired by the\nrecent Large Reconstruction Model (LRM), we present the Large Interpolation\nModel (LIM), a transformer-based feed-forward solution, guided by a novel\ncausal consistency loss, for interpolating implicit 3D representations across\ntime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces\na deformed shape at any continuous time $t\\in[t_0,t_1]$, delivering\nhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicit\nmesh tracking across time, producing a consistently uv-textured mesh sequence\nready for integration into existing production pipelines. We also use LIM, in\nconjunction with a diffusion-based multiview generator, to produce dynamic 4D\nreconstructions from monocular videos. We evaluate LIM on various dynamic\ndatasets, benchmarking against image-space interpolation methods (e.g., FiLM)\nand direct triplane linear interpolation, and demonstrate clear advantages. In\nsummary, LIM is the first feed-forward model capable of high-speed tracked 4D\nasset reconstruction across diverse categories.",
      "tldr_zh": "本研究提出 LIM（Large Interpolator Model），一个基于 Transformer 的前向模型，用于从视频数据高效重建动态 4D 资产，解决现有方法的类别局限性和优化速度问题。LIM 通过一个新颖的因果一致性损失（causal consistency loss）来插值隐式 3D 表示，从而从给定时间 $t_0$ 和 $t_1$ 的表示快速生成任意连续时间 $t$ 的变形形状，并支持显式网格跟踪以输出一致的 UV-纹理化网格序列。该模型还可与扩散-based 多视图生成器结合，从单目视频实现动态 4D 重建。在各种动态数据集上的评估中，LIM 比图像空间插值方法（如 FiLM）和直接 triplane 线性插值表现出明显优势，作为第一个高速 feed-forward 模型，适用于跨类别的高质量 4D 资产重建。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22537v1",
      "published_date": "2025-03-28 15:36:53 UTC",
      "updated_date": "2025-03-28 15:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:06:52.258959"
    },
    {
      "arxiv_id": "2503.22526v1",
      "title": "AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with Fine-Grained Categorization",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Kišš",
        "Michal Hradiš",
        "Martina Dvořáková",
        "Václav Jiroušek",
        "Filip Kersch"
      ],
      "abstract": "We introduce the AnnoPage Dataset, a novel collection of 7550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.",
      "tldr_zh": "我们介绍了 AnnoPage Dataset，这是一个包含 7550 页历史文档（如捷克和德语文件，从 1485 年至今，主要聚焦 19 世纪末到 20 世纪初）的新数据集，旨在支持文档布局分析和对象检测研究。每个页面标注了 25 类非文本元素的轴对齐边界框 (AABB)，包括图像、地图、装饰元素和图表等，标注工作由专家图书管理员完成，以确保准确性和一致性。数据集分为开发和测试子集，并提供了 YOLO 和 DETR 对象检测器的基线结果，作为未来研究的参考点；它已公开可用于 Zenodo（DOI: 10.5281/zenodo.12788419）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 2 tables, 6 figures; Submitted to ICDAR25",
      "pdf_url": "http://arxiv.org/pdf/2503.22526v1",
      "published_date": "2025-03-28 15:30:42 UTC",
      "updated_date": "2025-03-28 15:30:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:07:04.197355"
    },
    {
      "arxiv_id": "2503.22524v1",
      "title": "Robust Offline Imitation Learning Through State-level Trajectory Stitching",
      "title_zh": "通过状态级别轨迹拼接的鲁棒离线模仿学习",
      "authors": [
        "Shuze Wang",
        "Yunpeng Mei",
        "Hongjie Cao",
        "Yetian Yuan",
        "Gang Wang",
        "Jian Sun",
        "Jie Chen"
      ],
      "abstract": "Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance.",
      "tldr_zh": "本研究针对传统模仿学习(Imitation Learning)依赖高质量专家数据并受 covariate shift 影响的局限性，提出了一种鲁棒的离线 IL 方法，通过整合混合质量数据集来提升政策学习。方法引入 state-based search framework，从不完美演示中拼接状态-动作对，生成更多样化和信息丰富的训练轨迹片段。实验结果显示，该方法在标准 IL 基准和真实机器人任务上显著提高了泛化能力和性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22524v1",
      "published_date": "2025-03-28 15:28:36 UTC",
      "updated_date": "2025-03-28 15:28:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:07:15.359077"
    },
    {
      "arxiv_id": "2503.22517v2",
      "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities",
      "title_zh": "翻译失败",
      "authors": [
        "Raman Dutt",
        "Harleen Hanspal",
        "Guoxuan Xia",
        "Petru-Daniel Tudosiu",
        "Alexander Black",
        "Yongxin Yang",
        "Steven McDonagh",
        "Sarah Parisot"
      ],
      "abstract": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.",
      "tldr_zh": "本文提出一种方法，通过利用 Mixture-of-Experts (MoEs) 中的参数冗余，增强预训练的文本-only 大语言模型 (LLMs) 的多模态生成能力，同时满足两个核心约束：C1 保持原有语言生成性能的微小下降，以及 C2 在小参数预算下学习新模态。方法包括对新模态的 tokens 应用 low-rank adaptation，以及引入基于 Gromov-Wasserstein 距离的参数初始化方案，以提升训练收敛和稳定性。实验分析显示，该方法通过路由机制形成模态特定路径并减少专家冗余，能够无缝应用于各种 LLMs，实现从单模态到多模态架构的过渡。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22517v2",
      "published_date": "2025-03-28 15:21:24 UTC",
      "updated_date": "2025-04-01 10:42:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:07:28.561375"
    },
    {
      "arxiv_id": "2503.22513v1",
      "title": "Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets",
      "title_zh": "大规模数据集上文本识别Transformer的掩码自监督预训练",
      "authors": [
        "Martin Kišš",
        "Michal Hradiš"
      ],
      "abstract": "Self-supervised learning has emerged as a powerful approach for leveraging\nlarge-scale unlabeled data to improve model performance in various domains. In\nthis paper, we explore masked self-supervised pre-training for text recognition\ntransformers. Specifically, we propose two modifications to the pre-training\nphase: progressively increasing the masking probability, and modifying the loss\nfunction to incorporate both masked and non-masked patches. We conduct\nextensive experiments using a dataset of 50M unlabeled text lines for\npre-training and four differently sized annotated datasets for fine-tuning.\nFurthermore, we compare our pre-trained models against those trained with\ntransfer learning, demonstrating the effectiveness of the self-supervised\npre-training. In particular, pre-training consistently improves the character\nerror rate of models, in some cases up to 30 % relatively. It is also on par\nwith transfer learning but without relying on extra annotated text lines.",
      "tldr_zh": "这篇论文探索了masked self-supervised pre-training方法，用于提升文本识别transformers在大型无标签数据集上的性能。具体来说，作者提出逐步增加masking probability并修改loss function以同时处理masked和non-masked patches，从而更好地利用50M无标签文本行进行预训练，并在四个不同规模的标注数据集上进行微调。实验结果显示，这种预训练方法显著降低了字符错误率（character error rate），在某些情况下相对改善高达30%，并与transfer learning效果相当，但无需依赖额外标注数据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 7 tables, 6 figures; Submitted to ICDAR25",
      "pdf_url": "http://arxiv.org/pdf/2503.22513v1",
      "published_date": "2025-03-28 15:16:48 UTC",
      "updated_date": "2025-03-28 15:16:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:07:40.381729"
    },
    {
      "arxiv_id": "2504.01980v3",
      "title": "Information Gain Is Not All You Need",
      "title_zh": "信息增益并非你所需的一切",
      "authors": [
        "Ludvig Ericson",
        "José Pedro",
        "Patric Jensfelt"
      ],
      "abstract": "Autonomous exploration in mobile robotics often involves a trade-off between\ntwo objectives: maximizing environmental coverage and minimizing the total path\nlength. In the widely used information gain paradigm, exploration is guided by\nthe expected value of observations. While this approach is effective under\nbudget-constrained settings--where only a limited number of observations can be\nmade--it fails to align with quality-constrained scenarios, in which the robot\nmust fully explore the environment to a desired level of certainty or quality.\nIn such cases, total information gain is effectively fixed, and maximizing it\nper step can lead to inefficient, greedy behavior and unnecessary backtracking.\nThis paper argues that information gain should not serve as an optimization\nobjective in quality-constrained exploration. Instead, it should be used to\nfilter viable candidate actions. We propose a novel heuristic, distance\nadvantage, which selects candidate frontiers based on a trade-off between\nproximity to the robot and remoteness from other frontiers. This heuristic aims\nto reduce future detours by prioritizing exploration of isolated regions before\nthe robot's opportunity to visit them efficiently has passed. We evaluate our\nmethod in simulated environments against classical frontier-based exploration\nand gain-maximizing approaches. Results show that distance advantage\nsignificantly reduces total path length across a variety of environments, both\nwith and without access to prior map predictions. Our findings challenge the\nassumption that more accurate gain estimation improves performance and offer a\nmore suitable alternative for the quality-constrained exploration paradigm.",
      "tldr_zh": "本论文质疑了信息增益在移动机器人自主探索中的主导作用，指出在质量受限场景（需完全探索到所需质量）下，最大化信息增益会导致低效行为和不必要回溯，而非预算受限场景。作者提出一种新启发式方法distance advantage，通过权衡前沿的接近度和远离度来筛选候选动作，从而优先探索孤立区域并减少未来绕路。实验结果显示，该方法在各种模拟环境中显著降低总路径长度，与经典前沿探索和增益最大化方法相比，性能更优，并挑战了更准确信息增益估计就能改善表现的假设。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 6 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2504.01980v3",
      "published_date": "2025-03-28 15:03:52 UTC",
      "updated_date": "2025-04-20 13:01:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:07:51.521554"
    },
    {
      "arxiv_id": "2503.22478v1",
      "title": "Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent",
      "title_zh": "翻译失败",
      "authors": [
        "Max Hennick",
        "Stijn De Baerdemacker"
      ],
      "abstract": "We show that the behavior of stochastic gradient descent is related to\nBayesian statistics by showing that SGD is effectively diffusion on a fractal\nlandscape, where the fractal dimension can be accounted for in a purely\nBayesian way. By doing this we show that SGD can be regarded as a modified\nBayesian sampler which accounts for accessibility constraints induced by the\nfractal structure of the loss landscape. We verify our results experimentally\nby examining the diffusion of weights during training. These results offer\ninsight into the factors which determine the learning process, and seemingly\nanswer the question of how SGD and purely Bayesian sampling are related.",
      "tldr_zh": "本研究揭示了随机梯度下降 (SGD) 的行为与贝叶斯统计的关联，将 SGD 视为在分形景观上的扩散，并通过贝叶斯方式解释其分形维度。\n论文证明 SGD 可以被视为一个修改后的贝叶斯采样器，考虑了分形结构带来的可访问性约束，从而解决了 SGD 与纯贝叶斯采样的关系问题。\n通过实验验证权重在训练中的扩散，这些发现为理解机器学习过程中的关键因素提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22478v1",
      "published_date": "2025-03-28 14:38:39 UTC",
      "updated_date": "2025-03-28 14:38:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:08:04.001204"
    },
    {
      "arxiv_id": "2504.03713v1",
      "title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
      "title_zh": "RLDBF：通过强化学习",
      "authors": [
        "Weichen Dai",
        "Zijie Dai",
        "Zhijie Huang",
        "Yixuan Pan",
        "Xinhe Li",
        "Xi Li",
        "Yi Zhou",
        "Ji Qi",
        "Wu Jiang"
      ],
      "abstract": "While current large language models (LLMs) demonstrate remarkable linguistic\ncapabilities through training on massive unstructured text corpora, they remain\ninadequate in leveraging structured scientific data (e.g., chemical molecular\nproperties in databases) that encapsulate centuries of accumulated scientific\nexpertise. These structured datasets hold strategic significance for advancing\nAI for Science yet current approaches merely treat them as auxiliary\nsupplements to unstructured text. This study pioneers a systematic\ninvestigation into enhancing LLMs with structured scientific data, using\nchemical molecular science as a testbed. We investigate the impact of\nincorporating molecular property data on LLM across distinct training phases,\nincluding continual pre-training, supervised fine-tuning, and reinforcement\nlearning. Notably, to address the inherent limitation of numerical\ninsensitivity in large models, we propose an innovative methodology termed\n\"Reinforcement Learning with Database Feedback\" (RLDBF). Experimental\nevaluations demonstrate the efficacy of the proposed approach, with the model\nexhibiting remarkable generalization capabilities on previously unseen data and\nother chemical tasks. The results substantiate the potential of our method in\nadvancing the field of structured scientific data processing within LLMs.",
      "tldr_zh": "这篇论文探讨了如何增强大型语言模型（LLMs），以更好地利用结构化科学数据（如化学分子属性数据库），而非仅将其作为非结构化文本的补充。研究以化学分子科学为测试床，调查了在持续预训练、监督微调和强化学习阶段整合这些数据的影响，并提出创新方法RLDBF（Reinforcement Learning with Database Feedback），以解决模型对数字不敏感的问题。实验结果显示，该方法显著提升了模型在未见过数据和其他化学任务上的泛化能力，并证明了其在处理结构化科学数据方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03713v1",
      "published_date": "2025-03-28 14:18:29 UTC",
      "updated_date": "2025-03-28 14:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:08:16.196339"
    },
    {
      "arxiv_id": "2503.22458v1",
      "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Shengyue Guan",
        "Haoyi Xiong",
        "Jindong Wang",
        "Jiang Bian",
        "Bin Zhu",
        "Jian-guang Lou"
      ],
      "abstract": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.",
      "tldr_zh": "本调查系统评估了大型语言模型 (LLM)-based 代理在多轮对话中的表现，使用 PRISMA-inspired 框架审查了近 250 篇学术来源。论文提出了两个相关分类系统：一个定义了评估内容，包括任务完成、响应质量、用户体验、记忆和上下文保留、规划和工具集成；另一个解释了评估方法，涵盖基于注解的评估、自动化指标（如 BLEU 和 ROUGE 分数）、混合策略以及利用 LLM 的自评方法。这些分类确保了对对话代理的整体性能进行全面和有意义的评估，为未来研究提供了坚实基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22458v1",
      "published_date": "2025-03-28 14:08:40 UTC",
      "updated_date": "2025-03-28 14:08:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:08:28.092670"
    },
    {
      "arxiv_id": "2503.22456v2",
      "title": "Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning",
      "title_zh": "熵引导的序列加权，用于基于强化学习的LLM微调中的高效探索",
      "authors": [
        "Abdullah Vanlioglu"
      ],
      "abstract": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.",
      "tldr_zh": "我们引入了 Entropy-Guided Sequence Weighting (EGSW)，一种新方法，用于强化学习 (RL) 基于的大语言模型 (LLM) 微调，通过动态基于优势和熵分配权重来平衡探索-利用权衡。EGSW 整合了 entropy regularization 和 advantage-based weighting，并采用 temperature-scaled softmax 来优先高奖励和高不确定性的序列，从而提高训练稳定性。实验结果表明，EGSW 显著提升了 Group Relative Policy Optimization (GRPO) 的推理能力和样本效率，且可推广到其他 RL 算法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22456v2",
      "published_date": "2025-03-28 14:07:51 UTC",
      "updated_date": "2025-03-31 10:13:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:08:40.184492"
    },
    {
      "arxiv_id": "2503.22454v1",
      "title": "A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination",
      "title_zh": "一种测量和缓解非二元处理歧视的因果",
      "authors": [
        "Ayan Majumdar",
        "Deborah D. Kanubala",
        "Kavya Gupta",
        "Isabel Valera"
      ],
      "abstract": "Fairness studies of algorithmic decision-making systems often simplify\ncomplex decision processes, such as bail or loan approvals, into binary\nclassification tasks. However, these approaches overlook that such decisions\nare not inherently binary (e.g., approve or not approve bail or loan); they\nalso involve non-binary treatment decisions (e.g., bail conditions or loan\nterms) that can influence the downstream outcomes (e.g., loan repayment or\nreoffending). In this paper, we argue that non-binary treatment decisions are\nintegral to the decision process and controlled by decision-makers and,\ntherefore, should be central to fairness analyses in algorithmic\ndecision-making. We propose a causal framework that extends fairness analyses\nand explicitly distinguishes between decision-subjects' covariates and the\ntreatment decisions. This specification allows decision-makers to use our\nframework to (i) measure treatment disparity and its downstream effects in\nhistorical data and, using counterfactual reasoning, (ii) mitigate the impact\nof past unfair treatment decisions when automating decision-making. We use our\nframework to empirically analyze four widely used loan approval datasets to\nreveal potential disparity in non-binary treatment decisions and their\ndiscriminatory impact on outcomes, highlighting the need to incorporate\ntreatment decisions in fairness assessments. Moreover, by intervening in\ntreatment decisions, we show that our framework effectively mitigates treatment\ndiscrimination from historical data to ensure fair risk score estimation and\n(non-binary) decision-making processes that benefit all stakeholders.",
      "tldr_zh": "本论文提出一个因果框架（causal framework），用于测量和缓解算法决策中的非二元治疗歧视（non-binary treatment discrimination），强调这些决策（如保释条件或贷款条款）对下游结果的影响往往被传统二元分类方法忽略。该框架通过区分决策主体的协变量和治疗决策，允许分析历史数据中的治疗差异及其影响，并利用反事实推理（counterfactual reasoning）来减轻过去不公平决策的负面效应。在四个常用贷款批准数据集的实证分析中，该框架揭示了非二元治疗决策中的潜在歧视，并通过干预这些决策，实现了更公平的风险评分和决策过程，从而惠及所有利益相关者。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.22454v1",
      "published_date": "2025-03-28 14:06:35 UTC",
      "updated_date": "2025-03-28 14:06:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:08:51.852928"
    },
    {
      "arxiv_id": "2503.22424v1",
      "title": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching",
      "title_zh": "CoSIL：通过LLM驱动的代码仓库图搜索进行软件问题定位",
      "authors": [
        "Zhonghao Jiang",
        "Xiaoxue Ren",
        "Meng Yan",
        "Wei Jiang",
        "Yong Li",
        "Zhongxin Liu"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent.",
      "tldr_zh": "本文提出 CoSIL，一种基于 LLM 的软件问题定位方法，通过模块调用图减少搜索空间，并利用 LLM 动态构建函数调用图进行迭代搜索和上下文修剪，以平衡上下文的简洁性和全面性。CoSIL 无需训练或索引，直接在函数级别实现高效定位。实验结果显示，在 SWE bench Lite 和 SWE bench Verified 上，CoSIL 的 Top-1 成功率分别为 43% 和 44.6%，比现有方法高 8.6% 到 98.2%；当应用于补丁生成时，解决率进一步提升 9.3% 到 31.5%。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22424v1",
      "published_date": "2025-03-28 13:36:26 UTC",
      "updated_date": "2025-03-28 13:36:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:09:04.667497"
    },
    {
      "arxiv_id": "2503.22406v1",
      "title": "Training Large Language Models for Advanced Typosquatting Detection",
      "title_zh": "训练大型语言模型用于高级错别字劫持检测",
      "authors": [
        "Jackson Welch"
      ],
      "abstract": "Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection.",
      "tldr_zh": "本研究针对 Typosquatting（一种利用用户输入 URL 错误的网络威胁）提出了一种先进检测方法，通过训练 Large Language Models (LLMs) 来提升识别复杂攻击的能力。方法基于字符级转换和模式启发式规则进行训练，而非依赖特定领域数据，从而实现更强的适应性和鲁棒性。实验结果显示，Phi-4 14B 模型在微调后仅需几千样本即可达到 98% 的准确率，显著优于传统方法。该工作突显了 LLMs 在网络安全中的潜力，特别是防范基于域名的欺骗策略，并为优化机器学习威胁检测策略提供了宝贵见解。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.22406v1",
      "published_date": "2025-03-28 13:16:27 UTC",
      "updated_date": "2025-03-28 13:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:09:15.263192"
    },
    {
      "arxiv_id": "2504.03712v1",
      "title": "Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Lewen",
        "Max Pargmann",
        "Jenia Jitsev",
        "Mehdi Cherti",
        "Robert Pitz-Paal",
        "Daniel Maldonado Quinto"
      ],
      "abstract": "Concentrating Solar Power (CSP) plants are a key technology in the transition\ntoward sustainable energy. A critical factor for their safe and efficient\noperation is the distribution of concentrated solar flux on the receiver.\nHowever, flux distributions from individual heliostats are sensitive to surface\nimperfections. Measuring these surfaces across many heliostats remains\nimpractical in real-world deployments. As a result, control systems often\nassume idealized heliostat surfaces, leading to suboptimal performance and\npotential safety risks. To address this, inverse Deep Learning Raytracing\n(iDLR) has been introduced as a novel method for inferring heliostat surface\nprofiles from target images recorded during standard calibration procedures. In\nthis work, we present the first successful Sim-to-Real transfer of iDLR,\nenabling accurate surface predictions directly from real-world target images.\nWe evaluate our method on 63 heliostats under real operational conditions. iDLR\nsurface predictions achieve a median mean absolute error (MAE) of 0.17 mm and\nshow good agreement with deflectometry ground truth in 84% of cases. When used\nin raytracing simulations, it enables flux density predictions with a mean\naccuracy of 90% compared to deflectometry over our dataset, and outperforms the\ncommonly used ideal heliostat surface assumption by 26%. We tested this\napproach in a challenging double-extrapolation scenario-involving unseen sun\npositions and receiver projection-and found that iDLR maintains high predictive\naccuracy, highlighting its generalization capabilities. Our results demonstrate\nthat iDLR is a scalable, automated, and cost-effective solution for integrating\nrealistic heliostat surface models into digital twins. This opens the door to\nimproved flux control, more precise performance modeling, and ultimately,\nenhanced efficiency and safety in future CSP plants.",
      "tldr_zh": "本研究针对Concentrating Solar Power (CSP) 植物中heliostat表面缺陷导致的太阳光集中分布问题，提出inverse Deep Learning Raytracing (iDLR)方法，通过从真实目标图像推断heliostat表面轮廓，实现首次Sim-to-Real transfer。实验在63个heliostat上进行，iDLR预测的表面中值均绝对误差(MAE)为0.17 mm，与deflectometry实测一致性达84%。在raytracing模拟中，iDLR使flux density预测准确率达到90%，比理想heliostat表面假设高26%，并在双重外推场景（如未见太阳位置和接收器投影）中保持高泛化性能。该方法提供了一个可扩展、自动化的解决方案，提升CSP植物的flux控制、性能建模以及整体效率和安全。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03712v1",
      "published_date": "2025-03-28 13:15:05 UTC",
      "updated_date": "2025-03-28 13:15:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:09:29.030782"
    },
    {
      "arxiv_id": "2503.22402v1",
      "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhang Zhu",
        "Runzhi Jiang",
        "Boyan Li",
        "Nan Tang",
        "Yuyu Luo"
      ],
      "abstract": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.",
      "tldr_zh": "该论文提出 EllieSQL，一种基于复杂性感知路由的成本高效 Text-to-SQL 框架，旨在解决现有 LLM-based 方法的高计算成本问题，通过将查询根据复杂度分配到合适的 SQL 生成管道，实现资源优化。论文引入 Token Elasticity of Performance (TEP) 指标，从经济学视角量化性能提升相对于 token 投资的响应性，以评估成本效率。实验结果显示，使用 Qwen2.5-0.5B-DPO 路由器，EllieSQL 相比始终采用高级方法，可减少 token 使用超过 40%，在 Bird 开发集上保持性能不变，同时 TEP 提升超过 2 倍。该框架不仅推动 Text-to-SQL 的可持续发展，还呼吁研究社区在追求性能的同时重视资源效率。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "comment": "19 pages, 8 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.22402v1",
      "published_date": "2025-03-28 13:11:27 UTC",
      "updated_date": "2025-03-28 13:11:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:09:40.450145"
    },
    {
      "arxiv_id": "2503.22396v1",
      "title": "On-site estimation of battery electrochemical parameters via transfer learning based physics-informed neural network approach",
      "title_zh": "通过基于迁移学习的物理信息神经网络方法进行电池电化学参数的现场估计",
      "authors": [
        "Josu Yeregui",
        "Iker Lopetegi",
        "Sergio Fernandez",
        "Erik Garayalde",
        "Unai Iraola"
      ],
      "abstract": "This paper presents a novel physical parameter estimation framework for\non-site model characterization, using a two-phase modelling strategy with\nPhysics-Informed Neural Networks (PINNs) and transfer learning (TL). In the\nfirst phase, a PINN is trained using only the physical principles of the single\nparticle model (SPM) equations. In the second phase, the majority of the PINN\nparameters are frozen, while critical electrochemical parameters are set as\ntrainable and adjusted using real-world voltage profile data. The proposed\napproach significantly reduces computational costs, making it suitable for\nreal-time implementation on Battery Management Systems (BMS). Additionally, as\nthe initial phase does not require field data, the model is easy to deploy with\nminimal setup requirements. With the proposed methodology, we have been able to\neffectively estimate relevant electrochemical parameters with operating data.\nThis has been proved estimating diffusivities and active material volume\nfractions with charge data in different degradation conditions. The methodology\nis experimentally validated in a Raspberry Pi device using data from a standard\ncharge profile with a 3.89\\% relative accuracy estimating the active material\nvolume fractions of a NMC cell with 82.09\\% of its nominal capacity.",
      "tldr_zh": "本研究提出了一种基于 Physics-Informed Neural Networks (PINNs) 和 transfer learning (TL) 的新型框架，用于现场估计电池电化学参数，采用两阶段建模策略。第一阶段仅使用 single particle model (SPM) 方程的物理原理训练 PINN，无需实地数据；第二阶段冻结大部分参数，并通过真实电压数据调整关键电化学参数，从而显著降低计算成本，适合实时部署在 Battery Management Systems (BMS) 上。该方法成功估计了 diffusivities 和 active material volume fractions 等参数，并在不同退化条件下进行了验证。实验在 Raspberry Pi 设备上使用 NMC 电池数据证明了其有效性，估计算得 active material volume fractions 的相对准确率达 3.89%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22396v1",
      "published_date": "2025-03-28 13:06:41 UTC",
      "updated_date": "2025-03-28 13:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:09:52.579035"
    },
    {
      "arxiv_id": "2503.22394v1",
      "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Rulin Zhou",
        "Wenlong He",
        "An Wang",
        "Qiqi Yao",
        "Haijun Hu",
        "Jiankun Wang",
        "Xi Zhang an Hongliang Ren"
      ],
      "abstract": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5.",
      "tldr_zh": "该研究提出 Endo-TTAP 框架，用于内镜视频中实现鲁棒的组织点跟踪，解决复杂变形、仪器遮挡和标注稀缺等挑战。框架的核心包括 Multi-Facet Guided Attention (MFGA) 模块，该模块整合多尺度流动态、DINOv2 语义嵌入和显式运动模式，以预测点位置并处理不确定性和遮挡；以及两阶段课程学习策略，通过 Auxiliary Curriculum Adapter (ACA) 利用合成数据、光学流监督和混合学习（如无监督流一致性及半监督伪标签）进行渐进初始化和训练。在多个 MICCAI 挑战数据集和自收集数据集上，Endo-TTAP 实现了最先进跟踪性能，尤其在复杂内镜条件下，显著提升了手术导航和场景理解的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22394v1",
      "published_date": "2025-03-28 13:00:07 UTC",
      "updated_date": "2025-03-28 13:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:10:04.966888"
    },
    {
      "arxiv_id": "2503.22374v1",
      "title": "ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation",
      "title_zh": "ViSketch-GPT：协作多尺度特征提取用于草图识别和生成",
      "authors": [
        "Giulio Federico",
        "Giuseppe Amato",
        "Fabio Carrara",
        "Claudio Gennaro",
        "Marco Di Benedetto"
      ],
      "abstract": "Understanding the nature of human sketches is challenging because of the wide\nvariation in how they are created. Recognizing complex structural patterns\nimproves both the accuracy in recognizing sketches and the fidelity of the\ngenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm\ndesigned to address these challenges through a multi-scale context extraction\napproach. The model captures intricate details at multiple scales and combines\nthem using an ensemble-like mechanism, where the extracted features work\ncollaboratively to enhance the recognition and generation of key details\ncrucial for classification and generation tasks.\n  The effectiveness of ViSketch-GPT is validated through extensive experiments\non the QuickDraw dataset. Our model establishes a new benchmark, significantly\noutperforming existing methods in both classification and generation tasks,\nwith substantial improvements in accuracy and the fidelity of generated\nsketches.\n  The proposed algorithm offers a robust framework for understanding complex\nstructures by extracting features that collaborate to recognize intricate\ndetails, enhancing the understanding of structures like sketches and making it\na versatile tool for various applications in computer vision and machine\nlearning.",
      "tldr_zh": "本研究引入了ViSketch-GPT，一种新型算法，通过multi-scale context extraction的多尺度特征提取方法，捕捉草图的复杂细节，并利用协作机制结合这些特征，提升草图识别和生成的准确性与保真度。模型在QuickDraw数据集上进行广泛实验，显著超越现有方法，在classification和generation任务中建立了新基准，准确率和生成质量均有实质性提升。该框架为理解复杂结构提供了一个鲁棒的工具，可应用于计算机视觉和机器学习的各种场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22374v1",
      "published_date": "2025-03-28 12:28:30 UTC",
      "updated_date": "2025-03-28 12:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:10:14.983374"
    },
    {
      "arxiv_id": "2503.22363v1",
      "title": "ForcePose: A Deep Learning Approach for Force Calculation Based on Action Recognition Using MediaPipe Pose Estimation Combined with Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Nandakishor M",
        "Vrinda Govind V",
        "Anuradha Puthalath",
        "Anzy L",
        "Swathi P S",
        "Aswathi R",
        "Devaprabha A R",
        "Varsha Raj",
        "Midhuna Krishnan K",
        "Akhila Anilkumar T V",
        "Yamuna P V"
      ],
      "abstract": "Force estimation in human-object interactions is crucial for various fields\nlike ergonomics, physical therapy, and sports science. Traditional methods\ndepend on specialized equipment such as force plates and sensors, which makes\naccurate assessments both expensive and restricted to laboratory settings. In\nthis paper, we introduce ForcePose, a novel deep learning framework that\nestimates applied forces by combining human pose estimation with object\ndetection. Our approach leverages MediaPipe for skeletal tracking and SSD\nMobileNet for object recognition to create a unified representation of\nhuman-object interaction. We've developed a specialized neural network that\nprocesses both spatial and temporal features to predict force magnitude and\ndirection without needing any physical sensors. After training on our dataset\nof 850 annotated videos with corresponding force measurements, our model\nachieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees in\nforce direction. When compared to existing computer vision approaches, our\nmethod performs 27.5% better while still offering real-time performance on\nstandard computing hardware. ForcePose opens up new possibilities for force\nanalysis in diverse real-world scenarios where traditional measurement tools\nare impractical or intrusive. This paper discusses our methodology, the dataset\ncreation process, evaluation metrics, and potential applications across\nrehabilitation, ergonomics assessment, and athletic performance analysis.",
      "tldr_zh": "该研究提出ForcePose，一种新型深度学习框架，用于通过MediaPipe人体姿势估计和SSD MobileNet物体检测相结合，估算人类-物体互动中的施力大小和方向，从而避免依赖昂贵的物理传感器。框架利用神经网络处理空间和时间特征，在850个带注释视频的数据集上训练后，实现了5.83 N的力大小平均绝对误差和7.4度的力方向误差，并比现有计算机视觉方法提高了27.5%的性能，同时支持标准硬件的实时处理。ForcePose为康复、工效学评估和运动表现分析等领域的实际应用提供了新的可能性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22363v1",
      "published_date": "2025-03-28 12:13:56 UTC",
      "updated_date": "2025-03-28 12:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:10:27.293198"
    },
    {
      "arxiv_id": "2503.22358v1",
      "title": "Shapley Revisited: Tractable Responsibility Measures for Query Answers",
      "title_zh": "翻译失败",
      "authors": [
        "Meghyn Bienvenu",
        "Diego Figueira",
        "Pierre Lafourcade"
      ],
      "abstract": "The Shapley value, originating from cooperative game theory, has been\nemployed to define responsibility measures that quantify the contributions of\ndatabase facts to obtaining a given query answer. For non-numeric queries, this\nis done by considering a cooperative game whose players are the facts and whose\nwealth function assigns 1 or 0 to each subset of the database, depending on\nwhether the query answer holds in the given subset. While conceptually simple,\nthis approach suffers from a notable drawback: the problem of computing such\nShapley values is #P-hard in data complexity, even for simple conjunctive\nqueries. This motivates us to revisit the question of what constitutes a\nreasonable responsibility measure and to introduce a new family of\nresponsibility measures -- weighted sums of minimal supports (WSMS) -- which\nsatisfy intuitive properties. Interestingly, while the definition of WSMSs is\nsimple and bears no obvious resemblance to the Shapley value formula, we prove\nthat every WSMS measure can be equivalently seen as the Shapley value of a\nsuitably defined cooperative game. Moreover, WSMS measures enjoy tractable data\ncomplexity for a large class of queries, including all unions of conjunctive\nqueries. We further explore the combined complexity of WSMS computation and\nestablish (in)tractability results for various subclasses of conjunctive\nqueries.",
      "tldr_zh": "这篇论文重新审视了Shapley value在数据库查询答案责任度量中的应用，因为其计算问题在数据复杂度上为#P-hard，即使对于简单的conjunctive queries。作者引入了weighted sums of minimal supports (WSMS)作为一种新责任度量家族，该方法满足直观的属性，并证明WSMS可以等价视为某个合作博弈的Shapley value。WSMS在大量查询类（如unions of conjunctive queries）上具有可计算的数据复杂度，且论文进一步分析了其组合复杂性，为各种conjunctive queries子类建立了（不可）计算性结果。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Long version of PODS'25 paper",
      "pdf_url": "http://arxiv.org/pdf/2503.22358v1",
      "published_date": "2025-03-28 11:52:26 UTC",
      "updated_date": "2025-03-28 11:52:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:10:40.887026"
    },
    {
      "arxiv_id": "2503.22353v1",
      "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Yubo Li",
        "Yidi Miao",
        "Xueying Ding",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.",
      "tldr_zh": "本论文评估大型语言模型(LLMs)在多轮交互中的一致性表现，提出一个全面框架以提升其在高风险领域的可靠性。主要贡献包括：引入Position-Weighted Consistency (PWC)评分系统，以量化早期稳定性及恢复模式；构建一个覆盖多种领域和难度的基准数据集，用于测试各种挑战性后续场景；以及开发Confidence-Aware Response Generation (CARG)框架，通过整合模型置信度信号来改善响应稳定性。实验结果表明，CARG显著提升了LLMs的稳定性，而不降低准确性，为其在关键应用中的可靠部署提供了重要潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.22353v1",
      "published_date": "2025-03-28 11:49:56 UTC",
      "updated_date": "2025-03-28 11:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:10:51.125168"
    },
    {
      "arxiv_id": "2503.22342v1",
      "title": "CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models",
      "title_zh": "CPPO：加速基于群相对策略优化的推理模型训练",
      "authors": [
        "Zhihang Lin",
        "Mingbao Lin",
        "Yuan Xie",
        "Rongrong Ji"
      ],
      "abstract": "This paper introduces Completion Pruning Policy Optimization (CPPO) to\naccelerate the training of reasoning models based on Group Relative Policy\nOptimization (GRPO). GRPO, while effective, incurs high training costs due to\nthe need for sampling multiple completions for each question. Our experiment\nand theoretical analysis reveals that the number of completions impacts model\naccuracy yet increases training time multiplicatively, and not all completions\ncontribute equally to policy training -- their contribution depends on their\nrelative advantage. To address these issues, we propose CPPO, which prunes\ncompletions with low absolute advantages, significantly reducing the number\nneeded for gradient calculation and updates. Additionally, we introduce a\ndynamic completion allocation strategy to maximize GPU utilization by\nincorporating additional questions, further enhancing training efficiency.\nExperimental results demonstrate that CPPO achieves up to $8.32\\times$ speedup\non GSM8K and $3.51\\times$ on Math while preserving or even enhancing the\naccuracy compared to the original GRPO. We release our code at\nhttps://github.com/lzhxmu/CPPO.",
      "tldr_zh": "本文提出 CPPO（Completion Pruning Policy Optimization）方法，以加速基于 GRPO（Group Relative Policy Optimization）的推理模型训练，解决 GRPO 因采样多个完成而导致的高训练成本问题。CPPO 通过修剪低绝对优势的完成，减少梯度计算所需的数量，并引入动态完成分配策略来最大化 GPU 利用率，从而显著提升训练效率。实验结果显示，CPPO 在 GSM8K 数据集上实现高达 8.32 倍加速，在 Math 数据集上实现 3.51 倍加速，同时保持或提升模型准确率。代码已开源于 https://github.com/lzhxmu/CPPO。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.22342v1",
      "published_date": "2025-03-28 11:30:05 UTC",
      "updated_date": "2025-03-28 11:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:11:04.418099"
    },
    {
      "arxiv_id": "2503.22328v2",
      "title": "VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow",
      "title_zh": "VoteFlow：强制局部刚性于自监督场景流",
      "authors": [
        "Yancong Lin",
        "Shiming Wang",
        "Liangliang Nan",
        "Julian Kooij",
        "Holger Caesar"
      ],
      "abstract": "Scene flow estimation aims to recover per-point motion from two adjacent\nLiDAR scans. However, in real-world applications such as autonomous driving,\npoints rarely move independently of others, especially for nearby points\nbelonging to the same object, which often share the same motion. Incorporating\nthis locally rigid motion constraint has been a key challenge in\nself-supervised scene flow estimation, which is often addressed by\npost-processing or appending extra regularization. While these approaches are\nable to improve the rigidity of predicted flows, they lack an architectural\ninductive bias for local rigidity within the model structure, leading to\nsuboptimal learning efficiency and inferior performance. In contrast, we\nenforce local rigidity with a lightweight add-on module in neural network\ndesign, enabling end-to-end learning. We design a discretized voting space that\naccommodates all possible translations and then identify the one shared by\nnearby points by differentiable voting. Additionally, to ensure computational\nefficiency, we operate on pillars rather than points and learn representative\nfeatures for voting per pillar. We plug the Voting Module into popular model\ndesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. We\noutperform baseline works with only marginal compute overhead. Code is\navailable at https://github.com/tudelft-iv/VoteFlow.",
      "tldr_zh": "该论文针对自监督场景流估计中局部刚性约束的挑战，提出VoteFlow框架，以强制执行附近点（如同一物体）的共享运动。该框架通过一个轻量级Voting Module在神经网络中实现端到端学习，利用离散化投票空间和可微投票来识别所有可能的平移，并为提高计算效率在柱子上操作代表性特征。在Argoverse 2和Waymo数据集上，VoteFlow仅增加微小计算开销，便显著优于基线模型，改善了场景流估计的准确性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Code is available at\n  https://github.com/tudelft-iv/VoteFlow. Yancong Lin and Shiming Wang have\n  equal contributions",
      "pdf_url": "http://arxiv.org/pdf/2503.22328v2",
      "published_date": "2025-03-28 11:06:27 UTC",
      "updated_date": "2025-04-16 07:36:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:11:17.072116"
    },
    {
      "arxiv_id": "2503.22776v1",
      "title": "Post-Incorporating Code Structural Knowledge into LLMs via In-Context Learning for Code Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Yali Du",
        "Hui Sun",
        "Ming Li"
      ],
      "abstract": "Code translation migrates codebases across programming languages. Recently,\nlarge language models (LLMs) have achieved significant advancements in software\nmining. However, handling the syntactic structure of source code remains a\nchallenge. Classic syntax-aware methods depend on intricate model architectures\nand loss functions, rendering their integration into LLM training\nresource-intensive. This paper employs in-context learning (ICL), which\ndirectly integrates task exemplars into the input context, to post-incorporate\ncode structural knowledge into pre-trained LLMs. We revisit exemplar selection\nin ICL from an information-theoretic perspective, proposing that list-wise\nselection based on information coverage is more precise and general objective\nthan traditional methods based on combining similarity and diversity. To\naddress the challenges of quantifying information coverage, we introduce a\nsurrogate measure, Coverage of Abstract Syntax Tree (CAST). Furthermore, we\nformulate the NP-hard CAST maximization for exemplar selection and prove that\nit is a standard submodular maximization problem. Therefore, we propose a\ngreedy algorithm for CAST submodular maximization, which theoretically\nguarantees a (1-1/e)-approximate solution in polynomial time complexity. Our\nmethod is the first training-free and model-agnostic approach to\npost-incorporate code structural knowledge into existing LLMs at test time.\nExperimental results show that our method significantly improves LLMs\nperformance and reveals two meaningful insights: 1) Code structural knowledge\ncan be effectively post-incorporated into pre-trained LLMs during inference,\ndespite being overlooked during training; 2) Scaling up model size or training\ndata does not lead to the emergence of code structural knowledge, underscoring\nthe necessity of explicitly considering code syntactic structure.",
      "tldr_zh": "该论文提出了一种通过 In-Context Learning (ICL) 在测试时后整合代码结构知识到预训练大型语言模型 (LLMs) 的方法，以提升代码翻译任务的性能。\n他们从信息理论角度优化 ICL 的示例选择，引入 CAST (Coverage of Abstract Syntax Tree) 作为信息覆盖的代理措施，并证明其最大化问题为子模函数问题，进而提出贪心算法提供高效近似解。\n该方法是首个无需额外训练且模型无关的方案，实验结果显示它显著提高了 LLMs 的翻译准确性，并揭示代码结构知识需显式整合，而非依赖模型规模或训练数据的扩展。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22776v1",
      "published_date": "2025-03-28 10:59:42 UTC",
      "updated_date": "2025-03-28 10:59:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:11:28.078121"
    },
    {
      "arxiv_id": "2503.22324v1",
      "title": "AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation",
      "title_zh": "AH-GS：增强 3D 高斯喷溅用于高频细节表示",
      "authors": [
        "Chenyang Xu",
        "XingGuo Deng",
        "Rui Zhong"
      ],
      "abstract": "The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation\nand view synthesis. Although Scaffold-GS achieves higher quality real-time\nrendering compared to the original 3D-GS, its fine-grained rendering of the\nscene is extremely dependent on adequate viewing angles. The spectral bias of\nneural network learning results in Scaffold-GS's poor ability to perceive and\nlearn high-frequency information in the scene. In this work, we propose\nenhancing the manifold complexity of input features and using network-based\nfeature map loss to improve the image reconstruction quality of 3D-GS models.\nWe introduce AH-GS, which enables 3D Gaussians in structurally complex regions\nto obtain higher-frequency encodings, allowing the model to more effectively\nlearn the high-frequency information of the scene. Additionally, we incorporate\nhigh-frequency reinforce loss to further enhance the model's ability to capture\ndetailed frequency information. Our result demonstrates that our model\nsignificantly improves rendering fidelity, and in specific scenarios (e.g.,\nMipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in\njust 15K iterations.",
      "tldr_zh": "该论文提出 AH-GS，一种增强的 3D Gaussian Splatting 方法，旨在解决 Scaffold-GS 在高频细节表示上的不足，例如对视角依赖性和神经网络谱偏差导致的场景高频信息学习问题。AH-GS 通过增强输入特征的流形复杂性、引入基于网络的特征图损失以及高频强化损失，使 3D Gaussians 在结构复杂区域获得更高频编码，从而提升图像重建质量。实验结果显示，该方法显著提高了渲染保真度，并在特定场景（如 MipNeRf360-garden）中，仅用 15K 迭代就超过了 Scaffold-GS 的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22324v1",
      "published_date": "2025-03-28 10:57:33 UTC",
      "updated_date": "2025-03-28 10:57:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:11:40.501547"
    },
    {
      "arxiv_id": "2504.13866v1",
      "title": "Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises",
      "title_zh": "翻译失败",
      "authors": [
        "Aleksa Marusic",
        "Sao Mai Nguyen",
        "Adriana Tapus"
      ],
      "abstract": "Physical rehabilitation exercises suggested by healthcare professionals can\nhelp recovery from various musculoskeletal disorders and prevent re-injury.\nHowever, patients' engagement tends to decrease over time without direct\nsupervision, which is why there is a need for an automated monitoring system.\nIn recent years, there has been great progress in quality assessment of\nphysical rehabilitation exercises. Most of them only provide a binary\nclassification if the performance is correct or incorrect, and a few provide a\ncontinuous score. This information is not sufficient for patients to improve\ntheir performance. In this work, we propose an algorithm for error\nclassification of rehabilitation exercises, thus making the first step toward\nmore detailed feedback to patients. We focus on skeleton-based exercise\nassessment, which utilizes human pose estimation to evaluate motion. Inspired\nby recent algorithms for quality assessment during rehabilitation exercises, we\npropose a Transformer-based model for the described classification. Our model\nis inspired by the HyperFormer method for human action recognition, and adapted\nto our problem and dataset. The evaluation is done on the KERAAL dataset, as it\nis the only medical dataset with clear error labels for the exercises, and our\nmodel significantly surpasses state-of-the-art methods. Furthermore, we bridge\nthe gap towards better feedback to the patients by presenting a way to\ncalculate the importance of joints for each exercise.",
      "tldr_zh": "本文提出一种基于骨骼的Transformer模型，用于腰痛物理康复锻炼的错误分类，从而提供更详细的反馈。该模型受HyperFormer方法启发，并针对特定问题和KERAAL数据集进行适应，专注于利用人体姿势估计评估动作。在KERAAL数据集上的实验显示，该模型显著超越现有方法，错误分类准确率提升明显。此外，通过计算关节重要性，该框架桥接了向患者提供更好反馈的差距，帮助提升康复效果。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "ICORR 2025 - 19th IEEE/RAS-EMBS International Conference on\n  Rehabilitation Robotics, INTERNATIONAL CONSORTIUM FOR REHABILITATION\n  ROBOTICS, May 2025, Michigan, USA, United States",
      "pdf_url": "http://arxiv.org/pdf/2504.13866v1",
      "published_date": "2025-03-28 10:30:39 UTC",
      "updated_date": "2025-03-28 10:30:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:11:52.422179"
    },
    {
      "arxiv_id": "2503.22276v1",
      "title": "Machine Learning Models for Soil Parameter Prediction Based on Satellite, Weather, Clay and Yield Data",
      "title_zh": "基于卫星、天气、粘土和产量数据的土壤参数预测机器学习模型",
      "authors": [
        "Calvin Kammerlander",
        "Viola Kolb",
        "Marinus Luegmair",
        "Lou Scheermann",
        "Maximilian Schmailzl",
        "Marco Seufert",
        "Jiayun Zhang",
        "Denis Dalic",
        "Torsten Schön"
      ],
      "abstract": "Efficient nutrient management and precise fertilization are essential for\nadvancing modern agriculture, particularly in regions striving to optimize crop\nyields sustainably. The AgroLens project endeavors to address this challenge by\ndevelop ing Machine Learning (ML)-based methodologies to predict soil nutrient\nlevels without reliance on laboratory tests. By leveraging state of the art\ntechniques, the project lays a foundation for acionable insights to improve\nagricultural productivity in resource-constrained areas, such as Africa. The\napproach begins with the development of a robust European model using the LUCAS\nSoil dataset and Sentinel-2 satellite imagery to estimate key soil properties,\nincluding phosphorus, potassium, nitrogen, and pH levels. This model is then\nenhanced by integrating supplementary features, such as weather data, harvest\nrates, and Clay AI-generated embeddings. This report details the methodological\nframework, data preprocessing strategies, and ML pipelines employed in this\nproject. Advanced algorithms, including Random Forests, Extreme Gradient\nBoosting (XGBoost), and Fully Connected Neural Networks (FCNN), were\nimplemented and finetuned for precise nutrient prediction. Results showcase\nrobust model performance, with root mean square error values meeting stringent\naccuracy thresholds. By establishing a reproducible and scalable pipeline for\nsoil nutrient prediction, this research paves the way for transformative\nagricultural applications, including precision fertilization and improved\nresource allocation in underresourced regions like Africa.",
      "tldr_zh": "该研究提出了一种基于机器学习的模型，用于预测土壤参数，包括磷、钾、氮和 pH 水平，旨在实现高效养分管理和精确施肥，尤其针对资源有限的地区如非洲。研究利用 LUCAS Soil 数据集、Sentinel-2 卫星图像、天气数据、收获率和 Clay AI 生成的嵌入作为输入特征，构建了一个鲁棒的欧洲模型，并通过数据预处理和集成补充特征来增强预测准确性。采用 Random Forests、XGBoost 和 Fully Connected Neural Networks (FCNN) 等算法进行模型训练，结果显示根均方误差（RMSE）达到严格阈值，为可复制的土壤养分预测管道奠定基础，促进精准农业应用和资源优化。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This technical report is the documentation of a student project\n  collaboration between Technische Hochschule Ingolstadt and MI4People",
      "pdf_url": "http://arxiv.org/pdf/2503.22276v1",
      "published_date": "2025-03-28 09:44:32 UTC",
      "updated_date": "2025-03-28 09:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:12:03.772193"
    },
    {
      "arxiv_id": "2503.22275v1",
      "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
      "title_zh": "翻译失败",
      "authors": [
        "Shivam Mehta",
        "Nebojsa Jojic",
        "Hannes Gamper"
      ],
      "abstract": "Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance.",
      "tldr_zh": "本文提出了一种新方法，使用Variational Quantization和Conditional Flow Matching，将音频转换为超低比特率（0.23kbps）的离散tokens，从而实现与文本tokens的无缝整合到LLM中。研究通过Low-Rank Adaptation (LoRA)微调预训练的文本-based LLM，评估其在音频理解和生成的多模态能力。结果显示，该tokenizer在各种数据集上优于传统VQ-VAE，尽管tokenization导致细节损失，但多模态LLM在音频理解上与最先进方法相当，而音频生成表现较差。该研究强调，需要更大、更多样化的数据集和改进的评估指标来提升多模态LLM的整体性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "68T07",
        "I.2.7; I.2.6; H.5.5"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 2 figures, Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.22275v1",
      "published_date": "2025-03-28 09:43:47 UTC",
      "updated_date": "2025-03-28 09:43:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:12:16.501056"
    },
    {
      "arxiv_id": "2503.22250v2",
      "title": "Modeling Challenging Patient Interactions: LLMs for Medical Communication Training",
      "title_zh": "翻译失败",
      "authors": [
        "Anna Bodonhelyi",
        "Christian Stegemann-Philipps",
        "Alessandra Sonanini",
        "Lea Herschbach",
        "Marton Szep",
        "Anne Herrmann-Werner",
        "Teresa Festl-Wietek",
        "Enkelejda Kasneci",
        "Friederike Holderried"
      ],
      "abstract": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.",
      "tldr_zh": "本文提出使用 Large Language Models (LLMs) 模拟基于 Satir 模型的 \"accuser\" 和 \"rationalizer\" 患者角色，以解决传统医疗训练中缺乏多样化患者互动的问题，并确保多语言适用性。研究通过 advanced prompt engineering，包括 behavioral prompts、author's notes 和 stubbornness mechanisms，开发了虚拟患者 (VPs)，这些 VPs 体现了细微的情感和对话特征。评估结果显示，医疗专业人员对 VPs 的真实性评分较高（accuser: 3.8 ± 1.0； rationalizer: 3.7 ± 0.8，在 5 点 Likert 量表上），情感分析和情感分数（accuser: 负向 3.1 ± 0.6； rationalizer: 中性 4.0 ± 0.4）验证了沟通风格的差异，这为医疗教育提供了一种可扩展的 AI 驱动工具，提升从业者的同理心和诊断能力。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22250v2",
      "published_date": "2025-03-28 09:04:10 UTC",
      "updated_date": "2025-04-08 17:25:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:12:30.651010"
    },
    {
      "arxiv_id": "2503.22241v2",
      "title": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ziye Chen",
        "Yiqun Duan",
        "Riheng Zhu",
        "Zhenbang Sun",
        "Mingming Gong"
      ],
      "abstract": "Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%.",
      "tldr_zh": "该研究针对个性化多聚类（Personalized multiple clustering）问题，提出了一种以代理为中心的框架，利用多模态大语言模型（Multi-Modal LLMs）作为代理，在关系图上遍历搜索基于用户兴趣的聚类分区，以克服传统 CLIP 嵌入方法在深度上下文理解上的局限。\n该框架通过 MLLMs 的高级推理机制生成更贴合用户偏好的聚类表示，并通过用户兴趣偏置嵌入构建关系图，过滤弱连接边以降低计算开销。\n实验结果显示，该方法在 Card Order 和 Card Suits 基准上分别获得 NMI 分数 0.9667 和 0.9481，比现有最先进模型（SOTA）提升超过 140%。",
      "categories": [
        "cs.AI",
        "68T07, 68T05, 05C82"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22241v2",
      "published_date": "2025-03-28 08:45:15 UTC",
      "updated_date": "2025-03-31 02:56:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:12:41.658091"
    },
    {
      "arxiv_id": "2503.22235v1",
      "title": "WeatherMesh-3: Fast and accurate operational global weather forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Haoxing Du",
        "Lyna Kim",
        "Joan Creus-Costa",
        "Jack Michaels",
        "Anuj Shetty",
        "Todd Hutchinson",
        "Christopher Riedel",
        "John Dean"
      ],
      "abstract": "We present WeatherMesh-3 (WM-3), an operational transformer-based global\nweather forecasting system that improves the state of the art in both accuracy\nand computational efficiency. We introduce the following advances: 1) a latent\nrollout that enables arbitrary-length predictions in latent space without\nintermediate encoding or decoding; and 2) a modular architecture that flexibly\nutilizes mixed-horizon processors and encodes multiple real-time analyses to\ncreate blended initial conditions. WM-3 generates 14-day global forecasts at\n0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a\n>100,000-fold speedup over traditional NWP approaches while achieving superior\naccuracy with up to 37.7% improvement in RMSE over operational models,\nrequiring only a single consumer-grade GPU for deployment. We aim for WM-3 to\ndemocratize weather forecasting by providing an accessible, lightweight model\nfor operational use while pushing the performance boundaries of machine\nlearning-based weather prediction.",
      "tldr_zh": "该研究介绍了 WeatherMesh-3 (WM-3)，一个基于 Transformer 的全球天气预报系统，在准确性和计算效率上超越现有技术。主要创新包括 latent rollout 技术，实现潜空间中的任意长度预测，以及模块化架构利用 mixed-horizon processors 和多个实时分析创建混合初始条件。WM-3 能在单台 RTX 4090 GPU 上生成 14 天、0.25 度分辨率的全球预报，仅需 12 秒，比传统 NWP 方法快超过 100,000 倍，并将 RMSE 改善高达 37.7%。该系统旨在通过提供轻量级、可部署的模型，推动天气预报的民主化和机器学习在该领域的应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22235v1",
      "published_date": "2025-03-28 08:37:59 UTC",
      "updated_date": "2025-03-28 08:37:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:12:52.141108"
    },
    {
      "arxiv_id": "2503.22233v1",
      "title": "Process Reward Modeling with Entropy-Driven Uncertainty",
      "title_zh": "基于熵驱动不确定性的过程奖励建模",
      "authors": [
        "Lang Cao",
        "Renhong Chen",
        "Yingtian Zou",
        "Chao Peng",
        "Wu Ning",
        "Huacong Xu",
        "Qian Chen",
        "Yuxian Wang",
        "Peishuo Su",
        "Mofan Peng",
        "Zijie Chen",
        "Yitong Li"
      ],
      "abstract": "This paper presents the Entropy-Driven Unified Process Reward Model\n(EDU-PRM), a novel framework that approximates state-of-the-art performance in\nprocess supervision while drastically reducing training costs. EDU-PRM\nintroduces an entropy-guided dynamic step partitioning mechanism, using logit\ndistribution entropy to pinpoint high-uncertainty regions during token\ngeneration dynamically. This self-assessment capability enables precise\nstep-level feedback without manual fine-grained annotation, addressing a\ncritical challenge in process supervision. Experiments on the Qwen2.5-72B model\nwith only 7,500 EDU-PRM-generated training queries demonstrate accuracy closely\napproximating the full Qwen2.5-72B-PRM (71.1% vs. 71.6%), achieving a 98%\nreduction in query cost compared to prior methods. This work establishes\nEDU-PRM as an efficient approach for scalable process reward model training.",
      "tldr_zh": "本文提出了一种名为 EDU-PRM 的新框架，用于过程奖励建模，通过熵驱动的不确定性机制大幅降低训练成本，同时接近最先进性能。该框架引入熵-guided dynamic step partitioning 机制，利用 logit 分布熵动态识别高不确定性区域，提供精确的步骤级反馈，而无需手动细粒度标注。实验结果显示，在 Qwen2.5-72B 模型上，使用仅 7500 个 EDU-PRM 生成的训练查询，准确率达到 71.1%，与全模型的 71.6% 相当，并实现了 98% 的查询成本减少。这为可扩展的过程奖励模型训练提供了高效且实用的方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22233v1",
      "published_date": "2025-03-28 08:33:37 UTC",
      "updated_date": "2025-03-28 08:33:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:13:04.699869"
    },
    {
      "arxiv_id": "2503.22228v1",
      "title": "MFH: A Multi-faceted Heuristic Algorithm Selection Approach for Software Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Su",
        "Liansai Deng",
        "Cheng Wen",
        "Rong Wang",
        "Zhi Ma",
        "Nan Zhang",
        "Cong Tian",
        "Zhenhua Duan",
        "Shengchao Qin"
      ],
      "abstract": "Currently, many verification algorithms are available to improve the\nreliability of software systems. Selecting the appropriate verification\nalgorithm typically demands domain expertise and non-trivial manpower. An\nautomated algorithm selector is thus desired. However, existing selectors,\neither depend on machine-learned strategies or manually designed heuristics,\nencounter issues such as reliance on high-quality samples with algorithm labels\nand limited scalability. In this paper, an automated algorithm selection\napproach, namely MFH, is proposed for software verification. Our approach\nleverages the heuristics that verifiers producing correct results typically\nimplement certain appropriate algorithms, and the supported algorithms by these\nverifiers indirectly reflect which ones are potentially applicable.\nSpecifically, MFH embeds the code property graph (CPG) of a semantic-preserving\ntransformed program to enhance the robustness of the prediction model.\nFurthermore, our approach decomposes the selection task into the sub-tasks of\npredicting potentially applicable algorithms and matching the most appropriate\nverifiers. Additionally, MFH also introduces a feedback loop on incorrect\npredictions to improve model prediction accuracy. We evaluate MFH on 20\nverifiers and over 15,000 verification tasks. Experimental results demonstrate\nthe effectiveness of MFH, achieving a prediction accuracy of 91.47% even\nwithout ground truth algorithm labels provided during the training phase.\nMoreover, the prediction accuracy decreases only by 0.84% when introducing 10\nnew verifiers, indicating the strong scalability of the proposed approach.",
      "tldr_zh": "本研究提出了一种多方面启发式算法选择方法MFH，用于软件验证，以自动化选择合适的验证算法，解决现有选择器依赖高质量样本和可扩展性差的问题。MFH利用启发式规则，通过嵌入代码属性图(CPG)到语义保留转换的程序中，并将选择任务分解为预测潜在适用算法和匹配最适当验证器，同时引入反馈循环来改进预测准确率。实验结果显示，在20个验证器和超过15,000个验证任务上，MFH实现了91.47%的预测准确率，即使在训练阶段没有ground truth算法标签；此外，添加10个新验证器时，准确率仅下降0.84%，展示了其强大的可扩展性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "I.2.6; I.2.11; D.2.4"
      ],
      "primary_category": "cs.SE",
      "comment": "The implementation, along with all relevant publicly available data,\n  can be accessed on the Figshare platform:\n  https://figshare.com/s/4f34e1f6adaf98d9be53",
      "pdf_url": "http://arxiv.org/pdf/2503.22228v1",
      "published_date": "2025-03-28 08:21:00 UTC",
      "updated_date": "2025-03-28 08:21:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:13:15.982813"
    },
    {
      "arxiv_id": "2504.08752v1",
      "title": "Patience is all you need! An agentic system for performing scientific literature review",
      "title_zh": "翻译失败",
      "authors": [
        "David Brett",
        "Anniek Myatt"
      ],
      "abstract": "Large language models (LLMs) have grown in their usage to provide support for\nquestion answering across numerous disciplines. The models on their own have\nalready shown promise for answering basic questions, however fail quickly where\nexpert domain knowledge is required or the question is nuanced. Scientific\nresearch often involves searching for relevant literature, distilling pertinent\ninformation from that literature and analysing how the findings support or\ncontradict one another. The information is often encapsulated in the full text\nbody of research articles, rather than just in the abstracts. Statements within\nthese articles frequently require the wider article context to be fully\nunderstood. We have built an LLM-based system that performs such search and\ndistillation of information encapsulated in scientific literature, and we\nevaluate our keyword based search and information distillation system against a\nset of biology related questions from previously released literature\nbenchmarks. We demonstrate sparse retrieval methods exhibit results close to\nstate of the art without the need for dense retrieval, with its associated\ninfrastructure and complexity overhead. We also show how to increase the\ncoverage of relevant documents for literature review generation.",
      "tldr_zh": "本研究提出了一种基于大型语言模型（LLMs）的代理系统，用于执行科学文献综述，旨在解决LLMs在处理需要专家知识或复杂问题时的局限性。该系统通过关键词-based搜索和稀疏检索方法，从科学文章的全文中提取并提炼相关信息，避免了密集检索的复杂基础设施。在针对生物学相关问题的评估中，该系统表现接近最先进水平，并显著提高了相关文档的覆盖率，从而提升了文献综述的效率和准确性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.08752v1",
      "published_date": "2025-03-28 08:08:46 UTC",
      "updated_date": "2025-03-28 08:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:13:27.097986"
    },
    {
      "arxiv_id": "2503.22215v1",
      "title": "Learning to Instruct for Visual Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihan Zhou",
        "Feng Hong",
        "Jiaan Luo",
        "Jiangchao Yao",
        "Dongsheng Li",
        "Bo Han",
        "Ya Zhang",
        "Yanfeng Wang"
      ],
      "abstract": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.",
      "tldr_zh": "该研究提出LIT，一种改进视觉指令调整(VIT)的框架，旨在解决VIT在训练多模态大语言模型(MLLMs)时存在的过拟合和捷径学习问题，这些问题源于过度强调指令遵循而忽略视觉信息的主动理解。LIT通过将损失函数整合到指令和响应序列中，扩展训练数据并防止MLLMs过度依赖语言先验，从而无需额外数据和几乎无计算开销。实验结果显示，LIT在多模态基准测试中相对性能提升高达9%，并显著提升基础视觉能力，如标题生成性能改善18%并减少幻觉现象。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.22215v1",
      "published_date": "2025-03-28 08:04:51 UTC",
      "updated_date": "2025-03-28 08:04:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:13:39.408121"
    },
    {
      "arxiv_id": "2503.22182v1",
      "title": "Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items",
      "title_zh": "先卖再造：通过个性化的 AI 生成物品革新电子商务",
      "authors": [
        "Jianghao Lin",
        "Peng Du",
        "Jiaqi Liu",
        "Weite Li",
        "Yong Yu",
        "Weinan Zhang",
        "Yang Cao"
      ],
      "abstract": "E-commerce has revolutionized retail, yet its traditional workflows remain\ninefficient, with significant time and resource costs tied to product design\nand manufacturing inventory. This paper introduces a novel system deployed at\nAlibaba that leverages AI-generated items (AIGI) to address these challenges\nwith personalized text-to-image generation for e-commercial product design.\nAIGI enables an innovative business mode called \"sell it before you make it\",\nwhere merchants can design fashion items and generate photorealistic images\nwith digital models based on textual descriptions. Only when the items have\nreceived a certain number of orders, do the merchants start to produce them,\nwhich largely reduces reliance on physical prototypes and thus accelerates time\nto market. For such a promising application, we identify the underlying key\nscientific challenge, i.e., capturing the users' group-level personalized\npreferences towards multiple generated candidate images. To this end, we\npropose a Personalized Group-Level Preference Alignment Framework for Diffusion\nModels (i.e., PerFusion). We first design PerFusion Reward Model for user\npreference estimation with a feature-crossing-based personalized plug-in. Then\nwe develop PerFusion with a personalized adaptive network to model diverse\npreferences across users, and meanwhile derive the group-level preference\noptimization objective to capture the comparative behaviors among multiple\ncandidates. Both offline and online experiments demonstrate the effectiveness\nof our proposed algorithm. The AI-generated items have achieved over 13%\nrelative improvements for both click-through rate and conversion rate compared\nto their human-designed counterparts, validating the revolutionary potential of\nAI-generated items for e-commercial platforms.",
      "tldr_zh": "这篇论文介绍了在阿里巴巴部署的 AIGI 系统，通过个性化文本到图像生成，实现“sell it before you make it”的创新电子商务模式，允许商家基于文本描述生成逼真产品图像，并在收到足够订单后才生产，从而减少对物理原型的依赖并加速上市。针对关键挑战，该系统提出 PerFusion 框架，包括 PerFusion Reward Model 用于估计用户偏好，以及个性化自适应网络来建模多样用户偏好和群级优化目标。实验结果显示，AI 生成物品相较于人工设计物品，提高了点击率和转换率超过 13%，验证了其在电子商务平台的革命性潜力。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.22182v1",
      "published_date": "2025-03-28 07:00:33 UTC",
      "updated_date": "2025-03-28 07:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:13:52.323221"
    },
    {
      "arxiv_id": "2503.22181v1",
      "title": "e-person Architecture and Framework for Human-AI Co-adventure Relationship",
      "title_zh": "翻译失败",
      "authors": [
        "Kanako Esaki",
        "Tadayuki Matsumura",
        "Yang Shao",
        "Hiroyuki Mizuno"
      ],
      "abstract": "This paper proposes the e-person architecture for constructing a unified and\nincremental development of AI ethics. The e-person architecture takes the\nreduction of uncertainty through collaborative cognition and action with others\nas a unified basis for ethics. By classifying and defining uncertainty along\ntwo axes - (1) first, second, and third person perspectives, and (2) the\ndifficulty of inference based on the depth of information - we support the\ndevelopment of unified and incremental development of AI ethics. In addition,\nwe propose the e-person framework based on the free energy principle, which\nconsiders the reduction of uncertainty as a unifying principle of brain\nfunction, with the aim of implementing the e-person architecture, and we show\nour previous works and future challenges based on the proposed framework.",
      "tldr_zh": "这项论文提出了 e-person architecture 和 framework，以构建统一的、增量发展的 AI 伦理系统。该架构以通过协作认知和行动减少不确定性作为核心基础，并通过两个轴（第一、第二、第三人称视角，以及基于信息深度的推理难度）来分类和定义不确定性，从而支持 AI 伦理的统一发展。此外，基于 free energy principle 的 e-person framework 将不确定性减少视为大脑功能的统一原则，旨在实现 Human-AI Co-adventure Relationship，并展示了之前的工作和未来挑战。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "24 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.22181v1",
      "published_date": "2025-03-28 06:54:44 UTC",
      "updated_date": "2025-03-28 06:54:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:14:04.056375"
    },
    {
      "arxiv_id": "2503.22178v1",
      "title": "AdaRank: Adaptive Rank Pruning for Enhanced Model Merging",
      "title_zh": "AdaRank：自适应秩剪枝用于增强模型合并",
      "authors": [
        "Chanhyuk Lee",
        "Jiho Choi",
        "Chanryeol Lee",
        "Donggyun Kim",
        "Seunghoon Hong"
      ],
      "abstract": "Model merging has emerged as a promising approach for unifying independently\nfine-tuned models into an integrated framework, significantly enhancing\ncomputational efficiency in multi-task learning. Recently, several SVD-based\ntechniques have been introduced to exploit low-rank structures for enhanced\nmerging, but their reliance on such manually designed rank selection often\nleads to cross-task interference and suboptimal performance. In this paper, we\npropose AdaRank, a novel model merging framework that adaptively selects the\nmost beneficial singular directions of task vectors to merge multiple models.\nWe empirically show that the dominant singular components of task vectors can\ncause critical interference with other tasks, and that naive truncation across\ntasks and layers degrades performance. In contrast, AdaRank dynamically prunes\nthe singular components that cause interference and offers an optimal amount of\ninformation to each task vector by learning to prune ranks during test-time via\nentropy minimization. Our analysis demonstrates that such method mitigates\ndetrimental overlaps among tasks, while empirical results show that AdaRank\nconsistently achieves state-of-the-art performance with various backbones and\nnumber of tasks, reducing the performance gap between fine-tuned models to\nnearly 1%.",
      "tldr_zh": "该论文提出 AdaRank，一种自适应秩修剪框架，用于优化模型合并（model merging），以解决传统基于 SVD 的方法中手动秩选择导致的跨任务干扰和性能次优问题。AdaRank 通过动态修剪任务向量（task vectors）的干扰性奇异组件，并在测试时（test-time）利用熵最小化（entropy minimization）学习最佳修剪策略，从而缓解任务间的有害重叠。实验结果表明，AdaRank 在多种骨干模型和任务数量下实现了最先进性能，将其与微调模型的性能差距缩小到近 1%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Code Available at: https://github.com/david3684/AdaRank",
      "pdf_url": "http://arxiv.org/pdf/2503.22178v1",
      "published_date": "2025-03-28 06:49:06 UTC",
      "updated_date": "2025-03-28 06:49:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:14:16.563859"
    },
    {
      "arxiv_id": "2503.22164v2",
      "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Gao",
        "Yanwen Huang",
        "Yiqiao Liu",
        "Wenxuan Xie",
        "Wei-Ying Ma",
        "Ya-Qin Zhang",
        "Yanyan Lan"
      ],
      "abstract": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
      "tldr_zh": "本论文提出PharmAgents，一种基于Large Language Models (LLMs)的多智能体系统，用于构建虚拟制药生态系统，以加速药物发现流程。\n该系统模拟从目标发现到临床前评估的完整工作流程，通过LLM驱动的代理结合专用机器学习模型和计算工具，实现结构化知识交换、化合物优化、分子属性提升以及毒性和合成可行性的in silico分析。\nPharmAgents强调可解释性、代理互动和自我演化能力，能基于以往经验改进设计，从而为自治、可扩展的制药研究建立新范式，并扩展到药物生命周期管理。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22164v2",
      "published_date": "2025-03-28 06:02:53 UTC",
      "updated_date": "2025-03-31 16:26:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:14:28.523540"
    },
    {
      "arxiv_id": "2503.22152v1",
      "title": "EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Li",
        "Vijay Veerabadran",
        "Michael L. Iuzzolino",
        "Brett D. Roads",
        "Asli Celikyilmaz",
        "Karl Ridgeway"
      ],
      "abstract": "We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states.",
      "tldr_zh": "该研究引入了EgoToM基准，用于评估Theory of Mind (ToM)推理能力在第一人称视角视频中的表现，通过因果ToM模型从Ego4D数据集生成多选视频问答实例，以预测摄像机佩戴者的目标、信念和下一步行动。实验比较了人类和最先进的多模态大语言模型(MLLMs)的性能，结果显示MLLMs在推断目标时接近人类水平，但显著落后于人类在推断即时信念状态和与未来视频一致的行动上。作者认为，这将指导未来第一人称数字助手的设计，使其更准确地模拟用户的心理状态。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22152v1",
      "published_date": "2025-03-28 05:10:59 UTC",
      "updated_date": "2025-03-28 05:10:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:14:40.307237"
    },
    {
      "arxiv_id": "2503.22151v1",
      "title": "When Autonomy Breaks: The Hidden Existential Risk of AI",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Krook"
      ],
      "abstract": "AI risks are typically framed around physical threats to humanity, a loss of\ncontrol or an accidental error causing humanity's extinction. However, I argue\nin line with the gradual disempowerment thesis, that there is an\nunderappreciated risk in the slow and irrevocable decline of human autonomy. As\nAI starts to outcompete humans in various areas of life, a tipping point will\nbe reached where it no longer makes sense to rely on human decision-making,\ncreativity, social care or even leadership.\n  What may follow is a process of gradual de-skilling, where we lose skills\nthat we currently take for granted. Traditionally, it is argued that AI will\ngain human skills over time, and that these skills are innate and immutable in\nhumans. By contrast, I argue that humans may lose such skills as critical\nthinking, decision-making and even social care in an AGI world. The biggest\nthreat to humanity is therefore not that machines will become more like humans,\nbut that humans will become more like machines.",
      "tldr_zh": "该论文基于 gradual disempowerment thesis 论点，强调 AI 的潜在存在风险不限于物理威胁或失控，而是人类自治的缓慢、不可逆转衰退。随着 AI 在决策、创造力、社会关怀和领导力等领域超越人类，将达到一个临界点，不再依赖人类参与。作者争论，这可能导致人类技能的渐进式退化（de-skilling），如丧失批判性思维和社交能力，最终的威胁在于人类变得更像机器，而不是机器变得更像人类。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22151v1",
      "published_date": "2025-03-28 05:10:32 UTC",
      "updated_date": "2025-03-28 05:10:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:14:52.645167"
    },
    {
      "arxiv_id": "2503.22771v1",
      "title": "GroundHog: Revolutionizing GLDAS Groundwater Storage Downscaling for Enhanced Recharge Estimation in Bangladesh",
      "title_zh": "翻译失败",
      "authors": [
        "Saleh Sakib Ahmed",
        "Rashed Uz Zzaman",
        "Saifur Rahman Jony",
        "Faizur Rahman Himel",
        "Afroza Sharmin",
        "A. H. M. Khalequr Rahman",
        "M. Sohel Rahman",
        "Sara Nowreen"
      ],
      "abstract": "Long-term groundwater level (GWL) measurement is vital for effective\npolicymaking and recharge estimation using annual maxima and minima. However,\ncurrent methods prioritize short-term predictions and lack multi-year\napplicability, limiting their utility. Moreover, sparse in-situ measurements\nlead to reliance on low-resolution satellite data like GLDAS as the ground\ntruth for Machine Learning models, further constraining accuracy. To overcome\nthese challenges, we first develop an ML model to mitigate data gaps, achieving\n$R^2$ scores of 0.855 and 0.963 for maximum and minimum GWL predictions,\nrespectively. Subsequently, using these predictions and well observations as\nground truth, we train an Upsampling Model that uses low-resolution (25 km)\nGLDAS data as input to produce high-resolution (2 km) GWLs, achieving an\nexcellent $R^2$ score of 0.96. Our approach successfully upscales GLDAS data\nfor 2003-2024, allowing high-resolution recharge estimations and revealing\ncritical trends for proactive resource management. Our method allows upsampling\nof groundwater storage (GWS) from GLDAS to high-resolution GWLs for any points\nindependently of officially curated piezometer data, making it a valuable tool\nfor decision-making.",
      "tldr_zh": "该研究针对孟加拉国地下水资源管理的问题，开发了GroundHog框架，以解决当前地下水位(GWL)预测方法依赖低分辨率GLDAS数据并缺乏长期适用性的挑战。研究首先构建了一个ML模型来填补数据空白，实现GWL最大值和最小值的$R^2$分数分别为0.855和0.963；随后，使用这些预测和井观测作为基准，训练上采样模型将25 km分辨率的GLDAS数据提升到2 km分辨率，获得$R^2$分数0.96。整体方法成功处理2003-2024年的数据，提供高分辨率地下水存储(GWS)补给估计，并揭示关键趋势，支持独立的决策制定。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22771v1",
      "published_date": "2025-03-28 04:56:01 UTC",
      "updated_date": "2025-03-28 04:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:15:05.755444"
    },
    {
      "arxiv_id": "2503.22144v1",
      "title": "FRASE: Structured Representations for Generalizable SPARQL Query Generation",
      "title_zh": "FRASE：用于可泛化 SPARQL 查询生成的结构化表示",
      "authors": [
        "Papa Abdou Karim Karou Diallo",
        "Amal Zouaq"
      ],
      "abstract": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).",
      "tldr_zh": "这篇论文引入了 FRASE 方法，利用 Frame Semantic Role Labeling (FSRL) 来生成可泛化的 SPARQL 查询，解决现有数据集依赖模板导致模型泛化能力不足的问题。研究者创建了 LC-QuAD 3.0 数据集，通过 frame detection 和 frame-elements 映射来丰富问题语义。实验结果显示，在不同 fine-tuning 配置下，FRASE 显著提升了大型语言模型 (LLMs) 的 SPARQL 生成性能，尤其在未知模板分割和自然语言问题场景中。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22144v1",
      "published_date": "2025-03-28 04:39:52 UTC",
      "updated_date": "2025-03-28 04:39:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:15:15.670524"
    },
    {
      "arxiv_id": "2503.22143v1",
      "title": "A Self-Supervised Learning of a Foundation Model for Analog Layout Design Automation",
      "title_zh": "一种基础模型的自监督学习，用于模拟布局设计自动化",
      "authors": [
        "Sungyu Jeong",
        "Won Joon Choi",
        "Junung Choi",
        "Anik Biswas",
        "Byungsub Kim"
      ],
      "abstract": "We propose a UNet-based foundation model and its self-supervised learning\nmethod to address two key challenges: 1) lack of qualified annotated analog\nlayout data, and 2) excessive variety in analog layout design tasks. For\nself-supervised learning, we propose random patch sampling and random masking\ntechniques automatically to obtain enough training data from a small\nunannotated layout dataset. The obtained data are greatly augmented, less\nbiased, equally sized, and contain enough information for excessive varieties\nof qualified layout patterns. By pre-training with the obtained data, the\nproposed foundation model can learn implicit general knowledge on layout\npatterns so that it can be fine-tuned for various downstream layout tasks with\nsmall task-specific datasets. Fine-tuning provides an efficient and\nconsolidated methodology for diverse downstream tasks, reducing the enormous\nhuman effort to develop a model per task separately. In experiments, the\nfoundation model was pre-trained using 324,000 samples obtained from 6\nsilicon-proved manually designed analog circuits, then it was fine-tuned for\nthe five example downstream tasks: generating contacts, vias, dummy fingers,\nN-wells, and metal routings. The fine-tuned models successfully performed these\ntasks for more than one thousand unseen layout inputs, generating DRC/LVS-clean\nlayouts for 96.6% of samples. Compared with training the model from scratch for\nthe metal routing task, fine-tuning required only 1/8 of the data to achieve\nthe same dice score of 0.95. With the same data, fine-tuning achieved a 90%\nlower validation loss and a 40% higher benchmark score than training from\nscratch.",
      "tldr_zh": "我们提出了一种基于 UNet 的 foundation model 和自监督学习方法，用于模拟布局设计自动化，解决数据标注不足和任务多样性挑战。通过随机补丁采样和随机掩码技术，从小规模未标注数据集自动生成大量增强数据进行预训练，使模型学习布局模式的隐式一般知识，并可微调用于下游任务。实验结果显示，预训练模型微调后成功处理超过一千个未见布局输入，96.6% 生成的布局符合 DRC/LVS 标准；与从零训练相比，微调只需 1/8 数据达到相同的 Dice score 0.95，并显著降低验证损失和提高基准分数。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "8 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.22143v1",
      "published_date": "2025-03-28 04:37:33 UTC",
      "updated_date": "2025-03-28 04:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:15:29.558409"
    },
    {
      "arxiv_id": "2503.22141v1",
      "title": "Integrating Artificial Intelligence with Human Expertise: An In-depth Analysis of ChatGPT's Capabilities in Generating Metamorphic Relations",
      "title_zh": "整合人工智能与人类专长：ChatGPT 在生成变形关系方面的能力深入分析",
      "authors": [
        "Yifan Zhang",
        "Dave Towey",
        "Matthew Pike",
        "Quang-Hung Luu",
        "Huai Liu",
        "Tsong Yueh Chen"
      ],
      "abstract": "Context: This paper provides an in-depth examination of the generation and\nevaluation of Metamorphic Relations (MRs) using GPT models developed by OpenAI,\nwith a particular focus on the capabilities of GPT-4 in software testing\nenvironments.\n  Objective: The aim is to examine the quality of MRs produced by GPT-3.5 and\nGPT-4 for a specific System Under Test (SUT) adopted from an earlier study, and\nto introduce and apply an improved set of evaluation criteria for a diverse\nrange of SUTs.\n  Method: The initial phase evaluates MRs generated by GPT-3.5 and GPT-4 using\ncriteria from a prior study, followed by an application of an enhanced\nevaluation framework on MRs created by GPT-4 for a diverse range of nine SUTs,\nvarying from simple programs to complex systems incorporating AI/ML components.\nA custom-built GPT evaluator, alongside human evaluators, assessed the MRs,\nenabling a direct comparison between automated and human evaluation methods.\n  Results: The study finds that GPT-4 outperforms GPT-3.5 in generating\naccurate and useful MRs. With the advanced evaluation criteria, GPT-4\ndemonstrates a significant ability to produce high-quality MRs across a wide\nrange of SUTs, including complex systems incorporating AI/ML components.\n  Conclusions: GPT-4 exhibits advanced capabilities in generating MRs suitable\nfor various applications. The research underscores the growing potential of AI\nin software testing, particularly in the generation and evaluation of MRs, and\npoints towards the complementarity of human and AI skills in this domain.",
      "tldr_zh": "这篇论文深入分析了人工智能与人类专长的整合，重点评估 GPT-3.5 和 GPT-4 在生成 Metamorphic Relations (MRs) 的能力，应用于软件测试环境中的各种 System Under Test (SUTs)。研究方法包括先使用先前标准评估 GPT 模型生成的 MRs，然后引入改进的评估框架，由自定义 GPT 评估器和人类评估器对九个不同 SUTs 的 MRs 进行比较。结果显示，GPT-4 在生成准确且有用的 MRs 方面显著优于 GPT-3.5，尤其在包含 AI/ML 组件的复杂系统中。结论强调，GPT-4 展示了 AI 在软件测试中的强大潜力，并突出了人类和 AI 技能的互补作用。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Submitted to Information and Software Technology",
      "pdf_url": "http://arxiv.org/pdf/2503.22141v1",
      "published_date": "2025-03-28 04:31:32 UTC",
      "updated_date": "2025-03-28 04:31:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:15:41.979223"
    },
    {
      "arxiv_id": "2503.22137v1",
      "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
      "title_zh": "翻译失败",
      "authors": [
        "Syrine Belakaria",
        "Joshua Kazdan",
        "Charles Marx",
        "Chris Cundy",
        "Willie Neiswanger",
        "Sanmi Koyejo",
        "Barbara E. Engelhardt",
        "Stefano Ermon"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets.",
      "tldr_zh": "本研究针对强化学习从人类反馈（RLHF）中偏好优化的挑战，提出了一种基于Sharpe Ratio的主动学习方法，用于高效选择提示和偏好对，从而降低专家标注的成本。该方法通过评估所有潜在偏好标注的梯度，计算每个元组的Sharpe Ratio来评估风险影响，并利用直接偏好优化（DPO）损失的封闭形式表达式，确保计算高效。实验结果显示，该方法在多个语言模型和真实数据集上，使用有限偏好数据时，比基线提高了高达5%的胜率。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22137v1",
      "published_date": "2025-03-28 04:22:53 UTC",
      "updated_date": "2025-03-28 04:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:15:52.273386"
    },
    {
      "arxiv_id": "2503.22769v1",
      "title": "MediTools -- Medical Education Powered by LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Amr Alshatnawi",
        "Remi Sampaleanu",
        "David Liebovitz"
      ],
      "abstract": "Artificial Intelligence (AI) has been advancing rapidly and with the advent\nof large language models (LLMs) in late 2022, numerous opportunities have\nemerged for adopting this technology across various domains, including\nmedicine. These innovations hold immense potential to revolutionize and\nmodernize medical education. Our research project leverages large language\nmodels to enhance medical education and address workflow challenges through the\ndevelopment of MediTools - AI Medical Education. This prototype application\nfocuses on developing interactive tools that simulate real-life clinical\nscenarios, provide access to medical literature, and keep users updated with\nthe latest medical news. Our first tool is a dermatology case simulation tool\nthat uses real patient images depicting various dermatological conditions and\nenables interaction with LLMs acting as virtual patients. This platform allows\nusers to practice their diagnostic skills and enhance their clinical\ndecision-making abilities. The application also features two additional tools:\nan AI-enhanced PubMed tool for engaging with LLMs to gain deeper insights into\nresearch papers, and a Google News tool that offers LLM generated summaries of\narticles for various medical specialties. A comprehensive survey has been\nconducted among medical professionals and students to gather initial feedback\non the effectiveness and user satisfaction of MediTools, providing insights for\nfurther development and refinement of the application. This research\ndemonstrates the potential of AI-driven tools in transforming and\nrevolutionizing medical education, offering a scalable and interactive platform\nfor continuous learning and skill development.",
      "tldr_zh": "该研究利用大型语言模型（LLMs）开发了MediTools，一款AI驱动的医疗教育原型应用，旨在通过模拟真实临床场景、提供医疗文献访问和最新新闻摘要来解决医疗教育中的工作流程挑战。MediTools包括三个关键工具：皮肤病案例模拟工具，使用真实患者图像和LLMs作为虚拟患者，帮助用户练习诊断和决策技能；AI增强的PubMed工具，允许用户与LLMs互动深入研究论文；以及Google News工具，提供LLMs生成的医疗新闻摘要。研究通过对医疗专业人士和学生的调查收集反馈，证明了MediTools的有效性和用户满意度，并展示了AI在医疗教育中创建可扩展互动平台的潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, 17 figures, 2 tables. Code available at\n  https://github.com/NM-Streamlit-Team/meditools",
      "pdf_url": "http://arxiv.org/pdf/2503.22769v1",
      "published_date": "2025-03-28 03:57:32 UTC",
      "updated_date": "2025-03-28 03:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:16:04.928337"
    },
    {
      "arxiv_id": "2503.22122v1",
      "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation",
      "title_zh": "REMAC：自我反思与自我演化的多智能体协作，用于长时域机器人操作",
      "authors": [
        "Puzhen Yuan",
        "Angyuan Ma",
        "Yunchao Yao",
        "Huaxiu Yao",
        "Masayoshi Tomizuka",
        "Mingyu Ding"
      ],
      "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.",
      "tldr_zh": "该研究提出 REMAC 框架，一种自反式(self-reflective)和自演化(self-evolving)多智能体协作系统，用于处理长时机器人操作(long-horizon robot manipulation)中的动态场景挑战。REMAC 包括自反射模块（进行预条件和后条件检查以评估和优化计划）和自演化模块（基于场景特定推理动态调整计划），使机器人无需复杂提示即可探索环境并高效协作。实验在基于 RoboCasa 的多智能体环境中进行，涵盖 4 类任务、27 种任务风格和 50+ 对象，结果显示 REMAC 比单机器人基线提高了 40% 的成功率和 52.7% 的执行效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22122v1",
      "published_date": "2025-03-28 03:51:40 UTC",
      "updated_date": "2025-03-28 03:51:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:16:16.402267"
    },
    {
      "arxiv_id": "2503.22115v1",
      "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories",
      "title_zh": "翻译失败",
      "authors": [
        "Yazhou Zhang",
        "Qimeng Liu",
        "Qiuchi Li",
        "Peng Zhang",
        "Jing Qin"
      ],
      "abstract": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.",
      "tldr_zh": "这篇论文指出，传统评估大型语言模型(LLMs)的\"value alignment\" 方法依赖单句对抗提示，但这些测试易被模型的安全机制规避，无法揭示潜在偏差。为解决此问题，研究者提出升级基准，使用多轮对话(multi-turn dialogues)和叙事场景来创建更隐蔽和上下文丰富的评估数据集，包括对话陷阱和伦理模糊故事。实验结果显示，这种方法能有效暴露传统单句评估未发现的隐藏偏差，强调了采用动态、情境化的测试来提升AI伦理和安全评估的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22115v1",
      "published_date": "2025-03-28 03:31:37 UTC",
      "updated_date": "2025-03-28 03:31:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:16:28.718988"
    },
    {
      "arxiv_id": "2503.22093v2",
      "title": "How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark",
      "title_zh": "视觉语言模型在理解人类意图方面表现如何？一个开放式心智理论问题评估基准",
      "authors": [
        "Ximing Wen",
        "Mallika Mainali",
        "Anik Sen"
      ],
      "abstract": "Vision Language Models (VLMs) have demonstrated strong reasoning capabilities\nin Visual Question Answering (VQA) tasks; however, their ability to perform\nTheory of Mind (ToM) tasks, such as inferring human intentions, beliefs, and\nmental states, remains underexplored. We propose an open-ended question\nframework to evaluate VLMs' performance across diverse categories of ToM tasks.\nWe curated and annotated a benchmark dataset of 30 images and evaluated the\nperformance of four VLMs of varying sizes. Our results show that the GPT-4\nmodel outperformed all the others, with only one smaller model, GPT-4o-mini,\nachieving comparable performance. We observed that VLMs often struggle to infer\nintentions in complex scenarios such as bullying or cheating. Our findings\nreveal that smaller models can sometimes infer correct intentions despite\nrelying on incorrect visual cues. The dataset is available at\nhttps://github.com/ximingwen/ToM-AAAI25-Multimodal.",
      "tldr_zh": "本研究评估了 Vision Language Models (VLMs) 在推断人类意图、信念和心理状态的 Theory of Mind (ToM) 任务中的表现，填补了其在 Visual Question Answering (VQA) 之外的空白。研究者提出一个开放式问题框架，并构建了一个包含30张图像的基准数据集，对四种不同规模的VLMs进行了评估。结果显示，GPT-4 表现最佳，仅有较小的 GPT-4o-mini 能与之匹敌，而VLMs 在复杂场景如欺凌或欺骗中往往难以准确推断意图。令人注目的是，较小模型有时能基于错误视觉线索推断正确意图，为未来VLMs 的ToM 能力提升提供了宝贵洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, accepted by ToM@AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2503.22093v2",
      "published_date": "2025-03-28 02:26:32 UTC",
      "updated_date": "2025-04-24 03:20:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:16:40.753285"
    },
    {
      "arxiv_id": "2504.01979v1",
      "title": "Correlation-Attention Masked Temporal Transformer for User Identity Linkage Using Heterogeneous Mobility Data",
      "title_zh": "翻译失败",
      "authors": [
        "Ziang Yan",
        "Xingyu Zhao",
        "Hanqing Ma",
        "Wei Chen",
        "Jianpeng Qi",
        "Yanwei Yu",
        "Junyu Dong"
      ],
      "abstract": "With the rise of social media and Location-Based Social Networks (LBSN),\ncheck-in data across platforms has become crucial for User Identity Linkage\n(UIL). These data not only reveal users' spatio-temporal information but also\nprovide insights into their behavior patterns and interests. However,\ncross-platform identity linkage faces challenges like poor data quality, high\nsparsity, and noise interference, which hinder existing methods from extracting\ncross-platform user information. To address these issues, we propose a\nCorrelation-Attention Masked Transformer for User Identity Linkage Network\n(MT-Link), a transformer-based framework to enhance model performance by\nlearning spatio-temporal co-occurrence patterns of cross-platform users. Our\nmodel effectively captures spatio-temporal co-occurrence in cross-platform user\ncheck-in sequences. It employs a correlation attention mechanism to detect the\nspatio-temporal co-occurrence between user check-in sequences. Guided by\nattention weight maps, the model focuses on co-occurrence points while\nfiltering out noise, ultimately improving classification performance.\nExperimental results show that our model significantly outperforms\nstate-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in\nterms of Macro-F1 and Area Under Curve (AUC).",
      "tldr_zh": "该研究针对用户身份链接(User Identity Linkage, UIL)面临的挑战，如数据质量差、高稀疏性和噪声干扰，提出了一种Correlation-Attention Masked Temporal Transformer框架，名为MT-Link。模型通过学习跨平台用户签到序列的时空共现模式，并采用correlation attention机制来检测和关注这些共现点，同时过滤噪声，以提升分类性能。实验结果显示，MT-Link在异构移动数据上比现有最先进基线方法在Macro-F1和Area Under Curve (AUC)指标上分别提高了12.92%~17.76%和5.80%~8.38%。这项工作为基于LBSN的跨平台用户分析提供了更可靠的工具。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "9 pages, 5 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.01979v1",
      "published_date": "2025-03-28 02:18:16 UTC",
      "updated_date": "2025-03-28 02:18:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:16:52.742062"
    },
    {
      "arxiv_id": "2504.00020v1",
      "title": "Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Huan Zhao",
        "Yiming Liu",
        "Jina Yao",
        "Ling Xiong",
        "Zexin Zhou",
        "Zixing Zhang"
      ],
      "abstract": "Recent breakthroughs in single-cell technology have ushered in unparalleled\nopportunities to decode the molecular intricacy of intricate biological\nsystems, especially those linked to diseases unique to humans. However, these\nprogressions have also ushered in novel obstacles-specifically, the efficient\nannotation of extensive, long-tailed single-cell data pertaining to disease\nconditions. To effectively surmount this challenge, we introduce Celler, a\nstate-of-the-art generative pre-training model crafted specifically for the\nannotation of single-cell data. Celler incorporates two groundbreaking\nelements: First, we introduced the Gaussian Inflation (GInf) Loss function. By\ndynamically adjusting sample weights, GInf Loss significantly enhances the\nmodel's ability to learn from rare categories while reducing the risk of\noverfitting for common categories. Secondly, we introduce an innovative Hard\nData Mining (HDM) strategy into the training process, specifically targeting\nthe challenging-to-learn minority data samples, which significantly improved\nthe model's predictive accuracy. Additionally, to further advance research in\nthis field, we have constructed a large-scale single-cell dataset: Celler-75,\nwhich encompasses 40 million cells distributed across 80 human tissues and 75\nspecific diseases. This dataset provides critical support for comprehensively\nexploring the potential of single-cell technology in disease research. Our code\nis available at https://github.com/AI4science-ym/HiCeller.",
      "tldr_zh": "本研究提出 Celler，一种先进的生成预训练模型，针对长尾单细胞数据注释的挑战，尤其是在人类疾病相关生物系统中的应用。Celler 引入了 Gaussian Inflation (GInf) Loss 函数来动态调整样本权重，提升对稀有类别的学习能力并减少常见类别的过拟合风险，同时采用了 Hard Data Mining (HDM) 策略来针对难学样本，提高模型的预测准确性。为支持进一步研究，该论文构建了大规模数据集 Celler-75，包含 4000 万细胞、80 个人类组织和 75 种特定疾病，并开源了相关代码。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00020v1",
      "published_date": "2025-03-28 02:04:26 UTC",
      "updated_date": "2025-03-28 02:04:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:17:06.061378"
    },
    {
      "arxiv_id": "2503.22074v1",
      "title": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Chuan-Wei Kuo",
        "Siyu Chen",
        "Chenqi Yan",
        "Yu Yang Fredrik Liu"
      ],
      "abstract": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.",
      "tldr_zh": "该论文提出一个两阶段框架，用于高效适应大型语言模型 (LLMs) 到特定科学领域，如材料科学，解决数据有限和高知识密度的问题。第一阶段采用 Penrose-like 非周期性 tiling 的低秩压缩，将模型权重矩阵分解成局部 \"rank blocks\"，并通过谱变换（如离散余弦或傅立叶变换）和 Kullback-Leibler (KL) divergence-based alignment loss 保持压缩模型与原模型的分布相似性。第二阶段通过 section-wise Q&A fine-tuning 策略，按节处理技术文档，进行结构化问答以提取推理痕迹，并逐步注入领域知识，同时最小化灾难性遗忘。该框架在数据稀缺条件下实现 LLMs 的精确专业化，为材料科学等高价值领域的知识整合奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22074v1",
      "published_date": "2025-03-28 01:33:05 UTC",
      "updated_date": "2025-03-28 01:33:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:17:17.855464"
    },
    {
      "arxiv_id": "2503.22069v1",
      "title": "Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning",
      "title_zh": "对比低分辨率和高分辨率特征用于深度学习中的 HER2 评分",
      "authors": [
        "Ekansh Chauhan",
        "Anila Sharma",
        "Amit Sharma",
        "Vikas Nishadham",
        "Asha Ghughtyal",
        "Ankur Kumar",
        "Gurudutt Gupta",
        "Anurag Mehta",
        "C. V. Jawahar",
        "P. K. Vinod"
      ],
      "abstract": "Breast cancer, the most common malignancy among women, requires precise\ndetection and classification for effective treatment. Immunohistochemistry\n(IHC) biomarkers like HER2, ER, and PR are critical for identifying breast\ncancer subtypes. However, traditional IHC classification relies on\npathologists' expertise, making it labor-intensive and subject to significant\ninter-observer variability. To address these challenges, this study introduces\nthe India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHC\nslides (HER2, ER, and PR) aimed at automating receptor status classification.\nThe primary focus is on developing predictive models for HER2 3-way\nclassification (0, Low, High) to enhance prognosis. Evaluation of multiple deep\nlearning models revealed that an end-to-end ConvNeXt network utilizing\nlow-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%,\nand 83.56%, respectively, for 3-way classification, outperforming patch-based\nmethods by over 5.35% in F1 score. This study highlights the potential of\nsimple yet effective deep learning techniques to significantly improve accuracy\nand reproducibility in breast cancer classification, supporting their\nintegration into clinical workflows for better patient outcomes.",
      "tldr_zh": "这篇论文介绍了 India Pathology Breast Cancer Dataset (IPD-Breast)，一个包含 1,272 张 IHC 幻灯片的数据集，用于自动化乳腺癌受体状态分类，特别是针对 HER2 的 3-way 分类（0, Low, High），以减少传统病理学评估的变异性和劳动密集性。研究开发了基于 ConvNeXt 网络的端到端模型，利用低分辨率图像进行特征对比，显著提高了分类性能。结果显示，该模型在 HER2 分类中取得了 AUC 91.79%、F1 得分 83.52% 和准确率 83.56%，比基于 patch 的方法提升了 5.35% 的 F1 得分。该方法证明了简单有效的深度学习技术在提升乳腺癌诊断准确性和可重复性方面的潜力，支持其在临床工作流程中的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22069v1",
      "published_date": "2025-03-28 01:24:08 UTC",
      "updated_date": "2025-03-28 01:24:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:17:29.338993"
    },
    {
      "arxiv_id": "2503.22068v1",
      "title": "A Proposal for Networks Capable of Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zeki Doruk Erden",
        "Boi Faltings"
      ],
      "abstract": "We analyze the ability of computational units to retain past responses after\nparameter updates, a key property for system-wide continual learning. Neural\nnetworks trained with gradient descent lack this capability, prompting us to\npropose Modelleyen, an alternative approach with inherent response\npreservation. We demonstrate through experiments on modeling the dynamics of a\nsimple environment and on MNIST that, despite increased computational\ncomplexity and some representational limitations at its current stage,\nModelleyen achieves continual learning without relying on sample replay or\npredefined task boundaries.",
      "tldr_zh": "本论文分析了神经网络在使用gradient descent训练后，无法在参数更新后保留过去响应的缺陷，这阻碍了系统级的continual learning。作者提出Modelleyen作为一种替代方法，该方法具备固有的响应保留特性，能够实现持续学习，而不依赖于sample replay或预定义任务边界。通过在简单环境动态建模和MNIST数据集上的实验，Modelleyen尽管增加了计算复杂性和存在一些表示限制，但证明了其在持续学习中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at ICLR 2025 World Models Workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.22068v1",
      "published_date": "2025-03-28 01:23:18 UTC",
      "updated_date": "2025-03-28 01:23:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:17:40.455810"
    },
    {
      "arxiv_id": "2504.01025v1",
      "title": "Diagnosis of Pulmonary Hypertension by Integrating Multimodal Data with a Hybrid Graph Convolutional and Transformer Network",
      "title_zh": "翻译失败",
      "authors": [
        "Fubao Zhu",
        "Yang Zhang",
        "Gengmin Liang",
        "Jiaofen Nan",
        "Yanting Li",
        "Chuang Han",
        "Danyang Sun",
        "Zhiguo Wang",
        "Chen Zhao",
        "Wenxuan Zhou",
        "Jian He",
        "Yi Xu",
        "Iokfai Cheang",
        "Xu Zhu",
        "Yanli Zhou",
        "Weihua Zhou"
      ],
      "abstract": "Early and accurate diagnosis of pulmonary hypertension (PH) is essential for\noptimal patient management. Differentiating between pre-capillary and\npost-capillary PH is critical for guiding treatment decisions. This study\ndevelops and validates a deep learning-based diagnostic model for PH, designed\nto classify patients as non-PH, pre-capillary PH, or post-capillary PH. This\nretrospective study analyzed data from 204 patients (112 with pre-capillary PH,\n32 with post-capillary PH, and 60 non-PH controls) at the First Affiliated\nHospital of Nanjing Medical University. Diagnoses were confirmed through right\nheart catheterization. We selected 6 samples from each category for the test\nset (18 samples, 10%), with the remaining 186 samples used for the training\nset. This process was repeated 35 times for testing. This paper proposes a deep\nlearning model that combines Graph convolutional networks (GCN), Convolutional\nneural networks (CNN), and Transformers. The model was developed to process\nmultimodal data, including short-axis (SAX) sequences, four-chamber (4CH)\nsequences, and clinical parameters. Our model achieved a performance of Area\nunder the receiver operating characteristic curve (AUC) = 0.81 +- 0.06(standard\ndeviation) and Accuracy (ACC) = 0.73 +- 0.06 on the test set. The\ndiscriminative abilities were as follows: non-PH subjects (AUC = 0.74 +- 0.11),\npre-capillary PH (AUC = 0.86 +- 0.06), and post-capillary PH (AUC = 0.83 +-\n0.10). It has the potential to support clinical decision-making by effectively\nintegrating multimodal data to assist physicians in making accurate and timely\ndiagnoses.",
      "tldr_zh": "本研究开发了一种混合深度学习模型，结合Graph Convolutional Networks (GCN)、Convolutional Neural Networks (CNN)和Transformers，用于整合多模态数据（如短轴SAX序列、四腔室4CH序列和临床参数）诊断肺动脉高压（PH），以区分非PH、预毛细血管PH和后毛细血管PH。基于204名患者的回顾性数据，该模型在测试集上实现了AUC=0.81±0.06和准确率ACC=0.73±0.06，并在不同PH类型分类中表现出色（非PH AUC=0.74±0.11、预毛细血管PH AUC=0.86±0.06、后毛细血管PH AUC=0.83±0.10）。这项工作有助于临床决策，提供更准确及时的诊断支持。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "23 pages, 8 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.01025v1",
      "published_date": "2025-03-28 01:14:17 UTC",
      "updated_date": "2025-03-28 01:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:17:53.822394"
    },
    {
      "arxiv_id": "2503.22064v1",
      "title": "Multi-Task Semantic Communications via Large Models",
      "title_zh": "多任务语义通信通过大型模型",
      "authors": [
        "Wanli Ni",
        "Zhijin Qin",
        "Haofeng Sun",
        "Xiaoming Tao",
        "Zhu Han"
      ],
      "abstract": "Artificial intelligence (AI) promises to revolutionize the design,\noptimization and management of next-generation communication systems. In this\narticle, we explore the integration of large AI models (LAMs) into semantic\ncommunications (SemCom) by leveraging their multi-modal data processing and\ngeneration capabilities. Although LAMs bring unprecedented abilities to extract\nsemantics from raw data, this integration entails multifaceted challenges\nincluding high resource demands, model complexity, and the need for\nadaptability across diverse modalities and tasks. To overcome these challenges,\nwe propose a LAM-based multi-task SemCom (MTSC) architecture, which includes an\nadaptive model compression strategy and a federated split fine-tuning approach\nto facilitate the efficient deployment of LAM-based semantic models in\nresource-limited networks. Furthermore, a retrieval-augmented generation scheme\nis implemented to synthesize the most recent local and global knowledge bases\nto enhance the accuracy of semantic extraction and content generation, thereby\nimproving the inference performance. Finally, simulation results demonstrate\nthe efficacy of the proposed LAM-based MTSC architecture, highlighting the\nperformance enhancements across various downstream tasks under varying channel\nconditions.",
      "tldr_zh": "该研究探讨了将大型 AI 模型 (Large Models) 集成到语义通信 (Semantic Communications) 中，利用其多模态数据处理能力，以优化下一代通信系统。论文提出了一种基于 Large Models 的多任务语义通信 (Multi-Task SemCom, MTSC) 架构，包括自适应模型压缩策略和联邦分割微调方法，以应对高资源需求和模型复杂性的挑战，并通过检索增强生成方案整合本地与全局知识库，提升语义提取和内容生成的准确性。模拟结果表明，该架构在各种下游任务和通道条件下显著提高了性能表现。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.22064v1",
      "published_date": "2025-03-28 00:57:34 UTC",
      "updated_date": "2025-03-28 00:57:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:18:03.800982"
    },
    {
      "arxiv_id": "2503.22051v1",
      "title": "Non-Monotonic Attention-based Read/Write Policy Learning for Simultaneous Translation",
      "title_zh": "非单调注意力机制的读/写策略学习用于同时翻译",
      "authors": [
        "Zeeshan Ahmed",
        "Frank Seide",
        "Zhe Liu",
        "Rastislav Rabatin",
        "Jachym Kolar",
        "Niko Moritz",
        "Ruiming Xie",
        "Simone Merello",
        "Christian Fuegen"
      ],
      "abstract": "Simultaneous or streaming machine translation generates translation while\nreading the input stream. These systems face a quality/latency trade-off,\naiming to achieve high translation quality similar to non-streaming models with\nminimal latency. We propose an approach that efficiently manages this\ntrade-off. By enhancing a pretrained non-streaming model, which was trained\nwith a seq2seq mechanism and represents the upper bound in quality, we convert\nit into a streaming model by utilizing the alignment between source and target\ntokens. This alignment is used to learn a read/write decision boundary for\nreliable translation generation with minimal input. During training, the model\nlearns the decision boundary through a read/write policy module, employing\nsupervised learning on the alignment points (pseudo labels). The read/write\npolicy module, a small binary classification unit, can control the\nquality/latency trade-off during inference. Experimental results show that our\nmodel outperforms several strong baselines and narrows the gap with the\nnon-streaming baseline model.",
      "tldr_zh": "该论文针对同时翻译（Simultaneous Translation）系统提出了一种基于非单调注意力（Non-Monotonic Attention）的读/写策略学习方法，以平衡翻译质量和延迟问题。通过利用预训练的非流式 seq2seq 模型及其源目标标记对齐（alignment），该方法训练一个读/写政策模块（read/write policy module），这是一个小型二元分类单元，用于学习决策边界，从而在最小输入下生成可靠的翻译。实验结果显示，该模型优于多个强基线，并显著缩小了与非流式基线模型的质量差距。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22051v1",
      "published_date": "2025-03-28 00:00:33 UTC",
      "updated_date": "2025-03-28 00:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T07:18:16.080566"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 97,
  "processed_papers_count": 97,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T07:18:41.224895"
}
[
  {
    "arxiv_id": "2504.00030v2",
    "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding",
    "authors": [
      "Aayush Gautam",
      "Susav Shrestha",
      "Narasimha Reddy"
    ],
    "abstract": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2504.00030v2",
    "published_date": "2025-03-28 23:41:55 UTC",
    "updated_date": "2025-04-03 12:31:40 UTC"
  },
  {
    "arxiv_id": "2503.22909v1",
    "title": "Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation",
    "authors": [
      "Anas Berka",
      "Mohamed El Hajji",
      "Raphael Canals",
      "Youssef Es-saady",
      "Adel Hafiane"
    ],
    "abstract": "Aerial and satellite imagery are inherently complementary remote sensing\nsources, offering high-resolution detail alongside expansive spatial coverage.\nHowever, the use of these sources for land cover segmentation introduces\nseveral challenges, prompting the development of a variety of segmentation\nmethods. Among these approaches, the DeepLabV3+ architecture is considered as a\npromising approach in the field of single-source image segmentation. However,\ndespite its reliable results for segmentation, there is still a need to\nincrease its robustness and improve its performance. This is particularly\ncrucial for multimodal image segmentation, where the fusion of diverse types of\ninformation is essential.\n  An interesting approach involves enhancing this architectural framework\nthrough the integration of novel components and the modification of certain\ninternal processes.\n  In this paper, we enhance the DeepLabV3+ architecture by introducing a new\ntransposed conventional layers block for upsampling a second entry to fuse it\nwith high level features. This block is designed to amplify and integrate\ninformation from satellite images, thereby enriching the segmentation process\nthrough fusion with aerial images.\n  For experiments, we used the LandCover.ai (Land Cover from Aerial Imagery)\ndataset for aerial images, alongside the corresponding dataset sourced from\nSentinel 2 data.\n  Through the fusion of both sources, the mean Intersection over Union (mIoU)\nachieved a total mIoU of 84.91% without data augmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22909v1",
    "published_date": "2025-03-28 23:07:39 UTC",
    "updated_date": "2025-03-28 23:07:39 UTC"
  },
  {
    "arxiv_id": "2504.00029v1",
    "title": "Generating Structured Plan Representation of Procedures with LLMs",
    "authors": [
      "Deepeka Garg",
      "Sihan Zeng",
      "Sumitra Ganesh",
      "Leo Ardon"
    ],
    "abstract": "In this paper, we address the challenges of managing Standard Operating\nProcedures (SOPs), which often suffer from inconsistencies in language, format,\nand execution, leading to operational inefficiencies. Traditional process\nmodeling demands significant manual effort, domain expertise, and familiarity\nwith complex languages like Business Process Modeling Notation (BPMN), creating\nbarriers for non-techincal users. We introduce SOP Structuring (SOPStruct), a\nnovel approach that leverages Large Language Models (LLMs) to transform SOPs\ninto decision-tree-based structured representations. SOPStruct produces a\nstandardized representation of SOPs across different domains, reduces cognitive\nload, and improves user comprehension by effectively capturing task\ndependencies and ensuring sequential integrity. Our approach enables leveraging\nthe structured information to automate workflows as well as empower the human\nusers. By organizing procedures into logical graphs, SOPStruct facilitates\nbacktracking and error correction, offering a scalable solution for process\noptimization. We employ a novel evaluation framework, combining deterministic\nmethods with the Planning Domain Definition Language (PDDL) to verify graph\nsoundness, and non-deterministic assessment by an LLM to ensure completeness.\nWe empirically validate the robustness of our LLM-based structured SOP\nrepresentation methodology across SOPs from different domains and varying\nlevels of complexity. Despite the current lack of automation readiness in many\norganizations, our research highlights the transformative potential of LLMs to\nstreamline process modeling, paving the way for future advancements in\nautomated procedure optimization.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00029v1",
    "published_date": "2025-03-28 22:38:24 UTC",
    "updated_date": "2025-03-28 22:38:24 UTC"
  },
  {
    "arxiv_id": "2504.01029v1",
    "title": "Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents",
    "authors": [
      "Hilda Hadan",
      "Reza Hadi Mogavi",
      "Leah Zhang-Kennedy",
      "Lennart E. Nacke"
    ],
    "abstract": "The rapid growth of artificial intelligence (AI) technologies has changed\ndecision-making in many fields. But, it has also raised major privacy and\nethical concerns. However, many AI incidents taxonomies and guidelines for\nacademia, industry, and government lack grounding in real-world incidents. We\nanalyzed 202 real-world AI privacy and ethical incidents. This produced a\ntaxonomy that classifies incident types across AI lifecycle stages. It accounts\nfor contextual factors such as causes, responsible entities, disclosure\nsources, and impacts. Our findings show insufficient incident reporting from AI\ndevelopers and users. Many incidents are caused by poor organizational\ndecisions and legal non-compliance. Only a few legal actions and corrective\nmeasures exist, while risk-mitigation efforts are limited. Our taxonomy\ncontributes a structured approach in reporting of future AI incidents. Our\nfindings demonstrate that current AI governance frameworks are inadequate. We\nurgently need child-specific protections and AI policies on social media. They\nmust moderate and reduce the spread of harmful AI-generated content. Our\nresearch provides insights for policymakers and practitioners, which lets them\ndesign ethical AI. It also support AI incident detection and risk management.\nFinally, it guides AI policy development. Improved policies will protect people\nfrom harmful AI applications and support innovation in AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "63 pages, 7 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01029v1",
    "published_date": "2025-03-28 21:57:38 UTC",
    "updated_date": "2025-03-28 21:57:38 UTC"
  },
  {
    "arxiv_id": "2503.22881v1",
    "title": "Pairwise Matching of Intermediate Representations for Fine-grained Explainability",
    "authors": [
      "Lauren Shrack",
      "Timm Haucke",
      "Antoine Salaün",
      "Arjun Subramonian",
      "Sara Beery"
    ],
    "abstract": "The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts were in unanimous agreement that PAIR-X was an improvement over\nexisting baselines for deep model explainability, and suggested that its\nvisualizations would be directly applicable to their work. We also propose a\nnovel quantitative evaluation metric for our method, and demonstrate that\nPAIR-X visualizations appear more plausible for correct image matches than\nincorrect ones even when the model similarity score for the pairs is the same.\nBy improving interpretability, PAIR-X enables humans to better distinguish\ncorrect and incorrect matches. Our code is available at:\nhttps://github.com/pairx-explains/pairx",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22881v1",
    "published_date": "2025-03-28 21:13:43 UTC",
    "updated_date": "2025-03-28 21:13:43 UTC"
  },
  {
    "arxiv_id": "2503.22879v2",
    "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
    "authors": [
      "Hung-Yueh Chiang",
      "Chi-Chih Chang",
      "Natalia Frumkin",
      "Kai-Chiang Wu",
      "Mohamed S. Abdelfattah",
      "Diana Marculescu"
    ],
    "abstract": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22879v2",
    "published_date": "2025-03-28 21:10:39 UTC",
    "updated_date": "2025-04-03 15:04:19 UTC"
  },
  {
    "arxiv_id": "2503.22877v1",
    "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models",
    "authors": [
      "Bruno Coelho",
      "Shujaat Mirza",
      "Yuyuan Cui",
      "Christina Pöpper",
      "Damon McCoy"
    ],
    "abstract": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22877v1",
    "published_date": "2025-03-28 21:07:43 UTC",
    "updated_date": "2025-03-28 21:07:43 UTC"
  },
  {
    "arxiv_id": "2504.00027v3",
    "title": "Opioid Named Entity Recognition (ONER-2025) from Reddit",
    "authors": [
      "Grigori Sidorov",
      "Muhammad Ahmad",
      "Iqra Ameer",
      "Muhammad Usman",
      "Ildar Batyrshin"
    ],
    "abstract": "The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00027v3",
    "published_date": "2025-03-28 20:51:06 UTC",
    "updated_date": "2025-04-30 21:34:50 UTC"
  },
  {
    "arxiv_id": "2504.00026v1",
    "title": "Diffusion models applied to skin and oral cancer classification",
    "authors": [
      "José J. M. Uliana",
      "Renato A. Krohling"
    ],
    "abstract": "This study investigates the application of diffusion models in medical image\nclassification (DiffMIC), focusing on skin and oral lesions. Utilizing the\ndatasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the\ndiffusion model demonstrated competitive performance compared to\nstate-of-the-art deep learning models like Convolutional Neural Networks (CNNs)\nand Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved\na balanced accuracy of 0.6457 for six-class classification and 0.8357 for\nbinary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it\nattained a balanced accuracy of 0.9050. These results suggest that diffusion\nmodels are viable models for classifying medical images of skin and oral\nlesions. In addition, we investigate the robustness of the model trained on\nPAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA\ndataset.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00026v1",
    "published_date": "2025-03-28 20:29:35 UTC",
    "updated_date": "2025-03-28 20:29:35 UTC"
  },
  {
    "arxiv_id": "2503.22853v1",
    "title": "Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought Prompting: Pedagogical Strategies for Machines",
    "authors": [
      "Liam Pond",
      "Ichiro Fujinaga"
    ],
    "abstract": "This study evaluates the baseline capabilities of Large Language Models\n(LLMs) like ChatGPT, Claude, and Gemini to learn concepts in music theory\nthrough in-context learning and chain-of-thought prompting. Using carefully\ndesigned prompts (in-context learning) and step-by-step worked examples\n(chain-of-thought prompting), we explore how LLMs can be taught increasingly\ncomplex material and how pedagogical strategies for human learners translate to\neducating machines. Performance is evaluated using questions from an official\nCanadian Royal Conservatory of Music (RCM) Level 6 examination, which covers a\ncomprehensive range of topics, including interval and chord identification, key\ndetection, cadence classification, and metrical analysis. Additionally, we\nevaluate the suitability of various music encoding formats for these tasks\n(ABC, Humdrum, MEI, MusicXML). All experiments were run both with and without\ncontextual prompts. Results indicate that without context, ChatGPT with MEI\nperforms the best at 52%, while with context, Claude with MEI performs the best\nat 75%. Future work will further refine prompts and expand to cover more\nadvanced music theory concepts. This research contributes to the broader\nunderstanding of teaching LLMs and has applications for educators, students,\nand developers of AI music tools alike.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "11 pages, 4 figures, 3 tables. Published in Volume 1 of the\n  Proceedings of the 17th International Conference on Computer Supported Music\n  Education (CSME 2025). Presented on 3 April 2025 in Porto, Portugal",
    "pdf_url": "http://arxiv.org/pdf/2503.22853v1",
    "published_date": "2025-03-28 20:15:24 UTC",
    "updated_date": "2025-03-28 20:15:24 UTC"
  },
  {
    "arxiv_id": "2503.22851v2",
    "title": "RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation",
    "authors": [
      "Feng Lin",
      "Dong Jae Kim",
      "Zhenhao Li",
      "Jinqiu Yang",
      "Tse-Hsun",
      "Chen"
    ],
    "abstract": "When using LLMs to address Non-Functional Requirements (NFRs), developers may\nbehave differently (e.g., expressing the same NFR in different words). Robust\nLLMs should output consistent results across these variations; however, this\naspect remains underexplored. We propose RobuNFR for evaluating the robustness\nof LLMs in NFR-aware code generation across four NFR dimensions: design,\nreadability, reliability, and performance, using three methodologies: prompt\nvariation, regression testing, and diverse workflows. Our experiments show that\nRobuNFR reveals robustness issues in the tested LLMs when considering NFRs in\ncode generation. Specifically, under prompt variation, including NFRs leads to\na decrease in Pass@1 by up to 39 percent and an increase in the standard\ndeviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e.,\nFunction-Only). While incorporating NFRs generally improves overall NFR\nmetrics, it also results in higher prompt sensitivity. In regression settings,\nsome LLMs exhibit differences across versions, with improvements in one aspect\n(e.g., reduced code smells) often accompanied by regressions in another (e.g.,\ndecreased correctness), revealing inconsistencies that challenge their\nrobustness. When varying workflows, the tested LLMs show significantly\ndifferent NFR-aware code generation capabilities between two workflows: (1)\nintegrating NFRs and functional requirements into the initial prompt and (2)\nenhancing Function-Only-generated code with the same NFR.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Corrected metadata: fixed author name in submission form (TeX file\n  was already correct)",
    "pdf_url": "http://arxiv.org/pdf/2503.22851v2",
    "published_date": "2025-03-28 20:05:33 UTC",
    "updated_date": "2025-04-03 00:55:35 UTC"
  },
  {
    "arxiv_id": "2503.22829v2",
    "title": "Nonhuman Primate Brain Tissue Segmentation Using a Transfer Learning Approach",
    "authors": [
      "Zhen Lin",
      "Hongyu Yuan",
      "Richard Barcus",
      "Qing Lyu",
      "Sucheta Chakravarty",
      "Megan E. Lipford",
      "Carol A. Shively",
      "Suzanne Craft",
      "Mohammad Kawas",
      "Jeongchul Kim",
      "Christopher T. Whitlow"
    ],
    "abstract": "Non-human primates (NHPs) serve as critical models for understanding human\nbrain function and neurological disorders due to their close evolutionary\nrelationship with humans. Accurate brain tissue segmentation in NHPs is\ncritical for understanding neurological disorders, but challenging due to the\nscarcity of annotated NHP brain MRI datasets, the small size of the NHP brain,\nthe limited resolution of available imaging data and the anatomical differences\nbetween human and NHP brains. To address these challenges, we propose a novel\napproach utilizing STU-Net with transfer learning to leverage knowledge\ntransferred from human brain MRI data to enhance segmentation accuracy in the\nNHP brain MRI, particularly when training data is limited. The combination of\nSTU-Net and transfer learning effectively delineates complex tissue boundaries\nand captures fine anatomical details specific to NHP brains. Notably, our\nmethod demonstrated improvement in segmenting small subcortical structures such\nas putamen and thalamus that are challenging to resolve with limited spatial\nresolution and tissue contrast, and achieved DSC of over 0.88, IoU over 0.8 and\nHD95 under 7. This study introduces a robust method for multi-class brain\ntissue segmentation in NHPs, potentially accelerating research in evolutionary\nneuroscience and preclinical studies of neurological disorders relevant to\nhuman health.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22829v2",
    "published_date": "2025-03-28 18:51:22 UTC",
    "updated_date": "2025-04-01 11:52:54 UTC"
  },
  {
    "arxiv_id": "2503.22809v2",
    "title": "Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting",
    "authors": [
      "Uddhav Bhattarai",
      "Rajkishan Arikapudi",
      "Steven A. Fennimore",
      "Frank N Martin",
      "Stavros G. Vougioukas"
    ],
    "abstract": "Manual fruit harvesting is common in agriculture, but the amount of time\npickers spend on non-productive activities can make it very inefficient.\nAccurately identifying picking vs. non-picking activity is crucial for\nestimating picker efficiency and optimising labour management and harvest\nprocesses. In this study, a practical system was developed to calculate the\nefficiency of pickers in commercial strawberry harvesting. Instrumented picking\ncarts were developed to record the harvested fruit weight, geolocation, and\ncart movement in real time. These carts were deployed during the commercial\nstrawberry harvest season in Santa Maria, CA. The collected data was then used\nto train a CNN-LSTM-based deep neural network to classify a picker's activity\ninto \"Pick\" and \"NoPick\" classes. Experimental evaluations showed that the\nCNN-LSTM model showed promising activity recognition performance with an F1\nscore accuracy of over 0.97. The recognition results were then used to compute\npicker efficiency and the time required to fill a tray. Analysis of the\nseason-long harvest data showed that the average picker efficiency was 75.07%\nwith an estimation accuracy of 95.22%. Furthermore, the average tray fill time\nwas 6.79 minutes with an estimation accuracy of 96.43%. When integrated into\ncommercial harvesting, the proposed technology can aid growers in monitoring\nautomated worker activity and optimising harvests to reduce non-productive time\nand enhance overall harvest efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22809v2",
    "published_date": "2025-03-28 18:16:28 UTC",
    "updated_date": "2025-04-28 23:11:07 UTC"
  },
  {
    "arxiv_id": "2503.22796v1",
    "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
    "authors": [
      "Hanling Zhang",
      "Rundong Su",
      "Zhihang Yuan",
      "Pengtao Chen",
      "Mingzhu Shen Yibo Fan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22796v1",
    "published_date": "2025-03-28 18:00:12 UTC",
    "updated_date": "2025-03-28 18:00:12 UTC"
  },
  {
    "arxiv_id": "2503.22677v1",
    "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ruiningli.com/dso",
    "pdf_url": "http://arxiv.org/pdf/2503.22677v1",
    "published_date": "2025-03-28 17:59:53 UTC",
    "updated_date": "2025-03-28 17:59:53 UTC"
  },
  {
    "arxiv_id": "2503.22675v2",
    "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation",
    "authors": [
      "Jiakai Tang",
      "Sunhao Dai",
      "Teng Shi",
      "Jun Xu",
      "Xu Chen",
      "Wen Chen",
      "Wu Jian",
      "Yuning Jiang"
    ],
    "abstract": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22675v2",
    "published_date": "2025-03-28 17:59:03 UTC",
    "updated_date": "2025-04-16 10:20:11 UTC"
  },
  {
    "arxiv_id": "2503.22674v1",
    "title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?",
    "authors": [
      "Belinda Z. Li",
      "Been Kim",
      "Zi Wang"
    ],
    "abstract": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Code and dataset are available at\n  \\url{https://github.com/google-deepmind/questbench}",
    "pdf_url": "http://arxiv.org/pdf/2503.22674v1",
    "published_date": "2025-03-28 17:58:40 UTC",
    "updated_date": "2025-03-28 17:58:40 UTC"
  },
  {
    "arxiv_id": "2503.22673v2",
    "title": "ActionStudio: A Lightweight Framework for Data and Training of Large Action Models",
    "authors": [
      "Jianguo Zhang",
      "Thai Hoang",
      "Ming Zhu",
      "Zuxin Liu",
      "Shiyu Wang",
      "Tulika Awalgaonkar",
      "Akshara Prabhakar",
      "Haolin Chen",
      "Weiran Yao",
      "Zhiwei Liu",
      "Juntao Tan",
      "Juan Carlos Niebles",
      "Shelby Heinecke",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong"
    ],
    "abstract": "Action models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We present ActionStudio, a lightweight\nand extensible data and training framework designed for large action models.\nActionStudio unifies heterogeneous agent trajectories through a standardized\nformat, supports diverse training paradigms including LoRA, full fine-tuning,\nand distributed setups, and integrates robust preprocessing and verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to\nfacilitate research in the community.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages; large action models; xLAM",
    "pdf_url": "http://arxiv.org/pdf/2503.22673v2",
    "published_date": "2025-03-28 17:58:33 UTC",
    "updated_date": "2025-03-31 16:38:50 UTC"
  },
  {
    "arxiv_id": "2503.22672v1",
    "title": "Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder Re-rankers",
    "authors": [
      "Francesca Pezzuti",
      "Sean MacAvaney",
      "Nicola Tonellotto"
    ],
    "abstract": "State-of-the-art cross-encoders can be fine-tuned to be highly effective in\npassage re-ranking. The typical fine-tuning process of cross-encoders as\nre-rankers requires large amounts of manually labelled data, a contrastive\nlearning objective, and a set of heuristically sampled negatives. An\nalternative recent approach for fine-tuning instead involves teaching the model\nto mimic the rankings of a highly effective large language model using a\ndistillation objective. These fine-tuning strategies can be applied either\nindividually, or in sequence. In this work, we systematically investigate the\neffectiveness of point-wise cross-encoders when fine-tuned independently in a\nsingle stage, or sequentially in two stages. Our experiments show that the\neffectiveness of point-wise cross-encoders fine-tuned using contrastive\nlearning is indeed on par with that of models fine-tuned with multi-stage\napproaches. Code is available for reproduction at\nhttps://github.com/fpezzuti/multistage-finetuning.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages. To be published as short paper in the Proceedings of the\n  European Conference on Information Retrieval (ECIR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.22672v1",
    "published_date": "2025-03-28 17:58:31 UTC",
    "updated_date": "2025-03-28 17:58:31 UTC"
  },
  {
    "arxiv_id": "2503.22658v1",
    "title": "Evaluation of Machine-generated Biomedical Images via A Tally-based Similarity Measure",
    "authors": [
      "Frank J. Brooks",
      "Rucha Deshpande"
    ],
    "abstract": "Super-resolution, in-painting, whole-image generation, unpaired\nstyle-transfer, and network-constrained image reconstruction each include an\naspect of machine-learned image synthesis where the actual ground truth is not\nknown at time of use. It is generally difficult to quantitatively and\nauthoritatively evaluate the quality of synthetic images; however, in\nmission-critical biomedical scenarios robust evaluation is paramount. In this\nwork, all practical image-to-image comparisons really are relative\nqualifications, not absolute difference quantifications; and, therefore,\nmeaningful evaluation of generated image quality can be accomplished using the\nTversky Index, which is a well-established measure for assessing perceptual\nsimilarity. This evaluation procedure is developed and then demonstrated using\nmultiple image data sets, both real and simulated. The main result is that when\nthe subjectivity and intrinsic deficiencies of any feature-encoding choice are\nput upfront, Tversky's method leads to intuitive results, whereas traditional\nmethods based on summarizing distances in deep feature spaces do not.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages. Manuscript under review at IEEE. Data available at\n  https://doi.org/10.13012/B2IDB-2642688_V1",
    "pdf_url": "http://arxiv.org/pdf/2503.22658v1",
    "published_date": "2025-03-28 17:44:01 UTC",
    "updated_date": "2025-03-28 17:44:01 UTC"
  },
  {
    "arxiv_id": "2503.22655v1",
    "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
    "authors": [
      "Xiaomin Yu",
      "Pengxiang Ding",
      "Wenjie Zhang",
      "Siteng Huang",
      "Songyang Gao",
      "Chengwei Qin",
      "Kejian Wu",
      "Zhaoxin Fan",
      "Ziyue Qiao",
      "Donglin Wang"
    ],
    "abstract": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22655v1",
    "published_date": "2025-03-28 17:43:00 UTC",
    "updated_date": "2025-03-28 17:43:00 UTC"
  },
  {
    "arxiv_id": "2503.22782v1",
    "title": "Patronus: Bringing Transparency to Diffusion Models with Prototypes",
    "authors": [
      "Nina Weng",
      "Aasa Feragen",
      "Siavash Bigdeli"
    ],
    "abstract": "Diffusion-based generative models, such as Denoising Diffusion Probabilistic\nModels (DDPMs), have achieved remarkable success in image generation, but their\nstep-by-step denoising process remains opaque, leaving critical aspects of the\ngeneration mechanism unexplained. To address this, we introduce\n\\emph{Patronus}, an interpretable diffusion model inspired by ProtoPNet.\nPatronus integrates a prototypical network into DDPMs, enabling the extraction\nof prototypes and conditioning of the generation process on their prototype\nactivation vector. This design enhances interpretability by showing the learned\nprototypes and how they influence the generation process. Additionally, the\nmodel supports downstream tasks like image manipulation, enabling more\ntransparent and controlled modifications. Moreover, Patronus could reveal\nshortcut learning in the generation process by detecting unwanted correlations\nbetween learned prototypes. Notably, Patronus operates entirely without any\nannotations or text prompts. This work opens new avenues for understanding and\ncontrolling diffusion models through prototype-based interpretability. Our code\nis available at\n\\href{https://github.com/nina-weng/patronus}{https://github.com/nina-weng/patronus}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22782v1",
    "published_date": "2025-03-28 17:31:40 UTC",
    "updated_date": "2025-03-28 17:31:40 UTC"
  },
  {
    "arxiv_id": "2503.22634v1",
    "title": "Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels",
    "authors": [
      "Adam Wei",
      "Abhinav Agarwal",
      "Boyuan Chen",
      "Rohan Bosworth",
      "Nicholas Pfaff",
      "Russ Tedrake"
    ],
    "abstract": "In imitation learning for robotics, cotraining with demonstration data\ngenerated both in simulation and on real hardware has emerged as a powerful\nrecipe to overcome the sim2real gap. This work seeks to elucidate basic\nprinciples of this sim-and-real cotraining to help inform simulation design,\nsim-and-real dataset creation, and policy training. Focusing narrowly on the\ncanonical task of planar pushing from camera inputs enabled us to be thorough\nin our study. These experiments confirm that cotraining with simulated data\n\\emph{can} dramatically improve performance in real, especially when real data\nis limited. Performance gains scale with simulated data, but eventually\nplateau; real-world data increases this performance ceiling. The results also\nsuggest that reducing the domain gap in physics may be more important than\nvisual fidelity for non-prehensile manipulation tasks. Perhaps surprisingly,\nhaving some visual domain gap actually helps the cotrained policy -- binary\nprobes reveal that high-performing policies learn to distinguish simulated\ndomains from real. We conclude by investigating this nuance and mechanisms that\nfacilitate positive transfer between sim-and-real. In total, our experiments\nspan over 40 real-world policies (evaluated on 800+ trials) and 200 simulated\npolicies (evaluated on 40,000+ trials).",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 15 figures, In Submission to IROS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.22634v1",
    "published_date": "2025-03-28 17:25:57 UTC",
    "updated_date": "2025-03-28 17:25:57 UTC"
  },
  {
    "arxiv_id": "2503.22625v1",
    "title": "Challenges and Paths Towards AI for Software Engineering",
    "authors": [
      "Alex Gu",
      "Naman Jain",
      "Wen-Ding Li",
      "Manish Shetty",
      "Yijia Shao",
      "Ziyang Li",
      "Diyi Yang",
      "Kevin Ellis",
      "Koushik Sen",
      "Armando Solar-Lezama"
    ],
    "abstract": "AI for software engineering has made remarkable progress recently, becoming a\nnotable success within generative AI. Despite this, there are still many\nchallenges that need to be addressed before automated software engineering\nreaches its full potential. It should be possible to reach high levels of\nautomation where humans can focus on the critical decisions of what to build\nand how to balance difficult tradeoffs while most routine development effort is\nautomated away. Reaching this level of automation will require substantial\nresearch and engineering efforts across academia and industry. In this paper,\nwe aim to discuss progress towards this in a threefold manner. First, we\nprovide a structured taxonomy of concrete tasks in AI for software engineering,\nemphasizing the many other tasks in software engineering beyond code generation\nand completion. Second, we outline several key bottlenecks that limit current\napproaches. Finally, we provide an opinionated list of promising research\ndirections toward making progress on these bottlenecks, hoping to inspire\nfuture research in this rapidly maturing field.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "75 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.22625v1",
    "published_date": "2025-03-28 17:17:57 UTC",
    "updated_date": "2025-03-28 17:17:57 UTC"
  },
  {
    "arxiv_id": "2504.03715v1",
    "title": "Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces",
    "authors": [
      "Hannah Janmohamed",
      "Antoine Cully"
    ],
    "abstract": "Quality-Diversity algorithms are powerful tools for discovering diverse,\nhigh-performing solutions. Recently, Multi-Objective Quality-Diversity (MOQD)\nextends QD to problems with several objectives while preserving solution\ndiversity. MOQD has shown promise in fields such as robotics and materials\nscience, where finding trade-offs between competing objectives like energy\nefficiency and speed, or material properties is essential. However, existing\nmethods in MOQD rely on tessellating the feature space into a grid structure,\nwhich prevents their application in domains where feature spaces are unknown or\nmust be learned, such as complex biological systems or latent exploration\ntasks. In this work, we introduce Multi-Objective Unstructured Repertoire for\nQuality-Diversity (MOUR-QD), a MOQD algorithm designed for unstructured and\nunbounded feature spaces. We evaluate MOUR-QD on five robotic tasks.\nImportantly, we show that our method excels in tasks where features must be\nlearned, paving the way for applying MOQD to unsupervised domains. We also\ndemonstrate that MOUR-QD is advantageous in domains with unbounded feature\nspaces, outperforming existing grid-based methods. Finally, we demonstrate that\nMOUR-QD is competitive with established MOQD methods on existing MOQD tasks and\nachieves double the MOQD-score in some environments. MOUR-QD opens up new\nopportunities for MOQD in domains like protein design and image generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted GECCO 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03715v1",
    "published_date": "2025-03-28 16:55:39 UTC",
    "updated_date": "2025-03-28 16:55:39 UTC"
  },
  {
    "arxiv_id": "2503.22610v1",
    "title": "Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users",
    "authors": [
      "Antonia Karamolegkou",
      "Malvina Nikandrou",
      "Georgios Pantazopoulos",
      "Danae Sanchez Villegas",
      "Phillip Rust",
      "Ruchira Dhar",
      "Daniel Hershcovich",
      "Anders Søgaard"
    ],
    "abstract": "This paper explores the effectiveness of Multimodal Large Language models\n(MLLMs) as assistive technologies for visually impaired individuals. We conduct\na user survey to identify adoption patterns and key challenges users face with\nsuch technologies. Despite a high adoption rate of these models, our findings\nhighlight concerns related to contextual understanding, cultural sensitivity,\nand complex scene understanding, particularly for individuals who may rely\nsolely on them for visual interpretation. Informed by these results, we collate\nfive user-centred tasks with image and video inputs, including a novel task on\nOptical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals\nthat further advancements are necessary to overcome limitations related to\ncultural context, multilingual support, Braille reading comprehension,\nassistive object recognition, and hallucinations. This work provides critical\ninsights into the future direction of multimodal AI for accessibility,\nunderscoring the need for more inclusive, robust, and trustworthy visual\nassistance technologies.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22610v1",
    "published_date": "2025-03-28 16:54:25 UTC",
    "updated_date": "2025-03-28 16:54:25 UTC"
  },
  {
    "arxiv_id": "2503.22600v1",
    "title": "Generative Latent Neural PDE Solver using Flow Matching",
    "authors": [
      "Zijie Li",
      "Anthony Zhou",
      "Amir Barati Farimani"
    ],
    "abstract": "Autoregressive next-step prediction models have become the de-facto standard\nfor building data-driven neural solvers to forecast time-dependent partial\ndifferential equations (PDEs). Denoise training that is closely related to\ndiffusion probabilistic model has been shown to enhance the temporal stability\nof neural solvers, while its stochastic inference mechanism enables ensemble\npredictions and uncertainty quantification. In principle, such training\ninvolves sampling a series of discretized diffusion timesteps during both\ntraining and inference, inevitably increasing computational overhead. In\naddition, most diffusion models apply isotropic Gaussian noise on structured,\nuniform grids, limiting their adaptability to irregular domains. We propose a\nlatent diffusion model for PDE simulation that embeds the PDE state in a\nlower-dimensional latent space, which significantly reduces computational\ncosts. Our framework uses an autoencoder to map different types of meshes onto\na unified structured latent grid, capturing complex geometries. By analyzing\ncommon diffusion paths, we propose to use a coarsely sampled noise schedule\nfrom flow matching for both training and testing. Numerical experiments show\nthat the proposed model outperforms several deterministic baselines in both\naccuracy and long-term stability, highlighting the potential of diffusion-based\napproaches for robust data-driven PDE learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.22600v1",
    "published_date": "2025-03-28 16:44:28 UTC",
    "updated_date": "2025-03-28 16:44:28 UTC"
  },
  {
    "arxiv_id": "2503.22592v1",
    "title": "KEVS: Enhancing Segmentation of Visceral Adipose Tissue in Pre-Cystectomy CT with Gaussian Kernel Density Estimation",
    "authors": [
      "Thomas Boucher",
      "Nicholas Tetlow",
      "Annie Fung",
      "Amy Dewar",
      "Pietro Arina",
      "Sven Kerneis",
      "John Whittle",
      "Evangelos B. Mazomenos"
    ],
    "abstract": "Purpose: The distribution of visceral adipose tissue (VAT) in cystectomy\npatients is indicative of the incidence of post-operative complications.\nExisting VAT segmentation methods for computed tomography (CT) employing\nintensity thresholding have limitations relating to inter-observer variability.\nMoreover, the difficulty in creating ground-truth masks limits the development\nof deep learning (DL) models for this task. This paper introduces a novel\nmethod for VAT prediction in pre-cystectomy CT, which is fully automated and\ndoes not require ground-truth VAT masks for training, overcoming aforementioned\nlimitations. Methods: We introduce the Kernel density Enhanced VAT Segmentator\n( KEVS), combining a DL semantic segmentation model, for multi-body feature\nprediction, with Gaussian kernel density estimation analysis of predicted\nsubcutaneous adipose tissue to achieve accurate scan-specific predictions of\nVAT in the abdominal cavity. Uniquely for a DL pipeline, KEVS does not require\nground-truth VAT masks. Results: We verify the ability of KEVS to accurately\nsegment abdominal organs in unseen CT data and compare KEVS VAT segmentation\npredictions to existing state-of-the-art (SOTA) approaches in a dataset of 20\npre-cystectomy CT scans, collected from University College London Hospital\n(UCLH-Cyst), with expert ground-truth annotations. KEVS presents a 4.80% and\n6.02% improvement in Dice Coefficient over the second best DL and\nthresholding-based VAT segmentation techniques respectively when evaluated on\nUCLH-Cyst. Conclusion: This research introduces KEVS; an automated, SOTA method\nfor the prediction of VAT in pre-cystectomy CT which eliminates inter-observer\nvariability and is trained entirely on open-source CT datasets which do not\ncontain ground-truth VAT masks.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Preprint for submission to IPCAI special edition of IJCARS 2025,\n  version prior to any peer review",
    "pdf_url": "http://arxiv.org/pdf/2503.22592v1",
    "published_date": "2025-03-28 16:41:09 UTC",
    "updated_date": "2025-03-28 16:41:09 UTC"
  },
  {
    "arxiv_id": "2503.22589v1",
    "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos, 1952-2012",
    "authors": [
      "Adam Breuer",
      "Bryce J. Dietrich",
      "Michael H. Crespin",
      "Matthew Butler",
      "J. A. Pyrse",
      "Kosuke Imai"
    ],
    "abstract": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.MM",
    "comment": "17 pages, 7 tables, 4 figures, and linked datasets",
    "pdf_url": "http://arxiv.org/pdf/2503.22589v1",
    "published_date": "2025-03-28 16:36:23 UTC",
    "updated_date": "2025-03-28 16:36:23 UTC"
  },
  {
    "arxiv_id": "2503.22585v1",
    "title": "Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish",
    "authors": [
      "Kevin Cohen",
      "Laura Manrique-Gómez",
      "Rubén Manrique"
    ],
    "abstract": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22585v1",
    "published_date": "2025-03-28 16:33:24 UTC",
    "updated_date": "2025-03-28 16:33:24 UTC"
  },
  {
    "arxiv_id": "2503.22577v2",
    "title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization",
    "authors": [
      "Iñigo Pikabea",
      "Iñaki Lacunza",
      "Oriol Pareras",
      "Carlos Escolano",
      "Aitor Gonzalez-Agirre",
      "Javier Hernando",
      "Marta Villegas"
    ],
    "abstract": "Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "v2: Expanded model merging experiments. Fix duplicated subsection on\n  limitations",
    "pdf_url": "http://arxiv.org/pdf/2503.22577v2",
    "published_date": "2025-03-28 16:26:52 UTC",
    "updated_date": "2025-05-20 10:29:41 UTC"
  },
  {
    "arxiv_id": "2503.22575v1",
    "title": "On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations",
    "authors": [
      "Rajdeep Singh Hundal",
      "Yan Xiao",
      "Xiaochun Cao",
      "Jin Song Dong",
      "Manuel Rigger"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence\nwhere an agent uses a neural network to learn which actions to take in a given\nenvironment. DRL has recently gained traction from being able to solve complex\nenvironments like driving simulators, 3D robotic control, and\nmultiplayer-online-battle-arena video games. Numerous implementations of the\nstate-of-the-art algorithms responsible for training these agents, like the\nDeep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms,\ncurrently exist. However, studies make the mistake of assuming implementations\nof the same algorithm to be consistent and thus, interchangeable. In this\npaper, through a differential testing lens, we present the results of studying\nthe extent of implementation inconsistencies, their effect on the\nimplementations' performance, as well as their impact on the conclusions of\nprior studies under the assumption of interchangeable implementations. The\noutcomes of our differential tests showed significant discrepancies between the\ntested algorithm implementations, indicating that they are not interchangeable.\nIn particular, out of the five PPO implementations tested on 56 games, three\nimplementations achieved superhuman performance for 50% of their total trials\nwhile the other two implementations only achieved superhuman performance for\nless than 15% of their total trials. As part of a meticulous manual analysis of\nthe implementations' source code, we analyzed implementation discrepancies and\ndetermined that code-level inconsistencies primarily caused these\ndiscrepancies. Lastly, we replicated a study and showed that this assumption of\nimplementation interchangeability was sufficient to flip experiment outcomes.\nTherefore, this calls for a shift in how implementations are being used.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2.5; I.2.6"
    ],
    "primary_category": "cs.SE",
    "comment": "To be published in the 47th International Conference on Software\n  Engineering (ICSE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.22575v1",
    "published_date": "2025-03-28 16:25:06 UTC",
    "updated_date": "2025-03-28 16:25:06 UTC"
  },
  {
    "arxiv_id": "2504.03714v1",
    "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models",
    "authors": [
      "Runpeng Dai",
      "Run Yang",
      "Fan Zhou",
      "Hongtu Zhu"
    ],
    "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have become\nessential to general artificial intelligence, exhibiting remarkable\ncapabilities in task understanding and problem-solving. However, the real-world\nreliability of these models critically depends on their stability, which\nremains an underexplored area. Despite their widespread use, rigorous studies\nexamining the stability of LLMs under various perturbations are still lacking.\nIn this paper, we address this gap by proposing a novel stability measure for\nLLMs, inspired by statistical methods rooted in information geometry. Our\nmeasure possesses desirable invariance properties, making it well-suited for\nanalyzing model sensitivity to both parameter and input perturbations. To\nassess the effectiveness of our approach, we conduct extensive experiments on\nmodels ranging in size from 1.5B to 13B parameters. Our results demonstrate the\nutility of our measure in identifying salient parameters and detecting\nvulnerable regions in input images or critical dimensions in token embeddings.\nFurthermore, leveraging our stability framework, we enhance model robustness\nduring model merging, leading to improved performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03714v1",
    "published_date": "2025-03-28 16:23:59 UTC",
    "updated_date": "2025-03-28 16:23:59 UTC"
  },
  {
    "arxiv_id": "2503.22573v1",
    "title": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines",
    "authors": [
      "Kar Balan",
      "Robert Learney",
      "Tim Wood"
    ],
    "abstract": "The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to 11th ACM International Workshop on Security and Privacy\n  Analytics (IWSPA 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.22573v1",
    "published_date": "2025-03-28 16:20:57 UTC",
    "updated_date": "2025-03-28 16:20:57 UTC"
  },
  {
    "arxiv_id": "2503.22562v1",
    "title": "Niyama : Breaking the Silos of LLM Inference Serving",
    "authors": [
      "Kanishk Goel",
      "Jayashree Mohan",
      "Nipun Kwatra",
      "Ravi Shreyas Anupindi",
      "Ramachandran Ramjee"
    ],
    "abstract": "The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22562v1",
    "published_date": "2025-03-28 16:04:20 UTC",
    "updated_date": "2025-03-28 16:04:20 UTC"
  },
  {
    "arxiv_id": "2504.08754v3",
    "title": "Towards Personalized Conversational Sales Agents : Contextual User Profiling for Strategic Action",
    "authors": [
      "Tongyoung Kim",
      "Jeongeun Lee",
      "Soojin Yoon",
      "Sunghwan Kim",
      "Dongha Lee"
    ],
    "abstract": "Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08754v3",
    "published_date": "2025-03-28 15:49:52 UTC",
    "updated_date": "2025-04-16 07:59:48 UTC"
  },
  {
    "arxiv_id": "2504.00024v1",
    "title": "A multi-locus predictiveness curve and its summary assessment for genetic risk prediction",
    "authors": [
      "Changshuai Wei",
      "Ming Li",
      "Yalu Wen",
      "Chengyin Ye",
      "Qing Lu"
    ],
    "abstract": "With the advance of high-throughput genotyping and sequencing technologies,\nit becomes feasible to comprehensive evaluate the role of massive genetic\npredictors in disease prediction. There exists, therefore, a critical need for\ndeveloping appropriate statistical measurements to access the combined effects\nof these genetic variants in disease prediction. Predictiveness curve is\ncommonly used as a graphical tool to measure the predictive ability of a risk\nprediction model on a single continuous biomarker. Yet, for most complex\ndiseases, risk prediciton models are formed on multiple genetic variants. We\ntherefore propose a multi-marker predictiveness curve and provide a\nnon-parametric method to construct the curve for case-control studies. We\nfurther introduce a global predictiveness U and a partial predictiveness U to\nsummarize prediction curve across the whole population and sub-population of\nclinical interest, respectively. We also demonstrate the connections of\npredictiveness curve with ROC curve and Lorenz curve. Through simulation, we\ncompared the performance of the predictiveness U to other three summary\nindices: R square, Total Gain, and Average Entropy, and showed that\nPredictiveness U outperformed the other three indexes in terms of unbiasedness\nand robustness. Moreover, we simulated a series of rare-variants disease model,\nfound partial predictiveness U performed better than global predictiveness U.\nFinally, we conducted a real data analysis, using predictiveness curve and\npredictiveness U to evaluate a risk prediction model for Nicotine Dependence.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00024v1",
    "published_date": "2025-03-28 15:49:39 UTC",
    "updated_date": "2025-03-28 15:49:39 UTC"
  },
  {
    "arxiv_id": "2504.01981v1",
    "title": "NLS: Natural-Level Synthesis for Hardware Implementation Through GenAI",
    "authors": [
      "Kaiyuan Yang",
      "Huang Ouyang",
      "Xinyi Wang",
      "Bingjie Lu",
      "Yanbo Wang",
      "Charith Abhayaratne",
      "Sizhao Li",
      "Long Jin",
      "Tiantai Deng"
    ],
    "abstract": "This paper introduces Natural-Level Synthesis, an innovative approach for\ngenerating hardware using generative artificial intelligence on both the system\nlevel and component-level. NLS bridges a gap in current hardware development\nprocesses, where algorithm and application engineers' involvement typically\nends at the requirements stage. With NLS, engineers can participate more deeply\nin the development, synthesis, and test stages by using Gen-AI models to\nconvert natural language descriptions directly into Hardware Description\nLanguage code. This approach not only streamlines hardware development but also\nimproves accessibility, fostering a collaborative workflow between hardware and\nalgorithm engineers. We developed the NLS tool to facilitate natural\nlanguage-driven HDL synthesis, enabling rapid generation of system-level HDL\ndesigns while significantly reducing development complexity. Evaluated through\ncase studies and benchmarks using Performance, Power, and Area metrics, NLS\nshows its potential to enhance resource efficiency in hardware development.\nThis work provides a extensible, efficient solution for hardware synthesis and\nestablishes a Visual Studio Code Extension to assess Gen-AI-driven HDL\ngeneration and system integration, laying a foundation for future AI-enhanced\nand AI-in-the-loop Electronic Design Automation tools.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "9 pages, 4 figures, and 5 tables. Submitted for IEEE Transactions on\n  CAD. The same content was accepted by Design Automation Conference 2025 as a\n  WIP Poster (not count as publication, so it's ok to submit the content\n  elsewhere). TCAD info: https://ieeexplore.ieee.org/document/10186100\n  Submitted for review on 26th of Feb. Reference - TCAD-2025-0203",
    "pdf_url": "http://arxiv.org/pdf/2504.01981v1",
    "published_date": "2025-03-28 15:46:01 UTC",
    "updated_date": "2025-03-28 15:46:01 UTC"
  },
  {
    "arxiv_id": "2503.22541v1",
    "title": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles",
    "authors": [
      "Haicheng Liao",
      "Hanlin Kong",
      "Bin Rao",
      "Bonan Wang",
      "Chengyue Wang",
      "Guyang Yu",
      "Yuming Huang",
      "Ruru Tang",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "abstract": "Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22541v1",
    "published_date": "2025-03-28 15:38:21 UTC",
    "updated_date": "2025-03-28 15:38:21 UTC"
  },
  {
    "arxiv_id": "2503.22537v1",
    "title": "LIM: Large Interpolator Model for Dynamic Reconstruction",
    "authors": [
      "Remy Sabathier",
      "Niloy J. Mitra",
      "David Novotny"
    ],
    "abstract": "Reconstructing dynamic assets from video data is central to many in computer\nvision and graphics tasks. Existing 4D reconstruction approaches are limited by\ncategory-specific models or slow optimization-based methods. Inspired by the\nrecent Large Reconstruction Model (LRM), we present the Large Interpolation\nModel (LIM), a transformer-based feed-forward solution, guided by a novel\ncausal consistency loss, for interpolating implicit 3D representations across\ntime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces\na deformed shape at any continuous time $t\\in[t_0,t_1]$, delivering\nhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicit\nmesh tracking across time, producing a consistently uv-textured mesh sequence\nready for integration into existing production pipelines. We also use LIM, in\nconjunction with a diffusion-based multiview generator, to produce dynamic 4D\nreconstructions from monocular videos. We evaluate LIM on various dynamic\ndatasets, benchmarking against image-space interpolation methods (e.g., FiLM)\nand direct triplane linear interpolation, and demonstrate clear advantages. In\nsummary, LIM is the first feed-forward model capable of high-speed tracked 4D\nasset reconstruction across diverse categories.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22537v1",
    "published_date": "2025-03-28 15:36:53 UTC",
    "updated_date": "2025-03-28 15:36:53 UTC"
  },
  {
    "arxiv_id": "2503.22526v1",
    "title": "AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with Fine-Grained Categorization",
    "authors": [
      "Martin Kišš",
      "Michal Hradiš",
      "Martina Dvořáková",
      "Václav Jiroušek",
      "Filip Kersch"
    ],
    "abstract": "We introduce the AnnoPage Dataset, a novel collection of 7550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 2 tables, 6 figures; Submitted to ICDAR25",
    "pdf_url": "http://arxiv.org/pdf/2503.22526v1",
    "published_date": "2025-03-28 15:30:42 UTC",
    "updated_date": "2025-03-28 15:30:42 UTC"
  },
  {
    "arxiv_id": "2503.22524v1",
    "title": "Robust Offline Imitation Learning Through State-level Trajectory Stitching",
    "authors": [
      "Shuze Wang",
      "Yunpeng Mei",
      "Hongjie Cao",
      "Yetian Yuan",
      "Gang Wang",
      "Jian Sun",
      "Jie Chen"
    ],
    "abstract": "Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22524v1",
    "published_date": "2025-03-28 15:28:36 UTC",
    "updated_date": "2025-03-28 15:28:36 UTC"
  },
  {
    "arxiv_id": "2503.22517v2",
    "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities",
    "authors": [
      "Raman Dutt",
      "Harleen Hanspal",
      "Guoxuan Xia",
      "Petru-Daniel Tudosiu",
      "Alexander Black",
      "Yongxin Yang",
      "Steven McDonagh",
      "Sarah Parisot"
    ],
    "abstract": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22517v2",
    "published_date": "2025-03-28 15:21:24 UTC",
    "updated_date": "2025-04-01 10:42:11 UTC"
  },
  {
    "arxiv_id": "2503.22513v1",
    "title": "Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets",
    "authors": [
      "Martin Kišš",
      "Michal Hradiš"
    ],
    "abstract": "Self-supervised learning has emerged as a powerful approach for leveraging\nlarge-scale unlabeled data to improve model performance in various domains. In\nthis paper, we explore masked self-supervised pre-training for text recognition\ntransformers. Specifically, we propose two modifications to the pre-training\nphase: progressively increasing the masking probability, and modifying the loss\nfunction to incorporate both masked and non-masked patches. We conduct\nextensive experiments using a dataset of 50M unlabeled text lines for\npre-training and four differently sized annotated datasets for fine-tuning.\nFurthermore, we compare our pre-trained models against those trained with\ntransfer learning, demonstrating the effectiveness of the self-supervised\npre-training. In particular, pre-training consistently improves the character\nerror rate of models, in some cases up to 30 % relatively. It is also on par\nwith transfer learning but without relying on extra annotated text lines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 7 tables, 6 figures; Submitted to ICDAR25",
    "pdf_url": "http://arxiv.org/pdf/2503.22513v1",
    "published_date": "2025-03-28 15:16:48 UTC",
    "updated_date": "2025-03-28 15:16:48 UTC"
  },
  {
    "arxiv_id": "2504.01980v3",
    "title": "Information Gain Is Not All You Need",
    "authors": [
      "Ludvig Ericson",
      "José Pedro",
      "Patric Jensfelt"
    ],
    "abstract": "Autonomous exploration in mobile robotics often involves a trade-off between\ntwo objectives: maximizing environmental coverage and minimizing the total path\nlength. In the widely used information gain paradigm, exploration is guided by\nthe expected value of observations. While this approach is effective under\nbudget-constrained settings--where only a limited number of observations can be\nmade--it fails to align with quality-constrained scenarios, in which the robot\nmust fully explore the environment to a desired level of certainty or quality.\nIn such cases, total information gain is effectively fixed, and maximizing it\nper step can lead to inefficient, greedy behavior and unnecessary backtracking.\nThis paper argues that information gain should not serve as an optimization\nobjective in quality-constrained exploration. Instead, it should be used to\nfilter viable candidate actions. We propose a novel heuristic, distance\nadvantage, which selects candidate frontiers based on a trade-off between\nproximity to the robot and remoteness from other frontiers. This heuristic aims\nto reduce future detours by prioritizing exploration of isolated regions before\nthe robot's opportunity to visit them efficiently has passed. We evaluate our\nmethod in simulated environments against classical frontier-based exploration\nand gain-maximizing approaches. Results show that distance advantage\nsignificantly reduces total path length across a variety of environments, both\nwith and without access to prior map predictions. Our findings challenge the\nassumption that more accurate gain estimation improves performance and offer a\nmore suitable alternative for the quality-constrained exploration paradigm.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 6 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2504.01980v3",
    "published_date": "2025-03-28 15:03:52 UTC",
    "updated_date": "2025-04-20 13:01:02 UTC"
  },
  {
    "arxiv_id": "2503.22478v1",
    "title": "Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent",
    "authors": [
      "Max Hennick",
      "Stijn De Baerdemacker"
    ],
    "abstract": "We show that the behavior of stochastic gradient descent is related to\nBayesian statistics by showing that SGD is effectively diffusion on a fractal\nlandscape, where the fractal dimension can be accounted for in a purely\nBayesian way. By doing this we show that SGD can be regarded as a modified\nBayesian sampler which accounts for accessibility constraints induced by the\nfractal structure of the loss landscape. We verify our results experimentally\nby examining the diffusion of weights during training. These results offer\ninsight into the factors which determine the learning process, and seemingly\nanswer the question of how SGD and purely Bayesian sampling are related.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22478v1",
    "published_date": "2025-03-28 14:38:39 UTC",
    "updated_date": "2025-03-28 14:38:39 UTC"
  },
  {
    "arxiv_id": "2504.03713v1",
    "title": "RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
    "authors": [
      "Weichen Dai",
      "Zijie Dai",
      "Zhijie Huang",
      "Yixuan Pan",
      "Xinhe Li",
      "Xi Li",
      "Yi Zhou",
      "Ji Qi",
      "Wu Jiang"
    ],
    "abstract": "While current large language models (LLMs) demonstrate remarkable linguistic\ncapabilities through training on massive unstructured text corpora, they remain\ninadequate in leveraging structured scientific data (e.g., chemical molecular\nproperties in databases) that encapsulate centuries of accumulated scientific\nexpertise. These structured datasets hold strategic significance for advancing\nAI for Science yet current approaches merely treat them as auxiliary\nsupplements to unstructured text. This study pioneers a systematic\ninvestigation into enhancing LLMs with structured scientific data, using\nchemical molecular science as a testbed. We investigate the impact of\nincorporating molecular property data on LLM across distinct training phases,\nincluding continual pre-training, supervised fine-tuning, and reinforcement\nlearning. Notably, to address the inherent limitation of numerical\ninsensitivity in large models, we propose an innovative methodology termed\n\"Reinforcement Learning with Database Feedback\" (RLDBF). Experimental\nevaluations demonstrate the efficacy of the proposed approach, with the model\nexhibiting remarkable generalization capabilities on previously unseen data and\nother chemical tasks. The results substantiate the potential of our method in\nadvancing the field of structured scientific data processing within LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03713v1",
    "published_date": "2025-03-28 14:18:29 UTC",
    "updated_date": "2025-03-28 14:18:29 UTC"
  },
  {
    "arxiv_id": "2503.22458v1",
    "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
    "authors": [
      "Shengyue Guan",
      "Haoyi Xiong",
      "Jindong Wang",
      "Jiang Bian",
      "Bin Zhu",
      "Jian-guang Lou"
    ],
    "abstract": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22458v1",
    "published_date": "2025-03-28 14:08:40 UTC",
    "updated_date": "2025-03-28 14:08:40 UTC"
  },
  {
    "arxiv_id": "2503.22456v2",
    "title": "Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning",
    "authors": [
      "Abdullah Vanlioglu"
    ],
    "abstract": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22456v2",
    "published_date": "2025-03-28 14:07:51 UTC",
    "updated_date": "2025-03-31 10:13:48 UTC"
  },
  {
    "arxiv_id": "2503.22454v1",
    "title": "A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination",
    "authors": [
      "Ayan Majumdar",
      "Deborah D. Kanubala",
      "Kavya Gupta",
      "Isabel Valera"
    ],
    "abstract": "Fairness studies of algorithmic decision-making systems often simplify\ncomplex decision processes, such as bail or loan approvals, into binary\nclassification tasks. However, these approaches overlook that such decisions\nare not inherently binary (e.g., approve or not approve bail or loan); they\nalso involve non-binary treatment decisions (e.g., bail conditions or loan\nterms) that can influence the downstream outcomes (e.g., loan repayment or\nreoffending). In this paper, we argue that non-binary treatment decisions are\nintegral to the decision process and controlled by decision-makers and,\ntherefore, should be central to fairness analyses in algorithmic\ndecision-making. We propose a causal framework that extends fairness analyses\nand explicitly distinguishes between decision-subjects' covariates and the\ntreatment decisions. This specification allows decision-makers to use our\nframework to (i) measure treatment disparity and its downstream effects in\nhistorical data and, using counterfactual reasoning, (ii) mitigate the impact\nof past unfair treatment decisions when automating decision-making. We use our\nframework to empirically analyze four widely used loan approval datasets to\nreveal potential disparity in non-binary treatment decisions and their\ndiscriminatory impact on outcomes, highlighting the need to incorporate\ntreatment decisions in fairness assessments. Moreover, by intervening in\ntreatment decisions, we show that our framework effectively mitigates treatment\ndiscrimination from historical data to ensure fair risk score estimation and\n(non-binary) decision-making processes that benefit all stakeholders.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.22454v1",
    "published_date": "2025-03-28 14:06:35 UTC",
    "updated_date": "2025-03-28 14:06:35 UTC"
  },
  {
    "arxiv_id": "2503.22424v1",
    "title": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching",
    "authors": [
      "Zhonghao Jiang",
      "Xiaoxue Ren",
      "Meng Yan",
      "Wei Jiang",
      "Yong Li",
      "Zhongxin Liu"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22424v1",
    "published_date": "2025-03-28 13:36:26 UTC",
    "updated_date": "2025-03-28 13:36:26 UTC"
  },
  {
    "arxiv_id": "2503.22406v1",
    "title": "Training Large Language Models for Advanced Typosquatting Detection",
    "authors": [
      "Jackson Welch"
    ],
    "abstract": "Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.22406v1",
    "published_date": "2025-03-28 13:16:27 UTC",
    "updated_date": "2025-03-28 13:16:27 UTC"
  },
  {
    "arxiv_id": "2504.03712v1",
    "title": "Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing",
    "authors": [
      "Jan Lewen",
      "Max Pargmann",
      "Jenia Jitsev",
      "Mehdi Cherti",
      "Robert Pitz-Paal",
      "Daniel Maldonado Quinto"
    ],
    "abstract": "Concentrating Solar Power (CSP) plants are a key technology in the transition\ntoward sustainable energy. A critical factor for their safe and efficient\noperation is the distribution of concentrated solar flux on the receiver.\nHowever, flux distributions from individual heliostats are sensitive to surface\nimperfections. Measuring these surfaces across many heliostats remains\nimpractical in real-world deployments. As a result, control systems often\nassume idealized heliostat surfaces, leading to suboptimal performance and\npotential safety risks. To address this, inverse Deep Learning Raytracing\n(iDLR) has been introduced as a novel method for inferring heliostat surface\nprofiles from target images recorded during standard calibration procedures. In\nthis work, we present the first successful Sim-to-Real transfer of iDLR,\nenabling accurate surface predictions directly from real-world target images.\nWe evaluate our method on 63 heliostats under real operational conditions. iDLR\nsurface predictions achieve a median mean absolute error (MAE) of 0.17 mm and\nshow good agreement with deflectometry ground truth in 84% of cases. When used\nin raytracing simulations, it enables flux density predictions with a mean\naccuracy of 90% compared to deflectometry over our dataset, and outperforms the\ncommonly used ideal heliostat surface assumption by 26%. We tested this\napproach in a challenging double-extrapolation scenario-involving unseen sun\npositions and receiver projection-and found that iDLR maintains high predictive\naccuracy, highlighting its generalization capabilities. Our results demonstrate\nthat iDLR is a scalable, automated, and cost-effective solution for integrating\nrealistic heliostat surface models into digital twins. This opens the door to\nimproved flux control, more precise performance modeling, and ultimately,\nenhanced efficiency and safety in future CSP plants.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03712v1",
    "published_date": "2025-03-28 13:15:05 UTC",
    "updated_date": "2025-03-28 13:15:05 UTC"
  },
  {
    "arxiv_id": "2503.22402v1",
    "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
    "authors": [
      "Yizhang Zhu",
      "Runzhi Jiang",
      "Boyan Li",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "abstract": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "19 pages, 8 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.22402v1",
    "published_date": "2025-03-28 13:11:27 UTC",
    "updated_date": "2025-03-28 13:11:27 UTC"
  },
  {
    "arxiv_id": "2503.22396v1",
    "title": "On-site estimation of battery electrochemical parameters via transfer learning based physics-informed neural network approach",
    "authors": [
      "Josu Yeregui",
      "Iker Lopetegi",
      "Sergio Fernandez",
      "Erik Garayalde",
      "Unai Iraola"
    ],
    "abstract": "This paper presents a novel physical parameter estimation framework for\non-site model characterization, using a two-phase modelling strategy with\nPhysics-Informed Neural Networks (PINNs) and transfer learning (TL). In the\nfirst phase, a PINN is trained using only the physical principles of the single\nparticle model (SPM) equations. In the second phase, the majority of the PINN\nparameters are frozen, while critical electrochemical parameters are set as\ntrainable and adjusted using real-world voltage profile data. The proposed\napproach significantly reduces computational costs, making it suitable for\nreal-time implementation on Battery Management Systems (BMS). Additionally, as\nthe initial phase does not require field data, the model is easy to deploy with\nminimal setup requirements. With the proposed methodology, we have been able to\neffectively estimate relevant electrochemical parameters with operating data.\nThis has been proved estimating diffusivities and active material volume\nfractions with charge data in different degradation conditions. The methodology\nis experimentally validated in a Raspberry Pi device using data from a standard\ncharge profile with a 3.89\\% relative accuracy estimating the active material\nvolume fractions of a NMC cell with 82.09\\% of its nominal capacity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22396v1",
    "published_date": "2025-03-28 13:06:41 UTC",
    "updated_date": "2025-03-28 13:06:41 UTC"
  },
  {
    "arxiv_id": "2503.22394v1",
    "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision",
    "authors": [
      "Rulin Zhou",
      "Wenlong He",
      "An Wang",
      "Qiqi Yao",
      "Haijun Hu",
      "Jiankun Wang",
      "Xi Zhang an Hongliang Ren"
    ],
    "abstract": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22394v1",
    "published_date": "2025-03-28 13:00:07 UTC",
    "updated_date": "2025-03-28 13:00:07 UTC"
  },
  {
    "arxiv_id": "2503.22374v1",
    "title": "ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation",
    "authors": [
      "Giulio Federico",
      "Giuseppe Amato",
      "Fabio Carrara",
      "Claudio Gennaro",
      "Marco Di Benedetto"
    ],
    "abstract": "Understanding the nature of human sketches is challenging because of the wide\nvariation in how they are created. Recognizing complex structural patterns\nimproves both the accuracy in recognizing sketches and the fidelity of the\ngenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm\ndesigned to address these challenges through a multi-scale context extraction\napproach. The model captures intricate details at multiple scales and combines\nthem using an ensemble-like mechanism, where the extracted features work\ncollaboratively to enhance the recognition and generation of key details\ncrucial for classification and generation tasks.\n  The effectiveness of ViSketch-GPT is validated through extensive experiments\non the QuickDraw dataset. Our model establishes a new benchmark, significantly\noutperforming existing methods in both classification and generation tasks,\nwith substantial improvements in accuracy and the fidelity of generated\nsketches.\n  The proposed algorithm offers a robust framework for understanding complex\nstructures by extracting features that collaborate to recognize intricate\ndetails, enhancing the understanding of structures like sketches and making it\na versatile tool for various applications in computer vision and machine\nlearning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22374v1",
    "published_date": "2025-03-28 12:28:30 UTC",
    "updated_date": "2025-03-28 12:28:30 UTC"
  },
  {
    "arxiv_id": "2503.22363v1",
    "title": "ForcePose: A Deep Learning Approach for Force Calculation Based on Action Recognition Using MediaPipe Pose Estimation Combined with Object Detection",
    "authors": [
      "Nandakishor M",
      "Vrinda Govind V",
      "Anuradha Puthalath",
      "Anzy L",
      "Swathi P S",
      "Aswathi R",
      "Devaprabha A R",
      "Varsha Raj",
      "Midhuna Krishnan K",
      "Akhila Anilkumar T V",
      "Yamuna P V"
    ],
    "abstract": "Force estimation in human-object interactions is crucial for various fields\nlike ergonomics, physical therapy, and sports science. Traditional methods\ndepend on specialized equipment such as force plates and sensors, which makes\naccurate assessments both expensive and restricted to laboratory settings. In\nthis paper, we introduce ForcePose, a novel deep learning framework that\nestimates applied forces by combining human pose estimation with object\ndetection. Our approach leverages MediaPipe for skeletal tracking and SSD\nMobileNet for object recognition to create a unified representation of\nhuman-object interaction. We've developed a specialized neural network that\nprocesses both spatial and temporal features to predict force magnitude and\ndirection without needing any physical sensors. After training on our dataset\nof 850 annotated videos with corresponding force measurements, our model\nachieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees in\nforce direction. When compared to existing computer vision approaches, our\nmethod performs 27.5% better while still offering real-time performance on\nstandard computing hardware. ForcePose opens up new possibilities for force\nanalysis in diverse real-world scenarios where traditional measurement tools\nare impractical or intrusive. This paper discusses our methodology, the dataset\ncreation process, evaluation metrics, and potential applications across\nrehabilitation, ergonomics assessment, and athletic performance analysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22363v1",
    "published_date": "2025-03-28 12:13:56 UTC",
    "updated_date": "2025-03-28 12:13:56 UTC"
  },
  {
    "arxiv_id": "2503.22358v1",
    "title": "Shapley Revisited: Tractable Responsibility Measures for Query Answers",
    "authors": [
      "Meghyn Bienvenu",
      "Diego Figueira",
      "Pierre Lafourcade"
    ],
    "abstract": "The Shapley value, originating from cooperative game theory, has been\nemployed to define responsibility measures that quantify the contributions of\ndatabase facts to obtaining a given query answer. For non-numeric queries, this\nis done by considering a cooperative game whose players are the facts and whose\nwealth function assigns 1 or 0 to each subset of the database, depending on\nwhether the query answer holds in the given subset. While conceptually simple,\nthis approach suffers from a notable drawback: the problem of computing such\nShapley values is #P-hard in data complexity, even for simple conjunctive\nqueries. This motivates us to revisit the question of what constitutes a\nreasonable responsibility measure and to introduce a new family of\nresponsibility measures -- weighted sums of minimal supports (WSMS) -- which\nsatisfy intuitive properties. Interestingly, while the definition of WSMSs is\nsimple and bears no obvious resemblance to the Shapley value formula, we prove\nthat every WSMS measure can be equivalently seen as the Shapley value of a\nsuitably defined cooperative game. Moreover, WSMS measures enjoy tractable data\ncomplexity for a large class of queries, including all unions of conjunctive\nqueries. We further explore the combined complexity of WSMS computation and\nestablish (in)tractability results for various subclasses of conjunctive\nqueries.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Long version of PODS'25 paper",
    "pdf_url": "http://arxiv.org/pdf/2503.22358v1",
    "published_date": "2025-03-28 11:52:26 UTC",
    "updated_date": "2025-03-28 11:52:26 UTC"
  },
  {
    "arxiv_id": "2503.22353v1",
    "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions",
    "authors": [
      "Yubo Li",
      "Yidi Miao",
      "Xueying Ding",
      "Ramayya Krishnan",
      "Rema Padman"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.22353v1",
    "published_date": "2025-03-28 11:49:56 UTC",
    "updated_date": "2025-03-28 11:49:56 UTC"
  },
  {
    "arxiv_id": "2503.22342v1",
    "title": "CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models",
    "authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Yuan Xie",
      "Rongrong Ji"
    ],
    "abstract": "This paper introduces Completion Pruning Policy Optimization (CPPO) to\naccelerate the training of reasoning models based on Group Relative Policy\nOptimization (GRPO). GRPO, while effective, incurs high training costs due to\nthe need for sampling multiple completions for each question. Our experiment\nand theoretical analysis reveals that the number of completions impacts model\naccuracy yet increases training time multiplicatively, and not all completions\ncontribute equally to policy training -- their contribution depends on their\nrelative advantage. To address these issues, we propose CPPO, which prunes\ncompletions with low absolute advantages, significantly reducing the number\nneeded for gradient calculation and updates. Additionally, we introduce a\ndynamic completion allocation strategy to maximize GPU utilization by\nincorporating additional questions, further enhancing training efficiency.\nExperimental results demonstrate that CPPO achieves up to $8.32\\times$ speedup\non GSM8K and $3.51\\times$ on Math while preserving or even enhancing the\naccuracy compared to the original GRPO. We release our code at\nhttps://github.com/lzhxmu/CPPO.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.22342v1",
    "published_date": "2025-03-28 11:30:05 UTC",
    "updated_date": "2025-03-28 11:30:05 UTC"
  },
  {
    "arxiv_id": "2503.22328v2",
    "title": "VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow",
    "authors": [
      "Yancong Lin",
      "Shiming Wang",
      "Liangliang Nan",
      "Julian Kooij",
      "Holger Caesar"
    ],
    "abstract": "Scene flow estimation aims to recover per-point motion from two adjacent\nLiDAR scans. However, in real-world applications such as autonomous driving,\npoints rarely move independently of others, especially for nearby points\nbelonging to the same object, which often share the same motion. Incorporating\nthis locally rigid motion constraint has been a key challenge in\nself-supervised scene flow estimation, which is often addressed by\npost-processing or appending extra regularization. While these approaches are\nable to improve the rigidity of predicted flows, they lack an architectural\ninductive bias for local rigidity within the model structure, leading to\nsuboptimal learning efficiency and inferior performance. In contrast, we\nenforce local rigidity with a lightweight add-on module in neural network\ndesign, enabling end-to-end learning. We design a discretized voting space that\naccommodates all possible translations and then identify the one shared by\nnearby points by differentiable voting. Additionally, to ensure computational\nefficiency, we operate on pillars rather than points and learn representative\nfeatures for voting per pillar. We plug the Voting Module into popular model\ndesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. We\noutperform baseline works with only marginal compute overhead. Code is\navailable at https://github.com/tudelft-iv/VoteFlow.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Code is available at\n  https://github.com/tudelft-iv/VoteFlow. Yancong Lin and Shiming Wang have\n  equal contributions",
    "pdf_url": "http://arxiv.org/pdf/2503.22328v2",
    "published_date": "2025-03-28 11:06:27 UTC",
    "updated_date": "2025-04-16 07:36:24 UTC"
  },
  {
    "arxiv_id": "2503.22776v1",
    "title": "Post-Incorporating Code Structural Knowledge into LLMs via In-Context Learning for Code Translation",
    "authors": [
      "Yali Du",
      "Hui Sun",
      "Ming Li"
    ],
    "abstract": "Code translation migrates codebases across programming languages. Recently,\nlarge language models (LLMs) have achieved significant advancements in software\nmining. However, handling the syntactic structure of source code remains a\nchallenge. Classic syntax-aware methods depend on intricate model architectures\nand loss functions, rendering their integration into LLM training\nresource-intensive. This paper employs in-context learning (ICL), which\ndirectly integrates task exemplars into the input context, to post-incorporate\ncode structural knowledge into pre-trained LLMs. We revisit exemplar selection\nin ICL from an information-theoretic perspective, proposing that list-wise\nselection based on information coverage is more precise and general objective\nthan traditional methods based on combining similarity and diversity. To\naddress the challenges of quantifying information coverage, we introduce a\nsurrogate measure, Coverage of Abstract Syntax Tree (CAST). Furthermore, we\nformulate the NP-hard CAST maximization for exemplar selection and prove that\nit is a standard submodular maximization problem. Therefore, we propose a\ngreedy algorithm for CAST submodular maximization, which theoretically\nguarantees a (1-1/e)-approximate solution in polynomial time complexity. Our\nmethod is the first training-free and model-agnostic approach to\npost-incorporate code structural knowledge into existing LLMs at test time.\nExperimental results show that our method significantly improves LLMs\nperformance and reveals two meaningful insights: 1) Code structural knowledge\ncan be effectively post-incorporated into pre-trained LLMs during inference,\ndespite being overlooked during training; 2) Scaling up model size or training\ndata does not lead to the emergence of code structural knowledge, underscoring\nthe necessity of explicitly considering code syntactic structure.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22776v1",
    "published_date": "2025-03-28 10:59:42 UTC",
    "updated_date": "2025-03-28 10:59:42 UTC"
  },
  {
    "arxiv_id": "2503.22324v1",
    "title": "AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation",
    "authors": [
      "Chenyang Xu",
      "XingGuo Deng",
      "Rui Zhong"
    ],
    "abstract": "The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation\nand view synthesis. Although Scaffold-GS achieves higher quality real-time\nrendering compared to the original 3D-GS, its fine-grained rendering of the\nscene is extremely dependent on adequate viewing angles. The spectral bias of\nneural network learning results in Scaffold-GS's poor ability to perceive and\nlearn high-frequency information in the scene. In this work, we propose\nenhancing the manifold complexity of input features and using network-based\nfeature map loss to improve the image reconstruction quality of 3D-GS models.\nWe introduce AH-GS, which enables 3D Gaussians in structurally complex regions\nto obtain higher-frequency encodings, allowing the model to more effectively\nlearn the high-frequency information of the scene. Additionally, we incorporate\nhigh-frequency reinforce loss to further enhance the model's ability to capture\ndetailed frequency information. Our result demonstrates that our model\nsignificantly improves rendering fidelity, and in specific scenarios (e.g.,\nMipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in\njust 15K iterations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22324v1",
    "published_date": "2025-03-28 10:57:33 UTC",
    "updated_date": "2025-03-28 10:57:33 UTC"
  },
  {
    "arxiv_id": "2504.13866v1",
    "title": "Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises",
    "authors": [
      "Aleksa Marusic",
      "Sao Mai Nguyen",
      "Adriana Tapus"
    ],
    "abstract": "Physical rehabilitation exercises suggested by healthcare professionals can\nhelp recovery from various musculoskeletal disorders and prevent re-injury.\nHowever, patients' engagement tends to decrease over time without direct\nsupervision, which is why there is a need for an automated monitoring system.\nIn recent years, there has been great progress in quality assessment of\nphysical rehabilitation exercises. Most of them only provide a binary\nclassification if the performance is correct or incorrect, and a few provide a\ncontinuous score. This information is not sufficient for patients to improve\ntheir performance. In this work, we propose an algorithm for error\nclassification of rehabilitation exercises, thus making the first step toward\nmore detailed feedback to patients. We focus on skeleton-based exercise\nassessment, which utilizes human pose estimation to evaluate motion. Inspired\nby recent algorithms for quality assessment during rehabilitation exercises, we\npropose a Transformer-based model for the described classification. Our model\nis inspired by the HyperFormer method for human action recognition, and adapted\nto our problem and dataset. The evaluation is done on the KERAAL dataset, as it\nis the only medical dataset with clear error labels for the exercises, and our\nmodel significantly surpasses state-of-the-art methods. Furthermore, we bridge\nthe gap towards better feedback to the patients by presenting a way to\ncalculate the importance of joints for each exercise.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "ICORR 2025 - 19th IEEE/RAS-EMBS International Conference on\n  Rehabilitation Robotics, INTERNATIONAL CONSORTIUM FOR REHABILITATION\n  ROBOTICS, May 2025, Michigan, USA, United States",
    "pdf_url": "http://arxiv.org/pdf/2504.13866v1",
    "published_date": "2025-03-28 10:30:39 UTC",
    "updated_date": "2025-03-28 10:30:39 UTC"
  },
  {
    "arxiv_id": "2503.22276v1",
    "title": "Machine Learning Models for Soil Parameter Prediction Based on Satellite, Weather, Clay and Yield Data",
    "authors": [
      "Calvin Kammerlander",
      "Viola Kolb",
      "Marinus Luegmair",
      "Lou Scheermann",
      "Maximilian Schmailzl",
      "Marco Seufert",
      "Jiayun Zhang",
      "Denis Dalic",
      "Torsten Schön"
    ],
    "abstract": "Efficient nutrient management and precise fertilization are essential for\nadvancing modern agriculture, particularly in regions striving to optimize crop\nyields sustainably. The AgroLens project endeavors to address this challenge by\ndevelop ing Machine Learning (ML)-based methodologies to predict soil nutrient\nlevels without reliance on laboratory tests. By leveraging state of the art\ntechniques, the project lays a foundation for acionable insights to improve\nagricultural productivity in resource-constrained areas, such as Africa. The\napproach begins with the development of a robust European model using the LUCAS\nSoil dataset and Sentinel-2 satellite imagery to estimate key soil properties,\nincluding phosphorus, potassium, nitrogen, and pH levels. This model is then\nenhanced by integrating supplementary features, such as weather data, harvest\nrates, and Clay AI-generated embeddings. This report details the methodological\nframework, data preprocessing strategies, and ML pipelines employed in this\nproject. Advanced algorithms, including Random Forests, Extreme Gradient\nBoosting (XGBoost), and Fully Connected Neural Networks (FCNN), were\nimplemented and finetuned for precise nutrient prediction. Results showcase\nrobust model performance, with root mean square error values meeting stringent\naccuracy thresholds. By establishing a reproducible and scalable pipeline for\nsoil nutrient prediction, this research paves the way for transformative\nagricultural applications, including precision fertilization and improved\nresource allocation in underresourced regions like Africa.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This technical report is the documentation of a student project\n  collaboration between Technische Hochschule Ingolstadt and MI4People",
    "pdf_url": "http://arxiv.org/pdf/2503.22276v1",
    "published_date": "2025-03-28 09:44:32 UTC",
    "updated_date": "2025-03-28 09:44:32 UTC"
  },
  {
    "arxiv_id": "2503.22275v1",
    "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
    "authors": [
      "Shivam Mehta",
      "Nebojsa Jojic",
      "Hannes Gamper"
    ],
    "abstract": "Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "68T07",
      "I.2.7; I.2.6; H.5.5"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 2 figures, Accepted at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.22275v1",
    "published_date": "2025-03-28 09:43:47 UTC",
    "updated_date": "2025-03-28 09:43:47 UTC"
  },
  {
    "arxiv_id": "2503.22250v2",
    "title": "Modeling Challenging Patient Interactions: LLMs for Medical Communication Training",
    "authors": [
      "Anna Bodonhelyi",
      "Christian Stegemann-Philipps",
      "Alessandra Sonanini",
      "Lea Herschbach",
      "Marton Szep",
      "Anne Herrmann-Werner",
      "Teresa Festl-Wietek",
      "Enkelejda Kasneci",
      "Friederike Holderried"
    ],
    "abstract": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22250v2",
    "published_date": "2025-03-28 09:04:10 UTC",
    "updated_date": "2025-04-08 17:25:48 UTC"
  },
  {
    "arxiv_id": "2503.22241v2",
    "title": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs",
    "authors": [
      "Ziye Chen",
      "Yiqun Duan",
      "Riheng Zhu",
      "Zhenbang Sun",
      "Mingming Gong"
    ],
    "abstract": "Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%.",
    "categories": [
      "cs.AI",
      "68T07, 68T05, 05C82"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22241v2",
    "published_date": "2025-03-28 08:45:15 UTC",
    "updated_date": "2025-03-31 02:56:24 UTC"
  },
  {
    "arxiv_id": "2503.22235v1",
    "title": "WeatherMesh-3: Fast and accurate operational global weather forecasting",
    "authors": [
      "Haoxing Du",
      "Lyna Kim",
      "Joan Creus-Costa",
      "Jack Michaels",
      "Anuj Shetty",
      "Todd Hutchinson",
      "Christopher Riedel",
      "John Dean"
    ],
    "abstract": "We present WeatherMesh-3 (WM-3), an operational transformer-based global\nweather forecasting system that improves the state of the art in both accuracy\nand computational efficiency. We introduce the following advances: 1) a latent\nrollout that enables arbitrary-length predictions in latent space without\nintermediate encoding or decoding; and 2) a modular architecture that flexibly\nutilizes mixed-horizon processors and encodes multiple real-time analyses to\ncreate blended initial conditions. WM-3 generates 14-day global forecasts at\n0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a\n>100,000-fold speedup over traditional NWP approaches while achieving superior\naccuracy with up to 37.7% improvement in RMSE over operational models,\nrequiring only a single consumer-grade GPU for deployment. We aim for WM-3 to\ndemocratize weather forecasting by providing an accessible, lightweight model\nfor operational use while pushing the performance boundaries of machine\nlearning-based weather prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22235v1",
    "published_date": "2025-03-28 08:37:59 UTC",
    "updated_date": "2025-03-28 08:37:59 UTC"
  },
  {
    "arxiv_id": "2503.22233v1",
    "title": "Process Reward Modeling with Entropy-Driven Uncertainty",
    "authors": [
      "Lang Cao",
      "Renhong Chen",
      "Yingtian Zou",
      "Chao Peng",
      "Wu Ning",
      "Huacong Xu",
      "Qian Chen",
      "Yuxian Wang",
      "Peishuo Su",
      "Mofan Peng",
      "Zijie Chen",
      "Yitong Li"
    ],
    "abstract": "This paper presents the Entropy-Driven Unified Process Reward Model\n(EDU-PRM), a novel framework that approximates state-of-the-art performance in\nprocess supervision while drastically reducing training costs. EDU-PRM\nintroduces an entropy-guided dynamic step partitioning mechanism, using logit\ndistribution entropy to pinpoint high-uncertainty regions during token\ngeneration dynamically. This self-assessment capability enables precise\nstep-level feedback without manual fine-grained annotation, addressing a\ncritical challenge in process supervision. Experiments on the Qwen2.5-72B model\nwith only 7,500 EDU-PRM-generated training queries demonstrate accuracy closely\napproximating the full Qwen2.5-72B-PRM (71.1% vs. 71.6%), achieving a 98%\nreduction in query cost compared to prior methods. This work establishes\nEDU-PRM as an efficient approach for scalable process reward model training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22233v1",
    "published_date": "2025-03-28 08:33:37 UTC",
    "updated_date": "2025-03-28 08:33:37 UTC"
  },
  {
    "arxiv_id": "2503.22228v1",
    "title": "MFH: A Multi-faceted Heuristic Algorithm Selection Approach for Software Verification",
    "authors": [
      "Jie Su",
      "Liansai Deng",
      "Cheng Wen",
      "Rong Wang",
      "Zhi Ma",
      "Nan Zhang",
      "Cong Tian",
      "Zhenhua Duan",
      "Shengchao Qin"
    ],
    "abstract": "Currently, many verification algorithms are available to improve the\nreliability of software systems. Selecting the appropriate verification\nalgorithm typically demands domain expertise and non-trivial manpower. An\nautomated algorithm selector is thus desired. However, existing selectors,\neither depend on machine-learned strategies or manually designed heuristics,\nencounter issues such as reliance on high-quality samples with algorithm labels\nand limited scalability. In this paper, an automated algorithm selection\napproach, namely MFH, is proposed for software verification. Our approach\nleverages the heuristics that verifiers producing correct results typically\nimplement certain appropriate algorithms, and the supported algorithms by these\nverifiers indirectly reflect which ones are potentially applicable.\nSpecifically, MFH embeds the code property graph (CPG) of a semantic-preserving\ntransformed program to enhance the robustness of the prediction model.\nFurthermore, our approach decomposes the selection task into the sub-tasks of\npredicting potentially applicable algorithms and matching the most appropriate\nverifiers. Additionally, MFH also introduces a feedback loop on incorrect\npredictions to improve model prediction accuracy. We evaluate MFH on 20\nverifiers and over 15,000 verification tasks. Experimental results demonstrate\nthe effectiveness of MFH, achieving a prediction accuracy of 91.47% even\nwithout ground truth algorithm labels provided during the training phase.\nMoreover, the prediction accuracy decreases only by 0.84% when introducing 10\nnew verifiers, indicating the strong scalability of the proposed approach.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.11; D.2.4"
    ],
    "primary_category": "cs.SE",
    "comment": "The implementation, along with all relevant publicly available data,\n  can be accessed on the Figshare platform:\n  https://figshare.com/s/4f34e1f6adaf98d9be53",
    "pdf_url": "http://arxiv.org/pdf/2503.22228v1",
    "published_date": "2025-03-28 08:21:00 UTC",
    "updated_date": "2025-03-28 08:21:00 UTC"
  },
  {
    "arxiv_id": "2504.08752v1",
    "title": "Patience is all you need! An agentic system for performing scientific literature review",
    "authors": [
      "David Brett",
      "Anniek Myatt"
    ],
    "abstract": "Large language models (LLMs) have grown in their usage to provide support for\nquestion answering across numerous disciplines. The models on their own have\nalready shown promise for answering basic questions, however fail quickly where\nexpert domain knowledge is required or the question is nuanced. Scientific\nresearch often involves searching for relevant literature, distilling pertinent\ninformation from that literature and analysing how the findings support or\ncontradict one another. The information is often encapsulated in the full text\nbody of research articles, rather than just in the abstracts. Statements within\nthese articles frequently require the wider article context to be fully\nunderstood. We have built an LLM-based system that performs such search and\ndistillation of information encapsulated in scientific literature, and we\nevaluate our keyword based search and information distillation system against a\nset of biology related questions from previously released literature\nbenchmarks. We demonstrate sparse retrieval methods exhibit results close to\nstate of the art without the need for dense retrieval, with its associated\ninfrastructure and complexity overhead. We also show how to increase the\ncoverage of relevant documents for literature review generation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08752v1",
    "published_date": "2025-03-28 08:08:46 UTC",
    "updated_date": "2025-03-28 08:08:46 UTC"
  },
  {
    "arxiv_id": "2503.22215v1",
    "title": "Learning to Instruct for Visual Instruction Tuning",
    "authors": [
      "Zhihan Zhou",
      "Feng Hong",
      "Jiaan Luo",
      "Jiangchao Yao",
      "Dongsheng Li",
      "Bo Han",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "abstract": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.22215v1",
    "published_date": "2025-03-28 08:04:51 UTC",
    "updated_date": "2025-03-28 08:04:51 UTC"
  },
  {
    "arxiv_id": "2503.22182v1",
    "title": "Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items",
    "authors": [
      "Jianghao Lin",
      "Peng Du",
      "Jiaqi Liu",
      "Weite Li",
      "Yong Yu",
      "Weinan Zhang",
      "Yang Cao"
    ],
    "abstract": "E-commerce has revolutionized retail, yet its traditional workflows remain\ninefficient, with significant time and resource costs tied to product design\nand manufacturing inventory. This paper introduces a novel system deployed at\nAlibaba that leverages AI-generated items (AIGI) to address these challenges\nwith personalized text-to-image generation for e-commercial product design.\nAIGI enables an innovative business mode called \"sell it before you make it\",\nwhere merchants can design fashion items and generate photorealistic images\nwith digital models based on textual descriptions. Only when the items have\nreceived a certain number of orders, do the merchants start to produce them,\nwhich largely reduces reliance on physical prototypes and thus accelerates time\nto market. For such a promising application, we identify the underlying key\nscientific challenge, i.e., capturing the users' group-level personalized\npreferences towards multiple generated candidate images. To this end, we\npropose a Personalized Group-Level Preference Alignment Framework for Diffusion\nModels (i.e., PerFusion). We first design PerFusion Reward Model for user\npreference estimation with a feature-crossing-based personalized plug-in. Then\nwe develop PerFusion with a personalized adaptive network to model diverse\npreferences across users, and meanwhile derive the group-level preference\noptimization objective to capture the comparative behaviors among multiple\ncandidates. Both offline and online experiments demonstrate the effectiveness\nof our proposed algorithm. The AI-generated items have achieved over 13%\nrelative improvements for both click-through rate and conversion rate compared\nto their human-designed counterparts, validating the revolutionary potential of\nAI-generated items for e-commercial platforms.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.22182v1",
    "published_date": "2025-03-28 07:00:33 UTC",
    "updated_date": "2025-03-28 07:00:33 UTC"
  },
  {
    "arxiv_id": "2503.22181v1",
    "title": "e-person Architecture and Framework for Human-AI Co-adventure Relationship",
    "authors": [
      "Kanako Esaki",
      "Tadayuki Matsumura",
      "Yang Shao",
      "Hiroyuki Mizuno"
    ],
    "abstract": "This paper proposes the e-person architecture for constructing a unified and\nincremental development of AI ethics. The e-person architecture takes the\nreduction of uncertainty through collaborative cognition and action with others\nas a unified basis for ethics. By classifying and defining uncertainty along\ntwo axes - (1) first, second, and third person perspectives, and (2) the\ndifficulty of inference based on the depth of information - we support the\ndevelopment of unified and incremental development of AI ethics. In addition,\nwe propose the e-person framework based on the free energy principle, which\nconsiders the reduction of uncertainty as a unifying principle of brain\nfunction, with the aim of implementing the e-person architecture, and we show\nour previous works and future challenges based on the proposed framework.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "24 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.22181v1",
    "published_date": "2025-03-28 06:54:44 UTC",
    "updated_date": "2025-03-28 06:54:44 UTC"
  },
  {
    "arxiv_id": "2503.22178v1",
    "title": "AdaRank: Adaptive Rank Pruning for Enhanced Model Merging",
    "authors": [
      "Chanhyuk Lee",
      "Jiho Choi",
      "Chanryeol Lee",
      "Donggyun Kim",
      "Seunghoon Hong"
    ],
    "abstract": "Model merging has emerged as a promising approach for unifying independently\nfine-tuned models into an integrated framework, significantly enhancing\ncomputational efficiency in multi-task learning. Recently, several SVD-based\ntechniques have been introduced to exploit low-rank structures for enhanced\nmerging, but their reliance on such manually designed rank selection often\nleads to cross-task interference and suboptimal performance. In this paper, we\npropose AdaRank, a novel model merging framework that adaptively selects the\nmost beneficial singular directions of task vectors to merge multiple models.\nWe empirically show that the dominant singular components of task vectors can\ncause critical interference with other tasks, and that naive truncation across\ntasks and layers degrades performance. In contrast, AdaRank dynamically prunes\nthe singular components that cause interference and offers an optimal amount of\ninformation to each task vector by learning to prune ranks during test-time via\nentropy minimization. Our analysis demonstrates that such method mitigates\ndetrimental overlaps among tasks, while empirical results show that AdaRank\nconsistently achieves state-of-the-art performance with various backbones and\nnumber of tasks, reducing the performance gap between fine-tuned models to\nnearly 1%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Code Available at: https://github.com/david3684/AdaRank",
    "pdf_url": "http://arxiv.org/pdf/2503.22178v1",
    "published_date": "2025-03-28 06:49:06 UTC",
    "updated_date": "2025-03-28 06:49:06 UTC"
  },
  {
    "arxiv_id": "2503.22164v2",
    "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
    "authors": [
      "Bowen Gao",
      "Yanwen Huang",
      "Yiqiao Liu",
      "Wenxuan Xie",
      "Wei-Ying Ma",
      "Ya-Qin Zhang",
      "Yanyan Lan"
    ],
    "abstract": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22164v2",
    "published_date": "2025-03-28 06:02:53 UTC",
    "updated_date": "2025-03-31 16:26:42 UTC"
  },
  {
    "arxiv_id": "2503.22152v1",
    "title": "EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos",
    "authors": [
      "Yuxuan Li",
      "Vijay Veerabadran",
      "Michael L. Iuzzolino",
      "Brett D. Roads",
      "Asli Celikyilmaz",
      "Karl Ridgeway"
    ],
    "abstract": "We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22152v1",
    "published_date": "2025-03-28 05:10:59 UTC",
    "updated_date": "2025-03-28 05:10:59 UTC"
  },
  {
    "arxiv_id": "2503.22151v1",
    "title": "When Autonomy Breaks: The Hidden Existential Risk of AI",
    "authors": [
      "Joshua Krook"
    ],
    "abstract": "AI risks are typically framed around physical threats to humanity, a loss of\ncontrol or an accidental error causing humanity's extinction. However, I argue\nin line with the gradual disempowerment thesis, that there is an\nunderappreciated risk in the slow and irrevocable decline of human autonomy. As\nAI starts to outcompete humans in various areas of life, a tipping point will\nbe reached where it no longer makes sense to rely on human decision-making,\ncreativity, social care or even leadership.\n  What may follow is a process of gradual de-skilling, where we lose skills\nthat we currently take for granted. Traditionally, it is argued that AI will\ngain human skills over time, and that these skills are innate and immutable in\nhumans. By contrast, I argue that humans may lose such skills as critical\nthinking, decision-making and even social care in an AGI world. The biggest\nthreat to humanity is therefore not that machines will become more like humans,\nbut that humans will become more like machines.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22151v1",
    "published_date": "2025-03-28 05:10:32 UTC",
    "updated_date": "2025-03-28 05:10:32 UTC"
  },
  {
    "arxiv_id": "2503.22771v1",
    "title": "GroundHog: Revolutionizing GLDAS Groundwater Storage Downscaling for Enhanced Recharge Estimation in Bangladesh",
    "authors": [
      "Saleh Sakib Ahmed",
      "Rashed Uz Zzaman",
      "Saifur Rahman Jony",
      "Faizur Rahman Himel",
      "Afroza Sharmin",
      "A. H. M. Khalequr Rahman",
      "M. Sohel Rahman",
      "Sara Nowreen"
    ],
    "abstract": "Long-term groundwater level (GWL) measurement is vital for effective\npolicymaking and recharge estimation using annual maxima and minima. However,\ncurrent methods prioritize short-term predictions and lack multi-year\napplicability, limiting their utility. Moreover, sparse in-situ measurements\nlead to reliance on low-resolution satellite data like GLDAS as the ground\ntruth for Machine Learning models, further constraining accuracy. To overcome\nthese challenges, we first develop an ML model to mitigate data gaps, achieving\n$R^2$ scores of 0.855 and 0.963 for maximum and minimum GWL predictions,\nrespectively. Subsequently, using these predictions and well observations as\nground truth, we train an Upsampling Model that uses low-resolution (25 km)\nGLDAS data as input to produce high-resolution (2 km) GWLs, achieving an\nexcellent $R^2$ score of 0.96. Our approach successfully upscales GLDAS data\nfor 2003-2024, allowing high-resolution recharge estimations and revealing\ncritical trends for proactive resource management. Our method allows upsampling\nof groundwater storage (GWS) from GLDAS to high-resolution GWLs for any points\nindependently of officially curated piezometer data, making it a valuable tool\nfor decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22771v1",
    "published_date": "2025-03-28 04:56:01 UTC",
    "updated_date": "2025-03-28 04:56:01 UTC"
  },
  {
    "arxiv_id": "2503.22144v1",
    "title": "FRASE: Structured Representations for Generalizable SPARQL Query Generation",
    "authors": [
      "Papa Abdou Karim Karou Diallo",
      "Amal Zouaq"
    ],
    "abstract": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22144v1",
    "published_date": "2025-03-28 04:39:52 UTC",
    "updated_date": "2025-03-28 04:39:52 UTC"
  },
  {
    "arxiv_id": "2503.22143v1",
    "title": "A Self-Supervised Learning of a Foundation Model for Analog Layout Design Automation",
    "authors": [
      "Sungyu Jeong",
      "Won Joon Choi",
      "Junung Choi",
      "Anik Biswas",
      "Byungsub Kim"
    ],
    "abstract": "We propose a UNet-based foundation model and its self-supervised learning\nmethod to address two key challenges: 1) lack of qualified annotated analog\nlayout data, and 2) excessive variety in analog layout design tasks. For\nself-supervised learning, we propose random patch sampling and random masking\ntechniques automatically to obtain enough training data from a small\nunannotated layout dataset. The obtained data are greatly augmented, less\nbiased, equally sized, and contain enough information for excessive varieties\nof qualified layout patterns. By pre-training with the obtained data, the\nproposed foundation model can learn implicit general knowledge on layout\npatterns so that it can be fine-tuned for various downstream layout tasks with\nsmall task-specific datasets. Fine-tuning provides an efficient and\nconsolidated methodology for diverse downstream tasks, reducing the enormous\nhuman effort to develop a model per task separately. In experiments, the\nfoundation model was pre-trained using 324,000 samples obtained from 6\nsilicon-proved manually designed analog circuits, then it was fine-tuned for\nthe five example downstream tasks: generating contacts, vias, dummy fingers,\nN-wells, and metal routings. The fine-tuned models successfully performed these\ntasks for more than one thousand unseen layout inputs, generating DRC/LVS-clean\nlayouts for 96.6% of samples. Compared with training the model from scratch for\nthe metal routing task, fine-tuning required only 1/8 of the data to achieve\nthe same dice score of 0.95. With the same data, fine-tuning achieved a 90%\nlower validation loss and a 40% higher benchmark score than training from\nscratch.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "8 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.22143v1",
    "published_date": "2025-03-28 04:37:33 UTC",
    "updated_date": "2025-03-28 04:37:33 UTC"
  },
  {
    "arxiv_id": "2503.22141v1",
    "title": "Integrating Artificial Intelligence with Human Expertise: An In-depth Analysis of ChatGPT's Capabilities in Generating Metamorphic Relations",
    "authors": [
      "Yifan Zhang",
      "Dave Towey",
      "Matthew Pike",
      "Quang-Hung Luu",
      "Huai Liu",
      "Tsong Yueh Chen"
    ],
    "abstract": "Context: This paper provides an in-depth examination of the generation and\nevaluation of Metamorphic Relations (MRs) using GPT models developed by OpenAI,\nwith a particular focus on the capabilities of GPT-4 in software testing\nenvironments.\n  Objective: The aim is to examine the quality of MRs produced by GPT-3.5 and\nGPT-4 for a specific System Under Test (SUT) adopted from an earlier study, and\nto introduce and apply an improved set of evaluation criteria for a diverse\nrange of SUTs.\n  Method: The initial phase evaluates MRs generated by GPT-3.5 and GPT-4 using\ncriteria from a prior study, followed by an application of an enhanced\nevaluation framework on MRs created by GPT-4 for a diverse range of nine SUTs,\nvarying from simple programs to complex systems incorporating AI/ML components.\nA custom-built GPT evaluator, alongside human evaluators, assessed the MRs,\nenabling a direct comparison between automated and human evaluation methods.\n  Results: The study finds that GPT-4 outperforms GPT-3.5 in generating\naccurate and useful MRs. With the advanced evaluation criteria, GPT-4\ndemonstrates a significant ability to produce high-quality MRs across a wide\nrange of SUTs, including complex systems incorporating AI/ML components.\n  Conclusions: GPT-4 exhibits advanced capabilities in generating MRs suitable\nfor various applications. The research underscores the growing potential of AI\nin software testing, particularly in the generation and evaluation of MRs, and\npoints towards the complementarity of human and AI skills in this domain.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to Information and Software Technology",
    "pdf_url": "http://arxiv.org/pdf/2503.22141v1",
    "published_date": "2025-03-28 04:31:32 UTC",
    "updated_date": "2025-03-28 04:31:32 UTC"
  },
  {
    "arxiv_id": "2503.22137v1",
    "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
    "authors": [
      "Syrine Belakaria",
      "Joshua Kazdan",
      "Charles Marx",
      "Chris Cundy",
      "Willie Neiswanger",
      "Sanmi Koyejo",
      "Barbara E. Engelhardt",
      "Stefano Ermon"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22137v1",
    "published_date": "2025-03-28 04:22:53 UTC",
    "updated_date": "2025-03-28 04:22:53 UTC"
  },
  {
    "arxiv_id": "2503.22769v1",
    "title": "MediTools -- Medical Education Powered by LLMs",
    "authors": [
      "Amr Alshatnawi",
      "Remi Sampaleanu",
      "David Liebovitz"
    ],
    "abstract": "Artificial Intelligence (AI) has been advancing rapidly and with the advent\nof large language models (LLMs) in late 2022, numerous opportunities have\nemerged for adopting this technology across various domains, including\nmedicine. These innovations hold immense potential to revolutionize and\nmodernize medical education. Our research project leverages large language\nmodels to enhance medical education and address workflow challenges through the\ndevelopment of MediTools - AI Medical Education. This prototype application\nfocuses on developing interactive tools that simulate real-life clinical\nscenarios, provide access to medical literature, and keep users updated with\nthe latest medical news. Our first tool is a dermatology case simulation tool\nthat uses real patient images depicting various dermatological conditions and\nenables interaction with LLMs acting as virtual patients. This platform allows\nusers to practice their diagnostic skills and enhance their clinical\ndecision-making abilities. The application also features two additional tools:\nan AI-enhanced PubMed tool for engaging with LLMs to gain deeper insights into\nresearch papers, and a Google News tool that offers LLM generated summaries of\narticles for various medical specialties. A comprehensive survey has been\nconducted among medical professionals and students to gather initial feedback\non the effectiveness and user satisfaction of MediTools, providing insights for\nfurther development and refinement of the application. This research\ndemonstrates the potential of AI-driven tools in transforming and\nrevolutionizing medical education, offering a scalable and interactive platform\nfor continuous learning and skill development.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages, 17 figures, 2 tables. Code available at\n  https://github.com/NM-Streamlit-Team/meditools",
    "pdf_url": "http://arxiv.org/pdf/2503.22769v1",
    "published_date": "2025-03-28 03:57:32 UTC",
    "updated_date": "2025-03-28 03:57:32 UTC"
  },
  {
    "arxiv_id": "2503.22122v1",
    "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation",
    "authors": [
      "Puzhen Yuan",
      "Angyuan Ma",
      "Yunchao Yao",
      "Huaxiu Yao",
      "Masayoshi Tomizuka",
      "Mingyu Ding"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22122v1",
    "published_date": "2025-03-28 03:51:40 UTC",
    "updated_date": "2025-03-28 03:51:40 UTC"
  },
  {
    "arxiv_id": "2503.22115v1",
    "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories",
    "authors": [
      "Yazhou Zhang",
      "Qimeng Liu",
      "Qiuchi Li",
      "Peng Zhang",
      "Jing Qin"
    ],
    "abstract": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22115v1",
    "published_date": "2025-03-28 03:31:37 UTC",
    "updated_date": "2025-03-28 03:31:37 UTC"
  },
  {
    "arxiv_id": "2503.22093v2",
    "title": "How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark",
    "authors": [
      "Ximing Wen",
      "Mallika Mainali",
      "Anik Sen"
    ],
    "abstract": "Vision Language Models (VLMs) have demonstrated strong reasoning capabilities\nin Visual Question Answering (VQA) tasks; however, their ability to perform\nTheory of Mind (ToM) tasks, such as inferring human intentions, beliefs, and\nmental states, remains underexplored. We propose an open-ended question\nframework to evaluate VLMs' performance across diverse categories of ToM tasks.\nWe curated and annotated a benchmark dataset of 30 images and evaluated the\nperformance of four VLMs of varying sizes. Our results show that the GPT-4\nmodel outperformed all the others, with only one smaller model, GPT-4o-mini,\nachieving comparable performance. We observed that VLMs often struggle to infer\nintentions in complex scenarios such as bullying or cheating. Our findings\nreveal that smaller models can sometimes infer correct intentions despite\nrelying on incorrect visual cues. The dataset is available at\nhttps://github.com/ximingwen/ToM-AAAI25-Multimodal.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, accepted by ToM@AAAI25",
    "pdf_url": "http://arxiv.org/pdf/2503.22093v2",
    "published_date": "2025-03-28 02:26:32 UTC",
    "updated_date": "2025-04-24 03:20:58 UTC"
  },
  {
    "arxiv_id": "2504.01979v1",
    "title": "Correlation-Attention Masked Temporal Transformer for User Identity Linkage Using Heterogeneous Mobility Data",
    "authors": [
      "Ziang Yan",
      "Xingyu Zhao",
      "Hanqing Ma",
      "Wei Chen",
      "Jianpeng Qi",
      "Yanwei Yu",
      "Junyu Dong"
    ],
    "abstract": "With the rise of social media and Location-Based Social Networks (LBSN),\ncheck-in data across platforms has become crucial for User Identity Linkage\n(UIL). These data not only reveal users' spatio-temporal information but also\nprovide insights into their behavior patterns and interests. However,\ncross-platform identity linkage faces challenges like poor data quality, high\nsparsity, and noise interference, which hinder existing methods from extracting\ncross-platform user information. To address these issues, we propose a\nCorrelation-Attention Masked Transformer for User Identity Linkage Network\n(MT-Link), a transformer-based framework to enhance model performance by\nlearning spatio-temporal co-occurrence patterns of cross-platform users. Our\nmodel effectively captures spatio-temporal co-occurrence in cross-platform user\ncheck-in sequences. It employs a correlation attention mechanism to detect the\nspatio-temporal co-occurrence between user check-in sequences. Guided by\nattention weight maps, the model focuses on co-occurrence points while\nfiltering out noise, ultimately improving classification performance.\nExperimental results show that our model significantly outperforms\nstate-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in\nterms of Macro-F1 and Area Under Curve (AUC).",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "9 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01979v1",
    "published_date": "2025-03-28 02:18:16 UTC",
    "updated_date": "2025-03-28 02:18:16 UTC"
  },
  {
    "arxiv_id": "2504.00020v1",
    "title": "Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation",
    "authors": [
      "Huan Zhao",
      "Yiming Liu",
      "Jina Yao",
      "Ling Xiong",
      "Zexin Zhou",
      "Zixing Zhang"
    ],
    "abstract": "Recent breakthroughs in single-cell technology have ushered in unparalleled\nopportunities to decode the molecular intricacy of intricate biological\nsystems, especially those linked to diseases unique to humans. However, these\nprogressions have also ushered in novel obstacles-specifically, the efficient\nannotation of extensive, long-tailed single-cell data pertaining to disease\nconditions. To effectively surmount this challenge, we introduce Celler, a\nstate-of-the-art generative pre-training model crafted specifically for the\nannotation of single-cell data. Celler incorporates two groundbreaking\nelements: First, we introduced the Gaussian Inflation (GInf) Loss function. By\ndynamically adjusting sample weights, GInf Loss significantly enhances the\nmodel's ability to learn from rare categories while reducing the risk of\noverfitting for common categories. Secondly, we introduce an innovative Hard\nData Mining (HDM) strategy into the training process, specifically targeting\nthe challenging-to-learn minority data samples, which significantly improved\nthe model's predictive accuracy. Additionally, to further advance research in\nthis field, we have constructed a large-scale single-cell dataset: Celler-75,\nwhich encompasses 40 million cells distributed across 80 human tissues and 75\nspecific diseases. This dataset provides critical support for comprehensively\nexploring the potential of single-cell technology in disease research. Our code\nis available at https://github.com/AI4science-ym/HiCeller.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00020v1",
    "published_date": "2025-03-28 02:04:26 UTC",
    "updated_date": "2025-03-28 02:04:26 UTC"
  },
  {
    "arxiv_id": "2503.22074v1",
    "title": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation",
    "authors": [
      "Chuan-Wei Kuo",
      "Siyu Chen",
      "Chenqi Yan",
      "Yu Yang Fredrik Liu"
    ],
    "abstract": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22074v1",
    "published_date": "2025-03-28 01:33:05 UTC",
    "updated_date": "2025-03-28 01:33:05 UTC"
  },
  {
    "arxiv_id": "2503.22069v1",
    "title": "Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning",
    "authors": [
      "Ekansh Chauhan",
      "Anila Sharma",
      "Amit Sharma",
      "Vikas Nishadham",
      "Asha Ghughtyal",
      "Ankur Kumar",
      "Gurudutt Gupta",
      "Anurag Mehta",
      "C. V. Jawahar",
      "P. K. Vinod"
    ],
    "abstract": "Breast cancer, the most common malignancy among women, requires precise\ndetection and classification for effective treatment. Immunohistochemistry\n(IHC) biomarkers like HER2, ER, and PR are critical for identifying breast\ncancer subtypes. However, traditional IHC classification relies on\npathologists' expertise, making it labor-intensive and subject to significant\ninter-observer variability. To address these challenges, this study introduces\nthe India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHC\nslides (HER2, ER, and PR) aimed at automating receptor status classification.\nThe primary focus is on developing predictive models for HER2 3-way\nclassification (0, Low, High) to enhance prognosis. Evaluation of multiple deep\nlearning models revealed that an end-to-end ConvNeXt network utilizing\nlow-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%,\nand 83.56%, respectively, for 3-way classification, outperforming patch-based\nmethods by over 5.35% in F1 score. This study highlights the potential of\nsimple yet effective deep learning techniques to significantly improve accuracy\nand reproducibility in breast cancer classification, supporting their\nintegration into clinical workflows for better patient outcomes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22069v1",
    "published_date": "2025-03-28 01:24:08 UTC",
    "updated_date": "2025-03-28 01:24:08 UTC"
  },
  {
    "arxiv_id": "2503.22068v1",
    "title": "A Proposal for Networks Capable of Continual Learning",
    "authors": [
      "Zeki Doruk Erden",
      "Boi Faltings"
    ],
    "abstract": "We analyze the ability of computational units to retain past responses after\nparameter updates, a key property for system-wide continual learning. Neural\nnetworks trained with gradient descent lack this capability, prompting us to\npropose Modelleyen, an alternative approach with inherent response\npreservation. We demonstrate through experiments on modeling the dynamics of a\nsimple environment and on MNIST that, despite increased computational\ncomplexity and some representational limitations at its current stage,\nModelleyen achieves continual learning without relying on sample replay or\npredefined task boundaries.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2025 World Models Workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.22068v1",
    "published_date": "2025-03-28 01:23:18 UTC",
    "updated_date": "2025-03-28 01:23:18 UTC"
  },
  {
    "arxiv_id": "2504.01025v1",
    "title": "Diagnosis of Pulmonary Hypertension by Integrating Multimodal Data with a Hybrid Graph Convolutional and Transformer Network",
    "authors": [
      "Fubao Zhu",
      "Yang Zhang",
      "Gengmin Liang",
      "Jiaofen Nan",
      "Yanting Li",
      "Chuang Han",
      "Danyang Sun",
      "Zhiguo Wang",
      "Chen Zhao",
      "Wenxuan Zhou",
      "Jian He",
      "Yi Xu",
      "Iokfai Cheang",
      "Xu Zhu",
      "Yanli Zhou",
      "Weihua Zhou"
    ],
    "abstract": "Early and accurate diagnosis of pulmonary hypertension (PH) is essential for\noptimal patient management. Differentiating between pre-capillary and\npost-capillary PH is critical for guiding treatment decisions. This study\ndevelops and validates a deep learning-based diagnostic model for PH, designed\nto classify patients as non-PH, pre-capillary PH, or post-capillary PH. This\nretrospective study analyzed data from 204 patients (112 with pre-capillary PH,\n32 with post-capillary PH, and 60 non-PH controls) at the First Affiliated\nHospital of Nanjing Medical University. Diagnoses were confirmed through right\nheart catheterization. We selected 6 samples from each category for the test\nset (18 samples, 10%), with the remaining 186 samples used for the training\nset. This process was repeated 35 times for testing. This paper proposes a deep\nlearning model that combines Graph convolutional networks (GCN), Convolutional\nneural networks (CNN), and Transformers. The model was developed to process\nmultimodal data, including short-axis (SAX) sequences, four-chamber (4CH)\nsequences, and clinical parameters. Our model achieved a performance of Area\nunder the receiver operating characteristic curve (AUC) = 0.81 +- 0.06(standard\ndeviation) and Accuracy (ACC) = 0.73 +- 0.06 on the test set. The\ndiscriminative abilities were as follows: non-PH subjects (AUC = 0.74 +- 0.11),\npre-capillary PH (AUC = 0.86 +- 0.06), and post-capillary PH (AUC = 0.83 +-\n0.10). It has the potential to support clinical decision-making by effectively\nintegrating multimodal data to assist physicians in making accurate and timely\ndiagnoses.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "23 pages, 8 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01025v1",
    "published_date": "2025-03-28 01:14:17 UTC",
    "updated_date": "2025-03-28 01:14:17 UTC"
  },
  {
    "arxiv_id": "2503.22064v1",
    "title": "Multi-Task Semantic Communications via Large Models",
    "authors": [
      "Wanli Ni",
      "Zhijin Qin",
      "Haofeng Sun",
      "Xiaoming Tao",
      "Zhu Han"
    ],
    "abstract": "Artificial intelligence (AI) promises to revolutionize the design,\noptimization and management of next-generation communication systems. In this\narticle, we explore the integration of large AI models (LAMs) into semantic\ncommunications (SemCom) by leveraging their multi-modal data processing and\ngeneration capabilities. Although LAMs bring unprecedented abilities to extract\nsemantics from raw data, this integration entails multifaceted challenges\nincluding high resource demands, model complexity, and the need for\nadaptability across diverse modalities and tasks. To overcome these challenges,\nwe propose a LAM-based multi-task SemCom (MTSC) architecture, which includes an\nadaptive model compression strategy and a federated split fine-tuning approach\nto facilitate the efficient deployment of LAM-based semantic models in\nresource-limited networks. Furthermore, a retrieval-augmented generation scheme\nis implemented to synthesize the most recent local and global knowledge bases\nto enhance the accuracy of semantic extraction and content generation, thereby\nimproving the inference performance. Finally, simulation results demonstrate\nthe efficacy of the proposed LAM-based MTSC architecture, highlighting the\nperformance enhancements across various downstream tasks under varying channel\nconditions.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.22064v1",
    "published_date": "2025-03-28 00:57:34 UTC",
    "updated_date": "2025-03-28 00:57:34 UTC"
  },
  {
    "arxiv_id": "2503.22051v1",
    "title": "Non-Monotonic Attention-based Read/Write Policy Learning for Simultaneous Translation",
    "authors": [
      "Zeeshan Ahmed",
      "Frank Seide",
      "Zhe Liu",
      "Rastislav Rabatin",
      "Jachym Kolar",
      "Niko Moritz",
      "Ruiming Xie",
      "Simone Merello",
      "Christian Fuegen"
    ],
    "abstract": "Simultaneous or streaming machine translation generates translation while\nreading the input stream. These systems face a quality/latency trade-off,\naiming to achieve high translation quality similar to non-streaming models with\nminimal latency. We propose an approach that efficiently manages this\ntrade-off. By enhancing a pretrained non-streaming model, which was trained\nwith a seq2seq mechanism and represents the upper bound in quality, we convert\nit into a streaming model by utilizing the alignment between source and target\ntokens. This alignment is used to learn a read/write decision boundary for\nreliable translation generation with minimal input. During training, the model\nlearns the decision boundary through a read/write policy module, employing\nsupervised learning on the alignment points (pseudo labels). The read/write\npolicy module, a small binary classification unit, can control the\nquality/latency trade-off during inference. Experimental results show that our\nmodel outperforms several strong baselines and narrows the gap with the\nnon-streaming baseline model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22051v1",
    "published_date": "2025-03-28 00:00:33 UTC",
    "updated_date": "2025-03-28 00:00:33 UTC"
  }
]
[
  {
    "arxiv_id": "2510.27055v1",
    "title": "Detecting Data Contamination in LLMs via In-Context Learning",
    "authors": [
      "Michał Zawalski",
      "Meriem Boubdir",
      "Klaudia Bałazy",
      "Besmira Nushi",
      "Pablo Ribalta"
    ],
    "abstract": "We present Contamination Detection via Context (CoDeC), a practical and accurate method to detect and quantify training data contamination in large language models. CoDeC distinguishes between data memorized during training and data outside the training distribution by measuring how in-context learning affects model performance. We find that in-context examples typically boost confidence for unseen datasets but may reduce it when the dataset was part of training, due to disrupted memorization patterns. Experiments show that CoDeC produces interpretable contamination scores that clearly separate seen and unseen datasets, and reveals strong evidence of memorization in open-weight models with undisclosed training corpora. The method is simple, automated, and both model- and dataset-agnostic, making it easy to integrate with benchmark evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.27055v1",
    "published_date": "2025-10-30 23:50:05 UTC",
    "updated_date": "2025-10-30 23:50:05 UTC"
  },
  {
    "arxiv_id": "2510.27051v1",
    "title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement",
    "authors": [
      "Aaditya Shukla",
      "Sidney Knowles",
      "Meenakshi Madugula",
      "Dave Farris",
      "Ryan Angilly",
      "Santiago Pombo",
      "Anbang Xu",
      "Lu An",
      "Abhinav Balasubramanian",
      "Tan Yu",
      "Jiaxiang Ren",
      "Rama Akkiraju"
    ],
    "abstract": "Enterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major failure modes: routing errors (5.25\\%) and query rephrasal errors (3.2\\%). Using NVIDIA NeMo microservices, we implemented targeted improvements through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a fine-tuned 8B variant, achieving 96\\% accuracy, a 10x reduction in model size, and 70\\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\\% gain in accuracy and a 40\\% latency reduction. Our approach demonstrates how human-in-the-loop (HITL) feedback, when structured within a data flywheel, transforms enterprise AI agents into self-improving systems. Key learnings include approaches to ensure agent robustness despite limited user feedback, navigating privacy constraints, and executing staged rollouts in production. This work offers a repeatable blueprint for building robust, adaptive enterprise AI agents capable of learning from real-world usage at scale.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 5 figures, 5 tables. Presents MAPE-K control loop application to enterprise AI agent improvement with experimental validation on NVIDIA's NVInfo AI system",
    "pdf_url": "https://arxiv.org/pdf/2510.27051v1",
    "published_date": "2025-10-30 23:41:06 UTC",
    "updated_date": "2025-10-30 23:41:06 UTC"
  },
  {
    "arxiv_id": "2511.20660v1",
    "title": "Transforming Higher Education with AI-Powered Video Lectures",
    "authors": [
      "Dengsheng Zhang"
    ],
    "abstract": "The integration of artificial intelligence (AI) into video lecture production has the potential to transform higher education by streamlining content creation and enhancing accessibility. This paper investigates a semi automated workflow that combines Google Gemini for script generation, Amazon Polly for voice synthesis, and Microsoft PowerPoint for video assembly. Unlike fully automated text to video platforms, this hybrid approach preserves pedagogical intent while ensuring script to slide synchronization, narrative coherence, and customization. Case studies demonstrate the effectiveness of Gemini in generating accurate and context-sensitive scripts for visually rich academic presentations, while Polly provides natural-sounding narration with controllable pacing. A two course pilot study was conducted to evaluate AI generated instructional videos (AIIV) against human instructional videos (HIV). Both qualitative and quantitative results indicate that AIIVs are comparable to HIVs in terms of learning outcomes, with students reporting high levels of clarity, coherence, and usability. However, limitations remain, particularly regarding audio quality and the absence of human-like avatars. The findings suggest that AI assisted video production can reduce instructor workload, improve scalability, and deliver effective learning resources, while future improvements in synthetic voices and avatars may further enhance learner engagement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "27 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.20660v1",
    "published_date": "2025-10-30 23:33:10 UTC",
    "updated_date": "2025-10-30 23:33:10 UTC"
  },
  {
    "arxiv_id": "2510.27042v2",
    "title": "e1: Learning Adaptive Control of Reasoning Effort",
    "authors": [
      "Michael Kleinman",
      "Matthew Trager",
      "Alessandro Achille",
      "Wei Xia",
      "Stefano Soatto"
    ],
    "abstract": "Increasing the thinking budget of AI models can significantly improve accuracy, but not all questions warrant the same amount of reasoning. Users may prefer to allocate different amounts of reasoning effort depending on how they value output quality versus latency and cost. To leverage this tradeoff effectively, users need fine-grained control over the amount of thinking used for a particular query, but few approaches enable such control. Existing methods require users to specify the absolute number of desired tokens, but this requires knowing the difficulty of the problem beforehand to appropriately set the token budget for a query. To address these issues, we propose Adaptive Effort Control, a self-adaptive reinforcement learning method that trains models to use a user-specified fraction of tokens relative to the current average chain-of-thought length for each query. This approach eliminates dataset- and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. We observe that the model automatically learns to allocate resources proportionally to the task difficulty and, across model scales ranging from 1.5B to 32B parameters, our approach enables a 2-3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.27042v2",
    "published_date": "2025-10-30 23:12:21 UTC",
    "updated_date": "2025-11-12 04:15:18 UTC"
  },
  {
    "arxiv_id": "2510.27038v1",
    "title": "Dataset Creation and Baseline Models for Sexism Detection in Hausa",
    "authors": [
      "Fatima Adam Muhammad",
      "Shamsuddeen Muhammad Hassan",
      "Isa Inuwa-Dutse"
    ],
    "abstract": "Sexism reinforces gender inequality and social exclusion by perpetuating stereotypes, bias, and discriminatory norms. Noting how online platforms enable various forms of sexism to thrive, there is a growing need for effective sexism detection and mitigation strategies. While computational approaches to sexism detection are widespread in high-resource languages, progress remains limited in low-resource languages where limited linguistic resources and cultural differences affect how sexism is expressed and perceived. This study introduces the first Hausa sexism detection dataset, developed through community engagement, qualitative coding, and data augmentation. For cultural nuances and linguistic representation, we conducted a two-stage user study (n=66) involving native speakers to explore how sexism is defined and articulated in everyday discourse. We further experiment with both traditional machine learning classifiers and pre-trained multilingual language models and evaluating the effectiveness few-shot learning in detecting sexism in Hausa. Our findings highlight challenges in capturing cultural nuance, particularly with clarification-seeking and idiomatic expressions, and reveal a tendency for many false positives in such cases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 figure, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.27038v1",
    "published_date": "2025-10-30 22:57:35 UTC",
    "updated_date": "2025-10-30 22:57:35 UTC"
  },
  {
    "arxiv_id": "2510.27037v1",
    "title": "Elastic Architecture Search for Efficient Language Models",
    "authors": [
      "Shang Wang"
    ],
    "abstract": "As large pre-trained language models become increasingly critical to natural language understanding (NLU) tasks, their substantial computational and memory requirements have raised significant economic and environmental concerns. Addressing these challenges, this paper introduces the Elastic Language Model (ELM), a novel neural architecture search (NAS) method optimized for compact language models. ELM extends existing NAS approaches by introducing a flexible search space with efficient transformer blocks and dynamic modules for dimension and head number adjustment. These innovations enhance the efficiency and flexibility of the search process, which facilitates more thorough and effective exploration of model architectures. We also introduce novel knowledge distillation losses that preserve the unique characteristics of each block, in order to improve the discrimination between architectural choices during the search process. Experiments on masked language modeling and causal language modeling tasks demonstrate that models discovered by ELM significantly outperform existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "comment": "ICME 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.27037v1",
    "published_date": "2025-10-30 22:57:30 UTC",
    "updated_date": "2025-10-30 22:57:30 UTC"
  },
  {
    "arxiv_id": "2511.00112v1",
    "title": "Real-DRL: Teach and Learn in Reality",
    "authors": [
      "Yanbing Mao",
      "Yihao Cai",
      "Lui Sha"
    ],
    "abstract": "This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "37 pages",
    "pdf_url": "https://arxiv.org/pdf/2511.00112v1",
    "published_date": "2025-10-30 22:51:28 UTC",
    "updated_date": "2025-10-30 22:51:28 UTC"
  },
  {
    "arxiv_id": "2510.27033v1",
    "title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics",
    "authors": [
      "Simindokht Jahangard",
      "Mehrzad Mohammadi",
      "Abhinav Dhall",
      "Hamid Rezatofighi"
    ],
    "abstract": "Visual reasoning, particularly spatial reasoning, is a challenging cognitive task that requires understanding object relationships and their interactions within complex environments, especially in robotics domain. Existing vision_language models (VLMs) excel at perception tasks but struggle with fine-grained spatial reasoning due to their implicit, correlation-driven reasoning and reliance solely on images. We propose a novel neuro_symbolic framework that integrates both panoramic-image and 3D point cloud information, combining neural perception with symbolic reasoning to explicitly model spatial and logical relationships. Our framework consists of a perception module for detecting entities and extracting attributes, and a reasoning module that constructs a structured scene graph to support precise, interpretable queries. Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superior performance and reliability in crowded, human_built environments while maintaining a lightweight design suitable for robotics and embodied AI applications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.27033v1",
    "published_date": "2025-10-30 22:40:23 UTC",
    "updated_date": "2025-10-30 22:40:23 UTC"
  },
  {
    "arxiv_id": "2511.04694v3",
    "title": "Reasoning Up the Instruction Ladder for Controllable Language Models",
    "authors": [
      "Zishuo Zheng",
      "Vidhisha Balachandran",
      "Chan Young Park",
      "Faeze Brahman",
      "Sachin Kumar"
    ],
    "abstract": "As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first \"think\" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises ~7K aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks, achieving roughly a 20% improvement on the IHEval conflict setup. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks, providing up to a 20% reduction in attack success rate (ASR). These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.04694v3",
    "published_date": "2025-10-30 22:13:31 UTC",
    "updated_date": "2025-12-01 21:07:46 UTC"
  },
  {
    "arxiv_id": "2511.00110v1",
    "title": "Chain of Time: In-Context Physical Simulation with Image Generation Models",
    "authors": [
      "YingQiao Wang",
      "Eric Bigelow",
      "Boyi Li",
      "Tomer Ullman"
    ],
    "abstract": "We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our ``Chain of Time\" method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00110v1",
    "published_date": "2025-10-30 21:46:26 UTC",
    "updated_date": "2025-10-30 21:46:26 UTC"
  },
  {
    "arxiv_id": "2511.11601v1",
    "title": "Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators",
    "authors": [
      "Elliott Wen",
      "Sean Ma",
      "Ewan Tempero",
      "Jens Dietrich",
      "Daniel Luo",
      "Jiaxing Shen",
      "Kaiqi Zhao",
      "Bruce Sham",
      "Yousong Song",
      "Jiayi Hua",
      "Jia Hong"
    ],
    "abstract": "While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.11601v1",
    "published_date": "2025-10-30 21:31:50 UTC",
    "updated_date": "2025-10-30 21:31:50 UTC"
  },
  {
    "arxiv_id": "2510.27009v1",
    "title": "Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models",
    "authors": [
      "Jared Junkin",
      "Samuel Nathanson"
    ],
    "abstract": "Language models are traditionally designed around causal masking. In domains with spatial or relational structure, causal masking is often viewed as inappropriate, and sequential linearizations are instead used. Yet the question of whether it is viable to accept the information loss introduced by causal masking on nonsequential data has received little direct study, in part because few domains offer both spatial and sequential representations of the same dataset. In this work, we investigate this issue in the domain of chess, which naturally supports both representations. We train language models with bidirectional and causal self-attention mechanisms on both spatial (board-based) and sequential (move-based) data. Our results show that models trained on spatial board states - \\textit{even with causal masking} - consistently achieve stronger playing strength than models trained on sequential data. While our experiments are conducted on chess, our results are methodological and may have broader implications: applying causal masking to spatial data is a viable procedure for training unimodal LLMs on spatial data, and in some domains is even preferable to sequentialization.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.27009v1",
    "published_date": "2025-10-30 21:22:36 UTC",
    "updated_date": "2025-10-30 21:22:36 UTC"
  },
  {
    "arxiv_id": "2510.27002v1",
    "title": "Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase",
    "authors": [
      "Mihir Mahajan",
      "Alfred Nguyen",
      "Franz Srambical",
      "Stefan Bauer"
    ],
    "abstract": "While world models are increasingly positioned as a pathway to overcoming data scarcity in domains such as robotics, open training infrastructure for world modeling remains nascent. We introduce Jasmine, a performant JAX-based world modeling codebase that scales from single hosts to hundreds of accelerators with minimal code changes. Jasmine achieves an order-of-magnitude faster reproduction of the CoinRun case study compared to prior open implementations, enabled by performance optimizations across data loading, training and checkpointing. The codebase guarantees fully reproducible training and supports diverse sharding configurations. By pairing Jasmine with curated large-scale datasets, we establish infrastructure for rigorous benchmarking pipelines across model families and architectural ablations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Blog post: https://pdoom.org/jasmine.html",
    "pdf_url": "https://arxiv.org/pdf/2510.27002v1",
    "published_date": "2025-10-30 21:03:57 UTC",
    "updated_date": "2025-10-30 21:03:57 UTC"
  },
  {
    "arxiv_id": "2510.27001v1",
    "title": "A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms",
    "authors": [
      "Elise Wolf"
    ],
    "abstract": "Multi-armed bandit (MAB) problems serve as a fundamental building block for more complex reinforcement learning algorithms. However, evaluating and comparing MAB algorithms remains challenging due to the lack of standardized conditions and replicability. This is particularly problematic for variance-aware extensions of classical methods like UCB, whose performance can heavily depend on the underlying environment. In this study, we address how performance differences between bandit algorithms can be reliably observed, and under what conditions variance-aware algorithms outperform classical ones. We present a reproducible evaluation designed to systematically compare eight classical and variance-aware MAB algorithms. The evaluation framework, implemented in our Bandit Playground codebase, features clearly defined experimental setups, multiple performance metrics (reward, regret, reward distribution, value-at-risk, and action optimality), and an interactive evaluation interface that supports consistent and transparent analysis. We show that variance-aware algorithms can offer advantages in settings with high uncertainty where the difficulty arises from subtle differences between arm rewards. In contrast, classical algorithms often perform equally well or better in more separable scenarios or if fine-tuned extensively. Our contributions are twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2) insights into the conditions under which variance-aware approaches outperform their classical counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint of a paper presented at GI Skill 2025. The final version will appear in the conference proceedings",
    "pdf_url": "https://arxiv.org/pdf/2510.27001v1",
    "published_date": "2025-10-30 21:01:23 UTC",
    "updated_date": "2025-10-30 21:01:23 UTC"
  },
  {
    "arxiv_id": "2510.26999v1",
    "title": "AIOT based Smart Education System: A Dual Layer Authentication and Context-Aware Tutoring Framework for Learning Environments",
    "authors": [
      "Adithya Neelakantan",
      "Pratik Satpute",
      "Prerna Shinde",
      "Tejas Manjunatha Devang"
    ],
    "abstract": "The AIoT-Based Smart Education System integrates Artificial Intelligence and IoT to address persistent challenges in contemporary classrooms: attendance fraud, lack of personalization, student disengagement, and inefficient resource use. The unified platform combines four core modules: (1) a dual-factor authentication system leveraging RFID-based ID scans and WiFi verification for secure, fraud-resistant attendance; (2) an AI-powered assistant that provides real-time, context-aware support and dynamic quiz generation based on instructor-supplied materials; (3) automated test generators to streamline adaptive assessment and reduce administrative overhead; and (4) the EcoSmart Campus module, which autonomously regulates classroom lighting, air quality, and temperature using IoT sensors and actuators. Simulated evaluations demonstrate the system's effectiveness in delivering robust real-time monitoring, fostering inclusive engagement, preventing fraudulent practices, and supporting operational scalability. Collectively, the AIoT-Based Smart Education System offers a secure, adaptive, and efficient learning environment, providing a scalable blueprint for future educational innovation and improved student outcomes through the synergistic application of artificial intelligence and IoT technologies.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26999v1",
    "published_date": "2025-10-30 21:00:22 UTC",
    "updated_date": "2025-10-30 21:00:22 UTC"
  },
  {
    "arxiv_id": "2510.26995v1",
    "title": "LLMs are Overconfident: Evaluating Confidence Interval Calibration with FermiEval",
    "authors": [
      "Elliot L. Epstein",
      "John Winnicki",
      "Thanawat Sornwanee",
      "Rajat Dwaraknath"
    ],
    "abstract": "Large language models (LLMs) excel at numerical estimation but struggle to correctly quantify uncertainty. We study how well LLMs construct confidence intervals around their own answers and find that they are systematically overconfident. To evaluate this behavior, we introduce FermiEval, a benchmark of Fermi-style estimation questions with a rigorous scoring rule for confidence interval coverage and sharpness. Across several modern models, nominal 99\\% intervals cover the true answer only 65\\% of the time on average. With a conformal prediction based approach that adjusts the intervals, we obtain accurate 99\\% observed coverage, and the Winkler interval score decreases by 54\\%. We also propose direct log-probability elicitation and quantile adjustment methods, which further reduce overconfidence at high confidence levels. Finally, we develop a perception-tunnel theory explaining why LLMs exhibit overconfidence: when reasoning under uncertainty, they act as if sampling from a truncated region of their inferred distribution, neglecting its tails.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26995v1",
    "published_date": "2025-10-30 20:49:41 UTC",
    "updated_date": "2025-10-30 20:49:41 UTC"
  },
  {
    "arxiv_id": "2510.26989v2",
    "title": "SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation",
    "authors": [
      "Agorakis Bompotas",
      "Konstantinos Koutras",
      "Nikitas Rigas Kalogeropoulos",
      "Panagiotis Kechagias",
      "Dimitra Gariza",
      "Athanasios P. Kalogeras",
      "Christos Alexakos"
    ],
    "abstract": "The global agricultural sector is undergoing a transformative shift, driven by increasing food demands, climate variability and the need for sustainable practices. SUSTAINABLE is a smart farming platform designed to integrate IoT, AI, satellite imaging, and role-based task orchestration to enable efficient, traceable, and sustainable agriculture with a pilot usecase in viticulture. This paper explores current smart agriculture solutions, presents a comparative evaluation, and introduces SUSTAINABLE's key features, including satellite index integration, real-time environmental data, and role-aware task management tailored to Mediterranean vineyards.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for presentation to 11th IEEE International Smart Cities Conference (ISC2 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.26989v2",
    "published_date": "2025-10-30 20:32:17 UTC",
    "updated_date": "2026-01-02 08:29:17 UTC"
  },
  {
    "arxiv_id": "2510.26981v1",
    "title": "Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget",
    "authors": [
      "Zhichao Hou",
      "Weizhi Gao",
      "Xiaorui Liu"
    ],
    "abstract": "This work tackles a critical challenge in AI safety research under limited compute: given a fixed computation budget, how can one maximize the strength of iterative adversarial attacks? Coarsely reducing the number of attack iterations lowers cost but substantially weakens effectiveness. To fulfill the attainable attack efficacy within a constrained budget, we propose a fine-grained control mechanism that selectively recomputes layer activations across both iteration-wise and layer-wise levels. Extensive experiments show that our method consistently outperforms existing baselines at equal cost. Moreover, when integrated into adversarial training, it attains comparable performance with only 30% of the original budget.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26981v1",
    "published_date": "2025-10-30 20:13:06 UTC",
    "updated_date": "2025-10-30 20:13:06 UTC"
  },
  {
    "arxiv_id": "2510.26974v1",
    "title": "Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations",
    "authors": [
      "Jean-Philippe Corbeil",
      "Asma Ben Abacha",
      "Jerome Tremblay",
      "Phillip Swazinna",
      "Akila Jeeson Daniel",
      "Miguel Del-Agua",
      "Francois Beaulieu"
    ],
    "abstract": "Clinical documentation increasingly uses automatic speech recognition and summarization, yet converting conversations into actionable medical orders for Electronic Health Records remains unexplored. A solution to this problem can significantly reduce the documentation burden of clinicians and directly impact downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first challenge on extracting medical orders from doctor-patient conversations. Six teams participated in the shared task and experimented with a broad range of approaches, and both closed- and open-weight large language models (LLMs). In this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking, and participants' solutions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26974v1",
    "published_date": "2025-10-30 19:56:05 UTC",
    "updated_date": "2025-10-30 19:56:05 UTC"
  },
  {
    "arxiv_id": "2511.00108v2",
    "title": "Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence",
    "authors": [
      "Yi Zhang",
      "Che Liu",
      "Xiancong Ren",
      "Hanchu Ni",
      "Shuai Zhang",
      "Zeyuan Ding",
      "Jiayu Hu",
      "Hanzhe Shan",
      "Zhenwei Niu",
      "Zhaoyang Liu",
      "Shuang Liu",
      "Yue Zhao",
      "Junbo Qi",
      "Qinfan Zhang",
      "Dengjie Li",
      "Yidong Wang",
      "Jiachen Luo",
      "Yong Dai",
      "Zenglin Xu",
      "Bin Shen",
      "Qifan Wang",
      "Jian Tang",
      "Xiaozhu Ju"
    ],
    "abstract": "This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00108v2",
    "published_date": "2025-10-30 19:55:13 UTC",
    "updated_date": "2025-11-14 13:54:15 UTC"
  },
  {
    "arxiv_id": "2510.26969v1",
    "title": "Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence",
    "authors": [
      "Lívia Dutra",
      "Arthur Lorenzi",
      "Laís Berno",
      "Franciany Campos",
      "Karoline Biscardi",
      "Kenneth Brown",
      "Marcelo Viridiano",
      "Frederico Belcavello",
      "Ely Matos",
      "Olívia Guaranha",
      "Erik Santos",
      "Sofia Reinach",
      "Tiago Timponi Torrent"
    ],
    "abstract": "We introduce a methodology for the identification of notifiable events in the domain of healthcare. The methodology harnesses semantic frames to define fine-grained patterns and search them in unstructured data, namely, open-text fields in e-medical records. We apply the methodology to the problem of underreporting of gender-based violence (GBV) in e-medical records produced during patients' visits to primary care units. A total of eight patterns are defined and searched on a corpus of 21 million sentences in Brazilian Portuguese extracted from e-SUS APS. The results are manually evaluated by linguists and the precision of each pattern measured. Our findings reveal that the methodology effectively identifies reports of violence with a precision of 0.726, confirming its robustness. Designed as a transparent, efficient, low-carbon, and language-agnostic pipeline, the approach can be easily adapted to other health surveillance contexts, contributing to the broader, ethical, and explainable use of NLP in public health systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26969v1",
    "published_date": "2025-10-30 19:52:24 UTC",
    "updated_date": "2025-10-30 19:52:24 UTC"
  },
  {
    "arxiv_id": "2510.26967v1",
    "title": "Using Salient Object Detection to Identify Manipulative Cookie Banners that Circumvent GDPR",
    "authors": [
      "Riley Grossman",
      "Michael Smith",
      "Cristian Borcea",
      "Yi Chen"
    ],
    "abstract": "The main goal of this paper is to study how often cookie banners that comply with the General Data Protection Regulation (GDPR) contain aesthetic manipulation, a design tactic to draw users' attention to the button that permits personal data sharing. As a byproduct of this goal, we also evaluate how frequently the banners comply with GDPR and the recommendations of national data protection authorities regarding banner designs. We visited 2,579 websites and identified the type of cookie banner implemented. Although 45% of the relevant websites have fully compliant banners, we found aesthetic manipulation on 38% of the compliant banners. Unlike prior studies of aesthetic manipulation, we use a computer vision model for salient object detection to measure how salient (i.e., attention-drawing) each banner element is. This enables the discovery of new types of aesthetic manipulation (e.g., button placement), and leads us to conclude that aesthetic manipulation is more common than previously reported (38% vs 27% of banners). To study the effects of user and/or website location on cookie banner design, we include websites within the European Union (EU), where privacy regulation enforcement is more stringent, and websites outside the EU. We visited websites from IP addresses in the EU and from IP addresses in the United States (US). We find that 13.9% of EU websites change their banner design when the user is from the US, and EU websites are roughly 48.3% more likely to use aesthetic manipulation than non-EU websites, highlighting their innovative responses to privacy regulation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to International AAAI Conference on Web and Social Media 2026 (ICWSM'26)",
    "pdf_url": "https://arxiv.org/pdf/2510.26967v1",
    "published_date": "2025-10-30 19:50:13 UTC",
    "updated_date": "2025-10-30 19:50:13 UTC"
  },
  {
    "arxiv_id": "2510.26954v2",
    "title": "Can machines think efficiently?",
    "authors": [
      "Adam Winchell"
    ],
    "abstract": "The Turing Test is no longer adequate for distinguishing human and machine intelligence. With advanced artificial intelligence systems already passing the original Turing Test and contributing to serious ethical and environmental concerns, we urgently need to update the test. This work expands upon the original imitation game by accounting for an additional factor: the energy spent answering the questions. By adding the constraint of energy, the new test forces us to evaluate intelligence through the lens of efficiency, connecting the abstract problem of thinking to the concrete reality of finite resources. Further, this proposed new test ensures the evaluation of intelligence has a measurable, practical finish line that the original test lacks. This additional constraint compels society to weigh the time savings of using artificial intelligence against its total resource cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26954v2",
    "published_date": "2025-10-30 19:26:24 UTC",
    "updated_date": "2025-12-31 17:49:19 UTC"
  },
  {
    "arxiv_id": "2511.07433v1",
    "title": "Benchmarking Simulacra AI's Quantum Accurate Synthetic Data Generation for Chemical Sciences",
    "authors": [
      "Fabio Falcioni",
      "Elena Orlova",
      "Timothy Heightman",
      "Philip Mantrov",
      "Aleksei Ustimenko"
    ],
    "abstract": "In this work, we benchmark \\simulacra's synthetic data generation pipeline against a state-of-the-art Microsoft pipeline on a dataset of small to large systems. By analyzing the energy quality, autocorrelation times, and effective sample size, our findings show that Simulacra's Large Wavefunction Models (LWM) pipeline, paired with state-of-the-art Variational Monte Carlo (VMC) sampling algorithms, reduces data generation costs by 15-50x, while maintaining parity in energy accuracy, and 2-3x compared to traditional CCSD methods on the scale of amino acids. This enables the creation of affordable, large-scale \\textit{ab-initio} datasets, accelerating AI-driven optimization and discovery in the pharmaceutical industry and beyond. Our improvements are based on a novel and proprietary sampling scheme called Replica Exchange with Langevin Adaptive eXploration (RELAX).",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.07433v1",
    "published_date": "2025-10-30 19:19:56 UTC",
    "updated_date": "2025-10-30 19:19:56 UTC"
  },
  {
    "arxiv_id": "2510.26941v1",
    "title": "LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks",
    "authors": [
      "Seif Ikbarieh",
      "Maanak Gupta",
      "Elmahedi Mahalal"
    ],
    "abstract": "The Internet of Things has expanded rapidly, transforming communication and operations across industries but also increasing the attack surface and security breaches. Artificial Intelligence plays a key role in securing IoT, enabling attack detection, attack behavior analysis, and mitigation suggestion. Despite advancements, evaluations remain purely qualitative, and the lack of a standardized, objective benchmark for quantitatively measuring AI-based attack analysis and mitigation hinders consistent assessment of model effectiveness. In this work, we propose a hybrid framework combining Machine Learning (ML) for multi-class attack detection with Large Language Models (LLMs) for attack behavior analysis and mitigation suggestion. After benchmarking several ML and Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we applied structured role-play prompt engineering with Retrieval-Augmented Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed, context-aware responses. We introduce novel evaluation metrics for quantitative assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o, DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the responses. Results show that Random Forest has the best detection model, and ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26941v1",
    "published_date": "2025-10-30 18:55:08 UTC",
    "updated_date": "2025-10-30 18:55:08 UTC"
  },
  {
    "arxiv_id": "2510.26940v1",
    "title": "Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction",
    "authors": [
      "Ashwin Kumar",
      "Hanyu Zhang",
      "David A. Schweidel",
      "William Yeoh"
    ],
    "abstract": "Next location prediction underpins a growing number of mobility, retail, and public-health applications, yet its societal impacts remain largely unexplored. In this paper, we audit state-of-the-art mobility prediction models trained on a large-scale dataset, highlighting hidden disparities based on user demographics. Drawing from aggregate census data, we compute the difference in predictive performance on racial and ethnic user groups and show a systematic disparity resulting from the underlying dataset, resulting in large differences in accuracy based on location and user groups. To address this, we propose Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy designed for incremental data collection settings. Because individual-level demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a clustering method that partitions users in latent mobility space while enforcing census-derived group proportions. This yields proxy racial labels for the four largest groups in the state: Asian, Black, Hispanic, and White. Built on these labels, our sampling algorithm prioritizes users based on expected performance gains and current group representation. This method incrementally constructs training datasets that reduce demographic performance gaps while preserving overall accuracy. Our method reduces total disparity between groups by up to 40\\% with minimal accuracy trade-offs, as evaluated on a state-of-art MetaPath2Vec model and a transformer-encoder model. Improvements are most significant in early sampling stages, highlighting the potential for fairness-aware strategies to deliver meaningful gains even in low-resource settings. Our findings expose structural inequities in mobility prediction pipelines and demonstrate how lightweight, data-centric interventions can improve fairness with little added complexity, especially for low-data applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26940v1",
    "published_date": "2025-10-30 18:54:33 UTC",
    "updated_date": "2025-10-30 18:54:33 UTC"
  },
  {
    "arxiv_id": "2511.00107v1",
    "title": "AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency",
    "authors": [
      "Piyushkumar Patel"
    ],
    "abstract": "Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00107v1",
    "published_date": "2025-10-30 18:46:59 UTC",
    "updated_date": "2025-10-30 18:46:59 UTC"
  },
  {
    "arxiv_id": "2510.26935v1",
    "title": "RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification",
    "authors": [
      "Yunhao Yang",
      "Neel P. Bhatt",
      "Pranay Samineni",
      "Rohan Siva",
      "Zhanyang Wang",
      "Ufuk Topcu"
    ],
    "abstract": "As AI systems migrate to safety-critical domains, verifying that their actions comply with well-defined rules remains a challenge. Formal methods provide provable guarantees but demand hand-crafted temporal-logic specifications, offering limited expressiveness and accessibility. Deep learning approaches enable evaluation of plans against natural-language constraints, yet their opaque decision process invites misclassifications with potentially severe consequences. We introduce RepV, a neurosymbolic verifier that unifies both views by learning a latent space where safe and unsafe plans are linearly separable. Starting from a modest seed set of plans labeled by an off-the-shelf model checker, RepV trains a lightweight projector that embeds each plan, together with a language model-generated rationale, into a low-dimensional space; a frozen linear boundary then verifies compliance for unseen natural-language rules in a single forward pass.\n  Beyond binary classification, RepV provides a probabilistic guarantee on the likelihood of correct verification based on its position in the latent space. This guarantee enables a guarantee-driven refinement of the planner, improving rule compliance without human annotations. Empirical evaluations show that RepV improves compliance prediction accuracy by up to 15% compared to baseline methods while adding fewer than 0.2M parameters. Furthermore, our refinement framework outperforms ordinary fine-tuning baselines across various planning domains. These results show that safety-separable latent spaces offer a scalable, plug-and-play primitive for reliable neurosymbolic plan verification. Code and data are available at: https://repv-project.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.FL"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and data are available at: https://repv-project.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.26935v1",
    "published_date": "2025-10-30 18:46:34 UTC",
    "updated_date": "2025-10-30 18:46:34 UTC"
  },
  {
    "arxiv_id": "2511.00106v1",
    "title": "Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies",
    "authors": [
      "Anuj Gupta",
      "Ann Shivers-McNair"
    ],
    "abstract": "In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Published in the journal Computers and Composition, Issue 74 (2024)",
    "pdf_url": "https://arxiv.org/pdf/2511.00106v1",
    "published_date": "2025-10-30 18:36:51 UTC",
    "updated_date": "2025-10-30 18:36:51 UTC"
  },
  {
    "arxiv_id": "2511.00105v2",
    "title": "Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges",
    "authors": [
      "Majid Memari",
      "Krista Ruggles"
    ],
    "abstract": "Artificial intelligence (AI) is transforming elementary STEM education, yet evidence remains fragmented. This systematic review synthesizes 258 studies (2020-2025) examining AI applications across eight categories: intelligent tutoring systems (45% of studies), learning analytics (18%), automated assessment (12%), computer vision (8%), educational robotics (7%), multimodal sensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive content generation. The analysis shows that most studies focus on upper elementary grades (65%) and mathematics (38%), with limited cross-disciplinary STEM integration (15%). While conversational AI demonstrates moderate effectiveness (d = 0.45-0.70 where reported), only 34% of studies include standardized effect sizes. Eight major gaps limit real-world impact: fragmented ecosystems, developmental inappropriateness, infrastructure barriers, lack of privacy frameworks, weak STEM integration, equity disparities, teacher marginalization, and narrow assessment scopes. Geographic distribution is also uneven, with 90% of studies originating from North America, East Asia, and Europe. Future directions call for interoperable architectures that support authentic STEM integration, grade-appropriate design, privacy-preserving analytics, and teacher-centered implementations that enhance rather than replace human expertise.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00105v2",
    "published_date": "2025-10-30 18:35:42 UTC",
    "updated_date": "2025-11-06 16:36:05 UTC"
  },
  {
    "arxiv_id": "2510.26923v1",
    "title": "Scale-Aware Curriculum Learning for Ddata-Efficient Lung Nodule Detection with YOLOv11",
    "authors": [
      "Yi Luo",
      "Yike Guo",
      "Hamed Hooshangnejad",
      "Kai Ding"
    ],
    "abstract": "Lung nodule detection in chest CT is crucial for early lung cancer diagnosis, yet existing deep learning approaches face challenges when deployed in clinical settings with limited annotated data. While curriculum learning has shown promise in improving model training, traditional static curriculum strategies fail in data-scarce scenarios. We propose Scale Adaptive Curriculum Learning (SACL), a novel training strategy that dynamically adjusts curriculum design based on available data scale. SACL introduces three key mechanisms:(1) adaptive epoch scheduling, (2) hard sample injection, and (3) scale-aware optimization. We evaluate SACL on the LUNA25 dataset using YOLOv11 as the base detector. Experimental results demonstrate that while SACL achieves comparable performance to static curriculum learning on the full dataset in mAP50, it shows significant advantages under data-limited conditions with 4.6%, 3.5%, and 2.0% improvements over baseline at 10%, 20%, and 50% of training data respectively. By enabling robust training across varying data scales without architectural modifications, SACL provides a practical solution for healthcare institutions to develop effective lung nodule detection systems despite limited annotation resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26923v1",
    "published_date": "2025-10-30 18:33:59 UTC",
    "updated_date": "2025-10-30 18:33:59 UTC"
  },
  {
    "arxiv_id": "2510.26915v1",
    "title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence",
    "authors": [
      "Zachary Ravichandran",
      "Fernando Cladera",
      "Ankit Prabhu",
      "Jason Hughes",
      "Varun Murali",
      "Camillo Taylor",
      "George J. Pappas",
      "Vijay Kumar"
    ],
    "abstract": "Heterogeneous robot teams operating in realistic settings often must accomplish complex missions requiring collaboration and adaptation to information acquired online. Because robot teams frequently operate in unstructured environments -- uncertain, open-world settings without prior maps -- subtasks must be grounded in robot capabilities and the physical world. While heterogeneous teams have typically been designed for fixed specifications, generative intelligence opens the possibility of teams that can accomplish a wide range of missions described in natural language. However, current large language model (LLM)-enabled teaming methods typically assume well-structured and known environments, limiting deployment in unstructured environments. We present SPINE-HT, a framework that addresses these limitations by grounding the reasoning abilities of LLMs in the context of a heterogeneous robot team through a three-stage process. Given language specifications describing mission goals and team capabilities, an LLM generates grounded subtasks which are validated for feasibility. Subtasks are then assigned to robots based on capabilities such as traversability or perception and refined given feedback collected during online operation. In simulation experiments with closed-loop perception and control, our framework achieves nearly twice the success rate compared to prior LLM-enabled heterogeneous teaming approaches. In real-world experiments with a Clearpath Jackal, a Clearpath Husky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an 87\\% success rate in missions requiring reasoning about robot capabilities and refining subtasks with online feedback. More information is provided at https://zacravichandran.github.io/SPINE-HT.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26915v1",
    "published_date": "2025-10-30 18:24:38 UTC",
    "updated_date": "2025-10-30 18:24:38 UTC"
  },
  {
    "arxiv_id": "2510.26905v1",
    "title": "Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations",
    "authors": [
      "Pedro Antonio Alarcón Granadeno",
      "Arturo Miguel Bernal Russell",
      "Sofia Nelson",
      "Demetrius Hernandez",
      "Maureen Petterson",
      "Michael Murphy",
      "Walter J. Scheirer",
      "Jane Cleland-Huang"
    ],
    "abstract": "Cyber-physical systems increasingly rely on Foundational Models such as Large Language Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy through enhanced perception, inference, and planning. However, these models also introduce new types of errors, such as hallucinations, overgeneralizations, and context misalignments, resulting in incorrect and flawed decisions. To address this, we introduce the concept of Cognition Envelopes, designed to establish reasoning boundaries that constrain AI-generated decisions while complementing the use of meta-cognition and traditional safety envelopes. As with safety envelopes, Cognition Envelopes require practical guidelines and systematic processes for their definition, validation, and assurance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10.5 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26905v1",
    "published_date": "2025-10-30 18:11:32 UTC",
    "updated_date": "2025-10-30 18:11:32 UTC"
  },
  {
    "arxiv_id": "2510.26899v3",
    "title": "How Similar Are Grokipedia and Wikipedia? A Multi-Dimensional Textual and Structural Comparison",
    "authors": [
      "Taha Yasseri",
      "Saeedeh Mohammadi"
    ],
    "abstract": "The launch of Grokipedia - an AI-generated encyclopedia developed by Elon Musk's xAI - was presented as a response to perceived ideological and structural biases in Wikipedia, aiming to produce \"truthful\" entries using the Grok large language model. Yet whether an AI-driven alternative can escape the biases and limitations of human-edited platforms remains unclear. This study conducts a large-scale computational comparison of more than 17,000 matched article pairs from the 20,000 most-edited English Wikipedia pages. Using metrics spanning lexical richness, readability, reference density, structural features, and semantic similarity, we assess how closely the two platforms align in form and substance. We find that Grokipedia articles are substantially longer and contain significantly fewer references per word. Moreover, Grokipedia's content divides into two distinct groups: one that remains semantically and stylistically aligned with Wikipedia, and another that diverges sharply. Among the dissimilar articles, we observe a systematic rightward shift in the political bias of cited news sources, concentrated primarily in entries related to politics, history, and religion. These findings suggest that AI-generated encyclopedic content diverges from established editorial norms-favouring narrative expansion over citation-based verification. The implications highlight emerging tensions around transparency, provenance, and the governance of knowledge in an era of automated text generation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "20 pages, 8 figures, 2 tables, updated with a larger sample size of 20,000 articles, better text cleaning procedure + Reference analysis, topical analysis",
    "pdf_url": "https://arxiv.org/pdf/2510.26899v3",
    "published_date": "2025-10-30 18:04:46 UTC",
    "updated_date": "2025-11-30 22:10:18 UTC"
  },
  {
    "arxiv_id": "2510.26892v1",
    "title": "BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs",
    "authors": [
      "Mahsa Valizadeh",
      "Rui Tuo",
      "James Caverlee"
    ],
    "abstract": "Generative Adversarial Networks (GANs) are proficient at generating synthetic data but continue to suffer from mode collapse, where the generator produces a narrow range of outputs that fool the discriminator but fail to capture the full data distribution. This limitation is particularly problematic, as generative models are increasingly deployed in real-world applications that demand both diversity and uncertainty awareness. In response, we introduce BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty into the generative process while maintaining computational efficiency. BI-DCGAN integrates Bayes by Backprop to learn a distribution over network weights and employs mean-field variational inference to efficiently approximate the posterior distribution during GAN training. We establishes the first theoretical proof, based on covariance matrix analysis, that Bayesian modeling enhances sample diversity in GANs. We validate this theoretical result through extensive experiments on standard generative benchmarks, demonstrating that BI-DCGAN produces more diverse and robust outputs than conventional DCGANs, while maintaining training efficiency. These findings position BI-DCGAN as a scalable and timely solution for applications where both diversity and uncertainty are critical, and where modern alternatives like diffusion models remain too resource-intensive.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26892v1",
    "published_date": "2025-10-30 18:00:38 UTC",
    "updated_date": "2025-10-30 18:00:38 UTC"
  },
  {
    "arxiv_id": "2510.26887v1",
    "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
    "authors": [
      "Francisco Villaescusa-Navarro",
      "Boris Bolliet",
      "Pablo Villanueva-Domingo",
      "Adrian E. Bayer",
      "Aidan Acquah",
      "Chetana Amancharla",
      "Almog Barzilay-Siegal",
      "Pablo Bermejo",
      "Camille Bilodeau",
      "Pablo Cárdenas Ramírez",
      "Miles Cranmer",
      "Urbano L. França",
      "ChangHoon Hahn",
      "Yan-Fei Jiang",
      "Raul Jimenez",
      "Jun-Young Lee",
      "Antonio Lerario",
      "Osman Mamun",
      "Thomas Meier",
      "Anupam A. Ojha",
      "Pavlos Protopapas",
      "Shimanto Roy",
      "David N. Spergel",
      "Pedro Tarancón-Álvarez",
      "Ujjwal Tiwari",
      "Matteo Viel",
      "Digvijay Wadekar",
      "Chi Wang",
      "Bonny Y. Wang",
      "Licong Xu",
      "Yossi Yovel",
      "Shuwen Yue",
      "Wen-Han Zhou",
      "Qiyao Zhu",
      "Jiajun Zou",
      "Íñigo Zubeldia"
    ],
    "abstract": "We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "272 pages. Examples of 11 AI-generated paper drafts from different scientific disciplines. Code publicly available at https://github.com/AstroPilot-AI/Denario",
    "pdf_url": "https://arxiv.org/pdf/2510.26887v1",
    "published_date": "2025-10-30 18:00:12 UTC",
    "updated_date": "2025-10-30 18:00:12 UTC"
  },
  {
    "arxiv_id": "2511.00103v1",
    "title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video",
    "authors": [
      "Rotem Ezra",
      "Hedi Zisling",
      "Nimrod Berman",
      "Ilan Naiman",
      "Alexey Gorkor",
      "Liran Nochumsohn",
      "Eliya Nachmani",
      "Omri Azencot"
    ],
    "abstract": "Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: https://azencot-group.github.io/FreeSliders/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00103v1",
    "published_date": "2025-10-30 17:59:58 UTC",
    "updated_date": "2025-10-30 17:59:58 UTC"
  },
  {
    "arxiv_id": "2510.26802v1",
    "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark",
    "authors": [
      "Ziyu Guo",
      "Xinyan Chen",
      "Renrui Zhang",
      "Ruichuan An",
      "Yu Qi",
      "Dongzhi Jiang",
      "Xiangtai Li",
      "Manyuan Zhang",
      "Hongsheng Li",
      "Pheng-Ann Heng"
    ],
    "abstract": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://video-cof.github.io",
    "pdf_url": "https://arxiv.org/pdf/2510.26802v1",
    "published_date": "2025-10-30 17:59:55 UTC",
    "updated_date": "2025-10-30 17:59:55 UTC"
  },
  {
    "arxiv_id": "2511.05541v1",
    "title": "Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability",
    "authors": [
      "Usha Bhalla",
      "Alex Oesterling",
      "Claudio Mayrink Verdun",
      "Himabindu Lakkaraju",
      "Flavio P. Calmon"
    ],
    "abstract": "Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as \"the phrase 'The' at the start of sentences\". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 Pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05541v1",
    "published_date": "2025-10-30 17:59:30 UTC",
    "updated_date": "2025-10-30 17:59:30 UTC"
  },
  {
    "arxiv_id": "2510.26790v1",
    "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
    "authors": [
      "Hyunji Lee",
      "Minseon Kim",
      "Chinmay Singh",
      "Matheus Pereira",
      "Atharv Sonwane",
      "Isadora White",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Zhengyan Shi",
      "Alessandro Sordoni",
      "Marc-Alexandre Côté",
      "Xingdi Yuan",
      "Lucas Caccia"
    ],
    "abstract": "As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26790v1",
    "published_date": "2025-10-30 17:58:26 UTC",
    "updated_date": "2025-10-30 17:58:26 UTC"
  },
  {
    "arxiv_id": "2510.26788v1",
    "title": "Defeating the Training-Inference Mismatch via FP16",
    "authors": [
      "Penghui Qi",
      "Zichen Liu",
      "Xiangxin Zhou",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "abstract": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \\textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26788v1",
    "published_date": "2025-10-30 17:58:11 UTC",
    "updated_date": "2025-10-30 17:58:11 UTC"
  },
  {
    "arxiv_id": "2510.26787v1",
    "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
    "authors": [
      "Mantas Mazeika",
      "Alice Gatti",
      "Cristina Menghini",
      "Udari Madhushani Sehwag",
      "Shivam Singhal",
      "Yury Orlovskiy",
      "Steven Basart",
      "Manasi Sharma",
      "Denis Peskoff",
      "Elaine Lau",
      "Jaehyuk Lim",
      "Lachlan Carroll",
      "Alice Blair",
      "Vinaya Sivakumar",
      "Sumana Basu",
      "Brad Kenstler",
      "Yuntao Ma",
      "Julian Michael",
      "Xiaoke Li",
      "Oliver Ingebretsen",
      "Aditya Mehta",
      "Jean Mottola",
      "John Teichmann",
      "Kevin Yu",
      "Zaina Shaik",
      "Adam Khoja",
      "Richard Ren",
      "Jason Hausenloy",
      "Long Phan",
      "Ye Htet",
      "Ankit Aich",
      "Tahseen Rabbani",
      "Vivswan Shah",
      "Andriy Novykov",
      "Felix Binder",
      "Kirill Chugunov",
      "Luis Ramirez",
      "Matias Geralnik",
      "Hernán Mesura",
      "Dean Lee",
      "Ed-Yeremai Hernandez Cardona",
      "Annette Diamond",
      "Summer Yue",
      "Alexandr Wang",
      "Bing Liu",
      "Ernesto Hernandez",
      "Dan Hendrycks"
    ],
    "abstract": "AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Website: https://www.remotelabor.ai",
    "pdf_url": "https://arxiv.org/pdf/2510.26787v1",
    "published_date": "2025-10-30 17:58:04 UTC",
    "updated_date": "2025-10-30 17:58:04 UTC"
  },
  {
    "arxiv_id": "2510.26784v1",
    "title": "LLMs Process Lists With General Filter Heads",
    "authors": [
      "Arnab Sen Sharma",
      "Giordano Rogers",
      "Natalie Shapira",
      "David Bau"
    ],
    "abstract": "We investigate the mechanisms underlying a range of list-processing tasks in LLMs, and we find that LLMs have learned to encode a compact, causal representation of a general filtering operation that mirrors the generic \"filter\" function of functional programming. Using causal mediation analysis on a diverse set of list-processing tasks, we find that a small number of attention heads, which we dub filter heads, encode a compact representation of the filtering predicate in their query states at certain tokens. We demonstrate that this predicate representation is general and portable: it can be extracted and reapplied to execute the same filtering operation on different collections, presented in different formats, languages, or even in tasks. However, we also identify situations where transformer LMs can exploit a different strategy for filtering: eagerly evaluating if an item satisfies the predicate and storing this intermediate result as a flag directly in the item representations. Our results reveal that transformer LMs can develop human-interpretable implementations of abstract computational operations that generalize in ways that are surprisingly similar to strategies used in traditional functional programming patterns.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Code and data at https://filter.baulab.info/",
    "pdf_url": "https://arxiv.org/pdf/2510.26784v1",
    "published_date": "2025-10-30 17:57:17 UTC",
    "updated_date": "2025-10-30 17:57:17 UTC"
  },
  {
    "arxiv_id": "2510.26782v2",
    "title": "Clone Deterministic 3D Worlds",
    "authors": [
      "Zaishuo Xia",
      "Yukuan Lu",
      "Xinyi Li",
      "Yifan Xu",
      "Yubei Chen"
    ],
    "abstract": "A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future physical state of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. However, existing world models often focus on random generation of open worlds, but neglect the need for high-fidelity modeling of deterministic scenarios (such as fixed-map mazes and static space robot navigation). In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone a deterministic 3D world. 1) Through diagnostic experiment, we quantitatively demonstrate that high-fidelity cloning is feasible and the primary bottleneck for long-horizon fidelity is the geometric structure of the latent representation, not the dynamics model itself. 2) Building on this insight, we show that applying temporal contrastive learning principle as a geometric regularization can effectively curate a latent space that better reflects the underlying physical state manifold, demonstrating that contrastive constraints can serve as a powerful inductive bias for stable world modeling; we call this approach Geometrically-Regularized World Models (GRWM). At its core is a lightweight geometric regularization module that can be seamlessly integrated into standard autoencoders, reshaping their latent space to provide a stable foundation for effective dynamics modeling. By focusing on representation quality, GRWM offers a simple yet powerful pipeline for improving world model fidelity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26782v2",
    "published_date": "2025-10-30 17:56:43 UTC",
    "updated_date": "2025-11-18 04:52:38 UTC"
  },
  {
    "arxiv_id": "2510.26776v2",
    "title": "Faithful and Fast Influence Function via Advanced Sampling",
    "authors": [
      "Jungyeon Koh",
      "Hyeonsu Lyu",
      "Jonggyu Jang",
      "Hyun Jong Yang"
    ],
    "abstract": "How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26776v2",
    "published_date": "2025-10-30 17:55:19 UTC",
    "updated_date": "2025-10-31 01:18:32 UTC"
  },
  {
    "arxiv_id": "2510.26771v1",
    "title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization",
    "authors": [
      "Marco Federici",
      "Riccardo Del Chiaro",
      "Boris van Breugel",
      "Paul Whatmough",
      "Markus Nagel"
    ],
    "abstract": "Quantization is the key method for reducing inference latency, power and memory footprint of generative AI models. However, accuracy often degrades sharply when activations are quantized below eight bits. Recent work suggests that invertible linear transformations (e.g. rotations) can aid quantization, by reparameterizing feature channels and weights. In this paper, we propose \\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a novel strategy that applies linear transformations along the \\textit{sequence} dimension to exploit the strong local correlation in language and visual data. By keeping a small number of tokens in each intermediate activation at higher precision, we can maintain model accuracy at lower (average) activations bit-widths. We evaluate STaMP on recent LVM and LLM architectures, demonstrating that it significantly improves low bit width activation quantization and complements established activation and weight quantization methods including recent feature transformations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages main text, 8 pages supplementary material",
    "pdf_url": "https://arxiv.org/pdf/2510.26771v1",
    "published_date": "2025-10-30 17:53:42 UTC",
    "updated_date": "2025-10-30 17:53:42 UTC"
  },
  {
    "arxiv_id": "2510.26768v1",
    "title": "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions",
    "authors": [
      "Shengnan An",
      "Xunliang Cai",
      "Xuezhi Cao",
      "Xiaoyu Li",
      "Yehao Lin",
      "Junlin Liu",
      "Xinxuan Lv",
      "Dan Ma",
      "Xuanlin Wang",
      "Ziwen Wang",
      "Shuang Zhou"
    ],
    "abstract": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26768v1",
    "published_date": "2025-10-30 17:52:02 UTC",
    "updated_date": "2025-10-30 17:52:02 UTC"
  },
  {
    "arxiv_id": "2510.26752v1",
    "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy",
    "authors": [
      "William Overman",
      "Mohsen Bayati"
    ],
    "abstract": "As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26752v1",
    "published_date": "2025-10-30 17:46:49 UTC",
    "updated_date": "2025-10-30 17:46:49 UTC"
  },
  {
    "arxiv_id": "2510.26745v2",
    "title": "Deep sequence models tend to memorize geometrically; it is unclear why",
    "authors": [
      "Shahriar Noroozizadeh",
      "Vaishnavh Nagarajan",
      "Elan Rosenfeld",
      "Sanjiv Kumar"
    ],
    "abstract": "Deep sequence models are said to store atomic facts predominantly in the form of associative memory: a brute-force lookup of co-occurring entities. We identify a dramatically different form of storage of atomic facts that we term as geometric memory. Here, the model has synthesized embeddings encoding novel global relationships between all entities, including ones that do not co-occur in training. Such storage is powerful: for instance, we show how it transforms a hard reasoning task involving an $\\ell$-fold composition into an easy-to-learn $1$-step navigation task.\n  From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, as against a lookup of local associations, cannot be straightforwardly attributed to typical supervisory, architectural, or optimizational pressures. Counterintuitively, a geometry is learned even when it is more complex than the brute-force lookup.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points out to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery, and unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26745v2",
    "published_date": "2025-10-30 17:40:22 UTC",
    "updated_date": "2025-12-31 18:57:25 UTC"
  },
  {
    "arxiv_id": "2510.26740v1",
    "title": "A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation",
    "authors": [
      "Ashwin Kumar",
      "William Yeoh"
    ],
    "abstract": "We introduce the General Incentives-based Framework for Fairness (GIFF), a novel approach for fair multi-agent resource allocation that infers fair decision-making from standard value functions. In resource-constrained settings, agents optimizing for efficiency often create inequitable outcomes. Our approach leverages the action-value (Q-)function to balance efficiency and fairness without requiring additional training. Specifically, our method computes a local fairness gain for each action and introduces a counterfactual advantage correction term to discourage over-allocation to already well-off agents. This approach is formalized within a centralized control setting, where an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing, homelessness prevention, and a complex job allocation task-demonstrate that our framework consistently outperforms strong baselines and can discover far-sighted, equitable policies. The framework's effectiveness is supported by a theoretical foundation; we prove its fairness surrogate is a principled lower bound on the true fairness improvement and that its trade-off parameter offers monotonic tuning. Our findings establish GIFF as a robust and principled framework for leveraging standard reinforcement learning components to achieve more equitable outcomes in complex multi-agent systems.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26740v1",
    "published_date": "2025-10-30 17:37:51 UTC",
    "updated_date": "2025-10-30 17:37:51 UTC"
  },
  {
    "arxiv_id": "2511.00102v1",
    "title": "Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers",
    "authors": [
      "Vivan Doshi"
    ],
    "abstract": "The discovery of conservation laws is a cornerstone of scientific progress. However, identifying these invariants from observational data remains a significant challenge. We propose a hybrid framework to automate the discovery of conserved quantities from noisy trajectory data. Our approach integrates three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that learns a continuous model of the system's dynamics, (2) a Transformer that generates symbolic candidate invariants conditioned on the learned vector field, and (3) a symbolic-numeric verifier that provides a strong numerical certificate for the validity of these candidates. We test our framework on canonical physical systems and show that it significantly outperforms baselines that operate directly on trajectory data. This work demonstrates the robustness of a decoupled learn-then-search approach for discovering mathematical principles from imperfect data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5th Math-AI Workshop - Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2511.00102v1",
    "published_date": "2025-10-30 17:32:04 UTC",
    "updated_date": "2025-10-30 17:32:04 UTC"
  },
  {
    "arxiv_id": "2510.26732v1",
    "title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models",
    "authors": [
      "J. de Curtò",
      "I. de Zarzà",
      "Pablo García",
      "Jordi Cabot"
    ],
    "abstract": "This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.\n  The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26732v1",
    "published_date": "2025-10-30 17:31:03 UTC",
    "updated_date": "2025-10-30 17:31:03 UTC"
  },
  {
    "arxiv_id": "2510.26730v1",
    "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference",
    "authors": [
      "Zixu Shen",
      "Kexin Chu",
      "Yifan Zhang",
      "Dawei Xiang",
      "Runxin Wu",
      "Wei Zhang"
    ],
    "abstract": "The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "12 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26730v1",
    "published_date": "2025-10-30 17:29:27 UTC",
    "updated_date": "2025-10-30 17:29:27 UTC"
  },
  {
    "arxiv_id": "2510.26722v3",
    "title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off",
    "authors": [
      "Muhammad Faraz Ul Abrar",
      "Nicolò Michelusi"
    ],
    "abstract": "Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26722v3",
    "published_date": "2025-10-30 17:22:57 UTC",
    "updated_date": "2025-11-06 17:41:05 UTC"
  },
  {
    "arxiv_id": "2510.26721v1",
    "title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis",
    "authors": [
      "Xinhan Zheng",
      "Huyu Wu",
      "Xueting Wang",
      "Haiyun Jiang"
    ],
    "abstract": "Multimodal large language models (MLLMs) exhibit a pronounced preference for textual inputs when processing vision-language data, limiting their ability to reason effectively from visual evidence. Unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning, we propose that the bias originates from the model's internal architecture. Specifically, we hypothesize that visual key vectors (Visual Keys) are out-of-distribution (OOD) relative to the text key space learned during language-only pretraining. Consequently, these visual keys receive systematically lower similarity scores during attention computation, leading to their under-utilization in the context representation. To validate this hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their distributional structures using qualitative (t-SNE) and quantitative (Jensen-Shannon divergence) methods. The results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space. The inter-modal divergence is statistically significant, exceeding intra-modal variation by several orders of magnitude. These findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26721v1",
    "published_date": "2025-10-30 17:22:22 UTC",
    "updated_date": "2025-10-30 17:22:22 UTC"
  },
  {
    "arxiv_id": "2510.26865v1",
    "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench",
    "authors": [
      "Fenfen Lin",
      "Yesheng Liu",
      "Haiyu Xu",
      "Chen Yue",
      "Zheqi He",
      "Mingxuan Zhao",
      "Miguel Hu Chen",
      "Jiakang Liu",
      "JG Yao",
      "Xi Yang"
    ],
    "abstract": "Reading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://flageval-baai.github.io/MeasureBenchPage/",
    "pdf_url": "https://arxiv.org/pdf/2510.26865v1",
    "published_date": "2025-10-30 17:20:51 UTC",
    "updated_date": "2025-10-30 17:20:51 UTC"
  },
  {
    "arxiv_id": "2511.08595v2",
    "title": "Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning",
    "authors": [
      "Joongho Kim",
      "Xirui Huang",
      "Zarreen Reza",
      "Gabriel Grand"
    ],
    "abstract": "Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Efficient Reasoning",
    "pdf_url": "https://arxiv.org/pdf/2511.08595v2",
    "published_date": "2025-10-30 17:18:45 UTC",
    "updated_date": "2025-12-07 05:42:16 UTC"
  },
  {
    "arxiv_id": "2511.00101v1",
    "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving",
    "authors": [
      "Yuchen Zhang",
      "Hanyue Du",
      "Chun Cao",
      "Jingwei Xu"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages including 10 pages of main text, 6 figures, 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2511.00101v1",
    "published_date": "2025-10-30 17:14:27 UTC",
    "updated_date": "2025-10-30 17:14:27 UTC"
  },
  {
    "arxiv_id": "2510.26714v4",
    "title": "On the limitation of evaluating machine unlearning using only a single training seed",
    "authors": [
      "Jamie Lanyon",
      "Axel Finke",
      "Petros Andreou",
      "Georgina Cosma"
    ],
    "abstract": "Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We illustrate that this is particularly relevant for MU methods that are deterministic, i.e., which always produce the same result when started from the same trained model. We therefore recommend that empirical comparisons of MU algorithms should also reflect the variability across different model training seeds.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "mini paper, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26714v4",
    "published_date": "2025-10-30 17:13:42 UTC",
    "updated_date": "2025-12-30 19:14:10 UTC"
  },
  {
    "arxiv_id": "2510.26702v1",
    "title": "Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching",
    "authors": [
      "Majed El Helou",
      "Chiara Troiani",
      "Benjamin Ryder",
      "Jean Diaconu",
      "Hervé Muyal",
      "Marcelo Yannuzzi"
    ],
    "abstract": "Authorizing Large Language Model driven agents to dynamically invoke tools and access protected resources introduces significant risks, since current methods for delegating authorization grant overly broad permissions and give access to tools allowing agents to operate beyond the intended task scope. We introduce and assess a delegated authorization model enabling authorization servers to semantically inspect access requests to protected resources, and issue access tokens constrained to the minimal set of scopes necessary for the agents' assigned tasks. Given the unavailability of datasets centered on delegated authorization flows, particularly including both semantically appropriate and inappropriate scope requests for a given task, we introduce ASTRA, a dataset and data generation pipeline for benchmarking semantic matching between tasks and scopes. Our experiments show both the potential and current limitations of model-based matching, particularly as the number of scopes needed for task completion increases. Our results highlight the need for further research into semantic matching techniques enabling intent-aware authorization for multi-agent and tool-augmented applications, including fine-grained control, such as Task-Based Access Control (TBAC).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper page at https://outshift-open.github.io/ASTRA",
    "pdf_url": "https://arxiv.org/pdf/2510.26702v1",
    "published_date": "2025-10-30 17:07:00 UTC",
    "updated_date": "2025-10-30 17:07:00 UTC"
  },
  {
    "arxiv_id": "2510.26697v2",
    "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
    "authors": [
      "Zhichao Wang",
      "Dongyang Ma",
      "Xinting Huang",
      "Deng Cai",
      "Tian Lan",
      "Jiahao Xu",
      "Haitao Mi",
      "Xiaoying Tang",
      "Yan Wang"
    ],
    "abstract": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly \"end-to-end\" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from \"hacking the test set\"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., \"generate with low randomness\") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26697v2",
    "published_date": "2025-10-30 17:01:43 UTC",
    "updated_date": "2025-10-31 17:36:35 UTC"
  },
  {
    "arxiv_id": "2511.03743v1",
    "title": "A convolutional neural network deep learning method for model class selection",
    "authors": [
      "Marios Impraimakis"
    ],
    "abstract": "The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.SY",
    "comment": "31 pages, 16 figures, published in Earthquake Engineering & Structural Dynamics",
    "pdf_url": "https://arxiv.org/pdf/2511.03743v1",
    "published_date": "2025-10-30 16:58:15 UTC",
    "updated_date": "2025-10-30 16:58:15 UTC"
  },
  {
    "arxiv_id": "2510.26684v1",
    "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill",
    "authors": [
      "Vaibhav Kurrey",
      "Sivakalyan Pujari",
      "Gagan Raj Gupta"
    ],
    "abstract": "We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26684v1",
    "published_date": "2025-10-30 16:54:16 UTC",
    "updated_date": "2025-10-30 16:54:16 UTC"
  },
  {
    "arxiv_id": "2510.26683v1",
    "title": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models",
    "authors": [
      "Mingchen Tu",
      "Zhiqiang Liu",
      "Juan Li",
      "Liangyurui Liu",
      "Junjie Wang",
      "Lei Liang",
      "Wen Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities across multiple domains by leveraging massive pre-training and curated fine-tuning data. However, in data-sensitive fields such as healthcare, the lack of high-quality, domain-specific training corpus hinders LLMs' adaptation for specialized applications. Meanwhile, domain experts have distilled domain wisdom into ontology rules, which formalize relationships among concepts and ensure the integrity of knowledge management repositories. Viewing LLMs as implicit repositories of human knowledge, we propose Evontree, a novel framework that leverages a small set of high-quality ontology rules to systematically extract, validate, and enhance domain knowledge within LLMs, without requiring extensive external datasets. Specifically, Evontree extracts domain ontology from raw models, detects inconsistencies using two core ontology rules, and reinforces the refined knowledge via self-distilled fine-tuning. Extensive experiments on medical QA benchmarks with Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both unmodified models and leading supervised baselines, achieving up to a 3.7% improvement in accuracy. These results confirm the effectiveness, efficiency, and robustness of our approach for low-resource domain adaptation of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26683v1",
    "published_date": "2025-10-30 16:53:45 UTC",
    "updated_date": "2025-10-30 16:53:45 UTC"
  },
  {
    "arxiv_id": "2510.26658v1",
    "title": "The Era of Agentic Organization: Learning to Organize with Language Models",
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Qingxiu Dong",
      "Yaru Hao",
      "Xun Wu",
      "Shaohan Huang",
      "Furu Wei"
    ],
    "abstract": "We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26658v1",
    "published_date": "2025-10-30 16:25:10 UTC",
    "updated_date": "2025-10-30 16:25:10 UTC"
  },
  {
    "arxiv_id": "2510.26646v1",
    "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments",
    "authors": [
      "Xiaoyi He",
      "Danggui Chen",
      "Zhenshuo Zhang",
      "Zimeng Bai"
    ],
    "abstract": "This paper presents a hierarchical path-planning and control framework that combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller for continuous actuation. The high-level module selects behaviors and sub-goals; the low-level module executes smooth velocity commands. We design a practical reward shaping scheme (direction, distance, obstacle avoidance, action smoothness, collision penalty, time penalty, and progress), together with a LiDAR-based safety gate that prevents unsafe motions. The system is implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics, including success rate, collision rate, path efficiency, and re-planning efficiency, in dynamic and partially observable environments. Experiments show improved success rate and sample efficiency over single-algorithm baselines (DQN or TD3 alone) and rule-based planners, with better generalization to unseen obstacle configurations and reduced abrupt control changes. Code and evaluation scripts are available at the project repository.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation; evaluation with PathBench metrics; code (primary): https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation",
    "pdf_url": "https://arxiv.org/pdf/2510.26646v1",
    "published_date": "2025-10-30 16:12:01 UTC",
    "updated_date": "2025-10-30 16:12:01 UTC"
  },
  {
    "arxiv_id": "2511.00099v1",
    "title": "A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation",
    "authors": [
      "Marios Impraimakis",
      "Evangelia Nektaria Palkanoglou"
    ],
    "abstract": "The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 23 figures, published in Structural and Multidisciplinary Optimization",
    "pdf_url": "https://arxiv.org/pdf/2511.00099v1",
    "published_date": "2025-10-30 16:04:47 UTC",
    "updated_date": "2025-10-30 16:04:47 UTC"
  },
  {
    "arxiv_id": "2510.26616v2",
    "title": "Aeolus: A Multi-structural Flight Delay Dataset",
    "authors": [
      "Lin Xu",
      "Xinyun Yuan",
      "Yuxuan Liang",
      "Suwan Yin",
      "Yuankai Wu"
    ],
    "abstract": "We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed to advance research on flight delay prediction and support the development of foundation models for tabular data. Existing datasets in this domain are typically limited to flat tabular structures and fail to capture the spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this limitation by providing three aligned modalities: (i) a tabular dataset with rich operational, meteorological, and airportlevel features for over 50 million flights; (ii) a flight chain module that models delay propagation along sequential flight legs, capturing upstream and downstream dependencies; and (iii) a flight network graph that encodes shared aircraft, crew, and airport resource connections, enabling cross-flight relational reasoning. The dataset is carefully constructed with temporal splits, comprehensive features, and strict leakage prevention to support realistic and reproducible machine learning evaluation. Aeolus supports a broad range of tasks, including regression, classification, temporal structure modeling, and graph learning, serving as a unified benchmark across tabular, sequential, and graph modalities. We release baseline experiments and preprocessing tools to facilitate adoption. Aeolus fills a key gap for both domain-specific modeling and general-purpose structured data research.Our source code and data can be accessed at https://github.com/Flnny/Delay-data",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26616v2",
    "published_date": "2025-10-30 15:41:43 UTC",
    "updated_date": "2025-10-31 08:49:49 UTC"
  },
  {
    "arxiv_id": "2510.26855v1",
    "title": "Leveraging Foundation Models for Enhancing Robot Perception and Action",
    "authors": [
      "Reihaneh Mirjalili"
    ],
    "abstract": "This thesis investigates how foundation models can be systematically leveraged to enhance robotic capabilities, enabling more effective localization, interaction, and manipulation in unstructured environments. The work is structured around four core lines of inquiry, each addressing a fundamental challenge in robotics while collectively contributing to a cohesive framework for semantics-aware robotic intelligence.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Doctoral thesis",
    "pdf_url": "https://arxiv.org/pdf/2510.26855v1",
    "published_date": "2025-10-30 15:40:47 UTC",
    "updated_date": "2025-10-30 15:40:47 UTC"
  },
  {
    "arxiv_id": "2510.26854v3",
    "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base",
    "authors": [
      "Yu Li",
      "Yuan Huang",
      "Tao Wang",
      "Caiyu Fan",
      "Xiansheng Cai",
      "Sihan Hu",
      "Xinzijian Liu",
      "Cheng Shi",
      "Mingjun Xu",
      "Zhen Wang",
      "Yan Wang",
      "Xiangqi Jin",
      "Tianhan Zhang",
      "Linfeng Zhang",
      "Lei Wang",
      "Youjin Deng",
      "Pan Zhang",
      "Weijie Sun",
      "Xinyu Li",
      "Weinan E",
      "Linfeng Zhang",
      "Zhiyuan Yao",
      "Kun Chen"
    ],
    "abstract": "Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "43 pages, 4 figures. This work is part of the SciencePedia project (sciencepedia.bohrium.com). Corrected author name spelling",
    "pdf_url": "https://arxiv.org/pdf/2510.26854v3",
    "published_date": "2025-10-30 15:38:50 UTC",
    "updated_date": "2026-01-17 07:26:05 UTC"
  },
  {
    "arxiv_id": "2510.26606v2",
    "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives",
    "authors": [
      "Kentaro Ozeki",
      "Risako Ando",
      "Takanobu Morishita",
      "Hirohiko Abe",
      "Koji Mineshima",
      "Mitsuhiro Okada"
    ],
    "abstract": "Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.26606v2",
    "published_date": "2025-10-30 15:35:13 UTC",
    "updated_date": "2025-10-31 05:11:24 UTC"
  },
  {
    "arxiv_id": "2510.26603v1",
    "title": "Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling",
    "authors": [
      "Reda El Makroum",
      "Sebastian Zwickl-Bernhard",
      "Lukas Kranzl"
    ],
    "abstract": "The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters. While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling. This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations. A hierarchical architecture combining one orchestrator with three specialist agents uses the ReAct pattern for iterative reasoning, enabling dynamic coordination without hardcoded workflows while integrating Google Calendar for context-aware deadline extraction. Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences. Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously. Progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models' general reasoning capabilities. We open-source the complete system including orchestration logic, agent prompts, tools, and web interfaces to enable reproducibility, extension, and future research.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 9 figures. Code available at https://github.com/RedaElMakroum/agentic-ai-hems",
    "pdf_url": "https://arxiv.org/pdf/2510.26603v1",
    "published_date": "2025-10-30 15:33:52 UTC",
    "updated_date": "2025-10-30 15:33:52 UTC"
  },
  {
    "arxiv_id": "2510.26601v2",
    "title": "ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching",
    "authors": [
      "Anirban Ray",
      "Vera Galinova",
      "Florian Jug"
    ],
    "abstract": "Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26601v2",
    "published_date": "2025-10-30 15:29:20 UTC",
    "updated_date": "2025-11-21 15:25:30 UTC"
  },
  {
    "arxiv_id": "2510.26852v1",
    "title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions",
    "authors": [
      "Lingyue Fu",
      "Xin Ding",
      "Yaoming Zhu",
      "Shao Zhang",
      "Lin Qiu",
      "Weiwen Liu",
      "Weinan Zhang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Jiaxin Ding",
      "Yong Yu"
    ],
    "abstract": "Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26852v1",
    "published_date": "2025-10-30 15:22:53 UTC",
    "updated_date": "2025-10-30 15:22:53 UTC"
  },
  {
    "arxiv_id": "2510.26585v1",
    "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems",
    "authors": [
      "Fulin Lin",
      "Shaowen Chen",
      "Ruishan Fang",
      "Hongwei Wang",
      "Tao Lin"
    ],
    "abstract": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing autonomy with operational complexity often leads to critical inefficiencies, such as excessive token consumption and failures arising from misinformation. Existing methods primarily focus on post-hoc failure attribution, lacking proactive, real-time interventions to enhance robustness and efficiency. To this end, we introduce SupervisorAgent, a lightweight and modular framework for runtime, adaptive supervision that operates without altering the base agent's architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent intervenes at critical junctures to proactively correct errors, guide inefficient behaviors, and purify observations. On the challenging GAIA benchmark, SupervisorAgent reduces the token consumption of the Smolagent framework by an average of 29.45% without compromising its success rate. Extensive experiments across five additional benchmarks (math reasoning, code generation, and question answering) and various SoTA foundation models validate the broad applicability and robustness of our approach. The code is available at https://github.com/LINs-lab/SupervisorAgent.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26585v1",
    "published_date": "2025-10-30 15:12:59 UTC",
    "updated_date": "2025-10-30 15:12:59 UTC"
  },
  {
    "arxiv_id": "2511.00098v2",
    "title": "A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning",
    "authors": [
      "Nils Porsche",
      "Flurin Müller-Diesing",
      "Sweta Banerjee",
      "Miguel Goncalves",
      "Marc Aubreville"
    ],
    "abstract": "Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00098v2",
    "published_date": "2025-10-30 15:07:11 UTC",
    "updated_date": "2025-11-14 09:38:35 UTC"
  },
  {
    "arxiv_id": "2510.26575v1",
    "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
    "authors": [
      "Kun Luo",
      "Hongjin Qian",
      "Zheng Liu",
      "Ziyi Xia",
      "Shitao Xiao",
      "Siqi Bao",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \\textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \\textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \\textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \\textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \\textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \\textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26575v1",
    "published_date": "2025-10-30 15:03:21 UTC",
    "updated_date": "2025-10-30 15:03:21 UTC"
  },
  {
    "arxiv_id": "2510.26566v1",
    "title": "Multiclass Local Calibration With the Jensen-Shannon Distance",
    "authors": [
      "Cesare Barbera",
      "Lorenzo Perini",
      "Giovanni De Toni",
      "Andrea Passerini",
      "Andrea Pugnana"
    ],
    "abstract": "Developing trustworthy Machine Learning (ML) models requires their predicted probabilities to be well-calibrated, meaning they should reflect true-class frequencies. Among calibration notions in multiclass classification, strong calibration is the most stringent, as it requires all predicted probabilities to be simultaneously calibrated across all classes. However, existing approaches to multiclass calibration lack a notion of distance among inputs, which makes them vulnerable to proximity bias: predictions in sparse regions of the feature space are systematically miscalibrated. This is especially relevant in high-stakes settings, such as healthcare, where the sparse instances are exactly those most at risk of biased treatment. In this work, we address this main shortcoming by introducing a local perspective on multiclass calibration. First, we formally define multiclass local calibration and establish its relationship with strong calibration. Second, we theoretically analyze the pitfalls of existing evaluation metrics when applied to multiclass local calibration. Third, we propose a practical method for enhancing local calibration in Neural Networks, which enforces alignment between predicted probabilities and local estimates of class frequencies using the Jensen-Shannon distance. Finally, we empirically validate our approach against existing multiclass calibration techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26566v1",
    "published_date": "2025-10-30 14:56:07 UTC",
    "updated_date": "2025-10-30 14:56:07 UTC"
  },
  {
    "arxiv_id": "2510.26551v1",
    "title": "Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics",
    "authors": [
      "Prathamesh Kothavale",
      "Sravani Boddepalli"
    ],
    "abstract": "Conventional robots possess a limited understanding of their kinematics and are confined to preprogrammed tasks, hindering their ability to leverage tools efficiently. Driven by the essential components of tool usage - grasping the desired outcome, selecting the most suitable tool, determining optimal tool orientation, and executing precise manipulations - we introduce a pioneering framework. Our novel approach expands the capabilities of the robot's inverse kinematics solver, empowering it to acquire a sequential repertoire of actions using tools of varying lengths. By integrating a simulation-learned action trajectory with the tool, we showcase the practicality of transferring acquired skills from simulation to real-world scenarios through comprehensive experimentation. Remarkably, our extended inverse kinematics solver demonstrates an impressive error rate of less than 1 cm. Furthermore, our trained policy achieves a mean error of 8 cm in simulation. Noteworthy, our model achieves virtually indistinguishable performance when employing two distinct tools of different lengths. This research provides an indication of potential advances in the exploration of all four fundamental aspects of tool usage, enabling robots to master the intricate art of tool manipulation across diverse tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 5 figures. Demonstrates a reinforcement learning framework for adaptive tool manipulation with variable-length extensions",
    "pdf_url": "https://arxiv.org/pdf/2510.26551v1",
    "published_date": "2025-10-30 14:44:24 UTC",
    "updated_date": "2025-10-30 14:44:24 UTC"
  },
  {
    "arxiv_id": "2510.26550v2",
    "title": "EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge",
    "authors": [
      "Jack FitzGerald",
      "Aristotelis Lazaridis",
      "Dylan Bates",
      "Aman Sharma",
      "Jonnathan Castillo",
      "Yousif Azami",
      "Sean Bailey",
      "Jeremy Cao",
      "Peter Damianov",
      "Kevin de Haan",
      "Luke Kerbs",
      "Vincent Lu",
      "Joseph Madigan",
      "Jeremy McLaurin",
      "Jonathan Tainer",
      "Dave Anderson",
      "Jonathan Beck",
      "Jamie Cuticello",
      "Colton Malkerson",
      "Tyler Saltsman"
    ],
    "abstract": "We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated from military documentation and websites. We also present four new tests sets: (a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k (general military knowledge). On these military test sets, EdgeRunner 20B matches or exceeds GPT-5 task performance with 95%+ statistical significance, except for the high reasoning setting on the combat medic test set and the low reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no statistically-significant regression on general-purpose benchmarks like ARC-C, GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the low reasoning setting. We also present analyses on hyperparameter settings, cost, and throughput. These findings show that small, locally-hosted models are ideal solutions for data-sensitive operations such as in the military domain, allowing for deployment in air-gapped edge devices.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages; v2 includes an additional appendix with test set examples",
    "pdf_url": "https://arxiv.org/pdf/2510.26550v2",
    "published_date": "2025-10-30 14:43:26 UTC",
    "updated_date": "2025-11-11 12:37:09 UTC"
  },
  {
    "arxiv_id": "2510.26543v1",
    "title": "The Structure of Relation Decoding Linear Operators in Large Language Models",
    "authors": [
      "Miranda Anna Christ",
      "Adrián Csiszárik",
      "Gergely Becsó",
      "Dániel Varga"
    ],
    "abstract": "This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.26543v1",
    "published_date": "2025-10-30 14:36:09 UTC",
    "updated_date": "2025-10-30 14:36:09 UTC"
  },
  {
    "arxiv_id": "2510.26518v1",
    "title": "Human-AI Complementarity: A Goal for Amplified Oversight",
    "authors": [
      "Rishub Jain",
      "Sophie Bridgers",
      "Lili Janzer",
      "Rory Greig",
      "Tian Huey Teh",
      "Vladimir Mikulik"
    ],
    "abstract": "Human feedback is critical for aligning AI systems to human values. As AI capabilities improve and AI is used to tackle more challenging tasks, verifying quality and safety becomes increasingly challenging. This paper explores how we can leverage AI to improve the quality of human oversight. We focus on an important safety problem that is already challenging for humans: fact-verification of AI outputs. We find that combining AI ratings and human ratings based on AI rater confidence is better than relying on either alone. Giving humans an AI fact-verification assistant further improves their accuracy, but the type of assistance matters. Displaying AI explanation, confidence, and labels leads to over-reliance, but just showing search results and evidence fosters more appropriate trust. These results have implications for Amplified Oversight -- the challenge of combining humans and AI to supervise AI systems even as they surpass human expert performance.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26518v1",
    "published_date": "2025-10-30 14:11:52 UTC",
    "updated_date": "2025-10-30 14:11:52 UTC"
  },
  {
    "arxiv_id": "2510.26512v2",
    "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs",
    "authors": [
      "Dipak Meher",
      "Carlotta Domeniconi"
    ],
    "abstract": "Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.25% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.29% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICDM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.26512v2",
    "published_date": "2025-10-30 14:05:55 UTC",
    "updated_date": "2025-11-08 21:37:40 UTC"
  },
  {
    "arxiv_id": "2510.26494v1",
    "title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents",
    "authors": [
      "Sadegh Shirani",
      "Mohsen Bayati"
    ],
    "abstract": "Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26494v1",
    "published_date": "2025-10-30 13:43:28 UTC",
    "updated_date": "2025-10-30 13:43:28 UTC"
  },
  {
    "arxiv_id": "2510.26493v1",
    "title": "Context Engineering 2.0: The Context of Context Engineering",
    "authors": [
      "Qishuo Hua",
      "Lyumanshan Ye",
      "Dayuan Fu",
      "Yang Xiao",
      "Xiaojie Cai",
      "Yunze Wu",
      "Jifan Lin",
      "Junfei Wang",
      "Pengfei Liu"
    ],
    "abstract": "Karl Marx once wrote that ``the human essence is the ensemble of social relations'', suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities, within which contexts play a constitutive and essential role. With the advent of computers and artificial intelligence, these contexts are no longer limited to purely human--human interactions: human--machine interactions are included as well. Then a central question emerges: How can machines better understand our situations and purposes? To address this challenge, researchers have recently introduced the concept of context engineering. Although it is often regarded as a recent innovation of the agent era, we argue that related practices can be traced back more than twenty years. Since the early 1990s, the field has evolved through distinct historical phases, each shaped by the intelligence level of machines: from early human--computer interaction frameworks built around primitive computers, to today's human--agent interaction paradigms driven by intelligent agents, and potentially to human--level or superhuman intelligence in the future. In this paper, we situate context engineering, provide a systematic definition, outline its historical and conceptual landscape, and examine key design considerations for practice. By addressing these questions, we aim to offer a conceptual foundation for context engineering and sketch its promising future. This paper is a stepping stone for a broader community effort toward systematic context engineering in AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26493v1",
    "published_date": "2025-10-30 13:43:10 UTC",
    "updated_date": "2025-10-30 13:43:10 UTC"
  },
  {
    "arxiv_id": "2510.26486v1",
    "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks",
    "authors": [
      "Dipak Meher",
      "Carlotta Domeniconi",
      "Guadalupe Correa-Cabrera"
    ],
    "abstract": "Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26486v1",
    "published_date": "2025-10-30 13:39:08 UTC",
    "updated_date": "2025-10-30 13:39:08 UTC"
  },
  {
    "arxiv_id": "2510.26484v1",
    "title": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis",
    "authors": [
      "Rasoul Amirzadeh",
      "Dhananjay Thiruvady",
      "Fatemeh Shiri"
    ],
    "abstract": "Large language models (LLMs) continue to advance, with an increasing number of domain-specific variants tailored for specialised tasks. However, these models often lack transparency and explainability, can be costly to fine-tune, require substantial prompt engineering, yield inconsistent results across domains, and impose significant adverse environmental impact due to their high computational demands. To address these challenges, we propose the Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three human-annotated financial corpora with distinct linguistic and contextual characteristics, BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs, underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26484v1",
    "published_date": "2025-10-30 13:37:58 UTC",
    "updated_date": "2025-10-30 13:37:58 UTC"
  },
  {
    "arxiv_id": "2510.26481v1",
    "title": "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections",
    "authors": [
      "Clarissa Sabrina Arlinghaus",
      "Tristan Kenneweg",
      "Barbara Hammer",
      "Günter W. Maier"
    ],
    "abstract": "Large language models (LLMs) such as ChatGPT are increasingly integrated into high-stakes decision-making, yet little is known about their susceptibility to social influence. We conducted three preregistered conformity experiments with GPT-4o in a hiring context. In a baseline study, GPT consistently favored the same candidate (Profile C), reported moderate expertise (M = 3.01) and high certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT faced unanimous opposition from eight simulated partners and almost always conformed (99.9%), reporting lower certainty and significantly elevated self-reported informational and normative conformity (p < .001). In Study 2 (GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of disagreement trials, reporting less certainty and more normative conformity. Across studies, results demonstrate that GPT does not act as an independent observer but adapts to perceived social consensus. These findings highlight risks of treating LLMs as neutral decision aids and underline the need to elicit AI judgments prior to exposing them to human opinions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 5 figures, HAI 2025: Workshop on Socially Aware and Cooperative Intelligent Systems",
    "pdf_url": "https://arxiv.org/pdf/2510.26481v1",
    "published_date": "2025-10-30 13:35:32 UTC",
    "updated_date": "2025-10-30 13:35:32 UTC"
  },
  {
    "arxiv_id": "2510.26474v1",
    "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing",
    "authors": [
      "Xin Guo",
      "Zhiheng Xi",
      "Yiwen Ding",
      "Yitao Zhai",
      "Xiaowei Shi",
      "Xunliang Cai",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew effect\"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.26474v1",
    "published_date": "2025-10-30 13:26:58 UTC",
    "updated_date": "2025-10-30 13:26:58 UTC"
  },
  {
    "arxiv_id": "2511.00097v1",
    "title": "GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation",
    "authors": [
      "Zihao Guo",
      "Qingyun Sun",
      "Ziwei Zhang",
      "Haonan Yuan",
      "Huiping Zhuang",
      "Xingcheng Fu",
      "Jianxin Li"
    ],
    "abstract": "Graph incremental learning (GIL), which continuously updates graph models by sequential knowledge acquisition, has garnered significant interest recently. However, existing GIL approaches focus on task-incremental and class-incremental scenarios within a single domain. Graph domain-incremental learning (Domain-IL), aiming at updating models across multiple graph domains, has become critical with the development of graph foundation models (GFMs), but remains unexplored in the literature. In this paper, we propose Graph Domain-Incremental Learning via Knowledge Dientanglement and Preservation (GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from the perspectives of embedding shifts and decision boundary deviations. Specifically, to prevent embedding shifts and confusion across incremental graph domains, we first propose the domain-specific parameter-efficient fine-tuning together with intra- and inter-domain disentanglement objectives. Consequently, to maintain a stable decision boundary, we introduce deviation-free knowledge preservation to continuously fit incremental domains. Additionally, for graphs with unobservable domains, we perform domain-aware distribution discrimination to obtain precise embeddings. Extensive experiments demonstrate the proposed GraphKeeper achieves state-of-the-art results with 6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover, we show GraphKeeper can be seamlessly integrated with various representative GFMs, highlighting its broad applicative potential.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the Main Track of NeurIPS-2025",
    "pdf_url": "https://arxiv.org/pdf/2511.00097v1",
    "published_date": "2025-10-30 13:14:51 UTC",
    "updated_date": "2025-10-30 13:14:51 UTC"
  },
  {
    "arxiv_id": "2510.26457v1",
    "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning",
    "authors": [
      "Fang Liu",
      "Simiao Liu",
      "Yinghao Zhu",
      "Xiaoli Lian",
      "Li Zhang"
    ],
    "abstract": "Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ICSE 2026. Code and data: https://github.com/SIMIAO515/SecureReviewer",
    "pdf_url": "https://arxiv.org/pdf/2510.26457v1",
    "published_date": "2025-10-30 13:06:11 UTC",
    "updated_date": "2025-10-30 13:06:11 UTC"
  },
  {
    "arxiv_id": "2510.26451v2",
    "title": "Robust Graph Condensation via Classification Complexity Mitigation",
    "authors": [
      "Jiayi Luo",
      "Qingyun Sun",
      "Beining Yang",
      "Haonan Yuan",
      "Xingcheng Fu",
      "Yanbiao Ma",
      "Jianxin Li",
      "Philip S. Yu"
    ],
    "abstract": "Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel Manifold-constrained Robust Graph Condensation framework named MRGC. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of \\ModelName\\ across diverse attack scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Neurips 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.26451v2",
    "published_date": "2025-10-30 12:55:21 UTC",
    "updated_date": "2025-11-22 09:10:45 UTC"
  },
  {
    "arxiv_id": "2510.26444v1",
    "title": "Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion",
    "authors": [
      "Wenjie Chen",
      "Li Zhuang",
      "Ziying Luo",
      "Yu Liu",
      "Jiahao Wu",
      "Shengcai Liu"
    ],
    "abstract": "Personalized treatment outcome prediction based on trial data for small-sample and rare patient groups is critical in precision medicine. However, the costly trial data limit the prediction performance. To address this issue, we propose a cross-fidelity knowledge distillation and adaptive fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN incorporates a dual-channel knowledge distillation module to extract complementary knowledge from the low-fidelity model, along with an attention-guided fusion module to dynamically integrate multi-source information. Experiments on treatment outcome prediction for the chronic obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN over state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to 74.55\\%, and strong robustness to varying high-fidelity dataset sizes. Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the exploration of latent medical semantics to support clinical decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26444v1",
    "published_date": "2025-10-30 12:50:12 UTC",
    "updated_date": "2025-10-30 12:50:12 UTC"
  },
  {
    "arxiv_id": "2510.26847v1",
    "title": "Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token",
    "authors": [
      "Shaked Zychlinski",
      "Yuval Kainan"
    ],
    "abstract": "Large Language Models (LLMs) are susceptible to jailbreak attacks where malicious prompts are disguised using ciphers and character-level encodings to bypass safety guardrails. While these guardrails often fail to interpret the encoded content, the underlying models can still process the harmful instructions. We introduce CPT-Filtering, a novel, model-agnostic with negligible-costs and near-perfect accuracy guardrail technique that aims to mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair Encoding (BPE) tokenizers. Our method is based on the principle that tokenizers, trained on natural language, represent out-of-distribution text, such as ciphers, using a significantly higher number of shorter tokens. Our technique uses a simple yet powerful artifact of using language models: the average number of Characters Per Token (CPT) in the text. This approach is motivated by the high compute cost of modern methods - relying on added modules such as dedicated LLMs or perplexity models. We validate our approach across a large dataset of over 100,000 prompts, testing numerous encoding schemes with several popular tokenizers. Our experiments demonstrate that a simple CPT threshold robustly identifies encoded text with high accuracy, even for very short inputs. CPT-Filtering provides a practical defense layer that can be immediately deployed for real-time text filtering and offline data curation.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.IT"
    ],
    "primary_category": "cs.CR",
    "comment": "16 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26847v1",
    "published_date": "2025-10-30 12:42:45 UTC",
    "updated_date": "2025-10-30 12:42:45 UTC"
  },
  {
    "arxiv_id": "2511.11600v1",
    "title": "CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models",
    "authors": [
      "Piyushkumar Patel"
    ],
    "abstract": "While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This \"hallucination\" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.\n  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.\n  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\\% of the time while missing only 8.3\\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.11600v1",
    "published_date": "2025-10-30 12:41:51 UTC",
    "updated_date": "2025-10-30 12:41:51 UTC"
  },
  {
    "arxiv_id": "2512.08942v1",
    "title": "Beyond Technical Debt: How AI-Assisted Development Creates Comprehension Debt in Resource-Constrained Indie Teams",
    "authors": [
      "Yujie Zhang"
    ],
    "abstract": "Junior indie game developers in distributed, part-time teams lack production frameworks suited to their specific context, as traditional methodologies are often inaccessible. This study introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, an alternative approach for integrating AI tools to address persistent challenges of technical debt, coordination, and burnout.\n  The framework emerged from a three-month reflective practice and autoethnographic study of a three-person distributed team developing the 2D narrative game \"The Worm's Memoirs\". Based on analysis of development data (N=157 Jira tasks, N=333 GitHub commits, N=13+ Miro boards, N=8 reflection sessions), CIGDI is proposed as a seven-stage iterative process structured around human-in-the-loop decision points (Priority Criteria and Timeboxing).\n  While AI support democratized knowledge access and reduced cognitive load, our analysis identified a significant challenge: \"comprehension debt.\" We define this as a novel form of technical debt where AI helps teams build systems more sophisticated than their independent skill level can create or maintain. This paradox (possessing functional systems the team incompletely understands) creates fragility and AI dependency, distinct from traditional code quality debt.\n  This work contributes a practical production framework for resource-constrained teams and identifies critical questions about whether AI assistance constitutes a learning ladder or a dependency trap for developer skill.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08942v1",
    "published_date": "2025-10-30 12:41:26 UTC",
    "updated_date": "2025-10-30 12:41:26 UTC"
  },
  {
    "arxiv_id": "2511.05540v2",
    "title": "Token Is All You Need: Cognitive Planning through Belief-Intent Co-Evolution",
    "authors": [
      "Shiyao Sang"
    ],
    "abstract": "We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Inspired by cognitive science, we propose that effective planning arises not from reconstructing the world, but from the co-evolution of belief and intent within a minimal set of semantically rich tokens. Experiments on the nuPlan benchmark (720 scenarios, 11k+ samples) reveal three principles: (1) sparse intent tokens alone achieve 0.487 m ADE, demonstrating strong performance without future prediction; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.382 m, a 21.6% improvement, showing that performance emerges from cognitive planning; and (3) explicit reconstruction loss degrades performance, confirming that task-driven belief-intent co-evolution suffices under reliable perception inputs. Crucially, we observe the emergence of cognitive consistency: through prolonged training, the model spontaneously develops stable token dynamics that balance current perception (belief) and future goals (intent). This process, accompanied by \"temporal fuzziness,\" enables robustness under uncertainty and continuous self-optimization. Our work establishes a new paradigm: intelligence lies not in pixel fidelity, but in the tokenized duality of belief and intent. By reframing planning as understanding rather than reaction, TIWM bridges the gap between world models and VLA systems, paving the way for foresightful agents that plan through imagination. Note: Numerical comparisons with methods reporting results on nuScenes are indicative only, as nuPlan presents a more challenging planning-focused evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 3 figures. A paradigm shift from reconstructing the world to understanding it: planning through belief-intent co-evolution",
    "pdf_url": "https://arxiv.org/pdf/2511.05540v2",
    "published_date": "2025-10-30 12:16:45 UTC",
    "updated_date": "2025-11-11 18:17:53 UTC"
  },
  {
    "arxiv_id": "2510.26420v1",
    "title": "SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification",
    "authors": [
      "Yingjia Wang",
      "Ting Qiao",
      "Xing Liu",
      "Chongzuo Li",
      "Sixing Wu",
      "Jianbin Li"
    ],
    "abstract": "The rapid advancement of deep neural networks (DNNs) heavily relies on large-scale, high-quality datasets. However, unauthorized commercial use of these datasets severely violates the intellectual property rights of dataset owners. Existing backdoor-based dataset ownership verification methods suffer from inherent limitations: poison-label watermarks are easily detectable due to label inconsistencies, while clean-label watermarks face high technical complexity and failure on high-resolution images. Moreover, both approaches employ static watermark patterns that are vulnerable to detection and removal. To address these issues, this paper proposes a sample-specific clean-label backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked sample generator, this method generates unique watermarks for each sample, fundamentally overcoming the vulnerability of static watermark patterns. The core innovation lies in designing a composite loss function with three components: target sample loss ensures watermark effectiveness, non-target sample loss guarantees trigger reliability, and perceptual similarity loss maintains visual imperceptibility. During ownership verification, black-box testing is employed to check whether suspicious models exhibit predefined backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method and its robustness against potential watermark removal attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages,9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26420v1",
    "published_date": "2025-10-30 12:13:53 UTC",
    "updated_date": "2025-10-30 12:13:53 UTC"
  },
  {
    "arxiv_id": "2510.26418v2",
    "title": "Chain-of-Thought Hijacking",
    "authors": [
      "Jianli Zhao",
      "Tingchen Fu",
      "Rylan Schaeffer",
      "Mrinank Sharma",
      "Fazl Barez"
    ],
    "abstract": "Large reasoning models (LRMs) achieve higher task performance with more inference-time computation, and prior works suggest this scaled reasoning may also strengthen safety by improving refusal. Yet we find the opposite: the same reasoning can be used to bypass safeguards. We introduce Chain-of-Thought Hijacking, a jailbreak attack on reasoning models. The attack pads harmful requests with long sequences of harmless puzzle reasoning. Across HarmBench, CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively - far exceeding prior jailbreak methods for LRMs. To understand the effectiveness of our attack, we turn to a mechanistic analysis, which shows that mid layers encode the strength of safety checking, while late layers encode the verification outcome. Long benign CoT dilutes both signals by shifting attention away from harmful tokens. Targeted ablations of attention heads identified by this analysis causally decrease refusal, confirming their role in a safety subnetwork. These results show that the most interpretable form of reasoning - explicit CoT - can itself become a jailbreak vector when combined with final-answer cues. We release prompts, outputs, and judge decisions to facilitate replication.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26418v2",
    "published_date": "2025-10-30 12:10:03 UTC",
    "updated_date": "2025-11-11 14:33:12 UTC"
  },
  {
    "arxiv_id": "2510.26412v1",
    "title": "LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation",
    "authors": [
      "Xiangqing Zheng",
      "Chengyue Wu",
      "Kehai Chen",
      "Min Zhang"
    ],
    "abstract": "Recently text-to-video generation has made impressive progress in producing short, high-quality clips, but evaluating long-form outputs remains a major challenge especially when processing complex prompts. Existing benchmarks mostly rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions such as narrative coherence and thematic expression. To address these gaps, we propose LoCoT2V-Bench, a benchmark specifically designed for long video generation (LVG) under complex input conditions. Based on various real-world videos, LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating elements like scene transitions and event dynamics. Moreover, it constructs a multi-dimensional evaluation framework that includes our newly proposed metrics such as event-level alignment, fine-grained temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD) that focuses on more abstract attributes like narrative flow, emotional response, and character development. Using this framework, we conduct a comprehensive evaluation of nine representative LVG models, finding that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence, etc. Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26412v1",
    "published_date": "2025-10-30 12:00:46 UTC",
    "updated_date": "2025-10-30 12:00:46 UTC"
  },
  {
    "arxiv_id": "2510.26411v1",
    "title": "MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders",
    "authors": [
      "Riccardo Renzulli",
      "Colas Lepoutre",
      "Enrico Cassano",
      "Marco Grangetto"
    ],
    "abstract": "Artificial intelligence in healthcare requires models that are accurate and interpretable. We advance mechanistic interpretability in medical vision by applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP, a vision-language model trained on chest radiographs and reports. To quantify interpretability, we propose an evaluation framework that combines correlation metrics, entropy analyzes, and automated neuron naming via the MedGEMMA foundation model. Experiments on the CheXpert dataset show that MedSAE neurons achieve higher monosemanticity and interpretability than raw MedCLIP features. Our findings bridge high-performing medical AI and transparency, offering a scalable step toward clinically reliable representations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26411v1",
    "published_date": "2025-10-30 11:58:36 UTC",
    "updated_date": "2025-10-30 11:58:36 UTC"
  },
  {
    "arxiv_id": "2512.10961v1",
    "title": "AI as Cognitive Amplifier: Rethinking Human Judgment in the Age of Generative AI",
    "authors": [
      "Tao An"
    ],
    "abstract": "Through extensive experience training professionals and individual users in AI tool adoption since the GPT-3 era, I have observed a consistent pattern: the same AI tool produces dramatically different results depending on who uses it. While some frame AI as a replacement for human intelligence, and others warn of cognitive decline, this position paper argues for a third perspective grounded in practical observation: AI as a cognitive amplifier that magnifies existing human capabilities rather than substituting for them. Drawing on research in human-computer interaction, cognitive augmentation theory, and educational technology, alongside field observations from corporate training across writing, software development, and data analysis domains, I present a framework positioning AI tools as intelligence amplification systems where output quality depends fundamentally on user expertise and judgment. Through analysis of empirical studies on expert-novice differences and systematic observations from professional training contexts, I demonstrate that domain knowledge, quality judgment, and iterative refinement capabilities create substantial performance gaps between users. I propose a three-level model of AI engagement -- from passive acceptance through iterative collaboration to cognitive direction -- and argue that the transition between levels requires not technical training but development of domain expertise and metacognitive skills. This position has critical implications for workforce development and AI system design. Rather than focusing solely on AI literacy or technical prompt engineering, I advocate for integrated approaches that strengthen domain expertise, evaluative judgment, and reflective practice.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, 7 figures. Position paper based on field observations from training 500+ professionals since 2023",
    "pdf_url": "https://arxiv.org/pdf/2512.10961v1",
    "published_date": "2025-10-30 11:55:34 UTC",
    "updated_date": "2025-10-30 11:55:34 UTC"
  },
  {
    "arxiv_id": "2510.26406v1",
    "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation",
    "authors": [
      "Guanxing Lu",
      "Rui Zhao",
      "Haitao Lin",
      "He Zhang",
      "Yansong Tang"
    ],
    "abstract": "Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26406v1",
    "published_date": "2025-10-30 11:53:08 UTC",
    "updated_date": "2025-10-30 11:53:08 UTC"
  },
  {
    "arxiv_id": "2510.26402v1",
    "title": "Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education",
    "authors": [
      "Vikrant Sahu",
      "Gagan Raj Gupta",
      "Raghav Borikar",
      "Nitin Mane"
    ],
    "abstract": "The rapid growth of programming education has outpaced traditional assessment tools, leaving faculty with limited means to provide meaningful, scalable feedback. Conventional autograders, while efficient, act as black-box systems that simply return pass/fail results, offering little insight into student thinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process to a formative learning experience. It introduces two key capabilities: automated feedback generation using a fine-tuned Large Language Model, and visualization of student code submissions to uncover learning patterns. The model is fine-tuned on curated student code and expert feedback to ensure pedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks, the system produced feedback with strong semantic alignment to instructor comments. For visualization, contrastively learned code embeddings trained on 1,000 annotated submissions enable grouping solutions into meaningful clusters based on functionality and approach. The system also supports prompt-pooling, allowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive visualization, Autograder+ reduces instructor workload while supporting targeted instruction and promoting stronger learning outcomes.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26402v1",
    "published_date": "2025-10-30 11:41:50 UTC",
    "updated_date": "2025-10-30 11:41:50 UTC"
  },
  {
    "arxiv_id": "2510.26396v1",
    "title": "A Pragmatic View of AI Personhood",
    "authors": [
      "Joel Z. Leibo",
      "Alexander Sasha Vezhnevets",
      "William A. Cunningham",
      "Stanley M. Bileschi"
    ],
    "abstract": "The emergence of agentic Artificial Intelligence (AI) is set to trigger a \"Cambrian explosion\" of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target \"individual\" that can be sanctioned -- without needing to resolve intractable debates about an AI's consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both \"personhood as a problem\", where design choices can create \"dark patterns\" that exploit human social heuristics, and \"personhood as a solution\", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "40 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26396v1",
    "published_date": "2025-10-30 11:36:34 UTC",
    "updated_date": "2025-10-30 11:36:34 UTC"
  },
  {
    "arxiv_id": "2510.26390v1",
    "title": "SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for Multi-Organ Segmentation",
    "authors": [
      "Xizhi Tian",
      "Changjun Zhou",
      "Yulin. Yang"
    ],
    "abstract": "Multi-organ segmentation is a critical task in computer-aided diagnosis. While recent deep learning methods have achieved remarkable success in image segmentation, huge variations in organ size and shape challenge their effectiveness in multi-organ segmentation. To address these challenges, we propose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel two-stage segmentation paradigm designed to improve multi-organ segmentation accuracy. Our SPG-CDENet consists of two key components: a spatial prior network and a cross dual encoder network. The prior network generates coarse localization maps that delineate the approximate ROI, serving as spatial guidance for the dual encoder network. The cross dual encoder network comprises four essential components: a global encoder, a local encoder, a symmetric cross-attention module, and a flow-based decoder. The global encoder captures global semantic features from the entire image, while the local encoder focuses on features from the prior network. To enhance the interaction between the global and local encoders, a symmetric cross-attention module is proposed across all layers of the encoders to fuse and refine features. Furthermore, the flow-based decoder directly propagates high-level semantic features from the final encoder layer to all decoder layers, maximizing feature preservation and utilization. Extensive qualitative and quantitative experiments on two public datasets demonstrate the superior performance of SPG-CDENet compared to existing segmentation methods. Furthermore, ablation studies further validate the effectiveness of the proposed modules in improving segmentation accuracy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26390v1",
    "published_date": "2025-10-30 11:33:29 UTC",
    "updated_date": "2025-10-30 11:33:29 UTC"
  },
  {
    "arxiv_id": "2510.26384v1",
    "title": "Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings",
    "authors": [
      "Andrew M. Bean",
      "Nabeel Seedat",
      "Shengzhuang Chen",
      "Jonathan Richard Schwarz"
    ],
    "abstract": "The prohibitive cost of evaluating large language models (LLMs) on comprehensive benchmarks necessitates the creation of small yet representative data subsets (i.e., tiny benchmarks) that enable efficient assessment while retaining predictive fidelity. Current methods for this task operate under a model-centric paradigm, selecting benchmarking items based on the collective performance of existing models. Such approaches are limited by large upfront costs, an inability to immediately handle new benchmarks (`cold-start'), and the fragile assumption that future models will share the failure patterns of their predecessors. In this work, we challenge this paradigm and propose a item-centric approach to benchmark subset selection, arguing that selection should be based on the intrinsic properties of the task items themselves, rather than on model-specific failure patterns. We instantiate this item-centric efficient benchmarking approach via a novel method, Scales++, where data selection is based on the cognitive demands of the benchmark samples. Empirically, we show Scales++ reduces the upfront selection cost by over 18x while achieving competitive predictive fidelity. On the Open LLM Leaderboard, using just a 0.5\\% data subset, we predict full benchmark scores with a 2.9% mean absolute error. We demonstrate that this item-centric approach enables more efficient model evaluation without significant fidelity degradation, while also providing better cold-start performance and more interpretable benchmarking.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.26384v1",
    "published_date": "2025-10-30 11:28:58 UTC",
    "updated_date": "2025-10-30 11:28:58 UTC"
  },
  {
    "arxiv_id": "2510.26380v1",
    "title": "AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory",
    "authors": [
      "Yuanhang Liu",
      "Beichen Wang",
      "Peng Li",
      "Yang Liu"
    ],
    "abstract": "Artificial intelligence (AI) has demonstrated impressive progress in mathematical reasoning, yet its integration into the practice of mathematical research remains limited. In this study, we investigate how the AI Mathematician (AIM) system can operate as a research partner rather than a mere problem solver. Focusing on a challenging problem in homogenization theory, we analyze the autonomous reasoning trajectories of AIM and incorporate targeted human interventions to structure the discovery process. Through iterative decomposition of the problem into tractable subgoals, selection of appropriate analytical methods, and validation of intermediate results, we reveal how human intuition and machine computation can complement one another. This collaborative paradigm enhances the reliability, transparency, and interpretability of the resulting proofs, while retaining human oversight for formal rigor and correctness. The approach leads to a complete and verifiable proof, and more broadly, demonstrates how systematic human-AI co-reasoning can advance the frontier of mathematical discovery.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "52 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.26380v1",
    "published_date": "2025-10-30 11:22:15 UTC",
    "updated_date": "2025-10-30 11:22:15 UTC"
  },
  {
    "arxiv_id": "2510.26374v2",
    "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning",
    "authors": [
      "Qianli Shen",
      "Daoyuan Chen",
      "Yilun Huang",
      "Zhenqing Ling",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that are either trivial or unsolvable, while existing task selection methods often suffer from high rollout costs, poor adaptivity, or incomplete evidence. We introduce BOTS, a unified framework for Bayesian Online Task Selection in LLM reinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior estimates of task difficulty as the model evolves. It jointly incorporates explicit evidence from direct evaluations of selected tasks and implicit evidence inferred from these evaluations for unselected tasks, with Thompson sampling ensuring a principled balance between exploration and exploitation. To make implicit evidence practical, we instantiate it with an ultra-light interpolation-based plug-in that estimates difficulties of unevaluated tasks without extra rollouts, adding negligible overhead. Empirically, across diverse domains and LLM scales, BOTS consistently improves data efficiency and performance over baselines and ablations, providing a practical and extensible solution for dynamic task selection in RFT.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26374v2",
    "published_date": "2025-10-30 11:15:23 UTC",
    "updated_date": "2025-11-06 09:27:20 UTC"
  },
  {
    "arxiv_id": "2510.26352v1",
    "title": "The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration",
    "authors": [
      "Kotaro Furuya",
      "Yuichi Kitagawa"
    ],
    "abstract": "While a multi-agent approach based on large language models (LLMs) represents a promising strategy to surpass the capabilities of single models, its success is critically dependent on synergistic team composition. However, forming optimal teams is a significant challenge, as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration. In this paper, we propose an interaction-centric framework for automatic team composition that does not require any prior knowledge including their internal architectures, training data, or task performances. Our method constructs a \"language model graph\" that maps relationships between models from the semantic coherence of pairwise conversations, and then applies community detection to identify synergistic model clusters. Our experiments with diverse LLMs demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations. Priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manually-curated teams based on known model specializations. Our findings provide a new basis for the automated design of collaborative multi-agent LLM teams.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26352v1",
    "published_date": "2025-10-30 11:04:15 UTC",
    "updated_date": "2025-10-30 11:04:15 UTC"
  },
  {
    "arxiv_id": "2510.26347v1",
    "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle",
    "authors": [
      "Sebastian Zieglmeier",
      "Niklas Erdmann",
      "Narada D. Warakagoda"
    ],
    "abstract": "Reinforcement learning (RL) algorithms are designed to optimize problem-solving by learning actions that maximize rewards, a task that becomes particularly challenging in random and nonstationary environments. Even advanced RL algorithms are often limited in their ability to solve problems in these conditions. In applications such as searching for underwater pollution clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate reward-sparse environments, where actions frequently result in a zero reward. This paper aims to address these challenges by revisiting and modifying classical RL approaches to efficiently operate in sparse, randomized, and nonstationary environments. We systematically study a large number of modifications, including hierarchical algorithm changes, multigoal learning, and the integration of a location memory as an external output filter to prevent state revisits. Our results demonstrate that a modified Monte Carlo-based approach significantly outperforms traditional Q-learning and two exhaustive search patterns, illustrating its potential in adapting RL to complex environments. These findings suggest that reinforcement learning approaches can be effectively adapted for use in random, nonstationary, and reward-sparse environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26347v1",
    "published_date": "2025-10-30 10:55:05 UTC",
    "updated_date": "2025-10-30 10:55:05 UTC"
  },
  {
    "arxiv_id": "2510.26346v1",
    "title": "Discovering State Equivalences in UCT Search Trees By Action Pruning",
    "authors": [
      "Robin Schmöcker",
      "Alexander Dockhorn",
      "Bodo Rosenhahn"
    ],
    "abstract": "One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its sample efficiency by grouping/abstracting states or state-action pairs and sharing statistics within a group. Though state-action pair abstractions are mostly easy to find in algorithms such as On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are found in either noisy or large action space settings due to constraining conditions. We provide theoretical and empirical evidence for this claim, and we slightly alleviate this state abstraction problem by proposing a weaker state abstraction condition that trades a minor loss in accuracy for finding many more abstractions. We name this technique Ideal Pruning Abstractions in UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a large range of test domains and iteration budgets as experimentally validated. IPA-UCT uses a different abstraction framework from Abstraction of State-Action Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore, we show that both IPA and ASAP are special cases of a more general framework that we call p-ASAP which itself is a special case of the ASASAP framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26346v1",
    "published_date": "2025-10-30 10:54:43 UTC",
    "updated_date": "2025-10-30 10:54:43 UTC"
  },
  {
    "arxiv_id": "2510.26345v1",
    "title": "MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data",
    "authors": [
      "Mykhailo Poliakov",
      "Nadiya Shvai"
    ],
    "abstract": "Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on https://github.com/mxpoliakov/MisSynth.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26345v1",
    "published_date": "2025-10-30 10:52:43 UTC",
    "updated_date": "2025-10-30 10:52:43 UTC"
  },
  {
    "arxiv_id": "2510.26342v1",
    "title": "Linear Causal Discovery with Interventional Constraints",
    "authors": [
      "Zhigao Guo",
      "Feng Dong"
    ],
    "abstract": "Incorporating causal knowledge and mechanisms is essential for refining causal models and improving downstream tasks such as designing new treatments. In this paper, we introduce a novel concept in causal discovery, termed interventional constraints, which differs fundamentally from interventional data. While interventional data require direct perturbations of variables, interventional constraints encode high-level causal knowledge in the form of inequality constraints on causal effects. For instance, in the Sachs dataset (Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3 exerts a positive causal effect on Akt. Existing causal discovery methods allow enforcing structural constraints (for example, requiring a causal path from PIP3 to Akt), but they may still produce incorrect causal conclusions such as learning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap by explicitly constraining the total causal effect between variable pairs, ensuring learned models respect known causal influences. To formalize interventional constraints, we propose a metric to quantify total causal effects for linear causal models and formulate the problem as a constrained optimization task, solved using a two-stage constrained optimization method. We evaluate our approach on real-world datasets and demonstrate that integrating interventional constraints not only improves model accuracy and ensures consistency with established findings, making models more explainable, but also facilitates the discovery of new causal relationships that would otherwise be costly to identify.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26342v1",
    "published_date": "2025-10-30 10:49:25 UTC",
    "updated_date": "2025-10-30 10:49:25 UTC"
  },
  {
    "arxiv_id": "2510.26339v1",
    "title": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?",
    "authors": [
      "Mingyu Sung",
      "Seungjae Ham",
      "Kangwoo Kim",
      "Yeokyoung Yoon",
      "Sangseok Yun",
      "Il-Min Kim",
      "Jae-Mo Kang"
    ],
    "abstract": "Image super-resolution(SR) is fundamental to many vision system-from surveillance and autonomy to document analysis and retail analytics-because recovering high-frequency details, especially scene-text, enables reliable downstream perception. Scene-text, i.e., text embedded in natural images such as signs, product labels, and storefronts, often carries the most actionable information; when characters are blurred or hallucinated, optical character recognition(OCR) and subsequent decisions fail even if the rest of the image appears sharp. Yet previous SR research has often been tuned to distortion (PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that are largely insensitive to character-level errors. Furthermore, studies that do address text SR often focus on simplified benchmarks with isolated characters, overlooking the challenges of text within complex natural scenes. As a result, scene-text is effectively treated as generic texture. For SR to be effective in practical deployments, it is therefore essential to explicitly optimize for both text legibility and perceptual quality. We present GLYPH-SR, a vision-language-guided diffusion framework that aims to achieve both objectives jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by OCR data, and a ping-pong scheduler that alternates between text- and scene-centric guidance. To enable targeted text restoration, we train these components on a synthetic corpus while keeping the main SR branch frozen. Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR) while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed to satisfy both objectives simultaneously-high readability and high visual realism-delivering SR that looks right and reds right.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 6 figures. Includes supplementary material. Under review as a conference paper at ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.26339v1",
    "published_date": "2025-10-30 10:46:28 UTC",
    "updated_date": "2025-10-30 10:46:28 UTC"
  },
  {
    "arxiv_id": "2510.26336v1",
    "title": "From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning",
    "authors": [
      "Nishit Neema",
      "Srinjoy Mukherjee",
      "Sapan Shah",
      "Gokul Ramakrishnan",
      "Ganesh Venkatesh"
    ],
    "abstract": "Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26336v1",
    "published_date": "2025-10-30 10:43:40 UTC",
    "updated_date": "2025-10-30 10:43:40 UTC"
  },
  {
    "arxiv_id": "2511.00096v1",
    "title": "Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System",
    "authors": [
      "Shangyu Lou"
    ],
    "abstract": "Urban Artificial Intelligence (Urban AI) has advanced human-centered urban tasks such as perception prediction and human dynamics. Large Language Models (LLMs) can integrate multimodal inputs to address heterogeneous data in complex urban systems but often underperform on domain-specific tasks. Urban-MAS, an LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered urban prediction under zero-shot settings. It includes three agent types: Predictive Factor Guidance Agents, which prioritize key predictive factors to guide knowledge extraction and enhance the effectiveness of compressed urban knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve robustness by comparing multiple outputs, validating consistency, and re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which integrate extracted multi-source information across dimensions for prediction. Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors compared to single-LLM baselines. Ablation studies indicate that Predictive Factor Guidance Agents are most critical for enhancing predictive performance, positioning Urban-MAS as a scalable paradigm for human-centered urban AI prediction. Code is available on the project website:https://github.com/THETUREHOOHA/UrbanMAS",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted to The 3rd ACM SIGSPATIAL International Workshop on Advances in Urban AI (UrbanAI'25)",
    "pdf_url": "https://arxiv.org/pdf/2511.00096v1",
    "published_date": "2025-10-30 10:26:02 UTC",
    "updated_date": "2025-10-30 10:26:02 UTC"
  },
  {
    "arxiv_id": "2510.26324v2",
    "title": "Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics",
    "authors": [
      "Zhiyang Xun",
      "Shivam Gupta",
      "Eric Price"
    ],
    "abstract": "Given a noisy linear measurement $y = Ax + ξ$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.26324v2",
    "published_date": "2025-10-30 10:17:27 UTC",
    "updated_date": "2025-11-18 01:09:19 UTC"
  },
  {
    "arxiv_id": "2511.00095v1",
    "title": "SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation",
    "authors": [
      "Jiaming Liu",
      "Dingwei Fan",
      "Junyong Zhao",
      "Chunlin Li",
      "Haipeng Si",
      "Liang Sun"
    ],
    "abstract": "The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3\\% parsing accuracy and sub-800 ms response times. The software is released on https://github.com/6jm233333/spinalsam-r1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "2 Tables,5 Figures,16 Equations",
    "pdf_url": "https://arxiv.org/pdf/2511.00095v1",
    "published_date": "2025-10-30 10:14:42 UTC",
    "updated_date": "2025-10-30 10:14:42 UTC"
  },
  {
    "arxiv_id": "2510.26309v1",
    "title": "GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance",
    "authors": [
      "Jiseong Chung",
      "Ronny Ko",
      "Wonchul Yoo",
      "Makoto Onizuka",
      "Sungmok Kim",
      "Tae-Wan Kim",
      "Won-Yong Shin"
    ],
    "abstract": "Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review at The Web Conference 2026 (Semantics & Knowledge track). Code will be released upon acceptance. This arXiv v1 contains no repository links to preserve double-blind review",
    "pdf_url": "https://arxiv.org/pdf/2510.26309v1",
    "published_date": "2025-10-30 09:53:16 UTC",
    "updated_date": "2025-10-30 09:53:16 UTC"
  },
  {
    "arxiv_id": "2510.26303v2",
    "title": "Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime",
    "authors": [
      "Beomhan Baek",
      "Minhak Song",
      "Chulhee Yun"
    ],
    "abstract": "Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\\ell_\\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\\ell_2$-max-margin classifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $β_2 \\to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\\ell_\\infty$-max-margin classifier for any batch size by taking $β$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "50 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26303v2",
    "published_date": "2025-10-30 09:41:33 UTC",
    "updated_date": "2025-11-01 03:55:48 UTC"
  },
  {
    "arxiv_id": "2510.26302v1",
    "title": "Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens",
    "authors": [
      "Ziliang Chen",
      "Tianang Xiao",
      "Jusheng Zhang",
      "Yongsen Zheng",
      "Xipeng Chen"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal generalization by aligning images and texts in a shared embedding space, yet it persistently fails at compositional reasoning over objects, attributes, and relations often behaving like a bag-of-words matcher. Prior causal accounts typically model text as a single vector, obscuring token-level structure and leaving core phenomena-such as prompt sensitivity and failures on hard negatives unexplained. We address this gap with a token-aware causal representation learning (CRL) framework grounded in a sequential, language-token SCM. Our theory extends block identifiability to tokenized text, proving that CLIP's contrastive objective can recover the modal-invariant latent variable under both sentence-level and token-level SCMs. Crucially, token granularity yields the first principled explanation of CLIP's compositional brittleness: composition nonidentifiability. We show the existence of pseudo-optimal text encoders that achieve perfect modal-invariant alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations over atomic concepts, thereby failing to distinguish correct captions from hard negatives despite optimizing the same training objective as true-optimal encoders. The analysis further links language-side nonidentifiability to visual-side failures via the modality gap and shows how iterated composition operators compound hardness, motivating improved negative mining strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26302v1",
    "published_date": "2025-10-30 09:41:21 UTC",
    "updated_date": "2025-10-30 09:41:21 UTC"
  },
  {
    "arxiv_id": "2510.27190v1",
    "title": "Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures",
    "authors": [
      "Dominik Schwarz"
    ],
    "abstract": "As Large Language Models (LLMs) are increasingly integrated into automated, multi-stage pipelines, risk patterns that arise from unvalidated trust between processing stages become a practical concern. This paper presents a mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs. The analysis shows that inputs are often interpreted non-neutrally and can trigger implementation-shaped responses or unintended state changes even without explicit commands. We argue that these behaviors constitute architectural failure modes and that string-level filtering alone is insufficient. To mitigate such cross-stage vulnerabilities, we recommend zero-trust architectural principles, including provenance enforcement, context sealing, and plan revalidation, and we introduce \"Countermind\" as a conceptual blueprint for implementing these defenses.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "178 pages, mechanism-centered taxonomy of 41 LLM risk patterns, extensive appendix with experiment prompts and consolidation tables. Full traces available to reviewers and affected providers",
    "pdf_url": "https://arxiv.org/pdf/2510.27190v1",
    "published_date": "2025-10-30 09:38:45 UTC",
    "updated_date": "2025-10-30 09:38:45 UTC"
  },
  {
    "arxiv_id": "2510.26298v1",
    "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games",
    "authors": [
      "Jingran Zhang",
      "Ning Li",
      "Justin Cui"
    ],
    "abstract": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26298v1",
    "published_date": "2025-10-30 09:35:51 UTC",
    "updated_date": "2025-10-30 09:35:51 UTC"
  },
  {
    "arxiv_id": "2511.11599v2",
    "title": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection",
    "authors": [
      "Arefeh Kazemi",
      "Hamza Qadeer",
      "Joachim Wagner",
      "Hossein Hosseini",
      "Sri Balaaji Natarajan Kalaivendan",
      "Brian Davis"
    ],
    "abstract": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.11599v2",
    "published_date": "2025-10-30 09:27:36 UTC",
    "updated_date": "2025-12-09 14:42:26 UTC"
  },
  {
    "arxiv_id": "2511.00094v2",
    "title": "Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments",
    "authors": [
      "Angelos Alexopoulos",
      "Agorakis Bompotas",
      "Nikitas Rigas Kalogeropoulos",
      "Panagiotis Kechagias",
      "Athanasios P. Kalogeras",
      "Christos Alexakos"
    ],
    "abstract": "Robotic systems have become integral to smart environments, enabling applications ranging from urban surveillance and automated agriculture to industrial automation. However, their effective operation in dynamic settings - such as smart cities and precision farming - is challenged by continuously evolving topographies and environmental conditions. Traditional control systems often struggle to adapt quickly, leading to inefficiencies or operational failures. To address this limitation, we propose a novel framework for autonomous and dynamic reconfiguration of robotic controllers using Digital Twin technology. Our approach leverages a virtual replica of the robot's operational environment to simulate and optimize movement trajectories in response to real-world changes. By recalculating paths and control parameters in the Digital Twin and deploying the updated code to the physical robot, our method ensures rapid and reliable adaptation without manual intervention. This work advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for presentation to 11th IEEE International Smart Cities Conference (ISC2 2025)",
    "pdf_url": "https://arxiv.org/pdf/2511.00094v2",
    "published_date": "2025-10-30 09:20:57 UTC",
    "updated_date": "2026-01-02 08:33:29 UTC"
  },
  {
    "arxiv_id": "2510.26285v1",
    "title": "Unravelling the Mechanisms of Manipulating Numbers in Language Models",
    "authors": [
      "Michal Štefánik",
      "Timothee Mickus",
      "Marek Kadlčík",
      "Bertram Højer",
      "Michal Spiegel",
      "Raúl Vázquez",
      "Aman Sinha",
      "Josef Kuchař",
      "Philipp Mondorf"
    ],
    "abstract": "Recent work has shown that different large language models (LLMs) converge to similar and accurate input embedding representations for numbers. These findings conflict with the documented propensity of LLMs to produce erroneous outputs when dealing with numeric information. In this work, we aim to explain this conflict by exploring how language models manipulate numbers and quantify the lower bounds of accuracy of these mechanisms. We find that despite surfacing errors, different language models learn interchangeable representations of numbers that are systematic, highly accurate and universal across their hidden states and the types of input contexts. This allows us to create universal probes for each LLM and to trace information -- including the causes of output errors -- to specific layers. Our results lay a fundamental understanding of how pre-trained LLMs manipulate numbers and outline the potential of more accurate probing techniques in addressed refinements of LLMs' architectures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26285v1",
    "published_date": "2025-10-30 09:08:50 UTC",
    "updated_date": "2025-10-30 09:08:50 UTC"
  },
  {
    "arxiv_id": "2510.26278v1",
    "title": "Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation",
    "authors": [
      "Kim Yong Tan",
      "Yueming Lyu",
      "Ivor Tsang",
      "Yew-Soon Ong"
    ],
    "abstract": "Diffusion models have been successful in learning complex data distributions. This capability has driven their application to high-dimensional multi-objective black-box optimization problem. Existing approaches often employ an external optimization loop, such as an evolutionary algorithm, to the diffusion model. However, these approaches treat the diffusion model as a black-box refiner, which overlooks the internal distribution transition of the diffusion generation process, limiting their efficiency. To address these challenges, we propose the Inference-time Multi-target Generation (IMG) algorithm, which optimizes the diffusion process at inference-time to generate samples that simultaneously satisfy multiple objectives. Specifically, our IMG performs weighted resampling during the diffusion generation process according to the expected aggregated multi-objective values. This weighted resampling strategy ensures the diffusion-generated samples are distributed according to our desired multi-target Boltzmann distribution. We further derive that the multi-target Boltzmann distribution has an interesting log-likelihood interpretation, where it is the optimal solution to the distributional multi-objective optimization problem. We implemented IMG for a multi-objective molecule generation task. Experiments show that IMG, requiring only a single generation pass, achieves a significantly higher hypervolume than baseline optimization algorithms that often require hundreds of diffusion generations. Notably, our algorithm can be viewed as an optimized diffusion process and can be integrated into existing methods to further improve their performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26278v1",
    "published_date": "2025-10-30 09:00:42 UTC",
    "updated_date": "2025-10-30 09:00:42 UTC"
  },
  {
    "arxiv_id": "2510.26275v1",
    "title": "A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI",
    "authors": [
      "Domenico Amalfitano",
      "Andreas Metzger",
      "Marco Autili",
      "Tommaso Fulcini",
      "Tobias Hey",
      "Jan Keim",
      "Patrizio Pelliccione",
      "Vincenzo Scotti",
      "Anne Koziolek",
      "Raffaela Mirandola",
      "Andreas Vogelsang"
    ],
    "abstract": "Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The process consists of three cycles that incrementally integrate multiple sources of evidence, including collaborative discussions from the FSE 2025 \"Software Engineering 2030\" workshop, rapid literature reviews, and external feedback sessions involving peers. McLuhan's tetrads were used as a conceptual instrument to systematically capture the transforming effects of GenAI on SE processes and software products.The resulting roadmap identifies four fundamental forms of GenAI augmentation in SE and systematically characterizes their related research challenges and opportunities. These insights are then consolidated into a set of future research directions. By grounding the roadmap in a rigorous multi-cycle process and cross-validating it among independent author teams and peers, the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes, methods and tools, and for framing future research within this rapidly evolving area. Based on these findings, the article finally makes ten predictions for SE in the year 2030.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26275v1",
    "published_date": "2025-10-30 08:59:01 UTC",
    "updated_date": "2025-10-30 08:59:01 UTC"
  },
  {
    "arxiv_id": "2510.26270v1",
    "title": "Graph-Enhanced Policy Optimization in LLM Agent Training",
    "authors": [
      "Jiazhen Yuan",
      "Wei Zhao",
      "Zhengbiao Bai"
    ],
    "abstract": "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review as a conference paper",
    "pdf_url": "https://arxiv.org/pdf/2510.26270v1",
    "published_date": "2025-10-30 08:53:41 UTC",
    "updated_date": "2025-10-30 08:53:41 UTC"
  },
  {
    "arxiv_id": "2510.26843v1",
    "title": "CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs",
    "authors": [
      "Zhiyuan Ning",
      "Jiawei Shao",
      "Ruge Xu",
      "Xinfei Guo",
      "Jun Zhang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs). While on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by methods relying on specialized training. Cascading a hierarchy of draft models promises further acceleration and flexibility, but the high cost of training multiple models has limited its practical application. In this paper, we propose a novel Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs speculative draft models by leveraging dynamically switchable inference acceleration (DSIA) strategies, including layer sparsity and activation quantization. Furthermore, traditional vertical and horizontal cascade algorithms are inefficient when applied to self-speculative decoding methods. We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the multi-level draft models and assigns the draft lengths, based on the heuristics of acceptance rates and latency prediction. Our CAS-Spec method achieves state-of-the-art acceleration compared to existing on-the-fly speculative decoding methods, with an average speedup from $1.1\\times$ to $2.3\\times$ over autoregressive decoding across various LLMs and datasets. DyTC improves the average speedup by $47$\\% and $48$\\% over cascade-based baseline and tree-based baseline algorithms, respectively. CAS-Spec can be easily integrated into most existing LLMs and holds promising potential for further acceleration as self-speculative decoding techniques continue to evolve.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 figures, NeurIPS 2025 poster",
    "pdf_url": "https://arxiv.org/pdf/2510.26843v1",
    "published_date": "2025-10-30 08:51:29 UTC",
    "updated_date": "2025-10-30 08:51:29 UTC"
  },
  {
    "arxiv_id": "2510.26243v1",
    "title": "Angular Steering: Behavior Control via Rotation in Activation Space",
    "authors": [
      "Hieu M. Vu",
      "Tan M. Nguyen"
    ],
    "abstract": "Controlling specific behaviors in large language models while preserving their general capabilities is a central challenge for safe and reliable artificial intelligence deployment. Current steering methods, such as vector addition and directional ablation, are constrained within a two-dimensional subspace defined by the activation and feature direction, making them sensitive to chosen parameters and potentially affecting unrelated features due to unintended interactions in activation space. We introduce Angular Steering, a novel and flexible method for behavior modulation that operates by rotating activations within a fixed two-dimensional subspace. By formulating steering as a geometric rotation toward or away from a target behavior direction, Angular Steering provides continuous, fine-grained control over behaviors such as refusal and compliance. We demonstrate this method using refusal steering emotion steering as use cases. Additionally, we propose Adaptive Angular Steering, a selective variant that rotates only activations aligned with the target feature, further enhancing stability and coherence. Angular Steering generalizes existing addition and orthogonalization techniques under a unified geometric rotation framework, simplifying parameter selection and maintaining model stability across a broader range of adjustments. Experiments across multiple model families and sizes show that Angular Steering achieves robust behavioral control while maintaining general language modeling performance, underscoring its flexibility, generalization, and robustness compared to prior approaches. Code and artifacts are available at https://github.com/lone17/angular-steering/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.26243v1",
    "published_date": "2025-10-30 08:23:35 UTC",
    "updated_date": "2025-10-30 08:23:35 UTC"
  },
  {
    "arxiv_id": "2510.26242v1",
    "title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles",
    "authors": [
      "Xinhang Li",
      "Qing Guo",
      "Junyu Chen",
      "Zheng Guo",
      "Shengzhe Xu",
      "Lei Li",
      "Lin Zhang"
    ],
    "abstract": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26242v1",
    "published_date": "2025-10-30 08:23:08 UTC",
    "updated_date": "2025-10-30 08:23:08 UTC"
  },
  {
    "arxiv_id": "2510.26238v1",
    "title": "Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses",
    "authors": [
      "Duc-Hai Nguyen",
      "Vijayakumar Nanjappan",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ],
    "abstract": "Millions of people take surveys every day, from market polls and academic studies to medical questionnaires and customer feedback forms. These datasets capture valuable insights, but their scale and structure present a unique challenge for large language models (LLMs), which otherwise excel at few-shot reasoning over open-ended text. Yet, their ability to process questionnaire data or lists of questions crossed with hundreds of respondent rows remains underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics, SPSS, REDCap) are typically designed for humans in the workflow, limiting such data integration with LLM and AI-empowered automation. This gap leaves scientists, surveyors, and everyday users without evidence-based guidance on how to best represent questionnaires for LLM consumption. We address this by introducing QASU (Questionnaire Analysis and Structural Understanding), a benchmark that probes six structural skills, including answer lookup, respondent count, and multi-hop inference, across six serialization formats and multiple prompt strategies. Experiments on contemporary LLMs show that choosing an effective format and prompt combination can improve accuracy by up to 8.8% points compared to suboptimal formats. For specific tasks, carefully adding a lightweight structural hint through self-augmented prompting can yield further improvements of 3-4% points on average. By systematically isolating format and prompting effects, our open source benchmark offers a simple yet versatile foundation for advancing both research and real-world practice in LLM-based questionnaire analysis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 3 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.26238v1",
    "published_date": "2025-10-30 08:18:37 UTC",
    "updated_date": "2025-10-30 08:18:37 UTC"
  },
  {
    "arxiv_id": "2510.26230v1",
    "title": "MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines",
    "authors": [
      "Minyi Peng",
      "Darian Gunamardi",
      "Ivan Tjuawinata",
      "Kwok-Yan Lam"
    ],
    "abstract": "As a new and promising approach, existing machine unlearning (MU) works typically emphasize theoretical formulations or optimization objectives to achieve knowledge removal. However, when deployed in real-world scenarios, such solutions typically face scalability issues and have to address practical requirements such as full access to original datasets and model. In contrast to the existing approaches, we regard classification training as a sequential process where classes are learned sequentially, which we call \\emph{inductive approach}. Unlearning can then be done by reversing the last training sequence. This is implemented by appending a projection-redistribution layer in the end of the model. Such an approach does not require full access to the original dataset or the model, addressing the challenges of existing methods. This enables modular and model-agnostic deployment as an output filter into existing classification pipelines with minimal alterations. We conducted multiple experiments across multiple datasets including image (CIFAR-10/100 using CNN-based model) and tabular datasets (Covertype using tree-based model). Experiment results show consistently similar output to a fully retrained model with a high computational cost reduction. This demonstrates the applicability, scalability, and system compatibility of our solution while maintaining the performance of the output in a more practical setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26230v1",
    "published_date": "2025-10-30 08:09:37 UTC",
    "updated_date": "2025-10-30 08:09:37 UTC"
  },
  {
    "arxiv_id": "2510.26219v1",
    "title": "Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space",
    "authors": [
      "Sekitoshi Kanai",
      "Tsukasa Yoshida",
      "Hiroshi Takahashi",
      "Haru Kuroki",
      "Kazumune Hashimoto"
    ],
    "abstract": "Test-time alignment of large language models (LLMs) attracts attention because fine-tuning LLMs requires high computational costs. In this paper, we propose a new test-time alignment method called adaptive importance sampling on pre-logits (AISP) on the basis of the sampling-based model predictive control with the stochastic control input. AISP applies the Gaussian perturbation into pre-logits, which are outputs of the penultimate layer, so as to maximize expected rewards with respect to the mean of the perturbation. We demonstrate that the optimal mean is obtained by importance sampling with sampled rewards. AISP outperforms best-of-n sampling in terms of rewards over the number of used samples and achieves higher rewards than other reward-based test-time alignment methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.26219v1",
    "published_date": "2025-10-30 07:52:14 UTC",
    "updated_date": "2025-10-30 07:52:14 UTC"
  },
  {
    "arxiv_id": "2510.26217v1",
    "title": "Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA Collateral Management",
    "authors": [
      "Tao Jin",
      "Stuart Florescu",
      "Heyu",
      "Jin"
    ],
    "abstract": "We address finance-native collateral optimization under ISDA Credit Support Annexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and issuer/currency/class caps create rugged, legally bounded search spaces. We introduce a certifiable hybrid pipeline purpose-built for this domain: (i) an evidence-gated LLM that extracts CSA terms to a normalized JSON (abstain-by-default, span-cited); (ii) a quantum-inspired explorer that interleaves simulated annealing with micro higher order QAOA (HO-QAOA) on binding sub-QUBOs (subset size n <= 16, order k <= 4) to coordinate multi-asset moves across caps and RA-induced discreteness; (iii) a weighted risk-aware objective (Movement, CVaR, funding-priced overshoot) with an explicit coverage window U <= Reff+B; and (iv) CP-SAT as single arbiter to certify feasibility and gaps, including a U-cap pre-check that reports the minimal feasible buffer B*. Encoding caps/rounding as higher-order terms lets HO-QAOA target the domain couplings that defeat local swaps. On government bond datasets and multi-CSA inputs, the hybrid improves a strong classical baseline (BL-3) by 9.1%, 9.6%, and 10.7% across representative harnesses, delivering better cost-movement-tail frontiers under governance settings. We release governance grade artifacts-span citations, valuation matrix audit, weight provenance, QUBO manifests, and CP-SAT traces-to make results auditable and reproducible.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "q-fin.CP",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26217v1",
    "published_date": "2025-10-30 07:46:40 UTC",
    "updated_date": "2025-10-30 07:46:40 UTC"
  },
  {
    "arxiv_id": "2510.26205v2",
    "title": "Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning",
    "authors": [
      "Qi Luo",
      "Xiaonan Li",
      "Tingshuo Fan",
      "Xinchi Chen",
      "Xipeng Qiu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a leading approach to reducing hallucinations in large language models (LLMs). Current RAG evaluation benchmarks primarily focus on what we call local RAG: retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks. However, many real-world applications require a fundamentally different capability -- global RAG -- which involves aggregating and analyzing information across entire document collections to derive corpus-level insights (for example, \"What are the top 10 most cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first benchmark specifically designed to evaluate global RAG capabilities, covering four core task types: counting, extremum queries, sorting, and top-k extraction. Through systematic evaluation across different models and baselines, we find that existing RAG methods perform poorly on global tasks, with the strongest baseline achieving only 1.51 F1 score. To address these challenges, we propose GlobalRAG, a multi-tool collaborative framework that preserves structural coherence through chunk-level retrieval, incorporates LLM-driven intelligent filters to eliminate noisy documents, and integrates aggregation modules for precise symbolic computation. On the Qwen2.5-14B model, GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1, validating the effectiveness of our method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26205v2",
    "published_date": "2025-10-30 07:29:14 UTC",
    "updated_date": "2025-11-04 05:03:35 UTC"
  },
  {
    "arxiv_id": "2510.26202v1",
    "title": "What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data",
    "authors": [
      "Rajiv Movva",
      "Smitha Milli",
      "Sewon Min",
      "Emma Pierson"
    ],
    "abstract": "Human feedback can alter language models in unpredictable and undesirable ways, as practitioners lack a clear understanding of what feedback data encodes. While prior work studies preferences over certain attributes (e.g., length or sycophancy), automatically extracting relevant features without pre-specifying hypotheses remains challenging. We introduce What's In My Human Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders. WIMHF characterizes both (1) the preferences a dataset is capable of measuring and (2) the preferences that the annotators actually express. Across 7 datasets, WIMHF identifies a small number of human-interpretable features that account for the majority of the preference prediction signal achieved by black-box models. These features reveal a wide diversity in what humans prefer, and the role of dataset-level context: for example, users on Reddit prefer informality and jokes, while annotators in HH-RLHF and PRISM disprefer them. WIMHF also surfaces potentially unsafe preferences, such as that LMArena users tend to vote against refusals, often in favor of toxic content. The learned features enable effective data curation: re-labeling the harmful examples in Arena yields large safety gains (+37%) with no cost to general performance. They also allow fine-grained personalization: on the Community Alignment dataset, we learn annotator-specific weights over subjective features that improve preference prediction. WIMHF provides a human-centered analysis method for practitioners to better understand and use preference data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/rmovva/wimhf",
    "pdf_url": "https://arxiv.org/pdf/2510.26202v1",
    "published_date": "2025-10-30 07:25:10 UTC",
    "updated_date": "2025-10-30 07:25:10 UTC"
  },
  {
    "arxiv_id": "2510.26200v1",
    "title": "Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation",
    "authors": [
      "Woojin Kim",
      "Jaeyoung Do"
    ],
    "abstract": "While diffusion language models (DLMs) enable fine-grained refinement, their practical controllability remains fragile. We identify and formally characterize a central failure mode called update forgetting, in which uniform and context agnostic updates induce token level fluctuations across timesteps, erasing earlier semantic edits and disrupting the cumulative refinement process, thereby degrading fluency and coherence. As this failure originates in uniform and context agnostic updates, effective control demands explicit token ordering. We propose Token Timestep Allocation (TTA), which realizes soft and semantic token ordering via per token timestep schedules: critical tokens are frozen early, while uncertain tokens receive continued refinement. This timestep based ordering can be instantiated as either a fixed policy or an adaptive policy driven by task signals, thereby supporting a broad spectrum of refinement strategies. Because it operates purely at inference time, it applies uniformly across various DLMs and naturally extends to diverse supervision sources. Empirically, TTA improves controllability and fluency: on sentiment control, it yields more than 20 percent higher accuracy and nearly halves perplexity using less than one fifth the steps; in detoxification, it lowers maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0). Together, these results demonstrate that softened ordering via timestep allocation is the critical lever for mitigating update forgetting and achieving stable and controllable diffusion text generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.26200v1",
    "published_date": "2025-10-30 07:21:05 UTC",
    "updated_date": "2025-10-30 07:21:05 UTC"
  },
  {
    "arxiv_id": "2510.26841v1",
    "title": "Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility",
    "authors": [
      "Kangkang Sun",
      "Jun Wu",
      "Minyi Guo",
      "Jianhua Li",
      "Jianwei Huang"
    ],
    "abstract": "Federated Learning (FL) enables collaborative model training without data sharing, yet participants face a fundamental challenge, e.g., simultaneously ensuring fairness across demographic groups while protecting sensitive client data. We introduce a differentially private fair FL algorithm (\\textit{FedPF}) that transforms this multi-objective optimization into a zero-sum game where fairness and privacy constraints compete against model utility. Our theoretical analysis reveals a surprising inverse relationship, i.e., stricter privacy protection fundamentally limits the system's ability to detect and correct demographic biases, creating an inherent tension between privacy and fairness. Counterintuitively, we prove that moderate fairness constraints initially improve model generalization before causing performance degradation, where a non-monotonic relationship that challenges conventional wisdom about fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 % discrimination reduction across three datasets while maintaining competitive accuracy, but more importantly, reveals that the privacy-fairness tension is unavoidable, i.e., achieving both objectives simultaneously requires carefully balanced compromises rather than optimization of either in isolation. The source code for our proposed algorithm is publicly accessible at https://github.com/szpsunkk/FedPF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 4 figures, 30 conference",
    "pdf_url": "https://arxiv.org/pdf/2510.26841v1",
    "published_date": "2025-10-30 07:14:55 UTC",
    "updated_date": "2025-10-30 07:14:55 UTC"
  },
  {
    "arxiv_id": "2510.26188v1",
    "title": "Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients",
    "authors": [
      "Avinash Kadimisetty",
      "Arun Rajagopalan",
      "Vijendra SK"
    ],
    "abstract": "Reducing preventable hospital readmissions is a national priority for payers, providers, and policymakers seeking to improve health care and lower costs. The rate of readmission is being used as a benchmark to determine the quality of healthcare provided by the hospitals. In thisproject, we have used machine learning techniques like Logistic Regression, Random Forest and Support Vector Machines to analyze the health claims data and identify demographic and medical factors that play a crucial role in predicting all-cause readmissions. As the health claims data is high dimensional, we have used Principal Component Analysis as a dimension reduction technique and used the results for building regression models. We compared and evaluated these models based on the Area Under Curve (AUC) metric. Random Forest model gave the highest performance followed by Logistic Regression and Support Vector Machine models. These models can be used to identify the crucial factors causing readmissions and help identify patients to focus on to reduce the chances of readmission, ultimately bringing down the cost and increasing the quality of healthcare provided to the patients.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NCMLAI 2018",
    "pdf_url": "https://arxiv.org/pdf/2510.26188v1",
    "published_date": "2025-10-30 06:54:19 UTC",
    "updated_date": "2025-10-30 06:54:19 UTC"
  },
  {
    "arxiv_id": "2510.26186v1",
    "title": "ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts",
    "authors": [
      "Jinho Choi",
      "Hyesu Lim",
      "Steffen Schneider",
      "Jaegul Choo"
    ],
    "abstract": "Dataset bias, where data points are skewed to certain concepts, is ubiquitous in machine learning datasets. Yet, systematically identifying these biases is challenging without costly, fine-grained attribute annotations. We present ConceptScope, a scalable and automated framework for analyzing visual datasets by discovering and quantifying human-interpretable concepts using Sparse Autoencoders trained on representations from vision foundation models. ConceptScope categorizes concepts into target, context, and bias types based on their semantic relevance and statistical correlation to class labels, enabling class-level dataset characterization, bias identification, and robustness evaluation through concept-based subgrouping. We validate that ConceptScope captures a wide range of visual concepts, including objects, textures, backgrounds, facial attributes, emotions, and actions, through comparisons with annotated datasets. Furthermore, we show that concept activations produce spatial attributions that align with semantically meaningful image regions. ConceptScope reliably detects known biases (e.g., background bias in Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects in ImageNet), offering a practical tool for dataset auditing and model diagnostics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.26186v1",
    "published_date": "2025-10-30 06:46:17 UTC",
    "updated_date": "2025-10-30 06:46:17 UTC"
  },
  {
    "arxiv_id": "2510.26185v1",
    "title": "Accumulative SGD Influence Estimation for Data Attribution",
    "authors": [
      "Yunxiao Shi",
      "Shuo Yang",
      "Yixin Su",
      "Rui Zhang",
      "Min Xu"
    ],
    "abstract": "Modern data-centric AI needs precise per-sample influence. Standard SGD-IE approximates leave-one-out effects by summing per-epoch surrogates and ignores cross-epoch compounding, which misranks critical examples. We propose ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out perturbation across training and updates an accumulative influence state at each step. In smooth strongly convex settings it achieves geometric error contraction and, in smooth non-convex regimes, it tightens error bounds; larger mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups, and MNIST under clean and corrupted data and both convex and non-convex training, ACC-SGD-IE yields more accurate influence estimates, especially over long epochs. For downstream data cleansing it more reliably flags noisy samples, producing models trained on ACC-SGD-IE cleaned data that outperform those cleaned with SGD-IE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26185v1",
    "published_date": "2025-10-30 06:45:22 UTC",
    "updated_date": "2025-10-30 06:45:22 UTC"
  },
  {
    "arxiv_id": "2511.00092v1",
    "title": "QuantumBench: A Benchmark for Quantum Problem Solving",
    "authors": [
      "Shunya Minami",
      "Tatsuya Ishigaki",
      "Ikko Hamamura",
      "Taku Mikuriya",
      "Youmi Ma",
      "Naoaki Okazaki",
      "Hiroya Takamura",
      "Yohichi Suzuki",
      "Tadashi Kadowaki"
    ],
    "abstract": "Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.00092v1",
    "published_date": "2025-10-30 06:44:03 UTC",
    "updated_date": "2025-10-30 06:44:03 UTC"
  },
  {
    "arxiv_id": "2510.26172v1",
    "title": "Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis",
    "authors": [
      "Shifu Chen",
      "Dazhen Deng",
      "Zhihong Xu",
      "Sijia Xu",
      "Tai-Quan Peng",
      "Yingcai Wu"
    ],
    "abstract": "Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26172v1",
    "published_date": "2025-10-30 06:22:49 UTC",
    "updated_date": "2025-10-30 06:22:49 UTC"
  },
  {
    "arxiv_id": "2510.26167v2",
    "title": "ToolRM: Towards Agentic Tool-Use Reward Modeling",
    "authors": [
      "Renhao Li",
      "Jianhong Tu",
      "Yang Su",
      "Yantao Liu",
      "Fei Huang",
      "Hamid Alinejad-Rokny",
      "Derek F. Wong",
      "Junyang Lin",
      "Min Yang"
    ],
    "abstract": "Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight reward models tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs high-quality pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging preference dataset that supports both generative and discriminative reward modeling. We also introduce TRBench$_{BFCL}$, a benchmark built on the agent evaluation suite BFCL to evaluate RMs on tool calling tasks. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 17.94% higher accuracy, substantially outperforming frontier LLMs and RMs in pairwise reward judgments. Beyond training objectives, generative ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling while reducing output token usage by over 66%. Its support for downstream RL training further validates its practical utility. We release data to facilitate future research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26167v2",
    "published_date": "2025-10-30 06:08:27 UTC",
    "updated_date": "2026-01-13 13:28:26 UTC"
  },
  {
    "arxiv_id": "2510.26165v1",
    "title": "Learning to Manage Investment Portfolios beyond Simple Utility Functions",
    "authors": [
      "Maarten P. Scholl",
      "Mahmoud Mahfouz",
      "Anisoara Calinescu",
      "J. Doyne Farmer"
    ],
    "abstract": "While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.\n  Our approach directly models the conditional probability of a fund's portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund's strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.\n  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as \"growth\" and \"value,\" while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.\n  To analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark's expert labeling are contained in our model's encoding in a linear interpretable way.\n  Our framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "q-fin.PM",
    "comment": "6th ACM International Conference on AI in Finance, November 15-18, 2025, Singapore",
    "pdf_url": "https://arxiv.org/pdf/2510.26165v1",
    "published_date": "2025-10-30 06:01:20 UTC",
    "updated_date": "2025-10-30 06:01:20 UTC"
  },
  {
    "arxiv_id": "2510.26159v1",
    "title": "Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series",
    "authors": [
      "Emilio Mastriani",
      "Alessandro Costa",
      "Federico Incardona",
      "Kevin Munari",
      "Sebastiano Spinello"
    ],
    "abstract": "In this study, we investigate the effectiveness of advanced feature engineering and hybrid model architectures for anomaly detection in a multivariate industrial time series, focusing on a steam turbine system. We evaluate the impact of change point-derived statistical features, clustering-based substructure representations, and hybrid learning strategies on detection performance. Despite their theoretical appeal, these complex approaches consistently underperformed compared to a simple Random Forest + XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of 0.976, F1-score of 0.41, and 100% early detection within the defined time window. Our findings highlight that, in scenarios with highly imbalanced and temporally uncertain data, model simplicity combined with optimized segmentation can outperform more sophisticated architectures, offering greater robustness, interpretability, and operational utility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is currently under review for presentation at the IEEE SAMI 2026 Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.26159v1",
    "published_date": "2025-10-30 05:39:44 UTC",
    "updated_date": "2025-10-30 05:39:44 UTC"
  },
  {
    "arxiv_id": "2510.26157v1",
    "title": "Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment",
    "authors": [
      "Hyuntae Park",
      "Yeachan Kim",
      "SangKeun Lee"
    ],
    "abstract": "Molecule and text representation learning has gained increasing interest due to its potential for enhancing the understanding of chemical information. However, existing models often struggle to capture subtle differences between molecules and their descriptions, as they lack the ability to learn fine-grained alignments between molecular substructures and chemical phrases. To address this limitation, we introduce MolBridge, a novel molecule-text learning framework based on substructure-aware alignments. Specifically, we augment the original molecule-description pairs with additional alignment signals derived from molecular substructures and chemical phrases. To effectively learn from these enriched alignments, MolBridge employs substructure-aware contrastive learning, coupled with a self-refinement mechanism that filters out noisy alignment signals. Experimental results show that MolBridge effectively captures fine-grained correspondences and outperforms state-of-the-art baselines on a wide range of molecular benchmarks, highlighting the significance of substructure-aware alignment in molecule-text learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025 (main)",
    "pdf_url": "https://arxiv.org/pdf/2510.26157v1",
    "published_date": "2025-10-30 05:36:31 UTC",
    "updated_date": "2025-10-30 05:36:31 UTC"
  },
  {
    "arxiv_id": "2510.26151v1",
    "title": "MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction",
    "authors": [
      "Shunjie-Fabian Zheng",
      "Hyeonjun Lee",
      "Thijs Kooi",
      "Ali Diba"
    ],
    "abstract": "Large annotated datasets are essential for training robust Computer-Aided Diagnosis (CAD) models for breast cancer detection or risk prediction. However, acquiring such datasets with fine-detailed annotation is both costly and time-consuming. Vision-Language Models (VLMs), such as CLIP, which are pre-trained on large image-text pairs, offer a promising solution by enhancing robustness and data efficiency in medical imaging tasks. This paper introduces a novel Multi-View Mammography and Language Model for breast cancer classification and risk prediction, trained on a dataset of paired mammogram images and synthetic radiology reports. Our MV-MLM leverages multi-view supervision to learn rich representations from extensive radiology data by employing cross-modal self-supervision across image-text pairs. This includes multiple views and the corresponding pseudo-radiology reports. We propose a novel joint visual-textual learning strategy to enhance generalization and accuracy performance over different data types and tasks to distinguish breast tissues or cancer characteristics(calcification, mass) and utilize these patterns to understand mammography images and predict cancer risk. We evaluated our method on both private and publicly available datasets, demonstrating that the proposed model achieves state-of-the-art performance in three classification tasks: (1) malignancy classification, (2) subtype classification, and (3) image-based cancer risk prediction. Furthermore, the model exhibits strong data efficiency, outperforming existing fully supervised or VLM baselines while trained on synthetic text reports and without the need for actual radiology reports.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD) Workshop at ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.26151v1",
    "published_date": "2025-10-30 05:12:29 UTC",
    "updated_date": "2025-10-30 05:12:29 UTC"
  },
  {
    "arxiv_id": "2510.26144v1",
    "title": "The FM Agent",
    "authors": [
      "Annan Li",
      "Chufan Wu",
      "Zengle Ge",
      "Yee Hin Chong",
      "Zhinan Hou",
      "Lizhe Cao",
      "Cheng Ju",
      "Jianmin Wu",
      "Huaiming Li",
      "Haobo Zhang",
      "Shenghao Feng",
      "Mo Zhao",
      "Fengzhi Qiu",
      "Rui Yang",
      "Mengmeng Zhang",
      "Wenyi Zhu",
      "Yingying Sun",
      "Quan Sun",
      "Shunhao Yan",
      "Danyu Liu",
      "Dawei Yin",
      "Dou Shen"
    ],
    "abstract": "Large language models (LLMs) are catalyzing the development of autonomous AI research agents for scientific and engineering discovery. We present FM Agent, a novel and general-purpose multi-agent framework that leverages a synergistic combination of LLM-based reasoning and large-scale evolutionary search to address complex real-world challenges. The core of FM Agent integrates several key innovations: 1) a cold-start initialization phase incorporating expert guidance, 2) a novel evolutionary sampling strategy for iterative optimization, 3) domain-specific evaluators that combine correctness, effectiveness, and LLM-supervised feedback, and 4) a distributed, asynchronous execution infrastructure built on Ray. Demonstrating broad applicability, our system has been evaluated across diverse domains, including operations research, machine learning, GPU kernel optimization, and classical mathematical problems. FM Agent reaches state-of-the-art results autonomously, without human interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\\%), 43.56\\% on MLE-Bench (+4.0pp), up to 20x speedups on KernelBench, and establishes new state-of-the-art(SOTA) results on several classical mathematical problems. Beyond academic benchmarks, FM Agent shows considerable promise for both large-scale enterprise R\\&D workflows and fundamental scientific research, where it can accelerate innovation, automate complex discovery processes, and deliver substantial engineering and scientific advances with broader societal impact.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26144v1",
    "published_date": "2025-10-30 04:57:57 UTC",
    "updated_date": "2025-10-30 04:57:57 UTC"
  },
  {
    "arxiv_id": "2511.00090v3",
    "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
    "authors": [
      "Huanlin Gao",
      "Ping Chen",
      "Fuyuan Shi",
      "Chao Tan",
      "Zhaoxiang Liu",
      "Fang Zhao",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025 Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2511.00090v3",
    "published_date": "2025-10-30 04:57:26 UTC",
    "updated_date": "2025-12-11 08:10:13 UTC"
  },
  {
    "arxiv_id": "2510.26143v1",
    "title": "Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math",
    "authors": [
      "Bo Pang",
      "Deqian Kong",
      "Silvio Savarese",
      "Caiming Xiong",
      "Yingbo Zhou"
    ],
    "abstract": "Reinforcement learning (RL) can elicit strong reasoning in large language models (LLMs), yet most open efforts focus on math and code. We propose Reasoning Curriculum, a simple two-stage curriculum that first elicits reasoning skills in pretraining-aligned domains such as math, then adapts and refines these skills across other domains via joint RL. Stage 1 performs a brief cold start and then math-only RL with verifiable rewards to develop reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and consolidate these skills. The curriculum is minimal and backbone-agnostic, requiring no specialized reward models beyond standard verifiability checks. Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning curriculum yields consistent gains. Ablations and a cognitive-skill analysis indicate that both stages are necessary and that math-first elicitation increases cognitive behaviors important for solving complex problems. Reasoning Curriculum provides a compact, easy-to-adopt recipe for general reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26143v1",
    "published_date": "2025-10-30 04:56:44 UTC",
    "updated_date": "2025-10-30 04:56:44 UTC"
  },
  {
    "arxiv_id": "2510.26136v1",
    "title": "Beyond Benchmarks: The Economics of AI Inference",
    "authors": [
      "Boqin Zhuang",
      "Jiacheng Qiao",
      "Mingqian Liu",
      "Mingxing Yu",
      "Ping Hong",
      "Rui Li",
      "Xiaoxia Song",
      "Xiangjun Xu",
      "Xu Chen",
      "Yaoyao Ma",
      "Yujie Gao"
    ],
    "abstract": "The inference cost of Large Language Models (LLMs) has become a critical factor in determining their commercial viability and widespread adoption. This paper introduces a quantitative ``economics of inference'' framework, treating the LLM inference process as a compute-driven intelligent production activity. We analyze its marginal cost, economies of scale, and quality of output under various performance configurations. Based on empirical data from WiNEval-3.0, we construct the first ``LLM Inference Production Frontier,'' revealing three principles: diminishing marginal cost, diminishing returns to scale, and an optimal cost-effectiveness zone. This paper not only provides an economic basis for model deployment decisions but also lays an empirical foundation for the future market-based pricing and optimization of AI inference resources.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26136v1",
    "published_date": "2025-10-30 04:49:27 UTC",
    "updated_date": "2025-10-30 04:49:27 UTC"
  },
  {
    "arxiv_id": "2510.26130v2",
    "title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation",
    "authors": [
      "Musfiqur Rahman",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.\n  We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.\n  Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.\n  These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Pre-print submitted for reviwer to TOSEM",
    "pdf_url": "https://arxiv.org/pdf/2510.26130v2",
    "published_date": "2025-10-30 04:30:23 UTC",
    "updated_date": "2025-11-04 21:33:03 UTC"
  },
  {
    "arxiv_id": "2510.26125v3",
    "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios",
    "authors": [
      "Runsheng Xu",
      "Hubert Lin",
      "Wonseok Jeon",
      "Hao Feng",
      "Yuliang Zou",
      "Liting Sun",
      "John Gorman",
      "Ekaterina Tolstaya",
      "Sarah Tang",
      "Brandyn White",
      "Ben Sapp",
      "Mingxing Tan",
      "Jyh-Jing Hwang",
      "Dragomir Anguelov"
    ],
    "abstract": "Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26125v3",
    "published_date": "2025-10-30 04:25:33 UTC",
    "updated_date": "2025-11-13 01:49:11 UTC"
  },
  {
    "arxiv_id": "2510.26113v1",
    "title": "EgoExo-Con: Exploring View-Invariant Video Temporal Understanding",
    "authors": [
      "Minjoon Jung",
      "Junbin Xiao",
      "Junghyun Kim",
      "Byoung-Tak Zhang",
      "Angela Yao"
    ],
    "abstract": "Can Video-LLMs achieve consistent temporal understanding when videos capture the same event from different viewpoints? To study this, we introduce EgoExo-Con (Consistency), a benchmark of comprehensively synchronized egocentric and exocentric video pairs with human-refined queries in natural language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal Verification and Temporal Grounding. It evaluates not only correctness but consistency across viewpoints. Our analysis reveals two critical limitations of existing Video-LLMs: (1) models often fail to maintain consistency, with results far worse than their single-view performances. (2) When naively finetuned with synchronized videos of both viewpoints, the models show improved consistency but often underperform those trained on a single view. For improvements, we propose View-GRPO, a novel reinforcement learning framework that effectively strengthens view-specific temporal reasoning while encouraging consistent comprehension across viewpoints. Our method demonstrates its superiority over naive SFT and GRPO, especially for improving cross-view consistency. All resources will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "project page: \\url{https://minjoong507.github.io/projects/EgoExo-Con/}",
    "pdf_url": "https://arxiv.org/pdf/2510.26113v1",
    "published_date": "2025-10-30 03:53:22 UTC",
    "updated_date": "2025-10-30 03:53:22 UTC"
  },
  {
    "arxiv_id": "2511.17532v2",
    "title": "Denoising Refinement Diffusion Models for Simultaneous Generation of Multi-scale Mobile Network Traffic",
    "authors": [
      "Xiaoqian Qi",
      "Haoye Chai",
      "Sichang Liu",
      "Lei Yue",
      "Raoyuan Pan",
      "Yue Wang",
      "Yong Li"
    ],
    "abstract": "The planning, management, and resource scheduling of cellular mobile networks require joint estimation of mobile traffic across different layers and nodes. Mobile traffic generation can proactively anticipate user demands and capture the dynamics of network load. However, existing methods mainly focus on generating traffic at a single spatiotemporal resolution, making it difficult to jointly model multi-scale traffic patterns. In this paper, we propose ZoomDiff, a diffusion-based model for multi-scale mobile traffic generation. ZoomDiff maps urban environmental context into mobile traffic with multiple spatial and temporal resolutions through a set of customized Denoising Refinement Diffusion Models (DRDM). DRDM employs a multi-stage noise-adding and denoising mechanism, enabling different stages to generate traffic at distinct spatiotemporal resolutions. This design aligns the progressive denoising process with hierarchical network layers, including base stations, cells, and grids of varying granularities. Experiments on real-world mobile traffic datasets show that ZoomDiff achieves at least an 18.4% improvement over state-of-the-art baselines in multi-scale traffic generation tasks. Moreover, ZoomDiff demonstrates strong efficiency and cross-city generalization, highlighting its potential as a powerful generative framework for modeling multi-scale mobile network dynamics.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.17532v2",
    "published_date": "2025-10-30 03:42:56 UTC",
    "updated_date": "2025-11-25 03:14:35 UTC"
  },
  {
    "arxiv_id": "2510.26105v1",
    "title": "Security Risk of Misalignment between Text and Image in Multi-modal Model",
    "authors": [
      "Xiaosen Wang",
      "Zhijin Ge",
      "Shaokang Wang"
    ],
    "abstract": "Despite the notable advancements and versatility of multi-modal diffusion models, such as text-to-image models, their susceptibility to adversarial inputs remains underexplored. Contrary to expectations, our investigations reveal that the alignment between textual and Image modalities in existing diffusion models is inadequate. This misalignment presents significant risks, especially in the generation of inappropriate or Not-Safe-For-Work (NSFW) content. To this end, we propose a novel attack called Prompt-Restricted Multi-modal Attack (PReMA) to manipulate the generated content by modifying the input image in conjunction with any specified prompt, without altering the prompt itself. PReMA is the first attack that manipulates model outputs by solely creating adversarial images, distinguishing itself from prior methods that primarily generate adversarial prompts to produce NSFW content. Consequently, PReMA poses a novel threat to the integrity of multi-modal diffusion models, particularly in image-editing applications that operate with fixed prompts. Comprehensive evaluations conducted on image inpainting and style transfer tasks across various models confirm the potent efficacy of PReMA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26105v1",
    "published_date": "2025-10-30 03:31:20 UTC",
    "updated_date": "2025-10-30 03:31:20 UTC"
  },
  {
    "arxiv_id": "2510.26099v1",
    "title": "SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth",
    "authors": [
      "Nick Masi",
      "Randall Balestriero"
    ],
    "abstract": "The dominant paradigm in machine learning is to assess model performance based on average loss across all samples in some test set. This amounts to averaging performance geospatially across the Earth in weather and climate settings, failing to account for the non-uniform distribution of human development and geography. We introduce Stratified Assessments of Forecasts over Earth (SAFE), a package for elucidating the stratified performance of a set of predictions made over Earth. SAFE integrates various data domains to stratify by different attributes associated with geospatial gridpoints: territory (usually country), global subregion, income, and landcover (land or water). This allows us to examine the performance of models for each individual stratum of the different attributes (e.g., the accuracy in every individual country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of state-of-the-art AI-based weather prediction models, finding that they all exhibit disparities in forecasting skill across every attribute. We use this to seed a benchmark of model forecast fairness through stratification at different lead times for various climatic variables. By moving beyond globally-averaged metrics, we for the first time ask: where do models perform best or worst, and which models are most fair? To support further work in this direction, the SAFE package is open source and available at https://github.com/N-Masi/safe",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26099v1",
    "published_date": "2025-10-30 03:22:55 UTC",
    "updated_date": "2025-10-30 03:22:55 UTC"
  },
  {
    "arxiv_id": "2510.26098v1",
    "title": "GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks",
    "authors": [
      "Chenrui Shi",
      "Zedong Yu",
      "Zhi Gao",
      "Ruining Feng",
      "Enqi Liu",
      "Yuwei Wu",
      "Yunde Jia",
      "Liuyu Xiang",
      "Zhaofeng He",
      "Qing Li"
    ],
    "abstract": "Large vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface perception, knowledge about recognizing widgets and system states; (2) interaction prediction, knowledge about reasoning action state transitions; and (3) instruction understanding, knowledge about planning, verifying, and assessing task completion progress. We further introduce GUI Knowledge Bench, a benchmark with multiple choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Our evaluation shows that current VLMs identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments on real world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26098v1",
    "published_date": "2025-10-30 03:22:30 UTC",
    "updated_date": "2025-10-30 03:22:30 UTC"
  },
  {
    "arxiv_id": "2510.26094v1",
    "title": "Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4",
    "authors": [
      "Yuxin Li",
      "Minghao Liu",
      "Ruida Wang",
      "Wenzhao Ji",
      "Zhitao He",
      "Rui Pan",
      "Junming Huang",
      "Tong Zhang",
      "Yi R. Fung"
    ],
    "abstract": "We present **Lean4PHYS**, a comprehensive reasoning framework for college-level physics problems in Lean4. **Lean4PHYS** includes *LeanPhysBench*, a college-level benchmark for formal physics reasoning in Lean4, which contains 200 hand-crafted and peer-reviewed statements derived from university textbooks and physics competition problems. To establish a solid foundation for formal reasoning in physics, we also introduce *PhysLib*, a community-driven repository containing fundamental unit systems and theorems essential for formal physics reasoning. Based on the benchmark and Lean4 repository we composed in **Lean4PHYS**, we report baseline results using major expert Math Lean4 provers and state-of-the-art closed-source models, with the best performance of DeepSeek-Prover-V2-7B achieving only 16% and Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that our *PhysLib* can achieve an average improvement of 11.75% in model performance. This demonstrates the challenging nature of our *LeanPhysBench* and the effectiveness of *PhysLib*. To the best of our knowledge, this is the first study to provide a physics benchmark in Lean4.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26094v1",
    "published_date": "2025-10-30 03:09:40 UTC",
    "updated_date": "2025-10-30 03:09:40 UTC"
  },
  {
    "arxiv_id": "2510.26089v1",
    "title": "Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing",
    "authors": [
      "Fazel Arasteh",
      "Arian Haghparast",
      "Manos Papagelis"
    ],
    "abstract": "Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 12 figures. Fazel Arasteh and Arian Haghparast contributed equally to this research. Submitted to ACM Transactions on Spatial Algorithms and Systems (TSAS). The code for this work is publicly available at https://github.com/Arianhgh/HHAN",
    "pdf_url": "https://arxiv.org/pdf/2510.26089v1",
    "published_date": "2025-10-30 02:49:46 UTC",
    "updated_date": "2025-10-30 02:49:46 UTC"
  },
  {
    "arxiv_id": "2510.26083v1",
    "title": "Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism",
    "authors": [
      "Yuhua Jiang",
      "Shuang Cheng",
      "Yihao Liu",
      "Ermo Hua",
      "Che Jiang",
      "Weigao Sun",
      "Yu Cheng",
      "Feifei Gao",
      "Biqing Qi",
      "Bowen Zhou"
    ],
    "abstract": "Specialized Generalist Models (SGMs) aim to preserve broad capabilities while achieving expert-level performance in target domains. However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. In this paper, we present Nirvana, an SGM with specialized memory mechanism, linear time complexity, and test-time task information extraction. Besides, we propose the Task-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory mechanism based on the current task's requirements. In Trigger, each incoming sample is treated as a self-supervised fine-tuning task, enabling Nirvana to adapt its task-related parameters on the fly to domain shifts. We also design the Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes the context guided by Trigger. We conduct experiments on both general language tasks and specialized medical tasks. On a variety of natural language modeling benchmarks, Nirvana achieves competitive or superior results compared to the existing LLM structures. To prove the effectiveness of Trigger on specialized tasks, we test Nirvana's performance on a challenging medical task, i.e., Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with lightweight codecs on paired electromagnetic signals and MRI images. Despite the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI domain with the change of task-related parameters. Nirvana achieves higher-quality MRI reconstruction compared to conventional MRI models as well as the models with traditional LLMs' backbone, and can also generate accurate preliminary clinical reports accordingly.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26083v1",
    "published_date": "2025-10-30 02:41:54 UTC",
    "updated_date": "2025-10-30 02:41:54 UTC"
  },
  {
    "arxiv_id": "2510.26840v1",
    "title": "SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification",
    "authors": [
      "Rocky Klopfenstein",
      "Yang He",
      "Andrew Tremante",
      "Yuepeng Wang",
      "Nina Narodytska",
      "Haoze Wu"
    ],
    "abstract": "Community-driven Text-to-SQL evaluation platforms play a pivotal role in tracking the state of the art of Text-to-SQL performance. The reliability of the evaluation process is critical for driving progress in the field. Current evaluation methods are largely test-based, which involves comparing the execution results of a generated SQL query and a human-labeled ground-truth on a static test database. Such an evaluation is optimistic, as two queries can coincidentally produce the same output on the test database while actually being different. In this work, we propose a new alternative evaluation pipeline, called SpotIt, where a formal bounded equivalence verification engine actively searches for a database that differentiates the generated and ground-truth SQL queries. We develop techniques to extend existing verifiers to support a richer SQL subset relevant to Text-to-SQL. A performance evaluation of ten Text-to-SQL methods on the high-profile BIRD dataset suggests that test-based methods can often overlook differences between the generated query and the ground-truth. Further analysis of the verification results reveals a more complex picture of the current Text-to-SQL evaluation.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.FL",
      "cs.LO"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26840v1",
    "published_date": "2025-10-30 02:29:54 UTC",
    "updated_date": "2025-10-30 02:29:54 UTC"
  },
  {
    "arxiv_id": "2510.26068v1",
    "title": "Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization",
    "authors": [
      "Di Zhang"
    ],
    "abstract": "This paper proposes a novel paradigm for machine learning that moves beyond traditional parameter optimization. Unlike conventional approaches that search for optimal parameters within a fixed geometric space, our core idea is to treat the model itself as a malleable geometric entity. Specifically, we optimize the metric tensor field on a manifold with a predefined topology, thereby dynamically shaping the geometric structure of the model space. To achieve this, we construct a variational framework whose loss function carefully balances data fidelity against the intrinsic geometric complexity of the manifold. The former ensures the model effectively explains observed data, while the latter acts as a regularizer, penalizing overly curved or irregular geometries to encourage simpler models and prevent overfitting. To address the computational challenges of this infinite-dimensional optimization problem, we introduce a practical method based on discrete differential geometry: the continuous manifold is discretized into a triangular mesh, and the metric tensor is parameterized by edge lengths, enabling efficient optimization using automatic differentiation tools. Theoretical analysis reveals a profound analogy between our framework and the Einstein-Hilbert action in general relativity, providing an elegant physical interpretation for the concept of \"data-driven geometry\". We further argue that even with fixed topology, metric optimization offers significantly greater expressive power than models with fixed geometry. This work lays a solid foundation for constructing fully dynamic \"meta-learners\" capable of autonomously evolving their geometry and topology, and it points to broad application prospects in areas such as scientific model discovery and robust representation learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DG",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.26068v1",
    "published_date": "2025-10-30 01:53:32 UTC",
    "updated_date": "2025-10-30 01:53:32 UTC"
  },
  {
    "arxiv_id": "2510.26061v1",
    "title": "Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems",
    "authors": [
      "Tomoharu Iwata",
      "Futoshi Futami"
    ],
    "abstract": "We propose a data-driven framework for efficiently solving quadratic programming (QP) problems by reducing the number of variables in high-dimensional QPs using instance-specific projection. A graph neural network-based model is designed to generate projections tailored to each QP instance, enabling us to produce high-quality solutions even for previously unseen problems. The model is trained on heterogeneous QPs to minimize the expected objective value evaluated on the projected solutions. This is formulated as a bilevel optimization problem; the inner optimization solves the QP under a given projection using a QP solver, while the outer optimization updates the model parameters. We develop an efficient algorithm to solve this bilevel optimization problem, which computes parameter gradients without backpropagating through the solver. We provide a theoretical analysis of the generalization ability of solving QPs with projection matrices generated by neural networks. Experimental results demonstrate that our method produces high-quality feasible solutions with reduced computation time, outperforming existing methods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26061v1",
    "published_date": "2025-10-30 01:32:21 UTC",
    "updated_date": "2025-10-30 01:32:21 UTC"
  },
  {
    "arxiv_id": "2511.00088v2",
    "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
    "authors": [
      "NVIDIA",
      ":",
      "Yan Wang",
      "Wenjie Luo",
      "Junjie Bai",
      "Yulong Cao",
      "Tong Che",
      "Ke Chen",
      "Yuxiao Chen",
      "Jenna Diamond",
      "Yifan Ding",
      "Wenhao Ding",
      "Liang Feng",
      "Greg Heinrich",
      "Jack Huang",
      "Peter Karkus",
      "Boyi Li",
      "Pinyi Li",
      "Tsung-Yi Lin",
      "Dongran Liu",
      "Ming-Yu Liu",
      "Langechuan Liu",
      "Zhijian Liu",
      "Jason Lu",
      "Yunxiang Mao",
      "Pavlo Molchanov",
      "Lindsey Pavao",
      "Zhenghao Peng",
      "Mike Ranzinger",
      "Ed Schmerling",
      "Shida Shen",
      "Yunfei Shi",
      "Sarah Tariq",
      "Ran Tian",
      "Tilman Wekel",
      "Xinshuo Weng",
      "Tianjun Xiao",
      "Eric Yang",
      "Xiaodong Yang",
      "Yurong You",
      "Xiaohui Zeng",
      "Wenyuan Zhang",
      "Boris Ivanovic",
      "Marco Pavone"
    ],
    "abstract": "End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. We introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning for complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a vision-language model pre-trained for Physical AI, with a diffusion-based trajectory decoder that generates dynamically feasible trajectories in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to enforce reasoning-action consistency and optimize reasoning quality. AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. Model weights are available at https://huggingface.co/nvidia/Alpamayo-R1-10B with inference code at https://github.com/NVlabs/alpamayo.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.00088v2",
    "published_date": "2025-10-30 01:25:34 UTC",
    "updated_date": "2026-01-07 09:09:57 UTC"
  },
  {
    "arxiv_id": "2511.07429v1",
    "title": "Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs",
    "authors": [
      "Hari Lee"
    ],
    "abstract": "We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.07429v1",
    "published_date": "2025-10-30 01:18:55 UTC",
    "updated_date": "2025-10-30 01:18:55 UTC"
  },
  {
    "arxiv_id": "2510.26057v1",
    "title": "Can AI be Accountable?",
    "authors": [
      "Andrew L. Kun"
    ],
    "abstract": "The AI we use is powerful, and its power is increasing rapidly. If this powerful AI is to serve the needs of consumers, voters, and decision makers, then it is imperative that the AI is accountable. In general, an agent is accountable to a forum if the forum can request information from the agent about its actions, if the forum and the agent can discuss this information, and if the forum can sanction the agent. Unfortunately, in too many cases today's AI is not accountable -- we cannot question it, enter into a discussion with it, let alone sanction it. In this chapter we relate the general definition of accountability to AI, we illustrate what it means for AI to be accountable and unaccountable, and we explore approaches that can improve our chances of living in a world where all AI is accountable to those who are affected by it.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "To be published as a chapter in Daniele Quercia and Marios Constantinides (Eds.). Operationalizing Responsible AI. Cambridge University Press. Forthcoming",
    "pdf_url": "https://arxiv.org/pdf/2510.26057v1",
    "published_date": "2025-10-30 01:16:33 UTC",
    "updated_date": "2025-10-30 01:16:33 UTC"
  },
  {
    "arxiv_id": "2510.26052v1",
    "title": "Dynamic VLM-Guided Negative Prompting for Diffusion Models",
    "authors": [
      "Hoyeon Chang",
      "Seungjin Kim",
      "Yoonseok Choi"
    ],
    "abstract": "We propose a novel approach for dynamic negative prompting in diffusion models that leverages Vision-Language Models (VLMs) to adaptively generate negative prompts during the denoising process. Unlike traditional Negative Prompting methods that use fixed negative prompts, our method generates intermediate image predictions at specific denoising steps and queries a VLM to produce contextually appropriate negative prompts. We evaluate our approach on various benchmark datasets and demonstrate the trade-offs between negative guidance strength and text-image alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: The First Workshop on Generative and Protective AI for Content Creation",
    "pdf_url": "https://arxiv.org/pdf/2510.26052v1",
    "published_date": "2025-10-30 01:10:25 UTC",
    "updated_date": "2025-10-30 01:10:25 UTC"
  },
  {
    "arxiv_id": "2511.00087v1",
    "title": "Adding New Capability in Existing Scientific Application with LLM Assistance",
    "authors": [
      "Anshu Dubey",
      "Akash Dhruv"
    ],
    "abstract": "With the emergence and rapid evolution of large language models (LLM), automating coding tasks has become an important research topic. Many efforts are underway and literature abounds about the efficacy of models and their ability to generate code. A less explored aspect of code generation is for new algorithms, where the training dataset would not have included any previous example of similar code. In this paper we propose a new methodology for writing code from scratch for a new algorithm using LLM assistance, and describe enhancement of a previously developed code-translation tool, Code-Scribe, for new code generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 4 figures, submitted to The 1st International Workshop on Foundational large Language Models Advances for HPC in Asia",
    "pdf_url": "https://arxiv.org/pdf/2511.00087v1",
    "published_date": "2025-10-30 01:09:25 UTC",
    "updated_date": "2025-10-30 01:09:25 UTC"
  },
  {
    "arxiv_id": "2510.26038v1",
    "title": "Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods",
    "authors": [
      "Jiali Cheng",
      "Chirag Agarwal",
      "Hadi Amiri"
    ],
    "abstract": "Knowledge distillation (KD) is an effective method for model compression and transferring knowledge between models. However, its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored. This study investigates the effect of knowledge distillation on the transferability of ``debiasing'' capabilities from teacher models to student models on natural language inference (NLI) and image classification tasks. Through extensive experiments, we illustrate several key findings: (i) overall the debiasing capability of a model is undermined post-KD; (ii) training a debiased model does not benefit from injecting teacher knowledge; (iii) although the overall robustness of a model may remain stable post-distillation, significant variations can occur across different types of biases; and (iv) we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD. Given the above findings, we propose three effective solutions to improve the distillability of debiasing methods: developing high quality data for augmentation, implementing iterative knowledge distillation, and initializing student models with weights obtained from teacher models. To the best of our knowledge, this is the first study on the effect of KD on debiasing and its interenal mechanism at scale. Our findings provide understandings on how KD works and how to design better debiasing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26038v1",
    "published_date": "2025-10-30 00:34:16 UTC",
    "updated_date": "2025-10-30 00:34:16 UTC"
  },
  {
    "arxiv_id": "2510.26037v1",
    "title": "SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning",
    "authors": [
      "Kaiwen Zhou",
      "Ahmed Elgohary",
      "A S M Iftekhar",
      "Amin Saied"
    ],
    "abstract": "The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26037v1",
    "published_date": "2025-10-30 00:32:58 UTC",
    "updated_date": "2025-10-30 00:32:58 UTC"
  },
  {
    "arxiv_id": "2510.26032v1",
    "title": "Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings",
    "authors": [
      "Felipe Larios",
      "Mariana Borras-Osorio",
      "Yuqi Wu",
      "Ana Gabriela Claros",
      "David Toro-Tobon",
      "Esteban Cabezas",
      "Ricardo Loor-Torres",
      "Maria Mateo Chavez",
      "Kerly Guevara Maldonado",
      "Luis Vilatuna Andrango",
      "Maria Lizarazo Jimenez",
      "Ivan Mateo Alzamora",
      "Misk Al Zahidy",
      "Marcelo Montero",
      "Ana Cristina Proano",
      "Cristian Soto Jacome",
      "Jungwei W. Fan",
      "Oscar J. Ponce-Ponte",
      "Megan E. Branda",
      "Naykky Singh Ospina",
      "Juan P. Brito"
    ],
    "abstract": "Importance Incidental thyroid findings (ITFs) are increasingly detected on imaging performed for non-thyroid indications. Their prevalence, features, and clinical consequences remain undefined. Objective To develop, validate, and deploy a natural language processing (NLP) pipeline to identify ITFs in radiology reports and assess their prevalence, features, and clinical outcomes. Design, Setting, and Participants Retrospective cohort of adults without prior thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline identified ITFs and extracted nodule characteristics from image reports from multiple modalities and body regions. Main Outcomes and Measures Prevalence of ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer diagnosis. Logistic regression identified demographic and imaging-related factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9% women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more likely in women, older adults, those with higher BMI, and when imaging was ordered by oncology or internal medicine. Compared with chest CT, ITFs were more likely via neck CT, PET, and nuclear medicine scans. Nodule characteristics were poorly documented, with size reported in 44% and other features in fewer than 15% (e.g. calcifications). Compared with patients without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis, biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were common and strongly associated with cascades leading to the detection of small, low-risk cancers. These findings underscore the role of ITFs in thyroid cancer overdiagnosis and the need for standardized reporting and more selective follow-up.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.26032v1",
    "published_date": "2025-10-30 00:15:07 UTC",
    "updated_date": "2025-10-30 00:15:07 UTC"
  }
]
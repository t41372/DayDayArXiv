{
  "date": "2025-03-09",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间2025-03-09的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的研究热点集中在大型语言模型（LLM）的深入探索，特别是在多智能体系统、多模态理解与生成、推理能力增强以及隐私安全审计方面。同时，强化学习在各种应用中的新进展、生成模型在科学发现和图像处理中的应用也备受关注。值得注意的论文包括对 LLM 隐私审计提出新标准的研究、利用强化学习提升多模态模型推理能力的 Vision-R1、探索 LLM 幻觉新形式“妄想”的研究、以及统一序列和结构生成的 UniGenX 框架。\n\n以下是今天值得关注的论文：\n\n---\n\n**1. 大型语言模型的隐私审计 (Privacy Auditing of Large Language Models)**\n\n这篇 ICLR 2025 论文提出了一种更有效的 LLM 隐私审计方法。研究者开发了比以往工作更强大的\"金丝雀\"（canaries）数据，用于在多种现实威胁模型下进行成员推断攻击。实验表明，该方法显著提高了隐私泄露检测的准确率（例如，在 Qwen2.5-0.5B 上，TPR 达到 49.6% @ 1% FPR，远超之前的 4.2%），并首次在攻击者无法训练影子模型、插入梯度金丝雀或访问模型中间状态的情况下，实现了对 LLM 训练隐私（ε ≈ 1）的有效审计，为衡量和验证 LLM 的隐私保护水平设定了新标准。\n\n**13. Vision-R1：激励多模态大型语言模型中的推理能力 (Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models)**\n\n受 DeepSeek-R1-Zero 的启发，本文探索如何利用强化学习（RL）增强多模态大语言模型（MLLM）的推理能力。研究者提出了 Vision-R1 模型，首先利用现有 MLLM 和 DeepSeek-R1 构建了一个高质量的多模态思维链（CoT）数据集（Vision-R1-cold dataset）进行冷启动。然后，采用渐进式思维抑制训练（PTST）策略和组相对策略优化（GRPO）算法，在多模态数学数据集上逐步优化模型的复杂推理过程。实验表明，Vision-R1 在多个多模态数学推理基准上平均提升约 6%，7B 模型在 MathVista 上准确率达 73.5%，接近 OpenAI O1。\n\n**20. 大型语言模型的妄想 (Delusions of Large Language Models)**\n\n本文识别并研究了 LLM 中的一种比幻觉更隐蔽的现象——“妄想”（delusion），即模型以异常高的置信度产生的事实错误但看似合理的输出。研究发现，妄想普遍存在且不同于普通幻觉，它们具有较低的不确定性，难以通过微调或自我反思来纠正，对模型可靠性构成重大挑战。文章探讨了妄想的形成与训练动态和数据集噪声的关系，并探索了如检索增强生成（RAG）和多智能体辩论等缓解策略。\n\n**23. InftyThink：打破大型语言模型长上下文推理的长度限制 (InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models)**\n\n针对当前长上下文推理面临的计算成本高、受限于最大上下文长度和性能衰减等问题，本文提出了 InftyThink 范式。该方法将长推理过程分解为迭代的短推理片段和中间摘要，实现了无界推理深度和有界计算成本，形成独特的“锯齿状”内存模式。研究者还将 OpenR1-Math 数据集转换为迭代格式进行训练。实验证明，该方法在降低计算成本的同时提高了性能，为复杂推理提供了一种更具可扩展性的方法。\n\n**37. Agent 模型：将行动链生成内化到推理模型中 (Agent models: Internalizing Chain-of-Action Generation into Reasoning models)**\n\n传统 Agent 工作流依赖外部提示管理工具交互，限制了模型的自主性。本文提出大型 Agent 模型（LAMs）的概念，将“行动链”（Chain-of-Action, CoA）的生成内化到模型中，使其能自主决定何时以及如何使用外部工具。提出的 AutoCoA 框架结合了监督微调（SFT）和强化学习（RL），使模型能在推理和行动间无缝切换，并通过内部世界模型减少与真实环境的交互成本。在开放域 QA 任务上的评估表明，AutoCoA 训练的 Agent 模型在需要长期推理和多步行动的任务中显著优于基于 ReAct 的工作流。\n\n**65. HuixiangDou2：一个稳健优化的 GraphRAG 方法 (HuixiangDou2: A Robustly Optimized GraphRAG Approach)**\n\n针对 LLM 在专业或新兴主题上表现不佳的问题，GraphRAG 通过构建知识图谱进行动态检索来增强。本文介绍 HuixiangDou2，一个优化的 GraphRAG 框架。该框架利用双层检索并在 32k 上下文中优化其性能，比较了基于逻辑的检索和双层检索。通过在特定领域数据集上的实验，证明了双层检索增强模糊匹配，逻辑形式检索改进结构化推理。此外，还提出了多阶段验证机制以提高检索鲁棒性。该方法显著提高了 Qwen2.5-7B-Instruct 在测试集上的得分（从 60 到 74.5），并已开源。\n\n**75. 用于对话推荐系统的图检索增强 LLM (Graph Retrieval-Augmented LLM for Conversational Recommendation Systems)**\n\n为解决对话推荐系统（CRS）中知识稀疏和复杂偏好推理的挑战，本文提出了 G-CRS 框架。该框架无需领域特定训练，结合了图检索增强生成和上下文学习。G-CRS 使用基于 GNN 的图推理器识别候选项目，并通过个性化 PageRank 探索发现潜在项目和相似用户交互。这些检索到的上下文被转换为结构化提示，供 LLM 进行推理，从而实现基于上下文的推荐。实验表明，G-CRS 在无需任务特定训练的情况下优于现有方法。\n\n**89. 通用量表通过解释和预测能力解锁 AI 评估 (General Scales Unlock AI Evaluation with Explanatory and Predictive Power)**\n\n为理解和预测通用 AI 系统在不同任务上的表现，本文提出了一种基于通用量表的 AI 评估方法。该方法使用 18 个新构建的、不会饱和的评分标准（rubrics）来评估实例需求，从而解释现有基准的测量内容、提取 AI 系统的能力画像，并预测其在新任务（包括分布内和分布外）上的表现。通过对 15 个 LLM 和 63 个任务的评估，该方法展示了强大的解释力（揭示基准的敏感性和特异性，模型规模、CoT 等对能力的影响）和预测力（优于基于嵌入或微调的基线预测器，尤其在 OOD 场景下）。该研究为 AI 评估提供了重要进展。\n\n---\n\n**多模态与视觉语言模型 (Multimodal & Vision-Language Models)**\n\n*   **12. SemHiTok：用于多模态理解和生成的基于语义引导分层码本的统一图像 Tokenizer (SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation)**：提出 SemHiTok，通过语义引导的分层码本解决多模态理解和生成任务中特征表示不一致的问题，解耦语义和像素重建训练，在不降低高级语义特征提取能力的情况下增强低级纹理特征提取。\n*   **26. 小型视觉语言模型：紧凑架构和技术综述 (Small Vision-Language Models: A Survey on Compact Architectures and Techniques)**：综述了小型视觉语言模型（sVLM）的发展，介绍了 transformer、mamba 和混合架构，讨论了知识蒸馏、轻量级注意力等技术，分析了精度、效率和可扩展性之间的权衡，并指出了数据偏差、泛化等挑战。\n*   **28. AA-CLIP：通过异常感知 CLIP 增强零样本异常检测 (AA-CLIP: Enhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP)**：提出 AA-CLIP，通过两阶段方法（创建异常感知文本锚点、对齐视觉特征）增强 CLIP 在文本和视觉空间中的异常区分能力，同时保留其泛化能力，在工业和医疗零样本异常检测任务中取得 SOTA 结果。\n*   **35. DiffCLIP：差分注意力遇见 CLIP (DiffCLIP: Differential Attention Meets CLIP)**：将用于 LLM 的差分注意力机制扩展到 CLIP 架构，提出 DiffCLIP。通过少量额外参数，在零样本分类、检索和鲁棒性基准上优于基线 CLIP，且计算开销可忽略。\n*   **40. Conceptrol：零样本个性化图像生成的概念控制 (Conceptrol: Concept Control of Zero-shot Personalized Image Generation)**：提出 Conceptrol 框架，通过文本概念掩码约束零样本适配器（如 IP-Adapter）的视觉注意力，增强主题驱动生成能力，显著改善个性化内容保留和文本提示遵循之间的平衡，效果优于原版 IP-Adapter，甚至可超过 Dreambooth LoRA 等微调方法。\n*   **45. ARMOR v0.1：通过非对称协同赋能自回归多模态理解模型进行交错多模态生成 (ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy)**：提出 ARMOR，一个资源高效的纯自回归框架，通过微调现有 MLLM 实现理解和生成。引入非对称编码器-解码器架构、精心策划的交错数据集和“生成什么或如何生成”算法，使 MLLM 具备多模态生成能力，同时保留理解能力。\n*   **51. GFlowVLM：利用生成流网络增强视觉语言模型中的多步推理 (GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks)**：提出 GFlowVLM 框架，使用生成流网络（GFlowNets）微调 VLM，以促进复杂推理任务中多样化解决方案的生成。该方法将环境建模为非马尔可夫决策过程，通过 CoT 推理指导行动选择，并在卡牌游戏和具身规划任务中展示了优于 SFT 和 RL 的训练效率、方案多样性和泛化能力。\n*   **55. DynamicID：具有灵活面部可编辑性的零样本多 ID 图像个性化 (DynamicID: Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability)**：提出 DynamicID，一个免调优框架，通过双阶段训练支持单 ID 和多 ID 个性化生成，具有高保真度和灵活的面部编辑能力。关键创新包括语义激活注意力（SAA）和身份-运动重构器（IMR）。\n*   **57. 自动驾驶视觉语言模型安全认知能力评估 (Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving)**：提出 SCD-Bench，一个评估 VLM 在自动驾驶交互中安全认知能力的新基准。开发了自动驾驶图文标注系统（ADA）和基于 LLM 的自动评估方法。初步结果显示现有开源模型（尤其轻量级模型）安全认知不足，与 GPT-4o 差距显著。\n*   **59. PerturboLLaVA：通过扰动视觉训练减少多模态幻觉 (PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training)**：为解决 MLLM 中的幻觉问题（尤其在密集图像描述任务中），提出新指标 HalFscore 用于细粒度评估描述质量，并提出 PerturboLLaVA 方法。该方法通过在训练中加入对抗性扰动的文本，减少模型对语言先验的依赖，增强对视觉输入的关注，从而减少幻觉，提高描述保真度。\n*   **60. 使用帧和事件流的标志语言翻译：基准数据集和算法 (Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms)**：为解决 RGB 帧在手语翻译中的局限性（帧率、光照、运动模糊），提出利用事件流辅助 RGB 相机。收集了大规模 RGB-事件手语翻译数据集 VECSL，并提出了新的 RGB-事件手语翻译框架 M$^2$-SLT，结合了细粒度微手语和粗粒度宏手语检索，取得了 SOTA 结果。\n*   **68. 三思而后行，一键点击：通过快慢系统增强 GUI 定位 (Think Twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems)**：受人类双系统认知启发，提出 Focus 框架用于 GUI 定位。该框架结合快速预测和系统分析，通过自适应系统切换处理不同复杂度的任务，将定位分解为界面摘要、视觉聚焦分析和精确坐标预测。实验表明，Focus 在复杂 GUI 场景下表现优越。\n*   **88. CLIMB：大规模多模态临床基础模型的数据基础 (CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models)**：推出 CLIMB，一个大规模临床基准，整合了成像、语言、时序和图谱等多种模态的 451 万患者样本（19.01 TB）。评估表明，多任务预训练显著提升了模型在欠研究领域（如超声、心电图）的性能和泛化能力，为临床 AI 发展奠定基础。\n\n---\n\n**生成模型与图像处理 (Generative Models & Image Processing)**\n\n*   **6. GenDR：闪电生成细节恢复器 (GenDR: Lightning Generative Detail Restorator)**：提出 GenDR，一个用于生成细节恢复的单步扩散模型。通过从具有更大潜空间的定制扩散模型中蒸馏得到。具体包括训练新的 SD2.1-VAE16，提出一致性分数同一性蒸馏（CiD）和对抗性学习扩展（CiDA），以弥合 T2I 和 SR 任务之间的差距，实现更快的推理速度和更好的细节保真度。\n*   **8. 海底无限联盟：通过潜在分形扩散模型生成逼真的 3D 水下地形 (Infinite Leagues Under the Sea: Photorealistic 3D Underwater Terrain Generation by Latent Fractal Diffusion Models)**：提出 DreamSea，一个用于生成超现实水下场景的生成模型。该模型在水下机器人勘测收集的真实图像数据库上训练，利用视觉基础模型提取 3D 几何和语义，并训练一个扩散模型生成 RGBD 水下图像。最终融合生成图像构建 3DGS 模型，实现逼真的新视角渲染。\n*   **25. UniGenX：通过自回归扩散统一生成序列和结构 (UniGenX: Unified Generation of Sequence and Structure with Autoregressive Diffusion)**：提出 UniGenX 框架，结合自回归下一令牌预测和条件扩散模型，用于统一生成科学数据（材料、分子、蛋白质）的序列和结构。该方法利用自回归模型简化条件扩散模型的训练，同时扩散生成头提高自回归预测的精度。在材料和分子生成任务上取得 SOTA 性能。\n*   **42. LSA：面向染色无关宫颈癌筛查的潜在风格增强 (LSA: Latent Style Augmentation Towards Stain-Agnostic Cervical Cancer Screening)**：为解决全切片图像（WSI）宫颈癌筛查中染色差异导致的域漂移问题，提出潜在风格增强（LSA）框架。该框架在 WSI 级潜在特征上执行高效的在线染色增强，通过 WSI 级染色增强方法 WSAug 和 Stain Transformer 模拟目标风格，显著提高模型在不同扫描仪数据上的性能。\n*   **53. LightMotion：一种用于模拟视频生成中相机运动的轻量级免调优方法 (LightMotion: A Light and Tuning-free Method for Simulating Camera Motion in Video Generation)**：提出 LightMotion，一种在潜在空间中模拟相机运动（平移、缩放、旋转）的轻量级免调优方法。通过潜在空间置换操作模拟运动，通过潜在空间重采样策略（背景感知采样和跨帧对齐）填充新视角，并提出潜在空间校正解决 SNR 偏移问题，提高视频生成质量。\n*   **70. StructGS：用于卓越 3D 高斯溅射的自适应球谐函数和渲染增强 (StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting)**：提出 StructGS 框架，通过引入基于补丁的 SSIM 损失、动态球谐函数初始化和多尺度残差网络（MSRN），增强 3D 高斯溅射（3DGS）的性能，以更好地捕捉非局部结构信息、减少计算冗余并支持从低分辨率输入渲染高分辨率输出。\n\n---\n\n**强化学习与智能体 (Reinforcement Learning & Agents)**\n\n*   **5. AutoMisty：用于 Misty 社交机器人自动化代码生成的多智能体 LLM 框架 (AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot)**：提出 AutoMisty，首个基于 LLM 的多智能体协作框架，可将自然语言指令转换为 Misty 机器人的可执行代码。包含任务分解、分配、解决和综合四个智能体模块，并采用自反思和人机回圈优化。实验证明其能生成高质量代码并实现精确控制。\n*   **9. Dr Genre：用于通用文本重写的解耦 LLM 反馈强化学习 (Dr Genre: Reinforcement Learning from Decoupled LLM Feedback for Generic Text Rewriting)**：提出 Dr Genre，一个用于通用文本重写的解耦奖励学习框架。该框架使用面向目标的奖励模型和任务特定权重，训练一个能处理事实性、风格化和对话式重写任务的通用模型。同时构建了对话式重写数据集 ChatRewrite。评估表明该方法在指令遵循、一致性和简洁性方面均有提升。\n*   **14. 网络化智能体的完全去中心化 MADDPG (Fully-Decentralized MADDPG with Networked Agents)**：为多智能体强化学习设计了三种具有去中心化训练的 actor-critic 算法，适用于合作、对抗和混合环境。通过引入代理策略和允许训练期间的局部通信，使去中心化算法在经验测试中达到与原始 MADDPG 相当的结果，同时降低了计算成本。\n*   **15. 超越黑盒基准测试：Agentic 系统的可观察性、分析和优化 (Beyond Black-Box Benchmarking: Observability, Analytics, and Optimization of Agentic Systems)**：探讨了观察、分析和优化 Agentic AI 系统（多个 Agent 协作）面临的挑战，如非确定性、上下文敏感性和动态性。提出需要超越传统基准测试，引入分类法来定义预期分析结果和收集方法，并演示了一种基于 Agent 运行时日志的新基准测试方法。\n*   **21. 用于安全强化学习的概率屏蔽 (Probabilistic Shielding for Safe Reinforcement Learning)**：提出一种新的、可扩展的安全强化学习方法，适用于 MDP 安全动态已知且安全定义为概率规避属性的情况。该方法基于状态增强和设计一个限制 Agent 行动的“盾牌”，提供严格的形式化安全保证，并在实验中验证了可行性。\n*   **24. 用于估计带删失结果的动态治疗方案的删失感知基于树的强化学习 (Censoring-Aware Tree-Based Reinforcement Learning for Estimating Dynamic Treatment Regimes with Censored Outcomes)**：提出 CA-TRL 框架，通过增强传统基于树的强化学习方法（结合 AIPW 和删失感知修改），处理删失数据以估计最优动态治疗方案（DTR）。在模拟和真实癫痫数据集上的实验证明了其有效性。\n*   **34. 通过轮次级优化重新审视性掠夺者的早期检测 (Revisiting Early Detection of Sexual Predators via Turn-level Optimization)**：为解决在线诱骗（grooming）早期检测中聊天级标签导致的弱监督问题，提出速度控制强化学习（SCoRL）。该方法基于诱骗通信理论（LCT）使用轮次级风险标签，并设计了平衡速度和准确性的奖励函数，以识别最佳干预时机。同时引入轮次级评估指标。\n*   **76. 视觉生成溯因学习的元规则选择策略预训练 (Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive Learning)**：为解决视觉生成溯因学习中逻辑溯因耗时的问题，提出预训练元规则选择策略的方法。该方法基于案例和元规则的嵌入表示构建选择模型，在纯符号数据上预训练，显著减少候选元规则集，提高学习效率。\n*   **80. Swift Hydra：用于异常检测的多 Mamba 模型自增强生成框架 (Swift Hydra: Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models)**：提出 Swift Hydra 框架，结合生成 AI 和强化学习进行异常检测。RL 策略操作生成模型的潜变量以合成能绕过检测器的新异常样本，这些样本反过来增强检测模型。框架还使用 Mamba 模型的 MoE 结构，在 ADBench 上表现优于 SOTA 方法。\n*   **85. 通过 GCN 辅助的启发式算法优化最小顶点覆盖求解 (Optimizing Minimum Vertex Cover Solving via a GCN-assisted Heuristic Algorithm)**：提出 GCNIVC，一种用于求解大规模图最小顶点覆盖（MVC）问题的新启发式搜索算法。利用 GCN 捕获全局结构生成高质量初始解，并引入基于三个容器和双覆盖边（dc-edges）概念的新启发式策略。实验证明其优于 SOTA 算法。\n\n---\n\n**其他值得关注的论文**\n\n*   **3. 用类星形胶质细胞单元表征脉冲神经网络的学习 (Characterizing Learning in Spiking Neural Networks with Astrocyte-Like Units)**：在脉冲神经网络中加入类星形胶质细胞单元，发现神经元和星形胶质细胞的组合对驱动学习至关重要，且当两者比例约为 2:1 时学习率最高，表明模拟更真实的生物网络结构可能提升学习效果。\n*   **4. 用于癌症恶病质早期检测的多模态 AI 驱动生物标志物 (Multimodal AI-driven Biomarker for Early Detection of Cancer Cachexia)**：提出一种基于多模态 AI 的生物标志物，利用 LLM 和基础模型整合患者的人口统计、疾病状态、实验室报告、影像（CT）和临床笔记等异构数据，用于早期检测癌症恶病质，优于现有固定阈值方法。\n*   **10. 大型语言模型是有效的人类标注助手，但不是好的独立标注者 (Large Language Models Are Effective Human Annotation Assistants, But Not Good Independent Annotators)**：评估 LLM 在事件标注工作流（文档去重、合并、标注）中的作用，发现 LLM 作为自动化标注器不如人类专家可靠，但作为专家助手的 Event Set Curation 可以减少时间和精力。\n*   **16. 绿色提示 (Green Prompting)**：研究不同提示和响应特性对 LLM 推理能耗的影响。发现提示的语义意义比长度更重要，特定关键词与能耗相关，强调了提示设计对优化推理效率的重要性。\n*   **17. 预训练语言模型表示中的性别编码模式 (Gender Encoding Patterns in Pretrained Language Model Representations)**：使用信息论方法分析不同编码器架构如何内部表示和传播性别偏见。发现不同模型存在一致的性别编码模式，而去偏见技术效果有限，有时甚至会增加内部表示的偏见。\n*   **22. PFDial：一种基于 UML 流程图的结构化对话指令微调方法 (PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts)**：为解决 LLM 在严格流程驱动对话任务中的困难，构建了包含 1.2 万条高质量中文对话指令的 PFDial 数据集。实验表明，在该数据集上微调的小模型（0.5B-8B）能显著超越 GPT-4o。\n*   **32. BTFL：一种用于联邦学习中内外部数据分布的基于贝叶斯的测试时泛化方法 (BTFL: A Bayesian-based Test-Time Generalization Method for Internal and External Data Distributions in Federated learning)**：针对联邦学习在线部署中的分布漂移问题，提出 TGFL 场景和 BTFL 方法。BTFL 在测试时通过双贝叶斯框架平衡个性化和泛化，根据历史测试数据和当前样本特性进行预测插值，提高了模型在 IND 和 EXD 下的适应性。\n*   **38. WildIFEval：真实世界中的指令遵循 (WildIFEval: Instruction Following in the Wild)**：发布 WildIFEval 数据集，包含 1.2 万条带有多个约束条件的真实用户指令。评估发现所有主流 LLM 在处理多约束指令时性能都会下降，且约束类型对性能有显著影响。\n*   **41. 人类认知启发的知识图谱增强 RAG 用于复杂问题解决 (Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving)**：提出 CogGRAG，一个受人类认知启发的基于图的 RAG 框架。通过分解、检索、带自验证的推理三阶段方法，增强 LLM 在知识图谱问答（KGQA）等复杂问题解决任务中的准确性。\n*   **43. ProJudge：用于基于 MLLM 的过程评判的多模态多学科基准和指令调优数据集 (ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges)**：发布 ProJudgeBench，首个评估 MLLM 作为科学问题解决过程评判者能力的基准。包含 2400 个跨学科案例和 5 万多个步骤级标签。同时提出 ProJudge-173k 指令调优数据集和动态双阶段微调策略，提升开源模型的过程评估能力。\n*   **49. 小型语言模型能可靠抵御越狱攻击吗？全面评估 (Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation)**：首次对 SLM 的越狱攻击脆弱性进行大规模实证研究。评估 63 个 SLM 后发现近半数易受攻击，并识别出模型大小、架构、训练数据和技术是关键影响因素。现有防御方法效果有限，强调了 SLM 安全设计的重要性。\n*   **62. PDB：并非所有驾驶员都一样——用于理解驾驶行为的个性化数据集 (PDB: Not All Drivers Are the Same -- A Personalized Dataset for Understanding Driving Behavior)**：发布 PDB 数据集，一个捕捉自然驾驶条件下驾驶行为个性化的多模态数据集。包含 LiDAR、摄像头、GNSS、IMU、CAN 总线以及驾驶员面部视频和心率数据，旨在支持人因分析、驾驶员识别和个性化移动应用。\n*   **67. 用于大脑-图像对齐的最优传输：揭示神经信息处理中的冗余与协同 (Optimal Transport for Brain-Image Alignment: Unveiling Redundancy and Synergy in Neural Information Processing)**：提出使用最优传输（OT）来对齐大脑信号（体素嵌入）和图像嵌入，优于传统的 MSE。该方法构建传输计划实现更精确匹配，并通过控制传输量减轻冗余信息影响。在脑图谱描述任务上取得 SOTA，并揭示了大脑信息处理的冗余和协同作用。\n*   **74. Seesaw：通过模型重分片实现高吞吐量 LLM 推理 (Seesaw: High-throughput LLM Inference via Model Re-sharding)**：提出 Seesaw 推理引擎，通过动态模型重分片技术，在 LLM 推理的预填充和解码阶段动态调整并行策略，以最大化吞吐量。结合分层 KV 缓存缓冲和最小化转换调度，减少重分片开销。评估显示其吞吐量比 vLLM 最高提升 1.78 倍。\n*   **91. 机器学习遇见代数组合学：捕捉纯数学研究级猜想能力的数据集套件 (Machine Learning meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics)**：发布 ACD Repo，一个包含代数组合学领域基础结果或开放问题的数据集集合，旨在评估和训练 AI 的数学猜想能力。每个数据集包含开放式研究级问题和大量示例。\n\n---\n\n今天的快报就到这里，希望能帮助你快速了解 arXiv 的最新动态！",
  "papers": [
    {
      "arxiv_id": "2503.06808v1",
      "title": "Privacy Auditing of Large Language Models",
      "title_zh": "大型语言模型的隐私审计",
      "authors": [
        "Ashwinee Panda",
        "Xinyu Tang",
        "Milad Nasr",
        "Christopher A. Choquette-Choo",
        "Prateek Mittal"
      ],
      "abstract": "Current techniques for privacy auditing of large language models (LLMs) have\nlimited efficacy -- they rely on basic approaches to generate canaries which\nleads to weak membership inference attacks that in turn give loose lower bounds\non the empirical privacy leakage. We develop canaries that are far more\neffective than those used in prior work under threat models that cover a range\nof realistic settings. We demonstrate through extensive experiments on multiple\nfamilies of fine-tuned LLMs that our approach sets a new standard for detection\nof privacy leakage. For measuring the memorization rate of non-privately\ntrained LLMs, our designed canaries surpass prior approaches. For example, on\nthe Qwen2.5-0.5B model, our designed canaries achieve $49.6\\%$ TPR at $1\\%$\nFPR, vastly surpassing the prior approach's $4.2\\%$ TPR at $1\\%$ FPR. Our\nmethod can be used to provide a privacy audit of $\\varepsilon \\approx 1$ for a\nmodel trained with theoretical $\\varepsilon$ of 4. To the best of our\nknowledge, this is the first time that a privacy audit of LLM training has\nachieved nontrivial auditing success in the setting where the attacker cannot\ntrain shadow models, insert gradient canaries, or access the model at every\niteration.",
      "tldr_zh": "该研究提出了一种更有效的大型语言模型(LLMs)隐私审计方法，通过设计更强大的canaries（测试样本）来检测隐私泄露。实验表明，该方法在多种微调LLMs上显著优于现有技术，例如在Qwen2.5-0.5B模型上，True Positive Rate (TPR)从4.2%提升至49.6%。该方法首次在攻击者无法训练影子模型、插入梯度canaries或访问每轮迭代模型的情况下，实现了非平凡的隐私审计成功，为LLMs的隐私保护提供了更严格的评估标准。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06808v1",
      "published_date": "2025-03-09 23:32:15 UTC",
      "updated_date": "2025-03-09 23:32:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:23:22.172139"
    },
    {
      "arxiv_id": "2503.06803v1",
      "title": "Actionable AI: Enabling Non Experts to Understand and Configure AI Systems",
      "title_zh": "可操作的人工智能：使非专家能够理解并配置AI系统",
      "authors": [
        "Cécile Boulard",
        "Sruthi Viswanathan",
        "Wanda Fey",
        "Thierry Jacquin"
      ],
      "abstract": "Interaction between humans and AI systems raises the question of how people\nunderstand AI systems. This has been addressed with explainable AI, the\ninterpretability arising from users' domain expertise, or collaborating with AI\nin a stable environment. In the absence of these elements, we discuss designing\nActionable AI, which allows non-experts to configure black-box agents. In this\npaper, we experiment with an AI-powered cartpole game and observe 22 pairs of\nparticipants to configure it via direct manipulation. Our findings suggest\nthat, in uncertain conditions, non-experts were able to achieve good levels of\nperformance. By influencing the behaviour of the agent, they exhibited an\noperational understanding of it, which proved sufficient to reach their goals.\nBased on this, we derive implications for designing Actionable AI systems. In\nconclusion, we propose Actionable AI as a way to open access to AI-based\nagents, giving end users the agency to influence such agents towards their own\ngoals.",
      "tldr_zh": "本文提出\"可操作AI\"(Actionable AI)概念，旨在让非专家用户也能配置黑箱AI系统。研究者通过AI驱动的倒立摆游戏实验，发现非专业用户在不确定条件下仍能通过直接操控获得良好操作效果，并展现出对AI行为的运作性理解。该研究为设计面向终端用户的可操作AI系统提供了新思路，使普通用户能够根据自身目标影响AI代理行为。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06803v1",
      "published_date": "2025-03-09 23:09:04 UTC",
      "updated_date": "2025-03-09 23:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:23:42.070503"
    },
    {
      "arxiv_id": "2503.06798v1",
      "title": "Characterizing Learning in Spiking Neural Networks with Astrocyte-Like Units",
      "title_zh": "基于星形胶质细胞单元的特性研究脉冲神经网络中的学习机制",
      "authors": [
        "Christopher S. Yang",
        "Sylvester J. Gates III",
        "Dulara De Zoysa",
        "Jaehoon Choe",
        "Wolfgang Losert",
        "Corey B. Hart"
      ],
      "abstract": "Traditional artificial neural networks take inspiration from biological\nnetworks, using layers of neuron-like nodes to pass information for processing.\nMore realistic models include spiking in the neural network, capturing the\nelectrical characteristics more closely. However, a large proportion of brain\ncells are of the glial cell type, in particular astrocytes which have been\nsuggested to play a role in performing computations. Here, we introduce a\nmodified spiking neural network model with added astrocyte-like units in a\nneural network and asses their impact on learning. We implement the network as\na liquid state machine and task the network with performing a chaotic\ntime-series prediction task. We varied the number and ratio of neuron-like and\nastrocyte-like units in the network to examine the latter units effect on\nlearning. We show that the combination of neurons and astrocytes together, as\nopposed to neural- and astrocyte-only networks, are critical for driving\nlearning. Interestingly, we found that the highest learning rate was achieved\nwhen the ratio between astrocyte-like and neuron-like units was roughly 2 to 1,\nmirroring some estimates of the ratio of biological astrocytes to neurons. Our\nresults demonstrate that incorporating astrocyte-like units which represent\ninformation across longer timescales can alter the learning rates of neural\nnetworks, and the proportion of astrocytes to neurons should be tuned\nappropriately to a given task.",
      "tldr_zh": "本研究提出了一种改进的脉冲神经网络模型，在传统神经元单元基础上加入了类似星形胶质细胞（astrocyte-like）的计算单元。通过采用液态状态机架构执行混沌时间序列预测任务，研究发现神经元与星形胶质细胞的协同作用对学习能力至关重要。实验表明当星形胶质细胞与神经元比例约为2:1时（接近生物大脑中的估算比例），网络达到最佳学习速率，证实了这些具备长时程信息处理能力的非神经元单元能够显著影响网络学习性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.bio-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06798v1",
      "published_date": "2025-03-09 22:36:58 UTC",
      "updated_date": "2025-03-09 22:36:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:23:42.108169"
    },
    {
      "arxiv_id": "2503.06797v1",
      "title": "Multimodal AI-driven Biomarker for Early Detection of Cancer Cachexia",
      "title_zh": "多模态AI驱动的癌症恶病质早期检测生物标志物",
      "authors": [
        "Sabeen Ahmed",
        "Nathan Parker",
        "Margaret Park",
        "Evan W. Davis",
        "Jennifer B. Permuth",
        "Matthew B. Schabath",
        "Yasin Yilmaz",
        "Ghulam Rasool"
      ],
      "abstract": "Cancer cachexia is a multifactorial syndrome characterized by progressive\nmuscle wasting, metabolic dysfunction, and systemic inflammation, leading to\nreduced quality of life and increased mortality. Despite extensive research, no\nsingle definitive biomarker exists, as cachexia-related indicators such as\nserum biomarkers, skeletal muscle measurements, and metabolic abnormalities\noften overlap with other conditions. Existing composite indices, including the\nCancer Cachexia Index (CXI), Modified CXI (mCXI), and Cachexia Score (CASCO),\nintegrate multiple biomarkers but lack standardized thresholds, limiting their\nclinical utility. This study proposes a multimodal AI-based biomarker for early\ncancer cachexia detection, leveraging open-source large language models (LLMs)\nand foundation models trained on medical data. The approach integrates\nheterogeneous patient data, including demographics, disease status, lab\nreports, radiological imaging (CT scans), and clinical notes, using a machine\nlearning framework that can handle missing data. Unlike previous AI-based\nmodels trained on curated datasets, this method utilizes routinely collected\nclinical data, enhancing real-world applicability. Additionally, the model\nincorporates confidence estimation, allowing the identification of cases\nrequiring expert review for precise clinical interpretation. Preliminary\nfindings demonstrate that integrating multiple data modalities improves\ncachexia prediction accuracy at the time of cancer diagnosis. The AI-based\nbiomarker dynamically adapts to patient-specific factors such as age, race,\nethnicity, weight, cancer type, and stage, avoiding the limitations of\nfixed-threshold biomarkers. This multimodal AI biomarker provides a scalable\nand clinically viable solution for early cancer cachexia detection,\nfacilitating personalized interventions and potentially improving treatment\noutcomes and patient survival.",
      "tldr_zh": "该研究提出了一种基于多模态AI的癌症恶病质早期检测生物标志物，利用开源大语言模型(LLMs)和医学数据训练的基础模型，整合患者人口统计、疾病状态、实验室报告、CT影像和临床记录等异构数据。该AI模型能够处理缺失数据，并结合置信度估计，识别需要专家审查的病例，从而提高临床实用性。初步结果表明，多模态数据的整合显著提高了癌症诊断时恶病质的预测准确性。该模型动态适应患者的年龄、种族、体重、癌症类型和分期等个体化因素，避免了固定阈值生物标志物的局限性，为早期癌症恶病质检测提供了可扩展且临床可行的解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "17 pages, 6 figures, 3 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.06797v1",
      "published_date": "2025-03-09 22:32:37 UTC",
      "updated_date": "2025-03-09 22:32:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:23:54.813179"
    },
    {
      "arxiv_id": "2503.06791v1",
      "title": "AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot",
      "title_zh": "AutoMisty：面向Misty社交机器人自动代码生成的多智能体大语言模型框架",
      "authors": [
        "Xiao Wang",
        "Lu Dong",
        "Sahana Rangasrinivasan",
        "Ifeoma Nwogu",
        "Srirangaraj Setlur",
        "Venugopal Govindaraju"
      ],
      "abstract": "The social robot's open API allows users to customize open-domain\ninteractions. However, it remains inaccessible to those without programming\nexperience. In this work, we introduce AutoMisty, the first multi-agent\ncollaboration framework powered by large language models (LLMs), to enable the\nseamless generation of executable Misty robot code from natural language\ninstructions. AutoMisty incorporates four specialized agent modules to manage\ntask decomposition, assignment, problem-solving, and result synthesis. Each\nagent incorporates a two-layer optimization mechanism, with self-reflection for\niterative refinement and human-in-the-loop for better alignment with user\npreferences. AutoMisty ensures a transparent reasoning process, allowing users\nto iteratively refine tasks through natural language feedback for precise\nexecution. To evaluate AutoMisty's effectiveness, we designed a benchmark task\nset spanning four levels of complexity and conducted experiments in a real\nMisty robot environment. Extensive evaluations demonstrate that AutoMisty not\nonly consistently generates high-quality code but also enables precise code\ncontrol, significantly outperforming direct reasoning with ChatGPT-4o and\nChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly\nreleased through the webpage: https://wangxiaoshawn.github.io/AutoMisty.html",
      "tldr_zh": "该研究提出了AutoMisty，首个基于大语言模型(LLMs)的多智能体协作框架，旨在为Misty社交机器人实现从自然语言指令到可执行代码的自动化生成。AutoMisty包含四个专用智能体模块，分别负责任务分解、分配、问题解决和结果合成，并通过自反思和人类反馈优化机制确保代码质量与用户需求对齐。实验表明，AutoMisty在复杂任务中生成的代码质量显著优于ChatGPT-4o和ChatGPT-o1，并支持通过自然语言反馈进行精确控制。相关代码和实验资料已公开。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06791v1",
      "published_date": "2025-03-09 22:07:46 UTC",
      "updated_date": "2025-03-09 22:07:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:23:57.535016"
    },
    {
      "arxiv_id": "2503.06790v1",
      "title": "GenDR: Lightning Generative Detail Restorator",
      "title_zh": "GenDR：闪电生成细节修复器",
      "authors": [
        "Yan Wang",
        "Shijie Zhao",
        "Kai Chen",
        "Kexin Zhang",
        "Junlin Li",
        "Li Zhang"
      ],
      "abstract": "Recent research applying text-to-image (T2I) diffusion models to real-world\nsuper-resolution (SR) has achieved remarkable success. However, fundamental\nmisalignments between T2I and SR targets result in a dilemma between inference\nspeed and detail fidelity. Specifically, T2I tasks prioritize multi-step\ninversion to synthesize coherent outputs aligned with textual prompts and\nshrink the latent space to reduce generating complexity. Contrariwise, SR tasks\npreserve most information from low-resolution input while solely restoring\nhigh-frequency details, thus necessitating sufficient latent space and fewer\ninference steps. To bridge the gap, we present a one-step diffusion model for\ngenerative detail restoration, GenDR, distilled from a tailored diffusion model\nwith larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via\nrepresentation alignment to expand latent space without enlarging the model\nsize. Regarding step-distillation, we propose consistent score identity\ndistillation (CiD) that incorporates SR task-specific loss into score\ndistillation to leverage more SR priors and align the training target.\nFurthermore, we extend CiD with adversarial learning and representation\nalignment (CiDA) to enhance perceptual quality and accelerate training. We also\npolish the pipeline to achieve a more efficient inference. Experimental results\ndemonstrate that GenDR achieves state-of-the-art performance in both\nquantitative metrics and visual fidelity.",
      "tldr_zh": "该研究提出了GenDR，一种基于一步扩散模型的生成式细节修复方法，解决了现有文本到图像(T2I)扩散模型在超分辨率(SR)任务中存在的推理速度与细节保真度矛盾。关键创新包括：通过训练SD2.1-VAE16(0.9B)扩展潜在空间但不增加模型规模；提出一致性分数身份蒸馏(CiD)方法，将SR特定损失融入分数蒸馏；并进一步结合对抗学习(CiDA)提升感知质量。实验表明，GenDR在定量指标和视觉保真度上均达到最先进水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06790v1",
      "published_date": "2025-03-09 22:02:18 UTC",
      "updated_date": "2025-03-09 22:02:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:23:58.635136"
    },
    {
      "arxiv_id": "2503.06788v1",
      "title": "Dubito Ergo Sum: Exploring AI Ethics",
      "title_zh": "Dubito Ergo Sum：探索人工智能伦理",
      "authors": [
        "Viktor Dorfler",
        "Giles Cuthbert"
      ],
      "abstract": "We paraphrase Descartes' famous dictum in the area of AI ethics where the \"I\ndoubt and therefore I am\" is suggested as a necessary aspect of morality.\nTherefore AI, which cannot doubt itself, cannot possess moral agency. Of\ncourse, this is not the end of the story. We explore various aspects of the\nhuman mind that substantially differ from AI, which includes the sensory\ngrounding of our knowing, the act of understanding, and the significance of\nbeing able to doubt ourselves. The foundation of our argument is the discipline\nof ethics, one of the oldest and largest knowledge projects of human history,\nyet, we seem only to be beginning to get a grasp of it. After a couple of\nthousand years of studying the ethics of humans, we (humans) arrived at a point\nwhere moral psychology suggests that our moral decisions are intuitive, and all\nthe models from ethics become relevant only when we explain ourselves. This\nrecognition has a major impact on what and how we can do regarding AI ethics.\nWe do not offer a solution, we explore some ideas and leave the problem open,\nbut we hope somewhat better understood than before our study.",
      "tldr_zh": "这篇论文从笛卡尔的“我思故我在”出发，探讨了人工智能（AI）伦理的核心问题，提出“我怀疑故我在”是道德主体性的必要条件，而AI由于无法自我怀疑，因此不具备道德主体性。研究通过比较人类心智与AI的本质差异，如感官基础、理解能力和自我怀疑的重要性，深入分析了伦理学的历史与现状。尽管人类经过数千年的伦理学研究，发现道德决策往往是直觉性的，伦理模型仅在解释行为时发挥作用。论文并未提供解决方案，但旨在通过探讨相关理念，推动对AI伦理问题的更深层次理解。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 1 figure, HICSS 57: Hawaii International Conference on\n  System Sciences, Honolulu, HI, published January 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.06788v1",
      "published_date": "2025-03-09 21:59:43 UTC",
      "updated_date": "2025-03-09 21:59:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:24:08.825959"
    },
    {
      "arxiv_id": "2503.06784v1",
      "title": "Infinite Leagues Under the Sea: Photorealistic 3D Underwater Terrain Generation by Latent Fractal Diffusion Models",
      "title_zh": "海底无限联盟：基于潜在分形扩散模型的光真实感3D水下地形生成",
      "authors": [
        "Tianyi Zhang",
        "Weiming Zhi",
        "Joshua Mangelson",
        "Matthew Johnson-Roberson"
      ],
      "abstract": "This paper tackles the problem of generating representations of underwater 3D\nterrain. Off-the-shelf generative models, trained on Internet-scale data but\nnot on specialized underwater images, exhibit downgraded realism, as images of\nthe seafloor are relatively uncommon. To this end, we introduce DreamSea, a\ngenerative model to generate hyper-realistic underwater scenes. DreamSea is\ntrained on real-world image databases collected from underwater robot surveys.\nImages from these surveys contain massive real seafloor observations and\ncovering large areas, but are prone to noise and artifacts from the real world.\nWe extract 3D geometry and semantics from the data with visual foundation\nmodels, and train a diffusion model that generates realistic seafloor images in\nRGBD channels, conditioned on novel fractal distribution-based latent\nembeddings. We then fuse the generated images into a 3D map, building a 3DGS\nmodel supervised by 2D diffusion priors which allows photorealistic novel view\nrendering. DreamSea is rigorously evaluated, demonstrating the ability to\nrobustly generate large-scale underwater scenes that are consistent, diverse,\nand photorealistic. Our work drives impact in multiple domains, spanning\nfilming, gaming, and robot simulation.",
      "tldr_zh": "本研究提出了DreamSea，一种基于潜在分形扩散模型(Latent Fractal Diffusion Models)的超真实感海底3D地形生成方法。该模型利用水下机器人调查获取的真实海底图像数据库进行训练，通过视觉基础模型提取3D几何和语义信息，并结合RGBD通道生成逼真的海底图像。生成的图像被融合成3D地图，并通过2D扩散先验监督的3D高斯散射(3DGS)模型实现逼真的新视角渲染。实验表明，DreamSea能够一致、多样且逼真地生成大规模海底场景，对电影制作、游戏开发和机器人模拟等多个领域具有重要影响。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.GR",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06784v1",
      "published_date": "2025-03-09 21:43:37 UTC",
      "updated_date": "2025-03-09 21:43:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:24:12.258014"
    },
    {
      "arxiv_id": "2503.06781v1",
      "title": "Dr Genre: Reinforcement Learning from Decoupled LLM Feedback for Generic Text Rewriting",
      "title_zh": "Dr Genre：基于解耦大语言模型反馈的强化学习通用文本重写方法",
      "authors": [
        "Yufei Li",
        "John Nham",
        "Ganesh Jawahar",
        "Lei Shu",
        "David Uthus",
        "Yun-Hsuan Sung",
        "Chengrun Yang",
        "Itai Rolnick",
        "Yi Qiao",
        "Cong Liu"
      ],
      "abstract": "Generic text rewriting is a prevalent large language model (LLM) application\nthat covers diverse real-world tasks, such as style transfer, fact correction,\nand email editing. These tasks vary in rewriting objectives (e.g., factual\nconsistency vs. semantic preservation), making it challenging to develop a\nunified model that excels across all dimensions. Existing methods often\nspecialize in either a single task or a specific objective, limiting their\ngeneralizability. In this work, we introduce a generic model proficient in\nfactuality, stylistic, and conversational rewriting tasks. To simulate\nreal-world user rewrite requests, we construct a conversational rewrite\ndataset, ChatRewrite, that presents ``natural''-sounding instructions, from raw\nemails using LLMs. Combined with other popular rewrite datasets, including\nLongFact for the factuality rewrite task and RewriteLM for the stylistic\nrewrite task, this forms a broad benchmark for training and evaluating generic\nrewrite models. To align with task-specific objectives, we propose Dr Genre, a\nDecoupled-reward learning framework for Generic rewriting, that utilizes\nobjective-oriented reward models with a task-specific weighting. Evaluation\nshows that \\approach delivers higher-quality rewrites across all targeted\ntasks, improving objectives including instruction following (agreement),\ninternal consistency (coherence), and minimal unnecessary edits (conciseness).",
      "tldr_zh": "本文提出Dr Genre框架，通过解耦式强化学习(Decoupled-reward learning)实现通用文本改写任务。该方法创新性地构建了包含ChatRewrite（对话改写）、LongFact（事实修正）和RewriteLM（风格转换）的多任务基准数据集，并采用面向特定目标的奖励模型加权机制。实验表明，该框架在事实性、风格化和对话式改写任务中均优于现有方法，显著提升了指令遵循度、内容连贯性和表达简洁性等核心指标。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 4 figures, 25 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.06781v1",
      "published_date": "2025-03-09 21:23:52 UTC",
      "updated_date": "2025-03-09 21:23:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:24:21.359073"
    },
    {
      "arxiv_id": "2503.06778v1",
      "title": "Large Language Models Are Effective Human Annotation Assistants, But Not Good Independent Annotators",
      "title_zh": "大语言模型是有效的人类标注助手，但并非优秀的独立标注者",
      "authors": [
        "Feng Gu",
        "Zongxia Li",
        "Carlos Rafael Colon",
        "Benjamin Evans",
        "Ishani Mondal",
        "Jordan Lee Boyd-Graber"
      ],
      "abstract": "Event annotation is important for identifying market changes, monitoring\nbreaking news, and understanding sociological trends. Although expert\nannotators set the gold standards, human coding is expensive and inefficient.\nUnlike information extraction experiments that focus on single contexts, we\nevaluate a holistic workflow that removes irrelevant documents, merges\ndocuments about the same event, and annotates the events. Although LLM-based\nautomated annotations are better than traditional TF-IDF-based methods or Event\nSet Curation, they are still not reliable annotators compared to human experts.\nHowever, adding LLMs to assist experts for Event Set Curation can reduce the\ntime and mental effort required for Variable Annotation. When using LLMs to\nextract event variables to assist expert annotators, they agree more with the\nextracted variables than fully automated LLMs for annotation.",
      "tldr_zh": "该研究评估了大型语言模型(LLMs)在事件标注任务中的表现，发现虽然LLMs在自动标注方面优于传统的TF-IDF方法和事件集整理(Event Set Curation)技术，但仍无法达到人类专家的可靠性水平。然而，当LLMs作为人类专家的辅助工具时，能显著降低变量标注(Variable Annotation)所需的时间和认知负荷，且专家对LLMs提取的事件变量认可度高于完全自动化的LLM标注结果。研究表明LLMs更适合作为人类标注助手而非独立标注者。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06778v1",
      "published_date": "2025-03-09 21:14:14 UTC",
      "updated_date": "2025-03-09 21:14:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:24:31.877665"
    },
    {
      "arxiv_id": "2503.06765v1",
      "title": "Effectiveness of Zero-shot-CoT in Japanese Prompts",
      "title_zh": "零样本思维链（Zero-shot-CoT）在日语提示中的有效性研究",
      "authors": [
        "Shusuke Takayama",
        "Ian Frank"
      ],
      "abstract": "We compare the effectiveness of zero-shot Chain-of-Thought (CoT) prompting in\nJapanese and English using ChatGPT-3.5 and 4o-mini. The technique of zero-shot\nCoT, which involves appending a phrase such as \"Let's think step by step\" to a\nprompt to encourage reasoning before answering, has been shown to offer LLM\nperformance improvements in mathematical and reasoning tasks, particularly in\nEnglish. We investigate how these effects transfer to Japanese using the\nJapanese Multi-task Language Understanding Benchmark (JMMLU) and the Multi-task\nLanguage Understanding Benchmark (MMLU). Our results show that while zero-shot\nCoT prompting can lead to notable performance gains for some prompt categories\nin GPT-3.5, its impact in GPT-4o-mini is associated with significant\nperformance declines. However, for Japanese prompts there remain certain\ncategories, such as college mathematics and abstract algebra, that still\nexhibit improvements, despite the broader trend of diminishing effectiveness in\nmore advanced models.",
      "tldr_zh": "本研究对比了零样本思维链（Zero-shot-CoT）提示在日语和英语中的效果，使用ChatGPT-3.5和4o-mini进行测试。结果显示，尽管在GPT-3.5中零样本CoT能显著提升部分日语提示（如大学数学和抽象代数）的表现，但在更先进的GPT-4o-mini模型中却普遍导致性能下降。这一发现揭示了语言模型性能提升策略（如CoT）在不同语言和模型版本中的不一致性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NLP2025 Workshop on Japanese Language Resources (JLR2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.06765v1",
      "published_date": "2025-03-09 20:42:38 UTC",
      "updated_date": "2025-03-09 20:42:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:24:50.269110"
    },
    {
      "arxiv_id": "2503.06764v3",
      "title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation",
      "title_zh": "SemHiTok：基于语义引导分层码本的多模态理解与生成统一图像分词器",
      "authors": [
        "Zisheng Chen",
        "Chunwei Wang",
        "Xiuwei Chen",
        "Hang Xu",
        "Jianhua Han",
        "Xiaodan Liang"
      ],
      "abstract": "We present SemHiTok, a unified image Tokenizer via Semantic-Guided\nHierarchical codebook that provides consistent discrete feature representations\nfor multimodal understanding and generation tasks. Recently, unified multimodal\nlarge models (MLLMs) for understanding and generation have sparked exploration\nwithin research community. Previous works attempt to train a unified image\ntokenizer by combining loss functions for semantic feature reconstruction and\npixel reconstruction. However, due to the differing levels of features\nprioritized by multimodal understanding and generation tasks, joint training\nmethods face significant challenges in achieving a good trade-off. SemHiTok\naddresses this challenge through Semantic-Guided Hierarchical codebook which\nbuilds texture sub-codebooks on pre-trained semantic codebook. This design\ndecouples the training of semantic reconstruction and pixel reconstruction and\nequips the tokenizer with low-level texture feature extraction capability\nwithout degradation of high-level semantic feature extraction ability. Our\nexperiments demonstrate that SemHiTok achieves excellent rFID score at\n256X256resolution compared to other unified tokenizers, and exhibits\ncompetitive performance on multimodal understanding and generation tasks.",
      "tldr_zh": "本文提出SemHiTok，一种通过语义引导分层码本的统一图像分词器，为多模态理解和生成任务提供一致的离散特征表示。该方法采用分层码本设计，在预训练语义码本上构建纹理子码本，解耦了语义重建和像素重建的训练过程，使分词器在保持高级语义特征提取能力的同时具备低级纹理特征提取功能。实验表明，SemHiTok在256×256分辨率下相比其他统一分词器取得了优异的rFID分数，并在多模态理解和生成任务上展现出竞争力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review, Refer to the latest version",
      "pdf_url": "http://arxiv.org/pdf/2503.06764v3",
      "published_date": "2025-03-09 20:42:34 UTC",
      "updated_date": "2025-03-20 06:13:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:24:55.957275"
    },
    {
      "arxiv_id": "2503.06749v2",
      "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
      "title_zh": "Vision-R1：激励多模态大语言模型的推理能力",
      "authors": [
        "Wenxuan Huang",
        "Bohan Jia",
        "Zijie Zhai",
        "Shaosheng Cao",
        "Zheyu Ye",
        "Fei Zhao",
        "Zhe Xu",
        "Yao Hu",
        "Shaohui Lin"
      ],
      "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of $\\sim$6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
      "tldr_zh": "该研究提出了Vision-R1，一种通过强化学习（RL）提升多模态大语言模型（MLLMs）推理能力的方法。针对高质量多模态推理数据缺乏的问题，研究团队利用现有MLLM和DeepSeek-R1构建了一个无人工标注的20万条多模态链式思维（CoT）数据集Vision-R1-cold，并提出了渐进式思维抑制训练（PTST）策略和基于硬格式化结果奖励的组相对策略优化（GRPO）方法。实验表明，Vision-R1在多种多模态数学推理基准上平均提升约6%，其中Vision-R1-7B在MathVista基准上达到73.5%的准确率，接近领先的OpenAI O1模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06749v2",
      "published_date": "2025-03-09 20:06:45 UTC",
      "updated_date": "2025-03-11 09:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:25:32.018705"
    },
    {
      "arxiv_id": "2503.06747v1",
      "title": "Fully-Decentralized MADDPG with Networked Agents",
      "title_zh": "完全去中心化的MADDPG：面向网络化智能体的多智能体强化学习",
      "authors": [
        "Diego Bolliger",
        "Lorenz Zauter",
        "Robert Ziegler"
      ],
      "abstract": "In this paper, we devise three actor-critic algorithms with decentralized\ntraining for multi-agent reinforcement learning in cooperative, adversarial,\nand mixed settings with continuous action spaces. To this goal, we adapt the\nMADDPG algorithm by applying a networked communication approach between agents.\nWe introduce surrogate policies in order to decentralize the training while\nallowing for local communication during training. The decentralized algorithms\nachieve comparable results to the original MADDPG in empirical tests, while\nreducing computational cost. This is more pronounced with larger numbers of\nagents.",
      "tldr_zh": "本文提出了三种基于去中心化训练的多智能体强化学习算法，适用于连续动作空间的合作、对抗和混合场景。这些算法通过引入网络化通信方法和代理策略，对MADDPG算法进行了改进，实现了训练过程的完全去中心化，同时允许训练期间的局部通信。实验结果表明，这些去中心化算法在性能上与原始MADDPG相当，但显著降低了计算成本，尤其在智能体数量较多时效果更为明显。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06747v1",
      "published_date": "2025-03-09 20:05:32 UTC",
      "updated_date": "2025-03-09 20:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:25:16.277843"
    },
    {
      "arxiv_id": "2503.06745v1",
      "title": "Beyond Black-Box Benchmarking: Observability, Analytics, and Optimization of Agentic Systems",
      "title_zh": "超越黑盒基准测试：智能体系统的可观测性、分析与优化",
      "authors": [
        "Dany Moshkovich",
        "Hadar Mulian",
        "Sergey Zeltyn",
        "Natti Eder",
        "Inna Skarbovsky",
        "Roy Abitbol"
      ],
      "abstract": "The rise of agentic AI systems, where agents collaborate to perform diverse\ntasks, poses new challenges with observing, analyzing and optimizing their\nbehavior. Traditional evaluation and benchmarking approaches struggle to handle\nthe non-deterministic, context-sensitive, and dynamic nature of these systems.\nThis paper explores key challenges and opportunities in analyzing and\noptimizing agentic systems across development, testing, and maintenance. We\nexplore critical issues such as natural language variability and unpredictable\nexecution flows, which hinder predictability and control, demanding adaptive\nstrategies to manage input variability and evolving behaviors. Through our user\nstudy, we supported these hypotheses. In particular, we showed a 79% agreement\nthat non deterministic flow of agentic systems acts as a major challenge.\nFinally, we validated our statements empirically advocating the need for moving\nbeyond classical benchmarking. To bridge these gaps, we introduce taxonomies to\npresent expected analytics outcomes and the ways to collect them by extending\nstandard observability frameworks. Building on these foundations, we introduce\nand demonstrate novel approach for benchmarking of agent evaluation systems.\nUnlike traditional \"black box\" performance evaluation approaches, our benchmark\nis built from agent runtime logs as input, and analytics outcome including\ndiscovered flows and issues. By addressing key limitations in existing\nmethodologies, we aim to set the stage for more advanced and holistic\nevaluation strategies, which could foster the development of adaptive,\ninterpretable, and robust agentic AI systems.",
      "tldr_zh": "该研究探讨了智能体系统（agentic AI systems）面临的观测、分析和优化挑战，指出传统黑盒评估方法难以应对其非确定性、上下文敏感和动态特性。论文通过用户研究证实了自然语言变异性和不可预测执行流是主要挑战（79%认同率），并提出了扩展的可观测性框架和新型基准测试方法。研究者引入基于运行时日志的评估系统，能分析智能体流程和问题，为开发自适应、可解释的智能体系统提供了更全面的评估策略。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 19 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06745v1",
      "published_date": "2025-03-09 20:02:04 UTC",
      "updated_date": "2025-03-09 20:02:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:25:21.439152"
    },
    {
      "arxiv_id": "2503.10666v1",
      "title": "Green Prompting",
      "title_zh": "绿色提示",
      "authors": [
        "Marta Adamska",
        "Daria Smirnova",
        "Hamid Nasiri",
        "Zhengxin Yu",
        "Peter Garraghan"
      ],
      "abstract": "Large Language Models (LLMs) have become widely used across various domains\nspanning search engines, code generation, and text creation. However, a major\nconcern associated with their adoption is the high cost of inference, impacting\nboth their sustainability and financial feasibility. In this study, we\nempirically study how different prompt and response characteristics directly\nimpact LLM inference energy cost. We conduct experiments leveraging three\nopen-source transformer-based LLMs across three task types$-$question\nanswering, sentiment analysis, and text generation. For each inference, we\nanalyzed prompt and response characteristics (length, semantic meaning, time\ntaken, energy consumption). Our results demonstrate that even when presented\nwith identical tasks, models generate responses with varying characteristics\nand subsequently exhibit distinct energy consumption patterns. We found that\nprompt length is less significant than the semantic meaning of the task itself.\nIn addition, we identified specific keywords associated with higher or lower\nenergy usage that vary between associated tasks. These findings highlight the\nimportance of prompt design in optimizing inference efficiency. We conclude\nthat the semantic meaning of prompts and certain task-related keywords\nsignificantly impact inference costs, leading the way for deeper exploration\ntowards creating energy-adaptive LLMs.",
      "tldr_zh": "该研究提出了\"绿色提示(Green Prompting)\"概念，通过实证分析揭示了提示词设计对大型语言模型(LLMs)推理能耗的影响。实验基于三种开源Transformer模型，在问答、情感分析和文本生成任务中发现：提示词的语义含义比长度对能耗影响更大，且特定任务关键词会显著改变能耗模式。研究为开发能耗自适应的LLMs提供了新思路，表明优化提示设计可提升模型能效。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.10666v1",
      "published_date": "2025-03-09 19:49:31 UTC",
      "updated_date": "2025-03-09 19:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:26:15.950159"
    },
    {
      "arxiv_id": "2503.06734v1",
      "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
      "title_zh": "预训练语言模型表示中的性别编码模式",
      "authors": [
        "Mahdi Zakizadeh",
        "Mohammad Taher Pilehvar"
      ],
      "abstract": "Gender bias in pretrained language models (PLMs) poses significant social and\nethical challenges. Despite growing awareness, there is a lack of comprehensive\ninvestigation into how different models internally represent and propagate such\nbiases. This study adopts an information-theoretic approach to analyze how\ngender biases are encoded within various encoder-based architectures. We focus\non three key aspects: identifying how models encode gender information and\nbiases, examining the impact of bias mitigation techniques and fine-tuning on\nthe encoded biases and their effectiveness, and exploring how model design\ndifferences influence the encoding of biases. Through rigorous and systematic\ninvestigation, our findings reveal a consistent pattern of gender encoding\nacross diverse models. Surprisingly, debiasing techniques often exhibit limited\nefficacy, sometimes inadvertently increasing the encoded bias in internal\nrepresentations while reducing bias in model output distributions. This\nhighlights a disconnect between mitigating bias in output distributions and\naddressing its internal representations. This work provides valuable guidance\nfor advancing bias mitigation strategies and fostering the development of more\nequitable language models.",
      "tldr_zh": "该研究采用信息论方法系统分析了预训练语言模型(PLMs)中性别偏见的编码机制。研究发现不同encoder架构模型存在一致的性别编码模式，且现有去偏技术往往效果有限——虽然能减少输出分布的偏见，却可能意外增加模型内部表征的偏见。这项工作揭示了模型输出去偏与内部表征修正之间的脱节现象，为开发更公平的语言模型提供了重要指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of the 5th Workshop on Trustworthy Natural Language\n  Processing (TrustNLP 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.06734v1",
      "published_date": "2025-03-09 19:17:46 UTC",
      "updated_date": "2025-03-09 19:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:25:40.586136"
    },
    {
      "arxiv_id": "2503.06729v1",
      "title": "ACAI for SBOs: AI Co-creation for Advertising and Inspiration for Small Business Owners",
      "title_zh": "ACAI 助力小企业主：面向广告与灵感的 AI 协同创作",
      "authors": [
        "Nimisha Karnatak",
        "Adrien Baranes",
        "Rob Marchant",
        "Triona Butler",
        "Kristen Olson"
      ],
      "abstract": "Small business owners (SBOs) often lack the resources and design experience\nneeded to produce high-quality advertisements. To address this, we developed\nACAI (AI Co-Creation for Advertising and Inspiration), an GenAI-powered\nmultimodal advertisement creation tool, and conducted a user study with 16 SBOs\nin London to explore their perceptions of and interactions with ACAI in\nadvertisement creation. Our findings reveal that structured inputs enhance user\nagency and control while improving AI outputs by facilitating better brand\nalignment, enhancing AI transparency, and offering scaffolding that assists\nnovice designers, such as SBOs, in formulating prompts. We also found that\nACAI's multimodal interface bridges the design skill gap for SBOs with a clear\nadvertisement vision, but who lack the design jargon necessary for effective\nprompting. Building on our findings, we propose three capabilities: contextual\nintelligence, adaptive interactions, and data management, with corresponding\ndesign recommendations to advance the co-creative attributes of AI-mediated\ndesign tools.",
      "tldr_zh": "该研究开发了ACAI，一种基于生成式AI的多模态广告创作工具，旨在帮助缺乏设计资源的小企业主(SBOs)制作高质量广告。通过对16位伦敦SBOs的用户研究发现，结构化输入不仅提高了用户对广告创作的控制力，还增强了AI输出的品牌一致性和透明度，同时为新手设计师提供了有效的提示指导。研究还表明，ACAI的多模态界面能够弥补SBOs在设计术语上的不足，帮助他们更好地实现广告创意。基于这些发现，研究提出了情境智能、自适应交互和数据管理三项功能，并提供了相应的设计建议，以提升AI辅助设计工具的共创能力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06729v1",
      "published_date": "2025-03-09 19:00:36 UTC",
      "updated_date": "2025-03-09 19:00:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:25:41.052543"
    },
    {
      "arxiv_id": "2503.06725v1",
      "title": "Pull-Based Query Scheduling for Goal-Oriented Semantic Communication",
      "title_zh": "面向目标语义通信的拉取式查询调度",
      "authors": [
        "Pouya Agheli",
        "Nikolaos Pappas",
        "Marios Kountouris"
      ],
      "abstract": "This paper addresses query scheduling for goal-oriented semantic\ncommunication in pull-based status update systems. We consider a system where\nmultiple sensing agents (SAs) observe a source characterized by various\nattributes and provide updates to multiple actuation agents (AAs), which act\nupon the received information to fulfill their heterogeneous goals at the\nendpoint. A hub serves as an intermediary, querying the SAs for updates on\nobserved attributes and maintaining a knowledge base, which is then broadcast\nto the AAs. The AAs leverage the knowledge to perform their actions\neffectively. To quantify the semantic value of updates, we introduce a grade of\neffectiveness (GoE) metric. Furthermore, we integrate cumulative perspective\ntheory (CPT) into the long-term effectiveness analysis to account for risk\nawareness and loss aversion in the system. Leveraging this framework, we\ncompute effect-aware scheduling policies aimed at maximizing the expected\ndiscounted sum of CPT-based total GoE provided by the transmitted updates while\ncomplying with a given query cost constraint. To achieve this, we propose a\nmodel-based solution based on dynamic programming and model-free solutions\nemploying state-of-the-art deep reinforcement learning (DRL) algorithms. Our\nfindings demonstrate that effect-aware scheduling significantly enhances the\neffectiveness of communicated updates compared to benchmark scheduling methods,\nparticularly in settings with stringent cost constraints where optimal query\nscheduling is vital for system performance and overall effectiveness.",
      "tldr_zh": "本文研究了面向目标语义通信的查询调度问题，提出了一种基于拉取模式的状态更新系统。通过引入有效性等级（GoE）指标和累积前景理论（CPT），量化更新信息的语义价值，并考虑系统的风险意识和损失规避。研究设计了基于动态规划的模型驱动方法和基于深度强化学习（DRL）的模型无关方法，以最大化CPT下的总GoE，同时满足查询成本约束。实验表明，该调度策略在严格成本限制下显著提升了通信更新的有效性，优于基准方法。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.NI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "Submitted for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2503.06725v1",
      "published_date": "2025-03-09 18:51:14 UTC",
      "updated_date": "2025-03-09 18:51:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:25:50.546682"
    },
    {
      "arxiv_id": "2503.06709v1",
      "title": "Delusions of Large Language Models",
      "title_zh": "大型语言模型的妄想症",
      "authors": [
        "Hongshen Xu",
        "Zixv yang",
        "Zichen Zhu",
        "Kunyao Lan",
        "Zihan Wang",
        "Mengyue Wu",
        "Ziwei Ji",
        "Lu Chen",
        "Pascale Fung",
        "Kai Yu"
      ],
      "abstract": "Large Language Models often generate factually incorrect but plausible\noutputs, known as hallucinations. We identify a more insidious phenomenon, LLM\ndelusion, defined as high belief hallucinations, incorrect outputs with\nabnormally high confidence, making them harder to detect and mitigate. Unlike\nordinary hallucinations, delusions persist with low uncertainty, posing\nsignificant challenges to model reliability. Through empirical analysis across\ndifferent model families and sizes on several Question Answering tasks, we show\nthat delusions are prevalent and distinct from hallucinations. LLMs exhibit\nlower honesty with delusions, which are harder to override via finetuning or\nself reflection. We link delusion formation with training dynamics and dataset\nnoise and explore mitigation strategies such as retrieval augmented generation\nand multi agent debating to mitigate delusions. By systematically investigating\nthe nature, prevalence, and mitigation of LLM delusions, our study provides\ninsights into the underlying causes of this phenomenon and outlines future\ndirections for improving model reliability.",
      "tldr_zh": "该研究揭示了大型语言模型(LLMs)中一种更为隐蔽的问题——\"LLM妄想\"(delusion)，即模型以异常高置信度生成错误但看似合理的内容，比普通幻觉(hallucination)更难检测和缓解。通过在不同模型家族和规模上的实证分析，研究发现LLM妄想普遍存在，且难以通过微调或自我反思纠正。论文探讨了训练动态和数据集噪声与妄想形成的关系，并提出了检索增强生成和多方辩论等缓解策略，为提高模型可靠性提供了新的见解和方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06709v1",
      "published_date": "2025-03-09 17:59:16 UTC",
      "updated_date": "2025-03-09 17:59:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:26:01.181395"
    },
    {
      "arxiv_id": "2503.07671v3",
      "title": "Probabilistic Shielding for Safe Reinforcement Learning",
      "title_zh": "概率防护机制在安全强化学习中的应用",
      "authors": [
        "Edwin Hamel-De le Court",
        "Francesco Belardinelli",
        "Alexander W. Goodall"
      ],
      "abstract": "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
      "tldr_zh": "该论文提出了一种基于概率屏蔽（Probabilistic Shielding）的安全强化学习（Safe RL）新方法，用于确保智能体在训练和测试阶段始终满足安全性约束。该方法通过状态增强（state-augmentation）和动作屏蔽机制（shield），在已知马尔可夫决策过程（MDP）安全动态的情况下，为无折扣概率规避特性提供严格的形式化安全保证。实验证明，这种可扩展的方法在实际应用中具有可行性，同时避免了传统线性规划方法的扩展性限制。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "13 pages, 3 figures, Conference: AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.07671v3",
      "published_date": "2025-03-09 17:54:33 UTC",
      "updated_date": "2025-03-25 11:31:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:26:29.464262"
    },
    {
      "arxiv_id": "2503.06706v1",
      "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts",
      "title_zh": "PFDial：基于UML流程图的结构化对话指令微调方法",
      "authors": [
        "Ming Zhang",
        "Yuhui Wang",
        "Yujiong Shen",
        "Tingyi Yang",
        "Changhao Jiang",
        "Yilong Wu",
        "Shihan Dou",
        "Qinhao Chen",
        "Zhiheng Xi",
        "Zhihao Zhang",
        "Yi Dong",
        "Zhen Wang",
        "Zhihui Fei",
        "Mingyang Wan",
        "Tao Liang",
        "Guojun Ma",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang"
      ],
      "abstract": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.",
      "tldr_zh": "该研究提出了PFDial，一种基于UML流程图的对话指令微调方法，旨在解决大语言模型(LLMs)在处理严格约束的流程驱动对话任务时的不足。研究团队构建了包含12,705条高质量中文对话指令的PFDial数据集，这些指令源自440个包含5,055个流程节点的UML流程图。实验表明，仅用800个样本训练的7B模型和用全部数据训练的0.5B模型均能达到90%以上的准确率，且8B模型在部分任务上可超越GPT-4o达43.88%。研究还深入分析了不同数据集格式对模型处理决策和顺序分支任务的影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06706v1",
      "published_date": "2025-03-09 17:43:30 UTC",
      "updated_date": "2025-03-09 17:43:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:26:36.837233"
    },
    {
      "arxiv_id": "2503.06692v2",
      "title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models",
      "title_zh": "InftyThink：突破大语言模型长上下文推理的长度限制",
      "authors": [
        "Yuchen Yan",
        "Yongliang Shen",
        "Yang Liu",
        "Jin Jiang",
        "Mengdi Zhang",
        "Jian Shao",
        "Yueting Zhuang"
      ],
      "abstract": "Advanced reasoning in large language models has achieved remarkable\nperformance on challenging tasks, but the prevailing long-context reasoning\nparadigm faces critical limitations: quadratic computational scaling with\nsequence length, reasoning constrained by maximum context boundaries, and\nperformance degradation beyond pre-training context windows. Existing\napproaches primarily compress reasoning chains without addressing the\nfundamental scaling problem. To overcome these challenges, we introduce\nInftyThink, a paradigm that transforms monolithic reasoning into an iterative\nprocess with intermediate summarization. By interleaving short reasoning\nsegments with concise progress summaries, our approach enables unbounded\nreasoning depth while maintaining bounded computational costs. This creates a\ncharacteristic sawtooth memory pattern that significantly reduces computational\ncomplexity compared to traditional approaches. Furthermore, we develop a\nmethodology for reconstructing long-context reasoning datasets into our\niterative format, transforming OpenR1-Math into 333K training instances.\nExperiments across multiple model architectures demonstrate that our approach\nreduces computational costs while improving performance, with Qwen2.5-Math-7B\nshowing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks.\nOur work challenges the assumed trade-off between reasoning depth and\ncomputational efficiency, providing a more scalable approach to complex\nreasoning without architectural modifications.",
      "tldr_zh": "该研究提出了InftyThink，一种突破大语言模型长上下文推理限制的新范式。通过将单一推理过程转化为包含中间总结的迭代过程，该方法实现了无限制的推理深度，同时将计算成本控制在可接受范围内。实验表明，该方法在多个模型架构上均能降低计算成本并提升性能，例如Qwen2.5-Math-7B在MATH500等基准测试中表现提升3-13%。这一工作挑战了推理深度与计算效率之间的固有权衡，为复杂推理提供了更可扩展的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06692v2",
      "published_date": "2025-03-09 16:59:14 UTC",
      "updated_date": "2025-03-13 16:00:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:26:45.893988"
    },
    {
      "arxiv_id": "2503.06690v1",
      "title": "Censoring-Aware Tree-Based Reinforcement Learning for Estimating Dynamic Treatment Regimes with Censored Outcomes",
      "title_zh": "基于树结构的抗删失强化学习：针对含删失结果的动态治疗方案估计方法",
      "authors": [
        "Animesh Kumar Paul",
        "Russell Greiner"
      ],
      "abstract": "Dynamic Treatment Regimes (DTRs) provide a systematic approach for making\nsequential treatment decisions that adapt to individual patient\ncharacteristics, particularly in clinical contexts where survival outcomes are\nof interest. Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) is a\nnovel framework to address the complexities associated with censored data when\nestimating optimal DTRs. We explore ways to learn effective DTRs, from\nobservational data. By enhancing traditional tree-based reinforcement learning\nmethods with augmented inverse probability weighting (AIPW) and censoring-aware\nmodifications, CA-TRL delivers robust and interpretable treatment strategies.\nWe demonstrate its effectiveness through extensive simulations and real-world\napplications using the SANAD epilepsy dataset, where it outperformed the\nrecently proposed ASCL method in key metrics such as restricted mean survival\ntime (RMST) and decision-making accuracy. This work represents a step forward\nin advancing personalized and data-driven treatment strategies across diverse\nhealthcare settings.",
      "tldr_zh": "该研究提出了一种新型的**Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL)**框架，用于处理带**删失数据**的**动态治疗方案(DTRs)**优化问题。该方法通过结合**增强逆概率加权(AIPW)**和删失感知改进，增强了传统基于树的强化学习模型，能够从观察数据中学习鲁棒且可解释的治疗策略。实验表明，CA-TRL在模拟数据和真实癫痫数据集(SANAD)上均表现优异，在**限制平均生存时间(RMST)**和决策准确性等关键指标上超越了现有方法(ASCL)。这一成果为临床个性化治疗策略提供了新的数据驱动解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06690v1",
      "published_date": "2025-03-09 16:53:09 UTC",
      "updated_date": "2025-03-09 16:53:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:26:59.275998"
    },
    {
      "arxiv_id": "2503.06687v1",
      "title": "UniGenX: Unified Generation of Sequence and Structure with Autoregressive Diffusion",
      "title_zh": "UniGenX：基于自回归扩散的序列与结构统一生成框架",
      "authors": [
        "Gongbo Zhang",
        "Yanting Li",
        "Renqian Luo",
        "Pipi Hu",
        "Zeru Zhao",
        "Lingbo Li",
        "Guoqing Liu",
        "Zun Wang",
        "Ran Bi",
        "Kaiyuan Gao",
        "Liya Guo",
        "Yu Xie",
        "Chang Liu",
        "Jia Zhang",
        "Tian Xie",
        "Robert Pinsler",
        "Claudio Zeni",
        "Ziheng Lu",
        "Yingce Xia",
        "Marwin Segler",
        "Maik Riechert",
        "Li Yuan",
        "Lei Chen",
        "Haiguang Liu",
        "Tao Qin"
      ],
      "abstract": "Unified generation of sequence and structure for scientific data (e.g.,\nmaterials, molecules, proteins) is a critical task. Existing approaches\nprimarily rely on either autoregressive sequence models or diffusion models,\neach offering distinct advantages and facing notable limitations.\nAutoregressive models, such as GPT, Llama, and Phi-4, have demonstrated\nremarkable success in natural language generation and have been extended to\nmultimodal tasks (e.g., image, video, and audio) using advanced encoders like\nVQ-VAE to represent complex modalities as discrete sequences. However, their\ndirect application to scientific domains is challenging due to the high\nprecision requirements and the diverse nature of scientific data. On the other\nhand, diffusion models excel at generating high-dimensional scientific data,\nsuch as protein, molecule, and material structures, with remarkable accuracy.\nYet, their inability to effectively model sequences limits their potential as\ngeneral-purpose multimodal foundation models. To address these challenges, we\npropose UniGenX, a unified framework that combines autoregressive next-token\nprediction with conditional diffusion models. This integration leverages the\nstrengths of autoregressive models to ease the training of conditional\ndiffusion models, while diffusion-based generative heads enhance the precision\nof autoregressive predictions. We validate the effectiveness of UniGenX on\nmaterial and small molecule generation tasks, achieving a significant leap in\nstate-of-the-art performance for material crystal structure prediction and\nestablishing new state-of-the-art results for small molecule structure\nprediction, de novo design, and conditional generation. Notably, UniGenX\ndemonstrates significant improvements, especially in handling long sequences\nfor complex structures, showcasing its efficacy as a versatile tool for\nscientific data generation.",
      "tldr_zh": "该研究提出UniGenX框架，通过结合自回归(Autoregressive)模型和扩散模型(Diffusion Models)的优势，实现了科学数据序列与结构的统一生成。该方法利用自回归的next-token预测简化条件扩散模型的训练，同时通过扩散生成头提升自回归预测的精度。在材料和分子生成任务中，UniGenX显著提升了晶体结构预测性能，并在小分子结构预测、从头设计(de novo design)和条件生成等任务上创下新纪录，特别在复杂结构的长序列处理方面展现出卓越性能。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.bio-ph",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06687v1",
      "published_date": "2025-03-09 16:43:07 UTC",
      "updated_date": "2025-03-09 16:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:27:19.620866"
    },
    {
      "arxiv_id": "2503.10665v1",
      "title": "Small Vision-Language Models: A Survey on Compact Architectures and Techniques",
      "title_zh": "小型视觉语言模型：紧凑架构与技术综述",
      "authors": [
        "Nitesh Patnaik",
        "Navdeep Nayak",
        "Himani Bansal Agrawal",
        "Moinak Chinmoy Khamaru",
        "Gourav Bal",
        "Saishree Smaranika Panda",
        "Rishi Raj",
        "Vishal Meena",
        "Kartheek Vadlamani"
      ],
      "abstract": "The emergence of small vision-language models (sVLMs) marks a critical\nadvancement in multimodal AI, enabling efficient processing of visual and\ntextual data in resource-constrained environments. This survey offers a\ncomprehensive exploration of sVLM development, presenting a taxonomy of\narchitectures - transformer-based, mamba-based, and hybrid - that highlight\ninnovations in compact design and computational efficiency. Techniques such as\nknowledge distillation, lightweight attention mechanisms, and modality\npre-fusion are discussed as enablers of high performance with reduced resource\nrequirements. Through an in-depth analysis of models like TinyGPT-V, MiniGPT-4,\nand VL-Mamba, we identify trade-offs between accuracy, efficiency, and\nscalability. Persistent challenges, including data biases and generalization to\ncomplex tasks, are critically examined, with proposed pathways for addressing\nthem. By consolidating advancements in sVLMs, this work underscores their\ntransformative potential for accessible AI, setting a foundation for future\nresearch into efficient multimodal systems.",
      "tldr_zh": "这篇综述系统梳理了小型视觉语言模型(sVLMs)的最新进展，重点分析了三类紧凑架构：基于Transformer、基于Mamba以及混合架构。研究总结了知识蒸馏、轻量级注意力机制和模态预融合等关键技术，这些方法在保持高性能的同时显著降低了计算资源需求。通过分析TinyGPT-V、MiniGPT-4和VL-Mamba等典型模型，论文揭示了模型精度、效率和可扩展性之间的权衡关系，并指出数据偏差和复杂任务泛化等现存挑战，为未来高效多模态系统的研究提供了方向性指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10665v1",
      "published_date": "2025-03-09 16:14:46 UTC",
      "updated_date": "2025-03-09 16:14:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:27:15.674377"
    },
    {
      "arxiv_id": "2503.06664v1",
      "title": "Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets",
      "title_zh": "探索用于清理表格机器学习数据集的LLM智能体",
      "authors": [
        "Tommaso Bendinelli",
        "Artur Dox",
        "Christian Holz"
      ],
      "abstract": "High-quality, error-free datasets are a key ingredient in building reliable,\naccurate, and unbiased machine learning (ML) models. However, real world\ndatasets often suffer from errors due to sensor malfunctions, data entry\nmistakes, or improper data integration across multiple sources that can\nseverely degrade model performance. Detecting and correcting these issues\ntypically require tailor-made solutions and demand extensive domain expertise.\nConsequently, automation is challenging, rendering the process labor-intensive\nand tedious. In this study, we investigate whether Large Language Models (LLMs)\ncan help alleviate the burden of manual data cleaning. We set up an experiment\nin which an LLM, paired with Python, is tasked with cleaning the training\ndataset to improve the performance of a learning algorithm without having the\nability to modify the training pipeline or perform any feature engineering. We\nrun this experiment on multiple Kaggle datasets that have been intentionally\ncorrupted with errors. Our results show that LLMs can identify and correct\nerroneous entries, such as illogical values or outlier, by leveraging\ncontextual information from other features within the same row, as well as\nfeedback from previous iterations. However, they struggle to detect more\ncomplex errors that require understanding data distribution across multiple\nrows, such as trends and biases.",
      "tldr_zh": "该研究探索了利用大语言模型(LLMs)自动清理表格机器学习数据集的方法。实验表明，LLMs能够通过上下文信息和迭代反馈成功识别并修正单行数据中的逻辑错误和异常值，但对需要跨行理解数据分布(如趋势和偏差)的复杂错误检测能力有限。研究采用Python辅助的LLM框架，在故意注入错误的Kaggle数据集上进行测试，为自动化数据清洗提供了新思路，同时揭示了当前技术在处理复杂数据模式时的局限性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 1 main figure, 3 plots, Published at ICLR 2025 Workshop on\n  Foundation Models in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2503.06664v1",
      "published_date": "2025-03-09 15:29:46 UTC",
      "updated_date": "2025-03-09 15:29:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:27:17.031072"
    },
    {
      "arxiv_id": "2503.06661v1",
      "title": "AA-CLIP: Enhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP",
      "title_zh": "AA-CLIP：通过异常感知CLIP增强零样本异常检测",
      "authors": [
        "Wenxin Ma",
        "Xu Zhang",
        "Qingsong Yao",
        "Fenghe Tang",
        "Chenxu Wu",
        "Yingtai Li",
        "Rui Yan",
        "Zihang Jiang",
        "S. Kevin Zhou"
      ],
      "abstract": "Anomaly detection (AD) identifies outliers for applications like defect and\nlesion detection. While CLIP shows promise for zero-shot AD tasks due to its\nstrong generalization capabilities, its inherent Anomaly-Unawareness leads to\nlimited discrimination between normal and abnormal features. To address this\nproblem, we propose Anomaly-Aware CLIP (AA-CLIP), which enhances CLIP's anomaly\ndiscrimination ability in both text and visual spaces while preserving its\ngeneralization capability. AA-CLIP is achieved through a straightforward yet\neffective two-stage approach: it first creates anomaly-aware text anchors to\ndifferentiate normal and abnormal semantics clearly, then aligns patch-level\nvisual features with these anchors for precise anomaly localization. This\ntwo-stage strategy, with the help of residual adapters, gradually adapts CLIP\nin a controlled manner, achieving effective AD while maintaining CLIP's class\nknowledge. Extensive experiments validate AA-CLIP as a resource-efficient\nsolution for zero-shot AD tasks, achieving state-of-the-art results in\nindustrial and medical applications. The code is available at\nhttps://github.com/Mwxinnn/AA-CLIP.",
      "tldr_zh": "本研究提出了一种基于CLIP的改进模型AA-CLIP，用于增强零样本异常检测任务。AA-CLIP通过两阶段策略解决CLIP模型在异常检测中的局限性：首先创建异常感知的文本锚点以区分正常与异常语义，然后将图像块级视觉特征与这些锚点对齐以实现精确的异常定位。实验表明，AA-CLIP在工业和医疗领域的异常检测任务中表现优异，达到了最先进的性能，同时保持了CLIP的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06661v1",
      "published_date": "2025-03-09 15:22:52 UTC",
      "updated_date": "2025-03-09 15:22:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:27:35.333738"
    },
    {
      "arxiv_id": "2503.06648v1",
      "title": "Enhancing NLP Robustness and Generalization through LLM-Generated Contrast Sets: A Scalable Framework for Systematic Evaluation and Adversarial Training",
      "title_zh": "通过LLM生成的对比集增强NLP的鲁棒性与泛化能力：一种面向系统评估与对抗训练的可扩展框架",
      "authors": [
        "Hender Lin"
      ],
      "abstract": "Standard NLP benchmarks often fail to capture vulnerabilities stemming from\ndataset artifacts and spurious correlations. Contrast sets address this gap by\nchallenging models near decision boundaries but are traditionally\nlabor-intensive to create and limited in diversity. This study leverages large\nlanguage models to automate the generation of diverse contrast sets. Using the\nSNLI dataset, we created a 3,000-example contrast set to evaluate and improve\nmodel robustness. Fine-tuning on these contrast sets enhanced performance on\nsystematically perturbed examples, maintained standard test accuracy, and\nmodestly improved generalization to novel perturbations. This automated\napproach offers a scalable solution for evaluating and improving NLP models,\naddressing systematic generalization challenges, and advancing robustness in\nreal-world applications.",
      "tldr_zh": "该研究提出利用大语言模型(LLM)自动生成多样化对比集(contrast sets)的框架，以解决传统NLP基准测试难以捕捉模型脆弱性的问题。通过在SNLI数据集上构建3000个对比样本的实验表明，基于这些数据微调的模型不仅提升了系统性扰动示例的鲁棒性，还保持了标准测试准确率，并能适度改善对新扰动的泛化能力。该方法为评估和改进NLP模型提供了可扩展的解决方案，有助于提升模型在真实场景中的系统泛化能力和对抗性鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06648v1",
      "published_date": "2025-03-09 14:52:53 UTC",
      "updated_date": "2025-03-09 14:52:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:27:41.991971"
    },
    {
      "arxiv_id": "2503.06635v1",
      "title": "Deep Cut-informed Graph Embedding and Clustering",
      "title_zh": "深度割信息图嵌入与聚类",
      "authors": [
        "Zhiyuan Ning",
        "Zaitian Wang",
        "Ran Zhang",
        "Ping Xu",
        "Kunpeng Liu",
        "Pengyang Wang",
        "Chong Chen",
        "Pengfei Wang",
        "Yuanchun Zhou",
        "Erik Cambria"
      ],
      "abstract": "Graph clustering aims to divide the graph into different clusters. The\nrecently emerging deep graph clustering approaches are largely built on graph\nneural networks (GNN). However, GNN is designed for general graph encoding and\nthere is a common issue of representation collapse in existing GNN-based deep\ngraph clustering algorithms. We attribute two main reasons for such issue: (i)\nthe inductive bias of GNN models: GNNs tend to generate similar representations\nfor proximal nodes. Since graphs often contain a non-negligible amount of\ninter-cluster links, the bias results in error message passing and leads to\nbiased clustering; (ii) the clustering guided loss function: most traditional\napproaches strive to make all samples closer to pre-learned cluster centers,\nwhich cause a degenerate solution assigning all data points to a single label\nthus make all samples and less discriminative. To address these challenges, we\ninvestigate graph clustering from a graph cut perspective and propose an\ninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clustering\nframework, namely DCGC. This framework includes two modules: (i) cut-informed\ngraph encoding; (ii) self-supervised graph clustering via optimal transport.\nFor the encoding module, we derive a cut-informed graph embedding objective to\nfuse graph structure and attributes by minimizing their joint normalized cut.\nFor the clustering module, we utilize the optimal transport theory to obtain\nthe clustering assignments, which can balance the guidance of proximity to the\npre-learned cluster center. With the above two tailored designs, DCGC is more\nsuitable for the graph clustering task, which can effectively alleviate the\nproblem of representation collapse and achieve better performance. We conduct\nextensive experiments to demonstrate that our method is simple but effective\ncompared with benchmarks.",
      "tldr_zh": "本文提出了一种基于图割的深度图嵌入与聚类框架DCGC，旨在解决现有基于图神经网络(GNN)的深度图聚类算法中普遍存在的表示崩溃问题。该框架包含两个核心模块：(1) 图割信息引导的图编码模块，通过最小化联合归一化割来融合图结构与属性；(2) 基于最优传输理论的自监督图聚类模块，平衡样本与预学习聚类中心的接近程度。实验表明，DCGC能够有效缓解表示崩溃问题，在性能上优于现有基准方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06635v1",
      "published_date": "2025-03-09 14:24:09 UTC",
      "updated_date": "2025-03-09 14:24:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:27:44.466277"
    },
    {
      "arxiv_id": "2503.08705v1",
      "title": "A Block-Based Heuristic Algorithm for the Three-Dimensional Nuclear Waste Packing Problem",
      "title_zh": "三维核废料装箱问题的基于块体启发式算法",
      "authors": [
        "Yajie Wen",
        "Defu Zhang"
      ],
      "abstract": "In this study, we present a block-based heuristic search algorithm to address\nthe nuclear waste container packing problem in the context of real-world\nnuclear power plants. Additionally, we provide a dataset comprising 1600\nproblem instances for future researchers to use. Experimental results on this\ndataset demonstrate that the proposed algorithm effectively enhances the\ndisposal pool's space utilization while minimizing the radiation dose within\nthe pool. The code and data employed in this study are publicly available to\nfacilitate reproducibility and further investigation.",
      "tldr_zh": "本研究提出了一种基于块体（block-based）的启发式算法，用于解决核电站实际场景中的三维核废料容器装箱问题。该算法通过优化装箱方案，显著提高了核废料处置池的空间利用率，同时有效降低了池内的辐射剂量。为促进相关研究，作者还公开了包含1600个问题实例的数据集及算法代码。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "10 pages,7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08705v1",
      "published_date": "2025-03-09 14:20:48 UTC",
      "updated_date": "2025-03-09 14:20:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:08.407825"
    },
    {
      "arxiv_id": "2503.06633v1",
      "title": "BTFL: A Bayesian-based Test-Time Generalization Method for Internal and External Data Distributions in Federated learning",
      "title_zh": "BTFL：一种基于贝叶斯的联邦学习内外部数据分布测试时泛化方法",
      "authors": [
        "Yu Zhou",
        "Bingyan Liu"
      ],
      "abstract": "Federated Learning (FL) enables multiple clients to collaboratively develop a\nglobal model while maintaining data privacy. However, online FL deployment\nfaces challenges due to distribution shifts and evolving test samples.\nPersonalized Federated Learning (PFL) tailors the global model to individual\nclient distributions, but struggles with Out-Of-Distribution (OOD) samples\nduring testing, leading to performance degradation. In real-world scenarios,\nbalancing personalization and generalization during online testing is crucial\nand existing methods primarily focus on training-phase generalization. To\naddress the test-time trade-off, we introduce a new scenario: Test-time\nGeneralization for Internal and External Distributions in Federated Learning\n(TGFL), which evaluates adaptability under Internal Distribution (IND) and\nExternal Distribution (EXD). We propose BTFL, a Bayesian-based test-time\ngeneralization method for TGFL, which balances generalization and\npersonalization at the sample level during testing. BTFL employs a two-head\narchitecture to store local and global knowledge, interpolating predictions via\na dual-Bayesian framework that considers both historical test data and current\nsample characteristics with theoretical guarantee and faster speed. Our\nexperiments demonstrate that BTFL achieves improved performance across various\ndatasets and models with less time cost. The source codes are made publicly\navailable at https://github.com/ZhouYuCS/BTFL .",
      "tldr_zh": "本研究提出了BTFL，一种基于贝叶斯框架的测试时泛化方法，用于解决联邦学习（FL）中内部分布（IND）和外部分布（EXD）数据的适应性问题。BTFL通过双头架构分别存储本地和全局知识，并利用双贝叶斯框架在测试时动态平衡泛化与个性化，同时考虑历史测试数据和当前样本特征。实验表明，BTFL在多种数据集和模型上显著提升了性能，且具有更快的计算速度，为联邦学习在实际部署中的测试时泛化提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted as KDD 2025 research track paper",
      "pdf_url": "http://arxiv.org/pdf/2503.06633v1",
      "published_date": "2025-03-09 14:16:34 UTC",
      "updated_date": "2025-03-09 14:16:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:11.281413"
    },
    {
      "arxiv_id": "2503.06629v1",
      "title": "Hardware-Accelerated Event-Graph Neural Networks for Low-Latency Time-Series Classification on SoC FPGA",
      "title_zh": "硬件加速的事件图神经网络：面向SoC FPGA的低延迟时间序列分类",
      "authors": [
        "Hiroshi Nakano",
        "Krzysztof Blachut",
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Tomasz Kryjak",
        "Thomas Dalgaty"
      ],
      "abstract": "As the quantities of data recorded by embedded edge sensors grow, so too does\nthe need for intelligent local processing. Such data often comes in the form of\ntime-series signals, based on which real-time predictions can be made locally\nusing an AI model. However, a hardware-software approach capable of making\nlow-latency predictions with low power consumption is required. In this paper,\nwe present a hardware implementation of an event-graph neural network for\ntime-series classification. We leverage an artificial cochlea model to convert\nthe input time-series signals into a sparse event-data format that allows the\nevent-graph to drastically reduce the number of calculations relative to other\nAI methods. We implemented the design on a SoC FPGA and applied it to the\nreal-time processing of the Spiking Heidelberg Digits (SHD) dataset to\nbenchmark our approach against competitive solutions. Our method achieves a\nfloating-point accuracy of 92.7% on the SHD dataset for the base model, which\nis only 2.4% and 2% less than the state-of-the-art models with over 10% and 67%\nfewer model parameters, respectively. It also outperforms FPGA-based spiking\nneural network implementations by 19.3% and 4.5%, achieving 92.3% accuracy for\nthe quantised model while using fewer computational resources and reducing\nlatency.",
      "tldr_zh": "本文提出了一种基于SoC FPGA的事件图神经网络硬件加速方案，用于低延迟时间序列分类。该方法采用仿生耳蜗模型将输入信号转换为稀疏事件数据格式，相比传统AI方法大幅减少计算量。在Spiking Heidelberg Digits数据集上的实验表明，其浮点模型准确率达92.7%，仅比最优模型低2.4%，但参数量减少10-67%；量化模型准确率达92.3%，优于现有FPGA脉冲神经网络方案4.5-19.3%，同时降低延迟和计算资源消耗。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted for the 21st International Symposium on Applied\n  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06629v1",
      "published_date": "2025-03-09 14:08:46 UTC",
      "updated_date": "2025-03-09 14:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:38.633247"
    },
    {
      "arxiv_id": "2503.06627v1",
      "title": "Revisiting Early Detection of Sexual Predators via Turn-level Optimization",
      "title_zh": "重探基于回合级别优化的性侵害者早期检测",
      "authors": [
        "Jinmyeong An",
        "Sangwon Ryu",
        "Heejin Do",
        "Yunsu Kim",
        "Jungseul Ok",
        "Gary Geunbae Lee"
      ],
      "abstract": "Online grooming is a severe social threat where sexual predators gradually\nentrap child victims with subtle and gradual manipulation. Therefore, timely\nintervention for online grooming is critical for proactive protection. However,\nprevious methods fail to determine the optimal intervention points (i.e., jump\nto conclusions) as they rely on chat-level risk labels by causing weak\nsupervision of risky utterances. For timely detection, we propose speed control\nreinforcement learning (SCoRL) (The code and supplementary materials are\navailable at https://github.com/jinmyeongAN/SCoRL), incorporating a practical\nstrategy derived from luring communication theory (LCT). To capture the\npredator's turn-level entrapment, we use a turn-level risk label based on the\nLCT. Then, we design a novel speed control reward function that balances the\ntrade-off between speed and accuracy based on turn-level risk label; thus,\nSCoRL can identify the optimal intervention moment. In addition, we introduce a\nturn-level metric for precise evaluation, identifying limitations in previously\nused chat-level metrics. Experimental results show that SCoRL effectively\npreempted online grooming, offering a more proactive and timely solution.\nFurther analysis reveals that our method enhances performance while intuitively\nidentifying optimal early intervention points.",
      "tldr_zh": "该研究提出了一种基于轮次优化的速度控制强化学习(SCoRL)方法，用于早期检测网络性诱骗行为。该方法结合诱骗沟通理论(LCT)，通过轮次风险标签和设计的速度控制奖励函数，平衡检测速度与准确性，从而确定最佳干预时机。实验结果表明，SCoRL能有效预防网络性诱骗，提供更主动和及时的解决方案，同时直观地识别出最佳早期干预点。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a main conference paper at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06627v1",
      "published_date": "2025-03-09 14:05:27 UTC",
      "updated_date": "2025-03-09 14:05:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:37.482797"
    },
    {
      "arxiv_id": "2503.06626v1",
      "title": "DiffCLIP: Differential Attention Meets CLIP",
      "title_zh": "DiffCLIP：差分注意力机制与CLIP的融合",
      "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Bernard Ghanem"
      ],
      "abstract": "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps://github.com/hammoudhasan/DiffCLIP.",
      "tldr_zh": "本文提出DiffCLIP模型，将差分注意力机制(differential attention)引入CLIP架构，通过放大相关上下文并抑制噪声信息来增强多模态表示。该方法仅需少量额外参数，就在零样本分类、检索和鲁棒性测试中全面超越基准CLIP模型，且计算开销几乎不变。研究证实差分注意力能有效提升视觉-语言模型的性能而不牺牲效率，代码已开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.06626v1",
      "published_date": "2025-03-09 14:04:09 UTC",
      "updated_date": "2025-03-09 14:04:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:29.462606"
    },
    {
      "arxiv_id": "2503.06614v1",
      "title": "Using Subgraph GNNs for Node Classification:an Overlooked Potential Approach",
      "title_zh": "利用子图GNN进行节点分类：一种被忽视的潜力方法",
      "authors": [
        "Qian Zeng",
        "Xin Lin",
        "Jingyi Gao",
        "Yang Yu"
      ],
      "abstract": "Previous studies have demonstrated the strong performance of Graph Neural\nNetworks (GNNs) in node classification. However, most existing GNNs adopt a\nnode-centric perspective and rely on global message passing, leading to high\ncomputational and memory costs that hinder scalability. To mitigate these\nchallenges, subgraph-based methods have been introduced, leveraging local\nsubgraphs as approximations of full computational trees. While this approach\nimproves efficiency, it often suffers from performance degradation due to the\nloss of global contextual information, limiting its effectiveness compared to\nglobal GNNs. To address this trade-off between scalability and classification\naccuracy, we reformulate the node classification task as a subgraph\nclassification problem and propose SubGND (Subgraph GNN for NoDe). This\nframework introduces a differentiated zero-padding strategy and an Ego-Alter\nsubgraph representation method to resolve label conflicts while incorporating\nan Adaptive Feature Scaling Mechanism to dynamically adjust feature\ncontributions based on dataset-specific dependencies. Experimental results on\nsix benchmark datasets demonstrate that SubGND achieves performance comparable\nto or surpassing global message-passing GNNs, particularly in heterophilic\nsettings, highlighting its effectiveness and scalability as a promising\nsolution for node classification.",
      "tldr_zh": "该论文针对图神经网络(GNN)节点分类任务中的效率与精度矛盾，提出创新性解决方案SubGND框架。通过将节点分类重构为子图分类问题，结合差异化零填充策略和Ego-Alter子图表示法，有效解决了传统子图方法丢失全局信息的问题。实验表明，该框架在六个基准数据集上达到或超越全局消息传递GNN的性能，尤其在异配性(heterophilic)场景表现突出，同时保持了良好的可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06614v1",
      "published_date": "2025-03-09 13:37:38 UTC",
      "updated_date": "2025-03-09 13:37:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:48.384040"
    },
    {
      "arxiv_id": "2503.06580v1",
      "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning models",
      "title_zh": "智能体模型：将行动链生成内化至推理模型",
      "authors": [
        "Yuxiang Zhang",
        "Yuqi Yang",
        "Jiangming Shu",
        "Xinyan Wen",
        "Jitao Sang"
      ],
      "abstract": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position \\emph{Large Agent Models (LAMs)} that internalize the generation of\n\\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
      "tldr_zh": "该研究提出了大型智能体模型(LAMs)，通过将链式动作生成(Chain-of-Action, CoA)内化到推理模型中，提升了智能体的自主性。提出的AutoCoA框架结合了监督微调(SFT)和强化学习(RL)，使模型能够在推理和动作之间无缝切换，并高效管理环境交互。关键组件包括步骤级动作触发、轨迹级CoA优化和内部世界模型，以减少实际环境交互成本。实验表明，在开放域问答任务中，AutoCoA训练的智能体模型显著优于基于ReAct的工作流，尤其在需要长期推理和多步动作的任务中表现突出。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06580v1",
      "published_date": "2025-03-09 12:19:47 UTC",
      "updated_date": "2025-03-09 12:19:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:28:53.864457"
    },
    {
      "arxiv_id": "2503.06573v1",
      "title": "WildIFEval: Instruction Following in the Wild",
      "title_zh": "WildIFEval：真实场景下的指令遵循",
      "authors": [
        "Gili Lior",
        "Asaf Yehudai",
        "Ariel Gera",
        "Liat Ein-Dor"
      ],
      "abstract": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, in natural user prompts. We categorize these constraints into\neight high-level classes to capture their distribution and dynamics in\nreal-world scenarios. Leveraging WildIFEval, we conduct extensive experiments\nto benchmark the instruction-following capabilities of leading LLMs. Our\nfindings reveal that all evaluated models experience performance degradation\nwith an increasing number of constraints. Thus, we show that all models have a\nlarge room for improvement on such tasks. Moreover, we observe that the\nspecific type of constraint plays a critical role in model performance. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.",
      "tldr_zh": "该研究提出了WildIFEval数据集，包含12K条真实用户的多约束指令，涵盖广泛的词汇和主题范围。研究发现，当前主流大语言模型（LLMs）在约束条件增加时性能均会下降，且不同约束类型对模型表现影响显著。该工作揭示了现有模型在复杂真实场景下的指令跟随能力仍有很大提升空间，并开源数据集以推动相关研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06573v1",
      "published_date": "2025-03-09 12:06:29 UTC",
      "updated_date": "2025-03-09 12:06:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:15.404366"
    },
    {
      "arxiv_id": "2503.06571v2",
      "title": "SHIP: A Shapelet-based Approach for Interpretable Patient-Ventilator Asynchrony Detection",
      "title_zh": "SHIP：一种基于Shapelet的可解释性患者-呼吸机异步检测方法",
      "authors": [
        "Xuan-May Le",
        "Ling Luo",
        "Uwe Aickelin",
        "Minh-Tuan Tran",
        "David Berlowitz",
        "Mark Howard"
      ],
      "abstract": "Patient-ventilator asynchrony (PVA) is a common and critical issue during\nmechanical ventilation, affecting up to 85% of patients. PVA can result in\nclinical complications such as discomfort, sleep disruption, and potentially\nmore severe conditions like ventilator-induced lung injury and diaphragm\ndysfunction. Traditional PVA management, which relies on manual adjustments by\nhealthcare providers, is often inadequate due to delays and errors. While\nvarious computational methods, including rule-based, statistical, and deep\nlearning approaches, have been developed to detect PVA events, they face\nchallenges related to dataset imbalances and lack of interpretability. In this\nwork, we propose a shapelet-based approach SHIP for PVA detection, utilizing\nshapelets - discriminative subsequences in time-series data - to enhance\ndetection accuracy and interpretability. Our method addresses dataset\nimbalances through shapelet-based data augmentation and constructs a shapelet\npool to transform the dataset for more effective classification. The combined\nshapelet and statistical features are then used in a classifier to identify PVA\nevents. Experimental results on medical datasets show that SHIP significantly\nimproves PVA detection while providing interpretable insights into model\ndecisions.",
      "tldr_zh": "本研究提出基于shapelet（时间序列判别性子序列）的SHIP方法，用于检测患者-呼吸机异步(PVA)问题。该方法通过shapelet数据增强解决数据集不平衡问题，并构建shapelet池进行特征转换，结合统计特征实现高精度分类。实验表明，SHIP在提高PVA检测准确率的同时，提供了可解释的决策依据，克服了现有深度学习方法缺乏可解释性的缺陷。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at PAKDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06571v2",
      "published_date": "2025-03-09 11:58:03 UTC",
      "updated_date": "2025-03-13 02:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:00.284957"
    },
    {
      "arxiv_id": "2503.06568v1",
      "title": "Conceptrol: Concept Control of Zero-shot Personalized Image Generation",
      "title_zh": "Conceptrol：零样本个性化图像生成的概念控制",
      "authors": [
        "Qiyuan He",
        "Angela Yao"
      ],
      "abstract": "Personalized image generation with text-to-image diffusion models generates\nunseen images based on reference image content. Zero-shot adapter methods such\nas IP-Adapter and OminiControl are especially interesting because they do not\nrequire test-time fine-tuning. However, they struggle to balance preserving\npersonalized content and adherence to the text prompt. We identify a critical\ndesign flaw resulting in this performance gap: current adapters inadequately\nintegrate personalization images with the textual descriptions. The generated\nimages, therefore, replicate the personalized content rather than adhere to the\ntext prompt instructions. Yet the base text-to-image has strong conceptual\nunderstanding capabilities that can be leveraged.\n  We propose Conceptrol, a simple yet effective framework that enhances\nzero-shot adapters without adding computational overhead. Conceptrol constrains\nthe attention of visual specification with a textual concept mask that improves\nsubject-driven generation capabilities. It achieves as much as 89% improvement\non personalization benchmarks over the vanilla IP-Adapter and can even\noutperform fine-tuning approaches such as Dreambooth LoRA. The source code is\navailable at https://github.com/QY-H00/Conceptrol.",
      "tldr_zh": "该研究提出了Conceptrol框架，用于改进零样本个性化图像生成中的概念控制问题。现有的零样本适配器方法（如IP-Adapter和OminiControl）在平衡个性化内容与文本提示的遵从性上表现不佳，原因是未能充分整合个性化图像与文本描述。Conceptrol通过引入文本概念掩码约束视觉规范的注意力，显著提升了主题驱动生成能力。实验表明，Conceptrol在个性化基准测试中比IP-Adapter提升高达89%，甚至优于微调方法如Dreambooth LoRA，且无需增加计算开销。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06568v1",
      "published_date": "2025-03-09 11:54:08 UTC",
      "updated_date": "2025-03-09 11:54:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:16.584433"
    },
    {
      "arxiv_id": "2503.06567v1",
      "title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
      "title_zh": "受人类认知启发的基于知识图谱的RAG框架用于复杂问题求解",
      "authors": [
        "Yao Cheng",
        "Yibo Zhao",
        "Jiapeng Zhu",
        "Yao Liu",
        "Xing Sun",
        "Xiang Li"
      ],
      "abstract": "Large language models (LLMs) have demonstrated transformative potential\nacross various domains, yet they face significant challenges in knowledge\nintegration and complex problem reasoning, often leading to hallucinations and\nunreliable outputs. Retrieval-Augmented Generation (RAG) has emerged as a\npromising solution to enhance LLMs accuracy by incorporating external\nknowledge. However, traditional RAG systems struggle with processing complex\nrelational information and multi-step reasoning, limiting their effectiveness\nin advanced problem-solving tasks. To address these limitations, we propose\nCogGRAG, a cognition inspired graph-based RAG framework, designed to improve\nLLMs performance in Knowledge Graph Question Answering (KGQA). Inspired by the\nhuman cognitive process of decomposing complex problems and performing\nself-verification, our framework introduces a three-stage methodology:\ndecomposition, retrieval, and reasoning with self-verification. By integrating\nthese components, CogGRAG enhances the accuracy of LLMs in complex problem\nsolving. We conduct systematic experiments with three LLM backbones on four\nbenchmark datasets, where CogGRAG outperforms the baselines.",
      "tldr_zh": "该研究提出CogGRAG，一种受人类认知启发的基于知识图谱的检索增强生成（RAG）框架，旨在提升大语言模型（LLMs）在复杂问题解决中的表现。CogGRAG模仿人类分解复杂问题并进行自我验证的认知过程，采用三阶段方法：问题分解、知识检索和带自我验证的推理。实验表明，CogGRAG在四个基准数据集上显著优于传统RAG方法，为知识图谱问答（KGQA）任务提供了更可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06567v1",
      "published_date": "2025-03-09 11:50:39 UTC",
      "updated_date": "2025-03-09 11:50:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:29.986775"
    },
    {
      "arxiv_id": "2503.06563v1",
      "title": "LSA: Latent Style Augmentation Towards Stain-Agnostic Cervical Cancer Screening",
      "title_zh": "LSA：面向染色无关的宫颈癌筛查的潜在风格增强方法",
      "authors": [
        "Jiangdong Cai",
        "Haotian Jiang",
        "Zhenrong Shen",
        "Yonghao Li",
        "Honglin Xiong",
        "Lichi Zhang",
        "Qian Wang"
      ],
      "abstract": "The deployment of computer-aided diagnosis systems for cervical cancer\nscreening using whole slide images (WSIs) faces critical challenges due to\ndomain shifts caused by staining variations across different scanners and\nimaging environments. While existing stain augmentation methods improve\npatch-level robustness, they fail to scale to WSIs due to two key limitations:\n(1) inconsistent stain patterns when extending patch operations to gigapixel\nslides, and (2) prohibitive computational/storage costs from offline processing\nof augmented WSIs.To address this, we propose Latent Style Augmentation (LSA),\na framework that performs efficient, online stain augmentation directly on\nWSI-level latent features. We first introduce WSAug, a WSI-level stain\naugmentation method ensuring consistent stain across patches within a WSI.\nUsing offline-augmented WSIs by WSAug, we design and train Stain Transformer,\nwhich can simulate targeted style in the latent space, efficiently enhancing\nthe robustness of the WSI-level classifier. We validate our method on a\nmulti-scanner WSI dataset for cervical cancer diagnosis. Despite being trained\non data from a single scanner, our approach achieves significant performance\nimprovements on out-of-distribution data from other scanners. Code will be\navailable at https://github.com/caijd2000/LSA.",
      "tldr_zh": "该研究提出Latent Style Augmentation (LSA)框架，通过WSI级潜在空间染色增强技术解决宫颈癌筛查中全切片图像(WSI)的染色变异问题。创新性地开发了WSAug染色增强方法和Stain Transformer模型，直接在潜在特征空间进行在线染色风格转换，避免了传统方法在千兆像素级WSI上计算成本过高和染色不一致的问题。实验表明，该方法在跨扫描仪数据集上显著提升模型泛化能力，即使仅用单扫描仪数据训练，也能有效应对其他扫描仪的分布外数据。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06563v1",
      "published_date": "2025-03-09 11:33:59 UTC",
      "updated_date": "2025-03-09 11:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:37.586523"
    },
    {
      "arxiv_id": "2503.06553v1",
      "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges",
      "title_zh": "ProJudge：面向多模态大语言模型流程评判的多学科基准与指令微调数据集",
      "authors": [
        "Jiaxin Ai",
        "Pengfei Zhou",
        "Zhaopan Xu",
        "Ming Li",
        "Fanrui Zhang",
        "Zizhen Li",
        "Jianwen Sun",
        "Yukang Feng",
        "Baojin Huang",
        "Zhongyuan Wang",
        "Kaipeng Zhang"
      ],
      "abstract": "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
      "tldr_zh": "该研究提出了ProJudgeBench，首个专为评估多模态大语言模型(MLLM)过程判断能力设计的综合基准，涵盖四个科学领域的2400个测试案例和50118个步骤级标签，每步均经过专家标注正确性、错误类型和解释，以系统评估模型检测、分类和诊断错误的能力。研究发现开源模型与专有模型存在显著性能差距，为此提出了ProJudge-173k大规模指令微调数据集和动态双阶段微调策略，显著提升了开源模型的过程评估能力。所有资源将公开以推动可靠多模态过程评估的研究。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06553v1",
      "published_date": "2025-03-09 10:55:51 UTC",
      "updated_date": "2025-03-09 10:55:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:53.206406"
    },
    {
      "arxiv_id": "2503.06551v2",
      "title": "ChatGPT-4 in the Turing Test: A Critical Analysis",
      "title_zh": "ChatGPT-4在图灵测试中的表现：一项批判性分析",
      "authors": [
        "Marco Giunti"
      ],
      "abstract": "This paper critically examines the recent publication \"ChatGPT-4 in the\nTuring Test\" by Restrepo Echavarr\\'ia (2025), challenging its central claims\nregarding the absence of minimally serious test implementations and the\nconclusion that ChatGPT-4 fails the Turing Test. The analysis reveals that the\ncriticisms based on rigid criteria and limited experimental data are not fully\njustified. More importantly, the paper makes several constructive contributions\nthat enrich our understanding of Turing Test implementations. It demonstrates\nthat two distinct formats--the three-player and two-player tests--are both\nvalid, each with unique methodological implications. The work distinguishes\nbetween absolute criteria (reflecting an optimal 50% identification rate in a\nthree-player format) and relative criteria (which measure how closely a\nmachine's performance approximates that of a human), offering a more nuanced\nevaluation framework. Furthermore, the paper clarifies the probabilistic\nunderpinnings of both test types by modeling them as Bernoulli\nexperiments--correlated in the three-player version and uncorrelated in the\ntwo-player version. This formalization allows for a rigorous separation between\nthe theoretical criteria for passing the test, defined in probabilistic terms,\nand the experimental data that require robust statistical methods for proper\ninterpretation. In doing so, the paper not only refutes key aspects of the\ncriticized study but also lays a solid foundation for future research on\nobjective measures of how closely an AI's behavior aligns with, or deviates\nfrom, that of a human being.",
      "tldr_zh": "本文对Restrepo Echavarría(2025)关于ChatGPT-4未能通过图灵测试的研究提出批判性分析，指出其基于严格标准和有限实验数据的结论不够充分。研究提出两种有效的测试格式——三参与者和双参与者测试，并建立了更精细的评估框架，区分绝对标准(三参与者测试中50%识别率)和相对标准(机器表现接近人类的程度)。通过将测试建模为伯努利实验(三参与者版本相关、双参与者版本独立)，研究不仅反驳了原研究的关键论点，还为未来衡量AI行为与人类相似度的客观标准奠定了理论基础。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "68T01"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 1 Appendix, added 1 missing item in References, corrected\n  typos",
      "pdf_url": "http://arxiv.org/pdf/2503.06551v2",
      "published_date": "2025-03-09 10:43:17 UTC",
      "updated_date": "2025-03-11 12:33:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:29:58.456369"
    },
    {
      "arxiv_id": "2503.06542v1",
      "title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy",
      "title_zh": "ARMOR v0.1：通过非对称协同实现交错多模态生成的自回归多模态理解模型增强",
      "authors": [
        "Jianwen Sun",
        "Yukang Feng",
        "Chuanhao Li",
        "Fanrui Zhang",
        "Zizhen Li",
        "Jiaxin Ai",
        "Sizhuo Zhou",
        "Yu Dai",
        "Shenglin Zhang",
        "Kaipeng Zhang"
      ],
      "abstract": "Unified models (UniMs) for multimodal understanding and generation have\nrecently received much attention in the area of vision and language. Existing\nUniMs are designed to simultaneously learn both multimodal understanding and\ngeneration capabilities, demanding substantial computational resources, and\noften struggle to generate interleaved text-image. We present ARMOR, a\nresource-efficient and pure autoregressive framework that achieves both\nunderstanding and generation by fine-tuning existing multimodal large language\nmodels (MLLMs). Specifically, ARMOR extends existing MLLMs from three\nperspectives: (1) For model architecture, an asymmetric encoder-decoder\narchitecture with a forward-switching mechanism is introduced to unify\nembedding space integrating textual and visual modalities for enabling natural\ntext-image interleaved generation with minimal computational overhead. (2) For\ntraining data, a meticulously curated, high-quality interleaved dataset is\ncollected for fine-tuning MLLMs. (3) For the training algorithm, we propose a\n``what or how to generate\" algorithm to empower existing MLLMs with multimodal\ngeneration capabilities while preserving their multimodal understanding\ncapabilities, through three progressive training stages based on the collected\ndataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to\nUniMs with promising image generation capabilities, using limited training\nresources. Our code will be released soon at https://armor.github.io.",
      "tldr_zh": "本研究提出ARMOR框架，通过非对称协同机制将现有多模态大语言模型(MLLMs)升级为兼具理解与生成能力的统一模型。该框架采用前向切换机制的非对称编码器-解码器架构，以最小计算开销实现文本-图像交错生成；通过精心构建的交错数据集和三阶段渐进训练算法，在保留原有理解能力的同时赋予模型多模态生成能力。实验表明，ARMOR能以有限训练资源将现有MLLMs转化为具有图像生成潜力的统一模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06542v1",
      "published_date": "2025-03-09 10:15:39 UTC",
      "updated_date": "2025-03-09 10:15:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:30:03.302473"
    },
    {
      "arxiv_id": "2503.06529v2",
      "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
      "title_zh": "AnywhereDoor：针对目标检测的多目标后门攻击",
      "authors": [
        "Jialin Lu",
        "Junjie Shan",
        "Ziqi Zhao",
        "Ka-Ho Chow"
      ],
      "abstract": "As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a serious threat by implanting hidden triggers in victim\nmodels, which adversaries can later exploit to induce malicious behaviors\nduring inference. However, current understanding is limited to single-target\nattacks, where adversaries must define a fixed malicious behavior (target)\nbefore training, making inference-time adaptability impossible. Given the large\noutput space of object detection (including object existence prediction,\nbounding box estimation, and classification), the feasibility of flexible,\ninference-time model control remains unexplored. This paper introduces\nAnywhereDoor, a multi-target backdoor attack for object detection. Once\nimplanted, AnywhereDoor allows adversaries to make objects disappear, fabricate\nnew ones, or mislabel them, either across all object classes or specific ones,\noffering an unprecedented degree of control. This flexibility is enabled by\nthree key innovations: (i) objective disentanglement to scale the number of\nsupported targets; (ii) trigger mosaicking to ensure robustness even against\nregion-based detectors; and (iii) strategic batching to address object-level\ndata imbalances that hinder manipulation. Extensive experiments demonstrate\nthat AnywhereDoor grants attackers a high degree of control, improving attack\nsuccess rates by 26% compared to adaptations of existing methods for such\nflexible control.",
      "tldr_zh": "该研究提出了AnywhereDoor，一种针对目标检测的多目标后门攻击方法。与传统的单目标攻击不同，AnywhereDoor允许攻击者在推理阶段灵活控制模型行为，例如使物体消失、伪造新物体或错误标记物体，且可针对所有类别或特定类别。其核心创新包括目标解耦以支持多目标、触发马赛克以确保对区域检测器的鲁棒性，以及策略性批处理以解决数据不平衡问题。实验表明，AnywhereDoor显著提升了攻击成功率，较现有方法提高了26%。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "This work was intended as a replacement of arXiv:2411.14243 and any\n  subsequent updates will appear there",
      "pdf_url": "http://arxiv.org/pdf/2503.06529v2",
      "published_date": "2025-03-09 09:24:24 UTC",
      "updated_date": "2025-03-13 04:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:30:24.145657"
    },
    {
      "arxiv_id": "2503.06525v1",
      "title": "From Motion Signals to Insights: A Unified Framework for Student Behavior Analysis and Feedback in Physical Education Classes",
      "title_zh": "从运动信号到洞察：体育课学生行为分析与反馈的统一框架",
      "authors": [
        "Xian Gao",
        "Jiacheng Ruan",
        "Jingsheng Gao",
        "Mingye Xie",
        "Zongyun Zhang",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Analyzing student behavior in educational scenarios is crucial for enhancing\nteaching quality and student engagement. Existing AI-based models often rely on\nclassroom video footage to identify and analyze student behavior. While these\nvideo-based methods can partially capture and analyze student actions, they\nstruggle to accurately track each student's actions in physical education\nclasses, which take place in outdoor, open spaces with diverse activities, and\nare challenging to generalize to the specialized technical movements involved\nin these settings. Furthermore, current methods typically lack the ability to\nintegrate specialized pedagogical knowledge, limiting their ability to provide\nin-depth insights into student behavior and offer feedback for optimizing\ninstructional design. To address these limitations, we propose a unified\nend-to-end framework that leverages human activity recognition technologies\nbased on motion signals, combined with advanced large language models, to\nconduct more detailed analyses and feedback of student behavior in physical\neducation classes. Our framework begins with the teacher's instructional\ndesigns and the motion signals from students during physical education\nsessions, ultimately generating automated reports with teaching insights and\nsuggestions for improving both learning and class instructions. This solution\nprovides a motion signal-based approach for analyzing student behavior and\noptimizing instructional design tailored to physical education classes.\nExperimental results demonstrate that our framework can accurately identify\nstudent behaviors and produce meaningful pedagogical insights.",
      "tldr_zh": "该研究提出了一种统一的端到端框架，用于体育课堂中学生行为的分析与反馈。该框架结合基于运动信号的人类活动识别技术和大型语言模型，能够更精确地追踪学生在户外开放空间中的多样化动作，并生成教学优化建议。实验表明，该框架不仅能准确识别学生行为，还能提供有意义的教学洞察，为体育课堂的教学设计优化提供了基于运动信号的分析方法。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.06525v1",
      "published_date": "2025-03-09 09:04:36 UTC",
      "updated_date": "2025-03-09 09:04:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:30:19.997114"
    },
    {
      "arxiv_id": "2503.06523v1",
      "title": "Generative AI as Digital Media",
      "title_zh": "生成式AI作为数字媒体",
      "authors": [
        "Gilad Abiri"
      ],
      "abstract": "Generative AI is frequently portrayed as revolutionary or even apocalyptic,\nprompting calls for novel regulatory approaches. This essay argues that such\nviews are misguided. Instead, generative AI should be understood as an\nevolutionary step in the broader algorithmic media landscape, alongside search\nengines and social media. Like these platforms, generative AI centralizes\ninformation control, relies on complex algorithms to shape content, and\nextensively uses user data, thus perpetuating common problems: unchecked\ncorporate power, echo chambers, and weakened traditional gatekeepers.\nRegulation should therefore share a consistent objective: ensuring media\ninstitutions remain trustworthy. Without trust, public discourse risks\nfragmenting into isolated communities dominated by comforting, tribal beliefs\n-- a threat intensified by generative AI's capacity to bypass gatekeepers and\npersonalize truth. Current governance frameworks, such as the EU's AI Act and\nthe US Executive Order 14110, emphasize reactive risk mitigation, addressing\nmeasurable threats like national security, public health, and algorithmic bias.\nWhile effective for novel technological risks, this reactive approach fails to\nadequately address broader issues of trust and legitimacy inherent to digital\nmedia. Proactive regulation fostering transparency, accountability, and public\nconfidence is essential. Viewing generative AI exclusively as revolutionary\nrisks repeating past regulatory failures that left social media and search\nengines insufficiently regulated. Instead, regulation must proactively shape an\nalgorithmic media environment serving the public good, supporting quality\ninformation and robust civic discourse.",
      "tldr_zh": "这篇论文提出，生成式AI（Generative AI）不应被视为革命性或颠覆性技术，而应看作是算法媒体（如搜索引擎和社交媒体）的自然演进。它延续了集中化信息控制、算法内容操纵和用户数据依赖等问题，加剧了企业权力垄断、信息茧房和传统把关机制弱化等风险。作者认为当前欧盟《AI法案》和美国《14110号行政令》等被动式监管框架过于侧重可量化风险（如国家安全、算法偏见），却忽视了数字媒体固有的公信力危机。文章主张采取主动监管策略，通过提升透明度、问责制和公共信任，构建服务于公共利益和健康公共讨论的算法媒体生态，避免重蹈社交媒体时代监管滞后的覆辙。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06523v1",
      "published_date": "2025-03-09 08:58:17 UTC",
      "updated_date": "2025-03-09 08:58:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:30:32.030588"
    },
    {
      "arxiv_id": "2503.06519v1",
      "title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation",
      "title_zh": "小型语言模型能否可靠抵御越狱攻击？一项全面评估",
      "authors": [
        "Wenhui Zhang",
        "Huiyu Xu",
        "Zhibo Wang",
        "Zeqing He",
        "Ziqi Zhu",
        "Kui Ren"
      ],
      "abstract": "Small language models (SLMs) have emerged as promising alternatives to large\nlanguage models (LLMs) due to their low computational demands, enhanced privacy\nguarantees and comparable performance in specific domains through light-weight\nfine-tuning. Deploying SLMs on edge devices, such as smartphones and smart\nvehicles, has become a growing trend. However, the security implications of\nSLMs have received less attention than LLMs, particularly regarding jailbreak\nattacks, which is recognized as one of the top threats of LLMs by the OWASP. In\nthis paper, we conduct the first large-scale empirical study of SLMs'\nvulnerabilities to jailbreak attacks. Through systematically evaluation on 63\nSLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak\nmethods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility\nto jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct\nharmful query (ASR > 50%). We further analyze the reasons behind the\nvulnerabilities and identify four key factors: model size, model architecture,\ntraining datasets and training techniques. Moreover, we assess the\neffectiveness of three prompt-level defense methods and find that none of them\nachieve perfect performance, with detection accuracy varying across different\nSLMs and attack methods. Notably, we point out that the inherent security\nawareness play a critical role in SLM security, and models with strong security\nawareness could timely terminate unsafe response with little reminder. Building\nupon the findings, we highlight the urgent need for security-by-design\napproaches in SLM development and provide valuable insights for building more\ntrustworthy SLM ecosystem.",
      "tldr_zh": "本研究首次对小语言模型(SLMs)的越狱攻击(jailbreak attacks)脆弱性进行了大规模实证评估。通过对15类主流SLM家族的63个模型进行系统测试，发现47.6%的SLM对越狱攻击高度敏感(攻击成功率>40%)，38.1%甚至无法抵御直接有害查询。研究揭示了模型规模、架构、训练数据和技巧四个关键影响因素，并发现现有提示级防御方法效果有限。特别指出，模型内在的安全意识对防御能力至关重要，具备强安全意识的SLM能及时终止不安全响应。该研究为构建更可信的SLM生态系统提供了重要洞见，强调了安全设计方法的紧迫性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "19 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06519v1",
      "published_date": "2025-03-09 08:47:16 UTC",
      "updated_date": "2025-03-09 08:47:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:30:41.307093"
    },
    {
      "arxiv_id": "2503.06518v1",
      "title": "Towards Superior Quantization Accuracy: A Layer-sensitive Approach",
      "title_zh": "迈向更高量化精度：一种层敏感方法",
      "authors": [
        "Feng Zhang",
        "Yanbin Liu",
        "Weihua Li",
        "Jie Lv",
        "Xiaodan Wang",
        "Quan Bai"
      ],
      "abstract": "Large Vision and Language Models have exhibited remarkable human-like\nintelligence in tasks such as natural language comprehension, problem-solving,\nlogical reasoning, and knowledge retrieval. However, training and serving these\nmodels require substantial computational resources, posing a significant\nbarrier to their widespread application and further research. To mitigate this\nchallenge, various model compression techniques have been developed to reduce\ncomputational requirements. Nevertheless, existing methods often employ uniform\nquantization configurations, failing to account for the varying difficulties\nacross different layers in quantizing large neural network models. This paper\ntackles this issue by leveraging layer-sensitivity features, such as activation\nsensitivity and weight distribution Kurtosis, to identify layers that are\nchallenging to quantize accurately and allocate additional memory budget. The\nproposed methods, named SensiBoost and KurtBoost, respectively, demonstrate\nnotable improvement in quantization accuracy, achieving up to 9% lower\nperplexity with only a 2% increase in memory budget on LLama models compared to\nthe baseline.",
      "tldr_zh": "本文提出了一种基于层敏感性的量化方法，旨在提高大视觉和语言模型的量化精度。传统方法通常采用统一的量化配置，忽略了不同层在量化难度上的差异。为解决这一问题，研究提出了SensiBoost和KurtBoost两种方法，分别利用激活敏感性和权重分布峰度等特征，识别难以量化的层并分配额外的内存预算。实验表明，该方法在LLama模型上显著提升了量化精度，仅增加2%的内存预算即可降低高达9%的困惑度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06518v1",
      "published_date": "2025-03-09 08:45:03 UTC",
      "updated_date": "2025-03-09 08:45:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:30:50.637271"
    },
    {
      "arxiv_id": "2503.06514v2",
      "title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks",
      "title_zh": "GFlowVLM：利用生成流网络增强视觉语言模型的多步推理能力",
      "authors": [
        "Haoqiang Kang",
        "Enna Sachdeva",
        "Piyush Gupta",
        "Sangjae Bae",
        "Kwonjoon Lee"
      ],
      "abstract": "Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.",
      "tldr_zh": "该研究提出了GFlowVLM框架，利用生成流网络(GFlowNets)微调视觉语言模型(VLMs)，以增强其在多步推理任务中的表现。与传统的监督微调(SFT)和强化学习(RL)方法相比，GFlowVLM通过建模非马尔可夫决策过程，捕捉长期依赖关系，并结合链式思维推理(CoT)指导动作选择。实验表明，GFlowVLM在卡片游戏（如NumberLine、BlackJack）和具身规划任务（如ALFWorld）中表现出更高的训练效率、解决方案多样性和更强的泛化能力，优于现有微调方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06514v2",
      "published_date": "2025-03-09 08:38:10 UTC",
      "updated_date": "2025-03-25 07:37:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:07.201556"
    },
    {
      "arxiv_id": "2503.06511v1",
      "title": "HFedCKD: Toward Robust Heterogeneous Federated Learning via Data-free Knowledge Distillation and Two-way Contrast",
      "title_zh": "HFedCKD：通过无数据知识蒸馏与双向对比实现稳健的异构联邦学习",
      "authors": [
        "Yiting Zheng",
        "Bohan Lin",
        "Jinqian Chen",
        "Jihua Zhu"
      ],
      "abstract": "Most current federated learning frameworks are modeled as static processes,\nignoring the dynamic characteristics of the learning system. Under the limited\ncommunication budget of the central server, the flexible model architecture of\na large number of clients participating in knowledge transfer requires a lower\nparticipation rate, active clients have uneven contributions, and the client\nscale seriously hinders the performance of FL. We consider a more general and\npractical federation scenario and propose a system heterogeneous federation\nmethod based on data-free knowledge distillation and two-way contrast\n(HFedCKD). We apply the Inverse Probability Weighted Distillation (IPWD)\nstrategy to the data-free knowledge transfer framework. The generator completes\nthe data features of the nonparticipating clients. IPWD implements a dynamic\nevaluation of the prediction contribution of each client under different data\ndistributions. Based on the antibiased weighting of its prediction loss, the\nweight distribution of each client is effectively adjusted to fairly integrate\nthe knowledge of participating clients. At the same time, the local model is\nsplit into a feature extractor and a classifier. Through differential contrast\nlearning, the feature extractor is aligned with the global model in the feature\nspace, while the classifier maintains personalized decision-making\ncapabilities. HFedCKD effectively alleviates the knowledge offset caused by a\nlow participation rate under data-free knowledge distillation and improves the\nperformance and stability of the model. We conduct extensive experiments on\nimage and IoT datasets to comprehensively evaluate and verify the\ngeneralization and robustness of the proposed HFedCKD framework.",
      "tldr_zh": "该研究提出了HFedCKD，一种基于数据无关知识蒸馏(Data-free Knowledge Distillation)和双向对比学习的异构联邦学习框架，旨在解决动态联邦学习系统在通信受限、客户端参与率低以及贡献不均衡等问题。通过引入逆概率加权蒸馏(IPWD)策略，动态评估客户端在不同数据分布下的预测贡献，并调整权重分配以公平整合知识。同时，将本地模型拆分为特征提取器和分类器，通过对比学习实现特征空间对齐，同时保留个性化决策能力。实验表明，HFedCKD在图像和物联网数据集上显著提升了模型的性能和稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06511v1",
      "published_date": "2025-03-09 08:32:57 UTC",
      "updated_date": "2025-03-09 08:32:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:32.069375"
    },
    {
      "arxiv_id": "2503.06508v2",
      "title": "LightMotion: A Light and Tuning-free Method for Simulating Camera Motion in Video Generation",
      "title_zh": "LightMotion：一种轻量且无需调优的视频生成中相机运动模拟方法",
      "authors": [
        "Quanjian Song",
        "Zhihang Lin",
        "Zhanpeng Zeng",
        "Ziyue Zhang",
        "Liujuan Cao",
        "Rongrong Ji"
      ],
      "abstract": "Existing camera motion-controlled video generation methods face computational\nbottlenecks in fine-tuning and inference. This paper proposes LightMotion, a\nlight and tuning-free method for simulating camera motion in video generation.\nOperating in the latent space, it eliminates additional fine-tuning,\ninpainting, and depth estimation, making it more streamlined than existing\nmethods. The endeavors of this paper comprise: (i) The latent space permutation\noperation effectively simulates various camera motions like panning, zooming,\nand rotation. (ii) The latent space resampling strategy combines\nbackground-aware sampling and cross-frame alignment to accurately fill new\nperspectives while maintaining coherence across frames. (iii) Our in-depth\nanalysis shows that the permutation and resampling cause an SNR shift in latent\nspace, leading to poor-quality generation. To address this, we propose latent\nspace correction, which reintroduces noise during denoising to mitigate SNR\nshift and enhance video generation quality. Exhaustive experiments show that\nour LightMotion outperforms existing methods, both quantitatively and\nqualitatively.",
      "tldr_zh": "本文提出LightMotion方法，无需调参即可在视频生成中模拟相机运动。该方法直接在潜空间进行置换和重采样操作，有效实现平移、缩放和旋转等多样化镜头运动，同时通过背景感知采样和跨帧对齐保持画面连贯性。研究还发现潜空间操作会导致信噪比偏移问题，进而提出噪声重引入策略来提升生成质量。实验表明，这种轻量化方法在质量和效率上均优于现有技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages in total",
      "pdf_url": "http://arxiv.org/pdf/2503.06508v2",
      "published_date": "2025-03-09 08:28:40 UTC",
      "updated_date": "2025-03-11 02:28:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:30.203753"
    },
    {
      "arxiv_id": "2503.17368v1",
      "title": "Non-Canonical Crosslinks Confound Evolutionary Protein Structure Models",
      "title_zh": "非典型交联干扰进化蛋白质结构模型的预测",
      "authors": [
        "Romain Lacombe"
      ],
      "abstract": "Evolution-based protein structure prediction models have achieved\nbreakthrough success in recent years. However, they struggle to generalize\nbeyond evolutionary priors and on sequences lacking rich homologous data. Here\nwe present a novel, out-of-domain benchmark based on sactipeptides, a rare\nclass of ribosomally synthesized and post-translationally modified peptides\n(RiPPs) characterized by sulfur-to-$\\alpha$-carbon thioether bridges creating\ncross-links between cysteine residues and backbone. We evaluate recent models\non predicting conformations compatible with these cross-links bridges for the\n10 known sactipeptides with elucidated post-translational modifications.\nCrucially, the structures of 5 of them have not yet been experimentally\nresolved. This makes the task a challenging problem for evolution-based models,\nwhich we find exhibit limited performance (0.0% to 19.2% GDT-TS on\nsulfur-to-$\\alpha$-carbon distance). Our results point at the need for\nphysics-informed models to sustain progress in biomolecular structure\nprediction.",
      "tldr_zh": "该研究揭示了基于进化的蛋白质结构预测模型在非经典交联结构（如硫醚桥键）上的局限性。通过构建基于sactipeptides（一类含有半胱氨酸与主链α碳之间硫醚桥键的核糖体合成翻译后修饰肽）的新基准测试，研究发现现有模型对这些特殊结构的预测准确率极低（GDT-TS仅为0.0%-19.2%）。该工作表明，要突破进化先验的限制，未来蛋白质结构预测需要更多结合物理原理的新型建模方法。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17368v1",
      "published_date": "2025-03-09 08:18:11 UTC",
      "updated_date": "2025-03-09 08:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:44.631239"
    },
    {
      "arxiv_id": "2503.06505v1",
      "title": "DynamicID: Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability",
      "title_zh": "DynamicID：具备灵活面部编辑能力的零样本多ID图像个性化方法",
      "authors": [
        "Xirui Hu",
        "Jiahao Wang",
        "Hao Chen",
        "Weizhan Zhang",
        "Benqi Wang",
        "Yikun Li",
        "Haishun Nan"
      ],
      "abstract": "Recent advancements in text-to-image generation have spurred interest in\npersonalized human image generation, which aims to create novel images\nfeaturing specific human identities as reference images indicate. Although\nexisting methods achieve high-fidelity identity preservation, they often\nstruggle with limited multi-ID usability and inadequate facial editability. We\npresent DynamicID, a tuning-free framework supported by a dual-stage training\nparadigm that inherently facilitates both single-ID and multi-ID personalized\ngeneration with high fidelity and flexible facial editability. Our key\ninnovations include: 1) Semantic-Activated Attention (SAA), which employs\nquery-level activation gating to minimize disruption to the original model when\ninjecting ID features and achieve multi-ID personalization without requiring\nmulti-ID samples during training. 2) Identity-Motion Reconfigurator (IMR),\nwhich leverages contrastive learning to effectively disentangle and re-entangle\nfacial motion and identity features, thereby enabling flexible facial editing.\nAdditionally, we have developed a curated VariFace-10k facial dataset,\ncomprising 10k unique individuals, each represented by 35 distinct facial\nimages. Experimental results demonstrate that DynamicID outperforms\nstate-of-the-art methods in identity fidelity, facial editability, and multi-ID\npersonalization capability.",
      "tldr_zh": "本研究提出了DynamicID，一种无需微调的框架，通过双阶段训练范式实现高保真度和灵活面部编辑能力的单ID和多ID个性化图像生成。其核心创新包括：1) 语义激活注意力机制(Semantic-Activated Attention)，通过查询级激活门控在注入ID特征时最小化对原始模型的干扰，无需多ID样本即可实现多ID个性化；2) 身份运动重构器(Identity-Motion Reconfigurator)，利用对比学习有效解耦并重新耦合面部运动和身份特征，从而实现灵活的面部编辑。此外，研究还构建了包含10,000个独特个体的VariFace-10k面部数据集。实验表明，DynamicID在身份保真度、面部编辑能力和多ID个性化方面均优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06505v1",
      "published_date": "2025-03-09 08:16:19 UTC",
      "updated_date": "2025-03-09 08:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:56.570717"
    },
    {
      "arxiv_id": "2503.06499v2",
      "title": "ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis",
      "title_zh": "ExGes：面向音频驱动手势合成的富有表现力的人体动作检索与调制",
      "authors": [
        "Xukun Zhou",
        "Fengxin Li",
        "Ming Chen",
        "Yan Zhou",
        "Pengfei Wan",
        "Di Zhang",
        "Yeying Jin",
        "Zhaoxin Fan",
        "Hongyan Liu",
        "Jun He"
      ],
      "abstract": "Audio-driven human gesture synthesis is a crucial task with broad\napplications in virtual avatars, human-computer interaction, and creative\ncontent generation. Despite notable progress, existing methods often produce\ngestures that are coarse, lack expressiveness, and fail to fully align with\naudio semantics. To address these challenges, we propose ExGes, a novel\nretrieval-enhanced diffusion framework with three key designs: (1) a Motion\nBase Construction, which builds a gesture library using training dataset; (2) a\nMotion Retrieval Module, employing constrative learning and momentum\ndistillation for fine-grained reference poses retreiving; and (3) a Precision\nControl Module, integrating partial masking and stochastic masking to enable\nflexible and fine-grained control. Experimental evaluations on BEAT2\ndemonstrate that ExGes reduces Fr\\'echet Gesture Distance by 6.2\\% and improves\nmotion diversity by 5.3\\% over EMAGE, with user studies revealing a 71.3\\%\npreference for its naturalness and semantic relevance. Code will be released\nupon acceptance.",
      "tldr_zh": "该研究提出了ExGes，一种基于检索增强的扩散框架，用于音频驱动的手势合成，解决了现有方法生成手势粗糙、缺乏表现力且与音频语义不完全对齐的问题。ExGes通过三个关键设计实现：构建手势库的Motion Base Construction、基于对比学习和动量蒸馏的Motion Retrieval Module，以及结合部分掩码和随机掩码的Precision Control Module。实验表明，ExGes在BEAT2数据集上显著提升了手势的自然度和语义相关性，用户研究显示其偏好度高达71.3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06499v2",
      "published_date": "2025-03-09 07:59:39 UTC",
      "updated_date": "2025-03-15 04:31:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:42.486289"
    },
    {
      "arxiv_id": "2503.06497v1",
      "title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving",
      "title_zh": "自动驾驶视觉语言模型安全认知能力评估",
      "authors": [
        "Enming Zhang",
        "Peizhe Gong",
        "Xingyuan Dai",
        "Yisheng Lv",
        "Qinghai Miao"
      ],
      "abstract": "Assessing the safety of vision-language models (VLMs) in autonomous driving\nis particularly important; however, existing work mainly focuses on traditional\nbenchmark evaluations. As interactive components within autonomous driving\nsystems, VLMs must maintain strong safety cognition during interactions. From\nthis perspective, we propose a novel evaluation method: Safety Cognitive\nDriving Benchmark (SCD-Bench) . To address the large-scale annotation challenge\nfor SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System\n(ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset\nundergoes manual refinement by experts with professional knowledge in\nautonomous driving. We further develop an automated evaluation method based on\nlarge language models (LLMs). To verify its effectiveness, we compare its\nevaluation results with those of expert human evaluations, achieving a\nconsistency rate of 99.74%. Preliminary experimental results indicate that\nexisting open-source models still lack sufficient safety cognition, showing a\nsignificant gap compared to GPT-4o. Notably, lightweight models (1B-4B)\ndemonstrate minimal safety cognition. However, since lightweight models are\ncrucial for autonomous driving systems, this presents a significant challenge\nfor integrating VLMs into the field.",
      "tldr_zh": "该研究提出Safety Cognitive Driving Benchmark (SCD-Bench)，用于评估自动驾驶中视觉语言模型(VLMs)的安全认知能力。为解决大规模标注难题，开发了Autonomous Driving Image-Text Annotation System (ADA)系统，并通过专家人工校验确保数据质量。实验显示现有开源模型安全认知能力不足，轻量级模型(1B-4B)表现尤其薄弱，与GPT-4o存在显著差距，这为VLMs在自动驾驶领域的应用带来重大挑战。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06497v1",
      "published_date": "2025-03-09 07:53:19 UTC",
      "updated_date": "2025-03-09 07:53:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:31:50.413570"
    },
    {
      "arxiv_id": "2503.16487v1",
      "title": "PythonPal: Enhancing Online Programming Education through Chatbot-Driven Personalized Feedback",
      "title_zh": "PythonPal：通过聊天机器人驱动的个性化反馈增强在线编程教育",
      "authors": [
        "Sirinda Palahan"
      ],
      "abstract": "The rise of online programming education has necessitated more effective,\npersonalized interactions, a gap that PythonPal aims to fill through its\ninnovative learning system integrated with a chatbot. This research delves into\nPythonPal's potential to enhance the online learning experience, especially in\ncontexts with high student-to-teacher ratios where there is a need for\npersonalized feedback. PythonPal's design, featuring modules for conversation,\ntutorials, and exercises, was evaluated through student interactions and\nfeedback. Key findings reveal PythonPal's proficiency in syntax error\nrecognition and user query comprehension, with its intent classification model\nshowing high accuracy. The system's performance in error feedback, though\nvaried, demonstrates both strengths and areas for enhancement. Student feedback\nindicated satisfactory query understanding and feedback accuracy but also\npointed out the need for faster responses and improved interaction quality.\nPythonPal's deployment promises to significantly enhance online programming\neducation by providing immediate, personalized feedback and interactive\nlearning experiences, fostering a deeper understanding of programming concepts\namong students. These benefits mark a step forward in addressing the challenges\nof distance learning, making programming education more accessible and\neffective.",
      "tldr_zh": "PythonPal是一款基于聊天机器人的在线编程学习系统，旨在通过个性化反馈提升编程教育效果。该系统集成了对话、教程和练习模块，能够有效识别语法错误并理解用户查询，其意图分类模型表现出高准确率。学生反馈表明，PythonPal在查询理解和反馈准确性方面表现良好，但响应速度和交互质量仍需改进。该系统的部署为在线编程教育提供了即时、个性化的反馈和互动学习体验，有助于学生更深入地理解编程概念，推动远程编程教育的可及性和有效性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16487v1",
      "published_date": "2025-03-09 07:28:42 UTC",
      "updated_date": "2025-03-09 07:28:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:32:03.544850"
    },
    {
      "arxiv_id": "2503.06486v1",
      "title": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training",
      "title_zh": "PerturboLLaVA：通过扰动视觉训练减少多模态幻觉现象",
      "authors": [
        "Cong Chen",
        "Mingyu Liu",
        "Chenchen Jing",
        "Yizhou Zhou",
        "Fengyun Rao",
        "Hao Chen",
        "Bo Zhang",
        "Chunhua Shen"
      ],
      "abstract": "This paper aims to address the challenge of hallucinations in Multimodal\nLarge Language Models (MLLMs) particularly for dense image captioning tasks. To\ntackle the challenge, we identify the current lack of a metric that finely\nmeasures the caption quality in concept level. We hereby introduce HalFscore, a\nnovel metric built upon the language graph and is designed to evaluate both the\naccuracy and completeness of dense captions at a granular level. Additionally,\nwe identify the root cause of hallucination as the model's over-reliance on its\nlanguage prior. To address this, we propose PerturboLLaVA, which reduces the\nmodel's reliance on the language prior by incorporating adversarially perturbed\ntext during training. This method enhances the model's focus on visual inputs,\neffectively reducing hallucinations and producing accurate, image-grounded\ndescriptions without incurring additional computational overhead. PerturboLLaVA\nsignificantly improves the fidelity of generated captions, outperforming\nexisting approaches in handling multimodal hallucinations and achieving\nimproved performance across general multimodal benchmarks.",
      "tldr_zh": "该论文提出PerturboLLaVA方法，通过对抗性扰动文本训练来减少多模态大语言模型(MLLMs)在密集图像描述任务中的幻觉问题。研究者首先开发了HalFscore新指标，基于语言图谱从概念层面精细评估描述的准确性和完整性；同时发现幻觉根源在于模型过度依赖语言先验知识。实验表明，该方法在不增加计算开销的情况下有效增强模型对视觉输入的关注，在通用多模态基准测试中优于现有方法，显著提升了生成描述的保真度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06486v1",
      "published_date": "2025-03-09 07:07:03 UTC",
      "updated_date": "2025-03-09 07:07:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:32:18.253084"
    },
    {
      "arxiv_id": "2503.06484v1",
      "title": "Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms",
      "title_zh": "基于帧与事件流的手语翻译：基准数据集与算法",
      "authors": [
        "Xiao Wang",
        "Yuehang Li",
        "Fuling Wang",
        "Bo Jiang",
        "Yaowei Wang",
        "Yonghong Tian",
        "Jin Tang",
        "Bin Luo"
      ],
      "abstract": "Accurate sign language understanding serves as a crucial communication\nchannel for individuals with disabilities. Current sign language translation\nalgorithms predominantly rely on RGB frames, which may be limited by fixed\nframe rates, variable lighting conditions, and motion blur caused by rapid hand\nmovements. Inspired by the recent successful application of event cameras in\nother fields, we propose to leverage event streams to assist RGB cameras in\ncapturing gesture data, addressing the various challenges mentioned above.\nSpecifically, we first collect a large-scale RGB-Event sign language\ntranslation dataset using the DVS346 camera, termed VECSL, which contains\n15,676 RGB-Event samples, 15,191 glosses, and covers 2,568 Chinese characters.\nThese samples were gathered across a diverse range of indoor and outdoor\nenvironments, capturing multiple viewing angles, varying light intensities, and\ndifferent camera motions. Due to the absence of benchmark algorithms for\ncomparison in this new task, we retrained and evaluated multiple\nstate-of-the-art SLT algorithms, and believe that this benchmark can\neffectively support subsequent related research. Additionally, we propose a\nnovel RGB-Event sign language translation framework (i.e., M$^2$-SLT) that\nincorporates fine-grained micro-sign and coarse-grained macro-sign retrieval,\nachieving state-of-the-art results on the proposed dataset. Both the source\ncode and dataset will be released on https://github.com/Event-AHU/OpenESL.",
      "tldr_zh": "该研究提出了一种结合RGB帧和事件流(event stream)的手语翻译新方法，以解决传统RGB视频在固定帧率、光照变化和快速手部运动模糊等方面的局限性。研究团队首先构建了VECSL数据集，包含15,676个RGB-Event样本和2,568个汉字，覆盖多种环境条件；同时提出M²-SLT框架，通过细粒度微符号(micro-sign)和粗粒度宏符号(macro-sign)检索机制，在该数据集上取得了最优性能。这项工作为基于事件相机的手语理解提供了首个基准数据集和算法框架。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2503.06484v1",
      "published_date": "2025-03-09 06:55:46 UTC",
      "updated_date": "2025-03-09 06:55:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:33:04.268308"
    },
    {
      "arxiv_id": "2503.06479v1",
      "title": "ExKG-LLM: Leveraging Large Language Models for Automated Expansion of Cognitive Neuroscience Knowledge Graphs",
      "title_zh": "ExKG-LLM：利用大型语言模型自动化扩展认知神经科学知识图谱",
      "authors": [
        "Ali Sarabadani",
        "Kheirolah Rahsepar Fard",
        "Hamid Dalvand"
      ],
      "abstract": "The paper introduces ExKG-LLM, a framework designed to automate the expansion\nof cognitive neuroscience knowledge graphs (CNKG) using large language models\n(LLMs). It addresses limitations in existing tools by enhancing accuracy,\ncompleteness, and usefulness in CNKG. The framework leverages a large dataset\nof scientific papers and clinical reports, applying state-of-the-art LLMs to\nextract, optimize, and integrate new entities and relationships. Evaluation\nmetrics include precision, recall, and graph density. Results show significant\nimprovements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score\n(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density\nslightly decreased, reflecting a broader but more fragmented structure.\nEngagement rates rose by 20%, while CNKG diameter increased to 15, indicating a\nmore distributed structure. Time complexity improved to O(n log n), but space\ncomplexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates\npotential for enhancing knowledge generation, semantic search, and clinical\ndecision-making in cognitive neuroscience, adaptable to broader scientific\nfields.",
      "tldr_zh": "该研究提出ExKG-LLM框架，利用大语言模型(LLMs)自动扩展认知神经科学知识图谱(CNKG)。该系统通过处理大量科学论文和临床报告数据，采用前沿LLM技术实现新实体和关系的提取与整合，显著提升了知识图谱的准确率(提升6.67%)、召回率(提升15.71%)和F1值(提升11.81%)。实验表明，该框架能生成更全面的知识结构(节点数增加21.13%-31.92%)，同时将时间复杂度优化至O(n log n)，为认知神经科学领域的知识发现和临床决策支持提供了有效工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06479v1",
      "published_date": "2025-03-09 06:32:56 UTC",
      "updated_date": "2025-03-09 06:32:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:33:25.444648"
    },
    {
      "arxiv_id": "2503.06477v1",
      "title": "PDB: Not All Drivers Are the Same -- A Personalized Dataset for Understanding Driving Behavior",
      "title_zh": "PDB：并非所有驾驶员都相同——用于理解驾驶行为的个性化数据集",
      "authors": [
        "Chuheng Wei",
        "Ziye Qin",
        "Siyan Li",
        "Ziyan Zhang",
        "Xuanpeng Zhao",
        "Amr Abdelraouf",
        "Rohit Gupta",
        "Kyungtae Han",
        "Matthew J. Barth",
        "Guoyuan Wu"
      ],
      "abstract": "Driving behavior is inherently personal, influenced by individual habits,\ndecision-making styles, and physiological states. However, most existing\ndatasets treat all drivers as homogeneous, overlooking driver-specific\nvariability. To address this gap, we introduce the Personalized Driving\nBehavior (PDB) dataset, a multi-modal dataset designed to capture\npersonalization in driving behavior under naturalistic driving conditions.\nUnlike conventional datasets, PDB minimizes external influences by maintaining\nconsistent routes, vehicles, and lighting conditions across sessions. It\nincludes sources from 128-line LiDAR, front-facing camera video, GNSS, 9-axis\nIMU, CAN bus data (throttle, brake, steering angle), and driver-specific\nsignals such as facial video and heart rate. The dataset features 12\nparticipants, approximately 270,000 LiDAR frames, 1.6 million images, and 6.6\nTB of raw sensor data. The processed trajectory dataset consists of 1,669\nsegments, each spanning 10 seconds with a 0.2-second interval. By explicitly\ncapturing drivers' behavior, PDB serves as a unique resource for human factor\nanalysis, driver identification, and personalized mobility applications,\ncontributing to the development of human-centric intelligent transportation\nsystems.",
      "tldr_zh": "该研究提出了个性化驾驶行为数据集（PDB），旨在解决现有数据集忽视驾驶员个体差异的问题。该数据集通过多模态传感器（包括LiDAR、摄像头、GNSS、IMU、CAN总线及驾驶员生理信号）采集12名参与者在相同路线、车辆和光照条件下的自然驾驶数据，共包含27万帧LiDAR、160万张图像和6.6TB原始数据。PDB特别强调记录驾驶员个性化行为特征（如决策习惯和生理状态），为驾驶员识别、人因分析和个性化智能交通系统研究提供了独特资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06477v1",
      "published_date": "2025-03-09 06:28:39 UTC",
      "updated_date": "2025-03-09 06:28:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:33:26.586816"
    },
    {
      "arxiv_id": "2503.16486v1",
      "title": "Accodemy: AI Powered Code Learning Platform to Assist Novice Programmers in Overcoming the Fear of Coding",
      "title_zh": "Accodemy：AI驱动的代码学习平台，助力编程新手克服编码恐惧",
      "authors": [
        "M. A. F. Aamina",
        "V. Kavishcan",
        "W. M. P. B. B. Jayaratne",
        "K. K. D. S. N. Kannangara",
        "A. A. Aamil",
        "Achini Adikari"
      ],
      "abstract": "Computer programming represents a rapidly evolving and sought-after career\npath in the 21st century. Nevertheless, novice learners may find the process\nintimidating for several reasons, such as limited and highly competitive career\nopportunities, peer and parental pressure for academic success, and course\ndifficulties. These factors frequently contribute to anxiety and eventual\ndropout as a result of fear. Furthermore, research has demonstrated that\nbeginners are significantly deterred by the fear of failure, which results in\nprogramming anxiety and and a sense of being overwhelmed by intricate topics,\nultimately leading to dropping out. This project undertakes an exploration\nbeyond the scope of conventional code learning platforms by identifying and\nutilising effective and personalised strategies of learning. The proposed\nsolution incorporates features such as AI-generated challenging questions,\nmindfulness quotes, and tips to motivate users, along with an AI chatbot that\nfunctions as a motivational aid. In addition, the suggested solution integrates\npersonalized roadmaps and gamification elements to maintain user involvement.\nThe project aims to systematically monitor the progress of novice programmers\nand enhance their knowledge of coding with a personalised, revised curriculum\nto help mitigate the fear of coding and boost confidence.",
      "tldr_zh": "该研究提出Accodemy——一个AI驱动的代码学习平台，旨在帮助编程新手克服\"编程恐惧症\"。平台采用个性化学习策略，整合AI生成挑战题、正念名言、激励提示和情感支持聊天机器人等功能，同时结合个性化学习路径和游戏化元素保持用户参与度。系统通过监测学习者进度并动态调整课程内容，有效缓解编程焦虑，提升学习信心，降低新手辍学率。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16486v1",
      "published_date": "2025-03-09 06:28:06 UTC",
      "updated_date": "2025-03-09 06:28:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:33:41.953771"
    },
    {
      "arxiv_id": "2503.06475v1",
      "title": "SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction Using Large Language Models",
      "title_zh": "SKG-LLM：基于大型语言模型的卒中知识图谱构建数学模型开发",
      "authors": [
        "Ali Sarabadani",
        "Kheirolah Rahsepar Fard",
        "Hamid Dalvand"
      ],
      "abstract": "The purpose of this study is to introduce SKG-LLM. A knowledge graph (KG) is\nconstructed from stroke-related articles using mathematical and large language\nmodels (LLMs). SKG-LLM extracts and organizes complex relationships from the\nbiomedical literature, using it to increase the accuracy and depth of KG in\nstroke research. In the proposed method, GPT-4 was used for data\npre-processing, and the extraction of embeddings was also done by GPT-4 in the\nwhole KG construction process. The performance of the proposed model was tested\nwith two evaluation criteria: Precision and Recall. For further validation of\nthe proposed model, GPT-4 was used. Compared with Wikidata and WN18RR, the\nproposed KG-LLM approach performs better, especially in precision and recall.\nBy including GPT-4 in the preprocessing process, the SKG-LLM model achieved a\nprecision score of 0.906 and a recall score of 0.923. Expert reviews further\nimproved the results and increased precision to 0.923 and recall to 0.918. The\nknowledge graph constructed by SKG-LLM contains 2692 nodes and 5012 edges,\nwhich are 13 distinct types of nodes and 24 types of edges.",
      "tldr_zh": "该研究提出了SKG-LLM模型，利用大型语言模型（GPT-4）构建中风领域的知识图谱（KG）。该方法通过GPT-4进行数据预处理和嵌入提取，在精确度（0.923）和召回率（0.918）上显著优于Wikidata和WN18RR基准。最终构建的知识图谱包含2692个节点和5012条边，涵盖13种节点类型和24种边类型，有效提升了中风研究的深度和准确性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06475v1",
      "published_date": "2025-03-09 06:25:37 UTC",
      "updated_date": "2025-03-09 06:25:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:33:47.410142"
    },
    {
      "arxiv_id": "2503.06474v1",
      "title": "HuixiangDou2: A Robustly Optimized GraphRAG Approach",
      "title_zh": "HuixiangDou2：一种经过稳健优化的GraphRAG方法",
      "authors": [
        "Huanjun Kong",
        "Zhefan Wang",
        "Chenyang Wang",
        "Zhe Ma",
        "Nanqing Dong"
      ],
      "abstract": "Large Language Models (LLMs) perform well on familiar queries but struggle\nwith specialized or emerging topics. Graph-based Retrieval-Augmented Generation\n(GraphRAG) addresses this by structuring domain knowledge as a graph for\ndynamic retrieval. However, existing pipelines involve complex engineering\nworkflows, making it difficult to isolate the impact of individual components.\nEvaluating retrieval effectiveness is also challenging due to dataset overlap\nwith LLM pretraining data. In this work, we introduce HuixiangDou2, a robustly\noptimized GraphRAG framework. Specifically, we leverage the effectiveness of\ndual-level retrieval and optimize its performance in a 32k context for maximum\nprecision, and compare logic-based retrieval and dual-level retrieval to\nenhance overall functionality. Our implementation includes comparative\nexperiments on a test set, where Qwen2.5-7B-Instruct initially underperformed.\nWith our approach, the score improved significantly from 60 to 74.5, as\nillustrated in the Figure. Experiments on domain-specific datasets reveal that\ndual-level retrieval enhances fuzzy matching, while logic-form retrieval\nimproves structured reasoning. Furthermore, we propose a multi-stage\nverification mechanism to improve retrieval robustness without increasing\ncomputational cost. Empirical results show significant accuracy gains over\nbaselines, highlighting the importance of adaptive retrieval. To support\nresearch and adoption, we release HuixiangDou2 as an open-source resource\nhttps://github.com/tpoisonooo/huixiangdou2.",
      "tldr_zh": "该研究提出了HuixiangDou2，一种针对GraphRAG（基于图的检索增强生成）框架的优化方法，旨在解决大语言模型（LLMs）在专业或新兴领域查询中的表现不足问题。通过引入双级检索机制，并结合逻辑检索与双级检索的比较，显著提升了检索精度与功能。实验表明，该方法在特定领域数据集上有效增强了模糊匹配和结构化推理能力，并通过多阶段验证机制提高了检索的鲁棒性。最终，Qwen2.5-7B-Instruct模型的得分从60提升至74.5，展现了自适应检索的重要性。研究团队将HuixiangDou2开源，以支持进一步研究和应用。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06474v1",
      "published_date": "2025-03-09 06:20:24 UTC",
      "updated_date": "2025-03-09 06:20:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:33:57.633035"
    },
    {
      "arxiv_id": "2503.06473v3",
      "title": "Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals",
      "title_zh": "通过剪枝冗余检索提升层注意力效率",
      "authors": [
        "Hanze Li",
        "Xiande Huang"
      ],
      "abstract": "Growing evidence suggests that layer attention mechanisms, which enhance\ninteraction among layers in deep neural networks, have significantly advanced\nnetwork architectures. However, existing layer attention methods suffer from\nredundancy, as attention weights learned by adjacent layers often become highly\nsimilar. This redundancy causes multiple layers to extract nearly identical\nfeatures, reducing the model's representational capacity and increasing\ntraining time. To address this issue, we propose a novel approach to quantify\nredundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent\nlayers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM)\nmethod that accurately identifies and skips redundant layers, thereby\nmaintaining model stability. Our proposed Efficient Layer Attention (ELA)\narchitecture, improves both training efficiency and overall performance,\nachieving a 30\\% reduction in training time while enhancing performance in\ntasks such as image classification and object detection.",
      "tldr_zh": "该研究提出了一种高效的层注意力机制ELA，通过量化相邻层间的冗余性来优化深度神经网络。研究者利用KL散度度量层间相似度，并开发了增强型Beta分位数映射(EBQM)方法来自动跳过冗余层计算。实验表明，该架构在保持模型稳定性的同时，将训练时间缩短30%，并在图像分类和目标检测任务中提升了性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06473v3",
      "published_date": "2025-03-09 06:20:11 UTC",
      "updated_date": "2025-03-22 12:05:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:01.483856"
    },
    {
      "arxiv_id": "2503.10663v1",
      "title": "Optimal Transport for Brain-Image Alignment: Unveiling Redundancy and Synergy in Neural Information Processing",
      "title_zh": "基于最优传输的大脑图像对齐：揭示神经信息处理中的冗余与协同",
      "authors": [
        "Yang Xiao",
        "Wang Lu",
        "Jie Ji",
        "Ruimeng Ye",
        "Gen Li",
        "Xiaolong Ma",
        "Bo Hui"
      ],
      "abstract": "The design of artificial neural networks (ANNs) is inspired by the structure\nof the human brain, and in turn, ANNs offer a potential means to interpret and\nunderstand brain signals. Existing methods primarily align brain signals with\nreal-world signals using Mean Squared Error (MSE), which solely focuses on\nlocal point-wise alignment, and ignores global matching, leading to coarse\ninterpretations and inaccuracies in brain signal decoding.\n  In this paper, we address these issues through optimal transport (OT) and\ntheoretically demonstrate why OT provides a more effective alignment strategy\nthan MSE. Specifically, we construct a transport plan between brain voxel\nembeddings and image embeddings, enabling more precise matching. By controlling\nthe amount of transport, we mitigate the influence of redundant information. We\napply our alignment model directly to the Brain Captioning task by feeding\nbrain siginals into a large language model (LLM) instead of images. Our\napproach achieves state-of-the-art performance across ten evaluation metrics,\nsurpassing the previous best method by an average of 6.11\\% in single-subject\ntraining and 3.81\\% in cross-subject training. Additionally, we have uncovered\nseveral insightful conclusions that align with existing brain research. We\nunveil the redundancy and synergy of brain information processing through\nregion masking and data dimensionality reduction visualization experiments. We\nbelieve our approach paves the way for a more precise understanding of brain\nsignals in the future. The code is available soon.",
      "tldr_zh": "本研究提出了一种基于最优传输（Optimal Transport, OT）的脑信号与图像对齐方法，解决了传统均方误差（MSE）仅关注局部对齐而忽略全局匹配的问题。通过构建脑体素嵌入与图像嵌入之间的传输计划，该方法有效减少了冗余信息的影响，并在脑图描述任务（Brain Captioning）中取得了显著性能提升，单主体和跨主体训练分别比现有最佳方法平均提高了6.11%和3.81%。此外，实验揭示了脑信息处理中的冗余性和协同性，为未来更精确的脑信号理解提供了新思路。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "14pages",
      "pdf_url": "http://arxiv.org/pdf/2503.10663v1",
      "published_date": "2025-03-09 06:14:23 UTC",
      "updated_date": "2025-03-09 06:14:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:10.426945"
    },
    {
      "arxiv_id": "2503.06470v1",
      "title": "Think Twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems",
      "title_zh": "三思而后行：通过快慢系统增强图形用户界面定位",
      "authors": [
        "Fei Tang",
        "Yongliang Shen",
        "Hang Zhang",
        "Siqi Chen",
        "Guiyang Hou",
        "Wenqi Zhang",
        "Wenqiao Zhang",
        "Kaitao Song",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "abstract": "Humans can flexibly switch between different modes of thinking based on task\ncomplexity: from rapid intuitive judgments to in-depth analytical\nunderstanding. However, current Graphical User Interface (GUI) grounding\nsystems which locate interface elements based on natural language instructions\nrely solely on immediate prediction without reasoning, struggling to understand\ncomplex interface layouts with nested structures and hierarchical\nrelationships, limiting their effectiveness on complex interfaces. Inspired by\nhuman dual-system cognition, we present Focus, a novel GUI grounding framework\nthat combines fast prediction with systematic analysis. The framework\ndynamically switches between rapid and deliberate processing through an\nadaptive system switching based on task complexity, optimizing both efficiency\nand accuracy. Focus decomposes grounding into progressive stages: interface\nsummarization, visual focused analysis, and precise coordinate prediction. This\nstructured decomposition enables systematic understanding of both interface\nlayouts and visual relationships. Extensive experiments show that Focus\nachieves state-of-the-art performance using only 300K of the training data with\na 2B parameter model compared to existing approaches. Focus demonstrates\nsuperior performance particularly in complex GUI scenarios, achieving 77.4%\naverage accuracy on ScreenSpot and 13.3% on the more challenging\nScreenSpot-Pro. Our analysis reveals the effectiveness of this dual-system\napproach while demonstrating its potential for improving complex GUI\ninteraction scenarios.",
      "tldr_zh": "这篇论文提出了Focus框架，受人类双系统认知启发，将快速直觉判断和深度分析理解相结合，用于提升图形用户界面(GUI)的语义定位任务。该框架通过自适应系统切换机制，根据任务复杂度动态选择快速预测或渐进式分析，将定位过程分解为界面摘要、视觉聚焦分析和精确坐标预测三个阶段。实验表明，Focus仅用300K训练数据和2B参数模型就达到最先进性能，在复杂GUI场景下表现尤为突出，在ScreenSpot和ScreenSpot-Pro数据集上分别达到77.4%和13.3%的平均准确率，验证了双系统方法在改善复杂界面交互方面的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06470v1",
      "published_date": "2025-03-09 06:14:17 UTC",
      "updated_date": "2025-03-09 06:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:25.000040"
    },
    {
      "arxiv_id": "2503.08704v1",
      "title": "Life-Cycle Routing Vulnerabilities of LLM Router",
      "title_zh": "LLM路由器的全生命周期路由漏洞研究",
      "authors": [
        "Qiqi Lin",
        "Xiaoyang Ji",
        "Shengfang Zhai",
        "Qingni Shen",
        "Zhi Zhang",
        "Yuejian Fang",
        "Yansong Gao"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing, yet their performance and computational costs vary\nsignificantly. LLM routers play a crucial role in dynamically balancing these\ntrade-offs. While previous studies have primarily focused on routing\nefficiency, security vulnerabilities throughout the entire LLM router life\ncycle, from training to inference, remain largely unexplored. In this paper, we\npresent a comprehensive investigation into the life-cycle routing\nvulnerabilities of LLM routers. We evaluate both white-box and black-box\nadversarial robustness, as well as backdoor robustness, across several\nrepresentative routing models under extensive experimental settings. Our\nexperiments uncover several key findings: 1) Mainstream DNN-based routers tend\nto exhibit the weakest adversarial and backdoor robustness, largely due to\ntheir strong feature extraction capabilities that amplify vulnerabilities\nduring both training and inference; 2) Training-free routers demonstrate the\nstrongest robustness across different attack types, benefiting from the absence\nof learnable parameters that can be manipulated. These findings highlight\ncritical security risks spanning the entire life cycle of LLM routers and\nprovide insights for developing more robust models.",
      "tldr_zh": "本研究全面探讨了LLM路由器在训练到推理全生命周期中的路由漏洞问题。通过评估多种代表性路由模型在白盒、黑盒对抗攻击以及后门攻击下的鲁棒性，研究发现：1) 基于DNN的主流路由器由于强大的特征提取能力，在训练和推理阶段均表现出最弱的对抗和后门鲁棒性；2) 无训练路由器由于缺乏可被操纵的可学习参数，在不同攻击类型下展现出最强的鲁棒性。这些发现揭示了LLM路由器全生命周期的关键安全风险，为开发更鲁棒的模型提供了重要见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.08704v1",
      "published_date": "2025-03-09 06:00:35 UTC",
      "updated_date": "2025-03-09 06:00:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:25.048691"
    },
    {
      "arxiv_id": "2503.06462v1",
      "title": "StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting",
      "title_zh": "StructGS：自适应球谐函数与渲染增强技术实现卓越的3D高斯泼溅效果",
      "authors": [
        "Zexu Huang",
        "Min Xu",
        "Stuart Perry"
      ],
      "abstract": "Recent advancements in 3D reconstruction coupled with neural rendering\ntechniques have greatly improved the creation of photo-realistic 3D scenes,\ninfluencing both academic research and industry applications. The technique of\n3D Gaussian Splatting and its variants incorporate the strengths of both\nprimitive-based and volumetric representations, achieving superior rendering\nquality. While 3D Geometric Scattering (3DGS) and its variants have advanced\nthe field of 3D representation, they fall short in capturing the stochastic\nproperties of non-local structural information during the training process.\nAdditionally, the initialisation of spherical functions in 3DGS-based methods\noften fails to engage higher-order terms in early training rounds, leading to\nunnecessary computational overhead as training progresses. Furthermore, current\n3DGS-based approaches require training on higher resolution images to render\nhigher resolution outputs, significantly increasing memory demands and\nprolonging training durations. We introduce StructGS, a framework that enhances\n3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D\nreconstruction. StructGS innovatively incorporates a patch-based SSIM loss,\ndynamic spherical harmonics initialisation and a Multi-scale Residual Network\n(MSRN) to address the above-mentioned limitations, respectively. Our framework\nsignificantly reduces computational redundancy, enhances detail capture and\nsupports high-resolution rendering from low-resolution inputs. Experimentally,\nStructGS demonstrates superior performance over state-of-the-art (SOTA) models,\nachieving higher quality and more detailed renderings with fewer artifacts.",
      "tldr_zh": "该论文提出StructGS框架，通过三项创新改进3D高斯溅射(3DGS)技术：1) 采用基于patch的SSIM损失函数；2) 动态球谐函数初始化方法；3) 多尺度残差网络(MSRN)。这些改进有效解决了现有3DGS方法在非局部结构信息捕获、高阶球谐项训练效率和高分辨率渲染方面的瓶颈问题。实验表明，StructGS能以低分辨率输入实现高保真渲染，在减少计算冗余的同时显著提升渲染质量和细节表现，性能优于当前最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06462v1",
      "published_date": "2025-03-09 05:39:44 UTC",
      "updated_date": "2025-03-09 05:39:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:30.599339"
    },
    {
      "arxiv_id": "2503.06457v1",
      "title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning",
      "title_zh": "几何知识引导的局部全局分布对齐联邦学习方法",
      "authors": [
        "Yanbiao Ma",
        "Wei Dai",
        "Wenke Huang",
        "Jiayi Chen"
      ],
      "abstract": "Data heterogeneity in federated learning, characterized by a significant\nmisalignment between local and global distributions, leads to divergent local\noptimization directions and hinders global model training. Existing studies\nmainly focus on optimizing local updates or global aggregation, but these\nindirect approaches demonstrate instability when handling highly heterogeneous\ndata distributions, especially in scenarios where label skew and domain skew\ncoexist. To address this, we propose a geometry-guided data generation method\nthat centers on simulating the global embedding distribution locally. We first\nintroduce the concept of the geometric shape of an embedding distribution and\nthen address the challenge of obtaining global geometric shapes under privacy\nconstraints. Subsequently, we propose GGEUR, which leverages global geometric\nshapes to guide the generation of new samples, enabling a closer approximation\nto the ideal global distribution. In single-domain scenarios, we augment\nsamples based on global geometric shapes to enhance model generalization; in\nmulti-domain scenarios, we further employ class prototypes to simulate the\nglobal distribution across domains. Extensive experimental results demonstrate\nthat our method significantly enhances the performance of existing approaches\nin handling highly heterogeneous data, including scenarios with label skew,\ndomain skew, and their coexistence. Code published at:\nhttps://github.com/WeiDai-David/2025CVPR_GGEUR",
      "tldr_zh": "该研究提出了一种几何知识引导的局部全局分布对齐方法（GGEUR），用于解决联邦学习中的数据异质性问题。通过引入嵌入分布的几何形状概念，并在隐私约束下获取全局几何形状，该方法生成了新的样本以更接近理想的全局分布。在单域场景中，基于全局几何形状增强样本以提升模型泛化能力；在多域场景中，利用类别原型模拟跨域全局分布。实验表明，该方法显著提升了现有方法在处理高度异质数据（包括标签偏斜、域偏斜及其共存场景）时的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06457v1",
      "published_date": "2025-03-09 05:30:28 UTC",
      "updated_date": "2025-03-09 05:30:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:52.924216"
    },
    {
      "arxiv_id": "2503.06444v1",
      "title": "CtrTab: Tabular Data Synthesis with High-Dimensional and Limited Data",
      "title_zh": "CtrTab：高维有限数据下的表格数据合成方法",
      "authors": [
        "Zuqing Li",
        "Jianzhong Qi",
        "Junhao Gan"
      ],
      "abstract": "Diffusion-based tabular data synthesis models have yielded promising results.\nHowever, we observe that when the data dimensionality increases, existing\nmodels tend to degenerate and may perform even worse than simpler,\nnon-diffusion-based models. This is because limited training samples in\nhigh-dimensional space often hinder generative models from capturing the\ndistribution accurately. To address this issue, we propose CtrTab-a condition\ncontrolled diffusion model for tabular data synthesis-to improve the\nperformance of diffusion-based generative models in high-dimensional, low-data\nscenarios. Through CtrTab, we inject samples with added Laplace noise as\ncontrol signals to improve data diversity and show its resemblance to L2\nregularization, which enhances model robustness. Experimental results across\nmultiple datasets show that CtrTab outperforms state-of-the-art models, with\nperformance gap in accuracy over 80% on average. Our source code will be\nreleased upon paper publication.",
      "tldr_zh": "该研究提出了CtrTab，一种针对高维小样本表格数据合成的条件控制扩散模型。现有扩散模型在高维数据下性能下降明显，而CtrTab通过注入拉普拉斯噪声作为控制信号提升数据多样性，其机制类似于L2正则化增强模型鲁棒性。实验表明，CtrTab在多个数据集上超越现有最优模型，平均准确率提升超过80%，有效解决了高维稀疏数据下的分布捕获难题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06444v1",
      "published_date": "2025-03-09 05:01:56 UTC",
      "updated_date": "2025-03-09 05:01:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:34:52.055335"
    },
    {
      "arxiv_id": "2503.06436v1",
      "title": "Physics-Informed Residual Neural Ordinary Differential Equations for Enhanced Tropical Cyclone Intensity Forecasting",
      "title_zh": "基于物理信息的残差神经常微分方程增强热带气旋强度预报",
      "authors": [
        "Fan Meng"
      ],
      "abstract": "Accurate tropical cyclone (TC) intensity prediction is crucial for mitigating\nstorm hazards, yet its complex dynamics pose challenges to traditional methods.\nHere, we introduce a Physics-Informed Residual Neural Ordinary Differential\nEquation (PIR-NODE) model to precisely forecast TC intensity evolution. This\nmodel leverages the powerful non-linear fitting capabilities of deep learning,\nintegrates residual connections to enhance model depth and training stability,\nand explicitly models the continuous temporal evolution of TC intensity using\nNeural ODEs. Experimental results in the SHIPS dataset demonstrate that the\nPIR-NODE model achieves a significant improvement in 24-hour intensity\nprediction accuracy compared to traditional statistical models and benchmark\ndeep learning methods, with a 25. 2\\% reduction in the root mean square error\n(RMSE) and a 19.5\\% increase in R-square (R2) relative to a baseline of neural\nnetwork. Crucially, the residual structure effectively preserves initial state\ninformation, and the model exhibits robust generalization capabilities. This\nstudy details the PIR-NODE model architecture, physics-informed integration\nstrategies, and comprehensive experimental validation, revealing the\nsubstantial potential of deep learning techniques in predicting complex\ngeophysical systems and laying the foundation for future refined TC forecasting\nresearch.",
      "tldr_zh": "该研究提出了一种物理信息残差神经常微分方程模型（PIR-NODE），用于提升热带气旋（TC）强度预测精度。该模型结合深度学习的非线性拟合能力、残差连接结构以及神经ODE的连续时间建模优势，在SHIPS数据集上实现了显著改进：相比传统方法和基准深度学习模型，24小时强度预测的均方根误差降低25.2%，R平方值提升19.5%。研究特别指出，残差结构有效保持了初始状态信息，且模型展现出强大的泛化能力，为复杂地球物理系统预测提供了新范式。",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "14 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06436v1",
      "published_date": "2025-03-09 04:23:07 UTC",
      "updated_date": "2025-03-09 04:23:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:20.247019"
    },
    {
      "arxiv_id": "2503.06433v1",
      "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
      "title_zh": "Seesaw：基于模型重分片的高吞吐量大语言模型推理系统",
      "authors": [
        "Qidong Su",
        "Wei Zhao",
        "Xin Li",
        "Muralidhar Andoorveedu",
        "Chenhao Jiang",
        "Zhanda Zhu",
        "Kevin Song",
        "Christina Giannoula",
        "Gennady Pekhimenko"
      ],
      "abstract": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
      "tldr_zh": "该研究提出了Seesaw，一种面向高吞吐量任务的大语言模型(LLM)动态推理引擎。其核心创新是模型重分片(model re-sharding)技术，通过在不同推理阶段(预填充和解码)动态调整并行化策略，显著提升整体吞吐量。系统采用分层KV缓存缓冲和转移最小化调度技术，有效降低了阶段转换开销，相比主流推理引擎vLLM实现了最高1.78倍(平均1.36倍)的吞吐量提升。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06433v1",
      "published_date": "2025-03-09 04:14:06 UTC",
      "updated_date": "2025-03-09 04:14:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:06.568609"
    },
    {
      "arxiv_id": "2503.06430v1",
      "title": "Graph Retrieval-Augmented LLM for Conversational Recommendation Systems",
      "title_zh": "图检索增强型大语言模型在对话推荐系统中的应用",
      "authors": [
        "Zhangchi Qiu",
        "Linhao Luo",
        "Zicheng Zhao",
        "Shirui Pan",
        "Alan Wee-Chung Liew"
      ],
      "abstract": "Conversational Recommender Systems (CRSs) have emerged as a transformative\nparadigm for offering personalized recommendations through natural language\ndialogue. However, they face challenges with knowledge sparsity, as users often\nprovide brief, incomplete preference statements. While recent methods have\nintegrated external knowledge sources to mitigate this, they still struggle\nwith semantic understanding and complex preference reasoning. Recent Large\nLanguage Models (LLMs) demonstrate promising capabilities in natural language\nunderstanding and reasoning, showing significant potential for CRSs.\nNevertheless, due to the lack of domain knowledge, existing LLM-based CRSs\neither produce hallucinated recommendations or demand expensive domain-specific\ntraining, which largely limits their applicability. In this work, we present\nG-CRS (Graph Retrieval-Augmented Large Language Model for Conversational\nRecommender Systems), a novel training-free framework that combines graph\nretrieval-augmented generation and in-context learning to enhance LLMs'\nrecommendation capabilities. Specifically, G-CRS employs a two-stage\nretrieve-and-recommend architecture, where a GNN-based graph reasoner first\nidentifies candidate items, followed by Personalized PageRank exploration to\njointly discover potential items and similar user interactions. These retrieved\ncontexts are then transformed into structured prompts for LLM reasoning,\nenabling contextually grounded recommendations without task-specific training.\nExtensive experiments on two public datasets show that G-CRS achieves superior\nrecommendation performance compared to existing methods without requiring\ntask-specific training.",
      "tldr_zh": "该研究提出了G-CRS，一种基于图检索增强生成(Graph Retrieval-Augmented Generation)和上下文学习(In-Context Learning)的训练免费框架，用于提升大语言模型(LLMs)在对话推荐系统(CRSs)中的能力。G-CRS采用两阶段架构：首先通过图神经网络(GNN)推理器识别候选物品，然后利用个性化PageRank算法探索潜在物品和相似用户交互，最终将检索到的上下文转化为结构化提示，支持LLM进行上下文感知的推荐。实验表明，G-CRS在两个公开数据集上优于现有方法，且无需任务特定训练。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by PAKDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06430v1",
      "published_date": "2025-03-09 03:56:22 UTC",
      "updated_date": "2025-03-09 03:56:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:25.945322"
    },
    {
      "arxiv_id": "2503.06427v1",
      "title": "Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive Learning",
      "title_zh": "《面向视觉生成溯因学习的元规则选择策略预训练方法》",
      "authors": [
        "Yu Jin",
        "Jingming Liu",
        "Zhexu Luo",
        "Yifei Peng",
        "Ziang Qin",
        "Wang-Zhou Dai",
        "Yao-Xiang Ding",
        "Kun Zhou"
      ],
      "abstract": "Visual generative abductive learning studies jointly training symbol-grounded\nneural visual generator and inducing logic rules from data, such that after\nlearning, the visual generation process is guided by the induced logic rules. A\nmajor challenge for this task is to reduce the time cost of logic abduction\nduring learning, an essential step when the logic symbol set is large and the\nlogic rule to induce is complicated. To address this challenge, we propose a\npre-training method for obtaining meta-rule selection policy for the recently\nproposed visual generative learning approach AbdGen [Peng et al., 2023], aiming\nat significantly reducing the candidate meta-rule set and pruning the search\nspace. The selection model is built based on the embedding representation of\nboth symbol grounding of cases and meta-rules, which can be effectively\nintegrated with both neural model and logic reasoning system. The pre-training\nprocess is done on pure symbol data, not involving symbol grounding learning of\nraw visual inputs, making the entire learning process low-cost. An additional\ninteresting observation is that the selection policy can rectify symbol\ngrounding errors unseen during pre-training, which is resulted from the\nmemorization ability of attention mechanism and the relative stability of\nsymbolic patterns. Experimental results show that our method is able to\neffectively address the meta-rule selection problem for visual abduction,\nboosting the efficiency of visual generative abductive learning. Code is\navailable at https://github.com/future-item/metarule-select.",
      "tldr_zh": "该研究提出了一种预训练元规则选择策略，用于视觉生成溯因学习(Visual Generative Abductive Learning)，旨在显著减少逻辑溯因的时间成本。通过构建基于符号案例和元规则嵌入表示的选择模型，并将其与神经网络和逻辑推理系统有效整合，该方法在纯符号数据上进行预训练，避免了原始视觉输入的符号接地学习，从而降低了学习成本。实验表明，该策略能够有效解决视觉溯因中的元规则选择问题，提升视觉生成溯因学习的效率，并具备纠正预训练期间未见符号接地错误的能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at IJCLR'24",
      "pdf_url": "http://arxiv.org/pdf/2503.06427v1",
      "published_date": "2025-03-09 03:41:11 UTC",
      "updated_date": "2025-03-09 03:41:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:27.702839"
    },
    {
      "arxiv_id": "2503.06422v1",
      "title": "GenAI for Simulation Model in Model-Based Systems Engineering",
      "title_zh": "基于生成式人工智能的模型驱动系统工程仿真模型",
      "authors": [
        "Lin Zhang",
        "Yuteng Zhang",
        "Dusit Niyato",
        "Lei Ren",
        "Pengfei Gu",
        "Zhen Chen",
        "Yuanjun Laili",
        "Wentong Cai",
        "Agostino Bruzzone"
      ],
      "abstract": "Generative AI (GenAI) has demonstrated remarkable capabilities in code\ngeneration, and its integration into complex product modeling and simulation\ncode generation can significantly enhance the efficiency of the system design\nphase in Model-Based Systems Engineering (MBSE). In this study, we introduce a\ngenerative system design methodology framework for MBSE, offering a practical\napproach for the intelligent generation of simulation models for system\nphysical properties. First, we employ inference techniques, generative models,\nand integrated modeling and simulation languages to construct simulation models\nfor system physical properties based on product design documents. Subsequently,\nwe fine-tune the language model used for simulation model generation on an\nexisting library of simulation models and additional datasets generated through\ngenerative modeling. Finally, we introduce evaluation metrics for the generated\nsimulation models for system physical properties. Our proposed approach to\nsimulation model generation presents the innovative concept of scalable\ntemplates for simulation models. Using these templates, GenAI generates\nsimulation models for system physical properties through code completion. The\nexperimental results demonstrate that, for mainstream open-source\nTransformer-based models, the quality of the simulation model is significantly\nimproved using the simulation model generation method proposed in this paper.",
      "tldr_zh": "本研究提出了一种基于生成式人工智能(GenAI)的系统设计方法框架，用于提升基于模型的系统工程(MBSE)中系统物理属性仿真模型的生成效率。该方法通过推理技术、生成模型和集成建模与仿真语言，从产品设计文档构建仿真模型，并在现有仿真模型库和生成数据集上微调语言模型。实验结果表明，采用该框架后，主流开源Transformer模型的仿真模型质量显著提高，为MBSE中的仿真模型生成提供了可扩展的模板化解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2503.06422v1",
      "published_date": "2025-03-09 03:33:25 UTC",
      "updated_date": "2025-03-09 03:33:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:45.819786"
    },
    {
      "arxiv_id": "2503.06420v2",
      "title": "Explaining Control Policies through Predicate Decision Diagrams",
      "title_zh": "基于谓词决策图的控制策略解释方法",
      "authors": [
        "Debraj Chakraborty",
        "Clemens Dubslaff",
        "Sudeep Kanav",
        "Jan Kretinsky",
        "Christoph Weinhuber"
      ],
      "abstract": "Safety-critical controllers of complex systems are hard to construct\nmanually. Automated approaches such as controller synthesis or learning provide\na tempting alternative but usually lack explainability. To this end, learning\ndecision trees (DTs) have been prevalently used towards an interpretable model\nof the generated controllers. However, DTs do not exploit shared\ndecision-making, a key concept exploited in binary decision diagrams (BDDs) to\nreduce their size and thus improve explainability. In this work, we introduce\npredicate decision diagrams (PDDs) that extend BDDs with predicates and thus\nunite the advantages of DTs and BDDs for controller representation. We\nestablish a synthesis pipeline for efficient construction of PDDs from DTs\nrepresenting controllers, exploiting reduction techniques for BDDs also for\nPDDs.",
      "tldr_zh": "该研究提出了谓词决策图(PDDs)，通过将二元决策图(BDDs)扩展为包含谓词的形式，结合了决策树(DTs)的可解释性和BDDs利用共享决策机制的优势。研究者开发了一个从控制器DTs高效构建PDDs的合成流程，并针对PDDs应用了BDDs的简化技术，从而在保持控制器可解释性的同时显著提升了表达效率。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of the HSCC 2025 paper",
      "pdf_url": "http://arxiv.org/pdf/2503.06420v2",
      "published_date": "2025-03-09 03:31:48 UTC",
      "updated_date": "2025-03-25 16:57:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:38.391184"
    },
    {
      "arxiv_id": "2503.06416v1",
      "title": "Advancing AI Negotiations: New Theory and Evidence from a Large-Scale Autonomous Negotiations Competition",
      "title_zh": "推进人工智能谈判：基于大规模自主谈判竞赛的新理论与实证",
      "authors": [
        "Michelle Vaccaro",
        "Michael Caoson",
        "Harang Ju",
        "Sinan Aral",
        "Jared R. Curhan"
      ],
      "abstract": "Despite the rapid proliferation of artificial intelligence (AI) negotiation\nagents, there has been limited integration of computer science research and\nestablished negotiation theory to develop new theories of AI negotiation. To\nbridge this gap, we conducted an International AI Negotiations Competition in\nwhich participants iteratively designed and refined prompts for large language\nmodel (LLM) negotiation agents. We then facilitated over 120,000 negotiations\nbetween these agents across multiple scenarios with diverse characteristics and\nobjectives. Our findings revealed that fundamental principles from established\nhuman-human negotiation theory remain crucial in AI-AI negotiations.\nSpecifically, agents exhibiting high warmth fostered higher counterpart\nsubjective value and reached deals more frequently, which enabled them to\ncreate and claim more value in integrative settings. However, conditional on\nreaching a deal, warm agents claimed less value while dominant agents claimed\nmore value. These results align with classic negotiation theory emphasizing\nrelationship-building, assertiveness, and preparation. Our analysis also\nrevealed unique dynamics in AI-AI negotiations not fully explained by\nnegotiation theory, particularly regarding the effectiveness of AI-specific\nstrategies like chain-of-thought reasoning and prompt injection. The agent that\nwon our competition implemented an approach that blended traditional\nnegotiation preparation frameworks with AI-specific methods. Together, these\nresults suggest the importance of establishing a new theory of AI negotiations\nthat integrates established negotiation theory with AI-specific strategies to\noptimize agent performance. Our research suggests this new theory must account\nfor the unique characteristics of autonomous agents and establish the\nconditions under which traditional negotiation theory applies in automated\nsettings.",
      "tldr_zh": "本研究通过大规模国际AI谈判竞赛，探讨了AI谈判代理的理论与实践。研究发现，传统人类谈判理论在AI-AI谈判中依然适用，例如高亲和力代理能提升对方主观价值并达成更多协议，但在协议中可能让渡更多价值。同时，AI特有的策略（如链式思维推理和提示注入）也展现出独特效果。研究提出，需要建立一种整合传统谈判理论与AI特有策略的新理论，以优化AI谈判代理的表现，并明确传统理论在自动化环境中的适用条件。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06416v1",
      "published_date": "2025-03-09 03:25:48 UTC",
      "updated_date": "2025-03-09 03:25:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:56.370834"
    },
    {
      "arxiv_id": "2503.06413v2",
      "title": "Swift Hydra: Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models",
      "title_zh": "Swift Hydra：基于多Mamba模型的自增强生成式异常检测框架",
      "authors": [
        "Nguyen Do",
        "Truc Nguyen",
        "Malik Hassanaly",
        "Raed Alharbi",
        "Jung Taek Seo",
        "My T. Thai"
      ],
      "abstract": "Despite a plethora of anomaly detection models developed over the years,\ntheir ability to generalize to unseen anomalies remains an issue, particularly\nin critical systems. This paper aims to address this challenge by introducing\nSwift Hydra, a new framework for training an anomaly detection method based on\ngenerative AI and reinforcement learning (RL). Through featuring an RL policy\nthat operates on the latent variables of a generative model, the framework\nsynthesizes novel and diverse anomaly samples that are capable of bypassing a\ndetection model. These generated synthetic samples are, in turn, used to\naugment the detection model, further improving its ability to handle\nchallenging anomalies. Swift Hydra also incorporates Mamba models structured as\na Mixture of Experts (MoE) to enable scalable adaptation of the number of Mamba\nexperts based on data complexity, effectively capturing diverse feature\ndistributions without increasing the model's inference time. Empirical\nevaluations on ADBench benchmark demonstrate that Swift Hydra outperforms other\nstate-of-the-art anomaly detection models while maintaining a relatively short\ninference time. From these results, our research highlights a new and\nauspicious paradigm of integrating RL and generative AI for advancing anomaly\ndetection.",
      "tldr_zh": "本文提出了Swift Hydra，一种基于生成式AI和强化学习(RL)的异常检测框架，旨在提升模型对未见异常的泛化能力。该框架通过RL策略在生成模型的潜在变量上操作，合成多样化的异常样本以增强检测模型的鲁棒性。同时，Swift Hydra采用基于Mamba模型的专家混合(MoE)结构，根据数据复杂度动态调整专家数量，有效捕捉多样特征分布且不增加推理时间。实验表明，Swift Hydra在ADBench基准测试中优于现有最先进模型，同时保持了较短的推理时间，为RL与生成式AI结合推动异常检测提供了新范式。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06413v2",
      "published_date": "2025-03-09 03:15:15 UTC",
      "updated_date": "2025-03-25 02:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:35:55.049202"
    },
    {
      "arxiv_id": "2503.06411v1",
      "title": "Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance",
      "title_zh": "解码黑箱：将道德想象力融入技术性人工智能治理",
      "authors": [
        "Krti Tallam"
      ],
      "abstract": "This paper examines the intricate interplay among AI safety, security, and\ngovernance by integrating technical systems engineering with principles of\nmoral imagination and ethical philosophy. Drawing on foundational insights from\nWeapons of Math Destruction and Thinking in Systems alongside contemporary\ndebates in AI ethics, we develop a comprehensive multi-dimensional framework\ndesigned to regulate AI technologies deployed in high-stakes domains such as\ndefense, finance, healthcare, and education. Our approach combines rigorous\ntechnical analysis, quantitative risk assessment, and normative evaluation to\nexpose systemic vulnerabilities inherent in opaque, black-box models. Detailed\ncase studies, including analyses of Microsoft Tay (2016) and the UK A-Level\nGrading Algorithm (2020), demonstrate how security lapses, bias amplification,\nand lack of accountability can precipitate cascading failures that undermine\npublic trust. We conclude by outlining targeted strategies for enhancing AI\nresilience through adaptive regulatory mechanisms, robust security protocols,\nand interdisciplinary oversight, thereby advancing the state of the art in\nethical and technical AI governance.",
      "tldr_zh": "这篇论文提出了一种将技术系统工程与道德想象力相结合的AI治理新框架，旨在解决高风险领域（如国防、金融、医疗和教育）中AI系统的安全与伦理问题。研究通过分析微软Tay（2016）和英国A-Level评分算法（2020）等案例，揭示了黑箱模型存在的系统性漏洞、偏见放大和问责缺失等问题。作者最终提出了通过自适应监管机制、强健安全协议和跨学科监督来增强AI韧性的具体策略，为AI伦理治理提供了技术解决方案。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06411v1",
      "published_date": "2025-03-09 03:11:32 UTC",
      "updated_date": "2025-03-09 03:11:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:36:17.251591"
    },
    {
      "arxiv_id": "2503.06410v1",
      "title": "Performant LLM Agentic Framework for Conversational AI",
      "title_zh": "高效的LLM智能体框架：面向对话式人工智能",
      "authors": [
        "Alex Casella",
        "Wayne Wang"
      ],
      "abstract": "The rise of Agentic applications and automation in the Voice AI industry has\nled to an increased reliance on Large Language Models (LLMs) to navigate\ngraph-based logic workflows composed of nodes and edges. However, existing\nmethods face challenges such as alignment errors in complex workflows and\nhallucinations caused by excessive context size. To address these limitations,\nwe introduce the Performant Agentic Framework (PAF), a novel system that\nassists LLMs in selecting appropriate nodes and executing actions in order when\ntraversing complex graphs. PAF combines LLM-based reasoning with a\nmathematically grounded vector scoring mechanism, achieving both higher\naccuracy and reduced latency. Our approach dynamically balances strict\nadherence to predefined paths with flexible node jumps to handle various user\ninputs efficiently. Experiments demonstrate that PAF significantly outperforms\nbaseline methods, paving the way for scalable, real-time Conversational AI\nsystems in complex business environments.",
      "tldr_zh": "该研究提出了Performant Agentic Framework (PAF)，一种新型对话AI代理框架，用于解决当前基于大语言模型(LLMs)的语音AI系统在复杂图逻辑工作流中面临的节点对齐错误和上下文幻觉问题。该框架创新性地结合了LLM推理与数学向量评分机制，通过动态平衡预设路径严格遵循与灵活节点跳转策略，显著提升了系统准确性和响应速度。实验表明，PAF在复杂商业环境中展现出优越性能，为可扩展的实时对话AI系统提供了有效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06410v1",
      "published_date": "2025-03-09 02:58:34 UTC",
      "updated_date": "2025-03-09 02:58:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:36:53.949560"
    },
    {
      "arxiv_id": "2503.06405v2",
      "title": "Heterogeneous bimodal attention fusion for speech emotion recognition",
      "title_zh": "异构双模态注意力融合用于语音情感识别",
      "authors": [
        "Jiachen Luo",
        "Huy Phan",
        "Lin Wang",
        "Joshua Reiss"
      ],
      "abstract": "Multi-modal emotion recognition in conversations is a challenging problem due\nto the complex and complementary interactions between different modalities.\nAudio and textual cues are particularly important for understanding emotions\nfrom a human perspective. Most existing studies focus on exploring interactions\nbetween audio and text modalities at the same representation level. However, a\ncritical issue is often overlooked: the heterogeneous modality gap between\nlow-level audio representations and high-level text representations. To address\nthis problem, we propose a novel framework called Heterogeneous Bimodal\nAttention Fusion (HBAF) for multi-level multi-modal interaction in\nconversational emotion recognition. The proposed method comprises three key\nmodules: the uni-modal representation module, the multi-modal fusion module,\nand the inter-modal contrastive learning module. The uni-modal representation\nmodule incorporates contextual content into low-level audio representations to\nbridge the heterogeneous multi-modal gap, enabling more effective fusion. The\nmulti-modal fusion module uses dynamic bimodal attention and a dynamic gating\nmechanism to filter incorrect cross-modal relationships and fully exploit both\nintra-modal and inter-modal interactions. Finally, the inter-modal contrastive\nlearning module captures complex absolute and relative interactions between\naudio and text modalities. Experiments on the MELD and IEMOCAP datasets\ndemonstrate that the proposed HBAF method outperforms existing state-of-the-art\nbaselines.",
      "tldr_zh": "该研究提出了一种新型的异质双模态注意力融合框架（HBAF），用于解决语音情感识别中音频和文本模态之间的异质性问题。该框架包含三个关键模块：单模态表示模块通过将上下文信息融入低层音频表征来缩小模态差异；多模态融合模块利用动态双模态注意力和门控机制过滤错误的跨模态关系，充分挖掘模态内和模态间的交互；跨模态对比学习模块则捕捉音频和文本模态之间的复杂绝对与相对关系。实验结果表明，HBAF在MELD和IEMOCAP数据集上均优于现有的最先进方法。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06405v2",
      "published_date": "2025-03-09 02:50:49 UTC",
      "updated_date": "2025-03-23 08:21:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:36:25.591476"
    },
    {
      "arxiv_id": "2503.06398v1",
      "title": "Causality Enhanced Origin-Destination Flow Prediction in Data-Scarce Cities",
      "title_zh": "数据稀缺城市中因果增强的起讫点流量预测",
      "authors": [
        "Tao Feng",
        "Yunke Zhang",
        "Huandong Wang",
        "Yong Li"
      ],
      "abstract": "Accurate origin-destination (OD) flow prediction is of great importance to\ndeveloping cities, as it can contribute to optimize urban structures and\nlayouts. However, with the common issues of missing regional features and\nlacking OD flow data, it is quite daunting to predict OD flow in developing\ncities. To address this challenge, we propose a novel Causality-Enhanced OD\nFlow Prediction (CE-OFP), a unified framework that aims to transfer urban\nknowledge between cities and achieve accuracy improvements in OD flow\npredictions across data-scarce cities. In specific, we propose a novel\nreinforcement learning model to discover universal causalities among urban\nfeatures in data-rich cities and build corresponding causal graphs. Then, we\nfurther build Causality-Enhanced Variational Auto-Encoder (CE-VAE) to\nincorporate causal graphs for effective feature reconstruction in data-scarce\ncities. Finally, with the reconstructed features, we devise a knowledge\ndistillation method with a graph attention network to migrate the OD prediction\nmodel from data-rich cities to data-scare cities. Extensive experiments on two\npairs of real-world datasets validate that the proposed CE-OFP remarkably\noutperforms state-of-the-art baselines, which can reduce the RMSE of OD flow\nprediction for data-scarce cities by up to 11%.",
      "tldr_zh": "本文提出了一种基于因果增强的OD流量预测框架(CE-OFP)，用于解决数据稀缺城市的流量预测难题。该框架创新性地利用强化学习从数据丰富城市挖掘城市特征间的因果关系，构建因果图，并通过因果增强变分自编码器(CE-VAE)实现特征重建。结合图注意力网络和知识蒸馏技术，成功将预测模型迁移至数据稀缺城市。实验表明，该方法能显著降低预测误差(RMSE最高减少11%)，为发展中国家的城市规划提供了有效工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06398v1",
      "published_date": "2025-03-09 02:36:36 UTC",
      "updated_date": "2025-03-09 02:36:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:37:07.698976"
    },
    {
      "arxiv_id": "2503.06396v1",
      "title": "Optimizing Minimum Vertex Cover Solving via a GCN-assisted Heuristic Algorithm",
      "title_zh": "基于图卷积网络辅助启发式算法优化最小顶点覆盖求解",
      "authors": [
        "Enqiang Zhu",
        "Qiqi Bao",
        "Yu Zhang",
        "Chanjuan Liu"
      ],
      "abstract": "The problem of finding a minimum vertex cover (MVC) in a graph is a\nwell-known NP-hard problem with significant practical applications in\noptimization and scheduling. Its complexity, combined with the increasing scale\nof problems, underscores the need for efficient and effective algorithms.\nHowever, existing heuristic algorithms for MVC often rely on simplistic\ninitialization strategies and overlook the impact of edge attributes and\nneighborhood information on vertex selection. In this paper, we introduce\nGCNIVC, a novel heuristic search algorithm designed to address the limitations\nof existing methods for solving MVC problems in large-scale graphs. Our\napproach features two main innovations. First, it utilizes a Graph\nConvolutional Network (GCN) to capture the global structure of graphs, which\nenables the generation of high-quality initial solutions that enhance the\nefficiency of the subsequent search process. Second, GCNIVC introduces a new\nheuristic that employs three containers and the concept of double-covered edges\n(dc-edges), improving search efficiency and providing greater flexibility for\nadding and removing operations based on edge attributes. Through extensive\nexperiments on benchmark datasets, we demonstrate that GCNIVC outperforms\nstate-of-the-art MVC algorithms in terms of both accuracy and efficiency. Our\nresults highlight the effectiveness of GCNIVC's GCN-assisted initialization and\nits edge-informed search strategy. This study not only advances the\nunderstanding of MVC problem-solving but also contributes a new tool for\naddressing large-scale graph optimization challenges.",
      "tldr_zh": "本研究提出了一种基于图卷积网络(GCN)辅助的启发式算法GCNIVC，用于优化最小顶点覆盖(MVC)问题的求解。该算法通过GCN捕捉图的全局结构，生成高质量初始解，并结合双覆盖边(dc-edges)概念设计了一种新启发式策略，提升了搜索效率。实验表明，GCNIVC在精度和效率上均优于现有MVC算法，为大规模图优化问题提供了新工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06396v1",
      "published_date": "2025-03-09 02:31:03 UTC",
      "updated_date": "2025-03-09 02:31:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:36:41.622330"
    },
    {
      "arxiv_id": "2503.06395v1",
      "title": "Causal Discovery and Inference towards Urban Elements and Associated Factors",
      "title_zh": "面向城市要素及其关联因素的因果发现与推理",
      "authors": [
        "Tao Feng",
        "Yunke Zhang",
        "Xiaochen Fan",
        "Huandong Wang",
        "Yong Li"
      ],
      "abstract": "To uncover the city's fundamental functioning mechanisms, it is important to\nacquire a deep understanding of complicated relationships among citizens,\nlocation, and mobility behaviors. Previous research studies have applied direct\ncorrelation analysis to investigate such relationships. Nevertheless, due to\nthe ubiquitous confounding effects, empirical correlation analysis may not\naccurately reflect underlying causal relationships among basic urban elements.\nIn this paper, we propose a novel urban causal computing framework to\ncomprehensively explore causalities and confounding effects among a variety of\nfactors across different types of urban elements. In particular, we design a\nreinforcement learning algorithm to discover the potential causal graph, which\ndepicts the causal relations between urban factors. The causal graph further\nserves as the guidance for estimating causal effects between pair-wise urban\nfactors by propensity score matching. After removing the confounding effects\nfrom correlations, we leverage significance levels of causal effects in\ndownstream urban mobility prediction tasks. Experimental studies on open-source\nurban datasets show that the discovered causal graph demonstrates a\nhierarchical structure, where citizens affect locations, and they both cause\nchanges in urban mobility behaviors. Experimental results in urban mobility\nprediction tasks further show that the proposed method can effectively reduce\nconfounding effects and enhance performance of urban computing tasks.",
      "tldr_zh": "本研究提出了一种新的城市因果计算框架，旨在揭示城市元素（如市民、地点和出行行为）之间的因果关系和混杂效应。通过设计强化学习算法，该方法发现了描述城市因素间因果关系的因果图，并利用倾向评分匹配估计成对城市因素的因果效应。实验表明，因果图呈现层次结构，其中市民影响地点，两者共同导致城市出行行为的变化。在城市出行预测任务中，该方法有效减少了混杂效应，提升了城市计算任务的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06395v1",
      "published_date": "2025-03-09 02:15:04 UTC",
      "updated_date": "2025-03-09 02:15:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:36:51.280574"
    },
    {
      "arxiv_id": "2503.06392v1",
      "title": "EPR-GAIL: An EPR-Enhanced Hierarchical Imitation Learning Framework to Simulate Complex User Consumption Behaviors",
      "title_zh": "EPR-GAIL：一种EPR增强的层次化模仿学习框架，用于模拟复杂用户消费行为",
      "authors": [
        "Tao Feng",
        "Yunke Zhang",
        "Huandong Wang",
        "Yong Li"
      ],
      "abstract": "User consumption behavior data, which records individuals' online spending\nhistory at various types of stores, has been widely used in various\napplications, such as store recommendation, site selection, and sale\nforecasting. However, its high worth is limited due to deficiencies in data\ncomprehensiveness and changes of application scenarios. Thus, generating\nhigh-quality sequential consumption data by simulating complex user consumption\nbehaviors is of great importance to real-world applications. Two branches of\nexisting sequence generation methods are both limited in quality. Model-based\nmethods with simplified assumptions fail to model the complex decision process\nof user consumption, while data-driven methods that emulate real-world data are\nprone to noises, unobserved behaviors, and dynamic decision space. In this\nwork, we propose to enhance the fidelity and trustworthiness of the data-driven\nGenerative Adversarial Imitation Learning (GAIL) method by blending it with the\nExploration and Preferential Return EPR model . The core idea of our EPR-GAIL\nframework is to model user consumption behaviors as a complex EPR decision\nprocess, which consists of purchase, exploration, and preference decisions.\nSpecifically, we design the hierarchical policy function in the generator as a\nrealization of the EPR decision process and employ the probability\ndistributions of the EPR model to guide the reward function in the\ndiscriminator. Extensive experiments on two real-world datasets of user\nconsumption behaviors on an online platform demonstrate that the EPR-GAIL\nframework outperforms the best state-of-the-art baseline by over 19\\% in terms\nof data fidelity. Furthermore, the generated consumption behavior data can\nimprove the performance of sale prediction and location recommendation by up to\n35.29% and 11.19%, respectively, validating its advantage for practical\napplications.",
      "tldr_zh": "本研究提出了EPR-GAIL框架，通过将探索与偏好返回模型(EPR)与生成对抗模仿学习(GAIL)相结合，模拟复杂的用户消费行为。该框架将用户消费行为建模为包含购买、探索和偏好决策的复杂EPR决策过程，并在生成器中设计分层策略函数，同时利用EPR模型的概率分布指导判别器的奖励函数。实验表明，EPR-GAIL在数据保真度上比现有最佳基线模型提高了19%，生成的消费行为数据显著提升了销售预测和位置推荐的性能，验证了其在实际应用中的优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06392v1",
      "published_date": "2025-03-09 01:56:42 UTC",
      "updated_date": "2025-03-09 01:56:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:36:58.465533"
    },
    {
      "arxiv_id": "2503.07667v2",
      "title": "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models",
      "title_zh": "CLIMB：大规模多模态临床基础模型的数据基础",
      "authors": [
        "Wei Dai",
        "Peilin Chen",
        "Malinda Lu",
        "Daniel Li",
        "Haowen Wei",
        "Hejie Cui",
        "Paul Pu Liang"
      ],
      "abstract": "Recent advances in clinical AI have enabled remarkable progress across many\nclinical domains. However, existing benchmarks and models are primarily limited\nto a small set of modalities and tasks, which hinders the development of\nlarge-scale multimodal methods that can make holistic assessments of patient\nhealth and well-being. To bridge this gap, we introduce Clinical Large-Scale\nIntegrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark\nunifying diverse clinical data across imaging, language, temporal, and graph\nmodalities. CLIMB comprises 4.51 million patient samples totaling 19.01\nterabytes distributed across 2D imaging, 3D video, time series, graphs, and\nmultimodal data. Through extensive empirical evaluation, we demonstrate that\nmultitask pretraining significantly improves performance on understudied\ndomains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis\nover single-task learning. Pretraining on CLIMB also effectively improves\nmodels' generalization capability to new tasks, and strong unimodal encoder\nperformance translates well to multimodal performance when paired with\ntask-appropriate fusion strategies. Our findings provide a foundation for new\narchitecture designs and pretraining strategies to advance clinical AI\nresearch. Code is released at https://github.com/DDVD233/climb.",
      "tldr_zh": "该研究提出了CLIMB（Clinical Large-Scale Integrative Multimodal Benchmark），一个整合了影像、语言、时序和图数据等多模态临床数据的大规模基准测试，包含451万患者样本（19.01TB数据）。实验表明，多任务预训练显著提升模型在超声（29%）和心电图（23%）等领域的性能，并有效增强模型对新任务的泛化能力。该基准为开发能全面评估患者健康的多模态临床基础模型提供了数据基础，推动了临床AI架构设计和预训练策略的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07667v2",
      "published_date": "2025-03-09 01:45:05 UTC",
      "updated_date": "2025-03-20 05:05:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:37:07.826005"
    },
    {
      "arxiv_id": "2503.06378v2",
      "title": "General Scales Unlock AI Evaluation with Explanatory and Predictive Power",
      "title_zh": "通用尺度解锁具备解释与预测能力的AI评估",
      "authors": [
        "Lexin Zhou",
        "Lorenzo Pacchiardi",
        "Fernando Martínez-Plumed",
        "Katherine M. Collins",
        "Yael Moros-Daval",
        "Seraphina Zhang",
        "Qinlin Zhao",
        "Yitian Huang",
        "Luning Sun",
        "Jonathan E. Prunty",
        "Zongqian Li",
        "Pablo Sánchez-García",
        "Kexin Jiang Chen",
        "Pablo A. M. Casares",
        "Jiyun Zu",
        "John Burden",
        "Behzad Mehrbakhsh",
        "David Stillwell",
        "Manuel Cebrian",
        "Jindong Wang",
        "Peter Henderson",
        "Sherry Tongshuang Wu",
        "Patrick C. Kyllonen",
        "Lucy Cheke",
        "Xing Xie",
        "José Hernández-Orallo"
      ],
      "abstract": "Ensuring safe and effective use of AI requires understanding and anticipating\nits performance on novel tasks, from advanced scientific challenges to\ntransformed workplace activities. So far, benchmarking has guided progress in\nAI, but it has offered limited explanatory and predictive power for\ngeneral-purpose AI systems, given the low transferability across diverse tasks.\nIn this paper, we introduce general scales for AI evaluation that can explain\nwhat common AI benchmarks really measure, extract ability profiles of AI\nsystems, and predict their performance for new task instances, in- and\nout-of-distribution. Our fully-automated methodology builds on 18 newly-crafted\nrubrics that place instance demands on general scales that do not saturate.\nIllustrated for 15 large language models and 63 tasks, high explanatory power\nis unleashed from inspecting the demand and ability profiles, bringing insights\non the sensitivity and specificity exhibited by different benchmarks, and how\nknowledge, metacognition and reasoning are affected by model size,\nchain-of-thought and distillation. Surprisingly, high predictive power at the\ninstance level becomes possible using these demand levels, providing superior\nestimates over black-box baseline predictors based on embeddings or finetuning,\nespecially in out-of-distribution settings (new tasks and new benchmarks). The\nscales, rubrics, battery, techniques and results presented here represent a\nmajor step for AI evaluation, underpinning the reliable deployment of AI in the\nyears ahead. (Collaborative platform:\nhttps://kinds-of-intelligence-cfi.github.io/ADELE.)",
      "tldr_zh": "该研究提出了一种具有解释性和预测力的AI通用评估体系（general scales），用于解决当前基准测试在通用AI系统评估上的局限性。通过18个新设计的评估标准（rubrics），该方法能解析AI基准测试的实际测量内容、提取AI系统的能力特征，并预测其在分布内外新任务中的表现。实验表明，该体系在15个大语言模型和63项任务上展现出卓越的解释力，能深入分析模型规模、思维链和蒸馏等技术对知识、元认知和推理能力的影响。尤其值得注意的是，该方法在分布外场景（新任务和新基准）中实现了优于基于嵌入或微调的黑盒预测模型的性能，为AI系统的可靠部署提供了重要支撑。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06378v2",
      "published_date": "2025-03-09 01:13:56 UTC",
      "updated_date": "2025-03-16 02:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:37:16.312586"
    },
    {
      "arxiv_id": "2503.06368v1",
      "title": "VORTEX: Challenging CNNs at Texture Recognition by using Vision Transformers with Orderless and Randomized Token Encodings",
      "title_zh": "VORTEX：通过无序随机标记编码的视觉Transformer挑战CNN在纹理识别领域的性能",
      "authors": [
        "Leonardo Scabini",
        "Kallil M. Zielinski",
        "Emir Konuk",
        "Ricardo T. Fares",
        "Lucas C. Ribas",
        "Kevin Smith",
        "Odemir M. Bruno"
      ],
      "abstract": "Texture recognition has recently been dominated by ImageNet-pre-trained deep\nConvolutional Neural Networks (CNNs), with specialized modifications and\nfeature engineering required to achieve state-of-the-art (SOTA) performance.\nHowever, although Vision Transformers (ViTs) were introduced a few years ago,\nlittle is known about their texture recognition ability. Therefore, in this\nwork, we introduce VORTEX (ViTs with Orderless and Randomized Token Encodings\nfor Texture Recognition), a novel method that enables the effective use of ViTs\nfor texture analysis. VORTEX extracts multi-depth token embeddings from\npre-trained ViT backbones and employs a lightweight module to aggregate\nhierarchical features and perform orderless encoding, obtaining a better image\nrepresentation for texture recognition tasks. This approach allows seamless\nintegration with any ViT with the common transformer architecture. Moreover, no\nfine-tuning of the backbone is performed, since they are used only as frozen\nfeature extractors, and the features are fed to a linear SVM. We evaluate\nVORTEX on nine diverse texture datasets, demonstrating its ability to achieve\nor surpass SOTA performance in a variety of texture analysis scenarios. By\nbridging the gap between texture recognition with CNNs and transformer-based\narchitectures, VORTEX paves the way for adopting emerging transformer\nfoundation models. Furthermore, VORTEX demonstrates robust computational\nefficiency when coupled with ViT backbones compared to CNNs with similar costs.\nThe method implementation and experimental scripts are publicly available in\nour online repository.",
      "tldr_zh": "该论文提出VORTEX方法，首次系统探索了Vision Transformers（ViTs）在纹理识别任务中的应用潜力。该方法通过提取预训练ViT的多层次token嵌入，结合轻量级模块实现层级特征聚合和无序编码，无需微调主干网络即可生成优化的纹理表征。在九个纹理数据集上的实验表明，VORTEX不仅达到或超越当前基于CNN的SOTA方法，还展现出与计算成本相当的CNN相比更优的运行效率，为基于transformer基础模型的纹理分析开辟了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06368v1",
      "published_date": "2025-03-09 00:36:02 UTC",
      "updated_date": "2025-03-09 00:36:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:37:22.069829"
    },
    {
      "arxiv_id": "2503.06366v1",
      "title": "Machine Learning meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics",
      "title_zh": "机器学习邂逅代数组合学：一套捕获纯数学研究级猜想能力的数据集",
      "authors": [
        "Herman Chau",
        "Helen Jenne",
        "Davis Brown",
        "Jesse He",
        "Mark Raugas",
        "Sara Billey",
        "Henry Kvinge"
      ],
      "abstract": "With recent dramatic increases in AI system capabilities, there has been\ngrowing interest in utilizing machine learning for reasoning-heavy,\nquantitative tasks, particularly mathematics. While there are many resources\ncapturing mathematics at the high-school, undergraduate, and graduate level,\nthere are far fewer resources available that align with the level of difficulty\nand open endedness encountered by professional mathematicians working on open\nproblems. To address this, we introduce a new collection of datasets, the\nAlgebraic Combinatorics Dataset Repository (ACD Repo), representing either\nfoundational results or open problems in algebraic combinatorics, a subfield of\nmathematics that studies discrete structures arising from abstract algebra.\nFurther differentiating our dataset collection is the fact that it aims at the\nconjecturing process. Each dataset includes an open-ended research-level\nquestion and a large collection of examples (up to 10M in some cases) from\nwhich conjectures should be generated. We describe all nine datasets, the\ndifferent ways machine learning models can be applied to them (e.g., training\nwith narrow models followed by interpretability analysis or program synthesis\nwith LLMs), and discuss some of the challenges involved in designing datasets\nlike these.",
      "tldr_zh": "该研究推出了代数组合学数据集库（ACD Repo），填补了机器学习与纯数学研究级问题之间的数据空白。该资源包含9个数据集，涵盖代数组合学领域的基础结果和开放问题，每个数据集都包含开放式研究问题及海量示例（部分达1000万条），专门用于数学猜想生成任务。区别于现有教育资源，该数据集直接对标专业数学家的研究难度，支持多种机器学习方法应用（如可解释性分析和LLM程序合成），为AI参与前沿数学研究提供了基准测试平台。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.CO",
        "math.RT"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, comments welcome",
      "pdf_url": "http://arxiv.org/pdf/2503.06366v1",
      "published_date": "2025-03-09 00:11:40 UTC",
      "updated_date": "2025-03-09 00:11:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:37:35.314094"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 91,
  "processed_papers_count": 91,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T04:38:51.150506"
}